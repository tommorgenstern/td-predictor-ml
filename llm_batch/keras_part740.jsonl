{"custom_id": "keras#95a20df758dc44e6f04e533acf03571722131867", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 235 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 6 | Complexity Δ (Sum/Max): 15/15 | Churn Δ: 235 | Churn Cumulative: 235 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,235 @@\n+from keras_core.backend.common.backend_utils import (\n+    _convert_conv_tranpose_padding_args_from_keras_to_jax,\n+)\n+from keras_core.backend.common.backend_utils import (\n+    _convert_conv_tranpose_padding_args_from_keras_to_torch,\n+)\n+from keras_core.backend.common.backend_utils import (\n+    _get_output_shape_given_tf_padding,\n+)\n+from keras_core.backend.common.backend_utils import (\n+    compute_conv_transpose_padding_args_for_jax,\n+)\n+from keras_core.backend.common.backend_utils import (\n+    compute_conv_transpose_padding_args_for_torch,\n+)\n+from keras_core.testing import test_case\n+\n+\n+class ConvertConvTransposePaddingArgsJAXTest(test_case.TestCase):\n+    def test_valid_padding_without_output_padding(self):\n+        \"\"\"Test conversion with 'valid' padding and no output padding\"\"\"\n+        (\n+            left_pad,\n+            right_pad,\n+        ) = _convert_conv_tranpose_padding_args_from_keras_to_jax(\n+            kernel_size=3,\n+            stride=2,\n+            dilation_rate=1,\n+            padding=\"valid\",\n+            output_padding=None,\n+        )\n+        self.assertEqual(left_pad, 2)\n+        self.assertEqual(right_pad, 2)\n+\n+    def test_same_padding_without_output_padding(self):\n+        \"\"\"Test conversion with 'same' padding and no output padding.\"\"\"\n+        (\n+            left_pad,\n+            right_pad,\n+        ) = _convert_conv_tranpose_padding_args_from_keras_to_jax(\n+            kernel_size=3,\n+            stride=2,\n+            dilation_rate=1,\n+            padding=\"same\",\n+            output_padding=None,\n+        )\n+        self.assertEqual(left_pad, 2)\n+        self.assertEqual(right_pad, 1)\n+\n+\n+class ConvertConvTransposePaddingArgsTorchTest(test_case.TestCase):\n+    def test_valid_padding_without_output_padding(self):\n+        \"\"\"Test conversion with 'valid' padding and no output padding\"\"\"\n+        (\n+            torch_padding,\n+            torch_output_padding,\n+        ) = _convert_conv_tranpose_padding_args_from_keras_to_torch(\n+            kernel_size=3,\n+            stride=2,\n+            dilation_rate=1,\n+            padding=\"valid\",\n+            output_padding=None,\n+        )\n+        self.assertEqual(torch_padding, 0)\n+        self.assertEqual(torch_output_padding, 0)\n+\n+    def test_same_padding_without_output_padding(self):\n+        \"\"\"Test conversion with 'same' padding and no output padding\"\"\"\n+        (\n+            torch_padding,\n+            torch_output_padding,\n+        ) = _convert_conv_tranpose_padding_args_from_keras_to_torch(\n+            kernel_size=3,\n+            stride=2,\n+            dilation_rate=1,\n+            padding=\"same\",\n+            output_padding=None,\n+        )\n+        self.assertEqual(torch_padding, 1)\n+        self.assertEqual(torch_output_padding, 1)\n+\n+\n+class ComputeConvTransposePaddingArgsForJAXTest(test_case.TestCase):\n+    def test_valid_padding_without_output_padding(self):\n+        \"\"\"Test computation with 'valid' padding and no output padding\"\"\"\n+        jax_padding = compute_conv_transpose_padding_args_for_jax(\n+            input_shape=(1, 5, 5, 3),\n+            kernel_shape=(3, 3, 3, 3),\n+            strides=2,\n+            padding=\"valid\",\n+            output_padding=None,\n+            dilation_rate=1,\n+        )\n+        self.assertEqual(jax_padding, [(2, 2), (2, 2)])\n+\n+    def test_same_padding_without_output_padding(self):\n+        \"\"\"Test computation with 'same' padding and no output padding\"\"\"\n+        jax_padding = compute_conv_transpose_padding_args_for_jax(\n+            input_shape=(1, 5, 5, 3),\n+            kernel_shape=(3, 3, 3, 3),\n+            strides=2,\n+            padding=\"same\",\n+            output_padding=None,\n+            dilation_rate=1,\n+        )\n+\n+        self.assertEqual(jax_padding, [(2, 1), (2, 1)])\n+\n+\n+class ComputeConvTransposePaddingArgsForTorchTest(test_case.TestCase):\n+    def test_valid_padding_without_output_padding(self):\n+        \"\"\"Test computation with 'valid' padding and no output padding\"\"\"\n+        (\n+            torch_paddings,\n+            torch_output_paddings,\n+        ) = compute_conv_transpose_padding_args_for_torch(\n+            input_shape=(1, 5, 5, 3),\n+            kernel_shape=(3, 3, 3, 3),\n+            strides=2,\n+            padding=\"valid\",\n+            output_padding=None,\n+            dilation_rate=1,\n+        )\n+        self.assertEqual(torch_paddings, [0, 0])\n+        self.assertEqual(torch_output_paddings, [0, 0])\n+\n+    def test_same_padding_without_output_padding(self):\n+        \"\"\"Test computation with 'same' padding and no output padding\"\"\"\n+        (\n+            torch_paddings,\n+            torch_output_paddings,\n+        ) = compute_conv_transpose_padding_args_for_torch(\n+            input_shape=(1, 5, 5, 3),\n+            kernel_shape=(3, 3, 3, 3),\n+            strides=2,\n+            padding=\"same\",\n+            output_padding=None,\n+            dilation_rate=1,\n+        )\n+        self.assertEqual(torch_paddings, [1, 1])\n+        self.assertEqual(torch_output_paddings, [1, 1])\n+\n+    def test_valid_padding_with_none_output_padding(self):\n+        \"\"\"Test conversion with 'valid' padding and no output padding\"\"\"\n+        (\n+            torch_padding,\n+            torch_output_padding,\n+        ) = _convert_conv_tranpose_padding_args_from_keras_to_torch(\n+            kernel_size=3,\n+            stride=2,\n+            dilation_rate=1,\n+            padding=\"valid\",\n+            output_padding=None,\n+        )\n+        self.assertEqual(torch_padding, 0)\n+        self.assertEqual(torch_output_padding, 0)\n+\n+    def test_valid_padding_with_output_padding(self):\n+        \"\"\"Test conversion with 'valid' padding and output padding for Torch.\"\"\"\n+        (\n+            torch_padding,\n+            torch_output_padding,\n+        ) = _convert_conv_tranpose_padding_args_from_keras_to_torch(\n+            kernel_size=3,\n+            stride=2,\n+            dilation_rate=1,\n+            padding=\"valid\",\n+            output_padding=1,\n+        )\n+        self.assertEqual(torch_padding, 0)\n+        self.assertEqual(torch_output_padding, 1)\n+\n+\n+class GetOutputShapeGivenTFPaddingTest(test_case.TestCase):\n+    def test_valid_padding_without_output_padding(self):\n+        \"\"\"Test computation with 'valid' padding and no output padding.\"\"\"\n+        output_shape = _get_output_shape_given_tf_padding(\n+            input_size=5,\n+            kernel_size=3,\n+            strides=2,\n+            padding=\"valid\",\n+            output_padding=None,\n+            dilation_rate=1,\n+        )\n+        self.assertEqual(output_shape, 11)\n+\n+    def test_same_padding_without_output_padding(self):\n+        \"\"\"Test computation with 'same' padding and no output padding.\"\"\"\n+        output_shape = _get_output_shape_given_tf_padding(\n+            input_size=5,\n+            kernel_size=3,\n+            strides=2,\n+            padding=\"same\",\n+            output_padding=None,\n+            dilation_rate=1,\n+        )\n+        self.assertEqual(output_shape, 10)\n+\n+    def test_valid_padding_with_output_padding(self):\n+        \"\"\"Test computation with 'valid' padding and output padding.\"\"\"\n+        output_shape = _get_output_shape_given_tf_padding(\n+            input_size=5,\n+            kernel_size=3,\n+            strides=2,\n+            padding=\"valid\",\n+            output_padding=1,\n+            dilation_rate=1,\n+        )\n+        self.assertEqual(output_shape, 12)\n+\n+    def test_warning_for_inconsistencies(self):\n+        \"\"\"Test that a warning is raised for potential inconsistencies\"\"\"\n+        with self.assertWarns(Warning):\n+            _convert_conv_tranpose_padding_args_from_keras_to_torch(\n+                kernel_size=3,\n+                stride=2,\n+                dilation_rate=1,\n+                padding=\"same\",\n+                output_padding=1,\n+            )\n+\n+    def test_same_padding_without_output_padding_for_torch_(self):\n+        \"\"\"Test conversion with 'same' padding and no output padding.\"\"\"\n+        (\n+            torch_padding,\n+            torch_output_padding,\n+        ) = _convert_conv_tranpose_padding_args_from_keras_to_torch(\n+            kernel_size=3,\n+            stride=2,\n+            dilation_rate=1,\n+            padding=\"same\",\n+            output_padding=None,\n+        )\n+        self.assertEqual(torch_padding, max(-((3 % 2 - 3) // 2), 0))\n+        self.assertEqual(torch_output_padding, 1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#10d0349a754062ed61f7fd76ba12aee14cf1c42a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 235 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 6 | Complexity Δ (Sum/Max): 15/15 | Churn Δ: 235 | Churn Cumulative: 705 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,235 @@\n+from keras_core.backend.common.backend_utils import (\n+    _convert_conv_tranpose_padding_args_from_keras_to_jax,\n+)\n+from keras_core.backend.common.backend_utils import (\n+    _convert_conv_tranpose_padding_args_from_keras_to_torch,\n+)\n+from keras_core.backend.common.backend_utils import (\n+    _get_output_shape_given_tf_padding,\n+)\n+from keras_core.backend.common.backend_utils import (\n+    compute_conv_transpose_padding_args_for_jax,\n+)\n+from keras_core.backend.common.backend_utils import (\n+    compute_conv_transpose_padding_args_for_torch,\n+)\n+from keras_core.testing import test_case\n+\n+\n+class ConvertConvTransposePaddingArgsJAXTest(test_case.TestCase):\n+    def test_valid_padding_without_output_padding(self):\n+        \"\"\"Test conversion with 'valid' padding and no output padding\"\"\"\n+        (\n+            left_pad,\n+            right_pad,\n+        ) = _convert_conv_tranpose_padding_args_from_keras_to_jax(\n+            kernel_size=3,\n+            stride=2,\n+            dilation_rate=1,\n+            padding=\"valid\",\n+            output_padding=None,\n+        )\n+        self.assertEqual(left_pad, 2)\n+        self.assertEqual(right_pad, 2)\n+\n+    def test_same_padding_without_output_padding(self):\n+        \"\"\"Test conversion with 'same' padding and no output padding.\"\"\"\n+        (\n+            left_pad,\n+            right_pad,\n+        ) = _convert_conv_tranpose_padding_args_from_keras_to_jax(\n+            kernel_size=3,\n+            stride=2,\n+            dilation_rate=1,\n+            padding=\"same\",\n+            output_padding=None,\n+        )\n+        self.assertEqual(left_pad, 2)\n+        self.assertEqual(right_pad, 1)\n+\n+\n+class ConvertConvTransposePaddingArgsTorchTest(test_case.TestCase):\n+    def test_valid_padding_without_output_padding(self):\n+        \"\"\"Test conversion with 'valid' padding and no output padding\"\"\"\n+        (\n+            torch_padding,\n+            torch_output_padding,\n+        ) = _convert_conv_tranpose_padding_args_from_keras_to_torch(\n+            kernel_size=3,\n+            stride=2,\n+            dilation_rate=1,\n+            padding=\"valid\",\n+            output_padding=None,\n+        )\n+        self.assertEqual(torch_padding, 0)\n+        self.assertEqual(torch_output_padding, 0)\n+\n+    def test_same_padding_without_output_padding(self):\n+        \"\"\"Test conversion with 'same' padding and no output padding\"\"\"\n+        (\n+            torch_padding,\n+            torch_output_padding,\n+        ) = _convert_conv_tranpose_padding_args_from_keras_to_torch(\n+            kernel_size=3,\n+            stride=2,\n+            dilation_rate=1,\n+            padding=\"same\",\n+            output_padding=None,\n+        )\n+        self.assertEqual(torch_padding, 1)\n+        self.assertEqual(torch_output_padding, 1)\n+\n+\n+class ComputeConvTransposePaddingArgsForJAXTest(test_case.TestCase):\n+    def test_valid_padding_without_output_padding(self):\n+        \"\"\"Test computation with 'valid' padding and no output padding\"\"\"\n+        jax_padding = compute_conv_transpose_padding_args_for_jax(\n+            input_shape=(1, 5, 5, 3),\n+            kernel_shape=(3, 3, 3, 3),\n+            strides=2,\n+            padding=\"valid\",\n+            output_padding=None,\n+            dilation_rate=1,\n+        )\n+        self.assertEqual(jax_padding, [(2, 2), (2, 2)])\n+\n+    def test_same_padding_without_output_padding(self):\n+        \"\"\"Test computation with 'same' padding and no output padding\"\"\"\n+        jax_padding = compute_conv_transpose_padding_args_for_jax(\n+            input_shape=(1, 5, 5, 3),\n+            kernel_shape=(3, 3, 3, 3),\n+            strides=2,\n+            padding=\"same\",\n+            output_padding=None,\n+            dilation_rate=1,\n+        )\n+\n+        self.assertEqual(jax_padding, [(2, 1), (2, 1)])\n+\n+\n+class ComputeConvTransposePaddingArgsForTorchTest(test_case.TestCase):\n+    def test_valid_padding_without_output_padding(self):\n+        \"\"\"Test computation with 'valid' padding and no output padding\"\"\"\n+        (\n+            torch_paddings,\n+            torch_output_paddings,\n+        ) = compute_conv_transpose_padding_args_for_torch(\n+            input_shape=(1, 5, 5, 3),\n+            kernel_shape=(3, 3, 3, 3),\n+            strides=2,\n+            padding=\"valid\",\n+            output_padding=None,\n+            dilation_rate=1,\n+        )\n+        self.assertEqual(torch_paddings, [0, 0])\n+        self.assertEqual(torch_output_paddings, [0, 0])\n+\n+    def test_same_padding_without_output_padding(self):\n+        \"\"\"Test computation with 'same' padding and no output padding\"\"\"\n+        (\n+            torch_paddings,\n+            torch_output_paddings,\n+        ) = compute_conv_transpose_padding_args_for_torch(\n+            input_shape=(1, 5, 5, 3),\n+            kernel_shape=(3, 3, 3, 3),\n+            strides=2,\n+            padding=\"same\",\n+            output_padding=None,\n+            dilation_rate=1,\n+        )\n+        self.assertEqual(torch_paddings, [1, 1])\n+        self.assertEqual(torch_output_paddings, [1, 1])\n+\n+    def test_valid_padding_with_none_output_padding(self):\n+        \"\"\"Test conversion with 'valid' padding and no output padding\"\"\"\n+        (\n+            torch_padding,\n+            torch_output_padding,\n+        ) = _convert_conv_tranpose_padding_args_from_keras_to_torch(\n+            kernel_size=3,\n+            stride=2,\n+            dilation_rate=1,\n+            padding=\"valid\",\n+            output_padding=None,\n+        )\n+        self.assertEqual(torch_padding, 0)\n+        self.assertEqual(torch_output_padding, 0)\n+\n+    def test_valid_padding_with_output_padding(self):\n+        \"\"\"Test conversion with 'valid' padding and output padding for Torch.\"\"\"\n+        (\n+            torch_padding,\n+            torch_output_padding,\n+        ) = _convert_conv_tranpose_padding_args_from_keras_to_torch(\n+            kernel_size=3,\n+            stride=2,\n+            dilation_rate=1,\n+            padding=\"valid\",\n+            output_padding=1,\n+        )\n+        self.assertEqual(torch_padding, 0)\n+        self.assertEqual(torch_output_padding, 1)\n+\n+\n+class GetOutputShapeGivenTFPaddingTest(test_case.TestCase):\n+    def test_valid_padding_without_output_padding(self):\n+        \"\"\"Test computation with 'valid' padding and no output padding.\"\"\"\n+        output_shape = _get_output_shape_given_tf_padding(\n+            input_size=5,\n+            kernel_size=3,\n+            strides=2,\n+            padding=\"valid\",\n+            output_padding=None,\n+            dilation_rate=1,\n+        )\n+        self.assertEqual(output_shape, 11)\n+\n+    def test_same_padding_without_output_padding(self):\n+        \"\"\"Test computation with 'same' padding and no output padding.\"\"\"\n+        output_shape = _get_output_shape_given_tf_padding(\n+            input_size=5,\n+            kernel_size=3,\n+            strides=2,\n+            padding=\"same\",\n+            output_padding=None,\n+            dilation_rate=1,\n+        )\n+        self.assertEqual(output_shape, 10)\n+\n+    def test_valid_padding_with_output_padding(self):\n+        \"\"\"Test computation with 'valid' padding and output padding.\"\"\"\n+        output_shape = _get_output_shape_given_tf_padding(\n+            input_size=5,\n+            kernel_size=3,\n+            strides=2,\n+            padding=\"valid\",\n+            output_padding=1,\n+            dilation_rate=1,\n+        )\n+        self.assertEqual(output_shape, 12)\n+\n+    def test_warning_for_inconsistencies(self):\n+        \"\"\"Test that a warning is raised for potential inconsistencies\"\"\"\n+        with self.assertWarns(Warning):\n+            _convert_conv_tranpose_padding_args_from_keras_to_torch(\n+                kernel_size=3,\n+                stride=2,\n+                dilation_rate=1,\n+                padding=\"same\",\n+                output_padding=1,\n+            )\n+\n+    def test_same_padding_without_output_padding_for_torch_(self):\n+        \"\"\"Test conversion with 'same' padding and no output padding.\"\"\"\n+        (\n+            torch_padding,\n+            torch_output_padding,\n+        ) = _convert_conv_tranpose_padding_args_from_keras_to_torch(\n+            kernel_size=3,\n+            stride=2,\n+            dilation_rate=1,\n+            padding=\"same\",\n+            output_padding=None,\n+        )\n+        self.assertEqual(torch_padding, max(-((3 % 2 - 3) // 2), 0))\n+        self.assertEqual(torch_output_padding, 1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b9be76a13a1edbea5e17a54bf91da386323222bf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5788 | Lines Deleted: 5788 | Files Changed: 697 | Hunks: 2698 | Methods Changed: 382 | Complexity Δ (Sum/Max): 11524/811 | Churn Δ: 11576 | Churn Cumulative: 287176 | Contributors (this commit): 242 | Commits (past 90d): 984 | Contributors (cumulative): 2335 | DMM Complexity: None\n\nDIFF:\n@@ -4,7 +4,7 @@ import numpy as np\n import tensorflow as tf\n from absl import flags\n \n-import keras_core\n+import keras\n \n FLAGS = flags.FLAGS\n \n@@ -66,7 +66,7 @@ class BenchmarkMetricsCallback:\n             self.state[\"throughput\"] = throughput\n \n \n-class KerasCoreBenchmarkMetricsCallback(keras_core.callbacks.Callback):\n+class KerasCoreBenchmarkMetricsCallback(keras.callbacks.Callback):\n     def __init__(self, start_batch=1, stop_batch=None):\n         self._callback = BenchmarkMetricsCallback(start_batch, stop_batch)\n \n@@ -108,21 +108,21 @@ class LayerBenchmark:\n         input_shape,\n         flat_call_inputs=True,\n         jit_compile=True,\n-        keras_core_layer=None,\n+        keras_layer=None,\n         tf_keras_layer=None,\n     ):\n         self.layer_name = layer_name\n-        _keras_core_layer_class = getattr(keras_core.layers, layer_name)\n+        _keras_layer_class = getattr(keras.layers, layer_name)\n         _tf_keras_layer_class = getattr(tf.keras.layers, layer_name)\n \n-        if keras_core_layer is None:\n-            # Sometimes you want to initialize the keras_core layer and tf_keras\n+        if keras_layer is None:\n+            # Sometimes you want to initialize the keras layer and tf_keras\n             # layer in a different way. For example, `Bidirectional` layer,\n-            # which takes in `keras_core.layers.Layer` and\n+            # which takes in `keras.layers.Layer` and\n             # `tf.keras.layer.Layer` separately.\n-            self._keras_core_layer = _keras_core_layer_class(**init_args)\n+            self._keras_layer = _keras_layer_class(**init_args)\n         else:\n-            self._keras_core_layer = keras_core_layer\n+            self._keras_layer = keras_layer\n \n         if tf_keras_layer is None:\n             self._tf_keras_layer = _tf_keras_layer_class(**init_args)\n@@ -130,14 +130,14 @@ class LayerBenchmark:\n             self._tf_keras_layer = tf_keras_layer\n \n         self.input_shape = input_shape\n-        self._keras_core_model = self._build_keras_core_model(\n+        self._keras_model = self._build_keras_model(\n             input_shape, flat_call_inputs\n         )\n         self._tf_keras_model = self._build_tf_keras_model(\n             input_shape, flat_call_inputs\n         )\n \n-        self._keras_core_model.compile(\n+        self._keras_model.compile(\n             loss=\"mse\", optimizer=\"sgd\", jit_compile=jit_compile\n         )\n         self._tf_keras_model.compile(\n@@ -148,19 +148,19 @@ class LayerBenchmark:\n         self.jit_compile = jit_compile\n         self.input_shape = input_shape\n \n-    def _build_keras_core_model(self, input_shape, flat_call_inputs=True):\n+    def _build_keras_model(self, input_shape, flat_call_inputs=True):\n         inputs = []\n         if not isinstance(input_shape[0], (tuple, list)):\n             input_shape = [input_shape]\n \n         for shape in input_shape:\n-            inputs.append(keras_core.Input(shape=shape))\n+            inputs.append(keras.Input(shape=shape))\n \n         if flat_call_inputs:\n-            outputs = self._keras_core_layer(*inputs)\n+            outputs = self._keras_layer(*inputs)\n         else:\n-            outputs = self._keras_core_layer(inputs)\n-        return keras_core.Model(inputs=inputs, outputs=outputs)\n+            outputs = self._keras_layer(inputs)\n+        return keras.Model(inputs=inputs, outputs=outputs)\n \n     def _build_tf_keras_model(self, input_shape, flat_call_inputs=True):\n         inputs = []\n@@ -195,7 +195,7 @@ class LayerBenchmark:\n             stop_batch=num_iterations\n         )\n \n-        self._keras_core_model.predict(\n+        self._keras_model.predict(\n             data,\n             batch_size=batch_size,\n             callbacks=[callback],\n@@ -207,15 +207,15 @@ class LayerBenchmark:\n             callbacks=[tf_keras_callback],\n         )\n \n-        keras_core_throughput = (\n+        keras_throughput = (\n             callback._callback.state[\"throughput\"] * batch_size\n         )\n         tf_keras_throughput = (\n             tf_keras_callback._callback.state[\"throughput\"] * batch_size\n         )\n         print(\n-            f\"Keras Core throughput of forward pass of {self.layer_name}: \"\n-            f\"{keras_core_throughput:.2f} samples/sec.\"\n+            f\"Keras 3 throughput of forward pass of {self.layer_name}: \"\n+            f\"{keras_throughput:.2f} samples/sec.\"\n         )\n         print(\n             f\"TF Keras throughput of forward pass of {self.layer_name}: \"\n@@ -240,15 +240,15 @@ class LayerBenchmark:\n             if self.flat_call_inputs:\n                 # Scale by a small factor to avoid zero gradients.\n                 label = (\n-                    keras_core.backend.convert_to_numpy(\n-                        self._keras_core_layer(*data)\n+                    keras.backend.convert_to_numpy(\n+                        self._keras_layer(*data)\n                     )\n                     * 1.001\n                 )\n             else:\n                 label = (\n-                    keras_core.backend.convert_to_numpy(\n-                        self._keras_core_layer(data)\n+                    keras.backend.convert_to_numpy(\n+                        self._keras_layer(data)\n                     )\n                     * 1.001\n                 )\n@@ -259,7 +259,7 @@ class LayerBenchmark:\n             stop_batch=num_iterations\n         )\n \n-        self._keras_core_model.fit(\n+        self._keras_model.fit(\n             data,\n             label,\n             batch_size=batch_size,\n@@ -272,15 +272,15 @@ class LayerBenchmark:\n             callbacks=[tf_keras_callback],\n         )\n \n-        keras_core_throughput = (\n+        keras_throughput = (\n             callback._callback.state[\"throughput\"] * batch_size\n         )\n         tf_keras_throughput = (\n             tf_keras_callback._callback.state[\"throughput\"] * batch_size\n         )\n         print(\n-            f\"Keras Core throughput of forward & backward pass of \"\n-            f\"{self.layer_name}: {keras_core_throughput:.2f} samples/sec.\"\n+            f\"Keras 3 throughput of forward & backward pass of \"\n+            f\"{self.layer_name}: {keras_throughput:.2f} samples/sec.\"\n         )\n         print(\n             f\"TF Keras  throughput of forward & backward pass of \"\n\n@@ -16,7 +16,7 @@ import tensorflow as tf\n from absl import app\n from absl import flags\n \n-import keras_core\n+import keras\n from benchmarks.layer_benchmark.base_benchmark import LayerBenchmark\n \n FLAGS = flags.FLAGS\n@@ -194,8 +194,8 @@ def benchmark_bidirectional(\n ):\n     layer_name = \"Bidirectional\"\n     init_args = {}\n-    keras_core_layer = keras_core.layers.Bidirectional(\n-        keras_core.layers.LSTM(32)\n+    keras_layer = keras.layers.Bidirectional(\n+        keras.layers.LSTM(32)\n     )\n     tf_keras_layer = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))\n     benchmark = LayerBenchmark(\n@@ -203,7 +203,7 @@ def benchmark_bidirectional(\n         init_args,\n         input_shape=[256, 256],\n         jit_compile=jit_compile,\n-        keras_core_layer=keras_core_layer,\n+        keras_layer=keras_layer,\n         tf_keras_layer=tf_keras_layer,\n     )\n \n@@ -225,8 +225,8 @@ def benchmark_time_distributed(\n ):\n     layer_name = \"TimeDistributed\"\n     init_args = {}\n-    keras_core_layer = keras_core.layers.TimeDistributed(\n-        keras_core.layers.Conv2D(16, (3, 3))\n+    keras_layer = keras.layers.TimeDistributed(\n+        keras.layers.Conv2D(16, (3, 3))\n     )\n     tf_keras_layer = tf.keras.layers.TimeDistributed(\n         tf.keras.layers.Conv2D(16, (3, 3))\n@@ -236,7 +236,7 @@ def benchmark_time_distributed(\n         init_args,\n         input_shape=[10, 32, 32, 3],\n         jit_compile=jit_compile,\n-        keras_core_layer=keras_core_layer,\n+        keras_layer=keras_layer,\n         tf_keras_layer=tf_keras_layer,\n     )\n \n\n@@ -1,9 +1,9 @@\n import time\n \n-import keras_core\n+import keras\n \n \n-class BenchmarkMetricsCallback(keras_core.callbacks.Callback):\n+class BenchmarkMetricsCallback(keras.callbacks.Callback):\n     def __init__(self, start_batch=1, stop_batch=None):\n         self.start_batch = start_batch\n         self.stop_batch = stop_batch\n\n@@ -21,7 +21,7 @@ from absl import flags\n from absl import logging\n from model_benchmark.benchmark_utils import BenchmarkMetricsCallback\n \n-import keras_core as keras\n+import keras as keras\n \n flags.DEFINE_string(\"model_size\", \"small\", \"The size of model to benchmark.\")\n flags.DEFINE_string(\n\n@@ -27,7 +27,7 @@ from absl import flags\n from absl import logging\n from model_benchmark.benchmark_utils import BenchmarkMetricsCallback\n \n-import keras_core as keras\n+import keras as keras\n \n flags.DEFINE_string(\"model\", \"EfficientNetV2B0\", \"The model to benchmark.\")\n flags.DEFINE_integer(\"epochs\", 1, \"The number of epochs.\")\n\n@@ -9,9 +9,9 @@ import torch\n import torch.nn as nn\n import torch.optim as optim\n \n-import keras_core\n+import keras\n from benchmarks.torch_ctl_benchmark.benchmark_utils import train_loop\n-from keras_core import layers\n+from keras import layers\n \n num_classes = 2\n input_shape = (3, 256, 256)\n@@ -55,8 +55,8 @@ class TorchModel(torch.nn.Module):\n         return x\n \n \n-def run_keras_core_custom_training_loop():\n-    keras_model = keras_core.Sequential(\n+def run_keras_custom_training_loop():\n+    keras_model = keras.Sequential(\n         [\n             layers.Input(shape=input_shape),\n             layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n@@ -74,7 +74,7 @@ def run_keras_core_custom_training_loop():\n         num_epochs=num_epochs,\n         optimizer=optimizer,\n         loss_fn=loss_fn,\n-        framework=\"keras_core\",\n+        framework=\"keras\",\n     )\n \n \n@@ -93,5 +93,5 @@ def run_torch_custom_training_loop():\n \n \n if __name__ == \"__main__\":\n-    run_keras_core_custom_training_loop()\n+    run_keras_custom_training_loop()\n     run_torch_custom_training_loop()\n\n@@ -9,9 +9,9 @@ import torch\n import torch.nn as nn\n import torch.optim as optim\n \n-import keras_core\n+import keras\n from benchmarks.torch_ctl_benchmark.benchmark_utils import train_loop\n-from keras_core import layers\n+from keras import layers\n \n num_classes = 2\n input_shape = (8192,)\n@@ -55,8 +55,8 @@ class TorchModel(torch.nn.Module):\n         return x\n \n \n-def run_keras_core_custom_training_loop():\n-    keras_model = keras_core.Sequential(\n+def run_keras_custom_training_loop():\n+    keras_model = keras.Sequential(\n         [\n             layers.Input(shape=input_shape),\n             layers.Dense(64, activation=\"relu\"),\n@@ -73,7 +73,7 @@ def run_keras_core_custom_training_loop():\n         num_epochs=num_epochs,\n         optimizer=optimizer,\n         loss_fn=loss_fn,\n-        framework=\"keras_core\",\n+        framework=\"keras\",\n     )\n \n \n@@ -92,5 +92,5 @@ def run_torch_custom_training_loop():\n \n \n if __name__ == \"__main__\":\n-    run_keras_core_custom_training_loop()\n+    run_keras_custom_training_loop()\n     run_torch_custom_training_loop()\n\n@@ -8,7 +8,7 @@ except ImportError:\n \n import pytest\n \n-from keras_core.backend import backend\n+from keras.backend import backend\n \n \n def pytest_configure(config):\n\n@@ -7,12 +7,12 @@ os.environ[\"KERAS_BACKEND\"] = \"jax\"\n import jax\n import numpy as np\n \n-from keras_core import Model\n-from keras_core import backend\n-from keras_core import initializers\n-from keras_core import layers\n-from keras_core import ops\n-from keras_core import optimizers\n+from keras import Model\n+from keras import backend\n+from keras import initializers\n+from keras import layers\n+from keras import ops\n+from keras import optimizers\n \n \n class MyDense(layers.Layer):\n\n@@ -1,13 +1,13 @@\n import numpy as np\n \n-import keras_core\n-from keras_core import Model\n-from keras_core import initializers\n-from keras_core import layers\n-from keras_core import losses\n-from keras_core import metrics\n-from keras_core import ops\n-from keras_core import optimizers\n+import keras\n+from keras import Model\n+from keras import initializers\n+from keras import layers\n+from keras import losses\n+from keras import metrics\n+from keras import ops\n+from keras import optimizers\n \n \n class MyDense(layers.Layer):\n@@ -43,11 +43,11 @@ class MyDropout(layers.Layer):\n         # Use seed_generator for managing RNG state.\n         # It is a state element and its seed variable is\n         # tracked as part of `layer.variables`.\n-        self.seed_generator = keras_core.random.SeedGenerator(1337)\n+        self.seed_generator = keras.random.SeedGenerator(1337)\n \n     def call(self, inputs):\n-        # Use `keras_core.random` for random ops.\n-        return keras_core.random.dropout(\n+        # Use `keras.random` for random ops.\n+        return keras.random.dropout(\n             inputs, self.rate, seed=self.seed_generator\n         )\n \n\n@@ -7,12 +7,12 @@ os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n import numpy as np\n import tensorflow as tf\n \n-from keras_core import Model\n-from keras_core import backend\n-from keras_core import initializers\n-from keras_core import layers\n-from keras_core import ops\n-from keras_core import optimizers\n+from keras import Model\n+from keras import backend\n+from keras import initializers\n+from keras import layers\n+from keras import ops\n+from keras import optimizers\n \n \n class MyDense(layers.Layer):\n\n@@ -7,8 +7,8 @@ os.environ[\"KERAS_BACKEND\"] = \"torch\"\n import torch\n import torch.nn as nn\n import torch.optim as optim\n-from keras_core import layers\n-import keras_core\n+from keras import layers\n+import keras\n import numpy as np\n \n # Model / data parameters\n@@ -19,7 +19,7 @@ batch_size = 64\n num_epochs = 1\n \n # Load the data and split it between train and test sets\n-(x_train, y_train), (x_test, y_test) = keras_core.datasets.mnist.load_data()\n+(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n \n # Scale images to the [0, 1] range\n x_train = x_train.astype(\"float32\") / 255\n@@ -32,7 +32,7 @@ print(x_train.shape[0], \"train samples\")\n print(x_test.shape[0], \"test samples\")\n \n # Create the Keras model\n-model = keras_core.Sequential(\n+model = keras.Sequential(\n     [\n         layers.Input(shape=(28, 28, 1)),\n         layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n@@ -102,7 +102,7 @@ train(model, train_loader, num_epochs, optimizer, loss_fn)\n class MyModel(nn.Module):\n     def __init__(self):\n         super().__init__()\n-        self.model = keras_core.Sequential(\n+        self.model = keras.Sequential(\n             [\n                 layers.Input(shape=(28, 28, 1)),\n                 layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n\n@@ -11,7 +11,7 @@ pp = pprint.PrettyPrinter()\n import jax\n import jax.numpy as jnp\n import tensorflow as tf  # just for tf.data\n-import keras_core as keras  # Keras multi-backend\n+import keras as keras  # Keras multi-backend\n \n import numpy as np\n from tqdm import tqdm\n\n@@ -1,14 +1,14 @@\n import numpy as np\n-import keras_core\n-from keras_core import layers\n-from keras_core.utils import to_categorical\n+import keras\n+from keras import layers\n+from keras.utils import to_categorical\n \n # Model / data parameters\n num_classes = 10\n input_shape = (28, 28, 1)\n \n # Load the data and split it between train and test sets\n-(x_train, y_train), (x_test, y_test) = keras_core.datasets.mnist.load_data()\n+(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n \n # Scale images to the [0, 1] range\n x_train = x_train.astype(\"float32\") / 255\n@@ -28,7 +28,7 @@ y_test = to_categorical(y_test, num_classes)\n batch_size = 128\n epochs = 3\n \n-model = keras_core.Sequential(\n+model = keras.Sequential(\n     [\n         layers.Input(shape=input_shape),\n         layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n\n@@ -1,10 +1,10 @@\n import numpy as np\n \n-from keras_core import Model\n-from keras_core import layers\n-from keras_core import losses\n-from keras_core import metrics\n-from keras_core import optimizers\n+from keras import Model\n+from keras import layers\n+from keras import losses\n+from keras import metrics\n+from keras import optimizers\n \n \n class MyModel(Model):\n\n@@ -7,8 +7,8 @@ os.environ[\"KERAS_BACKEND\"] = \"torch\"\n import torch\n import torch.nn as nn\n import torch.optim as optim\n-from keras_core import layers\n-import keras_core\n+from keras import layers\n+import keras\n import numpy as np\n \n import torch.multiprocessing as mp\n@@ -27,7 +27,7 @@ num_epochs = 1\n \n def get_data():\n     # Load the data and split it between train and test sets\n-    (x_train, y_train), (x_test, y_test) = keras_core.datasets.mnist.load_data()\n+    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n \n     # Scale images to the [0, 1] range\n     x_train = x_train.astype(\"float32\") / 255\n@@ -48,7 +48,7 @@ def get_data():\n \n def get_model():\n     # Create the Keras model\n-    model = keras_core.Sequential(\n+    model = keras.Sequential(\n         [\n             layers.Input(shape=(28, 28, 1)),\n             layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n@@ -66,7 +66,7 @@ def get_model():\n class MyModel(nn.Module):\n     def __init__(self):\n         super().__init__()\n-        self.model = keras_core.Sequential(\n+        self.model = keras.Sequential(\n             [\n                 layers.Input(shape=(28, 28, 1)),\n                 layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n\n@@ -41,7 +41,7 @@ import os\n os.environ[\"KERAS_BACKEND\"] = \"jax\"\n \n import keras_nlp\n-import keras_core as keras\n+import keras as keras\n \n import tensorflow.data as tf_data\n import tensorflow.strings as tf_strings\n\n@@ -42,10 +42,10 @@ with TensorFlow 2.3 or higher.\n import os\n os.environ['KERAS_BACKEND'] = 'tensorflow'\n \n-import keras_core as keras\n-from keras_core import layers\n-from keras_core import ops\n-from keras_core.layers import TextVectorization\n+import keras as keras\n+from keras import layers\n+from keras import ops\n+from keras.layers import TextVectorization\n import numpy as np\n import os\n import string\n\n@@ -46,8 +46,8 @@ Five digits (reversed):\n ## Setup\n \"\"\"\n \n-import keras_core as keras\n-from keras_core import layers\n+import keras as keras\n+from keras import layers\n import numpy as np\n \n # Parameters for the model and dataset.\n\n@@ -11,8 +11,8 @@ Accelerator: GPU\n \"\"\"\n \n import numpy as np\n-import keras_core as keras\n-from keras_core import layers\n+import keras as keras\n+from keras import layers\n \n max_features = 20000  # Only consider the top 20k words\n maxlen = 200  # Only consider the first 200 words of each movie review\n\n@@ -21,7 +21,7 @@ unlike question answering. We will use SWAG dataset to demonstrate this example.\n \"\"\"\n \n \"\"\"shell\n-pip install -q keras-core --upgrade\n+pip install -q keras --upgrade\n pip install -q keras-nlp --upgrade\n \"\"\"\n \n@@ -29,7 +29,7 @@ import os\n os.environ[\"KERAS_BACKEND\"] = \"jax\"  # or \"tensorflow\" or \"torch\"\n \n import keras_nlp\n-import keras_core as keras\n+import keras as keras\n import tensorflow as tf\n \n import numpy as np\n@@ -194,7 +194,7 @@ content of the options themselves, rather than being influenced by their positio\n \n **Note:** Even though `option_shuffle` function is written in pure\n tensorflow, it can be used with any backend (e.g. JAX, PyTorch) as it is only used\n-in `tf.data.Dataset` pipeline which is compatible with Keras Core routines.\n+in `tf.data.Dataset` pipeline which is compatible with Keras 3 routines.\n \"\"\"\n \n \n\n@@ -52,8 +52,8 @@ import keras_nlp\n import pathlib\n import random\n \n-import keras_core as keras\n-from keras_core import ops\n+import keras as keras\n+from keras import ops\n \n import tensorflow.data as tf_data\n from tensorflow_text.tools.wordpiece_vocab import (\n\n@@ -53,10 +53,10 @@ import numpy as np\n import tensorflow.data as tf_data\n import tensorflow.strings as tf_strings\n \n-import keras_core as keras\n-from keras_core import layers\n-from keras_core import ops\n-from keras_core.layers import TextVectorization\n+import keras as keras\n+from keras import layers\n+from keras import ops\n+from keras.layers import TextVectorization\n \n \"\"\"\n ## Downloading the data\n@@ -231,7 +231,7 @@ sure that it only uses information from target tokens 0 to N when predicting tok\n (otherwise, it could use information from the future, which would\n result in a model that cannot be used at inference time).\n \"\"\"\n-import keras_core.ops as ops\n+import keras.ops as ops\n \n class TransformerEncoder(layers.Layer):\n     def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n\n@@ -84,7 +84,7 @@ import nltk\n import random\n import logging\n \n-import keras_core as keras\n+import keras as keras\n \n nltk.download(\"punkt\")\n # Set random seed\n\n@@ -21,7 +21,7 @@ Dataloaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\n ### References:\n \n - [Customizing what happens in `fit()` with\n-PyTorch](https://keras.io/keras_core/guides/custom_train_step_in_torch/)\n+PyTorch](https://keras.io/keras/guides/custom_train_step_in_torch/)\n - [PyTorch Datasets and\n Dataloaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n - [Transfer learning for Computer Vision using\n@@ -46,8 +46,8 @@ import torch.nn.functional as F\n import torchvision\n from torchvision import datasets, models, transforms\n \n-import keras_core as keras\n-from keras_core.layers import TorchModuleWrapper\n+import keras as keras\n+from keras.layers import TorchModuleWrapper\n \n \"\"\"\n ## Define the Hyperparameters\n@@ -165,9 +165,9 @@ resnet_18.fc = nn.Identity()\n \n \"\"\"\n Even though Keras supports PyTorch as a backend, it does not mean that we can nest torch\n-modules inside a [`keras_core.Model`](https://keras.io/keras_core/api/models/), because\n+modules inside a [`keras.Model`](https://keras.io/keras/api/models/), because\n trainable variables inside a Keras Model is tracked exclusively via [Keras\n-Layers](https://keras.io/keras_core/api/layers/).\n+Layers](https://keras.io/keras/api/layers/).\n \n KerasCore provides us with a feature called `TorchModuleWrapper` which enables us to do\n exactly this. The `TorchModuleWrapper` is a Keras Layer that accepts a torch module and\n\n@@ -37,9 +37,9 @@ import matplotlib.pyplot as plt\n import numpy as np\n from zipfile import ZipFile\n \n-import keras_core as keras\n-from keras_core import layers\n-from keras_core import ops\n+import keras as keras\n+from keras import layers\n+from keras import ops\n \n \"\"\"\n ## First, load the data and apply preprocessing\n\n@@ -33,10 +33,10 @@ and 9 categorical features.\n ## Setup\n \"\"\"\n \n-import keras_core as keras\n-from keras_core import layers\n-from keras_core.layers import StringLookup\n-from keras_core import ops\n+import keras as keras\n+from keras import layers\n+from keras.layers import StringLookup\n+from keras import ops\n \n \n from tensorflow import data as tf_data\n\n@@ -21,9 +21,9 @@ into robust contextual embeddings to achieve higher predictive accuracy.\n \n ## Setup\n \"\"\"\n-import keras_core as keras\n-from keras_core import layers\n-from keras_core import ops\n+import keras as keras\n+from keras import layers\n+from keras import ops\n \n import math\n import numpy as np\n\n@@ -1,7 +1,7 @@\n \"\"\"\n Title: Speaker Recognition\n Author: [Fadi Badine](https://twitter.com/fadibadine)\n-Converted to Keras Core by: [Fadi Badine](https://twitter.com/fadibadine)\n+Converted to Keras 3 by: [Fadi Badine](https://twitter.com/fadibadine)\n Date created: 14/06/2020\n Last modified: 19/07/2023\n Description: Classify speakers using Fast Fourier Transform (FFT) and a 1D Convnet.\n@@ -47,7 +47,7 @@ import shutil\n import numpy as np\n \n import tensorflow as tf\n-import keras_core as keras\n+import keras as keras\n \n from pathlib import Path\n from IPython.display import display, Audio\n\n@@ -1,7 +1,7 @@\n \"\"\"\n Title: English speaker accent recognition using Transfer Learning\n Author: [Fadi Badine](https://twitter.com/fadibadine)\n-Converted to Keras Core by: [Fadi Badine](https://twitter.com/fadibadine)\n+Converted to Keras 3 by: [Fadi Badine](https://twitter.com/fadibadine)\n Date created: 2022/04/16\n Last modified: 2023/07/19\n Description: Training a model to classify UK & Ireland accents using feature extraction from Yamnet.\n@@ -108,7 +108,7 @@ import pandas as pd\n import tensorflow as tf\n import tensorflow_hub as hub\n import tensorflow_io as tfio\n-import keras_core as keras\n+import keras as keras\n import matplotlib.pyplot as plt\n import seaborn as sns\n from scipy import stats\n\n@@ -27,9 +27,9 @@ using cycle-consistent adversarial networks.\n \n import numpy as np\n import matplotlib.pyplot as plt\n-import keras_core as keras\n-from keras_core import layers\n-from keras_core import ops\n+import keras as keras\n+from keras import layers\n+from keras import ops\n \n import tensorflow as tf\n import tensorflow_datasets as tfds\n@@ -664,13 +664,13 @@ and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io\n # data and check the model's performance.\n \n \"\"\"shell\n-curl -LO https://github.com/freedomtan/cyclegan-keras-core/archive/refs/tags/2.0.zip\n+curl -LO https://github.com/freedomtan/cyclegan-keras/archive/refs/tags/2.0.zip\n unzip -qq 2.0.zip\n \"\"\"\n \n \n # Load the checkpoints\n-weight_file = \"./cyclegan-keras-core-2.0/model_checkpoints/cyclegan_checkpoints.090.weights.h5\"\n+weight_file = \"./cyclegan-keras-2.0/model_checkpoints/cyclegan_checkpoints.090.weights.h5\"\n cycle_gan_model.load_weights(weight_file)\n print(\"Weights loaded successfully\")\n \n\n@@ -11,8 +11,8 @@ Accelerator: GPU\n \"\"\"\n \n import tensorflow as tf\n-import keras_core as keras\n-from keras_core import layers\n+import keras as keras\n+from keras import layers\n import matplotlib.pyplot as plt\n import os\n import gdown\n\n@@ -72,9 +72,9 @@ import matplotlib.pyplot as plt\n import tensorflow as tf\n import tensorflow_datasets as tfds\n \n-import keras_core as keras\n-from keras_core import layers\n-from keras_core import ops\n+import keras as keras\n+from keras import layers\n+from keras import ops\n \n \"\"\"\n ## Hyperparameters\n\n@@ -90,8 +90,8 @@ import matplotlib.pyplot as plt\n \n # Requires TensorFlow >=2.11 for the GroupNormalization layer.\n import tensorflow as tf\n-import keras_core as keras\n-from keras_core import layers\n+import keras as keras\n+from keras import layers\n import tensorflow_datasets as tfds\n \n \"\"\"\n\n@@ -38,8 +38,8 @@ and compare the result to the (resized) original image.\n \n import numpy as np\n import tensorflow as tf\n-import keras_core as keras\n-from keras_core.applications import inception_v3\n+import keras as keras\n+from keras.applications import inception_v3\n \n base_image_path = keras.utils.get_file(\n     \"sky.jpg\", \"https://i.imgur.com/aGBdQyK.jpg\"\n\n@@ -25,8 +25,8 @@ has at least ~100k characters. ~1M is better.\n \"\"\"\n ## Setup\n \"\"\"\n-import keras_core as keras\n-from keras_core import layers\n+import keras as keras\n+from keras import layers\n \n import numpy as np\n import random\n\n@@ -40,8 +40,8 @@ keeping the generated image close enough to the original one.\n \n import numpy as np\n import tensorflow as tf\n-import keras_core as keras\n-from keras_core.applications import vgg19\n+import keras as keras\n+from keras.applications import vgg19\n \n base_image_path = keras.utils.get_file(\n     \"paris.jpg\", \"https://i.imgur.com/F28w3Ac.jpg\"\n\n@@ -13,8 +13,8 @@ Accelerator: GPU\n \n import numpy as np\n import tensorflow as tf\n-import keras_core as keras\n-from keras_core import layers\n+import keras as keras\n+from keras import layers\n \n \"\"\"\n ## Create a sampling layer\n\n@@ -30,8 +30,8 @@ that keeps the L2 norm of the discriminator gradients close to 1.\n \"\"\"\n \n import tensorflow as tf\n-import keras_core as keras\n-from keras_core import layers\n+import keras as keras\n+from keras import layers\n \n \n \"\"\"\n\n@@ -23,8 +23,8 @@ features back to a space of the original size.\n \"\"\"\n \n import tensorflow as tf\n-import keras_core as keras\n-from keras_core import layers\n+import keras as keras\n+from keras import layers\n \n \"\"\"\n ## The Antirectifier layer\n\n@@ -12,7 +12,7 @@ Accelerator: GPU\n \"\"\"\n \n import tensorflow as tf\n-import keras_core as keras\n+import keras as keras\n import numpy as np\n \n \"\"\"\n\n@@ -27,8 +27,8 @@ Using this approach, we can quickly implement a\n [StandardizedConv2D](https://arxiv.org/abs/1903.10520) as shown below.\n \"\"\"\n import tensorflow as tf\n-import keras_core as keras\n-from keras_core import layers\n+import keras as keras\n+from keras import layers\n import numpy as np\n \n \n\n@@ -29,8 +29,8 @@ TensorFlow NumPy requires TensorFlow 2.5 or later.\n \n import tensorflow as tf\n import tensorflow.experimental.numpy as tnp\n-import keras_core as keras\n-from keras_core import layers\n+import keras as keras\n+from keras import layers\n \n \"\"\"\n Optionally, you can call `tnp.experimental_enable_numpy_behavior()` to enable type promotion in TensorFlow.\n\n@@ -60,7 +60,7 @@ import shutil\n import requests\n import numpy as np\n import tensorflow as tf\n-import keras_core as keras\n+import keras as keras\n import matplotlib.pyplot as plt\n \n \"\"\"\n\n@@ -25,7 +25,7 @@ by putting the custom training step in the Trainer class definition.\n \"\"\"\n \n import tensorflow as tf\n-import keras_core as keras\n+import keras as keras\n \n # Load MNIST dataset and standardize the data\n mnist = keras.datasets.mnist\n\n@@ -50,8 +50,8 @@ from pathlib import Path\n from dataclasses import dataclass\n \n import tensorflow as tf\n-import keras_core as keras\n-from keras_core import layers\n+import keras as keras\n+from keras import layers\n \n \"\"\"\n ## Configuration\n\n@@ -47,7 +47,7 @@ models are more common in this domain.\n \"\"\"\n \n import numpy as np\n-import keras_core as keras\n+import keras as keras\n import os\n from pathlib import Path\n \n\n@@ -34,8 +34,8 @@ wget https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py\n \n import os\n import numpy as np\n-import keras_core as keras\n-from keras_core import layers\n+import keras as keras\n+from keras import layers\n from datasets import load_dataset\n from collections import Counter\n from conlleval import evaluate\n\n@@ -13,7 +13,7 @@ Accelerator: GPU\n \n import numpy as np\n import tensorflow.data as tf_data\n-import keras_core as keras\n+import keras as keras\n \n \"\"\"\n ## Introduction\n@@ -123,7 +123,7 @@ Our layer will only consider the top 20,000 words, and will truncate or pad sequ\n be actually 200 tokens long.\n \"\"\"\n \n-from keras_core.layers import TextVectorization\n+from keras.layers import TextVectorization\n \n vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n text_ds = tf_data.Dataset.from_tensor_slices(train_samples).batch(128)\n@@ -228,7 +228,7 @@ Note that we set `trainable=False` so as to keep the embeddings fixed (we don't\n update them during training).\n \"\"\"\n \n-from keras_core.layers import Embedding\n+from keras.layers import Embedding\n \n embedding_layer = Embedding(\n     num_tokens,\n@@ -244,7 +244,7 @@ embedding_layer.set_weights([embedding_matrix])\n A simple 1D convnet with global max pooling and a classifier at the end.\n \"\"\"\n \n-from keras_core import layers\n+from keras import layers\n \n int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n embedded_sequences = embedding_layer(int_sequences_input)\n\n@@ -20,9 +20,9 @@ classification dataset (unprocessed version). We use the `TextVectorization` lay\n \"\"\"\n \n import tensorflow as tf\n-import keras_core as keras\n-from keras_core.layers import TextVectorization\n-from keras_core import layers\n+import keras as keras\n+from keras.layers import TextVectorization\n+from keras import layers\n import string\n import re\n import os\n\n@@ -1,7 +1,7 @@\n \"\"\"\n Title: Actor Critic Method\n Author: [Apoorv Nandan](https://twitter.com/NandanApoorv)\n-Converted to Keras Core by: [Muhammad Anas Raza](https://anasrz.com)\n+Converted to Keras 3 by: [Muhammad Anas Raza](https://anasrz.com)\n Date created: 2020/05/13\n Last modified: 2023/07/19\n Description: Implement Actor Critic Method in CartPole environment.\n@@ -44,8 +44,8 @@ import os\n \n os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n \n-import keras_core as keras\n-from keras_core import layers\n+import keras as keras\n+from keras import layers\n \n import gym\n import numpy as np\n\n@@ -20,7 +20,7 @@ to train a classification model on data with highly imbalanced classes.\n \"\"\"\n \n import numpy as np\n-import keras_core as keras\n+import keras as keras\n \n # Get the real data from https://www.kaggle.com/mlg-ulb/creditcardfraud/\n fname = \"/Users/fchollet/Downloads/creditcard.csv\"\n\n@@ -51,8 +51,8 @@ Target | Diagnosis of heart disease (1 = true; 0 = false) | Target\n \n import tensorflow as tf\n import pandas as pd\n-import keras_core as keras\n-from keras_core import layers\n+import keras as keras\n+from keras import layers\n \n \"\"\"\n ## Preparing the data\n@@ -172,9 +172,9 @@ then one-hot encode these integer indices.\n - `encode_integer_categorical_feature` to one-hot encode integer categorical features.\n \"\"\"\n \n-from keras_core.layers import IntegerLookup\n-from keras_core.layers import Normalization\n-from keras_core.layers import StringLookup\n+from keras.layers import IntegerLookup\n+from keras.layers import Normalization\n+from keras.layers import StringLookup\n \n \n def encode_numerical_feature(feature, name, dataset):\n\n@@ -61,8 +61,8 @@ Target | Diagnosis of heart disease (1 = true; 0 = false) | Target\n \n import tensorflow as tf\n import pandas as pd\n-import keras_core as keras\n-from keras_core.utils import FeatureSpace\n+import keras as keras\n+from keras.utils import FeatureSpace\n \n keras.config.disable_traceback_filtering()\n \n\n@@ -65,8 +65,8 @@ import pandas as pd\n import matplotlib.pyplot as plt\n import json\n import numpy as np\n-import keras_core as keras\n-from keras_core import layers\n+import keras as keras\n+from keras import layers\n import tensorflow as tf\n from sklearn import preprocessing, model_selection\n import random\n\n@@ -47,9 +47,9 @@ import typing\n import matplotlib.pyplot as plt\n \n import tensorflow as tf\n-import keras_core as keras\n-from keras_core import layers\n-from keras_core.utils import timeseries_dataset_from_array\n+import keras as keras\n+from keras import layers\n+from keras.utils import timeseries_dataset_from_array\n \n \"\"\"\n ## Data preparation\n\n@@ -14,7 +14,7 @@ This example requires TensorFlow 2.3 or higher.\n \n import pandas as pd\n import matplotlib.pyplot as plt\n-import keras_core as keras\n+import keras as keras\n \n \"\"\"\n ## Climate Data Time-Series\n\n@@ -42,7 +42,7 @@ import os\n \n os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n \n-import keras_core as keras\n+import keras as keras\n import numpy as np\n import matplotlib.pyplot as plt\n \n\n@@ -1,7 +1,7 @@\n \"\"\"\n Title: CutMix data augmentation for image classification\n Author: [Sayan Nath](https://twitter.com/sayannath2350)\n-Converted to Keras Core By: [Piyush Thakur](https://github.com/cosmo3769)\n+Converted to Keras 3 By: [Piyush Thakur](https://github.com/cosmo3769)\n Date created: 2021/06/08\n Last modified: 2023/07/24\n Description: Data augmentation with CutMix for image classification on CIFAR-10.\n@@ -48,10 +48,10 @@ where `rx, ry` are randomly drawn from a uniform distribution with upper bound.\n \n import numpy as np\n import pandas as pd\n-import keras_core as keras\n+import keras as keras\n import matplotlib.pyplot as plt\n \n-from keras_core import layers\n+from keras import layers\n \n # TF imports related to tf.data preprocessing\n from tensorflow import clip_by_value\n\n@@ -49,8 +49,8 @@ os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n \n import tensorflow as tf\n import tensorflow_datasets as tfds\n-import keras_core as keras\n-from keras_core import layers\n+import keras as keras\n+from keras import layers\n \n tfds.disable_progress_bar()\n keras.utils.set_random_seed(42)\n\n@@ -13,7 +13,7 @@ Adapted from Deep Learning with Python (2017).\n \n import numpy as np\n import tensorflow as tf\n-import keras_core as keras\n+import keras as keras\n \n # Display\n from IPython.display import Image, display\n\n@@ -17,10 +17,10 @@ import numpy as np\n import matplotlib.pyplot as plt\n \n import tensorflow as tf\n-import keras_core as keras\n-from keras_core import layers\n-from keras_core.applications import efficientnet\n-from keras_core.layers import TextVectorization\n+import keras as keras\n+from keras import layers\n+from keras.applications import efficientnet\n+from keras.layers import TextVectorization\n \n \n seed = 111\n\n@@ -23,8 +23,8 @@ we use Keras image preprocessing layers for image standardization and data augme\n \"\"\"\n \n import tensorflow as tf\n-import keras_core as keras\n-from keras_core import layers\n+import keras as keras\n+from keras import layers\n import os\n from pathlib import Path\n import matplotlib.pyplot as plt\n\n@@ -58,9 +58,9 @@ from scipy import ndimage\n from IPython.display import Image, display\n \n import tensorflow as tf\n-import keras_core as keras\n-from keras_core import layers\n-from keras_core.applications import xception\n+import keras as keras\n+from keras import layers\n+from keras.applications import xception\n \n keras.config.disable_traceback_filtering()\n \n\n@@ -36,7 +36,7 @@ layer.\n \"\"\"\n \n import tensorflow as tf\n-import keras_core as keras\n+import keras as keras\n import matplotlib.pyplot as plt\n \n # Set seed for reproducibility.\n\n@@ -32,9 +32,9 @@ import tensorflow as tf\n from collections import defaultdict\n from PIL import Image\n from sklearn.metrics import ConfusionMatrixDisplay\n-import keras_core as keras\n-from keras_core import layers\n-from keras_core.datasets import cifar10\n+import keras as keras\n+from keras import layers\n+from keras.datasets import cifar10\n \n \"\"\"\n ## Dataset\n\n@@ -1,7 +1,7 @@\n \"\"\"\n Title: Low-light image enhancement using MIRNet\n Author: [Soumik Rakshit](http://github.com/soumik12345)\n-Converted to Keras Core by: [Soumik Rakshit](http://github.com/soumik12345)\n+Converted to Keras 3 by: [Soumik Rakshit](http://github.com/soumik12345)\n Date created: 2021/09/11\n Last modified: 2023/07/15\n Description: Implementing the MIRNet architecture for low-light image enhancement.\n@@ -34,7 +34,7 @@ consists of a low-light input image and its corresponding well-exposed reference\n \"\"\"\n \n \"\"\"shell\n-pip install -q git+https://github.com/keras-team/keras-core\n+pip install -q git+https://github.com/keras-team/keras\n \"\"\"\n \n import os\n@@ -47,8 +47,8 @@ from glob import glob\n from PIL import Image, ImageOps\n import matplotlib.pyplot as plt\n \n-import keras_core as keras\n-from keras_core import layers\n+import keras as keras\n+from keras import layers\n \n import tensorflow as tf\n \n\n@@ -37,10 +37,10 @@ processing, speech, and so on.\n \"\"\"\n \n import numpy as np\n-import keras_core as keras\n+import keras as keras\n import matplotlib.pyplot as plt\n \n-from keras_core import layers\n+from keras import layers\n \n # TF imports related to tf.data preprocessing\n from tensorflow import data as tf_data\n\n@@ -43,8 +43,8 @@ pip install -U tensorflow-addons\n \n import numpy as np\n import tensorflow as tf\n-import keras_core as keras\n-from keras_core import layers\n+import keras as keras\n+from keras import layers\n \n \"\"\"\n ## Prepare the data\n\n@@ -1,7 +1,7 @@\n \"\"\"\n Title: Few-Shot learning with Reptile\n Author: [ADMoreau](https://github.com/ADMoreau)\n-Converted to Keras Core By: [Muhammad Anas Raza](https://anasrz.com)\n+Converted to Keras 3 By: [Muhammad Anas Raza](https://anasrz.com)\n Date created: 2020/05/21\n Last modified: 2023/07/20\n Description: Few-shot classification on the Omniglot dataset using Reptile.\n@@ -22,8 +22,8 @@ import os\n \n os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n \n-import keras_core as keras\n-from keras_core import layers\n+import keras as keras\n+from keras import layers\n \n import matplotlib.pyplot as plt\n import numpy as np\n\n@@ -77,8 +77,8 @@ import matplotlib.pyplot as plt\n import tensorflow as tf\n import tensorflow_datasets as tfds\n \n-import keras_core as keras\n-from keras_core import layers\n+import keras as keras\n+from keras import layers\n \n \"\"\"\n ## Hyperparameter setup\n\n@@ -1,7 +1,7 @@\n \"\"\"\n Title: A Vision Transformer without Attention\n Author: [Aritra Roy Gosthipaty](https://twitter.com/ariG23498), [Ritwik Raha](https://twitter.com/ritwik_raha)\n-Converted to Keras Core: [Muhammad Anas Raza](https://anasrz.com)\n+Converted to Keras 3: [Muhammad Anas Raza](https://anasrz.com)\n Date created: 2022/02/24\n Last modified: 2023/07/15\n Description: A minimal implementation of ShiftViT.\n@@ -40,8 +40,8 @@ import numpy as np\n import matplotlib.pyplot as plt\n \n import tensorflow as tf\n-import keras_core as keras\n-from keras_core import layers\n+import keras as keras\n+from keras import layers\n \n \n # Setting seed for reproducibiltiy\n\n@@ -38,11 +38,11 @@ import numpy as np\n import os\n import tensorflow as tf\n from pathlib import Path\n-from keras_core import layers\n-from keras_core import optimizers\n-from keras_core import metrics\n-from keras_core import Model\n-from keras_core.applications import resnet\n+from keras import layers\n+from keras import optimizers\n+from keras import metrics\n+from keras import Model\n+from keras.applications import resnet\n \n \n target_shape = (200, 200)\n\n@@ -49,9 +49,9 @@ versions of our dataset.\n import os\n os.environ['KERAS_BACKEND'] = 'tensorflow'\n \n-from keras_core import layers\n-from keras_core import regularizers\n-import keras_core as keras\n+from keras import layers\n+from keras import regularizers\n+import keras as keras\n import tensorflow as tf\n \n import matplotlib.pyplot as plt\n\n@@ -29,8 +29,8 @@ This example requires TensorFlow 2.5 or higher.\n import matplotlib.pyplot as plt\n import numpy as np\n import tensorflow as tf\n-import keras_core as keras\n-from keras_core import layers\n+import keras as keras\n+from keras import layers\n \n \"\"\"\n ## Prepare the data\n\n@@ -26,7 +26,7 @@ import os\n \n os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n \n-import keras_core as keras\n+import keras as keras\n \n \n import numpy as np\n\n@@ -1,7 +1,7 @@\n \"\"\"\n Title: Zero-DCE for low-light image enhancement\n Author: [Soumik Rakshit](http://github.com/soumik12345)\n-Converted to Keras Core by: [Soumik Rakshit](http://github.com/soumik12345)\n+Converted to Keras 3 by: [Soumik Rakshit](http://github.com/soumik12345)\n Date created: 2021/09/18\n Last modified: 2023/07/15\n Description: Implementing Zero-Reference Deep Curve Estimation for low-light image enhancement.\n@@ -49,8 +49,8 @@ from glob import glob\n from PIL import Image, ImageOps\n import matplotlib.pyplot as plt\n \n-import keras_core as keras\n-from keras_core import layers\n+import keras as keras\n+from keras import layers\n \n import tensorflow as tf\n \n\n@@ -20,8 +20,8 @@ autoencoder model to detect anomalies in timeseries data.\n \n import numpy as np\n import pandas as pd\n-import keras_core as keras\n-from keras_core import layers\n+import keras as keras\n+from keras import layers\n from matplotlib import pyplot as plt\n \n \"\"\"\n\n@@ -19,7 +19,7 @@ CSV timeseries files on disk. We demonstrate the workflow on the FordA dataset f\n ## Setup\n \n \"\"\"\n-import keras_core as keras\n+import keras as keras\n import numpy as np\n import matplotlib.pyplot as plt\n \n\n@@ -62,8 +62,8 @@ You can replace your classification RNN layers with this one: the\n inputs are fully compatible!\n \"\"\"\n \n-import keras_core as keras\n-from keras_core import layers\n+import keras as keras\n+from keras import layers\n \n \"\"\"\n We include residual connections, layer normalization, and dropout.\n\n@@ -54,9 +54,9 @@ by TensorFlow.\n \"\"\"\n \n import numpy as np\n-import keras_core as keras\n-from keras_core import layers\n-from keras_core import ops\n+import keras as keras\n+from keras import layers\n+from keras import ops\n from tqdm import tqdm\n from matplotlib import pyplot as plt\n \n\n@@ -24,9 +24,9 @@ by [François Chollet](https://twitter.com/fchollet).\n import numpy as np\n import matplotlib.pyplot as plt\n \n-from keras_core import layers\n-from keras_core.datasets import mnist\n-from keras_core.models import Model\n+from keras import layers\n+from keras.datasets import mnist\n+from keras.models import Model\n \n \n def preprocess(array):\n\n@@ -1,7 +1,7 @@\n \"\"\"\n Title: Compact Convolutional Transformers\n Author: [Sayak Paul](https://twitter.com/RisingSayak)\n-Converted to Keras Core by: [Muhammad Anas Raza](https://anasrz.com), [Guillaume Baquiast](https://www.linkedin.com/in/guillaume-baquiast-478965ba/)\n+Converted to Keras 3 by: [Muhammad Anas Raza](https://anasrz.com), [Guillaume Baquiast](https://www.linkedin.com/in/guillaume-baquiast-478965ba/)\n Date created: 2021/06/30\n Last modified: 2023/08/07\n Description: Compact Convolutional Transformers for efficient image classification.\n@@ -39,8 +39,8 @@ code snippets from another example,\n ## Imports\n \"\"\"\n \n-from keras_core import layers\n-import keras_core as keras\n+from keras import layers\n+import keras as keras\n \n import matplotlib.pyplot as plt\n import numpy as np\n\n@@ -25,8 +25,8 @@ of predicting what video frames come next given a series of past frames.\n import numpy as np\n import matplotlib.pyplot as plt\n \n-import keras_core as keras\n-from keras_core import layers\n+import keras as keras\n+from keras import layers\n \n import io\n import imageio\n\n@@ -1,7 +1,7 @@\n \"\"\"\n Title: Multiclass semantic segmentation using DeepLabV3+\n Author: [Soumik Rakshit](http://github.com/soumik12345)\n-Converted to Keras Core: [Muhammad Anas Raza](https://anasrz.com)\n+Converted to Keras 3: [Muhammad Anas Raza](https://anasrz.com)\n Date created: 2021/08/31\n Last modified: 2023/07/19\n Description: Implement DeepLabV3+ architecture for Multi-class Semantic Segmentation.\n@@ -32,9 +32,9 @@ This dataset can be used for the \"human part segmentation\" task.\n \"\"\"\n \n \n-import keras_core as keras\n-from keras_core import layers\n-from keras_core import ops\n+import keras as keras\n+from keras import layers\n+from keras import ops\n \n import cv2\n import numpy as np\n\n@@ -1,7 +1,7 @@\n \"\"\"\n Title: Image classification with EANet (External Attention Transformer)\n Author: [ZhiYong Chang](https://github.com/czy00000)\n-Converted to Keras Core: [Muhammad Anas Raza](https://anasrz.com)\n+Converted to Keras 3: [Muhammad Anas Raza](https://anasrz.com)\n Date created: 2021/10/19\n Last modified: 2023/07/18\n Description: Image classification with a Transformer that leverages external attention.\n@@ -25,9 +25,9 @@ implicitly considers the correlations between all samples.\n ## Setup\n \"\"\"\n \n-import keras_core as keras\n-from keras_core import layers\n-from keras_core import ops\n+import keras as keras\n+from keras import layers\n+from keras import ops\n \n import matplotlib.pyplot as plt\n \n\n@@ -27,8 +27,8 @@ to fix this discrepancy.\n ## Imports\n \"\"\"\n \n-import keras_core as keras\n-from keras_core import layers\n+import keras as keras\n+from keras import layers\n import tensorflow as tf  # just for image processing and pipeline\n \n import tensorflow_datasets as tfds\n\n@@ -1,7 +1,7 @@\n \"\"\"\n Title: Gradient Centralization for Better Training Performance\n Author: [Rishit Dagli](https://github.com/Rishit-dagli)\n-Converted to Keras Core by: [Muhammad Anas Raza](https://anasrz.com)\n+Converted to Keras 3 by: [Muhammad Anas Raza](https://anasrz.com)\n Date created: 06/18/21\n Last modified: 07/25/23\n Description: Implement Gradient Centralization to improve training performance of DNNs.\n@@ -34,10 +34,10 @@ pip install tensorflow-datasets\n \n from time import time\n \n-import keras_core as keras\n-from keras_core import layers\n-from keras_core.optimizers import RMSprop\n-from keras_core import ops\n+import keras as keras\n+from keras import layers\n+from keras.optimizers import RMSprop\n+from keras import ops\n \n from tensorflow import data as tf_data\n import tensorflow_datasets as tfds\n\n@@ -1,7 +1,7 @@\n \"\"\"\n Title: Image classification with Vision Transformer\n Author: [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)\n-Converted to Keras Core by: [divyasreepat](https://github.com/divyashreepathihalli), [Soumik Rakshit](http://github.com/soumik12345)\n+Converted to Keras 3 by: [divyasreepat](https://github.com/divyashreepathihalli), [Soumik Rakshit](http://github.com/soumik12345)\n Date created: 2021/01/18\n Last modified: 2021/01/18\n Description: Implementing the Vision Transformer (ViT) model for image classification.\n@@ -27,9 +27,9 @@ import os\n \n os.environ[\"KERAS_BACKEND\"] = \"jax\"  # @param [\"tensorflow\", \"jax\", \"torch\"]\n \n-import keras_core as keras\n-from keras_core import layers\n-from keras_core import ops\n+import keras as keras\n+from keras import layers\n+from keras import ops\n \n import numpy as np\n import matplotlib.pyplot as plt\n\n@@ -25,7 +25,7 @@ classification problems at three levels of complexity:\n ## Multi-Backend Support\n \n KerasCV's `ImageClassifier` model supports several backends like JAX, PyTorch,\n-and TensorFlow with the help of `keras_core`. To enable multi-backend support\n+and TensorFlow with the help of `keras`. To enable multi-backend support\n in KerasCV, set the `KERAS_CV_MULTI_BACKEND` environment variable. We can\n then switch between different backends by setting the `KERAS_BACKEND`\n environment variable. Currently, `\"tensorflow\"`, `\"jax\"`, and `\"torch\"` are\n@@ -42,12 +42,12 @@ os.environ[\"KERAS_BACKEND\"] = \"jax\"\n import json\n import math\n import keras_cv\n-import keras_core as keras\n-from keras_core import ops\n-from keras_core import losses\n-from keras_core import optimizers\n-from keras_core.optimizers import schedules\n-from keras_core import metrics\n+import keras as keras\n+from keras import ops\n+from keras import losses\n+from keras import optimizers\n+from keras.optimizers import schedules\n+from keras import metrics\n import tensorflow as tf\n from tensorflow import data as tf_data\n import tensorflow_datasets as tfds\n\n@@ -1,7 +1,7 @@\n \"\"\"\n Title: Keypoint Detection with Transfer Learning\n Author: [Sayak Paul](https://twitter.com/RisingSayak)\n-Converted to Keras Core by: [Muhammad Anas Raza](https://anasrz.com)\n+Converted to Keras 3 by: [Muhammad Anas Raza](https://anasrz.com)\n Date created: 2021/05/02\n Last modified: 2023/07/19\n Description: Training a keypoint detector with data augmentation and transfer learning.\n@@ -57,8 +57,8 @@ unzip -qq ~/stanfordextra_v12.zip\n \"\"\"\n ## Imports\n \"\"\"\n-from keras_core import layers\n-import keras_core as keras\n+from keras import layers\n+import keras as keras\n \n from imgaug.augmentables.kps import KeypointsOnImage\n from imgaug.augmentables.kps import Keypoint\n\n@@ -39,9 +39,9 @@ using the [DenseNet-121](https://arxiv.org/abs/1608.06993) architecture.\n ## Setup\n \"\"\"\n \n-from keras_core import layers\n-import keras_core as keras\n-from keras_core import ops\n+from keras import layers\n+import keras as keras\n+from keras import ops\n \n from tensorflow import data as tf_data\n from tensorflow import image as tf_image\n\n@@ -1,7 +1,7 @@\n \"\"\"\n Title: Image classification with modern MLP models\n Author: [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)\n-Converted to Keras Core by: [Guillaume Baquiast](https://www.linkedin.com/in/guillaume-baquiast-478965ba/), [divyasreepat](https://github.com/divyashreepathihalli)\n+Converted to Keras 3 by: [Guillaume Baquiast](https://www.linkedin.com/in/guillaume-baquiast-478965ba/), [divyasreepat](https://github.com/divyashreepathihalli)\n Date created: 2021/05/30\n Last modified: 2023/08/03\n Description: Implementing the MLP-Mixer, FNet, and gMLP models for CIFAR-100 image classification.\n@@ -29,8 +29,8 @@ main building blocks.\n \"\"\"\n \n import numpy as np\n-import keras_core as keras\n-from keras_core import layers\n+import keras as keras\n+from keras import layers\n \n \"\"\"\n ## Prepare the data\n\n@@ -12,8 +12,8 @@ Accelerator: GPU\n \"\"\"\n \n import numpy as np\n-import keras_core as keras\n-from keras_core import layers\n+import keras as keras\n+from keras import layers\n \n \"\"\"\n ## Prepare the data\n\n@@ -1,7 +1,7 @@\n \"\"\"\n Title: Object detection with Vision Transformers\n Author: [Karan V. Dave](https://www.linkedin.com/in/karan-dave-811413164/)\n-Converted to Keras Core by: [Gabriel Rasskin](https://github.com/grasskin), [Soumik Rakshit](http://github.com/soumik12345)\n+Converted to Keras 3 by: [Gabriel Rasskin](https://github.com/grasskin), [Soumik Rakshit](http://github.com/soumik12345)\n Date created: 2022/03/27\n Last modified: 2022/03/27\n Description: A simple Keras implementation of object detection using Vision Transformers.\n@@ -29,7 +29,7 @@ from which we import the `AdamW` optimizer.\n TensorFlow Addons can be installed via the following command:\n \n ```\n-pip install -U git+https://github.com/keras-team/keras-core\n+pip install -U git+https://github.com/keras-team/keras\n ```\n \"\"\"\n \n@@ -43,9 +43,9 @@ os.environ[\"KERAS_BACKEND\"] = \"jax\"  # @param [\"tensorflow\", \"jax\", \"torch\"]\n \n \n import numpy as np\n-import keras_core as keras\n-from keras_core import layers\n-from keras_core import ops\n+import keras as keras\n+from keras import layers\n+from keras import ops\n import matplotlib.pyplot as plt\n import numpy as np\n import cv2\n\n@@ -58,7 +58,7 @@ for input_path, target_path in zip(input_img_paths[:10], target_img_paths[:10]):\n \"\"\"\n \n from IPython.display import Image, display\n-from keras_core.utils import load_img\n+from keras.utils import load_img\n from PIL import ImageOps\n \n # Display input image #7\n@@ -72,7 +72,7 @@ display(img)\n ## Prepare dataset to load & vectorize batches of data\n \"\"\"\n \n-import keras_core as keras\n+import keras as keras\n import numpy as np\n from tensorflow import data as tf_data\n from tensorflow import image as tf_image\n@@ -118,7 +118,7 @@ def get_dataset(\n ## Prepare U-Net Xception-style model\n \"\"\"\n \n-from keras_core import layers\n+from keras import layers\n \n \n def get_model(img_size, num_classes):\n\n@@ -26,8 +26,8 @@ the class segmentation of the training inputs.\n \n import random\n import numpy as np\n-import keras_core as keras\n-from keras_core import ops\n+import keras as keras\n+from keras import ops\n import matplotlib.pyplot as plt\n \n \"\"\"\n\n@@ -1,7 +1,7 @@\n \"\"\"\n Title: Learning to tokenize in Vision Transformers\n Authors: [Aritra Roy Gosthipaty](https://twitter.com/ariG23498), [Sayak Paul](https://twitter.com/RisingSayak) (equal contribution)\n-Converted to Keras Core by: [Muhammad Anas Raza](https://anasrz.com)\n+Converted to Keras 3 by: [Muhammad Anas Raza](https://anasrz.com)\n Date created: 2021/12/10\n Last modified: 2023/08/14\n Description: Adaptively generating a smaller number of tokens for Vision Transformers.\n@@ -49,9 +49,9 @@ references:\n ## Imports\n \"\"\"\n \n-import keras_core as keras\n-from keras_core import layers\n-from keras_core import ops\n+import keras as keras\n+from keras import layers\n+from keras import ops\n from tensorflow import data as tf_data\n \n \n\n@@ -1,7 +1,7 @@\n \"\"\"\n Title: Video Classification with Transformers\n Author: [Sayak Paul](https://twitter.com/RisingSayak)\n-Converted to Keras Core by: [Soumik Rakshit](http://github.com/soumik12345)\n+Converted to Keras 3 by: [Soumik Rakshit](http://github.com/soumik12345)\n Date created: 2021/06/08\n Last modified: 2023/22/07\n Description: Training a video classifier with hybrid transformers.\n@@ -19,7 +19,7 @@ classification that operate on CNN feature maps.\n \"\"\"\n \n \"\"\"shell\n-pip install -q git+https://github.com/keras-team/keras-core\n+pip install -q git+https://github.com/keras-team/keras\n pip install -q git+https://github.com/tensorflow/docs\n \"\"\"\n \n@@ -47,9 +47,9 @@ import os\n \n os.environ[\"KERAS_BACKEND\"] = \"jax\"  # @param [\"tensorflow\", \"jax\", \"torch\"]\n \n-import keras_core as keras\n-from keras_core import layers\n-from keras_core.applications.densenet import DenseNet121\n+import keras as keras\n+from keras import layers\n+from keras.applications.densenet import DenseNet121\n \n from tensorflow_docs.vis import embed\n \n\n@@ -48,7 +48,7 @@ import os\n os.environ[\"KERAS_BACKEND\"] = \"jax\"\n \n import jax\n-import keras_core as keras\n+import keras as keras\n import numpy as np\n \n \"\"\"\n\n@@ -48,8 +48,8 @@ import os\n os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n \n import tensorflow as tf\n-import keras_core as keras\n-from keras_core import layers\n+import keras as keras\n+from keras import layers\n import numpy as np\n \n \"\"\"\n\n@@ -48,8 +48,8 @@ import os\n os.environ[\"KERAS_BACKEND\"] = \"torch\"\n \n import torch\n-import keras_core as keras\n-from keras_core import layers\n+import keras as keras\n+from keras import layers\n import numpy as np\n \n \"\"\"\n\n@@ -46,7 +46,7 @@ os.environ[\"KERAS_BACKEND\"] = \"jax\"\n import jax\n import numpy as np\n import tensorflow as tf\n-import keras_core as keras\n+import keras as keras\n \n from jax.experimental import mesh_utils\n from jax.sharding import Mesh\n\n@@ -42,7 +42,7 @@ import os\n os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n \n import tensorflow as tf\n-import keras_core as keras\n+import keras as keras\n \n \"\"\"\n ## Single-host, multi-device synchronous training\n\n@@ -46,7 +46,7 @@ os.environ[\"KERAS_BACKEND\"] = \"torch\"\n \n import torch\n import numpy as np\n-import keras_core as keras\n+import keras as keras\n \n \n def get_model():\n\n@@ -11,9 +11,9 @@ Accelerator: GPU\n \"\"\"\n \n import numpy as np\n-import keras_core as keras\n-from keras_core import layers\n-from keras_core import ops\n+import keras as keras\n+from keras import layers\n+from keras import ops\n \n \"\"\"\n ## Introduction\n\n@@ -29,9 +29,9 @@ Let's dive in.\n \"\"\"\n \n import numpy as np\n-import keras_core as keras\n-from keras_core import ops\n-from keras_core import layers\n+import keras as keras\n+from keras import ops\n+from keras import layers\n \n \"\"\"\n ## The `Layer` class: the combination of state (weights) and some computation\n\n@@ -11,9 +11,9 @@ Accelerator: GPU\n \n \"\"\"\n \n-import keras_core as keras\n-from keras_core import layers\n-from keras_core import ops\n+import keras as keras\n+from keras import layers\n+from keras import ops\n \n \"\"\"\n ## When to use a Sequential model\n\n@@ -17,9 +17,9 @@ import tensorflow as tf\n \n import os\n import numpy as np\n-import keras_core as keras\n-from keras_core import layers\n-from keras_core import ops\n+import keras as keras\n+from keras import layers\n+from keras import ops\n \n \"\"\"\n ## Introduction\n@@ -51,7 +51,7 @@ This guide doesn't cover distributed training, which is covered in our\n When passing data to the built-in training loops of a model, you should either use:\n \n - NumPy arrays (if your data is small and fits in memory)\n-- Subclasses of `keras_core.utils.PyDataset`\n+- Subclasses of `keras.utils.PyDataset`\n - `tf.data.Dataset` objects\n - PyTorch `DataLoader` instances\n \n\n@@ -11,8 +11,8 @@ Accelerator: GPU\n \"\"\"\n \n import numpy as np\n-import keras_core as keras\n-from keras_core import layers\n+import keras as keras\n+from keras import layers\n import tensorflow_datasets as tfds\n import matplotlib.pyplot as plt\n \n\n@@ -10,9 +10,9 @@ Accelerator: None\n ## Setup\n \"\"\"\n import numpy as np\n-import keras_core as keras\n-from keras_core import ops\n-from keras_core import layers\n+import keras as keras\n+from keras import ops\n+from keras import layers\n \n \"\"\"\n ## Introduction\n\n@@ -19,7 +19,7 @@ import jax\n \n # We import TF so we can use tf.data.\n import tensorflow as tf\n-import keras_core as keras\n+import keras as keras\n import numpy as np\n \n \"\"\"\n@@ -45,7 +45,7 @@ your own training & evaluation loops from scratch. This is what this guide is ab\n To write a custom training loop, we need the following ingredients:\n \n - A model to train, of course.\n-- An optimizer. You could either use an optimizer from `keras_core.optimizers`, or\n+- An optimizer. You could either use an optimizer from `keras.optimizers`, or\n one from the `optax` package.\n - A loss function.\n - A dataset. The standard in the JAX ecosystem is to load data via `tf.data`,\n\n@@ -17,7 +17,7 @@ import os\n os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n \n import tensorflow as tf\n-import keras_core as keras\n+import keras as keras\n import numpy as np\n \n \"\"\"\n\n@@ -16,7 +16,7 @@ import os\n os.environ[\"KERAS_BACKEND\"] = \"torch\"\n \n import torch\n-import keras_core as keras\n+import keras as keras\n import numpy as np\n \n \"\"\"\n@@ -42,9 +42,9 @@ your own training & evaluation loops from scratch. This is what this guide is ab\n To write a custom training loop, we need the following ingredients:\n \n - A model to train, of course.\n-- An optimizer. You could either use a `keras_core.optimizers` optimizer,\n+- An optimizer. You could either use a `keras.optimizers` optimizer,\n or a native PyTorch optimizer from `torch.optim`.\n-- A loss function. You could either use a `keras_core.losses` loss,\n+- A loss function. You could either use a `keras.losses` loss,\n or a native PyTorch loss from `torch.nn`.\n - A dataset. You could use any format: a `tf.data.Dataset`,\n a PyTorch `DataLoader`, a Python generator, etc.\n\n@@ -24,7 +24,7 @@ started.\n \"\"\"\n \n import numpy as np\n-import keras_core as keras\n+import keras as keras\n \n \"\"\"\n ## Keras callbacks overview\n\n@@ -1,12 +1,12 @@\n import numpy as np\n import tensorflow as tf\n \n-from keras_core import layers\n-from keras_core import losses\n-from keras_core import metrics\n-from keras_core import models\n-from keras_core import optimizers\n-from keras_core.utils import rng_utils\n+from keras import layers\n+from keras import losses\n+from keras import metrics\n+from keras import models\n+from keras import optimizers\n+from keras.utils import rng_utils\n \n \n def test_model_fit():\n\n@@ -1,7 +1,7 @@\n import torch\n \n-from keras_core import layers\n-from keras_core.backend.common import KerasVariable\n+from keras import layers\n+from keras.backend.common import KerasVariable\n \n \n class Net(torch.nn.Module):\n\n@@ -1,13 +1,13 @@\n-import keras_core\n-from keras_core.utils import plot_model\n+import keras\n+from keras.utils import plot_model\n \n \n def plot_sequential_model():\n-    model = keras_core.Sequential(\n+    model = keras.Sequential(\n         [\n-            keras_core.Input((3,)),\n-            keras_core.layers.Dense(4, activation=\"relu\"),\n-            keras_core.layers.Dense(1, activation=\"sigmoid\"),\n+            keras.Input((3,)),\n+            keras.layers.Dense(4, activation=\"relu\"),\n+            keras.layers.Dense(1, activation=\"sigmoid\"),\n         ]\n     )\n     plot_model(model, \"sequential.png\")\n@@ -61,22 +61,22 @@ def plot_sequential_model():\n \n \n def plot_functional_model():\n-    inputs = keras_core.Input((3,))\n-    x = keras_core.layers.Dense(4, activation=\"relu\", trainable=False)(inputs)\n+    inputs = keras.Input((3,))\n+    x = keras.layers.Dense(4, activation=\"relu\", trainable=False)(inputs)\n     residual = x\n-    x = keras_core.layers.Dense(4, activation=\"relu\")(x)\n-    x = keras_core.layers.Dense(4, activation=\"relu\")(x)\n-    x = keras_core.layers.Dense(4, activation=\"relu\")(x)\n+    x = keras.layers.Dense(4, activation=\"relu\")(x)\n+    x = keras.layers.Dense(4, activation=\"relu\")(x)\n+    x = keras.layers.Dense(4, activation=\"relu\")(x)\n     x += residual\n     residual = x\n-    x = keras_core.layers.Dense(4, activation=\"relu\")(x)\n-    x = keras_core.layers.Dense(4, activation=\"relu\")(x)\n-    x = keras_core.layers.Dense(4, activation=\"relu\")(x)\n+    x = keras.layers.Dense(4, activation=\"relu\")(x)\n+    x = keras.layers.Dense(4, activation=\"relu\")(x)\n+    x = keras.layers.Dense(4, activation=\"relu\")(x)\n     x += residual\n-    x = keras_core.layers.Dropout(0.5)(x)\n-    outputs = keras_core.layers.Dense(1, activation=\"sigmoid\")(x)\n+    x = keras.layers.Dropout(0.5)(x)\n+    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n \n-    model = keras_core.Model(inputs, outputs)\n+    model = keras.Model(inputs, outputs)\n     plot_model(model, \"functional.png\")\n     plot_model(model, \"functional-show_shapes.png\", show_shapes=True)\n     plot_model(\n@@ -135,11 +135,11 @@ def plot_functional_model():\n \n \n def plot_subclassed_model():\n-    class MyModel(keras_core.Model):\n+    class MyModel(keras.Model):\n         def __init__(self, **kwargs):\n             super().__init__(**kwargs)\n-            self.dense_1 = keras_core.layers.Dense(3, activation=\"relu\")\n-            self.dense_2 = keras_core.layers.Dense(1, activation=\"sigmoid\")\n+            self.dense_1 = keras.layers.Dense(3, activation=\"relu\")\n+            self.dense_2 = keras.layers.Dense(1, activation=\"sigmoid\")\n \n         def call(self, x):\n             return self.dense_2(self.dense_1(x))\n@@ -205,25 +205,25 @@ def plot_subclassed_model():\n \n \n def plot_nested_functional_model():\n-    inputs = keras_core.Input((3,))\n-    x = keras_core.layers.Dense(4, activation=\"relu\")(inputs)\n-    x = keras_core.layers.Dense(4, activation=\"relu\")(x)\n-    outputs = keras_core.layers.Dense(3, activation=\"relu\")(x)\n-    inner_model = keras_core.Model(inputs, outputs)\n+    inputs = keras.Input((3,))\n+    x = keras.layers.Dense(4, activation=\"relu\")(inputs)\n+    x = keras.layers.Dense(4, activation=\"relu\")(x)\n+    outputs = keras.layers.Dense(3, activation=\"relu\")(x)\n+    inner_model = keras.Model(inputs, outputs)\n \n-    inputs = keras_core.Input((3,))\n-    x = keras_core.layers.Dense(3, activation=\"relu\", trainable=False)(inputs)\n+    inputs = keras.Input((3,))\n+    x = keras.layers.Dense(3, activation=\"relu\", trainable=False)(inputs)\n     residual = x\n     x = inner_model(x)\n     x += residual\n     residual = x\n-    x = keras_core.layers.Dense(4, activation=\"relu\")(x)\n-    x = keras_core.layers.Dense(4, activation=\"relu\")(x)\n-    x = keras_core.layers.Dense(3, activation=\"relu\")(x)\n+    x = keras.layers.Dense(4, activation=\"relu\")(x)\n+    x = keras.layers.Dense(4, activation=\"relu\")(x)\n+    x = keras.layers.Dense(3, activation=\"relu\")(x)\n     x += residual\n-    x = keras_core.layers.Dropout(0.5)(x)\n-    outputs = keras_core.layers.Dense(1, activation=\"sigmoid\")(x)\n-    model = keras_core.Model(inputs, outputs)\n+    x = keras.layers.Dropout(0.5)(x)\n+    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n+    model = keras.Model(inputs, outputs)\n \n     plot_model(model, \"nested-functional.png\", expand_nested=True)\n     plot_model(\n\n@@ -1,4 +1,4 @@\n-import keras_core  # isort: skip, keep it on top for torch test\n+import keras  # isort: skip, keep it on top for torch test\n \n import numpy as np\n from tensorflow import keras\n@@ -78,27 +78,27 @@ def eval_model(model, x, y):\n def numerical_test():\n     x_train, y_train, x_test, y_test = build_mnist_data(NUM_CLASSES)\n     keras_model = build_keras_model(keras, NUM_CLASSES)\n-    keras_core_model = build_keras_model(keras_core, NUM_CLASSES)\n+    keras_model = build_keras_model(keras, NUM_CLASSES)\n \n     # Make sure both model have same weights before training\n     weights = [weight.numpy() for weight in keras_model.weights]\n-    keras_core_model.set_weights(weights)\n+    keras_model.set_weights(weights)\n \n-    for kw, kcw in zip(keras_model.weights, keras_core_model.weights):\n+    for kw, kcw in zip(keras_model.weights, keras_model.weights):\n         np.testing.assert_allclose(kw.numpy(), kcw.numpy())\n \n     keras_history = train_model(keras_model, x_train, y_train)\n-    keras_core_history = train_model(keras_core_model, x_train, y_train)\n+    keras_history = train_model(keras_model, x_train, y_train)\n \n     for key in keras_history.history.keys():\n         np.testing.assert_allclose(\n             keras_history.history[key],\n-            keras_core_history.history[key],\n+            keras_history.history[key],\n             atol=1e-3,\n         )\n \n \n if __name__ == \"__main__\":\n     keras.utils.set_random_seed(1337)\n-    keras_core.utils.set_random_seed(1337)\n+    keras.utils.set_random_seed(1337)\n     numerical_test()\n\n@@ -1,15 +1,15 @@\n import numpy as np\n \n-import keras_core\n-from keras_core import layers\n-from keras_core import ops\n+import keras\n+from keras import layers\n+from keras import ops\n \n-keras_core.utils.set_random_seed(1337)\n+keras.utils.set_random_seed(1337)\n x = np.random.rand(100, 32, 32, 3)\n y = np.random.randint(0, 2, size=(100, 1))\n \n # Test sequential model.\n-model = keras_core.Sequential(\n+model = keras.Sequential(\n     [\n         layers.Conv2D(filters=10, kernel_size=3),\n         layers.GlobalAveragePooling2D(),\n@@ -35,11 +35,11 @@ model.test_on_batch(x, y)\n model.predict_on_batch(x)\n \n # Test functional model.\n-inputs = keras_core.Input(shape=(32, 32, 3))\n+inputs = keras.Input(shape=(32, 32, 3))\n outputs = layers.Conv2D(filters=10, kernel_size=3)(inputs)\n outputs = layers.GlobalAveragePooling2D()(outputs)\n outputs = layers.Dense(1, activation=\"sigmoid\")(outputs)\n-model = keras_core.Model(inputs, outputs)\n+model = keras.Model(inputs, outputs)\n model.compile(\n     loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"mae\", \"accuracy\"]\n )\n@@ -74,12 +74,12 @@ class Linear(layers.Layer):\n         return ops.matmul(inputs, self.w) + self.b\n \n \n-inputs = keras_core.Input(shape=(32, 32, 3))\n+inputs = keras.Input(shape=(32, 32, 3))\n outputs = layers.Conv2D(filters=10, kernel_size=3)(inputs)\n outputs = layers.GlobalAveragePooling2D()(outputs)\n outputs = Linear(1)(outputs)\n outputs = layers.Activation(\"sigmoid\")(outputs)\n-model = keras_core.Model(inputs, outputs)\n+model = keras.Model(inputs, outputs)\n model.compile(\n     loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"mae\", \"accuracy\"]\n )\n\n@@ -0,0 +1,19 @@\n+from keras import activations\n+from keras import applications\n+from keras import backend\n+from keras import constraints\n+from keras import datasets\n+from keras import initializers\n+from keras import layers\n+from keras import models\n+from keras import ops\n+from keras import optimizers\n+from keras import regularizers\n+from keras import utils\n+from keras.backend import KerasTensor\n+from keras.layers import Input\n+from keras.layers import Layer\n+from keras.models import Functional\n+from keras.models import Model\n+from keras.models import Sequential\n+from keras.version import __version__\n\n@@ -1,25 +1,25 @@\n import types\n \n-from keras_core.activations.activations import elu\n-from keras_core.activations.activations import exponential\n-from keras_core.activations.activations import gelu\n-from keras_core.activations.activations import hard_sigmoid\n-from keras_core.activations.activations import leaky_relu\n-from keras_core.activations.activations import linear\n-from keras_core.activations.activations import log_softmax\n-from keras_core.activations.activations import mish\n-from keras_core.activations.activations import relu\n-from keras_core.activations.activations import relu6\n-from keras_core.activations.activations import selu\n-from keras_core.activations.activations import sigmoid\n-from keras_core.activations.activations import silu\n-from keras_core.activations.activations import softmax\n-from keras_core.activations.activations import softplus\n-from keras_core.activations.activations import softsign\n-from keras_core.activations.activations import tanh\n-from keras_core.api_export import keras_core_export\n-from keras_core.saving import object_registration\n-from keras_core.saving import serialization_lib\n+from keras.activations.activations import elu\n+from keras.activations.activations import exponential\n+from keras.activations.activations import gelu\n+from keras.activations.activations import hard_sigmoid\n+from keras.activations.activations import leaky_relu\n+from keras.activations.activations import linear\n+from keras.activations.activations import log_softmax\n+from keras.activations.activations import mish\n+from keras.activations.activations import relu\n+from keras.activations.activations import relu6\n+from keras.activations.activations import selu\n+from keras.activations.activations import sigmoid\n+from keras.activations.activations import silu\n+from keras.activations.activations import softmax\n+from keras.activations.activations import softplus\n+from keras.activations.activations import softsign\n+from keras.activations.activations import tanh\n+from keras.api_export import keras_export\n+from keras.saving import object_registration\n+from keras.saving import serialization_lib\n \n ALL_OBJECTS = {\n     relu,\n@@ -46,7 +46,7 @@ ALL_OBJECTS_DICT = {fn.__name__: fn for fn in ALL_OBJECTS}\n ALL_OBJECTS_DICT[\"swish\"] = silu\n \n \n-@keras_core_export(\"keras_core.activations.serialize\")\n+@keras_export(\"keras.activations.serialize\")\n def serialize(activation):\n     fn_config = serialization_lib.serialize_keras_object(activation)\n     if \"config\" not in fn_config:\n@@ -55,7 +55,7 @@ def serialize(activation):\n             \"serialized due to invalid function name. Make sure to use \"\n             \"an activation name that matches the references defined in \"\n             \"activations.py or use \"\n-            \"`@keras_core.saving.register_keras_serializable()`\"\n+            \"`@keras.saving.register_keras_serializable()`\"\n             \"to register any custom activations. \"\n             f\"config={fn_config}\"\n         )\n@@ -75,7 +75,7 @@ def serialize(activation):\n     return fn_config[\"config\"]\n \n \n-@keras_core_export(\"keras_core.activations.deserialize\")\n+@keras_export(\"keras.activations.deserialize\")\n def deserialize(config, custom_objects=None):\n     \"\"\"Return a Keras activation function via its config.\"\"\"\n     return serialization_lib.deserialize_keras_object(\n@@ -85,7 +85,7 @@ def deserialize(config, custom_objects=None):\n     )\n \n \n-@keras_core_export(\"keras_core.activations.get\")\n+@keras_export(\"keras.activations.get\")\n def get(identifier):\n     \"\"\"Retrieve a Keras activation function via an identifier.\"\"\"\n     if identifier is None:\n\n@@ -1,9 +1,9 @@\n-from keras_core import backend\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n+from keras import backend\n+from keras import ops\n+from keras.api_export import keras_export\n \n \n-@keras_core_export(\"keras_core.activations.relu\")\n+@keras_export(\"keras.activations.relu\")\n def relu(x, negative_slope=0.0, max_value=None, threshold=0.0):\n     \"\"\"Applies the rectified linear unit activation function.\n \n@@ -17,13 +17,13 @@ def relu(x, negative_slope=0.0, max_value=None, threshold=0.0):\n     Examples:\n \n     >>> x = [-10, -5, 0.0, 5, 10]\n-    >>> keras_core.activations.relu(x)\n+    >>> keras.activations.relu(x)\n     [ 0.,  0.,  0.,  5., 10.]\n-    >>> keras_core.activations.relu(x, negative_slope=0.5)\n+    >>> keras.activations.relu(x, negative_slope=0.5)\n     [-5. , -2.5,  0. ,  5. , 10. ]\n-    >>> keras_core.activations.relu(x, max_value=5.)\n+    >>> keras.activations.relu(x, max_value=5.)\n     [0., 0., 0., 5., 5.]\n-    >>> keras_core.activations.relu(x, threshold=5.)\n+    >>> keras.activations.relu(x, threshold=5.)\n     [-0., -0.,  0.,  0., 10.]\n \n     Args:\n@@ -108,7 +108,7 @@ class ReLU(ops.Operation):\n         return x\n \n \n-@keras_core_export(\"keras_core.activations.leaky_relu\")\n+@keras_export(\"keras.activations.leaky_relu\")\n def leaky_relu(x, negative_slope=0.2):\n     \"\"\"Leaky relu activation function.\n \n@@ -120,7 +120,7 @@ def leaky_relu(x, negative_slope=0.2):\n     return ops.leaky_relu(x, negative_slope=negative_slope)\n \n \n-@keras_core_export(\"keras_core.activations.relu6\")\n+@keras_export(\"keras.activations.relu6\")\n def relu6(x):\n     \"\"\"Relu6 activation function.\n \n@@ -132,7 +132,7 @@ def relu6(x):\n     return ops.relu6(x)\n \n \n-@keras_core_export(\"keras_core.activations.softmax\")\n+@keras_export(\"keras.activations.softmax\")\n def softmax(x, axis=-1):\n     \"\"\"Softmax converts a vector of values to a probability distribution.\n \n@@ -165,7 +165,7 @@ def softmax(x, axis=-1):\n     return output\n \n \n-@keras_core_export(\"keras_core.activations.elu\")\n+@keras_export(\"keras.activations.elu\")\n def elu(x, alpha=1.0):\n     \"\"\"Exponential Linear Unit.\n \n@@ -193,7 +193,7 @@ def elu(x, alpha=1.0):\n     return ops.elu(x, alpha=alpha)\n \n \n-@keras_core_export(\"keras_core.activations.selu\")\n+@keras_export(\"keras.activations.selu\")\n def selu(x):\n     \"\"\"Scaled Exponential Linear Unit (SELU).\n \n@@ -206,13 +206,13 @@ def selu(x):\n     (`alpha=1.67326324` and `scale=1.05070098`).\n \n     Basically, the SELU activation function multiplies `scale` (> 1) with the\n-    output of the `keras_core.activations.elu` function to ensure a slope larger\n+    output of the `keras.activations.elu` function to ensure a slope larger\n     than one for positive inputs.\n \n     The values of `alpha` and `scale` are\n     chosen so that the mean and variance of the inputs are preserved\n     between two consecutive layers as long as the weights are initialized\n-    correctly (see `keras_core.initializers.LecunNormal` initializer)\n+    correctly (see `keras.initializers.LecunNormal` initializer)\n     and the number of input units is \"large enough\"\n     (see reference paper for more information).\n \n@@ -222,9 +222,9 @@ def selu(x):\n     Notes:\n \n     - To be used together with the\n-        `keras_core.initializers.LecunNormal` initializer.\n+        `keras.initializers.LecunNormal` initializer.\n     - To be used together with the dropout variant\n-        `keras_core.layers.AlphaDropout` (rather than regular dropout).\n+        `keras.layers.AlphaDropout` (rather than regular dropout).\n \n     Reference:\n \n@@ -233,7 +233,7 @@ def selu(x):\n     return ops.selu(x)\n \n \n-@keras_core_export(\"keras_core.activations.softplus\")\n+@keras_export(\"keras.activations.softplus\")\n def softplus(x):\n     \"\"\"Softplus activation function.\n \n@@ -245,7 +245,7 @@ def softplus(x):\n     return ops.softplus(x)\n \n \n-@keras_core_export(\"keras_core.activations.softsign\")\n+@keras_export(\"keras.activations.softsign\")\n def softsign(x):\n     \"\"\"Softsign activation function.\n \n@@ -257,8 +257,8 @@ def softsign(x):\n     return ops.softsign(x)\n \n \n-@keras_core_export(\n-    [\"keras_core.activations.silu\", \"keras_core.activations.swish\"]\n+@keras_export(\n+    [\"keras.activations.silu\", \"keras.activations.swish\"]\n )\n def silu(x):\n     \"\"\"Swish (or Silu) activation function.\n@@ -279,7 +279,7 @@ def silu(x):\n     return ops.silu(x)\n \n \n-@keras_core_export(\"keras_core.activations.gelu\")\n+@keras_export(\"keras.activations.gelu\")\n def gelu(x, approximate=False):\n     \"\"\"Gaussian error linear unit (GELU) activation function.\n \n@@ -302,7 +302,7 @@ def gelu(x, approximate=False):\n     return ops.gelu(x, approximate=approximate)\n \n \n-@keras_core_export(\"keras_core.activations.tanh\")\n+@keras_export(\"keras.activations.tanh\")\n def tanh(x):\n     \"\"\"Hyperbolic tangent activation function.\n \n@@ -316,7 +316,7 @@ def tanh(x):\n     return ops.tanh(x)\n \n \n-@keras_core_export(\"keras_core.activations.sigmoid\")\n+@keras_export(\"keras.activations.sigmoid\")\n def sigmoid(x):\n     \"\"\"Sigmoid activation function.\n \n@@ -343,7 +343,7 @@ def sigmoid(x):\n     return output\n \n \n-@keras_core_export(\"keras_core.activations.exponential\")\n+@keras_export(\"keras.activations.exponential\")\n def exponential(x):\n     \"\"\"Exponential activation function.\n \n@@ -353,7 +353,7 @@ def exponential(x):\n     return ops.exp(x)\n \n \n-@keras_core_export(\"keras_core.activations.hard_sigmoid\")\n+@keras_export(\"keras.activations.hard_sigmoid\")\n def hard_sigmoid(x):\n     \"\"\"Hard sigmoid activation function.\n \n@@ -376,7 +376,7 @@ def hard_sigmoid(x):\n     return ops.hard_sigmoid(x)\n \n \n-@keras_core_export(\"keras_core.activations.linear\")\n+@keras_export(\"keras.activations.linear\")\n def linear(x):\n     \"\"\"Linear activation function (pass-through).\n \n@@ -401,7 +401,7 @@ class Mish(ops.Operation):\n         return x * backend.nn.tanh(backend.nn.softplus(x))\n \n \n-@keras_core_export(\"keras_core.activations.mish\")\n+@keras_export(\"keras.activations.mish\")\n def mish(x):\n     \"\"\"Mish activation function.\n \n@@ -424,7 +424,7 @@ def mish(x):\n     return Mish.static_call(x)\n \n \n-@keras_core_export(\"keras_core.activations.log_softmax\")\n+@keras_export(\"keras.activations.log_softmax\")\n def log_softmax(x, axis=-1):\n     \"\"\"Log-Softmax activation function.\n \n\n@@ -1,8 +1,8 @@\n import numpy as np\n \n-from keras_core import activations\n-from keras_core import backend\n-from keras_core import testing\n+from keras import activations\n+from keras import backend\n+from keras import testing\n \n \n def _ref_softmax(values):\n\n@@ -30,9 +30,9 @@ def get_name_from_symbol(symbol):\n \n if namex:\n \n-    class keras_core_export(namex.export):\n+    class keras_export(namex.export):\n         def __init__(self, path):\n-            super().__init__(package=\"keras_core\", path=path)\n+            super().__init__(package=\"keras\", path=path)\n \n         def __call__(self, symbol):\n             register_internal_serializable(self.path, symbol)\n@@ -40,7 +40,7 @@ if namex:\n \n else:\n \n-    class keras_core_export:\n+    class keras_export:\n         def __init__(self, path):\n             self.path = path\n \n\n\n@@ -4,26 +4,26 @@ import numpy as np\n import pytest\n from absl.testing import parameterized\n \n-from keras_core import backend\n-from keras_core import testing\n-from keras_core.applications import convnext\n-from keras_core.applications import densenet\n-from keras_core.applications import efficientnet\n-from keras_core.applications import efficientnet_v2\n-from keras_core.applications import inception_resnet_v2\n-from keras_core.applications import inception_v3\n-from keras_core.applications import mobilenet\n-from keras_core.applications import mobilenet_v2\n-from keras_core.applications import mobilenet_v3\n-from keras_core.applications import nasnet\n-from keras_core.applications import resnet\n-from keras_core.applications import resnet_v2\n-from keras_core.applications import vgg16\n-from keras_core.applications import vgg19\n-from keras_core.applications import xception\n-from keras_core.saving import serialization_lib\n-from keras_core.utils import file_utils\n-from keras_core.utils import image_utils\n+from keras import backend\n+from keras import testing\n+from keras.applications import convnext\n+from keras.applications import densenet\n+from keras.applications import efficientnet\n+from keras.applications import efficientnet_v2\n+from keras.applications import inception_resnet_v2\n+from keras.applications import inception_v3\n+from keras.applications import mobilenet\n+from keras.applications import mobilenet_v2\n+from keras.applications import mobilenet_v3\n+from keras.applications import nasnet\n+from keras.applications import resnet\n+from keras.applications import resnet_v2\n+from keras.applications import vgg16\n+from keras.applications import vgg19\n+from keras.applications import xception\n+from keras.saving import serialization_lib\n+from keras.utils import file_utils\n+from keras.utils import image_utils\n \n try:\n     import PIL\n\n@@ -1,17 +1,17 @@\n import numpy as np\n \n-from keras_core import backend\n-from keras_core import initializers\n-from keras_core import layers\n-from keras_core import ops\n-from keras_core import random\n-from keras_core.api_export import keras_core_export\n-from keras_core.applications import imagenet_utils\n-from keras_core.layers.layer import Layer\n-from keras_core.models import Functional\n-from keras_core.models import Sequential\n-from keras_core.ops import operation_utils\n-from keras_core.utils import file_utils\n+from keras import backend\n+from keras import initializers\n+from keras import layers\n+from keras import ops\n+from keras import random\n+from keras.api_export import keras_export\n+from keras.applications import imagenet_utils\n+from keras.layers.layer import Layer\n+from keras.models import Functional\n+from keras.models import Sequential\n+from keras.ops import operation_utils\n+from keras.utils import file_utils\n \n BASE_WEIGHTS_PATH = (\n     \"https://storage.googleapis.com/tensorflow/keras-applications/convnext/\"\n@@ -539,10 +539,10 @@ def ConvNeXt(\n ## Instantiating variants ##\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.convnext.ConvNeXtTiny\",\n-        \"keras_core.applications.ConvNeXtTiny\",\n+        \"keras.applications.convnext.ConvNeXtTiny\",\n+        \"keras.applications.ConvNeXtTiny\",\n     ]\n )\n def ConvNeXtTiny(\n@@ -574,10 +574,10 @@ def ConvNeXtTiny(\n     )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.convnext.ConvNeXtSmall\",\n-        \"keras_core.applications.ConvNeXtSmall\",\n+        \"keras.applications.convnext.ConvNeXtSmall\",\n+        \"keras.applications.ConvNeXtSmall\",\n     ]\n )\n def ConvNeXtSmall(\n@@ -609,10 +609,10 @@ def ConvNeXtSmall(\n     )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.convnext.ConvNeXtBase\",\n-        \"keras_core.applications.ConvNeXtBase\",\n+        \"keras.applications.convnext.ConvNeXtBase\",\n+        \"keras.applications.ConvNeXtBase\",\n     ]\n )\n def ConvNeXtBase(\n@@ -644,10 +644,10 @@ def ConvNeXtBase(\n     )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.convnext.ConvNeXtLarge\",\n-        \"keras_core.applications.ConvNeXtLarge\",\n+        \"keras.applications.convnext.ConvNeXtLarge\",\n+        \"keras.applications.ConvNeXtLarge\",\n     ]\n )\n def ConvNeXtLarge(\n@@ -679,10 +679,10 @@ def ConvNeXtLarge(\n     )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.convnext.ConvNeXtXLarge\",\n-        \"keras_core.applications.ConvNeXtXLarge\",\n+        \"keras.applications.convnext.ConvNeXtXLarge\",\n+        \"keras.applications.ConvNeXtXLarge\",\n     ]\n )\n def ConvNeXtXLarge(\n@@ -721,7 +721,7 @@ ConvNeXtLarge.__doc__ = BASE_DOCSTRING.format(name=\"ConvNeXtLarge\")\n ConvNeXtXLarge.__doc__ = BASE_DOCSTRING.format(name=\"ConvNeXtXLarge\")\n \n \n-@keras_core_export(\"keras_core.applications.convnext.preprocess_input\")\n+@keras_export(\"keras.applications.convnext.preprocess_input\")\n def preprocess_input(x, data_format=None):\n     \"\"\"A placeholder method for backward compatibility.\n \n@@ -734,7 +734,7 @@ def preprocess_input(x, data_format=None):\n         x: A floating point `numpy.array` or a tensor.\n         data_format: Optional data format of the image tensor/array. Defaults to\n             None, in which case the global setting\n-            `keras_core.backend.image_data_format()` is used\n+            `keras.backend.image_data_format()` is used\n             (unless you changed it, it defaults to `\"channels_last\"`).{mode}\n \n     Returns:\n@@ -743,7 +743,7 @@ def preprocess_input(x, data_format=None):\n     return x\n \n \n-@keras_core_export(\"keras_core.applications.convnext.decode_predictions\")\n+@keras_export(\"keras.applications.convnext.decode_predictions\")\n def decode_predictions(preds, top=5):\n     return imagenet_utils.decode_predictions(preds, top=top)\n \n\n@@ -1,10 +1,10 @@\n-from keras_core import backend\n-from keras_core import layers\n-from keras_core.api_export import keras_core_export\n-from keras_core.applications import imagenet_utils\n-from keras_core.models import Functional\n-from keras_core.ops import operation_utils\n-from keras_core.utils import file_utils\n+from keras import backend\n+from keras import layers\n+from keras.api_export import keras_export\n+from keras.applications import imagenet_utils\n+from keras.models import Functional\n+from keras.ops import operation_utils\n+from keras.utils import file_utils\n \n BASE_WEIGHTS_PATH = (\n     \"https://storage.googleapis.com/tensorflow/keras-applications/densenet/\"\n@@ -132,7 +132,7 @@ def DenseNet(\n       https://keras.io/guides/transfer_learning/).\n \n     Note: each Keras Application expects a specific kind of input preprocessing.\n-    For DenseNet, call `keras_core.applications.densenet.preprocess_input`\n+    For DenseNet, call `keras.applications.densenet.preprocess_input`\n     on your inputs before passing them to the model.\n     `densenet.preprocess_input` will scale pixels between 0 and 1 and then\n     will normalize each channel with respect to the ImageNet\n@@ -317,10 +317,10 @@ def DenseNet(\n     return model\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.densenet.DenseNet121\",\n-        \"keras_core.applications.DenseNet121\",\n+        \"keras.applications.densenet.DenseNet121\",\n+        \"keras.applications.DenseNet121\",\n     ]\n )\n def DenseNet121(\n@@ -345,10 +345,10 @@ def DenseNet121(\n     )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.densenet.DenseNet169\",\n-        \"keras_core.applications.DenseNet169\",\n+        \"keras.applications.densenet.DenseNet169\",\n+        \"keras.applications.DenseNet169\",\n     ]\n )\n def DenseNet169(\n@@ -373,10 +373,10 @@ def DenseNet169(\n     )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.densenet.DenseNet201\",\n-        \"keras_core.applications.DenseNet201\",\n+        \"keras.applications.densenet.DenseNet201\",\n+        \"keras.applications.DenseNet201\",\n     ]\n )\n def DenseNet201(\n@@ -401,14 +401,14 @@ def DenseNet201(\n     )\n \n \n-@keras_core_export(\"keras_core.applications.densenet.preprocess_input\")\n+@keras_export(\"keras.applications.densenet.preprocess_input\")\n def preprocess_input(x, data_format=None):\n     return imagenet_utils.preprocess_input(\n         x, data_format=data_format, mode=\"torch\"\n     )\n \n \n-@keras_core_export(\"keras_core.applications.densenet.decode_predictions\")\n+@keras_export(\"keras.applications.densenet.decode_predictions\")\n def decode_predictions(preds, top=5):\n     return imagenet_utils.decode_predictions(preds, top=top)\n \n@@ -431,7 +431,7 @@ Note that the data format convention used by the model is\n the one specified in your Keras config at `~/.keras/keras.json`.\n \n Note: each Keras Application expects a specific kind of input preprocessing.\n-For DenseNet, call `keras_core.applications.densenet.preprocess_input`\n+For DenseNet, call `keras.applications.densenet.preprocess_input`\n on your inputs before passing them to the model.\n \n Args:\n\n@@ -1,13 +1,13 @@\n import copy\n import math\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core.api_export import keras_core_export\n-from keras_core.applications import imagenet_utils\n-from keras_core.models import Functional\n-from keras_core.ops import operation_utils\n-from keras_core.utils import file_utils\n+from keras import backend\n+from keras import layers\n+from keras.api_export import keras_export\n+from keras.applications import imagenet_utils\n+from keras.models import Functional\n+from keras.ops import operation_utils\n+from keras.utils import file_utils\n \n BASE_WEIGHTS_PATH = \"https://storage.googleapis.com/keras-applications/\"\n \n@@ -157,7 +157,7 @@ https://keras.io/guides/transfer_learning/).\n Note: each Keras Application expects a specific kind of input preprocessing.\n For EfficientNet, input preprocessing is included as part of the model\n (as a `Rescaling` layer), and thus\n-`keras_core.applications.efficientnet.preprocess_input` is actually a\n+`keras.applications.efficientnet.preprocess_input` is actually a\n pass-through function. EfficientNet models expect their inputs to be float\n tensors of pixels with values in the `[0-255]` range.\n \n@@ -549,10 +549,10 @@ def block(\n     return x\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.efficientnet.EfficientNetB0\",\n-        \"keras_core.applications.EfficientNetB0\",\n+        \"keras.applications.efficientnet.EfficientNetB0\",\n+        \"keras.applications.EfficientNetB0\",\n     ]\n )\n def EfficientNetB0(\n@@ -582,10 +582,10 @@ def EfficientNetB0(\n     )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.efficientnet.EfficientNetB1\",\n-        \"keras_core.applications.EfficientNetB1\",\n+        \"keras.applications.efficientnet.EfficientNetB1\",\n+        \"keras.applications.EfficientNetB1\",\n     ]\n )\n def EfficientNetB1(\n@@ -615,10 +615,10 @@ def EfficientNetB1(\n     )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.efficientnet.EfficientNetB2\",\n-        \"keras_core.applications.EfficientNetB2\",\n+        \"keras.applications.efficientnet.EfficientNetB2\",\n+        \"keras.applications.EfficientNetB2\",\n     ]\n )\n def EfficientNetB2(\n@@ -648,10 +648,10 @@ def EfficientNetB2(\n     )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.efficientnet.EfficientNetB3\",\n-        \"keras_core.applications.EfficientNetB3\",\n+        \"keras.applications.efficientnet.EfficientNetB3\",\n+        \"keras.applications.EfficientNetB3\",\n     ]\n )\n def EfficientNetB3(\n@@ -681,10 +681,10 @@ def EfficientNetB3(\n     )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.efficientnet.EfficientNetB4\",\n-        \"keras_core.applications.EfficientNetB4\",\n+        \"keras.applications.efficientnet.EfficientNetB4\",\n+        \"keras.applications.EfficientNetB4\",\n     ]\n )\n def EfficientNetB4(\n@@ -714,10 +714,10 @@ def EfficientNetB4(\n     )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.efficientnet.EfficientNetB5\",\n-        \"keras_core.applications.EfficientNetB5\",\n+        \"keras.applications.efficientnet.EfficientNetB5\",\n+        \"keras.applications.EfficientNetB5\",\n     ]\n )\n def EfficientNetB5(\n@@ -747,10 +747,10 @@ def EfficientNetB5(\n     )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.efficientnet.EfficientNetB6\",\n-        \"keras_core.applications.EfficientNetB6\",\n+        \"keras.applications.efficientnet.EfficientNetB6\",\n+        \"keras.applications.EfficientNetB6\",\n     ]\n )\n def EfficientNetB6(\n@@ -780,10 +780,10 @@ def EfficientNetB6(\n     )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.efficientnet.EfficientNetB7\",\n-        \"keras_core.applications.EfficientNetB7\",\n+        \"keras.applications.efficientnet.EfficientNetB7\",\n+        \"keras.applications.EfficientNetB7\",\n     ]\n )\n def EfficientNetB7(\n@@ -823,7 +823,7 @@ EfficientNetB6.__doc__ = BASE_DOCSTRING.format(name=\"EfficientNetB6\")\n EfficientNetB7.__doc__ = BASE_DOCSTRING.format(name=\"EfficientNetB7\")\n \n \n-@keras_core_export(\"keras_core.applications.efficientnet.preprocess_input\")\n+@keras_export(\"keras.applications.efficientnet.preprocess_input\")\n def preprocess_input(x, data_format=None):\n     \"\"\"A placeholder method for backward compatibility.\n \n@@ -835,7 +835,7 @@ def preprocess_input(x, data_format=None):\n     Args:\n         x: A floating point `numpy.array` or a tensor.\n         data_format: Optional data format of the image tensor/array. `None`\n-            means the global setting `keras_core.backend.image_data_format()`\n+            means the global setting `keras.backend.image_data_format()`\n             is used (unless you changed it, it uses `\"channels_last\"`).\n             Defaults to `None`.\n \n@@ -845,7 +845,7 @@ def preprocess_input(x, data_format=None):\n     return x\n \n \n-@keras_core_export(\"keras_core.applications.efficientnet.decode_predictions\")\n+@keras_export(\"keras.applications.efficientnet.decode_predictions\")\n def decode_predictions(preds, top=5):\n     return imagenet_utils.decode_predictions(preds, top=top)\n \n\n@@ -1,14 +1,14 @@\n import copy\n import math\n \n-from keras_core import backend\n-from keras_core import initializers\n-from keras_core import layers\n-from keras_core.api_export import keras_core_export\n-from keras_core.applications import imagenet_utils\n-from keras_core.models import Functional\n-from keras_core.ops import operation_utils\n-from keras_core.utils import file_utils\n+from keras import backend\n+from keras import initializers\n+from keras import layers\n+from keras.api_export import keras_export\n+from keras.applications import imagenet_utils\n+from keras.models import Functional\n+from keras.ops import operation_utils\n+from keras.utils import file_utils\n \n BASE_WEIGHTS_PATH = \"https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/\"  # noqa: E501\n \n@@ -538,7 +538,7 @@ https://keras.io/guides/transfer_learning/).\n Note: each Keras Application expects a specific kind of input preprocessing.\n For EfficientNetV2, by default input preprocessing is included as a part of\n the model (as a `Rescaling` layer), and thus\n-`keras_core.applications.efficientnet_v2.preprocess_input` is actually a\n+`keras.applications.efficientnet_v2.preprocess_input` is actually a\n pass-through function. In this use case, EfficientNetV2 models expect their\n inputs to be float tensors of pixels with values in the `[0, 255]` range.\n At the same time, preprocessing as a part of the model (i.e. `Rescaling`\n@@ -1081,10 +1081,10 @@ def EfficientNetV2(\n     return model\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.efficientnet_v2.EfficientNetV2B0\",\n-        \"keras_core.applications.EfficientNetV2B0\",\n+        \"keras.applications.efficientnet_v2.EfficientNetV2B0\",\n+        \"keras.applications.EfficientNetV2B0\",\n     ]\n )\n def EfficientNetV2B0(\n@@ -1113,10 +1113,10 @@ def EfficientNetV2B0(\n     )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.efficientnet_v2.EfficientNetV2B1\",\n-        \"keras_core.applications.EfficientNetV2B1\",\n+        \"keras.applications.efficientnet_v2.EfficientNetV2B1\",\n+        \"keras.applications.EfficientNetV2B1\",\n     ]\n )\n def EfficientNetV2B1(\n@@ -1145,10 +1145,10 @@ def EfficientNetV2B1(\n     )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.efficientnet_v2.EfficientNetV2B2\",\n-        \"keras_core.applications.EfficientNetV2B2\",\n+        \"keras.applications.efficientnet_v2.EfficientNetV2B2\",\n+        \"keras.applications.EfficientNetV2B2\",\n     ]\n )\n def EfficientNetV2B2(\n@@ -1177,10 +1177,10 @@ def EfficientNetV2B2(\n     )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.efficientnet_v2.EfficientNetV2B3\",\n-        \"keras_core.applications.EfficientNetV2B3\",\n+        \"keras.applications.efficientnet_v2.EfficientNetV2B3\",\n+        \"keras.applications.EfficientNetV2B3\",\n     ]\n )\n def EfficientNetV2B3(\n@@ -1209,10 +1209,10 @@ def EfficientNetV2B3(\n     )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.efficientnet_v2.EfficientNetV2S\",\n-        \"keras_core.applications.EfficientNetV2S\",\n+        \"keras.applications.efficientnet_v2.EfficientNetV2S\",\n+        \"keras.applications.EfficientNetV2S\",\n     ]\n )\n def EfficientNetV2S(\n@@ -1241,10 +1241,10 @@ def EfficientNetV2S(\n     )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.efficientnet_v2.EfficientNetV2M\",\n-        \"keras_core.applications.EfficientNetV2M\",\n+        \"keras.applications.efficientnet_v2.EfficientNetV2M\",\n+        \"keras.applications.EfficientNetV2M\",\n     ]\n )\n def EfficientNetV2M(\n@@ -1273,10 +1273,10 @@ def EfficientNetV2M(\n     )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.efficientnet_v2.EfficientNetV2L\",\n-        \"keras_core.applications.EfficientNetV2L\",\n+        \"keras.applications.efficientnet_v2.EfficientNetV2L\",\n+        \"keras.applications.EfficientNetV2L\",\n     ]\n )\n def EfficientNetV2L(\n@@ -1314,7 +1314,7 @@ EfficientNetV2M.__doc__ = BASE_DOCSTRING.format(name=\"EfficientNetV2M\")\n EfficientNetV2L.__doc__ = BASE_DOCSTRING.format(name=\"EfficientNetV2L\")\n \n \n-@keras_core_export(\"keras_core.applications.efficientnet_v2.preprocess_input\")\n+@keras_export(\"keras.applications.efficientnet_v2.preprocess_input\")\n def preprocess_input(x, data_format=None):\n     \"\"\"A placeholder method for backward compatibility.\n \n@@ -1327,7 +1327,7 @@ def preprocess_input(x, data_format=None):\n         x: A floating point `numpy.array` or a tensor.\n         data_format: Optional data format of the image tensor/array. Defaults to\n             None, in which case the global setting\n-            `keras_core.backend.image_data_format()` is used\n+            `keras.backend.image_data_format()` is used\n             (unless you changed it, it defaults to \"channels_last\").{mode}\n \n     Returns:\n@@ -1336,7 +1336,7 @@ def preprocess_input(x, data_format=None):\n     return x\n \n \n-@keras_core_export(\"keras_core.applications.efficientnet_v2.decode_predictions\")\n+@keras_export(\"keras.applications.efficientnet_v2.decode_predictions\")\n def decode_predictions(preds, top=5):\n     return imagenet_utils.decode_predictions(preds, top=top)\n \n\n@@ -3,11 +3,11 @@ import warnings\n \n import numpy as np\n \n-from keras_core import activations\n-from keras_core import backend\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.utils import file_utils\n+from keras import activations\n+from keras import backend\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.utils import file_utils\n \n CLASS_INDEX = None\n CLASS_INDEX_PATH = (\n@@ -22,12 +22,12 @@ PREPROCESS_INPUT_DOC = \"\"\"\n   Usage example with `applications.MobileNet`:\n \n   ```python\n-  i = keras_core.layers.Input([None, None, 3], dtype=\"uint8\")\n+  i = keras.layers.Input([None, None, 3], dtype=\"uint8\")\n   x = ops.cast(i, \"float32\")\n-  x = keras_core.applications.mobilenet.preprocess_input(x)\n-  core = keras_core.applications.MobileNet()\n+  x = keras.applications.mobilenet.preprocess_input(x)\n+  core = keras.applications.MobileNet()\n   x = core(x)\n-  model = keras_core.Model(inputs=[i], outputs=[x])\n+  model = keras.Model(inputs=[i], outputs=[x])\n   result = model(image)\n   ```\n \n@@ -39,7 +39,7 @@ PREPROCESS_INPUT_DOC = \"\"\"\n         if the data types are compatible. To avoid this\n         behaviour, `numpy.copy(x)` can be used.\n         data_format: Optional data format of the image tensor/array. None, means\n-        the global setting `keras_core.backend.image_data_format()` is used\n+        the global setting `keras.backend.image_data_format()` is used\n         (unless you changed it, it uses \"channels_last\").{mode}\n         Defaults to `None`.\n \n@@ -83,7 +83,7 @@ PREPROCESS_INPUT_RET_DOC_CAFFE = \"\"\"\n       zero-centered with respect to the ImageNet dataset, without scaling.\"\"\"\n \n \n-@keras_core_export(\"keras_core.applications.imagenet_utils.preprocess_input\")\n+@keras_export(\"keras.applications.imagenet_utils.preprocess_input\")\n def preprocess_input(x, data_format=None, mode=\"caffe\"):\n     \"\"\"Preprocesses a tensor or Numpy array encoding a batch of images.\"\"\"\n     if mode not in {\"caffe\", \"tf\", \"torch\"}:\n@@ -113,7 +113,7 @@ preprocess_input.__doc__ = PREPROCESS_INPUT_DOC.format(\n )\n \n \n-@keras_core_export(\"keras_core.applications.imagenet_utils.decode_predictions\")\n+@keras_export(\"keras.applications.imagenet_utils.decode_predictions\")\n def decode_predictions(preds, top=5):\n     \"\"\"Decodes the prediction of an ImageNet model.\n \n\n@@ -2,10 +2,10 @@ import numpy as np\n import pytest\n from absl.testing import parameterized\n \n-import keras_core as keras\n-from keras_core import testing\n-from keras_core.applications import imagenet_utils as utils\n-from keras_core.mixed_precision import set_dtype_policy\n+import keras as keras\n+from keras import testing\n+from keras.applications import imagenet_utils as utils\n+from keras.mixed_precision import set_dtype_policy\n \n \n class TestImageNetUtils(testing.TestCase, parameterized.TestCase):\n\n@@ -1,11 +1,11 @@\n-from keras_core import backend\n-from keras_core import layers\n-from keras_core.api_export import keras_core_export\n-from keras_core.applications import imagenet_utils\n-from keras_core.layers.layer import Layer\n-from keras_core.models import Functional\n-from keras_core.ops import operation_utils\n-from keras_core.utils import file_utils\n+from keras import backend\n+from keras import layers\n+from keras.api_export import keras_export\n+from keras.applications import imagenet_utils\n+from keras.layers.layer import Layer\n+from keras.models import Functional\n+from keras.ops import operation_utils\n+from keras.utils import file_utils\n \n BASE_WEIGHT_URL = (\n     \"https://storage.googleapis.com/tensorflow/\"\n@@ -13,10 +13,10 @@ BASE_WEIGHT_URL = (\n )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.inception_resnet_v2.InceptionResNetV2\",\n-        \"keras_core.applications.InceptionResNetV2\",\n+        \"keras.applications.inception_resnet_v2.InceptionResNetV2\",\n+        \"keras.applications.InceptionResNetV2\",\n     ]\n )\n def InceptionResNetV2(\n@@ -48,7 +48,7 @@ def InceptionResNetV2(\n \n     Note: each Keras Application expects a specific kind of\n     input preprocessing. For InceptionResNetV2, call\n-    `keras_core.applications.inception_resnet_v2.preprocess_input`\n+    `keras.applications.inception_resnet_v2.preprocess_input`\n     on your inputs before passing them to the model.\n     `inception_resnet_v2.preprocess_input`\n     will scale input pixels between -1 and 1.\n@@ -374,8 +374,8 @@ def inception_resnet_block(x, scale, block_type, block_idx, activation=\"relu\"):\n     return x\n \n \n-@keras_core_export(\n-    \"keras_core.applications.inception_resnet_v2.preprocess_input\"\n+@keras_export(\n+    \"keras.applications.inception_resnet_v2.preprocess_input\"\n )\n def preprocess_input(x, data_format=None):\n     return imagenet_utils.preprocess_input(\n@@ -383,8 +383,8 @@ def preprocess_input(x, data_format=None):\n     )\n \n \n-@keras_core_export(\n-    \"keras_core.applications.inception_resnet_v2.decode_predictions\"\n+@keras_export(\n+    \"keras.applications.inception_resnet_v2.decode_predictions\"\n )\n def decode_predictions(preds, top=5):\n     return imagenet_utils.decode_predictions(preds, top=top)\n\n@@ -1,10 +1,10 @@\n-from keras_core import backend\n-from keras_core import layers\n-from keras_core.api_export import keras_core_export\n-from keras_core.applications import imagenet_utils\n-from keras_core.models import Functional\n-from keras_core.ops import operation_utils\n-from keras_core.utils import file_utils\n+from keras import backend\n+from keras import layers\n+from keras.api_export import keras_export\n+from keras.applications import imagenet_utils\n+from keras.models import Functional\n+from keras.ops import operation_utils\n+from keras.utils import file_utils\n \n WEIGHTS_PATH = (\n     \"https://storage.googleapis.com/tensorflow/keras-applications/\"\n@@ -16,10 +16,10 @@ WEIGHTS_PATH_NO_TOP = (\n )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.inception_v3.InceptionV3\",\n-        \"keras_core.applications.InceptionV3\",\n+        \"keras.applications.inception_v3.InceptionV3\",\n+        \"keras.applications.InceptionV3\",\n     ]\n )\n def InceptionV3(\n@@ -50,7 +50,7 @@ def InceptionV3(\n \n     Note: each Keras Application expects a specific kind of input preprocessing.\n     For `InceptionV3`, call\n-    `keras_core.applications.inception_v3.preprocess_input` on your inputs\n+    `keras.applications.inception_v3.preprocess_input` on your inputs\n     before passing them to the model.\n     `inception_v3.preprocess_input` will scale input pixels between -1 and 1.\n \n@@ -420,14 +420,14 @@ def conv2d_bn(\n     return x\n \n \n-@keras_core_export(\"keras_core.applications.inception_v3.preprocess_input\")\n+@keras_export(\"keras.applications.inception_v3.preprocess_input\")\n def preprocess_input(x, data_format=None):\n     return imagenet_utils.preprocess_input(\n         x, data_format=data_format, mode=\"tf\"\n     )\n \n \n-@keras_core_export(\"keras_core.applications.inception_v3.decode_predictions\")\n+@keras_export(\"keras.applications.inception_v3.decode_predictions\")\n def decode_predictions(preds, top=5):\n     return imagenet_utils.decode_predictions(preds, top=top)\n \n\n@@ -1,22 +1,22 @@\n import warnings\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core.api_export import keras_core_export\n-from keras_core.applications import imagenet_utils\n-from keras_core.models import Functional\n-from keras_core.ops import operation_utils\n-from keras_core.utils import file_utils\n+from keras import backend\n+from keras import layers\n+from keras.api_export import keras_export\n+from keras.applications import imagenet_utils\n+from keras.models import Functional\n+from keras.ops import operation_utils\n+from keras.utils import file_utils\n \n BASE_WEIGHT_PATH = (\n     \"https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/\"\n )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.mobilenet.MobileNet\",\n-        \"keras_core.applications.MobileNet\",\n+        \"keras.applications.mobilenet.MobileNet\",\n+        \"keras.applications.MobileNet\",\n     ]\n )\n def MobileNet(\n@@ -50,7 +50,7 @@ def MobileNet(\n     https://keras.io/guides/transfer_learning/).\n \n     Note: each Keras Application expects a specific kind of input preprocessing.\n-    For MobileNet, call `keras_core.applications.mobilenet.preprocess_input`\n+    For MobileNet, call `keras.applications.mobilenet.preprocess_input`\n     on your inputs before passing them to the model.\n     `mobilenet.preprocess_input` will scale input pixels between -1 and 1.\n \n@@ -408,14 +408,14 @@ def _depthwise_conv_block(\n     return layers.ReLU(6.0, name=\"conv_pw_%d_relu\" % block_id)(x)\n \n \n-@keras_core_export(\"keras_core.applications.mobilenet.preprocess_input\")\n+@keras_export(\"keras.applications.mobilenet.preprocess_input\")\n def preprocess_input(x, data_format=None):\n     return imagenet_utils.preprocess_input(\n         x, data_format=data_format, mode=\"tf\"\n     )\n \n \n-@keras_core_export(\"keras_core.applications.mobilenet.decode_predictions\")\n+@keras_export(\"keras.applications.mobilenet.decode_predictions\")\n def decode_predictions(preds, top=5):\n     return imagenet_utils.decode_predictions(preds, top=top)\n \n\n@@ -1,22 +1,22 @@\n import warnings\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core.api_export import keras_core_export\n-from keras_core.applications import imagenet_utils\n-from keras_core.models import Functional\n-from keras_core.ops import operation_utils\n-from keras_core.utils import file_utils\n+from keras import backend\n+from keras import layers\n+from keras.api_export import keras_export\n+from keras.applications import imagenet_utils\n+from keras.models import Functional\n+from keras.ops import operation_utils\n+from keras.utils import file_utils\n \n BASE_WEIGHT_PATH = (\n     \"https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/\"\n )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.mobilenet_v2.MobileNetV2\",\n-        \"keras_core.applications.MobileNetV2\",\n+        \"keras.applications.mobilenet_v2.MobileNetV2\",\n+        \"keras.applications.MobileNetV2\",\n     ]\n )\n def MobileNetV2(\n@@ -56,7 +56,7 @@ def MobileNetV2(\n \n     Note: each Keras Application expects a specific kind of input preprocessing.\n     For MobileNetV2, call\n-    `keras_core.applications.mobilenet_v2.preprocess_input`\n+    `keras.applications.mobilenet_v2.preprocess_input`\n     on your inputs before passing them to the model.\n     `mobilenet_v2.preprocess_input` will scale input pixels between -1 and 1.\n \n@@ -480,14 +480,14 @@ def _make_divisible(v, divisor, min_value=None):\n     return new_v\n \n \n-@keras_core_export(\"keras_core.applications.mobilenet_v2.preprocess_input\")\n+@keras_export(\"keras.applications.mobilenet_v2.preprocess_input\")\n def preprocess_input(x, data_format=None):\n     return imagenet_utils.preprocess_input(\n         x, data_format=data_format, mode=\"tf\"\n     )\n \n \n-@keras_core_export(\"keras_core.applications.mobilenet_v2.decode_predictions\")\n+@keras_export(\"keras.applications.mobilenet_v2.decode_predictions\")\n def decode_predictions(preds, top=5):\n     return imagenet_utils.decode_predictions(preds, top=top)\n \n\n@@ -1,12 +1,12 @@\n import warnings\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core.api_export import keras_core_export\n-from keras_core.applications import imagenet_utils\n-from keras_core.models import Functional\n-from keras_core.ops import operation_utils\n-from keras_core.utils import file_utils\n+from keras import backend\n+from keras import layers\n+from keras.api_export import keras_export\n+from keras.applications import imagenet_utils\n+from keras.models import Functional\n+from keras.ops import operation_utils\n+from keras.utils import file_utils\n \n BASE_WEIGHT_PATH = (\n     \"https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/\"\n@@ -69,7 +69,7 @@ https://keras.io/guides/transfer_learning/).\n Note: each Keras Application expects a specific kind of input preprocessing.\n For MobileNetV3, by default input preprocessing is included as a part of the\n model (as a `Rescaling` layer), and thus\n-`keras_core.applications.mobilenet_v3.preprocess_input` is actually a\n+`keras.applications.mobilenet_v3.preprocess_input` is actually a\n pass-through function. In this use case, MobileNetV3 models expect their\n inputs to be float tensors of pixels with values in the `[0-255]` range.\n At the same time, preprocessing as a part of the model (i.e. `Rescaling`\n@@ -396,7 +396,7 @@ def MobileNetV3(\n     return model\n \n \n-@keras_core_export(\"keras_core.applications.MobileNetV3Small\")\n+@keras_export(\"keras.applications.MobileNetV3Small\")\n def MobileNetV3Small(\n     input_shape=None,\n     alpha=1.0,\n@@ -461,7 +461,7 @@ def MobileNetV3Small(\n     )\n \n \n-@keras_core_export(\"keras_core.applications.MobileNetV3Large\")\n+@keras_export(\"keras.applications.MobileNetV3Large\")\n def MobileNetV3Large(\n     input_shape=None,\n     alpha=1.0,\n@@ -646,7 +646,7 @@ def _inverted_res_block(\n     return x\n \n \n-@keras_core_export(\"keras_core.applications.mobilenet_v3.preprocess_input\")\n+@keras_export(\"keras.applications.mobilenet_v3.preprocess_input\")\n def preprocess_input(x, data_format=None):\n     \"\"\"A placeholder method for backward compatibility.\n \n@@ -659,7 +659,7 @@ def preprocess_input(x, data_format=None):\n         x: A floating point `numpy.array` or a tensor.\n         data_format: Optional data format of the image tensor/array.\n             `None` means the global setting\n-            `keras_core.config.image_data_format()` is used\n+            `keras.config.image_data_format()` is used\n             (unless you changed it, it uses `\"channels_last\"`).\n             Defaults to `None`.\n \n@@ -669,7 +669,7 @@ def preprocess_input(x, data_format=None):\n     return x\n \n \n-@keras_core_export(\"keras_core.applications.mobilenet_v3.decode_predictions\")\n+@keras_export(\"keras.applications.mobilenet_v3.decode_predictions\")\n def decode_predictions(preds, top=5):\n     return imagenet_utils.decode_predictions(preds, top=top)\n \n\n@@ -1,12 +1,12 @@\n import warnings\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core.api_export import keras_core_export\n-from keras_core.applications import imagenet_utils\n-from keras_core.models import Functional\n-from keras_core.ops import operation_utils\n-from keras_core.utils import file_utils\n+from keras import backend\n+from keras import layers\n+from keras.api_export import keras_export\n+from keras.applications import imagenet_utils\n+from keras.models import Functional\n+from keras.ops import operation_utils\n+from keras.utils import file_utils\n \n BASE_WEIGHTS_PATH = (\n     \"https://storage.googleapis.com/tensorflow/keras-applications/nasnet/\"\n@@ -47,7 +47,7 @@ def NASNet(\n       https://keras.io/guides/transfer_learning/).\n \n     Note: each Keras Application expects a specific kind of input preprocessing.\n-    For NasNet, call `keras_core.applications.nasnet.preprocess_input`\n+    For NasNet, call `keras.applications.nasnet.preprocess_input`\n     on your inputs before passing them to the model.\n     `nasnet.preprocess_input` will scale input pixels between -1 and 1.\n \n@@ -311,10 +311,10 @@ def NASNet(\n     return model\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.nasnet.NASNetMobile\",\n-        \"keras_core.applications.NASNetMobile\",\n+        \"keras.applications.nasnet.NASNetMobile\",\n+        \"keras.applications.NASNetMobile\",\n     ]\n )\n def NASNetMobile(\n@@ -337,7 +337,7 @@ def NASNetMobile(\n     the one specified in your Keras config at `~/.keras/keras.json`.\n \n     Note: each Keras Application expects a specific kind of input preprocessing.\n-    For NASNet, call `keras_core.applications.nasnet.preprocess_input` on your\n+    For NASNet, call `keras.applications.nasnet.preprocess_input` on your\n     inputs before passing them to the model.\n \n     Args:\n@@ -402,10 +402,10 @@ def NASNetMobile(\n     )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.nasnet.NASNetLarge\",\n-        \"keras_core.applications.NASNetLarge\",\n+        \"keras.applications.nasnet.NASNetLarge\",\n+        \"keras.applications.NASNetLarge\",\n     ]\n )\n def NASNetLarge(\n@@ -428,7 +428,7 @@ def NASNetLarge(\n     the one specified in your Keras config at `~/.keras/keras.json`.\n \n     Note: each Keras Application expects a specific kind of input preprocessing.\n-    For NASNet, call `keras_core.applications.nasnet.preprocess_input` on your\n+    For NASNet, call `keras.applications.nasnet.preprocess_input` on your\n     inputs before passing them to the model.\n \n     Args:\n@@ -841,14 +841,14 @@ def _reduction_a_cell(ip, p, filters, block_id=None):\n         return x, ip\n \n \n-@keras_core_export(\"keras_core.applications.nasnet.preprocess_input\")\n+@keras_export(\"keras.applications.nasnet.preprocess_input\")\n def preprocess_input(x, data_format=None):\n     return imagenet_utils.preprocess_input(\n         x, data_format=data_format, mode=\"tf\"\n     )\n \n \n-@keras_core_export(\"keras_core.applications.nasnet.decode_predictions\")\n+@keras_export(\"keras.applications.nasnet.decode_predictions\")\n def decode_predictions(preds, top=5):\n     return imagenet_utils.decode_predictions(preds, top=top)\n \n\n@@ -1,10 +1,10 @@\n-from keras_core import backend\n-from keras_core import layers\n-from keras_core.api_export import keras_core_export\n-from keras_core.applications import imagenet_utils\n-from keras_core.models import Functional\n-from keras_core.ops import operation_utils\n-from keras_core.utils import file_utils\n+from keras import backend\n+from keras import layers\n+from keras.api_export import keras_export\n+from keras.applications import imagenet_utils\n+from keras.models import Functional\n+from keras.ops import operation_utils\n+from keras.utils import file_utils\n \n BASE_WEIGHTS_PATH = (\n     \"https://storage.googleapis.com/tensorflow/keras-applications/resnet/\"\n@@ -379,11 +379,11 @@ def stack_residual_blocks_v2(x, filters, blocks, stride1=2, name=None):\n     return x\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.resnet50.ResNet50\",\n-        \"keras_core.applications.resnet.ResNet50\",\n-        \"keras_core.applications.ResNet50\",\n+        \"keras.applications.resnet50.ResNet50\",\n+        \"keras.applications.resnet.ResNet50\",\n+        \"keras.applications.ResNet50\",\n     ]\n )\n def ResNet50(\n@@ -418,10 +418,10 @@ def ResNet50(\n     )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.resnet.ResNet101\",\n-        \"keras_core.applications.ResNet101\",\n+        \"keras.applications.resnet.ResNet101\",\n+        \"keras.applications.ResNet101\",\n     ]\n )\n def ResNet101(\n@@ -456,10 +456,10 @@ def ResNet101(\n     )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.resnet.ResNet152\",\n-        \"keras_core.applications.ResNet152\",\n+        \"keras.applications.resnet.ResNet152\",\n+        \"keras.applications.ResNet152\",\n     ]\n )\n def ResNet152(\n@@ -494,10 +494,10 @@ def ResNet152(\n     )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.resnet50.preprocess_input\",\n-        \"keras_core.applications.resnet.preprocess_input\",\n+        \"keras.applications.resnet50.preprocess_input\",\n+        \"keras.applications.resnet.preprocess_input\",\n     ]\n )\n def preprocess_input(x, data_format=None):\n@@ -506,10 +506,10 @@ def preprocess_input(x, data_format=None):\n     )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.resnet50.decode_predictions\",\n-        \"keras_core.applications.resnet.decode_predictions\",\n+        \"keras.applications.resnet50.decode_predictions\",\n+        \"keras.applications.resnet.decode_predictions\",\n     ]\n )\n def decode_predictions(preds, top=5):\n@@ -537,7 +537,7 @@ For transfer learning use cases, make sure to read the\n     https://keras.io/guides/transfer_learning/).\n \n Note: each Keras Application expects a specific kind of input preprocessing.\n-For ResNet, call `keras_core.applications.resnet.preprocess_input` on your\n+For ResNet, call `keras.applications.resnet.preprocess_input` on your\n inputs before passing them to the model. `resnet.preprocess_input` will convert\n the input images from RGB to BGR, then will zero-center each color channel with\n respect to the ImageNet dataset, without scaling.\n\n@@ -1,12 +1,12 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.applications import imagenet_utils\n-from keras_core.applications import resnet\n+from keras.api_export import keras_export\n+from keras.applications import imagenet_utils\n+from keras.applications import resnet\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.ResNet50V2\",\n-        \"keras_core.applications.resnet_v2.ResNet50V2\",\n+        \"keras.applications.ResNet50V2\",\n+        \"keras.applications.resnet_v2.ResNet50V2\",\n     ]\n )\n def ResNet50V2(\n@@ -43,10 +43,10 @@ def ResNet50V2(\n     )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.ResNet101V2\",\n-        \"keras_core.applications.resnet_v2.ResNet101V2\",\n+        \"keras.applications.ResNet101V2\",\n+        \"keras.applications.resnet_v2.ResNet101V2\",\n     ]\n )\n def ResNet101V2(\n@@ -83,10 +83,10 @@ def ResNet101V2(\n     )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.ResNet152V2\",\n-        \"keras_core.applications.resnet_v2.ResNet152V2\",\n+        \"keras.applications.ResNet152V2\",\n+        \"keras.applications.resnet_v2.ResNet152V2\",\n     ]\n )\n def ResNet152V2(\n@@ -123,14 +123,14 @@ def ResNet152V2(\n     )\n \n \n-@keras_core_export(\"keras_core.applications.resnet_v2.preprocess_input\")\n+@keras_export(\"keras.applications.resnet_v2.preprocess_input\")\n def preprocess_input(x, data_format=None):\n     return imagenet_utils.preprocess_input(\n         x, data_format=data_format, mode=\"tf\"\n     )\n \n \n-@keras_core_export(\"keras_core.applications.resnet_v2.decode_predictions\")\n+@keras_export(\"keras.applications.resnet_v2.decode_predictions\")\n def decode_predictions(preds, top=5):\n     return imagenet_utils.decode_predictions(preds, top=top)\n \n@@ -157,7 +157,7 @@ For transfer learning use cases, make sure to read the\n     https://keras.io/guides/transfer_learning/).\n \n Note: each Keras Application expects a specific kind of input preprocessing.\n-For ResNet, call `keras_core.applications.resnet_v2.preprocess_input` on your\n+For ResNet, call `keras.applications.resnet_v2.preprocess_input` on your\n inputs before passing them to the model. `resnet_v2.preprocess_input` will\n scale input pixels between -1 and 1.\n \n\n@@ -1,10 +1,10 @@\n-from keras_core import backend\n-from keras_core import layers\n-from keras_core.api_export import keras_core_export\n-from keras_core.applications import imagenet_utils\n-from keras_core.models import Functional\n-from keras_core.ops import operation_utils\n-from keras_core.utils import file_utils\n+from keras import backend\n+from keras import layers\n+from keras.api_export import keras_export\n+from keras.applications import imagenet_utils\n+from keras.models import Functional\n+from keras.ops import operation_utils\n+from keras.utils import file_utils\n \n WEIGHTS_PATH = (\n     \"https://storage.googleapis.com/tensorflow/keras-applications/\"\n@@ -17,8 +17,8 @@ WEIGHTS_PATH_NO_TOP = (\n )\n \n \n-@keras_core_export(\n-    [\"keras_core.applications.vgg16.VGG16\", \"keras_core.applications.VGG16\"]\n+@keras_export(\n+    [\"keras.applications.vgg16.VGG16\", \"keras.applications.VGG16\"]\n )\n def VGG16(\n     include_top=True,\n@@ -46,7 +46,7 @@ def VGG16(\n     The default input size for this model is 224x224.\n \n     Note: each Keras Application expects a specific kind of input preprocessing.\n-    For VGG16, call `keras_core.applications.vgg16.preprocess_input` on your\n+    For VGG16, call `keras.applications.vgg16.preprocess_input` on your\n     inputs before passing them to the model.\n     `vgg16.preprocess_input` will convert the input images from RGB to BGR,\n     then will zero-center each color channel with respect to the ImageNet\n@@ -228,14 +228,14 @@ def VGG16(\n     return model\n \n \n-@keras_core_export(\"keras_core.applications.vgg16.preprocess_input\")\n+@keras_export(\"keras.applications.vgg16.preprocess_input\")\n def preprocess_input(x, data_format=None):\n     return imagenet_utils.preprocess_input(\n         x, data_format=data_format, mode=\"caffe\"\n     )\n \n \n-@keras_core_export(\"keras_core.applications.vgg16.decode_predictions\")\n+@keras_export(\"keras.applications.vgg16.decode_predictions\")\n def decode_predictions(preds, top=5):\n     return imagenet_utils.decode_predictions(preds, top=top)\n \n\n@@ -1,10 +1,10 @@\n-from keras_core import backend\n-from keras_core import layers\n-from keras_core.api_export import keras_core_export\n-from keras_core.applications import imagenet_utils\n-from keras_core.models import Functional\n-from keras_core.ops import operation_utils\n-from keras_core.utils import file_utils\n+from keras import backend\n+from keras import layers\n+from keras.api_export import keras_export\n+from keras.applications import imagenet_utils\n+from keras.models import Functional\n+from keras.ops import operation_utils\n+from keras.utils import file_utils\n \n WEIGHTS_PATH = (\n     \"https://storage.googleapis.com/tensorflow/keras-applications/\"\n@@ -17,8 +17,8 @@ WEIGHTS_PATH_NO_TOP = (\n )\n \n \n-@keras_core_export(\n-    [\"keras_core.applications.vgg19.VGG19\", \"keras_core.applications.VGG19\"]\n+@keras_export(\n+    [\"keras.applications.vgg19.VGG19\", \"keras.applications.VGG19\"]\n )\n def VGG19(\n     include_top=True,\n@@ -46,7 +46,7 @@ def VGG19(\n     The default input size for this model is 224x224.\n \n     Note: each Keras Application expects a specific kind of input preprocessing.\n-    For VGG19, call `keras_core.applications.vgg19.preprocess_input` on your\n+    For VGG19, call `keras.applications.vgg19.preprocess_input` on your\n     inputs before passing them to the model.\n     `vgg19.preprocess_input` will convert the input images from RGB to BGR,\n     then will zero-center each color channel with respect to the ImageNet\n@@ -236,14 +236,14 @@ def VGG19(\n     return model\n \n \n-@keras_core_export(\"keras_core.applications.vgg19.preprocess_input\")\n+@keras_export(\"keras.applications.vgg19.preprocess_input\")\n def preprocess_input(x, data_format=None):\n     return imagenet_utils.preprocess_input(\n         x, data_format=data_format, mode=\"caffe\"\n     )\n \n \n-@keras_core_export(\"keras_core.applications.vgg19.decode_predictions\")\n+@keras_export(\"keras.applications.vgg19.decode_predictions\")\n def decode_predictions(preds, top=5):\n     return imagenet_utils.decode_predictions(preds, top=top)\n \n\n@@ -1,10 +1,10 @@\n-from keras_core import backend\n-from keras_core import layers\n-from keras_core.api_export import keras_core_export\n-from keras_core.applications import imagenet_utils\n-from keras_core.models import Functional\n-from keras_core.ops import operation_utils\n-from keras_core.utils import file_utils\n+from keras import backend\n+from keras import layers\n+from keras.api_export import keras_export\n+from keras.applications import imagenet_utils\n+from keras.models import Functional\n+from keras.ops import operation_utils\n+from keras.utils import file_utils\n \n WEIGHTS_PATH = (\n     \"https://storage.googleapis.com/tensorflow/keras-applications/\"\n@@ -16,10 +16,10 @@ WEIGHTS_PATH_NO_TOP = (\n )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.applications.xception.Xception\",\n-        \"keras_core.applications.Xception\",\n+        \"keras.applications.xception.Xception\",\n+        \"keras.applications.Xception\",\n     ]\n )\n def Xception(\n@@ -48,7 +48,7 @@ def Xception(\n     The default input image size for this model is 299x299.\n \n     Note: each Keras Application expects a specific kind of input preprocessing.\n-    For Xception, call `keras_core.applications.xception.preprocess_input`\n+    For Xception, call `keras.applications.xception.preprocess_input`\n     on your inputs before passing them to the model.\n     `xception.preprocess_input` will scale input pixels between -1 and 1.\n \n@@ -333,14 +333,14 @@ def Xception(\n     return model\n \n \n-@keras_core_export(\"keras_core.applications.xception.preprocess_input\")\n+@keras_export(\"keras.applications.xception.preprocess_input\")\n def preprocess_input(x, data_format=None):\n     return imagenet_utils.preprocess_input(\n         x, data_format=data_format, mode=\"tf\"\n     )\n \n \n-@keras_core_export(\"keras_core.applications.xception.decode_predictions\")\n+@keras_export(\"keras.applications.xception.decode_predictions\")\n def decode_predictions(preds, top=5):\n     return imagenet_utils.decode_predictions(preds, top=top)\n \n\n@@ -0,0 +1,55 @@\n+from keras.backend.config import backend\n+\n+if backend() == \"torch\":\n+    # When using the torch backend,\n+    # torch needs to be imported first, otherwise it will segfault\n+    # upon import.\n+    import torch\n+\n+from keras.backend.common.keras_tensor import KerasTensor\n+from keras.backend.common.keras_tensor import any_symbolic_tensors\n+from keras.backend.common.keras_tensor import is_keras_tensor\n+from keras.backend.common.name_scope import name_scope\n+from keras.backend.common.stateless_scope import StatelessScope\n+from keras.backend.common.stateless_scope import get_stateless_scope\n+from keras.backend.common.stateless_scope import in_stateless_scope\n+from keras.backend.common.variables import AutocastScope\n+from keras.backend.common.variables import get_autocast_scope\n+from keras.backend.common.variables import is_float_dtype\n+from keras.backend.common.variables import is_int_dtype\n+from keras.backend.common.variables import standardize_dtype\n+from keras.backend.common.variables import standardize_shape\n+from keras.backend.config import epsilon\n+from keras.backend.config import floatx\n+from keras.backend.config import image_data_format\n+from keras.backend.config import set_epsilon\n+from keras.backend.config import set_floatx\n+from keras.backend.config import set_image_data_format\n+from keras.backend.config import standardize_data_format\n+from keras.utils.io_utils import print_msg\n+\n+# Import backend functions.\n+if backend() == \"tensorflow\":\n+    print_msg(\"Using TensorFlow backend\")\n+    from keras.backend.tensorflow import *  # noqa: F403\n+\n+    distribution_lib = None\n+elif backend() == \"jax\":\n+    print_msg(\"Using JAX backend.\")\n+    from keras.backend.jax import *  # noqa: F403\n+elif backend() == \"torch\":\n+    print_msg(\"Using PyTorch backend.\")\n+    from keras.backend.torch import *  # noqa: F403\n+\n+    distribution_lib = None\n+elif backend() == \"numpy\":\n+    print_msg(\n+        \"Using NumPy backend.\\nThe NumPy backend does not support \"\n+        \"training. It should only be used for inference, evaluation, \"\n+        \"and debugging.\"\n+    )\n+    from keras.backend.numpy import *  # noqa: F403\n+\n+    distribution_lib = None\n+else:\n+    raise ValueError(f\"Unable to import backend : {backend()}\")\n\n@@ -0,0 +1,9 @@\n+from keras.backend.common import backend_utils\n+from keras.backend.common.variables import AutocastScope\n+from keras.backend.common.variables import KerasVariable\n+from keras.backend.common.variables import get_autocast_scope\n+from keras.backend.common.variables import is_float_dtype\n+from keras.backend.common.variables import is_int_dtype\n+from keras.backend.common.variables import standardize_dtype\n+from keras.backend.common.variables import standardize_shape\n+from keras.random import random\n\n\n@@ -1,19 +1,19 @@\n-from keras_core.backend.common.backend_utils import (\n+from keras.backend.common.backend_utils import (\n     _convert_conv_tranpose_padding_args_from_keras_to_jax,\n )\n-from keras_core.backend.common.backend_utils import (\n+from keras.backend.common.backend_utils import (\n     _convert_conv_tranpose_padding_args_from_keras_to_torch,\n )\n-from keras_core.backend.common.backend_utils import (\n+from keras.backend.common.backend_utils import (\n     _get_output_shape_given_tf_padding,\n )\n-from keras_core.backend.common.backend_utils import (\n+from keras.backend.common.backend_utils import (\n     compute_conv_transpose_padding_args_for_jax,\n )\n-from keras_core.backend.common.backend_utils import (\n+from keras.backend.common.backend_utils import (\n     compute_conv_transpose_padding_args_for_torch,\n )\n-from keras_core.testing import test_case\n+from keras.testing import test_case\n \n \n class ConvertConvTransposePaddingArgsJAXTest(test_case.TestCase):\n\n@@ -1,7 +1,7 @@\n import pytest\n \n-from keras_core import backend\n-from keras_core import testing\n+from keras import backend\n+from keras import testing\n \n \n def example_fn(x):\n\n@@ -1,7 +1,7 @@\n import threading\n \n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n+from keras import backend\n+from keras.api_export import keras_export\n \n GLOBAL_STATE_TRACKER = threading.local()\n GLOBAL_SETTINGS_TRACKER = threading.local()\n@@ -20,8 +20,8 @@ def get_global_attribute(name, default=None, set_to_default=False):\n     return attr\n \n \n-@keras_core_export(\n-    [\"keras_core.utils.clear_session\", \"keras_core.backend.clear_session\"]\n+@keras_export(\n+    [\"keras.utils.clear_session\", \"keras.backend.clear_session\"]\n )\n def clear_session():\n     \"\"\"Resets all state generated by Keras.\n@@ -40,26 +40,26 @@ def clear_session():\n     for _ in range(100):\n       # Without `clear_session()`, each iteration of this loop will\n       # slightly increase the size of the global state managed by Keras\n-      model = keras_core.Sequential([\n-          keras_core.layers.Dense(10) for _ in range(10)])\n+      model = keras.Sequential([\n+          keras.layers.Dense(10) for _ in range(10)])\n \n     for _ in range(100):\n       # With `clear_session()` called at the beginning,\n       # Keras starts with a blank state at each iteration\n       # and memory consumption is constant over time.\n-      keras_core.backend.clear_session()\n-      model = keras_core.Sequential([\n-          keras_core.layers.Dense(10) for _ in range(10)])\n+      keras.backend.clear_session()\n+      model = keras.Sequential([\n+          keras.layers.Dense(10) for _ in range(10)])\n     ```\n \n     Example 2: resetting the layer name generation counter\n \n-    >>> layers = [keras_core.layers.Dense(10) for _ in range(10)]\n-    >>> new_layer = keras_core.layers.Dense(10)\n+    >>> layers = [keras.layers.Dense(10) for _ in range(10)]\n+    >>> new_layer = keras.layers.Dense(10)\n     >>> print(new_layer.name)\n     dense_10\n-    >>> keras_core.backend.clear_session()\n-    >>> new_layer = keras_core.layers.Dense(10)\n+    >>> keras.backend.clear_session()\n+    >>> new_layer = keras.layers.Dense(10)\n     >>> print(new_layer.name)\n     dense\n     \"\"\"\n@@ -70,7 +70,7 @@ def clear_session():\n     GLOBAL_SETTINGS_TRACKER = threading.local()\n \n     if backend.backend() == \"tensorflow\":\n-        from keras_core.utils.module_utils import tensorflow as tf\n+        from keras.utils.module_utils import tensorflow as tf\n \n         tf.compat.v1.reset_default_graph()\n         if tf.executing_eagerly():\n\n@@ -1,6 +1,6 @@\n-from keras_core.backend.common import global_state\n-from keras_core.testing import test_case\n-from keras_core.utils.naming import auto_name\n+from keras.backend.common import global_state\n+from keras.testing import test_case\n+from keras.utils.naming import auto_name\n \n \n class GlobalStateTest(test_case.TestCase):\n\n@@ -1,20 +1,20 @@\n import tree\n \n-from keras_core.api_export import keras_core_export\n-from keras_core.utils.naming import auto_name\n+from keras.api_export import keras_export\n+from keras.utils.naming import auto_name\n \n \n-@keras_core_export(\"keras_core.KerasTensor\")\n+@keras_export(\"keras.KerasTensor\")\n class KerasTensor:\n     \"\"\"Symbolic tensor -- encapsulates a shape and a dtype.\n \n     You can use `KerasTensor` instances to build computation\n-    graphs of Keras operations, such as `keras_core.Function`\n-    objects or Functional `keras_core.models.Model` objects.\n+    graphs of Keras operations, such as `keras.Function`\n+    objects or Functional `keras.models.Model` objects.\n \n     Example:\n \n-    >>> x = keras_core.KerasTensor(shape=(3, 4), dtype=\"float32\")\n+    >>> x = keras.KerasTensor(shape=(3, 4), dtype=\"float32\")\n     >>> x.shape\n     (3, 4)\n     >>> x.dtype\n@@ -36,7 +36,7 @@ class KerasTensor:\n         record_history=True,\n         name=None,\n     ):\n-        from keras_core import backend\n+        from keras import backend\n \n         self.shape = backend.standardize_shape(shape)\n         self.dtype = backend.standardize_dtype(dtype)\n@@ -49,12 +49,12 @@ class KerasTensor:\n         return len(self.shape)\n \n     def reshape(self, new_shape):\n-        from keras_core import ops\n+        from keras import ops\n \n         return ops.Reshape(new_shape)(self)\n \n     def squeeze(self, axis=None):\n-        from keras_core import ops\n+        from keras import ops\n \n         return ops.Squeeze(axis)(self)\n \n@@ -71,8 +71,8 @@ class KerasTensor:\n             \"A KerasTensor is a symbolic placeholder for a shape and dtype, \"\n             \"used when constructing Keras Functional models \"\n             \"or Keras Functions. You can only use it as input to a Keras layer \"\n-            \"or a Keras operation (from the namespaces `keras_core.layers` \"\n-            \"and `keras_core.operations`). \"\n+            \"or a Keras operation (from the namespaces `keras.layers` \"\n+            \"and `keras.operations`). \"\n             \"You are likely doing something like:\\n\\n\"\n             \"```\\n\"\n             \"x = Input(...)\\n\"\n@@ -94,8 +94,8 @@ class KerasTensor:\n             \"A KerasTensor is a symbolic placeholder for a shape and dtype, \"\n             \"used when constructing Keras Functional models \"\n             \"or Keras Functions. You can only use it as input to a Keras layer \"\n-            \"or a Keras operation (from the namespaces `keras_core.layers` \"\n-            \"and `keras_core.operations`). \"\n+            \"or a Keras operation (from the namespaces `keras.layers` \"\n+            \"and `keras.operations`). \"\n             \"You are likely doing something like:\\n\\n\"\n             \"```\\n\"\n             \"x = Input(...)\\n\"\n@@ -126,167 +126,167 @@ class KerasTensor:\n         raise TypeError(\"A symbolic KerasTensor cannot be used as a boolean.\")\n \n     def __add__(self, other):\n-        from keras_core import ops\n+        from keras import ops\n \n         return ops.Add().symbolic_call(self, other)\n \n     def __radd__(self, other):\n-        from keras_core import ops\n+        from keras import ops\n \n         return ops.Add().symbolic_call(other, self)\n \n     def __sub__(self, other):\n-        from keras_core import ops\n+        from keras import ops\n \n         return ops.Subtract().symbolic_call(self, other)\n \n     def __rsub__(self, other):\n-        from keras_core import ops\n+        from keras import ops\n \n         return ops.Subtract().symbolic_call(other, self)\n \n     def __mul__(self, other):\n-        from keras_core import ops\n+        from keras import ops\n \n         return ops.Multiply().symbolic_call(self, other)\n \n     def __rmul__(self, other):\n-        from keras_core import ops\n+        from keras import ops\n \n         return ops.Multiply().symbolic_call(other, self)\n \n     def __matmul__(self, other):\n-        from keras_core import ops\n+        from keras import ops\n \n         return ops.Matmul().symbolic_call(self, other)\n \n     def __rmatmul__(self, other):\n-        from keras_core import ops\n+        from keras import ops\n \n         return ops.Matmul().symbolic_call(other, self)\n \n     def __div__(self, other):\n-        from keras_core import ops\n+        from keras import ops\n \n         return ops.Divide().symbolic_call(self, other)\n \n     def __rdiv__(self, other):\n-        from keras_core import ops\n+        from keras import ops\n \n         return ops.Divide().symbolic_call(other, self)\n \n     def __truediv__(self, other):\n-        from keras_core import ops\n+        from keras import ops\n \n         return ops.TrueDivide().symbolic_call(self, other)\n \n     def __rtruediv__(self, other):\n-        from keras_core import ops\n+        from keras import ops\n \n         return ops.TrueDivide().symbolic_call(other, self)\n \n     def __neg__(self):\n-        from keras_core import ops\n+        from keras import ops\n \n         return ops.Negative().symbolic_call(self)\n \n     def __abs__(self):\n-        from keras_core import ops\n+        from keras import ops\n \n         return ops.Absolute().symbolic_call(self)\n \n     def __pow__(self, other):\n-        from keras_core import ops\n+        from keras import ops\n \n         return ops.Power().symbolic_call(self, other)\n \n     def __rpow__(self, other):\n-        from keras_core import ops\n+        from keras import ops\n \n         return ops.Power().symbolic_call(other, self)\n \n     def __floordiv__(self, other):\n-        from keras_core import ops\n+        from keras import ops\n \n         return ops.FloorDiv().symbolic_call(self, other)\n \n     def __rfloordiv__(self, other):\n-        from keras_core import ops\n+        from keras import ops\n \n         return ops.FloorDiv().symbolic_call(other, self)\n \n     def __mod__(self, other):\n-        from keras_core import ops\n+        from keras import ops\n \n         return ops.Mod().symbolic_call(self, other)\n \n     def __rmod__(self, other):\n-        from keras_core import ops\n+        from keras import ops\n \n         return ops.Mod().symbolic_call(other, self)\n \n     def __lt__(self, other):\n-        from keras_core import ops\n+        from keras import ops\n \n         return ops.Less().symbolic_call(self, other)\n \n     def __le__(self, other):\n-        from keras_core import ops\n+        from keras import ops\n \n         return ops.LessEqual().symbolic_call(self, other)\n \n     def __gt__(self, other):\n-        from keras_core import ops\n+        from keras import ops\n \n         return ops.Greater().symbolic_call(self, other)\n \n     def __ge__(self, other):\n-        from keras_core import ops\n+        from keras import ops\n \n         return ops.GreaterEqual().symbolic_call(self, other)\n \n     def __ne__(self, other):\n-        from keras_core import ops\n+        from keras import ops\n \n         return ops.NotEqual().symbolic_call(self, other)\n \n     def __and__(self, other):\n-        from keras_core import ops\n+        from keras import ops\n \n         return ops.LogicalAnd().symbolic_call(self, other)\n \n     def __rand__(self, other):\n-        from keras_core import ops\n+        from keras import ops\n \n         return ops.LogicalAnd().symbolic_call(other, self)\n \n     def __or__(self, other):\n-        from keras_core import ops\n+        from keras import ops\n \n         return ops.LogicalOr().symbolic_call(self, other)\n \n     def __ror__(self, other):\n-        from keras_core import ops\n+        from keras import ops\n \n         return ops.LogicalOr().symbolic_call(other, self)\n \n     def __invert__(self, other):\n-        from keras_core import ops\n+        from keras import ops\n \n         return ops.LogicalNot().symbolic_call(other, self)\n \n     def __xor__(self, other):\n-        from keras_core import ops\n+        from keras import ops\n \n         return ops.LogicalXor().symbolic_call(self, other)\n \n     def __rxor__(self, other):\n-        from keras_core import ops\n+        from keras import ops\n \n         return ops.LogicalXor().symbolic_call(other, self)\n \n     def __getitem__(self, key):\n-        from keras_core import ops\n+        from keras import ops\n \n         return ops.GetItem().symbolic_call(self, key)\n \n@@ -300,8 +300,8 @@ def any_symbolic_tensors(args=None, kwargs=None):\n     return False\n \n \n-@keras_core_export(\n-    [\"keras_core.utils.is_keras_tensor\", \"keras_core.backend.is_keras_tensor\"]\n+@keras_export(\n+    [\"keras.utils.is_keras_tensor\", \"keras.backend.is_keras_tensor\"]\n )\n def is_keras_tensor(x):\n     \"\"\"Returns whether `x` is a Keras tensor.\n\n@@ -4,10 +4,10 @@ from unittest.mock import patch\n import numpy as np\n import tensorflow as tf\n \n-from keras_core import backend\n-from keras_core import ops\n-from keras_core import testing\n-from keras_core.backend.common import keras_tensor\n+from keras import backend\n+from keras import ops\n+from keras import testing\n+from keras.backend.common import keras_tensor\n \n \n class KerasTensorTest(testing.TestCase):\n@@ -80,7 +80,7 @@ class KerasTensorTest(testing.TestCase):\n         y = np.array([1, 2, 3])\n         self.assertFalse(keras_tensor.is_keras_tensor(y))\n \n-    @patch(\"keras_core.ops.Absolute.symbolic_call\")\n+    @patch(\"keras.ops.Absolute.symbolic_call\")\n     def test_abs_method(self, mock_symbolic_call):\n         mock_tensor = Mock()\n         mock_symbolic_call.return_value = mock_tensor\n@@ -89,51 +89,51 @@ class KerasTensorTest(testing.TestCase):\n         mock_symbolic_call.assert_called_once_with(x)\n         self.assertEqual(abs_x, mock_tensor)\n \n-    @patch(\"keras_core.ops.Negative.symbolic_call\")\n+    @patch(\"keras.ops.Negative.symbolic_call\")\n     def test_neg_method(self, mock_method):\n         self._test_unary_op_method(mock_method, lambda x: -x)\n \n-    @patch(\"keras_core.ops.Subtract.symbolic_call\")\n+    @patch(\"keras.ops.Subtract.symbolic_call\")\n     def test_sub_method(self, mock_method):\n         y = Mock()\n         self._test_binary_op_method(mock_method, y, lambda x, y: x - y)\n \n-    @patch(\"keras_core.ops.Multiply.symbolic_call\")\n+    @patch(\"keras.ops.Multiply.symbolic_call\")\n     def test_mul_method(self, mock_method):\n         y = Mock()\n         self._test_binary_op_method(mock_method, y, lambda x, y: x * y)\n \n-    @patch(\"keras_core.ops.Matmul.symbolic_call\")\n+    @patch(\"keras.ops.Matmul.symbolic_call\")\n     def test_matmul_method(self, mock_method):\n         y = Mock()\n         self._test_binary_op_method(mock_method, y, lambda x, y: x @ y)\n \n-    @patch(\"keras_core.ops.Power.symbolic_call\")\n+    @patch(\"keras.ops.Power.symbolic_call\")\n     def test_pow_method(self, mock_method):\n         y = Mock()\n         self._test_binary_op_method(mock_method, y, lambda x, y: x**y)\n \n-    @patch(\"keras_core.ops.Mod.symbolic_call\")\n+    @patch(\"keras.ops.Mod.symbolic_call\")\n     def test_mod_method(self, mock_method):\n         y = Mock()\n         self._test_binary_op_method(mock_method, y, lambda x, y: x % y)\n \n-    @patch(\"keras_core.ops.Less.symbolic_call\")\n+    @patch(\"keras.ops.Less.symbolic_call\")\n     def test_lt_method(self, mock_method):\n         y = Mock()\n         self._test_binary_op_method(mock_method, y, lambda x, y: x < y)\n \n-    @patch(\"keras_core.ops.LogicalAnd.symbolic_call\")\n+    @patch(\"keras.ops.LogicalAnd.symbolic_call\")\n     def test_and_method(self, mock_method):\n         y = Mock()\n         self._test_binary_op_method(mock_method, y, lambda x, y: x & y)\n \n-    @patch(\"keras_core.ops.LogicalOr.symbolic_call\")\n+    @patch(\"keras.ops.LogicalOr.symbolic_call\")\n     def test_or_method(self, mock_method):\n         y = Mock()\n         self._test_binary_op_method(mock_method, y, lambda x, y: x | y)\n \n-    @patch(\"keras_core.ops.GetItem.symbolic_call\")\n+    @patch(\"keras.ops.GetItem.symbolic_call\")\n     def test_getitem_method(self, mock_method):\n         y = Mock()\n         self._test_binary_op_method(mock_method, y, lambda x, y: x[y])\n\n@@ -1,4 +1,4 @@\n-from keras_core.backend.common import global_state\n+from keras.backend.common import global_state\n \n \n class name_scope:\n\n@@ -1,6 +1,6 @@\n-from keras_core import testing\n-from keras_core.backend.common.name_scope import current_path\n-from keras_core.backend.common.name_scope import name_scope\n+from keras import testing\n+from keras.backend.common.name_scope import current_path\n+from keras.backend.common.name_scope import name_scope\n \n \n class NameScopeTest(testing.TestCase):\n\n@@ -1,8 +1,8 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.backend.common import global_state\n+from keras.api_export import keras_export\n+from keras.backend.common import global_state\n \n \n-@keras_core_export(\"keras_core.StatelessScope\")\n+@keras_export(\"keras.StatelessScope\")\n class StatelessScope:\n     def __init__(\n         self,\n@@ -10,8 +10,8 @@ class StatelessScope:\n         collect_losses=False,\n         initialize_variables=True,\n     ):\n-        from keras_core import backend\n-        from keras_core.backend.common.variables import KerasVariable\n+        from keras import backend\n+        from keras.backend.common.variables import KerasVariable\n \n         self.collect_losses = collect_losses\n         self.initialize_variables = initialize_variables\n@@ -59,7 +59,7 @@ class StatelessScope:\n             # We're back in eager scope;\n             # if any variables were created within the stateless\n             # scope, we initialize them here.\n-            from keras_core.backend.common.variables import (\n+            from keras.backend.common.variables import (\n                 initialize_all_variables,\n             )\n \n\n@@ -1,9 +1,9 @@\n import numpy as np\n \n-from keras_core import backend\n-from keras_core import ops\n-from keras_core import testing\n-from keras_core.backend.common.stateless_scope import StatelessScope\n+from keras import backend\n+from keras import ops\n+from keras import testing\n+from keras.backend.common.stateless_scope import StatelessScope\n \n \n class TestStatelessScope(testing.TestCase):\n\n@@ -1,12 +1,12 @@\n import numpy as np\n \n-from keras_core.api_export import keras_core_export\n-from keras_core.backend import config\n-from keras_core.backend.common import global_state\n-from keras_core.backend.common.name_scope import current_path\n-from keras_core.backend.common.stateless_scope import get_stateless_scope\n-from keras_core.backend.common.stateless_scope import in_stateless_scope\n-from keras_core.utils.naming import auto_name\n+from keras.api_export import keras_export\n+from keras.backend import config\n+from keras.backend.common import global_state\n+from keras.backend.common.name_scope import current_path\n+from keras.backend.common.stateless_scope import get_stateless_scope\n+from keras.backend.common.stateless_scope import in_stateless_scope\n+from keras.utils.naming import auto_name\n \n \n class KerasVariable:\n@@ -399,7 +399,7 @@ PYTHON_DTYPES_MAP = {\n }\n \n \n-@keras_core_export(\"keras_core.backend.standardize_dtype\")\n+@keras_export(\"keras.backend.standardize_dtype\")\n def standardize_dtype(dtype):\n     if dtype is None:\n         return config.floatx()\n@@ -456,13 +456,13 @@ def shape_equal(a_shape, b_shape):\n     return True\n \n \n-@keras_core_export(\"keras_core.backend.is_float_dtype\")\n+@keras_export(\"keras.backend.is_float_dtype\")\n def is_float_dtype(dtype):\n     dtype = standardize_dtype(dtype)\n     return dtype.startswith(\"float\") or dtype.startswith(\"bfloat\")\n \n \n-@keras_core_export(\"keras_core.backend.is_int_dtype\")\n+@keras_export(\"keras.backend.is_int_dtype\")\n def is_int_dtype(dtype):\n     dtype = standardize_dtype(dtype)\n     return dtype.startswith(\"int\") or dtype.startswith(\"uint\")\n@@ -492,7 +492,7 @@ class AutocastScope:\n         self.original_scope = None\n \n     def maybe_cast(self, value):\n-        from keras_core import backend\n+        from keras import backend\n \n         if self.dtype is not None and is_float_dtype(value.dtype):\n             return backend.cast(value, dtype=self.dtype)\n\n@@ -1,11 +1,11 @@\n import numpy as np\n \n-from keras_core import backend\n-from keras_core import initializers\n-from keras_core.backend.common.variables import AutocastScope\n-from keras_core.backend.common.variables import KerasVariable\n-from keras_core.backend.common.variables import standardize_shape\n-from keras_core.testing import test_case\n+from keras import backend\n+from keras import initializers\n+from keras.backend.common.variables import AutocastScope\n+from keras.backend.common.variables import KerasVariable\n+from keras.backend.common.variables import standardize_shape\n+from keras.testing import test_case\n \n \n class VariablesTest(test_case.TestCase):\n@@ -101,7 +101,7 @@ class VariablesTest(test_case.TestCase):\n         self.assertEqual(standardized_shape, (3, 4, 5))\n \n     # TODO\n-    # (3.9,torch) FAILED keras_core/backend/common/variables_test.py\n+    # (3.9,torch) FAILED keras/backend/common/variables_test.py\n     # ::VariablesTest::test_standardize_shape_with_non_integer_entry:\n     #  - AssertionError \"Cannot convert '\\(3, 4, 'a'\\)' to a shape.\n     # \" does not match \"invalid literal for int() with base 10: 'a'\"\n\n@@ -1,7 +1,7 @@\n import json\n import os\n \n-from keras_core.api_export import keras_core_export\n+from keras.api_export import keras_export\n \n # The type of float to use throughout a session.\n _FLOATX = \"float32\"\n@@ -16,7 +16,7 @@ _IMAGE_DATA_FORMAT = \"channels_last\"\n _BACKEND = \"tensorflow\"\n \n \n-@keras_core_export([\"keras_core.config.floatx\", \"keras_core.backend.floatx\"])\n+@keras_export([\"keras.config.floatx\", \"keras.backend.floatx\"])\n def floatx():\n     \"\"\"Return the default float type, as a string.\n \n@@ -27,15 +27,15 @@ def floatx():\n \n     Example:\n \n-    >>> keras_core.config.floatx()\n+    >>> keras.config.floatx()\n     'float32'\n \n     \"\"\"\n     return _FLOATX\n \n \n-@keras_core_export(\n-    [\"keras_core.config.set_floatx\", \"keras_core.backend.set_floatx\"]\n+@keras_export(\n+    [\"keras.config.set_floatx\", \"keras.backend.set_floatx\"]\n )\n def set_floatx(value):\n     \"\"\"Set the default float dtype.\n@@ -44,21 +44,21 @@ def set_floatx(value):\n     as this will likely cause numeric stability issues.\n     Instead, mixed precision, which leverages\n     a mix of `float16` and `float32`. It can be configured by calling\n-    `keras_core.mixed_precision.set_dtype_policy('mixed_float16')`.\n+    `keras.mixed_precision.set_dtype_policy('mixed_float16')`.\n \n     Args:\n         value: String; `'float16'`, `'float32'`, or `'float64'`.\n \n     Examples:\n-    >>> keras_core.config.floatx()\n+    >>> keras.config.floatx()\n     'float32'\n \n-    >>> keras_core.config.set_floatx('float64')\n-    >>> keras_core.config.floatx()\n+    >>> keras.config.set_floatx('float64')\n+    >>> keras.config.floatx()\n     'float64'\n \n     >>> # Set it back to float32\n-    >>> keras_core.config.set_floatx('float32')\n+    >>> keras.config.set_floatx('float32')\n \n     Raises:\n         ValueError: In case of invalid value.\n@@ -73,7 +73,7 @@ def set_floatx(value):\n     _FLOATX = str(value)\n \n \n-@keras_core_export([\"keras_core.config.epsilon\", \"keras_core.backend.epsilon\"])\n+@keras_export([\"keras.config.epsilon\", \"keras.backend.epsilon\"])\n def epsilon():\n     \"\"\"Return the value of the fuzz factor used in numeric expressions.\n \n@@ -82,15 +82,15 @@ def epsilon():\n \n     Example:\n \n-    >>> keras_core.config.epsilon()\n+    >>> keras.config.epsilon()\n     1e-07\n \n     \"\"\"\n     return _EPSILON\n \n \n-@keras_core_export(\n-    [\"keras_core.config.set_epsilon\", \"keras_core.backend.set_epsilon\"]\n+@keras_export(\n+    [\"keras.config.set_epsilon\", \"keras.backend.set_epsilon\"]\n )\n def set_epsilon(value):\n     \"\"\"Set the value of the fuzz factor used in numeric expressions.\n@@ -99,25 +99,25 @@ def set_epsilon(value):\n         value: float. New value of epsilon.\n \n     Examples:\n-    >>> keras_core.config.epsilon()\n+    >>> keras.config.epsilon()\n     1e-07\n \n-    >>> keras_core.config.set_epsilon(1e-5)\n-    >>> keras_core.config.epsilon()\n+    >>> keras.config.set_epsilon(1e-5)\n+    >>> keras.config.epsilon()\n     1e-05\n \n     >>> # Set it back to the default value.\n-    >>> keras_core.config.set_epsilon(1e-7)\n+    >>> keras.config.set_epsilon(1e-7)\n \n     \"\"\"\n     global _EPSILON\n     _EPSILON = value\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.config.image_data_format\",\n-        \"keras_core.backend.image_data_format\",\n+        \"keras.config.image_data_format\",\n+        \"keras.backend.image_data_format\",\n     ]\n )\n def image_data_format():\n@@ -128,17 +128,17 @@ def image_data_format():\n \n     Example:\n \n-    >>> keras_core.config.image_data_format()\n+    >>> keras.config.image_data_format()\n     'channels_last'\n \n     \"\"\"\n     return _IMAGE_DATA_FORMAT\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.config.set_image_data_format\",\n-        \"keras_core.backend.set_image_data_format\",\n+        \"keras.config.set_image_data_format\",\n+        \"keras.backend.set_image_data_format\",\n     ]\n )\n def set_image_data_format(data_format):\n@@ -149,15 +149,15 @@ def set_image_data_format(data_format):\n \n     Examples:\n \n-    >>> keras_core.config.image_data_format()\n+    >>> keras.config.image_data_format()\n     'channels_last'\n \n-    >>> keras_core.config.set_image_data_format('channels_first')\n-    >>> keras_core.config.image_data_format()\n+    >>> keras.config.set_image_data_format('channels_first')\n+    >>> keras.config.image_data_format()\n     'channels_first'\n \n     >>> # Set it back to `'channels_last'`\n-    >>> keras_core.config.set_image_data_format('channels_last')\n+    >>> keras.config.set_image_data_format('channels_last')\n \n     \"\"\"\n     global _IMAGE_DATA_FORMAT\n@@ -258,10 +258,10 @@ if _BACKEND != \"tensorflow\":\n     os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.config.backend\",\n-        \"keras_core.backend.backend\",\n+        \"keras.config.backend\",\n+        \"keras.backend.backend\",\n     ]\n )\n def backend():\n\n@@ -1,5 +1,5 @@\n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n+from keras import backend\n+from keras.api_export import keras_export\n \n if backend.backend() == \"tensorflow\":\n     BackendVariable = backend.tensorflow.core.Variable\n@@ -11,7 +11,7 @@ elif backend.backend() == \"torch\":\n     BackendVariable = backend.torch.core.Variable\n     backend_name_scope = backend.common.name_scope.name_scope\n elif backend.backend() == \"numpy\":\n-    from keras_core.backend.numpy.core import Variable as NumpyVariable\n+    from keras.backend.numpy.core import Variable as NumpyVariable\n \n     BackendVariable = NumpyVariable\n     backend_name_scope = backend.common.name_scope.name_scope\n@@ -19,11 +19,11 @@ else:\n     raise RuntimeError(f\"Invalid backend: {backend.backend()}\")\n \n \n-@keras_core_export(\"keras_core.Variable\")\n+@keras_export(\"keras.Variable\")\n class Variable(BackendVariable):\n     pass\n \n \n-@keras_core_export(\"keras_core.name_scope\")\n+@keras_export(\"keras.name_scope\")\n class name_scope(backend_name_scope):\n     pass\n\n@@ -0,0 +1,23 @@\n+from keras.backend.jax import core\n+from keras.backend.jax import distribution_lib\n+from keras.backend.jax import image\n+from keras.backend.jax import math\n+from keras.backend.jax import nn\n+from keras.backend.jax import numpy\n+from keras.backend.jax import random\n+from keras.backend.jax.core import SUPPORTS_SPARSE_TENSORS\n+from keras.backend.jax.core import Variable\n+from keras.backend.jax.core import cast\n+from keras.backend.jax.core import compute_output_spec\n+from keras.backend.jax.core import cond\n+from keras.backend.jax.core import convert_to_numpy\n+from keras.backend.jax.core import convert_to_tensor\n+from keras.backend.jax.core import is_tensor\n+from keras.backend.jax.core import scatter\n+from keras.backend.jax.core import shape\n+from keras.backend.jax.core import stop_gradient\n+from keras.backend.jax.core import vectorized_map\n+from keras.backend.jax.rnn import cudnn_ok\n+from keras.backend.jax.rnn import gru\n+from keras.backend.jax.rnn import lstm\n+from keras.backend.jax.rnn import rnn\n\n@@ -6,13 +6,13 @@ import numpy as np\n import tree\n from jax.tree_util import Partial\n \n-from keras_core.backend.common import KerasVariable\n-from keras_core.backend.common import global_state\n-from keras_core.backend.common import standardize_dtype\n-from keras_core.backend.common.keras_tensor import KerasTensor\n-from keras_core.backend.common.stateless_scope import StatelessScope\n-from keras_core.backend.jax import distribution_lib\n-from keras_core.utils.nest import pack_sequence_as\n+from keras.backend.common import KerasVariable\n+from keras.backend.common import global_state\n+from keras.backend.common import standardize_dtype\n+from keras.backend.common.keras_tensor import KerasTensor\n+from keras.backend.common.stateless_scope import StatelessScope\n+from keras.backend.jax import distribution_lib\n+from keras.utils.nest import pack_sequence_as\n \n SUPPORTS_SPARSE_TENSORS = False\n \n@@ -22,7 +22,7 @@ class Variable(KerasVariable):\n         value = jnp.array(value, dtype=self._dtype)\n         # Note that variable.shape is needed by distribution_lib\n         self._shape = tuple(value.shape)\n-        # We can't import the keras_core/distribution/distribution_lib\n+        # We can't import the keras/distribution/distribution_lib\n         # due to circular dependency.\n         distribution = global_state.get_global_attribute(\"distribution\")\n         if distribution is not None:\n\n\n@@ -3,7 +3,7 @@ import functools\n import jax\n import jax.numpy as jnp\n \n-from keras_core.backend.jax.core import convert_to_tensor\n+from keras.backend.jax.core import convert_to_tensor\n \n RESIZE_INTERPOLATIONS = (\n     \"bilinear\",\n\n\n@@ -3,9 +3,9 @@ import math\n import jax\n import jax.numpy as jnp\n \n-from keras_core.backend import standardize_dtype\n-from keras_core.backend.jax.core import convert_to_tensor\n-from keras_core.utils.module_utils import scipy\n+from keras.backend import standardize_dtype\n+from keras.backend.jax.core import convert_to_tensor\n+from keras.utils.module_utils import scipy\n \n \n def segment_sum(data, segment_ids, num_segments=None, sorted=False):\n\n@@ -4,14 +4,14 @@ import numpy as np\n from jax import lax\n from jax import nn as jnn\n \n-from keras_core.backend import standardize_data_format\n-from keras_core.backend import standardize_dtype\n-from keras_core.backend.common.backend_utils import (\n+from keras.backend import standardize_data_format\n+from keras.backend import standardize_dtype\n+from keras.backend.common.backend_utils import (\n     compute_conv_transpose_padding_args_for_jax,\n )\n-from keras_core.backend.config import epsilon\n-from keras_core.backend.jax.core import cast\n-from keras_core.backend.jax.core import convert_to_tensor\n+from keras.backend.config import epsilon\n+from keras.backend.jax.core import cast\n+from keras.backend.jax.core import convert_to_tensor\n \n \n def relu(x):\n\n@@ -1,8 +1,8 @@\n import jax.numpy as jnp\n \n-from keras_core.backend import config\n-from keras_core.backend.jax.core import cast\n-from keras_core.backend.jax.core import convert_to_tensor\n+from keras.backend import config\n+from keras.backend.jax.core import cast\n+from keras.backend.jax.core import convert_to_tensor\n \n \n def add(x1, x2):\n\n@@ -1,9 +1,9 @@\n import jax\n \n-from keras_core.backend.config import floatx\n-from keras_core.random.seed_generator import SeedGenerator\n-from keras_core.random.seed_generator import draw_seed\n-from keras_core.random.seed_generator import make_default_seed\n+from keras.backend.config import floatx\n+from keras.random.seed_generator import SeedGenerator\n+from keras.random.seed_generator import draw_seed\n+from keras.random.seed_generator import make_default_seed\n \n \n def jax_draw_seed(seed):\n\n@@ -4,8 +4,8 @@ import tree\n from jax import lax\n from jax import numpy as jnp\n \n-from keras_core.backend.common import stateless_scope\n-from keras_core.utils.nest import pack_sequence_as\n+from keras.backend.common import stateless_scope\n+from keras.utils.nest import pack_sequence_as\n \n \n def rnn(\n\n@@ -4,16 +4,16 @@ import jax\n import numpy as np\n import tree\n \n-from keras_core import backend\n-from keras_core import callbacks as callbacks_module\n-from keras_core import ops\n-from keras_core import optimizers as optimizers_module\n-from keras_core.backend import distribution_lib as jax_distribution_lib\n-from keras_core.distribution import distribution_lib\n-from keras_core.trainers import trainer as base_trainer\n-from keras_core.trainers.data_adapters import data_adapter_utils\n-from keras_core.trainers.epoch_iterator import EpochIterator\n-from keras_core.utils import traceback_utils\n+from keras import backend\n+from keras import callbacks as callbacks_module\n+from keras import ops\n+from keras import optimizers as optimizers_module\n+from keras.backend import distribution_lib as jax_distribution_lib\n+from keras.distribution import distribution_lib\n+from keras.trainers import trainer as base_trainer\n+from keras.trainers.data_adapters import data_adapter_utils\n+from keras.trainers.epoch_iterator import EpochIterator\n+from keras.utils import traceback_utils\n \n \n class JAXTrainer(base_trainer.Trainer):\n\n@@ -0,0 +1,20 @@\n+from keras.backend.numpy import core\n+from keras.backend.numpy import image\n+from keras.backend.numpy import math\n+from keras.backend.numpy import nn\n+from keras.backend.numpy import numpy\n+from keras.backend.numpy import random\n+from keras.backend.numpy.core import SUPPORTS_SPARSE_TENSORS\n+from keras.backend.numpy.core import Variable\n+from keras.backend.numpy.core import cast\n+from keras.backend.numpy.core import compute_output_spec\n+from keras.backend.numpy.core import cond\n+from keras.backend.numpy.core import convert_to_numpy\n+from keras.backend.numpy.core import convert_to_tensor\n+from keras.backend.numpy.core import is_tensor\n+from keras.backend.numpy.core import shape\n+from keras.backend.numpy.core import vectorized_map\n+from keras.backend.numpy.rnn import cudnn_ok\n+from keras.backend.numpy.rnn import gru\n+from keras.backend.numpy.rnn import lstm\n+from keras.backend.numpy.rnn import rnn\n\n@@ -1,11 +1,11 @@\n import numpy as np\n import tree\n \n-from keras_core.backend.common import KerasVariable\n-from keras_core.backend.common import standardize_dtype\n-from keras_core.backend.common.keras_tensor import KerasTensor\n-from keras_core.backend.common.stateless_scope import StatelessScope\n-from keras_core.utils.nest import pack_sequence_as\n+from keras.backend.common import KerasVariable\n+from keras.backend.common import standardize_dtype\n+from keras.backend.common.keras_tensor import KerasTensor\n+from keras.backend.common.stateless_scope import StatelessScope\n+from keras.utils.nest import pack_sequence_as\n \n SUPPORTS_SPARSE_TENSORS = False\n \n\n@@ -1,8 +1,8 @@\n import jax\n import numpy as np\n \n-from keras_core.backend.numpy.core import convert_to_tensor\n-from keras_core.utils.module_utils import scipy\n+from keras.backend.numpy.core import convert_to_tensor\n+from keras.utils.module_utils import scipy\n \n RESIZE_INTERPOLATIONS = (\n     \"bilinear\",\n\n\n@@ -1,10 +1,10 @@\n import numpy as np\n \n-from keras_core.backend import standardize_dtype\n-from keras_core.backend.jax.math import fft as jax_fft\n-from keras_core.backend.jax.math import fft2 as jax_fft2\n-from keras_core.backend.numpy.core import convert_to_tensor\n-from keras_core.utils.module_utils import scipy\n+from keras.backend import standardize_dtype\n+from keras.backend.jax.math import fft as jax_fft\n+from keras.backend.jax.math import fft2 as jax_fft2\n+from keras.backend.numpy.core import convert_to_tensor\n+from keras.utils.module_utils import scipy\n \n \n def segment_sum(data, segment_ids, num_segments=None, sorted=False):\n\n@@ -3,15 +3,15 @@ import numpy as np\n from jax import lax\n from jax import numpy as jnp\n \n-from keras_core.backend import standardize_data_format\n-from keras_core.backend import standardize_dtype\n-from keras_core.backend.common.backend_utils import (\n+from keras.backend import standardize_data_format\n+from keras.backend import standardize_dtype\n+from keras.backend.common.backend_utils import (\n     compute_conv_transpose_padding_args_for_jax,\n )\n-from keras_core.backend.config import epsilon\n-from keras_core.backend.numpy.core import cast\n-from keras_core.backend.numpy.core import is_tensor\n-from keras_core.utils.module_utils import scipy\n+from keras.backend.config import epsilon\n+from keras.backend.numpy.core import cast\n+from keras.backend.numpy.core import is_tensor\n+from keras.utils.module_utils import scipy\n \n \n def relu(x):\n\n@@ -1,7 +1,7 @@\n import numpy as np\n \n-from keras_core.backend import config\n-from keras_core.backend import standardize_dtype\n+from keras.backend import config\n+from keras.backend import standardize_dtype\n \n \n def add(x1, x2):\n\n@@ -1,10 +1,10 @@\n import numpy as np\n \n-from keras_core.backend.config import floatx\n-from keras_core.backend.numpy.nn import softmax\n-from keras_core.random.seed_generator import SeedGenerator\n-from keras_core.random.seed_generator import draw_seed\n-from keras_core.random.seed_generator import make_default_seed\n+from keras.backend.config import floatx\n+from keras.backend.numpy.nn import softmax\n+from keras.random.seed_generator import SeedGenerator\n+from keras.random.seed_generator import draw_seed\n+from keras.random.seed_generator import make_default_seed\n \n \n def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n\n@@ -1,7 +1,7 @@\n import numpy as np\n import tree\n \n-from keras_core.utils.nest import pack_sequence_as\n+from keras.utils.nest import pack_sequence_as\n \n \n def rnn(\n\n@@ -1,15 +1,15 @@\n import numpy as np\n import tree\n \n-from keras_core import backend\n-from keras_core import callbacks as callbacks_module\n-from keras_core.backend.common import standardize_dtype\n-from keras_core.backend.common.keras_tensor import KerasTensor\n-from keras_core.backend.numpy.core import is_tensor\n-from keras_core.trainers import trainer as base_trainer\n-from keras_core.trainers.data_adapters import data_adapter_utils\n-from keras_core.trainers.epoch_iterator import EpochIterator\n-from keras_core.utils import traceback_utils\n+from keras import backend\n+from keras import callbacks as callbacks_module\n+from keras.backend.common import standardize_dtype\n+from keras.backend.common.keras_tensor import KerasTensor\n+from keras.backend.numpy.core import is_tensor\n+from keras.trainers import trainer as base_trainer\n+from keras.trainers.data_adapters import data_adapter_utils\n+from keras.trainers.epoch_iterator import EpochIterator\n+from keras.utils import traceback_utils\n \n \n class NumpyTrainer(base_trainer.Trainer):\n\n@@ -0,0 +1,24 @@\n+from keras.backend.tensorflow import core\n+from keras.backend.tensorflow import image\n+from keras.backend.tensorflow import math\n+from keras.backend.tensorflow import nn\n+from keras.backend.tensorflow import numpy\n+from keras.backend.tensorflow import random\n+from keras.backend.tensorflow import tensorboard\n+from keras.backend.tensorflow.core import SUPPORTS_SPARSE_TENSORS\n+from keras.backend.tensorflow.core import Variable\n+from keras.backend.tensorflow.core import cast\n+from keras.backend.tensorflow.core import compute_output_spec\n+from keras.backend.tensorflow.core import cond\n+from keras.backend.tensorflow.core import convert_to_numpy\n+from keras.backend.tensorflow.core import convert_to_tensor\n+from keras.backend.tensorflow.core import is_tensor\n+from keras.backend.tensorflow.core import name_scope\n+from keras.backend.tensorflow.core import scatter\n+from keras.backend.tensorflow.core import shape\n+from keras.backend.tensorflow.core import stop_gradient\n+from keras.backend.tensorflow.core import vectorized_map\n+from keras.backend.tensorflow.rnn import cudnn_ok\n+from keras.backend.tensorflow.rnn import gru\n+from keras.backend.tensorflow.rnn import lstm\n+from keras.backend.tensorflow.rnn import rnn\n\n@@ -4,13 +4,13 @@ import numpy as np\n import tensorflow as tf\n from tensorflow.compiler.tf2xla.python.xla import dynamic_update_slice\n \n-from keras_core.backend.common import KerasVariable\n-from keras_core.backend.common import global_state\n-from keras_core.backend.common import standardize_dtype\n-from keras_core.backend.common.keras_tensor import KerasTensor\n-from keras_core.backend.common.name_scope import name_scope as base_name_scope\n-from keras_core.backend.common.stateless_scope import StatelessScope\n-from keras_core.utils.naming import auto_name\n+from keras.backend.common import KerasVariable\n+from keras.backend.common import global_state\n+from keras.backend.common import standardize_dtype\n+from keras.backend.common.keras_tensor import KerasTensor\n+from keras.backend.common.name_scope import name_scope as base_name_scope\n+from keras.backend.common.stateless_scope import StatelessScope\n+from keras.utils.naming import auto_name\n \n SUPPORTS_SPARSE_TENSORS = True\n \n\n@@ -5,11 +5,11 @@ import pytest\n import tensorflow as tf\n from tensorflow.python.eager import context\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import models\n-from keras_core import testing\n-from keras_core.backend.tensorflow import trainer as tf_trainer\n+from keras import backend\n+from keras import layers\n+from keras import models\n+from keras import testing\n+from keras.backend.tensorflow import trainer as tf_trainer\n \n \n @pytest.mark.skipif(\n\n@@ -4,7 +4,7 @@ import operator\n \n import tensorflow as tf\n \n-from keras_core.backend.tensorflow.core import convert_to_tensor\n+from keras.backend.tensorflow.core import convert_to_tensor\n \n RESIZE_INTERPOLATIONS = (\n     \"bilinear\",\n\n@@ -1,6 +1,6 @@\n import tensorflow as tf\n \n-from keras_core.utils import tf_utils\n+from keras.utils import tf_utils\n \n \n class TFLayer(tf.__internal__.tracking.AutoTrackable):\n@@ -64,9 +64,9 @@ class TFLayer(tf.__internal__.tracking.AutoTrackable):\n     def _default_save_signature(self):\n         \"\"\"For SavedModel support: returns the default serving signature.\"\"\"\n \n-        from keras_core.models.functional import Functional\n-        from keras_core.models.model import Model\n-        from keras_core.models.sequential import Sequential\n+        from keras.models.functional import Functional\n+        from keras.models.model import Model\n+        from keras.models.sequential import Sequential\n \n         if not isinstance(self, Model):\n             return None\n\n@@ -1,7 +1,7 @@\n import tensorflow as tf\n \n-from keras_core.backend import standardize_dtype\n-from keras_core.backend.tensorflow.core import convert_to_tensor\n+from keras.backend import standardize_dtype\n+from keras.backend.tensorflow.core import convert_to_tensor\n \n \n def segment_sum(data, segment_ids, num_segments=None, sorted=False):\n\n@@ -1,7 +1,7 @@\n import tensorflow as tf\n \n-from keras_core.backend.tensorflow.core import name_scope\n-from keras_core.testing import TestCase\n+from keras.backend.tensorflow.core import name_scope\n+from keras.testing import TestCase\n \n \n class TFNameScopeTest(TestCase):\n\n@@ -2,13 +2,13 @@ import warnings\n \n import tensorflow as tf\n \n-from keras_core.backend import standardize_data_format\n-from keras_core.backend import standardize_dtype\n-from keras_core.backend.common.backend_utils import (\n+from keras.backend import standardize_data_format\n+from keras.backend import standardize_dtype\n+from keras.backend.common.backend_utils import (\n     compute_conv_transpose_output_shape,\n )\n-from keras_core.backend.config import epsilon\n-from keras_core.backend.tensorflow.core import cast\n+from keras.backend.config import epsilon\n+from keras.backend.tensorflow.core import cast\n \n \n def relu(x):\n\n@@ -7,8 +7,8 @@ import tensorflow as tf\n from tensorflow.experimental import numpy as tfnp\n from tensorflow.python.ops.linalg.sparse import sparse_csr_matrix_ops\n \n-from keras_core.backend import config\n-from keras_core.backend.tensorflow.core import convert_to_tensor\n+from keras.backend import config\n+from keras.backend.tensorflow.core import convert_to_tensor\n \n \n def add(x1, x2):\n\n@@ -1,7 +1,7 @@\n import tensorflow as tf\n \n-from keras_core import backend\n-from keras_core.optimizers import base_optimizer\n+from keras import backend\n+from keras.optimizers import base_optimizer\n \n \n class TFOptimizer(base_optimizer.BaseOptimizer):\n\n@@ -5,9 +5,9 @@ import pytest\n import tensorflow as tf\n from tensorflow.python.eager import context\n \n-from keras_core import backend\n-from keras_core import testing\n-from keras_core.optimizers.sgd import SGD\n+from keras import backend\n+from keras import testing\n+from keras.optimizers.sgd import SGD\n \n \n @pytest.mark.skipif(\n\n@@ -1,11 +1,11 @@\n import tensorflow as tf\n from tensorflow.experimental import numpy as tfnp\n \n-from keras_core.backend.common import standardize_dtype\n-from keras_core.backend.config import floatx\n-from keras_core.random.seed_generator import SeedGenerator\n-from keras_core.random.seed_generator import draw_seed\n-from keras_core.random.seed_generator import make_default_seed\n+from keras.backend.common import standardize_dtype\n+from keras.backend.config import floatx\n+from keras.random.seed_generator import SeedGenerator\n+from keras.random.seed_generator import draw_seed\n+from keras.random.seed_generator import make_default_seed\n \n \n def tf_draw_seed(seed):\n\n@@ -1,7 +1,7 @@\n import tensorflow as tf\n import tree\n \n-from keras_core.utils.nest import pack_sequence_as\n+from keras.utils.nest import pack_sequence_as\n \n \n def rnn(\n@@ -467,7 +467,7 @@ def gru(\n     if not cudnn_supported or not inputs_supported:\n         raise NotImplementedError\n \n-    from keras_core.backend.tensorflow import Variable\n+    from keras.backend.tensorflow import Variable\n \n     if isinstance(kernel, Variable):\n         kernel = kernel.value\n@@ -503,8 +503,8 @@ def _do_gru_arguments_support_cudnn(\n     use_bias,\n     reset_after,\n ):\n-    from keras_core import activations\n-    from keras_core import ops\n+    from keras import activations\n+    from keras import ops\n \n     return (\n         activation in (activations.tanh, tf.tanh, ops.tanh)\n@@ -522,8 +522,8 @@ def _do_lstm_arguments_support_cudnn(\n     unroll,\n     use_bias,\n ):\n-    from keras_core import activations\n-    from keras_core import ops\n+    from keras import activations\n+    from keras import ops\n \n     return (\n         activation in (activations.tanh, tf.tanh, ops.tanh)\n@@ -823,7 +823,7 @@ def lstm(\n     if not cudnn_supported or not inputs_supported:\n         raise NotImplementedError\n \n-    from keras_core.backend.tensorflow import Variable\n+    from keras.backend.tensorflow import Variable\n \n     if isinstance(kernel, Variable):\n         kernel = kernel.value\n\n@@ -6,12 +6,12 @@ import numpy as np\n import pytest\n import tensorflow as tf\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import metrics\n-from keras_core import models\n-from keras_core import testing\n-from keras_core.saving import object_registration\n+from keras import backend\n+from keras import layers\n+from keras import metrics\n+from keras import models\n+from keras import testing\n+from keras.saving import object_registration\n \n \n @object_registration.register_keras_serializable(package=\"my_package\")\n@@ -40,7 +40,7 @@ class SavedModelTest(testing.TestCase):\n         X_train = np.random.rand(100, 3)\n         y_train = np.random.rand(100, 1)\n         model.fit(X_train, y_train)\n-        path = os.path.join(self.get_temp_dir(), \"my_keras_core_model\")\n+        path = os.path.join(self.get_temp_dir(), \"my_keras_model\")\n         tf.saved_model.save(model, path)\n         restored_model = tf.saved_model.load(path)\n         self.assertAllClose(\n@@ -64,7 +64,7 @@ class SavedModelTest(testing.TestCase):\n         X_train = np.random.rand(100, 3)\n         y_train = np.random.rand(100, 1)\n         model.fit(X_train, y_train)\n-        path = os.path.join(self.get_temp_dir(), \"my_keras_core_model\")\n+        path = os.path.join(self.get_temp_dir(), \"my_keras_model\")\n         tf.saved_model.save(model, path)\n         restored_model = tf.saved_model.load(path)\n         self.assertAllClose(\n@@ -86,7 +86,7 @@ class SavedModelTest(testing.TestCase):\n         X_train = np.random.rand(100, 3)\n         y_train = np.random.rand(100, 1)\n         model.fit(X_train, y_train)\n-        path = os.path.join(self.get_temp_dir(), \"my_keras_core_model\")\n+        path = os.path.join(self.get_temp_dir(), \"my_keras_model\")\n         tf.saved_model.save(model, path)\n         restored_model = tf.saved_model.load(path)\n         self.assertAllClose(\n@@ -117,7 +117,7 @@ class SavedModelTest(testing.TestCase):\n         model = Model()\n         inp = np.array([[1.0]])\n         result = model(inp)\n-        path = os.path.join(self.get_temp_dir(), \"my_keras_core_model\")\n+        path = os.path.join(self.get_temp_dir(), \"my_keras_model\")\n         tf.saved_model.save(model, path)\n         restored_model = tf.saved_model.load(path)\n         self.assertAllClose(\n@@ -131,7 +131,7 @@ class SavedModelTest(testing.TestCase):\n         input_1 = layers.Input(shape=(3,))\n         input_2 = layers.Input(shape=(5,))\n         model = models.Model([input_1, input_2], [input_1, input_2])\n-        path = os.path.join(self.get_temp_dir(), \"my_keras_core_model\")\n+        path = os.path.join(self.get_temp_dir(), \"my_keras_model\")\n \n         tf.saved_model.save(model, path)\n         restored_model = tf.saved_model.load(path)\n@@ -173,7 +173,7 @@ class SavedModelTest(testing.TestCase):\n             tf.constant(i, shape=[1, 1], dtype=tf.float32) for i in range(1, 4)\n         ]\n         expected = model(*inp)\n-        path = os.path.join(self.get_temp_dir(), \"my_keras_core_model\")\n+        path = os.path.join(self.get_temp_dir(), \"my_keras_model\")\n         tf.saved_model.save(model, path)\n         restored_model = tf.saved_model.load(path)\n         output = restored_model.call(*inp)\n\n\n@@ -7,13 +7,13 @@ import tree\n from packaging.version import Version\n from tensorflow.python.eager import context as tf_context\n \n-from keras_core import callbacks as callbacks_module\n-from keras_core import metrics as metrics_module\n-from keras_core import optimizers as optimizers_module\n-from keras_core.trainers import trainer as base_trainer\n-from keras_core.trainers.data_adapters import data_adapter_utils\n-from keras_core.trainers.epoch_iterator import EpochIterator\n-from keras_core.utils import traceback_utils\n+from keras import callbacks as callbacks_module\n+from keras import metrics as metrics_module\n+from keras import optimizers as optimizers_module\n+from keras.trainers import trainer as base_trainer\n+from keras.trainers.data_adapters import data_adapter_utils\n+from keras.trainers.epoch_iterator import EpochIterator\n+from keras.utils import traceback_utils\n \n \n class TensorFlowTrainer(base_trainer.Trainer):\n\n@@ -1,7 +1,7 @@\n import unittest\n \n-from keras_core import backend\n-from keras_core.backend.common.keras_tensor import KerasTensor\n+from keras import backend\n+from keras.backend.common.keras_tensor import KerasTensor\n \n \n def single_arg_test_fn(x):\n\n@@ -0,0 +1,39 @@\n+\"\"\"Torch backend APIs.\n+\n+# Note on device placement\n+\n+Torch has a different device placement style compared to TF and JAX.\n+In short, variables/tensors are not created on GPU by default,\n+and the GPU cannot directly communicate with the CPU.\n+To bring Torch behavior in line with TF and JAX automated device placement,\n+we are doing the following to automate device placement if a GPU is available:\n+\n+- Variables are created on GPU.\n+- Input data will be placed on GPU at the first `keras.layers.Layer` call.\n+- Tensor creation happens on GPU, e.g., `zeros()` will create a tensor on GPU.\n+- `convert_to_numpy` will bring the tensor to CPU before converting it to NumPy.\n+\"\"\"\n+\n+from keras.backend.torch import core\n+from keras.backend.torch import image\n+from keras.backend.torch import math\n+from keras.backend.torch import nn\n+from keras.backend.torch import numpy\n+from keras.backend.torch import random\n+from keras.backend.torch.core import SUPPORTS_SPARSE_TENSORS\n+from keras.backend.torch.core import Variable\n+from keras.backend.torch.core import cast\n+from keras.backend.torch.core import compute_output_spec\n+from keras.backend.torch.core import cond\n+from keras.backend.torch.core import convert_to_numpy\n+from keras.backend.torch.core import convert_to_tensor\n+from keras.backend.torch.core import is_tensor\n+from keras.backend.torch.core import scatter\n+from keras.backend.torch.core import shape\n+from keras.backend.torch.core import stop_gradient\n+from keras.backend.torch.core import to_torch_dtype\n+from keras.backend.torch.core import vectorized_map\n+from keras.backend.torch.rnn import cudnn_ok\n+from keras.backend.torch.rnn import gru\n+from keras.backend.torch.rnn import lstm\n+from keras.backend.torch.rnn import rnn\n\n@@ -5,12 +5,12 @@ import numpy as np\n import torch\n import tree\n \n-from keras_core.backend.common import KerasVariable\n-from keras_core.backend.common import global_state\n-from keras_core.backend.common import standardize_dtype\n-from keras_core.backend.common.keras_tensor import KerasTensor\n-from keras_core.backend.common.stateless_scope import StatelessScope\n-from keras_core.utils.nest import pack_sequence_as\n+from keras.backend.common import KerasVariable\n+from keras.backend.common import global_state\n+from keras.backend.common import standardize_dtype\n+from keras.backend.common.keras_tensor import KerasTensor\n+from keras.backend.common.stateless_scope import StatelessScope\n+from keras.utils.nest import pack_sequence_as\n \n SUPPORTS_SPARSE_TENSORS = False\n \n\n@@ -4,7 +4,7 @@ import operator\n \n import torch\n \n-from keras_core.backend.torch.core import convert_to_tensor\n+from keras.backend.torch.core import convert_to_tensor\n \n RESIZE_INTERPOLATIONS = {}  # populated after torchvision import\n \n\n@@ -1,7 +1,7 @@\n import torch\n \n-from keras_core.backend.common.stateless_scope import in_stateless_scope\n-from keras_core.ops.operation import Operation\n+from keras.backend.common.stateless_scope import in_stateless_scope\n+from keras.ops.operation import Operation\n \n \n class TorchLayer(torch.nn.Module):\n@@ -26,14 +26,14 @@ class TorchLayer(torch.nn.Module):\n         return Operation.__call__(self, *args, **kwargs)\n \n     def _setattr_hook(self, name, value):\n-        from keras_core.layers import Layer\n+        from keras.layers import Layer\n \n         if (\n             isinstance(value, torch.nn.Module)\n             and not isinstance(value, Layer)\n             and not name == \"torch_params\"\n         ):\n-            from keras_core.utils.torch_utils import TorchModuleWrapper\n+            from keras.utils.torch_utils import TorchModuleWrapper\n \n             if not isinstance(self, TorchModuleWrapper):\n                 value = TorchModuleWrapper(value)\n\n@@ -2,10 +2,10 @@ import math\n \n import torch\n \n-from keras_core.backend import standardize_dtype\n-from keras_core.backend.torch.core import convert_to_tensor\n-from keras_core.backend.torch.core import get_device\n-from keras_core.backend.torch.numpy import pad\n+from keras.backend import standardize_dtype\n+from keras.backend.torch.core import convert_to_tensor\n+from keras.backend.torch.core import get_device\n+from keras.backend.torch.numpy import pad\n \n \n def segment_sum(data, segment_ids, num_segments=None, **kwargs):\n\n@@ -2,19 +2,19 @@ import numpy as np\n import torch\n import torch.nn.functional as tnn\n \n-from keras_core.backend import standardize_data_format\n-from keras_core.backend import standardize_dtype\n-from keras_core.backend.common.backend_utils import (\n+from keras.backend import standardize_data_format\n+from keras.backend import standardize_dtype\n+from keras.backend.common.backend_utils import (\n     compute_conv_transpose_padding_args_for_torch,\n )\n-from keras_core.backend.config import epsilon\n-from keras_core.backend.torch.core import cast\n-from keras_core.backend.torch.core import convert_to_tensor\n-from keras_core.backend.torch.core import get_device\n-from keras_core.backend.torch.numpy import expand_dims\n-from keras_core.backend.torch.numpy import maximum\n-from keras_core.backend.torch.numpy import where\n-from keras_core.utils.argument_validation import standardize_tuple\n+from keras.backend.config import epsilon\n+from keras.backend.torch.core import cast\n+from keras.backend.torch.core import convert_to_tensor\n+from keras.backend.torch.core import get_device\n+from keras.backend.torch.numpy import expand_dims\n+from keras.backend.torch.numpy import maximum\n+from keras.backend.torch.numpy import where\n+from keras.utils.argument_validation import standardize_tuple\n \n \n def relu(x):\n\n@@ -1,12 +1,12 @@\n import numpy as np\n import torch\n \n-from keras_core.backend import config\n-from keras_core.backend.torch.core import cast\n-from keras_core.backend.torch.core import convert_to_tensor\n-from keras_core.backend.torch.core import get_device\n-from keras_core.backend.torch.core import is_tensor\n-from keras_core.backend.torch.core import to_torch_dtype\n+from keras.backend import config\n+from keras.backend.torch.core import cast\n+from keras.backend.torch.core import convert_to_tensor\n+from keras.backend.torch.core import get_device\n+from keras.backend.torch.core import is_tensor\n+from keras.backend.torch.core import to_torch_dtype\n \n TORCH_INT_TYPES = (\n     torch.int8,\n@@ -771,9 +771,9 @@ def repeat(x, repeats, axis=None):\n     if get_device() == \"meta\":\n         # Import upper level modules locally to avoid circular imports\n         # TODO: Refactor the upper level modules to avoid these imports.\n-        from keras_core.backend import KerasTensor\n-        from keras_core.backend import standardize_dtype\n-        from keras_core.ops.numpy import repeat\n+        from keras.backend import KerasTensor\n+        from keras.backend import standardize_dtype\n+        from keras.ops.numpy import repeat\n \n         x = KerasTensor(x.shape, standardize_dtype(x.dtype))\n         outputs = repeat(x, repeats, axis=axis)\n\n@@ -0,0 +1 @@\n+from keras.backend.torch.optimizers.torch_optimizer import TorchOptimizer\n\n@@ -1,8 +1,8 @@\n import torch\n \n-from keras_core import ops\n-from keras_core import optimizers\n-from keras_core.backend.torch.optimizers import torch_parallel_optimizer\n+from keras import ops\n+from keras import optimizers\n+from keras.backend.torch.optimizers import torch_parallel_optimizer\n \n \n class Adadelta(\n\n@@ -1,8 +1,8 @@\n import torch\n \n-from keras_core import ops\n-from keras_core import optimizers\n-from keras_core.backend.torch.optimizers import torch_parallel_optimizer\n+from keras import ops\n+from keras import optimizers\n+from keras.backend.torch.optimizers import torch_parallel_optimizer\n \n \n class Adagrad(\n\n@@ -1,8 +1,8 @@\n import torch\n \n-from keras_core import ops\n-from keras_core import optimizers\n-from keras_core.backend.torch.optimizers import torch_parallel_optimizer\n+from keras import ops\n+from keras import optimizers\n+from keras.backend.torch.optimizers import torch_parallel_optimizer\n \n \n class Adam(torch_parallel_optimizer.TorchParallelOptimizer, optimizers.Adam):\n\n@@ -1,8 +1,8 @@\n import torch\n \n-from keras_core import ops\n-from keras_core import optimizers\n-from keras_core.backend.torch.optimizers import torch_parallel_optimizer\n+from keras import ops\n+from keras import optimizers\n+from keras.backend.torch.optimizers import torch_parallel_optimizer\n \n \n class Adamax(\n\n@@ -0,0 +1,6 @@\n+from keras import optimizers\n+from keras.backend.torch.optimizers import torch_adam\n+\n+\n+class AdamW(torch_adam.Adam, optimizers.AdamW):\n+    pass\n\n@@ -1,8 +1,8 @@\n import torch\n \n-from keras_core import ops\n-from keras_core import optimizers\n-from keras_core.backend.torch.optimizers import torch_parallel_optimizer\n+from keras import ops\n+from keras import optimizers\n+from keras.backend.torch.optimizers import torch_parallel_optimizer\n \n \n class Lion(torch_parallel_optimizer.TorchParallelOptimizer, optimizers.Lion):\n\n@@ -1,9 +1,9 @@\n import torch\n \n-from keras_core import ops\n-from keras_core import optimizers\n-from keras_core.backend.torch import core\n-from keras_core.backend.torch.optimizers import torch_parallel_optimizer\n+from keras import ops\n+from keras import optimizers\n+from keras.backend.torch import core\n+from keras.backend.torch.optimizers import torch_parallel_optimizer\n \n \n class Nadam(torch_parallel_optimizer.TorchParallelOptimizer, optimizers.Nadam):\n\n@@ -1,21 +1,21 @@\n import torch\n \n-from keras_core import optimizers\n-from keras_core.optimizers.base_optimizer import BaseOptimizer\n+from keras import optimizers\n+from keras.optimizers.base_optimizer import BaseOptimizer\n \n \n class TorchOptimizer(BaseOptimizer):\n     def __new__(cls, *args, **kwargs):\n         # Import locally to avoid circular imports.\n-        from keras_core.backend.torch.optimizers import torch_adadelta\n-        from keras_core.backend.torch.optimizers import torch_adagrad\n-        from keras_core.backend.torch.optimizers import torch_adam\n-        from keras_core.backend.torch.optimizers import torch_adamax\n-        from keras_core.backend.torch.optimizers import torch_adamw\n-        from keras_core.backend.torch.optimizers import torch_lion\n-        from keras_core.backend.torch.optimizers import torch_nadam\n-        from keras_core.backend.torch.optimizers import torch_rmsprop\n-        from keras_core.backend.torch.optimizers import torch_sgd\n+        from keras.backend.torch.optimizers import torch_adadelta\n+        from keras.backend.torch.optimizers import torch_adagrad\n+        from keras.backend.torch.optimizers import torch_adam\n+        from keras.backend.torch.optimizers import torch_adamax\n+        from keras.backend.torch.optimizers import torch_adamw\n+        from keras.backend.torch.optimizers import torch_lion\n+        from keras.backend.torch.optimizers import torch_nadam\n+        from keras.backend.torch.optimizers import torch_rmsprop\n+        from keras.backend.torch.optimizers import torch_sgd\n \n         OPTIMIZERS = {\n             optimizers.Adadelta: torch_adadelta.Adadelta,\n\n@@ -1,4 +1,4 @@\n-from keras_core.optimizers.base_optimizer import BaseOptimizer\n+from keras.optimizers.base_optimizer import BaseOptimizer\n \n \n class TorchParallelOptimizer(BaseOptimizer):\n\n@@ -1,8 +1,8 @@\n import torch\n \n-from keras_core import ops\n-from keras_core import optimizers\n-from keras_core.backend.torch.optimizers import torch_parallel_optimizer\n+from keras import ops\n+from keras import optimizers\n+from keras.backend.torch.optimizers import torch_parallel_optimizer\n \n \n class RMSprop(\n\n@@ -1,7 +1,7 @@\n import torch\n \n-from keras_core import optimizers\n-from keras_core.backend.torch.optimizers import torch_parallel_optimizer\n+from keras import optimizers\n+from keras.backend.torch.optimizers import torch_parallel_optimizer\n \n \n class SGD(torch_parallel_optimizer.TorchParallelOptimizer, optimizers.SGD):\n\n@@ -1,13 +1,13 @@\n import torch\n import torch.nn.functional as tnn\n \n-from keras_core.backend.config import floatx\n-from keras_core.backend.torch.core import convert_to_tensor\n-from keras_core.backend.torch.core import get_device\n-from keras_core.backend.torch.core import to_torch_dtype\n-from keras_core.random.seed_generator import SeedGenerator\n-from keras_core.random.seed_generator import draw_seed\n-from keras_core.random.seed_generator import make_default_seed\n+from keras.backend.config import floatx\n+from keras.backend.torch.core import convert_to_tensor\n+from keras.backend.torch.core import get_device\n+from keras.backend.torch.core import to_torch_dtype\n+from keras.random.seed_generator import SeedGenerator\n+from keras.random.seed_generator import draw_seed\n+from keras.random.seed_generator import make_default_seed\n \n \n def torch_seed_generator(seed):\n\n@@ -1,8 +1,8 @@\n import torch\n import tree\n \n-from keras_core.backend.torch.core import convert_to_tensor\n-from keras_core.utils.nest import pack_sequence_as\n+from keras.backend.torch.core import convert_to_tensor\n+from keras.utils.nest import pack_sequence_as\n \n \n def rnn(\n\n@@ -4,16 +4,16 @@ import numpy as np\n import torch\n import tree\n \n-from keras_core import backend\n-from keras_core import callbacks as callbacks_module\n-from keras_core import optimizers as optimizers_module\n-from keras_core.backend.common import standardize_dtype\n-from keras_core.backend.common.keras_tensor import KerasTensor\n-from keras_core.backend.torch.core import is_tensor\n-from keras_core.trainers import trainer as base_trainer\n-from keras_core.trainers.data_adapters import data_adapter_utils\n-from keras_core.trainers.epoch_iterator import EpochIterator\n-from keras_core.utils import traceback_utils\n+from keras import backend\n+from keras import callbacks as callbacks_module\n+from keras import optimizers as optimizers_module\n+from keras.backend.common import standardize_dtype\n+from keras.backend.common.keras_tensor import KerasTensor\n+from keras.backend.torch.core import is_tensor\n+from keras.trainers import trainer as base_trainer\n+from keras.trainers.data_adapters import data_adapter_utils\n+from keras.trainers.epoch_iterator import EpochIterator\n+from keras.utils import traceback_utils\n \n \n class TorchTrainer(base_trainer.Trainer):\n\n@@ -0,0 +1,14 @@\n+from keras.callbacks.backup_and_restore_callback import BackupAndRestore\n+from keras.callbacks.callback import Callback\n+from keras.callbacks.callback_list import CallbackList\n+from keras.callbacks.csv_logger import CSVLogger\n+from keras.callbacks.early_stopping import EarlyStopping\n+from keras.callbacks.history import History\n+from keras.callbacks.lambda_callback import LambdaCallback\n+from keras.callbacks.learning_rate_scheduler import LearningRateScheduler\n+from keras.callbacks.model_checkpoint import ModelCheckpoint\n+from keras.callbacks.progbar_logger import ProgbarLogger\n+from keras.callbacks.reduce_lr_on_plateau import ReduceLROnPlateau\n+from keras.callbacks.remote_monitor import RemoteMonitor\n+from keras.callbacks.tensorboard import TensorBoard\n+from keras.callbacks.terminate_on_nan import TerminateOnNaN\n\n@@ -1,9 +1,9 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.callbacks.callback import Callback\n-from keras_core.utils import file_utils\n+from keras.api_export import keras_export\n+from keras.callbacks.callback import Callback\n+from keras.utils import file_utils\n \n \n-@keras_core_export(\"keras_core.callbacks.BackupAndRestore\")\n+@keras_export(\"keras.callbacks.BackupAndRestore\")\n class BackupAndRestore(Callback):\n     \"\"\"Callback to back up and restore the training state.\n \n\n@@ -1,11 +1,11 @@\n import numpy as np\n import pytest\n \n-from keras_core import callbacks\n-from keras_core import layers\n-from keras_core import testing\n-from keras_core.models import Sequential\n-from keras_core.utils import file_utils\n+from keras import callbacks\n+from keras import layers\n+from keras import testing\n+from keras.models import Sequential\n+from keras.utils import file_utils\n \n \n class InterruptingCallback(callbacks.Callback):\n\n@@ -1,8 +1,8 @@\n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n+from keras import backend\n+from keras.api_export import keras_export\n \n \n-@keras_core_export(\"keras_core.callbacks.Callback\")\n+@keras_export(\"keras.callbacks.Callback\")\n class Callback:\n     \"\"\"Base class used to build new callbacks.\n \n@@ -10,7 +10,7 @@ class Callback:\n     `predict()` in order to hook into the various stages of the model training,\n     evaluation, and inference lifecycle.\n \n-    To create a custom callback, subclass `keras_core.callbacks.Callback` and\n+    To create a custom callback, subclass `keras.callbacks.Callback` and\n     override the method associated with the stage of interest.\n \n     Example:\n@@ -37,7 +37,7 @@ class Callback:\n     Example:\n \n     ```python\n-    callbacks =  keras_core.callbacks.CallbackList([...])\n+    callbacks =  keras.callbacks.CallbackList([...])\n     callbacks.append(...)\n     callbacks.on_train_begin(...)\n     for epoch in range(EPOCHS):\n\n@@ -1,12 +1,12 @@\n import tree\n \n-from keras_core.api_export import keras_core_export\n-from keras_core.callbacks.callback import Callback\n-from keras_core.callbacks.history import History\n-from keras_core.callbacks.progbar_logger import ProgbarLogger\n+from keras.api_export import keras_export\n+from keras.callbacks.callback import Callback\n+from keras.callbacks.history import History\n+from keras.callbacks.progbar_logger import ProgbarLogger\n \n \n-@keras_core_export(\"keras_core.callbacks.CallbackList\")\n+@keras_export(\"keras.callbacks.CallbackList\")\n class CallbackList(Callback):\n     \"\"\"Container abstracting a list of callbacks.\"\"\"\n \n\n@@ -1,9 +1,9 @@\n import numpy as np\n import pytest\n \n-from keras_core import models\n-from keras_core import testing\n-from keras_core.callbacks.callback import Callback\n+from keras import models\n+from keras import testing\n+from keras.callbacks.callback import Callback\n \n \n class CallbackTest(testing.TestCase):\n\n@@ -3,12 +3,12 @@ import csv\n \n import numpy as np\n \n-from keras_core.api_export import keras_core_export\n-from keras_core.callbacks.callback import Callback\n-from keras_core.utils import file_utils\n+from keras.api_export import keras_export\n+from keras.callbacks.callback import Callback\n+from keras.utils import file_utils\n \n \n-@keras_core_export(\"keras_core.callbacks.CSVLogger\")\n+@keras_export(\"keras.callbacks.CSVLogger\")\n class CSVLogger(Callback):\n     \"\"\"Callback that streams epoch results to a CSV file.\n \n\n@@ -6,12 +6,12 @@ import tempfile\n import numpy as np\n import pytest\n \n-from keras_core import callbacks\n-from keras_core import initializers\n-from keras_core import layers\n-from keras_core import testing\n-from keras_core.models import Sequential\n-from keras_core.utils import numerical_utils\n+from keras import callbacks\n+from keras import initializers\n+from keras import layers\n+from keras import testing\n+from keras.models import Sequential\n+from keras.utils import numerical_utils\n \n TRAIN_SAMPLES = 10\n TEST_SAMPLES = 10\n\n@@ -1,12 +1,12 @@\n import warnings\n \n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.callbacks.callback import Callback\n-from keras_core.utils import io_utils\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.callbacks.callback import Callback\n+from keras.utils import io_utils\n \n \n-@keras_core_export(\"keras_core.callbacks.EarlyStopping\")\n+@keras_export(\"keras.callbacks.EarlyStopping\")\n class EarlyStopping(Callback):\n     \"\"\"Stop training when a monitored metric has stopped improving.\n \n@@ -52,12 +52,12 @@ class EarlyStopping(Callback):\n \n     Example:\n \n-    >>> callback = keras_core.callbacks.EarlyStopping(monitor='loss',\n+    >>> callback = keras.callbacks.EarlyStopping(monitor='loss',\n     ...                                               patience=3)\n     >>> # This callback will stop the training when there is no improvement in\n     >>> # the loss for three consecutive epochs.\n-    >>> model = keras_core.models.Sequential([keras_core.layers.Dense(10)])\n-    >>> model.compile(keras_core.optimizers.SGD(), loss='mse')\n+    >>> model = keras.models.Sequential([keras.layers.Dense(10)])\n+    >>> model.compile(keras.optimizers.SGD(), loss='mse')\n     >>> history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),\n     ...                     epochs=10, batch_size=1, callbacks=[callback],\n     ...                     verbose=0)\n\n@@ -1,10 +1,10 @@\n import numpy as np\n import pytest\n \n-from keras_core import callbacks\n-from keras_core import layers\n-from keras_core import models\n-from keras_core import testing\n+from keras import callbacks\n+from keras import layers\n+from keras import models\n+from keras import testing\n \n \n class EarlyStoppingTest(testing.TestCase):\n\n@@ -1,8 +1,8 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.callbacks.callback import Callback\n+from keras.api_export import keras_export\n+from keras.callbacks.callback import Callback\n \n \n-@keras_core_export(\"keras_core.callbacks.History\")\n+@keras_export(\"keras.callbacks.History\")\n class History(Callback):\n     \"\"\"Callback that records events into a `History` object.\n \n\n@@ -1,8 +1,8 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.callbacks.callback import Callback\n+from keras.api_export import keras_export\n+from keras.callbacks.callback import Callback\n \n \n-@keras_core_export(\"keras_core.callbacks.LambdaCallback\")\n+@keras_export(\"keras.callbacks.LambdaCallback\")\n class LambdaCallback(Callback):\n     \"\"\"Callback for creating simple, custom callbacks on-the-fly.\n \n\n@@ -2,12 +2,12 @@ import numpy as np\n import pytest\n from absl import logging\n \n-from keras_core import callbacks\n-from keras_core import layers\n-from keras_core import losses\n-from keras_core import optimizers\n-from keras_core import testing\n-from keras_core.models.sequential import Sequential\n+from keras import callbacks\n+from keras import layers\n+from keras import losses\n+from keras import optimizers\n+from keras import testing\n+from keras.models.sequential import Sequential\n \n \n class LambdaCallbackTest(testing.TestCase):\n\n@@ -1,12 +1,12 @@\n import numpy as np\n \n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.callbacks.callback import Callback\n-from keras_core.utils import io_utils\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.callbacks.callback import Callback\n+from keras.utils import io_utils\n \n \n-@keras_core_export(\"keras_core.callbacks.LearningRateScheduler\")\n+@keras_export(\"keras.callbacks.LearningRateScheduler\")\n class LearningRateScheduler(Callback):\n     \"\"\"Learning rate scheduler.\n \n@@ -31,12 +31,12 @@ class LearningRateScheduler(Callback):\n     ...     else:\n     ...         return lr * ops.exp(-0.1)\n     >>>\n-    >>> model = keras_core.models.Sequential([keras_core.layers.Dense(10)])\n-    >>> model.compile(keras_core.optimizers.SGD(), loss='mse')\n+    >>> model = keras.models.Sequential([keras.layers.Dense(10)])\n+    >>> model.compile(keras.optimizers.SGD(), loss='mse')\n     >>> round(model.optimizer.learning_rate, 5)\n     0.01\n \n-    >>> callback = keras_core.callbacks.LearningRateScheduler(scheduler)\n+    >>> callback = keras.callbacks.LearningRateScheduler(scheduler)\n     >>> history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),\n     ...                     epochs=15, callbacks=[callback], verbose=0)\n     >>> round(model.optimizer.learning_rate, 5)\n\n@@ -1,13 +1,13 @@\n import pytest\n \n-from keras_core import callbacks\n-from keras_core import layers\n-from keras_core import optimizers\n-from keras_core import testing\n-from keras_core.models import Sequential\n-from keras_core.testing import test_utils\n-from keras_core.utils import io_utils\n-from keras_core.utils import numerical_utils\n+from keras import callbacks\n+from keras import layers\n+from keras import optimizers\n+from keras import testing\n+from keras.models import Sequential\n+from keras.testing import test_utils\n+from keras.utils import io_utils\n+from keras.utils import numerical_utils\n \n \n class LearningRateSchedulerTest(testing.TestCase):\n\n@@ -4,13 +4,13 @@ import warnings\n \n import numpy as np\n \n-from keras_core.api_export import keras_core_export\n-from keras_core.callbacks.callback import Callback\n-from keras_core.utils import file_utils\n-from keras_core.utils import io_utils\n+from keras.api_export import keras_export\n+from keras.callbacks.callback import Callback\n+from keras.utils import file_utils\n+from keras.utils import io_utils\n \n \n-@keras_core_export(\"keras_core.callbacks.ModelCheckpoint\")\n+@keras_export(\"keras.callbacks.ModelCheckpoint\")\n class ModelCheckpoint(Callback):\n     \"\"\"Callback to save the Keras model or model weights at some frequency.\n \n@@ -38,7 +38,7 @@ class ModelCheckpoint(Callback):\n \n     EPOCHS = 10\n     checkpoint_filepath = '/tmp/ckpt/checkpoint.model.keras'\n-    model_checkpoint_callback = keras_core.callbacks.ModelCheckpoint(\n+    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n         filepath=checkpoint_filepath,\n         monitor='val_accuracy',\n         mode='max',\n@@ -48,11 +48,11 @@ class ModelCheckpoint(Callback):\n     model.fit(epochs=EPOCHS, callbacks=[model_checkpoint_callback])\n \n     # The model (that are considered the best) can be loaded as -\n-    keras_core.models.load_model(checkpoint_filepath)\n+    keras.models.load_model(checkpoint_filepath)\n \n     # Alternatively, one could checkpoint just the model weights as -\n     checkpoint_filepath = '/tmp/ckpt/checkpoint.weights.h5'\n-    model_checkpoint_callback = keras_core.callbacks.ModelCheckpoint(\n+    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n         filepath=checkpoint_filepath,\n         save_weights_only=True,\n         monitor='val_accuracy',\n\n@@ -3,15 +3,15 @@ import warnings\n \n import pytest\n \n-from keras_core import callbacks\n-from keras_core import layers\n-from keras_core import metrics\n-from keras_core import models\n-from keras_core import saving\n-from keras_core import testing\n-from keras_core.models import Sequential\n-from keras_core.testing import test_utils\n-from keras_core.utils import numerical_utils\n+from keras import callbacks\n+from keras import layers\n+from keras import metrics\n+from keras import models\n+from keras import saving\n+from keras import testing\n+from keras.models import Sequential\n+from keras.testing import test_utils\n+from keras.utils import numerical_utils\n \n try:\n     import h5py\n\n@@ -1,10 +1,10 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.callbacks.callback import Callback\n-from keras_core.utils import io_utils\n-from keras_core.utils.progbar import Progbar\n+from keras.api_export import keras_export\n+from keras.callbacks.callback import Callback\n+from keras.utils import io_utils\n+from keras.utils.progbar import Progbar\n \n \n-@keras_core_export(\"keras_core.callbacks.ProgbarLogger\")\n+@keras_export(\"keras.callbacks.ProgbarLogger\")\n class ProgbarLogger(Callback):\n     \"\"\"Callback that prints metrics to stdout.\n \n\n@@ -2,13 +2,13 @@ import warnings\n \n import numpy as np\n \n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.callbacks.callback import Callback\n-from keras_core.utils import io_utils\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.callbacks.callback import Callback\n+from keras.utils import io_utils\n \n \n-@keras_core_export(\"keras_core.callbacks.ReduceLROnPlateau\")\n+@keras_export(\"keras.callbacks.ReduceLROnPlateau\")\n class ReduceLROnPlateau(Callback):\n     \"\"\"Reduce learning rate when a metric has stopped improving.\n \n\n@@ -1,13 +1,13 @@\n import pytest\n \n-from keras_core import callbacks\n-from keras_core import layers\n-from keras_core import optimizers\n-from keras_core import testing\n-from keras_core.models import Sequential\n-from keras_core.testing import test_utils\n-from keras_core.utils import io_utils\n-from keras_core.utils import numerical_utils\n+from keras import callbacks\n+from keras import layers\n+from keras import optimizers\n+from keras import testing\n+from keras.models import Sequential\n+from keras.testing import test_utils\n+from keras.utils import io_utils\n+from keras.utils import numerical_utils\n \n \n class ReduceLROnPlateauTest(testing.TestCase):\n\n@@ -3,8 +3,8 @@ import warnings\n \n import numpy as np\n \n-from keras_core.api_export import keras_core_export\n-from keras_core.callbacks.callback import Callback\n+from keras.api_export import keras_export\n+from keras.callbacks.callback import Callback\n \n try:\n     import requests\n@@ -12,7 +12,7 @@ except ImportError:\n     requests = None\n \n \n-@keras_core_export(\"keras_core.callbacks.RemoteMonitor\")\n+@keras_export(\"keras.callbacks.RemoteMonitor\")\n class RemoteMonitor(Callback):\n     \"\"\"Callback used to stream events to a server.\n \n\n@@ -3,12 +3,12 @@ from unittest import mock\n \n import numpy as np\n \n-from keras_core import backend\n-from keras_core import callbacks\n-from keras_core import layers\n-from keras_core import testing\n-from keras_core.models import Sequential\n-from keras_core.utils import numerical_utils\n+from keras import backend\n+from keras import callbacks\n+from keras import layers\n+from keras import testing\n+from keras.models import Sequential\n+from keras.utils import numerical_utils\n \n try:\n     import requests\n\n@@ -6,16 +6,16 @@ import warnings\n \n import tree\n \n-from keras_core import backend\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.callbacks.callback import Callback\n-from keras_core.layers import Embedding\n-from keras_core.optimizers import Optimizer\n-from keras_core.utils import file_utils\n+from keras import backend\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.callbacks.callback import Callback\n+from keras.layers import Embedding\n+from keras.optimizers import Optimizer\n+from keras.utils import file_utils\n \n \n-@keras_core_export(\"keras_core.callbacks.TensorBoard\")\n+@keras_export(\"keras.callbacks.TensorBoard\")\n class TensorBoard(Callback):\n     \"\"\"Enable visualizations for TensorBoard.\n \n@@ -94,7 +94,7 @@ class TensorBoard(Callback):\n     Basic usage:\n \n     ```python\n-    tensorboard_callback = keras_core.callbacks.TensorBoard(log_dir=\"./logs\")\n+    tensorboard_callback = keras.callbacks.TensorBoard(log_dir=\"./logs\")\n     model.fit(x_train, y_train, epochs=2, callbacks=[tensorboard_callback])\n     # Then run the tensorboard command to view the visualizations.\n     ```\n@@ -102,10 +102,10 @@ class TensorBoard(Callback):\n     Custom batch-level summaries in a subclassed Model:\n \n     ```python\n-    class MyModel(keras_core.Model):\n+    class MyModel(keras.Model):\n \n         def build(self, _):\n-            self.dense = keras_core.layers.Dense(10)\n+            self.dense = keras.layers.Dense(10)\n \n         def call(self, x):\n             outputs = self.dense(x)\n@@ -118,7 +118,7 @@ class TensorBoard(Callback):\n     # Make sure to set `update_freq=N` to log a batch-level summary every N\n     # batches.  In addition to any `tf.summary` contained in `model.call()`,\n     # metrics added in `Model.compile` will be logged every N batches.\n-    tb_callback = keras_core.callbacks.TensorBoard('./logs', update_freq=1)\n+    tb_callback = keras.callbacks.TensorBoard('./logs', update_freq=1)\n     model.fit(x_train, y_train, callbacks=[tb_callback])\n     ```\n \n@@ -129,16 +129,16 @@ class TensorBoard(Callback):\n         tf.summary.histogram('x', x)\n         return x\n \n-    inputs = keras_core.Input(10)\n-    x = keras_core.layers.Dense(10)(inputs)\n-    outputs = keras_core.layers.Lambda(my_summary)(x)\n-    model = keras_core.Model(inputs, outputs)\n+    inputs = keras.Input(10)\n+    x = keras.layers.Dense(10)(inputs)\n+    outputs = keras.layers.Lambda(my_summary)(x)\n+    model = keras.Model(inputs, outputs)\n     model.compile('sgd', 'mse')\n \n     # Make sure to set `update_freq=N` to log a batch-level summary every N\n     # batches. In addition to any `tf.summary` contained in `Model.call`,\n     # metrics added in `Model.compile` will be logged every N batches.\n-    tb_callback = keras_core.callbacks.TensorBoard('./logs', update_freq=1)\n+    tb_callback = keras.callbacks.TensorBoard('./logs', update_freq=1)\n     model.fit(x_train, y_train, callbacks=[tb_callback])\n     ```\n \n@@ -146,12 +146,12 @@ class TensorBoard(Callback):\n \n     ```python\n     # Profile a single batch, e.g. the 5th batch.\n-    tensorboard_callback = keras_core.callbacks.TensorBoard(\n+    tensorboard_callback = keras.callbacks.TensorBoard(\n         log_dir='./logs', profile_batch=5)\n     model.fit(x_train, y_train, epochs=2, callbacks=[tensorboard_callback])\n \n     # Profile a range of batches, e.g. from 10 to 20.\n-    tensorboard_callback = keras_core.callbacks.TensorBoard(\n+    tensorboard_callback = keras.callbacks.TensorBoard(\n         log_dir='./logs', profile_batch=(10,20))\n     model.fit(x_train, y_train, epochs=2, callbacks=[tensorboard_callback])\n     ```\n@@ -301,7 +301,7 @@ class TensorBoard(Callback):\n         ):\n             raise ValueError(\n                 \"Unrecognized `Embedding` layer names passed to \"\n-                \"`keras_core.callbacks.TensorBoard` `embeddings_metadata` \"\n+                \"`keras.callbacks.TensorBoard` `embeddings_metadata` \"\n                 f\"argument: {self.embeddings_metadata.keys()}\"\n             )\n \n\n@@ -9,15 +9,15 @@ from tensorflow.compat.v1 import SummaryMetadata\n from tensorflow.core.util import event_pb2\n from tensorflow.python.lib.io import tf_record\n \n-from keras_core import backend\n-from keras_core import callbacks\n-from keras_core import layers\n-from keras_core import losses\n-from keras_core import models\n-from keras_core import ops\n-from keras_core import optimizers\n-from keras_core import testing\n-from keras_core.optimizers import schedules\n+from keras import backend\n+from keras import callbacks\n+from keras import layers\n+from keras import losses\n+from keras import models\n+from keras import ops\n+from keras import optimizers\n+from keras import testing\n+from keras.optimizers import schedules\n \n # Note: this file and tensorboard in general has a dependency on tensorflow\n \n\n@@ -1,11 +1,11 @@\n import numpy as np\n \n-from keras_core.api_export import keras_core_export\n-from keras_core.callbacks.callback import Callback\n-from keras_core.utils import io_utils\n+from keras.api_export import keras_export\n+from keras.callbacks.callback import Callback\n+from keras.utils import io_utils\n \n \n-@keras_core_export(\"keras_core.callbacks.TerminateOnNaN\")\n+@keras_export(\"keras.callbacks.TerminateOnNaN\")\n class TerminateOnNaN(Callback):\n     \"\"\"Callback that terminates training when a NaN loss is encountered.\"\"\"\n \n\n@@ -1,12 +1,12 @@\n import numpy as np\n import pytest\n \n-from keras_core import callbacks\n-from keras_core import initializers\n-from keras_core import layers\n-from keras_core import testing\n-from keras_core.models import Sequential\n-from keras_core.utils import numerical_utils\n+from keras import callbacks\n+from keras import initializers\n+from keras import layers\n+from keras import testing\n+from keras.models import Sequential\n+from keras.utils import numerical_utils\n \n \n class TerminateOnNaNTest(testing.TestCase):\n\n@@ -1,13 +1,13 @@\n import inspect\n \n-from keras_core.api_export import keras_core_export\n-from keras_core.constraints.constraints import Constraint\n-from keras_core.constraints.constraints import MaxNorm\n-from keras_core.constraints.constraints import MinMaxNorm\n-from keras_core.constraints.constraints import NonNeg\n-from keras_core.constraints.constraints import UnitNorm\n-from keras_core.saving import serialization_lib\n-from keras_core.utils.naming import to_snake_case\n+from keras.api_export import keras_export\n+from keras.constraints.constraints import Constraint\n+from keras.constraints.constraints import MaxNorm\n+from keras.constraints.constraints import MinMaxNorm\n+from keras.constraints.constraints import NonNeg\n+from keras.constraints.constraints import UnitNorm\n+from keras.saving import serialization_lib\n+from keras.utils.naming import to_snake_case\n \n ALL_OBJECTS = {\n     Constraint,\n@@ -23,12 +23,12 @@ ALL_OBJECTS_DICT.update(\n )\n \n \n-@keras_core_export(\"keras_core.constraints.serialize\")\n+@keras_export(\"keras.constraints.serialize\")\n def serialize(constraint):\n     return serialization_lib.serialize_keras_object(constraint)\n \n \n-@keras_core_export(\"keras_core.constraints.deserialize\")\n+@keras_export(\"keras.constraints.deserialize\")\n def deserialize(config, custom_objects=None):\n     \"\"\"Return a Keras constraint object via its config.\"\"\"\n     return serialization_lib.deserialize_keras_object(\n@@ -38,7 +38,7 @@ def deserialize(config, custom_objects=None):\n     )\n \n \n-@keras_core_export(\"keras_core.constraints.get\")\n+@keras_export(\"keras.constraints.get\")\n def get(identifier):\n     \"\"\"Retrieve a Keras constraint object via an identifier.\"\"\"\n     if identifier is None:\n\n@@ -1,9 +1,9 @@\n-from keras_core import backend\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n+from keras import backend\n+from keras import ops\n+from keras.api_export import keras_export\n \n \n-@keras_core_export(\"keras_core.constraints.Constraint\")\n+@keras_export(\"keras.constraints.Constraint\")\n class Constraint:\n     \"\"\"Base class for weight constraints.\n \n@@ -16,7 +16,7 @@ class Constraint:\n \n     Here's a simple example of a non-negative weight constraint:\n \n-    >>> class NonNegative(keras_core.constraints.Constraint):\n+    >>> class NonNegative(keras.constraints.Constraint):\n     ...\n     ...  def __call__(self, w):\n     ...    return w * ops.cast(ops.greater_equal(w, 0.), dtype=w.dtype)\n@@ -27,7 +27,7 @@ class Constraint:\n \n     Usage in a layer:\n \n-    >>> keras_core.layers.Dense(4, kernel_constraint=NonNegative())\n+    >>> keras.layers.Dense(4, kernel_constraint=NonNegative())\n     \"\"\"\n \n     def __call__(self, w):\n@@ -72,13 +72,13 @@ class Constraint:\n             config: A Python dictionary, the output of `get_config()`.\n \n         Returns:\n-            A `keras_core.constraints.Constraint` instance.\n+            A `keras.constraints.Constraint` instance.\n         \"\"\"\n         return cls(**config)\n \n \n-@keras_core_export(\n-    [\"keras_core.constraints.MaxNorm\", \"keras_core.constraints.max_norm\"]\n+@keras_export(\n+    [\"keras.constraints.MaxNorm\", \"keras.constraints.max_norm\"]\n )\n class MaxNorm(Constraint):\n     \"\"\"MaxNorm weight constraint.\n@@ -86,7 +86,7 @@ class MaxNorm(Constraint):\n     Constrains the weights incident to each hidden unit\n     to have a norm less than or equal to a desired value.\n \n-    Also available via the shortcut function `keras_core.constraints.max_norm`.\n+    Also available via the shortcut function `keras.constraints.max_norm`.\n \n     Args:\n         max_value: the maximum norm value for the incoming weights.\n@@ -118,8 +118,8 @@ class MaxNorm(Constraint):\n         return {\"max_value\": self.max_value, \"axis\": self.axis}\n \n \n-@keras_core_export(\n-    [\"keras_core.constraints.NonNeg\", \"keras_core.constraints.non_neg\"]\n+@keras_export(\n+    [\"keras.constraints.NonNeg\", \"keras.constraints.non_neg\"]\n )\n class NonNeg(Constraint):\n     \"\"\"Constrains the weights to be non-negative.\"\"\"\n@@ -129,8 +129,8 @@ class NonNeg(Constraint):\n         return w * ops.cast(ops.greater_equal(w, 0.0), dtype=w.dtype)\n \n \n-@keras_core_export(\n-    [\"keras_core.constraints.UnitNorm\", \"keras_core.constraints.unit_norm\"]\n+@keras_export(\n+    [\"keras.constraints.UnitNorm\", \"keras.constraints.unit_norm\"]\n )\n class UnitNorm(Constraint):\n     \"\"\"Constrains the weights incident to each hidden unit to have unit norm.\n@@ -163,8 +163,8 @@ class UnitNorm(Constraint):\n         return {\"axis\": self.axis}\n \n \n-@keras_core_export(\n-    [\"keras_core.constraints.MinMaxNorm\", \"keras_core.constraints.min_max_norm\"]\n+@keras_export(\n+    [\"keras.constraints.MinMaxNorm\", \"keras.constraints.min_max_norm\"]\n )\n class MinMaxNorm(Constraint):\n     \"\"\"MinMaxNorm weight constraint.\n\n@@ -1,8 +1,8 @@\n import numpy as np\n \n-from keras_core import backend\n-from keras_core import constraints\n-from keras_core import testing\n+from keras import backend\n+from keras import constraints\n+from keras import testing\n \n \n def get_example_array():\n\n@@ -0,0 +1,10 @@\n+\"\"\"Small NumPy datasets for debugging/testing.\"\"\"\n+\n+from keras.datasets import boston_housing\n+from keras.datasets import california_housing\n+from keras.datasets import cifar10\n+from keras.datasets import cifar100\n+from keras.datasets import fashion_mnist\n+from keras.datasets import imdb\n+from keras.datasets import mnist\n+from keras.datasets import reuters\n\n@@ -1,11 +1,11 @@\n-from keras_core.api_export import keras_core_export\n+from keras.api_export import keras_export\n \n \n-@keras_core_export(\"keras_core.datasets.boston_housing.load_data\")\n+@keras_export(\"keras.datasets.boston_housing.load_data\")\n def load_data(path=\"boston_housing.npz\", test_split=0.2, seed=113):\n     raise NotImplementedError(\n         \"The Boston Housing dataset is no longer distributed with Keras. \"\n         \"We recommend that you use instead the \"\n         \"California Housing dataset, available via \"\n-        \"`keras_core.datasets.california_housing.load_data()`.\"\n+        \"`keras.datasets.california_housing.load_data()`.\"\n     )\n\n@@ -2,11 +2,11 @@\n \n import numpy as np\n \n-from keras_core.api_export import keras_core_export\n-from keras_core.utils.file_utils import get_file\n+from keras.api_export import keras_export\n+from keras.utils.file_utils import get_file\n \n \n-@keras_core_export(\"keras_core.datasets.boston_housing.load_data\")\n+@keras_export(\"keras.datasets.boston_housing.load_data\")\n def load_data(path=\"california_housing.npz\", test_split=0.2, seed=113):\n     \"\"\"Loads the California Housing dataset.\n \n\n\n@@ -4,13 +4,13 @@ import os\n \n import numpy as np\n \n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.datasets.cifar import load_batch\n-from keras_core.utils.file_utils import get_file\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.datasets.cifar import load_batch\n+from keras.utils.file_utils import get_file\n \n \n-@keras_core_export(\"keras_core.datasets.cifar10.load_data\")\n+@keras_export(\"keras.datasets.cifar10.load_data\")\n def load_data():\n     \"\"\"Loads the CIFAR10 dataset.\n \n\n@@ -4,13 +4,13 @@ import os\n \n import numpy as np\n \n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.datasets.cifar import load_batch\n-from keras_core.utils.file_utils import get_file\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.datasets.cifar import load_batch\n+from keras.utils.file_utils import get_file\n \n \n-@keras_core_export(\"keras_core.datasets.cifar100.load_data\")\n+@keras_export(\"keras.datasets.cifar100.load_data\")\n def load_data(label_mode=\"fine\"):\n     \"\"\"Loads the CIFAR100 dataset.\n \n\n@@ -5,11 +5,11 @@ import os\n \n import numpy as np\n \n-from keras_core.api_export import keras_core_export\n-from keras_core.utils.file_utils import get_file\n+from keras.api_export import keras_export\n+from keras.utils.file_utils import get_file\n \n \n-@keras_core_export(\"keras_core.datasets.fashion_mnist.load_data\")\n+@keras_export(\"keras.datasets.fashion_mnist.load_data\")\n def load_data():\n     \"\"\"Loads the Fashion-MNIST dataset.\n \n\n@@ -4,12 +4,12 @@ import json\n \n import numpy as np\n \n-from keras_core.api_export import keras_core_export\n-from keras_core.utils.file_utils import get_file\n-from keras_core.utils.python_utils import remove_long_seq\n+from keras.api_export import keras_export\n+from keras.utils.file_utils import get_file\n+from keras.utils.python_utils import remove_long_seq\n \n \n-@keras_core_export(\"keras_core.datasets.imdb.load_data\")\n+@keras_export(\"keras.datasets.imdb.load_data\")\n def load_data(\n     path=\"imdb.npz\",\n     num_words=None,\n@@ -137,7 +137,7 @@ def load_data(\n     return (x_train, y_train), (x_test, y_test)\n \n \n-@keras_core_export(\"keras_core.datasets.imdb.get_word_index\")\n+@keras_export(\"keras.datasets.imdb.get_word_index\")\n def get_word_index(path=\"imdb_word_index.json\"):\n     \"\"\"Retrieves a dict mapping words to their index in the IMDB dataset.\n \n\n@@ -2,11 +2,11 @@\n \n import numpy as np\n \n-from keras_core.api_export import keras_core_export\n-from keras_core.utils.file_utils import get_file\n+from keras.api_export import keras_export\n+from keras.utils.file_utils import get_file\n \n \n-@keras_core_export(\"keras_core.datasets.mnist.load_data\")\n+@keras_export(\"keras.datasets.mnist.load_data\")\n def load_data(path=\"mnist.npz\"):\n     \"\"\"Loads the MNIST dataset.\n \n\n@@ -4,12 +4,12 @@ import json\n \n import numpy as np\n \n-from keras_core.api_export import keras_core_export\n-from keras_core.utils.file_utils import get_file\n-from keras_core.utils.python_utils import remove_long_seq\n+from keras.api_export import keras_export\n+from keras.utils.file_utils import get_file\n+from keras.utils.python_utils import remove_long_seq\n \n \n-@keras_core_export(\"keras_core.datasets.reuters.load_data\")\n+@keras_export(\"keras.datasets.reuters.load_data\")\n def load_data(\n     path=\"reuters.npz\",\n     num_words=None,\n@@ -132,7 +132,7 @@ def load_data(\n     return (x_train, y_train), (x_test, y_test)\n \n \n-@keras_core_export(\"keras_core.datasets.reuters.get_word_index\")\n+@keras_export(\"keras.datasets.reuters.get_word_index\")\n def get_word_index(path=\"reuters_word_index.json\"):\n     \"\"\"Retrieves a dict mapping words to their index in the Reuters dataset.\n \n@@ -162,7 +162,7 @@ def get_word_index(path=\"reuters_word_index.json\"):\n         return json.load(f)\n \n \n-@keras_core_export(\"keras_core.datasets.reuters.get_label_names\")\n+@keras_export(\"keras.datasets.reuters.get_label_names\")\n def get_label_names():\n     \"\"\"Returns labels as a list of strings with indices matching training data.\n \n\n\n@@ -13,15 +13,15 @@ import warnings\n \n import numpy as np\n \n-from keras_core.api_export import keras_core_export\n-from keras_core.backend import distribution_lib\n-from keras_core.backend.common import global_state\n+from keras.api_export import keras_export\n+from keras.backend import distribution_lib\n+from keras.backend.common import global_state\n \n DEFAULT_BATCH_DIM_NAME = \"batch\"\n GLOBAL_ATTRIBUTE_NAME = \"distribution\"\n \n \n-@keras_core_export(\"keras_core.distribution.list_devices\")\n+@keras_export(\"keras.distribution.list_devices\")\n def list_devices(device_type=None):\n     \"\"\"Return all the available devices based on the device type.\n \n@@ -39,7 +39,7 @@ def list_devices(device_type=None):\n     return distribution_lib.list_devices(device_type)\n \n \n-@keras_core_export(\"keras_core.distribution.DeviceMesh\")\n+@keras_export(\"keras.distribution.DeviceMesh\")\n class DeviceMesh:\n     \"\"\"A cluster of computation devices for distributed computation.\n \n@@ -61,7 +61,7 @@ class DeviceMesh:\n             match/create the `TensorLayout` when distribute the data and\n             variables.\n         devices: Optional list of devices. Defaults to all the available\n-            devices locally from `keras_core.distribution.list_devices()`.\n+            devices locally from `keras.distribution.list_devices()`.\n     \"\"\"\n \n     def __init__(\n@@ -108,7 +108,7 @@ class DeviceMesh:\n         return self._devices\n \n \n-@keras_core_export(\"keras_core.distribution.TensorLayout\")\n+@keras_export(\"keras.distribution.TensorLayout\")\n class TensorLayout:\n     \"\"\"A layout to apply to a tensor.\n \n@@ -220,7 +220,7 @@ class Distribution:\n         return self._device_mesh\n \n \n-@keras_core_export(\"keras_core.distribution.DataParallel\")\n+@keras_export(\"keras.distribution.DataParallel\")\n class DataParallel(Distribution):\n     \"\"\"Distribution for data parallelism.\n \n@@ -297,7 +297,7 @@ class DataParallel(Distribution):\n         return TensorLayout(variable_shard_spec, self.device_mesh)\n \n \n-@keras_core_export(\"keras_core.distribution.ModelParallel\")\n+@keras_export(\"keras.distribution.ModelParallel\")\n class ModelParallel(Distribution):\n     \"\"\"Distribution that shards model variables.\n \n@@ -394,7 +394,7 @@ class ModelParallel(Distribution):\n         return TensorLayout(variable_shard_spec, self.device_mesh)\n \n \n-@keras_core_export(\"keras_core.distribution.LayoutMap\")\n+@keras_export(\"keras.distribution.LayoutMap\")\n class LayoutMap(collections.abc.MutableMapping):\n     \"\"\"A dict-like object that maps string to `TensorLayout` instances.\n \n@@ -491,13 +491,13 @@ class LayoutMap(collections.abc.MutableMapping):\n             layout.device_mesh = self.device_mesh\n \n \n-@keras_core_export(\"keras_core.distribution.distribution\")\n+@keras_export(\"keras.distribution.distribution\")\n def distribution():\n     \"\"\"Retrieve the current distribution from global context.\"\"\"\n     return global_state.get_global_attribute(GLOBAL_ATTRIBUTE_NAME)\n \n \n-@keras_core_export(\"keras_core.distribution.set_distribution\")\n+@keras_export(\"keras.distribution.set_distribution\")\n def set_distribution(value):\n     \"\"\"Set the distribution as the global distribution setting.\n \n\n@@ -7,12 +7,12 @@ import jax\n import numpy as np\n import pytest\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import models\n-from keras_core import testing\n-from keras_core.backend import distribution_lib as backend_dlib\n-from keras_core.distribution import distribution_lib\n+from keras import backend\n+from keras import layers\n+from keras import models\n+from keras import testing\n+from keras.backend import distribution_lib as backend_dlib\n+from keras.distribution import distribution_lib\n \n if backend.backend() == \"jax\":\n     # Due to https://github.com/google/jax/issues/17188, we can't\n\n@@ -0,0 +1 @@\n+from keras.export.export_lib import ExportArchive\n\n@@ -1,15 +1,15 @@\n \"\"\"Library for exporting inference-only Keras models/layers.\"\"\"\n \n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers import Layer\n-from keras_core.models import Functional\n-from keras_core.models import Sequential\n-from keras_core.utils import io_utils\n-from keras_core.utils.module_utils import tensorflow as tf\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.layers import Layer\n+from keras.models import Functional\n+from keras.models import Sequential\n+from keras.utils import io_utils\n+from keras.utils.module_utils import tensorflow as tf\n \n \n-@keras_core_export(\"keras_core.export.ExportArchive\")\n+@keras_export(\"keras.export.ExportArchive\")\n class ExportArchive:\n     \"\"\"ExportArchive is used to write SavedModel artifacts (e.g. for inference).\n \n@@ -215,7 +215,7 @@ class ExportArchive:\n         a Functional model with 2 inputs):\n \n         ```python\n-        model = keras_core.Model(inputs=[x1, x2], outputs=outputs)\n+        model = keras.Model(inputs=[x1, x2], outputs=outputs)\n \n         export_archive = ExportArchive()\n         export_archive.track(model)\n@@ -234,7 +234,7 @@ class ExportArchive:\n         This also works with dictionary inputs:\n \n         ```python\n-        model = keras_core.Model(inputs={\"x1\": x1, \"x2\": x2}, outputs=outputs)\n+        model = keras.Model(inputs={\"x1\": x1, \"x2\": x2}, outputs=outputs)\n \n         export_archive = ExportArchive()\n         export_archive.track(model)\n@@ -345,7 +345,7 @@ class ExportArchive:\n                 f\"Received instead object of type '{type(variables)}'.\"\n             )\n         # Ensure that all variables added are either tf.Variables\n-        # or Variables created by Keras Core with the TF or JAX backends.\n+        # or Variables created by Keras 3 with the TF or JAX backends.\n         if not all(\n             isinstance(v, (tf.Variable, backend.Variable)) for v in variables\n         ):\n@@ -434,9 +434,9 @@ class ExportArchive:\n         # Next, track lookup tables.\n         # Hopefully, one day this will be automated at the tf.function level.\n         self._tf_trackable._misc_assets = []\n-        from keras_core.layers import IntegerLookup\n-        from keras_core.layers import StringLookup\n-        from keras_core.layers import TextVectorization\n+        from keras.layers import IntegerLookup\n+        from keras.layers import StringLookup\n+        from keras.layers import TextVectorization\n \n         if hasattr(self, \"_tracked\"):\n             for root in self._tracked:\n@@ -501,7 +501,7 @@ def _get_save_spec(model):\n     return specs\n \n \n-@keras_core_export(\"keras_core.layers.TFSMLayer\")\n+@keras_export(\"keras.layers.TFSMLayer\")\n class TFSMLayer(Layer):\n     \"\"\"Reload a Keras model/layer that was saved via SavedModel / ExportArchive.\n \n\n@@ -6,13 +6,13 @@ import numpy as np\n import pytest\n import tensorflow as tf\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import models\n-from keras_core import testing\n-from keras_core import utils\n-from keras_core.export import export_lib\n-from keras_core.saving import saving_lib\n+from keras import backend\n+from keras import layers\n+from keras import models\n+from keras import testing\n+from keras import utils\n+from keras.export import export_lib\n+from keras.saving import saving_lib\n \n \n def get_model():\n\n@@ -1,24 +1,24 @@\n import inspect\n \n-from keras_core.api_export import keras_core_export\n-from keras_core.initializers.constant_initializers import Constant\n-from keras_core.initializers.constant_initializers import Identity\n-from keras_core.initializers.constant_initializers import Ones\n-from keras_core.initializers.constant_initializers import Zeros\n-from keras_core.initializers.initializer import Initializer\n-from keras_core.initializers.random_initializers import GlorotNormal\n-from keras_core.initializers.random_initializers import GlorotUniform\n-from keras_core.initializers.random_initializers import HeNormal\n-from keras_core.initializers.random_initializers import HeUniform\n-from keras_core.initializers.random_initializers import LecunNormal\n-from keras_core.initializers.random_initializers import LecunUniform\n-from keras_core.initializers.random_initializers import OrthogonalInitializer\n-from keras_core.initializers.random_initializers import RandomNormal\n-from keras_core.initializers.random_initializers import RandomUniform\n-from keras_core.initializers.random_initializers import TruncatedNormal\n-from keras_core.initializers.random_initializers import VarianceScaling\n-from keras_core.saving import serialization_lib\n-from keras_core.utils.naming import to_snake_case\n+from keras.api_export import keras_export\n+from keras.initializers.constant_initializers import Constant\n+from keras.initializers.constant_initializers import Identity\n+from keras.initializers.constant_initializers import Ones\n+from keras.initializers.constant_initializers import Zeros\n+from keras.initializers.initializer import Initializer\n+from keras.initializers.random_initializers import GlorotNormal\n+from keras.initializers.random_initializers import GlorotUniform\n+from keras.initializers.random_initializers import HeNormal\n+from keras.initializers.random_initializers import HeUniform\n+from keras.initializers.random_initializers import LecunNormal\n+from keras.initializers.random_initializers import LecunUniform\n+from keras.initializers.random_initializers import OrthogonalInitializer\n+from keras.initializers.random_initializers import RandomNormal\n+from keras.initializers.random_initializers import RandomUniform\n+from keras.initializers.random_initializers import TruncatedNormal\n+from keras.initializers.random_initializers import VarianceScaling\n+from keras.saving import serialization_lib\n+from keras.utils.naming import to_snake_case\n \n ALL_OBJECTS = {\n     Initializer,\n@@ -54,13 +54,13 @@ ALL_OBJECTS_DICT.update(\n )\n \n \n-@keras_core_export(\"keras_core.initializers.serialize\")\n+@keras_export(\"keras.initializers.serialize\")\n def serialize(initializer):\n     \"\"\"Returns the initializer configuration as a Python dict.\"\"\"\n     return serialization_lib.serialize_keras_object(initializer)\n \n \n-@keras_core_export(\"keras_core.initializers.deserialize\")\n+@keras_export(\"keras.initializers.deserialize\")\n def deserialize(config, custom_objects=None):\n     \"\"\"Returns a Keras initializer object via its configuration.\"\"\"\n     return serialization_lib.deserialize_keras_object(\n@@ -70,7 +70,7 @@ def deserialize(config, custom_objects=None):\n     )\n \n \n-@keras_core_export(\"keras_core.initializers.get\")\n+@keras_export(\"keras.initializers.get\")\n def get(identifier):\n     \"\"\"Retrieves a Keras initializer object via an identifier.\n \n@@ -78,16 +78,16 @@ def get(identifier):\n     (case-sensitively).\n \n     >>> identifier = 'Ones'\n-    >>> keras_core.initializers.deserialize(identifier)\n-    <...keras_core.initializers.initializers.Ones...>\n+    >>> keras.initializers.deserialize(identifier)\n+    <...keras.initializers.initializers.Ones...>\n \n     You can also specify `config` of the initializer to this function by passing\n     dict containing `class_name` and `config` as an identifier. Also note that\n     the `class_name` must map to a `Initializer` class.\n \n     >>> cfg = {'class_name': 'Ones', 'config': {}}\n-    >>> keras_core.initializers.deserialize(cfg)\n-    <...keras_core.initializers.initializers.Ones...>\n+    >>> keras.initializers.deserialize(cfg)\n+    <...keras.initializers.initializers.Ones...>\n \n     In the case that the `identifier` is a class, this method will return a new\n     instance of the class by its constructor.\n\n@@ -1,11 +1,11 @@\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.backend import standardize_dtype\n-from keras_core.initializers.initializer import Initializer\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.backend import standardize_dtype\n+from keras.initializers.initializer import Initializer\n \n \n-@keras_core_export(\n-    [\"keras_core.initializers.Constant\", \"keras_core.initializers.constant\"]\n+@keras_export(\n+    [\"keras.initializers.Constant\", \"keras.initializers.constant\"]\n )\n class Constant(Initializer):\n     \"\"\"Initializer that generates tensors with constant values.\n@@ -41,8 +41,8 @@ class Constant(Initializer):\n         return {\"value\": self.value}\n \n \n-@keras_core_export(\n-    [\"keras_core.initializers.Zeros\", \"keras_core.initializers.zeros\"]\n+@keras_export(\n+    [\"keras.initializers.Zeros\", \"keras.initializers.zeros\"]\n )\n class Zeros(Initializer):\n     \"\"\"Initializer that generates tensors initialized to 0.\n@@ -64,16 +64,16 @@ class Zeros(Initializer):\n         Args:\n             shape: Shape of the tensor.\n             dtype: Optional dtype of the tensor. Only numeric or boolean dtypes\n-                are supported. If not specified, `keras_core.backend.floatx()`\n+                are supported. If not specified, `keras.backend.floatx()`\n                 is used, which default to `float32` unless you configured it\n-                otherwise (via `keras_core.backend.set_floatx(float_dtype)`).\n+                otherwise (via `keras.backend.set_floatx(float_dtype)`).\n         \"\"\"\n         dtype = standardize_dtype(dtype)\n         return ops.zeros(shape, dtype=dtype)\n \n \n-@keras_core_export(\n-    [\"keras_core.initializers.Ones\", \"keras_core.initializers.ones\"]\n+@keras_export(\n+    [\"keras.initializers.Ones\", \"keras.initializers.ones\"]\n )\n class Ones(Initializer):\n     \"\"\"Initializer that generates tensors initialized to 1.\n@@ -97,19 +97,19 @@ class Ones(Initializer):\n         Args:\n             shape: Shape of the tensor.\n             dtype: Optional dtype of the tensor. Only numeric or boolean dtypes\n-                are supported. If not specified, `keras_core.backend.floatx()`\n+                are supported. If not specified, `keras.backend.floatx()`\n                 is used, which default to `float32` unless you configured it\n-                otherwise (via `keras_core.backend.set_floatx(float_dtype)`).\n+                otherwise (via `keras.backend.set_floatx(float_dtype)`).\n         \"\"\"\n         dtype = standardize_dtype(dtype)\n         return ops.ones(shape, dtype=dtype)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.initializers.IdentityInitializer\",\n-        \"keras_core.initializers.Identity\",\n-        \"keras_core.initializers.identity\",\n+        \"keras.initializers.IdentityInitializer\",\n+        \"keras.initializers.Identity\",\n+        \"keras.initializers.identity\",\n     ]\n )\n class Identity(Initializer):\n@@ -140,9 +140,9 @@ class Identity(Initializer):\n         Args:\n             shape: Shape of the tensor.\n             dtype: Optional dtype of the tensor. Only numeric or boolean dtypes\n-                are supported. If not specified, `keras_core.backend.floatx()`\n+                are supported. If not specified, `keras.backend.floatx()`\n                 is used, which default to `float32` unless you configured it\n-                otherwise (via `keras_core.backend.set_floatx(float_dtype)`).\n+                otherwise (via `keras.backend.set_floatx(float_dtype)`).\n         \"\"\"\n         if len(shape) != 2:\n             raise ValueError(\n\n@@ -1,8 +1,8 @@\n import numpy as np\n \n-from keras_core import backend\n-from keras_core import initializers\n-from keras_core import testing\n+from keras import backend\n+from keras import initializers\n+from keras import testing\n \n \n class ConstantInitializersTest(testing.TestCase):\n\n@@ -1,8 +1,8 @@\n-from keras_core.api_export import keras_core_export\n+from keras.api_export import keras_export\n \n \n-@keras_core_export(\n-    [\"keras_core.Initializer\", \"keras_core.initializers.Initializer\"]\n+@keras_export(\n+    [\"keras.Initializer\", \"keras.initializers.Initializer\"]\n )\n class Initializer:\n     \"\"\"Initializer base class: all Keras initializers inherit from this class.\n@@ -29,7 +29,7 @@ class Initializer:\n             self.stddev = stddev\n \n         def __call__(self, shape, dtype=None, **kwargs):\n-            return keras_core.random.normal(\n+            return keras.random.normal(\n                 shape, mean=self.mean, stddev=self.stddev, dtype=dtype\n             )\n \n\n@@ -1,16 +1,16 @@\n import math\n \n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.backend import random\n-from keras_core.initializers.initializer import Initializer\n-from keras_core.saving import serialization_lib\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.backend import random\n+from keras.initializers.initializer import Initializer\n+from keras.saving import serialization_lib\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.initializers.RandomNormal\",\n-        \"keras_core.initializers.random_normal\",\n+        \"keras.initializers.RandomNormal\",\n+        \"keras.initializers.random_normal\",\n     ]\n )\n class RandomNormal(Initializer):\n@@ -34,13 +34,13 @@ class RandomNormal(Initializer):\n         stddev: A python scalar or a scalar keras tensor. Standard deviation of\n            the random values to generate.\n         seed: A Python integer or instance of\n-            `keras_core.backend.SeedGenerator`.\n+            `keras.backend.SeedGenerator`.\n             Used to make the behavior of the initializer\n             deterministic. Note that an initializer seeded with an integer\n             or `None` (unseeded) will produce the same random values\n             across multiple calls. To get different random values\n             across multiple calls, use as seed an instance\n-            of `keras_core.backend.SeedGenerator`.\n+            of `keras.backend.SeedGenerator`.\n     \"\"\"\n \n     def __init__(self, mean=0.0, stddev=0.05, seed=None):\n@@ -64,10 +64,10 @@ class RandomNormal(Initializer):\n         return {\"mean\": self.mean, \"stddev\": self.stddev, \"seed\": seed_config}\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.initializers.TruncatedNormal\",\n-        \"keras_core.initializers.truncated_normal\",\n+        \"keras.initializers.TruncatedNormal\",\n+        \"keras.initializers.truncated_normal\",\n     ]\n )\n class TruncatedNormal(Initializer):\n@@ -94,13 +94,13 @@ class TruncatedNormal(Initializer):\n         stddev: A python scalar or a scalar keras tensor. Standard deviation of\n            the random values to generate.\n         seed: A Python integer or instance of\n-            `keras_core.backend.SeedGenerator`.\n+            `keras.backend.SeedGenerator`.\n             Used to make the behavior of the initializer\n             deterministic. Note that an initializer seeded with an integer\n             or `None` (unseeded) will produce the same random values\n             across multiple calls. To get different random values\n             across multiple calls, use as seed an instance\n-            of `keras_core.backend.SeedGenerator`.\n+            of `keras.backend.SeedGenerator`.\n     \"\"\"\n \n     def __init__(self, mean=0.0, stddev=0.05, seed=None):\n@@ -124,10 +124,10 @@ class TruncatedNormal(Initializer):\n         return {\"mean\": self.mean, \"stddev\": self.stddev, \"seed\": seed_config}\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.initializers.RandomUniform\",\n-        \"keras_core.initializers.random_uniform\",\n+        \"keras.initializers.RandomUniform\",\n+        \"keras.initializers.random_uniform\",\n     ]\n )\n class RandomUniform(Initializer):\n@@ -151,13 +151,13 @@ class RandomUniform(Initializer):\n         maxval: A python scalar or a scalar keras tensor. Upper bound of the\n             range of random values to generate (exclusive).\n         seed: A Python integer or instance of\n-            `keras_core.backend.SeedGenerator`.\n+            `keras.backend.SeedGenerator`.\n             Used to make the behavior of the initializer\n             deterministic. Note that an initializer seeded with an integer\n             or `None` (unseeded) will produce the same random values\n             across multiple calls. To get different random values\n             across multiple calls, use as seed an instance\n-            of `keras_core.backend.SeedGenerator`.\n+            of `keras.backend.SeedGenerator`.\n     \"\"\"\n \n     def __init__(self, minval=-0.05, maxval=0.05, seed=None):\n@@ -185,10 +185,10 @@ class RandomUniform(Initializer):\n         }\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.initializers.VarianceScaling\",\n-        \"keras_core.initializers.variance_scaling\",\n+        \"keras.initializers.VarianceScaling\",\n+        \"keras.initializers.variance_scaling\",\n     ]\n )\n class VarianceScaling(Initializer):\n@@ -224,13 +224,13 @@ class VarianceScaling(Initializer):\n         distribution: Random distribution to use.\n             One of `\"truncated_normal\"`, `\"untruncated_normal\"`, or `\"uniform\"`.\n         seed: A Python integer or instance of\n-            `keras_core.backend.SeedGenerator`.\n+            `keras.backend.SeedGenerator`.\n             Used to make the behavior of the initializer\n             deterministic. Note that an initializer seeded with an integer\n             or `None` (unseeded) will produce the same random values\n             across multiple calls. To get different random values\n             across multiple calls, use as seed an instance\n-            of `keras_core.backend.SeedGenerator`.\n+            of `keras.backend.SeedGenerator`.\n     \"\"\"\n \n     def __init__(\n@@ -305,10 +305,10 @@ class VarianceScaling(Initializer):\n         }\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.initializers.GlorotUniform\",\n-        \"keras_core.initializers.glorot_uniform\",\n+        \"keras.initializers.GlorotUniform\",\n+        \"keras.initializers.glorot_uniform\",\n     ]\n )\n class GlorotUniform(VarianceScaling):\n@@ -330,13 +330,13 @@ class GlorotUniform(VarianceScaling):\n \n     Args:\n         seed: A Python integer or instance of\n-            `keras_core.backend.SeedGenerator`.\n+            `keras.backend.SeedGenerator`.\n             Used to make the behavior of the initializer\n             deterministic. Note that an initializer seeded with an integer\n             or `None` (unseeded) will produce the same random values\n             across multiple calls. To get different random values\n             across multiple calls, use as seed an instance\n-            of `keras_core.backend.SeedGenerator`.\n+            of `keras.backend.SeedGenerator`.\n \n     Reference:\n \n@@ -354,10 +354,10 @@ class GlorotUniform(VarianceScaling):\n         }\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.initializers.GlorotNormal\",\n-        \"keras_core.initializers.glorot_normal\",\n+        \"keras.initializers.GlorotNormal\",\n+        \"keras.initializers.glorot_normal\",\n     ]\n )\n class GlorotNormal(VarianceScaling):\n@@ -380,13 +380,13 @@ class GlorotNormal(VarianceScaling):\n \n     Args:\n         seed: A Python integer or instance of\n-            `keras_core.backend.SeedGenerator`.\n+            `keras.backend.SeedGenerator`.\n             Used to make the behavior of the initializer\n             deterministic. Note that an initializer seeded with an integer\n             or `None` (unseeded) will produce the same random values\n             across multiple calls. To get different random values\n             across multiple calls, use as seed an instance\n-            of `keras_core.backend.SeedGenerator`.\n+            of `keras.backend.SeedGenerator`.\n \n     Reference:\n \n@@ -407,10 +407,10 @@ class GlorotNormal(VarianceScaling):\n         }\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.initializers.LecunNormal\",\n-        \"keras_core.initializers.lecun_normal\",\n+        \"keras.initializers.LecunNormal\",\n+        \"keras.initializers.lecun_normal\",\n     ]\n )\n class LecunNormal(VarianceScaling):\n@@ -436,13 +436,13 @@ class LecunNormal(VarianceScaling):\n \n     Args:\n         seed: A Python integer or instance of\n-            `keras_core.backend.SeedGenerator`.\n+            `keras.backend.SeedGenerator`.\n             Used to make the behavior of the initializer\n             deterministic. Note that an initializer seeded with an integer\n             or `None` (unseeded) will produce the same random values\n             across multiple calls. To get different random values\n             across multiple calls, use as seed an instance\n-            of `keras_core.backend.SeedGenerator`.\n+            of `keras.backend.SeedGenerator`.\n \n     Reference:\n \n@@ -460,10 +460,10 @@ class LecunNormal(VarianceScaling):\n         }\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.initializers.LecunUniform\",\n-        \"keras_core.initializers.lecun_uniform\",\n+        \"keras.initializers.LecunUniform\",\n+        \"keras.initializers.lecun_uniform\",\n     ]\n )\n class LecunUniform(VarianceScaling):\n@@ -485,13 +485,13 @@ class LecunUniform(VarianceScaling):\n \n     Args:\n         seed: A Python integer or instance of\n-            `keras_core.backend.SeedGenerator`.\n+            `keras.backend.SeedGenerator`.\n             Used to make the behavior of the initializer\n             deterministic. Note that an initializer seeded with an integer\n             or `None` (unseeded) will produce the same random values\n             across multiple calls. To get different random values\n             across multiple calls, use as seed an instance\n-            of `keras_core.backend.SeedGenerator`.\n+            of `keras.backend.SeedGenerator`.\n \n     Reference:\n \n@@ -509,8 +509,8 @@ class LecunUniform(VarianceScaling):\n         }\n \n \n-@keras_core_export(\n-    [\"keras_core.initializers.HeNormal\", \"keras_core.initializers.he_normal\"]\n+@keras_export(\n+    [\"keras.initializers.HeNormal\", \"keras.initializers.he_normal\"]\n )\n class HeNormal(VarianceScaling):\n     \"\"\"He normal initializer.\n@@ -531,13 +531,13 @@ class HeNormal(VarianceScaling):\n \n     Args:\n         seed: A Python integer or instance of\n-            `keras_core.backend.SeedGenerator`.\n+            `keras.backend.SeedGenerator`.\n             Used to make the behavior of the initializer\n             deterministic. Note that an initializer seeded with an integer\n             or `None` (unseeded) will produce the same random values\n             across multiple calls. To get different random values\n             across multiple calls, use as seed an instance\n-            of `keras_core.backend.SeedGenerator`.\n+            of `keras.backend.SeedGenerator`.\n \n     Reference:\n \n@@ -555,8 +555,8 @@ class HeNormal(VarianceScaling):\n         }\n \n \n-@keras_core_export(\n-    [\"keras_core.initializers.HeUniform\", \"keras_core.initializers.he_uniform\"]\n+@keras_export(\n+    [\"keras.initializers.HeUniform\", \"keras.initializers.he_uniform\"]\n )\n class HeUniform(VarianceScaling):\n     \"\"\"He uniform variance scaling initializer.\n@@ -577,13 +577,13 @@ class HeUniform(VarianceScaling):\n \n     Args:\n         seed: A Python integer or instance of\n-            `keras_core.backend.SeedGenerator`.\n+            `keras.backend.SeedGenerator`.\n             Used to make the behavior of the initializer\n             deterministic. Note that an initializer seeded with an integer\n             or `None` (unseeded) will produce the same random values\n             across multiple calls. To get different random values\n             across multiple calls, use as seed an instance\n-            of `keras_core.backend.SeedGenerator`.\n+            of `keras.backend.SeedGenerator`.\n \n     Reference:\n \n@@ -629,11 +629,11 @@ def compute_fans(shape):\n     return int(fan_in), int(fan_out)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.initializers.OrthogonalInitializer\",\n-        \"keras_core.initializers.Orthogonal\",\n-        \"keras_core.initializers.orthogonal\",\n+        \"keras.initializers.OrthogonalInitializer\",\n+        \"keras.initializers.Orthogonal\",\n+        \"keras.initializers.orthogonal\",\n     ]\n )\n class OrthogonalInitializer(Initializer):\n@@ -653,12 +653,12 @@ class OrthogonalInitializer(Initializer):\n     Examples:\n \n     >>> # Standalone usage:\n-    >>> initializer = keras_core.initializers.Orthogonal()\n+    >>> initializer = keras.initializers.Orthogonal()\n     >>> values = initializer(shape=(2, 2))\n \n     >>> # Usage in a Keras layer:\n-    >>> initializer = keras_core.initializers.Orthogonal()\n-    >>> layer = keras_core.layers.Dense(3, kernel_initializer=initializer)\n+    >>> initializer = keras.initializers.Orthogonal()\n+    >>> layer = keras.layers.Dense(3, kernel_initializer=initializer)\n \n     Args:\n         gain: Multiplicative factor to apply to the orthogonal matrix.\n\n@@ -1,9 +1,9 @@\n import numpy as np\n \n-from keras_core import backend\n-from keras_core import initializers\n-from keras_core import testing\n-from keras_core import utils\n+from keras import backend\n+from keras import initializers\n+from keras import testing\n+from keras import utils\n \n \n class InitializersTest(testing.TestCase):\n\n@@ -0,0 +1,170 @@\n+from keras.api_export import keras_export\n+from keras.layers.activations.activation import Activation\n+from keras.layers.activations.elu import ELU\n+from keras.layers.activations.leaky_relu import LeakyReLU\n+from keras.layers.activations.prelu import PReLU\n+from keras.layers.activations.relu import ReLU\n+from keras.layers.activations.softmax import Softmax\n+from keras.layers.attention.additive_attention import AdditiveAttention\n+from keras.layers.attention.attention import Attention\n+from keras.layers.attention.multi_head_attention import MultiHeadAttention\n+from keras.layers.convolutional.conv1d import Conv1D\n+from keras.layers.convolutional.conv1d_transpose import Conv1DTranspose\n+from keras.layers.convolutional.conv2d import Conv2D\n+from keras.layers.convolutional.conv2d_transpose import Conv2DTranspose\n+from keras.layers.convolutional.conv3d import Conv3D\n+from keras.layers.convolutional.conv3d_transpose import Conv3DTranspose\n+from keras.layers.convolutional.depthwise_conv1d import DepthwiseConv1D\n+from keras.layers.convolutional.depthwise_conv2d import DepthwiseConv2D\n+from keras.layers.convolutional.separable_conv1d import SeparableConv1D\n+from keras.layers.convolutional.separable_conv2d import SeparableConv2D\n+from keras.layers.core.dense import Dense\n+from keras.layers.core.einsum_dense import EinsumDense\n+from keras.layers.core.embedding import Embedding\n+from keras.layers.core.identity import Identity\n+from keras.layers.core.input_layer import Input\n+from keras.layers.core.input_layer import InputLayer\n+from keras.layers.core.lambda_layer import Lambda\n+from keras.layers.core.masking import Masking\n+from keras.layers.core.wrapper import Wrapper\n+from keras.layers.layer import Layer\n+from keras.layers.merging.add import Add\n+from keras.layers.merging.add import add\n+from keras.layers.merging.average import Average\n+from keras.layers.merging.average import average\n+from keras.layers.merging.concatenate import Concatenate\n+from keras.layers.merging.concatenate import concatenate\n+from keras.layers.merging.dot import Dot\n+from keras.layers.merging.dot import dot\n+from keras.layers.merging.maximum import Maximum\n+from keras.layers.merging.maximum import maximum\n+from keras.layers.merging.minimum import Minimum\n+from keras.layers.merging.minimum import minimum\n+from keras.layers.merging.multiply import Multiply\n+from keras.layers.merging.multiply import multiply\n+from keras.layers.merging.subtract import Subtract\n+from keras.layers.merging.subtract import subtract\n+from keras.layers.normalization.batch_normalization import (\n+    BatchNormalization,\n+)\n+from keras.layers.normalization.group_normalization import (\n+    GroupNormalization,\n+)\n+from keras.layers.normalization.layer_normalization import (\n+    LayerNormalization,\n+)\n+from keras.layers.normalization.spectral_normalization import (\n+    SpectralNormalization,\n+)\n+from keras.layers.normalization.unit_normalization import UnitNormalization\n+from keras.layers.pooling.average_pooling1d import AveragePooling1D\n+from keras.layers.pooling.average_pooling2d import AveragePooling2D\n+from keras.layers.pooling.average_pooling3d import AveragePooling3D\n+from keras.layers.pooling.global_average_pooling1d import (\n+    GlobalAveragePooling1D,\n+)\n+from keras.layers.pooling.global_average_pooling2d import (\n+    GlobalAveragePooling2D,\n+)\n+from keras.layers.pooling.global_average_pooling3d import (\n+    GlobalAveragePooling3D,\n+)\n+from keras.layers.pooling.global_max_pooling1d import GlobalMaxPooling1D\n+from keras.layers.pooling.global_max_pooling2d import GlobalMaxPooling2D\n+from keras.layers.pooling.global_max_pooling3d import GlobalMaxPooling3D\n+from keras.layers.pooling.max_pooling1d import MaxPooling1D\n+from keras.layers.pooling.max_pooling2d import MaxPooling2D\n+from keras.layers.pooling.max_pooling3d import MaxPooling3D\n+from keras.layers.preprocessing.category_encoding import CategoryEncoding\n+from keras.layers.preprocessing.center_crop import CenterCrop\n+from keras.layers.preprocessing.discretization import Discretization\n+from keras.layers.preprocessing.hashed_crossing import HashedCrossing\n+from keras.layers.preprocessing.hashing import Hashing\n+from keras.layers.preprocessing.index_lookup import IndexLookup\n+from keras.layers.preprocessing.integer_lookup import IntegerLookup\n+from keras.layers.preprocessing.normalization import Normalization\n+from keras.layers.preprocessing.random_brightness import RandomBrightness\n+from keras.layers.preprocessing.random_contrast import RandomContrast\n+from keras.layers.preprocessing.random_crop import RandomCrop\n+from keras.layers.preprocessing.random_flip import RandomFlip\n+from keras.layers.preprocessing.random_rotation import RandomRotation\n+from keras.layers.preprocessing.random_translation import RandomTranslation\n+from keras.layers.preprocessing.random_zoom import RandomZoom\n+from keras.layers.preprocessing.rescaling import Rescaling\n+from keras.layers.preprocessing.resizing import Resizing\n+from keras.layers.preprocessing.string_lookup import StringLookup\n+from keras.layers.preprocessing.text_vectorization import TextVectorization\n+from keras.layers.regularization.activity_regularization import (\n+    ActivityRegularization,\n+)\n+from keras.layers.regularization.dropout import Dropout\n+from keras.layers.regularization.gaussian_dropout import GaussianDropout\n+from keras.layers.regularization.gaussian_noise import GaussianNoise\n+from keras.layers.regularization.spatial_dropout import SpatialDropout1D\n+from keras.layers.regularization.spatial_dropout import SpatialDropout2D\n+from keras.layers.regularization.spatial_dropout import SpatialDropout3D\n+from keras.layers.reshaping.cropping1d import Cropping1D\n+from keras.layers.reshaping.cropping2d import Cropping2D\n+from keras.layers.reshaping.cropping3d import Cropping3D\n+from keras.layers.reshaping.flatten import Flatten\n+from keras.layers.reshaping.permute import Permute\n+from keras.layers.reshaping.repeat_vector import RepeatVector\n+from keras.layers.reshaping.reshape import Reshape\n+from keras.layers.reshaping.up_sampling1d import UpSampling1D\n+from keras.layers.reshaping.up_sampling2d import UpSampling2D\n+from keras.layers.reshaping.up_sampling3d import UpSampling3D\n+from keras.layers.reshaping.zero_padding1d import ZeroPadding1D\n+from keras.layers.reshaping.zero_padding2d import ZeroPadding2D\n+from keras.layers.reshaping.zero_padding3d import ZeroPadding3D\n+from keras.layers.rnn.bidirectional import Bidirectional\n+from keras.layers.rnn.conv_lstm1d import ConvLSTM1D\n+from keras.layers.rnn.conv_lstm2d import ConvLSTM2D\n+from keras.layers.rnn.conv_lstm3d import ConvLSTM3D\n+from keras.layers.rnn.gru import GRU\n+from keras.layers.rnn.gru import GRUCell\n+from keras.layers.rnn.lstm import LSTM\n+from keras.layers.rnn.lstm import LSTMCell\n+from keras.layers.rnn.rnn import RNN\n+from keras.layers.rnn.simple_rnn import SimpleRNN\n+from keras.layers.rnn.simple_rnn import SimpleRNNCell\n+from keras.layers.rnn.stacked_rnn_cells import StackedRNNCells\n+from keras.layers.rnn.time_distributed import TimeDistributed\n+from keras.saving import serialization_lib\n+\n+\n+@keras_export(\"keras.layers.serialize\")\n+def serialize(layer):\n+    \"\"\"Returns the layer configuration as a Python dict.\n+\n+    Args:\n+        layer: A `keras.layers.Layer` instance to serialize.\n+\n+    Returns:\n+        Python dict which contains the configuration of the layer.\n+    \"\"\"\n+    return serialization_lib.serialize_keras_object(layer)\n+\n+\n+@keras_export(\"keras.layers.deserialize\")\n+def deserialize(config, custom_objects=None):\n+    \"\"\"Returns a Keras layer object via its configuration.\n+\n+    Args:\n+        config: A python dict containing a serialized layer configuration.\n+        custom_objects: Optional dictionary mapping names (strings) to custom\n+            objects (classes and functions) to be considered during\n+            deserialization.\n+\n+    Returns:\n+        A Keras layer instance.\n+    \"\"\"\n+    obj = serialization_lib.deserialize_keras_object(\n+        config,\n+        custom_objects=custom_objects,\n+    )\n+    if not isinstance(obj, Layer):\n+        raise ValueError(\n+            \"`keras.layers.deserialize` was passed a `config` object that is \"\n+            f\"not a `keras.layers.Layer`. Received: {config}\"\n+        )\n+    return obj\n\n@@ -0,0 +1,5 @@\n+from keras.layers.activations.elu import ELU\n+from keras.layers.activations.leaky_relu import LeakyReLU\n+from keras.layers.activations.prelu import PReLU\n+from keras.layers.activations.relu import ReLU\n+from keras.layers.activations.softmax import Softmax\n\n@@ -1,23 +1,23 @@\n-from keras_core import activations\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n+from keras import activations\n+from keras.api_export import keras_export\n+from keras.layers.layer import Layer\n \n \n-@keras_core_export(\"keras_core.layers.Activation\")\n+@keras_export(\"keras.layers.Activation\")\n class Activation(Layer):\n     \"\"\"Applies an activation function to an output.\n \n     Args:\n         activation: Activation function. It could be a callable, or the name of\n-            an activation from the `keras_core.activations` namespace.\n+            an activation from the `keras.activations` namespace.\n         **kwargs: Base layer keyword arguments, such as `name` and `dtype`.\n \n     Example:\n \n-    >>> layer = keras_core.layers.Activation('relu')\n+    >>> layer = keras.layers.Activation('relu')\n     >>> layer([-3.0, -1.0, 0.0, 2.0])\n     [0.0, 0.0, 0.0, 2.0]\n-    >>> layer = keras_core.layers.Activation(keras_core.activations.relu)\n+    >>> layer = keras.layers.Activation(keras.activations.relu)\n     >>> layer([-3.0, -1.0, 0.0, 2.0])\n     [0.0, 0.0, 0.0, 2.0]\n     \"\"\"\n\n@@ -1,8 +1,8 @@\n import pytest\n \n-from keras_core import activations\n-from keras_core import layers\n-from keras_core import testing\n+from keras import activations\n+from keras import layers\n+from keras import testing\n \n \n class ActivationTest(testing.TestCase):\n\n@@ -1,9 +1,9 @@\n-from keras_core import activations\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n+from keras import activations\n+from keras.api_export import keras_export\n+from keras.layers.layer import Layer\n \n \n-@keras_core_export(\"keras_core.layers.ELU\")\n+@keras_export(\"keras.layers.ELU\")\n class ELU(Layer):\n     \"\"\"Applies an Exponential Linear Unit function to an output.\n \n\n@@ -1,8 +1,8 @@\n import numpy as np\n import pytest\n \n-from keras_core import testing\n-from keras_core.layers.activations import elu\n+from keras import testing\n+from keras.layers.activations import elu\n \n \n class ELUTest(testing.TestCase):\n\n@@ -1,11 +1,11 @@\n import warnings\n \n-from keras_core import activations\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n+from keras import activations\n+from keras.api_export import keras_export\n+from keras.layers.layer import Layer\n \n \n-@keras_core_export(\"keras_core.layers.LeakyReLU\")\n+@keras_export(\"keras.layers.LeakyReLU\")\n class LeakyReLU(Layer):\n     \"\"\"Leaky version of a Rectified Linear Unit activation layer.\n \n\n@@ -1,8 +1,8 @@\n import numpy as np\n import pytest\n \n-from keras_core import testing\n-from keras_core.layers.activations import leaky_relu\n+from keras import testing\n+from keras.layers.activations import leaky_relu\n \n \n class LeakyReLUTest(testing.TestCase):\n\n@@ -1,13 +1,13 @@\n-from keras_core import activations\n-from keras_core import constraints\n-from keras_core import initializers\n-from keras_core import regularizers\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.input_spec import InputSpec\n-from keras_core.layers.layer import Layer\n+from keras import activations\n+from keras import constraints\n+from keras import initializers\n+from keras import regularizers\n+from keras.api_export import keras_export\n+from keras.layers.input_spec import InputSpec\n+from keras.layers.layer import Layer\n \n \n-@keras_core_export(\"keras_core.layers.PReLU\")\n+@keras_export(\"keras.layers.PReLU\")\n class PReLU(Layer):\n     \"\"\"Parametric Rectified Linear Unit activation layer.\n \n\n@@ -1,8 +1,8 @@\n import numpy as np\n import pytest\n \n-from keras_core import testing\n-from keras_core.layers.activations import prelu\n+from keras import testing\n+from keras.layers.activations import prelu\n \n \n class PReLUTest(testing.TestCase):\n\n@@ -1,9 +1,9 @@\n-from keras_core import activations\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n+from keras import activations\n+from keras.api_export import keras_export\n+from keras.layers.layer import Layer\n \n \n-@keras_core_export(\"keras_core.layers.ReLU\")\n+@keras_export(\"keras.layers.ReLU\")\n class ReLU(Layer):\n     \"\"\"Rectified Linear Unit activation function layer.\n \n@@ -17,7 +17,7 @@ class ReLU(Layer):\n \n     Example:\n     ``` python\n-    relu_layer = keras_core.layers.activations.ReLU(\n+    relu_layer = keras.layers.activations.ReLU(\n         max_value=10,\n         negative_slope=0.5,\n         threshold=0,\n\n@@ -1,8 +1,8 @@\n import numpy as np\n import pytest\n \n-from keras_core import testing\n-from keras_core.layers.activations import relu\n+from keras import testing\n+from keras.layers.activations import relu\n \n \n class ReLUTest(testing.TestCase):\n\n@@ -1,7 +1,7 @@\n-from keras_core import activations\n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n+from keras import activations\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.layers.layer import Layer\n \n \n def _large_negative_number(dtype):\n@@ -11,7 +11,7 @@ def _large_negative_number(dtype):\n     return -1e9\n \n \n-@keras_core_export(\"keras_core.layers.Softmax\")\n+@keras_export(\"keras.layers.Softmax\")\n class Softmax(Layer):\n     \"\"\"Softmax activation layer.\n \n@@ -22,7 +22,7 @@ class Softmax(Layer):\n     ```\n \n     Example:\n-    >>>softmax_layer = keras_core.layers.activations.Softmax()\n+    >>>softmax_layer = keras.layers.activations.Softmax()\n     >>>input = np.array([1.0, 2.0, 1.0])\n     >>>result = softmax_layer(input)\n     [0.21194157, 0.5761169, 0.21194157]\n\n@@ -1,8 +1,8 @@\n import numpy as np\n import pytest\n \n-from keras_core import testing\n-from keras_core.layers.activations import softmax\n+from keras import testing\n+from keras.layers.activations import softmax\n \n \n class SoftmaxTest(testing.TestCase):\n\n\n@@ -1,9 +1,9 @@\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.attention.attention import Attention\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.layers.attention.attention import Attention\n \n \n-@keras_core_export(\"keras_core.layers.AdditiveAttention\")\n+@keras_export(\"keras.layers.AdditiveAttention\")\n class AdditiveAttention(Attention):\n     \"\"\"Additive attention layer, a.k.a. Bahdanau-style attention.\n \n\n@@ -1,7 +1,7 @@\n import numpy as np\n \n-from keras_core import layers\n-from keras_core import testing\n+from keras import layers\n+from keras import testing\n \n \n class AdditiveAttentionTest(testing.TestCase):\n\n@@ -1,10 +1,10 @@\n-from keras_core import backend\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n+from keras import backend\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.layers.layer import Layer\n \n \n-@keras_core_export(\"keras_core.layers.Attention\")\n+@keras_export(\"keras.layers.Attention\")\n class Attention(Layer):\n     \"\"\"Dot-product attention layer, a.k.a. Luong-style attention.\n \n\n@@ -1,7 +1,7 @@\n import numpy as np\n \n-from keras_core import layers\n-from keras_core import testing\n+from keras import layers\n+from keras import testing\n \n \n class AttentionTest(testing.TestCase):\n\n@@ -4,18 +4,18 @@ import string\n \n import numpy as np\n \n-from keras_core import constraints\n-from keras_core import initializers\n-from keras_core import ops\n-from keras_core import regularizers\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.activations.softmax import Softmax\n-from keras_core.layers.core.einsum_dense import EinsumDense\n-from keras_core.layers.layer import Layer\n-from keras_core.layers.regularization.dropout import Dropout\n+from keras import constraints\n+from keras import initializers\n+from keras import ops\n+from keras import regularizers\n+from keras.api_export import keras_export\n+from keras.layers.activations.softmax import Softmax\n+from keras.layers.core.einsum_dense import EinsumDense\n+from keras.layers.layer import Layer\n+from keras.layers.regularization.dropout import Dropout\n \n \n-@keras_core_export(\"keras_core.layers.MultiHeadAttention\")\n+@keras_export(\"keras.layers.MultiHeadAttention\")\n class MultiHeadAttention(Layer):\n     \"\"\"MultiHeadAttention layer.\n \n\n@@ -2,10 +2,10 @@ import numpy as np\n import pytest\n from absl.testing import parameterized\n \n-from keras_core import backend\n-from keras_core import initializers\n-from keras_core import layers\n-from keras_core import testing\n+from keras import backend\n+from keras import initializers\n+from keras import layers\n+from keras import testing\n \n \n class MultiHeadAttentionTest(testing.TestCase, parameterized.TestCase):\n\n\n@@ -1,16 +1,16 @@\n \"\"\"Keras base class for convolution layers.\"\"\"\n \n-from keras_core import activations\n-from keras_core import constraints\n-from keras_core import initializers\n-from keras_core import ops\n-from keras_core import regularizers\n-from keras_core.backend import standardize_data_format\n-from keras_core.layers.input_spec import InputSpec\n-from keras_core.layers.layer import Layer\n-from keras_core.ops.operation_utils import compute_conv_output_shape\n-from keras_core.utils.argument_validation import standardize_padding\n-from keras_core.utils.argument_validation import standardize_tuple\n+from keras import activations\n+from keras import constraints\n+from keras import initializers\n+from keras import ops\n+from keras import regularizers\n+from keras.backend import standardize_data_format\n+from keras.layers.input_spec import InputSpec\n+from keras.layers.layer import Layer\n+from keras.ops.operation_utils import compute_conv_output_shape\n+from keras.utils.argument_validation import standardize_padding\n+from keras.utils.argument_validation import standardize_tuple\n \n \n class BaseConv(Layer):\n\n@@ -1,18 +1,18 @@\n \"\"\"Keras base class for transpose convolution layers.\"\"\"\n \n-from keras_core import activations\n-from keras_core import constraints\n-from keras_core import initializers\n-from keras_core import ops\n-from keras_core import regularizers\n-from keras_core.backend import standardize_data_format\n-from keras_core.backend.common.backend_utils import (\n+from keras import activations\n+from keras import constraints\n+from keras import initializers\n+from keras import ops\n+from keras import regularizers\n+from keras.backend import standardize_data_format\n+from keras.backend.common.backend_utils import (\n     compute_conv_transpose_output_shape,\n )\n-from keras_core.layers.input_spec import InputSpec\n-from keras_core.layers.layer import Layer\n-from keras_core.utils.argument_validation import standardize_padding\n-from keras_core.utils.argument_validation import standardize_tuple\n+from keras.layers.input_spec import InputSpec\n+from keras.layers.layer import Layer\n+from keras.utils.argument_validation import standardize_padding\n+from keras.utils.argument_validation import standardize_tuple\n \n \n class BaseConvTranspose(Layer):\n\n@@ -1,16 +1,16 @@\n \"\"\"Keras base class for depthwise convolution layers.\"\"\"\n \n-from keras_core import activations\n-from keras_core import constraints\n-from keras_core import initializers\n-from keras_core import ops\n-from keras_core import regularizers\n-from keras_core.backend import standardize_data_format\n-from keras_core.layers.input_spec import InputSpec\n-from keras_core.layers.layer import Layer\n-from keras_core.ops.operation_utils import compute_conv_output_shape\n-from keras_core.utils.argument_validation import standardize_padding\n-from keras_core.utils.argument_validation import standardize_tuple\n+from keras import activations\n+from keras import constraints\n+from keras import initializers\n+from keras import ops\n+from keras import regularizers\n+from keras.backend import standardize_data_format\n+from keras.layers.input_spec import InputSpec\n+from keras.layers.layer import Layer\n+from keras.ops.operation_utils import compute_conv_output_shape\n+from keras.utils.argument_validation import standardize_padding\n+from keras.utils.argument_validation import standardize_tuple\n \n \n class BaseDepthwiseConv(Layer):\n\n@@ -1,17 +1,17 @@\n \"\"\"Keras abstract base layer for separable convolution.\"\"\"\n \n \n-from keras_core import activations\n-from keras_core import constraints\n-from keras_core import initializers\n-from keras_core import ops\n-from keras_core import regularizers\n-from keras_core.backend import standardize_data_format\n-from keras_core.layers.input_spec import InputSpec\n-from keras_core.layers.layer import Layer\n-from keras_core.ops.operation_utils import compute_conv_output_shape\n-from keras_core.utils.argument_validation import standardize_padding\n-from keras_core.utils.argument_validation import standardize_tuple\n+from keras import activations\n+from keras import constraints\n+from keras import initializers\n+from keras import ops\n+from keras import regularizers\n+from keras.backend import standardize_data_format\n+from keras.layers.input_spec import InputSpec\n+from keras.layers.layer import Layer\n+from keras.ops.operation_utils import compute_conv_output_shape\n+from keras.utils.argument_validation import standardize_padding\n+from keras.utils.argument_validation import standardize_tuple\n \n \n class BaseSeparableConv(Layer):\n\n@@ -1,10 +1,10 @@\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.convolutional.base_conv import BaseConv\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.layers.convolutional.base_conv import BaseConv\n \n \n-@keras_core_export(\n-    [\"keras_core.layers.Conv1D\", \"keras_core.layers.Convolution1D\"]\n+@keras_export(\n+    [\"keras.layers.Conv1D\", \"keras.layers.Convolution1D\"]\n )\n class Conv1D(BaseConv):\n     \"\"\"1D convolution layer (e.g. temporal convolution).\n@@ -87,7 +87,7 @@ class Conv1D(BaseConv):\n     >>> # The inputs are 128-length vectors with 10 timesteps, and the\n     >>> # batch size is 4.\n     >>> x = np.random.rand(4, 10, 128)\n-    >>> y = keras_core.layers.Conv1D(32, 3, activation='relu')(x)\n+    >>> y = keras.layers.Conv1D(32, 3, activation='relu')(x)\n     >>> print(y.shape)\n     (4, 8, 32)\n     \"\"\"\n\n@@ -1,13 +1,13 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.convolutional.base_conv_transpose import (\n+from keras.api_export import keras_export\n+from keras.layers.convolutional.base_conv_transpose import (\n     BaseConvTranspose,\n )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.layers.Conv1DTranspose\",\n-        \"keras_core.layers.Convolution1DTranspose\",\n+        \"keras.layers.Conv1DTranspose\",\n+        \"keras.layers.Convolution1DTranspose\",\n     ]\n )\n class Conv1DTranspose(BaseConvTranspose):\n@@ -86,7 +86,7 @@ class Conv1DTranspose(BaseConvTranspose):\n     Examples:\n \n     >>> x = np.random.rand(4, 10, 128)\n-    >>> y = keras_core.layers.Conv1DTranspose(32, 3, 2, activation='relu')(x)\n+    >>> y = keras.layers.Conv1DTranspose(32, 3, 2, activation='relu')(x)\n     >>> print(y.shape)\n     (4, 21, 32)\n     \"\"\"\n\n@@ -1,9 +1,9 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.convolutional.base_conv import BaseConv\n+from keras.api_export import keras_export\n+from keras.layers.convolutional.base_conv import BaseConv\n \n \n-@keras_core_export(\n-    [\"keras_core.layers.Conv2D\", \"keras_core.layers.Convolution2D\"]\n+@keras_export(\n+    [\"keras.layers.Conv2D\", \"keras.layers.Convolution2D\"]\n )\n class Conv2D(BaseConv):\n     \"\"\"2D convolution layer.\n@@ -81,7 +81,7 @@ class Conv2D(BaseConv):\n     Examples:\n \n     >>> x = np.random.rand(4, 10, 10, 128)\n-    >>> y = keras_core.layers.Conv2D(32, 3, activation='relu')(x)\n+    >>> y = keras.layers.Conv2D(32, 3, activation='relu')(x)\n     >>> print(y.shape)\n     (4, 8, 8, 32)\n     \"\"\"\n\n@@ -1,13 +1,13 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.convolutional.base_conv_transpose import (\n+from keras.api_export import keras_export\n+from keras.layers.convolutional.base_conv_transpose import (\n     BaseConvTranspose,\n )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.layers.Conv2DTranspose\",\n-        \"keras_core.layers.Convolution2DTranspose\",\n+        \"keras.layers.Conv2DTranspose\",\n+        \"keras.layers.Convolution2DTranspose\",\n     ]\n )\n class Conv2DTranspose(BaseConvTranspose):\n@@ -88,7 +88,7 @@ class Conv2DTranspose(BaseConvTranspose):\n     Examples:\n \n     >>> x = np.random.rand(4, 10, 8, 128)\n-    >>> y = keras_core.layers.Conv2DTranspose(32, 2, 2, activation='relu')(x)\n+    >>> y = keras.layers.Conv2DTranspose(32, 2, 2, activation='relu')(x)\n     >>> print(y.shape)\n     (4, 20, 16, 32)\n     \"\"\"\n\n@@ -1,9 +1,9 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.convolutional.base_conv import BaseConv\n+from keras.api_export import keras_export\n+from keras.layers.convolutional.base_conv import BaseConv\n \n \n-@keras_core_export(\n-    [\"keras_core.layers.Conv3D\", \"keras_core.layers.Convolution3D\"]\n+@keras_export(\n+    [\"keras.layers.Conv3D\", \"keras.layers.Convolution3D\"]\n )\n class Conv3D(BaseConv):\n     \"\"\"3D convolution layer.\n@@ -87,7 +87,7 @@ class Conv3D(BaseConv):\n     Examples:\n \n     >>> x = np.random.rand(4, 10, 10, 10, 128)\n-    >>> y = keras_core.layers.Conv3D(32, 3, activation='relu')(x)\n+    >>> y = keras.layers.Conv3D(32, 3, activation='relu')(x)\n     >>> print(y.shape)\n     (4, 8, 8, 8, 32)\n     \"\"\"\n\n@@ -1,13 +1,13 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.convolutional.base_conv_transpose import (\n+from keras.api_export import keras_export\n+from keras.layers.convolutional.base_conv_transpose import (\n     BaseConvTranspose,\n )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.layers.Conv3DTranspose\",\n-        \"keras_core.layers.Convolution3DTranspose\",\n+        \"keras.layers.Conv3DTranspose\",\n+        \"keras.layers.Convolution3DTranspose\",\n     ]\n )\n class Conv3DTranspose(BaseConvTranspose):\n@@ -93,7 +93,7 @@ class Conv3DTranspose(BaseConvTranspose):\n     Examples:\n \n     >>> x = np.random.rand(4, 10, 8, 12, 128)\n-    >>> y = keras_core.layers.Conv3DTranspose(32, 2, 2, activation='relu')(x)\n+    >>> y = keras.layers.Conv3DTranspose(32, 2, 2, activation='relu')(x)\n     >>> print(y.shape)\n     (4, 20, 16, 24, 32)\n     \"\"\"\n\n@@ -3,8 +3,8 @@ import pytest\n from absl.testing import parameterized\n from numpy.lib.stride_tricks import as_strided\n \n-from keras_core import layers\n-from keras_core import testing\n+from keras import layers\n+from keras import testing\n \n \n def _same_padding(input_size, kernel_size, stride):\n\n@@ -2,16 +2,16 @@ import numpy as np\n import pytest\n from absl.testing import parameterized\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import testing\n-from keras_core.backend.common.backend_utils import (\n+from keras import backend\n+from keras import layers\n+from keras import testing\n+from keras.backend.common.backend_utils import (\n     _convert_conv_tranpose_padding_args_from_keras_to_torch,\n )\n-from keras_core.backend.common.backend_utils import (\n+from keras.backend.common.backend_utils import (\n     compute_conv_transpose_output_shape,\n )\n-from keras_core.backend.common.backend_utils import (\n+from keras.backend.common.backend_utils import (\n     compute_conv_transpose_padding_args_for_jax,\n )\n \n@@ -787,7 +787,7 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             dilation_rate=1,\n         )\n \n-        # keras-core layer\n+        # keras layer\n         kc_layer = layers.Conv1DTranspose(\n             filters=1,\n             kernel_size=kernel_size,\n\n@@ -1,10 +1,10 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.convolutional.base_depthwise_conv import (\n+from keras.api_export import keras_export\n+from keras.layers.convolutional.base_depthwise_conv import (\n     BaseDepthwiseConv,\n )\n \n \n-@keras_core_export(\"keras_core.layers.DepthwiseConv1D\")\n+@keras_export(\"keras.layers.DepthwiseConv1D\")\n class DepthwiseConv1D(BaseDepthwiseConv):\n     \"\"\"1D depthwise convolution layer.\n \n@@ -92,7 +92,7 @@ class DepthwiseConv1D(BaseDepthwiseConv):\n     Examples:\n \n     >>> x = np.random.rand(4, 10, 12)\n-    >>> y = keras_core.layers.DepthwiseConv1D(3, 3, 2, activation='relu')(x)\n+    >>> y = keras.layers.DepthwiseConv1D(3, 3, 2, activation='relu')(x)\n     >>> print(y.shape)\n     (4, 4, 36)\n     \"\"\"\n\n@@ -1,10 +1,10 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.convolutional.base_depthwise_conv import (\n+from keras.api_export import keras_export\n+from keras.layers.convolutional.base_depthwise_conv import (\n     BaseDepthwiseConv,\n )\n \n \n-@keras_core_export(\"keras_core.layers.DepthwiseConv2D\")\n+@keras_export(\"keras.layers.DepthwiseConv2D\")\n class DepthwiseConv2D(BaseDepthwiseConv):\n     \"\"\"2D depthwise convolution layer.\n \n@@ -92,7 +92,7 @@ class DepthwiseConv2D(BaseDepthwiseConv):\n     Examples:\n \n     >>> x = np.random.rand(4, 10, 10, 12)\n-    >>> y = keras_core.layers.DepthwiseConv2D(3, 3, activation='relu')(x)\n+    >>> y = keras.layers.DepthwiseConv2D(3, 3, activation='relu')(x)\n     >>> print(y.shape)\n     (4, 8, 8, 36)\n     \"\"\"\n\n@@ -3,8 +3,8 @@ import pytest\n from absl.testing import parameterized\n from numpy.lib.stride_tricks import as_strided\n \n-from keras_core import layers\n-from keras_core import testing\n+from keras import layers\n+from keras import testing\n \n \n def _same_padding(input_size, kernel_size, stride):\n\n@@ -1,13 +1,13 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.convolutional.base_separable_conv import (\n+from keras.api_export import keras_export\n+from keras.layers.convolutional.base_separable_conv import (\n     BaseSeparableConv,\n )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.layers.SeparableConv1D\",\n-        \"keras_core.layers.SeparableConvolution1D\",\n+        \"keras.layers.SeparableConv1D\",\n+        \"keras.layers.SeparableConvolution1D\",\n     ]\n )\n class SeparableConv1D(BaseSeparableConv):\n@@ -90,7 +90,7 @@ class SeparableConv1D(BaseSeparableConv):\n     Examples:\n \n     >>> x = np.random.rand(4, 10, 12)\n-    >>> y = keras_core.layers.SeparableConv1D(3, 4, 3, 2, activation='relu')(x)\n+    >>> y = keras.layers.SeparableConv1D(3, 4, 3, 2, activation='relu')(x)\n     >>> print(y.shape)\n     (4, 4, 4)\n     \"\"\"\n\n@@ -1,13 +1,13 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.convolutional.base_separable_conv import (\n+from keras.api_export import keras_export\n+from keras.layers.convolutional.base_separable_conv import (\n     BaseSeparableConv,\n )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.layers.SeparableConv2D\",\n-        \"keras_core.layers.SeparableConvolution2D\",\n+        \"keras.layers.SeparableConv2D\",\n+        \"keras.layers.SeparableConvolution2D\",\n     ]\n )\n class SeparableConv2D(BaseSeparableConv):\n@@ -90,7 +90,7 @@ class SeparableConv2D(BaseSeparableConv):\n     Examples:\n \n     >>> x = np.random.rand(4, 10, 10, 12)\n-    >>> y = keras_core.layers.SeparableConv2D(3, 4, 3, 2, activation='relu')(x)\n+    >>> y = keras.layers.SeparableConv2D(3, 4, 3, 2, activation='relu')(x)\n     >>> print(y.shape)\n     (4, 4, 4, 4)\n     \"\"\"\n\n@@ -2,14 +2,14 @@ import numpy as np\n import pytest\n from absl.testing import parameterized\n \n-from keras_core import layers\n-from keras_core import testing\n-from keras_core.layers.convolutional.conv_test import np_conv1d\n-from keras_core.layers.convolutional.conv_test import np_conv2d\n-from keras_core.layers.convolutional.depthwise_conv_test import (\n+from keras import layers\n+from keras import testing\n+from keras.layers.convolutional.conv_test import np_conv1d\n+from keras.layers.convolutional.conv_test import np_conv2d\n+from keras.layers.convolutional.depthwise_conv_test import (\n     np_depthwise_conv1d,\n )\n-from keras_core.layers.convolutional.depthwise_conv_test import (\n+from keras.layers.convolutional.depthwise_conv_test import (\n     np_depthwise_conv2d,\n )\n \n\n\n@@ -1,14 +1,14 @@\n-from keras_core import activations\n-from keras_core import constraints\n-from keras_core import initializers\n-from keras_core import ops\n-from keras_core import regularizers\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.input_spec import InputSpec\n-from keras_core.layers.layer import Layer\n+from keras import activations\n+from keras import constraints\n+from keras import initializers\n+from keras import ops\n+from keras import regularizers\n+from keras.api_export import keras_export\n+from keras.layers.input_spec import InputSpec\n+from keras.layers.layer import Layer\n \n \n-@keras_core_export(\"keras_core.layers.Dense\")\n+@keras_export(\"keras.layers.Dense\")\n class Dense(Layer):\n     \"\"\"Just your regular densely-connected NN layer.\n \n\n@@ -1,10 +1,10 @@\n import numpy as np\n import pytest\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import testing\n-from keras_core.backend.common import keras_tensor\n+from keras import backend\n+from keras import layers\n+from keras import testing\n+from keras.backend.common import keras_tensor\n \n \n class DenseTest(testing.TestCase):\n\n@@ -1,15 +1,15 @@\n import re\n \n-from keras_core import activations\n-from keras_core import constraints\n-from keras_core import initializers\n-from keras_core import ops\n-from keras_core import regularizers\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n+from keras import activations\n+from keras import constraints\n+from keras import initializers\n+from keras import ops\n+from keras import regularizers\n+from keras.api_export import keras_export\n+from keras.layers.layer import Layer\n \n \n-@keras_core_export(\"keras_core.layers.EinsumDense\")\n+@keras_export(\"keras.layers.EinsumDense\")\n class EinsumDense(Layer):\n     \"\"\"A layer that uses `einsum` as the backing computation.\n \n@@ -48,12 +48,12 @@ class EinsumDense(Layer):\n \n     This example shows how to instantiate a standard Keras dense layer using\n     einsum operations. This example is equivalent to\n-    `keras_core.layers.Dense(64, use_bias=True)`.\n+    `keras.layers.Dense(64, use_bias=True)`.\n \n-    >>> layer = keras_core.layers.EinsumDense(\"ab,bc->ac\",\n+    >>> layer = keras.layers.EinsumDense(\"ab,bc->ac\",\n     ...                                       output_shape=64,\n     ...                                       bias_axes=\"c\")\n-    >>> input_tensor = keras_core.Input(shape=[32])\n+    >>> input_tensor = keras.Input(shape=[32])\n     >>> output_tensor = layer(input_tensor)\n     >>> output_tensor.shape\n     (None, 64)\n@@ -66,10 +66,10 @@ class EinsumDense(Layer):\n     dimension in the `output_shape` is `None`, because the sequence dimension\n     `b` has an unknown shape.\n \n-    >>> layer = keras_core.layers.EinsumDense(\"abc,cd->abd\",\n+    >>> layer = keras.layers.EinsumDense(\"abc,cd->abd\",\n     ...                                       output_shape=(None, 64),\n     ...                                       bias_axes=\"d\")\n-    >>> input_tensor = keras_core.Input(shape=[32, 128])\n+    >>> input_tensor = keras.Input(shape=[32, 128])\n     >>> output_tensor = layer(input_tensor)\n     >>> output_tensor.shape\n     (None, 32, 64)\n@@ -85,10 +85,10 @@ class EinsumDense(Layer):\n     layer can handle any number of sequence dimensions - including the case\n     where no sequence dimension exists.\n \n-    >>> layer = keras_core.layers.EinsumDense(\"...x,xy->...y\",\n+    >>> layer = keras.layers.EinsumDense(\"...x,xy->...y\",\n     ...                                       output_shape=64,\n     ...                                       bias_axes=\"y\")\n-    >>> input_tensor = keras_core.Input(shape=[32, 128])\n+    >>> input_tensor = keras.Input(shape=[32, 128])\n     >>> output_tensor = layer(input_tensor)\n     >>> output_tensor.shape\n     (None, 32, 64)\n\n@@ -1,8 +1,8 @@\n import pytest\n from absl.testing import parameterized\n \n-from keras_core import layers\n-from keras_core import testing\n+from keras import layers\n+from keras import testing\n \n \n class EinsumDenseTest(testing.TestCase, parameterized.TestCase):\n\n@@ -1,12 +1,12 @@\n-from keras_core import constraints\n-from keras_core import initializers\n-from keras_core import ops\n-from keras_core import regularizers\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n+from keras import constraints\n+from keras import initializers\n+from keras import ops\n+from keras import regularizers\n+from keras.api_export import keras_export\n+from keras.layers.layer import Layer\n \n \n-@keras_core_export(\"keras_core.layers.Embedding\")\n+@keras_export(\"keras.layers.Embedding\")\n class Embedding(Layer):\n     \"\"\"Turns positive integers (indexes) into dense vectors of fixed size.\n \n@@ -16,8 +16,8 @@ class Embedding(Layer):\n \n     Example:\n \n-    >>> model = keras_core.Sequential()\n-    >>> model.add(keras_core.layers.Embedding(1000, 64, input_length=10))\n+    >>> model = keras.Sequential()\n+    >>> model.add(keras.layers.Embedding(1000, 64, input_length=10))\n     >>> # The model will take as input an integer matrix of size (batch,\n     >>> # input_length), and the largest integer (i.e. word index) in the input\n     >>> # should be no larger than 999 (vocabulary size).\n\n@@ -1,9 +1,9 @@\n import numpy as np\n import pytest\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core.testing import test_case\n+from keras import backend\n+from keras import layers\n+from keras.testing import test_case\n \n \n class EmbeddingTest(test_case.TestCase):\n\n@@ -1,8 +1,8 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n+from keras.api_export import keras_export\n+from keras.layers.layer import Layer\n \n \n-@keras_core_export(\"keras_core.layers.Identity\")\n+@keras_export(\"keras.layers.Identity\")\n class Identity(Layer):\n     \"\"\"Identity layer.\n \n\n@@ -1,9 +1,9 @@\n import pytest\n from absl.testing import parameterized\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import testing\n+from keras import backend\n+from keras import layers\n+from keras import testing\n \n \n class IdentityTest(testing.TestCase, parameterized.TestCase):\n\n@@ -1,12 +1,12 @@\n import warnings\n \n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n-from keras_core.ops.node import Node\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.layers.layer import Layer\n+from keras.ops.node import Node\n \n \n-@keras_core_export(\"keras_core.layers.InputLayer\")\n+@keras_export(\"keras.layers.InputLayer\")\n class InputLayer(Layer):\n     def __init__(\n         self,\n@@ -84,7 +84,7 @@ class InputLayer(Layer):\n         }\n \n \n-@keras_core_export([\"keras_core.layers.Input\", \"keras_core.Input\"])\n+@keras_export([\"keras.layers.Input\", \"keras.Input\"])\n def Input(\n     shape=None,\n     batch_size=None,\n\n@@ -1,10 +1,10 @@\n import numpy as np\n from absl.testing import parameterized\n \n-from keras_core import backend\n-from keras_core import testing\n-from keras_core.backend import KerasTensor\n-from keras_core.layers import InputLayer\n+from keras import backend\n+from keras import testing\n+from keras.backend import KerasTensor\n+from keras.layers import InputLayer\n \n \n class InputLayerTest(testing.TestCase, parameterized.TestCase):\n\n@@ -3,15 +3,15 @@ import types\n \n import tree\n \n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n-from keras_core.saving import serialization_lib\n-from keras_core.utils import python_utils\n-from keras_core.utils import shape_utils\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.layers.layer import Layer\n+from keras.saving import serialization_lib\n+from keras.utils import python_utils\n+from keras.utils import shape_utils\n \n \n-@keras_core_export(\"keras_core.layers.Lambda\")\n+@keras_export(\"keras.layers.Lambda\")\n class Lambda(Layer):\n     \"\"\"Wraps arbitrary expressions as a `Layer` object.\n \n@@ -177,7 +177,7 @@ class Lambda(Layer):\n                 \"config artifact, you can override this error \"\n                 \"by passing `safe_mode=False` \"\n                 \"to `from_config()`, or calling \"\n-                \"`keras_core.config.enable_unsafe_deserialization().\"\n+                \"`keras.config.enable_unsafe_deserialization().\"\n             )\n \n     @classmethod\n\n@@ -1,9 +1,9 @@\n import numpy as np\n import pytest\n \n-from keras_core import layers\n-from keras_core import ops\n-from keras_core import testing\n+from keras import layers\n+from keras import ops\n+from keras import testing\n \n \n class LambdaTest(testing.TestCase):\n\n@@ -1,10 +1,10 @@\n-from keras_core import backend\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n+from keras import backend\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.layers.layer import Layer\n \n \n-@keras_core_export(\"keras_core.layers.Masking\")\n+@keras_export(\"keras.layers.Masking\")\n class Masking(Layer):\n     \"\"\"Masks a sequence by using a mask value to skip timesteps.\n \n@@ -31,9 +31,9 @@ class Masking(Layer):\n     inputs[:, 3, :] = 0.\n     inputs[:, 5, :] = 0.\n \n-    model = keras_core.models.Sequential()\n-    model.add(keras_core.layers.Masking(mask_value=0.)\n-    model.add(keras_core.layers.LSTM(32))\n+    model = keras.models.Sequential()\n+    model.add(keras.layers.Masking(mask_value=0.)\n+    model.add(keras.layers.LSTM(32))\n     output = model(inputs)\n     # The time step 3 and 5 will be skipped from LSTM calculation.\n     ```\n\n@@ -1,9 +1,9 @@\n import numpy as np\n import pytest\n \n-from keras_core import layers\n-from keras_core import models\n-from keras_core import testing\n+from keras import layers\n+from keras import models\n+from keras import testing\n \n \n class MaskingTest(testing.TestCase):\n\n@@ -1,9 +1,9 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n-from keras_core.saving import serialization_lib\n+from keras.api_export import keras_export\n+from keras.layers.layer import Layer\n+from keras.saving import serialization_lib\n \n \n-@keras_core_export(\"keras_core.layers.Wrapper\")\n+@keras_export(\"keras.layers.Wrapper\")\n class Wrapper(Layer):\n     \"\"\"Abstract wrapper base class.\n \n\n@@ -1,8 +1,8 @@\n import pytest\n \n-from keras_core import layers\n-from keras_core import ops\n-from keras_core import testing\n+from keras import layers\n+from keras import ops\n+from keras import testing\n \n \n class ExampleWrapper(layers.Wrapper):\n\n@@ -1,10 +1,10 @@\n import tree\n \n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n+from keras import backend\n+from keras.api_export import keras_export\n \n \n-@keras_core_export([\"keras_core.InputSpec\", \"keras_core.layers.InputSpec\"])\n+@keras_export([\"keras.InputSpec\", \"keras.layers.InputSpec\"])\n class InputSpec:\n     \"\"\"Specifies the rank, dtype and shape of every input to a layer.\n \n\n@@ -22,38 +22,38 @@ from functools import wraps\n \n import tree\n \n-from keras_core import backend\n-from keras_core import initializers\n-from keras_core import mixed_precision\n-from keras_core import regularizers\n-from keras_core import utils\n-from keras_core.api_export import keras_core_export\n-from keras_core.backend import KerasTensor\n-from keras_core.backend.common import global_state\n-from keras_core.layers import input_spec\n-from keras_core.metrics.metric import Metric\n-from keras_core.ops.operation import Operation\n-from keras_core.utils import python_utils\n-from keras_core.utils import summary_utils\n-from keras_core.utils import traceback_utils\n-from keras_core.utils import tracking\n-from keras_core.utils.shape_utils import map_shape_structure\n+from keras import backend\n+from keras import initializers\n+from keras import mixed_precision\n+from keras import regularizers\n+from keras import utils\n+from keras.api_export import keras_export\n+from keras.backend import KerasTensor\n+from keras.backend.common import global_state\n+from keras.layers import input_spec\n+from keras.metrics.metric import Metric\n+from keras.ops.operation import Operation\n+from keras.utils import python_utils\n+from keras.utils import summary_utils\n+from keras.utils import traceback_utils\n+from keras.utils import tracking\n+from keras.utils.shape_utils import map_shape_structure\n \n if backend.backend() == \"tensorflow\":\n-    from keras_core.backend.tensorflow.layer import TFLayer as BackendLayer\n+    from keras.backend.tensorflow.layer import TFLayer as BackendLayer\n elif backend.backend() == \"jax\":\n-    from keras_core.backend.jax.layer import JaxLayer as BackendLayer\n+    from keras.backend.jax.layer import JaxLayer as BackendLayer\n elif backend.backend() == \"torch\":\n-    from keras_core.backend.torch.layer import TorchLayer as BackendLayer\n+    from keras.backend.torch.layer import TorchLayer as BackendLayer\n elif backend.backend() == \"numpy\":\n-    from keras_core.backend.numpy.layer import NumpyLayer as BackendLayer\n+    from keras.backend.numpy.layer import NumpyLayer as BackendLayer\n else:\n     raise RuntimeError(\n         f\"Backend '{backend.backend()}' must implement a layer mixin class.\"\n     )\n \n \n-@keras_core_export([\"keras_core.Layer\", \"keras_core.layers.Layer\"])\n+@keras_export([\"keras.Layer\", \"keras.layers.Layer\"])\n class Layer(BackendLayer, Operation):\n     \"\"\"This is the class from which all layers inherit.\n \n@@ -78,12 +78,12 @@ class Layer(BackendLayer, Operation):\n         trainable: Boolean, whether the layer's variables should be trainable.\n         name: String name of the layer.\n         dtype: The dtype of the layer's computations and weights. Can also be a\n-            `keras_core.mixed_precision.DTypePolicy`,\n+            `keras.mixed_precision.DTypePolicy`,\n             which allows the computation and\n             weight dtype to differ. Defaults to `None`. `None` means to use\n-            `keras_core.mixed_precision.dtype_policy()`,\n+            `keras.mixed_precision.dtype_policy()`,\n             which is a `float32` policy unless set to different value\n-            (via `keras_core.mixed_precision.set_dtype_policy()`).\n+            (via `keras.mixed_precision.set_dtype_policy()`).\n \n     Attributes:\n         name: The name of the layer (string).\n@@ -93,7 +93,7 @@ class Layer(BackendLayer, Operation):\n             Layers automatically cast inputs to this dtype, which causes\n             the computations and output to also be in this dtype.\n             When mixed precision is used with a\n-            `keras_core.mixed_precision.DTypePolicy`, this will be different\n+            `keras.mixed_precision.DTypePolicy`, this will be different\n             than `variable_dtype`.\n         trainable_weights: List of variables to be included in backprop.\n         non_trainable_weights: List of variables that should not be\n\n@@ -1,12 +1,12 @@\n import numpy as np\n import pytest\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import metrics\n-from keras_core import models\n-from keras_core import ops\n-from keras_core import testing\n+from keras import backend\n+from keras import layers\n+from keras import metrics\n+from keras import models\n+from keras import ops\n+from keras import testing\n \n \n class LayerTest(testing.TestCase):\n\n\n@@ -0,0 +1,69 @@\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.layers.merging.base_merge import Merge\n+\n+\n+@keras_export(\"keras.layers.Add\")\n+class Add(Merge):\n+    \"\"\"Performs elementwise addition operation.\n+\n+    It takes as input a list of tensors, all of the same shape,\n+    and returns a single tensor (also of the same shape).\n+\n+    Examples:\n+\n+    >>> input_shape = (2, 3, 4)\n+    >>> x1 = np.random.rand(*input_shape)\n+    >>> x2 = np.random.rand(*input_shape)\n+    >>> y = keras.layers.Add()([x1, x2])\n+\n+    Usage in a Keras model:\n+\n+    >>> input1 = keras.layers.Input(shape=(16,))\n+    >>> x1 = keras.layers.Dense(8, activation='relu')(input1)\n+    >>> input2 = keras.layers.Input(shape=(32,))\n+    >>> x2 = keras.layers.Dense(8, activation='relu')(input2)\n+    >>> # equivalent to `added = keras.layers.add([x1, x2])`\n+    >>> added = keras.layers.Add()([x1, x2])\n+    >>> out = keras.layers.Dense(4)(added)\n+    >>> model = keras.models.Model(inputs=[input1, input2], outputs=out)\n+\n+    \"\"\"\n+\n+    def _merge_function(self, inputs):\n+        output = inputs[0]\n+        for i in range(1, len(inputs)):\n+            output = ops.add(output, inputs[i])\n+        return output\n+\n+\n+@keras_export(\"keras.layers.add\")\n+def add(inputs, **kwargs):\n+    \"\"\"Functional interface to the `keras.layers.Add` layer.\n+\n+    Args:\n+        inputs: A list of input tensors with the same shape.\n+        **kwargs: Standard layer keyword arguments.\n+\n+    Returns:\n+        A tensor as the sum of the inputs. It has the same shape as the inputs.\n+\n+    Examples:\n+\n+    >>> input_shape = (2, 3, 4)\n+    >>> x1 = np.random.rand(*input_shape)\n+    >>> x2 = np.random.rand(*input_shape)\n+    >>> y = keras.layers.add([x1, x2])\n+\n+    Usage in a Keras model:\n+\n+    >>> input1 = keras.layers.Input(shape=(16,))\n+    >>> x1 = keras.layers.Dense(8, activation='relu')(input1)\n+    >>> input2 = keras.layers.Input(shape=(32,))\n+    >>> x2 = keras.layers.Dense(8, activation='relu')(input2)\n+    >>> added = keras.layers.add([x1, x2])\n+    >>> out = keras.layers.Dense(4)(added)\n+    >>> model = keras.models.Model(inputs=[input1, input2], outputs=out)\n+\n+    \"\"\"\n+    return Add(**kwargs)(inputs)\n\n@@ -0,0 +1,70 @@\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.layers.merging.base_merge import Merge\n+\n+\n+@keras_export(\"keras.layers.Average\")\n+class Average(Merge):\n+    \"\"\"Averages a list of inputs element-wise..\n+\n+    It takes as input a list of tensors, all of the same shape,\n+    and returns a single tensor (also of the same shape).\n+\n+    Examples:\n+\n+    >>> input_shape = (2, 3, 4)\n+    >>> x1 = np.random.rand(*input_shape)\n+    >>> x2 = np.random.rand(*input_shape)\n+    >>> y = keras.layers.Average()([x1, x2])\n+\n+    Usage in a Keras model:\n+\n+    >>> input1 = keras.layers.Input(shape=(16,))\n+    >>> x1 = keras.layers.Dense(8, activation='relu')(input1)\n+    >>> input2 = keras.layers.Input(shape=(32,))\n+    >>> x2 = keras.layers.Dense(8, activation='relu')(input2)\n+    >>> # equivalent to `y = keras.layers.average([x1, x2])`\n+    >>> y = keras.layers.Average()([x1, x2])\n+    >>> out = keras.layers.Dense(4)(y)\n+    >>> model = keras.models.Model(inputs=[input1, input2], outputs=out)\n+\n+    \"\"\"\n+\n+    def _merge_function(self, inputs):\n+        output = inputs[0]\n+        for i in range(1, len(inputs)):\n+            output = ops.add(output, inputs[i])\n+        return output / len(inputs)\n+\n+\n+@keras_export(\"keras.layers.average\")\n+def average(inputs, **kwargs):\n+    \"\"\"Functional interface to the `keras.layers.Average` layer.\n+\n+    Args:\n+        inputs: A list of input tensors , all of the same shape.\n+        **kwargs: Standard layer keyword arguments.\n+\n+    Returns:\n+        A tensor as the element-wise product of the inputs with the same\n+        shape as the inputs.\n+\n+    Examples:\n+\n+    >>> input_shape = (2, 3, 4)\n+    >>> x1 = np.random.rand(*input_shape)\n+    >>> x2 = np.random.rand(*input_shape)\n+    >>> y = keras.layers.average([x1, x2])\n+\n+    Usage in a Keras model:\n+\n+    >>> input1 = keras.layers.Input(shape=(16,))\n+    >>> x1 = keras.layers.Dense(8, activation='relu')(input1)\n+    >>> input2 = keras.layers.Input(shape=(32,))\n+    >>> x2 = keras.layers.Dense(8, activation='relu')(input2)\n+    >>> y = keras.layers.average([x1, x2])\n+    >>> out = keras.layers.Dense(4)(y)\n+    >>> model = keras.models.Model(inputs=[input1, input2], outputs=out)\n+\n+    \"\"\"\n+    return Average(**kwargs)(inputs)\n\n@@ -1,7 +1,7 @@\n-from keras_core import backend\n-from keras_core import ops\n-from keras_core.backend.common.keras_tensor import KerasTensor\n-from keras_core.layers.layer import Layer\n+from keras import backend\n+from keras import ops\n+from keras.backend.common.keras_tensor import KerasTensor\n+from keras.layers.layer import Layer\n \n \n class Merge(Layer):\n\n@@ -1,9 +1,9 @@\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.merging.base_merge import Merge\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.layers.merging.base_merge import Merge\n \n \n-@keras_core_export(\"keras_core.layers.Concatenate\")\n+@keras_export(\"keras.layers.Concatenate\")\n class Concatenate(Merge):\n     \"\"\"Concatenates a list of inputs.\n \n@@ -15,13 +15,13 @@ class Concatenate(Merge):\n \n     >>> x = np.arange(20).reshape(2, 2, 5)\n     >>> y = np.arange(20, 30).reshape(2, 1, 5)\n-    >>> keras_core.layers.Concatenate(axis=1)([x, y])\n+    >>> keras.layers.Concatenate(axis=1)([x, y])\n \n     Usage in a Keras model:\n \n-    >>> x1 = keras_core.layers.Dense(8)(np.arange(10).reshape(5, 2))\n-    >>> x2 = keras_core.layers.Dense(8)(np.arange(10, 20).reshape(5, 2))\n-    >>> y = keras_core.layers.Concatenate()([x1, x2])\n+    >>> x1 = keras.layers.Dense(8)(np.arange(10).reshape(5, 2))\n+    >>> x2 = keras.layers.Dense(8)(np.arange(10, 20).reshape(5, 2))\n+    >>> y = keras.layers.Concatenate()([x1, x2])\n \n     Args:\n         axis: Axis along which to concatenate.\n@@ -157,7 +157,7 @@ class Concatenate(Merge):\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n-@keras_core_export(\"keras_core.layers.concatenate\")\n+@keras_export(\"keras.layers.concatenate\")\n def concatenate(inputs, axis=-1, **kwargs):\n     \"\"\"Functional interface to the `Concatenate` layer.\n \n\n@@ -1,7 +1,7 @@\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.merging.base_merge import Merge\n-from keras_core.utils.numerical_utils import normalize\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.layers.merging.base_merge import Merge\n+from keras.utils.numerical_utils import normalize\n \n \n def batch_dot(x, y, axes=None):\n@@ -105,7 +105,7 @@ def batch_dot(x, y, axes=None):\n             \"Cannot perform batch_dot over axis 0. \"\n             \"If your inputs are not batched, \"\n             \"add a dummy batch dimension to your \"\n-            \"inputs using keras_core.ops.expand_dims(x, 0)\"\n+            \"inputs using keras.ops.expand_dims(x, 0)\"\n         )\n     a0, a1 = axes\n     d1 = x_shape[a0]\n@@ -196,7 +196,7 @@ def batch_dot(x, y, axes=None):\n     return result\n \n \n-@keras_core_export(\"keras_core.layers.Dot\")\n+@keras_export(\"keras.layers.Dot\")\n class Dot(Merge):\n     \"\"\"Computes element-wise dot product of two tensors.\n \n@@ -215,13 +215,13 @@ class Dot(Merge):\n \n     >>> x = np.arange(10).reshape(1, 5, 2)\n     >>> y = np.arange(10, 20).reshape(1, 2, 5)\n-    >>> keras_core.layers.Dot(axes=(1, 2))([x, y])\n+    >>> keras.layers.Dot(axes=(1, 2))([x, y])\n \n     Usage in a Keras model:\n \n-    >>> x1 = keras_core.layers.Dense(8)(np.arange(10).reshape(5, 2))\n-    >>> x2 = keras_core.layers.Dense(8)(np.arange(10, 20).reshape(5, 2))\n-    >>> y = keras_core.layers.Dot(axes=1)([x1, x2])\n+    >>> x1 = keras.layers.Dense(8)(np.arange(10).reshape(5, 2))\n+    >>> x2 = keras.layers.Dense(8)(np.arange(10, 20).reshape(5, 2))\n+    >>> y = keras.layers.Dot(axes=1)([x1, x2])\n \n     Args:\n         axes: Integer or tuple of integers, axis or axes along which to\n@@ -355,7 +355,7 @@ class Dot(Merge):\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n-@keras_core_export(\"keras_core.layers.dot\")\n+@keras_export(\"keras.layers.dot\")\n def dot(inputs, axes=-1, **kwargs):\n     \"\"\"Functional interface to the `Dot` layer.\n \n\n@@ -0,0 +1,70 @@\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.layers.merging.base_merge import Merge\n+\n+\n+@keras_export(\"keras.layers.Maximum\")\n+class Maximum(Merge):\n+    \"\"\"Computes element-wise maximum on a list of inputs.\n+\n+    It takes as input a list of tensors, all of the same shape,\n+    and returns a single tensor (also of the same shape).\n+\n+    Examples:\n+\n+    >>> input_shape = (2, 3, 4)\n+    >>> x1 = np.random.rand(*input_shape)\n+    >>> x2 = np.random.rand(*input_shape)\n+    >>> y = keras.layers.Maximum()([x1, x2])\n+\n+    Usage in a Keras model:\n+\n+    >>> input1 = keras.layers.Input(shape=(16,))\n+    >>> x1 = keras.layers.Dense(8, activation='relu')(input1)\n+    >>> input2 = keras.layers.Input(shape=(32,))\n+    >>> x2 = keras.layers.Dense(8, activation='relu')(input2)\n+    >>> # equivalent to `y = keras.layers.maximum([x1, x2])`\n+    >>> y = keras.layers.Maximum()([x1, x2])\n+    >>> out = keras.layers.Dense(4)(y)\n+    >>> model = keras.models.Model(inputs=[input1, input2], outputs=out)\n+\n+    \"\"\"\n+\n+    def _merge_function(self, inputs):\n+        output = inputs[0]\n+        for i in range(1, len(inputs)):\n+            output = ops.maximum(output, inputs[i])\n+        return output\n+\n+\n+@keras_export(\"keras.layers.maximum\")\n+def maximum(inputs, **kwargs):\n+    \"\"\"Functional interface to the `keras.layers.Maximum` layer.\n+\n+    Args:\n+        inputs: A list of input tensors , all of the same shape.\n+        **kwargs: Standard layer keyword arguments.\n+\n+    Returns:\n+        A tensor as the element-wise product of the inputs with the same\n+        shape as the inputs.\n+\n+    Examples:\n+\n+    >>> input_shape = (2, 3, 4)\n+    >>> x1 = np.random.rand(*input_shape)\n+    >>> x2 = np.random.rand(*input_shape)\n+    >>> y = keras.layers.maximum([x1, x2])\n+\n+    Usage in a Keras model:\n+\n+    >>> input1 = keras.layers.Input(shape=(16,))\n+    >>> x1 = keras.layers.Dense(8, activation='relu')(input1)\n+    >>> input2 = keras.layers.Input(shape=(32,))\n+    >>> x2 = keras.layers.Dense(8, activation='relu')(input2)\n+    >>> y = keras.layers.maximum([x1, x2])\n+    >>> out = keras.layers.Dense(4)(y)\n+    >>> model = keras.models.Model(inputs=[input1, input2], outputs=out)\n+\n+    \"\"\"\n+    return Maximum(**kwargs)(inputs)\n\n@@ -2,10 +2,10 @@ import numpy as np\n import pytest\n from absl.testing import parameterized\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import models\n-from keras_core import testing\n+from keras import backend\n+from keras import layers\n+from keras import models\n+from keras import testing\n \n \n def np_dot(a, b, axes):\n\n@@ -0,0 +1,70 @@\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.layers.merging.base_merge import Merge\n+\n+\n+@keras_export(\"keras.layers.Minimum\")\n+class Minimum(Merge):\n+    \"\"\"Computes elementwise minimum on a list of inputs.\n+\n+    It takes as input a list of tensors, all of the same shape,\n+    and returns a single tensor (also of the same shape).\n+\n+    Examples:\n+\n+    >>> input_shape = (2, 3, 4)\n+    >>> x1 = np.random.rand(*input_shape)\n+    >>> x2 = np.random.rand(*input_shape)\n+    >>> y = keras.layers.Minimum()([x1, x2])\n+\n+    Usage in a Keras model:\n+\n+    >>> input1 = keras.layers.Input(shape=(16,))\n+    >>> x1 = keras.layers.Dense(8, activation='relu')(input1)\n+    >>> input2 = keras.layers.Input(shape=(32,))\n+    >>> x2 = keras.layers.Dense(8, activation='relu')(input2)\n+    >>> # equivalent to `y = keras.layers.minimum([x1, x2])`\n+    >>> y = keras.layers.Minimum()([x1, x2])\n+    >>> out = keras.layers.Dense(4)(y)\n+    >>> model = keras.models.Model(inputs=[input1, input2], outputs=out)\n+\n+    \"\"\"\n+\n+    def _merge_function(self, inputs):\n+        output = inputs[0]\n+        for i in range(1, len(inputs)):\n+            output = ops.minimum(output, inputs[i])\n+        return output\n+\n+\n+@keras_export(\"keras.layers.minimum\")\n+def minimum(inputs, **kwargs):\n+    \"\"\"Functional interface to the `keras.layers.Minimum` layer.\n+\n+    Args:\n+        inputs: A list of input tensors , all of the same shape.\n+        **kwargs: Standard layer keyword arguments.\n+\n+    Returns:\n+        A tensor as the elementwise product of the inputs with the same\n+        shape as the inputs.\n+\n+    Examples:\n+\n+    >>> input_shape = (2, 3, 4)\n+    >>> x1 = np.random.rand(*input_shape)\n+    >>> x2 = np.random.rand(*input_shape)\n+    >>> y = keras.layers.minimum([x1, x2])\n+\n+    Usage in a Keras model:\n+\n+    >>> input1 = keras.layers.Input(shape=(16,))\n+    >>> x1 = keras.layers.Dense(8, activation='relu')(input1)\n+    >>> input2 = keras.layers.Input(shape=(32,))\n+    >>> x2 = keras.layers.Dense(8, activation='relu')(input2)\n+    >>> y = keras.layers.minimum([x1, x2])\n+    >>> out = keras.layers.Dense(4)(y)\n+    >>> model = keras.models.Model(inputs=[input1, input2], outputs=out)\n+\n+    \"\"\"\n+    return Minimum(**kwargs)(inputs)\n\n@@ -0,0 +1,70 @@\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.layers.merging.base_merge import Merge\n+\n+\n+@keras_export(\"keras.layers.Multiply\")\n+class Multiply(Merge):\n+    \"\"\"Performs elementwise multiplication.\n+\n+    It takes as input a list of tensors, all of the same shape,\n+    and returns a single tensor (also of the same shape).\n+\n+    Examples:\n+\n+    >>> input_shape = (2, 3, 4)\n+    >>> x1 = np.random.rand(*input_shape)\n+    >>> x2 = np.random.rand(*input_shape)\n+    >>> y = keras.layers.Multiply()([x1, x2])\n+\n+    Usage in a Keras model:\n+\n+    >>> input1 = keras.layers.Input(shape=(16,))\n+    >>> x1 = keras.layers.Dense(8, activation='relu')(input1)\n+    >>> input2 = keras.layers.Input(shape=(32,))\n+    >>> x2 = keras.layers.Dense(8, activation='relu')(input2)\n+    >>> # equivalent to `y = keras.layers.multiply([x1, x2])`\n+    >>> y = keras.layers.Multiply()([x1, x2])\n+    >>> out = keras.layers.Dense(4)(y)\n+    >>> model = keras.models.Model(inputs=[input1, input2], outputs=out)\n+\n+    \"\"\"\n+\n+    def _merge_function(self, inputs):\n+        output = inputs[0]\n+        for i in range(1, len(inputs)):\n+            output = ops.multiply(output, inputs[i])\n+        return output\n+\n+\n+@keras_export(\"keras.layers.multiply\")\n+def multiply(inputs, **kwargs):\n+    \"\"\"Functional interface to the `keras.layers.Multiply` layer.\n+\n+    Args:\n+        inputs: A list of input tensors , all of the same shape.\n+        **kwargs: Standard layer keyword arguments.\n+\n+    Returns:\n+        A tensor as the elementwise product of the inputs with the same\n+        shape as the inputs.\n+\n+    Examples:\n+\n+    >>> input_shape = (2, 3, 4)\n+    >>> x1 = np.random.rand(*input_shape)\n+    >>> x2 = np.random.rand(*input_shape)\n+    >>> y = keras.layers.multiply([x1, x2])\n+\n+    Usage in a Keras model:\n+\n+    >>> input1 = keras.layers.Input(shape=(16,))\n+    >>> x1 = keras.layers.Dense(8, activation='relu')(input1)\n+    >>> input2 = keras.layers.Input(shape=(32,))\n+    >>> x2 = keras.layers.Dense(8, activation='relu')(input2)\n+    >>> y = keras.layers.multiply([x1, x2])\n+    >>> out = keras.layers.Dense(4)(y)\n+    >>> model = keras.models.Model(inputs=[input1, input2], outputs=out)\n+\n+    \"\"\"\n+    return Multiply(**kwargs)(inputs)\n\n@@ -1,9 +1,9 @@\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.merging.base_merge import Merge\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.layers.merging.base_merge import Merge\n \n \n-@keras_core_export(\"keras_core.layers.Subtract\")\n+@keras_export(\"keras.layers.Subtract\")\n class Subtract(Merge):\n     \"\"\"Performs elementwise subtraction.\n \n@@ -16,18 +16,18 @@ class Subtract(Merge):\n     >>> input_shape = (2, 3, 4)\n     >>> x1 = np.random.rand(*input_shape)\n     >>> x2 = np.random.rand(*input_shape)\n-    >>> y = keras_core.layers.Subtract()([x1, x2])\n+    >>> y = keras.layers.Subtract()([x1, x2])\n \n     Usage in a Keras model:\n \n-    >>> input1 = keras_core.layers.Input(shape=(16,))\n-    >>> x1 = keras_core.layers.Dense(8, activation='relu')(input1)\n-    >>> input2 = keras_core.layers.Input(shape=(32,))\n-    >>> x2 = keras_core.layers.Dense(8, activation='relu')(input2)\n-    >>> # equivalent to `subtracted = keras_core.layers.subtract([x1, x2])`\n-    >>> subtracted = keras_core.layers.Subtract()([x1, x2])\n-    >>> out = keras_core.layers.Dense(4)(subtracted)\n-    >>> model = keras_core.models.Model(inputs=[input1, input2], outputs=out)\n+    >>> input1 = keras.layers.Input(shape=(16,))\n+    >>> x1 = keras.layers.Dense(8, activation='relu')(input1)\n+    >>> input2 = keras.layers.Input(shape=(32,))\n+    >>> x2 = keras.layers.Dense(8, activation='relu')(input2)\n+    >>> # equivalent to `subtracted = keras.layers.subtract([x1, x2])`\n+    >>> subtracted = keras.layers.Subtract()([x1, x2])\n+    >>> out = keras.layers.Dense(4)(subtracted)\n+    >>> model = keras.models.Model(inputs=[input1, input2], outputs=out)\n \n     \"\"\"\n \n@@ -48,9 +48,9 @@ class Subtract(Merge):\n         return ops.subtract(inputs[0], inputs[1])\n \n \n-@keras_core_export(\"keras_core.layers.subtract\")\n+@keras_export(\"keras.layers.subtract\")\n def subtract(inputs, **kwargs):\n-    \"\"\"Functional interface to the `keras_core.layers.Subtract` layer.\n+    \"\"\"Functional interface to the `keras.layers.Subtract` layer.\n \n     Args:\n         inputs: A list of input tensors of size 2, each tensor of\n@@ -66,17 +66,17 @@ def subtract(inputs, **kwargs):\n     >>> input_shape = (2, 3, 4)\n     >>> x1 = np.random.rand(*input_shape)\n     >>> x2 = np.random.rand(*input_shape)\n-    >>> y = keras_core.layers.subtract([x1, x2])\n+    >>> y = keras.layers.subtract([x1, x2])\n \n     Usage in a Keras model:\n \n-    >>> input1 = keras_core.layers.Input(shape=(16,))\n-    >>> x1 = keras_core.layers.Dense(8, activation='relu')(input1)\n-    >>> input2 = keras_core.layers.Input(shape=(32,))\n-    >>> x2 = keras_core.layers.Dense(8, activation='relu')(input2)\n-    >>> subtracted = keras_core.layers.subtract([x1, x2])\n-    >>> out = keras_core.layers.Dense(4)(subtracted)\n-    >>> model = keras_core.models.Model(inputs=[input1, input2], outputs=out)\n+    >>> input1 = keras.layers.Input(shape=(16,))\n+    >>> x1 = keras.layers.Dense(8, activation='relu')(input1)\n+    >>> input2 = keras.layers.Input(shape=(32,))\n+    >>> x2 = keras.layers.Dense(8, activation='relu')(input2)\n+    >>> subtracted = keras.layers.subtract([x1, x2])\n+    >>> out = keras.layers.Dense(4)(subtracted)\n+    >>> model = keras.models.Model(inputs=[input1, input2], outputs=out)\n \n     \"\"\"\n     return Subtract(**kwargs)(inputs)\n\n\n@@ -1,14 +1,14 @@\n-from keras_core import constraints\n-from keras_core import initializers\n-from keras_core import ops\n-from keras_core import regularizers\n-from keras_core.api_export import keras_core_export\n-from keras_core.backend import standardize_dtype\n-from keras_core.layers.input_spec import InputSpec\n-from keras_core.layers.layer import Layer\n+from keras import constraints\n+from keras import initializers\n+from keras import ops\n+from keras import regularizers\n+from keras.api_export import keras_export\n+from keras.backend import standardize_dtype\n+from keras.layers.input_spec import InputSpec\n+from keras.layers.layer import Layer\n \n \n-@keras_core_export(\"keras_core.layers.BatchNormalization\")\n+@keras_export(\"keras.layers.BatchNormalization\")\n class BatchNormalization(Layer):\n     \"\"\"Layer that normalizes its inputs.\n \n\n@@ -2,9 +2,9 @@ import numpy as np\n import pytest\n from absl.testing import parameterized\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import testing\n+from keras import backend\n+from keras import layers\n+from keras import testing\n \n \n class BatchNormalizationTest(testing.TestCase, parameterized.TestCase):\n\n@@ -1,13 +1,13 @@\n-from keras_core import constraints\n-from keras_core import initializers\n-from keras_core import ops\n-from keras_core import regularizers\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.input_spec import InputSpec\n-from keras_core.layers.layer import Layer\n+from keras import constraints\n+from keras import initializers\n+from keras import ops\n+from keras import regularizers\n+from keras.api_export import keras_export\n+from keras.layers.input_spec import InputSpec\n+from keras.layers.layer import Layer\n \n \n-@keras_core_export(\"keras_core.layers.GroupNormalization\")\n+@keras_export(\"keras.layers.GroupNormalization\")\n class GroupNormalization(Layer):\n     \"\"\"Group normalization layer.\n \n\n@@ -1,10 +1,10 @@\n import numpy as np\n import pytest\n \n-from keras_core import constraints\n-from keras_core import layers\n-from keras_core import regularizers\n-from keras_core import testing\n+from keras import constraints\n+from keras import layers\n+from keras import regularizers\n+from keras import testing\n \n \n class GroupNormalizationTest(testing.TestCase):\n\n@@ -1,12 +1,12 @@\n-from keras_core import constraints\n-from keras_core import initializers\n-from keras_core import ops\n-from keras_core import regularizers\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n+from keras import constraints\n+from keras import initializers\n+from keras import ops\n+from keras import regularizers\n+from keras.api_export import keras_export\n+from keras.layers.layer import Layer\n \n \n-@keras_core_export(\"keras_core.layers.LayerNormalization\")\n+@keras_export(\"keras.layers.LayerNormalization\")\n class LayerNormalization(Layer):\n     \"\"\"Layer normalization layer (Ba et al., 2016).\n \n@@ -53,7 +53,7 @@ class LayerNormalization(Layer):\n \n     For example:\n \n-    >>> layer = keras_core.layers.LayerNormalization(axis=[1, 2, 3])\n+    >>> layer = keras.layers.LayerNormalization(axis=[1, 2, 3])\n     >>> layer.build([5, 20, 30, 40])\n     >>> print(layer.beta.shape)\n     (20, 30, 40)\n\n@@ -1,10 +1,10 @@\n import numpy as np\n import pytest\n \n-from keras_core import layers\n-from keras_core import ops\n-from keras_core import regularizers\n-from keras_core import testing\n+from keras import layers\n+from keras import ops\n+from keras import regularizers\n+from keras import testing\n \n \n class LayerNormalizationTest(testing.TestCase):\n\n@@ -1,12 +1,12 @@\n-from keras_core import initializers\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers import Wrapper\n-from keras_core.layers.input_spec import InputSpec\n-from keras_core.utils.numerical_utils import normalize\n+from keras import initializers\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.layers import Wrapper\n+from keras.layers.input_spec import InputSpec\n+from keras.utils.numerical_utils import normalize\n \n \n-@keras_core_export(\"keras_core.layers.SpectralNormalization\")\n+@keras_export(\"keras.layers.SpectralNormalization\")\n class SpectralNormalization(Wrapper):\n     \"\"\"Performs spectral normalization on the weights of a target layer.\n \n@@ -14,7 +14,7 @@ class SpectralNormalization(Wrapper):\n     constraining their spectral norm, which can stabilize the training of GANs.\n \n     Args:\n-        layer: A `keras_core.layers.Layer` instance that\n+        layer: A `keras.layers.Layer` instance that\n             has either a `kernel` (e.g. `Conv2D`, `Dense`...)\n             or an `embeddings` attribute (`Embedding` layer).\n         power_iterations: int, the number of iterations during normalization.\n@@ -22,16 +22,16 @@ class SpectralNormalization(Wrapper):\n \n     Examples:\n \n-    Wrap `keras_core.layers.Conv2D`:\n+    Wrap `keras.layers.Conv2D`:\n     >>> x = np.random.rand(1, 10, 10, 1)\n-    >>> conv2d = SpectralNormalization(keras_core.layers.Conv2D(2, 2))\n+    >>> conv2d = SpectralNormalization(keras.layers.Conv2D(2, 2))\n     >>> y = conv2d(x)\n     >>> y.shape\n     (1, 9, 9, 2)\n \n-    Wrap `keras_core.layers.Dense`:\n+    Wrap `keras.layers.Dense`:\n     >>> x = np.random.rand(1, 10, 10, 1)\n-    >>> dense = SpectralNormalization(keras_core.layers.Dense(10))\n+    >>> dense = SpectralNormalization(keras.layers.Dense(10))\n     >>> y = dense(x)\n     >>> y.shape\n     (1, 10, 10, 10)\n\n@@ -1,9 +1,9 @@\n import numpy as np\n import pytest\n \n-from keras_core import initializers\n-from keras_core import layers\n-from keras_core import testing\n+from keras import initializers\n+from keras import layers\n+from keras import testing\n \n \n class SpectralNormalizationTest(testing.TestCase):\n\n@@ -1,9 +1,9 @@\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.layers.layer import Layer\n \n \n-@keras_core_export(\"keras_core.layers.UnitNormalization\")\n+@keras_export(\"keras.layers.UnitNormalization\")\n class UnitNormalization(Layer):\n     \"\"\"Unit normalization layer.\n \n@@ -13,7 +13,7 @@ class UnitNormalization(Layer):\n     Example:\n \n     >>> data = np.arange(6).reshape(2, 3)\n-    >>> normalized_data = keras_core.layers.UnitNormalization()(data)\n+    >>> normalized_data = keras.layers.UnitNormalization()(data)\n     >>> print(np.sum(normalized_data[0, :] ** 2)\n     1.0\n \n\n@@ -1,9 +1,9 @@\n import numpy as np\n import pytest\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import testing\n+from keras import backend\n+from keras import layers\n+from keras import testing\n \n \n def squared_l2_norm(x):\n\n\n@@ -1,9 +1,9 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.pooling.base_pooling import BasePooling\n+from keras.api_export import keras_export\n+from keras.layers.pooling.base_pooling import BasePooling\n \n \n-@keras_core_export(\n-    [\"keras_core.layers.AveragePooling1D\", \"keras_core.layers.AvgPool1D\"]\n+@keras_export(\n+    [\"keras.layers.AveragePooling1D\", \"keras.layers.AvgPool1D\"]\n )\n class AveragePooling1D(BasePooling):\n     \"\"\"Average pooling for temporal data.\n@@ -50,7 +50,7 @@ class AveragePooling1D(BasePooling):\n \n     >>> x = np.array([1., 2., 3., 4., 5.])\n     >>> x = np.reshape(x, [1, 5, 1])\n-    >>> avg_pool_1d = keras_core.layers.AveragePooling1D(pool_size=2,\n+    >>> avg_pool_1d = keras.layers.AveragePooling1D(pool_size=2,\n     ...    strides=1, padding=\"valid\")\n     >>> avg_pool_1d(x)\n \n@@ -58,7 +58,7 @@ class AveragePooling1D(BasePooling):\n \n     >>> x = np.array([1., 2., 3., 4., 5.])\n     >>> x = np.reshape(x, [1, 5, 1])\n-    >>> avg_pool_1d = keras_core.layers.AveragePooling1D(pool_size=2,\n+    >>> avg_pool_1d = keras.layers.AveragePooling1D(pool_size=2,\n     ...    strides=2, padding=\"valid\")\n     >>> avg_pool_1d(x)\n \n@@ -66,7 +66,7 @@ class AveragePooling1D(BasePooling):\n \n     >>> x = np.array([1., 2., 3., 4., 5.])\n     >>> x = np.reshape(x, [1, 5, 1])\n-    >>> avg_pool_1d = keras_core.layers.AveragePooling1D(pool_size=2,\n+    >>> avg_pool_1d = keras.layers.AveragePooling1D(pool_size=2,\n     ...    strides=1, padding=\"same\")\n     >>> avg_pool_1d(x)\n     \"\"\"\n\n@@ -1,9 +1,9 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.pooling.base_pooling import BasePooling\n+from keras.api_export import keras_export\n+from keras.layers.pooling.base_pooling import BasePooling\n \n \n-@keras_core_export(\n-    [\"keras_core.layers.AveragePooling2D\", \"keras_core.layers.AvgPool2D\"]\n+@keras_export(\n+    [\"keras.layers.AveragePooling2D\", \"keras.layers.AvgPool2D\"]\n )\n class AveragePooling2D(BasePooling):\n     \"\"\"Average pooling operation for 2D spatial data.\n@@ -63,7 +63,7 @@ class AveragePooling2D(BasePooling):\n     ...               [4., 5., 6.],\n     ...               [7., 8., 9.]])\n     >>> x = np.reshape(x, [1, 3, 3, 1])\n-    >>> avg_pool_2d = keras_core.layers.AveragePooling2D(pool_size=(2, 2),\n+    >>> avg_pool_2d = keras.layers.AveragePooling2D(pool_size=(2, 2),\n     ...    strides=(1, 1), padding=\"valid\")\n     >>> avg_pool_2d(x)\n \n@@ -73,7 +73,7 @@ class AveragePooling2D(BasePooling):\n     ...              [5., 6., 7., 8.],\n     ...              [9., 10., 11., 12.]])\n     >>> x = np.reshape(x, [1, 3, 4, 1])\n-    >>> avg_pool_2d = keras_core.layers.AveragePooling2D(pool_size=(2, 2),\n+    >>> avg_pool_2d = keras.layers.AveragePooling2D(pool_size=(2, 2),\n     ...    strides=(2, 2), padding=\"valid\")\n     >>> avg_pool_2d(x)\n \n@@ -83,7 +83,7 @@ class AveragePooling2D(BasePooling):\n     ...                  [4., 5., 6.],\n     ...                  [7., 8., 9.]])\n     >>> x = np.reshape(x, [1, 3, 3, 1])\n-    >>> avg_pool_2d = keras_core.layers.AveragePooling2D(pool_size=(2, 2),\n+    >>> avg_pool_2d = keras.layers.AveragePooling2D(pool_size=(2, 2),\n     ...    strides=(1, 1), padding=\"same\")\n     >>> avg_pool_2d(x)\n     \"\"\"\n\n@@ -1,9 +1,9 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.pooling.base_pooling import BasePooling\n+from keras.api_export import keras_export\n+from keras.layers.pooling.base_pooling import BasePooling\n \n \n-@keras_core_export(\n-    [\"keras_core.layers.AveragePooling3D\", \"keras_core.layers.AvgPool3D\"]\n+@keras_export(\n+    [\"keras.layers.AveragePooling3D\", \"keras.layers.AvgPool3D\"]\n )\n class AveragePooling3D(BasePooling):\n     \"\"\"Average pooling operation for 3D data (spatial or spatio-temporal).\n@@ -58,8 +58,8 @@ class AveragePooling3D(BasePooling):\n     width = 30\n     channels = 3\n \n-    inputs = keras_core.layers.Input(shape=(depth, height, width, channels))\n-    layer = keras_core.layers.AveragePooling3D(pool_size=3)\n+    inputs = keras.layers.Input(shape=(depth, height, width, channels))\n+    layer = keras.layers.AveragePooling3D(pool_size=3)\n     outputs = layer(inputs)  # Shape: (batch_size, 10, 10, 10, 3)\n     ```\n     \"\"\"\n\n@@ -3,9 +3,9 @@ import pytest\n from absl.testing import parameterized\n from numpy.lib.stride_tricks import as_strided\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import testing\n+from keras import backend\n+from keras import layers\n+from keras import testing\n \n \n @pytest.mark.requires_trainable_backend\n\n@@ -1,6 +1,6 @@\n-from keras_core.backend import image_data_format\n-from keras_core.layers.input_spec import InputSpec\n-from keras_core.layers.layer import Layer\n+from keras.backend import image_data_format\n+from keras.layers.input_spec import InputSpec\n+from keras.layers.layer import Layer\n \n \n class BaseGlobalPooling(Layer):\n\n@@ -1,9 +1,9 @@\n-from keras_core import backend\n-from keras_core import ops\n-from keras_core.layers.input_spec import InputSpec\n-from keras_core.layers.layer import Layer\n-from keras_core.ops.operation_utils import compute_pooling_output_shape\n-from keras_core.utils import argument_validation\n+from keras import backend\n+from keras import ops\n+from keras.layers.input_spec import InputSpec\n+from keras.layers.layer import Layer\n+from keras.ops.operation_utils import compute_pooling_output_shape\n+from keras.utils import argument_validation\n \n \n class BasePooling(Layer):\n\n@@ -1,13 +1,13 @@\n-from keras_core import backend\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.pooling.base_global_pooling import BaseGlobalPooling\n+from keras import backend\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.layers.pooling.base_global_pooling import BaseGlobalPooling\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.layers.GlobalAveragePooling1D\",\n-        \"keras_core.layers.GlobalAvgPool1D\",\n+        \"keras.layers.GlobalAveragePooling1D\",\n+        \"keras.layers.GlobalAvgPool1D\",\n     ]\n )\n class GlobalAveragePooling1D(BaseGlobalPooling):\n@@ -54,7 +54,7 @@ class GlobalAveragePooling1D(BaseGlobalPooling):\n     Example:\n \n     >>> x = np.random.rand(2, 3, 4)\n-    >>> y = keras_core.layers.GlobalAveragePooling1D()(x)\n+    >>> y = keras.layers.GlobalAveragePooling1D()(x)\n     >>> y.shape\n     (2, 4)\n     \"\"\"\n\n@@ -1,12 +1,12 @@\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.pooling.base_global_pooling import BaseGlobalPooling\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.layers.pooling.base_global_pooling import BaseGlobalPooling\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.layers.GlobalAveragePooling2D\",\n-        \"keras_core.layers.GlobalAvgPool2D\",\n+        \"keras.layers.GlobalAveragePooling2D\",\n+        \"keras.layers.GlobalAvgPool2D\",\n     ]\n )\n class GlobalAveragePooling2D(BaseGlobalPooling):\n@@ -49,7 +49,7 @@ class GlobalAveragePooling2D(BaseGlobalPooling):\n     Example:\n \n     >>> x = np.random.rand(2, 4, 5, 3)\n-    >>> y = keras_core.layers.GlobalAveragePooling2D()(x)\n+    >>> y = keras.layers.GlobalAveragePooling2D()(x)\n     >>> y.shape\n     (2, 3)\n     \"\"\"\n\n@@ -1,12 +1,12 @@\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.pooling.base_global_pooling import BaseGlobalPooling\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.layers.pooling.base_global_pooling import BaseGlobalPooling\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.layers.GlobalAveragePooling3D\",\n-        \"keras_core.layers.GlobalAvgPool3D\",\n+        \"keras.layers.GlobalAveragePooling3D\",\n+        \"keras.layers.GlobalAvgPool3D\",\n     ]\n )\n class GlobalAveragePooling3D(BaseGlobalPooling):\n@@ -50,7 +50,7 @@ class GlobalAveragePooling3D(BaseGlobalPooling):\n     Example:\n \n     >>> x = np.random.rand(2, 4, 5, 4, 3)\n-    >>> y = keras_core.layers.GlobalAveragePooling3D()(x)\n+    >>> y = keras.layers.GlobalAveragePooling3D()(x)\n     >>> y.shape\n     (2, 3)\n     \"\"\"\n\n@@ -2,8 +2,8 @@ import numpy as np\n import pytest\n from absl.testing import parameterized\n \n-from keras_core import layers\n-from keras_core import testing\n+from keras import layers\n+from keras import testing\n \n \n @pytest.mark.requires_trainable_backend\n\n@@ -1,12 +1,12 @@\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.pooling.base_global_pooling import BaseGlobalPooling\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.layers.pooling.base_global_pooling import BaseGlobalPooling\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.layers.GlobalMaxPooling1D\",\n-        \"keras_core.layers.GlobalMaxPool1D\",\n+        \"keras.layers.GlobalMaxPooling1D\",\n+        \"keras.layers.GlobalMaxPool1D\",\n     ]\n )\n class GlobalMaxPooling1D(BaseGlobalPooling):\n@@ -48,7 +48,7 @@ class GlobalMaxPooling1D(BaseGlobalPooling):\n     Example:\n \n     >>> x = np.random.rand(2, 3, 4)\n-    >>> y = keras_core.layers.GlobalMaxPooling1D()(x)\n+    >>> y = keras.layers.GlobalMaxPooling1D()(x)\n     >>> y.shape\n     (2, 4)\n     \"\"\"\n\n@@ -1,12 +1,12 @@\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.pooling.base_global_pooling import BaseGlobalPooling\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.layers.pooling.base_global_pooling import BaseGlobalPooling\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.layers.GlobalMaxPooling2D\",\n-        \"keras_core.layers.GlobalMaxPool2D\",\n+        \"keras.layers.GlobalMaxPooling2D\",\n+        \"keras.layers.GlobalMaxPool2D\",\n     ]\n )\n class GlobalMaxPooling2D(BaseGlobalPooling):\n@@ -49,7 +49,7 @@ class GlobalMaxPooling2D(BaseGlobalPooling):\n     Example:\n \n     >>> x = np.random.rand(2, 4, 5, 3)\n-    >>> y = keras_core.layers.GlobalMaxPooling2D()(x)\n+    >>> y = keras.layers.GlobalMaxPooling2D()(x)\n     >>> y.shape\n     (2, 3)\n     \"\"\"\n\n@@ -1,12 +1,12 @@\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.pooling.base_global_pooling import BaseGlobalPooling\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.layers.pooling.base_global_pooling import BaseGlobalPooling\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.layers.GlobalMaxPooling3D\",\n-        \"keras_core.layers.GlobalMaxPool3D\",\n+        \"keras.layers.GlobalMaxPooling3D\",\n+        \"keras.layers.GlobalMaxPool3D\",\n     ]\n )\n class GlobalMaxPooling3D(BaseGlobalPooling):\n@@ -50,7 +50,7 @@ class GlobalMaxPooling3D(BaseGlobalPooling):\n     Example:\n \n     >>> x = np.random.rand(2, 4, 5, 4, 3)\n-    >>> y = keras_core.layers.GlobalMaxPooling3D()(x)\n+    >>> y = keras.layers.GlobalMaxPooling3D()(x)\n     >>> y.shape\n     (2, 3)\n     \"\"\"\n\n@@ -2,8 +2,8 @@ import numpy as np\n import pytest\n from absl.testing import parameterized\n \n-from keras_core import layers\n-from keras_core import testing\n+from keras import layers\n+from keras import testing\n \n \n @pytest.mark.requires_trainable_backend\n\n@@ -1,9 +1,9 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.pooling.base_pooling import BasePooling\n+from keras.api_export import keras_export\n+from keras.layers.pooling.base_pooling import BasePooling\n \n \n-@keras_core_export(\n-    [\"keras_core.layers.MaxPooling1D\", \"keras_core.layers.MaxPool1D\"]\n+@keras_export(\n+    [\"keras.layers.MaxPooling1D\", \"keras.layers.MaxPool1D\"]\n )\n class MaxPooling1D(BasePooling):\n     \"\"\"Max pooling operation for 1D temporal data.\n@@ -51,7 +51,7 @@ class MaxPooling1D(BasePooling):\n \n     >>> x = np.array([1., 2., 3., 4., 5.])\n     >>> x = np.reshape(x, [1, 5, 1])\n-    >>> max_pool_1d = keras_core.layers.MaxPooling1D(pool_size=2,\n+    >>> max_pool_1d = keras.layers.MaxPooling1D(pool_size=2,\n     ...    strides=1, padding=\"valid\")\n     >>> max_pool_1d(x)\n \n@@ -59,7 +59,7 @@ class MaxPooling1D(BasePooling):\n \n     >>> x = np.array([1., 2., 3., 4., 5.])\n     >>> x = np.reshape(x, [1, 5, 1])\n-    >>> max_pool_1d = keras_core.layers.MaxPooling1D(pool_size=2,\n+    >>> max_pool_1d = keras.layers.MaxPooling1D(pool_size=2,\n     ...    strides=2, padding=\"valid\")\n     >>> max_pool_1d(x)\n \n@@ -67,7 +67,7 @@ class MaxPooling1D(BasePooling):\n \n     >>> x = np.array([1., 2., 3., 4., 5.])\n     >>> x = np.reshape(x, [1, 5, 1])\n-    >>> max_pool_1d = keras_core.layers.MaxPooling1D(pool_size=2,\n+    >>> max_pool_1d = keras.layers.MaxPooling1D(pool_size=2,\n     ...    strides=1, padding=\"same\")\n     >>> max_pool_1d(x)\n     \"\"\"\n\n@@ -1,9 +1,9 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.pooling.base_pooling import BasePooling\n+from keras.api_export import keras_export\n+from keras.layers.pooling.base_pooling import BasePooling\n \n \n-@keras_core_export(\n-    [\"keras_core.layers.MaxPooling2D\", \"keras_core.layers.MaxPool2D\"]\n+@keras_export(\n+    [\"keras.layers.MaxPooling2D\", \"keras.layers.MaxPool2D\"]\n )\n class MaxPooling2D(BasePooling):\n     \"\"\"Max pooling operation for 2D spatial data.\n@@ -63,7 +63,7 @@ class MaxPooling2D(BasePooling):\n     ...               [4., 5., 6.],\n     ...               [7., 8., 9.]])\n     >>> x = np.reshape(x, [1, 3, 3, 1])\n-    >>> max_pool_2d = keras_core.layers.MaxPooling2D(pool_size=(2, 2),\n+    >>> max_pool_2d = keras.layers.MaxPooling2D(pool_size=(2, 2),\n     ...    strides=(1, 1), padding=\"valid\")\n     >>> max_pool_2d(x)\n \n@@ -73,7 +73,7 @@ class MaxPooling2D(BasePooling):\n     ...               [5., 6., 7., 8.],\n     ...               [9., 10., 11., 12.]])\n     >>> x = np.reshape(x, [1, 3, 4, 1])\n-    >>> max_pool_2d = keras_core.layers.MaxPooling2D(pool_size=(2, 2),\n+    >>> max_pool_2d = keras.layers.MaxPooling2D(pool_size=(2, 2),\n     ...    strides=(2, 2), padding=\"valid\")\n     >>> max_pool_2d(x)\n \n@@ -83,7 +83,7 @@ class MaxPooling2D(BasePooling):\n     ...               [4., 5., 6.],\n     ...               [7., 8., 9.]])\n     >>> x = np.reshape(x, [1, 3, 3, 1])\n-    >>> max_pool_2d = keras_core.layers.MaxPooling2D(pool_size=(2, 2),\n+    >>> max_pool_2d = keras.layers.MaxPooling2D(pool_size=(2, 2),\n     ...    strides=(1, 1), padding=\"same\")\n     >>> max_pool_2d(x)\n     \"\"\"\n\n@@ -1,9 +1,9 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.pooling.base_pooling import BasePooling\n+from keras.api_export import keras_export\n+from keras.layers.pooling.base_pooling import BasePooling\n \n \n-@keras_core_export(\n-    [\"keras_core.layers.MaxPooling3D\", \"keras_core.layers.MaxPool3D\"]\n+@keras_export(\n+    [\"keras.layers.MaxPooling3D\", \"keras.layers.MaxPool3D\"]\n )\n class MaxPooling3D(BasePooling):\n     \"\"\"Max pooling operation for 3D data (spatial or spatio-temporal).\n@@ -58,8 +58,8 @@ class MaxPooling3D(BasePooling):\n     width = 30\n     channels = 3\n \n-    inputs = keras_core.layers.Input(shape=(depth, height, width, channels))\n-    layer = keras_core.layers.MaxPooling3D(pool_size=3)\n+    inputs = keras.layers.Input(shape=(depth, height, width, channels))\n+    layer = keras.layers.MaxPooling3D(pool_size=3)\n     outputs = layer(inputs)  # Shape: (batch_size, 10, 10, 10, 3)\n     ```\n     \"\"\"\n\n@@ -3,8 +3,8 @@ import pytest\n from absl.testing import parameterized\n from numpy.lib.stride_tricks import as_strided\n \n-from keras_core import layers\n-from keras_core import testing\n+from keras import layers\n+from keras import testing\n \n \n @pytest.mark.requires_trainable_backend\n\n\n@@ -1,10 +1,10 @@\n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n-from keras_core.utils import backend_utils\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.layers.preprocessing.tf_data_layer import TFDataLayer\n+from keras.utils import backend_utils\n \n \n-@keras_core_export(\"keras_core.layers.CategoryEncoding\")\n+@keras_export(\"keras.layers.CategoryEncoding\")\n class CategoryEncoding(TFDataLayer):\n     \"\"\"A preprocessing layer which encodes integer features.\n \n@@ -12,7 +12,7 @@ class CategoryEncoding(TFDataLayer):\n     when the total number of tokens are known in advance. It accepts integer\n     values as inputs, and it outputs a dense or sparse representation of those\n     inputs. For integer inputs where the total number of tokens is not known,\n-    use `keras_core.layers.IntegerLookup` instead.\n+    use `keras.layers.IntegerLookup` instead.\n \n     **Note:** This layer is safe to use inside a `tf.data` pipeline\n     (independently of which backend you're using).\n@@ -21,7 +21,7 @@ class CategoryEncoding(TFDataLayer):\n \n     **One-hot encoding data**\n \n-    >>> layer = keras_core.layers.CategoryEncoding(\n+    >>> layer = keras.layers.CategoryEncoding(\n     ...           num_tokens=4, output_mode=\"one_hot\")\n     >>> layer([3, 2, 0, 1])\n     array([[0., 0., 0., 1.],\n@@ -31,7 +31,7 @@ class CategoryEncoding(TFDataLayer):\n \n     **Multi-hot encoding data**\n \n-    >>> layer = keras_core.layers.CategoryEncoding(\n+    >>> layer = keras.layers.CategoryEncoding(\n     ...           num_tokens=4, output_mode=\"multi_hot\")\n     >>> layer([[0, 1], [0, 0], [1, 2], [3, 1]])\n     array([[1., 1., 0., 0.],\n@@ -41,7 +41,7 @@ class CategoryEncoding(TFDataLayer):\n \n     **Using weighted inputs in `\"count\"` mode**\n \n-    >>> layer = keras_core.layers.CategoryEncoding(\n+    >>> layer = keras.layers.CategoryEncoding(\n     ...           num_tokens=4, output_mode=\"count\")\n     >>> count_weights = np.array([[.1, .2], [.1, .1], [.2, .3], [.4, .2]])\n     >>> layer([[0, 1], [0, 0], [1, 2], [3, 1]], count_weights=count_weights)\n\n@@ -1,8 +1,8 @@\n import numpy as np\n from tensorflow import data as tf_data\n \n-from keras_core import layers\n-from keras_core import testing\n+from keras import layers\n+from keras import testing\n \n \n class CategoryEncodingTest(testing.TestCase):\n\n@@ -1,10 +1,10 @@\n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n-from keras_core.utils import image_utils\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.layers.preprocessing.tf_data_layer import TFDataLayer\n+from keras.utils import image_utils\n \n \n-@keras_core_export(\"keras_core.layers.CenterCrop\")\n+@keras_export(\"keras.layers.CenterCrop\")\n class CenterCrop(TFDataLayer):\n     \"\"\"A preprocessing layer which crops images.\n \n\n@@ -3,8 +3,8 @@ import pytest\n from absl.testing import parameterized\n from tensorflow import data as tf_data\n \n-from keras_core import layers\n-from keras_core import testing\n+from keras import layers\n+from keras import testing\n \n \n class CenterCropTest(testing.TestCase, parameterized.TestCase):\n\n@@ -1,14 +1,14 @@\n import numpy as np\n \n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n-from keras_core.utils import argument_validation\n-from keras_core.utils import numerical_utils\n-from keras_core.utils.module_utils import tensorflow as tf\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.layers.preprocessing.tf_data_layer import TFDataLayer\n+from keras.utils import argument_validation\n+from keras.utils import numerical_utils\n+from keras.utils.module_utils import tensorflow as tf\n \n \n-@keras_core_export(\"keras_core.layers.Discretization\")\n+@keras_export(\"keras.layers.Discretization\")\n class Discretization(TFDataLayer):\n     \"\"\"A preprocessing layer which buckets continuous features by ranges.\n \n\n@@ -3,11 +3,11 @@ import os\n import numpy as np\n from tensorflow import data as tf_data\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import models\n-from keras_core import testing\n-from keras_core.saving import saving_api\n+from keras import backend\n+from keras import layers\n+from keras import models\n+from keras import testing\n+from keras.saving import saving_api\n \n \n class DicretizationTest(testing.TestCase):\n\n@@ -1,14 +1,14 @@\n import tree\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n-from keras_core.saving import saving_lib\n-from keras_core.saving import serialization_lib\n-from keras_core.utils import backend_utils\n-from keras_core.utils.module_utils import tensorflow as tf\n-from keras_core.utils.naming import auto_name\n+from keras import backend\n+from keras import layers\n+from keras.api_export import keras_export\n+from keras.layers.layer import Layer\n+from keras.saving import saving_lib\n+from keras.saving import serialization_lib\n+from keras.utils import backend_utils\n+from keras.utils.module_utils import tensorflow as tf\n+from keras.utils.naming import auto_name\n \n \n class Cross:\n@@ -69,7 +69,7 @@ class Feature:\n         return cls(**config)\n \n \n-@keras_core_export(\"keras_core.utils.FeatureSpace\")\n+@keras_export(\"keras.utils.FeatureSpace\")\n class FeatureSpace(Layer):\n     \"\"\"One-stop utility for preprocessing and encoding structured data.\n \n@@ -106,11 +106,11 @@ class FeatureSpace(Layer):\n     FeatureSpace.float(name=None)\n \n     # Float values to be preprocessed via featurewise standardization\n-    # (i.e. via a `keras_core.layers.Normalization` layer).\n+    # (i.e. via a `keras.layers.Normalization` layer).\n     FeatureSpace.float_normalized(name=None)\n \n     # Float values to be preprocessed via linear rescaling\n-    # (i.e. via a `keras_core.layers.Rescaling` layer).\n+    # (i.e. via a `keras.layers.Rescaling` layer).\n     FeatureSpace.float_rescaled(scale=1., offset=0., name=None)\n \n     # Float values to be discretized. By default, the discrete\n@@ -184,8 +184,8 @@ class FeatureSpace(Layer):\n     # Retrieve the corresponding encoded Keras tensors\n     encoded_features = feature_space.get_encoded_features()\n     # Build a Functional model\n-    outputs = keras_core.layers.Dense(1, activation=\"sigmoid\")(encoded_features)\n-    model = keras_core.Model(inputs, outputs)\n+    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(encoded_features)\n+    model = keras.Model(inputs, outputs)\n     ```\n \n     **Customizing each feature or feature cross:**\n@@ -256,7 +256,7 @@ class FeatureSpace(Layer):\n     preprocessing_layer = feature_space.preprocessors[\"feature1\"]\n \n     # The crossing layer of each feature cross is available in `.crossers`.\n-    # It's an instance of keras_core.layers.HashedCrossing.\n+    # It's an instance of keras.layers.HashedCrossing.\n     crossing_layer = feature_space.crossers[\"feature1_X_feature2\"]\n     ```\n \n@@ -264,7 +264,7 @@ class FeatureSpace(Layer):\n \n     ```python\n     feature_space.save(\"featurespace.keras\")\n-    reloaded_feature_space = keras_core.models.load_model(\"featurespace.keras\")\n+    reloaded_feature_space = keras.models.load_model(\"featurespace.keras\")\n     ```\n     \"\"\"\n \n@@ -278,7 +278,7 @@ class FeatureSpace(Layer):\n \n     @classmethod\n     def float(cls, name=None):\n-        from keras_core.layers.core import identity\n+        from keras.layers.core import identity\n \n         name = name or auto_name(\"float\")\n         preprocessor = identity.Identity(\n@@ -781,11 +781,11 @@ class FeatureSpace(Layer):\n     def save(self, filepath):\n         \"\"\"Save the `FeatureSpace` instance to a `.keras` file.\n \n-        You can reload it via `keras_core.models.load_model()`:\n+        You can reload it via `keras.models.load_model()`:\n \n         ```python\n         feature_space.save(\"featurespace.keras\")\n-        reloaded_fs = keras_core.models.load_model(\"featurespace.keras\")\n+        reloaded_fs = keras.models.load_model(\"featurespace.keras\")\n         ```\n         \"\"\"\n         saving_lib.save_model(self, filepath)\n\n@@ -3,13 +3,13 @@ import os\n import pytest\n from tensorflow import data as tf_data\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import models\n-from keras_core import ops\n-from keras_core import testing\n-from keras_core.layers.preprocessing import feature_space\n-from keras_core.saving import saving_api\n+from keras import backend\n+from keras import layers\n+from keras import models\n+from keras import ops\n+from keras import testing\n+from keras.layers.preprocessing import feature_space\n+from keras.saving import saving_api\n \n \n class FeatureSpaceTest(testing.TestCase):\n\n@@ -1,13 +1,13 @@\n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n-from keras_core.utils import argument_validation\n-from keras_core.utils import backend_utils\n-from keras_core.utils import tf_utils\n-from keras_core.utils.module_utils import tensorflow as tf\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.layers.layer import Layer\n+from keras.utils import argument_validation\n+from keras.utils import backend_utils\n+from keras.utils import tf_utils\n+from keras.utils.module_utils import tensorflow as tf\n \n \n-@keras_core_export(\"keras_core.layers.HashedCrossing\")\n+@keras_export(\"keras.layers.HashedCrossing\")\n class HashedCrossing(Layer):\n     \"\"\"A preprocessing layer which crosses features using the \"hashing trick\".\n \n@@ -47,7 +47,7 @@ class HashedCrossing(Layer):\n \n     **Crossing two scalar features.**\n \n-    >>> layer = keras_core.layers.HashedCrossing(\n+    >>> layer = keras.layers.HashedCrossing(\n     ...     num_bins=5)\n     >>> feat1 = np.array(['A', 'B', 'A', 'B', 'A'])\n     >>> feat2 = np.array([101, 101, 101, 102, 102])\n@@ -56,7 +56,7 @@ class HashedCrossing(Layer):\n \n     **Crossing and one-hotting two scalar features.**\n \n-    >>> layer = keras_core.layers.HashedCrossing(\n+    >>> layer = keras.layers.HashedCrossing(\n     ...     num_bins=5, output_mode='one_hot')\n     >>> feat1 = np.array(['A', 'B', 'A', 'B', 'A'])\n     >>> feat2 = np.array([101, 101, 101, 102, 102])\n\n@@ -2,9 +2,9 @@ import numpy as np\n import pytest\n import tensorflow as tf\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import testing\n+from keras import backend\n+from keras import layers\n+from keras import testing\n \n \n class HashedCrossingTest(testing.TestCase):\n\n@@ -1,12 +1,12 @@\n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n-from keras_core.utils import backend_utils\n-from keras_core.utils import tf_utils\n-from keras_core.utils.module_utils import tensorflow as tf\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.layers.layer import Layer\n+from keras.utils import backend_utils\n+from keras.utils import tf_utils\n+from keras.utils.module_utils import tensorflow as tf\n \n \n-@keras_core_export(\"keras_core.layers.Hashing\")\n+@keras_export(\"keras.layers.Hashing\")\n class Hashing(Layer):\n     \"\"\"A preprocessing layer which hashes and bins categorical features.\n \n@@ -38,7 +38,7 @@ class Hashing(Layer):\n \n     **Example (FarmHash64)**\n \n-    >>> layer = keras_core.layers.Hashing(num_bins=3)\n+    >>> layer = keras.layers.Hashing(num_bins=3)\n     >>> inp = [['A'], ['B'], ['C'], ['D'], ['E']]\n     >>> layer(inp)\n     array([[1],\n@@ -49,7 +49,7 @@ class Hashing(Layer):\n \n     **Example (FarmHash64) with a mask value**\n \n-    >>> layer = keras_core.layers.Hashing(num_bins=3, mask_value='')\n+    >>> layer = keras.layers.Hashing(num_bins=3, mask_value='')\n     >>> inp = [['A'], ['B'], [''], ['C'], ['D']]\n     >>> layer(inp)\n     array([[1],\n@@ -60,7 +60,7 @@ class Hashing(Layer):\n \n     **Example (SipHash64)**\n \n-    >>> layer = keras_core.layers.Hashing(num_bins=3, salt=[133, 137])\n+    >>> layer = keras.layers.Hashing(num_bins=3, salt=[133, 137])\n     >>> inp = [['A'], ['B'], ['C'], ['D'], ['E']]\n     >>> layer(inp)\n     array([[1],\n@@ -71,7 +71,7 @@ class Hashing(Layer):\n \n     **Example (Siphash64 with a single integer, same as `salt=[133, 133]`)**\n \n-    >>> layer = keras_core.layers.Hashing(num_bins=3, salt=133)\n+    >>> layer = keras.layers.Hashing(num_bins=3, salt=133)\n     >>> inp = [['A'], ['B'], ['C'], ['D'], ['E']]\n     >>> layer(inp)\n     array([[0],\n\n@@ -5,11 +5,11 @@ import pytest\n import tensorflow as tf\n from absl.testing import parameterized\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import models\n-from keras_core import testing\n-from keras_core.saving import load_model\n+from keras import backend\n+from keras import layers\n+from keras import models\n+from keras import testing\n+from keras.saving import load_model\n \n \n class ArrayLike:\n\n@@ -2,11 +2,11 @@ import collections\n \n import numpy as np\n \n-from keras_core import backend\n-from keras_core.layers.layer import Layer\n-from keras_core.utils import argument_validation\n-from keras_core.utils import tf_utils\n-from keras_core.utils.module_utils import tensorflow as tf\n+from keras import backend\n+from keras.layers.layer import Layer\n+from keras.utils import argument_validation\n+from keras.utils import tf_utils\n+from keras.utils.module_utils import tensorflow as tf\n \n \n class IndexLookup(Layer):\n\n@@ -5,11 +5,11 @@ import pytest\n from absl.testing import parameterized\n from tensorflow import data as tf_data\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import models\n-from keras_core import testing\n-from keras_core.saving import saving_api\n+from keras import backend\n+from keras import layers\n+from keras import models\n+from keras import testing\n+from keras.saving import saving_api\n \n \n @pytest.mark.skipif(\n\n@@ -1,13 +1,13 @@\n import numpy as np\n \n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.preprocessing.index_lookup import IndexLookup\n-from keras_core.utils import backend_utils\n-from keras_core.utils.module_utils import tensorflow as tf\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.layers.preprocessing.index_lookup import IndexLookup\n+from keras.utils import backend_utils\n+from keras.utils.module_utils import tensorflow as tf\n \n \n-@keras_core_export(\"keras_core.layers.IntegerLookup\")\n+@keras_export(\"keras.layers.IntegerLookup\")\n class IntegerLookup(IndexLookup):\n     \"\"\"A preprocessing layer that maps integers to (possibly encoded) indices.\n \n\n@@ -1,9 +1,9 @@\n import numpy as np\n from tensorflow import data as tf_data\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import testing\n+from keras import backend\n+from keras import layers\n+from keras import testing\n \n \n class IntegerLookupTest(testing.TestCase):\n\n@@ -2,14 +2,14 @@ import math\n \n import numpy as np\n \n-from keras_core import backend\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n-from keras_core.utils.module_utils import tensorflow as tf\n+from keras import backend\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.layers.layer import Layer\n+from keras.utils.module_utils import tensorflow as tf\n \n \n-@keras_core_export(\"keras_core.layers.Normalization\")\n+@keras_export(\"keras.layers.Normalization\")\n class Normalization(Layer):\n     \"\"\"A preprocessing layer that normalizes continuous features.\n \n@@ -54,7 +54,7 @@ class Normalization(Layer):\n \n     >>> adapt_data = np.array([1., 2., 3., 4., 5.], dtype='float32')\n     >>> input_data = np.array([1., 2., 3.], dtype='float32')\n-    >>> layer = keras_core.layers.Normalization(axis=None)\n+    >>> layer = keras.layers.Normalization(axis=None)\n     >>> layer.adapt(adapt_data)\n     >>> layer(input_data)\n     array([-1.4142135, -0.70710677, 0.], dtype=float32)\n@@ -66,7 +66,7 @@ class Normalization(Layer):\n     ...                        [0., 7., 4.],\n     ...                        [2., 9., 6.]], dtype='float32')\n     >>> input_data = np.array([[0., 7., 4.]], dtype='float32')\n-    >>> layer = keras_core.layers.Normalization(axis=-1)\n+    >>> layer = keras.layers.Normalization(axis=-1)\n     >>> layer.adapt(adapt_data)\n     >>> layer(input_data)\n     array([-1., -1., -1.], dtype=float32)\n@@ -74,7 +74,7 @@ class Normalization(Layer):\n     Pass the mean and variance directly.\n \n     >>> input_data = np.array([[1.], [2.], [3.]], dtype='float32')\n-    >>> layer = keras_core.layers.Normalization(mean=3., variance=2.)\n+    >>> layer = keras.layers.Normalization(mean=3., variance=2.)\n     >>> layer(input_data)\n     array([[-1.4142135 ],\n            [-0.70710677],\n@@ -87,7 +87,7 @@ class Normalization(Layer):\n     ...                        [0., 7., 4.],\n     ...                        [2., 9., 6.]], dtype='float32')\n     >>> input_data = np.array([[1., 2., 3.]], dtype='float32')\n-    >>> layer = keras_core.layers.Normalization(axis=-1, invert=True)\n+    >>> layer = keras.layers.Normalization(axis=-1, invert=True)\n     >>> layer.adapt(adapt_data)\n     >>> layer(input_data)\n     array([2., 10., 8.], dtype=float32)\n\n@@ -3,9 +3,9 @@ import pytest\n from absl.testing import parameterized\n from tensorflow import data as tf_data\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import testing\n+from keras import backend\n+from keras import layers\n+from keras import testing\n \n \n class NormalizationTest(testing.TestCase, parameterized.TestCase):\n@@ -100,7 +100,7 @@ class NormalizationTest(testing.TestCase, parameterized.TestCase):\n         reason=\"Test symbolic call for torch meta device.\",\n     )\n     def test_call_on_meta_device_after_built(self):\n-        from keras_core.backend.torch import core\n+        from keras.backend.torch import core\n \n         layer = layers.Normalization()\n         data = np.random.random((32, 4))\n\n@@ -1,9 +1,9 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n-from keras_core.random.seed_generator import SeedGenerator\n+from keras.api_export import keras_export\n+from keras.layers.preprocessing.tf_data_layer import TFDataLayer\n+from keras.random.seed_generator import SeedGenerator\n \n \n-@keras_core_export(\"keras_core.layers.RandomBrightness\")\n+@keras_export(\"keras.layers.RandomBrightness\")\n class RandomBrightness(TFDataLayer):\n     \"\"\"A preprocessing layer which randomly adjusts brightness during training.\n \n@@ -44,7 +44,7 @@ class RandomBrightness(TFDataLayer):\n     Sample usage:\n \n     ```python\n-    random_bright = keras_core.layers.RandomBrightness(factor=0.2)\n+    random_bright = keras.layers.RandomBrightness(factor=0.2)\n \n     # An image with shape [2, 2, 3]\n     image = [[[1, 2, 3], [4 ,5 ,6]], [[7, 8, 9], [10, 11, 12]]]\n\n@@ -2,9 +2,9 @@ import numpy as np\n import pytest\n from tensorflow import data as tf_data\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import testing\n+from keras import backend\n+from keras import layers\n+from keras import testing\n \n \n class RandomBrightnessTest(testing.TestCase):\n\n@@ -1,9 +1,9 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n-from keras_core.random.seed_generator import SeedGenerator\n+from keras.api_export import keras_export\n+from keras.layers.preprocessing.tf_data_layer import TFDataLayer\n+from keras.random.seed_generator import SeedGenerator\n \n \n-@keras_core_export(\"keras_core.layers.RandomContrast\")\n+@keras_export(\"keras.layers.RandomContrast\")\n class RandomContrast(TFDataLayer):\n     \"\"\"A preprocessing layer which randomly adjusts contrast during training.\n \n\n@@ -2,9 +2,9 @@ import numpy as np\n import pytest\n from tensorflow import data as tf_data\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import testing\n+from keras import backend\n+from keras import layers\n+from keras import testing\n \n \n class RandomContrastTest(testing.TestCase):\n\n@@ -1,11 +1,11 @@\n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n-from keras_core.random.seed_generator import SeedGenerator\n-from keras_core.utils import image_utils\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.layers.preprocessing.tf_data_layer import TFDataLayer\n+from keras.random.seed_generator import SeedGenerator\n+from keras.utils import image_utils\n \n \n-@keras_core_export(\"keras_core.layers.RandomCrop\")\n+@keras_export(\"keras.layers.RandomCrop\")\n class RandomCrop(TFDataLayer):\n     \"\"\"A preprocessing layer which randomly crops images during training.\n \n\n@@ -1,8 +1,8 @@\n import numpy as np\n from tensorflow import data as tf_data\n \n-from keras_core import layers\n-from keras_core import testing\n+from keras import layers\n+from keras import testing\n \n \n class RandomCropTest(testing.TestCase):\n\n@@ -1,13 +1,13 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n-from keras_core.random.seed_generator import SeedGenerator\n+from keras.api_export import keras_export\n+from keras.layers.preprocessing.tf_data_layer import TFDataLayer\n+from keras.random.seed_generator import SeedGenerator\n \n HORIZONTAL = \"horizontal\"\n VERTICAL = \"vertical\"\n HORIZONTAL_AND_VERTICAL = \"horizontal_and_vertical\"\n \n \n-@keras_core_export(\"keras_core.layers.RandomFlip\")\n+@keras_export(\"keras.layers.RandomFlip\")\n class RandomFlip(TFDataLayer):\n     \"\"\"A preprocessing layer which randomly flips images during training.\n \n\n@@ -4,10 +4,10 @@ import numpy as np\n from absl.testing import parameterized\n from tensorflow import data as tf_data\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import testing\n-from keras_core import utils\n+from keras import backend\n+from keras import layers\n+from keras import testing\n+from keras import utils\n \n \n class MockedRandomFlip(layers.RandomFlip):\n\n@@ -1,12 +1,12 @@\n import numpy as np\n \n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n-from keras_core.random.seed_generator import SeedGenerator\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.layers.preprocessing.tf_data_layer import TFDataLayer\n+from keras.random.seed_generator import SeedGenerator\n \n \n-@keras_core_export(\"keras_core.layers.RandomRotation\")\n+@keras_export(\"keras.layers.RandomRotation\")\n class RandomRotation(TFDataLayer):\n     \"\"\"A preprocessing layer which randomly rotates images during training.\n \n\n@@ -2,9 +2,9 @@ import numpy as np\n from absl.testing import parameterized\n from tensorflow import data as tf_data\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import testing\n+from keras import backend\n+from keras import layers\n+from keras import testing\n \n \n class RandomRotationTest(testing.TestCase, parameterized.TestCase):\n\n@@ -1,10 +1,10 @@\n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n-from keras_core.random.seed_generator import SeedGenerator\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.layers.preprocessing.tf_data_layer import TFDataLayer\n+from keras.random.seed_generator import SeedGenerator\n \n \n-@keras_core_export(\"keras_core.layers.RandomTranslation\")\n+@keras_export(\"keras.layers.RandomTranslation\")\n class RandomTranslation(TFDataLayer):\n     \"\"\"A preprocessing layer which randomly translates images during training.\n \n\n@@ -2,9 +2,9 @@ import numpy as np\n from absl.testing import parameterized\n from tensorflow import data as tf_data\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import testing\n+from keras import backend\n+from keras import layers\n+from keras import testing\n \n \n class RandomTranslationTest(testing.TestCase, parameterized.TestCase):\n\n@@ -1,10 +1,10 @@\n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n-from keras_core.random.seed_generator import SeedGenerator\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.layers.preprocessing.tf_data_layer import TFDataLayer\n+from keras.random.seed_generator import SeedGenerator\n \n \n-@keras_core_export(\"keras_core.layers.RandomZoom\")\n+@keras_export(\"keras.layers.RandomZoom\")\n class RandomZoom(TFDataLayer):\n     \"\"\"A preprocessing layer which randomly zooms images during training.\n \n@@ -83,7 +83,7 @@ class RandomZoom(TFDataLayer):\n     Example:\n \n     >>> input_img = np.random.random((32, 224, 224, 3))\n-    >>> layer = keras_core.layers.RandomZoom(.5, .2)\n+    >>> layer = keras.layers.RandomZoom(.5, .2)\n     >>> out_img = layer(input_img)\n     \"\"\"\n \n\n@@ -2,9 +2,9 @@ import numpy as np\n from absl.testing import parameterized\n from tensorflow import data as tf_data\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import testing\n+from keras import backend\n+from keras import layers\n+from keras import testing\n \n \n class RandomZoomTest(testing.TestCase, parameterized.TestCase):\n\n@@ -1,9 +1,9 @@\n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.layers.preprocessing.tf_data_layer import TFDataLayer\n \n \n-@keras_core_export(\"keras_core.layers.Rescaling\")\n+@keras_export(\"keras.layers.Rescaling\")\n class Rescaling(TFDataLayer):\n     \"\"\"A preprocessing layer which rescales input values to a new range.\n \n\n@@ -2,9 +2,9 @@ import numpy as np\n import pytest\n from tensorflow import data as tf_data\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import testing\n+from keras import backend\n+from keras import layers\n+from keras import testing\n \n \n class RescalingTest(testing.TestCase):\n\n@@ -1,10 +1,10 @@\n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n-from keras_core.utils import image_utils\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.layers.preprocessing.tf_data_layer import TFDataLayer\n+from keras.utils import image_utils\n \n \n-@keras_core_export(\"keras_core.layers.Resizing\")\n+@keras_export(\"keras.layers.Resizing\")\n class Resizing(TFDataLayer):\n     \"\"\"A preprocessing layer which resizes images.\n \n\n@@ -3,10 +3,10 @@ import pytest\n from absl.testing import parameterized\n from tensorflow import data as tf_data\n \n-from keras_core import Sequential\n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import testing\n+from keras import Sequential\n+from keras import backend\n+from keras import layers\n+from keras import testing\n \n \n class ResizingTest(testing.TestCase, parameterized.TestCase):\n@@ -190,7 +190,7 @@ class ResizingTest(testing.TestCase, parameterized.TestCase):\n     )\n     def test_tf_data_compatibility_sequential(self):\n         # Test compatibility when wrapping in a Sequential\n-        # https://github.com/keras-team/keras-core/issues/347\n+        # https://github.com/keras-team/keras/issues/347\n         layer = layers.Resizing(8, 9)\n         input_data = np.random.random((2, 10, 12, 3))\n         ds = (\n\n@@ -1,20 +1,20 @@\n import numpy as np\n \n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.preprocessing.index_lookup import IndexLookup\n-from keras_core.utils import backend_utils\n-from keras_core.utils.module_utils import tensorflow as tf\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.layers.preprocessing.index_lookup import IndexLookup\n+from keras.utils import backend_utils\n+from keras.utils.module_utils import tensorflow as tf\n \n \n-@keras_core_export(\"keras_core.layers.StringLookup\")\n+@keras_export(\"keras.layers.StringLookup\")\n class StringLookup(IndexLookup):\n     \"\"\"A preprocessing layer that maps strings to (possibly encoded) indices.\n \n     This layer translates a set of arbitrary strings into integer output via a\n     table-based vocabulary lookup. This layer will perform no splitting or\n     transformation of input strings. For a layer than can split and tokenize\n-    natural language, see the `keras_core.layers.TextVectorization` layer.\n+    natural language, see the `keras.layers.TextVectorization` layer.\n \n     The vocabulary for the layer must be either supplied on construction or\n     learned via `adapt()`. During `adapt()`, the layer will analyze a data set,\n\n@@ -1,9 +1,9 @@\n import numpy as np\n from tensorflow import data as tf_data\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import testing\n+from keras import backend\n+from keras import layers\n+from keras import testing\n \n \n class StringLookupTest(testing.TestCase):\n\n@@ -1,18 +1,18 @@\n import numpy as np\n \n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n-from keras_core.layers.preprocessing.index_lookup import listify_tensors\n-from keras_core.layers.preprocessing.string_lookup import StringLookup\n-from keras_core.saving import serialization_lib\n-from keras_core.utils import argument_validation\n-from keras_core.utils import backend_utils\n-from keras_core.utils import tf_utils\n-from keras_core.utils.module_utils import tensorflow as tf\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.layers.layer import Layer\n+from keras.layers.preprocessing.index_lookup import listify_tensors\n+from keras.layers.preprocessing.string_lookup import StringLookup\n+from keras.saving import serialization_lib\n+from keras.utils import argument_validation\n+from keras.utils import backend_utils\n+from keras.utils import tf_utils\n+from keras.utils.module_utils import tensorflow as tf\n \n \n-@keras_core_export(\"keras_core.layers.TextVectorization\")\n+@keras_export(\"keras.layers.TextVectorization\")\n class TextVectorization(Layer):\n     \"\"\"A preprocessing layer which maps text features to integer sequences.\n \n@@ -46,7 +46,7 @@ class TextVectorization(Layer):\n \n     1. Any callable can be passed to this Layer, but if you want to serialize\n        this object you should only pass functions that are registered Keras\n-       serializables (see `keras_core.saving.register_keras_serializable`\n+       serializables (see `keras.saving.register_keras_serializable`\n        for more details).\n     2. When using a custom callable for `standardize`, the data received\n        by the callable will be exactly as passed to this layer. The callable\n@@ -186,7 +186,7 @@ class TextVectorization(Layer):\n     >>> # Create the layer, passing the vocab directly. You can also pass the\n     >>> # vocabulary arg a path to a file containing one vocabulary word per\n     >>> # line.\n-    >>> vectorize_layer = keras_core.layers.TextVectorization(\n+    >>> vectorize_layer = keras.layers.TextVectorization(\n     ...     max_tokens=max_tokens,\n     ...     output_mode='int',\n     ...     output_sequence_length=max_len,\n\n@@ -2,10 +2,10 @@ import numpy as np\n import pytest\n from tensorflow import data as tf_data\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import models\n-from keras_core import testing\n+from keras import backend\n+from keras import layers\n+from keras import models\n+from keras import testing\n \n \n class TextVectorizationTest(testing.TestCase):\n\n@@ -1,10 +1,10 @@\n import tree\n \n-import keras_core.backend\n-from keras_core.layers.layer import Layer\n-from keras_core.random.seed_generator import SeedGenerator\n-from keras_core.utils import backend_utils\n-from keras_core.utils import tracking\n+import keras.backend\n+from keras.layers.layer import Layer\n+from keras.random.seed_generator import SeedGenerator\n+from keras.utils import backend_utils\n+from keras.utils import tracking\n \n \n class TFDataLayer(Layer):\n@@ -22,7 +22,7 @@ class TFDataLayer(Layer):\n \n     def __call__(self, inputs, **kwargs):\n         if backend_utils.in_tf_graph() and not isinstance(\n-            inputs, keras_core.KerasTensor\n+            inputs, keras.KerasTensor\n         ):\n             # We're in a TF graph, e.g. a tf.data pipeline.\n             self.backend.set_backend(\"tensorflow\")\n@@ -47,7 +47,7 @@ class TFDataLayer(Layer):\n \n     @tracking.no_automatic_dependency_tracking\n     def _get_seed_generator(self, backend=None):\n-        if backend is None or backend == keras_core.backend.backend():\n+        if backend is None or backend == keras.backend.backend():\n             return self.generator\n         if not hasattr(self, \"_backend_generators\"):\n             self._backend_generators = {}\n\n\n@@ -1,9 +1,9 @@\n-from keras_core import regularizers\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n+from keras import regularizers\n+from keras.api_export import keras_export\n+from keras.layers.layer import Layer\n \n \n-@keras_core_export(\"keras_core.layers.ActivityRegularization\")\n+@keras_export(\"keras.layers.ActivityRegularization\")\n class ActivityRegularization(Layer):\n     \"\"\"Layer that applies an update to the cost function based input activity.\n \n\n@@ -1,8 +1,8 @@\n import numpy as np\n import pytest\n \n-from keras_core import layers\n-from keras_core.testing import test_case\n+from keras import layers\n+from keras.testing import test_case\n \n \n class ActivityRegularizationTest(test_case.TestCase):\n\n@@ -1,9 +1,9 @@\n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.layers.layer import Layer\n \n \n-@keras_core_export(\"keras_core.layers.Dropout\")\n+@keras_export(\"keras.layers.Dropout\")\n class Dropout(Layer):\n     \"\"\"Applies dropout to the input.\n \n\n@@ -1,9 +1,9 @@\n import numpy as np\n import pytest\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import testing\n+from keras import backend\n+from keras import layers\n+from keras import testing\n \n \n class DropoutTest(testing.TestCase):\n\n@@ -1,12 +1,12 @@\n import math\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n+from keras import backend\n+from keras import layers\n+from keras import ops\n+from keras.api_export import keras_export\n \n \n-@keras_core_export(\"keras_core.layers.GaussianDropout\")\n+@keras_export(\"keras.layers.GaussianDropout\")\n class GaussianDropout(layers.Layer):\n     \"\"\"Apply multiplicative 1-centered Gaussian noise.\n \n\n@@ -1,9 +1,9 @@\n import numpy as np\n import pytest\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import testing\n+from keras import backend\n+from keras import layers\n+from keras import testing\n \n \n class GaussianDropoutTest(testing.TestCase):\n\n@@ -1,10 +1,10 @@\n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n+from keras import backend\n+from keras import layers\n+from keras import ops\n+from keras.api_export import keras_export\n \n \n-@keras_core_export(\"keras_core.layers.GaussianNoise\")\n+@keras_export(\"keras.layers.GaussianNoise\")\n class GaussianNoise(layers.Layer):\n     \"\"\"Apply additive zero-centered Gaussian noise.\n \n\n@@ -1,9 +1,9 @@\n import numpy as np\n import pytest\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import testing\n+from keras import backend\n+from keras import layers\n+from keras import testing\n \n \n class GaussianNoiseTest(testing.TestCase):\n\n@@ -1,8 +1,8 @@\n-from keras_core import backend\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.input_spec import InputSpec\n-from keras_core.layers.regularization.dropout import Dropout\n+from keras import backend\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.layers.input_spec import InputSpec\n+from keras.layers.regularization.dropout import Dropout\n \n \n class BaseSpatialDropout(Dropout):\n@@ -28,7 +28,7 @@ class BaseSpatialDropout(Dropout):\n         }\n \n \n-@keras_core_export(\"keras_core.layers.SpatialDropout1D\")\n+@keras_export(\"keras.layers.SpatialDropout1D\")\n class SpatialDropout1D(BaseSpatialDropout):\n     \"\"\"Spatial 1D version of Dropout.\n \n@@ -68,7 +68,7 @@ class SpatialDropout1D(BaseSpatialDropout):\n         return (input_shape[0], 1, input_shape[2])\n \n \n-@keras_core_export(\"keras_core.layers.SpatialDropout2D\")\n+@keras_export(\"keras.layers.SpatialDropout2D\")\n class SpatialDropout2D(BaseSpatialDropout):\n     \"\"\"Spatial 2D version of Dropout.\n \n@@ -130,7 +130,7 @@ class SpatialDropout2D(BaseSpatialDropout):\n         return {**base_config, **config}\n \n \n-@keras_core_export(\"keras_core.layers.SpatialDropout3D\")\n+@keras_export(\"keras.layers.SpatialDropout3D\")\n class SpatialDropout3D(BaseSpatialDropout):\n     \"\"\"Spatial 3D version of Dropout.\n \n\n@@ -1,8 +1,8 @@\n import numpy as np\n import pytest\n \n-from keras_core import layers\n-from keras_core.testing import test_case\n+from keras import layers\n+from keras.testing import test_case\n \n \n class SpatialDropoutTest(test_case.TestCase):\n\n\n@@ -1,10 +1,10 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.input_spec import InputSpec\n-from keras_core.layers.layer import Layer\n-from keras_core.utils import argument_validation\n+from keras.api_export import keras_export\n+from keras.layers.input_spec import InputSpec\n+from keras.layers.layer import Layer\n+from keras.utils import argument_validation\n \n \n-@keras_core_export(\"keras_core.layers.Cropping1D\")\n+@keras_export(\"keras.layers.Cropping1D\")\n class Cropping1D(Layer):\n     \"\"\"Cropping layer for 1D input (e.g. temporal sequence).\n \n@@ -21,7 +21,7 @@ class Cropping1D(Layer):\n      [[ 6  7]\n       [ 8  9]\n       [10 11]]]\n-    >>> y = keras_core.layers.Cropping1D(cropping=1)(x)\n+    >>> y = keras.layers.Cropping1D(cropping=1)(x)\n     >>> y\n     [[[2 3]]\n      [[8 9]]]\n\n@@ -1,9 +1,9 @@\n import numpy as np\n import pytest\n \n-from keras_core import layers\n-from keras_core import ops\n-from keras_core import testing\n+from keras import layers\n+from keras import ops\n+from keras import testing\n \n \n class Cropping1DTest(testing.TestCase):\n\n@@ -1,11 +1,11 @@\n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.input_spec import InputSpec\n-from keras_core.layers.layer import Layer\n-from keras_core.utils import argument_validation\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.layers.input_spec import InputSpec\n+from keras.layers.layer import Layer\n+from keras.utils import argument_validation\n \n \n-@keras_core_export(\"keras_core.layers.Cropping2D\")\n+@keras_export(\"keras.layers.Cropping2D\")\n class Cropping2D(Layer):\n     \"\"\"Cropping layer for 2D input (e.g. picture).\n \n@@ -15,7 +15,7 @@ class Cropping2D(Layer):\n \n     >>> input_shape = (2, 28, 28, 3)\n     >>> x = np.arange(np.prod(input_shape)).reshape(input_shape)\n-    >>> y = keras_core.layers.Cropping2D(cropping=((2, 2), (4, 4)))(x)\n+    >>> y = keras.layers.Cropping2D(cropping=((2, 2), (4, 4)))(x)\n     >>> y.shape\n     (2, 24, 20, 3)\n \n\n@@ -2,9 +2,9 @@ import numpy as np\n import pytest\n from absl.testing import parameterized\n \n-from keras_core import layers\n-from keras_core import ops\n-from keras_core import testing\n+from keras import layers\n+from keras import ops\n+from keras import testing\n \n \n class Cropping2DTest(testing.TestCase, parameterized.TestCase):\n\n@@ -1,11 +1,11 @@\n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.input_spec import InputSpec\n-from keras_core.layers.layer import Layer\n-from keras_core.utils import argument_validation\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.layers.input_spec import InputSpec\n+from keras.layers.layer import Layer\n+from keras.utils import argument_validation\n \n \n-@keras_core_export(\"keras_core.layers.Cropping3D\")\n+@keras_export(\"keras.layers.Cropping3D\")\n class Cropping3D(Layer):\n     \"\"\"Cropping layer for 3D data (e.g. spatial or spatio-temporal).\n \n@@ -13,7 +13,7 @@ class Cropping3D(Layer):\n \n     >>> input_shape = (2, 28, 28, 10, 3)\n     >>> x = np.arange(np.prod(input_shape)).reshape(input_shape)\n-    >>> y = keras_core.layers.Cropping3D(cropping=(2, 4, 2))(x)\n+    >>> y = keras.layers.Cropping3D(cropping=(2, 4, 2))(x)\n     >>> y.shape\n     (2, 24, 20, 6, 3)\n \n\n@@ -2,9 +2,9 @@ import numpy as np\n import pytest\n from absl.testing import parameterized\n \n-from keras_core import layers\n-from keras_core import ops\n-from keras_core import testing\n+from keras import layers\n+from keras import ops\n+from keras import testing\n \n \n class Cropping3DTest(testing.TestCase, parameterized.TestCase):\n\n@@ -1,14 +1,14 @@\n import math\n \n-from keras_core import backend\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.backend.common.keras_tensor import KerasTensor\n-from keras_core.layers.input_spec import InputSpec\n-from keras_core.layers.layer import Layer\n+from keras import backend\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.backend.common.keras_tensor import KerasTensor\n+from keras.layers.input_spec import InputSpec\n+from keras.layers.layer import Layer\n \n \n-@keras_core_export(\"keras_core.layers.Flatten\")\n+@keras_export(\"keras.layers.Flatten\")\n class Flatten(Layer):\n     \"\"\"Flattens the input. Does not affect the batch size.\n \n@@ -27,8 +27,8 @@ class Flatten(Layer):\n \n     Example:\n \n-    >>> x = keras_core.Input(shape=(10, 64))\n-    >>> y = keras_core.layers.Flatten()(x)\n+    >>> x = keras.Input(shape=(10, 64))\n+    >>> y = keras.layers.Flatten()(x)\n     >>> y.shape\n     (None, 640)\n     \"\"\"\n\n@@ -2,10 +2,10 @@ import numpy as np\n import pytest\n from absl.testing import parameterized\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import ops\n-from keras_core import testing\n+from keras import backend\n+from keras import layers\n+from keras import ops\n+from keras import testing\n \n \n class FlattenTest(testing.TestCase, parameterized.TestCase):\n\n@@ -1,11 +1,11 @@\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.backend.common.keras_tensor import KerasTensor\n-from keras_core.layers.input_spec import InputSpec\n-from keras_core.layers.layer import Layer\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.backend.common.keras_tensor import KerasTensor\n+from keras.layers.input_spec import InputSpec\n+from keras.layers.layer import Layer\n \n \n-@keras_core_export(\"keras_core.layers.Permute\")\n+@keras_export(\"keras.layers.Permute\")\n class Permute(Layer):\n     \"\"\"Permutes the dimensions of the input according to a given pattern.\n \n@@ -26,8 +26,8 @@ class Permute(Layer):\n \n     Example:\n \n-    >>> x = keras_core.Input(shape=(10, 64))\n-    >>> y = keras_core.layers.Permute((2, 1))(x)\n+    >>> x = keras.Input(shape=(10, 64))\n+    >>> y = keras.layers.Permute((2, 1))(x)\n     >>> y.shape\n     (None, 64, 10)\n     \"\"\"\n\n@@ -2,10 +2,10 @@ import numpy as np\n import pytest\n from absl.testing import parameterized\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import ops\n-from keras_core import testing\n+from keras import backend\n+from keras import layers\n+from keras import ops\n+from keras import testing\n \n \n class PermuteTest(testing.TestCase, parameterized.TestCase):\n\n@@ -1,17 +1,17 @@\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.input_spec import InputSpec\n-from keras_core.layers.layer import Layer\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.layers.input_spec import InputSpec\n+from keras.layers.layer import Layer\n \n \n-@keras_core_export(\"keras_core.layers.RepeatVector\")\n+@keras_export(\"keras.layers.RepeatVector\")\n class RepeatVector(Layer):\n     \"\"\"Repeats the input n times.\n \n     Example:\n \n-    >>> x = keras_core.Input(shape=(32,))\n-    >>> y = keras_core.layers.RepeatVector(3)(x)\n+    >>> x = keras.Input(shape=(32,))\n+    >>> y = keras.layers.RepeatVector(3)(x)\n     >>> y.shape\n     (None, 3, 32)\n \n\n@@ -1,9 +1,9 @@\n import numpy as np\n import pytest\n \n-from keras_core import layers\n-from keras_core import ops\n-from keras_core import testing\n+from keras import layers\n+from keras import ops\n+from keras import testing\n \n \n class FlattenTest(testing.TestCase):\n\n@@ -1,11 +1,11 @@\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.backend.common.keras_tensor import KerasTensor\n-from keras_core.layers.layer import Layer\n-from keras_core.ops import operation_utils\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.backend.common.keras_tensor import KerasTensor\n+from keras.layers.layer import Layer\n+from keras.ops import operation_utils\n \n \n-@keras_core_export(\"keras_core.layers.Reshape\")\n+@keras_export(\"keras.layers.Reshape\")\n class Reshape(Layer):\n     \"\"\"Layer that reshapes inputs into the given shape.\n \n@@ -24,13 +24,13 @@ class Reshape(Layer):\n \n     Example:\n \n-    >>> x = keras_core.Input(shape=(12,))\n-    >>> y = keras_core.layers.Reshape((3, 4))(x)\n+    >>> x = keras.Input(shape=(12,))\n+    >>> y = keras.layers.Reshape((3, 4))(x)\n     >>> y.shape\n     (None, 3, 4)\n \n     >>> # also supports shape inference using `-1` as dimension\n-    >>> y = keras_core.layers.Reshape((-1, 2, 2))(x)\n+    >>> y = keras.layers.Reshape((-1, 2, 2))(x)\n     >>> y.shape\n     (None, 3, 2, 2)\n     \"\"\"\n\n@@ -1,9 +1,9 @@\n import pytest\n from absl.testing import parameterized\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import testing\n+from keras import backend\n+from keras import layers\n+from keras import testing\n \n \n class ReshapeTest(testing.TestCase, parameterized.TestCase):\n\n@@ -1,10 +1,10 @@\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.input_spec import InputSpec\n-from keras_core.layers.layer import Layer\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.layers.input_spec import InputSpec\n+from keras.layers.layer import Layer\n \n \n-@keras_core_export(\"keras_core.layers.UpSampling1D\")\n+@keras_export(\"keras.layers.UpSampling1D\")\n class UpSampling1D(Layer):\n     \"\"\"Upsampling layer for 1D inputs.\n \n@@ -19,7 +19,7 @@ class UpSampling1D(Layer):\n       [ 3  4  5]]\n      [[ 6  7  8]\n       [ 9 10 11]]]\n-    >>> y = keras_core.layers.UpSampling1D(size=2)(x)\n+    >>> y = keras.layers.UpSampling1D(size=2)(x)\n     >>> y\n     [[[ 0.  1.  2.]\n       [ 0.  1.  2.]\n\n@@ -1,9 +1,9 @@\n import numpy as np\n import pytest\n \n-from keras_core import layers\n-from keras_core import testing\n-from keras_core.backend.common.keras_tensor import KerasTensor\n+from keras import layers\n+from keras import testing\n+from keras.backend.common.keras_tensor import KerasTensor\n \n \n class UpSamplingTest(testing.TestCase):\n\n@@ -1,14 +1,14 @@\n import numpy as np\n \n-from keras_core import backend\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.input_spec import InputSpec\n-from keras_core.layers.layer import Layer\n-from keras_core.utils import argument_validation\n+from keras import backend\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.layers.input_spec import InputSpec\n+from keras.layers.layer import Layer\n+from keras.utils import argument_validation\n \n \n-@keras_core_export(\"keras_core.layers.UpSampling2D\")\n+@keras_export(\"keras.layers.UpSampling2D\")\n class UpSampling2D(Layer):\n     \"\"\"Upsampling layer for 2D inputs.\n \n@@ -25,7 +25,7 @@ class UpSampling2D(Layer):\n       [[ 3  4  5]]]\n      [[[ 6  7  8]]\n       [[ 9 10 11]]]]\n-    >>> y = keras_core.layers.UpSampling2D(size=(1, 2))(x)\n+    >>> y = keras.layers.UpSampling2D(size=(1, 2))(x)\n     >>> print(y)\n     [[[[ 0  1  2]\n        [ 0  1  2]]\n@@ -154,7 +154,7 @@ class UpSampling2D(Layer):\n \n         if data_format == \"channels_first\":\n             x = ops.transpose(x, [0, 2, 3, 1])\n-        # https://github.com/keras-team/keras-core/issues/294\n+        # https://github.com/keras-team/keras/issues/294\n         # Use `ops.repeat` for `nearest` interpolation\n         if interpolation == \"nearest\":\n             x = ops.repeat(x, height_factor, axis=1)\n\n@@ -3,9 +3,9 @@ import numpy as np\n import pytest\n from absl.testing import parameterized\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import testing\n+from keras import backend\n+from keras import layers\n+from keras import testing\n \n \n class UpSampling2dTest(testing.TestCase, parameterized.TestCase):\n\n@@ -1,12 +1,12 @@\n-from keras_core import backend\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.input_spec import InputSpec\n-from keras_core.layers.layer import Layer\n-from keras_core.utils import argument_validation\n+from keras import backend\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.layers.input_spec import InputSpec\n+from keras.layers.layer import Layer\n+from keras.utils import argument_validation\n \n \n-@keras_core_export(\"keras_core.layers.UpSampling3D\")\n+@keras_export(\"keras.layers.UpSampling3D\")\n class UpSampling3D(Layer):\n     \"\"\"Upsampling layer for 3D inputs.\n \n@@ -17,7 +17,7 @@ class UpSampling3D(Layer):\n \n     >>> input_shape = (2, 1, 2, 1, 3)\n     >>> x = np.ones(input_shape)\n-    >>> y = keras_core.layers.UpSampling3D(size=(2, 2, 2))(x)\n+    >>> y = keras.layers.UpSampling3D(size=(2, 2, 2))(x)\n     >>> y.shape\n     (2, 2, 4, 2, 3)\n \n\n@@ -2,9 +2,9 @@ import numpy as np\n import pytest\n from absl.testing import parameterized\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import testing\n+from keras import backend\n+from keras import layers\n+from keras import testing\n \n \n class UpSampling3dTest(testing.TestCase, parameterized.TestCase):\n\n@@ -1,11 +1,11 @@\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.input_spec import InputSpec\n-from keras_core.layers.layer import Layer\n-from keras_core.utils import argument_validation\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.layers.input_spec import InputSpec\n+from keras.layers.layer import Layer\n+from keras.utils import argument_validation\n \n \n-@keras_core_export(\"keras_core.layers.ZeroPadding1D\")\n+@keras_export(\"keras.layers.ZeroPadding1D\")\n class ZeroPadding1D(Layer):\n     \"\"\"Zero-padding layer for 1D input (e.g. temporal sequence).\n \n@@ -18,7 +18,7 @@ class ZeroPadding1D(Layer):\n       [ 3  4  5]]\n      [[ 6  7  8]\n       [ 9 10 11]]]\n-    >>> y = keras_core.layers.ZeroPadding1D(padding=2)(x)\n+    >>> y = keras.layers.ZeroPadding1D(padding=2)(x)\n     >>> y\n     [[[ 0  0  0]\n       [ 0  0  0]\n\n@@ -1,8 +1,8 @@\n import numpy as np\n from absl.testing import parameterized\n \n-from keras_core import layers\n-from keras_core import testing\n+from keras import layers\n+from keras import testing\n \n \n class ZeroPadding1DTest(testing.TestCase, parameterized.TestCase):\n\n@@ -1,12 +1,12 @@\n-from keras_core import backend\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.input_spec import InputSpec\n-from keras_core.layers.layer import Layer\n-from keras_core.utils import argument_validation\n+from keras import backend\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.layers.input_spec import InputSpec\n+from keras.layers.layer import Layer\n+from keras.utils import argument_validation\n \n \n-@keras_core_export(\"keras_core.layers.ZeroPadding2D\")\n+@keras_export(\"keras.layers.ZeroPadding2D\")\n class ZeroPadding2D(Layer):\n     \"\"\"Zero-padding layer for 2D input (e.g. picture).\n \n@@ -20,7 +20,7 @@ class ZeroPadding2D(Layer):\n     >>> x\n     [[[[0 1]\n        [2 3]]]]\n-    >>> y = keras_core.layers.ZeroPadding2D(padding=1)(x)\n+    >>> y = keras.layers.ZeroPadding2D(padding=1)(x)\n     >>> y\n     [[[[0 0]\n        [0 0]\n\n@@ -1,8 +1,8 @@\n import numpy as np\n from absl.testing import parameterized\n \n-from keras_core import layers\n-from keras_core import testing\n+from keras import layers\n+from keras import testing\n \n \n class ZeroPadding2DTest(testing.TestCase, parameterized.TestCase):\n\n@@ -1,12 +1,12 @@\n-from keras_core import backend\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.input_spec import InputSpec\n-from keras_core.layers.layer import Layer\n-from keras_core.utils import argument_validation\n+from keras import backend\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.layers.input_spec import InputSpec\n+from keras.layers.layer import Layer\n+from keras.utils import argument_validation\n \n \n-@keras_core_export(\"keras_core.layers.ZeroPadding3D\")\n+@keras_export(\"keras.layers.ZeroPadding3D\")\n class ZeroPadding3D(Layer):\n     \"\"\"Zero-padding layer for 3D data (spatial or spatio-temporal).\n \n@@ -14,7 +14,7 @@ class ZeroPadding3D(Layer):\n \n     >>> input_shape = (1, 1, 2, 2, 3)\n     >>> x = np.arange(np.prod(input_shape)).reshape(input_shape)\n-    >>> y = keras_core.layers.ZeroPadding3D(padding=2)(x)\n+    >>> y = keras.layers.ZeroPadding3D(padding=2)(x)\n     >>> y.shape\n     (1, 5, 6, 6, 3)\n \n\n@@ -1,8 +1,8 @@\n import numpy as np\n from absl.testing import parameterized\n \n-from keras_core import layers\n-from keras_core import testing\n+from keras import layers\n+from keras import testing\n \n \n class ZeroPadding3DTest(testing.TestCase, parameterized.TestCase):\n\n\n@@ -1,21 +1,21 @@\n import copy\n \n-from keras_core import ops\n-from keras_core import utils\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.core.wrapper import Wrapper\n-from keras_core.layers.layer import Layer\n-from keras_core.saving import serialization_lib\n+from keras import ops\n+from keras import utils\n+from keras.api_export import keras_export\n+from keras.layers.core.wrapper import Wrapper\n+from keras.layers.layer import Layer\n+from keras.saving import serialization_lib\n \n \n-@keras_core_export(\"keras_core.layers.Bidirectional\")\n+@keras_export(\"keras.layers.Bidirectional\")\n class Bidirectional(Wrapper):\n     \"\"\"Bidirectional wrapper for RNNs.\n \n     Args:\n-        layer: `keras_core.layers.RNN` instance, such as\n-            `keras_core.layers.LSTM` or `keras_core.layers.GRU`.\n-            It could also be a `keras_core.layers.Layer` instance\n+        layer: `keras.layers.RNN` instance, such as\n+            `keras.layers.LSTM` or `keras.layers.GRU`.\n+            It could also be a `keras.layers.Layer` instance\n             that meets the following criteria:\n             1. Be a sequence-processing layer (accepts 3D+ inputs).\n             2. Have a `go_backwards`, `return_sequences` and `return_state`\n@@ -23,8 +23,8 @@ class Bidirectional(Wrapper):\n             3. Have an `input_spec` attribute.\n             4. Implement serialization via `get_config()` and `from_config()`.\n             Note that the recommended way to create new RNN layers is to write a\n-            custom RNN cell and use it with `keras_core.layers.RNN`, instead of\n-            subclassing `keras_core.layers.Layer` directly.\n+            custom RNN cell and use it with `keras.layers.RNN`, instead of\n+            subclassing `keras.layers.Layer` directly.\n             When `return_sequences` is `True`, the output of the masked\n             timestep will be zero regardless of the layer's original\n             `zero_output_for_mask` value.\n@@ -32,8 +32,8 @@ class Bidirectional(Wrapper):\n             will be combined. One of `{\"sum\", \"mul\", \"concat\", \"ave\", None}`.\n             If `None`, the outputs will not be combined,\n             they will be returned as a list. Defaults to `\"concat\"`.\n-        backward_layer: Optional `keras_core.layers.RNN`,\n-            or `keras_core.layers.Layer` instance to be used to handle\n+        backward_layer: Optional `keras.layers.RNN`,\n+            or `keras.layers.Layer` instance to be used to handle\n             backwards input processing.\n             If `backward_layer` is not provided, the layer instance passed\n             as the `layer` argument will be used to generate the backward layer\n@@ -92,11 +92,11 @@ class Bidirectional(Wrapper):\n         if not isinstance(layer, Layer):\n             raise ValueError(\n                 \"Please initialize `Bidirectional` layer with a \"\n-                f\"`keras_core.layers.Layer` instance. Received: {layer}\"\n+                f\"`keras.layers.Layer` instance. Received: {layer}\"\n             )\n         if backward_layer is not None and not isinstance(backward_layer, Layer):\n             raise ValueError(\n-                \"`backward_layer` need to be a `keras_core.layers.Layer` \"\n+                \"`backward_layer` need to be a `keras.layers.Layer` \"\n                 f\"instance. Received: {backward_layer}\"\n             )\n         if merge_mode not in [\"sum\", \"mul\", \"ave\", \"concat\", None]:\n\n@@ -1,9 +1,9 @@\n import numpy as np\n import pytest\n \n-from keras_core import initializers\n-from keras_core import layers\n-from keras_core import testing\n+from keras import initializers\n+from keras import layers\n+from keras import testing\n \n \n class SimpleRNNTest(testing.TestCase):\n\n@@ -1,17 +1,17 @@\n import tree\n \n-from keras_core import activations\n-from keras_core import backend\n-from keras_core import constraints\n-from keras_core import initializers\n-from keras_core import ops\n-from keras_core import regularizers\n-from keras_core.layers.input_spec import InputSpec\n-from keras_core.layers.layer import Layer\n-from keras_core.layers.rnn.dropout_rnn_cell import DropoutRNNCell\n-from keras_core.layers.rnn.rnn import RNN\n-from keras_core.ops import operation_utils\n-from keras_core.utils import argument_validation\n+from keras import activations\n+from keras import backend\n+from keras import constraints\n+from keras import initializers\n+from keras import ops\n+from keras import regularizers\n+from keras.layers.input_spec import InputSpec\n+from keras.layers.layer import Layer\n+from keras.layers.rnn.dropout_rnn_cell import DropoutRNNCell\n+from keras.layers.rnn.rnn import RNN\n+from keras.ops import operation_utils\n+from keras.utils import argument_validation\n \n \n class ConvLSTMCell(Layer, DropoutRNNCell):\n\n@@ -1,8 +1,8 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.rnn.conv_lstm import ConvLSTM\n+from keras.api_export import keras_export\n+from keras.layers.rnn.conv_lstm import ConvLSTM\n \n \n-@keras_core_export(\"keras_core.layers.ConvLSTM1D\")\n+@keras_export(\"keras.layers.ConvLSTM1D\")\n class ConvLSTM1D(ConvLSTM):\n     \"\"\"1D Convolutional LSTM.\n \n\n@@ -1,9 +1,9 @@\n import numpy as np\n import pytest\n \n-from keras_core import initializers\n-from keras_core import layers\n-from keras_core import testing\n+from keras import initializers\n+from keras import layers\n+from keras import testing\n \n \n class ConvLSTM1DTest(testing.TestCase):\n\n@@ -1,8 +1,8 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.rnn.conv_lstm import ConvLSTM\n+from keras.api_export import keras_export\n+from keras.layers.rnn.conv_lstm import ConvLSTM\n \n \n-@keras_core_export(\"keras_core.layers.ConvLSTM2D\")\n+@keras_export(\"keras.layers.ConvLSTM2D\")\n class ConvLSTM2D(ConvLSTM):\n     \"\"\"2D Convolutional LSTM.\n \n\n@@ -1,9 +1,9 @@\n import numpy as np\n import pytest\n \n-from keras_core import initializers\n-from keras_core import layers\n-from keras_core import testing\n+from keras import initializers\n+from keras import layers\n+from keras import testing\n \n \n class ConvLSTM2DTest(testing.TestCase):\n\n@@ -1,8 +1,8 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.rnn.conv_lstm import ConvLSTM\n+from keras.api_export import keras_export\n+from keras.layers.rnn.conv_lstm import ConvLSTM\n \n \n-@keras_core_export(\"keras_core.layers.ConvLSTM3D\")\n+@keras_export(\"keras.layers.ConvLSTM3D\")\n class ConvLSTM3D(ConvLSTM):\n     \"\"\"3D Convolutional LSTM.\n \n\n@@ -1,9 +1,9 @@\n import numpy as np\n import pytest\n \n-from keras_core import initializers\n-from keras_core import layers\n-from keras_core import testing\n+from keras import initializers\n+from keras import layers\n+from keras import testing\n \n \n class ConvLSTM1DTest(testing.TestCase):\n\n@@ -1,10 +1,10 @@\n import numpy as np\n \n-from keras_core import backend\n-from keras_core import initializers\n-from keras_core import testing\n-from keras_core.layers.rnn.conv_lstm import ConvLSTM\n-from keras_core.layers.rnn.conv_lstm import ConvLSTMCell\n+from keras import backend\n+from keras import initializers\n+from keras import testing\n+from keras.layers.rnn.conv_lstm import ConvLSTM\n+from keras.layers.rnn.conv_lstm import ConvLSTMCell\n \n \n class ConvLSTMCellTest(testing.TestCase):\n\n@@ -1,5 +1,5 @@\n-from keras_core import backend\n-from keras_core import ops\n+from keras import backend\n+from keras import ops\n \n \n class DropoutRNNCell:\n\n@@ -1,10 +1,10 @@\n import pytest\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import ops\n-from keras_core import testing\n-from keras_core.layers.rnn.dropout_rnn_cell import DropoutRNNCell\n+from keras import backend\n+from keras import layers\n+from keras import ops\n+from keras import testing\n+from keras.layers.rnn.dropout_rnn_cell import DropoutRNNCell\n \n \n class RNNCellWithDropout(layers.Layer, DropoutRNNCell):\n\n@@ -1,24 +1,24 @@\n import tree\n \n-from keras_core import activations\n-from keras_core import backend\n-from keras_core import constraints\n-from keras_core import initializers\n-from keras_core import ops\n-from keras_core import regularizers\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.input_spec import InputSpec\n-from keras_core.layers.layer import Layer\n-from keras_core.layers.rnn.dropout_rnn_cell import DropoutRNNCell\n-from keras_core.layers.rnn.rnn import RNN\n+from keras import activations\n+from keras import backend\n+from keras import constraints\n+from keras import initializers\n+from keras import ops\n+from keras import regularizers\n+from keras.api_export import keras_export\n+from keras.layers.input_spec import InputSpec\n+from keras.layers.layer import Layer\n+from keras.layers.rnn.dropout_rnn_cell import DropoutRNNCell\n+from keras.layers.rnn.rnn import RNN\n \n \n-@keras_core_export(\"keras_core.layers.GRUCell\")\n+@keras_export(\"keras.layers.GRUCell\")\n class GRUCell(Layer, DropoutRNNCell):\n     \"\"\"Cell class for the GRU layer.\n \n     This class processes one step within the whole time sequence input, whereas\n-    `keras_core.layer.GRU` processes the whole sequence.\n+    `keras.layer.GRU` processes the whole sequence.\n \n     Args:\n         units: Positive integer, dimensionality of the output space.\n@@ -69,12 +69,12 @@ class GRUCell(Layer, DropoutRNNCell):\n     Example:\n \n     >>> inputs = np.random.random((32, 10, 8))\n-    >>> rnn = keras_core.layers.RNN(keras_core.layers.GRUCell(4))\n+    >>> rnn = keras.layers.RNN(keras.layers.GRUCell(4))\n     >>> output = rnn(inputs)\n     >>> output.shape\n     (32, 4)\n-    >>> rnn = keras_core.layers.RNN(\n-    ...    keras_core.layers.GRUCell(4),\n+    >>> rnn = keras.layers.RNN(\n+    ...    keras.layers.GRUCell(4),\n     ...    return_sequences=True,\n     ...    return_state=True)\n     >>> whole_sequence_output, final_state = rnn(inputs)\n@@ -328,7 +328,7 @@ class GRUCell(Layer, DropoutRNNCell):\n         ]\n \n \n-@keras_core_export(\"keras_core.layers.GRU\")\n+@keras_export(\"keras.layers.GRU\")\n class GRU(RNN):\n     \"\"\"Gated Recurrent Unit - Cho et al. 2014.\n \n@@ -363,11 +363,11 @@ class GRU(RNN):\n     For example:\n \n     >>> inputs = np.random.random((32, 10, 8))\n-    >>> gru = keras_core.layers.GRU(4)\n+    >>> gru = keras.layers.GRU(4)\n     >>> output = gru(inputs)\n     >>> output.shape\n     (32, 4)\n-    >>> gru = keras_core.layers.GRU(4, return_sequences=True, return_state=True)\n+    >>> gru = keras.layers.GRU(4, return_sequences=True, return_state=True)\n     >>> whole_sequence_output, final_state = gru(inputs)\n     >>> whole_sequence_output.shape\n     (32, 10, 4)\n\n@@ -2,9 +2,9 @@ import numpy as np\n import pytest\n from absl.testing import parameterized\n \n-from keras_core import initializers\n-from keras_core import layers\n-from keras_core import testing\n+from keras import initializers\n+from keras import layers\n+from keras import testing\n \n \n class GRUTest(testing.TestCase, parameterized.TestCase):\n\n@@ -1,24 +1,24 @@\n import tree\n \n-from keras_core import activations\n-from keras_core import backend\n-from keras_core import constraints\n-from keras_core import initializers\n-from keras_core import ops\n-from keras_core import regularizers\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.input_spec import InputSpec\n-from keras_core.layers.layer import Layer\n-from keras_core.layers.rnn.dropout_rnn_cell import DropoutRNNCell\n-from keras_core.layers.rnn.rnn import RNN\n+from keras import activations\n+from keras import backend\n+from keras import constraints\n+from keras import initializers\n+from keras import ops\n+from keras import regularizers\n+from keras.api_export import keras_export\n+from keras.layers.input_spec import InputSpec\n+from keras.layers.layer import Layer\n+from keras.layers.rnn.dropout_rnn_cell import DropoutRNNCell\n+from keras.layers.rnn.rnn import RNN\n \n \n-@keras_core_export(\"keras_core.layers.LSTMCell\")\n+@keras_export(\"keras.layers.LSTMCell\")\n class LSTMCell(Layer, DropoutRNNCell):\n     \"\"\"Cell class for the LSTM layer.\n \n     This class processes one step within the whole time sequence input, whereas\n-    `keras_core.layer.LSTM` processes the whole sequence.\n+    `keras.layer.LSTM` processes the whole sequence.\n \n     Args:\n         units: Positive integer, dimensionality of the output space.\n@@ -71,12 +71,12 @@ class LSTMCell(Layer, DropoutRNNCell):\n     Example:\n \n     >>> inputs = np.random.random((32, 10, 8))\n-    >>> rnn = keras_core.layers.RNN(keras_core.layers.LSTMCell(4))\n+    >>> rnn = keras.layers.RNN(keras.layers.LSTMCell(4))\n     >>> output = rnn(inputs)\n     >>> output.shape\n     (32, 4)\n-    >>> rnn = keras_core.layers.RNN(\n-    ...    keras_core.layers.LSTMCell(4),\n+    >>> rnn = keras.layers.RNN(\n+    ...    keras.layers.LSTMCell(4),\n     ...    return_sequences=True,\n     ...    return_state=True)\n     >>> whole_sequence_output, final_state = rnn(inputs)\n@@ -316,7 +316,7 @@ class LSTMCell(Layer, DropoutRNNCell):\n         ]\n \n \n-@keras_core_export(\"keras_core.layers.LSTM\")\n+@keras_export(\"keras.layers.LSTM\")\n class LSTM(RNN):\n     \"\"\"Long Short-Term Memory layer - Hochreiter 1997.\n \n@@ -339,11 +339,11 @@ class LSTM(RNN):\n     For example:\n \n     >>> inputs = np.random.random((32, 10, 8))\n-    >>> lstm = keras_core.layers.LSTM(4)\n+    >>> lstm = keras.layers.LSTM(4)\n     >>> output = lstm(inputs)\n     >>> output.shape\n     (32, 4)\n-    >>> lstm = keras_core.layers.LSTM(\n+    >>> lstm = keras.layers.LSTM(\n     ...     4, return_sequences=True, return_state=True)\n     >>> whole_seq_output, final_memory_state, final_carry_state = lstm(inputs)\n     >>> whole_seq_output.shape\n\n@@ -2,9 +2,9 @@ import numpy as np\n import pytest\n from absl.testing import parameterized\n \n-from keras_core import initializers\n-from keras_core import layers\n-from keras_core import testing\n+from keras import initializers\n+from keras import layers\n+from keras import testing\n \n \n class LSTMTest(testing.TestCase, parameterized.TestCase):\n\n@@ -1,16 +1,16 @@\n import tree\n \n-from keras_core import backend\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n-from keras_core.layers.rnn.dropout_rnn_cell import DropoutRNNCell\n-from keras_core.layers.rnn.stacked_rnn_cells import StackedRNNCells\n-from keras_core.saving import serialization_lib\n-from keras_core.utils import tracking\n+from keras import backend\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.layers.layer import Layer\n+from keras.layers.rnn.dropout_rnn_cell import DropoutRNNCell\n+from keras.layers.rnn.stacked_rnn_cells import StackedRNNCells\n+from keras.saving import serialization_lib\n+from keras.utils import tracking\n \n \n-@keras_core_export(\"keras_core.layers.RNN\")\n+@keras_export(\"keras.layers.RNN\")\n class RNN(Layer):\n     \"\"\"Base class for recurrent layers.\n \n@@ -93,7 +93,7 @@ class RNN(Layer):\n \n     This layer supports masking for input data with a variable number\n     of timesteps. To introduce masks to your data,\n-    use a `keras_core.layers.Embedding` layer with the `mask_zero` parameter\n+    use a `keras.layers.Embedding` layer with the `mask_zero` parameter\n     set to `True`.\n \n     Note on using statefulness in RNNs:\n@@ -134,11 +134,11 @@ class RNN(Layer):\n     Examples:\n \n     ```python\n-    from keras_core.layers import RNN\n-    from keras_core import ops\n+    from keras.layers import RNN\n+    from keras import ops\n \n     # First, let's define a RNN Cell, as a layer subclass.\n-    class MinimalRNNCell(keras_core.layers.Layer):\n+    class MinimalRNNCell(keras.layers.Layer):\n \n         def __init__(self, units, **kwargs):\n             super().__init__(**kwargs)\n@@ -164,14 +164,14 @@ class RNN(Layer):\n     # Let's use this cell in a RNN layer:\n \n     cell = MinimalRNNCell(32)\n-    x = keras_core.Input((None, 5))\n+    x = keras.Input((None, 5))\n     layer = RNN(cell)\n     y = layer(x)\n \n     # Here's how to use the cell to build a stacked RNN:\n \n     cells = [MinimalRNNCell(32), MinimalRNNCell(64)]\n-    x = keras_core.Input((None, 5))\n+    x = keras.Input((None, 5))\n     layer = RNN(cells)\n     y = layer(x)\n     ```\n\n@@ -1,9 +1,9 @@\n import numpy as np\n import pytest\n \n-from keras_core import layers\n-from keras_core import ops\n-from keras_core import testing\n+from keras import layers\n+from keras import ops\n+from keras import testing\n \n \n class OneStateRNNCell(layers.Layer):\n\n@@ -1,22 +1,22 @@\n-from keras_core import activations\n-from keras_core import backend\n-from keras_core import constraints\n-from keras_core import initializers\n-from keras_core import ops\n-from keras_core import regularizers\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.input_spec import InputSpec\n-from keras_core.layers.layer import Layer\n-from keras_core.layers.rnn.dropout_rnn_cell import DropoutRNNCell\n-from keras_core.layers.rnn.rnn import RNN\n+from keras import activations\n+from keras import backend\n+from keras import constraints\n+from keras import initializers\n+from keras import ops\n+from keras import regularizers\n+from keras.api_export import keras_export\n+from keras.layers.input_spec import InputSpec\n+from keras.layers.layer import Layer\n+from keras.layers.rnn.dropout_rnn_cell import DropoutRNNCell\n+from keras.layers.rnn.rnn import RNN\n \n \n-@keras_core_export(\"keras_core.layers.SimpleRNNCell\")\n+@keras_export(\"keras.layers.SimpleRNNCell\")\n class SimpleRNNCell(Layer, DropoutRNNCell):\n     \"\"\"Cell class for SimpleRNN.\n \n     This class processes one step within the whole time sequence input, whereas\n-    `keras_core.layer.SimpleRNN` processes the whole sequence.\n+    `keras.layer.SimpleRNN` processes the whole sequence.\n \n     Args:\n         units: Positive integer, dimensionality of the output space.\n@@ -63,10 +63,10 @@ class SimpleRNNCell(Layer, DropoutRNNCell):\n \n     ```python\n     inputs = np.random.random([32, 10, 8]).astype(np.float32)\n-    rnn = keras_core.layers.RNN(keras_core.layers.SimpleRNNCell(4))\n+    rnn = keras.layers.RNN(keras.layers.SimpleRNNCell(4))\n     output = rnn(inputs)  # The output has shape `(32, 4)`.\n-    rnn = keras_core.layers.RNN(\n-        keras_core.layers.SimpleRNNCell(4),\n+    rnn = keras.layers.RNN(\n+        keras.layers.SimpleRNNCell(4),\n         return_sequences=True,\n         return_state=True\n     )\n@@ -209,7 +209,7 @@ class SimpleRNNCell(Layer, DropoutRNNCell):\n         return {**base_config, **config}\n \n \n-@keras_core_export(\"keras_core.layers.SimpleRNN\")\n+@keras_export(\"keras.layers.SimpleRNN\")\n class SimpleRNN(RNN):\n     \"\"\"Fully-connected RNN where the output is to be fed back as the new input.\n \n@@ -283,9 +283,9 @@ class SimpleRNN(RNN):\n \n     ```python\n     inputs = np.random.random((32, 10, 8))\n-    simple_rnn = keras_core.layers.SimpleRNN(4)\n+    simple_rnn = keras.layers.SimpleRNN(4)\n     output = simple_rnn(inputs)  # The output has shape `(32, 4)`.\n-    simple_rnn = keras_core.layers.SimpleRNN(\n+    simple_rnn = keras.layers.SimpleRNN(\n         4, return_sequences=True, return_state=True\n     )\n     # whole_sequence_output has shape `(32, 10, 4)`.\n\n@@ -1,9 +1,9 @@\n import numpy as np\n import pytest\n \n-from keras_core import initializers\n-from keras_core import layers\n-from keras_core import testing\n+from keras import initializers\n+from keras import layers\n+from keras import testing\n \n \n class SimpleRNNTest(testing.TestCase):\n\n@@ -1,12 +1,12 @@\n import tree\n \n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n-from keras_core.saving import serialization_lib\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.layers.layer import Layer\n+from keras.saving import serialization_lib\n \n \n-@keras_core_export(\"keras_core.layers.StackedRNNCells\")\n+@keras_export(\"keras.layers.StackedRNNCells\")\n class StackedRNNCells(Layer):\n     \"\"\"Wrapper allowing a stack of RNN cells to behave as a single cell.\n \n@@ -24,9 +24,9 @@ class StackedRNNCells(Layer):\n     new_shape = (batch_size, sentence_length, num_features)\n     x = np.reshape(np.arange(30), new_shape)\n \n-    rnn_cells = [keras_core.layers.LSTMCell(128) for _ in range(2)]\n-    stacked_lstm = keras_core.layers.StackedRNNCells(rnn_cells)\n-    lstm_layer = keras_core.layers.RNN(stacked_lstm)\n+    rnn_cells = [keras.layers.LSTMCell(128) for _ in range(2)]\n+    stacked_lstm = keras.layers.StackedRNNCells(rnn_cells)\n+    lstm_layer = keras.layers.RNN(stacked_lstm)\n \n     result = lstm_layer(x)\n     ```\n\n@@ -1,10 +1,10 @@\n import numpy as np\n import pytest\n \n-from keras_core import layers\n-from keras_core import testing\n-from keras_core.layers.rnn.rnn_test import OneStateRNNCell\n-from keras_core.layers.rnn.rnn_test import TwoStatesRNNCell\n+from keras import layers\n+from keras import testing\n+from keras.layers.rnn.rnn_test import OneStateRNNCell\n+from keras.layers.rnn.rnn_test import TwoStatesRNNCell\n \n \n class StackedRNNTest(testing.TestCase):\n\n@@ -1,13 +1,13 @@\n \"\"\"Wrapper layer to apply every temporal slice of an input.\"\"\"\n \n-from keras_core import backend\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.core.wrapper import Wrapper\n-from keras_core.layers.layer import Layer\n+from keras import backend\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.layers.core.wrapper import Wrapper\n+from keras.layers.layer import Layer\n \n \n-@keras_core_export(\"keras_core.layers.TimeDistributed\")\n+@keras_export(\"keras.layers.TimeDistributed\")\n class TimeDistributed(Wrapper):\n     \"\"\"This wrapper allows to apply a layer to every temporal slice of an input.\n \n@@ -31,7 +31,7 @@ class TimeDistributed(Wrapper):\n     the timestamps, the same set of weights are used at each timestamp.\n \n     Args:\n-        layer: a `keras_core.layers.Layer` instance.\n+        layer: a `keras.layers.Layer` instance.\n \n     Call arguments:\n         inputs: Input tensor of shape (batch, time, ...) or nested tensors,\n@@ -48,7 +48,7 @@ class TimeDistributed(Wrapper):\n         if not isinstance(layer, Layer):\n             raise ValueError(\n                 \"Please initialize `TimeDistributed` layer with a \"\n-                f\"`keras_core.layers.Layer` instance. Received: {layer}\"\n+                f\"`keras.layers.Layer` instance. Received: {layer}\"\n             )\n         super().__init__(layer, **kwargs)\n         self.supports_masking = True\n\n@@ -1,10 +1,10 @@\n import numpy as np\n import pytest\n \n-from keras_core import initializers\n-from keras_core import layers\n-from keras_core import ops\n-from keras_core import testing\n+from keras import initializers\n+from keras import layers\n+from keras import ops\n+from keras import testing\n \n \n class TimeDistributedTest(testing.TestCase):\n\n\n@@ -4,47 +4,47 @@ import itertools\n \n import numpy as np\n \n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.utils.module_utils import tensorflow as tf\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.utils.module_utils import tensorflow as tf\n \n py_any = any\n py_all = all\n \n \n-@keras_core_export(\"keras_core._legacy.backend.abs\")\n+@keras_export(\"keras._legacy.backend.abs\")\n def abs(x):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.abs(x)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.all\")\n+@keras_export(\"keras._legacy.backend.all\")\n def all(x, axis=None, keepdims=False):\n     \"\"\"DEPRECATED.\"\"\"\n     x = tf.cast(x, tf.bool)\n     return tf.reduce_all(x, axis, keepdims)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.any\")\n+@keras_export(\"keras._legacy.backend.any\")\n def any(x, axis=None, keepdims=False):\n     \"\"\"DEPRECATED.\"\"\"\n     x = tf.cast(x, tf.bool)\n     return tf.reduce_any(x, axis, keepdims)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.argmax\")\n+@keras_export(\"keras._legacy.backend.argmax\")\n def argmax(x, axis=-1):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.argmax(x, axis)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.argmin\")\n+@keras_export(\"keras._legacy.backend.argmin\")\n def argmin(x, axis=-1):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.argmin(x, axis)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.arange\")\n+@keras_export(\"keras._legacy.backend.arange\")\n def arange(start, stop=None, step=1, dtype=\"int32\"):\n     \"\"\"DEPRECATED.\"\"\"\n     if stop is None and start < 0:\n@@ -55,7 +55,7 @@ def arange(start, stop=None, step=1, dtype=\"int32\"):\n     return result\n \n \n-@keras_core_export(\"keras_core._legacy.backend.batch_dot\")\n+@keras_export(\"keras._legacy.backend.batch_dot\")\n def batch_dot(x, y, axes=None):\n     \"\"\"DEPRECATED.\"\"\"\n     x_shape = x.shape\n@@ -217,20 +217,20 @@ def batch_dot(x, y, axes=None):\n     return result\n \n \n-@keras_core_export(\"keras_core._legacy.backend.batch_flatten\")\n+@keras_export(\"keras._legacy.backend.batch_flatten\")\n def batch_flatten(x):\n     \"\"\"DEPRECATED.\"\"\"\n     x = tf.reshape(x, tf.stack([-1, prod(tf.shape(x)[1:])]))\n     return x\n \n \n-@keras_core_export(\"keras_core._legacy.backend.batch_get_value\")\n+@keras_export(\"keras._legacy.backend.batch_get_value\")\n def batch_get_value(tensors):\n     \"\"\"DEPRECATED.\"\"\"\n     return [x.numpy() for x in tensors]\n \n \n-@keras_core_export(\"keras_core._legacy.backend.batch_set_value\")\n+@keras_export(\"keras._legacy.backend.batch_set_value\")\n def batch_set_value(tuples):\n     \"\"\"DEPRECATED.\"\"\"\n     if tf.executing_eagerly() or tf.inside_function():\n@@ -239,13 +239,13 @@ def batch_set_value(tuples):\n             x.assign(value)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.batch_normalization\")\n+@keras_export(\"keras._legacy.backend.batch_normalization\")\n def batch_normalization(x, mean, var, beta, gamma, axis=-1, epsilon=1e-3):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.nn.batch_normalization(x, mean, var, beta, gamma, epsilon)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.bias_add\")\n+@keras_export(\"keras._legacy.backend.bias_add\")\n def bias_add(x, bias, data_format=None):\n     \"\"\"DEPRECATED.\"\"\"\n     if data_format is None:\n@@ -271,7 +271,7 @@ def bias_add(x, bias, data_format=None):\n     return tf.nn.bias_add(x, bias)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.binary_crossentropy\")\n+@keras_export(\"keras._legacy.backend.binary_crossentropy\")\n def binary_crossentropy(target, output, from_logits=False):\n     \"\"\"DEPRECATED.\"\"\"\n     target = tf.convert_to_tensor(target)\n@@ -291,7 +291,7 @@ def binary_crossentropy(target, output, from_logits=False):\n     return -bce\n \n \n-@keras_core_export(\"keras_core._legacy.backend.binary_focal_crossentropy\")\n+@keras_export(\"keras._legacy.backend.binary_focal_crossentropy\")\n def binary_focal_crossentropy(\n     target,\n     output,\n@@ -323,13 +323,13 @@ def binary_focal_crossentropy(\n     return focal_bce\n \n \n-@keras_core_export(\"keras_core._legacy.backend.cast\")\n+@keras_export(\"keras._legacy.backend.cast\")\n def cast(x, dtype):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.cast(x, dtype)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.cast_to_floatx\")\n+@keras_export(\"keras._legacy.backend.cast_to_floatx\")\n def cast_to_floatx(x):\n     \"\"\"DEPRECATED.\"\"\"\n     if isinstance(x, (tf.Tensor, tf.Variable, tf.SparseTensor)):\n@@ -337,7 +337,7 @@ def cast_to_floatx(x):\n     return np.asarray(x, dtype=backend.floatx())\n \n \n-@keras_core_export(\"keras_core._legacy.backend.categorical_crossentropy\")\n+@keras_export(\"keras._legacy.backend.categorical_crossentropy\")\n def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n     \"\"\"DEPRECATED.\"\"\"\n     target = tf.convert_to_tensor(target)\n@@ -361,7 +361,7 @@ def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n     return -tf.reduce_sum(target * tf.math.log(output), axis)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.categorical_focal_crossentropy\")\n+@keras_export(\"keras._legacy.backend.categorical_focal_crossentropy\")\n def categorical_focal_crossentropy(\n     target,\n     output,\n@@ -400,7 +400,7 @@ def categorical_focal_crossentropy(\n     return focal_cce\n \n \n-@keras_core_export(\"keras_core._legacy.backend.clip\")\n+@keras_export(\"keras._legacy.backend.clip\")\n def clip(x, min_value, max_value):\n     \"\"\"DEPRECATED.\"\"\"\n     if isinstance(min_value, (int, float)) and isinstance(\n@@ -415,7 +415,7 @@ def clip(x, min_value, max_value):\n     return tf.clip_by_value(x, min_value, max_value)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.concatenate\")\n+@keras_export(\"keras._legacy.backend.concatenate\")\n def concatenate(tensors, axis=-1):\n     \"\"\"DEPRECATED.\"\"\"\n     if axis < 0:\n@@ -433,7 +433,7 @@ def concatenate(tensors, axis=-1):\n         return tf.concat([to_dense(x) for x in tensors], axis)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.constant\")\n+@keras_export(\"keras._legacy.backend.constant\")\n def constant(value, dtype=None, shape=None, name=None):\n     \"\"\"DEPRECATED.\"\"\"\n     if dtype is None:\n@@ -476,7 +476,7 @@ def _preprocess_padding(padding):\n     return padding\n \n \n-@keras_core_export(\"keras_core._legacy.backend.conv1d\")\n+@keras_export(\"keras._legacy.backend.conv1d\")\n def conv1d(\n     x, kernel, strides=1, padding=\"valid\", data_format=None, dilation_rate=1\n ):\n@@ -508,7 +508,7 @@ def conv1d(\n     return x\n \n \n-@keras_core_export(\"keras_core._legacy.backend.conv2d\")\n+@keras_export(\"keras._legacy.backend.conv2d\")\n def conv2d(\n     x,\n     kernel,\n@@ -538,7 +538,7 @@ def conv2d(\n     return x\n \n \n-@keras_core_export(\"keras_core._legacy.backend.conv2d_transpose\")\n+@keras_export(\"keras._legacy.backend.conv2d_transpose\")\n def conv2d_transpose(\n     x,\n     kernel,\n@@ -607,7 +607,7 @@ def conv2d_transpose(\n     return x\n \n \n-@keras_core_export(\"keras_core._legacy.backend.conv3d\")\n+@keras_export(\"keras._legacy.backend.conv3d\")\n def conv3d(\n     x,\n     kernel,\n@@ -637,19 +637,19 @@ def conv3d(\n     return x\n \n \n-@keras_core_export(\"keras_core._legacy.backend.cos\")\n+@keras_export(\"keras._legacy.backend.cos\")\n def cos(x):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.cos(x)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.count_params\")\n+@keras_export(\"keras._legacy.backend.count_params\")\n def count_params(x):\n     \"\"\"DEPRECATED.\"\"\"\n     return np.prod(x.shape.as_list())\n \n \n-@keras_core_export(\"keras_core._legacy.backend.ctc_batch_cost\")\n+@keras_export(\"keras._legacy.backend.ctc_batch_cost\")\n def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n     \"\"\"DEPRECATED.\"\"\"\n     label_length = tf.cast(tf.squeeze(label_length, axis=-1), tf.int32)\n@@ -670,7 +670,7 @@ def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n     )\n \n \n-@keras_core_export(\"keras_core._legacy.backend.ctc_label_dense_to_sparse\")\n+@keras_export(\"keras._legacy.backend.ctc_label_dense_to_sparse\")\n def ctc_label_dense_to_sparse(labels, label_lengths):\n     \"\"\"DEPRECATED.\"\"\"\n     label_shape = tf.shape(labels)\n@@ -711,7 +711,7 @@ def ctc_label_dense_to_sparse(labels, label_lengths):\n     )\n \n \n-@keras_core_export(\"keras_core._legacy.backend.ctc_decode\")\n+@keras_export(\"keras._legacy.backend.ctc_decode\")\n def ctc_decode(y_pred, input_length, greedy=True, beam_width=100, top_paths=1):\n     \"\"\"DEPRECATED.\"\"\"\n     input_shape = tf.shape(y_pred)\n@@ -739,19 +739,19 @@ def ctc_decode(y_pred, input_length, greedy=True, beam_width=100, top_paths=1):\n     return (decoded_dense, log_prob)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.cumsum\")\n+@keras_export(\"keras._legacy.backend.cumsum\")\n def cumsum(x, axis=0):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.cumsum(x, axis=axis)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.cumprod\")\n+@keras_export(\"keras._legacy.backend.cumprod\")\n def cumprod(x, axis=0):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.math.cumprod(x, axis=axis)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.depthwise_conv2d\")\n+@keras_export(\"keras._legacy.backend.depthwise_conv2d\")\n def depthwise_conv2d(\n     x,\n     depthwise_kernel,\n@@ -786,7 +786,7 @@ def depthwise_conv2d(\n     return x\n \n \n-@keras_core_export(\"keras_core._legacy.backend.dot\")\n+@keras_export(\"keras._legacy.backend.dot\")\n def dot(x, y):\n     \"\"\"DEPRECATED.\"\"\"\n     if ndim(x) is not None and (ndim(x) > 2 or ndim(y) > 2):\n@@ -818,7 +818,7 @@ def dot(x, y):\n     return out\n \n \n-@keras_core_export(\"keras_core._legacy.backend.dropout\")\n+@keras_export(\"keras._legacy.backend.dropout\")\n def dropout(x, level, noise_shape=None, seed=None):\n     \"\"\"DEPRECATED.\"\"\"\n     if seed is None:\n@@ -826,13 +826,13 @@ def dropout(x, level, noise_shape=None, seed=None):\n     return tf.nn.dropout(x, rate=level, noise_shape=noise_shape, seed=seed)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.dtype\")\n+@keras_export(\"keras._legacy.backend.dtype\")\n def dtype(x):\n     \"\"\"DEPRECATED.\"\"\"\n     return x.dtype.base_dtype.name\n \n \n-@keras_core_export(\"keras_core._legacy.backend.elu\")\n+@keras_export(\"keras._legacy.backend.elu\")\n def elu(x, alpha=1.0):\n     \"\"\"DEPRECATED.\"\"\"\n     res = tf.nn.elu(x)\n@@ -842,31 +842,31 @@ def elu(x, alpha=1.0):\n         return tf.where(x > 0, res, alpha * res)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.equal\")\n+@keras_export(\"keras._legacy.backend.equal\")\n def equal(x, y):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.equal(x, y)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.eval\")\n+@keras_export(\"keras._legacy.backend.eval\")\n def eval(x):\n     \"\"\"DEPRECATED.\"\"\"\n     return get_value(to_dense(x))\n \n \n-@keras_core_export(\"keras_core._legacy.backend.exp\")\n+@keras_export(\"keras._legacy.backend.exp\")\n def exp(x):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.exp(x)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.expand_dims\")\n+@keras_export(\"keras._legacy.backend.expand_dims\")\n def expand_dims(x, axis=-1):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.expand_dims(x, axis)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.eye\")\n+@keras_export(\"keras._legacy.backend.eye\")\n def eye(size, dtype=None, name=None):\n     \"\"\"DEPRECATED.\"\"\"\n     if dtype is None:\n@@ -875,31 +875,31 @@ def eye(size, dtype=None, name=None):\n     return variable(tf.eye(size, dtype=tf_dtype), dtype, name)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.flatten\")\n+@keras_export(\"keras._legacy.backend.flatten\")\n def flatten(x):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.reshape(x, [-1])\n \n \n-@keras_core_export(\"keras_core._legacy.backend.foldl\")\n+@keras_export(\"keras._legacy.backend.foldl\")\n def foldl(fn, elems, initializer=None, name=None):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.compat.v1.foldl(fn, elems, initializer=initializer, name=name)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.foldr\")\n+@keras_export(\"keras._legacy.backend.foldr\")\n def foldr(fn, elems, initializer=None, name=None):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.compat.v1.foldr(fn, elems, initializer=initializer, name=name)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.gather\")\n+@keras_export(\"keras._legacy.backend.gather\")\n def gather(reference, indices):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.compat.v1.gather(reference, indices)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.get_value\")\n+@keras_export(\"keras._legacy.backend.get_value\")\n def get_value(x):\n     \"\"\"DEPRECATED.\"\"\"\n     if not tf.is_tensor(x):\n@@ -915,7 +915,7 @@ def get_value(x):\n         return x.numpy()\n \n \n-@keras_core_export(\"keras_core._legacy.backend.gradients\")\n+@keras_export(\"keras._legacy.backend.gradients\")\n def gradients(loss, variables):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.compat.v1.gradients(\n@@ -923,19 +923,19 @@ def gradients(loss, variables):\n     )\n \n \n-@keras_core_export(\"keras_core._legacy.backend.greater\")\n+@keras_export(\"keras._legacy.backend.greater\")\n def greater(x, y):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.greater(x, y)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.greater_equal\")\n+@keras_export(\"keras._legacy.backend.greater_equal\")\n def greater_equal(x, y):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.greater_equal(x, y)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.hard_sigmoid\")\n+@keras_export(\"keras._legacy.backend.hard_sigmoid\")\n def hard_sigmoid(x):\n     \"\"\"DEPRECATED.\"\"\"\n     point_two = tf.convert_to_tensor(0.2, dtype=x.dtype)\n@@ -946,13 +946,13 @@ def hard_sigmoid(x):\n     return x\n \n \n-@keras_core_export(\"keras_core._legacy.backend.in_top_k\")\n+@keras_export(\"keras._legacy.backend.in_top_k\")\n def in_top_k(predictions, targets, k):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.compat.v1.math.in_top_k(predictions, targets, k)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.int_shape\")\n+@keras_export(\"keras._legacy.backend.int_shape\")\n def int_shape(x):\n     \"\"\"DEPRECATED.\"\"\"\n     try:\n@@ -964,7 +964,7 @@ def int_shape(x):\n         return None\n \n \n-@keras_core_export(\"keras_core._legacy.backend.is_sparse\")\n+@keras_export(\"keras._legacy.backend.is_sparse\")\n def is_sparse(tensor):\n     \"\"\"DEPRECATED.\"\"\"\n     spec = getattr(tensor, \"_type_spec\", None)\n@@ -973,49 +973,49 @@ def is_sparse(tensor):\n     return isinstance(tensor, tf.SparseTensor)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.l2_normalize\")\n+@keras_export(\"keras._legacy.backend.l2_normalize\")\n def l2_normalize(x, axis=None):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.linalg.l2_normalize(x, axis=axis)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.less\")\n+@keras_export(\"keras._legacy.backend.less\")\n def less(x, y):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.less(x, y)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.less_equal\")\n+@keras_export(\"keras._legacy.backend.less_equal\")\n def less_equal(x, y):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.less_equal(x, y)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.log\")\n+@keras_export(\"keras._legacy.backend.log\")\n def log(x):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.math.log(x)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.map_fn\")\n+@keras_export(\"keras._legacy.backend.map_fn\")\n def map_fn(fn, elems, name=None, dtype=None):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.compat.v1.map_fn(fn, elems, name=name, dtype=dtype)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.max\")\n+@keras_export(\"keras._legacy.backend.max\")\n def max(x, axis=None, keepdims=False):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.reduce_max(x, axis, keepdims)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.maximum\")\n+@keras_export(\"keras._legacy.backend.maximum\")\n def maximum(x, y):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.maximum(x, y)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.mean\")\n+@keras_export(\"keras._legacy.backend.mean\")\n def mean(x, axis=None, keepdims=False):\n     \"\"\"DEPRECATED.\"\"\"\n     if x.dtype.base_dtype == tf.bool:\n@@ -1023,19 +1023,19 @@ def mean(x, axis=None, keepdims=False):\n     return tf.reduce_mean(x, axis, keepdims)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.min\")\n+@keras_export(\"keras._legacy.backend.min\")\n def min(x, axis=None, keepdims=False):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.reduce_min(x, axis, keepdims)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.minimum\")\n+@keras_export(\"keras._legacy.backend.minimum\")\n def minimum(x, y):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.minimum(x, y)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.moving_average_update\")\n+@keras_export(\"keras._legacy.backend.moving_average_update\")\n def moving_average_update(x, value, momentum):\n     \"\"\"DEPRECATED.\"\"\"\n     momentum = tf.cast(momentum, x.dtype)\n@@ -1043,31 +1043,31 @@ def moving_average_update(x, value, momentum):\n     return x.assign_sub((x - value) * (1 - momentum))\n \n \n-@keras_core_export(\"keras_core._legacy.backend.name_scope\")\n+@keras_export(\"keras._legacy.backend.name_scope\")\n def name_scope(name):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.name_scope(name)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.ndim\")\n+@keras_export(\"keras._legacy.backend.ndim\")\n def ndim(x):\n     \"\"\"DEPRECATED.\"\"\"\n     return x.shape.rank\n \n \n-@keras_core_export(\"keras_core._legacy.backend.not_equal\")\n+@keras_export(\"keras._legacy.backend.not_equal\")\n def not_equal(x, y):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.not_equal(x, y)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.one_hot\")\n+@keras_export(\"keras._legacy.backend.one_hot\")\n def one_hot(indices, num_classes):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.one_hot(indices, depth=num_classes, axis=-1)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.ones\")\n+@keras_export(\"keras._legacy.backend.ones\")\n def ones(shape, dtype=None, name=None):\n     \"\"\"DEPRECATED.\"\"\"\n     with tf.init_scope():\n@@ -1080,19 +1080,19 @@ def ones(shape, dtype=None, name=None):\n         return v\n \n \n-@keras_core_export(\"keras_core._legacy.backend.ones_like\")\n+@keras_export(\"keras._legacy.backend.ones_like\")\n def ones_like(x, dtype=None, name=None):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.ones_like(x, dtype=dtype, name=name)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.permute_dimensions\")\n+@keras_export(\"keras._legacy.backend.permute_dimensions\")\n def permute_dimensions(x, pattern):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.transpose(x, perm=pattern)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.pool2d\")\n+@keras_export(\"keras._legacy.backend.pool2d\")\n def pool2d(\n     x,\n     pool_size,\n@@ -1136,7 +1136,7 @@ def pool2d(\n     return x\n \n \n-@keras_core_export(\"keras_core._legacy.backend.pool3d\")\n+@keras_export(\"keras._legacy.backend.pool3d\")\n def pool3d(\n     x,\n     pool_size,\n@@ -1176,19 +1176,19 @@ def pool3d(\n     return x\n \n \n-@keras_core_export(\"keras_core._legacy.backend.pow\")\n+@keras_export(\"keras._legacy.backend.pow\")\n def pow(x, a):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.pow(x, a)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.prod\")\n+@keras_export(\"keras._legacy.backend.prod\")\n def prod(x, axis=None, keepdims=False):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.reduce_prod(x, axis, keepdims)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.random_bernoulli\")\n+@keras_export(\"keras._legacy.backend.random_bernoulli\")\n def random_bernoulli(shape, p=0.0, dtype=None, seed=None):\n     \"\"\"DEPRECATED.\"\"\"\n     if dtype is None:\n@@ -1202,7 +1202,7 @@ def random_bernoulli(shape, p=0.0, dtype=None, seed=None):\n     )\n \n \n-@keras_core_export(\"keras_core._legacy.backend.random_normal\")\n+@keras_export(\"keras._legacy.backend.random_normal\")\n def random_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n     \"\"\"DEPRECATED.\"\"\"\n     if dtype is None:\n@@ -1214,7 +1214,7 @@ def random_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n     )\n \n \n-@keras_core_export(\"keras_core._legacy.backend.random_normal_variable\")\n+@keras_export(\"keras._legacy.backend.random_normal_variable\")\n def random_normal_variable(\n     shape, mean, scale, dtype=None, name=None, seed=None\n ):\n@@ -1231,7 +1231,7 @@ def random_normal_variable(\n     return variable(value, dtype=dtype, name=name)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.random_uniform\")\n+@keras_export(\"keras._legacy.backend.random_uniform\")\n def random_uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n     \"\"\"DEPRECATED.\"\"\"\n     if dtype is None:\n@@ -1243,7 +1243,7 @@ def random_uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n     )\n \n \n-@keras_core_export(\"keras_core._legacy.backend.random_uniform_variable\")\n+@keras_export(\"keras._legacy.backend.random_uniform_variable\")\n def random_uniform_variable(shape, low, high, dtype=None, name=None, seed=None):\n     \"\"\"DEPRECATED.\"\"\"\n     if dtype is None:\n@@ -1258,13 +1258,13 @@ def random_uniform_variable(shape, low, high, dtype=None, name=None, seed=None):\n     return variable(value, dtype=dtype, name=name)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.reshape\")\n+@keras_export(\"keras._legacy.backend.reshape\")\n def reshape(x, shape):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.reshape(x, shape)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.relu\")\n+@keras_export(\"keras._legacy.backend.relu\")\n def relu(x, alpha=0.0, max_value=None, threshold=0.0):\n     \"\"\"DEPRECATED.\"\"\"\n     # While x can be a tensor or variable, we also see cases where\n@@ -1303,7 +1303,7 @@ def relu(x, alpha=0.0, max_value=None, threshold=0.0):\n     return x\n \n \n-@keras_core_export(\"keras_core._legacy.backend.repeat\")\n+@keras_export(\"keras._legacy.backend.repeat\")\n def repeat(x, n):\n     \"\"\"DEPRECATED.\"\"\"\n     assert ndim(x) == 2\n@@ -1312,7 +1312,7 @@ def repeat(x, n):\n     return tf.tile(x, pattern)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.repeat_elements\")\n+@keras_export(\"keras._legacy.backend.repeat_elements\")\n def repeat_elements(x, rep, axis):\n     \"\"\"DEPRECATED.\"\"\"\n     x_shape = x.shape.as_list()\n@@ -1350,7 +1350,7 @@ def repeat_elements(x, rep, axis):\n     return x_rep\n \n \n-@keras_core_export(\"keras_core._legacy.backend.resize_images\")\n+@keras_export(\"keras._legacy.backend.resize_images\")\n def resize_images(\n     x, height_factor, width_factor, data_format, interpolation=\"nearest\"\n ):\n@@ -1397,7 +1397,7 @@ def resize_images(\n     return x\n \n \n-@keras_core_export(\"keras_core._legacy.backend.resize_volumes\")\n+@keras_export(\"keras._legacy.backend.resize_volumes\")\n def resize_volumes(x, depth_factor, height_factor, width_factor, data_format):\n     \"\"\"DEPRECATED.\"\"\"\n     if data_format == \"channels_first\":\n@@ -1414,7 +1414,7 @@ def resize_volumes(x, depth_factor, height_factor, width_factor, data_format):\n         raise ValueError(f\"Invalid data_format: {data_format}\")\n \n \n-@keras_core_export(\"keras_core._legacy.backend.reverse\")\n+@keras_export(\"keras._legacy.backend.reverse\")\n def reverse(x, axes):\n     \"\"\"DEPRECATED.\"\"\"\n     if isinstance(axes, int):\n@@ -1422,7 +1422,7 @@ def reverse(x, axes):\n     return tf.reverse(x, axes)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.rnn\")\n+@keras_export(\"keras._legacy.backend.rnn\")\n def rnn(\n     step_function,\n     inputs,\n@@ -1828,13 +1828,13 @@ def rnn(\n     return last_output, outputs, new_states\n \n \n-@keras_core_export(\"keras_core._legacy.backend.round\")\n+@keras_export(\"keras._legacy.backend.round\")\n def round(x):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.round(x)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.separable_conv2d\")\n+@keras_export(\"keras._legacy.backend.separable_conv2d\")\n def separable_conv2d(\n     x,\n     depthwise_kernel,\n@@ -1875,39 +1875,39 @@ def separable_conv2d(\n     return x\n \n \n-@keras_core_export(\"keras_core._legacy.backend.set_value\")\n+@keras_export(\"keras._legacy.backend.set_value\")\n def set_value(x, value):\n     \"\"\"DEPRECATED.\"\"\"\n     value = np.asarray(value, dtype=x.dtype.name)\n     x.assign(value)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.shape\")\n+@keras_export(\"keras._legacy.backend.shape\")\n def shape(x):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.shape(x)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.sigmoid\")\n+@keras_export(\"keras._legacy.backend.sigmoid\")\n def sigmoid(x):\n     \"\"\"DEPRECATED.\"\"\"\n     output = tf.sigmoid(x)\n     return output\n \n \n-@keras_core_export(\"keras_core._legacy.backend.sign\")\n+@keras_export(\"keras._legacy.backend.sign\")\n def sign(x):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.sign(x)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.sin\")\n+@keras_export(\"keras._legacy.backend.sin\")\n def sin(x):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.sin(x)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.softmax\")\n+@keras_export(\"keras._legacy.backend.softmax\")\n def softmax(x, axis=-1):\n     \"\"\"DEPRECATED.\"\"\"\n     if x.shape.rank <= 1:\n@@ -1928,19 +1928,19 @@ def softmax(x, axis=-1):\n     return output\n \n \n-@keras_core_export(\"keras_core._legacy.backend.softplus\")\n+@keras_export(\"keras._legacy.backend.softplus\")\n def softplus(x):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.math.softplus(x)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.softsign\")\n+@keras_export(\"keras._legacy.backend.softsign\")\n def softsign(x):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.math.softsign(x)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.sparse_categorical_crossentropy\")\n+@keras_export(\"keras._legacy.backend.sparse_categorical_crossentropy\")\n def sparse_categorical_crossentropy(\n     target, output, from_logits=False, axis=-1, ignore_class=None\n ):\n@@ -2013,7 +2013,7 @@ def sparse_categorical_crossentropy(\n     return res\n \n \n-@keras_core_export(\"keras_core._legacy.backend.spatial_2d_padding\")\n+@keras_export(\"keras._legacy.backend.spatial_2d_padding\")\n def spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None):\n     \"\"\"DEPRECATED.\"\"\"\n     assert len(padding) == 2\n@@ -2031,7 +2031,7 @@ def spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None):\n     return tf.compat.v1.pad(x, pattern)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.spatial_3d_padding\")\n+@keras_export(\"keras._legacy.backend.spatial_3d_padding\")\n def spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None):\n     \"\"\"DEPRECATED.\"\"\"\n     assert len(padding) == 3\n@@ -2062,7 +2062,7 @@ def spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None):\n     return tf.compat.v1.pad(x, pattern)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.sqrt\")\n+@keras_export(\"keras._legacy.backend.sqrt\")\n def sqrt(x):\n     \"\"\"DEPRECATED.\"\"\"\n     zero = tf.convert_to_tensor(0.0, x.dtype)\n@@ -2070,25 +2070,25 @@ def sqrt(x):\n     return tf.sqrt(x)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.square\")\n+@keras_export(\"keras._legacy.backend.square\")\n def square(x):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.square(x)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.squeeze\")\n+@keras_export(\"keras._legacy.backend.squeeze\")\n def squeeze(x, axis):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.squeeze(x, [axis])\n \n \n-@keras_core_export(\"keras_core._legacy.backend.stack\")\n+@keras_export(\"keras._legacy.backend.stack\")\n def stack(x, axis=0):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.stack(x, axis=axis)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.std\")\n+@keras_export(\"keras._legacy.backend.std\")\n def std(x, axis=None, keepdims=False):\n     \"\"\"DEPRECATED.\"\"\"\n     if x.dtype.base_dtype == tf.bool:\n@@ -2096,7 +2096,7 @@ def std(x, axis=None, keepdims=False):\n     return tf.math.reduce_std(x, axis=axis, keepdims=keepdims)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.stop_gradient\")\n+@keras_export(\"keras._legacy.backend.stop_gradient\")\n def stop_gradient(variables):\n     \"\"\"DEPRECATED.\"\"\"\n     if isinstance(variables, (list, tuple)):\n@@ -2104,13 +2104,13 @@ def stop_gradient(variables):\n     return tf.stop_gradient(variables)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.sum\")\n+@keras_export(\"keras._legacy.backend.sum\")\n def sum(x, axis=None, keepdims=False):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.reduce_sum(x, axis, keepdims)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.switch\")\n+@keras_export(\"keras._legacy.backend.switch\")\n def switch(condition, then_expression, else_expression):\n     \"\"\"DEPRECATED.\"\"\"\n     if condition.dtype != tf.bool:\n@@ -2166,13 +2166,13 @@ def switch(condition, then_expression, else_expression):\n     return x\n \n \n-@keras_core_export(\"keras_core._legacy.backend.tanh\")\n+@keras_export(\"keras._legacy.backend.tanh\")\n def tanh(x):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.tanh(x)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.temporal_padding\")\n+@keras_export(\"keras._legacy.backend.temporal_padding\")\n def temporal_padding(x, padding=(1, 1)):\n     \"\"\"DEPRECATED.\"\"\"\n     assert len(padding) == 2\n@@ -2180,7 +2180,7 @@ def temporal_padding(x, padding=(1, 1)):\n     return tf.compat.v1.pad(x, pattern)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.tile\")\n+@keras_export(\"keras._legacy.backend.tile\")\n def tile(x, n):\n     \"\"\"DEPRECATED.\"\"\"\n     if isinstance(n, int):\n@@ -2188,7 +2188,7 @@ def tile(x, n):\n     return tf.tile(x, n)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.to_dense\")\n+@keras_export(\"keras._legacy.backend.to_dense\")\n def to_dense(tensor):\n     \"\"\"DEPRECATED.\"\"\"\n     if is_sparse(tensor):\n@@ -2197,13 +2197,13 @@ def to_dense(tensor):\n         return tensor\n \n \n-@keras_core_export(\"keras_core._legacy.backend.transpose\")\n+@keras_export(\"keras._legacy.backend.transpose\")\n def transpose(x):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.transpose(x)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.truncated_normal\")\n+@keras_export(\"keras._legacy.backend.truncated_normal\")\n def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n     \"\"\"DEPRECATED.\"\"\"\n     if dtype is None:\n@@ -2215,25 +2215,25 @@ def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n     )\n \n \n-@keras_core_export(\"keras_core._legacy.backend.update\")\n+@keras_export(\"keras._legacy.backend.update\")\n def update(x, new_x):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.compat.v1.assign(x, new_x)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.update_add\")\n+@keras_export(\"keras._legacy.backend.update_add\")\n def update_add(x, increment):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.compat.v1.assign_add(x, increment)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.update_sub\")\n+@keras_export(\"keras._legacy.backend.update_sub\")\n def update_sub(x, decrement):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.compat.v1.assign_sub(x, decrement)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.var\")\n+@keras_export(\"keras._legacy.backend.var\")\n def var(x, axis=None, keepdims=False):\n     \"\"\"DEPRECATED.\"\"\"\n     if x.dtype.base_dtype == tf.bool:\n@@ -2241,7 +2241,7 @@ def var(x, axis=None, keepdims=False):\n     return tf.math.reduce_variance(x, axis=axis, keepdims=keepdims)\n \n \n-@keras_core_export(\"keras_core._legacy.backend.variable\")\n+@keras_export(\"keras._legacy.backend.variable\")\n def variable(value, dtype=None, name=None, constraint=None):\n     \"\"\"DEPRECATED.\"\"\"\n     if dtype is None:\n@@ -2268,7 +2268,7 @@ def variable(value, dtype=None, name=None, constraint=None):\n     return v\n \n \n-@keras_core_export(\"keras_core._legacy.backend.zeros\")\n+@keras_export(\"keras._legacy.backend.zeros\")\n def zeros(shape, dtype=None, name=None):\n     \"\"\"DEPRECATED.\"\"\"\n     with tf.init_scope():\n@@ -2281,7 +2281,7 @@ def zeros(shape, dtype=None, name=None):\n         return v\n \n \n-@keras_core_export(\"keras_core._legacy.backend.zeros_like\")\n+@keras_export(\"keras._legacy.backend.zeros_like\")\n def zeros_like(x, dtype=None, name=None):\n     \"\"\"DEPRECATED.\"\"\"\n     return tf.zeros_like(x, dtype=dtype, name=name)\n\n@@ -6,13 +6,13 @@ RandomWidth\n ThresholdedReLU\n \"\"\"\n \n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n-from keras_core.utils.module_utils import tensorflow as tf\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.layers.layer import Layer\n+from keras.utils.module_utils import tensorflow as tf\n \n \n-@keras_core_export(\"keras_core._legacy.layers.AlphaDropout\")\n+@keras_export(\"keras._legacy.layers.AlphaDropout\")\n class AlphaDropout(Layer):\n     \"\"\"DEPRECATED.\"\"\"\n \n@@ -62,7 +62,7 @@ class AlphaDropout(Layer):\n         return input_shape\n \n \n-@keras_core_export(\"keras_core._legacy.layers.RandomHeight\")\n+@keras_export(\"keras._legacy.layers.RandomHeight\")\n class RandomHeight(Layer):\n     \"\"\"DEPRECATED.\"\"\"\n \n@@ -138,7 +138,7 @@ class RandomHeight(Layer):\n         return {**base_config, **config}\n \n \n-@keras_core_export(\"keras_core._legacy.layers.RandomWidth\")\n+@keras_export(\"keras._legacy.layers.RandomWidth\")\n class RandomWidth(Layer):\n     \"\"\"DEPRECATED.\"\"\"\n \n@@ -213,7 +213,7 @@ class RandomWidth(Layer):\n         return {**base_config, **config}\n \n \n-@keras_core_export(\"keras_core._legacy.layers.ThresholdedReLU\")\n+@keras_export(\"keras._legacy.layers.ThresholdedReLU\")\n class ThresholdedReLU(Layer):\n     \"\"\"DEPRECATED.\"\"\"\n \n\n@@ -1,7 +1,7 @@\n-from keras_core.api_export import keras_core_export\n+from keras.api_export import keras_export\n \n \n-@keras_core_export(\"keras_core._legacy.losses.Reduction\")\n+@keras_export(\"keras._legacy.losses.Reduction\")\n class Reduction:\n     AUTO = \"auto\"\n     NONE = \"none\"\n\n\n@@ -8,15 +8,15 @@ import warnings\n \n import numpy as np\n \n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.trainers.data_adapters.py_dataset_adapter import PyDataset\n-from keras_core.utils import image_utils\n-from keras_core.utils import io_utils\n-from keras_core.utils.module_utils import scipy\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.trainers.data_adapters.py_dataset_adapter import PyDataset\n+from keras.utils import image_utils\n+from keras.utils import io_utils\n+from keras.utils.module_utils import scipy\n \n \n-@keras_core_export(\"keras_core._legacy.preprocessing.image.Iterator\")\n+@keras_export(\"keras._legacy.preprocessing.image.Iterator\")\n class Iterator(PyDataset):\n     \"\"\"Base class for image data iterators.\n \n@@ -389,7 +389,7 @@ class BatchFromFilesMixin:\n         )\n \n \n-@keras_core_export(\"keras_core._legacy.preprocessing.image.DirectoryIterator\")\n+@keras_export(\"keras._legacy.preprocessing.image.DirectoryIterator\")\n class DirectoryIterator(BatchFromFilesMixin, Iterator):\n     \"\"\"Iterator capable of reading images from a directory on disk.\n \n@@ -512,7 +512,7 @@ class DirectoryIterator(BatchFromFilesMixin, Iterator):\n         return None\n \n \n-@keras_core_export(\"keras_core._legacy.preprocessing.image.NumpyArrayIterator\")\n+@keras_export(\"keras._legacy.preprocessing.image.NumpyArrayIterator\")\n class NumpyArrayIterator(Iterator):\n     \"\"\"Iterator yielding data from a Numpy array.\n \n@@ -946,7 +946,7 @@ def flip_axis(x, axis):\n     return x\n \n \n-@keras_core_export(\"keras_core._legacy.preprocessing.image.ImageDataGenerator\")\n+@keras_export(\"keras._legacy.preprocessing.image.ImageDataGenerator\")\n class ImageDataGenerator:\n     \"\"\"DEPRECATED.\"\"\"\n \n@@ -1546,7 +1546,7 @@ class ImageDataGenerator:\n             self.zca_whitening_matrix = (u * s_inv).dot(u.T)\n \n \n-@keras_core_export(\"keras_core._legacy.preprocessing.image.random_rotation\")\n+@keras_export(\"keras._legacy.preprocessing.image.random_rotation\")\n def random_rotation(\n     x,\n     rg,\n@@ -1572,7 +1572,7 @@ def random_rotation(\n     return x\n \n \n-@keras_core_export(\"keras_core._legacy.preprocessing.image.random_shift\")\n+@keras_export(\"keras._legacy.preprocessing.image.random_shift\")\n def random_shift(\n     x,\n     wrg,\n@@ -1602,7 +1602,7 @@ def random_shift(\n     return x\n \n \n-@keras_core_export(\"keras_core._legacy.preprocessing.image.random_shear\")\n+@keras_export(\"keras._legacy.preprocessing.image.random_shear\")\n def random_shear(\n     x,\n     intensity,\n@@ -1628,7 +1628,7 @@ def random_shear(\n     return x\n \n \n-@keras_core_export(\"keras_core._legacy.preprocessing.image.random_zoom\")\n+@keras_export(\"keras._legacy.preprocessing.image.random_zoom\")\n def random_zoom(\n     x,\n     zoom_range,\n@@ -1664,7 +1664,7 @@ def random_zoom(\n     return x\n \n \n-@keras_core_export(\"keras_core._legacy.preprocessing.image.apply_channel_shift\")\n+@keras_export(\"keras._legacy.preprocessing.image.apply_channel_shift\")\n def apply_channel_shift(x, intensity, channel_axis=0):\n     \"\"\"Performs a channel shift.\n \n@@ -1688,8 +1688,8 @@ def apply_channel_shift(x, intensity, channel_axis=0):\n     return x\n \n \n-@keras_core_export(\n-    \"keras_core._legacy.preprocessing.image.random_channel_shift\"\n+@keras_export(\n+    \"keras._legacy.preprocessing.image.random_channel_shift\"\n )\n def random_channel_shift(x, intensity_range, channel_axis=0):\n     \"\"\"Performs a random channel shift.\n@@ -1708,8 +1708,8 @@ def random_channel_shift(x, intensity_range, channel_axis=0):\n     return apply_channel_shift(x, intensity, channel_axis=channel_axis)\n \n \n-@keras_core_export(\n-    \"keras_core._legacy.preprocessing.image.apply_brightness_shift\"\n+@keras_export(\n+    \"keras._legacy.preprocessing.image.apply_brightness_shift\"\n )\n def apply_brightness_shift(x, brightness, scale=True):\n     \"\"\"Performs a brightness shift.\n@@ -1741,7 +1741,7 @@ def apply_brightness_shift(x, brightness, scale=True):\n     return x\n \n \n-@keras_core_export(\"keras_core._legacy.preprocessing.image.random_brightness\")\n+@keras_export(\"keras._legacy.preprocessing.image.random_brightness\")\n def random_brightness(x, brightness_range, scale=True):\n     \"\"\"Performs a random brightness shift.\n \n@@ -1778,8 +1778,8 @@ def transform_matrix_offset_center(matrix, x, y):\n     return transform_matrix\n \n \n-@keras_core_export(\n-    \"keras_core._legacy.preprocessing.image.apply_affine_transform\"\n+@keras_export(\n+    \"keras._legacy.preprocessing.image.apply_affine_transform\"\n )\n def apply_affine_transform(\n     x,\n\n@@ -5,12 +5,12 @@ import random\n \n import numpy as np\n \n-from keras_core.api_export import keras_core_export\n-from keras_core.trainers.data_adapters.py_dataset_adapter import PyDataset\n+from keras.api_export import keras_export\n+from keras.trainers.data_adapters.py_dataset_adapter import PyDataset\n \n \n-@keras_core_export(\n-    \"keras_core._legacy.preprocessing.sequence.TimeseriesGenerator\"\n+@keras_export(\n+    \"keras._legacy.preprocessing.sequence.TimeseriesGenerator\"\n )\n class TimeseriesGenerator(PyDataset):\n     \"\"\"Utility class for generating batches of temporal data.\n@@ -178,8 +178,8 @@ class TimeseriesGenerator(PyDataset):\n         return json.dumps(timeseries_generator_config, **kwargs)\n \n \n-@keras_core_export(\n-    \"keras_core._legacy.preprocessing.sequence.make_sampling_table\"\n+@keras_export(\n+    \"keras._legacy.preprocessing.sequence.make_sampling_table\"\n )\n def make_sampling_table(size, sampling_factor=1e-5):\n     \"\"\"Generates a word rank-based probabilistic sampling table.\n@@ -222,7 +222,7 @@ def make_sampling_table(size, sampling_factor=1e-5):\n     return np.minimum(1.0, f / np.sqrt(f))\n \n \n-@keras_core_export(\"keras_core._legacy.preprocessing.sequence.skipgrams\")\n+@keras_export(\"keras._legacy.preprocessing.sequence.skipgrams\")\n def skipgrams(\n     sequence,\n     vocabulary_size,\n\n@@ -7,11 +7,11 @@ import warnings\n \n import numpy as np\n \n-from keras_core.api_export import keras_core_export\n+from keras.api_export import keras_export\n \n \n-@keras_core_export(\n-    \"keras_core._legacy.preprocessing.text.text_to_word_sequence\"\n+@keras_export(\n+    \"keras._legacy.preprocessing.text.text_to_word_sequence\"\n )\n def text_to_word_sequence(\n     input_text,\n@@ -31,7 +31,7 @@ def text_to_word_sequence(\n     return [i for i in seq if i]\n \n \n-@keras_core_export(\"keras_core._legacy.preprocessing.text.one_hot\")\n+@keras_export(\"keras._legacy.preprocessing.text.one_hot\")\n def one_hot(\n     input_text,\n     n,\n@@ -52,7 +52,7 @@ def one_hot(\n     )\n \n \n-@keras_core_export(\"keras_core._legacy.preprocessing.text.hashing_trick\")\n+@keras_export(\"keras._legacy.preprocessing.text.hashing_trick\")\n def hashing_trick(\n     text,\n     n,\n@@ -80,7 +80,7 @@ def hashing_trick(\n     return [(hash_function(w) % (n - 1) + 1) for w in seq]\n \n \n-@keras_core_export(\"keras_core._legacy.preprocessing.text.Tokenizer\")\n+@keras_export(\"keras._legacy.preprocessing.text.Tokenizer\")\n class Tokenizer(object):\n     \"\"\"DEPRECATED.\"\"\"\n \n@@ -314,7 +314,7 @@ class Tokenizer(object):\n         return json.dumps(tokenizer_config, **kwargs)\n \n \n-@keras_core_export(\"keras_core._legacy.preprocessing.text.tokenizer_from_json\")\n+@keras_export(\"keras._legacy.preprocessing.text.tokenizer_from_json\")\n def tokenizer_from_json(json_string):\n     \"\"\"DEPRECATED.\"\"\"\n     tokenizer_config = json.loads(json_string)\n\n\n@@ -7,9 +7,9 @@ import json\n \n import numpy as np\n \n-from keras_core.legacy.saving import serialization\n-from keras_core.saving import serialization_lib\n-from keras_core.utils.module_utils import tensorflow as tf\n+from keras.legacy.saving import serialization\n+from keras.saving import serialization_lib\n+from keras.utils.module_utils import tensorflow as tf\n \n _EXTENSION_TYPE_SPEC = \"_EXTENSION_TYPE_SPEC\"\n \n\n@@ -2,9 +2,9 @@ import enum\n \n import pytest\n \n-from keras_core import backend\n-from keras_core import testing\n-from keras_core.legacy.saving import json_utils\n+from keras import backend\n+from keras import testing\n+from keras.legacy.saving import json_utils\n \n if backend.backend() == \"tensorflow\":\n     import tensorflow as tf\n\n@@ -5,14 +5,14 @@ import warnings\n import numpy as np\n from absl import logging\n \n-from keras_core import backend\n-from keras_core import optimizers\n-from keras_core.backend.common import global_state\n-from keras_core.legacy.saving import json_utils\n-from keras_core.legacy.saving import saving_options\n-from keras_core.legacy.saving import saving_utils\n-from keras_core.saving import object_registration\n-from keras_core.utils import io_utils\n+from keras import backend\n+from keras import optimizers\n+from keras.backend.common import global_state\n+from keras.legacy.saving import json_utils\n+from keras.legacy.saving import saving_options\n+from keras.legacy.saving import saving_utils\n+from keras.saving import object_registration\n+from keras.utils import io_utils\n \n try:\n     import h5py\n@@ -200,7 +200,7 @@ def save_weights_to_hdf5_group(f, model):\n         f: HDF5 group.\n         model: Model instance.\n     \"\"\"\n-    from keras_core import __version__ as keras_version\n+    from keras import __version__ as keras_version\n \n     save_attributes_to_hdf5_group(\n         f, \"layer_names\", [layer.name.encode(\"utf8\") for layer in model.layers]\n\n@@ -3,14 +3,14 @@ import os\n import numpy as np\n import pytest\n \n-import keras_core\n-from keras_core import layers\n-from keras_core import models\n-from keras_core import ops\n-from keras_core import testing\n-from keras_core.legacy.saving import legacy_h5_format\n-from keras_core.saving import object_registration\n-from keras_core.saving import serialization_lib\n+import keras\n+from keras import layers\n+from keras import models\n+from keras import ops\n+from keras import testing\n+from keras.legacy.saving import legacy_h5_format\n+from keras.saving import object_registration\n+from keras.saving import serialization_lib\n \n # TODO: more thorough testing. Correctness depends\n # on exact weight ordering for each layer, so we need\n@@ -78,19 +78,19 @@ class LegacyH5WeightsTest(testing.TestCase):\n         self.assertAllClose(ref_output, output, atol=1e-5)\n \n     def DISABLED_test_sequential_model_weights(self):\n-        model = get_sequential_model(keras_core)\n+        model = get_sequential_model(keras)\n         tf_keras_model = get_sequential_model(tf_keras)\n         ref_input = np.random.random((2, 3))\n         self._check_reloading_weights(ref_input, model, tf_keras_model)\n \n     def DISABLED_test_functional_model_weights(self):\n-        model = get_functional_model(keras_core)\n+        model = get_functional_model(keras)\n         tf_keras_model = get_functional_model(tf_keras)\n         ref_input = np.random.random((2, 3))\n         self._check_reloading_weights(ref_input, model, tf_keras_model)\n \n     def DISABLED_test_subclassed_model_weights(self):\n-        model = get_subclassed_model(keras_core)\n+        model = get_subclassed_model(keras)\n         tf_keras_model = get_subclassed_model(tf_keras)\n         ref_input = np.random.random((2, 3))\n         self._check_reloading_weights(ref_input, model, tf_keras_model)\n@@ -108,12 +108,12 @@ class LegacyH5WholeModelTest(testing.TestCase):\n         self.assertAllClose(ref_output, output, atol=1e-5)\n \n     def DISABLED_test_sequential_model(self):\n-        model = get_sequential_model(keras_core)\n+        model = get_sequential_model(keras)\n         ref_input = np.random.random((2, 3))\n         self._check_reloading_model(ref_input, model)\n \n     def DISABLED_test_functional_model(self):\n-        model = get_functional_model(keras_core)\n+        model = get_functional_model(keras)\n         ref_input = np.random.random((2, 3))\n         self._check_reloading_model(ref_input, model)\n \n@@ -233,7 +233,7 @@ class LegacyH5WholeModelTest(testing.TestCase):\n         )\n         model = models.Sequential([layer])\n         with self.subTest(\"test_JSON\"):\n-            from keras_core.models.model import model_from_json\n+            from keras.models.model import model_from_json\n \n             model_json = model.to_json()\n             self.assertIn(\"Foo>RegisteredSubLayer\", model_json)\n@@ -274,14 +274,14 @@ class LegacyH5BackwardsCompatTest(testing.TestCase):\n         self.assertAllClose(ref_output, output, atol=1e-5)\n \n     def DISABLED_test_sequential_model(self):\n-        model = get_sequential_model(keras_core)\n+        model = get_sequential_model(keras)\n         tf_keras_model = get_sequential_model(tf_keras)\n         ref_input = np.random.random((2, 3))\n         self._check_reloading_model(ref_input, model, tf_keras_model)\n \n     def DISABLED_test_functional_model(self):\n         tf_keras_model = get_functional_model(tf_keras)\n-        model = get_functional_model(keras_core)\n+        model = get_functional_model(keras)\n         ref_input = np.random.random((2, 3))\n         self._check_reloading_model(ref_input, model, tf_keras_model)\n \n@@ -352,7 +352,7 @@ class LegacyH5BackwardsCompatTest(testing.TestCase):\n         custom_layer = MyDense(1)\n         tf_keras_model = tf_keras.Sequential(layers=[inputs, custom_layer])\n \n-        # Re-implement and re-register in Keras Core\n+        # Re-implement and re-register in Keras 3\n         @object_registration.register_keras_serializable(package=\"my_package\")\n         class MyDense(layers.Dense):\n             def __init__(self, units, **kwargs):\n@@ -375,7 +375,7 @@ class LegacyH5BackwardsCompatTest(testing.TestCase):\n         outputs = MyDense(1)(inputs)\n         tf_keras_model = tf_keras.Model(inputs, outputs)\n \n-        # Re-implement and re-register in Keras Core\n+        # Re-implement and re-register in Keras 3\n         @object_registration.register_keras_serializable(package=\"my_package\")\n         class MyDense(layers.Dense):\n             def __init__(self, units, **kwargs):\n@@ -434,7 +434,7 @@ class LegacyH5BackwardsCompatTest(testing.TestCase):\n         temp_filepath = os.path.join(self.get_temp_dir(), \"model.h5\")\n         tf_keras_model.save(temp_filepath)\n \n-        # Re-implement in Keras Core\n+        # Re-implement in Keras 3\n         class MyLayer(layers.Layer):\n             def __init__(self, sublayers, **kwargs):\n                 super().__init__(**kwargs)\n@@ -462,13 +462,13 @@ class LegacyH5BackwardsCompatTest(testing.TestCase):\n                 )\n                 return cls(**config)\n \n-        # Re-implement and re-register in Keras Core\n+        # Re-implement and re-register in Keras 3\n         @object_registration.register_keras_serializable(package=\"Foo\")\n         class RegisteredSubLayer(layers.Layer):\n             def call(self, x):\n                 return x\n \n-        # Load in Keras Core\n+        # Load in Keras 3\n         loaded_model = legacy_h5_format.load_model_from_hdf5(\n             temp_filepath, custom_objects={\"MyLayer\": MyLayer}\n         )\n@@ -489,7 +489,7 @@ class LegacyH5BackwardsCompatTest(testing.TestCase):\n class DirectoryCreationTest(testing.TestCase):\n     def DISABLED_test_directory_creation_on_save(self):\n         \"\"\"Test if directory is created on model save.\"\"\"\n-        model = get_sequential_model(keras_core)\n+        model = get_sequential_model(keras)\n         nested_dirpath = os.path.join(\n             self.get_temp_dir(), \"dir1\", \"dir2\", \"dir3\"\n         )\n\n@@ -1,6 +1,6 @@\n import contextlib\n \n-from keras_core.backend.common import global_state\n+from keras.backend.common import global_state\n \n \n @contextlib.contextmanager\n\n@@ -4,18 +4,18 @@ import threading\n import tree\n from absl import logging\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import losses\n-from keras_core import metrics as metrics_module\n-from keras_core import models\n-from keras_core import optimizers\n-from keras_core.legacy.saving import serialization\n-from keras_core.saving import object_registration\n+from keras import backend\n+from keras import layers\n+from keras import losses\n+from keras import metrics as metrics_module\n+from keras import models\n+from keras import optimizers\n+from keras.legacy.saving import serialization\n+from keras.saving import object_registration\n \n MODULE_OBJECTS = threading.local()\n \n-# Legacy lambda arguments not found in Keras Core\n+# Legacy lambda arguments not found in Keras 3\n LAMBDA_DEP_ARGS = (\n     \"module\",\n     \"function_type\",\n@@ -79,8 +79,8 @@ def model_from_config(config, custom_objects=None):\n             config[\"config\"][\"function\"] = function_dict\n \n     # TODO(nkovela): Swap find and replace args during Keras 3.0 release\n-    # Replace keras refs with keras_core\n-    config = _find_replace_nested_dict(config, \"keras.\", \"keras_core.\")\n+    # Replace keras refs with keras\n+    config = _find_replace_nested_dict(config, \"keras.\", \"keras.\")\n \n     return serialization.deserialize_keras_object(\n         config,\n@@ -92,7 +92,7 @@ def model_from_config(config, custom_objects=None):\n \n def model_metadata(model, include_optimizer=True, require_config=True):\n     \"\"\"Returns a dictionary containing the model metadata.\"\"\"\n-    from keras_core import __version__ as keras_version\n+    from keras import __version__ as keras_version\n \n     model_config = {\"class_name\": model.__class__.__name__}\n     try:\n@@ -238,7 +238,7 @@ def _resolve_compile_arguments_compat(obj, obj_config, module):\n \n     This helper function accepts built-in Keras modules such as optimizers,\n     losses, and metrics to ensure an object being deserialized is compatible\n-    with Keras Core built-ins. For legacy H5 files saved within Keras Core,\n+    with Keras 3 built-ins. For legacy H5 files saved within Keras 3,\n     this does nothing.\n     \"\"\"\n     if isinstance(obj, str) and obj not in module.ALL_OBJECTS_DICT:\n\n@@ -6,8 +6,8 @@ import threading\n import weakref\n \n # isort: off\n-from keras_core.api_export import keras_core_export\n-from keras_core.saving import object_registration\n+from keras.api_export import keras_export\n+from keras.saving import object_registration\n \n # Flag that determines whether to skip the NotImplementedError when calling\n # get_config in custom models and layers. This is only enabled when saving to\n@@ -261,10 +261,10 @@ def skip_failed_serialization():\n         _SKIP_FAILED_SERIALIZATION = prev\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.legacy.saving.serialize_keras_object\",\n-        \"keras_core.utils.legacy.serialize_keras_object\",\n+        \"keras.legacy.saving.serialize_keras_object\",\n+        \"keras.utils.legacy.serialize_keras_object\",\n     ]\n )\n def serialize_keras_object(instance):\n@@ -405,10 +405,10 @@ def class_and_config_for_serialized_keras_object(\n     return (cls, cls_config)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.legacy.saving.deserialize_keras_object\",\n-        \"keras_core.utils.legacy.deserialize_keras_object\",\n+        \"keras.legacy.saving.deserialize_keras_object\",\n+        \"keras.utils.legacy.deserialize_keras_object\",\n     ]\n )\n def deserialize_keras_object(\n@@ -485,9 +485,9 @@ def deserialize_keras_object(\n             custom_objects = custom_objects or {}\n \n             # TODO(nkovela): Swap find and replace args during Keras 3.0 release\n-            # Replace keras refs with keras_core\n+            # Replace keras refs with keras\n             cls_config = _find_replace_nested_dict(\n-                cls_config, \"keras.\", \"keras_core.\"\n+                cls_config, \"keras.\", \"keras.\"\n             )\n \n             if \"custom_objects\" in arg_spec.args:\n\n@@ -1,37 +1,37 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.losses.loss import Loss\n-from keras_core.losses.losses import BinaryCrossentropy\n-from keras_core.losses.losses import CategoricalCrossentropy\n-from keras_core.losses.losses import CategoricalHinge\n-from keras_core.losses.losses import CosineSimilarity\n-from keras_core.losses.losses import Hinge\n-from keras_core.losses.losses import Huber\n-from keras_core.losses.losses import KLDivergence\n-from keras_core.losses.losses import LogCosh\n-from keras_core.losses.losses import LossFunctionWrapper\n-from keras_core.losses.losses import MeanAbsoluteError\n-from keras_core.losses.losses import MeanAbsolutePercentageError\n-from keras_core.losses.losses import MeanSquaredError\n-from keras_core.losses.losses import MeanSquaredLogarithmicError\n-from keras_core.losses.losses import Poisson\n-from keras_core.losses.losses import SparseCategoricalCrossentropy\n-from keras_core.losses.losses import SquaredHinge\n-from keras_core.losses.losses import binary_crossentropy\n-from keras_core.losses.losses import categorical_crossentropy\n-from keras_core.losses.losses import categorical_hinge\n-from keras_core.losses.losses import cosine_similarity\n-from keras_core.losses.losses import hinge\n-from keras_core.losses.losses import huber\n-from keras_core.losses.losses import kl_divergence\n-from keras_core.losses.losses import log_cosh\n-from keras_core.losses.losses import mean_absolute_error\n-from keras_core.losses.losses import mean_absolute_percentage_error\n-from keras_core.losses.losses import mean_squared_error\n-from keras_core.losses.losses import mean_squared_logarithmic_error\n-from keras_core.losses.losses import poisson\n-from keras_core.losses.losses import sparse_categorical_crossentropy\n-from keras_core.losses.losses import squared_hinge\n-from keras_core.saving import serialization_lib\n+from keras.api_export import keras_export\n+from keras.losses.loss import Loss\n+from keras.losses.losses import BinaryCrossentropy\n+from keras.losses.losses import CategoricalCrossentropy\n+from keras.losses.losses import CategoricalHinge\n+from keras.losses.losses import CosineSimilarity\n+from keras.losses.losses import Hinge\n+from keras.losses.losses import Huber\n+from keras.losses.losses import KLDivergence\n+from keras.losses.losses import LogCosh\n+from keras.losses.losses import LossFunctionWrapper\n+from keras.losses.losses import MeanAbsoluteError\n+from keras.losses.losses import MeanAbsolutePercentageError\n+from keras.losses.losses import MeanSquaredError\n+from keras.losses.losses import MeanSquaredLogarithmicError\n+from keras.losses.losses import Poisson\n+from keras.losses.losses import SparseCategoricalCrossentropy\n+from keras.losses.losses import SquaredHinge\n+from keras.losses.losses import binary_crossentropy\n+from keras.losses.losses import categorical_crossentropy\n+from keras.losses.losses import categorical_hinge\n+from keras.losses.losses import cosine_similarity\n+from keras.losses.losses import hinge\n+from keras.losses.losses import huber\n+from keras.losses.losses import kl_divergence\n+from keras.losses.losses import log_cosh\n+from keras.losses.losses import mean_absolute_error\n+from keras.losses.losses import mean_absolute_percentage_error\n+from keras.losses.losses import mean_squared_error\n+from keras.losses.losses import mean_squared_logarithmic_error\n+from keras.losses.losses import poisson\n+from keras.losses.losses import sparse_categorical_crossentropy\n+from keras.losses.losses import squared_hinge\n+from keras.saving import serialization_lib\n \n ALL_OBJECTS = {\n     # Base\n@@ -94,7 +94,7 @@ ALL_OBJECTS_DICT.update(\n )\n \n \n-@keras_core_export(\"keras_core.losses.serialize\")\n+@keras_export(\"keras.losses.serialize\")\n def serialize(loss):\n     \"\"\"Serializes loss function or `Loss` instance.\n \n@@ -107,7 +107,7 @@ def serialize(loss):\n     return serialization_lib.serialize_keras_object(loss)\n \n \n-@keras_core_export(\"keras_core.losses.deserialize\")\n+@keras_export(\"keras.losses.deserialize\")\n def deserialize(name, custom_objects=None):\n     \"\"\"Deserializes a serialized loss class/function instance.\n \n@@ -127,7 +127,7 @@ def deserialize(name, custom_objects=None):\n     )\n \n \n-@keras_core_export(\"keras_core.losses.get\")\n+@keras_export(\"keras.losses.get\")\n def get(identifier):\n     \"\"\"Retrieves a Keras loss as a `function`/`Loss` class instance.\n \n\n@@ -1,12 +1,12 @@\n import tree\n \n-from keras_core import backend\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.utils.naming import auto_name\n+from keras import backend\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.utils.naming import auto_name\n \n \n-@keras_core_export([\"keras_core.Loss\", \"keras_core.losses.Loss\"])\n+@keras_export([\"keras.Loss\", \"keras.losses.Loss\"])\n class Loss:\n     \"\"\"Loss base class.\n \n\n@@ -1,11 +1,11 @@\n import numpy as np\n import pytest\n \n-from keras_core import backend\n-from keras_core import losses as losses_module\n-from keras_core import ops\n-from keras_core import testing\n-from keras_core.losses.loss import Loss\n+from keras import backend\n+from keras import losses as losses_module\n+from keras import ops\n+from keras import testing\n+from keras.losses.loss import Loss\n \n \n class ExampleLoss(Loss):\n\n@@ -1,12 +1,12 @@\n import warnings\n \n-from keras_core import backend\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.losses.loss import Loss\n-from keras_core.losses.loss import squeeze_to_same_rank\n-from keras_core.saving import serialization_lib\n-from keras_core.utils.numerical_utils import normalize\n+from keras import backend\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.losses.loss import Loss\n+from keras.losses.loss import squeeze_to_same_rank\n+from keras.saving import serialization_lib\n+from keras.utils.numerical_utils import normalize\n \n \n class LossFunctionWrapper(Loss):\n@@ -34,7 +34,7 @@ class LossFunctionWrapper(Loss):\n         return cls(**config)\n \n \n-@keras_core_export(\"keras_core.losses.MeanSquaredError\")\n+@keras_export(\"keras.losses.MeanSquaredError\")\n class MeanSquaredError(LossFunctionWrapper):\n     \"\"\"Computes the mean of squares of errors between labels and predictions.\n \n@@ -60,7 +60,7 @@ class MeanSquaredError(LossFunctionWrapper):\n         return Loss.get_config(self)\n \n \n-@keras_core_export(\"keras_core.losses.MeanAbsoluteError\")\n+@keras_export(\"keras.losses.MeanAbsoluteError\")\n class MeanAbsoluteError(LossFunctionWrapper):\n     \"\"\"Computes the mean of absolute difference between labels and predictions.\n \n@@ -86,7 +86,7 @@ class MeanAbsoluteError(LossFunctionWrapper):\n         return Loss.get_config(self)\n \n \n-@keras_core_export(\"keras_core.losses.MeanAbsolutePercentageError\")\n+@keras_export(\"keras.losses.MeanAbsolutePercentageError\")\n class MeanAbsolutePercentageError(LossFunctionWrapper):\n     \"\"\"Computes the mean absolute percentage error between `y_true` & `y_pred`.\n \n@@ -116,7 +116,7 @@ class MeanAbsolutePercentageError(LossFunctionWrapper):\n         return Loss.get_config(self)\n \n \n-@keras_core_export(\"keras_core.losses.MeanSquaredLogarithmicError\")\n+@keras_export(\"keras.losses.MeanSquaredLogarithmicError\")\n class MeanSquaredLogarithmicError(LossFunctionWrapper):\n     \"\"\"Computes the mean squared logarithmic error between `y_true` & `y_pred`.\n \n@@ -146,7 +146,7 @@ class MeanSquaredLogarithmicError(LossFunctionWrapper):\n         return Loss.get_config(self)\n \n \n-@keras_core_export(\"keras_core.losses.CosineSimilarity\")\n+@keras_export(\"keras.losses.CosineSimilarity\")\n class CosineSimilarity(LossFunctionWrapper):\n     \"\"\"Computes the cosine similarity between `y_true` & `y_pred`.\n \n@@ -183,7 +183,7 @@ class CosineSimilarity(LossFunctionWrapper):\n         )\n \n \n-@keras_core_export(\"keras_core.losses.Huber\")\n+@keras_export(\"keras.losses.Huber\")\n class Huber(LossFunctionWrapper):\n     \"\"\"Computes the Huber loss between `y_true` & `y_pred`.\n \n@@ -218,7 +218,7 @@ class Huber(LossFunctionWrapper):\n         super().__init__(huber, name=name, reduction=reduction, delta=delta)\n \n \n-@keras_core_export(\"keras_core.losses.LogCosh\")\n+@keras_export(\"keras.losses.LogCosh\")\n class LogCosh(LossFunctionWrapper):\n     \"\"\"Computes the logarithm of the hyperbolic cosine of the prediction error.\n \n@@ -241,7 +241,7 @@ class LogCosh(LossFunctionWrapper):\n         super().__init__(log_cosh, name=name, reduction=reduction)\n \n \n-@keras_core_export(\"keras_core.losses.Hinge\")\n+@keras_export(\"keras.losses.Hinge\")\n class Hinge(LossFunctionWrapper):\n     \"\"\"Computes the hinge loss between `y_true` & `y_pred`.\n \n@@ -268,7 +268,7 @@ class Hinge(LossFunctionWrapper):\n         return Loss.get_config(self)\n \n \n-@keras_core_export(\"keras_core.losses.SquaredHinge\")\n+@keras_export(\"keras.losses.SquaredHinge\")\n class SquaredHinge(LossFunctionWrapper):\n     \"\"\"Computes the squared hinge loss between `y_true` & `y_pred`.\n \n@@ -295,7 +295,7 @@ class SquaredHinge(LossFunctionWrapper):\n         return Loss.get_config(self)\n \n \n-@keras_core_export(\"keras_core.losses.CategoricalHinge\")\n+@keras_export(\"keras.losses.CategoricalHinge\")\n class CategoricalHinge(LossFunctionWrapper):\n     \"\"\"Computes the categorical hinge loss between `y_true` & `y_pred`.\n \n@@ -323,7 +323,7 @@ class CategoricalHinge(LossFunctionWrapper):\n         return Loss.get_config(self)\n \n \n-@keras_core_export(\"keras_core.losses.KLDivergence\")\n+@keras_export(\"keras.losses.KLDivergence\")\n class KLDivergence(LossFunctionWrapper):\n     \"\"\"Computes Kullback-Leibler divergence loss between `y_true` & `y_pred`.\n \n@@ -347,7 +347,7 @@ class KLDivergence(LossFunctionWrapper):\n         return Loss.get_config(self)\n \n \n-@keras_core_export(\"keras_core.losses.Poisson\")\n+@keras_export(\"keras.losses.Poisson\")\n class Poisson(LossFunctionWrapper):\n     \"\"\"Computes the Poisson loss between `y_true` & `y_pred`.\n \n@@ -371,7 +371,7 @@ class Poisson(LossFunctionWrapper):\n         return Loss.get_config(self)\n \n \n-@keras_core_export(\"keras_core.losses.BinaryCrossentropy\")\n+@keras_export(\"keras.losses.BinaryCrossentropy\")\n class BinaryCrossentropy(LossFunctionWrapper):\n     \"\"\"Computes the cross-entropy loss between true labels and predicted labels.\n \n@@ -409,7 +409,7 @@ class BinaryCrossentropy(LossFunctionWrapper):\n \n     ```python\n     model.compile(\n-        loss=keras_core.losses.BinaryCrossentropy(from_logits=True),\n+        loss=keras.losses.BinaryCrossentropy(from_logits=True),\n         ...\n     )\n     ```\n@@ -419,7 +419,7 @@ class BinaryCrossentropy(LossFunctionWrapper):\n     >>> # Example 1: (batch_size = 1, number of samples = 4)\n     >>> y_true = [0, 1, 0, 0]\n     >>> y_pred = [-18.6, 0.51, 2.94, -12.8]\n-    >>> bce = keras_core.losses.BinaryCrossentropy(from_logits=True)\n+    >>> bce = keras.losses.BinaryCrossentropy(from_logits=True)\n     >>> bce(y_true, y_pred)\n     0.865\n \n@@ -427,19 +427,19 @@ class BinaryCrossentropy(LossFunctionWrapper):\n     >>> y_true = [[0, 1], [0, 0]]\n     >>> y_pred = [[-18.6, 0.51], [2.94, -12.8]]\n     >>> # Using default 'auto'/'sum_over_batch_size' reduction type.\n-    >>> bce = keras_core.losses.BinaryCrossentropy(from_logits=True)\n+    >>> bce = keras.losses.BinaryCrossentropy(from_logits=True)\n     >>> bce(y_true, y_pred)\n     0.865\n     >>> # Using 'sample_weight' attribute\n     >>> bce(y_true, y_pred, sample_weight=[0.8, 0.2])\n     0.243\n     >>> # Using 'sum' reduction` type.\n-    >>> bce = keras_core.losses.BinaryCrossentropy(from_logits=True,\n+    >>> bce = keras.losses.BinaryCrossentropy(from_logits=True,\n     ...     reduction=\"sum\")\n     >>> bce(y_true, y_pred)\n     1.730\n     >>> # Using 'none' reduction type.\n-    >>> bce = keras_core.losses.BinaryCrossentropy(from_logits=True,\n+    >>> bce = keras.losses.BinaryCrossentropy(from_logits=True,\n     ...     reduction=None)\n     >>> bce(y_true, y_pred)\n     array([0.235, 1.496], dtype=float32)\n@@ -448,7 +448,7 @@ class BinaryCrossentropy(LossFunctionWrapper):\n \n     >>> # Make the following updates to the above \"Recommended Usage\" section\n     >>> # 1. Set `from_logits=False`\n-    >>> keras_core.losses.BinaryCrossentropy() # OR ...('from_logits=False')\n+    >>> keras.losses.BinaryCrossentropy() # OR ...('from_logits=False')\n     >>> # 2. Update `y_pred` to use probabilities instead of logits\n     >>> y_pred = [0.6, 0.3, 0.2, 0.8] # OR [[0.6, 0.3], [0.2, 0.8]]\n     \"\"\"\n@@ -483,7 +483,7 @@ class BinaryCrossentropy(LossFunctionWrapper):\n         }\n \n \n-@keras_core_export(\"keras_core.losses.BinaryFocalCrossentropy\")\n+@keras_export(\"keras.losses.BinaryFocalCrossentropy\")\n class BinaryFocalCrossentropy(LossFunctionWrapper):\n     \"\"\"Computes focal cross-entropy loss between true labels and predictions.\n \n@@ -537,7 +537,7 @@ class BinaryFocalCrossentropy(LossFunctionWrapper):\n \n     ```python\n     model.compile(\n-        loss=keras_core.losses.BinaryFocalCrossentropy(\n+        loss=keras.losses.BinaryFocalCrossentropy(\n             gamma=2.0, from_logits=True),\n         ...\n     )\n@@ -548,13 +548,13 @@ class BinaryFocalCrossentropy(LossFunctionWrapper):\n     >>> # Example 1: (batch_size = 1, number of samples = 4)\n     >>> y_true = [0, 1, 0, 0]\n     >>> y_pred = [-18.6, 0.51, 2.94, -12.8]\n-    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n+    >>> loss = keras.losses.BinaryFocalCrossentropy(\n     ...    gamma=2, from_logits=True)\n     >>> loss(y_true, y_pred)\n     0.691\n \n     >>> # Apply class weight\n-    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n+    >>> loss = keras.losses.BinaryFocalCrossentropy(\n     ...     apply_class_balancing=True, gamma=2, from_logits=True)\n     >>> loss(y_true, y_pred)\n     0.51\n@@ -563,52 +563,52 @@ class BinaryFocalCrossentropy(LossFunctionWrapper):\n     >>> y_true = [[0, 1], [0, 0]]\n     >>> y_pred = [[-18.6, 0.51], [2.94, -12.8]]\n     >>> # Using default 'auto'/'sum_over_batch_size' reduction type.\n-    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n+    >>> loss = keras.losses.BinaryFocalCrossentropy(\n     ...     gamma=3, from_logits=True)\n     >>> loss(y_true, y_pred)\n     0.647\n \n     >>> # Apply class weight\n-    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n+    >>> loss = keras.losses.BinaryFocalCrossentropy(\n     ...      apply_class_balancing=True, gamma=3, from_logits=True)\n     >>> loss(y_true, y_pred)\n     0.482\n \n     >>> # Using 'sample_weight' attribute with focal effect\n-    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n+    >>> loss = keras.losses.BinaryFocalCrossentropy(\n     ...     gamma=3, from_logits=True)\n     >>> loss(y_true, y_pred, sample_weight=[0.8, 0.2])\n     0.133\n \n     >>> # Apply class weight\n-    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n+    >>> loss = keras.losses.BinaryFocalCrossentropy(\n     ...      apply_class_balancing=True, gamma=3, from_logits=True)\n     >>> loss(y_true, y_pred, sample_weight=[0.8, 0.2])\n     0.097\n \n     >>> # Using 'sum' reduction` type.\n-    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n+    >>> loss = keras.losses.BinaryFocalCrossentropy(\n     ...     gamma=4, from_logits=True,\n     ...     reduction=\"sum\")\n     >>> loss(y_true, y_pred)\n     1.222\n \n     >>> # Apply class weight\n-    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n+    >>> loss = keras.losses.BinaryFocalCrossentropy(\n     ...     apply_class_balancing=True, gamma=4, from_logits=True,\n     ...     reduction=\"sum\")\n     >>> loss(y_true, y_pred)\n     0.914\n \n     >>> # Using 'none' reduction type.\n-    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n+    >>> loss = keras.losses.BinaryFocalCrossentropy(\n     ...     gamma=5, from_logits=True,\n     ...     reduction=None)\n     >>> loss(y_true, y_pred)\n     array([0.0017 1.1561], dtype=float32)\n \n     >>> # Apply class weight\n-    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n+    >>> loss = keras.losses.BinaryFocalCrossentropy(\n     ...     apply_class_balancing=True, gamma=5, from_logits=True,\n     ...     reduction=None)\n     >>> loss(y_true, y_pred)\n@@ -657,7 +657,7 @@ class BinaryFocalCrossentropy(LossFunctionWrapper):\n         }\n \n \n-@keras_core_export(\"keras_core.losses.CategoricalCrossentropy\")\n+@keras_export(\"keras.losses.CategoricalCrossentropy\")\n class CategoricalCrossentropy(LossFunctionWrapper):\n     \"\"\"Computes the crossentropy loss between the labels and predictions.\n \n@@ -689,7 +689,7 @@ class CategoricalCrossentropy(LossFunctionWrapper):\n     >>> y_true = [[0, 1, 0], [0, 0, 1]]\n     >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n     >>> # Using 'auto'/'sum_over_batch_size' reduction type.\n-    >>> cce = keras_core.losses.CategoricalCrossentropy()\n+    >>> cce = keras.losses.CategoricalCrossentropy()\n     >>> cce(y_true, y_pred)\n     1.177\n \n@@ -698,13 +698,13 @@ class CategoricalCrossentropy(LossFunctionWrapper):\n     0.814\n \n     >>> # Using 'sum' reduction type.\n-    >>> cce = keras_core.losses.CategoricalCrossentropy(\n+    >>> cce = keras.losses.CategoricalCrossentropy(\n     ...     reduction=\"sum\")\n     >>> cce(y_true, y_pred)\n     2.354\n \n     >>> # Using 'none' reduction type.\n-    >>> cce = keras_core.losses.CategoricalCrossentropy(\n+    >>> cce = keras.losses.CategoricalCrossentropy(\n     ...     reduction=None)\n     >>> cce(y_true, y_pred)\n     array([0.0513, 2.303], dtype=float32)\n@@ -713,7 +713,7 @@ class CategoricalCrossentropy(LossFunctionWrapper):\n \n     ```python\n     model.compile(optimizer='sgd',\n-                  loss=keras_core.losses.CategoricalCrossentropy())\n+                  loss=keras.losses.CategoricalCrossentropy())\n     ```\n     \"\"\"\n \n@@ -747,7 +747,7 @@ class CategoricalCrossentropy(LossFunctionWrapper):\n         }\n \n \n-@keras_core_export(\"keras_core.losses.CategoricalFocalCrossentropy\")\n+@keras_export(\"keras.losses.CategoricalFocalCrossentropy\")\n class CategoricalFocalCrossentropy(LossFunctionWrapper):\n     \"\"\"Computes the alpha balanced focal crossentropy loss.\n \n@@ -820,7 +820,7 @@ class CategoricalFocalCrossentropy(LossFunctionWrapper):\n     >>> y_true = [[0., 1., 0.], [0., 0., 1.]]\n     >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n     >>> # Using 'auto'/'sum_over_batch_size' reduction type.\n-    >>> cce = keras_core.losses.CategoricalFocalCrossentropy()\n+    >>> cce = keras.losses.CategoricalFocalCrossentropy()\n     >>> cce(y_true, y_pred)\n     0.23315276\n \n@@ -829,13 +829,13 @@ class CategoricalFocalCrossentropy(LossFunctionWrapper):\n     0.1632\n \n     >>> # Using 'sum' reduction type.\n-    >>> cce = keras_core.losses.CategoricalFocalCrossentropy(\n+    >>> cce = keras.losses.CategoricalFocalCrossentropy(\n     ...     reduction=\"sum\")\n     >>> cce(y_true, y_pred)\n     0.46631\n \n     >>> # Using 'none' reduction type.\n-    >>> cce = keras_core.losses.CategoricalFocalCrossentropy(\n+    >>> cce = keras.losses.CategoricalFocalCrossentropy(\n     ...     reduction=None)\n     >>> cce(y_true, y_pred)\n     array([3.2058331e-05, 4.6627346e-01], dtype=float32)\n@@ -844,7 +844,7 @@ class CategoricalFocalCrossentropy(LossFunctionWrapper):\n \n     ```python\n     model.compile(optimizer='adam',\n-                  loss=keras_core.losses.CategoricalFocalCrossentropy())\n+                  loss=keras.losses.CategoricalFocalCrossentropy())\n     ```\n     \"\"\"\n \n@@ -887,7 +887,7 @@ class CategoricalFocalCrossentropy(LossFunctionWrapper):\n         }\n \n \n-@keras_core_export(\"keras_core.losses.SparseCategoricalCrossentropy\")\n+@keras_export(\"keras.losses.SparseCategoricalCrossentropy\")\n class SparseCategoricalCrossentropy(LossFunctionWrapper):\n     \"\"\"Computes the crossentropy loss between the labels and predictions.\n \n@@ -916,7 +916,7 @@ class SparseCategoricalCrossentropy(LossFunctionWrapper):\n     >>> y_true = [1, 2]\n     >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n     >>> # Using 'auto'/'sum_over_batch_size' reduction type.\n-    >>> scce = keras_core.losses.SparseCategoricalCrossentropy()\n+    >>> scce = keras.losses.SparseCategoricalCrossentropy()\n     >>> scce(y_true, y_pred)\n     1.177\n \n@@ -925,13 +925,13 @@ class SparseCategoricalCrossentropy(LossFunctionWrapper):\n     0.814\n \n     >>> # Using 'sum' reduction type.\n-    >>> scce = keras_core.losses.SparseCategoricalCrossentropy(\n+    >>> scce = keras.losses.SparseCategoricalCrossentropy(\n     ...     reduction=\"sum\")\n     >>> scce(y_true, y_pred)\n     2.354\n \n     >>> # Using 'none' reduction type.\n-    >>> scce = keras_core.losses.SparseCategoricalCrossentropy(\n+    >>> scce = keras.losses.SparseCategoricalCrossentropy(\n     ...     reduction=None)\n     >>> scce(y_true, y_pred)\n     array([0.0513, 2.303], dtype=float32)\n@@ -940,7 +940,7 @@ class SparseCategoricalCrossentropy(LossFunctionWrapper):\n \n     ```python\n     model.compile(optimizer='sgd',\n-                  loss=keras_core.losses.SparseCategoricalCrossentropy())\n+                  loss=keras.losses.SparseCategoricalCrossentropy())\n     ```\n     \"\"\"\n \n@@ -990,10 +990,10 @@ def convert_binary_labels_to_hinge(y_true):\n     return updated_y_true\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.metrics.hinge\",\n-        \"keras_core.losses.hinge\",\n+        \"keras.metrics.hinge\",\n+        \"keras.losses.hinge\",\n     ]\n )\n def hinge(y_true, y_pred):\n@@ -1018,7 +1018,7 @@ def hinge(y_true, y_pred):\n \n     >>> y_true = np.random.choice([-1, 1], size=(2, 3))\n     >>> y_pred = np.random.random(size=(2, 3))\n-    >>> loss = keras_core.losses.hinge(y_true, y_pred)\n+    >>> loss = keras.losses.hinge(y_true, y_pred)\n     \"\"\"\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.cast(y_true, dtype=y_pred.dtype)\n@@ -1027,10 +1027,10 @@ def hinge(y_true, y_pred):\n     return ops.mean(ops.maximum(1.0 - y_true * y_pred, 0.0), axis=-1)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.metrics.squared_hinge\",\n-        \"keras_core.losses.squared_hinge\",\n+        \"keras.metrics.squared_hinge\",\n+        \"keras.losses.squared_hinge\",\n     ]\n )\n def squared_hinge(y_true, y_pred):\n@@ -1055,7 +1055,7 @@ def squared_hinge(y_true, y_pred):\n \n     >>> y_true = np.random.choice([-1, 1], size=(2, 3))\n     >>> y_pred = np.random.random(size=(2, 3))\n-    >>> loss = keras_core.losses.squared_hinge(y_true, y_pred)\n+    >>> loss = keras.losses.squared_hinge(y_true, y_pred)\n     \"\"\"\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.cast(y_true, y_pred.dtype)\n@@ -1065,10 +1065,10 @@ def squared_hinge(y_true, y_pred):\n     )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.metrics.categorical_hinge\",\n-        \"keras_core.losses.categorical_hinge\",\n+        \"keras.metrics.categorical_hinge\",\n+        \"keras.losses.categorical_hinge\",\n     ]\n )\n def categorical_hinge(y_true, y_pred):\n@@ -1096,7 +1096,7 @@ def categorical_hinge(y_true, y_pred):\n     >>> y_true = np.random.randint(0, 3, size=(2,))\n     >>> y_true = np.eye(np.max(y_true) + 1)[y_true]\n     >>> y_pred = np.random.random(size=(2, 3))\n-    >>> loss = keras_core.losses.categorical_hinge(y_true, y_pred)\n+    >>> loss = keras.losses.categorical_hinge(y_true, y_pred)\n     \"\"\"\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.cast(y_true, y_pred.dtype)\n@@ -1106,15 +1106,15 @@ def categorical_hinge(y_true, y_pred):\n     return ops.maximum(neg - pos + 1.0, zero)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.metrics.mean_squared_error\",\n-        \"keras_core.losses.mean_squared_error\",\n+        \"keras.metrics.mean_squared_error\",\n+        \"keras.losses.mean_squared_error\",\n         # Legacy aliases\n-        \"keras_core._legacy.losses.mse\",\n-        \"keras_core._legacy.losses.MSE\",\n-        \"keras_core._legacy.metrics.mse\",\n-        \"keras_core._legacy.metrics.MSE\",\n+        \"keras._legacy.losses.mse\",\n+        \"keras._legacy.losses.MSE\",\n+        \"keras._legacy.metrics.mse\",\n+        \"keras._legacy.metrics.MSE\",\n     ]\n )\n def mean_squared_error(y_true, y_pred):\n@@ -1130,7 +1130,7 @@ def mean_squared_error(y_true, y_pred):\n \n     >>> y_true = np.random.randint(0, 2, size=(2, 3))\n     >>> y_pred = np.random.random(size=(2, 3))\n-    >>> loss = keras_core.losses.mean_squared_error(y_true, y_pred)\n+    >>> loss = keras.losses.mean_squared_error(y_true, y_pred)\n \n     Args:\n         y_true: Ground truth values with shape = `[batch_size, d0, .. dN]`.\n@@ -1145,15 +1145,15 @@ def mean_squared_error(y_true, y_pred):\n     return ops.mean(ops.square(y_true - y_pred), axis=-1)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.metrics.mean_absolute_error\",\n-        \"keras_core.losses.mean_absolute_error\",\n+        \"keras.metrics.mean_absolute_error\",\n+        \"keras.losses.mean_absolute_error\",\n         # Legacy aliases\n-        \"keras_core._legacy.losses.MAE\",\n-        \"keras_core._legacy.losses.mae\",\n-        \"keras_core._legacy.metrics.MAE\",\n-        \"keras_core._legacy.metrics.mae\",\n+        \"keras._legacy.losses.MAE\",\n+        \"keras._legacy.losses.mae\",\n+        \"keras._legacy.metrics.MAE\",\n+        \"keras._legacy.metrics.mae\",\n     ]\n )\n def mean_absolute_error(y_true, y_pred):\n@@ -1174,7 +1174,7 @@ def mean_absolute_error(y_true, y_pred):\n \n     >>> y_true = np.random.randint(0, 2, size=(2, 3))\n     >>> y_pred = np.random.random(size=(2, 3))\n-    >>> loss = keras_core.losses.mean_absolute_error(y_true, y_pred)\n+    >>> loss = keras.losses.mean_absolute_error(y_true, y_pred)\n     \"\"\"\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.convert_to_tensor(y_true, dtype=y_pred.dtype)\n@@ -1182,15 +1182,15 @@ def mean_absolute_error(y_true, y_pred):\n     return ops.mean(ops.abs(y_true - y_pred), axis=-1)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.metrics.mean_absolute_percentage_error\",\n-        \"keras_core.losses.mean_absolute_percentage_error\",\n+        \"keras.metrics.mean_absolute_percentage_error\",\n+        \"keras.losses.mean_absolute_percentage_error\",\n         # Legacy aliases\n-        \"keras_core._legacy.losses.mape\",\n-        \"keras_core._legacy.losses.MAPE\",\n-        \"keras_core._legacy.metrics.mape\",\n-        \"keras_core._legacy.metrics.MAPE\",\n+        \"keras._legacy.losses.mape\",\n+        \"keras._legacy.losses.MAPE\",\n+        \"keras._legacy.metrics.mape\",\n+        \"keras._legacy.metrics.MAPE\",\n     ]\n )\n def mean_absolute_percentage_error(y_true, y_pred):\n@@ -1203,7 +1203,7 @@ def mean_absolute_percentage_error(y_true, y_pred):\n     ```\n \n     Division by zero is prevented by dividing by `maximum(y_true, epsilon)`\n-    where `epsilon = keras_core.backend.epsilon()`\n+    where `epsilon = keras.backend.epsilon()`\n     (default to `1e-7`).\n \n     Args:\n@@ -1218,7 +1218,7 @@ def mean_absolute_percentage_error(y_true, y_pred):\n \n     >>> y_true = np.random.random(size=(2, 3))\n     >>> y_pred = np.random.random(size=(2, 3))\n-    >>> loss = keras_core.losses.mean_absolute_percentage_error(y_true, y_pred)\n+    >>> loss = keras.losses.mean_absolute_percentage_error(y_true, y_pred)\n     \"\"\"\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.convert_to_tensor(y_true, dtype=y_pred.dtype)\n@@ -1228,15 +1228,15 @@ def mean_absolute_percentage_error(y_true, y_pred):\n     return 100.0 * ops.mean(diff, axis=-1)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.metrics.mean_squared_logarithmic_error\",\n-        \"keras_core.losses.mean_squared_logarithmic_error\",\n+        \"keras.metrics.mean_squared_logarithmic_error\",\n+        \"keras.losses.mean_squared_logarithmic_error\",\n         # Legacy aliases\n-        \"keras_core._legacy.losses.msle\",\n-        \"keras_core._legacy.losses.MSLE\",\n-        \"keras_core._legacy.metrics.msle\",\n-        \"keras_core._legacy.metrics.MSLE\",\n+        \"keras._legacy.losses.msle\",\n+        \"keras._legacy.losses.MSLE\",\n+        \"keras._legacy.metrics.msle\",\n+        \"keras._legacy.metrics.MSLE\",\n     ]\n )\n def mean_squared_logarithmic_error(y_true, y_pred):\n@@ -1249,7 +1249,7 @@ def mean_squared_logarithmic_error(y_true, y_pred):\n     ```\n \n     Note that `y_pred` and `y_true` cannot be less or equal to 0. Negative\n-    values and 0 values will be replaced with `keras_core.backend.epsilon()`\n+    values and 0 values will be replaced with `keras.backend.epsilon()`\n     (default to `1e-7`).\n \n     Args:\n@@ -1264,7 +1264,7 @@ def mean_squared_logarithmic_error(y_true, y_pred):\n \n     >>> y_true = np.random.randint(0, 2, size=(2, 3))\n     >>> y_pred = np.random.random(size=(2, 3))\n-    >>> loss = keras_core.losses.mean_squared_logarithmic_error(y_true, y_pred)\n+    >>> loss = keras.losses.mean_squared_logarithmic_error(y_true, y_pred)\n     \"\"\"\n     epsilon = ops.convert_to_tensor(backend.epsilon())\n     y_pred = ops.convert_to_tensor(y_pred)\n@@ -1275,7 +1275,7 @@ def mean_squared_logarithmic_error(y_true, y_pred):\n     return ops.mean(ops.square(first_log - second_log), axis=-1)\n \n \n-@keras_core_export(\"keras_core.losses.cosine_similarity\")\n+@keras_export(\"keras.losses.cosine_similarity\")\n def cosine_similarity(y_true, y_pred, axis=-1):\n     \"\"\"Computes the cosine similarity between labels and predictions.\n \n@@ -1304,7 +1304,7 @@ def cosine_similarity(y_true, y_pred, axis=-1):\n \n     >>> y_true = [[0., 1.], [1., 1.], [1., 1.]]\n     >>> y_pred = [[1., 0.], [1., 1.], [-1., -1.]]\n-    >>> loss = keras_core.losses.cosine_similarity(y_true, y_pred, axis=-1)\n+    >>> loss = keras.losses.cosine_similarity(y_true, y_pred, axis=-1)\n     [-0., -0.99999994, 0.99999994]\n     \"\"\"\n     y_pred = ops.convert_to_tensor(y_pred)\n@@ -1315,7 +1315,7 @@ def cosine_similarity(y_true, y_pred, axis=-1):\n     return -ops.sum(y_true * y_pred, axis=axis)\n \n \n-@keras_core_export([\"keras_core.losses.huber\", \"keras_core.metrics.huber\"])\n+@keras_export([\"keras.losses.huber\", \"keras.metrics.huber\"])\n def huber(y_true, y_pred, delta=1.0):\n     \"\"\"Computes Huber loss value.\n \n@@ -1335,7 +1335,7 @@ def huber(y_true, y_pred, delta=1.0):\n \n     >>> y_true = [[0, 1], [0, 0]]\n     >>> y_pred = [[0.6, 0.4], [0.4, 0.6]]\n-    >>> loss = keras_core.losses.huber(y_true, y_pred)\n+    >>> loss = keras.losses.huber(y_true, y_pred)\n     0.155\n \n \n@@ -1365,13 +1365,13 @@ def huber(y_true, y_pred, delta=1.0):\n     )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.losses.log_cosh\",\n-        \"keras_core.metrics.log_cosh\",\n+        \"keras.losses.log_cosh\",\n+        \"keras.metrics.log_cosh\",\n         # Legacy aliases\n-        \"keras_core._legacy.losses.logcosh\",\n-        \"keras_core._legacy.metrics.logcosh\",\n+        \"keras._legacy.losses.logcosh\",\n+        \"keras._legacy.metrics.logcosh\",\n     ]\n )\n def log_cosh(y_true, y_pred):\n@@ -1391,7 +1391,7 @@ def log_cosh(y_true, y_pred):\n \n     >>> y_true = [[0., 1.], [0., 0.]]\n     >>> y_pred = [[1., 1.], [0., 0.]]\n-    >>> loss = keras_core.losses.log_cosh(y_true, y_pred)\n+    >>> loss = keras.losses.log_cosh(y_true, y_pred)\n     0.108\n \n     Args:\n@@ -1412,17 +1412,17 @@ def log_cosh(y_true, y_pred):\n     return ops.mean(_logcosh(y_pred - y_true), axis=-1)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.metrics.kl_divergence\",\n-        \"keras_core.losses.kl_divergence\",\n+        \"keras.metrics.kl_divergence\",\n+        \"keras.losses.kl_divergence\",\n         # Legacy aliases\n-        \"keras_core._legacy.losses.KLD\",\n-        \"keras_core._legacy.losses.kld\",\n-        \"keras_core._legacy.losses.kullback_leibler_divergence\",\n-        \"keras_core._legacy.metrics.KLD\",\n-        \"keras_core._legacy.metrics.kld\",\n-        \"keras_core._legacy.metrics.kullback_leibler_divergence\",\n+        \"keras._legacy.losses.KLD\",\n+        \"keras._legacy.losses.kld\",\n+        \"keras._legacy.losses.kullback_leibler_divergence\",\n+        \"keras._legacy.metrics.KLD\",\n+        \"keras._legacy.metrics.kld\",\n+        \"keras._legacy.metrics.kullback_leibler_divergence\",\n     ]\n )\n def kl_divergence(y_true, y_pred):\n@@ -1445,7 +1445,7 @@ def kl_divergence(y_true, y_pred):\n \n     >>> y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float32)\n     >>> y_pred = np.random.random(size=(2, 3))\n-    >>> loss = keras_core.losses.kl_divergence(y_true, y_pred)\n+    >>> loss = keras.losses.kl_divergence(y_true, y_pred)\n     >>> assert loss.shape == (2,)\n     >>> y_true = ops.clip(y_true, 1e-7, 1)\n     >>> y_pred = ops.clip(y_pred, 1e-7, 1)\n@@ -1459,10 +1459,10 @@ def kl_divergence(y_true, y_pred):\n     return ops.sum(y_true * ops.log(y_true / y_pred), axis=-1)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.metrics.poisson\",\n-        \"keras_core.losses.poisson\",\n+        \"keras.metrics.poisson\",\n+        \"keras.losses.poisson\",\n     ]\n )\n def poisson(y_true, y_pred):\n@@ -1485,7 +1485,7 @@ def poisson(y_true, y_pred):\n \n     >>> y_true = np.random.randint(0, 2, size=(2, 3))\n     >>> y_pred = np.random.random(size=(2, 3))\n-    >>> loss = keras_core.losses.poisson(y_true, y_pred)\n+    >>> loss = keras.losses.poisson(y_true, y_pred)\n     >>> assert loss.shape == (2,)\n     >>> y_pred = y_pred + 1e-7\n     >>> assert np.allclose(\n@@ -1498,10 +1498,10 @@ def poisson(y_true, y_pred):\n     return ops.mean(y_pred - y_true * ops.log(y_pred + epsilon), axis=-1)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.metrics.categorical_crossentropy\",\n-        \"keras_core.losses.categorical_crossentropy\",\n+        \"keras.metrics.categorical_crossentropy\",\n+        \"keras.losses.categorical_crossentropy\",\n     ]\n )\n def categorical_crossentropy(\n@@ -1527,7 +1527,7 @@ def categorical_crossentropy(\n \n     >>> y_true = [[0, 1, 0], [0, 0, 1]]\n     >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n-    >>> loss = keras_core.losses.categorical_crossentropy(y_true, y_pred)\n+    >>> loss = keras.losses.categorical_crossentropy(y_true, y_pred)\n     >>> assert loss.shape == (2,)\n     >>> loss\n     array([0.0513, 2.303], dtype=float32)\n@@ -1561,10 +1561,10 @@ def categorical_crossentropy(\n     )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.metrics.categorical_focal_crossentropy\",\n-        \"keras_core.losses.categorical_focal_crossentropy\",\n+        \"keras.metrics.categorical_focal_crossentropy\",\n+        \"keras.losses.categorical_focal_crossentropy\",\n     ]\n )\n def categorical_focal_crossentropy(\n@@ -1605,7 +1605,7 @@ def categorical_focal_crossentropy(\n \n     >>> y_true = [[0, 1, 0], [0, 0, 1]]\n     >>> y_pred = [[0.05, 0.9, 0.05], [0.1, 0.85, 0.05]]\n-    >>> loss = keras_core.losses.categorical_focal_crossentropy(y_true, y_pred)\n+    >>> loss = keras.losses.categorical_focal_crossentropy(y_true, y_pred)\n     >>> assert loss.shape == (2,)\n     >>> loss\n     array([2.63401289e-04, 6.75912094e-01], dtype=float32)\n@@ -1657,10 +1657,10 @@ def categorical_focal_crossentropy(\n     return focal_cce\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.metrics.sparse_categorical_crossentropy\",\n-        \"keras_core.losses.sparse_categorical_crossentropy\",\n+        \"keras.metrics.sparse_categorical_crossentropy\",\n+        \"keras.losses.sparse_categorical_crossentropy\",\n     ]\n )\n def sparse_categorical_crossentropy(\n@@ -1688,7 +1688,7 @@ def sparse_categorical_crossentropy(\n \n     >>> y_true = [1, 2]\n     >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n-    >>> loss = keras_core.losses.sparse_categorical_crossentropy(y_true, y_pred)\n+    >>> loss = keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n     >>> assert loss.shape == (2,)\n     >>> loss\n     array([0.0513, 2.303], dtype=float32)\n@@ -1721,10 +1721,10 @@ def sparse_categorical_crossentropy(\n     return res\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.metrics.binary_crossentropy\",\n-        \"keras_core.losses.binary_crossentropy\",\n+        \"keras.metrics.binary_crossentropy\",\n+        \"keras.losses.binary_crossentropy\",\n     ]\n )\n def binary_crossentropy(\n@@ -1750,7 +1750,7 @@ def binary_crossentropy(\n \n     >>> y_true = [[0, 1], [0, 0]]\n     >>> y_pred = [[0.6, 0.4], [0.4, 0.6]]\n-    >>> loss = keras_core.losses.binary_crossentropy(y_true, y_pred)\n+    >>> loss = keras.losses.binary_crossentropy(y_true, y_pred)\n     >>> assert loss.shape == (2,)\n     >>> loss\n     array([0.916 , 0.714], dtype=float32)\n@@ -1767,10 +1767,10 @@ def binary_crossentropy(\n     )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.metrics.binary_focal_crossentropy\",\n-        \"keras_core.losses.binary_focal_crossentropy\",\n+        \"keras.metrics.binary_focal_crossentropy\",\n+        \"keras.losses.binary_focal_crossentropy\",\n     ]\n )\n def binary_focal_crossentropy(\n@@ -1826,7 +1826,7 @@ def binary_focal_crossentropy(\n \n     >>> y_true = [[0, 1], [0, 0]]\n     >>> y_pred = [[0.6, 0.4], [0.4, 0.6]]\n-    >>> loss = keras_core.losses.binary_focal_crossentropy(\n+    >>> loss = keras.losses.binary_focal_crossentropy(\n     ...        y_true, y_pred, gamma=2)\n     >>> assert loss.shape == (2,)\n     >>> loss\n\n@@ -1,7 +1,7 @@\n import numpy as np\n \n-from keras_core import testing\n-from keras_core.losses import losses\n+from keras import testing\n+from keras.losses import losses\n \n \n class MeanSquaredErrorTest(testing.TestCase):\n\n@@ -1,52 +1,52 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.metrics.accuracy_metrics import Accuracy\n-from keras_core.metrics.accuracy_metrics import BinaryAccuracy\n-from keras_core.metrics.accuracy_metrics import CategoricalAccuracy\n-from keras_core.metrics.accuracy_metrics import SparseCategoricalAccuracy\n-from keras_core.metrics.accuracy_metrics import SparseTopKCategoricalAccuracy\n-from keras_core.metrics.accuracy_metrics import TopKCategoricalAccuracy\n-from keras_core.metrics.confusion_metrics import AUC\n-from keras_core.metrics.confusion_metrics import FalseNegatives\n-from keras_core.metrics.confusion_metrics import FalsePositives\n-from keras_core.metrics.confusion_metrics import Precision\n-from keras_core.metrics.confusion_metrics import PrecisionAtRecall\n-from keras_core.metrics.confusion_metrics import Recall\n-from keras_core.metrics.confusion_metrics import RecallAtPrecision\n-from keras_core.metrics.confusion_metrics import SensitivityAtSpecificity\n-from keras_core.metrics.confusion_metrics import SpecificityAtSensitivity\n-from keras_core.metrics.confusion_metrics import TrueNegatives\n-from keras_core.metrics.confusion_metrics import TruePositives\n-from keras_core.metrics.f_score_metrics import F1Score\n-from keras_core.metrics.f_score_metrics import FBetaScore\n-from keras_core.metrics.hinge_metrics import CategoricalHinge\n-from keras_core.metrics.hinge_metrics import Hinge\n-from keras_core.metrics.hinge_metrics import SquaredHinge\n-from keras_core.metrics.iou_metrics import BinaryIoU\n-from keras_core.metrics.iou_metrics import IoU\n-from keras_core.metrics.iou_metrics import MeanIoU\n-from keras_core.metrics.iou_metrics import OneHotIoU\n-from keras_core.metrics.iou_metrics import OneHotMeanIoU\n-from keras_core.metrics.metric import Metric\n-from keras_core.metrics.probabilistic_metrics import BinaryCrossentropy\n-from keras_core.metrics.probabilistic_metrics import CategoricalCrossentropy\n-from keras_core.metrics.probabilistic_metrics import KLDivergence\n-from keras_core.metrics.probabilistic_metrics import Poisson\n-from keras_core.metrics.probabilistic_metrics import (\n+from keras.api_export import keras_export\n+from keras.metrics.accuracy_metrics import Accuracy\n+from keras.metrics.accuracy_metrics import BinaryAccuracy\n+from keras.metrics.accuracy_metrics import CategoricalAccuracy\n+from keras.metrics.accuracy_metrics import SparseCategoricalAccuracy\n+from keras.metrics.accuracy_metrics import SparseTopKCategoricalAccuracy\n+from keras.metrics.accuracy_metrics import TopKCategoricalAccuracy\n+from keras.metrics.confusion_metrics import AUC\n+from keras.metrics.confusion_metrics import FalseNegatives\n+from keras.metrics.confusion_metrics import FalsePositives\n+from keras.metrics.confusion_metrics import Precision\n+from keras.metrics.confusion_metrics import PrecisionAtRecall\n+from keras.metrics.confusion_metrics import Recall\n+from keras.metrics.confusion_metrics import RecallAtPrecision\n+from keras.metrics.confusion_metrics import SensitivityAtSpecificity\n+from keras.metrics.confusion_metrics import SpecificityAtSensitivity\n+from keras.metrics.confusion_metrics import TrueNegatives\n+from keras.metrics.confusion_metrics import TruePositives\n+from keras.metrics.f_score_metrics import F1Score\n+from keras.metrics.f_score_metrics import FBetaScore\n+from keras.metrics.hinge_metrics import CategoricalHinge\n+from keras.metrics.hinge_metrics import Hinge\n+from keras.metrics.hinge_metrics import SquaredHinge\n+from keras.metrics.iou_metrics import BinaryIoU\n+from keras.metrics.iou_metrics import IoU\n+from keras.metrics.iou_metrics import MeanIoU\n+from keras.metrics.iou_metrics import OneHotIoU\n+from keras.metrics.iou_metrics import OneHotMeanIoU\n+from keras.metrics.metric import Metric\n+from keras.metrics.probabilistic_metrics import BinaryCrossentropy\n+from keras.metrics.probabilistic_metrics import CategoricalCrossentropy\n+from keras.metrics.probabilistic_metrics import KLDivergence\n+from keras.metrics.probabilistic_metrics import Poisson\n+from keras.metrics.probabilistic_metrics import (\n     SparseCategoricalCrossentropy,\n )\n-from keras_core.metrics.reduction_metrics import Mean\n-from keras_core.metrics.reduction_metrics import MeanMetricWrapper\n-from keras_core.metrics.reduction_metrics import Sum\n-from keras_core.metrics.regression_metrics import CosineSimilarity\n-from keras_core.metrics.regression_metrics import LogCoshError\n-from keras_core.metrics.regression_metrics import MeanAbsoluteError\n-from keras_core.metrics.regression_metrics import MeanAbsolutePercentageError\n-from keras_core.metrics.regression_metrics import MeanSquaredError\n-from keras_core.metrics.regression_metrics import MeanSquaredLogarithmicError\n-from keras_core.metrics.regression_metrics import R2Score\n-from keras_core.metrics.regression_metrics import RootMeanSquaredError\n-from keras_core.saving import serialization_lib\n-from keras_core.utils.naming import to_snake_case\n+from keras.metrics.reduction_metrics import Mean\n+from keras.metrics.reduction_metrics import MeanMetricWrapper\n+from keras.metrics.reduction_metrics import Sum\n+from keras.metrics.regression_metrics import CosineSimilarity\n+from keras.metrics.regression_metrics import LogCoshError\n+from keras.metrics.regression_metrics import MeanAbsoluteError\n+from keras.metrics.regression_metrics import MeanAbsolutePercentageError\n+from keras.metrics.regression_metrics import MeanSquaredError\n+from keras.metrics.regression_metrics import MeanSquaredLogarithmicError\n+from keras.metrics.regression_metrics import R2Score\n+from keras.metrics.regression_metrics import RootMeanSquaredError\n+from keras.saving import serialization_lib\n+from keras.utils.naming import to_snake_case\n \n ALL_OBJECTS = {\n     # Base\n@@ -124,7 +124,7 @@ ALL_OBJECTS_DICT.update(\n )\n \n \n-@keras_core_export(\"keras_core.metrics.serialize\")\n+@keras_export(\"keras.metrics.serialize\")\n def serialize(metric):\n     \"\"\"Serializes metric function or `Metric` instance.\n \n@@ -137,7 +137,7 @@ def serialize(metric):\n     return serialization_lib.serialize_keras_object(metric)\n \n \n-@keras_core_export(\"keras_core.metrics.deserialize\")\n+@keras_export(\"keras.metrics.deserialize\")\n def deserialize(config, custom_objects=None):\n     \"\"\"Deserializes a serialized metric class/function instance.\n \n@@ -157,7 +157,7 @@ def deserialize(config, custom_objects=None):\n     )\n \n \n-@keras_core_export(\"keras_core.metrics.get\")\n+@keras_export(\"keras.metrics.get\")\n def get(identifier):\n     \"\"\"Retrieves a Keras metric as a `function`/`Metric` class instance.\n \n\n@@ -1,8 +1,8 @@\n-from keras_core import backend\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.losses.loss import squeeze_to_same_rank\n-from keras_core.metrics import reduction_metrics\n+from keras import backend\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.losses.loss import squeeze_to_same_rank\n+from keras.metrics import reduction_metrics\n \n \n def accuracy(y_true, y_pred):\n@@ -15,7 +15,7 @@ def accuracy(y_true, y_pred):\n     )\n \n \n-@keras_core_export(\"keras_core.metrics.Accuracy\")\n+@keras_export(\"keras.metrics.Accuracy\")\n class Accuracy(reduction_metrics.MeanMetricWrapper):\n     \"\"\"Calculates how often predictions equal labels.\n \n@@ -33,7 +33,7 @@ class Accuracy(reduction_metrics.MeanMetricWrapper):\n \n     Standalone usage:\n \n-    >>> m = keras_core.metrics.Accuracy()\n+    >>> m = keras.metrics.Accuracy()\n     >>> m.update_state([[1], [2], [3], [4]], [[0], [2], [3], [4]])\n     >>> m.result()\n     0.75\n@@ -49,7 +49,7 @@ class Accuracy(reduction_metrics.MeanMetricWrapper):\n     ```python\n     model.compile(optimizer='sgd',\n                   loss='binary_crossentropy',\n-                  metrics=[keras_core.metrics.Accuracy()])\n+                  metrics=[keras.metrics.Accuracy()])\n     ```\n     \"\"\"\n \n@@ -60,7 +60,7 @@ class Accuracy(reduction_metrics.MeanMetricWrapper):\n         return {\"name\": self.name, \"dtype\": self.dtype}\n \n \n-@keras_core_export(\"keras_core.metrics.binary_accuracy\")\n+@keras_export(\"keras.metrics.binary_accuracy\")\n def binary_accuracy(y_true, y_pred, threshold=0.5):\n     y_true = ops.convert_to_tensor(y_true)\n     y_pred = ops.convert_to_tensor(y_pred)\n@@ -73,7 +73,7 @@ def binary_accuracy(y_true, y_pred, threshold=0.5):\n     )\n \n \n-@keras_core_export(\"keras_core.metrics.BinaryAccuracy\")\n+@keras_export(\"keras.metrics.BinaryAccuracy\")\n class BinaryAccuracy(reduction_metrics.MeanMetricWrapper):\n     \"\"\"Calculates how often predictions match binary labels.\n \n@@ -93,7 +93,7 @@ class BinaryAccuracy(reduction_metrics.MeanMetricWrapper):\n \n     Standalone usage:\n \n-    >>> m = keras_core.metrics.BinaryAccuracy()\n+    >>> m = keras.metrics.BinaryAccuracy()\n     >>> m.update_state([[1], [1], [0], [0]], [[0.98], [1], [0], [0.6]])\n     >>> m.result()\n     0.75\n@@ -109,7 +109,7 @@ class BinaryAccuracy(reduction_metrics.MeanMetricWrapper):\n     ```python\n     model.compile(optimizer='sgd',\n                   loss='binary_crossentropy',\n-                  metrics=[keras_core.metrics.BinaryAccuracy()])\n+                  metrics=[keras.metrics.BinaryAccuracy()])\n     ```\n     \"\"\"\n \n@@ -120,7 +120,7 @@ class BinaryAccuracy(reduction_metrics.MeanMetricWrapper):\n         return {\"name\": self.name, \"dtype\": self.dtype}\n \n \n-@keras_core_export(\"keras_core.metrics.categorical_accuracy\")\n+@keras_export(\"keras.metrics.categorical_accuracy\")\n def categorical_accuracy(y_true, y_pred):\n     y_true = ops.argmax(y_true, axis=-1)\n \n@@ -152,7 +152,7 @@ def categorical_accuracy(y_true, y_pred):\n     return matches\n \n \n-@keras_core_export(\"keras_core.metrics.CategoricalAccuracy\")\n+@keras_export(\"keras.metrics.CategoricalAccuracy\")\n class CategoricalAccuracy(reduction_metrics.MeanMetricWrapper):\n     \"\"\"Calculates how often predictions match one-hot labels.\n \n@@ -177,7 +177,7 @@ class CategoricalAccuracy(reduction_metrics.MeanMetricWrapper):\n \n     Standalone usage:\n \n-    >>> m = keras_core.metrics.CategoricalAccuracy()\n+    >>> m = keras.metrics.CategoricalAccuracy()\n     >>> m.update_state([[0, 0, 1], [0, 1, 0]], [[0.1, 0.9, 0.8],\n     ...                 [0.05, 0.95, 0]])\n     >>> m.result()\n@@ -195,7 +195,7 @@ class CategoricalAccuracy(reduction_metrics.MeanMetricWrapper):\n     ```python\n     model.compile(optimizer='sgd',\n                   loss='categorical_crossentropy',\n-                  metrics=[keras_core.metrics.CategoricalAccuracy()])\n+                  metrics=[keras.metrics.CategoricalAccuracy()])\n     ```\n     \"\"\"\n \n@@ -206,7 +206,7 @@ class CategoricalAccuracy(reduction_metrics.MeanMetricWrapper):\n         return {\"name\": self.name, \"dtype\": self.dtype}\n \n \n-@keras_core_export(\"keras_core.metrics.sparse_categorical_accuracy\")\n+@keras_export(\"keras.metrics.sparse_categorical_accuracy\")\n def sparse_categorical_accuracy(y_true, y_pred):\n     reshape_matches = False\n     y_pred = ops.convert_to_tensor(y_pred)\n@@ -238,7 +238,7 @@ def sparse_categorical_accuracy(y_true, y_pred):\n     return matches\n \n \n-@keras_core_export(\"keras_core.metrics.SparseCategoricalAccuracy\")\n+@keras_export(\"keras.metrics.SparseCategoricalAccuracy\")\n class SparseCategoricalAccuracy(reduction_metrics.MeanMetricWrapper):\n     \"\"\"Calculates how often predictions match integer labels.\n \n@@ -263,7 +263,7 @@ class SparseCategoricalAccuracy(reduction_metrics.MeanMetricWrapper):\n \n     Standalone usage:\n \n-    >>> m = keras_core.metrics.SparseCategoricalAccuracy()\n+    >>> m = keras.metrics.SparseCategoricalAccuracy()\n     >>> m.update_state([[2], [1]], [[0.1, 0.6, 0.3], [0.05, 0.95, 0]])\n     >>> m.result()\n     0.5\n@@ -279,7 +279,7 @@ class SparseCategoricalAccuracy(reduction_metrics.MeanMetricWrapper):\n     ```python\n     model.compile(optimizer='sgd',\n                   loss='sparse_categorical_crossentropy',\n-                  metrics=[keras_core.metrics.SparseCategoricalAccuracy()])\n+                  metrics=[keras.metrics.SparseCategoricalAccuracy()])\n     ```\n     \"\"\"\n \n@@ -290,7 +290,7 @@ class SparseCategoricalAccuracy(reduction_metrics.MeanMetricWrapper):\n         return {\"name\": self.name, \"dtype\": self.dtype}\n \n \n-@keras_core_export(\"keras_core.metrics.top_k_categorical_accuracy\")\n+@keras_export(\"keras.metrics.top_k_categorical_accuracy\")\n def top_k_categorical_accuracy(y_true, y_pred, k=5):\n     reshape_matches = False\n     y_pred = ops.convert_to_tensor(y_pred)\n@@ -320,7 +320,7 @@ def top_k_categorical_accuracy(y_true, y_pred, k=5):\n     return matches\n \n \n-@keras_core_export(\"keras_core.metrics.TopKCategoricalAccuracy\")\n+@keras_export(\"keras.metrics.TopKCategoricalAccuracy\")\n class TopKCategoricalAccuracy(reduction_metrics.MeanMetricWrapper):\n     \"\"\"Computes how often targets are in the top `K` predictions.\n \n@@ -332,7 +332,7 @@ class TopKCategoricalAccuracy(reduction_metrics.MeanMetricWrapper):\n \n     Standalone usage:\n \n-    >>> m = keras_core.metrics.TopKCategoricalAccuracy(k=1)\n+    >>> m = keras.metrics.TopKCategoricalAccuracy(k=1)\n     >>> m.update_state([[0, 0, 1], [0, 1, 0]],\n     ...                [[0.1, 0.9, 0.8], [0.05, 0.95, 0]])\n     >>> m.result()\n@@ -350,7 +350,7 @@ class TopKCategoricalAccuracy(reduction_metrics.MeanMetricWrapper):\n     ```python\n     model.compile(optimizer='sgd',\n                   loss='categorical_crossentropy',\n-                  metrics=[keras_core.metrics.TopKCategoricalAccuracy()])\n+                  metrics=[keras.metrics.TopKCategoricalAccuracy()])\n     ```\n     \"\"\"\n \n@@ -367,7 +367,7 @@ class TopKCategoricalAccuracy(reduction_metrics.MeanMetricWrapper):\n         return {\"name\": self.name, \"dtype\": self.dtype, \"k\": self.k}\n \n \n-@keras_core_export(\"keras_core.metrics.sparse_top_k_categorical_accuracy\")\n+@keras_export(\"keras.metrics.sparse_top_k_categorical_accuracy\")\n def sparse_top_k_categorical_accuracy(y_true, y_pred, k=5):\n     reshape_matches = False\n     y_pred = ops.convert_to_tensor(y_pred)\n@@ -396,7 +396,7 @@ def sparse_top_k_categorical_accuracy(y_true, y_pred, k=5):\n     return matches\n \n \n-@keras_core_export(\"keras_core.metrics.SparseTopKCategoricalAccuracy\")\n+@keras_export(\"keras.metrics.SparseTopKCategoricalAccuracy\")\n class SparseTopKCategoricalAccuracy(reduction_metrics.MeanMetricWrapper):\n     \"\"\"Computes how often integer targets are in the top `K` predictions.\n \n@@ -408,7 +408,7 @@ class SparseTopKCategoricalAccuracy(reduction_metrics.MeanMetricWrapper):\n \n     Standalone usage:\n \n-    >>> m = keras_core.metrics.SparseTopKCategoricalAccuracy(k=1)\n+    >>> m = keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n     >>> m.update_state([2, 1], [[0.1, 0.9, 0.8], [0.05, 0.95, 0]])\n     >>> m.result()\n     0.5\n@@ -424,7 +424,7 @@ class SparseTopKCategoricalAccuracy(reduction_metrics.MeanMetricWrapper):\n     ```python\n     model.compile(optimizer='sgd',\n                   loss='sparse_categorical_crossentropy',\n-                  metrics=[keras_core.metrics.SparseTopKCategoricalAccuracy()])\n+                  metrics=[keras.metrics.SparseTopKCategoricalAccuracy()])\n     ```\n     \"\"\"\n \n\n@@ -1,7 +1,7 @@\n import numpy as np\n \n-from keras_core import testing\n-from keras_core.metrics import accuracy_metrics\n+from keras import testing\n+from keras.metrics import accuracy_metrics\n \n \n class AccuracyTest(testing.TestCase):\n\n@@ -1,13 +1,13 @@\n import numpy as np\n \n-from keras_core import activations\n-from keras_core import backend\n-from keras_core import initializers\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.metrics import metrics_utils\n-from keras_core.metrics.metric import Metric\n-from keras_core.utils.python_utils import to_list\n+from keras import activations\n+from keras import backend\n+from keras import initializers\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.metrics import metrics_utils\n+from keras.metrics.metric import Metric\n+from keras.utils.python_utils import to_list\n \n \n class _ConfusionMatrixConditionCount(Metric):\n@@ -75,7 +75,7 @@ class _ConfusionMatrixConditionCount(Metric):\n         return {**base_config, **config}\n \n \n-@keras_core_export(\"keras_core.metrics.FalsePositives\")\n+@keras_export(\"keras.metrics.FalsePositives\")\n class FalsePositives(_ConfusionMatrixConditionCount):\n     \"\"\"Calculates the number of false positives.\n \n@@ -99,7 +99,7 @@ class FalsePositives(_ConfusionMatrixConditionCount):\n \n     Standalone usage:\n \n-    >>> m = keras_core.metrics.FalsePositives()\n+    >>> m = keras.metrics.FalsePositives()\n     >>> m.update_state([0, 1, 0, 0], [0, 0, 1, 1])\n     >>> m.result()\n     2.0\n@@ -119,7 +119,7 @@ class FalsePositives(_ConfusionMatrixConditionCount):\n         )\n \n \n-@keras_core_export(\"keras_core.metrics.FalseNegatives\")\n+@keras_export(\"keras.metrics.FalseNegatives\")\n class FalseNegatives(_ConfusionMatrixConditionCount):\n     \"\"\"Calculates the number of false negatives.\n \n@@ -143,7 +143,7 @@ class FalseNegatives(_ConfusionMatrixConditionCount):\n \n     Standalone usage:\n \n-    >>> m = keras_core.metrics.FalseNegatives()\n+    >>> m = keras.metrics.FalseNegatives()\n     >>> m.update_state([0, 1, 1, 1], [0, 1, 0, 0])\n     >>> m.result()\n     2.0\n@@ -163,7 +163,7 @@ class FalseNegatives(_ConfusionMatrixConditionCount):\n         )\n \n \n-@keras_core_export(\"keras_core.metrics.TrueNegatives\")\n+@keras_export(\"keras.metrics.TrueNegatives\")\n class TrueNegatives(_ConfusionMatrixConditionCount):\n     \"\"\"Calculates the number of true negatives.\n \n@@ -187,7 +187,7 @@ class TrueNegatives(_ConfusionMatrixConditionCount):\n \n     Standalone usage:\n \n-    >>> m = keras_core.metrics.TrueNegatives()\n+    >>> m = keras.metrics.TrueNegatives()\n     >>> m.update_state([0, 1, 0, 0], [1, 1, 0, 0])\n     >>> m.result()\n     2.0\n@@ -207,7 +207,7 @@ class TrueNegatives(_ConfusionMatrixConditionCount):\n         )\n \n \n-@keras_core_export(\"keras_core.metrics.TruePositives\")\n+@keras_export(\"keras.metrics.TruePositives\")\n class TruePositives(_ConfusionMatrixConditionCount):\n     \"\"\"Calculates the number of true positives.\n \n@@ -231,7 +231,7 @@ class TruePositives(_ConfusionMatrixConditionCount):\n \n     Standalone usage:\n \n-    >>> m = keras_core.metrics.TruePositives()\n+    >>> m = keras.metrics.TruePositives()\n     >>> m.update_state([0, 1, 1, 1], [1, 0, 1, 1])\n     >>> m.result()\n     2.0\n@@ -251,7 +251,7 @@ class TruePositives(_ConfusionMatrixConditionCount):\n         )\n \n \n-@keras_core_export(\"keras_core.metrics.Precision\")\n+@keras_export(\"keras.metrics.Precision\")\n class Precision(Metric):\n     \"\"\"Computes the precision of the predictions with respect to the labels.\n \n@@ -293,7 +293,7 @@ class Precision(Metric):\n \n     Standalone usage:\n \n-    >>> m = keras_core.metrics.Precision()\n+    >>> m = keras.metrics.Precision()\n     >>> m.update_state([0, 1, 1, 1], [1, 0, 1, 1])\n     >>> m.result()\n     0.6666667\n@@ -305,14 +305,14 @@ class Precision(Metric):\n \n     >>> # With top_k=2, it will calculate precision over y_true[:2]\n     >>> # and y_pred[:2]\n-    >>> m = keras_core.metrics.Precision(top_k=2)\n+    >>> m = keras.metrics.Precision(top_k=2)\n     >>> m.update_state([0, 0, 1, 1], [1, 1, 1, 1])\n     >>> m.result()\n     0.0\n \n     >>> # With top_k=4, it will calculate precision over y_true[:4]\n     >>> # and y_pred[:4]\n-    >>> m = keras_core.metrics.Precision(top_k=4)\n+    >>> m = keras.metrics.Precision(top_k=4)\n     >>> m.update_state([0, 0, 1, 1], [1, 1, 1, 1])\n     >>> m.result()\n     0.5\n@@ -322,15 +322,15 @@ class Precision(Metric):\n     ```python\n     model.compile(optimizer='sgd',\n                   loss='mse',\n-                  metrics=[keras_core.metrics.Precision()])\n+                  metrics=[keras.metrics.Precision()])\n     ```\n \n     Usage with a loss with `from_logits=True`:\n \n     ```python\n     model.compile(optimizer='adam',\n-                  loss=keras_core.losses.BinaryCrossentropy(from_logits=True),\n-                  metrics=[keras_core.metrics.Precision(thresholds=0)])\n+                  loss=keras.losses.BinaryCrossentropy(from_logits=True),\n+                  metrics=[keras.metrics.Precision(thresholds=0)])\n     ```\n     \"\"\"\n \n@@ -408,7 +408,7 @@ class Precision(Metric):\n         return {**base_config, **config}\n \n \n-@keras_core_export(\"keras_core.metrics.Recall\")\n+@keras_export(\"keras.metrics.Recall\")\n class Recall(Metric):\n     \"\"\"Computes the recall of the predictions with respect to the labels.\n \n@@ -448,7 +448,7 @@ class Recall(Metric):\n \n     Standalone usage:\n \n-    >>> m = keras_core.metrics.Recall()\n+    >>> m = keras.metrics.Recall()\n     >>> m.update_state([0, 1, 1, 1], [1, 0, 1, 1])\n     >>> m.result()\n     0.6666667\n@@ -463,15 +463,15 @@ class Recall(Metric):\n     ```python\n     model.compile(optimizer='sgd',\n                   loss='mse',\n-                  metrics=[keras_core.metrics.Recall()])\n+                  metrics=[keras.metrics.Recall()])\n     ```\n \n     Usage with a loss with `from_logits=True`:\n \n     ```python\n     model.compile(optimizer='adam',\n-                  loss=keras_core.losses.BinaryCrossentropy(from_logits=True),\n-                  metrics=[keras_core.metrics.Recall(thresholds=0)])\n+                  loss=keras.losses.BinaryCrossentropy(from_logits=True),\n+                  metrics=[keras.metrics.Recall(thresholds=0)])\n     ```\n     \"\"\"\n \n@@ -666,7 +666,7 @@ class SensitivitySpecificityBase(Metric):\n         return ops.where(feasible_exists, max_dependent, 0.0)\n \n \n-@keras_core_export(\"keras_core.metrics.SensitivityAtSpecificity\")\n+@keras_export(\"keras.metrics.SensitivityAtSpecificity\")\n class SensitivityAtSpecificity(SensitivitySpecificityBase):\n     \"\"\"Computes best sensitivity where specificity is >= specified value.\n \n@@ -704,7 +704,7 @@ class SensitivityAtSpecificity(SensitivitySpecificityBase):\n \n     Standalone usage:\n \n-    >>> m = keras_core.metrics.SensitivityAtSpecificity(0.5)\n+    >>> m = keras.metrics.SensitivityAtSpecificity(0.5)\n     >>> m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])\n     >>> m.result()\n     0.5\n@@ -721,7 +721,7 @@ class SensitivityAtSpecificity(SensitivitySpecificityBase):\n     model.compile(\n         optimizer='sgd',\n         loss='mse',\n-        metrics=[keras_core.metrics.SensitivityAtSpecificity()])\n+        metrics=[keras.metrics.SensitivityAtSpecificity()])\n     ```\n     \"\"\"\n \n@@ -770,7 +770,7 @@ class SensitivityAtSpecificity(SensitivitySpecificityBase):\n         return {**base_config, **config}\n \n \n-@keras_core_export(\"keras_core.metrics.SpecificityAtSensitivity\")\n+@keras_export(\"keras.metrics.SpecificityAtSensitivity\")\n class SpecificityAtSensitivity(SensitivitySpecificityBase):\n     \"\"\"Computes best specificity where sensitivity is >= specified value.\n \n@@ -808,7 +808,7 @@ class SpecificityAtSensitivity(SensitivitySpecificityBase):\n \n     Standalone usage:\n \n-    >>> m = keras_core.metrics.SpecificityAtSensitivity(0.5)\n+    >>> m = keras.metrics.SpecificityAtSensitivity(0.5)\n     >>> m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])\n     >>> m.result()\n     0.66666667\n@@ -825,7 +825,7 @@ class SpecificityAtSensitivity(SensitivitySpecificityBase):\n     model.compile(\n         optimizer='sgd',\n         loss='mse',\n-        metrics=[keras_core.metrics.SpecificityAtSensitivity()])\n+        metrics=[keras.metrics.SpecificityAtSensitivity()])\n     ```\n     \"\"\"\n \n@@ -874,7 +874,7 @@ class SpecificityAtSensitivity(SensitivitySpecificityBase):\n         return {**base_config, **config}\n \n \n-@keras_core_export(\"keras_core.metrics.PrecisionAtRecall\")\n+@keras_export(\"keras.metrics.PrecisionAtRecall\")\n class PrecisionAtRecall(SensitivitySpecificityBase):\n     \"\"\"Computes best precision where recall is >= specified value.\n \n@@ -903,7 +903,7 @@ class PrecisionAtRecall(SensitivitySpecificityBase):\n \n     Standalone usage:\n \n-    >>> m = keras_core.metrics.PrecisionAtRecall(0.5)\n+    >>> m = keras.metrics.PrecisionAtRecall(0.5)\n     >>> m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])\n     >>> m.result()\n     0.5\n@@ -920,7 +920,7 @@ class PrecisionAtRecall(SensitivitySpecificityBase):\n     model.compile(\n         optimizer='sgd',\n         loss='mse',\n-        metrics=[keras_core.metrics.PrecisionAtRecall(recall=0.8)])\n+        metrics=[keras.metrics.PrecisionAtRecall(recall=0.8)])\n     ```\n     \"\"\"\n \n@@ -961,7 +961,7 @@ class PrecisionAtRecall(SensitivitySpecificityBase):\n         return {**base_config, **config}\n \n \n-@keras_core_export(\"keras_core.metrics.RecallAtPrecision\")\n+@keras_export(\"keras.metrics.RecallAtPrecision\")\n class RecallAtPrecision(SensitivitySpecificityBase):\n     \"\"\"Computes best recall where precision is >= specified value.\n \n@@ -993,7 +993,7 @@ class RecallAtPrecision(SensitivitySpecificityBase):\n \n     Standalone usage:\n \n-    >>> m = keras_core.metrics.RecallAtPrecision(0.8)\n+    >>> m = keras.metrics.RecallAtPrecision(0.8)\n     >>> m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])\n     >>> m.result()\n     0.5\n@@ -1010,7 +1010,7 @@ class RecallAtPrecision(SensitivitySpecificityBase):\n     model.compile(\n         optimizer='sgd',\n         loss='mse',\n-        metrics=[keras_core.metrics.RecallAtPrecision(precision=0.8)])\n+        metrics=[keras.metrics.RecallAtPrecision(precision=0.8)])\n     ```\n     \"\"\"\n \n@@ -1059,7 +1059,7 @@ class RecallAtPrecision(SensitivitySpecificityBase):\n         return {**base_config, **config}\n \n \n-@keras_core_export(\"keras_core.metrics.AUC\")\n+@keras_export(\"keras.metrics.AUC\")\n class AUC(Metric):\n     \"\"\"Approximates the AUC (Area under the curve) of the ROC or PR curves.\n \n@@ -1149,7 +1149,7 @@ class AUC(Metric):\n \n     Standalone usage:\n \n-    >>> m = keras_core.metrics.AUC(num_thresholds=3)\n+    >>> m = keras.metrics.AUC(num_thresholds=3)\n     >>> m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])\n     >>> # threshold values are [0 - 1e-7, 0.5, 1 + 1e-7]\n     >>> # tp = [2, 1, 0], fp = [2, 0, 0], fn = [0, 1, 2], tn = [0, 2, 2]\n@@ -1170,13 +1170,13 @@ class AUC(Metric):\n     ```python\n     # Reports the AUC of a model outputting a probability.\n     model.compile(optimizer='sgd',\n-                  loss=keras_core.losses.BinaryCrossentropy(),\n-                  metrics=[keras_core.metrics.AUC()])\n+                  loss=keras.losses.BinaryCrossentropy(),\n+                  metrics=[keras.metrics.AUC()])\n \n     # Reports the AUC of a model outputting a logit.\n     model.compile(optimizer='sgd',\n-                  loss=keras_core.losses.BinaryCrossentropy(from_logits=True),\n-                  metrics=[keras_core.metrics.AUC(from_logits=True)])\n+                  loss=keras.losses.BinaryCrossentropy(from_logits=True),\n+                  metrics=[keras.metrics.AUC(from_logits=True)])\n     ```\n     \"\"\"\n \n\n@@ -6,12 +6,12 @@ from absl import logging\n from absl.testing import parameterized\n from tensorflow.python.ops.numpy_ops import np_config\n \n-from keras_core import layers\n-from keras_core import metrics\n-from keras_core import models\n-from keras_core import ops\n-from keras_core import testing\n-from keras_core.metrics import metrics_utils\n+from keras import layers\n+from keras import metrics\n+from keras import models\n+from keras import ops\n+from keras import testing\n+from keras.metrics import metrics_utils\n \n # TODO: remove reliance on this (or alternatively, turn it on by default).\n # This is no longer needed with tf-nightly.\n@@ -1116,7 +1116,7 @@ class RecallAtPrecisionTest(testing.TestCase, parameterized.TestCase):\n \n     @pytest.mark.requires_trainable_backend\n     def test_end_to_end(self):\n-        # Test for https://github.com/keras-team/keras-core/issues/718\n+        # Test for https://github.com/keras-team/keras/issues/718\n         model = models.Sequential(\n             [\n                 layers.Input((1,)),\n\n@@ -1,11 +1,11 @@\n-from keras_core import backend\n-from keras_core import initializers\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.metrics.metric import Metric\n+from keras import backend\n+from keras import initializers\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.metrics.metric import Metric\n \n \n-@keras_core_export(\"keras_core.metrics.FBetaScore\")\n+@keras_export(\"keras.metrics.FBetaScore\")\n class FBetaScore(Metric):\n     \"\"\"Computes F-Beta score.\n \n@@ -50,7 +50,7 @@ class FBetaScore(Metric):\n \n     Example:\n \n-    >>> metric = keras_core.metrics.FBetaScore(beta=2.0, threshold=0.5)\n+    >>> metric = keras.metrics.FBetaScore(beta=2.0, threshold=0.5)\n     >>> y_true = np.array([[1, 1, 1],\n     ...                    [1, 0, 0],\n     ...                    [1, 1, 0]], np.int32)\n@@ -245,7 +245,7 @@ class FBetaScore(Metric):\n             v.assign(ops.zeros(v.shape, dtype=v.dtype))\n \n \n-@keras_core_export(\"keras_core.metrics.F1Score\")\n+@keras_export(\"keras.metrics.F1Score\")\n class F1Score(FBetaScore):\n     r\"\"\"Computes F-1 Score.\n \n@@ -285,7 +285,7 @@ class F1Score(FBetaScore):\n \n     Example:\n \n-    >>> metric = keras_core.metrics.F1Score(threshold=0.5)\n+    >>> metric = keras.metrics.F1Score(threshold=0.5)\n     >>> y_true = np.array([[1, 1, 1],\n     ...                    [1, 0, 0],\n     ...                    [1, 1, 0]], np.int32)\n\n@@ -1,8 +1,8 @@\n import numpy as np\n from absl.testing import parameterized\n \n-from keras_core import testing\n-from keras_core.metrics import f_score_metrics\n+from keras import testing\n+from keras.metrics import f_score_metrics\n \n \n class FBetaScoreTest(parameterized.TestCase, testing.TestCase):\n\n@@ -1,11 +1,11 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.losses.losses import categorical_hinge\n-from keras_core.losses.losses import hinge\n-from keras_core.losses.losses import squared_hinge\n-from keras_core.metrics import reduction_metrics\n+from keras.api_export import keras_export\n+from keras.losses.losses import categorical_hinge\n+from keras.losses.losses import hinge\n+from keras.losses.losses import squared_hinge\n+from keras.metrics import reduction_metrics\n \n \n-@keras_core_export(\"keras_core.metrics.Hinge\")\n+@keras_export(\"keras.metrics.Hinge\")\n class Hinge(reduction_metrics.MeanMetricWrapper):\n     \"\"\"Computes the hinge metric between `y_true` and `y_pred`.\n \n@@ -18,7 +18,7 @@ class Hinge(reduction_metrics.MeanMetricWrapper):\n \n     Standalone usage:\n \n-    >>> m = keras_core.metrics.Hinge()\n+    >>> m = keras.metrics.Hinge()\n     >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])\n     >>> m.result()\n     1.3\n@@ -36,7 +36,7 @@ class Hinge(reduction_metrics.MeanMetricWrapper):\n         return {\"name\": self.name, \"dtype\": self.dtype}\n \n \n-@keras_core_export(\"keras_core.metrics.SquaredHinge\")\n+@keras_export(\"keras.metrics.SquaredHinge\")\n class SquaredHinge(reduction_metrics.MeanMetricWrapper):\n     \"\"\"Computes the hinge metric between `y_true` and `y_pred`.\n \n@@ -49,7 +49,7 @@ class SquaredHinge(reduction_metrics.MeanMetricWrapper):\n \n     Standalone usage:\n \n-    >>> m = keras_core.metrics.SquaredHinge()\n+    >>> m = keras.metrics.SquaredHinge()\n     >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])\n     >>> m.result()\n     1.86\n@@ -67,7 +67,7 @@ class SquaredHinge(reduction_metrics.MeanMetricWrapper):\n         return {\"name\": self.name, \"dtype\": self.dtype}\n \n \n-@keras_core_export(\"keras_core.metrics.CategoricalHinge\")\n+@keras_export(\"keras.metrics.CategoricalHinge\")\n class CategoricalHinge(reduction_metrics.MeanMetricWrapper):\n     \"\"\"Computes the categorical hinge metric between `y_true` and `y_pred`.\n \n@@ -76,7 +76,7 @@ class CategoricalHinge(reduction_metrics.MeanMetricWrapper):\n         dtype: (Optional) data type of the metric result.\n \n     Standalone usage:\n-    >>> m = keras_core.metrics.CategoricalHinge()\n+    >>> m = keras.metrics.CategoricalHinge()\n     >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])\n     >>> m.result().numpy()\n     1.4000001\n\n@@ -1,7 +1,7 @@\n import numpy as np\n \n-from keras_core import testing\n-from keras_core.metrics import hinge_metrics\n+from keras import testing\n+from keras.metrics import hinge_metrics\n \n \n class HingeTest(testing.TestCase):\n\n@@ -1,9 +1,9 @@\n-from keras_core import backend\n-from keras_core import initializers\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.metrics.metric import Metric\n-from keras_core.metrics.metrics_utils import confusion_matrix\n+from keras import backend\n+from keras import initializers\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.metrics.metric import Metric\n+from keras.metrics.metrics_utils import confusion_matrix\n \n \n class _IoUBase(Metric):\n@@ -138,7 +138,7 @@ class _IoUBase(Metric):\n         )\n \n \n-@keras_core_export(\"keras_core.metrics.IoU\")\n+@keras_export(\"keras.metrics.IoU\")\n class IoU(_IoUBase):\n     \"\"\"Computes the Intersection-Over-Union metric for specific target classes.\n \n@@ -191,7 +191,7 @@ class IoU(_IoUBase):\n     >>> # sum_row = [2, 2], sum_col = [2, 2], true_positives = [1, 1]\n     >>> # iou = true_positives / (sum_row + sum_col - true_positives))\n     >>> # iou = [0.33, 0.33]\n-    >>> m = keras_core.metrics.IoU(num_classes=2, target_class_ids=[0])\n+    >>> m = keras.metrics.IoU(num_classes=2, target_class_ids=[0])\n     >>> m.update_state([0, 0, 1, 1], [0, 1, 0, 1])\n     >>> m.result()\n     0.33333334\n@@ -213,7 +213,7 @@ class IoU(_IoUBase):\n     model.compile(\n         optimizer='sgd',\n         loss='mse',\n-        metrics=[keras_core.metrics.IoU(num_classes=2, target_class_ids=[0])])\n+        metrics=[keras.metrics.IoU(num_classes=2, target_class_ids=[0])])\n     ```\n     \"\"\"\n \n@@ -295,7 +295,7 @@ class IoU(_IoUBase):\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n-@keras_core_export(\"keras_core.metrics.BinaryIoU\")\n+@keras_export(\"keras.metrics.BinaryIoU\")\n class BinaryIoU(IoU):\n     \"\"\"Computes the Intersection-Over-Union metric for class 0 and/or 1.\n \n@@ -340,7 +340,7 @@ class BinaryIoU(IoU):\n \n     Standalone usage:\n \n-    >>> m = keras_core.metrics.BinaryIoU(target_class_ids=[0, 1], threshold=0.3)\n+    >>> m = keras.metrics.BinaryIoU(target_class_ids=[0, 1], threshold=0.3)\n     >>> m.update_state([0, 1, 0, 1], [0.1, 0.2, 0.4, 0.7])\n     >>> m.result()\n     0.33333334\n@@ -362,7 +362,7 @@ class BinaryIoU(IoU):\n     model.compile(\n         optimizer='sgd',\n         loss='mse',\n-        metrics=[keras_core.metrics.BinaryIoU(\n+        metrics=[keras.metrics.BinaryIoU(\n             target_class_ids=[0],\n             threshold=0.5\n         )]\n@@ -417,7 +417,7 @@ class BinaryIoU(IoU):\n         }\n \n \n-@keras_core_export(\"keras_core.metrics.MeanIoU\")\n+@keras_export(\"keras.metrics.MeanIoU\")\n class MeanIoU(IoU):\n     \"\"\"Computes the mean Intersection-Over-Union metric.\n \n@@ -466,7 +466,7 @@ class MeanIoU(IoU):\n     >>> # sum_row = [2, 2], sum_col = [2, 2], true_positives = [1, 1]\n     >>> # iou = true_positives / (sum_row + sum_col - true_positives))\n     >>> # result = (1 / (2 + 2 - 1) + 1 / (2 + 2 - 1)) / 2 = 0.33\n-    >>> m = keras_core.metrics.MeanIoU(num_classes=2)\n+    >>> m = keras.metrics.MeanIoU(num_classes=2)\n     >>> m.update_state([0, 0, 1, 1], [0, 1, 0, 1])\n     >>> m.result()\n     0.33333334\n@@ -483,7 +483,7 @@ class MeanIoU(IoU):\n     model.compile(\n         optimizer='sgd',\n         loss='mse',\n-        metrics=[keras_core.metrics.MeanIoU(num_classes=2)])\n+        metrics=[keras.metrics.MeanIoU(num_classes=2)])\n     ```\n     \"\"\"\n \n@@ -521,7 +521,7 @@ class MeanIoU(IoU):\n         }\n \n \n-@keras_core_export(\"keras_core.metrics.OneHotIoU\")\n+@keras_export(\"keras.metrics.OneHotIoU\")\n class OneHotIoU(IoU):\n     \"\"\"Computes the Intersection-Over-Union metric for one-hot encoded labels.\n \n@@ -578,7 +578,7 @@ class OneHotIoU(IoU):\n     >>> y_pred = np.array([[0.2, 0.3, 0.5], [0.1, 0.2, 0.7], [0.5, 0.3, 0.1],\n     ...                       [0.1, 0.4, 0.5]])\n     >>> sample_weight = [0.1, 0.2, 0.3, 0.4]\n-    >>> m = keras_core.metrics.OneHotIoU(num_classes=3, target_class_ids=[0, 2])\n+    >>> m = keras.metrics.OneHotIoU(num_classes=3, target_class_ids=[0, 2])\n     >>> m.update_state(\n     ...     y_true=y_true, y_pred=y_pred, sample_weight=sample_weight)\n     >>> # cm = [[0, 0, 0.2+0.4],\n@@ -597,7 +597,7 @@ class OneHotIoU(IoU):\n     model.compile(\n         optimizer='sgd',\n         loss='mse',\n-        metrics=[keras_core.metrics.OneHotIoU(\n+        metrics=[keras.metrics.OneHotIoU(\n             num_classes=3,\n             target_class_id=[1]\n         )]\n@@ -638,7 +638,7 @@ class OneHotIoU(IoU):\n         }\n \n \n-@keras_core_export(\"keras_core.metrics.OneHotMeanIoU\")\n+@keras_export(\"keras.metrics.OneHotMeanIoU\")\n class OneHotMeanIoU(MeanIoU):\n     \"\"\"Computes mean Intersection-Over-Union metric for one-hot encoded labels.\n \n@@ -694,7 +694,7 @@ class OneHotMeanIoU(MeanIoU):\n     >>> y_pred = np.array([[0.2, 0.3, 0.5], [0.1, 0.2, 0.7], [0.5, 0.3, 0.1],\n     ...                       [0.1, 0.4, 0.5]])\n     >>> sample_weight = [0.1, 0.2, 0.3, 0.4]\n-    >>> m = keras_core.metrics.OneHotMeanIoU(num_classes=3)\n+    >>> m = keras.metrics.OneHotMeanIoU(num_classes=3)\n     >>> m.update_state(\n     ...     y_true=y_true, y_pred=y_pred, sample_weight=sample_weight)\n     >>> # cm = [[0, 0, 0.2+0.4],\n@@ -713,7 +713,7 @@ class OneHotMeanIoU(MeanIoU):\n     model.compile(\n         optimizer='sgd',\n         loss='mse',\n-        metrics=[keras_core.metrics.OneHotMeanIoU(num_classes=3)])\n+        metrics=[keras.metrics.OneHotMeanIoU(num_classes=3)])\n     ```\n     \"\"\"\n \n\n@@ -1,7 +1,7 @@\n import numpy as np\n \n-from keras_core import testing\n-from keras_core.metrics import iou_metrics as metrics\n+from keras import testing\n+from keras.metrics import iou_metrics as metrics\n \n \n class IoUTest(testing.TestCase):\n\n@@ -1,12 +1,12 @@\n-from keras_core import backend\n-from keras_core import initializers\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.utils.naming import auto_name\n-from keras_core.utils.tracking import Tracker\n+from keras import backend\n+from keras import initializers\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.utils.naming import auto_name\n+from keras.utils.tracking import Tracker\n \n \n-@keras_core_export([\"keras_core.Metric\", \"keras_core.metrics.Metric\"])\n+@keras_export([\"keras.Metric\", \"keras.metrics.Metric\"])\n class Metric:\n     \"\"\"Encapsulates metric logic and state.\n \n@@ -26,14 +26,14 @@ class Metric:\n     Usage with `compile()` API:\n \n     ```python\n-    model = keras_core.Sequential()\n-    model.add(keras_core.layers.Dense(64, activation='relu'))\n-    model.add(keras_core.layers.Dense(64, activation='relu'))\n-    model.add(keras_core.layers.Dense(10, activation='softmax'))\n+    model = keras.Sequential()\n+    model.add(keras.layers.Dense(64, activation='relu'))\n+    model.add(keras.layers.Dense(64, activation='relu'))\n+    model.add(keras.layers.Dense(10, activation='softmax'))\n \n-    model.compile(optimizer=keras_core.optimizers.RMSprop(0.01),\n-                  loss=keras_core.losses.CategoricalCrossentropy(),\n-                  metrics=[keras_core.metrics.CategoricalAccuracy()])\n+    model.compile(optimizer=keras.optimizers.RMSprop(0.01),\n+                  loss=keras.losses.CategoricalCrossentropy(),\n+                  metrics=[keras.metrics.CategoricalAccuracy()])\n \n     data = np.random.random((1000, 32))\n     labels = np.random.random((1000, 10))\n\n@@ -1,11 +1,11 @@\n import numpy as np\n \n-from keras_core import backend\n-from keras_core import initializers\n-from keras_core import metrics as metrics_module\n-from keras_core import ops\n-from keras_core import testing\n-from keras_core.metrics.metric import Metric\n+from keras import backend\n+from keras import initializers\n+from keras import metrics as metrics_module\n+from keras import ops\n+from keras import testing\n+from keras.metrics.metric import Metric\n \n \n class ExampleMetric(Metric):\n\n@@ -2,10 +2,10 @@ from enum import Enum\n \n import numpy as np\n \n-from keras_core import backend\n-from keras_core import ops\n-from keras_core.losses.loss import squeeze_to_same_rank\n-from keras_core.utils.python_utils import to_list\n+from keras import backend\n+from keras import ops\n+from keras.losses.loss import squeeze_to_same_rank\n+from keras.utils.python_utils import to_list\n \n NEG_INF = -1e10\n \n@@ -638,7 +638,7 @@ def confusion_matrix(\n     For example:\n \n     ```python\n-    keras_core.metrics.metrics_utils.confusion_matrix([1, 2, 4], [2, 2, 4]) ==>\n+    keras.metrics.metrics_utils.confusion_matrix([1, 2, 4], [2, 2, 4]) ==>\n         [[0 0 0 0 0]\n         [0 0 1 0 0]\n         [0 0 1 0 0]\n\n@@ -1,13 +1,13 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.losses.losses import binary_crossentropy\n-from keras_core.losses.losses import categorical_crossentropy\n-from keras_core.losses.losses import kl_divergence\n-from keras_core.losses.losses import poisson\n-from keras_core.losses.losses import sparse_categorical_crossentropy\n-from keras_core.metrics import reduction_metrics\n+from keras.api_export import keras_export\n+from keras.losses.losses import binary_crossentropy\n+from keras.losses.losses import categorical_crossentropy\n+from keras.losses.losses import kl_divergence\n+from keras.losses.losses import poisson\n+from keras.losses.losses import sparse_categorical_crossentropy\n+from keras.metrics import reduction_metrics\n \n \n-@keras_core_export(\"keras_core.metrics.KLDivergence\")\n+@keras_export(\"keras.metrics.KLDivergence\")\n class KLDivergence(reduction_metrics.MeanMetricWrapper):\n     \"\"\"Computes Kullback-Leibler divergence metric between `y_true` and\n     `y_pred`.\n@@ -24,7 +24,7 @@ class KLDivergence(reduction_metrics.MeanMetricWrapper):\n \n     Standalone usage:\n \n-    >>> m = keras_core.metrics.KLDivergence()\n+    >>> m = keras.metrics.KLDivergence()\n     >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])\n     >>> m.result()\n     0.45814306\n@@ -40,7 +40,7 @@ class KLDivergence(reduction_metrics.MeanMetricWrapper):\n     ```python\n     model.compile(optimizer='sgd',\n                   loss='mse',\n-                  metrics=[keras_core.metrics.KLDivergence()])\n+                  metrics=[keras.metrics.KLDivergence()])\n     ```\n     \"\"\"\n \n@@ -51,7 +51,7 @@ class KLDivergence(reduction_metrics.MeanMetricWrapper):\n         return {\"name\": self.name, \"dtype\": self.dtype}\n \n \n-@keras_core_export(\"keras_core.metrics.Poisson\")\n+@keras_export(\"keras.metrics.Poisson\")\n class Poisson(reduction_metrics.MeanMetricWrapper):\n     \"\"\"Computes the Poisson metric between `y_true` and `y_pred`.\n \n@@ -69,7 +69,7 @@ class Poisson(reduction_metrics.MeanMetricWrapper):\n \n     Standalone usage:\n \n-    >>> m = keras_core.metrics.Poisson()\n+    >>> m = keras.metrics.Poisson()\n     >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])\n     >>> m.result()\n     0.49999997\n@@ -85,7 +85,7 @@ class Poisson(reduction_metrics.MeanMetricWrapper):\n     ```python\n     model.compile(optimizer='sgd',\n                   loss='mse',\n-                  metrics=[keras_core.metrics.Poisson()])\n+                  metrics=[keras.metrics.Poisson()])\n     ```\n     \"\"\"\n \n@@ -96,7 +96,7 @@ class Poisson(reduction_metrics.MeanMetricWrapper):\n         return {\"name\": self.name, \"dtype\": self.dtype}\n \n \n-@keras_core_export(\"keras_core.metrics.BinaryCrossentropy\")\n+@keras_export(\"keras.metrics.BinaryCrossentropy\")\n class BinaryCrossentropy(reduction_metrics.MeanMetricWrapper):\n     \"\"\"Computes the crossentropy metric between the labels and predictions.\n \n@@ -119,7 +119,7 @@ class BinaryCrossentropy(reduction_metrics.MeanMetricWrapper):\n \n     Standalone usage:\n \n-    >>> m = keras_core.metrics.BinaryCrossentropy()\n+    >>> m = keras.metrics.BinaryCrossentropy()\n     >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])\n     >>> m.result()\n     0.81492424\n@@ -136,7 +136,7 @@ class BinaryCrossentropy(reduction_metrics.MeanMetricWrapper):\n     model.compile(\n         optimizer='sgd',\n         loss='mse',\n-        metrics=[keras_core.metrics.BinaryCrossentropy()])\n+        metrics=[keras.metrics.BinaryCrossentropy()])\n     ```\n     \"\"\"\n \n@@ -166,7 +166,7 @@ class BinaryCrossentropy(reduction_metrics.MeanMetricWrapper):\n         }\n \n \n-@keras_core_export(\"keras_core.metrics.CategoricalCrossentropy\")\n+@keras_export(\"keras.metrics.CategoricalCrossentropy\")\n class CategoricalCrossentropy(reduction_metrics.MeanMetricWrapper):\n     \"\"\"Computes the crossentropy metric between the labels and predictions.\n \n@@ -200,7 +200,7 @@ class CategoricalCrossentropy(reduction_metrics.MeanMetricWrapper):\n     >>> #      = -((log 0.95), (log 0.1))\n     >>> #      = [0.051, 2.302]\n     >>> # Reduced xent = (0.051 + 2.302) / 2\n-    >>> m = keras_core.metrics.CategoricalCrossentropy()\n+    >>> m = keras.metrics.CategoricalCrossentropy()\n     >>> m.update_state([[0, 1, 0], [0, 0, 1]],\n     ...                [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n     >>> m.result()\n@@ -219,7 +219,7 @@ class CategoricalCrossentropy(reduction_metrics.MeanMetricWrapper):\n     model.compile(\n         optimizer='sgd',\n         loss='mse',\n-        metrics=[keras_core.metrics.CategoricalCrossentropy()])\n+        metrics=[keras.metrics.CategoricalCrossentropy()])\n     ```\n     \"\"\"\n \n@@ -253,7 +253,7 @@ class CategoricalCrossentropy(reduction_metrics.MeanMetricWrapper):\n         }\n \n \n-@keras_core_export(\"keras_core.metrics.SparseCategoricalCrossentropy\")\n+@keras_export(\"keras.metrics.SparseCategoricalCrossentropy\")\n class SparseCategoricalCrossentropy(reduction_metrics.MeanMetricWrapper):\n     \"\"\"Computes the crossentropy metric between the labels and predictions.\n \n@@ -288,7 +288,7 @@ class SparseCategoricalCrossentropy(reduction_metrics.MeanMetricWrapper):\n     >>> # y_true * log(softmax) = [[0, -0.0513, 0], [0, 0, -2.3026]]\n     >>> # xent = [0.0513, 2.3026]\n     >>> # Reduced xent = (0.0513 + 2.3026) / 2\n-    >>> m = keras_core.metrics.SparseCategoricalCrossentropy()\n+    >>> m = keras.metrics.SparseCategoricalCrossentropy()\n     >>> m.update_state([1, 2],\n     ...                [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n     >>> m.result()\n@@ -307,7 +307,7 @@ class SparseCategoricalCrossentropy(reduction_metrics.MeanMetricWrapper):\n     model.compile(\n         optimizer='sgd',\n         loss='mse',\n-        metrics=[keras_core.metrics.SparseCategoricalCrossentropy()])\n+        metrics=[keras.metrics.SparseCategoricalCrossentropy()])\n     ```\n     \"\"\"\n \n\n@@ -1,7 +1,7 @@\n import numpy as np\n \n-from keras_core import metrics\n-from keras_core import testing\n+from keras import metrics\n+from keras import testing\n \n \n class KLDivergenceTest(testing.TestCase):\n\n@@ -1,10 +1,10 @@\n-from keras_core import backend\n-from keras_core import initializers\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.losses import loss\n-from keras_core.metrics.metric import Metric\n-from keras_core.saving import serialization_lib\n+from keras import backend\n+from keras import initializers\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.losses import loss\n+from keras.metrics.metric import Metric\n+from keras.saving import serialization_lib\n \n \n def reduce_to_samplewise_values(values, sample_weight, reduce_fn, dtype):\n@@ -38,7 +38,7 @@ def reduce_to_samplewise_values(values, sample_weight, reduce_fn, dtype):\n     return values, sample_weight\n \n \n-@keras_core_export(\"keras_core.metrics.Sum\")\n+@keras_export(\"keras.metrics.Sum\")\n class Sum(Metric):\n     \"\"\"Compute the (weighted) sum of the given values.\n \n@@ -87,7 +87,7 @@ class Sum(Metric):\n         return ops.cast(self.total, self.dtype)\n \n \n-@keras_core_export(\"keras_core.metrics.Mean\")\n+@keras_export(\"keras.metrics.Mean\")\n class Mean(Metric):\n     \"\"\"Compute the (weighted) mean of the given values.\n \n@@ -155,7 +155,7 @@ class Mean(Metric):\n         )\n \n \n-@keras_core_export(\"keras_core.metrics.MeanMetricWrapper\")\n+@keras_export(\"keras.metrics.MeanMetricWrapper\")\n class MeanMetricWrapper(Mean):\n     \"\"\"Wrap a stateless metric function with the Mean metric.\n \n\n@@ -1,7 +1,7 @@\n import numpy as np\n \n-from keras_core import testing\n-from keras_core.metrics import reduction_metrics\n+from keras import testing\n+from keras.metrics import reduction_metrics\n \n \n class SumTest(testing.TestCase):\n\n@@ -1,19 +1,19 @@\n import warnings\n \n-from keras_core import initializers\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.losses.loss import squeeze_to_same_rank\n-from keras_core.losses.losses import log_cosh\n-from keras_core.losses.losses import mean_absolute_error\n-from keras_core.losses.losses import mean_absolute_percentage_error\n-from keras_core.losses.losses import mean_squared_error\n-from keras_core.losses.losses import mean_squared_logarithmic_error\n-from keras_core.metrics import reduction_metrics\n-from keras_core.utils.numerical_utils import normalize\n+from keras import initializers\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.losses.loss import squeeze_to_same_rank\n+from keras.losses.losses import log_cosh\n+from keras.losses.losses import mean_absolute_error\n+from keras.losses.losses import mean_absolute_percentage_error\n+from keras.losses.losses import mean_squared_error\n+from keras.losses.losses import mean_squared_logarithmic_error\n+from keras.metrics import reduction_metrics\n+from keras.utils.numerical_utils import normalize\n \n \n-@keras_core_export(\"keras_core.metrics.MeanSquaredError\")\n+@keras_export(\"keras.metrics.MeanSquaredError\")\n class MeanSquaredError(reduction_metrics.MeanMetricWrapper):\n     \"\"\"Computes the mean squared error between `y_true` and `y_pred`.\n \n@@ -29,7 +29,7 @@ class MeanSquaredError(reduction_metrics.MeanMetricWrapper):\n \n     Example:\n \n-    >>> m = keras_core.metrics.MeanSquaredError()\n+    >>> m = keras.metrics.MeanSquaredError()\n     >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])\n     >>> m.result()\n     0.25\n@@ -42,7 +42,7 @@ class MeanSquaredError(reduction_metrics.MeanMetricWrapper):\n         return {\"name\": self.name, \"dtype\": self.dtype}\n \n \n-@keras_core_export(\"keras_core.metrics.MeanAbsoluteError\")\n+@keras_export(\"keras.metrics.MeanAbsoluteError\")\n class MeanAbsoluteError(reduction_metrics.MeanMetricWrapper):\n     \"\"\"Computes the mean absolute error between the labels and predictions.\n \n@@ -60,7 +60,7 @@ class MeanAbsoluteError(reduction_metrics.MeanMetricWrapper):\n \n     Standalone usage:\n \n-    >>> m = keras_core.metrics.MeanAbsoluteError()\n+    >>> m = keras.metrics.MeanAbsoluteError()\n     >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])\n     >>> m.result()\n     0.25\n@@ -76,7 +76,7 @@ class MeanAbsoluteError(reduction_metrics.MeanMetricWrapper):\n     model.compile(\n         optimizer='sgd',\n         loss='mse',\n-        metrics=[keras_core.metrics.MeanAbsoluteError()])\n+        metrics=[keras.metrics.MeanAbsoluteError()])\n     ```\n     \"\"\"\n \n@@ -87,7 +87,7 @@ class MeanAbsoluteError(reduction_metrics.MeanMetricWrapper):\n         return {\"name\": self.name, \"dtype\": self.dtype}\n \n \n-@keras_core_export(\"keras_core.metrics.MeanAbsolutePercentageError\")\n+@keras_export(\"keras.metrics.MeanAbsolutePercentageError\")\n class MeanAbsolutePercentageError(reduction_metrics.MeanMetricWrapper):\n     \"\"\"Computes mean absolute percentage error between `y_true` and `y_pred`.\n \n@@ -105,7 +105,7 @@ class MeanAbsolutePercentageError(reduction_metrics.MeanMetricWrapper):\n \n     Standalone usage:\n \n-    >>> m = keras_core.metrics.MeanAbsolutePercentageError()\n+    >>> m = keras.metrics.MeanAbsolutePercentageError()\n     >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])\n     >>> m.result()\n     250000000.0\n@@ -121,7 +121,7 @@ class MeanAbsolutePercentageError(reduction_metrics.MeanMetricWrapper):\n     model.compile(\n         optimizer='sgd',\n         loss='mse',\n-        metrics=[keras_core.metrics.MeanAbsolutePercentageError()])\n+        metrics=[keras.metrics.MeanAbsolutePercentageError()])\n     ```\n     \"\"\"\n \n@@ -132,7 +132,7 @@ class MeanAbsolutePercentageError(reduction_metrics.MeanMetricWrapper):\n         return {\"name\": self.name, \"dtype\": self.dtype}\n \n \n-@keras_core_export(\"keras_core.metrics.MeanSquaredLogarithmicError\")\n+@keras_export(\"keras.metrics.MeanSquaredLogarithmicError\")\n class MeanSquaredLogarithmicError(reduction_metrics.MeanMetricWrapper):\n     \"\"\"Computes mean squared logarithmic error between `y_true` and `y_pred`.\n \n@@ -150,7 +150,7 @@ class MeanSquaredLogarithmicError(reduction_metrics.MeanMetricWrapper):\n \n     Standalone usage:\n \n-    >>> m = keras_core.metrics.MeanSquaredLogarithmicError()\n+    >>> m = keras.metrics.MeanSquaredLogarithmicError()\n     >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])\n     >>> m.result()\n     0.12011322\n@@ -166,7 +166,7 @@ class MeanSquaredLogarithmicError(reduction_metrics.MeanMetricWrapper):\n     model.compile(\n         optimizer='sgd',\n         loss='mse',\n-        metrics=[keras_core.metrics.MeanSquaredLogarithmicError()])\n+        metrics=[keras.metrics.MeanSquaredLogarithmicError()])\n     ```\n     \"\"\"\n \n@@ -177,7 +177,7 @@ class MeanSquaredLogarithmicError(reduction_metrics.MeanMetricWrapper):\n         return {\"name\": self.name, \"dtype\": self.dtype}\n \n \n-@keras_core_export(\"keras_core.metrics.RootMeanSquaredError\")\n+@keras_export(\"keras.metrics.RootMeanSquaredError\")\n class RootMeanSquaredError(reduction_metrics.Mean):\n     \"\"\"Computes root mean squared error metric between `y_true` and `y_pred`.\n \n@@ -195,7 +195,7 @@ class RootMeanSquaredError(reduction_metrics.Mean):\n \n     Standalone usage:\n \n-    >>> m = keras_core.metrics.RootMeanSquaredError()\n+    >>> m = keras.metrics.RootMeanSquaredError()\n     >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])\n     >>> m.result()\n     0.5\n@@ -212,7 +212,7 @@ class RootMeanSquaredError(reduction_metrics.Mean):\n     model.compile(\n         optimizer='sgd',\n         loss='mse',\n-        metrics=[keras_core.metrics.RootMeanSquaredError()])\n+        metrics=[keras.metrics.RootMeanSquaredError()])\n     ```\n     \"\"\"\n \n@@ -243,7 +243,7 @@ class RootMeanSquaredError(reduction_metrics.Mean):\n         return ops.sqrt(super().result())\n \n \n-@keras_core_export(\"keras_core.metrics.CosineSimilarity\")\n+@keras_export(\"keras.metrics.CosineSimilarity\")\n class CosineSimilarity(reduction_metrics.MeanMetricWrapper):\n     \"\"\"Computes the cosine similarity between the labels and predictions.\n \n@@ -271,7 +271,7 @@ class CosineSimilarity(reduction_metrics.MeanMetricWrapper):\n     >>> # l2_norm(y_true) . l2_norm(y_pred) = [[0., 0.], [0.5, 0.5]]\n     >>> # result = mean(sum(l2_norm(y_true) . l2_norm(y_pred), axis=1))\n     >>> #        = ((0. + 0.) +  (0.5 + 0.5)) / 2\n-    >>> m = keras_core.metrics.CosineSimilarity(axis=1)\n+    >>> m = keras.metrics.CosineSimilarity(axis=1)\n     >>> m.update_state([[0., 1.], [1., 1.]], [[1., 0.], [1., 1.]])\n     >>> m.result()\n     0.49999997\n@@ -287,7 +287,7 @@ class CosineSimilarity(reduction_metrics.MeanMetricWrapper):\n     model.compile(\n         optimizer='sgd',\n         loss='mse',\n-        metrics=[keras_core.metrics.CosineSimilarity(axis=1)])\n+        metrics=[keras.metrics.CosineSimilarity(axis=1)])\n     ```\n     \"\"\"\n \n@@ -298,7 +298,7 @@ class CosineSimilarity(reduction_metrics.MeanMetricWrapper):\n         return {\"name\": self.name, \"dtype\": self.dtype}\n \n \n-@keras_core_export(\"keras_core.metrics.LogCoshError\")\n+@keras_export(\"keras.metrics.LogCoshError\")\n class LogCoshError(reduction_metrics.MeanMetricWrapper):\n     \"\"\"Computes the logarithm of the hyperbolic cosine of the prediction error.\n \n@@ -317,7 +317,7 @@ class LogCoshError(reduction_metrics.MeanMetricWrapper):\n \n     Standalone usage:\n \n-    >>> m = keras_core.metrics.LogCoshError()\n+    >>> m = keras.metrics.LogCoshError()\n     >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])\n     >>> m.result()\n     0.10844523\n@@ -332,7 +332,7 @@ class LogCoshError(reduction_metrics.MeanMetricWrapper):\n     ```python\n     model.compile(optimizer='sgd',\n                   loss='mse',\n-                  metrics=[keras_core.metrics.LogCoshError()])\n+                  metrics=[keras.metrics.LogCoshError()])\n     ```\n     \"\"\"\n \n@@ -344,7 +344,7 @@ class LogCoshError(reduction_metrics.MeanMetricWrapper):\n \n \n # Adapted from TF-Addons implementation (RSquare class).\n-@keras_core_export(\"keras_core.metrics.R2Score\")\n+@keras_export(\"keras.metrics.R2Score\")\n class R2Score(reduction_metrics.Metric):\n     \"\"\"Computes R2 score.\n \n@@ -389,7 +389,7 @@ class R2Score(reduction_metrics.Metric):\n \n     >>> y_true = np.array([[1], [4], [3]], dtype=np.float32)\n     >>> y_pred = np.array([[2], [4], [4]], dtype=np.float32)\n-    >>> metric = keras_core.metrics.R2Score()\n+    >>> metric = keras.metrics.R2Score()\n     >>> metric.update_state(y_true, y_pred)\n     >>> result = metric.result()\n     >>> result\n@@ -590,7 +590,7 @@ def cosine_similarity(y_true, y_pred, axis=-1):\n \n     >>> y_true = [[0., 1.], [1., 1.], [1., 1.]]\n     >>> y_pred = [[1., 0.], [1., 1.], [-1., -1.]]\n-    >>> loss = keras_core.losses.cosine_similarity(y_true, y_pred, axis=-1)\n+    >>> loss = keras.losses.cosine_similarity(y_true, y_pred, axis=-1)\n     [0., 0.99999994, -0.99999994]\n     \"\"\"\n     y_pred = ops.convert_to_tensor(y_pred)\n\n@@ -1,8 +1,8 @@\n import numpy as np\n from absl.testing import parameterized\n \n-from keras_core import testing\n-from keras_core.metrics import regression_metrics as metrics\n+from keras import testing\n+from keras.metrics import regression_metrics as metrics\n \n \n class MeanSquaredErrorTest(testing.TestCase):\n\n@@ -1,8 +1,8 @@\n-from keras_core import backend\n-from keras_core.mixed_precision.dtype_policy import DTypePolicy\n-from keras_core.mixed_precision.dtype_policy import dtype_policy\n-from keras_core.mixed_precision.dtype_policy import set_dtype_policy\n-from keras_core.saving import serialization_lib\n+from keras import backend\n+from keras.mixed_precision.dtype_policy import DTypePolicy\n+from keras.mixed_precision.dtype_policy import dtype_policy\n+from keras.mixed_precision.dtype_policy import set_dtype_policy\n+from keras.saving import serialization_lib\n \n \n def resolve_policy(identifier):\n\n@@ -1,12 +1,12 @@\n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.backend.common import global_state\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.backend.common import global_state\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.mixed_precision.DTypePolicy\",\n-        \"keras_core.mixed_precision.Policy\",\n+        \"keras.mixed_precision.DTypePolicy\",\n+        \"keras.mixed_precision.Policy\",\n     ]\n )\n class DTypePolicy:\n@@ -15,7 +15,7 @@ class DTypePolicy:\n     A dtype policy determines a layer's computation and variable dtypes. Each\n     layer has a policy. Policies can be passed to the `dtype` argument of layer\n     constructors, or a global policy can be set with\n-    `keras_core.mixed_precision.set_dtype_policy`.\n+    `keras.mixed_precision.set_dtype_policy`.\n \n     Args:\n         name: The policy name, which determines the compute and variable dtypes.\n@@ -30,23 +30,23 @@ class DTypePolicy:\n     precision, which is the use of float16 or bfloat16 for computations and\n     float32 for variables. This is why the term `mixed_precision` appears in the\n     API name. Mixed precision can be enabled by passing `\"mixed_float16\"` or\n-    `\"mixed_bfloat16\"` to `keras_core.mixed_precision.set_dtype_policy()`.\n+    `\"mixed_bfloat16\"` to `keras.mixed_precision.set_dtype_policy()`.\n \n-    >>> keras_core.mixed_precision.set_dtype_policy(\"mixed_float16\")\n-    >>> layer1 = keras_core.layers.Dense(10)\n+    >>> keras.mixed_precision.set_dtype_policy(\"mixed_float16\")\n+    >>> layer1 = keras.layers.Dense(10)\n     >>> layer1.dtype_policy  # layer1 will automatically use mixed precision\n     <DTypePolicy \"mixed_float16\">\n     >>> # Can optionally override layer to use float32\n     >>> # instead of mixed precision.\n-    >>> layer2 = keras_core.layers.Dense(10, dtype=\"float32\")\n+    >>> layer2 = keras.layers.Dense(10, dtype=\"float32\")\n     >>> layer2.dtype_policy\n     <DTypePolicy \"float32\">\n     >>> # Set policy back to initial float32.\n-    >>> keras_core.mixed_precision.set_dtype_policy('float32')\n+    >>> keras.mixed_precision.set_dtype_policy('float32')\n \n     In the example above, passing `dtype=\"float32\"` to the layer is\n     equivalent to passing\n-    `dtype=keras_core.mixed_precision.DTypePolicy(\"float32\")`.\n+    `dtype=keras.mixed_precision.DTypePolicy(\"float32\")`.\n     In general, passing a dtype policy name to a layer is equivalent\n     to passing the corresponding policy, so it is never necessary\n     to explicitly construct a `DTypePolicy` object.\n@@ -140,10 +140,10 @@ class DTypePolicy:\n         return cls(**config)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.mixed_precision.set_dtype_policy\",\n-        \"keras_core.mixed_precision.set_global_policy\",\n+        \"keras.mixed_precision.set_dtype_policy\",\n+        \"keras.mixed_precision.set_global_policy\",\n     ]\n )\n def set_dtype_policy(policy):\n@@ -151,7 +151,7 @@ def set_dtype_policy(policy):\n \n     Example:\n \n-    >>> keras_core.mixed_precision.set_dtype_policy(\"mixed_float16\")\n+    >>> keras.mixed_precision.set_dtype_policy(\"mixed_float16\")\n     \"\"\"\n     if not isinstance(policy, DTypePolicy):\n         if isinstance(policy, str):\n@@ -167,10 +167,10 @@ def set_dtype_policy(policy):\n     global_state.set_global_attribute(\"dtype_policy\", policy)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.mixed_precision.dtype_policy\",\n-        \"keras_core.mixed_precision.global_policy\",\n+        \"keras.mixed_precision.dtype_policy\",\n+        \"keras.mixed_precision.global_policy\",\n     ]\n )\n def dtype_policy():\n\n@@ -0,0 +1,3 @@\n+from keras.models.functional import Functional\n+from keras.models.model import Model\n+from keras.models.sequential import Sequential\n\n@@ -1,17 +1,17 @@\n import tree\n \n-from keras_core import backend\n-from keras_core import utils\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers import Input\n-from keras_core.layers import InputLayer\n-from keras_core.models.functional import Functional\n-from keras_core.models.functional import functional_like_constructor\n-from keras_core.models.sequential import Sequential\n-from keras_core.saving import serialization_lib\n+from keras import backend\n+from keras import utils\n+from keras.api_export import keras_export\n+from keras.layers import Input\n+from keras.layers import InputLayer\n+from keras.models.functional import Functional\n+from keras.models.functional import functional_like_constructor\n+from keras.models.sequential import Sequential\n+from keras.saving import serialization_lib\n \n \n-@keras_core_export(\"keras_core.models.clone_model\")\n+@keras_export(\"keras.models.clone_model\")\n def clone_model(model, input_tensors=None, clone_function=None):\n     \"\"\"Clone a Functional or Sequential `Model` instance.\n \n@@ -55,10 +55,10 @@ def clone_model(model, input_tensors=None, clone_function=None):\n \n     ```python\n     # Create a test Sequential model.\n-    model = keras_core.Sequential([\n-        keras_core.layers.Input(shape=(728,)),\n-        keras_core.layers.Dense(32, activation='relu'),\n-        keras_core.layers.Dense(1, activation='sigmoid'),\n+    model = keras.Sequential([\n+        keras.layers.Input(shape=(728,)),\n+        keras.layers.Dense(32, activation='relu'),\n+        keras.layers.Dense(1, activation='sigmoid'),\n     ])\n     # Create a copy of the test model (with freshly initialized weights).\n     new_model = clone_model(model)\n\n@@ -2,10 +2,10 @@ import numpy as np\n import pytest\n from absl.testing import parameterized\n \n-from keras_core import layers\n-from keras_core import models\n-from keras_core import testing\n-from keras_core.models.cloning import clone_model\n+from keras import layers\n+from keras import models\n+from keras import testing\n+from keras.models.cloning import clone_model\n \n \n def get_functional_model(shared_layers=False):\n\n@@ -4,18 +4,18 @@ import warnings\n \n import tree\n \n-from keras_core import backend\n-from keras_core import ops\n-from keras_core.backend.common import global_state\n-from keras_core.layers.input_spec import InputSpec\n-from keras_core.layers.layer import Layer\n-from keras_core.legacy.saving import saving_utils\n-from keras_core.legacy.saving import serialization as legacy_serialization\n-from keras_core.models.model import Model\n-from keras_core.ops.function import Function\n-from keras_core.ops.function import make_node_key\n-from keras_core.saving import serialization_lib\n-from keras_core.utils import tracking\n+from keras import backend\n+from keras import ops\n+from keras.backend.common import global_state\n+from keras.layers.input_spec import InputSpec\n+from keras.layers.layer import Layer\n+from keras.legacy.saving import saving_utils\n+from keras.legacy.saving import serialization as legacy_serialization\n+from keras.models.model import Model\n+from keras.ops.function import Function\n+from keras.ops.function import make_node_key\n+from keras.saving import serialization_lib\n+from keras.utils import tracking\n \n \n class Functional(Function, Model):\n@@ -25,7 +25,7 @@ class Functional(Function, Model):\n     and `Sequential` (a special case of `Functional`).\n \n     A `Functional` model can be instantiated by passing two arguments to\n-    `__init__()`. The first argument is the `keras_core.Input` objects\n+    `__init__()`. The first argument is the `keras.Input` objects\n     that represent the inputs to the model.\n     The second argument specifies the output tensors that represent\n     the outputs of this model. Both arguments can be a nested structure\n@@ -34,23 +34,23 @@ class Functional(Function, Model):\n     Example:\n \n     ```\n-    inputs = {'x1': keras_core.Input(shape=(10,), name='x1'),\n-              'x2': keras_core.Input(shape=(1,), name='x2')}\n-    t = keras_core.layers.Dense(1, activation='relu')(inputs['x1'])\n-    outputs = keras_core.layers.Add()([t, inputs['x2']])\n-    model = keras_core.Model(inputs, outputs)\n+    inputs = {'x1': keras.Input(shape=(10,), name='x1'),\n+              'x2': keras.Input(shape=(1,), name='x2')}\n+    t = keras.layers.Dense(1, activation='relu')(inputs['x1'])\n+    outputs = keras.layers.Add()([t, inputs['x2']])\n+    model = keras.Model(inputs, outputs)\n     ```\n \n     A `Functional` model constructed using the Functional API can also\n-    include raw Keras Core ops.\n+    include raw Keras 3 ops.\n \n     Example:\n \n     ```python\n-    inputs = keras_core.Input(shape=(10,))\n-    x = keras_core.layers.Dense(1)(inputs)\n+    inputs = keras.Input(shape=(10,))\n+    x = keras.layers.Dense(1)(inputs)\n     outputs = ops.nn.relu(x)\n-    model = keras_core.Model(inputs, outputs)\n+    model = keras.Model(inputs, outputs)\n     ```\n \n     A new `Functional` model can also be created by using the\n@@ -60,20 +60,20 @@ class Functional(Function, Model):\n     Example:\n \n     ```python\n-    inputs = keras_core.Input(shape=(None, None, 3))\n-    processed = keras_core.layers.RandomCrop(width=32, height=32)(inputs)\n-    conv = keras_core.layers.Conv2D(filters=2, kernel_size=3)(processed)\n-    pooling = keras_core.layers.GlobalAveragePooling2D()(conv)\n-    feature = keras_core.layers.Dense(10)(pooling)\n+    inputs = keras.Input(shape=(None, None, 3))\n+    processed = keras.layers.RandomCrop(width=32, height=32)(inputs)\n+    conv = keras.layers.Conv2D(filters=2, kernel_size=3)(processed)\n+    pooling = keras.layers.GlobalAveragePooling2D()(conv)\n+    feature = keras.layers.Dense(10)(pooling)\n \n-    full_model = keras_core.Model(inputs, feature)\n-    backbone = keras_core.Model(processed, conv)\n-    activations = keras_core.Model(conv, feature)\n+    full_model = keras.Model(inputs, feature)\n+    backbone = keras.Model(processed, conv)\n+    activations = keras.Model(conv, feature)\n     ```\n \n     Note that the `backbone` and `activations` models are not\n-    created with `keras_core.Input` objects, but with the tensors\n-    that are originated from `keras_core.Input` objects.\n+    created with `keras.Input` objects, but with the tensors\n+    that are originated from `keras.Input` objects.\n     Under the hood, the layers and weights will\n     be shared across these models, so that user can train the `full_model`, and\n     use `backbone` or `activations` to do feature extraction.\n@@ -82,8 +82,8 @@ class Functional(Function, Model):\n     all the existing API.\n \n     Args:\n-        inputs: List of input tensors (must be created via `keras_core.Input()`\n-            or originated from `keras_core.Input()`).\n+        inputs: List of input tensors (must be created via `keras.Input()`\n+            or originated from `keras.Input()`).\n         outputs: List of output tensors.\n         name: String, optional. Name of the model.\n         trainable: Boolean, optional. If the model's variables should be\n\n@@ -3,13 +3,13 @@ import warnings\n import numpy as np\n import pytest\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import testing\n-from keras_core.layers.core.input_layer import Input\n-from keras_core.layers.input_spec import InputSpec\n-from keras_core.models import Functional\n-from keras_core.models import Model\n+from keras import backend\n+from keras import layers\n+from keras import testing\n+from keras.layers.core.input_layer import Input\n+from keras.layers.input_spec import InputSpec\n+from keras.models import Functional\n+from keras.models import Model\n \n \n class FunctionalTest(testing.TestCase):\n\n@@ -3,36 +3,36 @@ import json\n import os\n import warnings\n \n-from keras_core import backend\n-from keras_core import utils\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n-from keras_core.legacy.saving import legacy_h5_format\n-from keras_core.models.variable_mapping import map_trackable_variables\n-from keras_core.saving import saving_api\n-from keras_core.saving import saving_lib\n-from keras_core.trainers import trainer as base_trainer\n-from keras_core.utils import io_utils\n-from keras_core.utils import summary_utils\n-from keras_core.utils import traceback_utils\n+from keras import backend\n+from keras import utils\n+from keras.api_export import keras_export\n+from keras.layers.layer import Layer\n+from keras.legacy.saving import legacy_h5_format\n+from keras.models.variable_mapping import map_trackable_variables\n+from keras.saving import saving_api\n+from keras.saving import saving_lib\n+from keras.trainers import trainer as base_trainer\n+from keras.utils import io_utils\n+from keras.utils import summary_utils\n+from keras.utils import traceback_utils\n \n if backend.backend() == \"tensorflow\":\n-    from keras_core.backend.tensorflow.trainer import (\n+    from keras.backend.tensorflow.trainer import (\n         TensorFlowTrainer as Trainer,\n     )\n elif backend.backend() == \"jax\":\n-    from keras_core.backend.jax.trainer import JAXTrainer as Trainer\n+    from keras.backend.jax.trainer import JAXTrainer as Trainer\n elif backend.backend() == \"torch\":\n-    from keras_core.backend.torch.trainer import TorchTrainer as Trainer\n+    from keras.backend.torch.trainer import TorchTrainer as Trainer\n elif backend.backend() == \"numpy\":\n-    from keras_core.backend.numpy.trainer import NumpyTrainer as Trainer\n+    from keras.backend.numpy.trainer import NumpyTrainer as Trainer\n else:\n     raise RuntimeError(\n         f\"Backend '{backend.backend()}' must implement the Trainer class.\"\n     )\n \n \n-@keras_core_export([\"keras_core.Model\", \"keras_core.models.Model\"])\n+@keras_export([\"keras.Model\", \"keras.models.Model\"])\n class Model(Trainer, Layer):\n     \"\"\"A model grouping layers into an object with training/inference features.\n \n@@ -45,10 +45,10 @@ class Model(Trainer, Layer):\n     and finally you create your model from inputs and outputs:\n \n     ```python\n-    inputs = keras_core.Input(shape=(37,))\n-    x = keras_core.layers.Dense(32, activation=\"relu\")(inputs)\n-    outputs = keras_core.layers.Dense(5, activation=\"softmax\")(x)\n-    model = keras_core.Model(inputs=inputs, outputs=outputs)\n+    inputs = keras.Input(shape=(37,))\n+    x = keras.layers.Dense(32, activation=\"relu\")(inputs)\n+    outputs = keras.layers.Dense(5, activation=\"softmax\")(x)\n+    model = keras.Model(inputs=inputs, outputs=outputs)\n     ```\n \n     Note: Only dicts, lists, and tuples of input tensors are supported. Nested\n@@ -61,20 +61,20 @@ class Model(Trainer, Layer):\n     Example:\n \n     ```python\n-    inputs = keras_core.Input(shape=(None, None, 3))\n-    processed = keras_core.layers.RandomCrop(width=128, height=128)(inputs)\n-    conv = keras_core.layers.Conv2D(filters=32, kernel_size=3)(processed)\n-    pooling = keras_core.layers.GlobalAveragePooling2D()(conv)\n-    feature = keras_core.layers.Dense(10)(pooling)\n+    inputs = keras.Input(shape=(None, None, 3))\n+    processed = keras.layers.RandomCrop(width=128, height=128)(inputs)\n+    conv = keras.layers.Conv2D(filters=32, kernel_size=3)(processed)\n+    pooling = keras.layers.GlobalAveragePooling2D()(conv)\n+    feature = keras.layers.Dense(10)(pooling)\n \n-    full_model = keras_core.Model(inputs, feature)\n-    backbone = keras_core.Model(processed, conv)\n-    activations = keras_core.Model(conv, feature)\n+    full_model = keras.Model(inputs, feature)\n+    backbone = keras.Model(processed, conv)\n+    activations = keras.Model(conv, feature)\n     ```\n \n     Note that the `backbone` and `activations` models are not\n-    created with `keras_core.Input` objects, but with the tensors that originate\n-    from `keras_core.Input` objects. Under the hood, the layers and weights will\n+    created with `keras.Input` objects, but with the tensors that originate\n+    from `keras.Input` objects. Under the hood, the layers and weights will\n     be shared across these models, so that user can train the `full_model`, and\n     use `backbone` or `activations` to do feature extraction.\n     The inputs and outputs of the model can be nested structures of tensors as\n@@ -88,11 +88,11 @@ class Model(Trainer, Layer):\n     in `call()`.\n \n     ```python\n-    class MyModel(keras_core.Model):\n+    class MyModel(keras.Model):\n         def __init__(self):\n             super().__init__()\n-            self.dense1 = keras_core.layers.Dense(32, activation=\"relu\")\n-            self.dense2 = keras_core.layers.Dense(5, activation=\"softmax\")\n+            self.dense1 = keras.layers.Dense(32, activation=\"relu\")\n+            self.dense2 = keras.layers.Dense(5, activation=\"softmax\")\n \n         def call(self, inputs):\n             x = self.dense1(inputs)\n@@ -106,12 +106,12 @@ class Model(Trainer, Layer):\n     a different behavior in training and inference:\n \n     ```python\n-    class MyModel(keras_core.Model):\n+    class MyModel(keras.Model):\n         def __init__(self):\n             super().__init__()\n-            self.dense1 = keras_core.layers.Dense(32, activation=\"relu\")\n-            self.dense2 = keras_core.layers.Dense(5, activation=\"softmax\")\n-            self.dropout = keras_core.layers.Dropout(0.5)\n+            self.dense1 = keras.layers.Dense(32, activation=\"relu\")\n+            self.dense2 = keras.layers.Dense(5, activation=\"softmax\")\n+            self.dropout = keras.layers.Dropout(0.5)\n \n         def call(self, inputs, training=False):\n             x = self.dense1(inputs)\n@@ -127,13 +127,13 @@ class Model(Trainer, Layer):\n \n     ## With the `Sequential` class\n \n-    In addition, `keras_core.Sequential` is a special case of model where\n+    In addition, `keras.Sequential` is a special case of model where\n     the model is purely a stack of single-input, single-output layers.\n \n     ```python\n-    model = keras_core.Sequential([\n-        keras_core.Input(shape=(None, None, 3)),\n-        keras_core.layers.Conv2D(filters=32, kernel_size=3),\n+    model = keras.Sequential([\n+        keras.Input(shape=(None, None, 3)),\n+        keras.layers.Conv2D(filters=32, kernel_size=3),\n     ])\n     ```\n     \"\"\"\n@@ -141,14 +141,14 @@ class Model(Trainer, Layer):\n     def __new__(cls, *args, **kwargs):\n         # Signature detection for usage of `Model` as a `Functional`\n         if functional_init_arguments(args, kwargs) and cls == Model:\n-            from keras_core.models import functional\n+            from keras.models import functional\n \n             return functional.Functional(*args, **kwargs)\n         return super().__new__(cls)\n \n     def __init__(self, *args, **kwargs):\n         Trainer.__init__(self)\n-        from keras_core.models import functional\n+        from keras.models import functional\n \n         # Signature detection for usage of a `Model` subclass\n         # as a `Functional` subclass\n@@ -279,20 +279,20 @@ class Model(Trainer, Layer):\n         Example:\n \n         ```python\n-        model = keras_core.Sequential(\n+        model = keras.Sequential(\n             [\n-                keras_core.layers.Dense(5, input_shape=(3,)),\n-                keras_core.layers.Softmax(),\n+                keras.layers.Dense(5, input_shape=(3,)),\n+                keras.layers.Softmax(),\n             ],\n         )\n         model.save(\"model.keras\")\n-        loaded_model = keras_core.saving.load_model(\"model.keras\")\n+        loaded_model = keras.saving.load_model(\"model.keras\")\n         x = keras.random.uniform((10, 3))\n         assert np.allclose(model.predict(x), loaded_model.predict(x))\n         ```\n \n         Note that `model.save()` is an alias for\n-        `keras_core.saving.save_model()`.\n+        `keras.saving.save_model()`.\n \n         The saved `.keras` file contains:\n \n@@ -314,14 +314,14 @@ class Model(Trainer, Layer):\n                 filepath\n             ).endswith(\".keras\"):\n                 warnings.warn(\n-                    \"The `save_format` argument is deprecated in Keras Core. \"\n+                    \"The `save_format` argument is deprecated in Keras 3. \"\n                     \"We recommend removing this argument as it can be inferred \"\n                     \"from the file path. \"\n                     f\"Received: save_format={save_format}\"\n                 )\n             else:\n                 raise ValueError(\n-                    \"The `save_format` argument is deprecated in Keras Core. \"\n+                    \"The `save_format` argument is deprecated in Keras 3. \"\n                     \"Please remove this argument and pass a file path with \"\n                     \"either `.keras` or `.h5` extension.\"\n                     f\"Received: save_format={save_format}\"\n@@ -472,7 +472,7 @@ class Model(Trainer, Layer):\n         Returns:\n             A JSON string.\n         \"\"\"\n-        from keras_core.saving import serialization_lib\n+        from keras.saving import serialization_lib\n \n         model_config = serialization_lib.serialize_keras_object(self)\n         return json.dumps(model_config, **kwargs)\n@@ -508,16 +508,16 @@ class Model(Trainer, Layer):\n         ```\n \n         If you would like to customize your serving endpoints, you can\n-        use the lower-level `keras_core.export.ExportArchive` class. The\n+        use the lower-level `keras.export.ExportArchive` class. The\n         `export()` method relies on `ExportArchive` internally.\n         \"\"\"\n-        from keras_core.export import export_lib\n+        from keras.export import export_lib\n \n         export_lib.export_model(self, filepath)\n \n     @classmethod\n     def from_config(cls, config, custom_objects=None):\n-        from keras_core.models.functional import Functional\n+        from keras.models.functional import Functional\n \n         functional_config_keys = [\n             \"name\",\n@@ -540,7 +540,7 @@ class Model(Trainer, Layer):\n         if is_functional_config and revivable_as_functional:\n             # Revive Functional model\n             # (but not Functional subclasses with a custom __init__)\n-            from keras_core.models.functional import functional_from_config\n+            from keras.models.functional import functional_from_config\n \n             return functional_from_config(\n                 cls, config, custom_objects=custom_objects\n@@ -576,17 +576,17 @@ class Model(Trainer, Layer):\n         return store\n \n \n-@keras_core_export(\"keras_core.models.model_from_json\")\n+@keras_export(\"keras.models.model_from_json\")\n def model_from_json(json_string, custom_objects=None):\n     \"\"\"Parses a JSON model configuration string and returns a model instance.\n \n     Usage:\n \n-    >>> model = keras_core.Sequential([\n-    ...     keras_core.layers.Dense(5, input_shape=(3,)),\n-    ...     keras_core.layers.Softmax()])\n+    >>> model = keras.Sequential([\n+    ...     keras.layers.Dense(5, input_shape=(3,)),\n+    ...     keras.layers.Softmax()])\n     >>> config = model.to_json()\n-    >>> loaded_model = keras_core.models.model_from_json(config)\n+    >>> loaded_model = keras.models.model_from_json(config)\n \n     Args:\n         json_string: JSON string encoding a model configuration.\n@@ -597,7 +597,7 @@ def model_from_json(json_string, custom_objects=None):\n     Returns:\n         A Keras model instance (uncompiled).\n     \"\"\"\n-    from keras_core.saving import serialization_lib\n+    from keras.saving import serialization_lib\n \n     model_config = json.loads(json_string)\n     return serialization_lib.deserialize_keras_object(\n@@ -615,7 +615,7 @@ def functional_init_arguments(args, kwargs):\n \n def inject_functional_model_class(cls):\n     \"\"\"Inject `Functional` into the hierarchy of this class if needed.\"\"\"\n-    from keras_core.models import functional\n+    from keras.models import functional\n \n     if cls == Model:\n         return functional.Functional\n\n@@ -2,12 +2,12 @@ import numpy as np\n import pytest\n from absl.testing import parameterized\n \n-from keras_core import layers\n-from keras_core import testing\n-from keras_core.layers.core.input_layer import Input\n-from keras_core.models.functional import Functional\n-from keras_core.models.model import Model\n-from keras_core.models.model import model_from_json\n+from keras import layers\n+from keras import testing\n+from keras.layers.core.input_layer import Input\n+from keras.models.functional import Functional\n+from keras.models.model import Model\n+from keras.models.model import model_from_json\n \n \n def _get_model():\n@@ -78,7 +78,7 @@ class ModelTest(testing.TestCase, parameterized.TestCase):\n         self.assertEqual(json_string, new_model.to_json())\n \n     def test_tuple_input_model_subclass(self):\n-        # https://github.com/keras-team/keras-core/issues/324\n+        # https://github.com/keras-team/keras/issues/324\n \n         class MultiInputModel(Model):\n             def __init__(self, **kwargs):\n\n@@ -2,58 +2,58 @@ import copy\n \n import tree\n \n-from keras_core.api_export import keras_core_export\n-from keras_core.backend.common import global_state\n-from keras_core.layers.core.input_layer import InputLayer\n-from keras_core.layers.layer import Layer\n-from keras_core.legacy.saving import saving_utils\n-from keras_core.legacy.saving import serialization as legacy_serialization\n-from keras_core.models.functional import Functional\n-from keras_core.models.model import Model\n-from keras_core.saving import serialization_lib\n+from keras.api_export import keras_export\n+from keras.backend.common import global_state\n+from keras.layers.core.input_layer import InputLayer\n+from keras.layers.layer import Layer\n+from keras.legacy.saving import saving_utils\n+from keras.legacy.saving import serialization as legacy_serialization\n+from keras.models.functional import Functional\n+from keras.models.model import Model\n+from keras.saving import serialization_lib\n \n \n-@keras_core_export([\"keras_core.Sequential\", \"keras_core.models.Sequential\"])\n+@keras_export([\"keras.Sequential\", \"keras.models.Sequential\"])\n class Sequential(Model):\n     \"\"\"`Sequential` groups a linear stack of layers into a `Model`.\n \n     Examples:\n \n     ```python\n-    model = keras_core.Sequential()\n-    model.add(keras_core.Input(shape=(16,)))\n-    model.add(keras_core.layers.Dense(8))\n+    model = keras.Sequential()\n+    model.add(keras.Input(shape=(16,)))\n+    model.add(keras.layers.Dense(8))\n \n     # Note that you can also omit the initial `Input`.\n     # In that case the model doesn't have any weights until the first call\n     # to a training/evaluation method (since it isn't yet built):\n-    model = keras_core.Sequential()\n-    model.add(keras_core.layers.Dense(8))\n-    model.add(keras_core.layers.Dense(4))\n+    model = keras.Sequential()\n+    model.add(keras.layers.Dense(8))\n+    model.add(keras.layers.Dense(4))\n     # model.weights not created yet\n \n     # Whereas if you specify an `Input`, the model gets built\n     # continuously as you are adding layers:\n-    model = keras_core.Sequential()\n-    model.add(keras_core.Input(shape=(16,)))\n-    model.add(keras_core.layers.Dense(8))\n+    model = keras.Sequential()\n+    model.add(keras.Input(shape=(16,)))\n+    model.add(keras.layers.Dense(8))\n     len(model.weights)  # Returns \"2\"\n \n     # When using the delayed-build pattern (no input shape specified), you can\n     # choose to manually build your model by calling\n     # `build(batch_input_shape)`:\n-    model = keras_core.Sequential()\n-    model.add(keras_core.layers.Dense(8))\n-    model.add(keras_core.layers.Dense(4))\n+    model = keras.Sequential()\n+    model.add(keras.layers.Dense(8))\n+    model.add(keras.layers.Dense(4))\n     model.build((None, 16))\n     len(model.weights)  # Returns \"4\"\n \n     # Note that when using the delayed-build pattern (no input shape specified),\n     # the model gets built the first time you call `fit`, `eval`, or `predict`,\n     # or the first time you call the model on some input data.\n-    model = keras_core.Sequential()\n-    model.add(keras_core.layers.Dense(8))\n-    model.add(keras_core.layers.Dense(1))\n+    model = keras.Sequential()\n+    model.add(keras.layers.Dense(8))\n+    model.add(keras.layers.Dense(1))\n     model.compile(optimizer='sgd', loss='mse')\n     # This builds the model for the first time:\n     model.fit(x, y, batch_size=32, epochs=10)\n@@ -89,7 +89,7 @@ class Sequential(Model):\n                 layer = origin_layer\n         if not isinstance(layer, Layer):\n             raise ValueError(\n-                \"Only instances of `keras_core.Layer` can be \"\n+                \"Only instances of `keras.Layer` can be \"\n                 f\"added to a Sequential model. Received: {layer} \"\n                 f\"(of type {type(layer)})\"\n             )\n\n@@ -1,12 +1,12 @@\n import numpy as np\n import pytest\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import testing\n-from keras_core.layers.core.input_layer import Input\n-from keras_core.models.functional import Functional\n-from keras_core.models.sequential import Sequential\n+from keras import backend\n+from keras import layers\n+from keras import testing\n+from keras.layers.core.input_layer import Input\n+from keras.models.functional import Functional\n+from keras.models.sequential import Sequential\n \n \n @pytest.mark.requires_trainable_backend\n\n@@ -1,7 +1,7 @@\n-from keras_core.layers.layer import Layer\n-from keras_core.metrics.metric import Metric\n-from keras_core.optimizers.optimizer import Optimizer\n-from keras_core.saving import saving_lib\n+from keras.layers.layer import Layer\n+from keras.metrics.metric import Metric\n+from keras.optimizers.optimizer import Optimizer\n+from keras.saving import saving_lib\n \n \n def map_trackable_variables(trackable, store, visited_trackables):\n\n@@ -1,7 +1,7 @@\n import numpy as np\n \n-from keras_core import testing\n-from keras_core.saving import saving_lib_test\n+from keras import testing\n+from keras.saving import saving_lib_test\n \n \n class VariableMappingTest(testing.TestCase):\n\n@@ -0,0 +1,15 @@\n+# from keras.ops.numpy import Matmul, matmul\n+# from keras.ops.numpy import Add, add\n+# from keras.ops.numpy import Multiply, multiply\n+\n+from keras.backend import cast\n+from keras.backend import cond\n+from keras.backend import is_tensor\n+from keras.backend import name_scope\n+from keras.backend import random\n+from keras.ops import image\n+from keras.ops import operation_utils\n+from keras.ops.core import *  # noqa: F403\n+from keras.ops.math import *  # noqa: F403\n+from keras.ops.nn import *  # noqa: F403\n+from keras.ops.numpy import *  # noqa: F403\n\n@@ -14,12 +14,12 @@ cond\n \n import numpy as np\n \n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.backend import KerasTensor\n-from keras_core.backend import any_symbolic_tensors\n-from keras_core.ops.operation import Operation\n-from keras_core.utils import traceback_utils\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.backend import KerasTensor\n+from keras.backend import any_symbolic_tensors\n+from keras.ops.operation import Operation\n+from keras.utils import traceback_utils\n \n \n class Scatter(Operation):\n@@ -30,7 +30,7 @@ class Scatter(Operation):\n         return KerasTensor(shape, dtype=values.dtype)\n \n \n-@keras_core_export(\"keras_core.ops.scatter\")\n+@keras_export(\"keras.ops.scatter\")\n def scatter(indices, values, shape):\n     \"\"\"Returns a tensor of shape `shape` where `indices` are set to `values`.\n \n@@ -38,8 +38,8 @@ def scatter(indices, values, shape):\n     returns the output. It is equivalent to:\n \n     ```python\n-    zeros = keras_core.ops.zeros(shape)\n-    output = keras_core.ops.scatter_update(zeros, indices, values)\n+    zeros = keras.ops.zeros(shape)\n+    output = keras.ops.scatter_update(zeros, indices, values)\n     ```\n \n     Args:\n@@ -52,7 +52,7 @@ def scatter(indices, values, shape):\n \n     >>> indices = [[0, 1], [1, 1]]\n     >>> values = np.array([1., 1.])\n-    >>> keras_core.ops.scatter(indices, values, shape=(2, 2))\n+    >>> keras.ops.scatter(indices, values, shape=(2, 2))\n     array([[0., 1.],\n            [0., 1.]])\n     \"\"\"\n@@ -69,7 +69,7 @@ class ScatterUpdate(Operation):\n         return KerasTensor(inputs.shape, dtype=inputs.dtype)\n \n \n-@keras_core_export(\"keras_core.ops.scatter_update\")\n+@keras_export(\"keras.ops.scatter_update\")\n def scatter_update(inputs, indices, updates):\n     \"\"\"Update inputs via updates at scattered (sparse) indices.\n \n@@ -87,7 +87,7 @@ def scatter_update(inputs, indices, updates):\n     inputs = np.zeros((4, 4, 4))\n     indices = [[1, 2, 3], [0, 1, 3]]\n     updates = np.array([1., 1.])\n-    inputs = keras_core.ops.scatter_update(inputs, indices, updates)\n+    inputs = keras.ops.scatter_update(inputs, indices, updates)\n     ```\n \n     2 `indices` is a 2D tensor of shape `(num_updates, k)`, where `num_updates`\n@@ -103,7 +103,7 @@ def scatter_update(inputs, indices, updates):\n     inputs = np.zeros((4, 4, 4))\n     indices = [[1, 2], [2, 3]]\n     updates = np.array([[1., 1., 1, 1,], [1., 1., 1, 1,])\n-    inputs = keras_core.ops.scatter_update(inputs, indices, updates)\n+    inputs = keras.ops.scatter_update(inputs, indices, updates)\n     ```\n \n     Args:\n@@ -129,7 +129,7 @@ class Slice(Operation):\n         return KerasTensor(shape, dtype=inputs.dtype)\n \n \n-@keras_core_export(\"keras_core.ops.slice\")\n+@keras_export(\"keras.ops.slice\")\n def slice(inputs, start_indices, shape):\n     \"\"\"Return a slice of an input tensor.\n \n@@ -143,7 +143,7 @@ def slice(inputs, start_indices, shape):\n     inputs = np.zeros((5, 5))\n     start_indices = np.array([3, 3])\n     shape = np.array([2, 2])\n-    inputs = keras_core.ops.slice(inputs, start_indices, updates)\n+    inputs = keras.ops.slice(inputs, start_indices, updates)\n     ```\n \n     Args:\n@@ -168,7 +168,7 @@ class SliceUpdate(Operation):\n         return KerasTensor(inputs.shape, dtype=inputs.dtype)\n \n \n-@keras_core_export(\"keras_core.ops.slice_update\")\n+@keras_export(\"keras.ops.slice_update\")\n def slice_update(inputs, start_indices, updates):\n     \"\"\"Update an input by slicing in a tensor of updated values.\n \n@@ -186,7 +186,7 @@ def slice_update(inputs, start_indices, updates):\n     inputs = np.zeros((5, 5))\n     start_indices = [3, 3]\n     updates = np.ones((2, 2))\n-    inputs = keras_core.ops.slice_update(inputs, start_indices, updates)\n+    inputs = keras.ops.slice_update(inputs, start_indices, updates)\n     ```\n \n     Args:\n@@ -223,7 +223,7 @@ class WhileLoop(Operation):\n         return [KerasTensor(v.shape, dtype=v.dtype) for v in loop_vars]\n \n \n-@keras_core_export(\"keras_core.ops.while_loop\")\n+@keras_export(\"keras.ops.while_loop\")\n def while_loop(\n     cond,\n     body,\n@@ -252,7 +252,7 @@ def while_loop(\n     >>> i = 0\n     >>> cond = lambda i: i < 10\n     >>> body = lambda i: i + 1\n-    >>> keras_core.ops.while_loop(cond, body, [i])[0]\n+    >>> keras.ops.while_loop(cond, body, [i])[0]\n     10\n     \"\"\"\n     return backend.core.while_loop(\n@@ -274,7 +274,7 @@ class StopGradient(Operation):\n         return KerasTensor(variable.shape, dtype=variable.dtype)\n \n \n-@keras_core_export(\"keras_core.ops.stop_gradient\")\n+@keras_export(\"keras.ops.stop_gradient\")\n def stop_gradient(variable):\n     \"\"\"Stops gradient computation.\n \n@@ -287,11 +287,11 @@ def stop_gradient(variable):\n \n     Examples:\n \n-    >>> var = keras_core.backend.convert_to_tensor(\n+    >>> var = keras.backend.convert_to_tensor(\n     ...     [1., 2., 3.],\n     ...     dtype=\"float32\"\n     ... )\n-    >>> var = keras_core.ops.stop_gradient(var)\n+    >>> var = keras.ops.stop_gradient(var)\n     \"\"\"\n     return backend.core.stop_gradient(variable)\n \n@@ -315,7 +315,7 @@ class ForiLoop(Operation):\n         return KerasTensor(init_val.shape, dtype=init_val.dtype)\n \n \n-@keras_core_export(\"keras_core.ops.fori_loop\")\n+@keras_export(\"keras.ops.fori_loop\")\n def fori_loop(lower, upper, body_fun, init_val):\n     \"\"\"For loop implementation.\n \n@@ -336,7 +336,7 @@ def fori_loop(lower, upper, body_fun, init_val):\n     >>> upper = 10\n     >>> body_fun = lambda i, s: (i + 1, s + i)\n     >>> init_val = 0\n-    >>> keras_core.ops.fori_loop(lower, upper, body_fun, init_val)\n+    >>> keras.ops.fori_loop(lower, upper, body_fun, init_val)\n     45\n     \"\"\"\n     if any_symbolic_tensors((lower, upper, init_val)):\n@@ -374,7 +374,7 @@ class Unstack(Operation):\n         return output\n \n \n-@keras_core_export(\"keras_core.ops.unstack\")\n+@keras_export(\"keras.ops.unstack\")\n def unstack(x, num=None, axis=0):\n     \"\"\"Unpacks the given dimension of a rank-R tensor into rank-(R-1) tensors.\n \n@@ -389,8 +389,8 @@ def unstack(x, num=None, axis=0):\n \n     Example:\n \n-    >>> x = keras_core.ops.array([[1, 2], [3, 4]])\n-    >>> keras_core.ops.unstack(x, axis=0)\n+    >>> x = keras.ops.array([[1, 2], [3, 4]])\n+    >>> keras.ops.unstack(x, axis=0)\n     [array([1, 2]), array([3, 4])]\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n@@ -398,7 +398,7 @@ def unstack(x, num=None, axis=0):\n     return backend.core.unstack(x, num=num, axis=axis)\n \n \n-@keras_core_export(\"keras_core.ops.shape\")\n+@keras_export(\"keras.ops.shape\")\n def shape(x):\n     \"\"\"Gets the shape of the tensor input.\n \n@@ -416,8 +416,8 @@ def shape(x):\n \n     Example:\n \n-    >>> x = keras_core.zeros((8, 12))\n-    >>> keras_core.ops.shape(x)\n+    >>> x = keras.zeros((8, 12))\n+    >>> keras.ops.shape(x)\n     (8, 12)\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n@@ -437,7 +437,7 @@ class Cast(Operation):\n         return backend.KerasTensor(shape=x.shape, dtype=self.dtype)\n \n \n-@keras_core_export(\"keras_core.ops.cast\")\n+@keras_export(\"keras.ops.cast\")\n def cast(x, dtype):\n     \"\"\"Cast a tensor to the desired dtype.\n \n@@ -450,8 +450,8 @@ def cast(x, dtype):\n \n     Example:\n \n-    >>> x = keras_core.ops.arange(4)\n-    >>> x = keras_core.ops.cast(x, dtype=\"float16\")\n+    >>> x = keras.ops.arange(4)\n+    >>> x = keras.ops.cast(x, dtype=\"float16\")\n     \"\"\"\n     dtype = backend.standardize_dtype(dtype)\n \n@@ -460,7 +460,7 @@ def cast(x, dtype):\n     return backend.core.cast(x, dtype)\n \n \n-@keras_core_export(\"keras_core.ops.convert_to_tensor\")\n+@keras_export(\"keras.ops.convert_to_tensor\")\n def convert_to_tensor(x, dtype=None):\n     \"\"\"Convert a NumPy array to a tensor.\n \n@@ -474,12 +474,12 @@ def convert_to_tensor(x, dtype=None):\n     Example:\n \n     >>> x = np.array([1, 2, 3])\n-    >>> y = keras_core.ops.convert_to_tensor(x)\n+    >>> y = keras.ops.convert_to_tensor(x)\n     \"\"\"\n     return backend.convert_to_tensor(x, dtype=dtype)\n \n \n-@keras_core_export(\"keras_core.ops.convert_to_numpy\")\n+@keras_export(\"keras.ops.convert_to_numpy\")\n def convert_to_numpy(x):\n     \"\"\"Convert a tensor to a NumPy array.\n \n@@ -579,7 +579,7 @@ class Cond(Operation):\n         return True\n \n \n-@keras_core_export(\"keras_core.ops.cond\")\n+@keras_export(\"keras.ops.cond\")\n def cond(pred, true_fn, false_fn):\n     \"\"\"Conditionally applies `true_fn` or `false_fn`.\n \n\n@@ -1,15 +1,15 @@\n import numpy as np\n import pytest\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import losses\n-from keras_core import models\n-from keras_core import ops\n-from keras_core import optimizers\n-from keras_core import testing\n-from keras_core.backend.common.keras_tensor import KerasTensor\n-from keras_core.ops import core\n+from keras import backend\n+from keras import layers\n+from keras import losses\n+from keras import models\n+from keras import ops\n+from keras import optimizers\n+from keras import testing\n+from keras.backend.common.keras_tensor import KerasTensor\n+from keras.ops import core\n \n \n class CoreOpsStaticShapeTest(testing.TestCase):\n\n@@ -2,14 +2,14 @@ import collections\n \n import tree\n \n-from keras_core.api_export import keras_core_export\n-from keras_core.backend import KerasTensor\n-from keras_core.backend.config import backend\n-from keras_core.ops.operation import Operation\n-from keras_core.utils.nest import pack_sequence_as\n+from keras.api_export import keras_export\n+from keras.backend import KerasTensor\n+from keras.backend.config import backend\n+from keras.ops.operation import Operation\n+from keras.utils.nest import pack_sequence_as\n \n \n-@keras_core_export(\"keras_core.Function\")\n+@keras_export(\"keras.Function\")\n class Function(Operation):\n     \"\"\"Class that encapsulates a computation graph of Keras operations.\n \n@@ -24,11 +24,11 @@ class Function(Operation):\n     Example:\n \n     ```python\n-    input_1 = keras_core.KerasTensor(shape=(None, 2, 3))\n-    input_2 = keras_core.KerasTensor(shape=(None, 2, 3))\n+    input_1 = keras.KerasTensor(shape=(None, 2, 3))\n+    input_2 = keras.KerasTensor(shape=(None, 2, 3))\n     x = input_1 + input_2\n-    output = keras_core.ops.sigmoid(x)\n-    fn = keras_core.Function(inputs=[input_1, input_2], outputs=output)\n+    output = keras.ops.sigmoid(x)\n+    fn = keras.Function(inputs=[input_1, input_2], outputs=output)\n \n     input_1_val = np.random.random((4, 2, 3))\n     input_2_val = np.random.random((4, 2, 3))\n@@ -49,7 +49,7 @@ class Function(Operation):\n \n         if backend() == \"tensorflow\":\n             # Temporary work around for\n-            # https://github.com/keras-team/keras-core/issues/931\n+            # https://github.com/keras-team/keras/issues/931\n             # This stop tensorflow from wrapping tf.function output in a\n             # _DictWrapper object.\n             _self_setattr_tracking = getattr(\n\n@@ -1,9 +1,9 @@\n import numpy as np\n \n-from keras_core import testing\n-from keras_core.backend.common import keras_tensor\n-from keras_core.ops import function\n-from keras_core.ops import numpy as knp\n+from keras import testing\n+from keras.backend.common import keras_tensor\n+from keras.ops import function\n+from keras.ops import numpy as knp\n \n \n class FunctionTest(testing.TestCase):\n\n@@ -1,9 +1,9 @@\n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.backend import KerasTensor\n-from keras_core.backend import any_symbolic_tensors\n-from keras_core.ops.operation import Operation\n-from keras_core.ops.operation_utils import compute_conv_output_shape\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.backend import KerasTensor\n+from keras.backend import any_symbolic_tensors\n+from keras.ops.operation import Operation\n+from keras.ops.operation_utils import compute_conv_output_shape\n \n \n class Resize(Operation):\n@@ -52,7 +52,7 @@ class Resize(Operation):\n         )\n \n \n-@keras_core_export(\"keras_core.ops.image.resize\")\n+@keras_export(\"keras.ops.image.resize\")\n def resize(\n     image,\n     size,\n@@ -84,17 +84,17 @@ def resize(\n     Examples:\n \n     >>> x = np.random.random((2, 4, 4, 3)) # batch of 2 RGB images\n-    >>> y = keras_core.ops.image.resize(x, (2, 2))\n+    >>> y = keras.ops.image.resize(x, (2, 2))\n     >>> y.shape\n     (2, 2, 2, 3)\n \n     >>> x = np.random.random((4, 4, 3)) # single RGB image\n-    >>> y = keras_core.ops.image.resize(x, (2, 2))\n+    >>> y = keras.ops.image.resize(x, (2, 2))\n     >>> y.shape\n     (2, 2, 3)\n \n     >>> x = np.random.random((2, 3, 4, 4)) # batch of 2 RGB images\n-    >>> y = keras_core.ops.image.resize(x, (2, 2),\n+    >>> y = keras.ops.image.resize(x, (2, 2),\n     ...     data_format=\"channels_first\")\n     >>> y.shape\n     (2, 3, 2, 2)\n@@ -156,7 +156,7 @@ class AffineTransform(Operation):\n         return KerasTensor(image.shape, dtype=image.dtype)\n \n \n-@keras_core_export(\"keras_core.ops.image.affine_transform\")\n+@keras_export(\"keras.ops.image.affine_transform\")\n def affine_transform(\n     image,\n     transform,\n@@ -218,13 +218,13 @@ def affine_transform(\n     ...         [1, 0, -20, 0, 1, -16, 0, 0],  # translation\n     ...     ]\n     ... )\n-    >>> y = keras_core.ops.image.affine_transform(x, transform)\n+    >>> y = keras.ops.image.affine_transform(x, transform)\n     >>> y.shape\n     (2, 64, 80, 3)\n \n     >>> x = np.random.random((64, 80, 3)) # single RGB image\n     >>> transform = np.array([1.0, 0.5, -20, 0.5, 1.0, -16, 0, 0])  # shear\n-    >>> y = keras_core.ops.image.affine_transform(x, transform)\n+    >>> y = keras.ops.image.affine_transform(x, transform)\n     >>> y.shape\n     (64, 80, 3)\n \n@@ -235,7 +235,7 @@ def affine_transform(\n     ...         [1, 0, -20, 0, 1, -16, 0, 0],  # translation\n     ...     ]\n     ... )\n-    >>> y = keras_core.ops.image.affine_transform(x, transform,\n+    >>> y = keras.ops.image.affine_transform(x, transform,\n     ...     data_format=\"channels_first\")\n     >>> y.shape\n     (2, 3, 64, 80)\n@@ -311,7 +311,7 @@ class ExtractPatches(Operation):\n         return KerasTensor(shape=out_shape, dtype=image.dtype)\n \n \n-@keras_core_export(\"keras_core.ops.image.extract_patches\")\n+@keras_export(\"keras.ops.image.extract_patches\")\n def extract_patches(\n     image,\n     size,\n@@ -349,11 +349,11 @@ def extract_patches(\n     >>> image = np.random.random(\n     ...     (2, 20, 20, 3)\n     ... ).astype(\"float32\") # batch of 2 RGB images\n-    >>> patches = keras_core.ops.image.extract_patches(image, (5, 5))\n+    >>> patches = keras.ops.image.extract_patches(image, (5, 5))\n     >>> patches.shape\n     (2, 4, 4, 75)\n     >>> image = np.random.random((20, 20, 3)).astype(\"float32\") # 1 RGB image\n-    >>> patches = keras_core.ops.image.extract_patches(image, (3, 3), (1, 1))\n+    >>> patches = keras.ops.image.extract_patches(image, (3, 3), (1, 1))\n     >>> patches.shape\n     (18, 18, 27)\n     \"\"\"\n@@ -448,7 +448,7 @@ class MapCoordinates(Operation):\n         return KerasTensor(coordinates.shape[1:], dtype=image.dtype)\n \n \n-@keras_core_export(\"keras_core.ops.image.map_coordinates\")\n+@keras_export(\"keras.ops.image.map_coordinates\")\n def map_coordinates(\n     input, coordinates, order, fill_mode=\"constant\", fill_value=0\n ):\n\n@@ -6,10 +6,10 @@ import scipy.ndimage\n import tensorflow as tf\n from absl.testing import parameterized\n \n-from keras_core import backend\n-from keras_core import testing\n-from keras_core.backend.common.keras_tensor import KerasTensor\n-from keras_core.ops import image as kimage\n+from keras import backend\n+from keras import testing\n+from keras.backend.common.keras_tensor import KerasTensor\n+from keras.ops import image as kimage\n \n \n class ImageOpsDynamicShapeTest(testing.TestCase):\n\n@@ -1,11 +1,11 @@\n \"\"\"Commonly used math operations not included in NumPy.\"\"\"\n \n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.backend import KerasTensor\n-from keras_core.backend import any_symbolic_tensors\n-from keras_core.ops.operation import Operation\n-from keras_core.ops.operation_utils import reduce_shape\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.backend import KerasTensor\n+from keras.backend import any_symbolic_tensors\n+from keras.ops.operation import Operation\n+from keras.ops.operation_utils import reduce_shape\n \n \n class SegmentSum(Operation):\n@@ -28,7 +28,7 @@ class SegmentSum(Operation):\n         )\n \n \n-@keras_core_export(\"keras_core.ops.segment_sum\")\n+@keras_export(\"keras.ops.segment_sum\")\n def segment_sum(data, segment_ids, num_segments=None, sorted=False):\n     \"\"\"Computes the sum of segments in a tensor.\n \n@@ -48,10 +48,10 @@ def segment_sum(data, segment_ids, num_segments=None, sorted=False):\n \n     Example:\n \n-    >>> data = keras_core.ops.convert_to_tensor([1, 2, 10, 20, 100, 200])\n-    >>> segment_ids = keras_core.ops.convert_to_tensor([0, 0, 1, 1, 2, 2])\n+    >>> data = keras.ops.convert_to_tensor([1, 2, 10, 20, 100, 200])\n+    >>> segment_ids = keras.ops.convert_to_tensor([0, 0, 1, 1, 2, 2])\n     >>> num_segments = 3\n-    >>> keras_core.ops.segment_sum(data, segment_ids,num_segments)\n+    >>> keras.ops.segment_sum(data, segment_ids,num_segments)\n     array([3, 30, 300], dtype=int32)\n     \"\"\"\n     if any_symbolic_tensors((data,)):\n@@ -81,7 +81,7 @@ class SegmentMax(Operation):\n         )\n \n \n-@keras_core_export(\"keras_core.ops.segment_max\")\n+@keras_export(\"keras.ops.segment_max\")\n def segment_max(data, segment_ids, num_segments=None, sorted=False):\n     \"\"\"Computes the max of segments in a tensor.\n \n@@ -101,10 +101,10 @@ def segment_max(data, segment_ids, num_segments=None, sorted=False):\n \n     Example:\n \n-    >>> data = keras_core.ops.convert_to_tensor([1, 2, 10, 20, 100, 200])\n-    >>> segment_ids = keras_core.ops.convert_to_tensor([0, 0, 1, 1, 2, 2])\n+    >>> data = keras.ops.convert_to_tensor([1, 2, 10, 20, 100, 200])\n+    >>> segment_ids = keras.ops.convert_to_tensor([0, 0, 1, 1, 2, 2])\n     >>> num_segments = 3\n-    >>> keras_core.ops.segment_max(data, segment_ids, num_segments)\n+    >>> keras.ops.segment_max(data, segment_ids, num_segments)\n     array([2, 20, 200], dtype=int32)\n     \"\"\"\n     if any_symbolic_tensors((data,)):\n@@ -133,7 +133,7 @@ class TopK(Operation):\n         return backend.math.top_k(x, self.k, self.sorted)\n \n \n-@keras_core_export(\"keras_core.ops.top_k\")\n+@keras_export(\"keras.ops.top_k\")\n def top_k(x, k, sorted=True):\n     \"\"\"Finds the top-k values and their indices in a tensor.\n \n@@ -150,7 +150,7 @@ def top_k(x, k, sorted=True):\n \n     Example:\n \n-    >>> x = keras_core.ops.convert_to_tensor([5, 2, 7, 1, 9, 3])\n+    >>> x = keras.ops.convert_to_tensor([5, 2, 7, 1, 9, 3])\n     >>> values, indices = top_k(x, k=3)\n     >>> print(values)\n     array([9 7 5], shape=(3,), dtype=int32)\n@@ -175,7 +175,7 @@ class InTopK(Operation):\n         return backend.math.in_top_k(targets, predictions, self.k)\n \n \n-@keras_core_export(\"keras_core.ops.in_top_k\")\n+@keras_export(\"keras.ops.in_top_k\")\n def in_top_k(targets, predictions, k):\n     \"\"\"Checks if the targets are in the top-k predictions.\n \n@@ -190,8 +190,8 @@ def in_top_k(targets, predictions, k):\n \n     Example:\n \n-    >>> targets = keras_core.ops.convert_to_tensor([2, 5, 3])\n-    >>> predictions = keras_core.ops.convert_to_tensor(\n+    >>> targets = keras.ops.convert_to_tensor([2, 5, 3])\n+    >>> predictions = keras.ops.convert_to_tensor(\n     ... [[0.1, 0.4, 0.6, 0.9, 0.5],\n     ...  [0.1, 0.7, 0.9, 0.8, 0.3],\n     ...  [0.1, 0.6, 0.9, 0.9, 0.5]])\n@@ -217,7 +217,7 @@ class Logsumexp(Operation):\n         return backend.math.logsumexp(x, axis=self.axis, keepdims=self.keepdims)\n \n \n-@keras_core_export(\"keras_core.ops.logsumexp\")\n+@keras_export(\"keras.ops.logsumexp\")\n def logsumexp(x, axis=None, keepdims=False):\n     \"\"\"Computes the logarithm of sum of exponentials of elements in a tensor.\n \n@@ -235,7 +235,7 @@ def logsumexp(x, axis=None, keepdims=False):\n \n     Example:\n \n-    >>> x = keras_core.ops.convert_to_tensor([1., 2., 3.])\n+    >>> x = keras.ops.convert_to_tensor([1., 2., 3.])\n     >>> logsumexp(x)\n     3.407606\n     \"\"\"\n@@ -286,7 +286,7 @@ class Qr(Operation):\n         return backend.math.qr(x, mode=self.mode)\n \n \n-@keras_core_export(\"keras_core.ops.qr\")\n+@keras_export(\"keras.ops.qr\")\n def qr(x, mode=\"reduced\"):\n     \"\"\"Computes the QR decomposition of a tensor.\n \n@@ -303,7 +303,7 @@ def qr(x, mode=\"reduced\"):\n \n     Example:\n \n-    >>> x = keras_core.ops.convert_to_tensor([[1., 2.], [3., 4.], [5., 6.]])\n+    >>> x = keras.ops.convert_to_tensor([[1., 2.], [3., 4.], [5., 6.]])\n     >>> q, r = qr(x)\n     >>> print(q)\n     array([[-0.16903079  0.897085]\n@@ -345,7 +345,7 @@ class ExtractSequences(Operation):\n         )\n \n \n-@keras_core_export(\"keras_core.ops.extract_sequences\")\n+@keras_export(\"keras.ops.extract_sequences\")\n def extract_sequences(x, sequence_length, sequence_stride):\n     \"\"\"Expands the dimension of last axis into sequences of `sequence_length`.\n \n@@ -368,7 +368,7 @@ def extract_sequences(x, sequence_length, sequence_stride):\n \n     Example:\n \n-    >>> x = keras_core.ops.convert_to_tensor([1, 2, 3, 4, 5, 6])\n+    >>> x = keras.ops.convert_to_tensor([1, 2, 3, 4, 5, 6])\n     >>> extract_sequences(x, 3, 2)\n     array([[1, 2, 3],\n        [3, 4, 5]])\n@@ -422,7 +422,7 @@ class FFT(Operation):\n         return backend.math.fft(x)\n \n \n-@keras_core_export(\"keras_core.ops.fft\")\n+@keras_export(\"keras.ops.fft\")\n def fft(x):\n     \"\"\"Computes the Fast Fourier Transform along last axis of input.\n \n@@ -437,8 +437,8 @@ def fft(x):\n     Example:\n \n     >>> x = (\n-    ...     keras_core.ops.convert_to_tensor([1., 2.]),\n-    ...     keras_core.ops.convert_to_tensor([0., 1.]),\n+    ...     keras.ops.convert_to_tensor([1., 2.]),\n+    ...     keras.ops.convert_to_tensor([0., 1.]),\n     ... )\n     >>> fft(x)\n     (array([ 3., -1.], dtype=float32), array([ 1., -1.], dtype=float32))\n@@ -490,7 +490,7 @@ class FFT2(Operation):\n         return backend.math.fft2(x)\n \n \n-@keras_core_export(\"keras_core.ops.fft2\")\n+@keras_export(\"keras.ops.fft2\")\n def fft2(x):\n     \"\"\"Computes the 2D Fast Fourier Transform along the last two axes of input.\n \n@@ -505,8 +505,8 @@ def fft2(x):\n     Example:\n \n     >>> x = (\n-    ...     keras_core.ops.convert_to_tensor([[1., 2.], [2., 1.]]),\n-    ...     keras_core.ops.convert_to_tensor([[0., 1.], [1., 0.]]),\n+    ...     keras.ops.convert_to_tensor([[1., 2.], [2., 1.]]),\n+    ...     keras.ops.convert_to_tensor([[0., 1.], [1., 0.]]),\n     ... )\n     >>> fft2(x)\n     (array([[ 6.,  0.],\n@@ -549,7 +549,7 @@ class RFFT(Operation):\n         return backend.math.rfft(x, fft_length=self.fft_length)\n \n \n-@keras_core_export(\"keras_core.ops.rfft\")\n+@keras_export(\"keras.ops.rfft\")\n def rfft(x, fft_length=None):\n     \"\"\"Real-valued Fast Fourier Transform along the last axis of the input.\n \n@@ -577,7 +577,7 @@ def rfft(x, fft_length=None):\n \n     Examples:\n \n-    >>> x = keras_core.ops.convert_to_tensor([0.0, 1.0, 2.0, 3.0, 4.0])\n+    >>> x = keras.ops.convert_to_tensor([0.0, 1.0, 2.0, 3.0, 4.0])\n     >>> rfft(x)\n     (array([10.0, -2.5, -2.5]), array([0.0, 3.4409548, 0.81229924]))\n \n@@ -630,7 +630,7 @@ class IRFFT(Operation):\n         return backend.math.irfft(x, fft_length=self.fft_length)\n \n \n-@keras_core_export(\"keras_core.ops.irfft\")\n+@keras_export(\"keras.ops.irfft\")\n def irfft(x, fft_length=None):\n     \"\"\"Inverse real-valued Fast Fourier transform along the last axis.\n \n@@ -661,8 +661,8 @@ def irfft(x, fft_length=None):\n \n     Examples:\n \n-    >>> real = keras_core.ops.convert_to_tensor([0.0, 1.0, 2.0, 3.0, 4.0])\n-    >>> imag = keras_core.ops.convert_to_tensor([0.0, 1.0, 2.0, 3.0, 4.0])\n+    >>> real = keras.ops.convert_to_tensor([0.0, 1.0, 2.0, 3.0, 4.0])\n+    >>> imag = keras.ops.convert_to_tensor([0.0, 1.0, 2.0, 3.0, 4.0])\n     >>> irfft((real, imag))\n     array([0.66666667, -0.9106836, 0.24401694])\n \n@@ -717,7 +717,7 @@ class STFT(Operation):\n         )\n \n \n-@keras_core_export(\"keras_core.ops.stft\")\n+@keras_export(\"keras.ops.stft\")\n def stft(\n     x, sequence_length, sequence_stride, fft_length, window=\"hann\", center=True\n ):\n@@ -748,7 +748,7 @@ def stft(\n \n     Example:\n \n-    >>> x = keras_core.ops.convert_to_tensor([0.0, 1.0, 2.0, 3.0, 4.0])\n+    >>> x = keras.ops.convert_to_tensor([0.0, 1.0, 2.0, 3.0, 4.0])\n     >>> stft(x, 3, 2, 3)\n     (array([[0.75, -0.375],\n        [3.75, -1.875],\n@@ -837,7 +837,7 @@ class ISTFT(Operation):\n         )\n \n \n-@keras_core_export(\"keras_core.ops.istft\")\n+@keras_export(\"keras.ops.istft\")\n def istft(\n     x,\n     sequence_length,\n@@ -876,7 +876,7 @@ def istft(\n \n     Example:\n \n-    >>> x = keras_core.ops.convert_to_tensor([0.0, 1.0, 2.0, 3.0, 4.0])\n+    >>> x = keras.ops.convert_to_tensor([0.0, 1.0, 2.0, 3.0, 4.0])\n     >>> istft(stft(x, 1, 1, 1), 1, 1, 1)\n     array([0.0, 1.0, 2.0, 3.0, 4.0])\n     \"\"\"\n@@ -910,8 +910,8 @@ class Rsqrt(Operation):\n \n     Example:\n \n-    >>> data = keras_core.ops.convert_to_tensor([1.0, 10.0, 100.0])\n-    >>> keras_core.ops.rsqrt(data)\n+    >>> data = keras.ops.convert_to_tensor([1.0, 10.0, 100.0])\n+    >>> keras.ops.rsqrt(data)\n     array([1.0, 0.31622776, 0.1], dtype=float32)\n     \"\"\"\n \n@@ -923,7 +923,7 @@ class Rsqrt(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export(\"keras_core.ops.rsqrt\")\n+@keras_export(\"keras.ops.rsqrt\")\n def rsqrt(x):\n     if any_symbolic_tensors((x,)):\n         return Rsqrt().symbolic_call(x)\n\n@@ -5,10 +5,10 @@ import pytest\n import scipy.signal\n from absl.testing import parameterized\n \n-from keras_core import backend\n-from keras_core import testing\n-from keras_core.backend.common.keras_tensor import KerasTensor\n-from keras_core.ops import math as kmath\n+from keras import backend\n+from keras import testing\n+from keras.backend.common.keras_tensor import KerasTensor\n+from keras.ops import math as kmath\n \n \n def _stft(\n\n@@ -1,16 +1,16 @@\n \"\"\"Commonly-used neural network operations not included in NumPy.\"\"\"\n \n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.backend import KerasTensor\n-from keras_core.backend import any_symbolic_tensors\n-from keras_core.backend import standardize_data_format\n-from keras_core.backend.common.backend_utils import (\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.backend import KerasTensor\n+from keras.backend import any_symbolic_tensors\n+from keras.backend import standardize_data_format\n+from keras.backend.common.backend_utils import (\n     compute_conv_transpose_output_shape,\n )\n-from keras_core.ops import operation_utils\n-from keras_core.ops.operation import Operation\n-from keras_core.ops.operation_utils import reduce_shape\n+from keras.ops import operation_utils\n+from keras.ops.operation import Operation\n+from keras.ops.operation_utils import reduce_shape\n \n \n class Relu(Operation):\n@@ -21,7 +21,7 @@ class Relu(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.relu\", \"keras_core.ops.nn.relu\"])\n+@keras_export([\"keras.ops.relu\", \"keras.ops.nn.relu\"])\n def relu(x):\n     \"\"\"Rectified linear unit activation function.\n \n@@ -35,8 +35,8 @@ def relu(x):\n \n     Example:\n \n-    >>> x1 = keras_core.ops.convert_to_tensor([-1.0, 0.0, 1.0, 0.2])\n-    >>> keras_core.ops.relu(x1)\n+    >>> x1 = keras.ops.convert_to_tensor([-1.0, 0.0, 1.0, 0.2])\n+    >>> keras.ops.relu(x1)\n     array([0.0, 0.0, 1.0, 0.2], dtype=float32)\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n@@ -52,7 +52,7 @@ class Relu6(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.relu6\", \"keras_core.ops.nn.relu6\"])\n+@keras_export([\"keras.ops.relu6\", \"keras.ops.nn.relu6\"])\n def relu6(x):\n     \"\"\"Rectified linear unit activation function with upper bound of 6.\n \n@@ -66,8 +66,8 @@ def relu6(x):\n \n     Example:\n \n-    >>> x = keras_core.ops.convert_to_tensor([-3.0, -2.0, 0.1, 0.2, 6.0, 8.0])\n-    >>> keras_core.ops.relu6(x)\n+    >>> x = keras.ops.convert_to_tensor([-3.0, -2.0, 0.1, 0.2, 6.0, 8.0])\n+    >>> keras.ops.relu6(x)\n     array([0.0, 0.0, 0.1, 0.2, 6.0, 6.0], dtype=float32)\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n@@ -83,7 +83,7 @@ class Sigmoid(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.sigmoid\", \"keras_core.ops.nn.sigmoid\"])\n+@keras_export([\"keras.ops.sigmoid\", \"keras.ops.nn.sigmoid\"])\n def sigmoid(x):\n     \"\"\"Sigmoid activation function.\n \n@@ -97,8 +97,8 @@ def sigmoid(x):\n \n     Example:\n \n-    >>> x = keras_core.ops.convert_to_tensor([-6.0, 1.0, 0.0, 1.0, 6.0])\n-    >>> keras_core.ops.sigmoid(x)\n+    >>> x = keras.ops.convert_to_tensor([-6.0, 1.0, 0.0, 1.0, 6.0])\n+    >>> keras.ops.sigmoid(x)\n     array([0.00247262, 0.7310586, 0.5, 0.7310586, 0.9975274], dtype=float32)\n \n     \"\"\"\n@@ -115,7 +115,7 @@ class Softplus(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.softplus\", \"keras_core.ops.nn.softplus\"])\n+@keras_export([\"keras.ops.softplus\", \"keras.ops.nn.softplus\"])\n def softplus(x):\n     \"\"\"Softplus activation function.\n \n@@ -130,8 +130,8 @@ def softplus(x):\n \n     Example:\n \n-    >>> x = keras_core.ops.convert_to_tensor([-0.555, 0.0, 0.555])\n-    >>> keras_core.ops.softplus(x)\n+    >>> x = keras.ops.convert_to_tensor([-0.555, 0.0, 0.555])\n+    >>> keras.ops.softplus(x)\n     array([0.45366603, 0.6931472, 1.008666], dtype=float32)\n \n     \"\"\"\n@@ -148,7 +148,7 @@ class Softsign(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.softsign\", \"keras_core.ops.nn.softsign\"])\n+@keras_export([\"keras.ops.softsign\", \"keras.ops.nn.softsign\"])\n def softsign(x):\n     \"\"\"Softsign activation function.\n \n@@ -162,8 +162,8 @@ def softsign(x):\n \n     Example:\n \n-    >>> x = keras_core.ops.convert_to_tensor([-0.100, -10.0, 1.0, 0.0, 100.0])\n-    >>> keras_core.ops.softsign(x)\n+    >>> x = keras.ops.convert_to_tensor([-0.100, -10.0, 1.0, 0.0, 100.0])\n+    >>> keras.ops.softsign(x)\n     Array([-0.09090909, -0.90909094, 0.5, 0.0, 0.990099], dtype=float32)\n \n     \"\"\"\n@@ -180,12 +180,12 @@ class Silu(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.ops.silu\",\n-        \"keras_core.ops.nn.silu\",\n-        \"keras_core.ops.swish\",\n-        \"keras_core.ops.nn.swish\",\n+        \"keras.ops.silu\",\n+        \"keras.ops.nn.silu\",\n+        \"keras.ops.swish\",\n+        \"keras.ops.nn.swish\",\n     ]\n )\n def silu(x):\n@@ -202,10 +202,10 @@ def silu(x):\n \n     Example:\n \n-    >>> x = keras_core.ops.convert_to_tensor([-6.0, 1.0, 0.0, 1.0, 6.0])\n-    >>> keras_core.ops.sigmoid(x)\n+    >>> x = keras.ops.convert_to_tensor([-6.0, 1.0, 0.0, 1.0, 6.0])\n+    >>> keras.ops.sigmoid(x)\n     array([0.00247262, 0.7310586, 0.5, 0.7310586, 0.9975274], dtype=float32)\n-    >>> keras_core.ops.silu(x)\n+    >>> keras.ops.silu(x)\n     array([-0.0148357, 0.7310586, 0.0, 0.7310586, 5.9851646], dtype=float32)\n \n     \"\"\"\n@@ -222,10 +222,10 @@ class LogSigmoid(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.ops.log_sigmoid\",\n-        \"keras_core.ops.nn.log_sigmoid\",\n+        \"keras.ops.log_sigmoid\",\n+        \"keras.ops.nn.log_sigmoid\",\n     ]\n )\n def log_sigmoid(x):\n@@ -241,8 +241,8 @@ def log_sigmoid(x):\n \n     Example:\n \n-    >>> x = keras_core.ops.convert_to_tensor([-0.541391, 0.0, 0.50, 5.0])\n-    >>> keras_core.ops.log_sigmoid(x)\n+    >>> x = keras.ops.convert_to_tensor([-0.541391, 0.0, 0.50, 5.0])\n+    >>> keras.ops.log_sigmoid(x)\n     array([-1.0000418, -0.6931472, -0.474077, -0.00671535], dtype=float32)\n \n     \"\"\"\n@@ -263,8 +263,8 @@ class LeakyRelu(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export(\n-    [\"keras_core.ops.leaky_relu\", \"keras_core.ops.nn.leaky_relu\"]\n+@keras_export(\n+    [\"keras.ops.leaky_relu\", \"keras.ops.nn.leaky_relu\"]\n )\n def leaky_relu(x, negative_slope=0.2):\n     \"\"\"Leaky version of a Rectified Linear Unit activation function.\n@@ -284,7 +284,7 @@ def leaky_relu(x, negative_slope=0.2):\n     Example:\n \n     >>> x = np.array([-1., 0., 1.])\n-    >>> x_leaky_relu = keras_core.ops.leaky_relu(x)\n+    >>> x_leaky_relu = keras.ops.leaky_relu(x)\n     >>> print(x_leaky_relu)\n     array([-0.2,  0. ,  1. ], shape=(3,), dtype=float64)\n \n@@ -302,10 +302,10 @@ class HardSigmoid(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.ops.hard_sigmoid\",\n-        \"keras_core.ops.nn.hard_sigmoid\",\n+        \"keras.ops.hard_sigmoid\",\n+        \"keras.ops.nn.hard_sigmoid\",\n     ]\n )\n def hard_sigmoid(x):\n@@ -324,7 +324,7 @@ def hard_sigmoid(x):\n     Example:\n \n     >>> x = np.array([-1., 0., 1.])\n-    >>> x_hard_sigmoid = keras_core.ops.hard_sigmoid(x)\n+    >>> x_hard_sigmoid = keras.ops.hard_sigmoid(x)\n     >>> print(x_hard_sigmoid)\n     array([0.3, 0.5, 0.7], shape=(3,), dtype=float64)\n \n@@ -346,7 +346,7 @@ class Elu(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.elu\", \"keras_core.ops.nn.elu\"])\n+@keras_export([\"keras.ops.elu\", \"keras.ops.nn.elu\"])\n def elu(x, alpha=1.0):\n     \"\"\"Exponential Linear Unit activation function.\n \n@@ -364,7 +364,7 @@ def elu(x, alpha=1.0):\n     Example:\n \n     >>> x = np.array([-1., 0., 1.])\n-    >>> x_elu = keras_core.ops.elu(x)\n+    >>> x_elu = keras.ops.elu(x)\n     >>> print(x_elu)\n     array([-0.63212055, 0., 1.], shape=(3,), dtype=float64)\n \n@@ -382,7 +382,7 @@ class Selu(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.selu\", \"keras_core.ops.nn.selu\"])\n+@keras_export([\"keras.ops.selu\", \"keras.ops.nn.selu\"])\n def selu(x):\n     \"\"\"Scaled Exponential Linear Unit (SELU) activation function.\n \n@@ -400,7 +400,7 @@ def selu(x):\n     Example:\n \n     >>> x = np.array([-1., 0., 1.])\n-    >>> x_selu = keras_core.ops.selu(x)\n+    >>> x_selu = keras.ops.selu(x)\n     >>> print(x_selu)\n     array([-1.11133055, 0., 1.05070098], shape=(3,), dtype=float64)\n \n@@ -422,7 +422,7 @@ class Gelu(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.gelu\", \"keras_core.ops.nn.gelu\"])\n+@keras_export([\"keras.ops.gelu\", \"keras.ops.nn.gelu\"])\n def gelu(x, approximate=True):\n     \"\"\"Gaussian Error Linear Unit (GELU) activation function.\n \n@@ -443,7 +443,7 @@ def gelu(x, approximate=True):\n     Example:\n \n     >>> x = np.array([-1., 0., 1.])\n-    >>> x_gelu = keras_core.ops.gelu(x)\n+    >>> x_gelu = keras.ops.gelu(x)\n     >>> print(x_gelu)\n     array([-0.15865525, 0., 0.84134475], shape=(3,), dtype=float64)\n \n@@ -465,7 +465,7 @@ class Softmax(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.softmax\", \"keras_core.ops.nn.softmax\"])\n+@keras_export([\"keras.ops.softmax\", \"keras.ops.nn.softmax\"])\n def softmax(x, axis=-1):\n     \"\"\"Softmax activation function.\n \n@@ -488,7 +488,7 @@ def softmax(x, axis=-1):\n     Example:\n \n     >>> x = np.array([-1., 0., 1.])\n-    >>> x_softmax = keras_core.ops.softmax(x)\n+    >>> x_softmax = keras.ops.softmax(x)\n     >>> print(x_softmax)\n     array([0.09003057, 0.24472847, 0.66524096], shape=(3,), dtype=float64)\n \n@@ -530,10 +530,10 @@ class LogSoftmax(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.ops.log_softmax\",\n-        \"keras_core.ops.nn.log_softmax\",\n+        \"keras.ops.log_softmax\",\n+        \"keras.ops.nn.log_softmax\",\n     ]\n )\n def log_softmax(x, axis=-1):\n@@ -553,7 +553,7 @@ def log_softmax(x, axis=-1):\n     Example:\n \n     >>> x = np.array([-1., 0., 1.])\n-    >>> x_log_softmax = keras_core.ops.log_softmax(x)\n+    >>> x_log_softmax = keras.ops.log_softmax(x)\n     >>> print(x_log_softmax)\n     array([-2.40760596, -1.40760596, -0.40760596], shape=(3,), dtype=float64)\n \n@@ -617,7 +617,7 @@ class MaxPool(Operation):\n         return KerasTensor(output_shape, dtype=inputs.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.max_pool\", \"keras_core.ops.nn.max_pool\"])\n+@keras_export([\"keras.ops.max_pool\", \"keras.ops.nn.max_pool\"])\n def max_pool(\n     inputs,\n     pool_size,\n@@ -702,10 +702,10 @@ class AveragePool(Operation):\n         return KerasTensor(output_shape, dtype=inputs.dtype)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.ops.average_pool\",\n-        \"keras_core.ops.nn.average_pool\",\n+        \"keras.ops.average_pool\",\n+        \"keras.ops.nn.average_pool\",\n     ]\n )\n def average_pool(\n@@ -797,7 +797,7 @@ class Conv(Operation):\n         return KerasTensor(output_shape, dtype=inputs.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.conv\", \"keras_core.ops.nn.conv\"])\n+@keras_export([\"keras.ops.conv\", \"keras.ops.nn.conv\"])\n def conv(\n     inputs,\n     kernel,\n@@ -889,10 +889,10 @@ class DepthwiseConv(Operation):\n         return KerasTensor(output_shape, dtype=inputs.dtype)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.ops.depthwise_conv\",\n-        \"keras_core.ops.nn.depthwise_conv\",\n+        \"keras.ops.depthwise_conv\",\n+        \"keras.ops.nn.depthwise_conv\",\n     ]\n )\n def depthwise_conv(\n@@ -997,10 +997,10 @@ class SeparableConv(Operation):\n         return KerasTensor(output_shape, dtype=inputs.dtype)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.ops.separable_conv\",\n-        \"keras_core.ops.nn.separable_conv\",\n+        \"keras.ops.separable_conv\",\n+        \"keras.ops.nn.separable_conv\",\n     ]\n )\n def separable_conv(\n@@ -1118,10 +1118,10 @@ class ConvTranspose(Operation):\n         return KerasTensor(output_shape, dtype=inputs.dtype)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.ops.conv_transpose\",\n-        \"keras_core.ops.nn.conv_transpose\",\n+        \"keras.ops.conv_transpose\",\n+        \"keras.ops.nn.conv_transpose\",\n     ]\n )\n def conv_transpose(\n@@ -1218,7 +1218,7 @@ class OneHot(Operation):\n         return KerasTensor(x_shape, dtype=self.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.one_hot\", \"keras_core.ops.nn.one_hot\"])\n+@keras_export([\"keras.ops.one_hot\", \"keras.ops.nn.one_hot\"])\n def one_hot(x, num_classes, axis=-1, dtype=None):\n     \"\"\"Converts integer tensor `x` into a one-hot tensor.\n \n@@ -1244,7 +1244,7 @@ def one_hot(x, num_classes, axis=-1, dtype=None):\n \n     Example:\n \n-    >>> x = keras_core.ops.convert_to_tensor([1, 3, 2, 0])\n+    >>> x = keras.ops.convert_to_tensor([1, 3, 2, 0])\n     >>> one_hot(x, num_classes=4)\n     array([[0. 1. 0. 0.]\n            [0. 0. 0. 1.]\n@@ -1278,10 +1278,10 @@ class BinaryCrossentropy(Operation):\n         return KerasTensor(output.shape, dtype=output.dtype)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.ops.binary_crossentropy\",\n-        \"keras_core.ops.nn.binary_crossentropy\",\n+        \"keras.ops.binary_crossentropy\",\n+        \"keras.ops.nn.binary_crossentropy\",\n     ]\n )\n def binary_crossentropy(target, output, from_logits=False):\n@@ -1310,8 +1310,8 @@ def binary_crossentropy(target, output, from_logits=False):\n \n     Example:\n \n-    >>> target = keras_core.ops.convert_to_tensor([0, 1, 1, 0])\n-    >>> output = keras_core.ops.convert_to_tensor([0.1, 0.9, 0.8, 0.2])\n+    >>> target = keras.ops.convert_to_tensor([0, 1, 1, 0])\n+    >>> output = keras.ops.convert_to_tensor([0.1, 0.9, 0.8, 0.2])\n     >>> binary_crossentropy(target, output)\n     array([0.10536054 0.10536054 0.22314355 0.22314355],\n           shape=(4,), dtype=float32)\n@@ -1352,10 +1352,10 @@ class CategoricalCrossentropy(Operation):\n         return KerasTensor(output.shape[:-1], dtype=output.dtype)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.ops.categorical_crossentropy\",\n-        \"keras_core.ops.nn.categorical_crossentropy\",\n+        \"keras.ops.categorical_crossentropy\",\n+        \"keras.ops.nn.categorical_crossentropy\",\n     ]\n )\n def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n@@ -1389,11 +1389,11 @@ def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n \n     Example:\n \n-    >>> target = keras_core.ops.convert_to_tensor(\n+    >>> target = keras.ops.convert_to_tensor(\n     ... [[1, 0, 0],\n     ...  [0, 1, 0],\n     ...  [0, 0, 1]])\n-    >>> output = keras_core.ops.convert_to_tensor(\n+    >>> output = keras.ops.convert_to_tensor(\n     ... [[0.9, 0.05, 0.05],\n     ...  [0.1, 0.8, 0.1],\n     ...  [0.2, 0.3, 0.5]])\n@@ -1439,10 +1439,10 @@ class SparseCategoricalCrossentropy(Operation):\n         return KerasTensor(output.shape[:-1], dtype=output.dtype)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.ops.sparse_categorical_crossentropy\",\n-        \"keras_core.ops.nn.sparse_categorical_crossentropy\",\n+        \"keras.ops.sparse_categorical_crossentropy\",\n+        \"keras.ops.nn.sparse_categorical_crossentropy\",\n     ]\n )\n def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n@@ -1477,8 +1477,8 @@ def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n \n     Example:\n \n-    >>> target = keras_core.ops.convert_to_tensor([0, 1, 2], dtype=int32)\n-    >>> output = keras_core.ops.convert_to_tensor(\n+    >>> target = keras.ops.convert_to_tensor([0, 1, 2], dtype=int32)\n+    >>> output = keras.ops.convert_to_tensor(\n     ... [[0.9, 0.05, 0.05],\n     ...  [0.1, 0.8, 0.1],\n     ...  [0.2, 0.3, 0.5]])\n@@ -1529,10 +1529,10 @@ class MultiHot(Operation):\n         return KerasTensor(x_shape, dtype=inputs.dtype)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.ops.multi_hot\",\n-        \"keras_core.ops.nn.multi_hot\",\n+        \"keras.ops.multi_hot\",\n+        \"keras.ops.nn.multi_hot\",\n     ]\n )\n def multi_hot(inputs, num_tokens, axis=-1, dtype=None):\n@@ -1554,8 +1554,8 @@ def multi_hot(inputs, num_tokens, axis=-1, dtype=None):\n \n     Example:\n \n-    >>> data = keras_core.ops.convert_to_tensor([0, 4])\n-    >>> keras_core.ops.multi_hot(data, num_tokens=5)\n+    >>> data = keras.ops.convert_to_tensor([0, 4])\n+    >>> keras.ops.multi_hot(data, num_tokens=5)\n     array([1.0, 0.0, 0.0, 0.0, 1.0], dtype=float32)\n \n     \"\"\"\n@@ -1587,10 +1587,10 @@ class Moments(Operation):\n         )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.ops.moments\",\n-        \"keras_core.ops.nn.moments\",\n+        \"keras.ops.moments\",\n+        \"keras.ops.nn.moments\",\n     ]\n )\n def moments(x, axes, keepdims=False):\n@@ -1611,8 +1611,8 @@ def moments(x, axes, keepdims=False):\n \n     Example:\n \n-    >>> x = keras_core.ops.convert_to_tensor([0, 1, 2, 3, 100], dtype=\"float32\")\n-    >>> keras_core.ops.moments(x, axes=[0])\n+    >>> x = keras.ops.convert_to_tensor([0, 1, 2, 3, 100], dtype=\"float32\")\n+    >>> keras.ops.moments(x, axes=[0])\n     (array(21.2, dtype=float32), array(1553.3601, dtype=float32))\n \n     \"\"\"\n\n@@ -3,10 +3,10 @@ import pytest\n import tensorflow as tf\n from absl.testing import parameterized\n \n-from keras_core import backend\n-from keras_core import testing\n-from keras_core.backend.common.keras_tensor import KerasTensor\n-from keras_core.ops import nn as knn\n+from keras import backend\n+from keras import testing\n+from keras.backend.common.keras_tensor import KerasTensor\n+from keras.ops import nn as knn\n \n \n class NNOpsDynamicShapeTest(testing.TestCase, parameterized.TestCase):\n\n@@ -2,8 +2,8 @@ import collections\n \n import tree\n \n-from keras_core.backend import KerasTensor\n-from keras_core.ops.symbolic_arguments import SymbolicArguments\n+from keras.backend import KerasTensor\n+from keras.ops.symbolic_arguments import SymbolicArguments\n \n \n class Node:\n\n@@ -1,9 +1,9 @@\n import numpy as np\n \n-from keras_core import Layer\n-from keras_core import testing\n-from keras_core.backend import KerasTensor\n-from keras_core.ops.node import Node\n+from keras import Layer\n+from keras import testing\n+from keras.backend import KerasTensor\n+from keras.ops.node import Node\n \n \n class DummyLayer(Layer):\n\n@@ -143,13 +143,13 @@ import re\n \n import numpy as np\n \n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.backend import KerasTensor\n-from keras_core.backend import any_symbolic_tensors\n-from keras_core.ops import operation_utils\n-from keras_core.ops.operation import Operation\n-from keras_core.ops.operation_utils import reduce_shape\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.backend import KerasTensor\n+from keras.backend import any_symbolic_tensors\n+from keras.ops import operation_utils\n+from keras.ops.operation import Operation\n+from keras.ops.operation_utils import reduce_shape\n \n \n def broadcast_shapes(shape1, shape2):\n@@ -262,11 +262,11 @@ class Absolute(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.absolute\", \"keras_core.ops.numpy.absolute\"])\n+@keras_export([\"keras.ops.absolute\", \"keras.ops.numpy.absolute\"])\n def absolute(x):\n     \"\"\"Compute the absolute value element-wise.\n \n-    `keras_core.ops.abs` is a shorthand for this function.\n+    `keras.ops.abs` is a shorthand for this function.\n \n     Args:\n         x: Input tensor.\n@@ -276,8 +276,8 @@ def absolute(x):\n \n     Example:\n \n-    >>> x = keras_core.ops.convert_to_tensor([-1.2, 1.2])\n-    >>> keras_core.ops.absolute(x)\n+    >>> x = keras.ops.convert_to_tensor([-1.2, 1.2])\n+    >>> keras.ops.absolute(x)\n     array([1.2, 1.2], dtype=float32)\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n@@ -289,9 +289,9 @@ class Abs(Absolute):\n     pass\n \n \n-@keras_core_export([\"keras_core.ops.abs\", \"keras_core.ops.numpy.abs\"])\n+@keras_export([\"keras.ops.abs\", \"keras.ops.numpy.abs\"])\n def abs(x):\n-    \"\"\"Shorthand for `keras_core.ops.absolute`.\"\"\"\n+    \"\"\"Shorthand for `keras.ops.absolute`.\"\"\"\n     return absolute(x)\n \n \n@@ -309,7 +309,7 @@ class Add(Operation):\n         return KerasTensor(output_shape, dtype=x1.dtype, sparse=output_sparse)\n \n \n-@keras_core_export([\"keras_core.ops.add\", \"keras_core.ops.numpy.add\"])\n+@keras_export([\"keras.ops.add\", \"keras.ops.numpy.add\"])\n def add(x1, x2):\n     \"\"\"Add arguments element-wise.\n \n@@ -321,18 +321,18 @@ def add(x1, x2):\n         The tensor containing the element-wise sum of `x1` and `x2`.\n \n     Examples:\n-    >>> x1 = keras_core.ops.convert_to_tensor([1, 4])\n-    >>> x2 = keras_core.ops.convert_to_tensor([5, 6])\n-    >>> keras_core.ops.add(x1, x2)\n+    >>> x1 = keras.ops.convert_to_tensor([1, 4])\n+    >>> x2 = keras.ops.convert_to_tensor([5, 6])\n+    >>> keras.ops.add(x1, x2)\n     array([6, 10], dtype=int32)\n \n-    `keras_core.ops.add` also broadcasts shapes:\n-    >>> x1 = keras_core.ops.convert_to_tensor(\n+    `keras.ops.add` also broadcasts shapes:\n+    >>> x1 = keras.ops.convert_to_tensor(\n     ...     [[5, 4],\n     ...      [5, 6]]\n     ... )\n-    >>> x2 = keras_core.ops.convert_to_tensor([5, 6])\n-    >>> keras_core.ops.add(x1, x2)\n+    >>> x2 = keras.ops.convert_to_tensor([5, 6])\n+    >>> keras.ops.add(x1, x2)\n     array([[10 10]\n            [10 12]], shape=(2, 2), dtype=int32)\n     \"\"\"\n@@ -368,7 +368,7 @@ class All(Operation):\n         )\n \n \n-@keras_core_export([\"keras_core.ops.all\", \"keras_core.ops.numpy.all\"])\n+@keras_export([\"keras.ops.all\", \"keras.ops.numpy.all\"])\n def all(x, axis=None, keepdims=False):\n     \"\"\"Test whether all array elements along a given axis evaluate to `True`.\n \n@@ -387,17 +387,17 @@ def all(x, axis=None, keepdims=False):\n         The tensor containing the logical AND reduction over the `axis`.\n \n     Examples:\n-    >>> x = keras_core.ops.convert_to_tensor([True, False])\n-    >>> keras_core.ops.all(x)\n+    >>> x = keras.ops.convert_to_tensor([True, False])\n+    >>> keras.ops.all(x)\n     array(False, shape=(), dtype=bool)\n \n-    >>> x = keras_core.ops.convert_to_tensor([[True, False], [True, True]])\n-    >>> keras_core.ops.all(x, axis=0)\n+    >>> x = keras.ops.convert_to_tensor([[True, False], [True, True]])\n+    >>> keras.ops.all(x, axis=0)\n     array([ True False], shape=(2,), dtype=bool)\n \n     `keepdims=True` outputs a tensor with dimensions reduced to one.\n-    >>> x = keras_core.ops.convert_to_tensor([[True, False], [True, True]])\n-    >>> keras_core.ops.all(x, keepdims=True)\n+    >>> x = keras.ops.convert_to_tensor([[True, False], [True, True]])\n+    >>> keras.ops.all(x, keepdims=True)\n     array([[False]], shape=(1, 1), dtype=bool)\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n@@ -432,7 +432,7 @@ class Any(Operation):\n         )\n \n \n-@keras_core_export([\"keras_core.ops.any\", \"keras_core.ops.numpy.any\"])\n+@keras_export([\"keras.ops.any\", \"keras.ops.numpy.any\"])\n def any(x, axis=None, keepdims=False):\n     \"\"\"Test whether any array element along a given axis evaluates to `True`.\n \n@@ -451,17 +451,17 @@ def any(x, axis=None, keepdims=False):\n         The tensor containing the logical OR reduction over the `axis`.\n \n     Examples:\n-    >>> x = keras_core.ops.convert_to_tensor([True, False])\n-    >>> keras_core.ops.any(x)\n+    >>> x = keras.ops.convert_to_tensor([True, False])\n+    >>> keras.ops.any(x)\n     array(True, shape=(), dtype=bool)\n \n-    >>> x = keras_core.ops.convert_to_tensor([[True, False], [True, True]])\n-    >>> keras_core.ops.any(x, axis=0)\n+    >>> x = keras.ops.convert_to_tensor([[True, False], [True, True]])\n+    >>> keras.ops.any(x, axis=0)\n     array([ True  True], shape=(2,), dtype=bool)\n \n     `keepdims=True` outputs a tensor with dimensions reduced to one.\n-    >>> x = keras_core.ops.convert_to_tensor([[True, False], [True, True]])\n-    >>> keras_core.ops.all(x, keepdims=True)\n+    >>> x = keras.ops.convert_to_tensor([[True, False], [True, True]])\n+    >>> keras.ops.all(x, keepdims=True)\n     array([[False]], shape=(1, 1), dtype=bool)\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n@@ -491,7 +491,7 @@ class Amax(Operation):\n         )\n \n \n-@keras_core_export([\"keras_core.ops.amax\", \"keras_core.ops.numpy.amax\"])\n+@keras_export([\"keras.ops.amax\", \"keras.ops.numpy.amax\"])\n def amax(x, axis=None, keepdims=False):\n     \"\"\"Returns the maximum of an array or maximum value along an axis.\n \n@@ -511,16 +511,16 @@ def amax(x, axis=None, keepdims=False):\n         the specified axis.\n \n     Examples:\n-    >>> x = keras_core.ops.convert_to_tensor([[1, 3, 5], [2, 3, 6]])\n-    >>> keras_core.ops.amax(x)\n+    >>> x = keras.ops.convert_to_tensor([[1, 3, 5], [2, 3, 6]])\n+    >>> keras.ops.amax(x)\n     array(6, dtype=int32)\n \n-    >>> x = keras_core.ops.convert_to_tensor([[1, 6, 8], [1, 5, 2]])\n-    >>> keras_core.ops.amax(x, axis=0)\n+    >>> x = keras.ops.convert_to_tensor([[1, 6, 8], [1, 5, 2]])\n+    >>> keras.ops.amax(x, axis=0)\n     array([1, 6, 8], dtype=int32)\n \n-    >>> x = keras_core.ops.convert_to_tensor([[1, 6, 8], [1, 5, 2]])\n-    >>> keras_core.ops.amax(x, axis=1, keepdims=True)\n+    >>> x = keras.ops.convert_to_tensor([[1, 6, 8], [1, 5, 2]])\n+    >>> keras.ops.amax(x, axis=1, keepdims=True)\n     array([[8], [5]], dtype=int32)\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n@@ -546,7 +546,7 @@ class Amin(Operation):\n         )\n \n \n-@keras_core_export([\"keras_core.ops.amin\", \"keras_core.ops.numpy.amin\"])\n+@keras_export([\"keras.ops.amin\", \"keras.ops.numpy.amin\"])\n def amin(x, axis=None, keepdims=False):\n     \"\"\"Returns the minimum of an array or minimum value along an axis.\n \n@@ -566,16 +566,16 @@ def amin(x, axis=None, keepdims=False):\n         the specified axis.\n \n     Examples:\n-    >>> x = keras_core.ops.convert_to_tensor([1, 3, 5, 2, 3, 6])\n-    >>> keras_core.ops.amin(x)\n+    >>> x = keras.ops.convert_to_tensor([1, 3, 5, 2, 3, 6])\n+    >>> keras.ops.amin(x)\n     array(1, dtype=int32)\n \n-    >>> x = keras_core.ops.convert_to_tensor([[1, 6, 8], [7, 5, 3]])\n-    >>> keras_core.ops.amin(x, axis=0)\n+    >>> x = keras.ops.convert_to_tensor([[1, 6, 8], [7, 5, 3]])\n+    >>> keras.ops.amin(x, axis=0)\n     array([1,5,3], dtype=int32)\n \n-    >>> x = keras_core.ops.convert_to_tensor([[1, 6, 8], [7, 5, 3]])\n-    >>> keras_core.ops.amin(x, axis=1, keepdims=True)\n+    >>> x = keras.ops.convert_to_tensor([[1, 6, 8], [7, 5, 3]])\n+    >>> keras.ops.amin(x, axis=1, keepdims=True)\n     array([[1],[3]], dtype=int32)\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n@@ -613,7 +613,7 @@ class Append(Operation):\n         return KerasTensor(output_shape, dtype=x1.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.append\", \"keras_core.ops.numpy.append\"])\n+@keras_export([\"keras.ops.append\", \"keras.ops.numpy.append\"])\n def append(\n     x1,\n     x2,\n@@ -631,20 +631,20 @@ def append(\n         A tensor with the values of `x2` appended to `x1`.\n \n     Examples:\n-    >>> x1 = keras_core.ops.convert_to_tensor([1, 2, 3])\n-    >>> x2 = keras_core.ops.convert_to_tensor([[4, 5, 6], [7, 8, 9]])\n-    >>> keras_core.ops.append(x1, x2)\n+    >>> x1 = keras.ops.convert_to_tensor([1, 2, 3])\n+    >>> x2 = keras.ops.convert_to_tensor([[4, 5, 6], [7, 8, 9]])\n+    >>> keras.ops.append(x1, x2)\n     array([1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)\n \n     When `axis` is specified, `x1` and `x2` must have compatible shapes.\n-    >>> x1 = keras_core.ops.convert_to_tensor([[1, 2, 3], [4, 5, 6]])\n-    >>> x2 = keras_core.ops.convert_to_tensor([[7, 8, 9]])\n-    >>> keras_core.ops.append(x1, x2, axis=0)\n+    >>> x1 = keras.ops.convert_to_tensor([[1, 2, 3], [4, 5, 6]])\n+    >>> x2 = keras.ops.convert_to_tensor([[7, 8, 9]])\n+    >>> keras.ops.append(x1, x2, axis=0)\n     array([[1, 2, 3],\n             [4, 5, 6],\n             [7, 8, 9]], dtype=int32)\n-    >>> x3 = keras_core.ops.convert_to_tensor([7, 8, 9])\n-    >>> keras_core.ops.append(x1, x3, axis=0)\n+    >>> x3 = keras.ops.convert_to_tensor([7, 8, 9])\n+    >>> keras.ops.append(x1, x3, axis=0)\n     Traceback (most recent call last):\n         ...\n     TypeError: Cannot concatenate arrays with different numbers of\n@@ -666,7 +666,7 @@ class Arange(Operation):\n         return KerasTensor(output_shape, dtype=dtype)\n \n \n-@keras_core_export([\"keras_core.ops.arange\", \"keras_core.ops.numpy.arange\"])\n+@keras_export([\"keras.ops.arange\", \"keras.ops.numpy.arange\"])\n def arange(start, stop=None, step=1, dtype=None):\n     \"\"\"Return evenly spaced values within a given interval.\n \n@@ -700,16 +700,16 @@ def arange(start, stop=None, step=1, dtype=None):\n         rule may result in the last element of out being greater than stop.\n \n     Examples:\n-    >>> keras_core.ops.arange(3)\n+    >>> keras.ops.arange(3)\n     array([0, 1, 2], dtype=int32)\n \n-    >>> keras_core.ops.arange(3.0)\n+    >>> keras.ops.arange(3.0)\n     array([0., 1., 2.], dtype=float32)\n \n-    >>> keras_core.ops.arange(3, 7)\n+    >>> keras.ops.arange(3, 7)\n     array([3, 4, 5, 6], dtype=int32)\n \n-    >>> keras_core.ops.arange(3, 7, 2)\n+    >>> keras.ops.arange(3, 7, 2)\n     array([3, 5], dtype=int32)\n     \"\"\"\n     return backend.numpy.arange(start, stop, step=step, dtype=dtype)\n@@ -723,7 +723,7 @@ class Arccos(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.arccos\", \"keras_core.ops.numpy.arccos\"])\n+@keras_export([\"keras.ops.arccos\", \"keras.ops.numpy.arccos\"])\n def arccos(x):\n     \"\"\"Trigonometric inverse cosine, element-wise.\n \n@@ -737,8 +737,8 @@ def arccos(x):\n         x-coordinate in radians `[0, pi]`.\n \n     Example:\n-    >>> x = keras_core.ops.convert_to_tensor([1, -1])\n-    >>> keras_core.ops.arccos(x)\n+    >>> x = keras.ops.convert_to_tensor([1, -1])\n+    >>> keras.ops.arccos(x)\n     array([0.0, 3.1415927], dtype=float32)\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n@@ -754,7 +754,7 @@ class Arccosh(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.arccosh\", \"keras_core.ops.numpy.arccosh\"])\n+@keras_export([\"keras.ops.arccosh\", \"keras.ops.numpy.arccosh\"])\n def arccosh(x):\n     \"\"\"Inverse hyperbolic cosine, element-wise.\n \n@@ -765,8 +765,8 @@ def arccosh(x):\n         Output tensor of same shape as x.\n \n     Example:\n-    >>> x = keras_core.ops.convert_to_tensor([10, 100])\n-    >>> keras_core.ops.arccosh(x)\n+    >>> x = keras.ops.convert_to_tensor([10, 100])\n+    >>> keras.ops.arccosh(x)\n     array([2.993223, 5.298292], dtype=float32)\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n@@ -782,7 +782,7 @@ class Arcsin(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.arcsin\", \"keras_core.ops.numpy.arcsin\"])\n+@keras_export([\"keras.ops.arcsin\", \"keras.ops.numpy.arcsin\"])\n def arcsin(x):\n     \"\"\"Inverse sine, element-wise.\n \n@@ -794,8 +794,8 @@ def arcsin(x):\n         the closed interval `[-pi/2, pi/2]`.\n \n     Example:\n-    >>> x = keras_core.ops.convert_to_tensor([1, -1, 0])\n-    >>> keras_core.ops.arcsin(x)\n+    >>> x = keras.ops.convert_to_tensor([1, -1, 0])\n+    >>> keras.ops.arcsin(x)\n     array([ 1.5707964, -1.5707964,  0.], dtype=float32)\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n@@ -811,7 +811,7 @@ class Arcsinh(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.arcsinh\", \"keras_core.ops.numpy.arcsinh\"])\n+@keras_export([\"keras.ops.arcsinh\", \"keras.ops.numpy.arcsinh\"])\n def arcsinh(x):\n     \"\"\"Inverse hyperbolic sine, element-wise.\n \n@@ -822,8 +822,8 @@ def arcsinh(x):\n         Output tensor of same shape as `x`.\n \n     Example:\n-    >>> x = keras_core.ops.convert_to_tensor([1, -1, 0])\n-    >>> keras_core.ops.arcsinh(x)\n+    >>> x = keras.ops.convert_to_tensor([1, -1, 0])\n+    >>> keras.ops.arcsinh(x)\n     array([0.88137364, -0.88137364, 0.0], dtype=float32)\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n@@ -839,7 +839,7 @@ class Arctan(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.arctan\", \"keras_core.ops.numpy.arctan\"])\n+@keras_export([\"keras.ops.arctan\", \"keras.ops.numpy.arctan\"])\n def arctan(x):\n     \"\"\"Trigonometric inverse tangent, element-wise.\n \n@@ -851,8 +851,8 @@ def arctan(x):\n         `[-pi/2, pi/2]`.\n \n     Example:\n-    >>> x = keras_core.ops.convert_to_tensor([0, 1])\n-    >>> keras_core.ops.arctan(x)\n+    >>> x = keras.ops.convert_to_tensor([0, 1])\n+    >>> keras.ops.arctan(x)\n     array([0., 0.7853982], dtype=float32)\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n@@ -871,7 +871,7 @@ class Arctan2(Operation):\n         return KerasTensor(outputs_shape, dtype=x1.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.arctan2\", \"keras_core.ops.numpy.arctan2\"])\n+@keras_export([\"keras.ops.arctan2\", \"keras.ops.numpy.arctan2\"])\n def arctan2(x1, x2):\n     \"\"\"Element-wise arc tangent of `x1/x2` choosing the quadrant correctly.\n \n@@ -892,21 +892,21 @@ def arctan2(x1, x2):\n \n     Examples:\n     Consider four points in different quadrants:\n-    >>> x = keras_core.ops.convert_to_tensor([-1, +1, +1, -1])\n-    >>> y = keras_core.ops.convert_to_tensor([-1, -1, +1, +1])\n-    >>> keras_core.ops.arctan2(y, x) * 180 / numpy.pi\n+    >>> x = keras.ops.convert_to_tensor([-1, +1, +1, -1])\n+    >>> y = keras.ops.convert_to_tensor([-1, -1, +1, +1])\n+    >>> keras.ops.arctan2(y, x) * 180 / numpy.pi\n     array([-135., -45., 45., 135.], dtype=float32)\n \n     Note the order of the parameters. `arctan2` is defined also when x2=0 and\n     at several other points, obtaining values in the range `[-pi, pi]`:\n-    >>> keras_core.ops.arctan2(\n-    ...     keras_core.ops.array([1., -1.]),\n-    ...     keras_core.ops.array([0., 0.]),\n+    >>> keras.ops.arctan2(\n+    ...     keras.ops.array([1., -1.]),\n+    ...     keras.ops.array([0., 0.]),\n     ... )\n     array([ 1.5707964, -1.5707964], dtype=float32)\n-    >>> keras_core.ops.arctan2(\n-    ...     keras_core.ops.array([0., 0., numpy.inf]),\n-    ...     keras_core.ops.array([+0., -0., numpy.inf]),\n+    >>> keras.ops.arctan2(\n+    ...     keras.ops.array([0., 0., numpy.inf]),\n+    ...     keras.ops.array([+0., -0., numpy.inf]),\n     ... )\n     array([0., 3.1415925, 0.7853982], dtype=float32)\n     \"\"\"\n@@ -923,7 +923,7 @@ class Arctanh(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.arctanh\", \"keras_core.ops.numpy.arctanh\"])\n+@keras_export([\"keras.ops.arctanh\", \"keras.ops.numpy.arctanh\"])\n def arctanh(x):\n     \"\"\"Inverse hyperbolic tangent, element-wise.\n \n@@ -954,7 +954,7 @@ class Argmax(Operation):\n         )\n \n \n-@keras_core_export([\"keras_core.ops.argmax\", \"keras_core.ops.numpy.argmax\"])\n+@keras_export([\"keras.ops.argmax\", \"keras.ops.numpy.argmax\"])\n def argmax(x, axis=None):\n     \"\"\"Returns the indices of the maximum values along an axis.\n \n@@ -968,15 +968,15 @@ def argmax(x, axis=None):\n         along `axis` removed.\n \n     Example:\n-    >>> x = keras_core.ops.arange(6).reshape(2, 3) + 10\n+    >>> x = keras.ops.arange(6).reshape(2, 3) + 10\n     >>> x\n     array([[10, 11, 12],\n            [13, 14, 15]], dtype=int32)\n-    >>> keras_core.ops.argmax(x)\n+    >>> keras.ops.argmax(x)\n     array(5, dtype=int32)\n-    >>> keras_core.ops.argmax(x, axis=0)\n+    >>> keras.ops.argmax(x, axis=0)\n     array([1, 1, 1], dtype=int32)\n-    >>> keras_core.ops.argmax(x, axis=1)\n+    >>> keras.ops.argmax(x, axis=1)\n     array([2, 2], dtype=int32)\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n@@ -1000,7 +1000,7 @@ class Argmin(Operation):\n         )\n \n \n-@keras_core_export([\"keras_core.ops.argmin\", \"keras_core.ops.numpy.argmin\"])\n+@keras_export([\"keras.ops.argmin\", \"keras.ops.numpy.argmin\"])\n def argmin(x, axis=None):\n     \"\"\"Returns the indices of the minium values along an axis.\n \n@@ -1014,15 +1014,15 @@ def argmin(x, axis=None):\n         along `axis` removed.\n \n     Example:\n-    >>> x = keras_core.ops.arange(6).reshape(2, 3) + 10\n+    >>> x = keras.ops.arange(6).reshape(2, 3) + 10\n     >>> x\n     array([[10, 11, 12],\n            [13, 14, 15]], dtype=int32)\n-    >>> keras_core.ops.argmin(x)\n+    >>> keras.ops.argmin(x)\n     array(0, dtype=int32)\n-    >>> keras_core.ops.argmin(x, axis=0)\n+    >>> keras.ops.argmin(x, axis=0)\n     array([0, 0, 0], dtype=int32)\n-    >>> keras_core.ops.argmin(x, axis=1)\n+    >>> keras.ops.argmin(x, axis=1)\n     array([0, 0], dtype=int32)\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n@@ -1044,7 +1044,7 @@ class Argsort(Operation):\n         return KerasTensor(x.shape, dtype=\"int32\")\n \n \n-@keras_core_export([\"keras_core.ops.argsort\", \"keras_core.ops.numpy.argsort\"])\n+@keras_export([\"keras.ops.argsort\", \"keras.ops.numpy.argsort\"])\n def argsort(x, axis=-1):\n     \"\"\"Returns the indices that would sort a tensor.\n \n@@ -1058,21 +1058,21 @@ def argsort(x, axis=-1):\n \n     Examples:\n     One dimensional array:\n-    >>> x = keras_core.ops.array([3, 1, 2])\n-    >>> keras_core.ops.argsort(x)\n+    >>> x = keras.ops.array([3, 1, 2])\n+    >>> keras.ops.argsort(x)\n     array([1, 2, 0], dtype=int32)\n \n     Two-dimensional array:\n-    >>> x = keras_core.ops.array([[0, 3], [3, 2], [4, 5]])\n+    >>> x = keras.ops.array([[0, 3], [3, 2], [4, 5]])\n     >>> x\n     array([[0, 3],\n            [3, 2],\n            [4, 5]], dtype=int32)\n-    >>> keras_core.ops.argsort(x, axis=0)\n+    >>> keras.ops.argsort(x, axis=0)\n     array([[0, 1],\n            [1, 0],\n            [2, 2]], dtype=int32)\n-    >>> keras_core.ops.argsort(x, axis=1)\n+    >>> keras.ops.argsort(x, axis=1)\n     array([[0, 1],\n            [1, 0],\n            [0, 1]], dtype=int32)\n@@ -1090,7 +1090,7 @@ class Array(Operation):\n         return KerasTensor(x.shape, dtype=dtype)\n \n \n-@keras_core_export([\"keras_core.ops.array\", \"keras_core.ops.numpy.array\"])\n+@keras_export([\"keras.ops.array\", \"keras.ops.numpy.array\"])\n def array(x, dtype=None):\n     \"\"\"Create a tensor.\n \n@@ -1102,10 +1102,10 @@ def array(x, dtype=None):\n         A tensor.\n \n     Examples:\n-    >>> keras_core.ops.array([1, 2, 3])\n+    >>> keras.ops.array([1, 2, 3])\n     array([1, 2, 3], dtype=int32)\n \n-    >>> keras_core.ops.array([1, 2, 3], dtype=\"float32\")\n+    >>> keras.ops.array([1, 2, 3], dtype=\"float32\")\n     array([1., 2., 3.], dtype=float32)\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n@@ -1159,7 +1159,7 @@ class Average(Operation):\n             )\n \n \n-@keras_core_export([\"keras_core.ops.average\", \"keras_core.ops.numpy.average\"])\n+@keras_export([\"keras.ops.average\", \"keras.ops.numpy.average\"])\n def average(x, axis=None, weights=None):\n     \"\"\"Compute the weighted average along the specified axis.\n \n@@ -1182,31 +1182,31 @@ def average(x, axis=None, weights=None):\n         Return the average along the specified axis.\n \n     Examples:\n-    >>> data = keras_core.ops.arange(1, 5)\n+    >>> data = keras.ops.arange(1, 5)\n     >>> data\n     array([1, 2, 3, 4], dtype=int32)\n-    >>> keras_core.ops.average(data)\n+    >>> keras.ops.average(data)\n     array(2.5, dtype=float32)\n-    >>> keras_core.ops.average(\n-    ...     keras_core.ops.arange(1, 11),\n-    ...     weights=keras_core.ops.arange(10, 0, -1)\n+    >>> keras.ops.average(\n+    ...     keras.ops.arange(1, 11),\n+    ...     weights=keras.ops.arange(10, 0, -1)\n     ... )\n     array(4., dtype=float32)\n \n-    >>> data = keras_core.ops.arange(6).reshape((3, 2))\n+    >>> data = keras.ops.arange(6).reshape((3, 2))\n     >>> data\n     array([[0, 1],\n            [2, 3],\n            [4, 5]], dtype=int32)\n-    >>> keras_core.ops.average(\n+    >>> keras.ops.average(\n     ...     data,\n     ...     axis=1,\n-    ...     weights=keras_core.ops.array([1./4, 3./4])\n+    ...     weights=keras.ops.array([1./4, 3./4])\n     ... )\n     array([0.75, 2.75, 4.75], dtype=float32)\n-    >>> keras_core.ops.average(\n+    >>> keras.ops.average(\n     ...     data,\n-    ...     weights=keras_core.ops.array([1./4, 3./4])\n+    ...     weights=keras.ops.array([1./4, 3./4])\n     ... )\n     Traceback (most recent call last):\n         ...\n@@ -1233,7 +1233,7 @@ class Bincount(Operation):\n         return KerasTensor(out_shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.bincount\", \"keras_core.ops.numpy.bincount\"])\n+@keras_export([\"keras.ops.bincount\", \"keras.ops.numpy.bincount\"])\n def bincount(x, weights=None, minlength=0):\n     \"\"\"Count the number of occurrences of each value in a tensor of integers.\n \n@@ -1257,16 +1257,16 @@ def bincount(x, weights=None, minlength=0):\n         minlength.\n \n     Examples:\n-    >>> x = keras_core.ops.array([1, 2, 2, 3], dtype=\"uint8\")\n-    >>> keras_core.ops.bincount(x)\n+    >>> x = keras.ops.array([1, 2, 2, 3], dtype=\"uint8\")\n+    >>> keras.ops.bincount(x)\n     array([0, 1, 2, 1], dtype=int32)\n     >>> weights = x / 2\n     >>> weights\n     array([0.5, 1., 1., 1.5], dtype=float64)\n-    >>> keras_core.ops.bincount(x, weights=weights)\n+    >>> keras.ops.bincount(x, weights=weights)\n     array([0., 0.5, 2., 1.5], dtype=float64)\n-    >>> minlength = (keras_core.ops.max(x).numpy() + 1) + 2 # 6\n-    >>> keras_core.ops.bincount(x, minlength=minlength)\n+    >>> minlength = (keras.ops.max(x).numpy() + 1) + 2 # 6\n+    >>> keras.ops.bincount(x, minlength=minlength)\n     array([0, 1, 2, 1, 0, 0], dtype=int32)\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n@@ -1288,10 +1288,10 @@ class BroadcastTo(Operation):\n         return KerasTensor(self.shape, dtype=x.dtype)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.ops.broadcast_to\",\n-        \"keras_core.ops.numpy.broadcast_to\",\n+        \"keras.ops.broadcast_to\",\n+        \"keras.ops.numpy.broadcast_to\",\n     ]\n )\n def broadcast_to(x, shape):\n@@ -1306,8 +1306,8 @@ def broadcast_to(x, shape):\n         A tensor with the desired shape.\n \n     Examples:\n-    >>> x = keras_core.ops.array([1, 2, 3])\n-    >>> keras_core.ops.broadcast_to(x, (3, 3))\n+    >>> x = keras.ops.array([1, 2, 3])\n+    >>> keras.ops.broadcast_to(x, (3, 3))\n     array([[1, 2, 3],\n            [1, 2, 3],\n            [1, 2, 3]])\n@@ -1325,7 +1325,7 @@ class Ceil(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.ceil\", \"keras_core.ops.numpy.ceil\"])\n+@keras_export([\"keras.ops.ceil\", \"keras.ops.numpy.ceil\"])\n def ceil(x):\n     \"\"\"Return the ceiling of the input, element-wise.\n \n@@ -1356,7 +1356,7 @@ class Clip(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.clip\", \"keras_core.ops.numpy.clip\"])\n+@keras_export([\"keras.ops.clip\", \"keras.ops.numpy.clip\"])\n def clip(x, x_min, x_max):\n     \"\"\"Clip (limit) the values in a tensor.\n \n@@ -1411,10 +1411,10 @@ class Concatenate(Operation):\n         return KerasTensor(output_shape, dtype=x.dtype, sparse=all_sparse)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.ops.concatenate\",\n-        \"keras_core.ops.numpy.concatenate\",\n+        \"keras.ops.concatenate\",\n+        \"keras.ops.numpy.concatenate\",\n     ]\n )\n def concatenate(xs, axis=0):\n@@ -1440,8 +1440,8 @@ class Conjugate(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export(\n-    [\"keras_core.ops.conjugate\", \"keras_core.ops.numpy.conjugate\"]\n+@keras_export(\n+    [\"keras.ops.conjugate\", \"keras.ops.numpy.conjugate\"]\n )\n def conjugate(x):\n     \"\"\"Returns the complex conjugate, element-wise.\n@@ -1449,7 +1449,7 @@ def conjugate(x):\n     The complex conjugate of a complex number is obtained by changing the sign\n     of its imaginary part.\n \n-    `keras_core.ops.conj` is a shorthand for this function.\n+    `keras.ops.conj` is a shorthand for this function.\n \n     Args:\n         x: Input tensor.\n@@ -1466,9 +1466,9 @@ class Conj(Conjugate):\n     pass\n \n \n-@keras_core_export([\"keras_core.ops.conj\", \"keras_core.ops.numpy.conj\"])\n+@keras_export([\"keras.ops.conj\", \"keras.ops.numpy.conj\"])\n def conj(x):\n-    \"\"\"Shorthand for `keras_core.ops.conjugate`.\"\"\"\n+    \"\"\"Shorthand for `keras.ops.conjugate`.\"\"\"\n     return conjugate(x)\n \n \n@@ -1480,7 +1480,7 @@ class Copy(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.copy\", \"keras_core.ops.numpy.copy\"])\n+@keras_export([\"keras.ops.copy\", \"keras.ops.numpy.copy\"])\n def copy(x):\n     \"\"\"Returns a copy of `x`.\n \n@@ -1503,7 +1503,7 @@ class Cos(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.cos\", \"keras_core.ops.numpy.cos\"])\n+@keras_export([\"keras.ops.cos\", \"keras.ops.numpy.cos\"])\n def cos(x):\n     \"\"\"Cosine, element-wise.\n \n@@ -1526,7 +1526,7 @@ class Cosh(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.cosh\", \"keras_core.ops.numpy.cosh\"])\n+@keras_export([\"keras.ops.cosh\", \"keras.ops.numpy.cosh\"])\n def cosh(x):\n     \"\"\"Hyperbolic cosine, element-wise.\n \n@@ -1559,10 +1559,10 @@ class CountNonzero(Operation):\n         )\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.ops.count_nonzero\",\n-        \"keras_core.ops.numpy.count_nonzero\",\n+        \"keras.ops.count_nonzero\",\n+        \"keras.ops.numpy.count_nonzero\",\n     ]\n )\n def count_nonzero(x, axis=None):\n@@ -1579,12 +1579,12 @@ def count_nonzero(x, axis=None):\n         int or tensor of ints.\n \n     Examples:\n-    >>> x = keras_core.ops.array([[0, 1, 7, 0], [3, 0, 2, 19]])\n-    >>> keras_core.ops.count_nonzero(x)\n+    >>> x = keras.ops.array([[0, 1, 7, 0], [3, 0, 2, 19]])\n+    >>> keras.ops.count_nonzero(x)\n     5\n-    >>> keras_core.ops.count_nonzero(x, axis=0)\n+    >>> keras.ops.count_nonzero(x, axis=0)\n     array([1, 1, 2, 1], dtype=int64)\n-    >>> keras_core.ops.count_nonzero(x, axis=1)\n+    >>> keras.ops.count_nonzero(x, axis=1)\n     array([2, 3], dtype=int64)\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n@@ -1639,7 +1639,7 @@ class Cross(Operation):\n         return KerasTensor(output_shape, dtype=x1.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.cross\", \"keras_core.ops.numpy.cross\"])\n+@keras_export([\"keras.ops.cross\", \"keras.ops.numpy.cross\"])\n def cross(x1, x2, axisa=-1, axisb=-1, axisc=-1, axis=None):\n     \"\"\"Returns the cross product of two (arrays of) vectors.\n \n@@ -1706,7 +1706,7 @@ class Cumprod(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.cumprod\", \"keras_core.ops.numpy.cumprod\"])\n+@keras_export([\"keras.ops.cumprod\", \"keras.ops.numpy.cumprod\"])\n def cumprod(x, axis=None):\n     \"\"\"Return the cumulative product of elements along a given axis.\n \n@@ -1741,7 +1741,7 @@ class Cumsum(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.cumsum\", \"keras_core.ops.numpy.cumsum\"])\n+@keras_export([\"keras.ops.cumsum\", \"keras.ops.numpy.cumsum\"])\n def cumsum(x, axis=None):\n     \"\"\"Returns the cumulative sum of elements along a given axis.\n \n@@ -1795,7 +1795,7 @@ class Diag(Operation):\n         return KerasTensor(output_shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.diag\", \"keras_core.ops.numpy.diag\"])\n+@keras_export([\"keras.ops.diag\", \"keras.ops.numpy.diag\"])\n def diag(x, k=0):\n     \"\"\"Extract a diagonal or construct a diagonal array.\n \n@@ -1810,7 +1810,7 @@ def diag(x, k=0):\n         The extracted diagonal or constructed diagonal tensor.\n \n     Examples:\n-    >>> from keras_core import ops\n+    >>> from keras import ops\n     >>> x = ops.arange(9).reshape((3, 3))\n     >>> x\n     array([[0, 1, 2],\n@@ -1876,7 +1876,7 @@ class Diagonal(Operation):\n         return KerasTensor(output_shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.diagonal\", \"keras_core.ops.numpy.diagonal\"])\n+@keras_export([\"keras.ops.diagonal\", \"keras.ops.numpy.diagonal\"])\n def diagonal(x, offset=0, axis1=0, axis2=1):\n     \"\"\"Return specified diagonals.\n \n@@ -1904,7 +1904,7 @@ def diagonal(x, offset=0, axis1=0, axis2=1):\n         Tensor of diagonals.\n \n     Examples:\n-    >>> from keras_core import ops\n+    >>> from keras import ops\n     >>> x = ops.arange(4).reshape((2, 2))\n     >>> x\n     array([[0, 1],\n@@ -1952,7 +1952,7 @@ class Digitize(Operation):\n         return KerasTensor(x.shape, dtype=\"int32\")\n \n \n-@keras_core_export([\"keras_core.ops.digitize\", \"keras_core.ops.numpy.digitize\"])\n+@keras_export([\"keras.ops.digitize\", \"keras.ops.numpy.digitize\"])\n def digitize(x, bins):\n     \"\"\"Returns the indices of the bins to which each value in `x` belongs.\n \n@@ -1967,7 +1967,7 @@ def digitize(x, bins):\n     Example:\n     >>> x = np.array([0.0, 1.0, 3.0, 1.6])\n     >>> bins = np.array([0.0, 3.0, 4.5, 7.0])\n-    >>> keras_core.ops.digitize(x, bins)\n+    >>> keras.ops.digitize(x, bins)\n     array([1, 1, 2, 1])\n     \"\"\"\n     if any_symbolic_tensors((x, bins)):\n@@ -2011,7 +2011,7 @@ class Dot(Operation):\n         )\n \n \n-@keras_core_export([\"keras_core.ops.dot\", \"keras_core.ops.numpy.dot\"])\n+@keras_export([\"keras.ops.dot\", \"keras.ops.numpy.dot\"])\n def dot(x1, x2):\n     \"\"\"Dot product of two tensors.\n \n@@ -2214,7 +2214,7 @@ class Einsum(Operation):\n         return KerasTensor(output_shape, dtype=dtype)\n \n \n-@keras_core_export([\"keras_core.ops.einsum\", \"keras_core.ops.numpy.einsum\"])\n+@keras_export([\"keras.ops.einsum\", \"keras.ops.numpy.einsum\"])\n def einsum(subscripts, *operands):\n     \"\"\"Evaluates the Einstein summation convention on the operands.\n \n@@ -2230,7 +2230,7 @@ def einsum(subscripts, *operands):\n         The calculation based on the Einstein summation convention.\n \n     Example:\n-    >>> from keras_core import ops\n+    >>> from keras import ops\n     >>> a = ops.arange(25).reshape(5, 5)\n     >>> b = ops.arange(5)\n     >>> c = ops.arange(6).reshape(2, 3)\n@@ -2311,7 +2311,7 @@ class Empty(Operation):\n         return KerasTensor(shape, dtype=dtype)\n \n \n-@keras_core_export([\"keras_core.ops.empty\", \"keras_core.ops.numpy.empty\"])\n+@keras_export([\"keras.ops.empty\", \"keras.ops.numpy.empty\"])\n def empty(shape, dtype=\"float32\"):\n     \"\"\"Return a tensor of given shape and type filled with uninitialized data.\n \n@@ -2336,7 +2336,7 @@ class Equal(Operation):\n         return KerasTensor(output_shape, dtype=x1.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.equal\", \"keras_core.ops.numpy.equal\"])\n+@keras_export([\"keras.ops.equal\", \"keras.ops.numpy.equal\"])\n def equal(x1, x2):\n     \"\"\"Returns `(x1 == x2)` element-wise.\n \n@@ -2360,7 +2360,7 @@ class Exp(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.exp\", \"keras_core.ops.numpy.exp\"])\n+@keras_export([\"keras.ops.exp\", \"keras.ops.numpy.exp\"])\n def exp(x):\n     \"\"\"Calculate the exponential of all elements in the input tensor.\n \n@@ -2398,10 +2398,10 @@ class ExpandDims(Operation):\n         return KerasTensor(output_shape, dtype=x.dtype, sparse=x.sparse)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.ops.expand_dims\",\n-        \"keras_core.ops.numpy.expand_dims\",\n+        \"keras.ops.expand_dims\",\n+        \"keras.ops.numpy.expand_dims\",\n     ]\n )\n def expand_dims(x, axis):\n@@ -2430,7 +2430,7 @@ class Expm1(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.expm1\", \"keras_core.ops.numpy.expm1\"])\n+@keras_export([\"keras.ops.expm1\", \"keras.ops.numpy.expm1\"])\n def expm1(x):\n     \"\"\"Calculate `exp(x) - 1` for all elements in the tensor.\n \n@@ -2457,7 +2457,7 @@ class Flip(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.flip\", \"keras_core.ops.numpy.flip\"])\n+@keras_export([\"keras.ops.flip\", \"keras.ops.numpy.flip\"])\n def flip(x, axis=None):\n     \"\"\"Reverse the order of elements in the tensor along the given axis.\n \n@@ -2484,7 +2484,7 @@ class Floor(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.floor\", \"keras_core.ops.numpy.floor\"])\n+@keras_export([\"keras.ops.floor\", \"keras.ops.numpy.floor\"])\n def floor(x):\n     \"\"\"Return the floor of the input, element-wise.\n \n@@ -2509,7 +2509,7 @@ class Full(Operation):\n         return KerasTensor(shape, dtype=dtype)\n \n \n-@keras_core_export([\"keras_core.ops.full\", \"keras_core.ops.numpy.full\"])\n+@keras_export([\"keras.ops.full\", \"keras.ops.numpy.full\"])\n def full(shape, fill_value, dtype=None):\n     \"\"\"Return a new tensor of given shape and type, filled with `fill_value`.\n \n@@ -2532,8 +2532,8 @@ class FullLike(Operation):\n         return KerasTensor(x.shape, dtype=dtype)\n \n \n-@keras_core_export(\n-    [\"keras_core.ops.full_like\", \"keras_core.ops.numpy.full_like\"]\n+@keras_export(\n+    [\"keras.ops.full_like\", \"keras.ops.numpy.full_like\"]\n )\n def full_like(x, fill_value, dtype=None):\n     \"\"\"Return a full tensor with the same shape and type as the given tensor.\n@@ -2622,7 +2622,7 @@ class GetItem(Operation):\n         return KerasTensor(tuple(new_shape), dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.get_item\", \"keras_core.ops.numpy.get_item\"])\n+@keras_export([\"keras.ops.get_item\", \"keras.ops.numpy.get_item\"])\n def get_item(x, key):\n     \"\"\"Return `x[key]`.\"\"\"\n     if any_symbolic_tensors((x,)):\n@@ -2641,7 +2641,7 @@ class Greater(Operation):\n         return KerasTensor(output_shape, dtype=x1.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.greater\", \"keras_core.ops.numpy.greater\"])\n+@keras_export([\"keras.ops.greater\", \"keras.ops.numpy.greater\"])\n def greater(x1, x2):\n     \"\"\"Return the truth value of `x1 > x2` element-wise.\n \n@@ -2668,10 +2668,10 @@ class GreaterEqual(Operation):\n         return KerasTensor(output_shape, dtype=x1.dtype)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.ops.greater_equal\",\n-        \"keras_core.ops.numpy.greater_equal\",\n+        \"keras.ops.greater_equal\",\n+        \"keras.ops.numpy.greater_equal\",\n     ]\n )\n def greater_equal(x1, x2):\n@@ -2713,7 +2713,7 @@ class Hstack(Operation):\n         return KerasTensor(output_shape)\n \n \n-@keras_core_export([\"keras_core.ops.hstack\", \"keras_core.ops.numpy.hstack\"])\n+@keras_export([\"keras.ops.hstack\", \"keras.ops.numpy.hstack\"])\n def hstack(xs):\n     \"\"\"Stack tensors in sequence horizontally (column wise).\n \n@@ -2739,7 +2739,7 @@ class Identity(Operation):\n         return KerasTensor([n, n], dtype=dtype)\n \n \n-@keras_core_export([\"keras_core.ops.identity\", \"keras_core.ops.numpy.identity\"])\n+@keras_export([\"keras.ops.identity\", \"keras.ops.numpy.identity\"])\n def identity(n, dtype=\"float32\"):\n     \"\"\"Return the identity tensor.\n \n@@ -2764,7 +2764,7 @@ class Imag(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.imag\", \"keras_core.ops.numpy.imag\"])\n+@keras_export([\"keras.ops.imag\", \"keras.ops.numpy.imag\"])\n def imag(x):\n     \"\"\"Return the imaginary part of the complex argument.\n \n@@ -2790,7 +2790,7 @@ class Isclose(Operation):\n         return KerasTensor(output_shape, dtype=x1.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.isclose\", \"keras_core.ops.numpy.isclose\"])\n+@keras_export([\"keras.ops.isclose\", \"keras.ops.numpy.isclose\"])\n def isclose(x1, x2):\n     \"\"\"Return whether two tensors are element-wise almost equal.\n \n@@ -2814,7 +2814,7 @@ class Isfinite(Operation):\n         return KerasTensor(x.shape, dtype=\"bool\")\n \n \n-@keras_core_export([\"keras_core.ops.isfinite\", \"keras_core.ops.numpy.isfinite\"])\n+@keras_export([\"keras.ops.isfinite\", \"keras.ops.numpy.isfinite\"])\n def isfinite(x):\n     \"\"\"Return whether a tensor is finite, element-wise.\n \n@@ -2841,7 +2841,7 @@ class Isinf(Operation):\n         return KerasTensor(x.shape, dtype=\"bool\")\n \n \n-@keras_core_export([\"keras_core.ops.isinf\", \"keras_core.ops.numpy.isinf\"])\n+@keras_export([\"keras.ops.isinf\", \"keras.ops.numpy.isinf\"])\n def isinf(x):\n     \"\"\"Test element-wise for positive or negative infinity.\n \n@@ -2864,7 +2864,7 @@ class Isnan(Operation):\n         return KerasTensor(x.shape, dtype=\"bool\")\n \n \n-@keras_core_export([\"keras_core.ops.isnan\", \"keras_core.ops.numpy.isnan\"])\n+@keras_export([\"keras.ops.isnan\", \"keras.ops.numpy.isnan\"])\n def isnan(x):\n     \"\"\"Test element-wise for NaN and return result as a boolean tensor.\n \n@@ -2890,7 +2890,7 @@ class Less(Operation):\n         return KerasTensor(output_shape, dtype=x1.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.less\", \"keras_core.ops.numpy.less\"])\n+@keras_export([\"keras.ops.less\", \"keras.ops.numpy.less\"])\n def less(x1, x2):\n     \"\"\"Return the truth value of `x1 < x2` element-wise.\n \n@@ -2917,10 +2917,10 @@ class LessEqual(Operation):\n         return KerasTensor(output_shape, dtype=x1.dtype)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.ops.less_equal\",\n-        \"keras_core.ops.numpy.less_equal\",\n+        \"keras.ops.less_equal\",\n+        \"keras.ops.numpy.less_equal\",\n     ]\n )\n def less_equal(x1, x2):\n@@ -2985,7 +2985,7 @@ class Linspace(Operation):\n         return KerasTensor(output_shape, dtype=dtype)\n \n \n-@keras_core_export([\"keras_core.ops.linspace\", \"keras_core.ops.numpy.linspace\"])\n+@keras_export([\"keras.ops.linspace\", \"keras.ops.numpy.linspace\"])\n def linspace(\n     start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0\n ):\n@@ -3040,7 +3040,7 @@ class Log(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.log\", \"keras_core.ops.numpy.log\"])\n+@keras_export([\"keras.ops.log\", \"keras.ops.numpy.log\"])\n def log(x):\n     \"\"\"Natural logarithm, element-wise.\n \n@@ -3063,7 +3063,7 @@ class Log10(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.log10\", \"keras_core.ops.numpy.log10\"])\n+@keras_export([\"keras.ops.log10\", \"keras.ops.numpy.log10\"])\n def log10(x):\n     \"\"\"Return the base 10 logarithm of the input tensor, element-wise.\n \n@@ -3086,7 +3086,7 @@ class Log1p(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.log1p\", \"keras_core.ops.numpy.log1p\"])\n+@keras_export([\"keras.ops.log1p\", \"keras.ops.numpy.log1p\"])\n def log1p(x):\n     \"\"\"Returns the natural logarithm of one plus the `x`, element-wise.\n \n@@ -3111,7 +3111,7 @@ class Log2(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.log2\", \"keras_core.ops.numpy.log2\"])\n+@keras_export([\"keras.ops.log2\", \"keras.ops.numpy.log2\"])\n def log2(x):\n     \"\"\"Base-2 logarithm of `x`, element-wise.\n \n@@ -3137,8 +3137,8 @@ class Logaddexp(Operation):\n         return KerasTensor(output_shape, dtype=x1.dtype)\n \n \n-@keras_core_export(\n-    [\"keras_core.ops.logaddexp\", \"keras_core.ops.numpy.logaddexp\"]\n+@keras_export(\n+    [\"keras.ops.logaddexp\", \"keras.ops.numpy.logaddexp\"]\n )\n def logaddexp(x1, x2):\n     \"\"\"Logarithm of the sum of exponentiations of the inputs.\n@@ -3169,10 +3169,10 @@ class LogicalAnd(Operation):\n         return KerasTensor(output_shape, dtype=x1.dtype)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.ops.logical_and\",\n-        \"keras_core.ops.numpy.logical_and\",\n+        \"keras.ops.logical_and\",\n+        \"keras.ops.numpy.logical_and\",\n     ]\n )\n def logical_and(x1, x2):\n@@ -3200,10 +3200,10 @@ class LogicalNot(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.ops.logical_not\",\n-        \"keras_core.ops.numpy.logical_not\",\n+        \"keras.ops.logical_not\",\n+        \"keras.ops.numpy.logical_not\",\n     ]\n )\n def logical_not(x):\n@@ -3233,10 +3233,10 @@ class LogicalOr(Operation):\n         return KerasTensor(output_shape, dtype=x1.dtype)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.ops.logical_or\",\n-        \"keras_core.ops.numpy.logical_or\",\n+        \"keras.ops.logical_or\",\n+        \"keras.ops.numpy.logical_or\",\n     ]\n )\n def logical_or(x1, x2):\n@@ -3299,7 +3299,7 @@ class Logspace(Operation):\n         return KerasTensor(output_shape, dtype=dtype)\n \n \n-@keras_core_export([\"keras_core.ops.logspace\", \"keras_core.ops.numpy.logspace\"])\n+@keras_export([\"keras.ops.logspace\", \"keras.ops.numpy.logspace\"])\n def logspace(start, stop, num=50, endpoint=True, base=10, dtype=None, axis=0):\n     \"\"\"Returns numbers spaced evenly on a log scale.\n \n@@ -3374,7 +3374,7 @@ class Matmul(Operation):\n         return KerasTensor(output_shape, dtype=x1.dtype, sparse=output_sparse)\n \n \n-@keras_core_export([\"keras_core.ops.matmul\", \"keras_core.ops.numpy.matmul\"])\n+@keras_export([\"keras.ops.matmul\", \"keras.ops.numpy.matmul\"])\n def matmul(x1, x2):\n     \"\"\"Matrix product of two tensors.\n \n@@ -3424,7 +3424,7 @@ class Max(Operation):\n         )\n \n \n-@keras_core_export([\"keras_core.ops.max\", \"keras_core.ops.numpy.max\"])\n+@keras_export([\"keras.ops.max\", \"keras.ops.numpy.max\"])\n def max(x, axis=None, keepdims=False, initial=None):\n     \"\"\"Return the maximum of a tensor or maximum along an axis.\n \n@@ -3460,7 +3460,7 @@ class Maximum(Operation):\n         return KerasTensor(output_shape, dtype=x1.dtype, sparse=output_sparse)\n \n \n-@keras_core_export([\"keras_core.ops.maximum\", \"keras_core.ops.numpy.maximum\"])\n+@keras_export([\"keras.ops.maximum\", \"keras.ops.numpy.maximum\"])\n def maximum(x1, x2):\n     \"\"\"Element-wise maximum of `x1` and `x2`.\n \n@@ -3508,7 +3508,7 @@ class Meshgrid(Operation):\n         return [KerasTensor(output_shape) for _ in range(len(x))]\n \n \n-@keras_core_export([\"keras_core.ops.meshgrid\", \"keras_core.ops.numpy.meshgrid\"])\n+@keras_export([\"keras.ops.meshgrid\", \"keras.ops.numpy.meshgrid\"])\n def meshgrid(*x, indexing=\"xy\"):\n     \"\"\"Creates grids of coordinates from coordinate vectors.\n \n@@ -3526,7 +3526,7 @@ def meshgrid(*x, indexing=\"xy\"):\n         Sequence of N tensors.\n \n     Example:\n-    >>> from keras_core import ops\n+    >>> from keras import ops\n     >>> x = ops.array([1, 2, 3])\n     >>> y = ops.array([4, 5, 6])\n \n@@ -3566,7 +3566,7 @@ class Min(Operation):\n         )\n \n \n-@keras_core_export([\"keras_core.ops.min\", \"keras_core.ops.numpy.min\"])\n+@keras_export([\"keras.ops.min\", \"keras.ops.numpy.min\"])\n def min(x, axis=None, keepdims=False, initial=None):\n     \"\"\"Return the minimum of a tensor or minimum along an axis.\n \n@@ -3602,7 +3602,7 @@ class Minimum(Operation):\n         return KerasTensor(output_shape, dtype=x1.dtype, sparse=output_sparse)\n \n \n-@keras_core_export([\"keras_core.ops.minimum\", \"keras_core.ops.numpy.minimum\"])\n+@keras_export([\"keras.ops.minimum\", \"keras.ops.numpy.minimum\"])\n def minimum(x1, x2):\n     \"\"\"Element-wise minimum of `x1` and `x2`.\n \n@@ -3629,7 +3629,7 @@ class Mod(Operation):\n         return KerasTensor(output_shape, dtype=x1.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.mod\", \"keras_core.ops.numpy.mod\"])\n+@keras_export([\"keras.ops.mod\", \"keras.ops.numpy.mod\"])\n def mod(x1, x2):\n     \"\"\"Returns the element-wise remainder of division.\n \n@@ -3689,7 +3689,7 @@ class Moveaxis(Operation):\n         return KerasTensor(output_shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.moveaxis\", \"keras_core.ops.numpy.moveaxis\"])\n+@keras_export([\"keras.ops.moveaxis\", \"keras.ops.numpy.moveaxis\"])\n def moveaxis(x, source, destination):\n     \"\"\"Move axes of a tensor to new positions.\n \n@@ -3714,10 +3714,10 @@ class NanToNum(Operation):\n         return backend.numpy.nan_to_num(x)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.ops.nan_to_num\",\n-        \"keras_core.ops.numpy.nan_to_num\",\n+        \"keras.ops.nan_to_num\",\n+        \"keras.ops.numpy.nan_to_num\",\n     ]\n )\n def nan_to_num(x):\n@@ -3742,7 +3742,7 @@ class Ndim(Operation):\n         return KerasTensor([len(x.shape)])\n \n \n-@keras_core_export([\"keras_core.ops.ndim\", \"keras_core.ops.numpy.ndim\"])\n+@keras_export([\"keras.ops.ndim\", \"keras.ops.numpy.ndim\"])\n def ndim(x):\n     \"\"\"Return the number of dimensions of a tensor.\n \n@@ -3762,7 +3762,7 @@ class Nonzero(Operation):\n         return backend.numpy.nonzero(x)\n \n \n-@keras_core_export([\"keras_core.ops.nonzero\", \"keras_core.ops.numpy.nonzero\"])\n+@keras_export([\"keras.ops.nonzero\", \"keras.ops.numpy.nonzero\"])\n def nonzero(x):\n     \"\"\"Return the indices of the elements that are non-zero.\n \n@@ -3786,8 +3786,8 @@ class NotEqual(Operation):\n         return KerasTensor(output_shape, dtype=x1.dtype)\n \n \n-@keras_core_export(\n-    [\"keras_core.ops.not_equal\", \"keras_core.ops.numpy.not_equal\"]\n+@keras_export(\n+    [\"keras.ops.not_equal\", \"keras.ops.numpy.not_equal\"]\n )\n def not_equal(x1, x2):\n     \"\"\"Return `(x1 != x2)` element-wise.\n@@ -3814,8 +3814,8 @@ class OnesLike(Operation):\n         return KerasTensor(x.shape, dtype=dtype)\n \n \n-@keras_core_export(\n-    [\"keras_core.ops.ones_like\", \"keras_core.ops.numpy.ones_like\"]\n+@keras_export(\n+    [\"keras.ops.ones_like\", \"keras.ops.numpy.ones_like\"]\n )\n def ones_like(x, dtype=None):\n     \"\"\"Return a tensor of ones with the same shape and type of `x`.\n@@ -3842,10 +3842,10 @@ class ZerosLike(Operation):\n         return KerasTensor(x.shape, dtype=dtype)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.ops.zeros_like\",\n-        \"keras_core.ops.numpy.zeros_like\",\n+        \"keras.ops.zeros_like\",\n+        \"keras.ops.numpy.zeros_like\",\n     ]\n )\n def zeros_like(x, dtype=None):\n@@ -3882,7 +3882,7 @@ class Outer(Operation):\n         return KerasTensor(output_shape, dtype=x1.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.outer\", \"keras_core.ops.numpy.outer\"])\n+@keras_export([\"keras.ops.outer\", \"keras.ops.numpy.outer\"])\n def outer(x1, x2):\n     \"\"\"Compute the outer product of two vectors.\n \n@@ -3951,7 +3951,7 @@ class Pad(Operation):\n         return KerasTensor(output_shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.pad\", \"keras_core.ops.numpy.pad\"])\n+@keras_export([\"keras.ops.pad\", \"keras.ops.numpy.pad\"])\n def pad(x, pad_width, mode=\"constant\"):\n     \"\"\"Pad a tensor.\n \n@@ -4011,7 +4011,7 @@ class Prod(Operation):\n         )\n \n \n-@keras_core_export([\"keras_core.ops.prod\", \"keras_core.ops.numpy.prod\"])\n+@keras_export([\"keras.ops.prod\", \"keras.ops.numpy.prod\"])\n def prod(x, axis=None, keepdims=False, dtype=None):\n     \"\"\"Return the product of tensor elements over a given axis.\n \n@@ -4046,7 +4046,7 @@ class Ravel(Operation):\n         return KerasTensor(output_shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.ravel\", \"keras_core.ops.numpy.ravel\"])\n+@keras_export([\"keras.ops.ravel\", \"keras.ops.numpy.ravel\"])\n def ravel(x):\n     \"\"\"Return a contiguous flattened tensor.\n \n@@ -4071,7 +4071,7 @@ class Real(Operation):\n         return KerasTensor(x.shape)\n \n \n-@keras_core_export([\"keras_core.ops.real\", \"keras_core.ops.numpy.real\"])\n+@keras_export([\"keras.ops.real\", \"keras.ops.numpy.real\"])\n def real(x):\n     \"\"\"Return the real part of the complex argument.\n \n@@ -4094,10 +4094,10 @@ class Reciprocal(Operation):\n         return KerasTensor(x.shape)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.ops.reciprocal\",\n-        \"keras_core.ops.numpy.reciprocal\",\n+        \"keras.ops.reciprocal\",\n+        \"keras.ops.numpy.reciprocal\",\n     ]\n )\n def reciprocal(x):\n@@ -4150,7 +4150,7 @@ class Repeat(Operation):\n         return KerasTensor(output_shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.repeat\", \"keras_core.ops.numpy.repeat\"])\n+@keras_export([\"keras.ops.repeat\", \"keras.ops.numpy.repeat\"])\n def repeat(x, repeats, axis=None):\n     \"\"\"Repeat each element of a tensor after themselves.\n \n@@ -4183,7 +4183,7 @@ class Reshape(Operation):\n         return KerasTensor(output_shape, dtype=x.dtype, sparse=x.sparse)\n \n \n-@keras_core_export([\"keras_core.ops.reshape\", \"keras_core.ops.numpy.reshape\"])\n+@keras_export([\"keras.ops.reshape\", \"keras.ops.numpy.reshape\"])\n def reshape(x, new_shape):\n     \"\"\"Gives a new shape to a tensor without changing its data.\n \n@@ -4214,7 +4214,7 @@ class Roll(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.roll\", \"keras_core.ops.numpy.roll\"])\n+@keras_export([\"keras.ops.roll\", \"keras.ops.numpy.roll\"])\n def roll(x, shift, axis=None):\n     \"\"\"Roll tensor elements along a given axis.\n \n@@ -4247,7 +4247,7 @@ class Round(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.round\", \"keras_core.ops.numpy.round\"])\n+@keras_export([\"keras.ops.round\", \"keras.ops.numpy.round\"])\n def round(x, decimals=0):\n     \"\"\"Evenly round to the given number of decimals.\n \n@@ -4271,7 +4271,7 @@ class Sign(Operation):\n         return KerasTensor(x.shape, dtype=\"int32\")\n \n \n-@keras_core_export([\"keras_core.ops.sign\", \"keras_core.ops.numpy.sign\"])\n+@keras_export([\"keras.ops.sign\", \"keras.ops.numpy.sign\"])\n def sign(x):\n     \"\"\"Returns a tensor with the signs of the elements of `x`.\n \n@@ -4294,7 +4294,7 @@ class Sin(Operation):\n         return KerasTensor(x.shape)\n \n \n-@keras_core_export([\"keras_core.ops.sin\", \"keras_core.ops.numpy.sin\"])\n+@keras_export([\"keras.ops.sin\", \"keras.ops.numpy.sin\"])\n def sin(x):\n     \"\"\"Trigonomeric sine, element-wise.\n \n@@ -4317,7 +4317,7 @@ class Sinh(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.sinh\", \"keras_core.ops.numpy.sinh\"])\n+@keras_export([\"keras.ops.sinh\", \"keras.ops.numpy.sinh\"])\n def sinh(x):\n     \"\"\"Hyperbolic sine, element-wise.\n \n@@ -4340,7 +4340,7 @@ class Size(Operation):\n         return KerasTensor([], dtype=\"int32\")\n \n \n-@keras_core_export([\"keras_core.ops.size\", \"keras_core.ops.numpy.size\"])\n+@keras_export([\"keras.ops.size\", \"keras.ops.numpy.size\"])\n def size(x):\n     \"\"\"Return the number of elements in a tensor.\n \n@@ -4367,7 +4367,7 @@ class Sort(Operation):\n         return KerasTensor(x.shape, x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.sort\", \"keras_core.ops.numpy.sort\"])\n+@keras_export([\"keras.ops.sort\", \"keras.ops.numpy.sort\"])\n def sort(x, axis=-1):\n     \"\"\"Sorts the elements of `x` along a given axis in ascending order.\n \n@@ -4429,7 +4429,7 @@ class Split(Operation):\n         return outputs\n \n \n-@keras_core_export([\"keras_core.ops.split\", \"keras_core.ops.numpy.split\"])\n+@keras_export([\"keras.ops.split\", \"keras.ops.numpy.split\"])\n def split(x, indices_or_sections, axis=0):\n     \"\"\"Split a tensor into chunks.\n \n@@ -4485,7 +4485,7 @@ class Stack(Operation):\n         return KerasTensor(output_shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.stack\", \"keras_core.ops.numpy.stack\"])\n+@keras_export([\"keras.ops.stack\", \"keras.ops.numpy.stack\"])\n def stack(x, axis=0):\n     \"\"\"Join a sequence of tensors along a new axis.\n \n@@ -4522,7 +4522,7 @@ class Std(Operation):\n         )\n \n \n-@keras_core_export([\"keras_core.ops.std\", \"keras_core.ops.numpy.std\"])\n+@keras_export([\"keras.ops.std\", \"keras.ops.numpy.std\"])\n def std(x, axis=None, keepdims=False):\n     \"\"\"Compute the standard deviation along the specified axis.\n \n@@ -4560,7 +4560,7 @@ class Swapaxes(Operation):\n         return KerasTensor(x_shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.swapaxes\", \"keras_core.ops.numpy.swapaxes\"])\n+@keras_export([\"keras.ops.swapaxes\", \"keras.ops.numpy.swapaxes\"])\n def swapaxes(x, axis1, axis2):\n     \"\"\"Interchange two axes of a tensor.\n \n@@ -4600,7 +4600,7 @@ class Take(Operation):\n         return KerasTensor(output_shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.take\", \"keras_core.ops.numpy.take\"])\n+@keras_export([\"keras.ops.take\", \"keras.ops.numpy.take\"])\n def take(x, indices, axis=None):\n     \"\"\"Take elements from a tensor along an axis.\n \n@@ -4652,10 +4652,10 @@ class TakeAlongAxis(Operation):\n         return KerasTensor(output_shape, dtype=x.dtype)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.ops.take_along_axis\",\n-        \"keras_core.ops.numpy.take_along_axis\",\n+        \"keras.ops.take_along_axis\",\n+        \"keras.ops.numpy.take_along_axis\",\n     ]\n )\n def take_along_axis(x, indices, axis=None):\n@@ -4683,7 +4683,7 @@ class Tan(Operation):\n         return KerasTensor(x.shape)\n \n \n-@keras_core_export([\"keras_core.ops.tan\", \"keras_core.ops.numpy.tan\"])\n+@keras_export([\"keras.ops.tan\", \"keras.ops.numpy.tan\"])\n def tan(x):\n     \"\"\"Compute tangent, element-wise.\n \n@@ -4706,7 +4706,7 @@ class Tanh(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.tanh\", \"keras_core.ops.numpy.tanh\"])\n+@keras_export([\"keras.ops.tanh\", \"keras.ops.numpy.tanh\"])\n def tanh(x):\n     \"\"\"Hyperbolic tangent, element-wise.\n \n@@ -4762,8 +4762,8 @@ class Tensordot(Operation):\n         return KerasTensor(output_shape, dtype=x1.dtype)\n \n \n-@keras_core_export(\n-    [\"keras_core.ops.tensordot\", \"keras_core.ops.numpy.tensordot\"]\n+@keras_export(\n+    [\"keras.ops.tensordot\", \"keras.ops.numpy.tensordot\"]\n )\n def tensordot(x1, x2, axes=2):\n     \"\"\"Compute the tensor dot product along specified axes.\n@@ -4811,7 +4811,7 @@ class Tile(Operation):\n         return KerasTensor(output_shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.tile\", \"keras_core.ops.numpy.tile\"])\n+@keras_export([\"keras.ops.tile\", \"keras.ops.numpy.tile\"])\n def tile(x, repeats):\n     \"\"\"Repeat `x` the number of times given by `repeats`.\n \n@@ -4857,7 +4857,7 @@ class Trace(Operation):\n         return KerasTensor(output_shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.trace\", \"keras_core.ops.numpy.trace\"])\n+@keras_export([\"keras.ops.trace\", \"keras.ops.numpy.trace\"])\n def trace(x, offset=0, axis1=0, axis2=1):\n     \"\"\"Return the sum along diagonals of the tensor.\n \n@@ -4900,7 +4900,7 @@ class Tri(Operation):\n         return KerasTensor((N, M), dtype=dtype)\n \n \n-@keras_core_export([\"keras_core.ops.tri\", \"keras_core.ops.numpy.tri\"])\n+@keras_export([\"keras.ops.tri\", \"keras.ops.numpy.tri\"])\n def tri(N, M=None, k=0, dtype=\"float32\"):\n     \"\"\"Return a tensor with ones at and below a diagonal and zeros elsewhere.\n \n@@ -4931,7 +4931,7 @@ class Tril(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.tril\", \"keras_core.ops.numpy.tril\"])\n+@keras_export([\"keras.ops.tril\", \"keras.ops.numpy.tril\"])\n def tril(x, k=0):\n     \"\"\"Return lower triangle of a tensor.\n \n@@ -4963,7 +4963,7 @@ class Triu(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.triu\", \"keras_core.ops.numpy.triu\"])\n+@keras_export([\"keras.ops.triu\", \"keras.ops.numpy.triu\"])\n def triu(x, k=0):\n     \"\"\"Return upper triangle of a tensor.\n \n@@ -4991,7 +4991,7 @@ class Vdot(Operation):\n         return KerasTensor([], dtype=x1.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.vdot\", \"keras_core.ops.numpy.vdot\"])\n+@keras_export([\"keras.ops.vdot\", \"keras.ops.numpy.vdot\"])\n def vdot(x1, x2):\n     \"\"\"Return the dot product of two vectors.\n \n@@ -5037,7 +5037,7 @@ class Vstack(Operation):\n         return KerasTensor(output_shape)\n \n \n-@keras_core_export([\"keras_core.ops.vstack\", \"keras_core.ops.numpy.vstack\"])\n+@keras_export([\"keras.ops.vstack\", \"keras.ops.numpy.vstack\"])\n def vstack(xs):\n     \"\"\"Stack tensors in sequence vertically (row wise).\n \n@@ -5066,7 +5066,7 @@ class Where(Operation):\n         return KerasTensor(output_shape, dtype=output_dtype)\n \n \n-@keras_core_export([\"keras_core.ops.where\", \"keras_core.ops.numpy.where\"])\n+@keras_export([\"keras.ops.where\", \"keras.ops.numpy.where\"])\n def where(condition, x1=None, x2=None):\n     \"\"\"Return elements chosen from `x1` or `x2` depending on `condition`.\n \n@@ -5103,7 +5103,7 @@ class Subtract(Operation):\n         return KerasTensor(output_shape, dtype=x1.dtype, sparse=output_sparse)\n \n \n-@keras_core_export([\"keras_core.ops.subtract\", \"keras_core.ops.numpy.subtract\"])\n+@keras_export([\"keras.ops.subtract\", \"keras.ops.numpy.subtract\"])\n def subtract(x1, x2):\n     \"\"\"Subtract arguments element-wise.\n \n@@ -5133,7 +5133,7 @@ class Multiply(Operation):\n         return KerasTensor(output_shape, dtype=x1.dtype, sparse=output_sparse)\n \n \n-@keras_core_export([\"keras_core.ops.multiply\", \"keras_core.ops.numpy.multiply\"])\n+@keras_export([\"keras.ops.multiply\", \"keras.ops.numpy.multiply\"])\n def multiply(x1, x2):\n     \"\"\"Multiply arguments element-wise.\n \n@@ -5160,11 +5160,11 @@ class Divide(Operation):\n         return KerasTensor(output_shape, dtype=x1.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.divide\", \"keras_core.ops.numpy.divide\"])\n+@keras_export([\"keras.ops.divide\", \"keras.ops.numpy.divide\"])\n def divide(x1, x2):\n     \"\"\"Divide arguments element-wise.\n \n-    `keras_core.ops.true_divide` is an alias for this function.\n+    `keras.ops.true_divide` is an alias for this function.\n \n     Args:\n         x1: First input tensor.\n@@ -5189,14 +5189,14 @@ class TrueDivide(Operation):\n         return KerasTensor(output_shape, dtype=x1.dtype)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.ops.true_divide\",\n-        \"keras_core.ops.numpy.true_divide\",\n+        \"keras.ops.true_divide\",\n+        \"keras.ops.numpy.true_divide\",\n     ]\n )\n def true_divide(x1, x2):\n-    \"\"\"Alias for `keras_core.ops.divide`.\"\"\"\n+    \"\"\"Alias for `keras.ops.divide`.\"\"\"\n     if any_symbolic_tensors((x1, x2)):\n         return TrueDivide().symbolic_call(x1, x2)\n     return backend.numpy.true_divide(x1, x2)\n@@ -5213,7 +5213,7 @@ class Power(Operation):\n         return KerasTensor(output_shape, dtype=x1.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.power\", \"keras_core.ops.numpy.power\"])\n+@keras_export([\"keras.ops.power\", \"keras.ops.numpy.power\"])\n def power(x1, x2):\n     \"\"\"First tensor elements raised to powers from second tensor, element-wise.\n \n@@ -5237,7 +5237,7 @@ class Negative(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.negative\", \"keras_core.ops.numpy.negative\"])\n+@keras_export([\"keras.ops.negative\", \"keras.ops.numpy.negative\"])\n def negative(x):\n     \"\"\"Numerical negative, element-wise.\n \n@@ -5260,7 +5260,7 @@ class Square(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.square\", \"keras_core.ops.numpy.square\"])\n+@keras_export([\"keras.ops.square\", \"keras.ops.numpy.square\"])\n def square(x):\n     \"\"\"Return the element-wise square of the input.\n \n@@ -5284,7 +5284,7 @@ class Sqrt(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.sqrt\", \"keras_core.ops.numpy.sqrt\"])\n+@keras_export([\"keras.ops.sqrt\", \"keras.ops.numpy.sqrt\"])\n def sqrt(x):\n     \"\"\"Return the non-negative square root of a tensor, element-wise.\n \n@@ -5323,7 +5323,7 @@ class Squeeze(Operation):\n             return KerasTensor(input_shape, dtype=x.dtype, sparse=x.sparse)\n \n \n-@keras_core_export([\"keras_core.ops.squeeze\", \"keras_core.ops.numpy.squeeze\"])\n+@keras_export([\"keras.ops.squeeze\", \"keras.ops.numpy.squeeze\"])\n def squeeze(x, axis=None):\n     \"\"\"Remove axes of length one from `x`.\n \n@@ -5364,8 +5364,8 @@ class Transpose(Operation):\n         return KerasTensor(output_shape, dtype=x.dtype, sparse=x.sparse)\n \n \n-@keras_core_export(\n-    [\"keras_core.ops.transpose\", \"keras_core.ops.numpy.transpose\"]\n+@keras_export(\n+    [\"keras.ops.transpose\", \"keras.ops.numpy.transpose\"]\n )\n def transpose(x, axes=None):\n     \"\"\"Returns a tensor with `axes` transposed.\n@@ -5401,7 +5401,7 @@ class Mean(Operation):\n         )\n \n \n-@keras_core_export([\"keras_core.ops.mean\", \"keras_core.ops.numpy.mean\"])\n+@keras_export([\"keras.ops.mean\", \"keras.ops.numpy.mean\"])\n def mean(x, axis=None, keepdims=False):\n     \"\"\"Compute the arithmetic mean along the specified axes.\n \n@@ -5438,7 +5438,7 @@ class Var(Operation):\n         )\n \n \n-@keras_core_export([\"keras_core.ops.var\", \"keras_core.ops.numpy.var\"])\n+@keras_export([\"keras.ops.var\", \"keras.ops.numpy.var\"])\n def var(x, axis=None, keepdims=False):\n     \"\"\"Compute the variance along the specified axes.\n \n@@ -5475,7 +5475,7 @@ class Sum(Operation):\n         )\n \n \n-@keras_core_export([\"keras_core.ops.sum\", \"keras_core.ops.numpy.sum\"])\n+@keras_export([\"keras.ops.sum\", \"keras.ops.numpy.sum\"])\n def sum(x, axis=None, keepdims=False):\n     \"\"\"Sum of a tensor over the given axes.\n \n@@ -5502,7 +5502,7 @@ class Zeros(Operation):\n         return KerasTensor(shape, dtype=dtype)\n \n \n-@keras_core_export([\"keras_core.ops.zeros\", \"keras_core.ops.numpy.zeros\"])\n+@keras_export([\"keras.ops.zeros\", \"keras.ops.numpy.zeros\"])\n def zeros(shape, dtype=\"float32\"):\n     \"\"\"Return a new tensor of given shape and type, filled with zeros.\n \n@@ -5524,7 +5524,7 @@ class Ones(Operation):\n         return KerasTensor(shape, dtype=dtype)\n \n \n-@keras_core_export([\"keras_core.ops.ones\", \"keras_core.ops.numpy.ones\"])\n+@keras_export([\"keras.ops.ones\", \"keras.ops.numpy.ones\"])\n def ones(shape, dtype=\"float32\"):\n     \"\"\"Return a new tensor of given shape and type, filled with ones.\n \n@@ -5548,7 +5548,7 @@ class Eye(Operation):\n         return KerasTensor((N, M), dtype=dtype)\n \n \n-@keras_core_export([\"keras_core.ops.eye\", \"keras_core.ops.numpy.eye\"])\n+@keras_export([\"keras.ops.eye\", \"keras.ops.numpy.eye\"])\n def eye(N, M=None, k=0, dtype=\"float32\"):\n     \"\"\"Return a 2-D tensor with ones on the diagonal and zeros elsewhere.\n \n@@ -5577,8 +5577,8 @@ class FloorDivide(Operation):\n         return KerasTensor(output_shape, dtype=x1.dtype)\n \n \n-@keras_core_export(\n-    [\"keras_core.ops.floor_divide\", \"keras_core.ops.numpy.floor_divide\"]\n+@keras_export(\n+    [\"keras.ops.floor_divide\", \"keras.ops.numpy.floor_divide\"]\n )\n def floor_divide(x1, x2):\n     \"\"\"Returns the largest integer smaller or equal to the division of inputs.\n@@ -5606,8 +5606,8 @@ class LogicalXor(Operation):\n         return KerasTensor(output_shape, dtype=x1.dtype)\n \n \n-@keras_core_export(\n-    [\"keras_core.ops.logical_xor\", \"keras_core.ops.numpy.logical_xor\"]\n+@keras_export(\n+    [\"keras.ops.logical_xor\", \"keras.ops.numpy.logical_xor\"]\n )\n def logical_xor(x1, x2):\n     \"\"\"Compute the truth value of `x1 XOR x2`, element-wise.\n\n@@ -3,11 +3,11 @@ import pytest\n from absl.testing import parameterized\n from tensorflow.python.ops.numpy_ops import np_config\n \n-from keras_core import backend\n-from keras_core import testing\n-from keras_core.backend.common import standardize_dtype\n-from keras_core.backend.common.keras_tensor import KerasTensor\n-from keras_core.ops import numpy as knp\n+from keras import backend\n+from keras import testing\n+from keras.backend.common import standardize_dtype\n+from keras.backend.common.keras_tensor import KerasTensor\n+from keras.ops import numpy as knp\n \n # TODO: remove reliance on this (or alternatively, turn it on by default).\n np_config.enable_numpy_behavior()\n\n@@ -3,16 +3,16 @@ import textwrap\n \n import tree\n \n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.backend.common.keras_tensor import any_symbolic_tensors\n-from keras_core.ops.node import Node\n-from keras_core.utils import python_utils\n-from keras_core.utils import traceback_utils\n-from keras_core.utils.naming import auto_name\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.backend.common.keras_tensor import any_symbolic_tensors\n+from keras.ops.node import Node\n+from keras.utils import python_utils\n+from keras.utils import traceback_utils\n+from keras.utils.naming import auto_name\n \n \n-@keras_core_export(\"keras_core.Operation\")\n+@keras_export(\"keras.Operation\")\n class Operation:\n     def __init__(self, name=None):\n         if name is None:\n@@ -105,7 +105,7 @@ class Operation:\n         try:\n             instance._lock = False\n             if auto_config:\n-                from keras_core.saving import serialization_lib\n+                from keras.saving import serialization_lib\n \n                 instance._auto_config = serialization_lib.SerializableDict(\n                     **kwargs\n\n@@ -1,10 +1,10 @@\n import numpy as np\n \n-from keras_core import backend\n-from keras_core import testing\n-from keras_core.backend.common import keras_tensor\n-from keras_core.ops import numpy as knp\n-from keras_core.ops import operation\n+from keras import backend\n+from keras import testing\n+from keras.backend.common import keras_tensor\n+from keras.ops import numpy as knp\n+from keras.ops import operation\n \n \n class OpWithMultipleInputs(operation.Operation):\n\n@@ -3,7 +3,7 @@ import math\n import numpy as np\n import tree\n \n-from keras_core.api_export import keras_core_export\n+from keras.api_export import keras_export\n \n \n def compute_pooling_output_shape(\n@@ -236,7 +236,7 @@ def reduce_shape(shape, axis=None, keepdims=False):\n         return tuple(shape)\n \n \n-@keras_core_export(\"keras_core.utils.get_source_inputs\")\n+@keras_export(\"keras.utils.get_source_inputs\")\n def get_source_inputs(tensor):\n     \"\"\"Returns the list of input tensors necessary to compute `tensor`.\n \n\n@@ -1,8 +1,8 @@\n-from keras_core import backend\n-from keras_core import ops\n-from keras_core import testing\n-from keras_core.layers.core import input_layer\n-from keras_core.ops import operation_utils\n+from keras import backend\n+from keras import ops\n+from keras import testing\n+from keras.layers.core import input_layer\n+from keras.ops import operation_utils\n \n \n class OperationUtilsTest(testing.TestCase):\n\n@@ -1,6 +1,6 @@\n import tree\n \n-from keras_core.backend import KerasTensor\n+from keras.backend import KerasTensor\n \n \n class SymbolicArguments:\n\n@@ -1,8 +1,8 @@\n import tree\n \n-from keras_core import testing\n-from keras_core.backend import KerasTensor\n-from keras_core.ops.symbolic_arguments import SymbolicArguments\n+from keras import testing\n+from keras.backend import KerasTensor\n+from keras.ops.symbolic_arguments import SymbolicArguments\n \n \n class SymbolicArgumentsTest(testing.TestCase):\n\n@@ -1,18 +1,18 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.optimizers.adadelta import Adadelta\n-from keras_core.optimizers.adafactor import Adafactor\n-from keras_core.optimizers.adagrad import Adagrad\n-from keras_core.optimizers.adam import Adam\n-from keras_core.optimizers.adamax import Adamax\n-from keras_core.optimizers.adamw import AdamW\n-from keras_core.optimizers.ftrl import Ftrl\n-from keras_core.optimizers.lion import Lion\n-from keras_core.optimizers.loss_scale_optimizer import LossScaleOptimizer\n-from keras_core.optimizers.nadam import Nadam\n-from keras_core.optimizers.optimizer import Optimizer\n-from keras_core.optimizers.rmsprop import RMSprop\n-from keras_core.optimizers.sgd import SGD\n-from keras_core.saving import serialization_lib\n+from keras.api_export import keras_export\n+from keras.optimizers.adadelta import Adadelta\n+from keras.optimizers.adafactor import Adafactor\n+from keras.optimizers.adagrad import Adagrad\n+from keras.optimizers.adam import Adam\n+from keras.optimizers.adamax import Adamax\n+from keras.optimizers.adamw import AdamW\n+from keras.optimizers.ftrl import Ftrl\n+from keras.optimizers.lion import Lion\n+from keras.optimizers.loss_scale_optimizer import LossScaleOptimizer\n+from keras.optimizers.nadam import Nadam\n+from keras.optimizers.optimizer import Optimizer\n+from keras.optimizers.rmsprop import RMSprop\n+from keras.optimizers.sgd import SGD\n+from keras.saving import serialization_lib\n \n ALL_OBJECTS = {\n     Optimizer,\n@@ -32,7 +32,7 @@ ALL_OBJECTS = {\n ALL_OBJECTS_DICT = {cls.__name__.lower(): cls for cls in ALL_OBJECTS}\n \n \n-@keras_core_export(\"keras_core.optimizers.serialize\")\n+@keras_export(\"keras.optimizers.serialize\")\n def serialize(optimizer):\n     \"\"\"Returns the optimizer configuration as a Python dict.\n \n@@ -45,7 +45,7 @@ def serialize(optimizer):\n     return serialization_lib.serialize_keras_object(optimizer)\n \n \n-@keras_core_export(\"keras_core.optimizers.deserialize\")\n+@keras_export(\"keras.optimizers.deserialize\")\n def deserialize(config, custom_objects=None):\n     \"\"\"Returns a Keras optimizer object via its configuration.\n \n@@ -69,7 +69,7 @@ def deserialize(config, custom_objects=None):\n     )\n \n \n-@keras_core_export(\"keras_core.optimizers.get\")\n+@keras_export(\"keras.optimizers.get\")\n def get(identifier):\n     \"\"\"Retrieves a Keras Optimizer instance.\n \n\n@@ -1,9 +1,9 @@\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.optimizers import optimizer\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.optimizers import optimizer\n \n \n-@keras_core_export([\"keras_core.optimizers.Adadelta\"])\n+@keras_export([\"keras.optimizers.Adadelta\"])\n class Adadelta(optimizer.Optimizer):\n     \"\"\"Optimizer that implements the Adadelta algorithm.\n \n@@ -22,7 +22,7 @@ class Adadelta(optimizer.Optimizer):\n \n     Args:\n         learning_rate: A float, a\n-            `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n+            `keras.optimizers.schedules.LearningRateSchedule` instance, or\n             a callable that takes no arguments and returns the actual value to\n             use. The learning rate. Defaults to `0.001`. Note that `Adadelta`\n             tends to benefit from higher initial learning rate values compared\n\n@@ -1,9 +1,9 @@\n import numpy as np\n \n-from keras_core import backend\n-from keras_core import ops\n-from keras_core import testing\n-from keras_core.optimizers.adadelta import Adadelta\n+from keras import backend\n+from keras import ops\n+from keras import testing\n+from keras.optimizers.adadelta import Adadelta\n \n \n class AdadeltaTest(testing.TestCase):\n\n@@ -1,10 +1,10 @@\n-from keras_core import backend\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.optimizers import optimizer\n+from keras import backend\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.optimizers import optimizer\n \n \n-@keras_core_export([\"keras_core.optimizers.Adafactor\"])\n+@keras_export([\"keras.optimizers.Adafactor\"])\n class Adafactor(optimizer.Optimizer):\n     \"\"\"Optimizer that implements the Adafactor algorithm.\n \n@@ -18,7 +18,7 @@ class Adafactor(optimizer.Optimizer):\n \n     Args:\n         learning_rate: A float, a\n-            `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n+            `keras.optimizers.schedules.LearningRateSchedule` instance, or\n             a callable that takes no arguments and returns the actual value to\n             use. The learning rate. Defaults to `0.001`.\n         beta_2_decay: float, defaults to -0.8. The decay rate of `beta_2`.\n\n@@ -3,9 +3,9 @@\n \n import numpy as np\n \n-from keras_core import backend\n-from keras_core import testing\n-from keras_core.optimizers.adafactor import Adafactor\n+from keras import backend\n+from keras import testing\n+from keras.optimizers.adafactor import Adafactor\n \n \n class AdafactorTest(testing.TestCase):\n\n@@ -1,10 +1,10 @@\n-from keras_core import initializers\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.optimizers import optimizer\n+from keras import initializers\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.optimizers import optimizer\n \n \n-@keras_core_export([\"keras_core.optimizers.Adagrad\"])\n+@keras_export([\"keras.optimizers.Adagrad\"])\n class Adagrad(optimizer.Optimizer):\n     \"\"\"Optimizer that implements the Adagrad algorithm.\n \n@@ -15,7 +15,7 @@ class Adagrad(optimizer.Optimizer):\n \n     Args:\n         learning_rate: A float, a\n-            `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n+            `keras.optimizers.schedules.LearningRateSchedule` instance, or\n             a callable that takes no arguments and returns the actual value to\n             use. The learning rate. Defaults to `0.001`. Note that `Adagrad`\n             tends to benefit from higher initial learning rate values compared\n\n@@ -3,10 +3,10 @@\n \n import numpy as np\n \n-from keras_core import backend\n-from keras_core import ops\n-from keras_core import testing\n-from keras_core.optimizers.adagrad import Adagrad\n+from keras import backend\n+from keras import ops\n+from keras import testing\n+from keras.optimizers.adagrad import Adagrad\n \n \n class AdagradTest(testing.TestCase):\n\n@@ -1,9 +1,9 @@\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.optimizers import optimizer\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.optimizers import optimizer\n \n \n-@keras_core_export([\"keras_core.optimizers.Adam\"])\n+@keras_export([\"keras.optimizers.Adam\"])\n class Adam(optimizer.Optimizer):\n     \"\"\"Optimizer that implements the Adam algorithm.\n \n@@ -19,7 +19,7 @@ class Adam(optimizer.Optimizer):\n \n     Args:\n         learning_rate: A float, a\n-            `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n+            `keras.optimizers.schedules.LearningRateSchedule` instance, or\n             a callable that takes no arguments and returns the actual value to\n             use. The learning rate. Defaults to `0.001`.\n         beta_1: A float value or a constant float tensor, or a callable\n\n@@ -1,11 +1,11 @@\n import numpy as np\n import pytest\n \n-import keras_core\n-from keras_core import backend\n-from keras_core import ops\n-from keras_core import testing\n-from keras_core.optimizers.adam import Adam\n+import keras\n+from keras import backend\n+from keras import ops\n+from keras import testing\n+from keras.optimizers.adam import Adam\n \n \n class AdamTest(testing.TestCase):\n@@ -79,8 +79,8 @@ class AdamTest(testing.TestCase):\n     @pytest.mark.requires_trainable_backend\n     def test_ema(self):\n         # TODO: test correctness\n-        model = keras_core.Sequential([keras_core.layers.Dense(10)])\n+        model = keras.Sequential([keras.layers.Dense(10)])\n         model.compile(optimizer=Adam(use_ema=True), loss=\"mse\")\n-        x = keras_core.ops.zeros((1, 5))\n-        y = keras_core.ops.zeros((1, 10))\n+        x = keras.ops.zeros((1, 5))\n+        y = keras.ops.zeros((1, 10))\n         model.fit(x, y)\n\n@@ -1,9 +1,9 @@\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.optimizers import optimizer\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.optimizers import optimizer\n \n \n-@keras_core_export([\"keras_core.optimizers.Adamax\"])\n+@keras_export([\"keras.optimizers.Adamax\"])\n class Adamax(optimizer.Optimizer):\n     \"\"\"Optimizer that implements the Adamax algorithm.\n \n@@ -35,7 +35,7 @@ class Adamax(optimizer.Optimizer):\n \n     Args:\n         learning_rate: A float, a\n-            `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n+            `keras.optimizers.schedules.LearningRateSchedule` instance, or\n             a callable that takes no arguments and returns the actual value to\n             use. The learning rate. Defaults to `0.001`.\n         beta_1: A float value or a constant float tensor. The exponential decay\n\n@@ -3,10 +3,10 @@\n \n import numpy as np\n \n-from keras_core import backend\n-from keras_core import ops\n-from keras_core import testing\n-from keras_core.optimizers.adamax import Adamax\n+from keras import backend\n+from keras import ops\n+from keras import testing\n+from keras.optimizers.adamax import Adamax\n \n \n class AdamaxTest(testing.TestCase):\n\n@@ -1,9 +1,9 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.optimizers import adam\n-from keras_core.optimizers import optimizer\n+from keras.api_export import keras_export\n+from keras.optimizers import adam\n+from keras.optimizers import optimizer\n \n \n-@keras_core_export([\"keras_core.optimizers.AdamW\"])\n+@keras_export([\"keras.optimizers.AdamW\"])\n class AdamW(adam.Adam):\n     \"\"\"Optimizer that implements the AdamW algorithm.\n \n@@ -22,7 +22,7 @@ class AdamW(adam.Adam):\n \n     Args:\n         learning_rate: A float, a\n-            `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n+            `keras.optimizers.schedules.LearningRateSchedule` instance, or\n             a callable that takes no arguments and returns the actual value to\n             use. The learning rate. Defaults to `0.001`.\n         beta_1: A float value or a constant float tensor, or a callable\n\n@@ -3,10 +3,10 @@\n \n import numpy as np\n \n-from keras_core import backend\n-from keras_core import ops\n-from keras_core import testing\n-from keras_core.optimizers.adamw import AdamW\n+from keras import backend\n+from keras import ops\n+from keras import testing\n+from keras.optimizers.adamw import AdamW\n \n \n class AdamWTest(testing.TestCase):\n\n@@ -1,13 +1,13 @@\n import re\n import warnings\n \n-from keras_core import backend\n-from keras_core import initializers\n-from keras_core import ops\n-from keras_core.optimizers.schedules import learning_rate_schedule\n-from keras_core.saving import serialization_lib\n-from keras_core.utils import tracking\n-from keras_core.utils.naming import auto_name\n+from keras import backend\n+from keras import initializers\n+from keras import ops\n+from keras.optimizers.schedules import learning_rate_schedule\n+from keras.saving import serialization_lib\n+from keras.utils import tracking\n+from keras.utils.naming import auto_name\n \n \n class BaseOptimizer:\n@@ -688,7 +688,7 @@ base_optimizer_keyword_args = \"\"\"name: String. The name to use\n           be multiplied the loss before computing gradients, and the inverse of\n           the scale factor will be multiplied by the gradients before updating\n           variables. Useful for preventing underflow during mixed precision\n-          training. Alternately, `keras_core.optimizers.LossScaleOptimizer` will\n+          training. Alternately, `keras.optimizers.LossScaleOptimizer` will\n           automatically set a loss scale factor.\n \"\"\"\n \n\n@@ -1,10 +1,10 @@\n-from keras_core import initializers\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.optimizers import optimizer\n+from keras import initializers\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.optimizers import optimizer\n \n \n-@keras_core_export([\"keras_core.optimizers.Ftrl\"])\n+@keras_export([\"keras.optimizers.Ftrl\"])\n class Ftrl(optimizer.Optimizer):\n     r\"\"\"Optimizer that implements the FTRL algorithm.\n \n@@ -53,7 +53,7 @@ class Ftrl(optimizer.Optimizer):\n \n     Args:\n         learning_rate: A float, a\n-            `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n+            `keras.optimizers.schedules.LearningRateSchedule` instance, or\n             a callable that takes no arguments and returns the actual value to\n             use. The learning rate. Defaults to `0.001`.\n         learning_rate_power: A float value, must be less or equal to zero.\n\n@@ -3,9 +3,9 @@\n \n import numpy as np\n \n-from keras_core import backend\n-from keras_core import testing\n-from keras_core.optimizers.ftrl import Ftrl\n+from keras import backend\n+from keras import testing\n+from keras.optimizers.ftrl import Ftrl\n \n \n class FtrlTest(testing.TestCase):\n\n@@ -1,9 +1,9 @@\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.optimizers import optimizer\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.optimizers import optimizer\n \n \n-@keras_core_export([\"keras_core.optimizers.Lion\"])\n+@keras_export([\"keras.optimizers.Lion\"])\n class Lion(optimizer.Optimizer):\n     \"\"\"Optimizer that implements the Lion algorithm.\n \n@@ -20,7 +20,7 @@ class Lion(optimizer.Optimizer):\n \n     Args:\n         learning_rate: A float, a\n-            `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n+            `keras.optimizers.schedules.LearningRateSchedule` instance, or\n             a callable that takes no arguments and returns the actual value to\n             use. The learning rate. Defaults to `0.001`.\n         beta_1: A float value or a constant float tensor, or a callable\n\n@@ -1,11 +1,11 @@\n import numpy as np\n import pytest\n \n-import keras_core\n-from keras_core import backend\n-from keras_core import ops\n-from keras_core import testing\n-from keras_core.optimizers.lion import Lion\n+import keras\n+from keras import backend\n+from keras import ops\n+from keras import testing\n+from keras.optimizers.lion import Lion\n \n \n class LionTest(testing.TestCase):\n@@ -78,8 +78,8 @@ class LionTest(testing.TestCase):\n     @pytest.mark.requires_trainable_backend\n     def test_ema(self):\n         # TODO: test correctness\n-        model = keras_core.Sequential([keras_core.layers.Dense(10)])\n+        model = keras.Sequential([keras.layers.Dense(10)])\n         model.compile(optimizer=Lion(use_ema=True), loss=\"mse\")\n-        x = keras_core.ops.zeros((1, 5))\n-        y = keras_core.ops.zeros((1, 10))\n+        x = keras.ops.zeros((1, 5))\n+        y = keras.ops.zeros((1, 10))\n         model.fit(x, y)\n\n@@ -1,16 +1,16 @@\n-from keras_core import backend\n-from keras_core import initializers\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.optimizers import optimizer\n-from keras_core.saving import serialization_lib\n-from keras_core.utils import tracking\n+from keras import backend\n+from keras import initializers\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.optimizers import optimizer\n+from keras.saving import serialization_lib\n+from keras.utils import tracking\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.optimizers.LossScaleOptimizer\",\n-        \"keras_core.mixed_precision.LossScaleOptimizer\",\n+        \"keras.optimizers.LossScaleOptimizer\",\n+        \"keras.mixed_precision.LossScaleOptimizer\",\n     ]\n )\n class LossScaleOptimizer(optimizer.Optimizer):\n@@ -32,7 +32,7 @@ class LossScaleOptimizer(optimizer.Optimizer):\n       is doubled.\n \n     Args:\n-        inner_optimizer: The `keras_core.optimizers.Optimizer` instance to wrap.\n+        inner_optimizer: The `keras.optimizers.Optimizer` instance to wrap.\n         initial_scale: Float. The initial loss scale. This scale will be updated\n             during training. It is recommended for this to be a very high\n             number, because a loss scale that is too high gets lowered far more\n\n@@ -1,11 +1,11 @@\n import numpy as np\n from absl.testing import parameterized\n \n-from keras_core import backend\n-from keras_core import ops\n-from keras_core import testing\n-from keras_core.optimizers.loss_scale_optimizer import LossScaleOptimizer\n-from keras_core.optimizers.sgd import SGD\n+from keras import backend\n+from keras import ops\n+from keras import testing\n+from keras.optimizers.loss_scale_optimizer import LossScaleOptimizer\n+from keras.optimizers.sgd import SGD\n \n \n class LossScaleOptimizerTest(testing.TestCase, parameterized.TestCase):\n\n@@ -1,10 +1,10 @@\n-from keras_core import backend\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.optimizers import optimizer\n+from keras import backend\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.optimizers import optimizer\n \n \n-@keras_core_export([\"keras_core.optimizers.Nadam\"])\n+@keras_export([\"keras.optimizers.Nadam\"])\n class Nadam(optimizer.Optimizer):\n     \"\"\"Optimizer that implements the Nadam algorithm.\n \n@@ -13,7 +13,7 @@ class Nadam(optimizer.Optimizer):\n \n     Args:\n         learning_rate: A float, a\n-            `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n+            `keras.optimizers.schedules.LearningRateSchedule` instance, or\n             a callable that takes no arguments and returns the actual value to\n             use. The learning rate. Defaults to `0.001`.\n         beta_1: A float value or a constant float tensor, or a callable\n\n@@ -3,10 +3,10 @@\n \n import numpy as np\n \n-from keras_core import backend\n-from keras_core import ops\n-from keras_core import testing\n-from keras_core.optimizers.nadam import Nadam\n+from keras import backend\n+from keras import ops\n+from keras import testing\n+from keras.optimizers.nadam import Nadam\n \n \n class NadamTest(testing.TestCase):\n\n@@ -1,20 +1,20 @@\n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.optimizers import base_optimizer\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.optimizers import base_optimizer\n \n if backend.backend() == \"tensorflow\":\n-    from keras_core.backend.tensorflow.optimizer import TFOptimizer\n+    from keras.backend.tensorflow.optimizer import TFOptimizer\n \n     BackendOptimizer = TFOptimizer\n elif backend.backend() == \"torch\":\n-    from keras_core.backend.torch.optimizers import TorchOptimizer\n+    from keras.backend.torch.optimizers import TorchOptimizer\n \n     BackendOptimizer = TorchOptimizer\n else:\n     BackendOptimizer = base_optimizer.BaseOptimizer\n \n \n-@keras_core_export([\"keras_core.Optimizer\", \"keras_core.optimizers.Optimizer\"])\n+@keras_export([\"keras.Optimizer\", \"keras.optimizers.Optimizer\"])\n class Optimizer(BackendOptimizer):\n     pass\n \n\n@@ -1,9 +1,9 @@\n import numpy as np\n \n-from keras_core import backend\n-from keras_core import constraints\n-from keras_core import optimizers\n-from keras_core import testing\n+from keras import backend\n+from keras import constraints\n+from keras import optimizers\n+from keras import testing\n \n \n class OptimizerTest(testing.TestCase):\n\n@@ -1,9 +1,9 @@\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.optimizers import optimizer\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.optimizers import optimizer\n \n \n-@keras_core_export([\"keras_core.optimizers.RMSprop\"])\n+@keras_export([\"keras.optimizers.RMSprop\"])\n class RMSprop(optimizer.Optimizer):\n     \"\"\"Optimizer that implements the RMSprop algorithm.\n \n@@ -19,7 +19,7 @@ class RMSprop(optimizer.Optimizer):\n \n     Args:\n         learning_rate: A float, a\n-            `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n+            `keras.optimizers.schedules.LearningRateSchedule` instance, or\n             a callable that takes no arguments and returns the actual value to\n             use. The learning rate. Defaults to `0.001`.\n         rho: float, defaults to 0.9. Discounting factor for the old gradients.\n@@ -37,8 +37,8 @@ class RMSprop(optimizer.Optimizer):\n \n     Usage:\n \n-    >>> opt = keras_core.optimizers.RMSprop(learning_rate=0.1)\n-    >>> var1 = keras_core.backend.Variable(10.0)\n+    >>> opt = keras.optimizers.RMSprop(learning_rate=0.1)\n+    >>> var1 = keras.backend.Variable(10.0)\n     >>> loss = lambda: (var1 ** 2) / 2.0  # d(loss) / d(var1) = var1\n     >>> opt.minimize(loss, [var1])\n     >>> var1\n\n@@ -1,9 +1,9 @@\n import numpy as np\n \n-from keras_core import backend\n-from keras_core import ops\n-from keras_core import testing\n-from keras_core.optimizers.rmsprop import RMSprop\n+from keras import backend\n+from keras import ops\n+from keras import testing\n+from keras.optimizers.rmsprop import RMSprop\n \n \n class RMSpropTest(testing.TestCase):\n\n@@ -0,0 +1,16 @@\n+from keras.optimizers.schedules.learning_rate_schedule import CosineDecay\n+from keras.optimizers.schedules.learning_rate_schedule import (\n+    CosineDecayRestarts,\n+)\n+from keras.optimizers.schedules.learning_rate_schedule import (\n+    ExponentialDecay,\n+)\n+from keras.optimizers.schedules.learning_rate_schedule import (\n+    InverseTimeDecay,\n+)\n+from keras.optimizers.schedules.learning_rate_schedule import (\n+    PiecewiseConstantDecay,\n+)\n+from keras.optimizers.schedules.learning_rate_schedule import (\n+    PolynomialDecay,\n+)\n\n@@ -2,12 +2,12 @@\n \n import math\n \n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.saving import serialization_lib\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.saving import serialization_lib\n \n \n-@keras_core_export(\"keras_core.optimizers.schedules.LearningRateSchedule\")\n+@keras_export(\"keras.optimizers.schedules.LearningRateSchedule\")\n class LearningRateSchedule:\n     \"\"\"The learning rate schedule base class.\n \n@@ -15,15 +15,15 @@ class LearningRateSchedule:\n     of your optimizer changes over time.\n \n     Several built-in learning rate schedules are available, such as\n-    `keras_core.optimizers.schedules.ExponentialDecay` or\n-    `keras_core.optimizers.schedules.PiecewiseConstantDecay`:\n+    `keras.optimizers.schedules.ExponentialDecay` or\n+    `keras.optimizers.schedules.PiecewiseConstantDecay`:\n \n     ```python\n-    lr_schedule = keras_core.optimizers.schedules.ExponentialDecay(\n+    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n         initial_learning_rate=1e-2,\n         decay_steps=10000,\n         decay_rate=0.9)\n-    optimizer = keras_core.optimizers.SGD(learning_rate=lr_schedule)\n+    optimizer = keras.optimizers.SGD(learning_rate=lr_schedule)\n     ```\n \n     A `LearningRateSchedule` instance can be passed in as the `learning_rate`\n@@ -39,7 +39,7 @@ class LearningRateSchedule:\n     Example:\n \n     ```python\n-    class MyLRSchedule(keras_core.optimizers.schedules.LearningRateSchedule):\n+    class MyLRSchedule(keras.optimizers.schedules.LearningRateSchedule):\n \n         def __init__(self, initial_learning_rate):\n             self.initial_learning_rate = initial_learning_rate\n@@ -47,7 +47,7 @@ class LearningRateSchedule:\n         def __call__(self, step):\n             return self.initial_learning_rate / (step + 1)\n \n-    optimizer = keras_core.optimizers.SGD(learning_rate=MyLRSchedule(0.1))\n+    optimizer = keras.optimizers.SGD(learning_rate=MyLRSchedule(0.1))\n     ```\n     \"\"\"\n \n@@ -76,7 +76,7 @@ class LearningRateSchedule:\n         return cls(**config)\n \n \n-@keras_core_export(\"keras_core.optimizers.schedules.ExponentialDecay\")\n+@keras_export(\"keras.optimizers.schedules.ExponentialDecay\")\n class ExponentialDecay(LearningRateSchedule):\n     \"\"\"A `LearningRateSchedule` that uses an exponential decay schedule.\n \n@@ -98,20 +98,20 @@ class ExponentialDecay(LearningRateSchedule):\n     an integer division and the decayed learning rate follows a\n     staircase function.\n \n-    You can pass this schedule directly into a `keras_core.optimizers.Optimizer`\n+    You can pass this schedule directly into a `keras.optimizers.Optimizer`\n     as the learning rate.\n     Example: When fitting a Keras model, decay every 100000 steps with a base\n     of 0.96:\n \n     ```python\n     initial_learning_rate = 0.1\n-    lr_schedule = keras_core.optimizers.schedules.ExponentialDecay(\n+    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n         initial_learning_rate,\n         decay_steps=100000,\n         decay_rate=0.96,\n         staircase=True)\n \n-    model.compile(optimizer=keras_core.optimizers.SGD(learning_rate=lr_schedule),\n+    model.compile(optimizer=keras.optimizers.SGD(learning_rate=lr_schedule),\n                   loss='sparse_categorical_crossentropy',\n                   metrics=['accuracy'])\n \n@@ -119,8 +119,8 @@ class ExponentialDecay(LearningRateSchedule):\n     ```\n \n     The learning rate schedule is also serializable and deserializable using\n-    `keras_core.optimizers.schedules.serialize` and\n-    `keras_core.optimizers.schedules.deserialize`.\n+    `keras.optimizers.schedules.serialize` and\n+    `keras.optimizers.schedules.deserialize`.\n \n     Args:\n         initial_learning_rate: A Python float. The initial learning rate.\n@@ -178,7 +178,7 @@ class ExponentialDecay(LearningRateSchedule):\n         }\n \n \n-@keras_core_export(\"keras_core.optimizers.schedules.PiecewiseConstantDecay\")\n+@keras_export(\"keras.optimizers.schedules.PiecewiseConstantDecay\")\n class PiecewiseConstantDecay(LearningRateSchedule):\n     \"\"\"A `LearningRateSchedule` that uses a piecewise constant decay schedule.\n \n@@ -193,17 +193,17 @@ class PiecewiseConstantDecay(LearningRateSchedule):\n     step = ops.array(0)\n     boundaries = [100000, 110000]\n     values = [1.0, 0.5, 0.1]\n-    learning_rate_fn = keras_core.optimizers.schedules.PiecewiseConstantDecay(\n+    learning_rate_fn = keras.optimizers.schedules.PiecewiseConstantDecay(\n         boundaries, values)\n \n     # Later, whenever we perform an optimization step, we pass in the step.\n     learning_rate = learning_rate_fn(step)\n     ```\n \n-    You can pass this schedule directly into a `keras_core.optimizers.Optimizer`\n+    You can pass this schedule directly into a `keras.optimizers.Optimizer`\n     as the learning rate. The learning rate schedule is also serializable and\n-    deserializable using `keras_core.optimizers.schedules.serialize` and\n-    `keras_core.optimizers.schedules.deserialize`.\n+    deserializable using `keras.optimizers.schedules.serialize` and\n+    `keras.optimizers.schedules.deserialize`.\n \n     Args:\n         boundaries: A list of Python numbers with strictly increasing\n@@ -295,7 +295,7 @@ class PiecewiseConstantDecay(LearningRateSchedule):\n         }\n \n \n-@keras_core_export(\"keras_core.optimizers.schedules.PolynomialDecay\")\n+@keras_export(\"keras.optimizers.schedules.PolynomialDecay\")\n class PolynomialDecay(LearningRateSchedule):\n     \"\"\"A `LearningRateSchedule` that uses a polynomial decay schedule.\n \n@@ -333,7 +333,7 @@ class PolynomialDecay(LearningRateSchedule):\n                ) + end_learning_rate\n     ```\n \n-    You can pass this schedule directly into a `keras_core.optimizers.Optimizer`\n+    You can pass this schedule directly into a `keras.optimizers.Optimizer`\n     as the learning rate.\n     Example: Fit a model while decaying from 0.1 to 0.01 in 10000 steps using\n     sqrt (i.e. power=0.5):\n@@ -343,13 +343,13 @@ class PolynomialDecay(LearningRateSchedule):\n     starter_learning_rate = 0.1\n     end_learning_rate = 0.01\n     decay_steps = 10000\n-    learning_rate_fn = keras_core.optimizers.schedules.PolynomialDecay(\n+    learning_rate_fn = keras.optimizers.schedules.PolynomialDecay(\n         starter_learning_rate,\n         decay_steps,\n         end_learning_rate,\n         power=0.5)\n \n-    model.compile(optimizer=keras_core.optimizers.SGD(\n+    model.compile(optimizer=keras.optimizers.SGD(\n                       learning_rate=learning_rate_fn),\n                   loss='sparse_categorical_crossentropy',\n                   metrics=['accuracy'])\n@@ -358,8 +358,8 @@ class PolynomialDecay(LearningRateSchedule):\n     ```\n \n     The learning rate schedule is also serializable and deserializable using\n-    `keras_core.optimizers.schedules.serialize` and\n-    `keras_core.optimizers.schedules.deserialize`.\n+    `keras.optimizers.schedules.serialize` and\n+    `keras.optimizers.schedules.deserialize`.\n \n     Args:\n         initial_learning_rate: A Python float. The initial learning rate.\n@@ -445,7 +445,7 @@ class PolynomialDecay(LearningRateSchedule):\n         }\n \n \n-@keras_core_export(\"keras_core.optimizers.schedules.InverseTimeDecay\")\n+@keras_export(\"keras.optimizers.schedules.InverseTimeDecay\")\n class InverseTimeDecay(LearningRateSchedule):\n     \"\"\"A `LearningRateSchedule` that uses an inverse time decay schedule.\n \n@@ -473,7 +473,7 @@ class InverseTimeDecay(LearningRateSchedule):\n                (1 + decay_rate * floor(step / decay_step))\n     ```\n \n-    You can pass this schedule directly into a `keras_core.optimizers.Optimizer`\n+    You can pass this schedule directly into a `keras.optimizers.Optimizer`\n     as the learning rate.\n     Example: Fit a Keras model when decaying 1/t with a rate of 0.5:\n \n@@ -482,10 +482,10 @@ class InverseTimeDecay(LearningRateSchedule):\n     initial_learning_rate = 0.1\n     decay_steps = 1.0\n     decay_rate = 0.5\n-    learning_rate_fn = keras_core.optimizers.schedules.InverseTimeDecay(\n+    learning_rate_fn = keras.optimizers.schedules.InverseTimeDecay(\n         initial_learning_rate, decay_steps, decay_rate)\n \n-    model.compile(optimizer=keras_core.optimizers.SGD(\n+    model.compile(optimizer=keras.optimizers.SGD(\n                       learning_rate=learning_rate_fn),\n                   loss='sparse_categorical_crossentropy',\n                   metrics=['accuracy'])\n@@ -551,7 +551,7 @@ class InverseTimeDecay(LearningRateSchedule):\n         }\n \n \n-@keras_core_export(\"keras_core.optimizers.schedules.CosineDecay\")\n+@keras_export(\"keras.optimizers.schedules.CosineDecay\")\n class CosineDecay(LearningRateSchedule):\n     \"\"\"A `LearningRateSchedule` that uses a cosine decay with optional warmup.\n \n@@ -606,7 +606,7 @@ class CosineDecay(LearningRateSchedule):\n     ```python\n     decay_steps = 1000\n     initial_learning_rate = 0.1\n-    lr_decayed_fn = keras_core.optimizers.schedules.CosineDecay(\n+    lr_decayed_fn = keras.optimizers.schedules.CosineDecay(\n         initial_learning_rate, decay_steps)\n     ```\n \n@@ -617,16 +617,16 @@ class CosineDecay(LearningRateSchedule):\n     initial_learning_rate = 0\n     warmup_steps = 1000\n     target_learning_rate = 0.1\n-    lr_warmup_decayed_fn = keras_core.optimizers.schedules.CosineDecay(\n+    lr_warmup_decayed_fn = keras.optimizers.schedules.CosineDecay(\n         initial_learning_rate, decay_steps, warmup_target=target_learning_rate,\n         warmup_steps=warmup_steps\n     )\n     ```\n \n-    You can pass this schedule directly into a `keras_core.optimizers.Optimizer`\n+    You can pass this schedule directly into a `keras.optimizers.Optimizer`\n     as the learning rate. The learning rate schedule is also serializable and\n-    deserializable using `keras_core.optimizers.schedules.serialize` and\n-    `keras_core.optimizers.schedules.deserialize`.\n+    deserializable using `keras.optimizers.schedules.serialize` and\n+    `keras.optimizers.schedules.deserialize`.\n \n     Args:\n         initial_learning_rate: A Python float. The initial learning rate.\n@@ -736,7 +736,7 @@ class CosineDecay(LearningRateSchedule):\n         }\n \n \n-@keras_core_export(\"keras_core.optimizers.schedules.CosineDecayRestarts\")\n+@keras_export(\"keras.optimizers.schedules.CosineDecayRestarts\")\n class CosineDecayRestarts(LearningRateSchedule):\n     \"\"\"A `LearningRateSchedule` that uses a cosine decay schedule with restarts.\n \n@@ -762,15 +762,15 @@ class CosineDecayRestarts(LearningRateSchedule):\n     ```python\n     first_decay_steps = 1000\n     lr_decayed_fn = (\n-        keras_core.optimizers.schedules.CosineDecayRestarts(\n+        keras.optimizers.schedules.CosineDecayRestarts(\n             initial_learning_rate,\n             first_decay_steps))\n     ```\n \n-    You can pass this schedule directly into a `keras_core.optimizers.Optimizer`\n+    You can pass this schedule directly into a `keras.optimizers.Optimizer`\n     as the learning rate. The learning rate schedule is also serializable and\n-    deserializable using `keras_core.optimizers.schedules.serialize` and\n-    `keras_core.optimizers.schedules.deserialize`.\n+    deserializable using `keras.optimizers.schedules.serialize` and\n+    `keras.optimizers.schedules.deserialize`.\n \n     Args:\n         initial_learning_rate: A Python float. The initial learning rate.\n@@ -873,7 +873,7 @@ class CosineDecayRestarts(LearningRateSchedule):\n         }\n \n \n-@keras_core_export(\"keras_core.optimizers.schedules.serialize\")\n+@keras_export(\"keras.optimizers.schedules.serialize\")\n def serialize(learning_rate_schedule):\n     \"\"\"Serializes a `LearningRateSchedule` into a JSON-compatible dict.\n \n@@ -885,17 +885,17 @@ def serialize(learning_rate_schedule):\n \n     Example:\n \n-    >>> lr_schedule = keras_core.optimizers.schedules.ExponentialDecay(\n+    >>> lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n     ...     0.1, decay_steps=100000, decay_rate=0.96, staircase=True)\n-    >>> keras_core.optimizers.schedules.serialize(lr_schedule)\n-    {'module': 'keras_core.optimizers.schedules',\n+    >>> keras.optimizers.schedules.serialize(lr_schedule)\n+    {'module': 'keras.optimizers.schedules',\n     'class_name': 'ExponentialDecay', 'config': {...},\n     'registered_name': None}\n     \"\"\"\n     return serialization_lib.serialize_keras_object(learning_rate_schedule)\n \n \n-@keras_core_export(\"keras_core.optimizers.schedules.deserialize\")\n+@keras_export(\"keras.optimizers.schedules.deserialize\")\n def deserialize(config, custom_objects=None):\n     \"\"\"Instantiates a `LearningRateSchedule` object from a serialized form.\n \n@@ -922,7 +922,7 @@ def deserialize(config, custom_objects=None):\n             'power': 0.5\n         }\n     }\n-    lr_schedule = keras_core.optimizers.schedules.deserialize(config)\n+    lr_schedule = keras.optimizers.schedules.deserialize(config)\n     ```\n     \"\"\"\n     return serialization_lib.deserialize_keras_object(\n\n@@ -5,12 +5,12 @@ import math\n import numpy as np\n import pytest\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import optimizers\n-from keras_core import testing\n-from keras_core.models import Sequential\n-from keras_core.optimizers import schedules\n+from keras import backend\n+from keras import layers\n+from keras import optimizers\n+from keras import testing\n+from keras.models import Sequential\n+from keras.optimizers import schedules\n \n \n class TestFitLRSchedulesFlow(testing.TestCase):\n\n@@ -1,9 +1,9 @@\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.optimizers import optimizer\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.optimizers import optimizer\n \n \n-@keras_core_export(\"keras_core.optimizers.SGD\")\n+@keras_export(\"keras.optimizers.SGD\")\n class SGD(optimizer.Optimizer):\n     \"\"\"Gradient descent (with momentum) optimizer.\n \n@@ -29,7 +29,7 @@ class SGD(optimizer.Optimizer):\n \n     Args:\n         learning_rate: A float, a\n-            `keras_core.optimizers.schedules.LearningRateSchedule` instance, or\n+            `keras.optimizers.schedules.LearningRateSchedule` instance, or\n             a callable that takes no arguments and returns the actual value to\n             use. The learning rate. Defaults to `0.01`.\n         momentum: float hyperparameter >= 0 that accelerates gradient descent in\n\n@@ -2,10 +2,10 @@\n \n import numpy as np\n \n-from keras_core import backend\n-from keras_core import ops\n-from keras_core import testing\n-from keras_core.optimizers.sgd import SGD\n+from keras import backend\n+from keras import ops\n+from keras import testing\n+from keras.optimizers.sgd import SGD\n \n \n class SGDTest(testing.TestCase):\n\n\n@@ -1,8 +1,8 @@\n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n+from keras import backend\n+from keras.api_export import keras_export\n \n \n-@keras_core_export(\"keras_core.random.normal\")\n+@keras_export(\"keras.random.normal\")\n def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n     \"\"\"Draw random samples from a normal (Gaussian) distribution.\n \n@@ -12,24 +12,24 @@ def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n         stddev: Floats, defaults to 1. Standard deviation of the random values\n             to generate.\n         dtype: Optional dtype of the tensor. Only floating point types are\n-            supported. If not specified, `keras_core.config.floatx()` is used,\n+            supported. If not specified, `keras.config.floatx()` is used,\n             which defaults to `float32` unless you configured it otherwise (via\n-            `keras_core.config.set_floatx(float_dtype)`).\n+            `keras.config.set_floatx(float_dtype)`).\n         seed: A Python integer or instance of\n-            `keras_core.random.SeedGenerator`.\n+            `keras.random.SeedGenerator`.\n             Used to make the behavior of the initializer\n             deterministic. Note that an initializer seeded with an integer\n             or None (unseeded) will produce the same random values\n             across multiple calls. To get different random values\n             across multiple calls, use as seed an instance\n-            of `keras_core.random.SeedGenerator`.\n+            of `keras.random.SeedGenerator`.\n     \"\"\"\n     return backend.random.normal(\n         shape, mean=mean, stddev=stddev, dtype=dtype, seed=seed\n     )\n \n \n-@keras_core_export(\"keras_core.random.categorical\")\n+@keras_export(\"keras.random.categorical\")\n def categorical(logits, num_samples, dtype=\"int32\", seed=None):\n     \"\"\"Draws samples from a categorical distribution.\n \n@@ -52,13 +52,13 @@ def categorical(logits, num_samples, dtype=\"int32\", seed=None):\n             tensor's shape.\n         dtype: Optional dtype of the output tensor.\n         seed: A Python integer or instance of\n-            `keras_core.random.SeedGenerator`.\n+            `keras.random.SeedGenerator`.\n             Used to make the behavior of the initializer\n             deterministic. Note that an initializer seeded with an integer\n             or None (unseeded) will produce the same random values\n             across multiple calls. To get different random values\n             across multiple calls, use as seed an instance\n-            of `keras_core.random.SeedGenerator`.\n+            of `keras.random.SeedGenerator`.\n \n     Returns:\n         A 2-D tensor with (batch_size, num_samples).\n@@ -74,7 +74,7 @@ def categorical(logits, num_samples, dtype=\"int32\", seed=None):\n     )\n \n \n-@keras_core_export(\"keras_core.random.uniform\")\n+@keras_export(\"keras.random.uniform\")\n def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n     \"\"\"Draw samples from a uniform distribution.\n \n@@ -91,21 +91,21 @@ def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n         maxval: Floats, defaults to 1. Upper bound of the range of\n             random values to generate (exclusive).\n         dtype: Optional dtype of the tensor. Only floating point types are\n-            supported. If not specified, `keras_core.config.floatx()` is used,\n+            supported. If not specified, `keras.config.floatx()` is used,\n             which defaults to `float32` unless you configured it otherwise (via\n-            `keras_core.config.set_floatx(float_dtype)`)\n+            `keras.config.set_floatx(float_dtype)`)\n         seed: A Python integer or instance of\n-            `keras_core.random.SeedGenerator`.\n+            `keras.random.SeedGenerator`.\n             Used to make the behavior of the initializer\n             deterministic. Note that an initializer seeded with an integer\n             or None (unseeded) will produce the same random values\n             across multiple calls. To get different random values\n             across multiple calls, use as seed an instance\n-            of `keras_core.random.SeedGenerator`.\n+            of `keras.random.SeedGenerator`.\n     \"\"\"\n     if dtype and not backend.is_float_dtype(dtype):\n         raise ValueError(\n-            \"`keras_core.random.uniform` requires a floating point `dtype`. \"\n+            \"`keras.random.uniform` requires a floating point `dtype`. \"\n             f\"Received: dtype={dtype} \"\n         )\n     return backend.random.uniform(\n@@ -113,7 +113,7 @@ def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n     )\n \n \n-@keras_core_export(\"keras_core.random.randint\")\n+@keras_export(\"keras.random.randint\")\n def randint(shape, minval, maxval, dtype=\"int32\", seed=None):\n     \"\"\"Draw random integers from a uniform distribution.\n \n@@ -130,21 +130,21 @@ def randint(shape, minval, maxval, dtype=\"int32\", seed=None):\n         maxval: Floats, defaults to 1. Upper bound of the range of\n             random values to generate (exclusive).\n         dtype: Optional dtype of the tensor. Only integer types are\n-            supported. If not specified, `keras_core.config.floatx()` is used,\n+            supported. If not specified, `keras.config.floatx()` is used,\n             which defaults to `float32` unless you configured it otherwise (via\n-            `keras_core.config.set_floatx(float_dtype)`)\n+            `keras.config.set_floatx(float_dtype)`)\n         seed: A Python integer or instance of\n-            `keras_core.random.SeedGenerator`.\n+            `keras.random.SeedGenerator`.\n             Used to make the behavior of the initializer\n             deterministic. Note that an initializer seeded with an integer\n             or None (unseeded) will produce the same random values\n             across multiple calls. To get different random values\n             across multiple calls, use as seed an instance\n-            of `keras_core.random.SeedGenerator`.\n+            of `keras.random.SeedGenerator`.\n     \"\"\"\n     if dtype and not backend.is_int_dtype(dtype):\n         raise ValueError(\n-            \"`keras_core.random.randint` requires an integer `dtype`. \"\n+            \"`keras.random.randint` requires an integer `dtype`. \"\n             f\"Received: dtype={dtype} \"\n         )\n     return backend.random.randint(\n@@ -152,7 +152,7 @@ def randint(shape, minval, maxval, dtype=\"int32\", seed=None):\n     )\n \n \n-@keras_core_export(\"keras_core.random.truncated_normal\")\n+@keras_export(\"keras.random.truncated_normal\")\n def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n     \"\"\"Draw samples from a truncated normal distribution.\n \n@@ -170,27 +170,27 @@ def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n             which defaults to `float32` unless you configured it otherwise (via\n             `keras.config.set_floatx(float_dtype)`)\n         seed: A Python integer or instance of\n-            `keras_core.random.SeedGenerator`.\n+            `keras.random.SeedGenerator`.\n             Used to make the behavior of the initializer\n             deterministic. Note that an initializer seeded with an integer\n             or None (unseeded) will produce the same random values\n             across multiple calls. To get different random values\n             across multiple calls, use as seed an instance\n-            of `keras_core.random.SeedGenerator`.\n+            of `keras.random.SeedGenerator`.\n     \"\"\"\n     return backend.random.truncated_normal(\n         shape, mean=mean, stddev=stddev, dtype=dtype, seed=seed\n     )\n \n \n-@keras_core_export(\"keras_core.random.dropout\")\n+@keras_export(\"keras.random.dropout\")\n def dropout(inputs, rate, noise_shape=None, seed=None):\n     return backend.random.dropout(\n         inputs, rate, noise_shape=noise_shape, seed=seed\n     )\n \n \n-@keras_core_export(\"keras_core.random.shuffle\")\n+@keras_export(\"keras.random.shuffle\")\n def shuffle(x, axis=0, seed=None):\n     \"\"\"Shuffle the elements of a tensor uniformly at random along an axis.\n \n@@ -199,12 +199,12 @@ def shuffle(x, axis=0, seed=None):\n         axis: An integer specifying the axis along which to shuffle. Defaults to\n             `0`.\n         seed: A Python integer or instance of\n-            `keras_core.random.SeedGenerator`.\n+            `keras.random.SeedGenerator`.\n             Used to make the behavior of the initializer\n             deterministic. Note that an initializer seeded with an integer\n             or None (unseeded) will produce the same random values\n             across multiple calls. To get different random values\n             across multiple calls, use as seed an instance\n-            of `keras_core.random.SeedGenerator`.\n+            of `keras.random.SeedGenerator`.\n     \"\"\"\n     return backend.random.shuffle(x, axis=axis, seed=seed)\n\n@@ -2,12 +2,12 @@ import numpy as np\n import pytest\n from absl.testing import parameterized\n \n-import keras_core\n-from keras_core import backend\n-from keras_core import ops\n-from keras_core import testing\n-from keras_core.random import random\n-from keras_core.random import seed_generator\n+import keras\n+from keras import backend\n+from keras import ops\n+from keras import testing\n+from keras.random import random\n+from keras.random import seed_generator\n \n \n class RandomTest(testing.TestCase, parameterized.TestCase):\n@@ -83,7 +83,7 @@ class RandomTest(testing.TestCase, parameterized.TestCase):\n         self.assertLessEqual(ops.max(res), max)\n         self.assertGreaterEqual(ops.max(res), min)\n         # Torch has incomplete dtype support for uints; will remap some dtypes.\n-        if keras_core.backend.backend() != \"torch\":\n+        if keras.backend.backend() != \"torch\":\n             self.assertEqual(backend.standardize_dtype(res.dtype), dtype)\n \n     @parameterized.parameters(\n@@ -114,7 +114,7 @@ class RandomTest(testing.TestCase, parameterized.TestCase):\n         self.assertGreater(ops.sum(x_res == 0), 2)\n \n     @pytest.mark.skipif(\n-        keras_core.backend.backend() != \"jax\",\n+        keras.backend.backend() != \"jax\",\n         reason=\"This test requires `jax` as the backend.\",\n     )\n     def test_dropout_jax_jit_stateless(self):\n@@ -125,8 +125,8 @@ class RandomTest(testing.TestCase, parameterized.TestCase):\n \n         @jax.jit\n         def train_step(x):\n-            with keras_core.backend.StatelessScope():\n-                x = keras_core.layers.Dropout(rate=0.1)(x, training=True)\n+            with keras.backend.StatelessScope():\n+                x = keras.layers.Dropout(rate=0.1)(x, training=True)\n             return x\n \n         x = train_step(x)\n@@ -140,7 +140,7 @@ class RandomTest(testing.TestCase, parameterized.TestCase):\n         self.assertEqual(x.shape, (2, 3, 5, 7))\n \n     @pytest.mark.skipif(\n-        keras_core.backend.backend() != \"jax\",\n+        keras.backend.backend() != \"jax\",\n         reason=\"This test requires `jax` as the backend.\",\n     )\n     def test_jax_rngkey_seed(self):\n@@ -155,7 +155,7 @@ class RandomTest(testing.TestCase, parameterized.TestCase):\n         self.assertIsInstance(x, jnp.ndarray)\n \n     @pytest.mark.skipif(\n-        keras_core.backend.backend() != \"jax\",\n+        keras.backend.backend() != \"jax\",\n         reason=\"This test requires `jax` as the backend.\",\n     )\n     def test_jax_unseed_disallowed_during_tracing(self):\n\n@@ -2,17 +2,17 @@ import random as python_random\n \n import numpy as np\n \n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.backend.common import global_state\n-from keras_core.utils import jax_utils\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.backend.common import global_state\n+from keras.utils import jax_utils\n \n \n-@keras_core_export(\"keras_core.random.SeedGenerator\")\n+@keras_export(\"keras.random.SeedGenerator\")\n class SeedGenerator:\n     \"\"\"Generates variable seeds upon each call to a RNG-using function.\n \n-    In Keras, all RNG-using methods (such as `keras_core.random.normal()`)\n+    In Keras, all RNG-using methods (such as `keras.random.normal()`)\n     are stateless, meaning that if you pass an integer seed to them\n     (such as `seed=42`), they will return the same values at each call.\n     In order to get different values at each call, you must use a\n@@ -22,22 +22,22 @@ class SeedGenerator:\n     Example:\n \n     ```python\n-    seed_gen = keras_core.random.SeedGenerator(seed=42)\n-    values = keras_core.random.normal(shape=(2, 3), seed=seed_gen)\n-    new_values = keras_core.random.normal(shape=(2, 3), seed=seed_gen)\n+    seed_gen = keras.random.SeedGenerator(seed=42)\n+    values = keras.random.normal(shape=(2, 3), seed=seed_gen)\n+    new_values = keras.random.normal(shape=(2, 3), seed=seed_gen)\n     ```\n \n     Usage in a layer:\n \n     ```python\n-    class Dropout(keras_core.Layer):\n+    class Dropout(keras.Layer):\n         def __init__(self, **kwargs):\n             super().__init__(**kwargs)\n-            self.seed_generator = keras_core.random.SeedGenerator(1337)\n+            self.seed_generator = keras.random.SeedGenerator(1337)\n \n         def call(self, x, training=False):\n             if training:\n-                return keras_core.random.dropout(\n+                return keras.random.dropout(\n                     x, rate=0.5, seed=self.seed_generator\n                 )\n             return x\n@@ -113,7 +113,7 @@ def make_default_seed():\n \n \n def draw_seed(seed):\n-    from keras_core.backend import convert_to_tensor\n+    from keras.backend import convert_to_tensor\n \n     if isinstance(seed, SeedGenerator):\n         return seed.next()\n\n@@ -1,13 +1,13 @@\n import inspect\n \n-from keras_core.api_export import keras_core_export\n-from keras_core.regularizers.regularizers import L1\n-from keras_core.regularizers.regularizers import L1L2\n-from keras_core.regularizers.regularizers import L2\n-from keras_core.regularizers.regularizers import OrthogonalRegularizer\n-from keras_core.regularizers.regularizers import Regularizer\n-from keras_core.saving import serialization_lib\n-from keras_core.utils.naming import to_snake_case\n+from keras.api_export import keras_export\n+from keras.regularizers.regularizers import L1\n+from keras.regularizers.regularizers import L1L2\n+from keras.regularizers.regularizers import L2\n+from keras.regularizers.regularizers import OrthogonalRegularizer\n+from keras.regularizers.regularizers import Regularizer\n+from keras.saving import serialization_lib\n+from keras.utils.naming import to_snake_case\n \n ALL_OBJECTS = {\n     Regularizer,\n@@ -23,12 +23,12 @@ ALL_OBJECTS_DICT.update(\n )\n \n \n-@keras_core_export(\"keras_core.regularizers.serialize\")\n+@keras_export(\"keras.regularizers.serialize\")\n def serialize(initializer):\n     return serialization_lib.serialize_keras_object(initializer)\n \n \n-@keras_core_export(\"keras_core.regularizers.deserialize\")\n+@keras_export(\"keras.regularizers.deserialize\")\n def deserialize(config, custom_objects=None):\n     \"\"\"Return a Keras regularizer object via its config.\"\"\"\n     return serialization_lib.deserialize_keras_object(\n@@ -38,7 +38,7 @@ def deserialize(config, custom_objects=None):\n     )\n \n \n-@keras_core_export(\"keras_core.regularizers.get\")\n+@keras_export(\"keras.regularizers.get\")\n def get(identifier):\n     \"\"\"Retrieve a Keras regularizer object via an identifier.\"\"\"\n     if identifier is None:\n\n@@ -1,12 +1,12 @@\n import math\n \n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.utils.numerical_utils import normalize\n+from keras import ops\n+from keras.api_export import keras_export\n+from keras.utils.numerical_utils import normalize\n \n \n-@keras_core_export(\n-    [\"keras_core.Regularizer\", \"keras_core.regularizers.Regularizer\"]\n+@keras_export(\n+    [\"keras.Regularizer\", \"keras.regularizers.Regularizer\"]\n )\n class Regularizer:\n     \"\"\"Regularizer base class.\n@@ -167,8 +167,8 @@ class Regularizer:\n         raise NotImplementedError(f\"{self} does not implement get_config()\")\n \n \n-@keras_core_export(\n-    [\"keras_core.regularizers.L1L2\", \"keras_core.regularizers.l1_l2\"]\n+@keras_export(\n+    [\"keras.regularizers.L1L2\", \"keras.regularizers.l1_l2\"]\n )\n class L1L2(Regularizer):\n     \"\"\"A regularizer that applies both L1 and L2 regularization penalties.\n@@ -214,7 +214,7 @@ class L1L2(Regularizer):\n         return {\"l1\": float(self.l1), \"l2\": float(self.l2)}\n \n \n-@keras_core_export([\"keras_core.regularizers.L1\", \"keras_core.regularizers.l1\"])\n+@keras_export([\"keras.regularizers.L1\", \"keras.regularizers.l1\"])\n class L1(Regularizer):\n     \"\"\"A regularizer that applies a L1 regularization penalty.\n \n@@ -243,7 +243,7 @@ class L1(Regularizer):\n         return {\"l1\": float(self.l1)}\n \n \n-@keras_core_export([\"keras_core.regularizers.L2\", \"keras_core.regularizers.l2\"])\n+@keras_export([\"keras.regularizers.L2\", \"keras.regularizers.l2\"])\n class L2(Regularizer):\n     \"\"\"A regularizer that applies a L2 regularization penalty.\n \n@@ -272,10 +272,10 @@ class L2(Regularizer):\n         return {\"l2\": float(self.l2)}\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.regularizers.OrthogonalRegularizer\",\n-        \"keras_core.regularizers.orthogonal_regularizer\",\n+        \"keras.regularizers.OrthogonalRegularizer\",\n+        \"keras.regularizers.orthogonal_regularizer\",\n     ]\n )\n class OrthogonalRegularizer(Regularizer):\n\n@@ -1,8 +1,8 @@\n import numpy as np\n \n-from keras_core import backend\n-from keras_core import regularizers\n-from keras_core import testing\n+from keras import backend\n+from keras import regularizers\n+from keras import testing\n \n \n # TODO: serialization tests\n\n@@ -0,0 +1,9 @@\n+from keras.saving.object_registration import CustomObjectScope\n+from keras.saving.object_registration import custom_object_scope\n+from keras.saving.object_registration import get_custom_objects\n+from keras.saving.object_registration import get_registered_name\n+from keras.saving.object_registration import get_registered_object\n+from keras.saving.object_registration import register_keras_serializable\n+from keras.saving.saving_api import load_model\n+from keras.saving.serialization_lib import deserialize_keras_object\n+from keras.saving.serialization_lib import serialize_keras_object\n\n@@ -1,26 +1,26 @@\n import inspect\n \n-from keras_core.api_export import keras_core_export\n-from keras_core.backend.common import global_state\n+from keras.api_export import keras_export\n+from keras.backend.common import global_state\n \n GLOBAL_CUSTOM_OBJECTS = {}\n GLOBAL_CUSTOM_NAMES = {}\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.saving.CustomObjectScope\",\n-        \"keras_core.saving.custom_object_scope\",\n-        \"keras_core.utils.CustomObjectScope\",\n-        \"keras_core.utils.custom_object_scope\",\n+        \"keras.saving.CustomObjectScope\",\n+        \"keras.saving.custom_object_scope\",\n+        \"keras.utils.CustomObjectScope\",\n+        \"keras.utils.custom_object_scope\",\n     ]\n )\n class CustomObjectScope:\n     \"\"\"Exposes custom classes/functions to Keras deserialization internals.\n \n     Under a scope `with custom_object_scope(objects_dict)`, Keras methods such\n-    as `keras_core.models.load_model()` or\n-    `keras_core.models.model_from_config()` will be able to deserialize any\n+    as `keras.models.load_model()` or\n+    `keras.models.model_from_config()` will be able to deserialize any\n     custom object referenced by a saved config (e.g. a custom layer or metric).\n \n     Example:\n@@ -64,10 +64,10 @@ class CustomObjectScope:\n custom_object_scope = CustomObjectScope\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.saving.get_custom_objects\",\n-        \"keras_core.utils.get_custom_objects\",\n+        \"keras.saving.get_custom_objects\",\n+        \"keras.utils.get_custom_objects\",\n     ]\n )\n def get_custom_objects():\n@@ -90,10 +90,10 @@ def get_custom_objects():\n     return GLOBAL_CUSTOM_OBJECTS\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.saving.register_keras_serializable\",\n-        \"keras_core.utils.register_keras_serializable\",\n+        \"keras.saving.register_keras_serializable\",\n+        \"keras.utils.register_keras_serializable\",\n     ]\n )\n def register_keras_serializable(package=\"Custom\", name=None):\n@@ -116,7 +116,7 @@ def register_keras_serializable(package=\"Custom\", name=None):\n     # Note that `'my_package'` is used as the `package` argument here, and since\n     # the `name` argument is not provided, `'MyDense'` is used as the `name`.\n     @register_keras_serializable('my_package')\n-    class MyDense(keras_core.layers.Dense):\n+    class MyDense(keras.layers.Dense):\n         pass\n \n     assert get_registered_object('my_package>MyDense') == MyDense\n@@ -155,10 +155,10 @@ def register_keras_serializable(package=\"Custom\", name=None):\n     return decorator\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.saving.get_registered_name\",\n-        \"keras_core.utils.get_registered_name\",\n+        \"keras.saving.get_registered_name\",\n+        \"keras.utils.get_registered_name\",\n     ]\n )\n def get_registered_name(obj):\n@@ -181,10 +181,10 @@ def get_registered_name(obj):\n         return obj.__name__\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.saving.get_registered_object\",\n-        \"keras_core.utils.get_registered_object\",\n+        \"keras.saving.get_registered_object\",\n+        \"keras.utils.get_registered_object\",\n     ]\n )\n def get_registered_object(name, custom_objects=None, module_objects=None):\n\n@@ -1,7 +1,7 @@\n-import keras_core\n-from keras_core import testing\n-from keras_core.saving import object_registration\n-from keras_core.saving import serialization_lib\n+import keras\n+from keras import testing\n+from keras.saving import object_registration\n+from keras.saving import serialization_lib\n \n \n class TestObjectRegistration(testing.TestCase):\n@@ -16,17 +16,17 @@ class TestObjectRegistration(testing.TestCase):\n             with object_registration.custom_object_scope(\n                 {\"CustomClass\": CustomClass, \"custom_fn\": custom_fn}\n             ):\n-                actual_custom_fn = keras_core.activations.get(\"custom_fn\")\n+                actual_custom_fn = keras.activations.get(\"custom_fn\")\n                 self.assertEqual(actual_custom_fn, custom_fn)\n-                actual_custom_class = keras_core.regularizers.get(\"CustomClass\")\n+                actual_custom_class = keras.regularizers.get(\"CustomClass\")\n                 self.assertEqual(actual_custom_class.__class__, CustomClass)\n \n             with object_registration.custom_object_scope(\n                 {\"CustomClass\": CustomClass, \"custom_fn\": custom_fn}\n             ):\n-                actual_custom_fn = keras_core.activations.get(\"custom_fn\")\n+                actual_custom_fn = keras.activations.get(\"custom_fn\")\n                 self.assertEqual(actual_custom_fn, custom_fn)\n-                actual_custom_class = keras_core.regularizers.get(\"CustomClass\")\n+                actual_custom_class = keras.regularizers.get(\"CustomClass\")\n                 self.assertEqual(actual_custom_class.__class__, CustomClass)\n                 checked_thread = self.checkedThread(check_get_in_thread)\n                 checked_thread.start()\n@@ -82,9 +82,9 @@ class TestObjectRegistration(testing.TestCase):\n         cls = object_registration.get_registered_object(fn_class_name)\n         self.assertEqual(OtherTestClass, cls)\n \n-        config = keras_core.saving.serialize_keras_object(inst)\n+        config = keras.saving.serialize_keras_object(inst)\n         self.assertEqual(\"OtherTestClass\", config[\"class_name\"])\n-        new_inst = keras_core.saving.deserialize_keras_object(config)\n+        new_inst = keras.saving.deserialize_keras_object(config)\n         self.assertIsNot(inst, new_inst)\n         self.assertIsInstance(new_inst, OtherTestClass)\n         self.assertEqual(5, new_inst._val)\n@@ -100,8 +100,8 @@ class TestObjectRegistration(testing.TestCase):\n         fn_class_name = object_registration.get_registered_name(my_fn)\n         self.assertEqual(fn_class_name, class_name)\n \n-        config = keras_core.saving.serialize_keras_object(my_fn)\n-        fn = keras_core.saving.deserialize_keras_object(config)\n+        config = keras.saving.serialize_keras_object(my_fn)\n+        fn = keras.saving.deserialize_keras_object(config)\n         self.assertEqual(42, fn())\n \n         fn_2 = object_registration.get_registered_object(fn_class_name)\n\n@@ -3,11 +3,11 @@ import zipfile\n \n from absl import logging\n \n-from keras_core.api_export import keras_core_export\n-from keras_core.legacy.saving import legacy_h5_format\n-from keras_core.saving import saving_lib\n-from keras_core.utils import file_utils\n-from keras_core.utils import io_utils\n+from keras.api_export import keras_export\n+from keras.legacy.saving import legacy_h5_format\n+from keras.saving import saving_lib\n+from keras.utils import file_utils\n+from keras.utils import io_utils\n \n try:\n     import h5py\n@@ -15,8 +15,8 @@ except ImportError:\n     h5py = None\n \n \n-@keras_core_export(\n-    [\"keras_core.saving.save_model\", \"keras_core.models.save_model\"]\n+@keras_export(\n+    [\"keras.saving.save_model\", \"keras.models.save_model\"]\n )\n def save_model(model, filepath, overwrite=True, **kwargs):\n     \"\"\"Saves a model as a `.keras` file.\n@@ -30,19 +30,19 @@ def save_model(model, filepath, overwrite=True, **kwargs):\n     Example:\n \n     ```python\n-    model = keras_core.Sequential(\n+    model = keras.Sequential(\n         [\n-            keras_core.layers.Dense(5, input_shape=(3,)),\n-            keras_core.layers.Softmax(),\n+            keras.layers.Dense(5, input_shape=(3,)),\n+            keras.layers.Softmax(),\n         ],\n     )\n     model.save(\"model.keras\")\n-    loaded_model = keras_core.saving.load_model(\"model.keras\")\n+    loaded_model = keras.saving.load_model(\"model.keras\")\n     x = keras.random.uniform((10, 3))\n     assert np.allclose(model.predict(x), loaded_model.predict(x))\n     ```\n \n-    Note that `model.save()` is an alias for `keras_core.saving.save_model()`.\n+    Note that `model.save()` is an alias for `keras.saving.save_model()`.\n \n     The saved `.keras` file contains:\n \n@@ -59,14 +59,14 @@ def save_model(model, filepath, overwrite=True, **kwargs):\n             \".keras\"\n         ):\n             logging.warning(\n-                \"The `save_format` argument is deprecated in Keras Core. \"\n+                \"The `save_format` argument is deprecated in Keras 3. \"\n                 \"We recommend removing this argument as it can be inferred \"\n                 \"from the file path. \"\n                 f\"Received: save_format={save_format}\"\n             )\n         else:\n             raise ValueError(\n-                \"The `save_format` argument is deprecated in Keras Core. \"\n+                \"The `save_format` argument is deprecated in Keras 3. \"\n                 \"Please remove this argument and pass a file path with \"\n                 \"either `.keras` or `.h5` extension.\"\n                 f\"Received: save_format={save_format}\"\n@@ -112,8 +112,8 @@ def save_model(model, filepath, overwrite=True, **kwargs):\n         )\n \n \n-@keras_core_export(\n-    [\"keras_core.saving.load_model\", \"keras_core.models.load_model\"]\n+@keras_export(\n+    [\"keras.saving.load_model\", \"keras.models.load_model\"]\n )\n def load_model(filepath, custom_objects=None, compile=True, safe_mode=True):\n     \"\"\"Loads a model saved via `model.save()`.\n@@ -137,11 +137,11 @@ def load_model(filepath, custom_objects=None, compile=True, safe_mode=True):\n     Example:\n \n     ```python\n-    model = keras_core.Sequential([\n-        keras_core.layers.Dense(5, input_shape=(3,)),\n-        keras_core.layers.Softmax()])\n+    model = keras.Sequential([\n+        keras.layers.Dense(5, input_shape=(3,)),\n+        keras.layers.Softmax()])\n     model.save(\"model.keras\")\n-    loaded_model = keras_core.saving.load_model(\"model.keras\")\n+    loaded_model = keras.saving.load_model(\"model.keras\")\n     x = np.random.random((10, 3))\n     assert np.allclose(model.predict(x), loaded_model.predict(x))\n     ```\n@@ -191,13 +191,13 @@ def load_model(filepath, custom_objects=None, compile=True, safe_mode=True):\n     else:\n         raise ValueError(\n             f\"File format not supported: filepath={filepath}. \"\n-            \"Keras Core only supports V3 `.keras` files and \"\n+            \"Keras 3 only supports V3 `.keras` files and \"\n             \"legacy H5 format files (`.h5` extension). \"\n             \"Note that the legacy SavedModel format is not \"\n-            \"supported by `load_model()` in Keras Core. In \"\n+            \"supported by `load_model()` in Keras 3. In \"\n             \"order to reload a TensorFlow SavedModel as an \"\n-            \"inference-only layer in Keras Core, use \"\n-            \"`keras_core.layers.TFSMLayer(\"\n+            \"inference-only layer in Keras 3, use \"\n+            \"`keras.layers.TFSMLayer(\"\n             f\"{filepath}, call_endpoint='serving_default')` \"\n             \"(note that your `call_endpoint` \"\n             \"might have a different name).\"\n@@ -237,6 +237,6 @@ def load_weights(model, filepath, skip_mismatch=False, **kwargs):\n     else:\n         raise ValueError(\n             f\"File format not supported: filepath={filepath}. \"\n-            \"Keras Core only supports V3 `.keras` and `.weights.h5` \"\n+            \"Keras 3 only supports V3 `.keras` and `.weights.h5` \"\n             \"files, or legacy V1/V2 `.h5` files.\"\n         )\n\n@@ -4,10 +4,10 @@ import unittest.mock as mock\n import numpy as np\n from absl import logging\n \n-from keras_core import layers\n-from keras_core.models import Sequential\n-from keras_core.saving import saving_api\n-from keras_core.testing import test_case\n+from keras import layers\n+from keras.models import Sequential\n+from keras.saving import saving_api\n+from keras.testing import test_case\n \n \n class SaveModelTests(test_case.TestCase):\n\n@@ -9,18 +9,18 @@ import zipfile\n \n import numpy as np\n \n-from keras_core.backend.common import global_state\n-from keras_core.layers.layer import Layer\n-from keras_core.losses.loss import Loss\n-from keras_core.metrics.metric import Metric\n-from keras_core.optimizers.optimizer import Optimizer\n-from keras_core.saving.serialization_lib import ObjectSharingScope\n-from keras_core.saving.serialization_lib import deserialize_keras_object\n-from keras_core.saving.serialization_lib import serialize_keras_object\n-from keras_core.trainers.compile_utils import CompileMetrics\n-from keras_core.utils import file_utils\n-from keras_core.utils import naming\n-from keras_core.version import __version__ as keras_version\n+from keras.backend.common import global_state\n+from keras.layers.layer import Layer\n+from keras.losses.loss import Loss\n+from keras.metrics.metric import Metric\n+from keras.optimizers.optimizer import Optimizer\n+from keras.saving.serialization_lib import ObjectSharingScope\n+from keras.saving.serialization_lib import deserialize_keras_object\n+from keras.saving.serialization_lib import serialize_keras_object\n+from keras.trainers.compile_utils import CompileMetrics\n+from keras.utils import file_utils\n+from keras.utils import naming\n+from keras.version import __version__ as keras_version\n \n try:\n     import h5py\n@@ -253,8 +253,8 @@ def _write_to_zip_recursively(zipfile_to_save, system_path, zip_path):\n \n \n def _walk_trackable(trackable):\n-    from keras_core.models import Functional\n-    from keras_core.models import Sequential\n+    from keras.models import Functional\n+    from keras.models import Sequential\n \n     if isinstance(trackable, Sequential):\n         obj_type = \"Sequential\"\n\n@@ -9,18 +9,18 @@ from unittest import mock\n import numpy as np\n import pytest\n \n-import keras_core\n-from keras_core import ops\n-from keras_core import testing\n-from keras_core.saving import saving_lib\n+import keras\n+from keras import ops\n+from keras import testing\n+from keras.saving import saving_lib\n \n \n-@keras_core.saving.register_keras_serializable(package=\"my_custom_package\")\n-class MyDense(keras_core.layers.Layer):\n+@keras.saving.register_keras_serializable(package=\"my_custom_package\")\n+class MyDense(keras.layers.Layer):\n     def __init__(self, units, **kwargs):\n         super().__init__(**kwargs)\n         self.units = units\n-        self.nested_layer = keras_core.layers.Dense(self.units, name=\"dense\")\n+        self.nested_layer = keras.layers.Dense(self.units, name=\"dense\")\n \n     def build(self, input_shape):\n         self.additional_weights = [\n@@ -58,7 +58,7 @@ ASSETS_DATA = \"These are my assets\"\n VARIABLES_DATA = np.random.random((10,))\n \n \n-@keras_core.saving.register_keras_serializable(package=\"my_custom_package\")\n+@keras.saving.register_keras_serializable(package=\"my_custom_package\")\n class LayerWithCustomSaving(MyDense):\n     def build(self, input_shape):\n         self.assets = ASSETS_DATA\n@@ -81,8 +81,8 @@ class LayerWithCustomSaving(MyDense):\n         self.stored_variables = np.array(store[\"variables\"])\n \n \n-@keras_core.saving.register_keras_serializable(package=\"my_custom_package\")\n-class CustomModelX(keras_core.Model):\n+@keras.saving.register_keras_serializable(package=\"my_custom_package\")\n+class CustomModelX(keras.Model):\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n         self.dense1 = MyDense(1, name=\"my_dense_1\")\n@@ -96,8 +96,8 @@ class CustomModelX(keras_core.Model):\n         return 1\n \n \n-@keras_core.saving.register_keras_serializable(package=\"my_custom_package\")\n-class ModelWithCustomSaving(keras_core.Model):\n+@keras.saving.register_keras_serializable(package=\"my_custom_package\")\n+class ModelWithCustomSaving(keras.Model):\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n         self.custom_dense = LayerWithCustomSaving(1)\n@@ -106,8 +106,8 @@ class ModelWithCustomSaving(keras_core.Model):\n         return self.custom_dense(inputs)\n \n \n-@keras_core.saving.register_keras_serializable(package=\"my_custom_package\")\n-class CompileOverridingModel(keras_core.Model):\n+@keras.saving.register_keras_serializable(package=\"my_custom_package\")\n+class CompileOverridingModel(keras.Model):\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n         self.dense1 = MyDense(1)\n@@ -119,13 +119,13 @@ class CompileOverridingModel(keras_core.Model):\n         return self.dense1(inputs)\n \n \n-@keras_core.saving.register_keras_serializable(package=\"my_custom_package\")\n-class CompileOverridingSequential(keras_core.Sequential):\n+@keras.saving.register_keras_serializable(package=\"my_custom_package\")\n+class CompileOverridingSequential(keras.Sequential):\n     def compile(self, *args, **kwargs):\n         super().compile(*args, **kwargs)\n \n \n-@keras_core.saving.register_keras_serializable(package=\"my_custom_package\")\n+@keras.saving.register_keras_serializable(package=\"my_custom_package\")\n def my_mean_squared_error(y_true, y_pred):\n     \"\"\"Identical to built-in `mean_squared_error`, but as a custom fn.\"\"\"\n     return ops.mean(ops.square(y_pred - y_true), axis=-1)\n@@ -137,29 +137,29 @@ def _get_subclassed_model(compile=True):\n         subclassed_model.compile(\n             optimizer=\"adam\",\n             loss=my_mean_squared_error,\n-            metrics=[keras_core.metrics.Hinge(), \"mse\"],\n+            metrics=[keras.metrics.Hinge(), \"mse\"],\n         )\n     return subclassed_model\n \n \n def _get_custom_sequential_model(compile=True):\n-    sequential_model = keras_core.Sequential(\n+    sequential_model = keras.Sequential(\n         [MyDense(1), MyDense(1)], name=\"sequential\"\n     )\n     if compile:\n         sequential_model.compile(\n             optimizer=\"adam\",\n             loss=my_mean_squared_error,\n-            metrics=[keras_core.metrics.Hinge(), \"mse\"],\n+            metrics=[keras.metrics.Hinge(), \"mse\"],\n         )\n     return sequential_model\n \n \n def _get_basic_sequential_model(compile=True):\n-    sequential_model = keras_core.Sequential(\n+    sequential_model = keras.Sequential(\n         [\n-            keras_core.layers.Dense(1, name=\"dense_1\"),\n-            keras_core.layers.Dense(1, name=\"dense_2\"),\n+            keras.layers.Dense(1, name=\"dense_1\"),\n+            keras.layers.Dense(1, name=\"dense_2\"),\n         ],\n         name=\"sequential\",\n     )\n@@ -167,35 +167,35 @@ def _get_basic_sequential_model(compile=True):\n         sequential_model.compile(\n             optimizer=\"adam\",\n             loss=my_mean_squared_error,\n-            metrics=[keras_core.metrics.Hinge(), \"mse\"],\n+            metrics=[keras.metrics.Hinge(), \"mse\"],\n         )\n     return sequential_model\n \n \n def _get_custom_functional_model(compile=True):\n-    inputs = keras_core.Input(shape=(4,), batch_size=2)\n+    inputs = keras.Input(shape=(4,), batch_size=2)\n     x = MyDense(1, name=\"first_dense\")(inputs)\n     outputs = MyDense(1, name=\"second_dense\")(x)\n-    functional_model = keras_core.Model(inputs, outputs)\n+    functional_model = keras.Model(inputs, outputs)\n     if compile:\n         functional_model.compile(\n             optimizer=\"adam\",\n             loss=my_mean_squared_error,\n-            metrics=[keras_core.metrics.Hinge(), \"mse\"],\n+            metrics=[keras.metrics.Hinge(), \"mse\"],\n         )\n     return functional_model\n \n \n def _get_basic_functional_model(compile=True):\n-    inputs = keras_core.Input(shape=(4,), batch_size=2)\n-    x = keras_core.layers.Dense(1, name=\"first_dense\")(inputs)\n-    outputs = keras_core.layers.Dense(1, name=\"second_dense\")(x)\n-    functional_model = keras_core.Model(inputs, outputs)\n+    inputs = keras.Input(shape=(4,), batch_size=2)\n+    x = keras.layers.Dense(1, name=\"first_dense\")(inputs)\n+    outputs = keras.layers.Dense(1, name=\"second_dense\")(x)\n+    functional_model = keras.Model(inputs, outputs)\n     if compile:\n         functional_model.compile(\n             optimizer=\"adam\",\n             loss=my_mean_squared_error,\n-            metrics=[keras_core.metrics.Hinge(), \"mse\"],\n+            metrics=[keras.metrics.Hinge(), \"mse\"],\n         )\n     return functional_model\n \n@@ -349,7 +349,7 @@ class SavingTest(testing.TestCase):\n             CompileOverridingModel()\n             if model_type == \"subclassed\"\n             else CompileOverridingSequential(\n-                [keras_core.layers.Embedding(4, 1), MyDense(1), MyDense(1)]\n+                [keras.layers.Embedding(4, 1), MyDense(1), MyDense(1)]\n             )\n         )\n         model.compile(\"sgd\", \"mse\")\n@@ -464,21 +464,21 @@ class SavingTest(testing.TestCase):\n \n     def test_partial_load(self):\n         temp_filepath = os.path.join(self.get_temp_dir(), \"mymodel.keras\")\n-        original_model = keras_core.Sequential(\n+        original_model = keras.Sequential(\n             [\n-                keras_core.Input(shape=(3,), batch_size=2),\n-                keras_core.layers.Dense(4),\n-                keras_core.layers.Dense(5),\n+                keras.Input(shape=(3,), batch_size=2),\n+                keras.layers.Dense(4),\n+                keras.layers.Dense(5),\n             ]\n         )\n         original_model.save(temp_filepath)\n \n         # Test with a model that has a differently shaped layer\n-        new_model = keras_core.Sequential(\n+        new_model = keras.Sequential(\n             [\n-                keras_core.Input(shape=(3,), batch_size=2),\n-                keras_core.layers.Dense(4),\n-                keras_core.layers.Dense(6),\n+                keras.Input(shape=(3,), batch_size=2),\n+                keras.layers.Dense(4),\n+                keras.layers.Dense(6),\n             ]\n         )\n         new_layer_kernel_value = np.array(new_model.layers[1].kernel)\n@@ -497,12 +497,12 @@ class SavingTest(testing.TestCase):\n         )\n \n         # Test with a model that has a new layer at the end\n-        new_model = keras_core.Sequential(\n+        new_model = keras.Sequential(\n             [\n-                keras_core.Input(shape=(3,), batch_size=2),\n-                keras_core.layers.Dense(4),\n-                keras_core.layers.Dense(5),\n-                keras_core.layers.Dense(5),\n+                keras.Input(shape=(3,), batch_size=2),\n+                keras.layers.Dense(4),\n+                keras.layers.Dense(5),\n+                keras.layers.Dense(5),\n             ]\n         )\n         new_layer_kernel_value = np.array(new_model.layers[2].kernel)\n@@ -525,7 +525,7 @@ class SavingTest(testing.TestCase):\n @pytest.mark.requires_trainable_backend\n class SavingAPITest(testing.TestCase):\n     def test_saving_api_errors(self):\n-        from keras_core.saving import saving_api\n+        from keras.saving import saving_api\n \n         model = _get_basic_functional_model()\n \n@@ -559,7 +559,7 @@ class SavingAPITest(testing.TestCase):\n         ref_input = np.random.random((2, 4))\n         ref_output = model.predict(ref_input)\n         model.save(temp_filepath)\n-        model = keras_core.saving.load_model(temp_filepath)\n+        model = keras.saving.load_model(temp_filepath)\n         self.assertAllClose(model.predict(ref_input), ref_output, atol=1e-6)\n \n     def test_model_api_endpoint_h5(self):\n@@ -568,7 +568,7 @@ class SavingAPITest(testing.TestCase):\n         ref_input = np.random.random((2, 4))\n         ref_output = model.predict(ref_input)\n         model.save(temp_filepath)\n-        model = keras_core.saving.load_model(temp_filepath)\n+        model = keras.saving.load_model(temp_filepath)\n         self.assertAllClose(model.predict(ref_input), ref_output, atol=1e-6)\n \n     def test_model_api_errors(self):\n@@ -590,10 +590,10 @@ class SavingAPITest(testing.TestCase):\n \n # def test_safe_mode(self):\n #     temp_filepath = os.path.join(self.get_temp_dir(), \"unsafe_model.keras\")\n-#     model = keras_core.Sequential(\n+#     model = keras.Sequential(\n #         [\n-#             keras_core.Input(shape=(3,)),\n-#             keras_core.layers.Dense(2, activation=lambda x: x * 2),\n+#             keras.Input(shape=(3,)),\n+#             keras.layers.Dense(2, activation=lambda x: x * 2),\n #         ]\n #     )\n #     model.save(temp_filepath)\n@@ -604,10 +604,10 @@ class SavingAPITest(testing.TestCase):\n #     def test_normalization_kpl(self):\n #         # With adapt\n #         temp_filepath = os.path.join(self.get_temp_dir(), \"norm_model.keras\")\n-#         model = keras_core.Sequential(\n+#         model = keras.Sequential(\n #             [\n-#                 keras_core.Input(shape=(3,)),\n-#                 keras_core.layers.Normalization(),\n+#                 keras.Input(shape=(3,)),\n+#                 keras.layers.Normalization(),\n #             ]\n #         )\n #         data = np.random.random((3, 3))\n@@ -619,10 +619,10 @@ class SavingAPITest(testing.TestCase):\n #         self.assertAllClose(ref_out, out, atol=1e-6)\n \n #         # Without adapt\n-#         model = keras_core.Sequential(\n+#         model = keras.Sequential(\n #             [\n-#                 keras_core.Input(shape=(3,)),\n-#                 keras_core.layers.Normalization(\n+#                 keras.Input(shape=(3,)),\n+#                 keras.layers.Normalization(\n #                     mean=np.random.random((3,)),\n #                     variance=np.random.random((3,)),\n #                 ),\n@@ -636,17 +636,17 @@ class SavingAPITest(testing.TestCase):\n \n \n # # This custom class lacks custom object registration.\n-# class CustomRNN(keras_core.layers.Layer):\n+# class CustomRNN(keras.layers.Layer):\n #     def __init__(self, units):\n #         super(CustomRNN, self).__init__()\n #         self.units = units\n-#         self.projection_1 = keras_core.layers.Dense(\n+#         self.projection_1 = keras.layers.Dense(\n #             units=units, activation=\"tanh\"\n #         )\n-#         self.projection_2 = keras_core.layers.Dense(\n+#         self.projection_2 = keras.layers.Dense(\n #             units=units, activation=\"tanh\"\n #         )\n-#         self.classifier = keras_core.layers.Dense(1)\n+#         self.classifier = keras.layers.Dense(1)\n \n #     def call(self, inputs):\n #         outputs = []\n@@ -662,9 +662,9 @@ class SavingAPITest(testing.TestCase):\n \n \n # # This class is properly registered with a `get_config()` method.\n-# # However, since it does not subclass keras_core.layers.Layer, it lacks\n+# # However, since it does not subclass keras.layers.Layer, it lacks\n # # `from_config()` for deserialization.\n-# @keras_core.saving.register_keras_serializable()\n+# @keras.saving.register_keras_serializable()\n # class GrowthFactor:\n #     def __init__(self, factor):\n #         self.factor = factor\n@@ -676,8 +676,8 @@ class SavingAPITest(testing.TestCase):\n #         return {\"factor\": self.factor}\n \n \n-# @keras_core.saving.register_keras_serializable(package=\"Complex\")\n-# class FactorLayer(keras_core.layers.Layer):\n+# @keras.saving.register_keras_serializable(package=\"Complex\")\n+# class FactorLayer(keras.layers.Layer):\n #     def __init__(self, factor):\n #         super().__init__()\n #         self.factor = factor\n@@ -692,15 +692,15 @@ class SavingAPITest(testing.TestCase):\n # # This custom model does not explicitly deserialize the layers it includes\n # # in its `get_config`. Explicit deserialization in a `from_config` override\n # # or `__init__` is needed here, or an error will be thrown at loading time.\n-# @keras_core.saving.register_keras_serializable(package=\"Complex\")\n-# class ComplexModel(keras_core.layers.Layer):\n+# @keras.saving.register_keras_serializable(package=\"Complex\")\n+# class ComplexModel(keras.layers.Layer):\n #     def __init__(self, first_layer, second_layer=None, **kwargs):\n #         super().__init__(**kwargs)\n #         self.first_layer = first_layer\n #         if second_layer is not None:\n #             self.second_layer = second_layer\n #         else:\n-#             self.second_layer = keras_core.layers.Dense(8)\n+#             self.second_layer = keras.layers.Dense(8)\n \n #     def get_config(self):\n #         config = super().get_config()\n@@ -725,48 +725,48 @@ class SavingAPITest(testing.TestCase):\n #         input_dim = 5\n #         batch_size = 16\n \n-#         inputs = keras_core.Input(\n+#         inputs = keras.Input(\n #             batch_shape=(batch_size, timesteps, input_dim)\n #         )\n-#         x = keras_core.layers.Conv1D(32, 3)(inputs)\n+#         x = keras.layers.Conv1D(32, 3)(inputs)\n #         outputs = CustomRNN(32)(x)\n \n-#         model = keras_core.Model(inputs, outputs)\n+#         model = keras.Model(inputs, outputs)\n \n #         with self.assertRaisesRegex(\n #             TypeError, \"is a custom class, please register it\"\n #         ):\n #             model.save(temp_filepath)\n-#             _ = keras_core.models.load_model(temp_filepath)\n+#             _ = keras.models.load_model(temp_filepath)\n \n #     def test_custom_object_without_from_config(self):\n #         temp_filepath = os.path.join(\n #             self.get_temp_dir(), \"custom_fn_model.keras\"\n #         )\n \n-#         inputs = keras_core.Input(shape=(4, 4))\n-#         outputs = keras_core.layers.Dense(\n+#         inputs = keras.Input(shape=(4, 4))\n+#         outputs = keras.layers.Dense(\n #             1, activation=GrowthFactor(0.5)\n #         )(inputs)\n-#         model = keras_core.Model(inputs, outputs)\n+#         model = keras.Model(inputs, outputs)\n \n #         model.save(temp_filepath)\n \n #         with self.assertRaisesRegex(\n #             TypeError, \"Unable to reconstruct an instance\"\n #         ):\n-#             _ = keras_core.models.load_model(temp_filepath)\n+#             _ = keras.models.load_model(temp_filepath)\n \n #     def test_complex_model_without_explicit_deserialization(self):\n #         temp_filepath = os.path.join(\n #             self.get_temp_dir(), \"complex_model.keras\"\n #         )\n \n-#         inputs = keras_core.Input((32,))\n+#         inputs = keras.Input((32,))\n #         outputs = ComplexModel(first_layer=FactorLayer(0.5))(inputs)\n-#         model = keras_core.Model(inputs, outputs)\n+#         model = keras.Model(inputs, outputs)\n \n #         model.save(temp_filepath)\n \n #         with self.assertRaisesRegex(TypeError, \"are explicitly deserialized\"):\n-#             _ = keras_core.models.load_model(temp_filepath)\n+#             _ = keras.models.load_model(temp_filepath)\n\n@@ -7,13 +7,13 @@ import warnings\n \n import numpy as np\n \n-from keras_core import api_export\n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.backend.common import global_state\n-from keras_core.saving import object_registration\n-from keras_core.utils import python_utils\n-from keras_core.utils.module_utils import tensorflow as tf\n+from keras import api_export\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.backend.common import global_state\n+from keras.saving import object_registration\n+from keras.utils import python_utils\n+from keras.utils.module_utils import tensorflow as tf\n \n PLAIN_TYPES = (str, int, float, bool)\n \n@@ -53,7 +53,7 @@ class SafeModeScope:\n         )\n \n \n-@keras_core_export(\"keras_core.config.enable_unsafe_deserialization\")\n+@keras_export(\"keras.config.enable_unsafe_deserialization\")\n def enable_unsafe_deserialization():\n     \"\"\"Disables safe mode globally, allowing deserialization of lambdas.\"\"\"\n     global_state.set_global_attribute(\"safe_mode_saving\", False)\n@@ -114,10 +114,10 @@ def record_object_after_deserialization(obj, obj_id):\n     id_to_obj_map[obj_id] = obj\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.saving.serialize_keras_object\",\n-        \"keras_core.utils.serialize_keras_object\",\n+        \"keras.saving.serialize_keras_object\",\n+        \"keras.utils.serialize_keras_object\",\n     ]\n )\n def serialize_keras_object(obj):\n@@ -289,10 +289,10 @@ def serialize_with_public_class(cls, inner_config=None):\n \n     Called to check and retrieve the config of any class that has a public\n     Keras API or has been registered as serializable via\n-    `keras_core.saving.register_keras_serializable()`.\n+    `keras.saving.register_keras_serializable()`.\n     \"\"\"\n-    # This gets the `keras_core.*` exported name, such as\n-    # \"keras_core.optimizers.Adam\".\n+    # This gets the `keras.*` exported name, such as\n+    # \"keras.optimizers.Adam\".\n     keras_api_name = api_export.get_name_from_symbol(cls)\n \n     # Case of custom or unknown class object\n@@ -324,7 +324,7 @@ def serialize_with_public_fn(fn, config, fn_module_name=None):\n \n     Called to check and retrieve the config of any function that has a public\n     Keras API or has been registered as serializable via\n-    `keras_core.saving.register_keras_serializable()`. If function's module name\n+    `keras.saving.register_keras_serializable()`. If function's module name\n     is already known, returns corresponding config.\n     \"\"\"\n     if fn_module_name:\n@@ -383,10 +383,10 @@ def serialize_dict(obj):\n     return {key: serialize_keras_object(value) for key, value in obj.items()}\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.saving.deserialize_keras_object\",\n-        \"keras_core.utils.deserialize_keras_object\",\n+        \"keras.saving.deserialize_keras_object\",\n+        \"keras.utils.deserialize_keras_object\",\n     ]\n )\n def deserialize_keras_object(\n@@ -405,9 +405,9 @@ def deserialize_keras_object(\n     - `config`: Dict. Library-defined or user-defined key-value pairs that store\n       the configuration of the object, as obtained by `object.get_config()`.\n     - `module`: String. The path of the python module. Built-in Keras classes\n-      expect to have prefix `keras_core`.\n+      expect to have prefix `keras`.\n     - `registered_name`: String. The key the class is registered under via\n-      `keras_core.saving.register_keras_serializable(package, name)` API. The\n+      `keras.saving.register_keras_serializable(package, name)` API. The\n       key has the format of '{package}>{name}', where `package` and `name` are\n       the arguments passed to `register_keras_serializable()`. If `name` is not\n       provided, it uses the class name. If `registered_name` successfully\n@@ -430,7 +430,7 @@ def deserialize_keras_object(\n             \"learning_rate\": 0.0010000000474974513,\n             \"name\": \"Adam\"\n         },\n-        \"module\": \"keras_core.optimizers\",\n+        \"module\": \"keras.optimizers\",\n         \"registered_name\": None\n     }\n     # Returns an `Adam` instance identical to the original one.\n@@ -446,7 +446,7 @@ def deserialize_keras_object(\n       \"config\": {\n           ...\n       },\n-      \"module\": \"keras_core.trainers.compile_utils\",\n+      \"module\": \"keras.trainers.compile_utils\",\n       \"registered_name\": \"MetricsList\"\n     }\n \n@@ -458,8 +458,8 @@ def deserialize_keras_object(\n     loss:\n \n     ```python\n-    @keras_core.saving.register_keras_serializable(package='my_package')\n-    class ModifiedMeanSquaredError(keras_core.losses.MeanSquaredError):\n+    @keras.saving.register_keras_serializable(package='my_package')\n+    class ModifiedMeanSquaredError(keras.losses.MeanSquaredError):\n       ...\n \n     dict_structure = {\n@@ -554,7 +554,7 @@ def deserialize_keras_object(\n                         f\"`{config['class_name']}`. If \"\n                         f\"`{config['class_name']}` is a custom class, please \"\n                         \"register it using the \"\n-                        \"`@keras_core.saving.register_keras_serializable()` \"\n+                        \"`@keras.saving.register_keras_serializable()` \"\n                         \"decorator.\"\n                     )\n                 config = config[\"class_name\"]\n@@ -637,7 +637,7 @@ def deserialize_keras_object(\n                 \"and thus it is disallowed by default. If you trust the \"\n                 \"source of the saved model, you can pass `safe_mode=False` to \"\n                 \"the loading function in order to allow `lambda` loading, \"\n-                \"or call `keras_core.config.enable_unsafe_deserialization()`.\"\n+                \"or call `keras.config.enable_unsafe_deserialization()`.\"\n             )\n         return python_utils.func_load(inner_config[\"value\"])\n     if tf is not None and config[\"class_name\"] == \"__typespec__\":\n@@ -741,7 +741,7 @@ def _retrieve_class_or_fn(\n         # we cannot always use direct import, because the exported\n         # module name might not match the package structure\n         # (e.g. experimental symbols).\n-        if module == \"keras_core\" or module.startswith(\"keras_core.\"):\n+        if module == \"keras\" or module.startswith(\"keras.\"):\n             api_name = module + \".\" + name\n \n             obj = api_export.get_symbol_from_name(api_name)\n@@ -755,7 +755,7 @@ def _retrieve_class_or_fn(\n         if obj_type == \"function\" and module == \"builtins\":\n             for mod in BUILTIN_MODULES:\n                 obj = api_export.get_symbol_from_name(\n-                    \"keras_core.\" + mod + \".\" + name\n+                    \"keras.\" + mod + \".\" + name\n                 )\n                 if obj is not None:\n                     return obj\n@@ -791,6 +791,6 @@ def _retrieve_class_or_fn(\n     raise TypeError(\n         f\"Could not locate {obj_type} '{name}'. \"\n         \"Make sure custom classes are decorated with \"\n-        \"`@keras_core.saving.register_keras_serializable()`. \"\n+        \"`@keras.saving.register_keras_serializable()`. \"\n         f\"Full object config: {full_config}\"\n     )\n\n@@ -5,17 +5,17 @@ import json\n import numpy as np\n import pytest\n \n-import keras_core\n-from keras_core import ops\n-from keras_core import testing\n-from keras_core.saving import serialization_lib\n+import keras\n+from keras import ops\n+from keras import testing\n+from keras.saving import serialization_lib\n \n \n def custom_fn(x):\n     return x**2\n \n \n-class CustomLayer(keras_core.layers.Layer):\n+class CustomLayer(keras.layers.Layer):\n     def __init__(self, factor):\n         super().__init__()\n         self.factor = factor\n@@ -27,13 +27,13 @@ class CustomLayer(keras_core.layers.Layer):\n         return {\"factor\": self.factor}\n \n \n-class NestedCustomLayer(keras_core.layers.Layer):\n+class NestedCustomLayer(keras.layers.Layer):\n     def __init__(self, factor, dense=None, activation=None):\n         super().__init__()\n         self.factor = factor\n \n         if dense is None:\n-            self.dense = keras_core.layers.Dense(1, activation=custom_fn)\n+            self.dense = keras.layers.Dense(1, activation=custom_fn)\n         else:\n             self.dense = serialization_lib.deserialize_keras_object(dense)\n         self.activation = serialization_lib.deserialize_keras_object(activation)\n@@ -49,7 +49,7 @@ class NestedCustomLayer(keras_core.layers.Layer):\n         }\n \n \n-class WrapperLayer(keras_core.layers.Wrapper):\n+class WrapperLayer(keras.layers.Wrapper):\n     def call(self, x):\n         return self.layer(x)\n \n@@ -82,7 +82,7 @@ class SerializationLibTest(testing.TestCase):\n             self.assertEqual(serialized, reserialized)\n \n     def test_builtin_layers(self):\n-        layer = keras_core.layers.Dense(\n+        layer = keras.layers.Dense(\n             3,\n             name=\"foo\",\n             trainable=False,\n@@ -112,7 +112,7 @@ class SerializationLibTest(testing.TestCase):\n         self.assertEqual(serialized, reserialized)\n \n         # Test inside layer\n-        dense = keras_core.layers.Dense(1, activation=custom_fn)\n+        dense = keras.layers.Dense(1, activation=custom_fn)\n         dense.build((None, 2))\n         _, new_dense, _ = self.roundtrip(\n             dense, custom_objects={\"custom_fn\": custom_fn}\n@@ -159,7 +159,7 @@ class SerializationLibTest(testing.TestCase):\n \n     # TODO\n     # def test_lambda_layer(self):\n-    #     lmbda = keras_core.layers.Lambda(lambda x: x**2)\n+    #     lmbda = keras.layers.Lambda(lambda x: x**2)\n     #     with self.assertRaisesRegex(ValueError, \"arbitrary code execution\"):\n     #         self.roundtrip(lmbda, safe_mode=True)\n \n@@ -170,7 +170,7 @@ class SerializationLibTest(testing.TestCase):\n     #     self.assertAllClose(y1, y2, atol=1e-5)\n \n     # def test_safe_mode_scope(self):\n-    #     lmbda = keras_core.layers.Lambda(lambda x: x**2)\n+    #     lmbda = keras.layers.Lambda(lambda x: x**2)\n     #     with serialization_lib.SafeModeScope(safe_mode=True):\n     #         with self.assertRaisesRegex(\n     #             ValueError, \"arbitrary code execution\"\n@@ -185,12 +185,12 @@ class SerializationLibTest(testing.TestCase):\n \n     @pytest.mark.requires_trainable_backend\n     def test_dict_inputs_outputs(self):\n-        input_foo = keras_core.Input((2,), name=\"foo\")\n-        input_bar = keras_core.Input((2,), name=\"bar\")\n-        dense = keras_core.layers.Dense(1)\n+        input_foo = keras.Input((2,), name=\"foo\")\n+        input_bar = keras.Input((2,), name=\"bar\")\n+        dense = keras.layers.Dense(1)\n         output_foo = dense(input_foo)\n         output_bar = dense(input_bar)\n-        model = keras_core.Model(\n+        model = keras.Model(\n             {\"foo\": input_foo, \"bar\": input_bar},\n             {\"foo\": output_foo, \"bar\": output_bar},\n         )\n@@ -207,13 +207,13 @@ class SerializationLibTest(testing.TestCase):\n     @pytest.mark.requires_trainable_backend\n     def test_shared_inner_layer(self):\n         with serialization_lib.ObjectSharingScope():\n-            input_1 = keras_core.Input((2,))\n-            input_2 = keras_core.Input((2,))\n-            shared_layer = keras_core.layers.Dense(1)\n+            input_1 = keras.Input((2,))\n+            input_2 = keras.Input((2,))\n+            shared_layer = keras.layers.Dense(1)\n             output_1 = shared_layer(input_1)\n             wrapper_layer = WrapperLayer(shared_layer)\n             output_2 = wrapper_layer(input_2)\n-            model = keras_core.Model([input_1, input_2], [output_1, output_2])\n+            model = keras.Model([input_1, input_2], [output_1, output_2])\n             _, new_model, _ = self.roundtrip(\n                 model, custom_objects={\"WrapperLayer\": WrapperLayer}\n             )\n@@ -223,11 +223,11 @@ class SerializationLibTest(testing.TestCase):\n \n     @pytest.mark.requires_trainable_backend\n     def test_functional_subclass(self):\n-        class PlainFunctionalSubclass(keras_core.Model):\n+        class PlainFunctionalSubclass(keras.Model):\n             pass\n \n-        inputs = keras_core.Input((2,), batch_size=3)\n-        outputs = keras_core.layers.Dense(1)(inputs)\n+        inputs = keras.Input((2,), batch_size=3)\n+        outputs = keras.layers.Dense(1)(inputs)\n         model = PlainFunctionalSubclass(inputs, outputs)\n         x = ops.random.normal((2, 2))\n         y1 = model(x)\n@@ -240,10 +240,10 @@ class SerializationLibTest(testing.TestCase):\n         self.assertAllClose(y1, y2, atol=1e-5)\n         self.assertIsInstance(new_model, PlainFunctionalSubclass)\n \n-        class FunctionalSubclassWCustomInit(keras_core.Model):\n+        class FunctionalSubclassWCustomInit(keras.Model):\n             def __init__(self, num_units=2):\n-                inputs = keras_core.Input((2,), batch_size=3)\n-                outputs = keras_core.layers.Dense(num_units)(inputs)\n+                inputs = keras.Input((2,), batch_size=3)\n+                outputs = keras.layers.Dense(num_units)(inputs)\n                 super().__init__(inputs, outputs)\n                 self.num_units = num_units\n \n@@ -265,7 +265,7 @@ class SerializationLibTest(testing.TestCase):\n         self.assertIsInstance(new_model, FunctionalSubclassWCustomInit)\n \n     def test_shared_object(self):\n-        class MyLayer(keras_core.layers.Layer):\n+        class MyLayer(keras.layers.Layer):\n             def __init__(self, activation, **kwargs):\n                 super().__init__(**kwargs)\n                 if isinstance(activation, dict):\n@@ -315,8 +315,8 @@ class SerializationLibTest(testing.TestCase):\n         self.assertIs(new_layers[0].activation, new_layers[1].activation)\n \n \n-@keras_core.saving.register_keras_serializable()\n-class MyDense(keras_core.layers.Layer):\n+@keras.saving.register_keras_serializable()\n+class MyDense(keras.layers.Layer):\n     def __init__(\n         self,\n         units,\n@@ -352,8 +352,8 @@ class MyDense(keras_core.layers.Layer):\n         return ops.matmul(inputs, self._kernel)\n \n \n-@keras_core.saving.register_keras_serializable()\n-class MyWrapper(keras_core.layers.Layer):\n+@keras.saving.register_keras_serializable()\n+class MyWrapper(keras.layers.Layer):\n     def __init__(self, wrapped, **kwargs):\n         super().__init__(**kwargs)\n         self._wrapped = wrapped\n@@ -363,7 +363,7 @@ class MyWrapper(keras_core.layers.Layer):\n \n     @classmethod\n     def from_config(cls, config):\n-        config[\"wrapped\"] = keras_core.saving.deserialize_keras_object(\n+        config[\"wrapped\"] = keras.saving.deserialize_keras_object(\n             config[\"wrapped\"]\n         )\n         return cls(**config)\n\n@@ -0,0 +1 @@\n+from keras.testing.test_case import TestCase\n\n@@ -6,14 +6,14 @@ import unittest\n import numpy as np\n import tree\n \n-from keras_core import backend\n-from keras_core import ops\n-from keras_core import utils\n-from keras_core.backend.common import is_float_dtype\n-from keras_core.backend.common import standardize_dtype\n-from keras_core.backend.common.keras_tensor import KerasTensor\n-from keras_core.models import Model\n-from keras_core.utils import traceback_utils\n+from keras import backend\n+from keras import ops\n+from keras import utils\n+from keras.backend.common import is_float_dtype\n+from keras.backend.common import standardize_dtype\n+from keras.backend.common.keras_tensor import KerasTensor\n+from keras.models import Model\n+from keras.utils import traceback_utils\n \n \n class TestCase(unittest.TestCase):\n@@ -69,9 +69,9 @@ class TestCase(unittest.TestCase):\n         self.assertEqual(len(iterable), expected_len, msg=msg)\n \n     def run_class_serialization_test(self, instance, custom_objects=None):\n-        from keras_core.saving import custom_object_scope\n-        from keras_core.saving import deserialize_keras_object\n-        from keras_core.saving import serialize_keras_object\n+        from keras.saving import custom_object_scope\n+        from keras.saving import deserialize_keras_object\n+        from keras.saving import serialize_keras_object\n \n         # get_config roundtrip\n         cls = instance.__class__\n@@ -421,7 +421,7 @@ def create_keras_tensors(input_shape, dtype, sparse):\n \n \n def create_eager_tensors(input_shape, dtype, sparse):\n-    from keras_core.backend import random\n+    from keras.backend import random\n \n     if dtype not in [\n         \"float16\",\n\n\n\n@@ -1,10 +1,10 @@\n import tree\n \n-from keras_core import backend\n-from keras_core import losses as losses_module\n-from keras_core import metrics as metrics_module\n-from keras_core import ops\n-from keras_core.utils.naming import get_object_name\n+from keras import backend\n+from keras import losses as losses_module\n+from keras import metrics as metrics_module\n+from keras import ops\n+from keras.utils.naming import get_object_name\n \n \n class MetricsList(metrics_module.Metric):\n\n@@ -1,12 +1,12 @@\n import numpy as np\n from absl.testing import parameterized\n \n-from keras_core import backend\n-from keras_core import metrics as losses_module\n-from keras_core import metrics as metrics_module\n-from keras_core import testing\n-from keras_core.trainers.compile_utils import CompileLoss\n-from keras_core.trainers.compile_utils import CompileMetrics\n+from keras import backend\n+from keras import metrics as losses_module\n+from keras import metrics as metrics_module\n+from keras import testing\n+from keras.trainers.compile_utils import CompileLoss\n+from keras.trainers.compile_utils import CompileMetrics\n \n \n class TestCompileMetrics(testing.TestCase):\n\n\n@@ -3,10 +3,10 @@ import math\n import numpy as np\n import tree\n \n-from keras_core import backend\n-from keras_core.trainers.data_adapters import data_adapter_utils\n-from keras_core.trainers.data_adapters.data_adapter import DataAdapter\n-from keras_core.utils.nest import lists_to_tuples\n+from keras import backend\n+from keras.trainers.data_adapters import data_adapter_utils\n+from keras.trainers.data_adapters.data_adapter import DataAdapter\n+from keras.utils.nest import lists_to_tuples\n \n try:\n     import pandas\n@@ -107,7 +107,7 @@ class ArrayDataAdapter(DataAdapter):\n             yield tree.map_structure(lambda x: x[start:stop], inputs)\n \n     def get_tf_dataset(self):\n-        from keras_core.utils.module_utils import tensorflow as tf\n+        from keras.utils.module_utils import tensorflow as tf\n \n         inputs = self._inputs\n         shuffle = self._shuffle\n@@ -301,7 +301,7 @@ def convert_to_arrays(arrays):\n             elif isinstance(x, pandas.DataFrame):\n                 x = x.to_numpy()\n         if is_tf_ragged_tensor(x):\n-            from keras_core.utils.module_utils import tensorflow as tf\n+            from keras.utils.module_utils import tensorflow as tf\n \n             # Convert floats to floatx.\n             if (\n\n@@ -4,9 +4,9 @@ import pytest\n import tensorflow as tf\n from absl.testing import parameterized\n \n-from keras_core import backend\n-from keras_core import testing\n-from keras_core.trainers.data_adapters import array_data_adapter\n+from keras import backend\n+from keras import testing\n+from keras.trainers.data_adapters import array_data_adapter\n \n \n class TestArrayDataAdapter(testing.TestCase, parameterized.TestCase):\n\n\n@@ -3,8 +3,8 @@ import math\n import numpy as np\n import tree\n \n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n+from keras import backend\n+from keras.api_export import keras_export\n \n try:\n     import pandas\n@@ -17,14 +17,14 @@ except ImportError:\n # backend framework we are not currently using just to do type-checking.\n ARRAY_TYPES = (np.ndarray,)\n if backend.backend() == \"tensorflow\":\n-    from keras_core.utils.module_utils import tensorflow as tf\n+    from keras.utils.module_utils import tensorflow as tf\n \n     ARRAY_TYPES = ARRAY_TYPES + (np.ndarray, tf.RaggedTensor)\n if pandas:\n     ARRAY_TYPES = ARRAY_TYPES + (pandas.Series, pandas.DataFrame)\n \n \n-@keras_core_export(\"keras_core.utils.unpack_x_y_sample_weight\")\n+@keras_export(\"keras.utils.unpack_x_y_sample_weight\")\n def unpack_x_y_sample_weight(data):\n     \"\"\"Unpacks user-provided data tuple.\n \n@@ -67,7 +67,7 @@ def unpack_x_y_sample_weight(data):\n     raise ValueError(error_msg)\n \n \n-@keras_core_export(\"keras_core.utils.pack_x_y_sample_weight\")\n+@keras_export(\"keras.utils.pack_x_y_sample_weight\")\n def pack_x_y_sample_weight(x, y=None, sample_weight=None):\n     \"\"\"Packs user-provided data into a tuple.\n \n\n@@ -2,7 +2,7 @@ import itertools\n \n import tree\n \n-from keras_core.trainers.data_adapters.data_adapter import DataAdapter\n+from keras.trainers.data_adapters.data_adapter import DataAdapter\n \n \n class GeneratorDataAdapter(DataAdapter):\n@@ -22,7 +22,7 @@ class GeneratorDataAdapter(DataAdapter):\n             )\n \n     def _set_tf_output_signature(self):\n-        from keras_core.utils.module_utils import tensorflow as tf\n+        from keras.utils.module_utils import tensorflow as tf\n \n         data, generator = peek_and_restore(self.generator)\n         self.generator = generator\n@@ -47,7 +47,7 @@ class GeneratorDataAdapter(DataAdapter):\n             yield batch\n \n     def get_tf_dataset(self):\n-        from keras_core.utils.module_utils import tensorflow as tf\n+        from keras.utils.module_utils import tensorflow as tf\n \n         if self._output_signature is None:\n             self._set_tf_output_signature()\n\n@@ -4,8 +4,8 @@ import numpy as np\n import tensorflow as tf\n from absl.testing import parameterized\n \n-from keras_core import testing\n-from keras_core.trainers.data_adapters import generator_data_adapter\n+from keras import testing\n+from keras.trainers.data_adapters import generator_data_adapter\n \n \n def example_generator(x, y, sample_weight=None, batch_size=32):\n\n@@ -10,12 +10,12 @@ from contextlib import closing\n import numpy as np\n import tree\n \n-from keras_core.api_export import keras_core_export\n-from keras_core.trainers.data_adapters import data_adapter_utils\n-from keras_core.trainers.data_adapters.data_adapter import DataAdapter\n+from keras.api_export import keras_export\n+from keras.trainers.data_adapters import data_adapter_utils\n+from keras.trainers.data_adapters.data_adapter import DataAdapter\n \n \n-@keras_core_export([\"keras_core.utils.PyDataset\", \"keras_core.utils.Sequence\"])\n+@keras_export([\"keras.utils.PyDataset\", \"keras.utils.Sequence\"])\n class PyDataset:\n     \"\"\"Base class for defining a parallel dataset using Python code.\n \n@@ -64,7 +64,7 @@ class PyDataset:\n     # Here, `x_set` is list of path to the images\n     # and `y_set` are the associated classes.\n \n-    class CIFAR10PyDataset(keras_core.utils.PyDataset):\n+    class CIFAR10PyDataset(keras.utils.PyDataset):\n \n         def __init__(self, x_set, y_set, batch_size, **kwargs):\n             super().__init__(**kwargs)\n@@ -173,7 +173,7 @@ class PyDataset:\n \n \n class PyDatasetAdapter(DataAdapter):\n-    \"\"\"Adapter for `keras_core.utils.PyDataset` instances.\"\"\"\n+    \"\"\"Adapter for `keras.utils.PyDataset` instances.\"\"\"\n \n     def __init__(\n         self,\n@@ -188,7 +188,7 @@ class PyDatasetAdapter(DataAdapter):\n         self._output_signature = None\n \n     def _set_tf_output_signature(self):\n-        from keras_core.utils.module_utils import tensorflow as tf\n+        from keras.utils.module_utils import tensorflow as tf\n \n         def get_tensor_spec(x):\n             shape = x.shape\n@@ -273,7 +273,7 @@ class PyDatasetAdapter(DataAdapter):\n                 self.enqueuer.stop()\n \n     def get_tf_dataset(self):\n-        from keras_core.utils.module_utils import tensorflow as tf\n+        from keras.utils.module_utils import tensorflow as tf\n \n         if self._output_signature is None:\n             self._set_tf_output_signature()\n@@ -483,7 +483,7 @@ class OrderedEnqueuer(PyDatasetEnqueuer):\n     \"\"\"Builds a Enqueuer from a PyDataset.\n \n     Args:\n-        py_dataset: A `keras_core.utils.PyDataset` object.\n+        py_dataset: A `keras.utils.PyDataset` object.\n         use_multiprocessing: use multiprocessing if True, otherwise threading\n         shuffle: whether to shuffle the data at the beginning of each epoch\n     \"\"\"\n\n@@ -5,9 +5,9 @@ import numpy as np\n import tensorflow as tf\n from absl.testing import parameterized\n \n-from keras_core import testing\n-from keras_core.trainers.data_adapters import py_dataset_adapter\n-from keras_core.utils.rng_utils import set_random_seed\n+from keras import testing\n+from keras.trainers.data_adapters import py_dataset_adapter\n+from keras.utils.rng_utils import set_random_seed\n \n \n class ExamplePyDataset(py_dataset_adapter.PyDataset):\n\n@@ -1,14 +1,14 @@\n import tree\n \n-from keras_core.trainers.data_adapters import data_adapter_utils\n-from keras_core.trainers.data_adapters.data_adapter import DataAdapter\n+from keras.trainers.data_adapters import data_adapter_utils\n+from keras.trainers.data_adapters.data_adapter import DataAdapter\n \n \n class TFDatasetAdapter(DataAdapter):\n     \"\"\"Adapter that handles `tf.data.Dataset`.\"\"\"\n \n     def __init__(self, dataset, class_weight=None):\n-        from keras_core.utils.module_utils import tensorflow as tf\n+        from keras.utils.module_utils import tensorflow as tf\n \n         if not isinstance(dataset, tf.data.Dataset):\n             raise ValueError(\n@@ -64,7 +64,7 @@ def make_class_weight_map_fn(class_weight):\n         A function that can be used with `tf.data.Dataset.map` to apply class\n         weighting.\n     \"\"\"\n-    from keras_core.utils.module_utils import tensorflow as tf\n+    from keras.utils.module_utils import tensorflow as tf\n \n     class_weight_tensor = tf.convert_to_tensor(\n         [\n\n@@ -1,8 +1,8 @@\n import numpy as np\n import tensorflow as tf\n \n-from keras_core import testing\n-from keras_core.trainers.data_adapters import tf_dataset_adapter\n+from keras import testing\n+from keras.trainers.data_adapters import tf_dataset_adapter\n \n \n class TestTFDatasetAdapter(testing.TestCase):\n\n@@ -1,6 +1,6 @@\n import tree\n \n-from keras_core.trainers.data_adapters.data_adapter import DataAdapter\n+from keras.trainers.data_adapters.data_adapter import DataAdapter\n \n \n class TorchDataLoaderAdapter(DataAdapter):\n@@ -28,7 +28,7 @@ class TorchDataLoaderAdapter(DataAdapter):\n         return self._dataloader\n \n     def get_tf_dataset(self):\n-        from keras_core.utils.module_utils import tensorflow as tf\n+        from keras.utils.module_utils import tensorflow as tf\n \n         output_signature = self.peek_and_get_tensor_spec()\n         return tf.data.Dataset.from_generator(\n@@ -37,7 +37,7 @@ class TorchDataLoaderAdapter(DataAdapter):\n         )\n \n     def peek_and_get_tensor_spec(self):\n-        from keras_core.utils.module_utils import tensorflow as tf\n+        from keras.utils.module_utils import tensorflow as tf\n \n         batch_data = next(iter(self._dataloader))\n \n\n@@ -2,9 +2,9 @@ import numpy as np\n import pytest\n import tensorflow as tf\n \n-from keras_core import backend\n-from keras_core import testing\n-from keras_core.trainers.data_adapters.torch_data_adapter import (\n+from keras import backend\n+from keras import testing\n+from keras.trainers.data_adapters.torch_data_adapter import (\n     TorchDataLoaderAdapter,\n )\n \n\n@@ -41,12 +41,12 @@ or until there is no data\n import types\n import warnings\n \n-from keras_core.trainers.data_adapters import array_data_adapter\n-from keras_core.trainers.data_adapters import generator_data_adapter\n-from keras_core.trainers.data_adapters import py_dataset_adapter\n-from keras_core.trainers.data_adapters import tf_dataset_adapter\n-from keras_core.trainers.data_adapters import torch_data_adapter\n-from keras_core.utils.module_utils import tensorflow as tf\n+from keras.trainers.data_adapters import array_data_adapter\n+from keras.trainers.data_adapters import generator_data_adapter\n+from keras.trainers.data_adapters import py_dataset_adapter\n+from keras.trainers.data_adapters import tf_dataset_adapter\n+from keras.trainers.data_adapters import torch_data_adapter\n+from keras.utils.module_utils import tensorflow as tf\n \n \n class EpochIterator:\n\n@@ -2,9 +2,9 @@ import numpy as np\n import pytest\n import tensorflow as tf\n \n-from keras_core import backend\n-from keras_core import testing\n-from keras_core.trainers import epoch_iterator\n+from keras import backend\n+from keras import testing\n+from keras.trainers import epoch_iterator\n \n \n class TestEpochIterator(testing.TestCase):\n\n@@ -1,16 +1,16 @@\n import platform\n import warnings\n \n-from keras_core import backend\n-from keras_core import metrics as metrics_module\n-from keras_core import ops\n-from keras_core import optimizers\n-from keras_core.optimizers.loss_scale_optimizer import LossScaleOptimizer\n-from keras_core.saving import serialization_lib\n-from keras_core.trainers.compile_utils import CompileLoss\n-from keras_core.trainers.compile_utils import CompileMetrics\n-from keras_core.utils import traceback_utils\n-from keras_core.utils import tracking\n+from keras import backend\n+from keras import metrics as metrics_module\n+from keras import ops\n+from keras import optimizers\n+from keras.optimizers.loss_scale_optimizer import LossScaleOptimizer\n+from keras.saving import serialization_lib\n+from keras.trainers.compile_utils import CompileLoss\n+from keras.trainers.compile_utils import CompileMetrics\n+from keras.utils import traceback_utils\n+from keras.utils import tracking\n \n \n class Trainer:\n@@ -42,20 +42,20 @@ class Trainer:\n \n         ```python\n         model.compile(\n-            optimizer=keras_core.optimizers.Adam(learning_rate=1e-3),\n-            loss=keras_core.losses.BinaryCrossentropy(),\n+            optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n+            loss=keras.losses.BinaryCrossentropy(),\n             metrics=[\n-                keras_core.metrics.BinaryAccuracy(),\n-                keras_core.metrics.FalseNegatives(),\n+                keras.metrics.BinaryAccuracy(),\n+                keras.metrics.FalseNegatives(),\n             ],\n         )\n         ```\n \n         Args:\n             optimizer: String (name of optimizer) or optimizer instance. See\n-                `keras_core.optimizers`.\n+                `keras.optimizers`.\n             loss: Loss function. May be a string (name of loss function), or\n-                a `keras_core.losses.Loss` instance. See `keras_core.losses`. A\n+                a `keras.losses.Loss` instance. See `keras.losses`. A\n                 loss function is any callable with the signature\n                 `loss = fn(y_true, y_pred)`, where `y_true` are the ground truth\n                 values, and `y_pred` are the model's predictions.\n@@ -75,8 +75,8 @@ class Trainer:\n                 coefficients.\n             metrics: List of metrics to be evaluated by the model during\n                 training and testing. Each of this can be a string (name of a\n-                built-in function), function or a `keras_core.metrics.Metric`\n-                instance. See `keras_core.metrics`. Typically you will use\n+                built-in function), function or a `keras.metrics.Metric`\n+                instance. See `keras.metrics`. Typically you will use\n                 `metrics=['accuracy']`. A function is any callable with the\n                 signature `result = fn(y_true, _pred)`. To specify different\n                 metrics for different outputs of a multi-output model, you could\n@@ -87,9 +87,9 @@ class Trainer:\n                 `metrics=[['accuracy'], ['accuracy', 'mse']]`\n                 or `metrics=['accuracy', ['accuracy', 'mse']]`. When you pass\n                 the strings 'accuracy' or 'acc', we convert this to one of\n-                `keras_core.metrics.BinaryAccuracy`,\n-                `keras_core.metrics.CategoricalAccuracy`,\n-                `keras_core.metrics.SparseCategoricalAccuracy` based on the\n+                `keras.metrics.BinaryAccuracy`,\n+                `keras.metrics.CategoricalAccuracy`,\n+                `keras.metrics.SparseCategoricalAccuracy` based on the\n                 shapes of the targets and of the model output. A similar\n                 conversion is done for the strings `\"crossentropy\"`\n                 and `\"ce\"` as well.\n@@ -345,7 +345,7 @@ class Trainer:\n \n         Returns:\n             A `dict` containing values that will be passed to\n-            `keras_core.callbacks.CallbackList.on_train_batch_end()`. Typically,\n+            `keras.callbacks.CallbackList.on_train_batch_end()`. Typically,\n             the values of the metrics listed in `self.metrics` are returned.\n             Example: `{'loss': 0.2, 'accuracy': 0.7}`.\n         \"\"\"\n@@ -405,18 +405,18 @@ class Trainer:\n               - A `tf.data.Dataset`. Should return a tuple\n                 of either `(inputs, targets)` or\n                 `(inputs, targets, sample_weights)`.\n-              - A `keras_core.utils.PyDataset` returning `(inputs,\n+              - A `keras.utils.PyDataset` returning `(inputs,\n                 targets)` or `(inputs, targets, sample_weights)`.\n             y: Target data. Like the input data `x`,\n                 it could be either NumPy array(s) or backend-native tensor(s).\n                 If `x` is a dataset, generator,\n-                or `keras_core.utils.PyDataset` instance, `y` should\n+                or `keras.utils.PyDataset` instance, `y` should\n                 not be specified (since targets will be obtained from `x`).\n             batch_size: Integer or `None`.\n                 Number of samples per gradient update.\n                 If unspecified, `batch_size` will default to 32.\n                 Do not specify the `batch_size` if your data is in the\n-                form of datasets, generators, or `keras_core.utils.PyDataset`\n+                form of datasets, generators, or `keras.utils.PyDataset`\n                 instances (since they generate batches).\n             epochs: Integer. Number of epochs to train the model.\n                 An epoch is an iteration over the entire `x` and `y`\n@@ -435,13 +435,13 @@ class Trainer:\n                 particularly useful when logged to a file,\n                 so `verbose=2` is recommended when not running interactively\n                 (e.g., in a production environment). Defaults to `\"auto\"`.\n-            callbacks: List of `keras_core.callbacks.Callback` instances.\n+            callbacks: List of `keras.callbacks.Callback` instances.\n                 List of callbacks to apply during training.\n-                See `keras_core.callbacks`. Note\n-                `keras_core.callbacks.ProgbarLogger` and\n-                `keras_core.callbacks.History` callbacks are created\n+                See `keras.callbacks`. Note\n+                `keras.callbacks.ProgbarLogger` and\n+                `keras.callbacks.History` callbacks are created\n                 automatically and need not be passed to `model.fit()`.\n-                `keras_core.callbacks.ProgbarLogger` is created\n+                `keras.callbacks.ProgbarLogger` is created\n                 or not based on the `verbose` argument in `model.fit()`.\n             validation_split: Float between 0 and 1.\n                 Fraction of the training data to be used as validation data.\n@@ -452,7 +452,7 @@ class Trainer:\n                 The validation data is selected from the last samples\n                 in the `x` and `y` data provided, before shuffling. This\n                 argument is not supported when `x` is a dataset, generator or\n-                `keras_core.utils.PyDataset` instance.\n+                `keras.utils.PyDataset` instance.\n                 If both `validation_data` and `validation_split` are provided,\n                 `validation_data` will override `validation_split`.\n             validation_data: Data on which to evaluate\n@@ -467,7 +467,7 @@ class Trainer:\n                   - A tuple `(x_val, y_val, val_sample_weights)` of NumPy\n                     arrays.\n                   - A `tf.data.Dataset`.\n-                  - A Python generator or `keras_core.utils.PyDataset` returning\n+                  - A Python generator or `keras.utils.PyDataset` returning\n                   `(inputs, targets)` or `(inputs, targets, sample_weights)`.\n             shuffle: Boolean, whether to shuffle the training data\n                 before each epoch. This argument is\n@@ -491,7 +491,7 @@ class Trainer:\n                 `(samples, sequence_length)`,\n                 to apply a different weight to every timestep of every sample.\n                 This argument is not supported when `x` is a dataset, generator,\n-                or `keras_core.utils.PyDataset` instance, instead provide the\n+                or `keras.utils.PyDataset` instance, instead provide the\n                 sample_weights as the third element of `x`.\n                 Note that sample weighting does not apply to metrics specified\n                 via the `metrics` argument in `compile()`. To apply sample\n@@ -528,7 +528,7 @@ class Trainer:\n                 Number of samples per validation batch.\n                 If unspecified, will default to `batch_size`.\n                 Do not specify the `validation_batch_size` if your data is in\n-                the form of datasets or `keras_core.utils.PyDataset`\n+                the form of datasets or `keras.utils.PyDataset`\n                 instances (since they generate batches).\n             validation_freq: Only relevant if validation data is provided.\n               Specifies how many training epochs to run\n@@ -537,7 +537,7 @@ class Trainer:\n \n         Unpacking behavior for iterator-like inputs:\n             A common pattern is to pass an iterator like object such as a\n-            `tf.data.Dataset` or a `keras_core.utils.PyDataset` to `fit()`,\n+            `tf.data.Dataset` or a `keras.utils.PyDataset` to `fit()`,\n             which will in fact yield not only features (`x`)\n             but optionally targets (`y`) and sample weights (`sample_weight`).\n             Keras requires that the output of such iterator-likes be\n@@ -596,17 +596,17 @@ class Trainer:\n                 - A `tf.data.Dataset`. Should return a tuple\n                     of either `(inputs, targets)` or\n                     `(inputs, targets, sample_weights)`.\n-                - A generator or `keras_core.utils.PyDataset` returning\n+                - A generator or `keras.utils.PyDataset` returning\n                     `(inputs, targets)` or `(inputs, targets, sample_weights)`.\n             y: Target data. Like the input data `x`, it could be either NumPy\n                 array(s) or backend-native tensor(s).\n-                If `x` is a `tf.data.Dataset` or `keras_core.utils.PyDataset`\n+                If `x` is a `tf.data.Dataset` or `keras.utils.PyDataset`\n                 instance, `y` should not be specified\n                 (since targets will be obtained from the iterator/dataset).\n             batch_size: Integer or `None`. Number of samples per batch of\n                 computation. If unspecified, `batch_size` will default to 32. Do\n                 not specify the `batch_size` if your data is in the form of a\n-                dataset, generators, or `keras_core.utils.PyDataset` instances\n+                dataset, generators, or `keras.utils.PyDataset` instances\n                 (since they generate batches).\n             verbose: `\"auto\"`, 0, 1, or 2. Verbosity mode.\n                 0 = silent, 1 = progress bar, 2 = single line.\n@@ -629,7 +629,7 @@ class Trainer:\n                 default value of `None`. If `x` is a `tf.data.Dataset` and\n                 `steps` is `None`, evaluation will run until the dataset\n                 is exhausted.\n-            callbacks: List of `keras_core.callbacks.Callback` instances.\n+            callbacks: List of `keras.callbacks.Callback` instances.\n                 List of callbacks to apply during evaluation.\n             return_dict: If `True`, loss and metric results are returned as a\n                 dict, with each key being the name of the metric.\n@@ -671,12 +671,12 @@ class Trainer:\n                 - A tensor, or a list of tensors\n                     (in case the model has multiple inputs).\n                 - A `tf.data.Dataset`.\n-                - A `keras_core.utils.PyDataset` instance.\n+                - A `keras.utils.PyDataset` instance.\n             batch_size: Integer or `None`.\n                 Number of samples per batch.\n                 If unspecified, `batch_size` will default to 32.\n                 Do not specify the `batch_size` if your data is in the\n-                form of dataset, generators, or `keras_core.utils.PyDataset`\n+                form of dataset, generators, or `keras.utils.PyDataset`\n                 instances (since they generate batches).\n             verbose: `\"auto\"`, 0, 1, or 2. Verbosity mode.\n                 0 = silent, 1 = progress bar, 2 = single line.\n@@ -689,7 +689,7 @@ class Trainer:\n                 Ignored with the default value of `None`.\n                 If `x` is a `tf.data.Dataset` and `steps` is `None`,\n                 `predict()` will run until the input dataset is exhausted.\n-            callbacks: List of `keras_core.callbacks.Callback` instances.\n+            callbacks: List of `keras.callbacks.Callback` instances.\n                 List of callbacks to apply during prediction.\n \n         Returns:\n@@ -878,7 +878,7 @@ def resolve_auto_jit_compile(model):\n def model_supports_jit(model):\n     if platform.system() == \"Darwin\" and \"arm\" in platform.processor().lower():\n         if backend.backend() == \"tensorflow\":\n-            from keras_core.utils.module_utils import tensorflow as tf\n+            from keras.utils.module_utils import tensorflow as tf\n \n             if tf.config.list_physical_devices(\"GPU\"):\n                 return False\n\n@@ -2,28 +2,28 @@ import numpy as np\n import pytest\n from absl.testing import parameterized\n \n-import keras_core\n-from keras_core import backend\n-from keras_core import initializers\n-from keras_core import layers\n-from keras_core import losses\n-from keras_core import metrics\n-from keras_core import ops\n-from keras_core import optimizers\n-from keras_core import testing\n-from keras_core.callbacks.callback import Callback\n-from keras_core.optimizers.rmsprop import RMSprop\n+import keras\n+from keras import backend\n+from keras import initializers\n+from keras import layers\n+from keras import losses\n+from keras import metrics\n+from keras import ops\n+from keras import optimizers\n+from keras import testing\n+from keras.callbacks.callback import Callback\n+from keras.optimizers.rmsprop import RMSprop\n \n if backend.backend() == \"jax\":\n-    from keras_core.backend.jax.trainer import JAXTrainer as Trainer\n+    from keras.backend.jax.trainer import JAXTrainer as Trainer\n elif backend.backend() == \"torch\":\n-    from keras_core.backend.torch.trainer import TorchTrainer as Trainer\n+    from keras.backend.torch.trainer import TorchTrainer as Trainer\n elif backend.backend() == \"tensorflow\":\n-    from keras_core.backend.tensorflow.trainer import (\n+    from keras.backend.tensorflow.trainer import (\n         TensorFlowTrainer as Trainer,\n     )\n elif backend.backend() == \"numpy\":\n-    from keras_core.backend.numpy.trainer import NumpyTrainer as Trainer\n+    from keras.backend.numpy.trainer import NumpyTrainer as Trainer\n else:\n     raise ImportError(f\"Invalid backend: {backend.backend()}\")\n \n@@ -563,9 +563,9 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n         self.assertAlmostEqual(logs[\"loss\"], 16.0)\n \n     def test_nested_input_predict(self):\n-        # https://github.com/keras-team/keras-core/issues/325\n+        # https://github.com/keras-team/keras/issues/325\n \n-        class TupleInputModel(keras_core.Model):\n+        class TupleInputModel(keras.Model):\n             def call(self, inputs):\n                 a, b = inputs\n                 return a + b\n@@ -575,7 +575,7 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n         out = model.predict((x1, x2))\n         self.assertEqual(out.shape, (3, 4))\n \n-        class DictInputModel(keras_core.Model):\n+        class DictInputModel(keras.Model):\n             def call(self, inputs):\n                 return inputs[\"a\"] + inputs[\"b\"]\n \n@@ -678,7 +678,7 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n                 self.add_loss(ops.sum(x))\n                 return x\n \n-        model = keras_core.Sequential(\n+        model = keras.Sequential(\n             [\n                 layers.Dense(2),\n                 LossLayer(),\n@@ -691,14 +691,14 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n         model.fit(x, y, batch_size=4)\n \n     def get_layer(self):\n-        class ExampleLayer(keras_core.Layer):\n+        class ExampleLayer(keras.Layer):\n             def call(self, x):\n                 return x * 2\n \n         return ExampleLayer\n \n     def get_model(self):\n-        class ExampleModel(keras_core.Model):\n+        class ExampleModel(keras.Model):\n             def call(self, x):\n                 return x * 2\n \n@@ -707,9 +707,9 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n     def get_functional(self):\n         ExampleLayer = self.get_layer()\n \n-        class ExampleFunctional(keras_core.Functional):\n+        class ExampleFunctional(keras.Functional):\n             def __init__(self, input_shape=(None,)):\n-                inputs = keras_core.Input(input_shape)\n+                inputs = keras.Input(input_shape)\n                 outputs = ExampleLayer()(inputs)\n                 super().__init__(inputs=inputs, outputs=outputs)\n \n@@ -733,11 +733,11 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n     )\n     @pytest.mark.requires_trainable_backend\n     @pytest.mark.skipif(\n-        keras_core.backend.backend() != \"tensorflow\",\n+        keras.backend.backend() != \"tensorflow\",\n         reason=\"Only tensorflow supports raggeds\",\n     )\n     def test_trainer_with_raggeds(self, model_class):\n-        from keras_core.utils.module_utils import tensorflow as tf\n+        from keras.utils.module_utils import tensorflow as tf\n \n         def loss_fn(y, y_pred, sample_weight=None):\n             return 0\n@@ -757,7 +757,7 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n             self.assertEqual(type(y), tf.RaggedTensor)\n \n         # test if everything works with the sequential model\n-        model = keras_core.Sequential([model])\n+        model = keras.Sequential([model])\n         model.compile(optimizer=\"adam\", loss=loss_fn)\n         model.fit(x, x)\n         y = model.predict(x)\n@@ -769,7 +769,7 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n \n         inputs = layers.Input((20,))\n         outputs = layers.Dropout(0.5, seed=1337)(inputs, training=True)\n-        model = keras_core.Model(inputs, outputs)\n+        model = keras.Model(inputs, outputs)\n         out1 = model.predict(np.ones((4, 20)), batch_size=2)\n         self.assertGreater(5, np.sum(np.abs(out1[:2, :] - out1[2:4, :])))\n \n@@ -781,7 +781,7 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n     def test_recompile(self):\n         inputs = layers.Input((2,))\n         outputs = layers.Dense(3)(inputs)\n-        model = keras_core.Model(inputs, outputs)\n+        model = keras.Model(inputs, outputs)\n         model.compile(\n             optimizer=\"sgd\", loss=\"mse\", metrics=[\"mean_squared_error\"]\n         )\n@@ -843,10 +843,10 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n         # validation only runs for the correct number of steps.\n         inputs = layers.Input((2,))\n         outputs = layers.Dense(3)(inputs)\n-        model = keras_core.Model(inputs, outputs)\n+        model = keras.Model(inputs, outputs)\n         model.compile(optimizer=\"sgd\", loss=\"mse\", metrics=[\"mse\"])\n \n-        class Recorder(keras_core.callbacks.Callback):\n+        class Recorder(keras.callbacks.Callback):\n             def __init__(self):\n                 self.train_counter = 0\n                 self.val_counter = 0\n\n@@ -0,0 +1,26 @@\n+from keras.utils.audio_dataset_utils import audio_dataset_from_directory\n+from keras.utils.dataset_utils import split_dataset\n+from keras.utils.file_utils import get_file\n+from keras.utils.image_dataset_utils import image_dataset_from_directory\n+from keras.utils.image_utils import array_to_img\n+from keras.utils.image_utils import img_to_array\n+from keras.utils.image_utils import load_img\n+from keras.utils.image_utils import save_img\n+from keras.utils.io_utils import disable_interactive_logging\n+from keras.utils.io_utils import enable_interactive_logging\n+from keras.utils.io_utils import is_interactive_logging_enabled\n+from keras.utils.model_visualization import model_to_dot\n+from keras.utils.model_visualization import plot_model\n+from keras.utils.numerical_utils import normalize\n+from keras.utils.numerical_utils import to_categorical\n+from keras.utils.progbar import Progbar\n+from keras.utils.python_utils import default\n+from keras.utils.python_utils import is_default\n+from keras.utils.python_utils import removeprefix\n+from keras.utils.python_utils import removesuffix\n+from keras.utils.rng_utils import set_random_seed\n+from keras.utils.sequence_utils import pad_sequences\n+from keras.utils.text_dataset_utils import text_dataset_from_directory\n+from keras.utils.timeseries_dataset_utils import (\n+    timeseries_dataset_from_array,\n+)\n\n\n@@ -1,14 +1,14 @@\n import numpy as np\n \n-from keras_core.api_export import keras_core_export\n-from keras_core.utils import dataset_utils\n-from keras_core.utils.module_utils import tensorflow as tf\n-from keras_core.utils.module_utils import tensorflow_io as tfio\n+from keras.api_export import keras_export\n+from keras.utils import dataset_utils\n+from keras.utils.module_utils import tensorflow as tf\n+from keras.utils.module_utils import tensorflow_io as tfio\n \n ALLOWED_FORMATS = (\".wav\",)\n \n \n-@keras_core_export(\"keras_core.utils.audio_dataset_from_directory\")\n+@keras_export(\"keras.utils.audio_dataset_from_directory\")\n def audio_dataset_from_directory(\n     directory,\n     labels=\"inferred\",\n\n@@ -2,9 +2,9 @@ import os\n \n import numpy as np\n \n-from keras_core import testing\n-from keras_core.utils import audio_dataset_utils\n-from keras_core.utils.module_utils import tensorflow as tf\n+from keras import testing\n+from keras.utils import audio_dataset_utils\n+from keras.utils.module_utils import tensorflow as tf\n \n \n class AudioDatasetFromDirectoryTest(testing.TestCase):\n\n@@ -1,7 +1,7 @@\n import sys\n \n-from keras_core import backend as backend_module\n-from keras_core.backend.common import global_state\n+from keras import backend as backend_module\n+from keras.backend.common import global_state\n \n \n def in_tf_graph():\n@@ -9,7 +9,7 @@ def in_tf_graph():\n         return True\n \n     if \"tensorflow\" in sys.modules:\n-        from keras_core.utils.module_utils import tensorflow as tf\n+        from keras.utils.module_utils import tensorflow as tf\n \n         return not tf.executing_eagerly()\n     return False\n@@ -57,22 +57,22 @@ class DynamicBackend:\n \n     def __getattr__(self, name):\n         if self._backend == \"tensorflow\":\n-            from keras_core.backend import tensorflow as tf_backend\n+            from keras.backend import tensorflow as tf_backend\n \n             return getattr(tf_backend, name)\n         if self._backend == \"jax\":\n-            from keras_core.backend import jax as jax_backend\n+            from keras.backend import jax as jax_backend\n \n             return getattr(jax_backend, name)\n         if self._backend == \"torch\":\n-            from keras_core.backend import torch as torch_backend\n+            from keras.backend import torch as torch_backend\n \n             return getattr(torch_backend, name)\n         if self._backend == \"numpy\":\n             # TODO (ariG23498):\n-            # The import `from keras_core.backend import numpy as numpy_backend`\n+            # The import `from keras.backend import numpy as numpy_backend`\n             # is not working. This is a temporary fix.\n-            # The import is redirected to `keras_core.backend.numpy.numpy.py`\n-            from keras_core import backend as numpy_backend\n+            # The import is redirected to `keras.backend.numpy.numpy.py`\n+            from keras import backend as numpy_backend\n \n             return getattr(numpy_backend, name)\n\n\n@@ -2,8 +2,8 @@ import os\n import sys\n from io import StringIO\n \n-from keras_core.testing import test_case\n-from keras_core.utils.code_stats import count_loc\n+from keras.testing import test_case\n+from keras.utils.code_stats import count_loc\n \n \n class TestCountLoc(test_case.TestCase):\n\n@@ -6,12 +6,12 @@ import warnings\n \n import numpy as np\n \n-from keras_core.api_export import keras_core_export\n-from keras_core.utils import io_utils\n-from keras_core.utils.module_utils import tensorflow as tf\n+from keras.api_export import keras_export\n+from keras.utils import io_utils\n+from keras.utils.module_utils import tensorflow as tf\n \n \n-@keras_core_export(\"keras_core.utils.split_dataset\")\n+@keras_export(\"keras.utils.split_dataset\")\n def split_dataset(\n     dataset, left_size=None, right_size=None, shuffle=False, seed=None\n ):\n@@ -42,7 +42,7 @@ def split_dataset(\n     Example:\n \n     >>> data = np.random.random(size=(1000, 4))\n-    >>> left_ds, right_ds = keras_core.utils.split_dataset(data, left_size=0.8)\n+    >>> left_ds, right_ds = keras.utils.split_dataset(data, left_size=0.8)\n     >>> int(left_ds.cardinality())\n     800\n     >>> int(right_ds.cardinality())\n\n@@ -1,8 +1,8 @@\n import numpy as np\n \n-from keras_core.testing import test_case\n-from keras_core.utils.dataset_utils import split_dataset\n-from keras_core.utils.module_utils import tensorflow as tf\n+from keras.testing import test_case\n+from keras.utils.dataset_utils import split_dataset\n+from keras.utils.module_utils import tensorflow as tf\n \n \n class DatasetUtilsTest(test_case.TestCase):\n\n@@ -1,5 +1,5 @@\n-from keras_core import backend\n-from keras_core import ops\n+from keras import backend\n+from keras import ops\n \n DTYPE_TO_SIZE = {\n     **{f\"float{i}\": i for i in (16, 32, 64)},\n\n@@ -1,6 +1,6 @@\n-from keras_core.backend.common.keras_tensor import KerasTensor\n-from keras_core.testing import test_case\n-from keras_core.utils import dtype_utils\n+from keras.backend.common.keras_tensor import KerasTensor\n+from keras.testing import test_case\n+from keras.utils import dtype_utils\n \n \n class DtypeSizeTests(test_case.TestCase):\n\n@@ -9,11 +9,11 @@ import warnings\n import zipfile\n from urllib.request import urlretrieve\n \n-from keras_core.api_export import keras_core_export\n-from keras_core.backend import config\n-from keras_core.utils import io_utils\n-from keras_core.utils.module_utils import gfile\n-from keras_core.utils.progbar import Progbar\n+from keras.api_export import keras_export\n+from keras.backend import config\n+from keras.utils import io_utils\n+from keras.utils.module_utils import gfile\n+from keras.utils.progbar import Progbar\n \n \n def path_to_string(path):\n@@ -127,7 +127,7 @@ def extract_archive(file_path, path=\".\", archive_format=\"auto\"):\n     return False\n \n \n-@keras_core_export(\"keras_core.utils.get_file\")\n+@keras_export(\"keras.utils.get_file\")\n def get_file(\n     fname=None,\n     origin=None,\n\n@@ -8,8 +8,8 @@ import urllib\n import zipfile\n from unittest.mock import patch\n \n-from keras_core.testing import test_case\n-from keras_core.utils import file_utils\n+from keras.testing import test_case\n+from keras.utils import file_utils\n \n \n class PathToStringTest(test_case.TestCase):\n\n@@ -1,18 +1,18 @@\n import numpy as np\n \n-from keras_core.api_export import keras_core_export\n-from keras_core.backend.config import standardize_data_format\n-from keras_core.utils import dataset_utils\n-from keras_core.utils import image_utils\n-from keras_core.utils.module_utils import tensorflow as tf\n+from keras.api_export import keras_export\n+from keras.backend.config import standardize_data_format\n+from keras.utils import dataset_utils\n+from keras.utils import image_utils\n+from keras.utils.module_utils import tensorflow as tf\n \n ALLOWLIST_FORMATS = (\".bmp\", \".gif\", \".jpeg\", \".jpg\", \".png\")\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.utils.image_dataset_from_directory\",\n-        \"keras_core.preprocessing.image_dataset_from_directory\",\n+        \"keras.utils.image_dataset_from_directory\",\n+        \"keras.preprocessing.image_dataset_from_directory\",\n     ]\n )\n def image_dataset_from_directory(\n@@ -113,7 +113,7 @@ def image_dataset_from_directory(\n             (of size `image_size`) that matches the target aspect ratio. By\n             default (`crop_to_aspect_ratio=False`), aspect ratio may not be\n             preserved.\n-        data_format: If None uses keras_core.config.image_data_format()\n+        data_format: If None uses keras.config.image_data_format()\n             otherwise either 'channel_last' or 'channel_first'.\n \n     Returns:\n@@ -387,7 +387,7 @@ def load_image(\n         img, channels=num_channels, expand_animations=False\n     )\n     if crop_to_aspect_ratio:\n-        from keras_core.backend import tensorflow as tf_backend\n+        from keras.backend import tensorflow as tf_backend\n \n         img = image_utils.smart_resize(\n             img,\n\n@@ -2,10 +2,10 @@ import os\n \n import numpy as np\n \n-from keras_core import testing\n-from keras_core.utils import image_dataset_utils\n-from keras_core.utils import image_utils\n-from keras_core.utils.module_utils import tensorflow as tf\n+from keras import testing\n+from keras.utils import image_dataset_utils\n+from keras.utils import image_utils\n+from keras.utils.module_utils import tensorflow as tf\n \n \n class ImageDatasetFromDirectoryTest(testing.TestCase):\n\n@@ -6,8 +6,8 @@ import warnings\n \n import numpy as np\n \n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n+from keras import backend\n+from keras.api_export import keras_export\n \n try:\n     from PIL import Image as pil_image\n@@ -32,10 +32,10 @@ if pil_image_resampling is not None:\n     }\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.utils.array_to_img\",\n-        \"keras_core.preprocessing.image.array_to_img\",\n+        \"keras.utils.array_to_img\",\n+        \"keras.preprocessing.image.array_to_img\",\n     ]\n )\n def array_to_img(x, data_format=None, scale=True, dtype=None):\n@@ -46,19 +46,19 @@ def array_to_img(x, data_format=None, scale=True, dtype=None):\n     ```python\n     from PIL import Image\n     img = np.random.random(size=(100, 100, 3))\n-    pil_img = keras_core.utils.array_to_img(img)\n+    pil_img = keras.utils.array_to_img(img)\n     ```\n \n     Args:\n         x: Input data, in any form that can be converted to a NumPy array.\n         data_format: Image data format, can be either `\"channels_first\"` or\n             `\"channels_last\"`. Defaults to `None`, in which case the global\n-            setting `keras_core.backend.image_data_format()` is used (unless you\n+            setting `keras.backend.image_data_format()` is used (unless you\n             changed it, it defaults to `\"channels_last\"`).\n         scale: Whether to rescale the image such that minimum and maximum values\n             are 0 and 255 respectively. Defaults to `True`.\n         dtype: Dtype to use. `None` means the global setting\n-            `keras_core.backend.floatx()` is used (unless you changed it, it\n+            `keras.backend.floatx()` is used (unless you changed it, it\n             defaults to `\"float32\"`). Defaults to `None`.\n \n     Returns:\n@@ -107,10 +107,10 @@ def array_to_img(x, data_format=None, scale=True, dtype=None):\n         raise ValueError(f\"Unsupported channel number: {x.shape[2]}\")\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.utils.img_to_array\",\n-        \"keras_core.preprocessing.image.img_to_array\",\n+        \"keras.utils.img_to_array\",\n+        \"keras.preprocessing.image.img_to_array\",\n     ]\n )\n def img_to_array(img, data_format=None, dtype=None):\n@@ -121,18 +121,18 @@ def img_to_array(img, data_format=None, dtype=None):\n     ```python\n     from PIL import Image\n     img_data = np.random.random(size=(100, 100, 3))\n-    img = keras_core.utils.array_to_img(img_data)\n-    array = keras_core.utils.image.img_to_array(img)\n+    img = keras.utils.array_to_img(img_data)\n+    array = keras.utils.image.img_to_array(img)\n     ```\n \n     Args:\n         img: Input PIL Image instance.\n         data_format: Image data format, can be either `\"channels_first\"` or\n             `\"channels_last\"`. Defaults to `None`, in which case the global\n-            setting `keras_core.backend.image_data_format()` is used (unless you\n+            setting `keras.backend.image_data_format()` is used (unless you\n             changed it, it defaults to `\"channels_last\"`).\n         dtype: Dtype to use. `None` means the global setting\n-            `keras_core.backend.floatx()` is used (unless you changed it, it\n+            `keras.backend.floatx()` is used (unless you changed it, it\n             defaults to `\"float32\"`).\n \n     Returns:\n@@ -159,8 +159,8 @@ def img_to_array(img, data_format=None, dtype=None):\n     return x\n \n \n-@keras_core_export(\n-    [\"keras_core.utils.save_img\", \"keras_core.preprocessing.image.save_img\"]\n+@keras_export(\n+    [\"keras.utils.save_img\", \"keras.preprocessing.image.save_img\"]\n )\n def save_img(path, x, data_format=None, file_format=None, scale=True, **kwargs):\n     \"\"\"Saves an image stored as a NumPy array to a path or file object.\n@@ -186,8 +186,8 @@ def save_img(path, x, data_format=None, file_format=None, scale=True, **kwargs):\n     img.save(path, format=file_format, **kwargs)\n \n \n-@keras_core_export(\n-    [\"keras_core.utils.load_img\", \"keras_core.preprocessing.image.load_img\"]\n+@keras_export(\n+    [\"keras.utils.load_img\", \"keras.preprocessing.image.load_img\"]\n )\n def load_img(\n     path,\n@@ -201,8 +201,8 @@ def load_img(\n     Usage:\n \n     ```python\n-    image = keras_core.utils.load_img(image_path)\n-    input_arr = keras_core.utils.img_to_array(image)\n+    image = keras.utils.load_img(image_path)\n+    input_arr = keras.utils.img_to_array(image)\n     input_arr = np.array([input_arr])  # Convert single image to a batch.\n     predictions = model.predict(input_arr)\n     ```\n\n@@ -2,14 +2,14 @@ import sys\n \n from absl import logging\n \n-from keras_core.api_export import keras_core_export\n-from keras_core.backend.common import global_state\n+from keras.api_export import keras_export\n+from keras.backend.common import global_state\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.config.enable_interactive_logging\",\n-        \"keras_core.utils.enable_interactive_logging\",\n+        \"keras.config.enable_interactive_logging\",\n+        \"keras.utils.enable_interactive_logging\",\n     ]\n )\n def enable_interactive_logging():\n@@ -22,10 +22,10 @@ def enable_interactive_logging():\n     global_state.set_global_attribute(\"interactive_logging\", True)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.config.disable_interactive_logging\",\n-        \"keras_core.utils.disable_interactive_logging\",\n+        \"keras.config.disable_interactive_logging\",\n+        \"keras.utils.disable_interactive_logging\",\n     ]\n )\n def disable_interactive_logging():\n@@ -38,10 +38,10 @@ def disable_interactive_logging():\n     global_state.set_global_attribute(\"interactive_logging\", False)\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.config.is_interactive_logging_enabled\",\n-        \"keras_core.utils.is_interactive_logging_enabled\",\n+        \"keras.config.is_interactive_logging_enabled\",\n+        \"keras.utils.is_interactive_logging_enabled\",\n     ]\n )\n def is_interactive_logging_enabled():\n\n@@ -1,7 +1,7 @@\n from unittest.mock import patch\n \n-from keras_core.testing import test_case\n-from keras_core.utils import io_utils\n+from keras.testing import test_case\n+from keras.utils import io_utils\n \n \n class TestIoUtils(test_case.TestCase):\n\n@@ -1,4 +1,4 @@\n-from keras_core import backend\n+from keras import backend\n \n \n def is_in_jax_tracing_scope():\n\n@@ -3,8 +3,8 @@\n import os\n import sys\n \n-from keras_core.api_export import keras_core_export\n-from keras_core.utils import io_utils\n+from keras.api_export import keras_export\n+from keras.utils import io_utils\n \n try:\n     # pydot-ng is a fork of pydot that is better maintained.\n@@ -164,7 +164,7 @@ def make_node(layer, **kwargs):\n     return node\n \n \n-@keras_core_export(\"keras_core.utils.model_to_dot\")\n+@keras_export(\"keras.utils.model_to_dot\")\n def model_to_dot(\n     model,\n     show_shapes=False,\n@@ -201,7 +201,7 @@ def model_to_dot(\n         a `pydot.Cluster` instance representing nested model if\n         `subgraph=True`.\n     \"\"\"\n-    from keras_core.ops.function import make_node_key\n+    from keras.ops.function import make_node_key\n \n     if not model.built:\n         raise ValueError(\n@@ -210,10 +210,10 @@ def model_to_dot(\n             \"the model on a batch of data.\"\n         )\n \n-    from keras_core.models import functional\n-    from keras_core.models import sequential\n+    from keras.models import functional\n+    from keras.models import sequential\n \n-    # from keras_core.layers import Wrapper\n+    # from keras.layers import Wrapper\n \n     if not check_pydot():\n         raise ImportError(\n@@ -336,7 +336,7 @@ def model_to_dot(\n     return dot\n \n \n-@keras_core_export(\"keras_core.utils.plot_model\")\n+@keras_export(\"keras.utils.plot_model\")\n def plot_model(\n     model,\n     to_file=\"model.png\",\n@@ -357,10 +357,10 @@ def plot_model(\n     ```python\n     inputs = ...\n     outputs = ...\n-    model = keras_core.Model(inputs=inputs, outputs=outputs)\n+    model = keras.Model(inputs=inputs, outputs=outputs)\n \n     dot_img_file = '/tmp/model_1.png'\n-    keras_core.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)\n+    keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)\n     ```\n \n     Args:\n\n\n@@ -1,8 +1,8 @@\n import collections\n import re\n \n-from keras_core.api_export import keras_core_export\n-from keras_core.backend.common import global_state\n+from keras.api_export import keras_export\n+from keras.backend.common import global_state\n \n \n def auto_name(prefix):\n@@ -31,7 +31,7 @@ def to_snake_case(name):\n     return name\n \n \n-@keras_core_export(\"keras_core.backend.get_uid\")\n+@keras_export(\"keras.backend.get_uid\")\n def get_uid(prefix=\"\"):\n     \"\"\"Associates a string prefix with an integer counter.\n \n\n@@ -1,5 +1,5 @@\n-from keras_core.testing import test_case\n-from keras_core.utils import naming\n+from keras.testing import test_case\n+from keras.utils import naming\n \n \n class NamingUtilsTest(test_case.TestCase):\n\n\n@@ -1,10 +1,10 @@\n import numpy as np\n \n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n+from keras import backend\n+from keras.api_export import keras_export\n \n \n-@keras_core_export(\"keras_core.utils.normalize\")\n+@keras_export(\"keras.utils.normalize\")\n def normalize(x, axis=-1, order=2):\n     \"\"\"Normalizes an array.\n \n@@ -19,7 +19,7 @@ def normalize(x, axis=-1, order=2):\n     Returns:\n         A normalized copy of the array.\n     \"\"\"\n-    from keras_core import ops\n+    from keras import ops\n \n     if not isinstance(order, int) or not order >= 1:\n         raise ValueError(\n@@ -49,7 +49,7 @@ def normalize(x, axis=-1, order=2):\n     return ops.multiply(x, norm)\n \n \n-@keras_core_export(\"keras_core.utils.to_categorical\")\n+@keras_export(\"keras.utils.to_categorical\")\n def to_categorical(x, num_classes=None):\n     \"\"\"Converts a class vector (integers) to binary class matrix.\n \n@@ -67,7 +67,7 @@ def to_categorical(x, num_classes=None):\n \n     Example:\n \n-    >>> a = keras_core.utils.to_categorical([0, 1, 2, 3], num_classes=4)\n+    >>> a = keras.utils.to_categorical([0, 1, 2, 3], num_classes=4)\n     >>> print(a)\n     [[1. 0. 0. 0.]\n      [0. 1. 0. 0.]\n@@ -79,11 +79,11 @@ def to_categorical(x, num_classes=None):\n     ...               .04, .01, .94, .05,\n     ...               .12, .21, .5, .17],\n     ...               shape=[4, 4])\n-    >>> loss = keras_core.backend.categorical_crossentropy(a, b)\n+    >>> loss = keras.backend.categorical_crossentropy(a, b)\n     >>> print(np.around(loss, 5))\n     [0.10536 0.82807 0.1011  1.77196]\n \n-    >>> loss = keras_core.backend.categorical_crossentropy(a, a)\n+    >>> loss = keras.backend.categorical_crossentropy(a, a)\n     >>> print(np.around(loss, 5))\n     [0. 0. 0. 0.]\n     \"\"\"\n\n@@ -1,9 +1,9 @@\n import numpy as np\n from absl.testing import parameterized\n \n-from keras_core import backend\n-from keras_core import testing\n-from keras_core.utils import numerical_utils\n+from keras import backend\n+from keras import testing\n+from keras.utils import numerical_utils\n \n NUM_CLASSES = 5\n \n\n@@ -3,12 +3,12 @@ import os\n import sys\n import time\n \n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.utils import io_utils\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.utils import io_utils\n \n \n-@keras_core_export(\"keras_core.utils.Progbar\")\n+@keras_export(\"keras.utils.Progbar\")\n class Progbar:\n     \"\"\"Displays a progress bar.\n \n\n\n@@ -1,8 +1,8 @@\n import base64\n import marshal\n \n-from keras_core import testing\n-from keras_core.utils import python_utils\n+from keras import testing\n+from keras.utils import python_utils\n \n \n class PythonUtilsTest(testing.TestCase):\n\n@@ -2,12 +2,12 @@ import random\n \n import numpy as np\n \n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.utils.module_utils import tensorflow as tf\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.utils.module_utils import tensorflow as tf\n \n \n-@keras_core_export(\"keras_core.utils.set_random_seed\")\n+@keras_export(\"keras.utils.set_random_seed\")\n def set_random_seed(seed):\n     \"\"\"Sets all random seeds (Python, NumPy, and backend framework, e.g. TF).\n \n@@ -22,7 +22,7 @@ def set_random_seed(seed):\n     ```python\n     import random\n     import numpy as np\n-    from keras_core.utils.module_utils import tensorflow as tf\n+    from keras.utils.module_utils import tensorflow as tf\n     random.seed(seed)\n     np.random.seed(seed)\n     tf.random.set_seed(seed)\n\n@@ -2,10 +2,10 @@ import numpy as np\n import pytest\n import tensorflow as tf\n \n-import keras_core\n-from keras_core import backend\n-from keras_core.testing import test_case\n-from keras_core.utils import rng_utils\n+import keras\n+from keras import backend\n+from keras.testing import test_case\n+from keras.utils import rng_utils\n \n \n class TestRandomSeedSetting(test_case.TestCase):\n@@ -15,11 +15,11 @@ class TestRandomSeedSetting(test_case.TestCase):\n     )\n     def test_set_random_seed(self):\n         def get_model_output():\n-            model = keras_core.Sequential(\n+            model = keras.Sequential(\n                 [\n-                    keras_core.layers.Dense(10),\n-                    keras_core.layers.Dropout(0.5),\n-                    keras_core.layers.Dense(10),\n+                    keras.layers.Dense(10),\n+                    keras.layers.Dropout(0.5),\n+                    keras.layers.Dense(10),\n                 ]\n             )\n             x = np.random.random((32, 10)).astype(\"float32\")\n\n@@ -1,12 +1,12 @@\n import numpy as np\n \n-from keras_core.api_export import keras_core_export\n+from keras.api_export import keras_export\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.utils.pad_sequences\",\n-        \"keras_core.preprocessing.sequence.pad_sequences\",\n+        \"keras.utils.pad_sequences\",\n+        \"keras.preprocessing.sequence.pad_sequences\",\n     ]\n )\n def pad_sequences(\n@@ -37,22 +37,22 @@ def pad_sequences(\n     default.\n \n     >>> sequence = [[1], [2, 3], [4, 5, 6]]\n-    >>> keras_core.utils.pad_sequences(sequence)\n+    >>> keras.utils.pad_sequences(sequence)\n     array([[0, 0, 1],\n            [0, 2, 3],\n            [4, 5, 6]], dtype=int32)\n \n-    >>> keras_core.utils.pad_sequences(sequence, value=-1)\n+    >>> keras.utils.pad_sequences(sequence, value=-1)\n     array([[-1, -1,  1],\n            [-1,  2,  3],\n            [ 4,  5,  6]], dtype=int32)\n \n-    >>> keras_core.utils.pad_sequences(sequence, padding='post')\n+    >>> keras.utils.pad_sequences(sequence, padding='post')\n     array([[1, 0, 0],\n            [2, 3, 0],\n            [4, 5, 6]], dtype=int32)\n \n-    >>> keras_core.utils.pad_sequences(sequence, maxlen=2)\n+    >>> keras.utils.pad_sequences(sequence, maxlen=2)\n     array([[0, 1],\n            [2, 3],\n            [5, 6]], dtype=int32)\n\n@@ -1,5 +1,5 @@\n-from keras_core import testing\n-from keras_core.utils import sequence_utils\n+from keras import testing\n+from keras.utils import sequence_utils\n \n \n class PadSequencesTest(testing.TestCase):\n\n\n@@ -6,14 +6,14 @@ import rich\n import rich.console\n import rich.markup\n \n-# See https://github.com/keras-team/keras-core/issues/448\n+# See https://github.com/keras-team/keras/issues/448\n # for below imports\n import rich.table\n import tree\n \n-from keras_core import backend\n-from keras_core.utils import dtype_utils\n-from keras_core.utils import io_utils\n+from keras import backend\n+from keras.utils import dtype_utils\n+from keras.utils import io_utils\n \n \n def count_params(weights):\n@@ -131,8 +131,8 @@ def print_summary(\n             matches `layer_range[1]`. By default (`None`) all\n             layers in the model are included in the summary.\n     \"\"\"\n-    from keras_core.models import Functional\n-    from keras_core.models import Sequential\n+    from keras.models import Functional\n+    from keras.models import Sequential\n \n     if not print_fn and not io_utils.is_interactive_logging_enabled():\n         print_fn = io_utils.print_msg\n\n@@ -2,10 +2,10 @@ import numpy as np\n import pytest\n from absl.testing import parameterized\n \n-from keras_core import layers\n-from keras_core import models\n-from keras_core import testing\n-from keras_core.utils import summary_utils\n+from keras import layers\n+from keras import models\n+from keras import testing\n+from keras.utils import summary_utils\n \n \n class SummaryUtilsTest(testing.TestCase, parameterized.TestCase):\n\n@@ -1,14 +1,14 @@\n import numpy as np\n \n-from keras_core.api_export import keras_core_export\n-from keras_core.utils import dataset_utils\n-from keras_core.utils.module_utils import tensorflow as tf\n+from keras.api_export import keras_export\n+from keras.utils import dataset_utils\n+from keras.utils.module_utils import tensorflow as tf\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.utils.text_dataset_from_directory\",\n-        \"keras_core.preprocessing.text_dataset_from_directory\",\n+        \"keras.utils.text_dataset_from_directory\",\n+        \"keras.preprocessing.text_dataset_from_directory\",\n     ]\n )\n def text_dataset_from_directory(\n\n@@ -2,8 +2,8 @@ import os\n import random\n import string\n \n-from keras_core import testing\n-from keras_core.utils import text_dataset_utils\n+from keras import testing\n+from keras.utils import text_dataset_utils\n \n \n class TextDatasetFromDirectoryTest(testing.TestCase):\n\n@@ -1,4 +1,4 @@\n-from keras_core.utils.module_utils import tensorflow as tf\n+from keras.utils.module_utils import tensorflow as tf\n \n \n def expand_dims(inputs, axis):\n\n@@ -1,13 +1,13 @@\n import numpy as np\n \n-from keras_core.api_export import keras_core_export\n-from keras_core.utils.module_utils import tensorflow as tf\n+from keras.api_export import keras_export\n+from keras.utils.module_utils import tensorflow as tf\n \n \n-@keras_core_export(\n+@keras_export(\n     [\n-        \"keras_core.utils.timeseries_dataset_from_array\",\n-        \"keras_core.preprocessing.timeseries_dataset_from_array\",\n+        \"keras.utils.timeseries_dataset_from_array\",\n+        \"keras.preprocessing.timeseries_dataset_from_array\",\n     ]\n )\n def timeseries_dataset_from_array(\n\n@@ -1,7 +1,7 @@\n import numpy as np\n \n-from keras_core import testing\n-from keras_core.utils import timeseries_dataset_utils\n+from keras import testing\n+from keras.utils import timeseries_dataset_utils\n \n \n class TimeseriesDatasetTest(testing.TestCase):\n\n@@ -1,8 +1,8 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers import Layer\n+from keras.api_export import keras_export\n+from keras.layers import Layer\n \n \n-@keras_core_export(\"keras_core.layers.TorchModuleWrapper\")\n+@keras_export(\"keras.layers.TorchModuleWrapper\")\n class TorchModuleWrapper(Layer):\n     \"\"\"Torch module wrapper layer.\n \n@@ -26,10 +26,10 @@ class TorchModuleWrapper(Layer):\n     import torch.nn as nn\n     import torch.nn.functional as F\n \n-    import keras_core\n-    from keras_core.layers import TorchModuleWrapper\n+    import keras\n+    from keras.layers import TorchModuleWrapper\n \n-    class Classifier(keras_core.Model):\n+    class Classifier(keras.Model):\n         def __init__(self, *args, **kwargs):\n             super().__init__(*args, **kwargs)\n             # Wrap `torch.nn.Module`s with `TorchModuleWrapper`\n@@ -83,7 +83,7 @@ class TorchModuleWrapper(Layer):\n                 f\"Received uninitialized LazyModule: module={module}\"\n             )\n \n-        from keras_core.backend.torch.core import get_device\n+        from keras.backend.torch.core import get_device\n \n         self.module = module.to(get_device())\n         self._track_module_parameters()\n@@ -92,7 +92,7 @@ class TorchModuleWrapper(Layer):\n         return self.module.parameters(recurse=recurse)\n \n     def _track_module_parameters(self):\n-        from keras_core.backend.torch import Variable\n+        from keras.backend.torch import Variable\n \n         for param in self.module.parameters():\n             variable = Variable(\n\n@@ -2,11 +2,11 @@ import numpy as np\n import pytest\n import torch\n \n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import models\n-from keras_core import testing\n-from keras_core.utils.torch_utils import TorchModuleWrapper\n+from keras import backend\n+from keras import layers\n+from keras import models\n+from keras import testing\n+from keras.utils.torch_utils import TorchModuleWrapper\n \n \n class Classifier(models.Model):\n\n@@ -6,9 +6,9 @@ from functools import wraps\n \n import tree\n \n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.backend.common import global_state\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.backend.common import global_state\n \n _EXCLUDED_PATHS = (\n     os.path.abspath(os.path.join(__file__, \"..\", \"..\")),\n@@ -16,7 +16,7 @@ _EXCLUDED_PATHS = (\n )\n \n \n-@keras_core_export(\"keras_core.config.enable_traceback_filtering\")\n+@keras_export(\"keras.config.enable_traceback_filtering\")\n def enable_traceback_filtering():\n     \"\"\"Turn on traceback filtering.\n \n@@ -27,17 +27,17 @@ def enable_traceback_filtering():\n     raises, to keep traceback short, readable, and focused on what's\n     actionable for you (your own code).\n \n-    See also `keras_core.config.disable_traceback_filtering()` and\n-    `keras_core.config.is_traceback_filtering_enabled()`.\n+    See also `keras.config.disable_traceback_filtering()` and\n+    `keras.config.is_traceback_filtering_enabled()`.\n \n     If you have previously disabled traceback filtering via\n-    `keras_core.config.disable_traceback_filtering()`, you can re-enable it via\n-    `keras_core.config.enable_traceback_filtering()`.\n+    `keras.config.disable_traceback_filtering()`, you can re-enable it via\n+    `keras.config.enable_traceback_filtering()`.\n     \"\"\"\n     global_state.set_global_attribute(\"traceback_filtering\", True)\n \n \n-@keras_core_export(\"keras_core.config.disable_traceback_filtering\")\n+@keras_export(\"keras.config.disable_traceback_filtering\")\n def disable_traceback_filtering():\n     \"\"\"Turn off traceback filtering.\n \n@@ -48,17 +48,17 @@ def disable_traceback_filtering():\n     raises, to keep traceback short, readable, and focused on what's\n     actionable for you (your own code).\n \n-    See also `keras_core.config.enable_traceback_filtering()` and\n-    `keras_core.config.is_traceback_filtering_enabled()`.\n+    See also `keras.config.enable_traceback_filtering()` and\n+    `keras.config.is_traceback_filtering_enabled()`.\n \n     If you have previously disabled traceback filtering via\n-    `keras_core.config.disable_traceback_filtering()`, you can re-enable it via\n-    `keras_core.config.enable_traceback_filtering()`.\n+    `keras.config.disable_traceback_filtering()`, you can re-enable it via\n+    `keras.config.enable_traceback_filtering()`.\n     \"\"\"\n     global_state.set_global_attribute(\"traceback_filtering\", False)\n \n \n-@keras_core_export(\"keras_core.config.is_traceback_filtering_enabled\")\n+@keras_export(\"keras.config.is_traceback_filtering_enabled\")\n def is_traceback_filtering_enabled():\n     \"\"\"Check if traceback filtering is enabled.\n \n@@ -69,12 +69,12 @@ def is_traceback_filtering_enabled():\n     raises, to keep traceback short, readable, and focused on what's\n     actionable for you (your own code).\n \n-    See also `keras_core.config.enable_traceback_filtering()` and\n-    `keras_core.config.disable_traceback_filtering()`.\n+    See also `keras.config.enable_traceback_filtering()` and\n+    `keras.config.disable_traceback_filtering()`.\n \n     If you have previously disabled traceback filtering via\n-    `keras_core.config.disable_traceback_filtering()`, you can re-enable it via\n-    `keras_core.config.enable_traceback_filtering()`.\n+    `keras.config.disable_traceback_filtering()`, you can re-enable it via\n+    `keras.config.enable_traceback_filtering()`.\n \n     Returns:\n         Boolean, `True` if traceback filtering is enabled,\n@@ -119,7 +119,7 @@ def filter_traceback(fn):\n         except Exception as e:\n             filtered_tb = _process_traceback_frames(e.__traceback__)\n             # To get the full stack trace, call:\n-            # `keras_core.config.disable_traceback_filtering()`\n+            # `keras.config.disable_traceback_filtering()`\n             raise e.with_traceback(filtered_tb) from None\n         finally:\n             del filtered_tb\n\n@@ -1,7 +1,7 @@\n from functools import wraps\n \n-from keras_core.backend.common.global_state import get_global_attribute\n-from keras_core.backend.common.global_state import set_global_attribute\n+from keras.backend.common.global_state import get_global_attribute\n+from keras.backend.common.global_state import set_global_attribute\n \n \n class DotNotTrackScope:\n\n\n\n@@ -1,19 +0,0 @@\n-from keras_core import activations\n-from keras_core import applications\n-from keras_core import backend\n-from keras_core import constraints\n-from keras_core import datasets\n-from keras_core import initializers\n-from keras_core import layers\n-from keras_core import models\n-from keras_core import ops\n-from keras_core import optimizers\n-from keras_core import regularizers\n-from keras_core import utils\n-from keras_core.backend import KerasTensor\n-from keras_core.layers import Input\n-from keras_core.layers import Layer\n-from keras_core.models import Functional\n-from keras_core.models import Model\n-from keras_core.models import Sequential\n-from keras_core.version import __version__\n\n@@ -1,55 +0,0 @@\n-from keras_core.backend.config import backend\n-\n-if backend() == \"torch\":\n-    # When using the torch backend,\n-    # torch needs to be imported first, otherwise it will segfault\n-    # upon import.\n-    import torch\n-\n-from keras_core.backend.common.keras_tensor import KerasTensor\n-from keras_core.backend.common.keras_tensor import any_symbolic_tensors\n-from keras_core.backend.common.keras_tensor import is_keras_tensor\n-from keras_core.backend.common.name_scope import name_scope\n-from keras_core.backend.common.stateless_scope import StatelessScope\n-from keras_core.backend.common.stateless_scope import get_stateless_scope\n-from keras_core.backend.common.stateless_scope import in_stateless_scope\n-from keras_core.backend.common.variables import AutocastScope\n-from keras_core.backend.common.variables import get_autocast_scope\n-from keras_core.backend.common.variables import is_float_dtype\n-from keras_core.backend.common.variables import is_int_dtype\n-from keras_core.backend.common.variables import standardize_dtype\n-from keras_core.backend.common.variables import standardize_shape\n-from keras_core.backend.config import epsilon\n-from keras_core.backend.config import floatx\n-from keras_core.backend.config import image_data_format\n-from keras_core.backend.config import set_epsilon\n-from keras_core.backend.config import set_floatx\n-from keras_core.backend.config import set_image_data_format\n-from keras_core.backend.config import standardize_data_format\n-from keras_core.utils.io_utils import print_msg\n-\n-# Import backend functions.\n-if backend() == \"tensorflow\":\n-    print_msg(\"Using TensorFlow backend\")\n-    from keras_core.backend.tensorflow import *  # noqa: F403\n-\n-    distribution_lib = None\n-elif backend() == \"jax\":\n-    print_msg(\"Using JAX backend.\")\n-    from keras_core.backend.jax import *  # noqa: F403\n-elif backend() == \"torch\":\n-    print_msg(\"Using PyTorch backend.\")\n-    from keras_core.backend.torch import *  # noqa: F403\n-\n-    distribution_lib = None\n-elif backend() == \"numpy\":\n-    print_msg(\n-        \"Using NumPy backend.\\nThe NumPy backend does not support \"\n-        \"training. It should only be used for inference, evaluation, \"\n-        \"and debugging.\"\n-    )\n-    from keras_core.backend.numpy import *  # noqa: F403\n-\n-    distribution_lib = None\n-else:\n-    raise ValueError(f\"Unable to import backend : {backend()}\")\n\n@@ -1,9 +0,0 @@\n-from keras_core.backend.common import backend_utils\n-from keras_core.backend.common.variables import AutocastScope\n-from keras_core.backend.common.variables import KerasVariable\n-from keras_core.backend.common.variables import get_autocast_scope\n-from keras_core.backend.common.variables import is_float_dtype\n-from keras_core.backend.common.variables import is_int_dtype\n-from keras_core.backend.common.variables import standardize_dtype\n-from keras_core.backend.common.variables import standardize_shape\n-from keras_core.random import random\n\n@@ -1,23 +0,0 @@\n-from keras_core.backend.jax import core\n-from keras_core.backend.jax import distribution_lib\n-from keras_core.backend.jax import image\n-from keras_core.backend.jax import math\n-from keras_core.backend.jax import nn\n-from keras_core.backend.jax import numpy\n-from keras_core.backend.jax import random\n-from keras_core.backend.jax.core import SUPPORTS_SPARSE_TENSORS\n-from keras_core.backend.jax.core import Variable\n-from keras_core.backend.jax.core import cast\n-from keras_core.backend.jax.core import compute_output_spec\n-from keras_core.backend.jax.core import cond\n-from keras_core.backend.jax.core import convert_to_numpy\n-from keras_core.backend.jax.core import convert_to_tensor\n-from keras_core.backend.jax.core import is_tensor\n-from keras_core.backend.jax.core import scatter\n-from keras_core.backend.jax.core import shape\n-from keras_core.backend.jax.core import stop_gradient\n-from keras_core.backend.jax.core import vectorized_map\n-from keras_core.backend.jax.rnn import cudnn_ok\n-from keras_core.backend.jax.rnn import gru\n-from keras_core.backend.jax.rnn import lstm\n-from keras_core.backend.jax.rnn import rnn\n\n@@ -1,20 +0,0 @@\n-from keras_core.backend.numpy import core\n-from keras_core.backend.numpy import image\n-from keras_core.backend.numpy import math\n-from keras_core.backend.numpy import nn\n-from keras_core.backend.numpy import numpy\n-from keras_core.backend.numpy import random\n-from keras_core.backend.numpy.core import SUPPORTS_SPARSE_TENSORS\n-from keras_core.backend.numpy.core import Variable\n-from keras_core.backend.numpy.core import cast\n-from keras_core.backend.numpy.core import compute_output_spec\n-from keras_core.backend.numpy.core import cond\n-from keras_core.backend.numpy.core import convert_to_numpy\n-from keras_core.backend.numpy.core import convert_to_tensor\n-from keras_core.backend.numpy.core import is_tensor\n-from keras_core.backend.numpy.core import shape\n-from keras_core.backend.numpy.core import vectorized_map\n-from keras_core.backend.numpy.rnn import cudnn_ok\n-from keras_core.backend.numpy.rnn import gru\n-from keras_core.backend.numpy.rnn import lstm\n-from keras_core.backend.numpy.rnn import rnn\n\n@@ -1,24 +0,0 @@\n-from keras_core.backend.tensorflow import core\n-from keras_core.backend.tensorflow import image\n-from keras_core.backend.tensorflow import math\n-from keras_core.backend.tensorflow import nn\n-from keras_core.backend.tensorflow import numpy\n-from keras_core.backend.tensorflow import random\n-from keras_core.backend.tensorflow import tensorboard\n-from keras_core.backend.tensorflow.core import SUPPORTS_SPARSE_TENSORS\n-from keras_core.backend.tensorflow.core import Variable\n-from keras_core.backend.tensorflow.core import cast\n-from keras_core.backend.tensorflow.core import compute_output_spec\n-from keras_core.backend.tensorflow.core import cond\n-from keras_core.backend.tensorflow.core import convert_to_numpy\n-from keras_core.backend.tensorflow.core import convert_to_tensor\n-from keras_core.backend.tensorflow.core import is_tensor\n-from keras_core.backend.tensorflow.core import name_scope\n-from keras_core.backend.tensorflow.core import scatter\n-from keras_core.backend.tensorflow.core import shape\n-from keras_core.backend.tensorflow.core import stop_gradient\n-from keras_core.backend.tensorflow.core import vectorized_map\n-from keras_core.backend.tensorflow.rnn import cudnn_ok\n-from keras_core.backend.tensorflow.rnn import gru\n-from keras_core.backend.tensorflow.rnn import lstm\n-from keras_core.backend.tensorflow.rnn import rnn\n\n@@ -1,39 +0,0 @@\n-\"\"\"Torch backend APIs.\n-\n-# Note on device placement\n-\n-Torch has a different device placement style compared to TF and JAX.\n-In short, variables/tensors are not created on GPU by default,\n-and the GPU cannot directly communicate with the CPU.\n-To bring Torch behavior in line with TF and JAX automated device placement,\n-we are doing the following to automate device placement if a GPU is available:\n-\n-- Variables are created on GPU.\n-- Input data will be placed on GPU at the first `keras_core.layers.Layer` call.\n-- Tensor creation happens on GPU, e.g., `zeros()` will create a tensor on GPU.\n-- `convert_to_numpy` will bring the tensor to CPU before converting it to NumPy.\n-\"\"\"\n-\n-from keras_core.backend.torch import core\n-from keras_core.backend.torch import image\n-from keras_core.backend.torch import math\n-from keras_core.backend.torch import nn\n-from keras_core.backend.torch import numpy\n-from keras_core.backend.torch import random\n-from keras_core.backend.torch.core import SUPPORTS_SPARSE_TENSORS\n-from keras_core.backend.torch.core import Variable\n-from keras_core.backend.torch.core import cast\n-from keras_core.backend.torch.core import compute_output_spec\n-from keras_core.backend.torch.core import cond\n-from keras_core.backend.torch.core import convert_to_numpy\n-from keras_core.backend.torch.core import convert_to_tensor\n-from keras_core.backend.torch.core import is_tensor\n-from keras_core.backend.torch.core import scatter\n-from keras_core.backend.torch.core import shape\n-from keras_core.backend.torch.core import stop_gradient\n-from keras_core.backend.torch.core import to_torch_dtype\n-from keras_core.backend.torch.core import vectorized_map\n-from keras_core.backend.torch.rnn import cudnn_ok\n-from keras_core.backend.torch.rnn import gru\n-from keras_core.backend.torch.rnn import lstm\n-from keras_core.backend.torch.rnn import rnn\n\n@@ -1 +0,0 @@\n-from keras_core.backend.torch.optimizers.torch_optimizer import TorchOptimizer\n\n@@ -1,6 +0,0 @@\n-from keras_core import optimizers\n-from keras_core.backend.torch.optimizers import torch_adam\n-\n-\n-class AdamW(torch_adam.Adam, optimizers.AdamW):\n-    pass\n\n@@ -1,14 +0,0 @@\n-from keras_core.callbacks.backup_and_restore_callback import BackupAndRestore\n-from keras_core.callbacks.callback import Callback\n-from keras_core.callbacks.callback_list import CallbackList\n-from keras_core.callbacks.csv_logger import CSVLogger\n-from keras_core.callbacks.early_stopping import EarlyStopping\n-from keras_core.callbacks.history import History\n-from keras_core.callbacks.lambda_callback import LambdaCallback\n-from keras_core.callbacks.learning_rate_scheduler import LearningRateScheduler\n-from keras_core.callbacks.model_checkpoint import ModelCheckpoint\n-from keras_core.callbacks.progbar_logger import ProgbarLogger\n-from keras_core.callbacks.reduce_lr_on_plateau import ReduceLROnPlateau\n-from keras_core.callbacks.remote_monitor import RemoteMonitor\n-from keras_core.callbacks.tensorboard import TensorBoard\n-from keras_core.callbacks.terminate_on_nan import TerminateOnNaN\n\n@@ -1,10 +0,0 @@\n-\"\"\"Small NumPy datasets for debugging/testing.\"\"\"\n-\n-from keras_core.datasets import boston_housing\n-from keras_core.datasets import california_housing\n-from keras_core.datasets import cifar10\n-from keras_core.datasets import cifar100\n-from keras_core.datasets import fashion_mnist\n-from keras_core.datasets import imdb\n-from keras_core.datasets import mnist\n-from keras_core.datasets import reuters\n\n@@ -1 +0,0 @@\n-from keras_core.export.export_lib import ExportArchive\n\n@@ -1,170 +0,0 @@\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.activations.activation import Activation\n-from keras_core.layers.activations.elu import ELU\n-from keras_core.layers.activations.leaky_relu import LeakyReLU\n-from keras_core.layers.activations.prelu import PReLU\n-from keras_core.layers.activations.relu import ReLU\n-from keras_core.layers.activations.softmax import Softmax\n-from keras_core.layers.attention.additive_attention import AdditiveAttention\n-from keras_core.layers.attention.attention import Attention\n-from keras_core.layers.attention.multi_head_attention import MultiHeadAttention\n-from keras_core.layers.convolutional.conv1d import Conv1D\n-from keras_core.layers.convolutional.conv1d_transpose import Conv1DTranspose\n-from keras_core.layers.convolutional.conv2d import Conv2D\n-from keras_core.layers.convolutional.conv2d_transpose import Conv2DTranspose\n-from keras_core.layers.convolutional.conv3d import Conv3D\n-from keras_core.layers.convolutional.conv3d_transpose import Conv3DTranspose\n-from keras_core.layers.convolutional.depthwise_conv1d import DepthwiseConv1D\n-from keras_core.layers.convolutional.depthwise_conv2d import DepthwiseConv2D\n-from keras_core.layers.convolutional.separable_conv1d import SeparableConv1D\n-from keras_core.layers.convolutional.separable_conv2d import SeparableConv2D\n-from keras_core.layers.core.dense import Dense\n-from keras_core.layers.core.einsum_dense import EinsumDense\n-from keras_core.layers.core.embedding import Embedding\n-from keras_core.layers.core.identity import Identity\n-from keras_core.layers.core.input_layer import Input\n-from keras_core.layers.core.input_layer import InputLayer\n-from keras_core.layers.core.lambda_layer import Lambda\n-from keras_core.layers.core.masking import Masking\n-from keras_core.layers.core.wrapper import Wrapper\n-from keras_core.layers.layer import Layer\n-from keras_core.layers.merging.add import Add\n-from keras_core.layers.merging.add import add\n-from keras_core.layers.merging.average import Average\n-from keras_core.layers.merging.average import average\n-from keras_core.layers.merging.concatenate import Concatenate\n-from keras_core.layers.merging.concatenate import concatenate\n-from keras_core.layers.merging.dot import Dot\n-from keras_core.layers.merging.dot import dot\n-from keras_core.layers.merging.maximum import Maximum\n-from keras_core.layers.merging.maximum import maximum\n-from keras_core.layers.merging.minimum import Minimum\n-from keras_core.layers.merging.minimum import minimum\n-from keras_core.layers.merging.multiply import Multiply\n-from keras_core.layers.merging.multiply import multiply\n-from keras_core.layers.merging.subtract import Subtract\n-from keras_core.layers.merging.subtract import subtract\n-from keras_core.layers.normalization.batch_normalization import (\n-    BatchNormalization,\n-)\n-from keras_core.layers.normalization.group_normalization import (\n-    GroupNormalization,\n-)\n-from keras_core.layers.normalization.layer_normalization import (\n-    LayerNormalization,\n-)\n-from keras_core.layers.normalization.spectral_normalization import (\n-    SpectralNormalization,\n-)\n-from keras_core.layers.normalization.unit_normalization import UnitNormalization\n-from keras_core.layers.pooling.average_pooling1d import AveragePooling1D\n-from keras_core.layers.pooling.average_pooling2d import AveragePooling2D\n-from keras_core.layers.pooling.average_pooling3d import AveragePooling3D\n-from keras_core.layers.pooling.global_average_pooling1d import (\n-    GlobalAveragePooling1D,\n-)\n-from keras_core.layers.pooling.global_average_pooling2d import (\n-    GlobalAveragePooling2D,\n-)\n-from keras_core.layers.pooling.global_average_pooling3d import (\n-    GlobalAveragePooling3D,\n-)\n-from keras_core.layers.pooling.global_max_pooling1d import GlobalMaxPooling1D\n-from keras_core.layers.pooling.global_max_pooling2d import GlobalMaxPooling2D\n-from keras_core.layers.pooling.global_max_pooling3d import GlobalMaxPooling3D\n-from keras_core.layers.pooling.max_pooling1d import MaxPooling1D\n-from keras_core.layers.pooling.max_pooling2d import MaxPooling2D\n-from keras_core.layers.pooling.max_pooling3d import MaxPooling3D\n-from keras_core.layers.preprocessing.category_encoding import CategoryEncoding\n-from keras_core.layers.preprocessing.center_crop import CenterCrop\n-from keras_core.layers.preprocessing.discretization import Discretization\n-from keras_core.layers.preprocessing.hashed_crossing import HashedCrossing\n-from keras_core.layers.preprocessing.hashing import Hashing\n-from keras_core.layers.preprocessing.index_lookup import IndexLookup\n-from keras_core.layers.preprocessing.integer_lookup import IntegerLookup\n-from keras_core.layers.preprocessing.normalization import Normalization\n-from keras_core.layers.preprocessing.random_brightness import RandomBrightness\n-from keras_core.layers.preprocessing.random_contrast import RandomContrast\n-from keras_core.layers.preprocessing.random_crop import RandomCrop\n-from keras_core.layers.preprocessing.random_flip import RandomFlip\n-from keras_core.layers.preprocessing.random_rotation import RandomRotation\n-from keras_core.layers.preprocessing.random_translation import RandomTranslation\n-from keras_core.layers.preprocessing.random_zoom import RandomZoom\n-from keras_core.layers.preprocessing.rescaling import Rescaling\n-from keras_core.layers.preprocessing.resizing import Resizing\n-from keras_core.layers.preprocessing.string_lookup import StringLookup\n-from keras_core.layers.preprocessing.text_vectorization import TextVectorization\n-from keras_core.layers.regularization.activity_regularization import (\n-    ActivityRegularization,\n-)\n-from keras_core.layers.regularization.dropout import Dropout\n-from keras_core.layers.regularization.gaussian_dropout import GaussianDropout\n-from keras_core.layers.regularization.gaussian_noise import GaussianNoise\n-from keras_core.layers.regularization.spatial_dropout import SpatialDropout1D\n-from keras_core.layers.regularization.spatial_dropout import SpatialDropout2D\n-from keras_core.layers.regularization.spatial_dropout import SpatialDropout3D\n-from keras_core.layers.reshaping.cropping1d import Cropping1D\n-from keras_core.layers.reshaping.cropping2d import Cropping2D\n-from keras_core.layers.reshaping.cropping3d import Cropping3D\n-from keras_core.layers.reshaping.flatten import Flatten\n-from keras_core.layers.reshaping.permute import Permute\n-from keras_core.layers.reshaping.repeat_vector import RepeatVector\n-from keras_core.layers.reshaping.reshape import Reshape\n-from keras_core.layers.reshaping.up_sampling1d import UpSampling1D\n-from keras_core.layers.reshaping.up_sampling2d import UpSampling2D\n-from keras_core.layers.reshaping.up_sampling3d import UpSampling3D\n-from keras_core.layers.reshaping.zero_padding1d import ZeroPadding1D\n-from keras_core.layers.reshaping.zero_padding2d import ZeroPadding2D\n-from keras_core.layers.reshaping.zero_padding3d import ZeroPadding3D\n-from keras_core.layers.rnn.bidirectional import Bidirectional\n-from keras_core.layers.rnn.conv_lstm1d import ConvLSTM1D\n-from keras_core.layers.rnn.conv_lstm2d import ConvLSTM2D\n-from keras_core.layers.rnn.conv_lstm3d import ConvLSTM3D\n-from keras_core.layers.rnn.gru import GRU\n-from keras_core.layers.rnn.gru import GRUCell\n-from keras_core.layers.rnn.lstm import LSTM\n-from keras_core.layers.rnn.lstm import LSTMCell\n-from keras_core.layers.rnn.rnn import RNN\n-from keras_core.layers.rnn.simple_rnn import SimpleRNN\n-from keras_core.layers.rnn.simple_rnn import SimpleRNNCell\n-from keras_core.layers.rnn.stacked_rnn_cells import StackedRNNCells\n-from keras_core.layers.rnn.time_distributed import TimeDistributed\n-from keras_core.saving import serialization_lib\n-\n-\n-@keras_core_export(\"keras_core.layers.serialize\")\n-def serialize(layer):\n-    \"\"\"Returns the layer configuration as a Python dict.\n-\n-    Args:\n-        layer: A `keras.layers.Layer` instance to serialize.\n-\n-    Returns:\n-        Python dict which contains the configuration of the layer.\n-    \"\"\"\n-    return serialization_lib.serialize_keras_object(layer)\n-\n-\n-@keras_core_export(\"keras_core.layers.deserialize\")\n-def deserialize(config, custom_objects=None):\n-    \"\"\"Returns a Keras layer object via its configuration.\n-\n-    Args:\n-        config: A python dict containing a serialized layer configuration.\n-        custom_objects: Optional dictionary mapping names (strings) to custom\n-            objects (classes and functions) to be considered during\n-            deserialization.\n-\n-    Returns:\n-        A Keras layer instance.\n-    \"\"\"\n-    obj = serialization_lib.deserialize_keras_object(\n-        config,\n-        custom_objects=custom_objects,\n-    )\n-    if not isinstance(obj, Layer):\n-        raise ValueError(\n-            \"`keras.layers.deserialize` was passed a `config` object that is \"\n-            f\"not a `keras.layers.Layer`. Received: {config}\"\n-        )\n-    return obj\n\n@@ -1,5 +0,0 @@\n-from keras_core.layers.activations.elu import ELU\n-from keras_core.layers.activations.leaky_relu import LeakyReLU\n-from keras_core.layers.activations.prelu import PReLU\n-from keras_core.layers.activations.relu import ReLU\n-from keras_core.layers.activations.softmax import Softmax\n\n@@ -1,69 +0,0 @@\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.merging.base_merge import Merge\n-\n-\n-@keras_core_export(\"keras_core.layers.Add\")\n-class Add(Merge):\n-    \"\"\"Performs elementwise addition operation.\n-\n-    It takes as input a list of tensors, all of the same shape,\n-    and returns a single tensor (also of the same shape).\n-\n-    Examples:\n-\n-    >>> input_shape = (2, 3, 4)\n-    >>> x1 = np.random.rand(*input_shape)\n-    >>> x2 = np.random.rand(*input_shape)\n-    >>> y = keras_core.layers.Add()([x1, x2])\n-\n-    Usage in a Keras model:\n-\n-    >>> input1 = keras_core.layers.Input(shape=(16,))\n-    >>> x1 = keras_core.layers.Dense(8, activation='relu')(input1)\n-    >>> input2 = keras_core.layers.Input(shape=(32,))\n-    >>> x2 = keras_core.layers.Dense(8, activation='relu')(input2)\n-    >>> # equivalent to `added = keras_core.layers.add([x1, x2])`\n-    >>> added = keras_core.layers.Add()([x1, x2])\n-    >>> out = keras_core.layers.Dense(4)(added)\n-    >>> model = keras_core.models.Model(inputs=[input1, input2], outputs=out)\n-\n-    \"\"\"\n-\n-    def _merge_function(self, inputs):\n-        output = inputs[0]\n-        for i in range(1, len(inputs)):\n-            output = ops.add(output, inputs[i])\n-        return output\n-\n-\n-@keras_core_export(\"keras_core.layers.add\")\n-def add(inputs, **kwargs):\n-    \"\"\"Functional interface to the `keras_core.layers.Add` layer.\n-\n-    Args:\n-        inputs: A list of input tensors with the same shape.\n-        **kwargs: Standard layer keyword arguments.\n-\n-    Returns:\n-        A tensor as the sum of the inputs. It has the same shape as the inputs.\n-\n-    Examples:\n-\n-    >>> input_shape = (2, 3, 4)\n-    >>> x1 = np.random.rand(*input_shape)\n-    >>> x2 = np.random.rand(*input_shape)\n-    >>> y = keras_core.layers.add([x1, x2])\n-\n-    Usage in a Keras model:\n-\n-    >>> input1 = keras_core.layers.Input(shape=(16,))\n-    >>> x1 = keras_core.layers.Dense(8, activation='relu')(input1)\n-    >>> input2 = keras_core.layers.Input(shape=(32,))\n-    >>> x2 = keras_core.layers.Dense(8, activation='relu')(input2)\n-    >>> added = keras_core.layers.add([x1, x2])\n-    >>> out = keras_core.layers.Dense(4)(added)\n-    >>> model = keras_core.models.Model(inputs=[input1, input2], outputs=out)\n-\n-    \"\"\"\n-    return Add(**kwargs)(inputs)\n\n@@ -1,70 +0,0 @@\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.merging.base_merge import Merge\n-\n-\n-@keras_core_export(\"keras_core.layers.Average\")\n-class Average(Merge):\n-    \"\"\"Averages a list of inputs element-wise..\n-\n-    It takes as input a list of tensors, all of the same shape,\n-    and returns a single tensor (also of the same shape).\n-\n-    Examples:\n-\n-    >>> input_shape = (2, 3, 4)\n-    >>> x1 = np.random.rand(*input_shape)\n-    >>> x2 = np.random.rand(*input_shape)\n-    >>> y = keras_core.layers.Average()([x1, x2])\n-\n-    Usage in a Keras model:\n-\n-    >>> input1 = keras_core.layers.Input(shape=(16,))\n-    >>> x1 = keras_core.layers.Dense(8, activation='relu')(input1)\n-    >>> input2 = keras_core.layers.Input(shape=(32,))\n-    >>> x2 = keras_core.layers.Dense(8, activation='relu')(input2)\n-    >>> # equivalent to `y = keras_core.layers.average([x1, x2])`\n-    >>> y = keras_core.layers.Average()([x1, x2])\n-    >>> out = keras_core.layers.Dense(4)(y)\n-    >>> model = keras_core.models.Model(inputs=[input1, input2], outputs=out)\n-\n-    \"\"\"\n-\n-    def _merge_function(self, inputs):\n-        output = inputs[0]\n-        for i in range(1, len(inputs)):\n-            output = ops.add(output, inputs[i])\n-        return output / len(inputs)\n-\n-\n-@keras_core_export(\"keras_core.layers.average\")\n-def average(inputs, **kwargs):\n-    \"\"\"Functional interface to the `keras_core.layers.Average` layer.\n-\n-    Args:\n-        inputs: A list of input tensors , all of the same shape.\n-        **kwargs: Standard layer keyword arguments.\n-\n-    Returns:\n-        A tensor as the element-wise product of the inputs with the same\n-        shape as the inputs.\n-\n-    Examples:\n-\n-    >>> input_shape = (2, 3, 4)\n-    >>> x1 = np.random.rand(*input_shape)\n-    >>> x2 = np.random.rand(*input_shape)\n-    >>> y = keras_core.layers.average([x1, x2])\n-\n-    Usage in a Keras model:\n-\n-    >>> input1 = keras_core.layers.Input(shape=(16,))\n-    >>> x1 = keras_core.layers.Dense(8, activation='relu')(input1)\n-    >>> input2 = keras_core.layers.Input(shape=(32,))\n-    >>> x2 = keras_core.layers.Dense(8, activation='relu')(input2)\n-    >>> y = keras_core.layers.average([x1, x2])\n-    >>> out = keras_core.layers.Dense(4)(y)\n-    >>> model = keras_core.models.Model(inputs=[input1, input2], outputs=out)\n-\n-    \"\"\"\n-    return Average(**kwargs)(inputs)\n\n@@ -1,70 +0,0 @@\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.merging.base_merge import Merge\n-\n-\n-@keras_core_export(\"keras_core.layers.Maximum\")\n-class Maximum(Merge):\n-    \"\"\"Computes element-wise maximum on a list of inputs.\n-\n-    It takes as input a list of tensors, all of the same shape,\n-    and returns a single tensor (also of the same shape).\n-\n-    Examples:\n-\n-    >>> input_shape = (2, 3, 4)\n-    >>> x1 = np.random.rand(*input_shape)\n-    >>> x2 = np.random.rand(*input_shape)\n-    >>> y = keras_core.layers.Maximum()([x1, x2])\n-\n-    Usage in a Keras model:\n-\n-    >>> input1 = keras_core.layers.Input(shape=(16,))\n-    >>> x1 = keras_core.layers.Dense(8, activation='relu')(input1)\n-    >>> input2 = keras_core.layers.Input(shape=(32,))\n-    >>> x2 = keras_core.layers.Dense(8, activation='relu')(input2)\n-    >>> # equivalent to `y = keras_core.layers.maximum([x1, x2])`\n-    >>> y = keras_core.layers.Maximum()([x1, x2])\n-    >>> out = keras_core.layers.Dense(4)(y)\n-    >>> model = keras_core.models.Model(inputs=[input1, input2], outputs=out)\n-\n-    \"\"\"\n-\n-    def _merge_function(self, inputs):\n-        output = inputs[0]\n-        for i in range(1, len(inputs)):\n-            output = ops.maximum(output, inputs[i])\n-        return output\n-\n-\n-@keras_core_export(\"keras_core.layers.maximum\")\n-def maximum(inputs, **kwargs):\n-    \"\"\"Functional interface to the `keras_core.layers.Maximum` layer.\n-\n-    Args:\n-        inputs: A list of input tensors , all of the same shape.\n-        **kwargs: Standard layer keyword arguments.\n-\n-    Returns:\n-        A tensor as the element-wise product of the inputs with the same\n-        shape as the inputs.\n-\n-    Examples:\n-\n-    >>> input_shape = (2, 3, 4)\n-    >>> x1 = np.random.rand(*input_shape)\n-    >>> x2 = np.random.rand(*input_shape)\n-    >>> y = keras_core.layers.maximum([x1, x2])\n-\n-    Usage in a Keras model:\n-\n-    >>> input1 = keras_core.layers.Input(shape=(16,))\n-    >>> x1 = keras_core.layers.Dense(8, activation='relu')(input1)\n-    >>> input2 = keras_core.layers.Input(shape=(32,))\n-    >>> x2 = keras_core.layers.Dense(8, activation='relu')(input2)\n-    >>> y = keras_core.layers.maximum([x1, x2])\n-    >>> out = keras_core.layers.Dense(4)(y)\n-    >>> model = keras_core.models.Model(inputs=[input1, input2], outputs=out)\n-\n-    \"\"\"\n-    return Maximum(**kwargs)(inputs)\n\n@@ -1,70 +0,0 @@\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.merging.base_merge import Merge\n-\n-\n-@keras_core_export(\"keras_core.layers.Minimum\")\n-class Minimum(Merge):\n-    \"\"\"Computes elementwise minimum on a list of inputs.\n-\n-    It takes as input a list of tensors, all of the same shape,\n-    and returns a single tensor (also of the same shape).\n-\n-    Examples:\n-\n-    >>> input_shape = (2, 3, 4)\n-    >>> x1 = np.random.rand(*input_shape)\n-    >>> x2 = np.random.rand(*input_shape)\n-    >>> y = keras_core.layers.Minimum()([x1, x2])\n-\n-    Usage in a Keras model:\n-\n-    >>> input1 = keras_core.layers.Input(shape=(16,))\n-    >>> x1 = keras_core.layers.Dense(8, activation='relu')(input1)\n-    >>> input2 = keras_core.layers.Input(shape=(32,))\n-    >>> x2 = keras_core.layers.Dense(8, activation='relu')(input2)\n-    >>> # equivalent to `y = keras_core.layers.minimum([x1, x2])`\n-    >>> y = keras_core.layers.Minimum()([x1, x2])\n-    >>> out = keras_core.layers.Dense(4)(y)\n-    >>> model = keras_core.models.Model(inputs=[input1, input2], outputs=out)\n-\n-    \"\"\"\n-\n-    def _merge_function(self, inputs):\n-        output = inputs[0]\n-        for i in range(1, len(inputs)):\n-            output = ops.minimum(output, inputs[i])\n-        return output\n-\n-\n-@keras_core_export(\"keras_core.layers.minimum\")\n-def minimum(inputs, **kwargs):\n-    \"\"\"Functional interface to the `keras_core.layers.Minimum` layer.\n-\n-    Args:\n-        inputs: A list of input tensors , all of the same shape.\n-        **kwargs: Standard layer keyword arguments.\n-\n-    Returns:\n-        A tensor as the elementwise product of the inputs with the same\n-        shape as the inputs.\n-\n-    Examples:\n-\n-    >>> input_shape = (2, 3, 4)\n-    >>> x1 = np.random.rand(*input_shape)\n-    >>> x2 = np.random.rand(*input_shape)\n-    >>> y = keras_core.layers.minimum([x1, x2])\n-\n-    Usage in a Keras model:\n-\n-    >>> input1 = keras_core.layers.Input(shape=(16,))\n-    >>> x1 = keras_core.layers.Dense(8, activation='relu')(input1)\n-    >>> input2 = keras_core.layers.Input(shape=(32,))\n-    >>> x2 = keras_core.layers.Dense(8, activation='relu')(input2)\n-    >>> y = keras_core.layers.minimum([x1, x2])\n-    >>> out = keras_core.layers.Dense(4)(y)\n-    >>> model = keras_core.models.Model(inputs=[input1, input2], outputs=out)\n-\n-    \"\"\"\n-    return Minimum(**kwargs)(inputs)\n\n@@ -1,70 +0,0 @@\n-from keras_core import ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.merging.base_merge import Merge\n-\n-\n-@keras_core_export(\"keras_core.layers.Multiply\")\n-class Multiply(Merge):\n-    \"\"\"Performs elementwise multiplication.\n-\n-    It takes as input a list of tensors, all of the same shape,\n-    and returns a single tensor (also of the same shape).\n-\n-    Examples:\n-\n-    >>> input_shape = (2, 3, 4)\n-    >>> x1 = np.random.rand(*input_shape)\n-    >>> x2 = np.random.rand(*input_shape)\n-    >>> y = keras_core.layers.Multiply()([x1, x2])\n-\n-    Usage in a Keras model:\n-\n-    >>> input1 = keras_core.layers.Input(shape=(16,))\n-    >>> x1 = keras_core.layers.Dense(8, activation='relu')(input1)\n-    >>> input2 = keras_core.layers.Input(shape=(32,))\n-    >>> x2 = keras_core.layers.Dense(8, activation='relu')(input2)\n-    >>> # equivalent to `y = keras_core.layers.multiply([x1, x2])`\n-    >>> y = keras_core.layers.Multiply()([x1, x2])\n-    >>> out = keras_core.layers.Dense(4)(y)\n-    >>> model = keras_core.models.Model(inputs=[input1, input2], outputs=out)\n-\n-    \"\"\"\n-\n-    def _merge_function(self, inputs):\n-        output = inputs[0]\n-        for i in range(1, len(inputs)):\n-            output = ops.multiply(output, inputs[i])\n-        return output\n-\n-\n-@keras_core_export(\"keras_core.layers.multiply\")\n-def multiply(inputs, **kwargs):\n-    \"\"\"Functional interface to the `keras_core.layers.Multiply` layer.\n-\n-    Args:\n-        inputs: A list of input tensors , all of the same shape.\n-        **kwargs: Standard layer keyword arguments.\n-\n-    Returns:\n-        A tensor as the elementwise product of the inputs with the same\n-        shape as the inputs.\n-\n-    Examples:\n-\n-    >>> input_shape = (2, 3, 4)\n-    >>> x1 = np.random.rand(*input_shape)\n-    >>> x2 = np.random.rand(*input_shape)\n-    >>> y = keras_core.layers.multiply([x1, x2])\n-\n-    Usage in a Keras model:\n-\n-    >>> input1 = keras_core.layers.Input(shape=(16,))\n-    >>> x1 = keras_core.layers.Dense(8, activation='relu')(input1)\n-    >>> input2 = keras_core.layers.Input(shape=(32,))\n-    >>> x2 = keras_core.layers.Dense(8, activation='relu')(input2)\n-    >>> y = keras_core.layers.multiply([x1, x2])\n-    >>> out = keras_core.layers.Dense(4)(y)\n-    >>> model = keras_core.models.Model(inputs=[input1, input2], outputs=out)\n-\n-    \"\"\"\n-    return Multiply(**kwargs)(inputs)\n\n@@ -1,3 +0,0 @@\n-from keras_core.models.functional import Functional\n-from keras_core.models.model import Model\n-from keras_core.models.sequential import Sequential\n\n@@ -1,15 +0,0 @@\n-# from keras_core.ops.numpy import Matmul, matmul\n-# from keras_core.ops.numpy import Add, add\n-# from keras_core.ops.numpy import Multiply, multiply\n-\n-from keras_core.backend import cast\n-from keras_core.backend import cond\n-from keras_core.backend import is_tensor\n-from keras_core.backend import name_scope\n-from keras_core.backend import random\n-from keras_core.ops import image\n-from keras_core.ops import operation_utils\n-from keras_core.ops.core import *  # noqa: F403\n-from keras_core.ops.math import *  # noqa: F403\n-from keras_core.ops.nn import *  # noqa: F403\n-from keras_core.ops.numpy import *  # noqa: F403\n\n@@ -1,16 +0,0 @@\n-from keras_core.optimizers.schedules.learning_rate_schedule import CosineDecay\n-from keras_core.optimizers.schedules.learning_rate_schedule import (\n-    CosineDecayRestarts,\n-)\n-from keras_core.optimizers.schedules.learning_rate_schedule import (\n-    ExponentialDecay,\n-)\n-from keras_core.optimizers.schedules.learning_rate_schedule import (\n-    InverseTimeDecay,\n-)\n-from keras_core.optimizers.schedules.learning_rate_schedule import (\n-    PiecewiseConstantDecay,\n-)\n-from keras_core.optimizers.schedules.learning_rate_schedule import (\n-    PolynomialDecay,\n-)\n\n@@ -1,9 +0,0 @@\n-from keras_core.saving.object_registration import CustomObjectScope\n-from keras_core.saving.object_registration import custom_object_scope\n-from keras_core.saving.object_registration import get_custom_objects\n-from keras_core.saving.object_registration import get_registered_name\n-from keras_core.saving.object_registration import get_registered_object\n-from keras_core.saving.object_registration import register_keras_serializable\n-from keras_core.saving.saving_api import load_model\n-from keras_core.saving.serialization_lib import deserialize_keras_object\n-from keras_core.saving.serialization_lib import serialize_keras_object\n\n@@ -1 +0,0 @@\n-from keras_core.testing.test_case import TestCase\n\n@@ -1,26 +0,0 @@\n-from keras_core.utils.audio_dataset_utils import audio_dataset_from_directory\n-from keras_core.utils.dataset_utils import split_dataset\n-from keras_core.utils.file_utils import get_file\n-from keras_core.utils.image_dataset_utils import image_dataset_from_directory\n-from keras_core.utils.image_utils import array_to_img\n-from keras_core.utils.image_utils import img_to_array\n-from keras_core.utils.image_utils import load_img\n-from keras_core.utils.image_utils import save_img\n-from keras_core.utils.io_utils import disable_interactive_logging\n-from keras_core.utils.io_utils import enable_interactive_logging\n-from keras_core.utils.io_utils import is_interactive_logging_enabled\n-from keras_core.utils.model_visualization import model_to_dot\n-from keras_core.utils.model_visualization import plot_model\n-from keras_core.utils.numerical_utils import normalize\n-from keras_core.utils.numerical_utils import to_categorical\n-from keras_core.utils.progbar import Progbar\n-from keras_core.utils.python_utils import default\n-from keras_core.utils.python_utils import is_default\n-from keras_core.utils.python_utils import removeprefix\n-from keras_core.utils.python_utils import removesuffix\n-from keras_core.utils.rng_utils import set_random_seed\n-from keras_core.utils.sequence_utils import pad_sequences\n-from keras_core.utils.text_dataset_utils import text_dataset_from_directory\n-from keras_core.utils.timeseries_dataset_utils import (\n-    timeseries_dataset_from_array,\n-)\n\n@@ -1,4 +1,4 @@\n-\"\"\"Script to create (and optionally install) a `.whl` archive for Keras Core.\n+\"\"\"Script to create (and optionally install) a `.whl` archive for Keras 3.\n \n Usage:\n \n@@ -25,7 +25,7 @@ import namex\n # Needed because importing torch after TF causes the runtime to crash\n import torch  # noqa: F401\n \n-package = \"keras_core\"\n+package = \"keras\"\n build_directory = \"tmp_build_dir\"\n dist_directory = \"dist\"\n to_copy = [\"setup.py\", \"README.md\"]\n@@ -36,7 +36,7 @@ def ignore_files(_, filenames):\n \n \n def copy_source_to_build_directory(root_path):\n-    # Copy sources (`keras_core/` directory and setup files) to build\n+    # Copy sources (`keras/` directory and setup files) to build\n     # directory\n     os.chdir(root_path)\n     os.mkdir(build_directory)\n@@ -49,22 +49,22 @@ def copy_source_to_build_directory(root_path):\n \n \n def run_namex_conversion():\n-    # Restructure the codebase so that source files live in `keras_core/src`\n+    # Restructure the codebase so that source files live in `keras/src`\n     namex.convert_codebase(package, code_directory=\"src\")\n \n-    # Generate API __init__.py files in `keras_core/`\n+    # Generate API __init__.py files in `keras/`\n     namex.generate_api_files(package, code_directory=\"src\", verbose=True)\n \n \n def create_legacy_directory():\n-    # Make keras_core/_tf_keras/ by copying keras_core/\n+    # Make keras/_tf_keras/ by copying keras/\n     tf_keras_dirpath = os.path.join(package, \"_tf_keras\")\n     os.makedirs(tf_keras_dirpath)\n     with open(os.path.join(package, \"__init__.py\")) as f:\n         init_file = f.read()\n         init_file = init_file.replace(\n-            \"from keras_core import _legacy\",\n-            \"from keras_core import _tf_keras\",\n+            \"from keras import _legacy\",\n+            \"from keras import _tf_keras\",\n         )\n     with open(os.path.join(package, \"__init__.py\"), \"w\") as f:\n         f.write(init_file)\n@@ -83,7 +83,7 @@ def create_legacy_directory():\n                 ignore=ignore_files,\n             )\n \n-    # Copy keras_core/_legacy/ file contents to keras_core/_tf_keras/\n+    # Copy keras/_legacy/ file contents to keras/_tf_keras/\n     legacy_submodules = [\n         path[:-3]\n         for path in os.listdir(os.path.join(package, \"src\", \"legacy\"))\n@@ -109,28 +109,28 @@ def create_legacy_directory():\n                 with open(legacy_fpath) as f:\n                     legacy_contents = f.read()\n                     legacy_contents = legacy_contents.replace(\n-                        \"keras_core._legacy\", \"keras_core._tf_keras\"\n+                        \"keras._legacy\", \"keras._tf_keras\"\n                     )\n                 if os.path.exists(core_api_fpath):\n                     with open(core_api_fpath) as f:\n                         core_api_contents = f.read()\n                     core_api_contents = core_api_contents.replace(\n-                        \"from keras_core import _tf_keras\\n\", \"\"\n+                        \"from keras import _tf_keras\\n\", \"\"\n                     )\n                     for legacy_submodule in legacy_submodules:\n                         core_api_contents = core_api_contents.replace(\n-                            f\"from keras_core import {legacy_submodule}\\n\",\n+                            f\"from keras import {legacy_submodule}\\n\",\n                             \"\",\n                         )\n                         core_api_contents = core_api_contents.replace(\n-                            f\"keras_core.{legacy_submodule}\",\n-                            f\"keras_core._tf_keras.{legacy_submodule}\",\n+                            f\"keras.{legacy_submodule}\",\n+                            f\"keras._tf_keras.{legacy_submodule}\",\n                         )\n                     legacy_contents = core_api_contents + \"\\n\" + legacy_contents\n                 with open(tf_keras_fpath, \"w\") as f:\n                     f.write(legacy_contents)\n \n-    # Delete keras_core/_legacy/\n+    # Delete keras/_legacy/\n     shutil.rmtree(os.path.join(package, \"_legacy\"))\n \n \n@@ -172,7 +172,7 @@ def build(root_path):\n         copy_source_to_build_directory(root_path)\n         run_namex_conversion()\n         create_legacy_directory()\n-        from keras_core.src.version import __version__  # noqa: E402\n+        from keras.src.version import __version__  # noqa: E402\n \n         export_version_string(__version__)\n         return build_and_save_output(root_path, __version__)\n\n@@ -23,18 +23,18 @@ def get_version(rel_path):\n \n HERE = pathlib.Path(__file__).parent\n README = (HERE / \"README.md\").read_text()\n-if os.path.exists(\"keras_core/version.py\"):\n-    VERSION = get_version(\"keras_core/version.py\")\n+if os.path.exists(\"keras/version.py\"):\n+    VERSION = get_version(\"keras/version.py\")\n else:\n-    VERSION = get_version(\"keras_core/__init__.py\")\n+    VERSION = get_version(\"keras/__init__.py\")\n \n setup(\n-    name=\"keras-core\",\n+    name=\"keras\",\n     description=\"Multi-backend Keras.\",\n     long_description_content_type=\"text/markdown\",\n     long_description=README,\n     version=VERSION,\n-    url=\"https://github.com/keras-team/keras-core\",\n+    url=\"https://github.com/keras-team/keras\",\n     author=\"Keras team\",\n     author_email=\"keras-users@googlegroups.com\",\n     license=\"Apache License 2.0\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
