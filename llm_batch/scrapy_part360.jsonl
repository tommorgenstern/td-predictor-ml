{"custom_id": "scrapy#d3b5c9be976851deee2253a8b86b80f57a2b695e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 1 | Files Changed: 3 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 6591 | Contributors (this commit): 50 | Commits (past 90d): 19 | Contributors (cumulative): 85 | DMM Complexity: 0.9\n\nDIFF:\n@@ -124,6 +124,7 @@ class Crawler:\n \n         self.settings.freeze()\n         self.crawling: bool = False\n+        self._started: bool = False\n         self.spider: Optional[Spider] = None\n         self.engine: Optional[ExecutionEngine] = None\n \n@@ -131,7 +132,13 @@ class Crawler:\n     def crawl(self, *args: Any, **kwargs: Any) -> Generator[Deferred, Any, None]:\n         if self.crawling:\n             raise RuntimeError(\"Crawling already taking place\")\n-        self.crawling = True\n+        if self._started:\n+            warnings.warn(\n+                \"Running Crawler.crawl() more than once is deprecated.\",\n+                ScrapyDeprecationWarning,\n+                stacklevel=2,\n+            )\n+        self.crawling = self._started = True\n \n         try:\n             self.spider = self._create_spider(*args, **kwargs)\n\n@@ -123,7 +123,9 @@ class CrawlTestCase(TestCase):\n         self.assertTrue(crawler.spider.t2 == 0)\n         self.assertTrue(crawler.spider.t2_err > 0)\n         self.assertTrue(crawler.spider.t2_err > crawler.spider.t1)\n+\n         # server hangs after receiving response headers\n+        crawler = get_crawler(DelaySpider, {\"DOWNLOAD_TIMEOUT\": 0.35})\n         yield crawler.crawl(n=0.5, b=1, mockserver=self.mockserver)\n         self.assertTrue(crawler.spider.t1 > 0)\n         self.assertTrue(crawler.spider.t2 == 0)\n@@ -201,6 +203,7 @@ class CrawlTestCase(TestCase):\n         )\n         self.assertEqual(crawler.spider.visited, 6)\n \n+        crawler = get_crawler(DuplicateStartRequestsSpider, settings)\n         yield crawler.crawl(\n             dont_filter=False,\n             distinct_urls=3,\n\n@@ -743,6 +743,7 @@ class Http11MockServerTestCase(unittest.TestCase):\n \n         # See issue https://twistedmatrix.com/trac/ticket/8175\n         raise unittest.SkipTest(\"xpayload fails on PY3\")\n+        crawler = get_crawler(SingleRequestSpider, self.settings_dict)\n         request.headers.setdefault(b\"Accept-Encoding\", b\"gzip,deflate\")\n         request = request.replace(url=self.mockserver.url(\"/xpayload\"))\n         yield crawler.crawl(seed=request)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b39d2d4353d95c24c56ee73c093c87268f2ef113", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 11 | Churn Cumulative: 1602 | Contributors (this commit): 22 | Commits (past 90d): 6 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,6 +6,7 @@ import sys\n import warnings\n from pathlib import Path\n \n+import pytest\n from packaging.version import parse as parse_version\n from pytest import mark, raises\n from twisted.internet import defer\n@@ -67,6 +68,16 @@ class CrawlerTestCase(BaseCrawlerTest):\n         with raises(ValueError):\n             Crawler(DefaultSpider())\n \n+    @defer.inlineCallbacks\n+    def test_crawler_crawl_twice_deprecated(self):\n+        crawler = Crawler(NoRequestsSpider)\n+        yield crawler.crawl()\n+        with pytest.warns(\n+            ScrapyDeprecationWarning,\n+            match=r\"Running Crawler.crawl\\(\\) more than once is deprecated\",\n+        ):\n+            yield crawler.crawl()\n+\n \n class SpiderSettingsTestCase(unittest.TestCase):\n     def test_spider_custom_settings(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#721df895f9ea9d8073c13fbd2f75a6fbdc75ffc7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 775 | Contributors (this commit): 24 | Commits (past 90d): 3 | Contributors (cumulative): 31 | DMM Complexity: None\n\nDIFF:\n@@ -96,10 +96,9 @@ class MailSender:\n             rcpts.extend(cc)\n             msg[\"Cc\"] = COMMASPACE.join(cc)\n \n+        if attachs:\n             if charset:\n                 msg.set_charset(charset)\n-\n-        if attachs:\n             msg.attach(MIMEText(body, \"plain\", charset or \"us-ascii\"))\n             for attach_name, mimetype, f in attachs:\n                 part = MIMEBase(*mimetype.split(\"/\"))\n@@ -110,7 +109,7 @@ class MailSender:\n                 )\n                 msg.attach(part)\n         else:\n-            msg.set_payload(body)\n+            msg.set_payload(body, charset)\n \n         if _callback:\n             _callback(to=to, subject=subject, body=body, cc=cc, attach=attachs, msg=msg)\n\n@@ -111,7 +111,7 @@ class MailSenderTest(unittest.TestCase):\n \n         msg = self.catched_msg[\"msg\"]\n         self.assertEqual(msg[\"subject\"], subject)\n-        self.assertEqual(msg.get_payload(), body)\n+        self.assertEqual(msg.get_payload(decode=True).decode(\"utf-8\"), body)\n         self.assertEqual(msg.get_charset(), Charset(\"utf-8\"))\n         self.assertEqual(msg.get(\"Content-Type\"), 'text/plain; charset=\"utf-8\"')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#97b98bf181874d22e4f7774a7950c10aaea6bd24", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 12 | Files Changed: 3 | Hunks: 11 | Methods Changed: 7 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 44 | Churn Cumulative: 3668 | Contributors (this commit): 43 | Commits (past 90d): 16 | Contributors (cumulative): 68 | DMM Complexity: 1.0\n\nDIFF:\n@@ -71,6 +71,7 @@ class Crawler:\n \n         self.spidercls: Type[Spider] = spidercls\n         self.settings: Settings = settings.copy()\n+        self.spidercls.update_settings(self.settings)\n \n         self.addons: AddonManager = AddonManager(self)\n         self.addons.load_settings(self.settings)\n@@ -104,10 +105,10 @@ class Crawler:\n             crawler=self,\n         )\n \n-        self._init_reactor = init_reactor\n-\n+        self._init_reactor: bool = init_reactor\n         self.crawling: bool = False\n         self._started: bool = False\n+        self.extensions: Optional[ExtensionManager] = None\n         self.spider: Optional[Spider] = None\n         self.engine: Optional[ExecutionEngine] = None\n \n@@ -125,7 +126,6 @@ class Crawler:\n \n         try:\n             self.spider = self._create_spider(*args, **kwargs)\n-            self.spider.update_settings(self.settings)\n \n             reactor_class: str = self.settings[\"TWISTED_REACTOR\"]\n             event_loop: str = self.settings[\"ASYNCIO_EVENT_LOOP\"]\n@@ -142,7 +142,7 @@ class Crawler:\n                 if is_asyncio_reactor_installed() and event_loop:\n                     verify_installed_asyncio_event_loop(event_loop)\n \n-            self.extensions: ExtensionManager = ExtensionManager.from_crawler(self)\n+            self.extensions = ExtensionManager.from_crawler(self)\n             self.settings.freeze()\n \n             self.engine = self._create_engine()\n\n@@ -83,8 +83,9 @@ class Spider(object_ref):\n             f\"{self.__class__.__name__}.parse callback is not defined\"\n         )\n \n-    def update_settings(self, settings: BaseSettings) -> None:\n-        settings.setdict(self.custom_settings or {}, priority=\"spider\")\n+    @classmethod\n+    def update_settings(cls, settings: BaseSettings) -> None:\n+        settings.setdict(cls.custom_settings or {}, priority=\"spider\")\n \n     @classmethod\n     def handles_request(cls, request: Request) -> bool:\n\n@@ -2,13 +2,16 @@ import gzip\n import inspect\n import warnings\n from io import BytesIO\n+from typing import Any\n from unittest import mock\n \n from testfixtures import LogCapture\n+from twisted.internet.defer import inlineCallbacks\n from twisted.trial import unittest\n from w3lib.url import safe_url_string\n \n from scrapy import signals\n+from scrapy.crawler import Crawler\n from scrapy.http import HtmlResponse, Request, Response, TextResponse, XmlResponse\n from scrapy.linkextractors import LinkExtractor\n from scrapy.settings import Settings\n@@ -91,16 +94,32 @@ class SpiderTest(unittest.TestCase):\n         self.spider_class.custom_settings = spider_settings\n         settings = Settings(project_settings, priority=\"project\")\n \n-        spider = self.spider_class(\"example.com\")\n-        spider.update_settings(settings)\n+        self.spider_class.update_settings(settings)\n         self.assertEqual(settings.get(\"TEST1\"), \"spider\")\n         self.assertEqual(settings.get(\"TEST2\"), \"spider\")\n         self.assertEqual(settings.get(\"TEST3\"), \"project\")\n \n-        spider_instance_settings = {\"TEST1\": \"spider_instance\"}\n-        spider.custom_settings = spider_instance_settings\n-        spider.update_settings(settings)\n-        self.assertEqual(settings.get(\"TEST1\"), \"spider_instance\")\n+    @inlineCallbacks\n+    def test_settings_in_from_crawler(self):\n+        spider_settings = {\"TEST1\": \"spider\", \"TEST2\": \"spider\"}\n+        project_settings = {\"TEST1\": \"project\", \"TEST3\": \"project\"}\n+\n+        class TestSpider(self.spider_class):\n+            name = \"test\"\n+            custom_settings = spider_settings\n+\n+            @classmethod\n+            def from_crawler(cls, crawler: Crawler, *args: Any, **kwargs: Any):\n+                spider = super().from_crawler(crawler, *args, **kwargs)\n+                spider.settings.set(\"TEST1\", \"spider_instance\", priority=\"spider\")\n+                return spider\n+\n+        crawler = get_crawler(TestSpider, settings_dict=project_settings)\n+        self.assertEqual(crawler.settings.get(\"TEST1\"), \"spider\")\n+        self.assertEqual(crawler.settings.get(\"TEST2\"), \"spider\")\n+        self.assertEqual(crawler.settings.get(\"TEST3\"), \"project\")\n+        yield crawler.crawl()\n+        self.assertEqual(crawler.settings.get(\"TEST1\"), \"spider_instance\")\n \n     def test_logger(self):\n         spider = self.spider_class(\"example.com\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6629a61dd98735994cce2fcfba63cad46e5d3683", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 888 | Contributors (this commit): 15 | Commits (past 90d): 2 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -431,12 +431,14 @@ class MockedMediaPipelineDeprecatedMethods(ImagesPipeline):\n class MediaPipelineDeprecatedMethodsTestCase(unittest.TestCase):\n     skip = skip_pillow\n \n+    @inlineCallbacks\n     def setUp(self):\n         settings_dict = {\n             \"IMAGES_STORE\": \"store-uri\",\n             \"IMAGES_THUMBS\": {\"small\": (50, 50)},\n         }\n-        crawler = get_crawler(spidercls=None, settings_dict=settings_dict)\n+        crawler = get_crawler(NoRequestsSpider, settings_dict=settings_dict)\n+        yield crawler.crawl()\n         self.pipe = MockedMediaPipelineDeprecatedMethods.from_crawler(crawler)\n         self.pipe.download_func = _mocked_download_func\n         self.pipe.open_spider(None)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bb15c93a2bbd7daaee4a02d2d6f6b52cea4b3313", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 265 | Lines Deleted: 395 | Files Changed: 27 | Hunks: 292 | Methods Changed: 108 | Complexity Δ (Sum/Max): -1/2 | Churn Δ: 660 | Churn Cumulative: 28936 | Contributors (this commit): 123 | Commits (past 90d): 102 | Contributors (cumulative): 428 | DMM Complexity: 0.5641025641025641\n\nDIFF:\n@@ -77,6 +77,7 @@ class Command(ScrapyCommand):\n         # The crawler is created this way since the Shell manually handles the\n         # crawling engine, so the set up in the crawl method won't work\n         crawler = self.crawler_process._create_crawler(spidercls)\n+        crawler._load_settings()\n         # The Shell class needs a persistent engine in the crawler\n         crawler.engine = crawler._create_engine()\n         crawler.engine.start()\n\n@@ -82,6 +82,7 @@ class Crawler:\n \n         self._init_reactor: bool = init_reactor\n         self.crawling: bool = False\n+        self._settings_loaded: bool = False\n         self._started: bool = False\n         self.extensions: Optional[ExtensionManager] = None\n         self.stats: Optional[StatsCollector] = None\n@@ -90,20 +91,10 @@ class Crawler:\n         self.spider: Optional[Spider] = None\n         self.engine: Optional[ExecutionEngine] = None\n \n-    @inlineCallbacks\n-    def crawl(self, *args: Any, **kwargs: Any) -> Generator[Deferred, Any, None]:\n-        if self.crawling:\n-            raise RuntimeError(\"Crawling already taking place\")\n-        if self._started:\n-            warnings.warn(\n-                \"Running Crawler.crawl() more than once is deprecated.\",\n-                ScrapyDeprecationWarning,\n-                stacklevel=2,\n-            )\n-        self.crawling = self._started = True\n-\n-        try:\n-            self.spider = self._create_spider(*args, **kwargs)\n+    def _load_settings(self) -> None:\n+        if self._settings_loaded:\n+            return\n+        self._settings_loaded = True\n \n         self.addons.load_settings(self.settings)\n         self.stats = load_object(self.settings[\"STATS_CLASS\"])(self)\n@@ -147,6 +138,21 @@ class Crawler:\n             \"Overridden settings:\\n%(settings)s\", {\"settings\": pprint.pformat(d)}\n         )\n \n+    @inlineCallbacks\n+    def crawl(self, *args: Any, **kwargs: Any) -> Generator[Deferred, Any, None]:\n+        if self.crawling:\n+            raise RuntimeError(\"Crawling already taking place\")\n+        if self._started:\n+            warnings.warn(\n+                \"Running Crawler.crawl() more than once is deprecated.\",\n+                ScrapyDeprecationWarning,\n+                stacklevel=2,\n+            )\n+        self.crawling = self._started = True\n+\n+        try:\n+            self.spider = self._create_spider(*args, **kwargs)\n+            self._load_settings()\n             self.engine = self._create_engine()\n             start_requests = iter(self.spider.start_requests())\n             yield self.engine.open_spider(self.spider, start_requests)\n\n@@ -69,28 +69,30 @@ def get_ftp_content_and_delete(\n     return b\"\".join(ftp_data)\n \n \n+class TestSpider(Spider):\n+    name = \"test\"\n+\n+\n def get_crawler(\n     spidercls: Optional[Type[Spider]] = None,\n     settings_dict: Optional[Dict[str, Any]] = None,\n     prevent_warnings: bool = True,\n-    disable_telnet: bool = True,\n ) -> Crawler:\n     \"\"\"Return an unconfigured Crawler object. If settings_dict is given, it\n     will be used to populate the crawler settings with a project level\n     priority.\n     \"\"\"\n     from scrapy.crawler import CrawlerRunner\n-    from scrapy.spiders import Spider\n \n     # Set by default settings that prevent deprecation warnings.\n     settings: Dict[str, Any] = {}\n     if prevent_warnings:\n         settings[\"REQUEST_FINGERPRINTER_IMPLEMENTATION\"] = \"2.7\"\n-    if disable_telnet:\n-        settings[\"TELNETCONSOLE_ENABLED\"] = False\n     settings.update(settings_dict or {})\n     runner = CrawlerRunner(settings)\n-    return runner.create_crawler(spidercls or Spider)\n+    crawler = runner.create_crawler(spidercls or TestSpider)\n+    crawler._load_settings()\n+    return crawler\n \n \n def get_pythonpath() -> str:\n\n@@ -18,13 +18,6 @@ from scrapy.utils.defer import deferred_to_future, maybe_deferred_to_future\n from scrapy.utils.test import get_from_asyncio_queue, get_web_client_agent_req\n \n \n-class NoRequestsSpider(Spider):\n-    name = \"no_request\"\n-\n-    def start_requests(self):\n-        return []\n-\n-\n class MockServerSpider(Spider):\n     def __init__(self, mockserver=None, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n\n@@ -1,15 +1,13 @@\n import itertools\n+import unittest\n from typing import Any, Dict\n from unittest.mock import patch\n \n-from twisted.internet.defer import inlineCallbacks\n-from twisted.trial import unittest\n-\n+from scrapy import Spider\n from scrapy.crawler import Crawler, CrawlerRunner\n from scrapy.exceptions import NotConfigured\n from scrapy.settings import BaseSettings, Settings\n from scrapy.utils.test import get_crawler\n-from tests.spiders import NoRequestsSpider\n \n \n class SimpleAddon:\n@@ -53,17 +51,14 @@ class AddonTest(unittest.TestCase):\n \n \n class AddonManagerTest(unittest.TestCase):\n-    @inlineCallbacks\n     def test_load_settings(self):\n         settings_dict = {\n             \"ADDONS\": {\"tests.test_addons.SimpleAddon\": 0},\n         }\n-        crawler = get_crawler(NoRequestsSpider, settings_dict=settings_dict)\n-        yield crawler.crawl()\n+        crawler = get_crawler(settings_dict=settings_dict)\n         manager = crawler.addons\n         self.assertIsInstance(manager.addons[0], SimpleAddon)\n \n-    @inlineCallbacks\n     def test_notconfigured(self):\n         class NotConfiguredAddon:\n             def update_settings(self, settings):\n@@ -72,12 +67,10 @@ class AddonManagerTest(unittest.TestCase):\n         settings_dict = {\n             \"ADDONS\": {NotConfiguredAddon: 0},\n         }\n-        crawler = get_crawler(NoRequestsSpider, settings_dict=settings_dict)\n-        yield crawler.crawl()\n+        crawler = get_crawler(settings_dict=settings_dict)\n         manager = crawler.addons\n         self.assertFalse(manager.addons)\n \n-    @inlineCallbacks\n     def test_load_settings_order(self):\n         # Get three addons with different settings\n         addonlist = []\n@@ -89,25 +82,21 @@ class AddonManagerTest(unittest.TestCase):\n         for ordered_addons in itertools.permutations(addonlist):\n             expected_order = [a.number for a in ordered_addons]\n             settings = {\"ADDONS\": {a: i for i, a in enumerate(ordered_addons)}}\n-            crawler = get_crawler(NoRequestsSpider, settings_dict=settings)\n-            yield crawler.crawl()\n+            crawler = get_crawler(settings_dict=settings)\n             manager = crawler.addons\n             self.assertEqual([a.number for a in manager.addons], expected_order)\n             self.assertEqual(crawler.settings.getint(\"KEY1\"), expected_order[-1])\n \n-    @inlineCallbacks\n     def test_create_instance(self):\n         settings_dict = {\n             \"ADDONS\": {\"tests.test_addons.CreateInstanceAddon\": 0},\n             \"MYADDON\": {\"MYADDON_KEY\": \"val\"},\n         }\n-        crawler = get_crawler(NoRequestsSpider, settings_dict=settings_dict)\n-        yield crawler.crawl()\n+        crawler = get_crawler(settings_dict=settings_dict)\n         manager = crawler.addons\n         self.assertIsInstance(manager.addons[0], CreateInstanceAddon)\n         self.assertEqual(crawler.settings.get(\"MYADDON_KEY\"), \"val\")\n \n-    @inlineCallbacks\n     def test_settings_priority(self):\n         config = {\n             \"KEY\": 15,  # priority=addon\n@@ -115,15 +104,14 @@ class AddonManagerTest(unittest.TestCase):\n         settings_dict = {\n             \"ADDONS\": {get_addon_cls(config): 1},\n         }\n-        crawler = get_crawler(NoRequestsSpider, settings_dict=settings_dict)\n-        yield crawler.crawl()\n+        crawler = get_crawler(settings_dict=settings_dict)\n         self.assertEqual(crawler.settings.getint(\"KEY\"), 15)\n \n         settings = Settings(settings_dict)\n         settings.set(\"KEY\", 0, priority=\"default\")\n         runner = CrawlerRunner(settings)\n-        crawler = runner.create_crawler(NoRequestsSpider)\n-        yield crawler.crawl()\n+        crawler = runner.create_crawler(Spider)\n+        crawler._load_settings()\n         self.assertEqual(crawler.settings.getint(\"KEY\"), 15)\n \n         settings_dict = {\n@@ -133,11 +121,9 @@ class AddonManagerTest(unittest.TestCase):\n         settings = Settings(settings_dict)\n         settings.set(\"KEY\", 0, priority=\"default\")\n         runner = CrawlerRunner(settings)\n-        crawler = runner.create_crawler(NoRequestsSpider)\n-        yield crawler.crawl()\n+        crawler = runner.create_crawler(Spider)\n         self.assertEqual(crawler.settings.getint(\"KEY\"), 20)\n \n-    @inlineCallbacks\n     def test_fallback_workflow(self):\n         FALLBACK_SETTING = \"MY_FALLBACK_DOWNLOAD_HANDLER\"\n \n@@ -154,8 +140,7 @@ class AddonManagerTest(unittest.TestCase):\n         settings_dict = {\n             \"ADDONS\": {AddonWithFallback: 1},\n         }\n-        crawler = get_crawler(NoRequestsSpider, settings_dict=settings_dict)\n-        yield crawler.crawl()\n+        crawler = get_crawler(settings_dict=settings_dict)\n         self.assertEqual(\n             crawler.settings.getwithbase(\"DOWNLOAD_HANDLERS\")[\"https\"], \"AddonHandler\"\n         )\n@@ -168,14 +153,12 @@ class AddonManagerTest(unittest.TestCase):\n             \"ADDONS\": {AddonWithFallback: 1},\n             \"DOWNLOAD_HANDLERS\": {\"https\": \"UserHandler\"},\n         }\n-        crawler = get_crawler(NoRequestsSpider, settings_dict=settings_dict)\n-        yield crawler.crawl()\n+        crawler = get_crawler(settings_dict=settings_dict)\n         self.assertEqual(\n             crawler.settings.getwithbase(\"DOWNLOAD_HANDLERS\")[\"https\"], \"AddonHandler\"\n         )\n         self.assertEqual(crawler.settings.get(FALLBACK_SETTING), \"UserHandler\")\n \n-    @inlineCallbacks\n     def test_logging_message(self):\n         class LoggedAddon:\n             def update_settings(self, settings):\n@@ -188,8 +171,7 @@ class AddonManagerTest(unittest.TestCase):\n                 }\n                 addon = LoggedAddon()\n                 create_instance_mock.return_value = addon\n-                crawler = get_crawler(NoRequestsSpider, settings_dict=settings_dict)\n-                yield crawler.crawl()\n+                crawler = get_crawler(settings_dict=settings_dict)\n                 logger_mock.info.assert_called_once_with(\n                     \"Enabled addons:\\n%(addons)s\",\n                     {\"addons\": [addon]},\n\n@@ -7,8 +7,6 @@ from scrapy.utils.testproc import ProcessTest\n from scrapy.utils.testsite import SiteTest\n from tests import NON_EXISTING_RESOLVABLE, tests_datadir\n \n-raise unittest.SkipTest(\"Broken for now\")\n-\n \n class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n     command = \"shell\"\n\n@@ -26,8 +26,7 @@ from scrapy.commands.startproject import IGNORE\n from scrapy.settings import Settings\n from scrapy.utils.python import to_unicode\n from scrapy.utils.test import get_testenv\n-from tests.spiders import NoRequestsSpider\n-from tests.test_crawler import ExceptionSpider\n+from tests.test_crawler import ExceptionSpider, NoRequestsSpider\n \n \n class CommandSettings(unittest.TestCase):\n@@ -713,7 +712,7 @@ class BadSpider(scrapy.Spider):\n \n     def test_run_good_spider(self):\n         proc, _, _ = self.runspider(\n-            \"from scrapy import Spider\\n\" + inspect.getsource(NoRequestsSpider)\n+            \"import scrapy\\n\" + inspect.getsource(NoRequestsSpider)\n         )\n         ret = proc.returncode\n         self.assertEqual(ret, 0)\n\n@@ -10,7 +10,6 @@ import pytest\n from packaging.version import parse as parse_version\n from pytest import mark, raises\n from twisted.internet import defer\n-from twisted.internet.defer import inlineCallbacks\n from twisted.trial import unittest\n from w3lib import __version__ as w3lib_version\n \n@@ -24,7 +23,6 @@ from scrapy.utils.log import configure_logging, get_scrapy_root_handler\n from scrapy.utils.spider import DefaultSpider\n from scrapy.utils.test import get_crawler\n from tests.mockserver import MockServer, get_mockserver_env\n-from tests.spiders import NoRequestsSpider\n \n \n class BaseCrawlerTest(unittest.TestCase):\n@@ -72,7 +70,6 @@ class CrawlerLoggingTestCase(unittest.TestCase):\n         get_crawler(MySpider)\n         assert get_scrapy_root_handler() is None\n \n-    @inlineCallbacks\n     def test_spider_custom_settings_log_level(self):\n         log_file = Path(self.mktemp())\n         log_file.write_text(\"previous message\\n\", encoding=\"utf-8\")\n@@ -82,20 +79,20 @@ class CrawlerLoggingTestCase(unittest.TestCase):\n             custom_settings = {\n                 \"LOG_LEVEL\": \"INFO\",\n                 \"LOG_FILE\": str(log_file),\n+                # settings to avoid extra warnings\n+                \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n+                \"TELNETCONSOLE_ENABLED\": telnet.TWISTED_CONCH_AVAILABLE,\n             }\n \n-            def start_requests(self):\n-                logging.debug(\"debug message\")\n-                logging.info(\"info message\")\n-                logging.warning(\"warning message\")\n-                logging.error(\"error message\")\n-                return []\n-\n         configure_logging()\n         self.assertEqual(get_scrapy_root_handler().level, logging.DEBUG)\n         crawler = get_crawler(MySpider)\n-        yield crawler.crawl()\n         self.assertEqual(get_scrapy_root_handler().level, logging.INFO)\n+        info_count = crawler.stats.get_value(\"log_count/INFO\")\n+        logging.debug(\"debug message\")\n+        logging.info(\"info message\")\n+        logging.warning(\"warning message\")\n+        logging.error(\"error message\")\n \n         logged = log_file.read_text(encoding=\"utf-8\")\n \n@@ -106,7 +103,7 @@ class CrawlerLoggingTestCase(unittest.TestCase):\n         self.assertIn(\"error message\", logged)\n         self.assertEqual(crawler.stats.get_value(\"log_count/ERROR\"), 1)\n         self.assertEqual(crawler.stats.get_value(\"log_count/WARNING\"), 1)\n-        # self.assertEqual(crawler.stats.get_value(\"log_count/INFO\") - info_count, 1)  TODO\n+        self.assertEqual(crawler.stats.get_value(\"log_count/INFO\") - info_count, 1)\n         self.assertEqual(crawler.stats.get_value(\"log_count/DEBUG\", 0), 0)\n \n     def test_spider_custom_settings_log_append(self):\n@@ -183,6 +180,13 @@ class ExceptionSpider(scrapy.Spider):\n         raise ValueError(\"Exception in from_crawler method\")\n \n \n+class NoRequestsSpider(scrapy.Spider):\n+    name = \"no_request\"\n+\n+    def start_requests(self):\n+        return []\n+\n+\n @mark.usefixtures(\"reactor_pytest\")\n class CrawlerRunnerHasSpider(unittest.TestCase):\n     def _runner(self):\n\n@@ -3,33 +3,33 @@ from unittest import mock\n \n from pytest import mark\n from twisted.internet import defer\n-from twisted.internet.defer import Deferred, inlineCallbacks\n+from twisted.internet.defer import Deferred\n from twisted.python.failure import Failure\n from twisted.trial.unittest import TestCase\n \n from scrapy.core.downloader.middleware import DownloaderMiddlewareManager\n from scrapy.exceptions import _InvalidOutput\n from scrapy.http import Request, Response\n+from scrapy.spiders import Spider\n from scrapy.utils.python import to_bytes\n from scrapy.utils.test import get_crawler, get_from_asyncio_queue\n-from tests.spiders import NoRequestsSpider\n \n \n class ManagerTestCase(TestCase):\n     settings_dict = None\n \n-    @inlineCallbacks\n     def setUp(self):\n-        self.crawler = get_crawler(NoRequestsSpider, self.settings_dict)\n-        yield self.crawler.crawl()\n-        self.spider = self.crawler.spider\n+        self.crawler = get_crawler(Spider, self.settings_dict)\n+        self.spider = self.crawler._create_spider(\"foo\")\n         self.mwman = DownloaderMiddlewareManager.from_crawler(self.crawler)\n-        yield self.mwman.open_spider(self.spider)\n+        # some mw depends on stats collector\n+        self.crawler.stats.open_spider(self.spider)\n+        return self.mwman.open_spider(self.spider)\n \n     def tearDown(self):\n+        self.crawler.stats.close_spider(self.spider, \"\")\n         return self.mwman.close_spider(self.spider)\n \n-    @inlineCallbacks\n     def _download(self, request, response=None):\n         \"\"\"Executes downloader mw manager's download method and returns\n         the result (Request or Response) or raise exception in case of\n@@ -41,21 +41,26 @@ class ManagerTestCase(TestCase):\n         def download_func(**kwargs):\n             return response\n \n-        ret = yield self.mwman.download(download_func, request, self.spider)\n+        dfd = self.mwman.download(download_func, request, self.spider)\n+        # catch deferred result and return the value\n+        results = []\n+        dfd.addBoth(results.append)\n+        self._wait(dfd)\n+        ret = results[0]\n+        if isinstance(ret, Failure):\n+            ret.raiseException()\n         return ret\n \n \n class DefaultsTest(ManagerTestCase):\n     \"\"\"Tests default behavior with default settings\"\"\"\n \n-    @inlineCallbacks\n     def test_request_response(self):\n         req = Request(\"http://example.com/index.html\")\n         resp = Response(req.url, status=200)\n-        ret = yield self._download(req, resp)\n+        ret = self._download(req, resp)\n         self.assertTrue(isinstance(ret, Response), \"Non-response returned\")\n \n-    @inlineCallbacks\n     def test_3xx_and_invalid_gzipped_body_must_redirect(self):\n         \"\"\"Regression test for a failure when redirecting a compressed\n         request.\n@@ -81,7 +86,7 @@ class DefaultsTest(ManagerTestCase):\n                 \"Location\": \"http://example.com/login\",\n             },\n         )\n-        ret = yield self._download(request=req, response=resp)\n+        ret = self._download(request=req, response=resp)\n         self.assertTrue(isinstance(ret, Request), f\"Not redirected: {ret!r}\")\n         self.assertEqual(\n             to_bytes(ret.url),\n@@ -89,7 +94,6 @@ class DefaultsTest(ManagerTestCase):\n             \"Not redirected to location header\",\n         )\n \n-    @inlineCallbacks\n     def test_200_and_invalid_gzipped_body_must_fail(self):\n         req = Request(\"http://example.com\")\n         body = b\"<p>You are being redirected</p>\"\n@@ -104,14 +108,12 @@ class DefaultsTest(ManagerTestCase):\n                 \"Location\": \"http://example.com/login\",\n             },\n         )\n-        with self.assertRaises(OSError):\n-            yield self._download(request=req, response=resp)\n+        self.assertRaises(OSError, self._download, request=req, response=resp)\n \n \n class ResponseFromProcessRequestTest(ManagerTestCase):\n     \"\"\"Tests middleware returning a response from process_request.\"\"\"\n \n-    @inlineCallbacks\n     def test_download_func_not_called(self):\n         resp = Response(\"http://example.com/index.html\")\n \n@@ -123,8 +125,12 @@ class ResponseFromProcessRequestTest(ManagerTestCase):\n \n         req = Request(\"http://example.com/index.html\")\n         download_func = mock.MagicMock()\n-        result = yield self.mwman.download(download_func, req, self.spider)\n-        self.assertIs(result, resp)\n+        dfd = self.mwman.download(download_func, req, self.spider)\n+        results = []\n+        dfd.addBoth(results.append)\n+        self._wait(dfd)\n+\n+        self.assertIs(results[0], resp)\n         self.assertFalse(download_func.called)\n \n \n@@ -191,7 +197,6 @@ class ProcessExceptionInvalidOutput(ManagerTestCase):\n class MiddlewareUsingDeferreds(ManagerTestCase):\n     \"\"\"Middlewares using Deferreds should work\"\"\"\n \n-    @inlineCallbacks\n     def test_deferred(self):\n         resp = Response(\"http://example.com/index.html\")\n \n@@ -208,8 +213,12 @@ class MiddlewareUsingDeferreds(ManagerTestCase):\n         self.mwman._add_middleware(DeferredMiddleware())\n         req = Request(\"http://example.com/index.html\")\n         download_func = mock.MagicMock()\n-        result = yield self.mwman.download(download_func, req, self.spider)\n-        self.assertIs(result, resp)\n+        dfd = self.mwman.download(download_func, req, self.spider)\n+        results = []\n+        dfd.addBoth(results.append)\n+        self._wait(dfd)\n+\n+        self.assertIs(results[0], resp)\n         self.assertFalse(download_func.called)\n \n \n@@ -217,7 +226,6 @@ class MiddlewareUsingDeferreds(ManagerTestCase):\n class MiddlewareUsingCoro(ManagerTestCase):\n     \"\"\"Middlewares using asyncio coroutines should work\"\"\"\n \n-    @inlineCallbacks\n     def test_asyncdef(self):\n         resp = Response(\"http://example.com/index.html\")\n \n@@ -229,12 +237,15 @@ class MiddlewareUsingCoro(ManagerTestCase):\n         self.mwman._add_middleware(CoroMiddleware())\n         req = Request(\"http://example.com/index.html\")\n         download_func = mock.MagicMock()\n-        result = yield self.mwman.download(download_func, req, self.spider)\n-        self.assertIs(result, resp)\n+        dfd = self.mwman.download(download_func, req, self.spider)\n+        results = []\n+        dfd.addBoth(results.append)\n+        self._wait(dfd)\n+\n+        self.assertIs(results[0], resp)\n         self.assertFalse(download_func.called)\n \n     @mark.only_asyncio()\n-    @inlineCallbacks\n     def test_asyncdef_asyncio(self):\n         resp = Response(\"http://example.com/index.html\")\n \n@@ -247,6 +258,10 @@ class MiddlewareUsingCoro(ManagerTestCase):\n         self.mwman._add_middleware(CoroMiddleware())\n         req = Request(\"http://example.com/index.html\")\n         download_func = mock.MagicMock()\n-        result = yield self.mwman.download(download_func, req, self.spider)\n-        self.assertIs(result, resp)\n+        dfd = self.mwman.download(download_func, req, self.spider)\n+        results = []\n+        dfd.addBoth(results.append)\n+        self._wait(dfd)\n+\n+        self.assertIs(results[0], resp)\n         self.assertFalse(download_func.called)\n\n@@ -2,31 +2,27 @@ import email.utils\n import shutil\n import tempfile\n import time\n+import unittest\n from contextlib import contextmanager\n \n-from twisted.internet.defer import inlineCallbacks\n-from twisted.trial import unittest\n-\n from scrapy.downloadermiddlewares.httpcache import HttpCacheMiddleware\n from scrapy.exceptions import IgnoreRequest\n from scrapy.http import HtmlResponse, Request, Response\n from scrapy.settings import Settings\n+from scrapy.spiders import Spider\n from scrapy.utils.test import get_crawler\n-from tests.spiders import NoRequestsSpider\n \n \n class _BaseTest(unittest.TestCase):\n     storage_class = \"scrapy.extensions.httpcache.DbmCacheStorage\"\n     policy_class = \"scrapy.extensions.httpcache.RFC2616Policy\"\n \n-    @inlineCallbacks\n     def setUp(self):\n         self.yesterday = email.utils.formatdate(time.time() - 86400)\n         self.today = email.utils.formatdate()\n         self.tomorrow = email.utils.formatdate(time.time() + 86400)\n-        self.crawler = get_crawler(NoRequestsSpider)\n-        yield self.crawler.crawl()\n-        self.spider = self.crawler.spider\n+        self.crawler = get_crawler(Spider)\n+        self.spider = self.crawler._create_spider(\"example.com\")\n         self.tmpdir = tempfile.mkdtemp()\n         self.request = Request(\"http://www.example.com\", headers={\"User-Agent\": \"test\"})\n         self.response = Response(\n@@ -35,6 +31,7 @@ class _BaseTest(unittest.TestCase):\n             body=b\"test body\",\n             status=202,\n         )\n+        self.crawler.stats.open_spider(self.spider)\n \n     def tearDown(self):\n         self.crawler.stats.close_spider(self.spider, \"\")\n@@ -569,3 +566,7 @@ class RFC2616PolicyTest(DefaultStorageTest):\n                 res2 = self._process_requestresponse(mw, req0, None)\n                 self.assertEqualResponse(res1, res2)\n                 assert \"cached\" in res2.flags\n+\n+\n+if __name__ == \"__main__\":\n+    unittest.main()\n\n@@ -1,10 +1,9 @@\n from gzip import GzipFile\n from io import BytesIO\n from pathlib import Path\n+from unittest import SkipTest, TestCase\n from warnings import catch_warnings\n \n-from twisted.internet.defer import inlineCallbacks\n-from twisted.trial import unittest\n from w3lib.encoding import resolve_encoding\n \n from scrapy.downloadermiddlewares.httpcompression import (\n@@ -14,10 +13,10 @@ from scrapy.downloadermiddlewares.httpcompression import (\n from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.http import HtmlResponse, Request, Response\n from scrapy.responsetypes import responsetypes\n+from scrapy.spiders import Spider\n from scrapy.utils.gz import gunzip\n from scrapy.utils.test import get_crawler\n from tests import tests_datadir\n-from tests.spiders import NoRequestsSpider\n \n SAMPLEDIR = Path(tests_datadir, \"compressed\")\n \n@@ -39,13 +38,12 @@ FORMAT = {\n }\n \n \n-class HttpCompressionTest(unittest.TestCase):\n-    @inlineCallbacks\n+class HttpCompressionTest(TestCase):\n     def setUp(self):\n-        self.crawler = get_crawler(NoRequestsSpider)\n-        yield self.crawler.crawl()\n-        self.spider = self.crawler.spider\n+        self.crawler = get_crawler(Spider)\n+        self.spider = self.crawler._create_spider(\"scrapytest.org\")\n         self.mw = HttpCompressionMiddleware.from_crawler(self.crawler)\n+        self.crawler.stats.open_spider(self.spider)\n \n     def _getresponse(self, coding):\n         if coding not in FORMAT:\n@@ -133,7 +131,7 @@ class HttpCompressionTest(unittest.TestCase):\n         try:\n             import brotli  # noqa: F401\n         except ImportError:\n-            raise unittest.SkipTest(\"no brotli\")\n+            raise SkipTest(\"no brotli\")\n         response = self._getresponse(\"br\")\n         request = response.request\n         self.assertEqual(response.headers[\"Content-Encoding\"], b\"br\")\n@@ -148,7 +146,7 @@ class HttpCompressionTest(unittest.TestCase):\n         try:\n             import zstandard  # noqa: F401\n         except ImportError:\n-            raise unittest.SkipTest(\"no zstd support (zstandard)\")\n+            raise SkipTest(\"no zstd support (zstandard)\")\n         raw_content = None\n         for check_key in FORMAT:\n             if not check_key.startswith(\"zstd-\"):\n@@ -376,15 +374,13 @@ class HttpCompressionTest(unittest.TestCase):\n         self.assertStatsEqual(\"httpcompression/response_bytes\", None)\n \n \n-class HttpCompressionSubclassTest(unittest.TestCase):\n-    @inlineCallbacks\n+class HttpCompressionSubclassTest(TestCase):\n     def test_init_missing_stats(self):\n         class HttpCompressionMiddlewareSubclass(HttpCompressionMiddleware):\n             def __init__(self):\n                 super().__init__()\n \n-        crawler = get_crawler(NoRequestsSpider)\n-        yield crawler.crawl()\n+        crawler = get_crawler(Spider)\n         with catch_warnings(record=True) as caught_warnings:\n             HttpCompressionMiddlewareSubclass.from_crawler(crawler)\n         messages = tuple(\n\n@@ -1,9 +1,9 @@\n import logging\n+import unittest\n import warnings\n \n from testfixtures import LogCapture\n from twisted.internet import defer\n-from twisted.internet.defer import inlineCallbacks\n from twisted.internet.error import (\n     ConnectError,\n     ConnectionDone,\n@@ -11,7 +11,6 @@ from twisted.internet.error import (\n     DNSLookupError,\n     TCPTimedOutError,\n )\n-from twisted.trial import unittest\n from twisted.web.client import ResponseFailed\n \n from scrapy.downloadermiddlewares.retry import RetryMiddleware, get_retry_request\n@@ -20,15 +19,12 @@ from scrapy.http import Request, Response\n from scrapy.settings.default_settings import RETRY_EXCEPTIONS\n from scrapy.spiders import Spider\n from scrapy.utils.test import get_crawler\n-from tests.spiders import NoRequestsSpider\n \n \n class RetryTest(unittest.TestCase):\n-    @inlineCallbacks\n     def setUp(self):\n-        self.crawler = get_crawler(NoRequestsSpider)\n-        yield self.crawler.crawl()\n-        self.spider = self.crawler.spider\n+        self.crawler = get_crawler(Spider)\n+        self.spider = self.crawler._create_spider(\"foo\")\n         self.mw = RetryMiddleware.from_crawler(self.crawler)\n         self.mw.max_retry_times = 2\n \n@@ -164,19 +160,16 @@ class RetryTest(unittest.TestCase):\n class MaxRetryTimesTest(unittest.TestCase):\n     invalid_url = \"http://www.scrapytest.org/invalid_url\"\n \n-    @inlineCallbacks\n     def get_spider_and_middleware(self, settings=None):\n-        crawler = get_crawler(NoRequestsSpider, settings or {})\n-        yield crawler.crawl()\n-        spider = crawler.spider\n+        crawler = get_crawler(Spider, settings or {})\n+        spider = crawler._create_spider(\"foo\")\n         middleware = RetryMiddleware.from_crawler(crawler)\n         return spider, middleware\n \n-    @inlineCallbacks\n     def test_with_settings_zero(self):\n         max_retry_times = 0\n         settings = {\"RETRY_TIMES\": max_retry_times}\n-        spider, middleware = yield self.get_spider_and_middleware(settings)\n+        spider, middleware = self.get_spider_and_middleware(settings)\n         req = Request(self.invalid_url)\n         self._test_retry(\n             req,\n@@ -186,10 +179,9 @@ class MaxRetryTimesTest(unittest.TestCase):\n             middleware=middleware,\n         )\n \n-    @inlineCallbacks\n     def test_with_metakey_zero(self):\n         max_retry_times = 0\n-        spider, middleware = yield self.get_spider_and_middleware()\n+        spider, middleware = self.get_spider_and_middleware()\n         meta = {\"max_retry_times\": max_retry_times}\n         req = Request(self.invalid_url, meta=meta)\n         self._test_retry(\n@@ -200,11 +192,10 @@ class MaxRetryTimesTest(unittest.TestCase):\n             middleware=middleware,\n         )\n \n-    @inlineCallbacks\n     def test_without_metakey(self):\n         max_retry_times = 5\n         settings = {\"RETRY_TIMES\": max_retry_times}\n-        spider, middleware = yield self.get_spider_and_middleware(settings)\n+        spider, middleware = self.get_spider_and_middleware(settings)\n         req = Request(self.invalid_url)\n         self._test_retry(\n             req,\n@@ -214,7 +205,6 @@ class MaxRetryTimesTest(unittest.TestCase):\n             middleware=middleware,\n         )\n \n-    @inlineCallbacks\n     def test_with_metakey_greater(self):\n         meta_max_retry_times = 3\n         middleware_max_retry_times = 2\n@@ -223,7 +213,7 @@ class MaxRetryTimesTest(unittest.TestCase):\n         req2 = Request(self.invalid_url)\n \n         settings = {\"RETRY_TIMES\": middleware_max_retry_times}\n-        spider, middleware = yield self.get_spider_and_middleware(settings)\n+        spider, middleware = self.get_spider_and_middleware(settings)\n \n         self._test_retry(\n             req1,\n@@ -240,7 +230,6 @@ class MaxRetryTimesTest(unittest.TestCase):\n             middleware=middleware,\n         )\n \n-    @inlineCallbacks\n     def test_with_metakey_lesser(self):\n         meta_max_retry_times = 4\n         middleware_max_retry_times = 5\n@@ -249,7 +238,7 @@ class MaxRetryTimesTest(unittest.TestCase):\n         req2 = Request(self.invalid_url)\n \n         settings = {\"RETRY_TIMES\": middleware_max_retry_times}\n-        spider, middleware = yield self.get_spider_and_middleware(settings)\n+        spider, middleware = self.get_spider_and_middleware(settings)\n \n         self._test_retry(\n             req1,\n@@ -266,10 +255,9 @@ class MaxRetryTimesTest(unittest.TestCase):\n             middleware=middleware,\n         )\n \n-    @inlineCallbacks\n     def test_with_dont_retry(self):\n         max_retry_times = 4\n-        spider, middleware = yield self.get_spider_and_middleware()\n+        spider, middleware = self.get_spider_and_middleware()\n         meta = {\n             \"max_retry_times\": max_retry_times,\n             \"dont_retry\": True,\n@@ -304,16 +292,13 @@ class MaxRetryTimesTest(unittest.TestCase):\n \n \n class GetRetryRequestTest(unittest.TestCase):\n-    @inlineCallbacks\n     def get_spider(self, settings=None):\n-        crawler = get_crawler(NoRequestsSpider, settings or {})\n-        yield crawler.crawl()\n-        return crawler.spider\n+        crawler = get_crawler(Spider, settings or {})\n+        return crawler._create_spider(\"foo\")\n \n-    @inlineCallbacks\n     def test_basic_usage(self):\n         request = Request(\"https://example.com\")\n-        spider = yield self.get_spider()\n+        spider = self.get_spider()\n         with LogCapture() as log:\n             new_request = get_retry_request(\n                 request,\n@@ -337,10 +322,9 @@ class GetRetryRequestTest(unittest.TestCase):\n             )\n         )\n \n-    @inlineCallbacks\n     def test_max_retries_reached(self):\n         request = Request(\"https://example.com\")\n-        spider = yield self.get_spider()\n+        spider = self.get_spider()\n         max_retry_times = 0\n         with LogCapture() as log:\n             new_request = get_retry_request(\n@@ -361,10 +345,9 @@ class GetRetryRequestTest(unittest.TestCase):\n             )\n         )\n \n-    @inlineCallbacks\n     def test_one_retry(self):\n         request = Request(\"https://example.com\")\n-        spider = yield self.get_spider()\n+        spider = self.get_spider()\n         with LogCapture() as log:\n             new_request = get_retry_request(\n                 request,\n@@ -389,9 +372,8 @@ class GetRetryRequestTest(unittest.TestCase):\n             )\n         )\n \n-    @inlineCallbacks\n     def test_two_retries(self):\n-        spider = yield self.get_spider()\n+        spider = self.get_spider()\n         request = Request(\"https://example.com\")\n         new_request = request\n         max_retry_times = 2\n@@ -445,10 +427,9 @@ class GetRetryRequestTest(unittest.TestCase):\n         with self.assertRaises(TypeError):\n             get_retry_request(request)  # pylint: disable=missing-kwoa\n \n-    @inlineCallbacks\n     def test_max_retry_times_setting(self):\n         max_retry_times = 0\n-        spider = yield self.get_spider({\"RETRY_TIMES\": max_retry_times})\n+        spider = self.get_spider({\"RETRY_TIMES\": max_retry_times})\n         request = Request(\"https://example.com\")\n         new_request = get_retry_request(\n             request,\n@@ -456,10 +437,9 @@ class GetRetryRequestTest(unittest.TestCase):\n         )\n         self.assertEqual(new_request, None)\n \n-    @inlineCallbacks\n     def test_max_retry_times_meta(self):\n         max_retry_times = 0\n-        spider = yield self.get_spider({\"RETRY_TIMES\": max_retry_times + 1})\n+        spider = self.get_spider({\"RETRY_TIMES\": max_retry_times + 1})\n         meta = {\"max_retry_times\": max_retry_times}\n         request = Request(\"https://example.com\", meta=meta)\n         new_request = get_retry_request(\n@@ -468,10 +448,9 @@ class GetRetryRequestTest(unittest.TestCase):\n         )\n         self.assertEqual(new_request, None)\n \n-    @inlineCallbacks\n     def test_max_retry_times_argument(self):\n         max_retry_times = 0\n-        spider = yield self.get_spider({\"RETRY_TIMES\": max_retry_times + 1})\n+        spider = self.get_spider({\"RETRY_TIMES\": max_retry_times + 1})\n         meta = {\"max_retry_times\": max_retry_times + 1}\n         request = Request(\"https://example.com\", meta=meta)\n         new_request = get_retry_request(\n@@ -481,10 +460,9 @@ class GetRetryRequestTest(unittest.TestCase):\n         )\n         self.assertEqual(new_request, None)\n \n-    @inlineCallbacks\n     def test_priority_adjust_setting(self):\n         priority_adjust = 1\n-        spider = yield self.get_spider({\"RETRY_PRIORITY_ADJUST\": priority_adjust})\n+        spider = self.get_spider({\"RETRY_PRIORITY_ADJUST\": priority_adjust})\n         request = Request(\"https://example.com\")\n         new_request = get_retry_request(\n             request,\n@@ -492,10 +470,9 @@ class GetRetryRequestTest(unittest.TestCase):\n         )\n         self.assertEqual(new_request.priority, priority_adjust)\n \n-    @inlineCallbacks\n     def test_priority_adjust_argument(self):\n         priority_adjust = 1\n-        spider = yield self.get_spider({\"RETRY_PRIORITY_ADJUST\": priority_adjust + 1})\n+        spider = self.get_spider({\"RETRY_PRIORITY_ADJUST\": priority_adjust + 1})\n         request = Request(\"https://example.com\")\n         new_request = get_retry_request(\n             request,\n@@ -504,10 +481,9 @@ class GetRetryRequestTest(unittest.TestCase):\n         )\n         self.assertEqual(new_request.priority, priority_adjust)\n \n-    @inlineCallbacks\n     def test_log_extra_retry_success(self):\n         request = Request(\"https://example.com\")\n-        spider = yield self.get_spider()\n+        spider = self.get_spider()\n         with LogCapture(attributes=(\"spider\",)) as log:\n             get_retry_request(\n                 request,\n@@ -515,10 +491,9 @@ class GetRetryRequestTest(unittest.TestCase):\n             )\n         log.check_present(spider)\n \n-    @inlineCallbacks\n     def test_log_extra_retries_exceeded(self):\n         request = Request(\"https://example.com\")\n-        spider = yield self.get_spider()\n+        spider = self.get_spider()\n         with LogCapture(attributes=(\"spider\",)) as log:\n             get_retry_request(\n                 request,\n@@ -527,10 +502,9 @@ class GetRetryRequestTest(unittest.TestCase):\n             )\n         log.check_present(spider)\n \n-    @inlineCallbacks\n     def test_reason_string(self):\n         request = Request(\"https://example.com\")\n-        spider = yield self.get_spider()\n+        spider = self.get_spider()\n         expected_reason = \"because\"\n         with LogCapture() as log:\n             get_retry_request(\n@@ -550,10 +524,9 @@ class GetRetryRequestTest(unittest.TestCase):\n             )\n         )\n \n-    @inlineCallbacks\n     def test_reason_builtin_exception(self):\n         request = Request(\"https://example.com\")\n-        spider = yield self.get_spider()\n+        spider = self.get_spider()\n         expected_reason = NotImplementedError()\n         expected_reason_string = \"builtins.NotImplementedError\"\n         with LogCapture() as log:\n@@ -576,10 +549,9 @@ class GetRetryRequestTest(unittest.TestCase):\n             )\n         )\n \n-    @inlineCallbacks\n     def test_reason_builtin_exception_class(self):\n         request = Request(\"https://example.com\")\n-        spider = yield self.get_spider()\n+        spider = self.get_spider()\n         expected_reason = NotImplementedError\n         expected_reason_string = \"builtins.NotImplementedError\"\n         with LogCapture() as log:\n@@ -602,10 +574,9 @@ class GetRetryRequestTest(unittest.TestCase):\n             )\n         )\n \n-    @inlineCallbacks\n     def test_reason_custom_exception(self):\n         request = Request(\"https://example.com\")\n-        spider = yield self.get_spider()\n+        spider = self.get_spider()\n         expected_reason = IgnoreRequest()\n         expected_reason_string = \"scrapy.exceptions.IgnoreRequest\"\n         with LogCapture() as log:\n@@ -628,10 +599,9 @@ class GetRetryRequestTest(unittest.TestCase):\n             )\n         )\n \n-    @inlineCallbacks\n     def test_reason_custom_exception_class(self):\n         request = Request(\"https://example.com\")\n-        spider = yield self.get_spider()\n+        spider = self.get_spider()\n         expected_reason = IgnoreRequest\n         expected_reason_string = \"scrapy.exceptions.IgnoreRequest\"\n         with LogCapture() as log:\n@@ -654,11 +624,10 @@ class GetRetryRequestTest(unittest.TestCase):\n             )\n         )\n \n-    @inlineCallbacks\n     def test_custom_logger(self):\n         logger = logging.getLogger(\"custom-logger\")\n         request = Request(\"https://example.com\")\n-        spider = yield self.get_spider()\n+        spider = self.get_spider()\n         expected_reason = \"because\"\n         with LogCapture() as log:\n             get_retry_request(\n@@ -675,10 +644,9 @@ class GetRetryRequestTest(unittest.TestCase):\n             )\n         )\n \n-    @inlineCallbacks\n     def test_custom_stats_key(self):\n         request = Request(\"https://example.com\")\n-        spider = yield self.get_spider()\n+        spider = self.get_spider()\n         expected_reason = \"because\"\n         stats_key = \"custom_retry\"\n         get_retry_request(\n@@ -692,3 +660,7 @@ class GetRetryRequestTest(unittest.TestCase):\n             f\"{stats_key}/reason_count/{expected_reason}\",\n         ):\n             self.assertEqual(spider.crawler.stats.get_value(stat), 1)\n+\n+\n+if __name__ == \"__main__\":\n+    unittest.main()\n\n@@ -1,29 +1,27 @@\n import warnings\n from itertools import product\n-\n-from twisted.internet.defer import inlineCallbacks\n-from twisted.trial import unittest\n+from unittest import TestCase\n \n from scrapy.downloadermiddlewares.stats import DownloaderStats\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Request, Response\n+from scrapy.spiders import Spider\n from scrapy.utils.response import response_httprepr\n from scrapy.utils.test import get_crawler\n-from tests.spiders import NoRequestsSpider\n \n \n class MyException(Exception):\n     pass\n \n \n-class TestDownloaderStats(unittest.TestCase):\n-    @inlineCallbacks\n+class TestDownloaderStats(TestCase):\n     def setUp(self):\n-        self.crawler = get_crawler(NoRequestsSpider)\n-        yield self.crawler.crawl()\n-        self.spider = self.crawler.spider\n+        self.crawler = get_crawler(Spider)\n+        self.spider = self.crawler._create_spider(\"scrapytest.org\")\n         self.mw = DownloaderStats(self.crawler.stats)\n \n+        self.crawler.stats.open_spider(self.spider)\n+\n         self.req = Request(\"http://scrapytest.org\")\n         self.res = Response(\"scrapytest.org\", status=400)\n \n\n@@ -2,25 +2,22 @@ import hashlib\n import shutil\n import sys\n import tempfile\n+import unittest\n from pathlib import Path\n \n from testfixtures import LogCapture\n-from twisted.internet.defer import inlineCallbacks\n-from twisted.trial import unittest\n \n from scrapy.core.scheduler import Scheduler\n from scrapy.dupefilters import RFPDupeFilter\n from scrapy.http import Request\n from scrapy.utils.python import to_bytes\n from scrapy.utils.test import get_crawler\n-from tests.spiders import NoRequestsSpider\n+from tests.spiders import SimpleSpider\n \n \n-@inlineCallbacks\n def _get_dupefilter(*, crawler=None, settings=None, open=True):\n     if crawler is None:\n-        crawler = get_crawler(NoRequestsSpider, settings_dict=settings)\n-        yield crawler.crawl()\n+        crawler = get_crawler(settings_dict=settings)\n     scheduler = Scheduler.from_crawler(crawler)\n     dupefilter = scheduler.df\n     if open:\n@@ -47,51 +44,41 @@ class FromSettingsRFPDupeFilter(RFPDupeFilter):\n class DirectDupeFilter:\n     method = \"n/a\"\n \n-    def open(self):\n-        pass\n-\n \n class RFPDupeFilterTest(unittest.TestCase):\n-    @inlineCallbacks\n     def test_df_from_crawler_scheduler(self):\n         settings = {\n             \"DUPEFILTER_DEBUG\": True,\n             \"DUPEFILTER_CLASS\": FromCrawlerRFPDupeFilter,\n             \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n         }\n-        crawler = get_crawler(NoRequestsSpider, settings_dict=settings)\n-        yield crawler.crawl()\n+        crawler = get_crawler(settings_dict=settings)\n         scheduler = Scheduler.from_crawler(crawler)\n         self.assertTrue(scheduler.df.debug)\n         self.assertEqual(scheduler.df.method, \"from_crawler\")\n \n-    @inlineCallbacks\n     def test_df_from_settings_scheduler(self):\n         settings = {\n             \"DUPEFILTER_DEBUG\": True,\n             \"DUPEFILTER_CLASS\": FromSettingsRFPDupeFilter,\n             \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n         }\n-        crawler = get_crawler(NoRequestsSpider, settings_dict=settings)\n-        yield crawler.crawl()\n+        crawler = get_crawler(settings_dict=settings)\n         scheduler = Scheduler.from_crawler(crawler)\n         self.assertTrue(scheduler.df.debug)\n         self.assertEqual(scheduler.df.method, \"from_settings\")\n \n-    @inlineCallbacks\n     def test_df_direct_scheduler(self):\n         settings = {\n             \"DUPEFILTER_CLASS\": DirectDupeFilter,\n             \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n         }\n-        crawler = get_crawler(NoRequestsSpider, settings_dict=settings)\n-        yield crawler.crawl()\n+        crawler = get_crawler(settings_dict=settings)\n         scheduler = Scheduler.from_crawler(crawler)\n         self.assertEqual(scheduler.df.method, \"n/a\")\n \n-    @inlineCallbacks\n     def test_filter(self):\n-        dupefilter = yield _get_dupefilter()\n+        dupefilter = _get_dupefilter()\n         r1 = Request(\"http://scrapytest.org/1\")\n         r2 = Request(\"http://scrapytest.org/2\")\n         r3 = Request(\"http://scrapytest.org/2\")\n@@ -104,14 +91,13 @@ class RFPDupeFilterTest(unittest.TestCase):\n \n         dupefilter.close(\"finished\")\n \n-    @inlineCallbacks\n     def test_dupefilter_path(self):\n         r1 = Request(\"http://scrapytest.org/1\")\n         r2 = Request(\"http://scrapytest.org/2\")\n \n         path = tempfile.mkdtemp()\n         try:\n-            df = yield _get_dupefilter(settings={\"JOBDIR\": path}, open=False)\n+            df = _get_dupefilter(settings={\"JOBDIR\": path}, open=False)\n             try:\n                 df.open()\n                 assert not df.request_seen(r1)\n@@ -119,7 +105,7 @@ class RFPDupeFilterTest(unittest.TestCase):\n             finally:\n                 df.close(\"finished\")\n \n-            df2 = yield _get_dupefilter(settings={\"JOBDIR\": path}, open=False)\n+            df2 = _get_dupefilter(settings={\"JOBDIR\": path}, open=False)\n             assert df != df2\n             try:\n                 df2.open()\n@@ -131,13 +117,12 @@ class RFPDupeFilterTest(unittest.TestCase):\n         finally:\n             shutil.rmtree(path)\n \n-    @inlineCallbacks\n     def test_request_fingerprint(self):\n         \"\"\"Test if customization of request_fingerprint method will change\n         output of request_seen.\n \n         \"\"\"\n-        dupefilter = yield _get_dupefilter()\n+        dupefilter = _get_dupefilter()\n         r1 = Request(\"http://scrapytest.org/index.html\")\n         r2 = Request(\"http://scrapytest.org/INDEX.html\")\n \n@@ -153,14 +138,13 @@ class RFPDupeFilterTest(unittest.TestCase):\n                 return fp.digest()\n \n         settings = {\"REQUEST_FINGERPRINTER_CLASS\": RequestFingerprinter}\n-        case_insensitive_dupefilter = yield _get_dupefilter(settings=settings)\n+        case_insensitive_dupefilter = _get_dupefilter(settings=settings)\n \n         assert not case_insensitive_dupefilter.request_seen(r1)\n         assert case_insensitive_dupefilter.request_seen(r2)\n \n         case_insensitive_dupefilter.close(\"finished\")\n \n-    @inlineCallbacks\n     def test_seenreq_newlines(self):\n         \"\"\"Checks against adding duplicate \\r to\n         line endings on Windows platforms.\"\"\"\n@@ -168,8 +152,7 @@ class RFPDupeFilterTest(unittest.TestCase):\n         r1 = Request(\"http://scrapytest.org/1\")\n \n         path = tempfile.mkdtemp()\n-        crawler = get_crawler(NoRequestsSpider, settings_dict={\"JOBDIR\": path})\n-        yield crawler.crawl()\n+        crawler = get_crawler(settings_dict={\"JOBDIR\": path})\n         try:\n             scheduler = Scheduler.from_crawler(crawler)\n             df = scheduler.df\n@@ -178,10 +161,7 @@ class RFPDupeFilterTest(unittest.TestCase):\n             df.close(\"finished\")\n \n             with Path(path, \"requests.seen\").open(\"rb\") as seen_file:\n-                try:\n                 line = next(seen_file).decode()\n-                except StopIteration:\n-                    return\n                 assert not line.endswith(\"\\r\\r\\n\")\n                 if sys.platform == \"win32\":\n                     assert line.endswith(\"\\r\\n\")\n@@ -191,7 +171,6 @@ class RFPDupeFilterTest(unittest.TestCase):\n         finally:\n             shutil.rmtree(path)\n \n-    @inlineCallbacks\n     def test_log(self):\n         with LogCapture() as log:\n             settings = {\n@@ -199,10 +178,9 @@ class RFPDupeFilterTest(unittest.TestCase):\n                 \"DUPEFILTER_CLASS\": FromCrawlerRFPDupeFilter,\n                 \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n             }\n-            crawler = get_crawler(NoRequestsSpider, settings_dict=settings)\n-            yield crawler.crawl()\n-            spider = crawler.spider\n-            dupefilter = yield _get_dupefilter(crawler=crawler)\n+            crawler = get_crawler(SimpleSpider, settings_dict=settings)\n+            spider = SimpleSpider.from_crawler(crawler)\n+            dupefilter = _get_dupefilter(crawler=crawler)\n \n             r1 = Request(\"http://scrapytest.org/index.html\")\n             r2 = Request(\"http://scrapytest.org/index.html\")\n@@ -222,7 +200,6 @@ class RFPDupeFilterTest(unittest.TestCase):\n \n             dupefilter.close(\"finished\")\n \n-    @inlineCallbacks\n     def test_log_debug(self):\n         with LogCapture() as log:\n             settings = {\n@@ -230,10 +207,9 @@ class RFPDupeFilterTest(unittest.TestCase):\n                 \"DUPEFILTER_CLASS\": FromCrawlerRFPDupeFilter,\n                 \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n             }\n-            crawler = get_crawler(NoRequestsSpider, settings_dict=settings)\n-            yield crawler.crawl()\n-            spider = crawler.spider\n-            dupefilter = yield _get_dupefilter(crawler=crawler)\n+            crawler = get_crawler(SimpleSpider, settings_dict=settings)\n+            spider = SimpleSpider.from_crawler(crawler)\n+            dupefilter = _get_dupefilter(crawler=crawler)\n \n             r1 = Request(\"http://scrapytest.org/index.html\")\n             r2 = Request(\n@@ -263,17 +239,15 @@ class RFPDupeFilterTest(unittest.TestCase):\n \n             dupefilter.close(\"finished\")\n \n-    @inlineCallbacks\n     def test_log_debug_default_dupefilter(self):\n         with LogCapture() as log:\n             settings = {\n                 \"DUPEFILTER_DEBUG\": True,\n                 \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n             }\n-            crawler = get_crawler(NoRequestsSpider, settings_dict=settings)\n-            yield crawler.crawl()\n-            spider = crawler.spider\n-            dupefilter = yield _get_dupefilter(crawler=crawler)\n+            crawler = get_crawler(SimpleSpider, settings_dict=settings)\n+            spider = SimpleSpider.from_crawler(crawler)\n+            dupefilter = _get_dupefilter(crawler=crawler)\n \n             r1 = Request(\"http://scrapytest.org/index.html\")\n             r2 = Request(\n\n@@ -420,16 +420,12 @@ class EngineTest(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_close_downloader(self):\n-        crawler = get_crawler(TestSpider)\n-        yield crawler.crawl()\n-        e = ExecutionEngine(crawler, lambda _: None)\n+        e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)\n         yield e.close()\n \n     @defer.inlineCallbacks\n     def test_start_already_running_exception(self):\n-        crawler = get_crawler(TestSpider)\n-        yield crawler.crawl()\n-        e = ExecutionEngine(crawler, lambda _: None)\n+        e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)\n         yield e.open_spider(TestSpider(), [])\n         e.start()\n         try:\n\n@@ -1,8 +1,6 @@\n import datetime\n import typing\n-\n-from twisted.internet.defer import inlineCallbacks\n-from twisted.trial import unittest\n+import unittest\n \n from scrapy.crawler import Crawler\n from scrapy.extensions.periodic_log import PeriodicLog\n@@ -61,44 +59,33 @@ class TestExtPeriodicLog(PeriodicLog):\n         self.stats._stats = stats_dump_2\n \n \n-@inlineCallbacks\n def extension(settings=None):\n-    crawler = Crawler(\n-        MetaSpider,\n-        settings=settings,\n-    )\n-    yield crawler.crawl()\n+    crawler = Crawler(MetaSpider, settings=settings)\n+    crawler._load_settings()\n     return TestExtPeriodicLog.from_crawler(crawler)\n \n \n class TestPeriodicLog(unittest.TestCase):\n-    @inlineCallbacks\n     def test_extension_enabled(self):\n         # Expected that settings for this extension loaded succesfully\n         # And on certain conditions - extension raising NotConfigured\n \n         # \"PERIODIC_LOG_STATS\": True -> set to {\"enabled\": True}\n         # due to TypeError exception from settings.getdict\n-        assert (yield extension({\"PERIODIC_LOG_STATS\": True, \"LOGSTATS_INTERVAL\": 60}))\n+        assert extension({\"PERIODIC_LOG_STATS\": True, \"LOGSTATS_INTERVAL\": 60})\n \n         # \"PERIODIC_LOG_STATS\": \"True\" -> set to {\"enabled\": True}\n         # due to JSONDecodeError(ValueError) exception from settings.getdict\n-        assert (\n-            yield extension({\"PERIODIC_LOG_STATS\": \"True\", \"LOGSTATS_INTERVAL\": 60})\n-        )\n+        assert extension({\"PERIODIC_LOG_STATS\": \"True\", \"LOGSTATS_INTERVAL\": 60})\n \n         # The ame for PERIODIC_LOG_DELTA:\n-        assert (yield extension({\"PERIODIC_LOG_DELTA\": True, \"LOGSTATS_INTERVAL\": 60}))\n-        assert (\n-            yield extension({\"PERIODIC_LOG_DELTA\": \"True\", \"LOGSTATS_INTERVAL\": 60})\n-        )\n+        assert extension({\"PERIODIC_LOG_DELTA\": True, \"LOGSTATS_INTERVAL\": 60})\n+        assert extension({\"PERIODIC_LOG_DELTA\": \"True\", \"LOGSTATS_INTERVAL\": 60})\n \n-    @inlineCallbacks\n     def test_log_delta(self):\n-        @inlineCallbacks\n         def emulate(settings=None):\n             spider = MetaSpider()\n-            ext = yield extension(settings)\n+            ext = extension(settings)\n             ext.spider_opened(spider)\n             ext.set_a()\n             a = ext.log_delta()\n@@ -107,9 +94,8 @@ class TestPeriodicLog(unittest.TestCase):\n             ext.spider_closed(spider, reason=\"finished\")\n             return ext, a, b\n \n-        @inlineCallbacks\n         def check(settings: dict, condition: typing.Callable):\n-            ext, a, b = yield emulate(settings)\n+            ext, a, b = emulate(settings)\n             assert list(a[\"delta\"].keys()) == [\n                 k for k, v in ext.stats._stats.items() if condition(k, v)\n             ]\n@@ -118,49 +104,45 @@ class TestPeriodicLog(unittest.TestCase):\n             ]\n \n         # Including all\n-        yield check(\n-            {\"PERIODIC_LOG_DELTA\": True}, lambda k, v: isinstance(v, (int, float))\n-        )\n+        check({\"PERIODIC_LOG_DELTA\": True}, lambda k, v: isinstance(v, (int, float)))\n \n         # include:\n-        yield check(\n+        check(\n             {\"PERIODIC_LOG_DELTA\": {\"include\": [\"downloader/\"]}},\n             lambda k, v: isinstance(v, (int, float)) and \"downloader/\" in k,\n         )\n \n         # include multiple\n-        yield check(\n+        check(\n             {\"PERIODIC_LOG_DELTA\": {\"include\": [\"downloader/\", \"scheduler/\"]}},\n             lambda k, v: isinstance(v, (int, float))\n             and (\"downloader/\" in k or \"scheduler/\" in k),\n         )\n \n         # exclude\n-        yield check(\n+        check(\n             {\"PERIODIC_LOG_DELTA\": {\"exclude\": [\"downloader/\"]}},\n             lambda k, v: isinstance(v, (int, float)) and \"downloader/\" not in k,\n         )\n \n         # exclude multiple\n-        yield check(\n+        check(\n             {\"PERIODIC_LOG_DELTA\": {\"exclude\": [\"downloader/\", \"scheduler/\"]}},\n             lambda k, v: isinstance(v, (int, float))\n             and (\"downloader/\" not in k and \"scheduler/\" not in k),\n         )\n \n         # include exclude combined\n-        yield check(\n+        check(\n             {\"PERIODIC_LOG_DELTA\": {\"include\": [\"downloader/\"], \"exclude\": [\"bytes\"]}},\n             lambda k, v: isinstance(v, (int, float))\n             and (\"downloader/\" in k and \"bytes\" not in k),\n         )\n \n-    @inlineCallbacks\n     def test_log_stats(self):\n-        @inlineCallbacks\n         def emulate(settings=None):\n             spider = MetaSpider()\n-            ext = yield extension(settings)\n+            ext = extension(settings)\n             ext.spider_opened(spider)\n             ext.set_a()\n             a = ext.log_crawler_stats()\n@@ -169,9 +151,8 @@ class TestPeriodicLog(unittest.TestCase):\n             ext.spider_closed(spider, reason=\"finished\")\n             return ext, a, b\n \n-        @inlineCallbacks\n         def check(settings: dict, condition: typing.Callable):\n-            ext, a, b = yield emulate(settings)\n+            ext, a, b = emulate(settings)\n             assert list(a[\"stats\"].keys()) == [\n                 k for k, v in ext.stats._stats.items() if condition(k, v)\n             ]\n@@ -180,34 +161,35 @@ class TestPeriodicLog(unittest.TestCase):\n             ]\n \n         # Including all\n-        yield check({\"PERIODIC_LOG_STATS\": True}, lambda k, v: True)\n+        check({\"PERIODIC_LOG_STATS\": True}, lambda k, v: True)\n \n         # include:\n-        yield check(\n+        check(\n             {\"PERIODIC_LOG_STATS\": {\"include\": [\"downloader/\"]}},\n             lambda k, v: \"downloader/\" in k,\n         )\n \n         # include multiple\n-        yield check(\n+        check(\n             {\"PERIODIC_LOG_STATS\": {\"include\": [\"downloader/\", \"scheduler/\"]}},\n             lambda k, v: \"downloader/\" in k or \"scheduler/\" in k,\n         )\n \n         # exclude\n-        yield check(\n+        check(\n             {\"PERIODIC_LOG_STATS\": {\"exclude\": [\"downloader/\"]}},\n             lambda k, v: \"downloader/\" not in k,\n         )\n \n         # exclude multiple\n-        yield check(\n+        check(\n             {\"PERIODIC_LOG_STATS\": {\"exclude\": [\"downloader/\", \"scheduler/\"]}},\n             lambda k, v: \"downloader/\" not in k and \"scheduler/\" not in k,\n         )\n \n         # include exclude combined\n-        yield check(\n+        check(\n             {\"PERIODIC_LOG_STATS\": {\"include\": [\"downloader/\"], \"exclude\": [\"bytes\"]}},\n             lambda k, v: \"downloader/\" in k and \"bytes\" not in k,\n         )\n+        #\n\n@@ -9,7 +9,7 @@ from scrapy.utils.test import get_crawler\n \n class TelnetExtensionTest(unittest.TestCase):\n     def _get_console_and_portal(self, settings=None):\n-        crawler = get_crawler(settings_dict=settings, disable_telnet=False)\n+        crawler = get_crawler(settings_dict=settings)\n         console = TelnetConsole(crawler)\n \n         # This function has some side effects we don't need for this test\n\n@@ -26,7 +26,6 @@ import lxml.etree\n import pytest\n from testfixtures import LogCapture\n from twisted.internet import defer\n-from twisted.internet.defer import inlineCallbacks\n from twisted.trial import unittest\n from w3lib.url import file_uri_to_path, path_to_file_uri\n from zope.interface import implementer\n@@ -52,7 +51,7 @@ from scrapy.settings import Settings\n from scrapy.utils.python import to_unicode\n from scrapy.utils.test import get_crawler, mock_google_cloud_storage, skip_if_no_boto\n from tests.mockserver import MockFTPServer, MockServer\n-from tests.spiders import ItemSpider, NoRequestsSpider\n+from tests.spiders import ItemSpider\n \n \n def path_to_url(path):\n@@ -2748,14 +2747,13 @@ class FeedExporterSignalsTest(unittest.TestCase):\n         d.callback(None)\n         return d\n \n-    @inlineCallbacks\n     def run_signaled_feed_exporter(\n         self, feed_exporter_signal_handler, feed_slot_signal_handler\n     ):\n-        crawler = get_crawler(NoRequestsSpider, settings_dict=self.settings)\n-        yield crawler.crawl()\n+        crawler = get_crawler(settings_dict=self.settings)\n         feed_exporter = FeedExporter.from_crawler(crawler)\n-        spider = crawler.spider\n+        spider = scrapy.Spider(\"default\")\n+        spider.crawler = crawler\n         crawler.signals.connect(\n             feed_exporter_signal_handler,\n             signal=signals.feed_exporter_closed,\n@@ -2766,26 +2764,24 @@ class FeedExporterSignalsTest(unittest.TestCase):\n         feed_exporter.open_spider(spider)\n         for item in self.items:\n             feed_exporter.item_scraped(item, spider)\n-        yield defer.ensureDeferred(feed_exporter.close_spider(spider))\n+        defer.ensureDeferred(feed_exporter.close_spider(spider))\n \n-    @inlineCallbacks\n     def test_feed_exporter_signals_sent(self):\n         self.feed_exporter_closed_received = False\n         self.feed_slot_closed_received = False\n \n-        yield self.run_signaled_feed_exporter(\n+        self.run_signaled_feed_exporter(\n             self.feed_exporter_closed_signal_handler,\n             self.feed_slot_closed_signal_handler,\n         )\n         self.assertTrue(self.feed_slot_closed_received)\n         self.assertTrue(self.feed_exporter_closed_received)\n \n-    @inlineCallbacks\n     def test_feed_exporter_signals_sent_deferred(self):\n         self.feed_exporter_closed_received = False\n         self.feed_slot_closed_received = False\n \n-        yield self.run_signaled_feed_exporter(\n+        self.run_signaled_feed_exporter(\n             self.feed_exporter_closed_signal_handler_deferred,\n             self.feed_slot_closed_signal_handler_deferred,\n         )\n\n@@ -15,7 +15,6 @@ import attr\n import pytest\n from itemadapter import ItemAdapter\n from twisted.internet import defer\n-from twisted.internet.defer import inlineCallbacks\n from twisted.trial import unittest\n \n from scrapy.http import Request, Response\n@@ -36,18 +35,15 @@ from scrapy.utils.test import (\n     skip_if_no_boto,\n )\n from tests.mockserver import MockFTPServer\n-from tests.spiders import NoRequestsSpider\n \n from .test_pipeline_media import _mocked_download_func\n \n \n class FilesPipelineTestCase(unittest.TestCase):\n-    @inlineCallbacks\n     def setUp(self):\n         self.tempdir = mkdtemp()\n         settings_dict = {\"FILES_STORE\": self.tempdir}\n-        crawler = get_crawler(NoRequestsSpider, settings_dict=settings_dict)\n-        yield crawler.crawl()\n+        crawler = get_crawler(spidercls=None, settings_dict=settings_dict)\n         self.pipeline = FilesPipeline.from_crawler(crawler)\n         self.pipeline.download_func = _mocked_download_func\n         self.pipeline.open_spider(None)\n\n@@ -14,11 +14,11 @@ from scrapy.pipelines.files import FileException\n from scrapy.pipelines.images import ImagesPipeline\n from scrapy.pipelines.media import MediaPipeline\n from scrapy.settings import Settings\n+from scrapy.spiders import Spider\n from scrapy.utils.deprecate import ScrapyDeprecationWarning\n from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.signal import disconnect_all\n from scrapy.utils.test import get_crawler\n-from tests.spiders import NoRequestsSpider\n \n try:\n     from PIL import Image  # noqa: imported just to check for the import error\n@@ -40,11 +40,10 @@ class BaseMediaPipelineTestCase(unittest.TestCase):\n     pipeline_class = MediaPipeline\n     settings = None\n \n-    @inlineCallbacks\n     def setUp(self):\n-        crawler = get_crawler(NoRequestsSpider, self.settings)\n-        yield crawler.crawl()\n-        self.spider = crawler.spider\n+        spider_cls = Spider\n+        self.spider = spider_cls(\"media.com\")\n+        crawler = get_crawler(spider_cls, self.settings)\n         self.pipe = self.pipeline_class.from_crawler(crawler)\n         self.pipe.download_func = _mocked_download_func\n         self.pipe.open_spider(self.spider)\n@@ -431,14 +430,12 @@ class MockedMediaPipelineDeprecatedMethods(ImagesPipeline):\n class MediaPipelineDeprecatedMethodsTestCase(unittest.TestCase):\n     skip = skip_pillow\n \n-    @inlineCallbacks\n     def setUp(self):\n         settings_dict = {\n             \"IMAGES_STORE\": \"store-uri\",\n             \"IMAGES_THUMBS\": {\"small\": (50, 50)},\n         }\n-        crawler = get_crawler(NoRequestsSpider, settings_dict=settings_dict)\n-        yield crawler.crawl()\n+        crawler = get_crawler(spidercls=None, settings_dict=settings_dict)\n         self.pipe = MockedMediaPipelineDeprecatedMethods.from_crawler(crawler)\n         self.pipe.download_func = _mocked_download_func\n         self.pipe.open_spider(None)\n\n@@ -1,10 +1,10 @@\n import collections\n import shutil\n import tempfile\n+import unittest\n \n from twisted.internet import defer\n-from twisted.internet.defer import inlineCallbacks\n-from twisted.trial import unittest\n+from twisted.trial.unittest import TestCase\n \n from scrapy.core.downloader import Downloader\n from scrapy.core.scheduler import Scheduler\n@@ -15,7 +15,6 @@ from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.misc import load_object\n from scrapy.utils.test import get_crawler\n from tests.mockserver import MockServer\n-from tests.spiders import NoRequestsSpider\n \n MockEngine = collections.namedtuple(\"MockEngine\", [\"downloader\"])\n MockSlot = collections.namedtuple(\"MockSlot\", [\"active\"])\n@@ -310,7 +309,7 @@ class StartUrlsSpider(Spider):\n         pass\n \n \n-class TestIntegrationWithDownloaderAwareInMemory(unittest.TestCase):\n+class TestIntegrationWithDownloaderAwareInMemory(TestCase):\n     def setUp(self):\n         self.crawler = get_crawler(\n             spidercls=StartUrlsSpider,\n@@ -337,19 +336,16 @@ class TestIntegrationWithDownloaderAwareInMemory(unittest.TestCase):\n \n \n class TestIncompatibility(unittest.TestCase):\n-    @inlineCallbacks\n     def _incompatible(self):\n         settings = dict(\n             SCHEDULER_PRIORITY_QUEUE=\"scrapy.pqueues.DownloaderAwarePriorityQueue\",\n             CONCURRENT_REQUESTS_PER_IP=1,\n         )\n-        crawler = get_crawler(NoRequestsSpider, settings)\n-        yield crawler.crawl()\n-        spider = crawler.spider\n+        crawler = get_crawler(Spider, settings)\n         scheduler = Scheduler.from_crawler(crawler)\n+        spider = Spider(name=\"spider\")\n         scheduler.open(spider)\n \n-    @inlineCallbacks\n     def test_incompatibility(self):\n         with self.assertRaises(ValueError):\n-            yield self._incompatible()\n+            self._incompatible()\n\n@@ -114,7 +114,7 @@ class SpiderTest(unittest.TestCase):\n                 spider.settings.set(\"TEST1\", \"spider_instance\", priority=\"spider\")\n                 return spider\n \n-        crawler = get_crawler(TestSpider, settings_dict=project_settings)\n+        crawler = Crawler(TestSpider, project_settings)\n         self.assertEqual(crawler.settings.get(\"TEST1\"), \"spider\")\n         self.assertEqual(crawler.settings.get(\"TEST2\"), \"spider\")\n         self.assertEqual(crawler.settings.get(\"TEST3\"), \"project\")\n\n@@ -1,9 +1,9 @@\n import logging\n+from unittest import TestCase\n \n from testfixtures import LogCapture\n from twisted.internet import defer\n-from twisted.internet.defer import inlineCallbacks\n-from twisted.trial.unittest import TestCase\n+from twisted.trial.unittest import TestCase as TrialTestCase\n \n from scrapy.http import Request, Response\n from scrapy.settings import Settings\n@@ -11,7 +11,7 @@ from scrapy.spidermiddlewares.httperror import HttpError, HttpErrorMiddleware\n from scrapy.spiders import Spider\n from scrapy.utils.test import get_crawler\n from tests.mockserver import MockServer\n-from tests.spiders import MockServerSpider, NoRequestsSpider\n+from tests.spiders import MockServerSpider\n \n \n class _HttpErrorSpider(MockServerSpider):\n@@ -59,11 +59,9 @@ def _responses(request, status_codes):\n \n \n class TestHttpErrorMiddleware(TestCase):\n-    @inlineCallbacks\n     def setUp(self):\n-        crawler = get_crawler(NoRequestsSpider)\n-        yield crawler.crawl()\n-        self.spider = crawler.spider\n+        crawler = get_crawler(Spider)\n+        self.spider = Spider.from_crawler(crawler, name=\"foo\")\n         self.mw = HttpErrorMiddleware(Settings({}))\n         self.req = Request(\"http://scrapytest.org\")\n         self.res200, self.res404 = _responses(self.req, [200, 404])\n@@ -173,7 +171,7 @@ class TestHttpErrorMiddlewareHandleAll(TestCase):\n         self.assertIsNone(mw.process_spider_input(res402, self.spider))\n \n \n-class TestHttpErrorMiddlewareIntegrational(TestCase):\n+class TestHttpErrorMiddlewareIntegrational(TrialTestCase):\n     def setUp(self):\n         self.mockserver = MockServer()\n         self.mockserver.__enter__()\n\n@@ -1,21 +1,17 @@\n import warnings\n+from unittest import TestCase\n from urllib.parse import urlparse\n \n-from twisted.internet.defer import inlineCallbacks\n-from twisted.trial import unittest\n-\n from scrapy.http import Request, Response\n from scrapy.spidermiddlewares.offsite import OffsiteMiddleware, PortWarning, URLWarning\n from scrapy.spiders import Spider\n from scrapy.utils.test import get_crawler\n \n \n-class TestOffsiteMiddleware(unittest.TestCase):\n-    @inlineCallbacks\n+class TestOffsiteMiddleware(TestCase):\n     def setUp(self):\n         crawler = get_crawler(Spider)\n-        yield crawler.crawl(**self._get_spiderargs())\n-        self.spider = crawler.spider\n+        self.spider = crawler._create_spider(**self._get_spiderargs())\n         self.mw = OffsiteMiddleware.from_crawler(crawler)\n         self.mw.spider_opened(self.spider)\n \n\n@@ -1,23 +1,21 @@\n+from unittest import TestCase\n+\n from testfixtures import LogCapture\n-from twisted.internet.defer import inlineCallbacks\n-from twisted.trial import unittest\n \n from scrapy.http import Request, Response\n from scrapy.settings import Settings\n from scrapy.spidermiddlewares.urllength import UrlLengthMiddleware\n+from scrapy.spiders import Spider\n from scrapy.utils.test import get_crawler\n-from tests.spiders import NoRequestsSpider\n \n \n-class TestUrlLengthMiddleware(unittest.TestCase):\n-    @inlineCallbacks\n+class TestUrlLengthMiddleware(TestCase):\n     def setUp(self):\n         self.maxlength = 25\n         settings = Settings({\"URLLENGTH_LIMIT\": self.maxlength})\n \n-        crawler = get_crawler(NoRequestsSpider)\n-        yield crawler.crawl()\n-        self.spider = crawler.spider\n+        crawler = get_crawler(Spider)\n+        self.spider = crawler._create_spider(\"foo\")\n         self.stats = crawler.stats\n         self.mw = UrlLengthMiddleware.from_settings(settings)\n \n\n@@ -1,10 +1,9 @@\n import logging\n import sys\n+import unittest\n \n from testfixtures import LogCapture\n-from twisted.internet.defer import inlineCallbacks\n from twisted.python.failure import Failure\n-from twisted.trial import unittest\n \n from scrapy.extensions import telnet\n from scrapy.utils.log import (\n@@ -14,7 +13,6 @@ from scrapy.utils.log import (\n     failure_to_exc_info,\n )\n from scrapy.utils.test import get_crawler\n-from tests.spiders import NoRequestsSpider\n \n \n class FailureToExcInfoTest(unittest.TestCase):\n@@ -62,7 +60,6 @@ class TopLevelFormatterTest(unittest.TestCase):\n \n \n class LogCounterHandlerTest(unittest.TestCase):\n-    @inlineCallbacks\n     def setUp(self):\n         settings = {\"LOG_LEVEL\": \"WARNING\"}\n         if not telnet.TWISTED_CONCH_AVAILABLE:\n@@ -71,8 +68,7 @@ class LogCounterHandlerTest(unittest.TestCase):\n         self.logger = logging.getLogger(\"test\")\n         self.logger.setLevel(logging.NOTSET)\n         self.logger.propagate = False\n-        self.crawler = get_crawler(NoRequestsSpider, settings_dict=settings)\n-        yield self.crawler.crawl()\n+        self.crawler = get_crawler(settings_dict=settings)\n         self.handler = LogCounterHandler(self.crawler)\n         self.logger.addHandler(self.handler)\n \n\n@@ -1,12 +1,11 @@\n import json\n+import unittest\n import warnings\n from hashlib import sha1\n from typing import Dict, Mapping, Optional, Tuple, Union\n from weakref import WeakKeyDictionary\n \n import pytest\n-from twisted.internet.defer import inlineCallbacks\n-from twisted.trial import unittest\n from w3lib.url import canonicalize_url\n \n from scrapy.http import Request\n@@ -23,7 +22,6 @@ from scrapy.utils.request import (\n     request_to_curl,\n )\n from scrapy.utils.test import get_crawler\n-from tests.spiders import NoRequestsSpider\n \n \n class UtilsRequestTest(unittest.TestCase):\n@@ -451,18 +449,15 @@ class BackwardCompatibilityTestCase(unittest.TestCase):\n                     )\n                     self.assertEqual(fp, old_fp)\n \n-    @inlineCallbacks\n     def test_component_backward_compatibility(self):\n         for request_object in REQUEST_OBJECTS_TO_TEST:\n             with warnings.catch_warnings():\n                 warnings.simplefilter(\"ignore\")\n-                crawler = get_crawler(NoRequestsSpider, prevent_warnings=False)\n-                yield crawler.crawl()\n+                crawler = get_crawler(prevent_warnings=False)\n                 fp = crawler.request_fingerprinter.fingerprint(request_object)\n             old_fp = request_fingerprint_2_6(request_object)\n             self.assertEqual(fp.hex(), old_fp)\n \n-    @inlineCallbacks\n     def test_custom_component_backward_compatibility(self):\n         \"\"\"Tests that the backward-compatible request fingerprinting class featured\n         in the documentation is indeed backward compatible and does not cause a\n@@ -485,8 +480,7 @@ class BackwardCompatibilityTestCase(unittest.TestCase):\n                 settings = {\n                     \"REQUEST_FINGERPRINTER_CLASS\": RequestFingerprinter,\n                 }\n-                crawler = get_crawler(NoRequestsSpider, settings_dict=settings)\n-                yield crawler.crawl()\n+                crawler = get_crawler(settings_dict=settings)\n                 fp = crawler.request_fingerprinter.fingerprint(request_object)\n             old_fp = request_fingerprint_2_6(request_object)\n             self.assertEqual(fp.hex(), old_fp)\n@@ -494,11 +488,9 @@ class BackwardCompatibilityTestCase(unittest.TestCase):\n \n \n class RequestFingerprinterTestCase(unittest.TestCase):\n-    @inlineCallbacks\n     def test_default_implementation(self):\n         with warnings.catch_warnings(record=True) as logged_warnings:\n-            crawler = get_crawler(NoRequestsSpider, prevent_warnings=False)\n-            yield crawler.crawl()\n+            crawler = get_crawler(prevent_warnings=False)\n         request = Request(\"https://example.com\")\n         self.assertEqual(\n             crawler.request_fingerprinter.fingerprint(request),\n@@ -506,14 +498,12 @@ class RequestFingerprinterTestCase(unittest.TestCase):\n         )\n         self.assertTrue(logged_warnings)\n \n-    @inlineCallbacks\n     def test_deprecated_implementation(self):\n         settings = {\n             \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.6\",\n         }\n         with warnings.catch_warnings(record=True) as logged_warnings:\n-            crawler = get_crawler(NoRequestsSpider, settings_dict=settings)\n-            yield crawler.crawl()\n+            crawler = get_crawler(settings_dict=settings)\n         request = Request(\"https://example.com\")\n         self.assertEqual(\n             crawler.request_fingerprinter.fingerprint(request),\n@@ -521,14 +511,12 @@ class RequestFingerprinterTestCase(unittest.TestCase):\n         )\n         self.assertTrue(logged_warnings)\n \n-    @inlineCallbacks\n     def test_recommended_implementation(self):\n         settings = {\n             \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n         }\n         with warnings.catch_warnings(record=True) as logged_warnings:\n-            crawler = get_crawler(NoRequestsSpider, settings_dict=settings)\n-            yield crawler.crawl()\n+            crawler = get_crawler(settings_dict=settings)\n         request = Request(\"https://example.com\")\n         self.assertEqual(\n             crawler.request_fingerprinter.fingerprint(request),\n@@ -536,18 +524,15 @@ class RequestFingerprinterTestCase(unittest.TestCase):\n         )\n         self.assertFalse(logged_warnings)\n \n-    @inlineCallbacks\n     def test_unknown_implementation(self):\n         settings = {\n             \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.5\",\n         }\n         with self.assertRaises(ValueError):\n-            crawler = get_crawler(NoRequestsSpider, settings_dict=settings)\n-            yield crawler.crawl()\n+            get_crawler(settings_dict=settings)\n \n \n class CustomRequestFingerprinterTestCase(unittest.TestCase):\n-    @inlineCallbacks\n     def test_include_headers(self):\n         class RequestFingerprinter:\n             def fingerprint(self, request):\n@@ -556,8 +541,7 @@ class CustomRequestFingerprinterTestCase(unittest.TestCase):\n         settings = {\n             \"REQUEST_FINGERPRINTER_CLASS\": RequestFingerprinter,\n         }\n-        crawler = get_crawler(NoRequestsSpider, settings_dict=settings)\n-        yield crawler.crawl()\n+        crawler = get_crawler(settings_dict=settings)\n \n         r1 = Request(\"http://www.example.com\", headers={\"X-ID\": \"1\"})\n         fp1 = crawler.request_fingerprinter.fingerprint(r1)\n@@ -565,7 +549,6 @@ class CustomRequestFingerprinterTestCase(unittest.TestCase):\n         fp2 = crawler.request_fingerprinter.fingerprint(r2)\n         self.assertNotEqual(fp1, fp2)\n \n-    @inlineCallbacks\n     def test_dont_canonicalize(self):\n         class RequestFingerprinter:\n             cache = WeakKeyDictionary()\n@@ -580,8 +563,7 @@ class CustomRequestFingerprinterTestCase(unittest.TestCase):\n         settings = {\n             \"REQUEST_FINGERPRINTER_CLASS\": RequestFingerprinter,\n         }\n-        crawler = get_crawler(NoRequestsSpider, settings_dict=settings)\n-        yield crawler.crawl()\n+        crawler = get_crawler(settings_dict=settings)\n \n         r1 = Request(\"http://www.example.com?a=1&a=2\")\n         fp1 = crawler.request_fingerprinter.fingerprint(r1)\n@@ -589,7 +571,6 @@ class CustomRequestFingerprinterTestCase(unittest.TestCase):\n         fp2 = crawler.request_fingerprinter.fingerprint(r2)\n         self.assertNotEqual(fp1, fp2)\n \n-    @inlineCallbacks\n     def test_meta(self):\n         class RequestFingerprinter:\n             def fingerprint(self, request):\n@@ -600,8 +581,7 @@ class CustomRequestFingerprinterTestCase(unittest.TestCase):\n         settings = {\n             \"REQUEST_FINGERPRINTER_CLASS\": RequestFingerprinter,\n         }\n-        crawler = get_crawler(NoRequestsSpider, settings_dict=settings)\n-        yield crawler.crawl()\n+        crawler = get_crawler(settings_dict=settings)\n \n         r1 = Request(\"http://www.example.com\")\n         fp1 = crawler.request_fingerprinter.fingerprint(r1)\n@@ -616,7 +596,6 @@ class CustomRequestFingerprinterTestCase(unittest.TestCase):\n         self.assertNotEqual(fp2, fp4)\n         self.assertEqual(fp2, fp3)\n \n-    @inlineCallbacks\n     def test_from_crawler(self):\n         class RequestFingerprinter:\n             @classmethod\n@@ -633,14 +612,12 @@ class CustomRequestFingerprinterTestCase(unittest.TestCase):\n             \"REQUEST_FINGERPRINTER_CLASS\": RequestFingerprinter,\n             \"FINGERPRINT\": b\"fingerprint\",\n         }\n-        crawler = get_crawler(NoRequestsSpider, settings_dict=settings)\n-        yield crawler.crawl()\n+        crawler = get_crawler(settings_dict=settings)\n \n         request = Request(\"http://www.example.com\")\n         fingerprint = crawler.request_fingerprinter.fingerprint(request)\n         self.assertEqual(fingerprint, settings[\"FINGERPRINT\"])\n \n-    @inlineCallbacks\n     def test_from_settings(self):\n         class RequestFingerprinter:\n             @classmethod\n@@ -657,14 +634,12 @@ class CustomRequestFingerprinterTestCase(unittest.TestCase):\n             \"REQUEST_FINGERPRINTER_CLASS\": RequestFingerprinter,\n             \"FINGERPRINT\": b\"fingerprint\",\n         }\n-        crawler = get_crawler(NoRequestsSpider, settings_dict=settings)\n-        yield crawler.crawl()\n+        crawler = get_crawler(settings_dict=settings)\n \n         request = Request(\"http://www.example.com\")\n         fingerprint = crawler.request_fingerprinter.fingerprint(request)\n         self.assertEqual(fingerprint, settings[\"FINGERPRINT\"])\n \n-    @inlineCallbacks\n     def test_from_crawler_and_settings(self):\n         class RequestFingerprinter:\n             # This method is ignored due to the presence of from_crawler\n@@ -686,8 +661,7 @@ class CustomRequestFingerprinterTestCase(unittest.TestCase):\n             \"REQUEST_FINGERPRINTER_CLASS\": RequestFingerprinter,\n             \"FINGERPRINT\": b\"fingerprint\",\n         }\n-        crawler = get_crawler(NoRequestsSpider, settings_dict=settings)\n-        yield crawler.crawl()\n+        crawler = get_crawler(settings_dict=settings)\n \n         request = Request(\"http://www.example.com\")\n         fingerprint = crawler.request_fingerprinter.fingerprint(request)\n@@ -754,3 +728,7 @@ class RequestToCurlTest(unittest.TestCase):\n             \" --data-raw '{\\\"foo\\\": \\\"bar\\\"}' --cookie 'foo=bar'\"\n         )\n         self._test_request(request_object, expected_curl_command)\n+\n+\n+if __name__ == \"__main__\":\n+    unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
