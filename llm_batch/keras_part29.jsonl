{"custom_id": "keras#c81d6ec93f6350eabec347acc4420456dc07312b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 75 | Lines Deleted: 72 | Files Changed: 7 | Hunks: 41 | Methods Changed: 19 | Complexity Δ (Sum/Max): 3/4 | Churn Δ: 147 | Churn Cumulative: 5376 | Contributors (this commit): 31 | Commits (past 90d): 208 | Contributors (cumulative): 55 | DMM Complexity: 0.2857142857142857\n\nDIFF:\n@@ -4,19 +4,10 @@ from __future__ import print_function\n \n import theano.tensor as T\n from ..layers.core import Layer, Merge\n+from ..utils.theano_utils import ndim_tensor\n from six.moves import range\n \n \n-def ndim_tensor(ndim):\n-    if ndim == 2:\n-        return T.matrix()\n-    elif ndim == 3:\n-        return T.tensor3()\n-    elif ndim == 4:\n-        return T.tensor4()\n-    return T.matrix()\n-\n-\n class Sequential(Layer):\n     '''\n         Simple linear stack of layers.\n@@ -32,6 +23,7 @@ class Sequential(Layer):\n         self.params = []\n         self.regularizers = []\n         self.constraints = []\n+        self.updates = []\n \n         for layer in layers:\n             self.add(layer)\n@@ -43,11 +35,15 @@ class Sequential(Layer):\n         self.layers.append(layer)\n         if len(self.layers) > 1:\n             self.layers[-1].set_previous(self.layers[-2])\n+            if not hasattr(self.layers[0], 'input'):\n+                self.set_input()\n+        layer.init_updates()\n \n-        params, regularizers, constraints = layer.get_params()\n+        params, regularizers, constraints, updates = layer.get_params()\n         self.params += params\n         self.regularizers += regularizers\n         self.constraints += constraints\n+        self.updates += updates\n \n     def get_output(self, train=False):\n         return self.layers[-1].get_output(train)\n@@ -115,6 +111,7 @@ class Graph(Layer):\n         self.params = []\n         self.regularizers = []\n         self.constraints = []\n+        self.updates = []\n \n     def set_previous(self, layer):\n         if len(self.inputs) != 1 or len(self.outputs) != 1:\n@@ -185,10 +182,12 @@ class Graph(Layer):\n                                  'input': input,\n                                  'inputs': inputs,\n                                  'merge_mode': merge_mode})\n-        params, regularizers, constraints = layer.get_params()\n+        layer.init_updates()\n+        params, regularizers, constraints, updates = layer.get_params()\n         self.params += params\n         self.regularizers += regularizers\n         self.constraints += constraints\n+        self.updates += updates\n \n     def add_output(self, name, input=None, inputs=[], merge_mode='concat'):\n         if name in self.namespace:\n\n@@ -18,6 +18,9 @@ class Layer(object):\n     def __init__(self):\n         self.params = []\n \n+    def init_updates(self):\n+        self.updates = []\n+\n     def set_previous(self, layer):\n         if not self.supports_masked_input() and layer.get_output_mask() is not None:\n             raise Exception(\"Attached non-masking layer to layer with masked output\")\n@@ -71,6 +74,7 @@ class Layer(object):\n \n     def get_params(self):\n         consts = []\n+        updates = []\n \n         if hasattr(self, 'regularizers'):\n             regularizers = self.regularizers\n@@ -88,7 +92,10 @@ class Layer(object):\n         else:\n             consts += [constraints.identity() for _ in range(len(self.params))]\n \n-        return self.params, regularizers, consts\n+        if hasattr(self, 'updates') and self.updates:\n+            updates += self.updates\n+\n+        return self.params, regularizers, consts, updates\n \n     def set_name(self, name):\n         for i in range(len(self.params)):\n@@ -144,7 +151,7 @@ class Masking(MaskedLayer):\n                 \"mask_value\": self.mask_value}\n \n \n-class Merge(object):\n+class Merge(Layer):\n     def __init__(self, layers, mode='sum'):\n         ''' Merge the output of a list of layers or containers into a single tensor.\n             mode: {'sum', 'concat'}\n@@ -156,9 +163,11 @@ class Merge(object):\n         self.params = []\n         self.regularizers = []\n         self.constraints = []\n+        self.updates = []\n         for l in self.layers:\n-            params, regs, consts = l.get_params()\n+            params, regs, consts, updates = l.get_params()\n             self.regularizers += regs\n+            self.updates += updates\n             # params and constraints have the same size\n             for p, c in zip(params, consts):\n                 if p not in self.params:\n@@ -166,7 +175,7 @@ class Merge(object):\n                     self.constraints.append(c)\n \n     def get_params(self):\n-        return self.params, self.regularizers, self.constraints\n+        return self.params, self.regularizers, self.constraints, self.updates\n \n     def get_output(self, train=False):\n         if self.mode == 'sum':\n@@ -514,9 +523,11 @@ class AutoEncoder(Layer):\n         self.params = []\n         self.regularizers = []\n         self.constraints = []\n+        self.updates = []\n         for layer in [self.encoder, self.decoder]:\n-            params, regularizers, constraints = layer.get_params()\n+            params, regularizers, constraints, updates = layer.get_params()\n             self.regularizers += regularizers\n+            self.updates += updates\n             for p, c in zip(params, constraints):\n                 if p not in self.params:\n                     self.params.append(p)\n\n@@ -1,9 +1,10 @@\n from ..layers.core import Layer\n-from ..utils.theano_utils import shared_zeros\n+from ..utils.theano_utils import shared_zeros, shared_ones, ndim_tensor\n from .. import initializations\n \n import theano.tensor as T\n \n+\n class BatchNormalization(Layer):\n     '''\n         Reference:\n@@ -22,36 +23,29 @@ class BatchNormalization(Layer):\n         self.epsilon = epsilon\n         self.mode = mode\n         self.momentum = momentum\n+        self.input = ndim_tensor(len(self.input_shape))\n \n         self.gamma = self.init((self.input_shape))\n         self.beta = shared_zeros(self.input_shape)\n \n-        self.running_mean = None\n-        self.running_std = None\n-\n         self.params = [self.gamma, self.beta]\n         if weights is not None:\n             self.set_weights(weights)\n \n+    def init_updates(self):\n+        self.running_mean = shared_zeros(self.input_shape)\n+        self.running_std = shared_ones((self.input_shape))\n+        X = self.get_input(train=True)\n+        m = X.mean(axis=0)\n+        std = std = T.mean((X - m) ** 2 + self.epsilon, axis=0) ** 0.5\n+        mean_update = self.momentum * self.running_mean + (1-self.momentum) * m\n+        std_update = self.momentum * self.running_std + (1-self.momentum) * std\n+        self.updates = [(self.running_mean, mean_update), (self.running_std, std_update)]\n+\n     def get_output(self, train):\n         X = self.get_input(train)\n \n         if self.mode == 0:\n-            if train:\n-                m = X.mean(axis=0)\n-                # manual computation of std to prevent NaNs\n-                std = T.mean((X-m)**2 + self.epsilon, axis=0) ** 0.5\n-                X_normed = (X - m) / (std + self.epsilon)\n-\n-                if self.running_mean is None:\n-                    self.running_mean = m\n-                    self.running_std = std\n-                else:\n-                    self.running_mean *= self.momentum\n-                    self.running_mean += (1-self.momentum) * m\n-                    self.running_std *= self.momentum\n-                    self.running_std += (1-self.momentum) * std\n-            else:\n             X_normed = (X - self.running_mean) / (self.running_std + self.epsilon)\n \n         elif self.mode == 1:\n\n@@ -378,6 +378,7 @@ class Sequential(Model, containers.Sequential):\n         for r in self.regularizers:\n             train_loss = r(train_loss)\n         updates = self.optimizer.get_updates(self.params, self.constraints, train_loss)\n+        updates += self.updates\n \n         if type(self.X_train) == list:\n             train_ins = self.X_train + [self.y, self.weights]\n@@ -388,10 +389,10 @@ class Sequential(Model, containers.Sequential):\n             test_ins = [self.X_test, self.y, self.weights]\n             predict_ins = [self.X_test]\n \n-        self._train = theano.function(train_ins, train_loss,\n-            updates=updates, allow_input_downcast=True, mode=theano_mode)\n-        self._train_with_acc = theano.function(train_ins, [train_loss, train_accuracy],\n-            updates=updates, allow_input_downcast=True, mode=theano_mode)\n+        self._train = theano.function(train_ins, train_loss, updates=updates,\n+                                      allow_input_downcast=True, mode=theano_mode)\n+        self._train_with_acc = theano.function(train_ins, [train_loss, train_accuracy], updates=updates,\n+                                               allow_input_downcast=True, mode=theano_mode)\n         self._predict = theano.function(predict_ins, self.y_test,\n                                         allow_input_downcast=True, mode=theano_mode)\n         self._test = theano.function(test_ins, test_loss,\n@@ -578,11 +579,12 @@ class Graph(Model, containers.Graph):\n             train_loss = r(train_loss)\n         self.optimizer = optimizers.get(optimizer)\n         updates = self.optimizer.get_updates(self.params, self.constraints, train_loss)\n+        updates += self.updates\n         self.theano_mode = theano_mode\n         self.loss = loss\n \n-        self._train = theano.function(train_ins, train_loss,\n-            updates=updates, allow_input_downcast=True, mode=theano_mode)\n+        self._train = theano.function(train_ins, train_loss, updates=updates,\n+                                      allow_input_downcast=True, mode=theano_mode)\n         self._test = theano.function(test_ins, test_loss,\n                                      allow_input_downcast=True, mode=theano_mode)\n         self._predict = theano.function(inputs=ins, outputs=ys_test,\n\n@@ -49,6 +49,7 @@ def binary_crossentropy(y_true, y_pred):\n     bce = T.nnet.binary_crossentropy(y_pred, y_true).mean(axis=-1)\n     return bce\n \n+\n def poisson_loss(y_true, y_pred):\n     return T.mean(y_pred - y_true * T.log(y_pred), axis=-1)\n \n\n@@ -26,3 +26,13 @@ def shared_ones(shape, dtype=theano.config.floatX, name=None):\n \n def alloc_zeros_matrix(*dims):\n     return T.alloc(np.cast[theano.config.floatX](0.), *dims)\n+\n+\n+def ndim_tensor(ndim):\n+    if ndim == 2:\n+        return T.matrix()\n+    elif ndim == 3:\n+        return T.tensor3()\n+    elif ndim == 4:\n+        return T.tensor4()\n+    return T.matrix()\n\n@@ -3,6 +3,8 @@ import numpy as np\n from numpy.testing import assert_allclose\n from theano import tensor as T\n from keras.layers import normalization\n+from keras.models import Sequential\n+\n \n class TestBatchNormalization(unittest.TestCase):\n     def setUp(self):\n@@ -20,40 +22,23 @@ class TestBatchNormalization(unittest.TestCase):\n         self.assertRaises(Exception, normalization.BatchNormalization((10, 10), mode=3))\n \n     def test_mode_0(self):\n-        \"\"\"\n-        Test the function of mode 0. Need to be somewhat lenient with the\n-        equality assertions because of the epsilon trick used to avoid NaNs.\n-        \"\"\"\n-        norm_m0 = normalization.BatchNormalization((10,), momentum=0.5)\n+        model = Sequential()\n+        norm_m0 = normalization.BatchNormalization((10,))\n+        model.add(norm_m0)\n+        model.compile(loss='mse', optimizer='sgd')\n \n-        norm_m0.input = self.input_1\n+        # centered on 5.0, variance 10.0\n+        X = np.random.normal(loc=5.0, scale=10.0, size=(1000, 10))\n+        model.fit(X, X, nb_epoch=5, verbose=0)\n+        norm_m0.input = X\n         out = (norm_m0.get_output(train=True) - norm_m0.beta) / norm_m0.gamma\n-        self.assertAlmostEqual(out.mean().eval(), 0.0)\n-        self.assertAlmostEqual(out.std().eval(), 1.0, places=2)\n \n-        self.assertAlmostEqual(norm_m0.running_mean, 4.5)\n-        self.assertAlmostEqual(norm_m0.running_std.eval(), np.arange(10).std(), places=2)\n-\n-        norm_m0.input = self.input_2\n-        out = (norm_m0.get_output(train=True) - norm_m0.beta)/norm_m0.gamma\n-        self.assertAlmostEqual(out.mean().eval(), 0.0)\n-        self.assertAlmostEqual(out.std().eval(), 0.0, places=2)\n-\n-        #Values calculated by hand\n-        self.assertAlmostEqual(norm_m0.running_mean, 2.25)\n-        self.assertAlmostEqual(norm_m0.running_std.eval(), 0.5*np.arange(10).std(), places=2)\n-\n-        out_test = (norm_m0.get_output(train=False) - norm_m0.beta)/norm_m0.gamma\n-        self.assertAlmostEqual(out_test.mean().eval(), -2.25 / (0.5*np.arange(10).std()),places=2)\n-        self.assertAlmostEqual(out_test.std().eval(), 0.0, places=2)\n-\n-        norm_m0.input = self.input_3\n-        out = (norm_m0.get_output(train=True) - norm_m0.beta)/norm_m0.gamma\n-        self.assertAlmostEqual(out.mean().eval(), 0.0)\n-        self.assertAlmostEqual(out.std().eval(), 0.0, places=2)\n+        self.assertAlmostEqual(out.mean().eval(), 0.0, places=1)\n+        self.assertAlmostEqual(out.std().eval(), 1.0, places=1)\n \n     def test_mode_1(self):\n         norm_m1 = normalization.BatchNormalization((10,), mode=1)\n+        norm_m1.init_updates()\n \n         for inp in [self.input_1, self.input_2, self.input_3]:\n             norm_m1.input = inp\n@@ -70,6 +55,7 @@ class TestBatchNormalization(unittest.TestCase):\n         \"\"\"\n         for inp in self.input_shapes:\n             norm_m0 = normalization.BatchNormalization(inp.shape, mode=0)\n+            norm_m0.init_updates()\n             norm_m0.input = inp\n             out = (norm_m0.get_output(train=True) - norm_m0.beta) / norm_m0.gamma\n \n@@ -83,6 +69,7 @@ class TestBatchNormalization(unittest.TestCase):\n         \"\"\"\n \n         norm_m1 = normalization.BatchNormalization((10,), mode=1, weights=[np.ones(10), np.ones(10)])\n+        norm_m1.init_updates()\n \n         for inp in [self.input_1, self.input_2, self.input_3]:\n             norm_m1.input = inp\n@@ -97,8 +84,7 @@ class TestBatchNormalization(unittest.TestCase):\n         assert_allclose(norm_m1.beta.eval(), np.ones(10))\n \n         # Weights must be an iterable of gamma AND beta.\n-        self.assertRaises(Exception,normalization.BatchNormalization(10,), weights = np.ones(10))\n-\n+        self.assertRaises(Exception, normalization.BatchNormalization((10,)), weights=np.ones(10))\n \n     def test_config(self):\n         norm = normalization.BatchNormalization((10, 10), mode=1, epsilon=0.1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9e7f67b6f9cd6f3646aa3c767abbf4a932d4f930", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 136 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -37,7 +37,7 @@ class BatchNormalization(Layer):\n         self.running_std = shared_ones((self.input_shape))\n         X = self.get_input(train=True)\n         m = X.mean(axis=0)\n-        std = std = T.mean((X - m) ** 2 + self.epsilon, axis=0) ** 0.5\n+        std = T.mean((X - m) ** 2 + self.epsilon, axis=0) ** 0.5\n         mean_update = self.momentum * self.running_mean + (1-self.momentum) * m\n         std_update = self.momentum * self.running_std + (1-self.momentum) * std\n         self.updates = [(self.running_mean, mean_update), (self.running_std, std_update)]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ed4acfae40e15c9598fb07e8bf67f63b98fea6d6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 185 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -7,6 +7,7 @@ import copy\n from ..layers.advanced_activations import LeakyReLU, PReLU\n from ..layers.core import Dense, Merge, Dropout, Activation, Reshape, Flatten, RepeatVector, Layer\n from ..layers.core import ActivityRegularization, TimeDistributedDense, AutoEncoder, MaxoutDense\n+from ..layers.convolutional import Convolution1D, Convolution2D, MaxPooling1D, MaxPooling2D, ZeroPadding2D\n from ..layers.embeddings import Embedding, WordContextProduct\n from ..layers.noise import GaussianNoise, GaussianDropout\n from ..layers.normalization import BatchNormalization\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b057624707be574d78a741516be2f98c03f3e193", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 20 | Churn Cumulative: 509 | Contributors (this commit): 7 | Commits (past 90d): 18 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -16,7 +16,7 @@ class Convolution1D(Layer):\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n                  W_constraint=None, b_constraint=None):\n \n-        if border_mode not in {'valid', 'full'}:\n+        if border_mode not in {'valid', 'full', 'same'}:\n             raise Exception('Invalid border mode for Convolution1D:', border_mode)\n \n         super(Convolution1D, self).__init__()\n@@ -63,9 +63,19 @@ class Convolution1D(Layer):\n     def get_output(self, train):\n         X = self.get_input(train)\n         X = T.reshape(X, (X.shape[0], X.shape[1], X.shape[2], 1)).dimshuffle(0, 2, 1, 3)\n-        conv_out = T.nnet.conv.conv2d(X, self.W, border_mode=self.border_mode, subsample=self.subsample)\n+\n+        border_mode = self.border_mode\n+        if border_mode == 'same':\n+            border_mode = 'full'\n+\n+        conv_out = T.nnet.conv.conv2d(X, self.W, border_mode=border_mode, subsample=self.subsample)\n+        if self.border_mode == 'same':\n+            shift_x = (self.filter_length - 1) // 2\n+            conv_out = conv_out[:, :, shift_x:X.shape[2] + shift_x, :]\n+\n         output = self.activation(conv_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n-        return T.reshape(output, (output.shape[0], output.shape[1], output.shape[2])).dimshuffle(0, 2, 1)\n+        output = T.reshape(output, (output.shape[0], output.shape[1], output.shape[2])).dimshuffle(0, 2, 1)\n+        return output\n \n     def get_config(self):\n         return {\"name\": self.__class__.__name__,\n@@ -171,8 +181,8 @@ class Convolution2D(Layer):\n             border_mode = 'full'\n \n         conv_out = T.nnet.conv.conv2d(X, self.W,\n-            border_mode=border_mode, subsample=self.subsample)\n-\n+                                      border_mode=border_mode,\n+                                      subsample=self.subsample)\n         if self.border_mode == 'same':\n             shift_x = (self.nb_row - 1) // 2\n             shift_y = (self.nb_col - 1) // 2\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#425f29038ad13e1f627f75aadb16e44d71a25d1b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 8 | Methods Changed: 5 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 38 | Churn Cumulative: 716 | Contributors (this commit): 5 | Commits (past 90d): 28 | Contributors (cumulative): 6 | DMM Complexity: 0.9473684210526315\n\nDIFF:\n@@ -147,19 +147,19 @@ class Graph(Layer):\n             if ndim == 2:\n                 layer.input = T.imatrix()\n             else:\n-                raise Exception('Type \"int\" can only be used with ndim==2.')\n+                raise Exception('Type \"int\" can only be used with ndim==2 (Embedding).')\n         layer.input.name = name\n         self.inputs[name] = layer\n         self.input_config.append({'name': name, 'ndim': ndim, 'dtype': dtype})\n \n-    def add_node(self, layer, name, input=None, inputs=[], merge_mode='concat'):\n+    def add_node(self, layer, name, input=None, inputs=[], merge_mode='concat', create_output=False):\n         if hasattr(layer, 'set_name'):\n             layer.set_name(name)\n         if name in self.namespace:\n             raise Exception('Duplicate node identifier: ' + name)\n         if input:\n             if input not in self.namespace:\n-                raise Exception('Unknown identifier: ' + input)\n+                raise Exception('Unknown node/input identifier: ' + input)\n             if input in self.nodes:\n                 layer.set_previous(self.nodes[input])\n             elif input in self.inputs:\n@@ -189,12 +189,15 @@ class Graph(Layer):\n         self.constraints += constraints\n         self.updates += updates\n \n+        if create_output:\n+            self.add_output(name, input=name)\n+\n     def add_output(self, name, input=None, inputs=[], merge_mode='concat'):\n-        if name in self.namespace:\n-            raise Exception('Duplicate node identifier: ' + name)\n+        if name in self.output_order:\n+            raise Exception('Duplicate output identifier: ' + name)\n         if input:\n             if input not in self.namespace:\n-                raise Exception('Unknown identifier: ' + input)\n+                raise Exception('Unknown node/input identifier: ' + input)\n             if input in self.nodes:\n                 self.outputs[name] = self.nodes[input]\n             elif input in self.inputs:\n@@ -207,7 +210,7 @@ class Graph(Layer):\n                 to_merge.append(self.nodes[n])\n             merge = Merge(to_merge, mode=merge_mode)\n             self.outputs[name] = merge\n-        self.namespace.add(name)\n+\n         self.output_order.append(name)\n         self.output_config.append({'name': name,\n                                    'input': input,\n\n@@ -186,6 +186,27 @@ class TestGraph(unittest.TestCase):\n         pred = seq.predict(X_test)\n         seq.get_config(verbose=1)\n \n+    def test_create_output(self):\n+        print('test create_output argument')\n+        graph = Graph()\n+        graph.add_input(name='input1', ndim=2)\n+\n+        graph.add_node(Dense(32, 16), name='dense1', input='input1')\n+        graph.add_node(Dense(32, 4), name='dense2', input='input1')\n+        graph.add_node(Dense(16, 4), name='dense3', input='dense1')\n+        graph.add_node(Dense(4, 4), name='output1', inputs=['dense2', 'dense3'], merge_mode='sum', create_output=True)\n+        graph.compile('rmsprop', {'output1': 'mse'})\n+\n+        history = graph.fit({'input1': X_train, 'output1': y_train}, nb_epoch=10)\n+        out = graph.predict({'input1': X_test})\n+        assert(type(out == dict))\n+        assert(len(out) == 1)\n+        loss = graph.test_on_batch({'input1': X_test, 'output1': y_test})\n+        loss = graph.train_on_batch({'input1': X_test, 'output1': y_test})\n+        loss = graph.evaluate({'input1': X_test, 'output1': y_test})\n+        print(loss)\n+        assert(loss < 2.5)\n+\n \n if __name__ == '__main__':\n     print('Test graph model')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#eac3bf8b587936782952539bafc6dfb1c9d49fa2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 5 | Methods Changed: 8 | Complexity Δ (Sum/Max): 8/6 | Churn Δ: 37 | Churn Cumulative: 2220 | Contributors (this commit): 17 | Commits (past 90d): 85 | Contributors (cumulative): 20 | DMM Complexity: 0.23529411764705882\n\nDIFF:\n@@ -113,11 +113,27 @@ class Graph(Layer):\n         self.constraints = []\n         self.updates = []\n \n-    def set_previous(self, layer):\n-        if len(self.inputs) != 1 or len(self.outputs) != 1:\n-            raise Exception('The Graph container can only be used as a layer \\\n-                when it has exactly one input and one output.')\n+    @property\n+    def nb_input(self):\n+        return len(self.inputs)\n+\n+    @property\n+    def nb_output(self):\n+        return len(self.outputs)\n+\n+    def set_previous(self, layer, connection_map={}):\n+        if self.nb_input != layer.nb_output:\n+            raise Exception('Cannot connect layers: input count does not match output count.')\n+        if self.nb_input == 1:\n             self.inputs[self.input_order[0]].set_previous(layer)\n+        else:\n+            if not connection_map:\n+                raise Exception('Cannot attach multi-input layer: no connection_map provided.')\n+            for k, v in connection_map:\n+                if k in self.inputs and v in layer.outputs:\n+                    self.inputs[k].set_previous(layer.outputs[v])\n+                else:\n+                    raise Exception('Invalid connection map.')\n \n     def get_input(self, train=False):\n         if len(self.inputs) != 1 or len(self.outputs) != 1:\n\n@@ -21,11 +21,20 @@ class Layer(object):\n     def init_updates(self):\n         self.updates = []\n \n-    def set_previous(self, layer):\n+    def set_previous(self, layer, connection_map={}):\n+        assert self.nb_input == layer.nb_output == 1, \"Cannot connect layers: input count and output count should be 1.\"\n         if not self.supports_masked_input() and layer.get_output_mask() is not None:\n-            raise Exception(\"Attached non-masking layer to layer with masked output\")\n+            raise Exception(\"Cannot connect non-masking layer to layer with masked output\")\n         self.previous = layer\n \n+    @property\n+    def nb_input(self):\n+        return 1\n+\n+    @property\n+    def nb_output(self):\n+        return 1\n+\n     def get_output(self, train=False):\n         return self.get_input(train)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ae4219e6b13fbe666d4ad5c34ac3cfa86355d8df", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 36 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,9 +1,11 @@\n import numpy as np\n+import unittest\n from keras.models import Sequential\n from keras.layers.core import TimeDistributedDense, Masking\n \n \n-def test_cost_masking():\n+class TestLossMasking(unittest.TestCase):\n+    def test_loss_masking(self):\n         X = np.array(\n             [[[1, 1], [2, 1], [3, 1], [5, 5]],\n              [[1, 5], [5, 0], [0, 0], [0, 0]]], dtype=np.int32)\n@@ -23,3 +25,8 @@ def test_cost_masking():\n         model.compile(loss='mse', optimizer='sgd', mask_cost=True)\n         loss = model.fit(X, 4*y, nb_epoch=1, batch_size=2, verbose=1).history['loss'][0]\n         assert loss == 282.375\n+\n+\n+if __name__ == '__main__':\n+    print('Test loss masking')\n+    unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3bc4b5249a942144bc5f6b8fc2cdae5b51ed4fa1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 75 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -69,7 +69,7 @@ class Tokenizer(object):\n         wcounts = list(self.word_counts.items())\n         wcounts.sort(key = lambda x: x[1], reverse=True)\n         sorted_voc = [wc[0] for wc in wcounts]\n-        self.word_index = dict(list(zip(sorted_voc, list(range(1, len(sorted_voc)+1)))))\n+        self.word_index = dict(list(zip(sorted_voc, list(range(0, len(sorted_voc))))))\n \n         self.index_docs = {}\n         for w, c in list(self.word_docs.items()):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ab75f215b6d65704e7272e0a411d9d082c029846", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 17 | Churn Cumulative: 526 | Contributors (this commit): 8 | Commits (past 90d): 19 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,6 +3,7 @@ from __future__ import absolute_import\n \n import theano\n import theano.tensor as T\n+from theano.sandbox.cuda import dnn\n \n from .. import activations, initializations, regularizers, constraints\n from ..utils.theano_utils import shared_zeros\n@@ -178,15 +179,17 @@ class Convolution2D(Layer):\n         X = self.get_input(train)\n         border_mode = self.border_mode\n         if border_mode == 'same':\n-            border_mode = 'full'\n-\n-        conv_out = T.nnet.conv.conv2d(X, self.W,\n+            assert(self.subsample == (1, 1))\n+            pad_x = (self.nb_row - self.subsample[0]) // 2\n+            pad_y = (self.nb_col - self.subsample[1]) // 2\n+            conv_out = dnn.dnn_conv(img=X,\n+                                    kerns=self.W,\n+                                    border_mode=(pad_x, pad_y))\n+        else:\n+            conv_out = dnn.dnn_conv(img=X,\n+                                    kerns=self.W,\n                                     border_mode=border_mode,\n                                     subsample=self.subsample)\n-        if self.border_mode == 'same':\n-            shift_x = (self.nb_row - 1) // 2\n-            shift_y = (self.nb_col - 1) // 2\n-            conv_out = conv_out[:, :, shift_x:X.shape[2] + shift_x, shift_y:X.shape[3] + shift_y]\n \n         return self.activation(conv_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a6aa7940bf76aa12ed13df07c753f9b4abeaaa27", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 2611 | Contributors (this commit): 21 | Commits (past 90d): 85 | Contributors (cumulative): 21 | DMM Complexity: None\n\nDIFF:\n@@ -638,8 +638,8 @@ class Graph(Model, containers.Graph):\n             val_ins = [validation_data[name] for name in self.input_order] + [standardize_y(validation_data[name]) for name in self.output_order] + sample_weight\n \n         f = self._train\n-        out_labels = self.output_order\n-        metrics = self.output_order + ['val_' + m for m in self.output_order]\n+        out_labels = ['loss']\n+        metrics = ['loss', 'val_loss']\n         history = self._fit(f, ins, out_labels=out_labels, batch_size=batch_size, nb_epoch=nb_epoch,\n                             verbose=verbose, callbacks=callbacks,\n                             validation_split=validation_split, val_f=val_f, val_ins=val_ins,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dec67bdb4e1d776239bf0f58d03b6743c8961988", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 80 | Contributors (this commit): 4 | Commits (past 90d): 7 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -29,7 +29,9 @@ def alloc_zeros_matrix(*dims):\n \n \n def ndim_tensor(ndim):\n-    if ndim == 2:\n+    if ndim == 1:\n+        return T.vector()\n+    elif ndim == 2:\n         return T.matrix()\n     elif ndim == 3:\n         return T.tensor3()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9d76926eba325202afb4f1e9d7e62ccbc4030386", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 11 | Churn Cumulative: 2622 | Contributors (this commit): 21 | Commits (past 90d): 84 | Contributors (cumulative): 21 | DMM Complexity: 1.0\n\nDIFF:\n@@ -556,7 +556,7 @@ class Sequential(Model, containers.Sequential):\n \n \n class Graph(Model, containers.Graph):\n-    def compile(self, optimizer, loss, theano_mode=None):\n+    def compile(self, optimizer, loss, theano_mode=None, mask_cost=False):\n         # loss is a dictionary mapping output name to loss functions\n         ys = []\n         ys_train = []\n@@ -574,11 +574,16 @@ class Graph(Model, containers.Graph):\n             ys_train.append(y_train)\n             ys_test.append(y_test)\n \n+            if mask_cost is None:\n+                mask = None\n+            else:\n+                mask = output.get_output_mask()\n+\n             weight = T.ones_like(y_test)\n             weights.append(weight)\n             weighted_loss = weighted_objective(objectives.get(loss_fn))\n-            train_loss += weighted_loss(y, y_train, weight)\n-            test_loss += weighted_loss(y, y_test, weight)\n+            train_loss += weighted_loss(y, y_train, weight, mask)\n+            test_loss += weighted_loss(y, y_test, weight, mask)\n \n         train_loss.name = 'train_loss'\n         test_loss.name = 'test_loss'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fe45d2f002347a64647e203694844d8d2b8ea26e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 32 | Churn Cumulative: 558 | Contributors (this commit): 9 | Commits (past 90d): 20 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -121,6 +121,21 @@ class MaxPooling1D(Layer):\n                 \"ignore_border\": self.ignore_border,\n                 \"subsample_length\": self.subsample_length}\n \n+class UpSample1D(Layer):\n+    def __init__(self, upsample_length=2):\n+        super(UpSample1D,self).__init__()\n+        self.upsample_length = upsample_length\n+        self.input = T.tensor3()\n+\n+    def get_output(self, train):\n+        X = self.get_input(train)\n+        output = theano.tensor.extra_ops.repeat(X, self.upsample_length, axis=1)\n+        return output\n+\n+    def get_config(self):\n+        return {\"name\":self.__class__.__name__,\n+                \"upsample_length\":self.upsample_length}\n+                \n \n class Convolution2D(Layer):\n     def __init__(self, nb_filter, stack_size, nb_row, nb_col,\n@@ -226,6 +241,23 @@ class MaxPooling2D(Layer):\n                 \"ignore_border\": self.ignore_border,\n                 \"stride\": self.stride}\n \n+class UpSample2D(Layer):\n+    def __init__(self, upsample_size=(2, 2)):\n+        super(UpSample2D,self).__init__()\n+        self.input = T.tensor4()\n+        self.upsample_size = upsample_size\n+\n+\n+    def get_output(self, train):\n+        X = self.get_input(train)\n+        Y = theano.tensor.extra_ops.repeat(X, self.upsample_size[0], axis = -2)\n+        output = theano.tensor.extra_ops.repeat(Y, self.upsample_size[1], axis = -1)\n+        return output\n+\n+    def get_config(self):\n+        return {\"name\":self.__class__.__name__,\n+                \"upsample_size\":self.upsample_size}\n+\n \n class ZeroPadding2D(Layer):\n     def __init__(self, width=1):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9b4fe767ae257065ab8876176ac0640800ae163b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 2 | Churn Cumulative: 2624 | Contributors (this commit): 21 | Commits (past 90d): 114 | Contributors (cumulative): 21 | DMM Complexity: None\n\nDIFF:\n@@ -327,7 +327,7 @@ class Model(object):\n         # dump model configuration to json string\n         import json\n         config = self.get_config()\n-        return json.dump(config)\n+        return json.dumps(config)\n \n \n class Sequential(Model, containers.Sequential):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7115efc37b84682d5c723d1939b81297af2b3f1c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 16 | Files Changed: 2 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 22 | Churn Cumulative: 2682 | Contributors (this commit): 21 | Commits (past 90d): 90 | Contributors (cumulative): 23 | DMM Complexity: 0.14285714285714285\n\nDIFF:\n@@ -345,8 +345,7 @@ class Sequential(Model, containers.Sequential):\n             - set_weights\n     '''\n \n-    def compile(self, optimizer, loss, class_mode=\"categorical\", theano_mode=None,\n-                mask_cost=False):\n+    def compile(self, optimizer, loss, class_mode=\"categorical\", theano_mode=None):\n         self.optimizer = optimizers.get(optimizer)\n \n         self.loss = objectives.get(loss)\n@@ -364,7 +363,7 @@ class Sequential(Model, containers.Sequential):\n \n         self.weights = T.ones_like(self.y_train)\n \n-        if mask_cost:\n+        if hasattr(self.layers[-1], \"get_output_mask\"):\n             mask = self.layers[-1].get_output_mask()\n         else:\n             mask = None\n@@ -556,7 +555,7 @@ class Sequential(Model, containers.Sequential):\n \n \n class Graph(Model, containers.Graph):\n-    def compile(self, optimizer, loss, theano_mode=None, mask_cost=False):\n+    def compile(self, optimizer, loss, theano_mode=None):\n         # loss is a dictionary mapping output name to loss functions\n         ys = []\n         ys_train = []\n@@ -574,10 +573,10 @@ class Graph(Model, containers.Graph):\n             ys_train.append(y_train)\n             ys_test.append(y_test)\n \n-            if mask_cost is None:\n-                mask = None\n+            if hasattr(self.layers[-1], \"get_output_mask\"):\n+                mask = self.layers[-1].get_output_mask()\n             else:\n-                mask = output.get_output_mask()\n+                mask = None\n \n             weight = T.ones_like(y_test)\n             weights.append(weight)\n\n@@ -9,20 +9,11 @@ class TestLossMasking(unittest.TestCase):\n         X = np.array(\n             [[[1, 1], [2, 1], [3, 1], [5, 5]],\n              [[1, 5], [5, 0], [0, 0], [0, 0]]], dtype=np.int32)\n-\n         model = Sequential()\n         model.add(Masking(mask_value=0))\n         model.add(TimeDistributedDense(2, 1, init='one'))\n         model.compile(loss='mse', optimizer='sgd')\n         y = model.predict(X)\n-\n-        loss = model.fit(X, 4*y, nb_epoch=1, batch_size=2, verbose=1).history['loss'][0]\n-        assert loss == 213.75\n-\n-        model = Sequential()\n-        model.add(Masking(mask_value=0))\n-        model.add(TimeDistributedDense(2, 1, init='one'))\n-        model.compile(loss='mse', optimizer='sgd', mask_cost=True)\n         loss = model.fit(X, 4*y, nb_epoch=1, batch_size=2, verbose=1).history['loss'][0]\n         assert loss == 282.375\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dbe948ec97271321fa60772ba1cb22509f59d76b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 64 | Lines Deleted: 37 | Files Changed: 1 | Hunks: 14 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 101 | Churn Cumulative: 291 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,23 +1,21 @@\n from __future__ import absolute_import\n+import abc\n import copy\n import numpy as np\n \n from ..utils.np_utils import to_categorical\n \n-class KerasClassifier(object):\n-    \"\"\"\n-    Implementation of the scikit-learn classifier API for Keras.\n \n-    Parameters\n-    ----------\n-    model : object\n-        An un-compiled Keras model object is required to use the scikit-learn wrapper.\n-    optimizer : string, optional\n-        Optimization method used by the model during compilation/training.\n-    loss : string, optional\n-        Loss function used by the model during compilation/training.\n+class BaseWrapper(object):\n     \"\"\"\n-    def __init__(self, model, optimizer='adam', loss='categorical_crossentropy'):\n+    Base class for the Keras scikit-learn wrapper.\n+\n+    Warning: This class should not be used directly. Use derived classes instead.\n+    \"\"\"\n+    __metaclass__ = abc.ABCMeta\n+\n+    @abc.abstractmethod\n+    def __init__(self, model, optimizer, loss):\n         self.model = model\n         self.optimizer = optimizer\n         self.loss = loss\n@@ -60,7 +58,8 @@ class KerasClassifier(object):\n             setattr(self, parameter, value)\n         return self\n \n-    def fit(self, X, y, batch_size=128, nb_epoch=100, verbose=0, shuffle=True):\n+    def fit(self, X, y, batch_size=128, nb_epoch=100, verbose=0, shuffle=True, show_accuracy=False,\n+            validation_split=0, validation_data=None, callbacks=[]):\n         \"\"\"\n         Fit the model according to the given training data.\n \n@@ -82,12 +81,20 @@ class KerasClassifier(object):\n         verbose : int, optional\n             Verbosity level.\n         shuffle : boolean, optional\n-            Indicator to shuffle the training data.\n+            Whether to shuffle the samples at each epoch.\n+        show_accuracy : boolean, optional\n+            Whether to display class accuracy in the logs at each epoch.\n+        validation_split : float [0, 1], optional\n+            Fraction of the data to use as held-out validation data.\n+        validation_data : tuple (X, y), optional\n+            Data to be used as held-out validation data. Will override validation_split.\n+        callbacks : list, optional\n+            List of callbacks to apply during training.\n \n         Returns\n         -------\n-        self : object\n-            Returns self.\n+        history : object\n+            Returns details about the training history at each epoch.\n         \"\"\"\n         if len(y.shape) == 1:\n             self.classes_ = list(np.unique(y))\n@@ -98,36 +105,32 @@ class KerasClassifier(object):\n \n         self.compiled_model_ = copy.deepcopy(self.model)\n         self.compiled_model_.compile(optimizer=self.optimizer, loss=self.loss)\n-        self.compiled_model_.fit(X, y, batch_size=batch_size, nb_epoch=nb_epoch, verbose=verbose, shuffle=shuffle)\n+        history = self.compiled_model_.fit(X, y, batch_size=batch_size, nb_epoch=nb_epoch, verbose=verbose,\n+                                           shuffle=shuffle, show_accuracy=show_accuracy,\n+                                           validation_split=validation_split, validation_data=validation_data,\n+                                           callbacks=callbacks)\n+\n         self.config_ = self.model.get_config()\n         self.weights_ = self.model.get_weights()\n \n-        return self\n+        return history\n \n-    def score(self, X, y, batch_size=128, verbose=0):\n+\n+class KerasClassifier(BaseWrapper):\n     \"\"\"\n-        Returns the mean accuracy on the given test data and labels.\n+    Implementation of the scikit-learn classifier API for Keras.\n \n     Parameters\n     ----------\n-        X : array-like, shape = (n_samples, n_features)\n-            Test samples where n_samples in the number of samples\n-            and n_features is the number of features.\n-        y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n-            True labels for X.\n-        batch_size : int, optional\n-            Number of test samples evaluated at a time.\n-        verbose : int, optional\n-            Verbosity level.\n-\n-        Returns\n-        -------\n-        score : float\n-            Mean accuracy of self.predict(X) wrt. y.\n+    model : object\n+        An un-compiled Keras model object is required to use the scikit-learn wrapper.\n+    optimizer : string\n+        Optimization method used by the model during compilation/training.\n+    loss : string\n+        Loss function used by the model during compilation/training.\n     \"\"\"\n-        loss, accuracy = self.compiled_model_.evaluate(X, y, batch_size=batch_size,\n-                                                       show_accuracy=True, verbose=verbose)\n-        return accuracy\n+    def __init__(self, model, optimizer='adam', loss='categorical_crossentropy'):\n+        super(KerasClassifier, self).__init__(model, optimizer, loss)\n \n     def predict(self, X, batch_size=128, verbose=0):\n         \"\"\"\n@@ -170,3 +173,27 @@ class KerasClassifier(object):\n             Class probability estimates.\n         \"\"\"\n         return self.compiled_model_.predict_proba(X, batch_size=batch_size, verbose=verbose)\n+\n+    def score(self, X, y, batch_size=128, verbose=0):\n+        \"\"\"\n+        Returns the mean accuracy on the given test data and labels.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape = (n_samples, n_features)\n+            Test samples where n_samples in the number of samples\n+            and n_features is the number of features.\n+        y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n+            True labels for X.\n+        batch_size : int, optional\n+            Number of test samples evaluated at a time.\n+        verbose : int, optional\n+            Verbosity level.\n+\n+        Returns\n+        -------\n+        score : float\n+            Mean accuracy of predictions on X wrt. y.\n+        \"\"\"\n+        loss, accuracy = self.compiled_model_.evaluate(X, y, batch_size=batch_size, show_accuracy=True, verbose=verbose)\n+        return accuracy\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c9461d71483b5cfd2639abc909cc7f145f1625c4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 62 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 62 | Churn Cumulative: 353 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -197,3 +197,65 @@ class KerasClassifier(BaseWrapper):\n         \"\"\"\n         loss, accuracy = self.compiled_model_.evaluate(X, y, batch_size=batch_size, show_accuracy=True, verbose=verbose)\n         return accuracy\n+\n+\n+class KerasRegressor(BaseWrapper):\n+    \"\"\"\n+    Implementation of the scikit-learn regressor API for Keras.\n+\n+    Parameters\n+    ----------\n+    model : object\n+        An un-compiled Keras model object is required to use the scikit-learn wrapper.\n+    optimizer : string\n+        Optimization method used by the model during compilation/training.\n+    loss : string\n+        Loss function used by the model during compilation/training.\n+    \"\"\"\n+    def __init__(self, model, optimizer='adam', loss='mean_squared_error'):\n+        super(KerasRegressor, self).__init__(model, optimizer, loss)\n+\n+    def predict(self, X, batch_size=128, verbose=0):\n+        \"\"\"\n+        Returns predictions for the given test data.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape = (n_samples, n_features)\n+            Test samples where n_samples in the number of samples\n+            and n_features is the number of features.\n+        batch_size : int, optional\n+            Number of test samples evaluated at a time.\n+        verbose : int, optional\n+            Verbosity level.\n+\n+        Returns\n+        -------\n+        preds : array-like, shape = (n_samples)\n+            Predictions.\n+        \"\"\"\n+        return self.compiled_model_.predict(X, batch_size=batch_size, verbose=verbose)\n+\n+    def score(self, X, y, batch_size=128, verbose=0):\n+        \"\"\"\n+        Returns the mean accuracy on the given test data and labels.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape = (n_samples, n_features)\n+            Test samples where n_samples in the number of samples\n+            and n_features is the number of features.\n+        y : array-like, shape = (n_samples)\n+            True labels for X.\n+        batch_size : int, optional\n+            Number of test samples evaluated at a time.\n+        verbose : int, optional\n+            Verbosity level.\n+\n+        Returns\n+        -------\n+        score : float\n+            Loss from predictions on X wrt. y.\n+        \"\"\"\n+        loss = self.compiled_model_.evaluate(X, y, batch_size=batch_size, show_accuracy=False, verbose=verbose)\n+        return loss\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
