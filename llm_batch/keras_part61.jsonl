{"custom_id": "keras#652d61be4675cb1e2244988b0262b17a924aaca6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 6538 | Contributors (this commit): 32 | Commits (past 90d): 100 | Contributors (cumulative): 32 | DMM Complexity: None\n\nDIFF:\n@@ -1530,7 +1530,7 @@ class Siamese(Layer):\n     def get_output_at(self, head, train=False):\n         X = self.inputs[head].get_output(train)\n         mask = self.inputs[head].get_output_mask(train)\n-        Y = self.layer(X), mask)\n+        Y = self.layer(X, mask)\n         return Y\n \n     def get_output_shape(self, head, train=False):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#810cdc4a33aa2641dbb00b92a1b1da538c1e90b9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 115 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 116 | Churn Cumulative: 809 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,7 +6,7 @@ np.random.seed(1337)\n \n from keras import backend as K\n from keras.models import Graph, Sequential, model_from_json, model_from_yaml\n-from keras.layers.core import Dense, Activation, Merge, Lambda, LambdaMerge\n+from keras.layers.core import Dense, Activation, Merge, Lambda, LambdaMerge, Siamese, add_shared_layer\n from keras.layers import containers\n from keras.utils import np_utils\n from keras.utils.test_utils import get_test_data\n@@ -395,6 +395,120 @@ def test_sequential_count_params():\n \n     assert(n == model.count_params())\n \n+def test_siamese_1():\n+    left = Sequential()\n+    left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    left.add(Activation('relu'))\n+\n+    right = Sequential()\n+    right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    right.add(Activation('relu'))\n+\n+    model = Sequential()\n+    model.add(Siamese(Dense(nb_hidden), [left, right], merge_mode='sum'))\n+    model.add(Dense(nb_class))\n+    model.add(Activation('softmax'))\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=([X_test, X_test], y_test))\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_data=([X_test, X_test], y_test))\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_split=0.1)\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_split=0.1)\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n+\n+    loss = model.evaluate([X_train, X_train], y_train, verbose=0)\n+    assert(loss < 0.7)\n+\n+    model.predict([X_test, X_test], verbose=0)\n+    model.predict_classes([X_test, X_test], verbose=0)\n+    model.predict_proba([X_test, X_test], verbose=0)\n+    model.get_config(verbose=0)\n+\n+    # test weight saving\n+    fname = 'test_merge_sum_temp.h5'\n+    model.save_weights(fname, overwrite=True)\n+    left = Sequential()\n+    left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    left.add(Activation('relu'))\n+    right = Sequential()\n+    right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    right.add(Activation('relu'))\n+    model = Sequential()\n+    model.add(Merge([left, right], mode='sum'))\n+    model.add(Dense(nb_class))\n+    model.add(Activation('softmax'))\n+    model.load_weights(fname)\n+    os.remove(fname)\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+    nloss = model.evaluate([X_train, X_train], y_train, verbose=0)\n+    assert(loss == nloss)\n+\n+\n+@pytest.mark.skipif(K._BACKEND == 'tensorflow',\n+                    reason='currently not working with TensorFlow')\n+\n+def test_siamese_2():\n+    left = Sequential()\n+    left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    left.add(Activation('relu'))\n+\n+    right = Sequential()\n+    right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    right.add(Activation('relu'))\n+\n+    add_shared_layer(Dense(nb_hidden), [left, right])\n+\n+    left.add(Dense(nb_hidden))\n+    right.add(Dense(nb_hidden))\n+\n+    add_shared_layer(Dense(nb_hidden), [left, right])\n+\n+    model = Sequential()\n+    model.add(Merge([left, right], mode='sum'))\n+    model.add(Dense(nb_class))\n+    model.add(Activation('softmax'))\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=([X_test, X_test], y_test))\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_data=([X_test, X_test], y_test))\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_split=0.1)\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_split=0.1)\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n+    model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n+\n+    loss = model.evaluate([X_train, X_train], y_train, verbose=0)\n+    assert(loss < 0.7)\n+\n+    model.predict([X_test, X_test], verbose=0)\n+    model.predict_classes([X_test, X_test], verbose=0)\n+    model.predict_proba([X_test, X_test], verbose=0)\n+    model.get_config(verbose=0)\n+\n+    # test weight saving\n+    fname = 'test_merge_sum_temp.h5'\n+    model.save_weights(fname, overwrite=True)\n+    left = Sequential()\n+    left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    left.add(Activation('relu'))\n+    right = Sequential()\n+    right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    right.add(Activation('relu'))\n+    model = Sequential()\n+    model.add(Merge([left, right], mode='sum'))\n+    model.add(Dense(nb_class))\n+    model.add(Activation('softmax'))\n+    model.load_weights(fname)\n+    os.remove(fname)\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+    nloss = model.evaluate([X_train, X_train], y_train, verbose=0)\n+    assert(loss == nloss)\n+\n+\n+@pytest.mark.skipif(K._BACKEND == 'tensorflow',\n+                    reason='currently not working with TensorFlow')\n \n ###############\n # GRAPH TEST  #\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0855541280de2264dc758ef8e7b9b0c41b5c901b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 79 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 79 | Churn Cumulative: 888 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -620,6 +620,85 @@ def test_1o_2i():\n \n     graph.get_config(verbose=1)\n \n+def test_siamese_3():\n+    graph = Graph()\n+    graph.add_input(name='input1', input_shape=(32,))\n+    graph.add_input(name='input2', input_shape=(32,))\n+\n+    graph.add_shared_node(Dense(16), name='shared', inputs=['input1', 'input2'], merge_mode='sum')\n+    graph.add_node(Dense(4), name='dense1', input='shared')\n+    graph.add_node(Dense(4), name='dense2', input='dense1')\n+\n+    graph.add_output(name='output1', input='dense2')\n+    graph.compile('rmsprop', {'output1': 'mse'})\n+\n+    graph.fit({'input1': X_train_graph, 'input2': X2_train_graph, 'output1': y_train_graph},\n+              nb_epoch=10)\n+    out = graph.predict({'input1': X_test_graph, 'input2': X2_test_graph})\n+    assert(type(out == dict))\n+    assert(len(out) == 1)\n+\n+    loss = graph.test_on_batch({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n+    loss = graph.train_on_batch({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n+    loss = graph.evaluate({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n+    assert(loss < 3.0)\n+\n+    graph.get_config(verbose=1)\n+\n+def test_siamese_4():\n+    graph = Graph()\n+    graph.add_input(name='input1', input_shape=(32,))\n+    graph.add_input(name='input2', input_shape=(32,))\n+\n+    graph.add_shared_node(Dense(16), name='shared1', inputs['input1', 'input2'])\n+    graph.add_shared_node(Dense(4), name='shared2', inputs=['shared1'])\n+    graph.add_shared_node(Dense(4), name='shared3', inputs=['shared2'], merge_mode='sum')\n+    graph.add_node(Dense(4), name='dense', input='shared3')\n+\n+    graph.add_output(name='output1', inputs=['dense2', 'dense3'],\n+                     merge_mode='sum')\n+    graph.compile('rmsprop', {'output1': 'mse'})\n+\n+    graph.fit({'input1': X_train_graph, 'input2': X2_train_graph, 'output1': y_train_graph},\n+              nb_epoch=10)\n+    out = graph.predict({'input1': X_test_graph, 'input2': X2_test_graph})\n+    assert(type(out == dict))\n+    assert(len(out) == 1)\n+\n+    loss = graph.test_on_batch({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n+    loss = graph.train_on_batch({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n+    loss = graph.evaluate({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n+    assert(loss < 3.0)\n+\n+    graph.get_config(verbose=1)\n+\n+def test_siamese_5():\n+    graph = Graph()\n+    graph.add_input(name='input1', input_shape=(32,))\n+    graph.add_input(name='input2', input_shape=(32,))\n+\n+    graph.add_shared_node(Dense(16), name='shared1', inputs['input1', 'input2'])\n+    graph.add_shared_node(Dense(4), name='shared2', inputs=['shared1'])\n+    graph.add_shared_node(Dense(4), name='shared3', inputs=['shared2'], outputs=['shared_output1','shared_output2'])\n+    graph.add_node(Dense(4), name='dense1',  input='shared_output1')\n+    graph.add_node(Dense(4), name='dense2',  input='shared_output2')\n+\n+    graph.add_output(name='output1', inputs=['dense2', 'dense3'],\n+                     merge_mode='sum')\n+    graph.compile('rmsprop', {'output1': 'mse'})\n+\n+    graph.fit({'input1': X_train_graph, 'input2': X2_train_graph, 'output1': y_train_graph},\n+              nb_epoch=10)\n+    out = graph.predict({'input1': X_test_graph, 'input2': X2_test_graph})\n+    assert(type(out == dict))\n+    assert(len(out) == 1)\n+\n+    loss = graph.test_on_batch({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n+    loss = graph.train_on_batch({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n+    loss = graph.evaluate({'input1': X_test_graph, 'input2': X2_test_graph, 'output1': y_test_graph})\n+    assert(loss < 3.0)\n+\n+    graph.get_config(verbose=1)\n     \n def test_2o_1i_weights():\n     # test a non-sequential graph with 1 input and 2 outputs\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#07723ccff44661ea3fd457eaf81803c643471312", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 8 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 904 | Contributors (this commit): 4 | Commits (past 90d): 7 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -427,17 +427,19 @@ def test_siamese_1():\n \n     # test weight saving\n     fname = 'test_merge_sum_temp.h5'\n-    model.save_weights(fname, overwrite=True)\n     left = Sequential()\n     left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n     left.add(Activation('relu'))\n+\n     right = Sequential()\n     right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n     right.add(Activation('relu'))\n+\n     model = Sequential()\n-    model.add(Merge([left, right], mode='sum'))\n+    model.add(Siamese(Dense(nb_hidden), [left, right], merge_mode='sum'))\n     model.add(Dense(nb_class))\n     model.add(Activation('softmax'))\n+\n     model.load_weights(fname)\n     os.remove(fname)\n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n@@ -492,13 +494,23 @@ def test_siamese_2():\n     left = Sequential()\n     left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n     left.add(Activation('relu'))\n+\n     right = Sequential()\n     right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n     right.add(Activation('relu'))\n+\n+    add_shared_layer(Dense(nb_hidden), [left, right])\n+\n+    left.add(Dense(nb_hidden))\n+    right.add(Dense(nb_hidden))\n+\n+    add_shared_layer(Dense(nb_hidden), [left, right])\n+\n     model = Sequential()\n     model.add(Merge([left, right], mode='sum'))\n     model.add(Dense(nb_class))\n     model.add(Activation('softmax'))\n+\n     model.load_weights(fname)\n     os.remove(fname)\n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#92f2717c99ed771767b2943c76ba47b1ec2d7c53", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 906 | Contributors (this commit): 4 | Commits (past 90d): 8 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -667,7 +667,7 @@ def test_siamese_4():\n     graph.add_shared_node(Dense(4), name='shared3', inputs=['shared2'], merge_mode='sum')\n     graph.add_node(Dense(4), name='dense', input='shared3')\n \n-    graph.add_output(name='output1', inputs=['dense2', 'dense3'],\n+    graph.add_output(name='output1', input='dense',\n                      merge_mode='sum')\n     graph.compile('rmsprop', {'output1': 'mse'})\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#91965e8f851144472974e1ec1ed64d7341975c66", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 913 | Contributors (this commit): 4 | Commits (past 90d): 9 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -447,10 +447,6 @@ def test_siamese_1():\n     nloss = model.evaluate([X_train, X_train], y_train, verbose=0)\n     assert(loss == nloss)\n \n-\n-@pytest.mark.skipif(K._BACKEND == 'tensorflow',\n-                    reason='currently not working with TensorFlow')\n-\n def test_siamese_2():\n     left = Sequential()\n     left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n@@ -519,9 +515,6 @@ def test_siamese_2():\n     assert(loss == nloss)\n \n \n-@pytest.mark.skipif(K._BACKEND == 'tensorflow',\n-                    reason='currently not working with TensorFlow')\n-\n ###############\n # GRAPH TEST  #\n ###############\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3dcff62578799ac92e62d2deccf55b5a54f9ecb9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 14 | Files Changed: 1 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 27 | Churn Cumulative: 6565 | Contributors (this commit): 32 | Commits (past 90d): 101 | Contributors (cumulative): 32 | DMM Complexity: 0.0\n\nDIFF:\n@@ -35,7 +35,8 @@ class Layer(object):\n     def __init__(self, **kwargs):\n         allowed_kwargs = {'input_shape',\n                           'trainable',\n-                          'batch_input_shape'}\n+                          'batch_input_shape',\n+                          'cache_enabled'}\n         for kwarg in kwargs:\n             assert kwarg in allowed_kwargs, \"Keyword argument not understood: \" + kwarg\n         if 'input_shape' in kwargs:\n@@ -46,20 +47,17 @@ class Layer(object):\n             self._trainable = kwargs['trainable']\n         if not hasattr(self, 'params'):\n             self.params = []\n+        self.cache_enabled = True\n+        if 'cache_enabled' in kwargs:\n+            self.cache_enabled = kwargs['cache_enabled']\n \n-    def __call__(self, X, mask=None, train=False):\n-        # set temporary input and mask\n-        tmp_input = self.get_input\n-        tmp_mask = None\n-        if hasattr(self, 'get_input_mask'):\n-            tmp_mask = self.get_input_mask\n-            self.get_input_mask = lambda _: mask\n+    def __call__(self, X, train=False):\n+        # set temporary input\n+        tmp = self.get_input\n         self.get_input = lambda _: X\n         Y = self.get_output(train=train)\n-        # return input and mask to what it was\n-        self.get_input = tmp_input\n-        if hasattr(self, 'get_input_mask'):\n-            self.get_input_mask = tmp_mask\n+        # return input to what it was\n+        self.get_input = tmp\n         return Y\n \n     def set_previous(self, layer, connection_map={}):\n@@ -138,12 +136,12 @@ class Layer(object):\n         if hasattr(self, 'previous'):\n             # to avoid redundant computations,\n             # layer outputs are cached when possible.\n-            if hasattr(self, 'layer_cache'):\n+            if hasattr(self, 'layer_cache') and self.cache_enabled:\n                 previous_layer_id = '%s_%s' % (id(self.previous), train)\n                 if previous_layer_id in self.layer_cache:\n                     return self.layer_cache[previous_layer_id]\n             previous_output = self.previous.get_output(train=train)\n-            if hasattr(self, 'layer_cache'):\n+            if hasattr(self, 'layer_cache') and self.cache_enabled:\n                 previous_layer_id = '%s_%s' % (id(self.previous), train)\n                 self.layer_cache[previous_layer_id] = previous_output\n             return previous_output\n@@ -218,6 +216,7 @@ class Layer(object):\n             config['input_shape'] = self._input_shape[1:]\n         if hasattr(self, '_trainable'):\n             config['trainable'] = self._trainable\n+        config['cache_enabled'] =  self.cache_enabled\n         return config\n \n     def get_params(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9620a497c7335053a75a59c5515e683813047991", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 11 | Churn Cumulative: 6576 | Contributors (this commit): 32 | Commits (past 90d): 102 | Contributors (cumulative): 32 | DMM Complexity: 1.0\n\nDIFF:\n@@ -49,7 +49,15 @@ class Layer(object):\n             self.params = []\n         self.cache_enabled = True\n         if 'cache_enabled' in kwargs:\n-            self.cache_enabled = kwargs['cache_enabled']\n+            self._cache_enabled = kwargs['cache_enabled']\n+\n+    @property\n+    def cache_enabled(self):\n+        return self._cache_enabled\n+\n+    @cache_enabled.setter\n+    def cache_enabled(self, value):\n+        self._cache_enabled = value\n \n     def __call__(self, X, train=False):\n         # set temporary input\n@@ -250,7 +258,6 @@ class Layer(object):\n         '''\n         return sum([K.count_params(p) for p in self.params])\n \n-\n class MaskedLayer(Layer):\n     '''If your layer trivially supports masking\n     (by simply copying the input mask to the output),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7fb4f4b073e34745f75b67ebd8e51e44a045ec5c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 12 | Churn Cumulative: 6588 | Contributors (this commit): 32 | Commits (past 90d): 103 | Contributors (cumulative): 32 | DMM Complexity: 1.0\n\nDIFF:\n@@ -59,13 +59,19 @@ class Layer(object):\n     def cache_enabled(self, value):\n         self._cache_enabled = value\n \n-    def __call__(self, X, train=False):\n+    def __call__(self, X, mask, train=False):\n         # set temporary input\n-        tmp = self.get_input\n+        tmp_input = self.get_input\n+        tmp_mask = None\n+        if hasattr(self, 'get_input_mask'):\n+            tmp_mask = self.get_input_mask\n+            self.get_input_mask = lambda _: mask\n         self.get_input = lambda _: X\n         Y = self.get_output(train=train)\n         # return input to what it was\n-        self.get_input = tmp\n+        if hasattr(self, 'get_input_mask'):\n+            self.get_input_mask = tmp_mask\n+        self.get_input = tmp_input\n         return Y\n \n     def set_previous(self, layer, connection_map={}):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#080a8199f408eef9698330e67a2000d388e58cac", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 10 | Churn Cumulative: 1109 | Contributors (this commit): 14 | Commits (past 90d): 33 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -25,6 +25,9 @@ class Sequential(Layer):\n             self.add(layer)\n \n     def __call__(self, X, mask=None, train=False):\n+        #turn off layer cache temporarily\n+        tmp_cache_enabled = self.cache_enabled\n+        self.cache_enabled = False\n         #recursively search for a layer which is not a Sequential model\n         layer = self\n         while issubclass(layer.__class__, Sequential):\n@@ -41,8 +44,15 @@ class Sequential(Layer):\n         layer.get_input = tmp_input\n         if hasattr(layer, 'get_input_mask'):\n             layer.get_input_mask = tmp_mask\n+        self.cache_enabled = tmp_cache_enabled\n         return Y\n \n+    @cache_enabled.setter\n+    def cache_enabled(self, value):\n+        self._cache_enabled = value\n+        for l in self.layers:\n+            l.cache_enabled = value\n+\n     def set_previous(self, layer):\n         self.layers[0].previous = layer\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f5d56fa2f721ede5efdb78750711080405579dc8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 915 | Contributors (this commit): 4 | Commits (past 90d): 10 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -655,7 +655,7 @@ def test_siamese_4():\n     graph.add_input(name='input1', input_shape=(32,))\n     graph.add_input(name='input2', input_shape=(32,))\n \n-    graph.add_shared_node(Dense(16), name='shared1', inputs['input1', 'input2'])\n+    graph.add_shared_node(Dense(16), name='shared1', inputs=['input1', 'input2'])\n     graph.add_shared_node(Dense(4), name='shared2', inputs=['shared1'])\n     graph.add_shared_node(Dense(4), name='shared3', inputs=['shared2'], merge_mode='sum')\n     graph.add_node(Dense(4), name='dense', input='shared3')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5c83c699363e5fc59e3045d06e1d3fc31c9b673e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 1113 | Contributors (this commit): 14 | Commits (past 90d): 34 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -47,6 +47,10 @@ class Sequential(Layer):\n         self.cache_enabled = tmp_cache_enabled\n         return Y\n \n+    @propery\n+    def cache_enabled(self):\n+        return  self._cache_enabled\n+\n     @cache_enabled.setter\n     def cache_enabled(self, value):\n         self._cache_enabled = value\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f20383b7f7baf3b82451a90fceef18af9bcb2cdc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1115 | Contributors (this commit): 14 | Commits (past 90d): 35 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -47,7 +47,7 @@ class Sequential(Layer):\n         self.cache_enabled = tmp_cache_enabled\n         return Y\n \n-    @propery\n+    @property\n     def cache_enabled(self):\n         return  self._cache_enabled\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e0d8c0919931358d349eedc7a649c3a72f6f5c47", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 6590 | Contributors (this commit): 32 | Commits (past 90d): 104 | Contributors (cumulative): 32 | DMM Complexity: None\n\nDIFF:\n@@ -59,7 +59,7 @@ class Layer(object):\n     def cache_enabled(self, value):\n         self._cache_enabled = value\n \n-    def __call__(self, X, mask, train=False):\n+    def __call__(self, X, mask=None, train=False):\n         # set temporary input\n         tmp_input = self.get_input\n         tmp_mask = None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3e19b0252fd7db4b245e12f6a644f734df5f23bb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 917 | Contributors (this commit): 4 | Commits (past 90d): 11 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -682,7 +682,7 @@ def test_siamese_5():\n     graph.add_input(name='input1', input_shape=(32,))\n     graph.add_input(name='input2', input_shape=(32,))\n \n-    graph.add_shared_node(Dense(16), name='shared1', inputs['input1', 'input2'])\n+    graph.add_shared_node(Dense(16), name='shared1', inputs=['input1', 'input2'])\n     graph.add_shared_node(Dense(4), name='shared2', inputs=['shared1'])\n     graph.add_shared_node(Dense(4), name='shared3', inputs=['shared2'], outputs=['shared_output1','shared_output2'])\n     graph.add_node(Dense(4), name='dense1',  input='shared_output1')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#775e91c57309b7849818c61ce52a5b067a4a1fe7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 6592 | Contributors (this commit): 32 | Commits (past 90d): 105 | Contributors (cumulative): 32 | DMM Complexity: None\n\nDIFF:\n@@ -47,7 +47,7 @@ class Layer(object):\n             self._trainable = kwargs['trainable']\n         if not hasattr(self, 'params'):\n             self.params = []\n-        self.cache_enabled = True\n+        self._cache_enabled = True\n         if 'cache_enabled' in kwargs:\n             self._cache_enabled = kwargs['cache_enabled']\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c2220bff6e3a1c323c86df49cd616e8406732b93", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 6594 | Contributors (this commit): 32 | Commits (past 90d): 106 | Contributors (cumulative): 32 | DMM Complexity: 0.0\n\nDIFF:\n@@ -476,6 +476,7 @@ class Merge(Layer):\n                 if p not in self.params:\n                     self.params.append(p)\n                     self.constraints.append(c)\n+        super(Merge, self).__init__()\n \n     @property\n     def output_shape(self):\n@@ -1495,6 +1496,7 @@ class Siamese(Layer):\n                 if p not in self.params:\n                     self.params.append(p)\n                     self.constraints.append(c)\n+        super(Siamese, self).__init__()\n \n     @property\n     def output_shape(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3adbb2bd4f2dce51df35a376de8a1fef75964e78", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 923 | Contributors (this commit): 4 | Commits (past 90d): 12 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -418,7 +418,7 @@ def test_siamese_1():\n     model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n \n     loss = model.evaluate([X_train, X_train], y_train, verbose=0)\n-    assert(loss < 0.7)\n+    assert(loss < 0.8)\n \n     model.predict([X_test, X_test], verbose=0)\n     model.predict_classes([X_test, X_test], verbose=0)\n@@ -477,7 +477,7 @@ def test_siamese_2():\n     model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n \n     loss = model.evaluate([X_train, X_train], y_train, verbose=0)\n-    assert(loss < 0.7)\n+    assert(loss < 0.8)\n \n     model.predict([X_test, X_test], verbose=0)\n     model.predict_classes([X_test, X_test], verbose=0)\n@@ -688,7 +688,7 @@ def test_siamese_5():\n     graph.add_node(Dense(4), name='dense1',  input='shared_output1')\n     graph.add_node(Dense(4), name='dense2',  input='shared_output2')\n \n-    graph.add_output(name='output1', inputs=['dense2', 'dense3'],\n+    graph.add_output(name='output1', inputs=['dense1', 'dense2'],\n                      merge_mode='sum')\n     graph.compile('rmsprop', {'output1': 'mse'})\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#534d81fc100c699c94060baa581ebcb605ba1681", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 6595 | Contributors (this commit): 32 | Commits (past 90d): 107 | Contributors (cumulative): 32 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1687,6 +1687,7 @@ class SiameseHead(Layer):\n     def __init__(self, head):\n         self.head = head\n         self.params = []\n+        super(SiameseHead, self).__init__()\n \n     def get_output(self, train=False):\n         return self.get_input(train)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#794c083f6b1a48538e5b9f23752fd729785ab0a2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 1116 | Contributors (this commit): 14 | Commits (past 90d): 36 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -23,6 +23,7 @@ class Sequential(Layer):\n         self.layer_cache = {}\n         for layer in layers:\n             self.add(layer)\n+        self._cache_enabled = True\n \n     def __call__(self, X, mask=None, train=False):\n         #turn off layer cache temporarily\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#97ba6aaaa1084d5c09911be3e63b5b99cbdfe880", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 6597 | Contributors (this commit): 32 | Commits (past 90d): 108 | Contributors (cumulative): 32 | DMM Complexity: 0.5\n\nDIFF:\n@@ -1304,6 +1304,7 @@ class Lambda(Layer):\n                 self._output_shape = marshal.dumps(output_shape.__code__)\n             else:\n                 self._output_shape = marshal.dumps(output_shape.func_code)\n+        super(Lambda, self).__init__\n \n     @property\n     def output_shape(self):\n@@ -1378,6 +1379,7 @@ class LambdaMerge(Lambda):\n                 self._output_shape = marshal.dumps(output_shape.__code__)\n             else:\n                 self._output_shape = marshal.dumps(output_shape.func_code)\n+        super(Lambda, self).__init__()\n \n     @property\n     def output_shape(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b2ab55611bbb01875cdac7a58717291ebccb1d4d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 928 | Contributors (this commit): 4 | Commits (past 90d): 13 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -426,7 +426,8 @@ def test_siamese_1():\n     model.get_config(verbose=0)\n \n     # test weight saving\n-    fname = 'test_merge_sum_temp.h5'\n+    fname = 'test_siamese_1.h5'\n+    model.save_weights(fname, overwrite=True)\n     left = Sequential()\n     left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n     left.add(Activation('relu'))\n@@ -485,7 +486,7 @@ def test_siamese_2():\n     model.get_config(verbose=0)\n \n     # test weight saving\n-    fname = 'test_merge_sum_temp.h5'\n+    fname = 'test_siamese_2.h5'\n     model.save_weights(fname, overwrite=True)\n     left = Sequential()\n     left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3d51a26749937cb1a1aec40c20bc505e82809dce", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1118 | Contributors (this commit): 14 | Commits (past 90d): 37 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -449,7 +449,7 @@ class Graph(Layer):\n                 self.namespace.add(sh_name)\n                 self.nodes[sh_name] = sh\n                 self.node_config.append({'name': sh_name,\n-                                         'inputs': [s],\n+                                         'inputs': [name],\n                                          'create_output': create_output})\n                 if create_output:\n                     self.add_output(sh_name, input=sh_name)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#54d332bf639bb77be39c46dbeac511c9d6582f8b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 4 | Churn Cumulative: 257 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -80,6 +80,10 @@ def get_method_signature(method):\n     for a in args:\n         st += str(a) + ', '\n     for a, v in kwargs:\n+        if  type(v) == str:\n+            v = '\\'' + v + '\\''\n+        elif type(v) == unicode:\n+            v = 'u\\'' + v + '\\''\n         st += str(a) + '=' + str(v) + ', '\n     if kwargs or args:\n         return st[:-2] + ')'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#85ba9aa8842bdd7bf867f057aa2960e87c282571", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 174 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -83,6 +83,7 @@ def test_batchnorm_config():\n     norm = normalization.BatchNormalization(input_shape=(10, 10), mode=1,\n                                             epsilon=0.1, momentum=0.9)\n     conf = norm.get_config()\n+    del conf['cache_enabled']\n     conf_target = {\"input_shape\": (10, 10),\n                    \"name\": normalization.BatchNormalization.__name__,\n                    \"epsilon\": 0.1, \"mode\": 1, \"momentum\": 0.9}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#515448f8596e2f9f1d0ffe91957c3da1a37dda23", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 6598 | Contributors (this commit): 32 | Commits (past 90d): 109 | Contributors (cumulative): 32 | DMM Complexity: None\n\nDIFF:\n@@ -1463,6 +1463,7 @@ class Siamese(Layer):\n         merge_mode: Same meaning as `mode` argument of Merge layer\n         concat_axis: Same meaning as `concat_axis` argument of Merge layer\n         dot_axes: Same meaning as `dot_axes` argument of Merge layer\n+        is_graph: Should be set to True when used inside `Graph`\n     '''\n     def __init__(self, layer, inputs, merge_mode='concat',\n                  concat_axis=1, dot_axes=-1, is_graph=False):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3c8618bb39a18d75c97e58421d867d70006e373f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 6600 | Contributors (this commit): 32 | Commits (past 90d): 110 | Contributors (cumulative): 32 | DMM Complexity: None\n\nDIFF:\n@@ -1304,7 +1304,7 @@ class Lambda(Layer):\n                 self._output_shape = marshal.dumps(output_shape.__code__)\n             else:\n                 self._output_shape = marshal.dumps(output_shape.func_code)\n-        super(Lambda, self).__init__\n+        super(Lambda, self).__init__()\n \n     @property\n     def output_shape(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1872e00beaee2c685384e0e3e1410ecce5945f19", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 163 | Lines Deleted: 104 | Files Changed: 1 | Hunks: 4 | Methods Changed: 6 | Complexity Δ (Sum/Max): 21/21 | Churn Δ: 267 | Churn Cumulative: 6867 | Contributors (this commit): 32 | Commits (past 90d): 111 | Contributors (cumulative): 32 | DMM Complexity: None\n\nDIFF:\n@@ -1048,107 +1048,6 @@ class TimeDistributedDense(MaskedLayer):\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n-class TimeDistributedHighway(MaskedLayer):\n-    '''\n-       Apply a same Highway layer for each dimension[1] (time_dimension) input.\n-       Especially useful after a recurrent network with 'return_sequence=True'.\n-       Tensor input dimensions:   (nb_sample, time_dimension, input_dim)\n-       Tensor output dimensions:  (nb_sample, time_dimension, input_dim)\n-\n-       NOTE that this layer carries it's input dimension, so an output dimension \n-       isn't necessary.\n-\n-    '''\n-    input_ndim = 3\n-\n-    def __init__(self, \n-                 init='glorot_uniform', transform_bias=-1, activation='linear', weights=None,\n-                 W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n-                 W_constraint=None, b_constraint=None,\n-                 input_dim=None, input_length=None, **kwargs):\n-        self.init = initializations.get(init)\n-        self.transform_bias = transform_bias\n-        self.activation = activations.get(activation)\n-        \n-        self.W_regularizer = regularizers.get(W_regularizer)\n-        self.b_regularizer = regularizers.get(b_regularizer)\n-        self.activity_regularizer = regularizers.get(activity_regularizer)\n-\n-        self.W_constraint = constraints.get(W_constraint)\n-        self.b_constraint = constraints.get(b_constraint)\n-        self.constraints = [self.W_constraint, self.b_constraint]\n-\n-        self.initial_weights = weights\n-\n-        self.input_dim = input_dim\n-        self.input_length = input_length\n-        if self.input_dim:\n-            kwargs['input_shape'] = (self.input_length, self.input_dim)\n-        self.input = K.placeholder(ndim=3)\n-        super(TimeDistributedHighway, self).__init__(**kwargs)\n-\n-    def build(self):\n-        input_dim = self.input_shape[2]\n-\n-        self.W = self.init((input_dim, input_dim))\n-        self.b = K.zeros((input_dim))\n-\n-        self.W_carry = self.init((input_dim, input_dim))\n-        self.b_carry = K.variable(np.ones((input_dim)) * self.transform_bias)\n-\n-        self.params = [self.W, self.b, self.W_carry, self.b_carry]\n-        self.regularizers = []\n-\n-        if self.W_regularizer:\n-            self.W_regularizer.set_param(self.W)\n-            self.regularizers.append(self.W_regularizer)\n-\n-        if self.b_regularizer:\n-            self.b_regularizer.set_param(self.b)\n-            self.regularizers.append(self.b_regularizer)\n-\n-        if self.activity_regularizer:\n-            self.activity_regularizer.set_layer(self)\n-            self.regularizers.append(self.activity_regularizer)\n-\n-        if self.initial_weights is not None:\n-            self.set_weights(self.initial_weights)\n-            del self.initial_weights\n-\n-    @property\n-    def output_shape(self):\n-        input_shape = self.input_shape\n-        return (input_shape[0], input_shape[1], input_shape[2])\n-\n-    def get_output(self, train=False):\n-        X = self.get_input(train)\n-\n-        def step(x, states):\n-            output = self.activation(K.dot(x, self.W) + self.b)\n-            transform_weight = activations.sigmoid(K.dot(x, self.W_carry) + self.b_carry)\n-            output *= transform_weight\n-            output = output + (1 - transform_weight) * x\n-            return output, []\n-\n-        last_output, outputs, states = K.rnn(step, X, [], masking=False)\n-        return outputs\n-\n-    def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"init\": self.init.__name__,\n-                  \"transform_bias\": self.transform_bias,\n-                  \"activation\": self.activation.__name__,\n-                  \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n-                  \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n-                  \"activity_regularizer\": self.activity_regularizer.get_config() if self.activity_regularizer else None,\n-                  \"W_constraint\": self.W_constraint.get_config() if self.W_constraint else None,\n-                  \"b_constraint\": self.b_constraint.get_config() if self.b_constraint else None,\n-                  \"input_dim\": self.input_dim,\n-                  \"input_length\": self.input_length}\n-        base_config = super(TimeDistributedHighway, self).get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n-\n-\n class AutoEncoder(Layer):\n     '''A customizable autoencoder model.\n \n@@ -1800,10 +1699,41 @@ class Highway(Layer):\n     '''Densely connected highway network, \n     a natural extension of LSTMs to feedforward networks\n \n-    This layer has identical output shape to input shape, and\n-    thus doesn't need an output_dim specified\n-\n     cite: http://arxiv.org/pdf/1505.00387v2.pdf\n+\n+    # Input shape\n+        2D tensor with shape: `(nb_samples, input_dim)`.\n+\n+    # Output shape\n+        2D tensor with shape: `(nb_samples, input_dim)`.\n+\n+    # Arguments\n+        init: name of initialization function for the weights of the layer\n+            (see [initializations](../initializations.md)),\n+            or alternatively, Theano function to use for weights\n+            initialization. This parameter is only relevant\n+            if you don't pass a `weights` argument.\n+        transform_bias: value for the bias to take on initially (default -2)\n+        activation: name of activation function to use\n+            (see [activations](../activations.md)),\n+            or alternatively, elementwise Theano function.\n+            If you don't specify anything, no activation is applied\n+            (ie. \"linear\" activation: a(x) = x).\n+        weights: list of numpy arrays to set as initial weights.\n+            The list should have 1 element, of shape `(input_dim, output_dim)`.\n+        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n+            (eg. L1 or L2 regularization), applied to the main weights matrix.\n+        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n+            applied to the bias.\n+        activity_regularizer: instance of [ActivityRegularizer](../regularizers.md),\n+            applied to the network output.\n+        W_constraint: instance of the [constraints](../constraints.md) module\n+            (eg. maxnorm, nonneg), applied to the main weights matrix.\n+        b_constraint: instance of the [constraints](../constraints.md) module,\n+            applied to the bias.\n+        input_dim: dimensionality of the input (integer).\n+            This argument (or alternatively, the keyword argument `input_shape`)\n+            is required when using this layer as the first layer in a model.\n     '''\n     input_ndim = 2\n \n@@ -1884,3 +1814,132 @@ class Highway(Layer):\n                   \"input_dim\": self.input_dim}\n         base_config = super(Highway, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+class TimeDistributedHighway(MaskedLayer):\n+    '''Apply a same Highway layer for each dimension[1] (time_dimension) input.\n+    Especially useful after a recurrent network with 'return_sequence=True'.\n+\n+    # Input shape\n+        3D tensor with shape `(nb_sample, time_dimension, input_dim)`.\n+\n+    # Output shape\n+        3D tensor with shape `(nb_sample, time_dimension, input_dim)`.\n+\n+    # Arguments\n+        init: name of initialization function for the weights of the layer\n+            (see [initializations](../initializations.md)),\n+            or alternatively, Theano function to use for weights\n+            initialization. This parameter is only relevant\n+            if you don't pass a `weights` argument.\n+        transform_bias: value for the bias to take on initially (default -2)\n+        activation: name of activation function to use\n+            (see [activations](../activations.md)),\n+            or alternatively, elementwise Theano function.\n+            If you don't specify anything, no activation is applied\n+            (ie. \"linear\" activation: a(x) = x).\n+        weights: list of numpy arrays to set as initial weights.\n+            The list should have 1 element, of shape `(input_dim, output_dim)`.\n+        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n+            (eg. L1 or L2 regularization), applied to the main weights matrix.\n+        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n+            applied to the bias.\n+        activity_regularizer: instance of [ActivityRegularizer](../regularizers.md),\n+            applied to the network output.\n+        W_constraint: instance of the [constraints](../constraints.md) module\n+            (eg. maxnorm, nonneg), applied to the main weights matrix.\n+        b_constraint: instance of the [constraints](../constraints.md) module,\n+            applied to the bias.\n+        input_dim: dimensionality of the input (integer).\n+            This argument (or alternatively, the keyword argument `input_shape`)\n+            is required when using this layer as the first layer in a model.\n+    '''\n+    input_ndim = 3\n+\n+    def __init__(self, \n+                 init='glorot_uniform', transform_bias=-1, activation='linear', weights=None,\n+                 W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n+                 W_constraint=None, b_constraint=None,\n+                 input_dim=None, input_length=None, **kwargs):\n+        self.init = initializations.get(init)\n+        self.transform_bias = transform_bias\n+        self.activation = activations.get(activation)\n+        \n+        self.W_regularizer = regularizers.get(W_regularizer)\n+        self.b_regularizer = regularizers.get(b_regularizer)\n+        self.activity_regularizer = regularizers.get(activity_regularizer)\n+\n+        self.W_constraint = constraints.get(W_constraint)\n+        self.b_constraint = constraints.get(b_constraint)\n+        self.constraints = [self.W_constraint, self.b_constraint]\n+\n+        self.initial_weights = weights\n+\n+        self.input_dim = input_dim\n+        self.input_length = input_length\n+        if self.input_dim:\n+            kwargs['input_shape'] = (self.input_length, self.input_dim)\n+        self.input = K.placeholder(ndim=3)\n+        super(TimeDistributedHighway, self).__init__(**kwargs)\n+\n+    def build(self):\n+        input_dim = self.input_shape[2]\n+\n+        self.W = self.init((input_dim, input_dim))\n+        self.b = K.zeros((input_dim))\n+\n+        self.W_carry = self.init((input_dim, input_dim))\n+        self.b_carry = K.variable(np.ones((input_dim)) * self.transform_bias)\n+\n+        self.params = [self.W, self.b, self.W_carry, self.b_carry]\n+        self.regularizers = []\n+\n+        if self.W_regularizer:\n+            self.W_regularizer.set_param(self.W)\n+            self.regularizers.append(self.W_regularizer)\n+\n+        if self.b_regularizer:\n+            self.b_regularizer.set_param(self.b)\n+            self.regularizers.append(self.b_regularizer)\n+\n+        if self.activity_regularizer:\n+            self.activity_regularizer.set_layer(self)\n+            self.regularizers.append(self.activity_regularizer)\n+\n+        if self.initial_weights is not None:\n+            self.set_weights(self.initial_weights)\n+            del self.initial_weights\n+\n+    @property\n+    def output_shape(self):\n+        input_shape = self.input_shape\n+        return (input_shape[0], input_shape[1], input_shape[2])\n+\n+    def get_output(self, train=False):\n+        X = self.get_input(train)\n+\n+        def step(x, states):\n+            output = self.activation(K.dot(x, self.W) + self.b)\n+            transform_weight = activations.sigmoid(K.dot(x, self.W_carry) + self.b_carry)\n+            output *= transform_weight\n+            output = output + (1 - transform_weight) * x\n+            return output, []\n+\n+        last_output, outputs, states = K.rnn(step, X, [], masking=False)\n+        return outputs\n+\n+    def get_config(self):\n+        config = {\"name\": self.__class__.__name__,\n+                  \"init\": self.init.__name__,\n+                  \"transform_bias\": self.transform_bias,\n+                  \"activation\": self.activation.__name__,\n+                  \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n+                  \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n+                  \"activity_regularizer\": self.activity_regularizer.get_config() if self.activity_regularizer else None,\n+                  \"W_constraint\": self.W_constraint.get_config() if self.W_constraint else None,\n+                  \"b_constraint\": self.b_constraint.get_config() if self.b_constraint else None,\n+                  \"input_dim\": self.input_dim,\n+                  \"input_length\": self.input_length}\n+        base_config = super(TimeDistributedHighway, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
