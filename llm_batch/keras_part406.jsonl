{"custom_id": "keras#65c9bbff1171c5bd23da6c3f5b4d2f4b7cf27334", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 38 | Files Changed: 2 | Hunks: 31 | Methods Changed: 13 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 70 | Churn Cumulative: 11706 | Contributors (this commit): 29 | Commits (past 90d): 8 | Contributors (cumulative): 36 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1079,7 +1079,7 @@ class _ConfusionMatrixConditionCount(Metric):\n     self.accumulator = self.add_weight(\n         'accumulator',\n         shape=(len(self.thresholds),),\n-        initializer=tf.compat.v1.zeros_initializer)\n+        initializer='zeros')\n \n   def update_state(self, y_true, y_pred, sample_weight=None):\n     \"\"\"Accumulates the metric statistics.\n@@ -1404,11 +1404,11 @@ class Precision(Metric):\n     self.true_positives = self.add_weight(\n         'true_positives',\n         shape=(len(self.thresholds),),\n-        initializer=tf.compat.v1.zeros_initializer)\n+        initializer='zeros')\n     self.false_positives = self.add_weight(\n         'false_positives',\n         shape=(len(self.thresholds),),\n-        initializer=tf.compat.v1.zeros_initializer)\n+        initializer='zeros')\n \n   def update_state(self, y_true, y_pred, sample_weight=None):\n     \"\"\"Accumulates true positive and false positive statistics.\n@@ -1534,11 +1534,11 @@ class Recall(Metric):\n     self.true_positives = self.add_weight(\n         'true_positives',\n         shape=(len(self.thresholds),),\n-        initializer=tf.compat.v1.zeros_initializer)\n+        initializer='zeros')\n     self.false_negatives = self.add_weight(\n         'false_negatives',\n         shape=(len(self.thresholds),),\n-        initializer=tf.compat.v1.zeros_initializer)\n+        initializer='zeros')\n \n   def update_state(self, y_true, y_pred, sample_weight=None):\n     \"\"\"Accumulates true positive and false negative statistics.\n@@ -1612,19 +1612,19 @@ class SensitivitySpecificityBase(Metric, metaclass=abc.ABCMeta):\n     self.true_positives = self.add_weight(\n         'true_positives',\n         shape=(num_thresholds,),\n-        initializer=tf.compat.v1.zeros_initializer)\n+        initializer='zeros')\n     self.true_negatives = self.add_weight(\n         'true_negatives',\n         shape=(num_thresholds,),\n-        initializer=tf.compat.v1.zeros_initializer)\n+        initializer='zeros')\n     self.false_positives = self.add_weight(\n         'false_positives',\n         shape=(num_thresholds,),\n-        initializer=tf.compat.v1.zeros_initializer)\n+        initializer='zeros')\n     self.false_negatives = self.add_weight(\n         'false_negatives',\n         shape=(num_thresholds,),\n-        initializer=tf.compat.v1.zeros_initializer)\n+        initializer='zeros')\n \n     # Compute `num_thresholds` thresholds in [0, 1]\n     if num_thresholds == 1:\n@@ -2249,12 +2249,9 @@ class AUC(Metric):\n     self.multi_label = multi_label\n     if label_weights is not None:\n       label_weights = tf.constant(label_weights, dtype=self.dtype)\n-      checks = [\n-          tf.compat.v1.assert_non_negative(\n+      tf.debugging.assert_non_negative(\n           label_weights,\n           message='All values of `label_weights` must be non-negative.')\n-      ]\n-      with tf.control_dependencies(checks):\n       self.label_weights = label_weights\n \n     else:\n@@ -2287,30 +2284,28 @@ class AUC(Metric):\n             f'Found rank {shape.ndims}. '\n             f'Full shape received for `y_true`: {shape}')\n       self._num_labels = shape[1]\n-      variable_shape = tf.TensorShape(\n-          [tf.compat.v1.Dimension(self.num_thresholds), self._num_labels])\n-\n+      variable_shape = tf.TensorShape([self.num_thresholds, self._num_labels])\n     else:\n-      variable_shape = tf.TensorShape(\n-          [tf.compat.v1.Dimension(self.num_thresholds)])\n+      variable_shape = tf.TensorShape([self.num_thresholds])\n+\n     self._build_input_shape = shape\n     # Create metric variables\n     self.true_positives = self.add_weight(\n         'true_positives',\n         shape=variable_shape,\n-        initializer=tf.compat.v1.zeros_initializer)\n+        initializer='zeros')\n     self.true_negatives = self.add_weight(\n         'true_negatives',\n         shape=variable_shape,\n-        initializer=tf.compat.v1.zeros_initializer)\n+        initializer='zeros')\n     self.false_positives = self.add_weight(\n         'false_positives',\n         shape=variable_shape,\n-        initializer=tf.compat.v1.zeros_initializer)\n+        initializer='zeros')\n     self.false_negatives = self.add_weight(\n         'false_negatives',\n         shape=variable_shape,\n-        initializer=tf.compat.v1.zeros_initializer)\n+        initializer='zeros')\n \n     if self.multi_label:\n       with tf.init_scope():\n@@ -2335,7 +2330,6 @@ class AUC(Metric):\n     Returns:\n       Update op.\n     \"\"\"\n-    deps = []\n     if not self._built:\n       self._build(tf.TensorShape(y_pred.shape))\n \n@@ -2354,10 +2348,8 @@ class AUC(Metric):\n       if self.label_weights is not None:\n         # label_weights should be of length equal to the number of labels.\n         shapes.append((self.label_weights, ('L',)))\n-      deps = [\n-          tf.compat.v1.debugging.assert_shapes(\n+        tf.debugging.assert_shapes(\n             shapes, message='Number of labels is not consistent.')\n-      ]\n \n     # Only forward label_weights to update_confusion_matrix_variables when\n     # multi_label is False. Otherwise the averaging of individual label AUCs is\n@@ -2367,7 +2359,6 @@ class AUC(Metric):\n     if self._from_logits:\n       y_pred = activations.sigmoid(y_pred)\n \n-    with tf.control_dependencies(deps):\n     return metrics_utils.update_confusion_matrix_variables(\n         {\n             metrics_utils.ConfusionMatrix.TRUE_POSITIVES:\n@@ -3060,7 +3051,7 @@ class _IoUBase(Metric):\n     self.total_cm = self.add_weight(\n         'total_confusion_matrix',\n         shape=(num_classes, num_classes),\n-        initializer=tf.compat.v1.zeros_initializer)\n+        initializer='zeros')\n \n   def update_state(self, y_true, y_pred, sample_weight=None):\n     \"\"\"Accumulates the confusion matrix statistics.\n@@ -3678,9 +3669,9 @@ class MeanTensor(Metric):\n     self._build_input_shape = self._shape\n     # Create new state variables\n     self._total = self.add_weight(\n-        'total', shape=shape, initializer=tf.compat.v1.zeros_initializer)\n+        name='total', shape=shape, initializer='zeros')\n     self._count = self.add_weight(\n-        'count', shape=shape, initializer=tf.compat.v1.zeros_initializer)\n+        name='count', shape=shape, initializer='zeros')\n     with tf.init_scope():\n       if not tf.executing_eagerly():\n         backend._initialize_variables(backend._get_session())  # pylint: disable=protected-access\n@@ -4065,7 +4056,7 @@ def categorical_accuracy(y_true, y_pred):\n   \"\"\"\n   return tf.cast(\n       tf.equal(\n-          tf.compat.v1.argmax(y_true, axis=-1), tf.compat.v1.argmax(y_pred, axis=-1)),\n+          tf.math.argmax(y_true, axis=-1), tf.math.argmax(y_pred, axis=-1)),\n       backend.floatx())\n \n \n@@ -4100,7 +4091,7 @@ def sparse_categorical_accuracy(y_true, y_pred):\n   if (y_true_rank is not None) and (y_pred_rank is not None) and (len(\n       backend.int_shape(y_true)) == len(backend.int_shape(y_pred))):\n     y_true = tf.squeeze(y_true, [-1])\n-  y_pred = tf.compat.v1.argmax(y_pred, axis=-1)\n+  y_pred = tf.math.argmax(y_pred, axis=-1)\n \n   # If the predicted output and actual output types don't match, force cast them\n   # to match.\n@@ -4133,8 +4124,9 @@ def top_k_categorical_accuracy(y_true, y_pred, k=5):\n     Top K categorical accuracy value.\n   \"\"\"\n   return tf.cast(\n-      tf.compat.v1.math.in_top_k(\n-          y_pred, tf.compat.v1.argmax(y_true, axis=-1), k), backend.floatx())\n+      tf.math.in_top_k(\n+          predictions=y_pred, targets=tf.math.argmax(y_true, axis=-1), k=k),\n+      dtype=backend.floatx())\n \n \n @keras_export('keras.metrics.sparse_top_k_categorical_accuracy')\n@@ -4170,7 +4162,9 @@ def sparse_top_k_categorical_accuracy(y_true, y_pred, k=5):\n       y_true = tf.reshape(y_true, [-1])\n \n   return tf.cast(\n-      tf.compat.v1.math.in_top_k(y_pred, tf.cast(y_true, 'int32'), k), backend.floatx())\n+      tf.math.in_top_k(\n+          predictions=y_pred, targets=tf.cast(y_true, 'int32'), k=k),\n+      dtype=backend.floatx())\n \n \n def cosine_proximity(y_true, y_pred, axis=-1):\n\n@@ -599,11 +599,11 @@ def update_confusion_matrix_variables(variables_to_update,\n         f'Valid variable key options are: \"{list(ConfusionMatrix)}\"')\n \n   with tf.control_dependencies([\n-      tf.compat.v1.assert_greater_equal(\n+      tf.debugging.assert_greater_equal(\n           y_pred,\n           tf.cast(0.0, dtype=y_pred.dtype),\n           message='predictions must be >= 0'),\n-      tf.compat.v1.assert_less_equal(\n+      tf.debugging.assert_less_equal(\n           y_pred,\n           tf.cast(1.0, dtype=y_pred.dtype),\n           message='predictions must be <= 1')\n@@ -835,7 +835,7 @@ def _assert_splits_match(nested_splits_lists):\n     if len(splits_list) != len(nested_splits_lists[0]):\n       raise ValueError(error_msg)\n   return [\n-      tf.compat.v1.assert_equal(s1, s2, message=error_msg)  # pylint: disable=g-complex-comprehension\n+      tf.debugging.assert_equal(s1, s2, message=error_msg)  # pylint: disable=g-complex-comprehension\n       for splits_list in nested_splits_lists[1:]\n       for (s1, s2) in zip(nested_splits_lists[0], splits_list)\n   ]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#85ccb4e108551b7444213276ffb4c4c09f22f886", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1281 | Lines Deleted: 1070 | Files Changed: 19 | Hunks: 53 | Methods Changed: 91 | Complexity Δ (Sum/Max): 23/52 | Churn Δ: 2351 | Churn Cumulative: 8478 | Contributors (this commit): 42 | Commits (past 90d): 37 | Contributors (cumulative): 89 | DMM Complexity: 1.0\n\nDIFF:\n@@ -29,7 +29,7 @@ from tensorflow.python.framework import test_util\n from keras import backend\n from keras.feature_column import dense_features\n from keras.feature_column import sequence_feature_column as ksfc\n-from keras.layers import merge\n+from keras.layers import merging\n from keras.layers import recurrent\n \n \n@@ -93,7 +93,7 @@ class SequenceFeatureColumnIntegrationTest(tf.test.TestCase):\n     dense_input_layer = dense_features.DenseFeatures(ctx_cols)\n     ctx_input = dense_input_layer(features)\n     ctx_input = backend.repeat(ctx_input, tf.shape(seq_input)[1])\n-    concatenated_input = merge.concatenate([seq_input, ctx_input])\n+    concatenated_input = merging.concatenate([seq_input, ctx_input])\n \n     rnn_layer = recurrent.RNN(recurrent.SimpleRNNCell(10))\n     output = rnn_layer(concatenated_input)\n\n@@ -127,23 +127,25 @@ from keras.layers.multi_head_attention import MultiHeadAttention\n from keras.layers.local import LocallyConnected1D\n from keras.layers.local import LocallyConnected2D\n \n-# Merge layers.\n-from keras.layers.merge import Add\n-from keras.layers.merge import Subtract\n-from keras.layers.merge import Multiply\n-from keras.layers.merge import Average\n-from keras.layers.merge import Maximum\n-from keras.layers.merge import Minimum\n-from keras.layers.merge import Concatenate\n-from keras.layers.merge import Dot\n-from keras.layers.merge import add\n-from keras.layers.merge import subtract\n-from keras.layers.merge import multiply\n-from keras.layers.merge import average\n-from keras.layers.merge import maximum\n-from keras.layers.merge import minimum\n-from keras.layers.merge import concatenate\n-from keras.layers.merge import dot\n+# Merging layers.\n+from keras.layers.merging.add import Add\n+from keras.layers.merging.subtract import Subtract\n+from keras.layers.merging.multiply import Multiply\n+from keras.layers.merging.average import Average\n+from keras.layers.merging.maximum import Maximum\n+from keras.layers.merging.minimum import Minimum\n+from keras.layers.merging.concatenate import Concatenate\n+from keras.layers.merging.dot import Dot\n+\n+# Merging functions.\n+from keras.layers.merging.add import add\n+from keras.layers.merging.subtract import subtract\n+from keras.layers.merging.multiply import multiply\n+from keras.layers.merging.average import average\n+from keras.layers.merging.maximum import maximum\n+from keras.layers.merging.minimum import minimum\n+from keras.layers.merging.concatenate import concatenate\n+from keras.layers.merging.dot import dot\n \n # Noise layers.\n from keras.layers.noise import AlphaDropout\n\n@@ -631,12 +631,6 @@ class CoreLayersTest(keras_parameterized.TestCase):\n     config = layer.get_config()\n     self.assertEqual(config.pop('l1'), 0.1)\n \n-  def test_numpy_inputs(self):\n-    if tf.executing_eagerly():\n-      layer = keras.layers.Concatenate()\n-      x, y = np.ones((10, 10)), np.ones((10, 10))\n-      self.assertAllEqual(np.ones((10, 20)), layer([x, y]))\n-\n \n @keras_parameterized.run_all_keras_modes\n class TFOpLambdaTest(keras_parameterized.TestCase):\n\n@@ -1,988 +0,0 @@\n-# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-# pylint: disable=not-callable\n-# pylint: disable=redefined-builtin\n-\"\"\"Layers that can merge several inputs into one.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n-\n-from keras import backend\n-from keras.engine import base_layer_utils\n-from keras.engine.base_layer import Layer\n-from keras.utils import tf_utils\n-from tensorflow.python.util.tf_export import keras_export\n-\n-\n-class _Merge(Layer):\n-  \"\"\"Generic merge layer for elementwise merge functions.\n-\n-  Used to implement `Sum`, `Average`, etc.\n-  \"\"\"\n-\n-  def __init__(self, **kwargs):\n-    \"\"\"Initializes a Merge layer.\n-\n-    Args:\n-      **kwargs: standard layer keyword arguments.\n-    \"\"\"\n-    super(_Merge, self).__init__(**kwargs)\n-    self.supports_masking = True\n-\n-  def _merge_function(self, inputs):\n-    raise NotImplementedError\n-\n-  def _compute_elemwise_op_output_shape(self, shape1, shape2):\n-    \"\"\"Computes the shape of the resultant of an elementwise operation.\n-\n-    Args:\n-        shape1: tuple or None. Shape of the first tensor\n-        shape2: tuple or None. Shape of the second tensor\n-\n-    Returns:\n-        expected output shape when an element-wise operation is\n-        carried out on 2 tensors with shapes shape1 and shape2.\n-        tuple or None.\n-\n-    Raises:\n-        ValueError: if shape1 and shape2 are not compatible for\n-            element-wise operations.\n-    \"\"\"\n-    if None in [shape1, shape2]:\n-      return None\n-    elif len(shape1) < len(shape2):\n-      return self._compute_elemwise_op_output_shape(shape2, shape1)\n-    elif not shape2:\n-      return shape1\n-    output_shape = list(shape1[:-len(shape2)])\n-    for i, j in zip(shape1[-len(shape2):], shape2):\n-      if i is None or j is None:\n-        output_shape.append(None)\n-      elif i == 1:\n-        output_shape.append(j)\n-      elif j == 1:\n-        output_shape.append(i)\n-      else:\n-        if i != j:\n-          raise ValueError(\n-              'Inputs have incompatible shapes. '\n-              f'Received shapes {shape1} and {shape2}')\n-        output_shape.append(i)\n-    return tuple(output_shape)\n-\n-  @tf_utils.shape_type_conversion\n-  def build(self, input_shape):\n-    # Used purely for shape validation.\n-    if not isinstance(input_shape[0], tuple):\n-      raise ValueError(\n-          'A merge layer should be called on a list of inputs. '\n-          f'Received: input_shape={input_shape} (not a list of shapes)')\n-    if len(input_shape) < 2:\n-      raise ValueError('A merge layer should be called '\n-                       'on a list of at least 2 inputs. '\n-                       f'Got {len(input_shape)} inputs. '\n-                       f'Full input_shape received: {input_shape}')\n-    batch_sizes = {s[0] for s in input_shape if s} - {None}\n-    if len(batch_sizes) > 1:\n-      raise ValueError(\n-          'Cannot merge tensors with different batch sizes. '\n-          f'Got tensors with shapes {input_shape}')\n-    if input_shape[0] is None:\n-      output_shape = None\n-    else:\n-      output_shape = input_shape[0][1:]\n-    for i in range(1, len(input_shape)):\n-      if input_shape[i] is None:\n-        shape = None\n-      else:\n-        shape = input_shape[i][1:]\n-      output_shape = self._compute_elemwise_op_output_shape(output_shape, shape)\n-    # If the inputs have different ranks, we have to reshape them\n-    # to make them broadcastable.\n-    if None not in input_shape and len(set(map(len, input_shape))) == 1:\n-      self._reshape_required = False\n-    else:\n-      self._reshape_required = True\n-\n-  def call(self, inputs):\n-    if not isinstance(inputs, (list, tuple)):\n-      raise ValueError(\n-          'A merge layer should be called on a list of inputs. '\n-          f'Received: inputs={inputs} (not a list of tensors)')\n-    if self._reshape_required:\n-      reshaped_inputs = []\n-      input_ndims = list(map(backend.ndim, inputs))\n-      if None not in input_ndims:\n-        # If ranks of all inputs are available,\n-        # we simply expand each of them at axis=1\n-        # until all of them have the same rank.\n-        max_ndim = max(input_ndims)\n-        for x in inputs:\n-          x_ndim = backend.ndim(x)\n-          for _ in range(max_ndim - x_ndim):\n-            x = tf.expand_dims(x, axis=1)\n-          reshaped_inputs.append(x)\n-        return self._merge_function(reshaped_inputs)\n-      else:\n-        # Transpose all inputs so that batch size is the last dimension.\n-        # (batch_size, dim1, dim2, ... ) -> (dim1, dim2, ... , batch_size)\n-        transposed = False\n-        for x in inputs:\n-          x_ndim = backend.ndim(x)\n-          if x_ndim is None:\n-            x_shape = tf.shape(x)\n-            batch_size = x_shape[0]\n-            new_shape = backend.concatenate(\n-                [x_shape[1:],\n-                 tf.expand_dims(batch_size, axis=-1)])\n-            x_transposed = tf.reshape(\n-                x,\n-                tf.stack(\n-                    [batch_size, tf.reduce_prod(x_shape[1:])], axis=0))\n-            x_transposed = tf.transpose(x_transposed, perm=(1, 0))\n-            x_transposed = tf.reshape(x_transposed, new_shape)\n-            reshaped_inputs.append(x_transposed)\n-            transposed = True\n-          elif x_ndim > 1:\n-            dims = list(range(1, x_ndim)) + [0]\n-            reshaped_inputs.append(tf.transpose(x, perm=dims))\n-            transposed = True\n-          else:\n-            # We don't transpose inputs if they are 1D vectors or scalars.\n-            reshaped_inputs.append(x)\n-        y = self._merge_function(reshaped_inputs)\n-        y_ndim = backend.ndim(y)\n-        if transposed:\n-          # If inputs have been transposed, we have to transpose the output too.\n-          if y_ndim is None:\n-            y_shape = tf.shape(y)\n-            y_ndim = tf.shape(y_shape)[0]\n-            batch_size = y_shape[y_ndim - 1]\n-            new_shape = backend.concatenate([\n-                tf.expand_dims(batch_size, axis=-1), y_shape[:y_ndim - 1]\n-            ])\n-            y = tf.reshape(y, (-1, batch_size))\n-            y = tf.transpose(y, perm=(1, 0))\n-            y = tf.reshape(y, new_shape)\n-          elif y_ndim > 1:\n-            dims = [y_ndim - 1] + list(range(y_ndim - 1))\n-            y = tf.transpose(y, perm=dims)\n-        return y\n-    else:\n-      return self._merge_function(inputs)\n-\n-  @tf_utils.shape_type_conversion\n-  def compute_output_shape(self, input_shape):\n-    if input_shape[0] is None:\n-      output_shape = None\n-    else:\n-      output_shape = input_shape[0][1:]\n-    for i in range(1, len(input_shape)):\n-      if input_shape[i] is None:\n-        shape = None\n-      else:\n-        shape = input_shape[i][1:]\n-      output_shape = self._compute_elemwise_op_output_shape(output_shape, shape)\n-    batch_sizes = {s[0] for s in input_shape if s is not None} - {None}\n-    if len(batch_sizes) == 1:\n-      output_shape = (list(batch_sizes)[0],) + output_shape\n-    else:\n-      output_shape = (None,) + output_shape\n-    return output_shape\n-\n-  def compute_mask(self, inputs, mask=None):\n-    if mask is None:\n-      return None\n-    if not isinstance(mask, (tuple, list)):\n-      raise ValueError(f'`mask` should be a list. Received: mask={mask}')\n-    if not isinstance(inputs, (tuple, list)):\n-      raise ValueError(f'`inputs` should be a list. Received: inputs={inputs}')\n-    if len(mask) != len(inputs):\n-      raise ValueError(\n-          'The lists `inputs` and `mask` should have the same length. '\n-          f'Received: inputs={inputs} of length {len(inputs)}, and '\n-          f'mask={mask} of length {len(mask)}')\n-    if all(m is None for m in mask):\n-      return None\n-    masks = [tf.expand_dims(m, axis=0) for m in mask if m is not None]\n-    return backend.all(\n-        backend.concatenate(masks, axis=0), axis=0, keepdims=False)\n-\n-  def get_config(self):  # pylint: disable=useless-super-delegation\n-    return super().get_config()\n-\n-\n-@keras_export('keras.layers.Add')\n-class Add(_Merge):\n-  \"\"\"Layer that adds a list of inputs.\n-\n-  It takes as input a list of tensors,\n-  all of the same shape, and returns\n-  a single tensor (also of the same shape).\n-\n-  Examples:\n-\n-  >>> input_shape = (2, 3, 4)\n-  >>> x1 = tf.random.normal(input_shape)\n-  >>> x2 = tf.random.normal(input_shape)\n-  >>> y = tf.keras.layers.Add()([x1, x2])\n-  >>> print(y.shape)\n-  (2, 3, 4)\n-\n-  Used in a functional model:\n-\n-  >>> input1 = tf.keras.layers.Input(shape=(16,))\n-  >>> x1 = tf.keras.layers.Dense(8, activation='relu')(input1)\n-  >>> input2 = tf.keras.layers.Input(shape=(32,))\n-  >>> x2 = tf.keras.layers.Dense(8, activation='relu')(input2)\n-  >>> # equivalent to `added = tf.keras.layers.add([x1, x2])`\n-  >>> added = tf.keras.layers.Add()([x1, x2])\n-  >>> out = tf.keras.layers.Dense(4)(added)\n-  >>> model = tf.keras.models.Model(inputs=[input1, input2], outputs=out)\n-\n-  \"\"\"\n-\n-  def _merge_function(self, inputs):\n-    output = inputs[0]\n-    for i in range(1, len(inputs)):\n-      output += inputs[i]\n-    return output\n-\n-\n-@keras_export('keras.layers.Subtract')\n-class Subtract(_Merge):\n-  \"\"\"Layer that subtracts two inputs.\n-\n-  It takes as input a list of tensors of size 2,\n-  both of the same shape, and returns a single tensor, (inputs[0] - inputs[1]),\n-  also of the same shape.\n-\n-  Examples:\n-\n-  ```python\n-      import keras\n-\n-      input1 = keras.layers.Input(shape=(16,))\n-      x1 = keras.layers.Dense(8, activation='relu')(input1)\n-      input2 = keras.layers.Input(shape=(32,))\n-      x2 = keras.layers.Dense(8, activation='relu')(input2)\n-      # Equivalent to subtracted = keras.layers.subtract([x1, x2])\n-      subtracted = keras.layers.Subtract()([x1, x2])\n-\n-      out = keras.layers.Dense(4)(subtracted)\n-      model = keras.models.Model(inputs=[input1, input2], outputs=out)\n-  ```\n-  \"\"\"\n-\n-  @tf_utils.shape_type_conversion\n-  def build(self, input_shape):\n-    super(Subtract, self).build(input_shape)\n-    if len(input_shape) != 2:\n-      raise ValueError(\n-          'A `Subtract` layer should be called on exactly 2 inputs. '\n-          f'Received: input_shape={input_shape}')\n-\n-  def _merge_function(self, inputs):\n-    if len(inputs) != 2:\n-      raise ValueError(\n-          'A `Subtract` layer should be called on exactly 2 inputs. '\n-          f'Received: inputs={inputs}')\n-    return inputs[0] - inputs[1]\n-\n-\n-@keras_export('keras.layers.Multiply')\n-class Multiply(_Merge):\n-  \"\"\"Layer that multiplies (element-wise) a list of inputs.\n-\n-  It takes as input a list of tensors, all of the same shape, and returns\n-  a single tensor (also of the same shape).\n-\n-  >>> tf.keras.layers.Multiply()([np.arange(5).reshape(5, 1),\n-  ...                             np.arange(5, 10).reshape(5, 1)])\n-  <tf.Tensor: shape=(5, 1), dtype=int64, numpy=\n-  array([[ 0],\n-       [ 6],\n-       [14],\n-       [24],\n-       [36]])>\n-\n-  >>> x1 = tf.keras.layers.Dense(8)(np.arange(10).reshape(5, 2))\n-  >>> x2 = tf.keras.layers.Dense(8)(np.arange(10, 20).reshape(5, 2))\n-  >>> multiplied = tf.keras.layers.Multiply()([x1, x2])\n-  >>> multiplied.shape\n-  TensorShape([5, 8])\n-  \"\"\"\n-\n-  def _merge_function(self, inputs):\n-    output = inputs[0]\n-    for i in range(1, len(inputs)):\n-      output = output * inputs[i]\n-    return output\n-\n-\n-@keras_export('keras.layers.Average')\n-class Average(_Merge):\n-  \"\"\"Layer that averages a list of inputs element-wise.\n-\n-  It takes as input a list of tensors, all of the same shape, and returns\n-  a single tensor (also of the same shape).\n-\n-  Example:\n-\n-  >>> x1 = np.ones((2, 2))\n-  >>> x2 = np.zeros((2, 2))\n-  >>> y = tf.keras.layers.Average()([x1, x2])\n-  >>> y.numpy().tolist()\n-  [[0.5, 0.5], [0.5, 0.5]]\n-\n-  Usage in a functional model:\n-\n-  >>> input1 = tf.keras.layers.Input(shape=(16,))\n-  >>> x1 = tf.keras.layers.Dense(8, activation='relu')(input1)\n-  >>> input2 = tf.keras.layers.Input(shape=(32,))\n-  >>> x2 = tf.keras.layers.Dense(8, activation='relu')(input2)\n-  >>> avg = tf.keras.layers.Average()([x1, x2])\n-  >>> out = tf.keras.layers.Dense(4)(avg)\n-  >>> model = tf.keras.models.Model(inputs=[input1, input2], outputs=out)\n-\n-  Raises:\n-    ValueError: If there is a shape mismatch between the inputs and the shapes\n-      cannot be broadcasted to match.\n-  \"\"\"\n-\n-  def _merge_function(self, inputs):\n-    output = inputs[0]\n-    for i in range(1, len(inputs)):\n-      output += inputs[i]\n-    return output / len(inputs)\n-\n-\n-@keras_export('keras.layers.Maximum')\n-class Maximum(_Merge):\n-  \"\"\"Layer that computes the maximum (element-wise) a list of inputs.\n-\n-  It takes as input a list of tensors, all of the same shape, and returns\n-  a single tensor (also of the same shape).\n-\n-  >>> tf.keras.layers.Maximum()([np.arange(5).reshape(5, 1),\n-  ...                            np.arange(5, 10).reshape(5, 1)])\n-  <tf.Tensor: shape=(5, 1), dtype=int64, numpy=\n-  array([[5],\n-       [6],\n-       [7],\n-       [8],\n-       [9]])>\n-\n-  >>> x1 = tf.keras.layers.Dense(8)(np.arange(10).reshape(5, 2))\n-  >>> x2 = tf.keras.layers.Dense(8)(np.arange(10, 20).reshape(5, 2))\n-  >>> maxed = tf.keras.layers.Maximum()([x1, x2])\n-  >>> maxed.shape\n-  TensorShape([5, 8])\n-  \"\"\"\n-\n-  def _merge_function(self, inputs):\n-    output = inputs[0]\n-    for i in range(1, len(inputs)):\n-      output = tf.maximum(output, inputs[i])\n-    return output\n-\n-\n-@keras_export('keras.layers.Minimum')\n-class Minimum(_Merge):\n-  \"\"\"Layer that computes the minimum (element-wise) a list of inputs.\n-\n-  It takes as input a list of tensors, all of the same shape, and returns\n-  a single tensor (also of the same shape).\n-\n-  >>> tf.keras.layers.Minimum()([np.arange(5).reshape(5, 1),\n-  ...                            np.arange(5, 10).reshape(5, 1)])\n-  <tf.Tensor: shape=(5, 1), dtype=int64, numpy=\n-  array([[0],\n-       [1],\n-       [2],\n-       [3],\n-       [4]])>\n-\n-  >>> x1 = tf.keras.layers.Dense(8)(np.arange(10).reshape(5, 2))\n-  >>> x2 = tf.keras.layers.Dense(8)(np.arange(10, 20).reshape(5, 2))\n-  >>> minned = tf.keras.layers.Minimum()([x1, x2])\n-  >>> minned.shape\n-  TensorShape([5, 8])\n-  \"\"\"\n-\n-  def _merge_function(self, inputs):\n-    output = inputs[0]\n-    for i in range(1, len(inputs)):\n-      output = tf.minimum(output, inputs[i])\n-    return output\n-\n-\n-@keras_export('keras.layers.Concatenate')\n-class Concatenate(_Merge):\n-  \"\"\"Layer that concatenates a list of inputs.\n-\n-  It takes as input a list of tensors, all of the same shape except\n-  for the concatenation axis, and returns a single tensor that is the\n-  concatenation of all inputs.\n-\n-  >>> x = np.arange(20).reshape(2, 2, 5)\n-  >>> print(x)\n-  [[[ 0  1  2  3  4]\n-    [ 5  6  7  8  9]]\n-   [[10 11 12 13 14]\n-    [15 16 17 18 19]]]\n-  >>> y = np.arange(20, 30).reshape(2, 1, 5)\n-  >>> print(y)\n-  [[[20 21 22 23 24]]\n-   [[25 26 27 28 29]]]\n-  >>> tf.keras.layers.Concatenate(axis=1)([x, y])\n-  <tf.Tensor: shape=(2, 3, 5), dtype=int64, numpy=\n-  array([[[ 0,  1,  2,  3,  4],\n-          [ 5,  6,  7,  8,  9],\n-          [20, 21, 22, 23, 24]],\n-         [[10, 11, 12, 13, 14],\n-          [15, 16, 17, 18, 19],\n-          [25, 26, 27, 28, 29]]])>\n-\n-  >>> x1 = tf.keras.layers.Dense(8)(np.arange(10).reshape(5, 2))\n-  >>> x2 = tf.keras.layers.Dense(8)(np.arange(10, 20).reshape(5, 2))\n-  >>> concatted = tf.keras.layers.Concatenate()([x1, x2])\n-  >>> concatted.shape\n-  TensorShape([5, 16])\n-\n-  \"\"\"\n-\n-  def __init__(self, axis=-1, **kwargs):\n-    \"\"\"Instantiates a Concatenate layer.\n-\n-    >>> x = np.arange(20).reshape(2, 2, 5)\n-    >>> print(x)\n-    [[[ 0  1  2  3  4]\n-      [ 5  6  7  8  9]]\n-     [[10 11 12 13 14]\n-      [15 16 17 18 19]]]\n-    >>> y = np.arange(20, 30).reshape(2, 1, 5)\n-    >>> print(y)\n-    [[[20 21 22 23 24]]\n-     [[25 26 27 28 29]]]\n-    >>> tf.keras.layers.Concatenate(axis=1)([x, y])\n-    <tf.Tensor: shape=(2, 3, 5), dtype=int64, numpy=\n-    array([[[ 0,  1,  2,  3,  4],\n-            [ 5,  6,  7,  8,  9],\n-            [20, 21, 22, 23, 24]],\n-           [[10, 11, 12, 13, 14],\n-            [15, 16, 17, 18, 19],\n-            [25, 26, 27, 28, 29]]])>\n-\n-    Args:\n-      axis: Axis along which to concatenate.\n-      **kwargs: standard layer keyword arguments.\n-    \"\"\"\n-    super(Concatenate, self).__init__(**kwargs)\n-    self.axis = axis\n-    self.supports_masking = True\n-    self._reshape_required = False\n-\n-  @tf_utils.shape_type_conversion\n-  def build(self, input_shape):\n-    # Used purely for shape validation.\n-    if not isinstance(input_shape[0], tuple) or len(input_shape) < 1:\n-      raise ValueError(\n-          'A `Concatenate` layer should be called on a list of '\n-          f'at least 1 input. Received: input_shape={input_shape}')\n-    if all(shape is None for shape in input_shape):\n-      return\n-    reduced_inputs_shapes = [list(shape) for shape in input_shape]\n-    shape_set = set()\n-    for i in range(len(reduced_inputs_shapes)):\n-      del reduced_inputs_shapes[i][self.axis]\n-      shape_set.add(tuple(reduced_inputs_shapes[i]))\n-\n-    if len(shape_set) != 1:\n-      err_msg = ('A `Concatenate` layer requires inputs with matching shapes '\n-                 'except for the concatenation axis. '\n-                 f'Received: input_shape={input_shape}')\n-      # Make sure all the shapes have same ranks.\n-      ranks = set(len(shape) for shape in shape_set)\n-      if len(ranks) != 1:\n-        raise ValueError(err_msg)\n-      # Get the only rank for the set.\n-      (rank,) = ranks\n-      for axis in range(rank):\n-        # Skip the Nones in the shape since they are dynamic, also the axis for\n-        # concat has been removed above.\n-        unique_dims = set(\n-            shape[axis] for shape in shape_set if shape[axis] is not None)\n-        if len(unique_dims) > 1:\n-          raise ValueError(err_msg)\n-\n-  def _merge_function(self, inputs):\n-    return backend.concatenate(inputs, axis=self.axis)\n-\n-  @tf_utils.shape_type_conversion\n-  def compute_output_shape(self, input_shape):\n-    if ((not isinstance(input_shape, (tuple, list))) or\n-        (not isinstance(input_shape[0], (tuple, list)))):\n-      # The tf_utils.shape_type_conversion decorator turns tensorshapes\n-      # into tuples, so we need to verify that `input_shape` is a list/tuple,\n-      # *and* that the individual elements are themselves shape tuples.\n-      raise ValueError(\n-          'A `Concatenate` layer should be called on a list of inputs. '\n-          f'Received: input_shape={input_shape}')\n-    input_shapes = input_shape\n-    output_shape = list(input_shapes[0])\n-    for shape in input_shapes[1:]:\n-      if output_shape[self.axis] is None or shape[self.axis] is None:\n-        output_shape[self.axis] = None\n-        break\n-      output_shape[self.axis] += shape[self.axis]\n-    return tuple(output_shape)\n-\n-  def compute_mask(self, inputs, mask=None):\n-    if mask is None:\n-      return None\n-    if not isinstance(mask, (tuple, list)):\n-      raise ValueError(f'`mask` should be a list. Received mask={mask}')\n-    if not isinstance(inputs, (tuple, list)):\n-      raise ValueError(f'`inputs` should be a list. Received: inputs={inputs}')\n-    if len(mask) != len(inputs):\n-      raise ValueError(\n-          'The lists `inputs` and `mask` should have the same length. '\n-          f'Received: inputs={inputs} of length {len(inputs)}, and '\n-          f'mask={mask} of length {len(mask)}')\n-    if all(m is None for m in mask):\n-      return None\n-    # Make a list of masks while making sure\n-    # the dimensionality of each mask\n-    # is the same as the corresponding input.\n-    masks = []\n-    for input_i, mask_i in zip(inputs, mask):\n-      if mask_i is None:\n-        # Input is unmasked. Append all 1s to masks,\n-        masks.append(tf.ones_like(input_i, dtype='bool'))\n-      elif backend.ndim(mask_i) < backend.ndim(input_i):\n-        # Mask is smaller than the input, expand it\n-        masks.append(tf.expand_dims(mask_i, axis=-1))\n-      else:\n-        masks.append(mask_i)\n-    concatenated = backend.concatenate(masks, axis=self.axis)\n-    return backend.all(concatenated, axis=-1, keepdims=False)\n-\n-  def get_config(self):\n-    config = {\n-        'axis': self.axis,\n-    }\n-    base_config = super(Concatenate, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-@keras_export('keras.layers.Dot')\n-class Dot(_Merge):\n-  \"\"\"Layer that computes a dot product between samples in two tensors.\n-\n-  E.g. if applied to a list of two tensors `a` and `b` of shape\n-  `(batch_size, n)`, the output will be a tensor of shape `(batch_size, 1)`\n-  where each entry `i` will be the dot product between\n-  `a[i]` and `b[i]`.\n-\n-  >>> x = np.arange(10).reshape(1, 5, 2)\n-  >>> print(x)\n-  [[[0 1]\n-    [2 3]\n-    [4 5]\n-    [6 7]\n-    [8 9]]]\n-  >>> y = np.arange(10, 20).reshape(1, 2, 5)\n-  >>> print(y)\n-  [[[10 11 12 13 14]\n-    [15 16 17 18 19]]]\n-  >>> tf.keras.layers.Dot(axes=(1, 2))([x, y])\n-  <tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=\n-  array([[[260, 360],\n-          [320, 445]]])>\n-\n-  >>> x1 = tf.keras.layers.Dense(8)(np.arange(10).reshape(5, 2))\n-  >>> x2 = tf.keras.layers.Dense(8)(np.arange(10, 20).reshape(5, 2))\n-  >>> dotted = tf.keras.layers.Dot(axes=1)([x1, x2])\n-  >>> dotted.shape\n-  TensorShape([5, 1])\n-\n-\n-  \"\"\"\n-\n-  def __init__(self, axes, normalize=False, **kwargs):\n-    \"\"\"Initializes a layer that computes the element-wise dot product.\n-\n-      >>> x = np.arange(10).reshape(1, 5, 2)\n-      >>> print(x)\n-      [[[0 1]\n-        [2 3]\n-        [4 5]\n-        [6 7]\n-        [8 9]]]\n-      >>> y = np.arange(10, 20).reshape(1, 2, 5)\n-      >>> print(y)\n-      [[[10 11 12 13 14]\n-        [15 16 17 18 19]]]\n-      >>> tf.keras.layers.Dot(axes=(1, 2))([x, y])\n-      <tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=\n-      array([[[260, 360],\n-              [320, 445]]])>\n-\n-    Args:\n-      axes: Integer or tuple of integers,\n-        axis or axes along which to take the dot product. If a tuple, should\n-        be two integers corresponding to the desired axis from the first input\n-        and the desired axis from the second input, respectively. Note that the\n-        size of the two selected axes must match.\n-      normalize: Whether to L2-normalize samples along the\n-        dot product axis before taking the dot product.\n-        If set to True, then the output of the dot product\n-        is the cosine proximity between the two samples.\n-      **kwargs: Standard layer keyword arguments.\n-    \"\"\"\n-    super(Dot, self).__init__(**kwargs)\n-    if not isinstance(axes, int):\n-      if not isinstance(axes, (list, tuple)):\n-        raise TypeError(\n-            'Invalid type for argument `axes`: it should be '\n-            f'a list or an int. Received: axes={axes}')\n-      if len(axes) != 2:\n-        raise ValueError(\n-            'Invalid format for argument `axes`: it should contain two '\n-            f'elements. Received: axes={axes}')\n-      if not isinstance(axes[0], int) or not isinstance(axes[1], int):\n-        raise ValueError(\n-            'Invalid format for argument `axes`: list elements should be '\n-            f'integers. Received: axes={axes}')\n-    self.axes = axes\n-    self.normalize = normalize\n-    self.supports_masking = True\n-    self._reshape_required = False\n-\n-  @tf_utils.shape_type_conversion\n-  def build(self, input_shape):\n-    # Used purely for shape validation.\n-    if not isinstance(input_shape[0], tuple) or len(input_shape) != 2:\n-      raise ValueError(\n-          'A `Dot` layer should be called on a list of 2 inputs. '\n-          f'Received: input_shape={input_shape}')\n-    shape1 = input_shape[0]\n-    shape2 = input_shape[1]\n-    if shape1 is None or shape2 is None:\n-      return\n-    if isinstance(self.axes, int):\n-      if self.axes < 0:\n-        axes = [self.axes % len(shape1), self.axes % len(shape2)]\n-      else:\n-        axes = [self.axes] * 2\n-    else:\n-      axes = self.axes\n-    if shape1[axes[0]] != shape2[axes[1]]:\n-      raise ValueError(\n-          'Incompatible input shapes: '\n-          f'axis values {shape1[axes[0]]} (at axis {axes[0]}) != '\n-          f'{shape2[axes[1]]} (at axis {axes[1]}). '\n-          f'Full input shapes: {shape1}, {shape2}')\n-\n-  def _merge_function(self, inputs):\n-    base_layer_utils.no_ragged_support(inputs, self.name)\n-    if len(inputs) != 2:\n-      raise ValueError(\n-          'A `Dot` layer should be called on exactly 2 inputs. '\n-          f'Received: inputs={inputs}')\n-    x1 = inputs[0]\n-    x2 = inputs[1]\n-    if isinstance(self.axes, int):\n-      if self.axes < 0:\n-        axes = [self.axes % backend.ndim(x1), self.axes % backend.ndim(x2)]\n-      else:\n-        axes = [self.axes] * 2\n-    else:\n-      axes = []\n-      for i in range(len(self.axes)):\n-        if self.axes[i] < 0:\n-          axes.append(self.axes[i] % backend.ndim(inputs[i]))\n-        else:\n-          axes.append(self.axes[i])\n-    if self.normalize:\n-      x1 = tf.linalg.l2_normalize(x1, axis=axes[0])\n-      x2 = tf.linalg.l2_normalize(x2, axis=axes[1])\n-    output = backend.batch_dot(x1, x2, axes)\n-    return output\n-\n-  @tf_utils.shape_type_conversion\n-  def compute_output_shape(self, input_shape):\n-    if not isinstance(input_shape, (tuple, list)) or len(input_shape) != 2:\n-      raise ValueError(\n-          'A `Dot` layer should be called on a list of 2 inputs. '\n-          f'Received: input_shape={input_shape}')\n-    shape1 = list(input_shape[0])\n-    shape2 = list(input_shape[1])\n-    if isinstance(self.axes, int):\n-      if self.axes < 0:\n-        axes = [self.axes % len(shape1), self.axes % len(shape2)]\n-      else:\n-        axes = [self.axes] * 2\n-    else:\n-      axes = self.axes\n-    shape1.pop(axes[0])\n-    shape2.pop(axes[1])\n-    shape2.pop(0)\n-    output_shape = shape1 + shape2\n-    if len(output_shape) == 1:\n-      output_shape += [1]\n-    return tuple(output_shape)\n-\n-  def compute_mask(self, inputs, mask=None):\n-    return None\n-\n-  def get_config(self):\n-    config = {\n-        'axes': self.axes,\n-        'normalize': self.normalize,\n-    }\n-    base_config = super(Dot, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-@keras_export('keras.layers.add')\n-def add(inputs, **kwargs):\n-  \"\"\"Functional interface to the `tf.keras.layers.Add` layer.\n-\n-  Args:\n-      inputs: A list of input tensors (at least 2) with the same shape.\n-      **kwargs: Standard layer keyword arguments.\n-\n-  Returns:\n-      A tensor as the sum of the inputs. It has the same shape as the inputs.\n-\n-  Examples:\n-\n-  >>> input_shape = (2, 3, 4)\n-  >>> x1 = tf.random.normal(input_shape)\n-  >>> x2 = tf.random.normal(input_shape)\n-  >>> y = tf.keras.layers.add([x1, x2])\n-  >>> print(y.shape)\n-  (2, 3, 4)\n-\n-  Used in a functional model:\n-\n-  >>> input1 = tf.keras.layers.Input(shape=(16,))\n-  >>> x1 = tf.keras.layers.Dense(8, activation='relu')(input1)\n-  >>> input2 = tf.keras.layers.Input(shape=(32,))\n-  >>> x2 = tf.keras.layers.Dense(8, activation='relu')(input2)\n-  >>> added = tf.keras.layers.add([x1, x2])\n-  >>> out = tf.keras.layers.Dense(4)(added)\n-  >>> model = tf.keras.models.Model(inputs=[input1, input2], outputs=out)\n-\n-  \"\"\"\n-  return Add(**kwargs)(inputs)\n-\n-\n-@keras_export('keras.layers.subtract')\n-def subtract(inputs, **kwargs):\n-  \"\"\"Functional interface to the `Subtract` layer.\n-\n-  Args:\n-      inputs: A list of input tensors (exactly 2).\n-      **kwargs: Standard layer keyword arguments.\n-\n-  Returns:\n-      A tensor, the difference of the inputs.\n-\n-  Examples:\n-\n-  ```python\n-      import keras\n-\n-      input1 = keras.layers.Input(shape=(16,))\n-      x1 = keras.layers.Dense(8, activation='relu')(input1)\n-      input2 = keras.layers.Input(shape=(32,))\n-      x2 = keras.layers.Dense(8, activation='relu')(input2)\n-      subtracted = keras.layers.subtract([x1, x2])\n-\n-      out = keras.layers.Dense(4)(subtracted)\n-      model = keras.models.Model(inputs=[input1, input2], outputs=out)\n-  ```\n-  \"\"\"\n-  return Subtract(**kwargs)(inputs)\n-\n-\n-@keras_export('keras.layers.multiply')\n-def multiply(inputs, **kwargs):\n-  \"\"\"Functional interface to the `Multiply` layer.\n-\n-  Example:\n-\n-  >>> x1 = np.arange(3.0)\n-  >>> x2 = np.arange(3.0)\n-  >>> tf.keras.layers.multiply([x1, x2])\n-  <tf.Tensor: shape=(3,), dtype=float32, numpy=array([0., 1., 4.], ...)>\n-\n-  Usage in a functional model:\n-\n-  >>> input1 = tf.keras.layers.Input(shape=(16,))\n-  >>> x1 = tf.keras.layers.Dense(8, activation='relu')(input1) #shape=(None, 8)\n-  >>> input2 = tf.keras.layers.Input(shape=(32,))\n-  >>> x2 = tf.keras.layers.Dense(8, activation='relu')(input2) #shape=(None, 8)\n-  >>> out = tf.keras.layers.multiply([x1,x2]) #shape=(None, 8)\n-  >>> out = tf.keras.layers.Dense(4)(out)\n-  >>> model = tf.keras.models.Model(inputs=[input1, input2], outputs=out)\n-\n-  Args:\n-      inputs: A list of input tensors (at least 2).\n-      **kwargs: Standard layer keyword arguments.\n-\n-  Returns:\n-      A tensor, the element-wise product of the inputs.\n-  \"\"\"\n-  return Multiply(**kwargs)(inputs)\n-\n-\n-@keras_export('keras.layers.average')\n-def average(inputs, **kwargs):\n-  \"\"\"Functional interface to the `tf.keras.layers.Average` layer.\n-\n-  Example:\n-\n-  >>> x1 = np.ones((2, 2))\n-  >>> x2 = np.zeros((2, 2))\n-  >>> y = tf.keras.layers.Average()([x1, x2])\n-  >>> y.numpy().tolist()\n-  [[0.5, 0.5], [0.5, 0.5]]\n-\n-  Usage in a functional model:\n-\n-  >>> input1 = tf.keras.layers.Input(shape=(16,))\n-  >>> x1 = tf.keras.layers.Dense(8, activation='relu')(input1)\n-  >>> input2 = tf.keras.layers.Input(shape=(32,))\n-  >>> x2 = tf.keras.layers.Dense(8, activation='relu')(input2)\n-  >>> avg = tf.keras.layers.Average()([x1, x2])\n-  >>> out = tf.keras.layers.Dense(4)(avg)\n-  >>> model = tf.keras.models.Model(inputs=[input1, input2], outputs=out)\n-\n-  Args:\n-      inputs: A list of input tensors (at least 2).\n-      **kwargs: Standard layer keyword arguments.\n-\n-  Returns:\n-      A tensor, the average of the inputs.\n-\n-  Raises:\n-    ValueError: If there is a shape mismatch between the inputs and the shapes\n-      cannot be broadcasted to match.\n-  \"\"\"\n-  return Average(**kwargs)(inputs)\n-\n-\n-@keras_export('keras.layers.maximum')\n-def maximum(inputs, **kwargs):\n-  \"\"\"Functional interface to compute maximum (element-wise) list of `inputs`.\n-\n-  This is equivalent to the `tf.keras.layers.Maximum` layer.\n-\n-  For example:\n-\n-  ```python\n-  input1 = tf.keras.layers.Input(shape=(16,))\n-  x1 = tf.keras.layers.Dense(8, activation='relu')(input1) #shape=(None, 8)\n-  input2 = tf.keras.layers.Input(shape=(32,))\n-  x2 = tf.keras.layers.Dense(8, activation='relu')(input2) #shape=(None, 8)\n-  max_inp=tf.keras.layers.maximum([x1,x2]) #shape=(None, 8)\n-  out = tf.keras.layers.Dense(4)(max_inp)\n-  model = tf.keras.models.Model(inputs=[input1, input2], outputs=out)\n-  ```\n-\n-  Args:\n-      inputs: A list of input tensors (at least 2) of same shape.\n-      **kwargs: Standard layer keyword arguments.\n-\n-  Returns:\n-      A tensor (of same shape as input tensor) with the element-wise\n-      maximum of the inputs.\n-\n-  Raises:\n-      ValueError: If input tensors are of different shape.\n-  \"\"\"\n-  return Maximum(**kwargs)(inputs)\n-\n-\n-@keras_export('keras.layers.minimum')\n-def minimum(inputs, **kwargs):\n-  \"\"\"Functional interface to the `Minimum` layer.\n-\n-  Args:\n-      inputs: A list of input tensors (at least 2).\n-      **kwargs: Standard layer keyword arguments.\n-\n-  Returns:\n-      A tensor, the element-wise minimum of the inputs.\n-  \"\"\"\n-  return Minimum(**kwargs)(inputs)\n-\n-\n-@keras_export('keras.layers.concatenate')\n-def concatenate(inputs, axis=-1, **kwargs):\n-  \"\"\"Functional interface to the `Concatenate` layer.\n-\n-  >>> x = np.arange(20).reshape(2, 2, 5)\n-  >>> print(x)\n-  [[[ 0  1  2  3  4]\n-    [ 5  6  7  8  9]]\n-   [[10 11 12 13 14]\n-    [15 16 17 18 19]]]\n-  >>> y = np.arange(20, 30).reshape(2, 1, 5)\n-  >>> print(y)\n-  [[[20 21 22 23 24]]\n-   [[25 26 27 28 29]]]\n-  >>> tf.keras.layers.concatenate([x, y],\n-  ...                             axis=1)\n-  <tf.Tensor: shape=(2, 3, 5), dtype=int64, numpy=\n-  array([[[ 0,  1,  2,  3,  4],\n-        [ 5,  6,  7,  8,  9],\n-        [20, 21, 22, 23, 24]],\n-       [[10, 11, 12, 13, 14],\n-        [15, 16, 17, 18, 19],\n-        [25, 26, 27, 28, 29]]])>\n-\n-  Args:\n-      inputs: A list of input tensors (at least 2).\n-      axis: Concatenation axis.\n-      **kwargs: Standard layer keyword arguments.\n-\n-  Returns:\n-      A tensor, the concatenation of the inputs alongside axis `axis`.\n-  \"\"\"\n-  return Concatenate(axis=axis, **kwargs)(inputs)\n-\n-\n-@keras_export('keras.layers.dot')\n-def dot(inputs, axes, normalize=False, **kwargs):\n-  \"\"\"Functional interface to the `Dot` layer.\n-\n-  Args:\n-      inputs: A list of input tensors (at least 2).\n-      axes: Integer or tuple of integers,\n-          axis or axes along which to take the dot product.\n-      normalize: Whether to L2-normalize samples along the\n-          dot product axis before taking the dot product.\n-          If set to True, then the output of the dot product\n-          is the cosine proximity between the two samples.\n-      **kwargs: Standard layer keyword arguments.\n-\n-  Returns:\n-      A tensor, the dot product of the samples from the inputs.\n-  \"\"\"\n-  return Dot(axes=axes, normalize=normalize, **kwargs)(inputs)\n\n@@ -0,0 +1,36 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Keras merging layers.\"\"\"\n+# pylint: disable=g-bad-import-order\n+\n+# Merging layers.\n+from keras.layers.merging.add import Add\n+from keras.layers.merging.subtract import Subtract\n+from keras.layers.merging.multiply import Multiply\n+from keras.layers.merging.average import Average\n+from keras.layers.merging.maximum import Maximum\n+from keras.layers.merging.minimum import Minimum\n+from keras.layers.merging.concatenate import Concatenate\n+from keras.layers.merging.dot import Dot\n+\n+# Merging functions.\n+from keras.layers.merging.add import add\n+from keras.layers.merging.subtract import subtract\n+from keras.layers.merging.multiply import multiply\n+from keras.layers.merging.average import average\n+from keras.layers.merging.maximum import maximum\n+from keras.layers.merging.minimum import minimum\n+from keras.layers.merging.concatenate import concatenate\n+from keras.layers.merging.dot import dot\n\n@@ -0,0 +1,91 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Layer that adds several inputs.\"\"\"\n+# pylint: disable=g-direct-tensorflow-import\n+\n+from keras.layers.merging.base_merge import _Merge\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.Add')\n+class Add(_Merge):\n+  \"\"\"Layer that adds a list of inputs.\n+\n+  It takes as input a list of tensors,\n+  all of the same shape, and returns\n+  a single tensor (also of the same shape).\n+\n+  Examples:\n+\n+  >>> input_shape = (2, 3, 4)\n+  >>> x1 = tf.random.normal(input_shape)\n+  >>> x2 = tf.random.normal(input_shape)\n+  >>> y = tf.keras.layers.Add()([x1, x2])\n+  >>> print(y.shape)\n+  (2, 3, 4)\n+\n+  Used in a functional model:\n+\n+  >>> input1 = tf.keras.layers.Input(shape=(16,))\n+  >>> x1 = tf.keras.layers.Dense(8, activation='relu')(input1)\n+  >>> input2 = tf.keras.layers.Input(shape=(32,))\n+  >>> x2 = tf.keras.layers.Dense(8, activation='relu')(input2)\n+  >>> # equivalent to `added = tf.keras.layers.add([x1, x2])`\n+  >>> added = tf.keras.layers.Add()([x1, x2])\n+  >>> out = tf.keras.layers.Dense(4)(added)\n+  >>> model = tf.keras.models.Model(inputs=[input1, input2], outputs=out)\n+\n+  \"\"\"\n+\n+  def _merge_function(self, inputs):\n+    output = inputs[0]\n+    for i in range(1, len(inputs)):\n+      output += inputs[i]\n+    return output\n+\n+\n+@keras_export('keras.layers.add')\n+def add(inputs, **kwargs):\n+  \"\"\"Functional interface to the `tf.keras.layers.Add` layer.\n+\n+  Args:\n+      inputs: A list of input tensors (at least 2) with the same shape.\n+      **kwargs: Standard layer keyword arguments.\n+\n+  Returns:\n+      A tensor as the sum of the inputs. It has the same shape as the inputs.\n+\n+  Examples:\n+\n+  >>> input_shape = (2, 3, 4)\n+  >>> x1 = tf.random.normal(input_shape)\n+  >>> x2 = tf.random.normal(input_shape)\n+  >>> y = tf.keras.layers.add([x1, x2])\n+  >>> print(y.shape)\n+  (2, 3, 4)\n+\n+  Used in a functional model:\n+\n+  >>> input1 = tf.keras.layers.Input(shape=(16,))\n+  >>> x1 = tf.keras.layers.Dense(8, activation='relu')(input1)\n+  >>> input2 = tf.keras.layers.Input(shape=(32,))\n+  >>> x2 = tf.keras.layers.Dense(8, activation='relu')(input2)\n+  >>> added = tf.keras.layers.add([x1, x2])\n+  >>> out = tf.keras.layers.Dense(4)(added)\n+  >>> model = tf.keras.models.Model(inputs=[input1, input2], outputs=out)\n+\n+  \"\"\"\n+  return Add(**kwargs)(inputs)\n\n@@ -0,0 +1,93 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Layer that averages several inputs.\"\"\"\n+# pylint: disable=g-direct-tensorflow-import\n+\n+from keras.layers.merging.base_merge import _Merge\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.Average')\n+class Average(_Merge):\n+  \"\"\"Layer that averages a list of inputs element-wise.\n+\n+  It takes as input a list of tensors, all of the same shape, and returns\n+  a single tensor (also of the same shape).\n+\n+  Example:\n+\n+  >>> x1 = np.ones((2, 2))\n+  >>> x2 = np.zeros((2, 2))\n+  >>> y = tf.keras.layers.Average()([x1, x2])\n+  >>> y.numpy().tolist()\n+  [[0.5, 0.5], [0.5, 0.5]]\n+\n+  Usage in a functional model:\n+\n+  >>> input1 = tf.keras.layers.Input(shape=(16,))\n+  >>> x1 = tf.keras.layers.Dense(8, activation='relu')(input1)\n+  >>> input2 = tf.keras.layers.Input(shape=(32,))\n+  >>> x2 = tf.keras.layers.Dense(8, activation='relu')(input2)\n+  >>> avg = tf.keras.layers.Average()([x1, x2])\n+  >>> out = tf.keras.layers.Dense(4)(avg)\n+  >>> model = tf.keras.models.Model(inputs=[input1, input2], outputs=out)\n+\n+  Raises:\n+    ValueError: If there is a shape mismatch between the inputs and the shapes\n+      cannot be broadcasted to match.\n+  \"\"\"\n+\n+  def _merge_function(self, inputs):\n+    output = inputs[0]\n+    for i in range(1, len(inputs)):\n+      output += inputs[i]\n+    return output / len(inputs)\n+\n+\n+@keras_export('keras.layers.average')\n+def average(inputs, **kwargs):\n+  \"\"\"Functional interface to the `tf.keras.layers.Average` layer.\n+\n+  Example:\n+\n+  >>> x1 = np.ones((2, 2))\n+  >>> x2 = np.zeros((2, 2))\n+  >>> y = tf.keras.layers.Average()([x1, x2])\n+  >>> y.numpy().tolist()\n+  [[0.5, 0.5], [0.5, 0.5]]\n+\n+  Usage in a functional model:\n+\n+  >>> input1 = tf.keras.layers.Input(shape=(16,))\n+  >>> x1 = tf.keras.layers.Dense(8, activation='relu')(input1)\n+  >>> input2 = tf.keras.layers.Input(shape=(32,))\n+  >>> x2 = tf.keras.layers.Dense(8, activation='relu')(input2)\n+  >>> avg = tf.keras.layers.Average()([x1, x2])\n+  >>> out = tf.keras.layers.Dense(4)(avg)\n+  >>> model = tf.keras.models.Model(inputs=[input1, input2], outputs=out)\n+\n+  Args:\n+      inputs: A list of input tensors (at least 2).\n+      **kwargs: Standard layer keyword arguments.\n+\n+  Returns:\n+      A tensor, the average of the inputs.\n+\n+  Raises:\n+    ValueError: If there is a shape mismatch between the inputs and the shapes\n+      cannot be broadcasted to match.\n+  \"\"\"\n+  return Average(**kwargs)(inputs)\n\n@@ -0,0 +1,218 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Private base class for layers that can merge several inputs into one.\"\"\"\n+\n+from keras import backend\n+from keras.engine.base_layer import Layer\n+from keras.utils import tf_utils\n+import tensorflow.compat.v2 as tf\n+\n+\n+class _Merge(Layer):\n+  \"\"\"Generic merge layer for elementwise merge functions.\n+\n+  Used to implement `Sum`, `Average`, etc.\n+  \"\"\"\n+\n+  def __init__(self, **kwargs):\n+    \"\"\"Initializes a Merge layer.\n+\n+    Args:\n+      **kwargs: standard layer keyword arguments.\n+    \"\"\"\n+    super(_Merge, self).__init__(**kwargs)\n+    self.supports_masking = True\n+\n+  def _merge_function(self, inputs):\n+    raise NotImplementedError\n+\n+  def _compute_elemwise_op_output_shape(self, shape1, shape2):\n+    \"\"\"Computes the shape of the resultant of an elementwise operation.\n+\n+    Args:\n+        shape1: tuple or None. Shape of the first tensor\n+        shape2: tuple or None. Shape of the second tensor\n+\n+    Returns:\n+        expected output shape when an element-wise operation is\n+        carried out on 2 tensors with shapes shape1 and shape2.\n+        tuple or None.\n+\n+    Raises:\n+        ValueError: if shape1 and shape2 are not compatible for\n+            element-wise operations.\n+    \"\"\"\n+    if None in [shape1, shape2]:\n+      return None\n+    elif len(shape1) < len(shape2):\n+      return self._compute_elemwise_op_output_shape(shape2, shape1)  # pylint: disable=arguments-out-of-order\n+    elif not shape2:\n+      return shape1\n+    output_shape = list(shape1[:-len(shape2)])\n+    for i, j in zip(shape1[-len(shape2):], shape2):\n+      if i is None or j is None:\n+        output_shape.append(None)\n+      elif i == 1:\n+        output_shape.append(j)\n+      elif j == 1:\n+        output_shape.append(i)\n+      else:\n+        if i != j:\n+          raise ValueError(\n+              'Inputs have incompatible shapes. '\n+              f'Received shapes {shape1} and {shape2}')\n+        output_shape.append(i)\n+    return tuple(output_shape)\n+\n+  @tf_utils.shape_type_conversion\n+  def build(self, input_shape):\n+    # Used purely for shape validation.\n+    if not isinstance(input_shape[0], tuple):\n+      raise ValueError(\n+          'A merge layer should be called on a list of inputs. '\n+          f'Received: input_shape={input_shape} (not a list of shapes)')\n+    if len(input_shape) < 2:\n+      raise ValueError('A merge layer should be called '\n+                       'on a list of at least 2 inputs. '\n+                       f'Got {len(input_shape)} inputs. '\n+                       f'Full input_shape received: {input_shape}')\n+    batch_sizes = {s[0] for s in input_shape if s} - {None}\n+    if len(batch_sizes) > 1:\n+      raise ValueError(\n+          'Cannot merge tensors with different batch sizes. '\n+          f'Got tensors with shapes {input_shape}')\n+    if input_shape[0] is None:\n+      output_shape = None\n+    else:\n+      output_shape = input_shape[0][1:]\n+    for i in range(1, len(input_shape)):\n+      if input_shape[i] is None:\n+        shape = None\n+      else:\n+        shape = input_shape[i][1:]\n+      output_shape = self._compute_elemwise_op_output_shape(output_shape, shape)\n+    # If the inputs have different ranks, we have to reshape them\n+    # to make them broadcastable.\n+    if None not in input_shape and len(set(map(len, input_shape))) == 1:\n+      self._reshape_required = False\n+    else:\n+      self._reshape_required = True\n+\n+  def call(self, inputs):\n+    if not isinstance(inputs, (list, tuple)):\n+      raise ValueError(\n+          'A merge layer should be called on a list of inputs. '\n+          f'Received: inputs={inputs} (not a list of tensors)')\n+    if self._reshape_required:\n+      reshaped_inputs = []\n+      input_ndims = list(map(backend.ndim, inputs))\n+      if None not in input_ndims:\n+        # If ranks of all inputs are available,\n+        # we simply expand each of them at axis=1\n+        # until all of them have the same rank.\n+        max_ndim = max(input_ndims)\n+        for x in inputs:\n+          x_ndim = backend.ndim(x)\n+          for _ in range(max_ndim - x_ndim):\n+            x = tf.expand_dims(x, axis=1)\n+          reshaped_inputs.append(x)\n+        return self._merge_function(reshaped_inputs)\n+      else:\n+        # Transpose all inputs so that batch size is the last dimension.\n+        # (batch_size, dim1, dim2, ... ) -> (dim1, dim2, ... , batch_size)\n+        transposed = False\n+        for x in inputs:\n+          x_ndim = backend.ndim(x)\n+          if x_ndim is None:\n+            x_shape = tf.shape(x)\n+            batch_size = x_shape[0]\n+            new_shape = backend.concatenate(\n+                [x_shape[1:],\n+                 tf.expand_dims(batch_size, axis=-1)])\n+            x_transposed = tf.reshape(\n+                x,\n+                tf.stack(\n+                    [batch_size, tf.reduce_prod(x_shape[1:])], axis=0))\n+            x_transposed = tf.transpose(x_transposed, perm=(1, 0))\n+            x_transposed = tf.reshape(x_transposed, new_shape)\n+            reshaped_inputs.append(x_transposed)\n+            transposed = True\n+          elif x_ndim > 1:\n+            dims = list(range(1, x_ndim)) + [0]\n+            reshaped_inputs.append(tf.transpose(x, perm=dims))\n+            transposed = True\n+          else:\n+            # We don't transpose inputs if they are 1D vectors or scalars.\n+            reshaped_inputs.append(x)\n+        y = self._merge_function(reshaped_inputs)\n+        y_ndim = backend.ndim(y)\n+        if transposed:\n+          # If inputs have been transposed, we have to transpose the output too.\n+          if y_ndim is None:\n+            y_shape = tf.shape(y)\n+            y_ndim = tf.shape(y_shape)[0]\n+            batch_size = y_shape[y_ndim - 1]\n+            new_shape = backend.concatenate([\n+                tf.expand_dims(batch_size, axis=-1), y_shape[:y_ndim - 1]\n+            ])\n+            y = tf.reshape(y, (-1, batch_size))\n+            y = tf.transpose(y, perm=(1, 0))\n+            y = tf.reshape(y, new_shape)\n+          elif y_ndim > 1:\n+            dims = [y_ndim - 1] + list(range(y_ndim - 1))\n+            y = tf.transpose(y, perm=dims)\n+        return y\n+    else:\n+      return self._merge_function(inputs)\n+\n+  @tf_utils.shape_type_conversion\n+  def compute_output_shape(self, input_shape):\n+    if input_shape[0] is None:\n+      output_shape = None\n+    else:\n+      output_shape = input_shape[0][1:]\n+    for i in range(1, len(input_shape)):\n+      if input_shape[i] is None:\n+        shape = None\n+      else:\n+        shape = input_shape[i][1:]\n+      output_shape = self._compute_elemwise_op_output_shape(output_shape, shape)\n+    batch_sizes = {s[0] for s in input_shape if s is not None} - {None}\n+    if len(batch_sizes) == 1:\n+      output_shape = (list(batch_sizes)[0],) + output_shape\n+    else:\n+      output_shape = (None,) + output_shape\n+    return output_shape\n+\n+  def compute_mask(self, inputs, mask=None):\n+    if mask is None:\n+      return None\n+    if not isinstance(mask, (tuple, list)):\n+      raise ValueError(f'`mask` should be a list. Received: mask={mask}')\n+    if not isinstance(inputs, (tuple, list)):\n+      raise ValueError(f'`inputs` should be a list. Received: inputs={inputs}')\n+    if len(mask) != len(inputs):\n+      raise ValueError(\n+          'The lists `inputs` and `mask` should have the same length. '\n+          f'Received: inputs={inputs} of length {len(inputs)}, and '\n+          f'mask={mask} of length {len(mask)}')\n+    if all(m is None for m in mask):\n+      return None\n+    masks = [tf.expand_dims(m, axis=0) for m in mask if m is not None]\n+    return backend.all(\n+        backend.concatenate(masks, axis=0), axis=0, keepdims=False)\n+\n+  def get_config(self):  # pylint: disable=useless-super-delegation\n+    return super().get_config()\n\n@@ -0,0 +1,217 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Layer that concatenates several inputs.\"\"\"\n+# pylint: disable=g-direct-tensorflow-import\n+\n+from keras import backend\n+from keras.layers.merging.base_merge import _Merge\n+from keras.utils import tf_utils\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.Concatenate')\n+class Concatenate(_Merge):\n+  \"\"\"Layer that concatenates a list of inputs.\n+\n+  It takes as input a list of tensors, all of the same shape except\n+  for the concatenation axis, and returns a single tensor that is the\n+  concatenation of all inputs.\n+\n+  >>> x = np.arange(20).reshape(2, 2, 5)\n+  >>> print(x)\n+  [[[ 0  1  2  3  4]\n+    [ 5  6  7  8  9]]\n+   [[10 11 12 13 14]\n+    [15 16 17 18 19]]]\n+  >>> y = np.arange(20, 30).reshape(2, 1, 5)\n+  >>> print(y)\n+  [[[20 21 22 23 24]]\n+   [[25 26 27 28 29]]]\n+  >>> tf.keras.layers.Concatenate(axis=1)([x, y])\n+  <tf.Tensor: shape=(2, 3, 5), dtype=int64, numpy=\n+  array([[[ 0,  1,  2,  3,  4],\n+          [ 5,  6,  7,  8,  9],\n+          [20, 21, 22, 23, 24]],\n+         [[10, 11, 12, 13, 14],\n+          [15, 16, 17, 18, 19],\n+          [25, 26, 27, 28, 29]]])>\n+\n+  >>> x1 = tf.keras.layers.Dense(8)(np.arange(10).reshape(5, 2))\n+  >>> x2 = tf.keras.layers.Dense(8)(np.arange(10, 20).reshape(5, 2))\n+  >>> concatted = tf.keras.layers.Concatenate()([x1, x2])\n+  >>> concatted.shape\n+  TensorShape([5, 16])\n+\n+  \"\"\"\n+\n+  def __init__(self, axis=-1, **kwargs):\n+    \"\"\"Instantiates a Concatenate layer.\n+\n+    >>> x = np.arange(20).reshape(2, 2, 5)\n+    >>> print(x)\n+    [[[ 0  1  2  3  4]\n+      [ 5  6  7  8  9]]\n+     [[10 11 12 13 14]\n+      [15 16 17 18 19]]]\n+    >>> y = np.arange(20, 30).reshape(2, 1, 5)\n+    >>> print(y)\n+    [[[20 21 22 23 24]]\n+     [[25 26 27 28 29]]]\n+    >>> tf.keras.layers.Concatenate(axis=1)([x, y])\n+    <tf.Tensor: shape=(2, 3, 5), dtype=int64, numpy=\n+    array([[[ 0,  1,  2,  3,  4],\n+            [ 5,  6,  7,  8,  9],\n+            [20, 21, 22, 23, 24]],\n+           [[10, 11, 12, 13, 14],\n+            [15, 16, 17, 18, 19],\n+            [25, 26, 27, 28, 29]]])>\n+\n+    Args:\n+      axis: Axis along which to concatenate.\n+      **kwargs: standard layer keyword arguments.\n+    \"\"\"\n+    super(Concatenate, self).__init__(**kwargs)\n+    self.axis = axis\n+    self.supports_masking = True\n+    self._reshape_required = False\n+\n+  @tf_utils.shape_type_conversion\n+  def build(self, input_shape):\n+    # Used purely for shape validation.\n+    if not isinstance(input_shape[0], tuple) or len(input_shape) < 1:\n+      raise ValueError(\n+          'A `Concatenate` layer should be called on a list of '\n+          f'at least 1 input. Received: input_shape={input_shape}')\n+    if all(shape is None for shape in input_shape):\n+      return\n+    reduced_inputs_shapes = [list(shape) for shape in input_shape]\n+    shape_set = set()\n+    for i in range(len(reduced_inputs_shapes)):\n+      del reduced_inputs_shapes[i][self.axis]\n+      shape_set.add(tuple(reduced_inputs_shapes[i]))\n+\n+    if len(shape_set) != 1:\n+      err_msg = ('A `Concatenate` layer requires inputs with matching shapes '\n+                 'except for the concatenation axis. '\n+                 f'Received: input_shape={input_shape}')\n+      # Make sure all the shapes have same ranks.\n+      ranks = set(len(shape) for shape in shape_set)\n+      if len(ranks) != 1:\n+        raise ValueError(err_msg)\n+      # Get the only rank for the set.\n+      (rank,) = ranks\n+      for axis in range(rank):\n+        # Skip the Nones in the shape since they are dynamic, also the axis for\n+        # concat has been removed above.\n+        unique_dims = set(\n+            shape[axis] for shape in shape_set if shape[axis] is not None)\n+        if len(unique_dims) > 1:\n+          raise ValueError(err_msg)\n+\n+  def _merge_function(self, inputs):\n+    return backend.concatenate(inputs, axis=self.axis)\n+\n+  @tf_utils.shape_type_conversion\n+  def compute_output_shape(self, input_shape):\n+    if ((not isinstance(input_shape, (tuple, list))) or\n+        (not isinstance(input_shape[0], (tuple, list)))):\n+      # The tf_utils.shape_type_conversion decorator turns tensorshapes\n+      # into tuples, so we need to verify that `input_shape` is a list/tuple,\n+      # *and* that the individual elements are themselves shape tuples.\n+      raise ValueError(\n+          'A `Concatenate` layer should be called on a list of inputs. '\n+          f'Received: input_shape={input_shape}')\n+    input_shapes = input_shape\n+    output_shape = list(input_shapes[0])\n+    for shape in input_shapes[1:]:\n+      if output_shape[self.axis] is None or shape[self.axis] is None:\n+        output_shape[self.axis] = None\n+        break\n+      output_shape[self.axis] += shape[self.axis]\n+    return tuple(output_shape)\n+\n+  def compute_mask(self, inputs, mask=None):\n+    if mask is None:\n+      return None\n+    if not isinstance(mask, (tuple, list)):\n+      raise ValueError(f'`mask` should be a list. Received mask={mask}')\n+    if not isinstance(inputs, (tuple, list)):\n+      raise ValueError(f'`inputs` should be a list. Received: inputs={inputs}')\n+    if len(mask) != len(inputs):\n+      raise ValueError(\n+          'The lists `inputs` and `mask` should have the same length. '\n+          f'Received: inputs={inputs} of length {len(inputs)}, and '\n+          f'mask={mask} of length {len(mask)}')\n+    if all(m is None for m in mask):\n+      return None\n+    # Make a list of masks while making sure\n+    # the dimensionality of each mask\n+    # is the same as the corresponding input.\n+    masks = []\n+    for input_i, mask_i in zip(inputs, mask):\n+      if mask_i is None:\n+        # Input is unmasked. Append all 1s to masks,\n+        masks.append(tf.ones_like(input_i, dtype='bool'))\n+      elif backend.ndim(mask_i) < backend.ndim(input_i):\n+        # Mask is smaller than the input, expand it\n+        masks.append(tf.expand_dims(mask_i, axis=-1))\n+      else:\n+        masks.append(mask_i)\n+    concatenated = backend.concatenate(masks, axis=self.axis)\n+    return backend.all(concatenated, axis=-1, keepdims=False)\n+\n+  def get_config(self):\n+    config = {\n+        'axis': self.axis,\n+    }\n+    base_config = super(Concatenate, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+@keras_export('keras.layers.concatenate')\n+def concatenate(inputs, axis=-1, **kwargs):\n+  \"\"\"Functional interface to the `Concatenate` layer.\n+\n+  >>> x = np.arange(20).reshape(2, 2, 5)\n+  >>> print(x)\n+  [[[ 0  1  2  3  4]\n+    [ 5  6  7  8  9]]\n+   [[10 11 12 13 14]\n+    [15 16 17 18 19]]]\n+  >>> y = np.arange(20, 30).reshape(2, 1, 5)\n+  >>> print(y)\n+  [[[20 21 22 23 24]]\n+   [[25 26 27 28 29]]]\n+  >>> tf.keras.layers.concatenate([x, y],\n+  ...                             axis=1)\n+  <tf.Tensor: shape=(2, 3, 5), dtype=int64, numpy=\n+  array([[[ 0,  1,  2,  3,  4],\n+        [ 5,  6,  7,  8,  9],\n+        [20, 21, 22, 23, 24]],\n+       [[10, 11, 12, 13, 14],\n+        [15, 16, 17, 18, 19],\n+        [25, 26, 27, 28, 29]]])>\n+\n+  Args:\n+      inputs: A list of input tensors (at least 2).\n+      axis: Concatenation axis.\n+      **kwargs: Standard layer keyword arguments.\n+\n+  Returns:\n+      A tensor, the concatenation of the inputs alongside axis `axis`.\n+  \"\"\"\n+  return Concatenate(axis=axis, **kwargs)(inputs)\n\n@@ -0,0 +1,214 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Layer that computes the dot product between two inputs.\"\"\"\n+# pylint: disable=g-direct-tensorflow-import\n+\n+from keras import backend\n+from keras.engine import base_layer_utils\n+from keras.layers.merging.base_merge import _Merge\n+from keras.utils import tf_utils\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.Dot')\n+class Dot(_Merge):\n+  \"\"\"Layer that computes a dot product between samples in two tensors.\n+\n+  E.g. if applied to a list of two tensors `a` and `b` of shape\n+  `(batch_size, n)`, the output will be a tensor of shape `(batch_size, 1)`\n+  where each entry `i` will be the dot product between\n+  `a[i]` and `b[i]`.\n+\n+  >>> x = np.arange(10).reshape(1, 5, 2)\n+  >>> print(x)\n+  [[[0 1]\n+    [2 3]\n+    [4 5]\n+    [6 7]\n+    [8 9]]]\n+  >>> y = np.arange(10, 20).reshape(1, 2, 5)\n+  >>> print(y)\n+  [[[10 11 12 13 14]\n+    [15 16 17 18 19]]]\n+  >>> tf.keras.layers.Dot(axes=(1, 2))([x, y])\n+  <tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=\n+  array([[[260, 360],\n+          [320, 445]]])>\n+\n+  >>> x1 = tf.keras.layers.Dense(8)(np.arange(10).reshape(5, 2))\n+  >>> x2 = tf.keras.layers.Dense(8)(np.arange(10, 20).reshape(5, 2))\n+  >>> dotted = tf.keras.layers.Dot(axes=1)([x1, x2])\n+  >>> dotted.shape\n+  TensorShape([5, 1])\n+\n+\n+  \"\"\"\n+\n+  def __init__(self, axes, normalize=False, **kwargs):\n+    \"\"\"Initializes a layer that computes the element-wise dot product.\n+\n+      >>> x = np.arange(10).reshape(1, 5, 2)\n+      >>> print(x)\n+      [[[0 1]\n+        [2 3]\n+        [4 5]\n+        [6 7]\n+        [8 9]]]\n+      >>> y = np.arange(10, 20).reshape(1, 2, 5)\n+      >>> print(y)\n+      [[[10 11 12 13 14]\n+        [15 16 17 18 19]]]\n+      >>> tf.keras.layers.Dot(axes=(1, 2))([x, y])\n+      <tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=\n+      array([[[260, 360],\n+              [320, 445]]])>\n+\n+    Args:\n+      axes: Integer or tuple of integers,\n+        axis or axes along which to take the dot product. If a tuple, should\n+        be two integers corresponding to the desired axis from the first input\n+        and the desired axis from the second input, respectively. Note that the\n+        size of the two selected axes must match.\n+      normalize: Whether to L2-normalize samples along the\n+        dot product axis before taking the dot product.\n+        If set to True, then the output of the dot product\n+        is the cosine proximity between the two samples.\n+      **kwargs: Standard layer keyword arguments.\n+    \"\"\"\n+    super(Dot, self).__init__(**kwargs)\n+    if not isinstance(axes, int):\n+      if not isinstance(axes, (list, tuple)):\n+        raise TypeError(\n+            'Invalid type for argument `axes`: it should be '\n+            f'a list or an int. Received: axes={axes}')\n+      if len(axes) != 2:\n+        raise ValueError(\n+            'Invalid format for argument `axes`: it should contain two '\n+            f'elements. Received: axes={axes}')\n+      if not isinstance(axes[0], int) or not isinstance(axes[1], int):\n+        raise ValueError(\n+            'Invalid format for argument `axes`: list elements should be '\n+            f'integers. Received: axes={axes}')\n+    self.axes = axes\n+    self.normalize = normalize\n+    self.supports_masking = True\n+    self._reshape_required = False\n+\n+  @tf_utils.shape_type_conversion\n+  def build(self, input_shape):\n+    # Used purely for shape validation.\n+    if not isinstance(input_shape[0], tuple) or len(input_shape) != 2:\n+      raise ValueError(\n+          'A `Dot` layer should be called on a list of 2 inputs. '\n+          f'Received: input_shape={input_shape}')\n+    shape1 = input_shape[0]\n+    shape2 = input_shape[1]\n+    if shape1 is None or shape2 is None:\n+      return\n+    if isinstance(self.axes, int):\n+      if self.axes < 0:\n+        axes = [self.axes % len(shape1), self.axes % len(shape2)]\n+      else:\n+        axes = [self.axes] * 2\n+    else:\n+      axes = self.axes\n+    if shape1[axes[0]] != shape2[axes[1]]:\n+      raise ValueError(\n+          'Incompatible input shapes: '\n+          f'axis values {shape1[axes[0]]} (at axis {axes[0]}) != '\n+          f'{shape2[axes[1]]} (at axis {axes[1]}). '\n+          f'Full input shapes: {shape1}, {shape2}')\n+\n+  def _merge_function(self, inputs):\n+    base_layer_utils.no_ragged_support(inputs, self.name)\n+    if len(inputs) != 2:\n+      raise ValueError(\n+          'A `Dot` layer should be called on exactly 2 inputs. '\n+          f'Received: inputs={inputs}')\n+    x1 = inputs[0]\n+    x2 = inputs[1]\n+    if isinstance(self.axes, int):\n+      if self.axes < 0:\n+        axes = [self.axes % backend.ndim(x1), self.axes % backend.ndim(x2)]\n+      else:\n+        axes = [self.axes] * 2\n+    else:\n+      axes = []\n+      for i in range(len(self.axes)):\n+        if self.axes[i] < 0:\n+          axes.append(self.axes[i] % backend.ndim(inputs[i]))\n+        else:\n+          axes.append(self.axes[i])\n+    if self.normalize:\n+      x1 = tf.linalg.l2_normalize(x1, axis=axes[0])\n+      x2 = tf.linalg.l2_normalize(x2, axis=axes[1])\n+    output = backend.batch_dot(x1, x2, axes)\n+    return output\n+\n+  @tf_utils.shape_type_conversion\n+  def compute_output_shape(self, input_shape):\n+    if not isinstance(input_shape, (tuple, list)) or len(input_shape) != 2:\n+      raise ValueError(\n+          'A `Dot` layer should be called on a list of 2 inputs. '\n+          f'Received: input_shape={input_shape}')\n+    shape1 = list(input_shape[0])\n+    shape2 = list(input_shape[1])\n+    if isinstance(self.axes, int):\n+      if self.axes < 0:\n+        axes = [self.axes % len(shape1), self.axes % len(shape2)]\n+      else:\n+        axes = [self.axes] * 2\n+    else:\n+      axes = self.axes\n+    shape1.pop(axes[0])\n+    shape2.pop(axes[1])\n+    shape2.pop(0)\n+    output_shape = shape1 + shape2\n+    if len(output_shape) == 1:\n+      output_shape += [1]\n+    return tuple(output_shape)\n+\n+  def compute_mask(self, inputs, mask=None):\n+    return None\n+\n+  def get_config(self):\n+    config = {\n+        'axes': self.axes,\n+        'normalize': self.normalize,\n+    }\n+    base_config = super(Dot, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+@keras_export('keras.layers.dot')\n+def dot(inputs, axes, normalize=False, **kwargs):\n+  \"\"\"Functional interface to the `Dot` layer.\n+\n+  Args:\n+      inputs: A list of input tensors (at least 2).\n+      axes: Integer or tuple of integers,\n+          axis or axes along which to take the dot product.\n+      normalize: Whether to L2-normalize samples along the\n+          dot product axis before taking the dot product.\n+          If set to True, then the output of the dot product\n+          is the cosine proximity between the two samples.\n+      **kwargs: Standard layer keyword arguments.\n+\n+  Returns:\n+      A tensor, the dot product of the samples from the inputs.\n+  \"\"\"\n+  return Dot(axes=axes, normalize=normalize, **kwargs)(inputs)\n\n@@ -0,0 +1,83 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Layer that computes the maximum (element-wise) of several inputs.\"\"\"\n+# pylint: disable=g-direct-tensorflow-import\n+\n+from keras.layers.merging.base_merge import _Merge\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.Maximum')\n+class Maximum(_Merge):\n+  \"\"\"Layer that computes the maximum (element-wise) a list of inputs.\n+\n+  It takes as input a list of tensors, all of the same shape, and returns\n+  a single tensor (also of the same shape).\n+\n+  >>> tf.keras.layers.Maximum()([np.arange(5).reshape(5, 1),\n+  ...                            np.arange(5, 10).reshape(5, 1)])\n+  <tf.Tensor: shape=(5, 1), dtype=int64, numpy=\n+  array([[5],\n+       [6],\n+       [7],\n+       [8],\n+       [9]])>\n+\n+  >>> x1 = tf.keras.layers.Dense(8)(np.arange(10).reshape(5, 2))\n+  >>> x2 = tf.keras.layers.Dense(8)(np.arange(10, 20).reshape(5, 2))\n+  >>> maxed = tf.keras.layers.Maximum()([x1, x2])\n+  >>> maxed.shape\n+  TensorShape([5, 8])\n+  \"\"\"\n+\n+  def _merge_function(self, inputs):\n+    output = inputs[0]\n+    for i in range(1, len(inputs)):\n+      output = tf.maximum(output, inputs[i])\n+    return output\n+\n+\n+@keras_export('keras.layers.maximum')\n+def maximum(inputs, **kwargs):\n+  \"\"\"Functional interface to compute maximum (element-wise) list of `inputs`.\n+\n+  This is equivalent to the `tf.keras.layers.Maximum` layer.\n+\n+  For example:\n+\n+  ```python\n+  input1 = tf.keras.layers.Input(shape=(16,))\n+  x1 = tf.keras.layers.Dense(8, activation='relu')(input1) #shape=(None, 8)\n+  input2 = tf.keras.layers.Input(shape=(32,))\n+  x2 = tf.keras.layers.Dense(8, activation='relu')(input2) #shape=(None, 8)\n+  max_inp=tf.keras.layers.maximum([x1,x2]) #shape=(None, 8)\n+  out = tf.keras.layers.Dense(4)(max_inp)\n+  model = tf.keras.models.Model(inputs=[input1, input2], outputs=out)\n+  ```\n+\n+  Args:\n+      inputs: A list of input tensors (at least 2) of same shape.\n+      **kwargs: Standard layer keyword arguments.\n+\n+  Returns:\n+      A tensor (of same shape as input tensor) with the element-wise\n+      maximum of the inputs.\n+\n+  Raises:\n+      ValueError: If input tensors are of different shape.\n+  \"\"\"\n+  return Maximum(**kwargs)(inputs)\n\n@@ -12,24 +12,22 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Tests for merge layers.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n+\"\"\"Tests for merging layers.\"\"\"\n \n from absl.testing import parameterized\n-import numpy as np\n-\n import keras\n from keras import backend\n from keras import combinations\n from keras import keras_parameterized\n from keras import testing_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n \n \n @keras_parameterized.run_all_keras_modes\n-class MergeLayersTest(keras_parameterized.TestCase):\n+class MergingLayersTest(keras_parameterized.TestCase):\n \n-  def test_merge_add(self):\n+  def test_add(self):\n     i1 = keras.layers.Input(shape=(4, 5))\n     i2 = keras.layers.Input(shape=(4, 5))\n     i3 = keras.layers.Input(shape=(4, 5))\n@@ -47,8 +45,7 @@ class MergeLayersTest(keras_parameterized.TestCase):\n     self.assertEqual(out.shape, (2, 4, 5))\n     self.assertAllClose(out, x1 + x2 + x3, atol=1e-4)\n \n-    self.assertEqual(\n-        add_layer.compute_mask([i1, i2, i3], [None, None, None]), None)\n+    self.assertIsNone(add_layer.compute_mask([i1, i2, i3], [None, None, None]))\n     self.assertTrue(\n         np.all(\n             backend.eval(\n@@ -62,7 +59,7 @@ class MergeLayersTest(keras_parameterized.TestCase):\n     with self.assertRaisesRegex(ValueError, ' should have the same length.'):\n       add_layer.compute_mask([i1, i2, i3], [None, None])\n \n-  def test_merge_subtract(self):\n+  def test_subtract(self):\n     i1 = keras.layers.Input(shape=(4, 5))\n     i2 = keras.layers.Input(shape=(4, 5))\n     i3 = keras.layers.Input(shape=(4, 5))\n@@ -79,7 +76,7 @@ class MergeLayersTest(keras_parameterized.TestCase):\n     self.assertEqual(out.shape, (2, 4, 5))\n     self.assertAllClose(out, x1 - x2, atol=1e-4)\n \n-    self.assertEqual(subtract_layer.compute_mask([i1, i2], [None, None]), None)\n+    self.assertIsNone(subtract_layer.compute_mask([i1, i2], [None, None]))\n     self.assertTrue(\n         np.all(\n             backend.eval(\n@@ -97,7 +94,7 @@ class MergeLayersTest(keras_parameterized.TestCase):\n                                 'layer should be called on exactly 2 inputs'):\n       subtract_layer([i1])\n \n-  def test_merge_multiply(self):\n+  def test_multiply(self):\n     i1 = keras.layers.Input(shape=(4, 5))\n     i2 = keras.layers.Input(shape=(4, 5))\n     i3 = keras.layers.Input(shape=(4, 5))\n@@ -113,7 +110,7 @@ class MergeLayersTest(keras_parameterized.TestCase):\n     self.assertEqual(out.shape, (2, 4, 5))\n     self.assertAllClose(out, x1 * x2 * x3, atol=1e-4)\n \n-  def test_merge_average(self):\n+  def test_average(self):\n     i1 = keras.layers.Input(shape=(4, 5))\n     i2 = keras.layers.Input(shape=(4, 5))\n     o = keras.layers.average([i1, i2])\n@@ -127,7 +124,7 @@ class MergeLayersTest(keras_parameterized.TestCase):\n     self.assertEqual(out.shape, (2, 4, 5))\n     self.assertAllClose(out, 0.5 * (x1 + x2), atol=1e-4)\n \n-  def test_merge_maximum(self):\n+  def test_maximum(self):\n     i1 = keras.layers.Input(shape=(4, 5))\n     i2 = keras.layers.Input(shape=(4, 5))\n     o = keras.layers.maximum([i1, i2])\n@@ -141,7 +138,7 @@ class MergeLayersTest(keras_parameterized.TestCase):\n     self.assertEqual(out.shape, (2, 4, 5))\n     self.assertAllClose(out, np.maximum(x1, x2), atol=1e-4)\n \n-  def test_merge_minimum(self):\n+  def test_minimum(self):\n     i1 = keras.layers.Input(shape=(4, 5))\n     i2 = keras.layers.Input(shape=(4, 5))\n     o = keras.layers.minimum([i1, i2])\n@@ -155,7 +152,7 @@ class MergeLayersTest(keras_parameterized.TestCase):\n     self.assertEqual(out.shape, (2, 4, 5))\n     self.assertAllClose(out, np.minimum(x1, x2), atol=1e-4)\n \n-  def test_merge_concatenate(self):\n+  def test_concatenate(self):\n     i1 = keras.layers.Input(shape=(4, 5))\n     i2 = keras.layers.Input(shape=(4, 5))\n     concat_layer = keras.layers.Concatenate(axis=1)\n@@ -170,7 +167,7 @@ class MergeLayersTest(keras_parameterized.TestCase):\n     self.assertEqual(out.shape, (2, 8, 5))\n     self.assertAllClose(out, np.concatenate([x1, x2], axis=1), atol=1e-4)\n \n-    self.assertEqual(concat_layer.compute_mask([i1, i2], [None, None]), None)\n+    self.assertIsNone(concat_layer.compute_mask([i1, i2], [None, None]))\n     self.assertTrue(\n         np.all(\n             backend.eval(\n@@ -191,7 +188,13 @@ class MergeLayersTest(keras_parameterized.TestCase):\n                                 'layer should be called on a list of inputs'):\n       concat_layer(i1)\n \n-  def test_merge_dot(self):\n+  def test_concatenate_numpy_inputs(self):\n+    if tf.executing_eagerly():\n+      layer = keras.layers.Concatenate()\n+      x, y = np.ones((10, 10)), np.ones((10, 10))\n+      self.assertAllEqual(np.ones((10, 20)), layer([x, y]))\n+\n+  def test_dot(self):\n     i1 = keras.layers.Input(shape=(4,))\n     i2 = keras.layers.Input(shape=(4,))\n     o = keras.layers.dot([i1, i2], axes=1)\n@@ -226,33 +229,44 @@ class MergeLayersTest(keras_parameterized.TestCase):\n       *testing_utils.generate_combinations_with_testcase_name(\n           layer=[keras.layers.Add, keras.layers.Subtract,\n                  keras.layers.Multiply, keras.layers.Minimum,\n-                 keras.layers.Maximum, keras.layers.Average,\n-                 keras.layers.Concatenate]))\n-  def test_merge_with_ragged_input(self, layer):\n+                 keras.layers.Maximum, keras.layers.Average]))\n+  def test_merging_with_ragged_input(self, layer):\n     ragged_data = tf.ragged.constant(\n         [[1., 1., 1.], [1., 1.], [1., 1., 1., 1.]], ragged_rank=1)\n     dense_data = ragged_data.to_tensor()\n     input1 = keras.Input(shape=(None,), ragged=True)\n     input2 = keras.Input(shape=(None,), ragged=True)\n-    out = keras.layers.Add()([input1, input2])\n+    out = layer()([input1, input2])\n     model = keras.models.Model(inputs=[input1, input2], outputs=out)\n     out_ragged = model.predict([ragged_data, ragged_data], steps=1)\n     out_ragged = convert_ragged_tensor_value(out_ragged).to_tensor()\n \n     input1 = keras.Input(shape=(None,))\n     input2 = keras.Input(shape=(None,))\n-    out = keras.layers.Add()([input1, input2])\n+    out = layer()([input1, input2])\n     model = keras.models.Model(inputs=[input1, input2], outputs=out)\n     out_dense = model.predict([dense_data, dense_data], steps=1)\n \n     self.assertAllEqual(out_dense, out_ragged)\n \n+  def test_concatenate_with_ragged_input(self):\n+    ragged1 = tf.ragged.constant([[1., 1.], [1.], [1., 1., 1.]], ragged_rank=1)\n+    ragged2 = tf.ragged.constant([[2., 2., 2.], [2.], [2., 2.]], ragged_rank=1)\n+    expected_concatenated_ragged = tf.ragged.constant(\n+        [[1., 1., 2., 2., 2.], [1., 2.], [1., 1., 1., 2., 2.]], ragged_rank=1)\n+    input1 = keras.Input(shape=(None,), ragged=True)\n+    input2 = keras.Input(shape=(None,), ragged=True)\n+    out = keras.layers.Concatenate(axis=1)([input1, input2])\n+    model = keras.models.Model(inputs=[input1, input2], outputs=out)\n+    out_ragged = model.predict([ragged1, ragged2], steps=1)\n+    self.assertAllEqual(out_ragged, expected_concatenated_ragged)\n+\n   @parameterized.named_parameters(\n       *testing_utils.generate_combinations_with_testcase_name(\n           layer=[keras.layers.Add, keras.layers.Subtract,\n                  keras.layers.Multiply, keras.layers.Minimum,\n                  keras.layers.Maximum, keras.layers.Average]))\n-  def test_merge_with_scalar_input(self, layer):\n+  def test_merging_with_scalar_input(self, layer):\n     x1 = np.array((1))\n     x2 = np.array((2))\n     out = layer()([x1, x2])\n@@ -260,9 +274,9 @@ class MergeLayersTest(keras_parameterized.TestCase):\n \n \n @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-class MergeLayersTestNoExecution(tf.test.TestCase):\n+class MergingLayersTestNoExecution(tf.test.TestCase):\n \n-  def test_merge_elementwise_errors(self):\n+  def test_add_elementwise_errors(self):\n     i1 = keras.layers.Input(shape=(4, 5))\n     i2 = keras.layers.Input(shape=(4, 6))\n     with self.assertRaises(ValueError):\n@@ -326,7 +340,7 @@ class MergeLayersTestNoExecution(tf.test.TestCase):\n       dot = keras.layers.Dot(1)\n       dot.compute_output_shape(1)\n \n-  def test_merge_subtract(self):\n+  def test_subtract(self):\n     i1 = keras.layers.Input(shape=(4, 5))\n     i2 = keras.layers.Input(shape=(4, 5))\n     y = keras.layers.subtract([i1, i2])\n@@ -340,7 +354,7 @@ class MergeLayersTestNoExecution(tf.test.TestCase):\n     with self.assertRaises(ValueError):\n       keras.layers.subtract([i1, i1, i1])\n \n-  def test_merge_add_masking(self):\n+  def test_add_masking(self):\n     i1 = keras.layers.Input(shape=(4, 5))\n     i2 = keras.layers.Input(shape=(4, 5))\n     m1 = keras.layers.Masking()(i1)\n@@ -350,14 +364,14 @@ class MergeLayersTestNoExecution(tf.test.TestCase):\n     mask = layer.output_mask\n     self.assertListEqual(mask.shape.as_list(), [None, 4])\n \n-  def test_merge_add_dynamic_shape(self):\n+  def test_add_dynamic_shape(self):\n     i1 = keras.Input(batch_shape=(4, None), dtype='float32')\n     i2 = keras.Input(batch_shape=(4, 5), dtype='float32')\n     layer = keras.layers.Add()\n     o = layer([i1, i2])\n     self.assertListEqual(o.shape.as_list(), [4, 5])\n \n-  def test_merge_concatenate_masking(self):\n+  def test_concatenate_masking(self):\n     i1 = keras.layers.Input(shape=(4, 5))\n     i2 = keras.layers.Input(shape=(4, 5))\n     m1 = keras.layers.Masking()(i1)\n@@ -367,7 +381,7 @@ class MergeLayersTestNoExecution(tf.test.TestCase):\n     mask = layer.output_mask\n     self.assertListEqual(mask.shape.as_list(), [None, 4])\n \n-  def test_merge_concatenate_sparse_shape(self):\n+  def test_concatenate_sparse_shape(self):\n     i1 = keras.layers.Input(shape=(1,), batch_size=2, sparse=True)\n     i2 = keras.layers.Input(shape=(2,), batch_size=2, sparse=True)\n     layer = keras.layers.Concatenate(axis=1)\n@@ -381,7 +395,7 @@ class MergeLayersTestNoExecution(tf.test.TestCase):\n     o = layer([i1, i2])\n     self.assertListEqual(o.shape.as_list(), [None, 3])\n \n-  def test_user_changes_to_input_structure(self):\n+  def test_concatenate_user_changes_to_input_structure(self):\n     a = keras.layers.Input(shape=(4, 5))\n     struct = [a, a]\n     concat1 = keras.layers.Concatenate(1)\n\n@@ -0,0 +1,65 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Layer that computes the minimum (element-wise) of several inputs.\"\"\"\n+# pylint: disable=g-direct-tensorflow-import\n+\n+from keras.layers.merging.base_merge import _Merge\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.Minimum')\n+class Minimum(_Merge):\n+  \"\"\"Layer that computes the minimum (element-wise) a list of inputs.\n+\n+  It takes as input a list of tensors, all of the same shape, and returns\n+  a single tensor (also of the same shape).\n+\n+  >>> tf.keras.layers.Minimum()([np.arange(5).reshape(5, 1),\n+  ...                            np.arange(5, 10).reshape(5, 1)])\n+  <tf.Tensor: shape=(5, 1), dtype=int64, numpy=\n+  array([[0],\n+       [1],\n+       [2],\n+       [3],\n+       [4]])>\n+\n+  >>> x1 = tf.keras.layers.Dense(8)(np.arange(10).reshape(5, 2))\n+  >>> x2 = tf.keras.layers.Dense(8)(np.arange(10, 20).reshape(5, 2))\n+  >>> minned = tf.keras.layers.Minimum()([x1, x2])\n+  >>> minned.shape\n+  TensorShape([5, 8])\n+  \"\"\"\n+\n+  def _merge_function(self, inputs):\n+    output = inputs[0]\n+    for i in range(1, len(inputs)):\n+      output = tf.minimum(output, inputs[i])\n+    return output\n+\n+\n+@keras_export('keras.layers.minimum')\n+def minimum(inputs, **kwargs):\n+  \"\"\"Functional interface to the `Minimum` layer.\n+\n+  Args:\n+      inputs: A list of input tensors (at least 2).\n+      **kwargs: Standard layer keyword arguments.\n+\n+  Returns:\n+      A tensor, the element-wise minimum of the inputs.\n+  \"\"\"\n+  return Minimum(**kwargs)(inputs)\n\n@@ -0,0 +1,81 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Layer that multiplies (element-wise) several inputs.\"\"\"\n+# pylint: disable=g-direct-tensorflow-import\n+\n+from keras.layers.merging.base_merge import _Merge\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.Multiply')\n+class Multiply(_Merge):\n+  \"\"\"Layer that multiplies (element-wise) a list of inputs.\n+\n+  It takes as input a list of tensors, all of the same shape, and returns\n+  a single tensor (also of the same shape).\n+\n+  >>> tf.keras.layers.Multiply()([np.arange(5).reshape(5, 1),\n+  ...                             np.arange(5, 10).reshape(5, 1)])\n+  <tf.Tensor: shape=(5, 1), dtype=int64, numpy=\n+  array([[ 0],\n+       [ 6],\n+       [14],\n+       [24],\n+       [36]])>\n+\n+  >>> x1 = tf.keras.layers.Dense(8)(np.arange(10).reshape(5, 2))\n+  >>> x2 = tf.keras.layers.Dense(8)(np.arange(10, 20).reshape(5, 2))\n+  >>> multiplied = tf.keras.layers.Multiply()([x1, x2])\n+  >>> multiplied.shape\n+  TensorShape([5, 8])\n+  \"\"\"\n+\n+  def _merge_function(self, inputs):\n+    output = inputs[0]\n+    for i in range(1, len(inputs)):\n+      output = output * inputs[i]\n+    return output\n+\n+\n+@keras_export('keras.layers.multiply')\n+def multiply(inputs, **kwargs):\n+  \"\"\"Functional interface to the `Multiply` layer.\n+\n+  Example:\n+\n+  >>> x1 = np.arange(3.0)\n+  >>> x2 = np.arange(3.0)\n+  >>> tf.keras.layers.multiply([x1, x2])\n+  <tf.Tensor: shape=(3,), dtype=float32, numpy=array([0., 1., 4.], ...)>\n+\n+  Usage in a functional model:\n+\n+  >>> input1 = tf.keras.layers.Input(shape=(16,))\n+  >>> x1 = tf.keras.layers.Dense(8, activation='relu')(input1) #shape=(None, 8)\n+  >>> input2 = tf.keras.layers.Input(shape=(32,))\n+  >>> x2 = tf.keras.layers.Dense(8, activation='relu')(input2) #shape=(None, 8)\n+  >>> out = tf.keras.layers.multiply([x1,x2]) #shape=(None, 8)\n+  >>> out = tf.keras.layers.Dense(4)(out)\n+  >>> model = tf.keras.models.Model(inputs=[input1, input2], outputs=out)\n+\n+  Args:\n+      inputs: A list of input tensors (at least 2).\n+      **kwargs: Standard layer keyword arguments.\n+\n+  Returns:\n+      A tensor, the element-wise product of the inputs.\n+  \"\"\"\n+  return Multiply(**kwargs)(inputs)\n\n@@ -0,0 +1,91 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Layer that subtracts two inputs.\"\"\"\n+# pylint: disable=g-direct-tensorflow-import\n+\n+from keras.layers.merging.base_merge import _Merge\n+from keras.utils import tf_utils\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.Subtract')\n+class Subtract(_Merge):\n+  \"\"\"Layer that subtracts two inputs.\n+\n+  It takes as input a list of tensors of size 2,\n+  both of the same shape, and returns a single tensor, (inputs[0] - inputs[1]),\n+  also of the same shape.\n+\n+  Examples:\n+\n+  ```python\n+      import keras\n+\n+      input1 = keras.layers.Input(shape=(16,))\n+      x1 = keras.layers.Dense(8, activation='relu')(input1)\n+      input2 = keras.layers.Input(shape=(32,))\n+      x2 = keras.layers.Dense(8, activation='relu')(input2)\n+      # Equivalent to subtracted = keras.layers.subtract([x1, x2])\n+      subtracted = keras.layers.Subtract()([x1, x2])\n+\n+      out = keras.layers.Dense(4)(subtracted)\n+      model = keras.models.Model(inputs=[input1, input2], outputs=out)\n+  ```\n+  \"\"\"\n+\n+  @tf_utils.shape_type_conversion\n+  def build(self, input_shape):\n+    super(Subtract, self).build(input_shape)\n+    if len(input_shape) != 2:\n+      raise ValueError(\n+          'A `Subtract` layer should be called on exactly 2 inputs. '\n+          f'Received: input_shape={input_shape}')\n+\n+  def _merge_function(self, inputs):\n+    if len(inputs) != 2:\n+      raise ValueError(\n+          'A `Subtract` layer should be called on exactly 2 inputs. '\n+          f'Received: inputs={inputs}')\n+    return inputs[0] - inputs[1]\n+\n+\n+@keras_export('keras.layers.subtract')\n+def subtract(inputs, **kwargs):\n+  \"\"\"Functional interface to the `Subtract` layer.\n+\n+  Args:\n+      inputs: A list of input tensors (exactly 2).\n+      **kwargs: Standard layer keyword arguments.\n+\n+  Returns:\n+      A tensor, the difference of the inputs.\n+\n+  Examples:\n+\n+  ```python\n+      import keras\n+\n+      input1 = keras.layers.Input(shape=(16,))\n+      x1 = keras.layers.Dense(8, activation='relu')(input1)\n+      input2 = keras.layers.Input(shape=(32,))\n+      x2 = keras.layers.Dense(8, activation='relu')(input2)\n+      subtracted = keras.layers.subtract([x1, x2])\n+\n+      out = keras.layers.Dense(4)(subtracted)\n+      model = keras.models.Model(inputs=[input1, input2], outputs=out)\n+  ```\n+  \"\"\"\n+  return Subtract(**kwargs)(inputs)\n\n@@ -24,7 +24,7 @@ from keras.engine import base_preprocessing_layer\n from keras.engine.input_layer import Input\n from keras.layers import convolutional\n from keras.layers import core\n-from keras.layers import merge\n+from keras.layers import merging\n from keras.layers.preprocessing import image_preprocessing\n from keras.layers.preprocessing import normalization\n from keras.layers.preprocessing import preprocessing_stage\n@@ -400,11 +400,11 @@ class PreprocessingStageTest(keras_parameterized.TestCase,\n     x1 = Input(shape=(10, 10, 3))\n     x2 = Input(shape=(10, 10, 3))\n \n-    y0 = merge.Add()([x0, x1])\n+    y0 = merging.Add()([x0, x1])\n     y1 = image_preprocessing.CenterCrop(8, 8)(x2)\n     y1 = convolutional.ZeroPadding2D(padding=1)(y1)\n \n-    z = merge.Add()([y0, y1])\n+    z = merging.Add()([y0, y1])\n     z = normalization.Normalization()(z)\n     z = convolutional.Conv2D(4, 3)(z)\n \n\n@@ -30,7 +30,7 @@ from keras.layers import dense_attention\n from keras.layers import einsum_dense\n from keras.layers import embeddings\n from keras.layers import local\n-from keras.layers import merge\n+from keras.layers import merging\n from keras.layers import multi_head_attention\n from keras.layers import noise\n from keras.layers import pooling\n@@ -59,7 +59,7 @@ from tensorflow.python.util.tf_export import keras_export\n \n ALL_MODULES = (base_layer, input_layer, advanced_activations, convolutional,\n                convolutional_recurrent, core, cudnn_recurrent, dense_attention,\n-               embeddings, einsum_dense, local, merge, noise,\n+               embeddings, einsum_dense, local, merging, noise,\n                batch_normalization_v1, layer_normalization, unit_normalization,\n                pooling, image_preprocessing, recurrent, reshaping, wrappers,\n                hashing, hashed_crossing, category_encoding, discretization,\n@@ -132,15 +132,15 @@ def populate_deserializable_objects():\n     from keras.feature_column.dense_features import DenseFeatures  # pylint: disable=g-import-not-at-top\n     LOCAL.ALL_OBJECTS['DenseFeatures'] = DenseFeatures\n \n-  # Merge layers, function versions.\n-  LOCAL.ALL_OBJECTS['add'] = merge.add\n-  LOCAL.ALL_OBJECTS['subtract'] = merge.subtract\n-  LOCAL.ALL_OBJECTS['multiply'] = merge.multiply\n-  LOCAL.ALL_OBJECTS['average'] = merge.average\n-  LOCAL.ALL_OBJECTS['maximum'] = merge.maximum\n-  LOCAL.ALL_OBJECTS['minimum'] = merge.minimum\n-  LOCAL.ALL_OBJECTS['concatenate'] = merge.concatenate\n-  LOCAL.ALL_OBJECTS['dot'] = merge.dot\n+  # Merging layers, function versions.\n+  LOCAL.ALL_OBJECTS['add'] = merging.add\n+  LOCAL.ALL_OBJECTS['subtract'] = merging.subtract\n+  LOCAL.ALL_OBJECTS['multiply'] = merging.multiply\n+  LOCAL.ALL_OBJECTS['average'] = merging.average\n+  LOCAL.ALL_OBJECTS['maximum'] = merging.maximum\n+  LOCAL.ALL_OBJECTS['minimum'] = merging.minimum\n+  LOCAL.ALL_OBJECTS['concatenate'] = merging.concatenate\n+  LOCAL.ALL_OBJECTS['dot'] = merging.dot\n \n \n @keras_export('keras.layers.serialize')\n\n@@ -28,7 +28,7 @@ from keras.layers import core\n from keras.layers import dense_attention\n from keras.layers import embeddings\n from keras.layers import local\n-from keras.layers import merge\n+from keras.layers import merging\n from keras.layers import noise\n from keras.layers import pooling\n from keras.layers import recurrent\n@@ -119,14 +119,14 @@ class LayerCorrectnessTest(keras_parameterized.TestCase):\n       ('LocallyConnected1D', lambda: local.LocallyConnected1D(2, 2), (2, 2, 1)),\n       ('LocallyConnected2D', lambda: local.LocallyConnected2D(2, 2),\n        (2, 2, 2, 1)),\n-      ('Add', merge.Add, [(2, 2), (2, 2)]),\n-      ('Subtract', merge.Subtract, [(2, 2), (2, 2)]),\n-      ('Multiply', merge.Multiply, [(2, 2), (2, 2)]),\n-      ('Average', merge.Average, [(2, 2), (2, 2)]),\n-      ('Maximum', merge.Maximum, [(2, 2), (2, 2)]),\n-      ('Minimum', merge.Minimum, [(2, 2), (2, 2)]),\n-      ('Concatenate', merge.Concatenate, [(2, 2), (2, 2)]),\n-      ('Dot', lambda: merge.Dot(1), [(2, 2), (2, 2)]),\n+      ('Add', merging.Add, [(2, 2), (2, 2)]),\n+      ('Subtract', merging.Subtract, [(2, 2), (2, 2)]),\n+      ('Multiply', merging.Multiply, [(2, 2), (2, 2)]),\n+      ('Average', merging.Average, [(2, 2), (2, 2)]),\n+      ('Maximum', merging.Maximum, [(2, 2), (2, 2)]),\n+      ('Minimum', merging.Minimum, [(2, 2), (2, 2)]),\n+      ('Concatenate', merging.Concatenate, [(2, 2), (2, 2)]),\n+      ('Dot', lambda: merging.Dot(1), [(2, 2), (2, 2)]),\n       ('GaussianNoise', lambda: noise.GaussianNoise(0.5), (2, 2)),\n       ('GaussianDropout', lambda: noise.GaussianDropout(0.5), (2, 2)),\n       ('AlphaDropout', lambda: noise.AlphaDropout(0.5), (2, 2)),\n\n@@ -14,11 +14,11 @@\n # ==============================================================================\n \"\"\"Utilities for multi-gpu training.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n from keras import backend\n from keras.engine.training import Model\n from keras.layers.core.lambda_layer import Lambda\n-from keras.layers.merge import concatenate\n+from keras.layers.merging import concatenate\n+import tensorflow.compat.v2 as tf\n \n \n def _get_available_devices():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
