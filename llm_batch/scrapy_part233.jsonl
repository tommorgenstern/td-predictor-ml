{"custom_id": "scrapy#efa50039ec4dc8d5cd5103e64c47ad8c86b9ed4e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 75 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 76 | Churn Cumulative: 1224 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,6 @@\n from six.moves.urllib.parse import urlparse\n from unittest import TestCase\n+import warnings\n \n from scrapy.exceptions import NotConfigured\n from scrapy.http import Response, Request\n@@ -400,6 +401,79 @@ class TestRequestMetaPredecence003(MixinUnsafeUrl, TestRefererMiddleware):\n     req_meta = {'referrer_policy': POLICY_UNSAFE_URL}\n \n \n+class TestRequestMetaSettingFallback(TestCase):\n+\n+    params = [\n+        (\n+            # When an unknown policy is referenced in Request.meta\n+            # (here, a typo error),\n+            # the policy defined in settings takes precedence\n+            {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy'},\n+            {},\n+            {'referrer_policy': 'ssscrapy-default'},\n+            OriginWhenCrossOriginPolicy,\n+            True\n+        ),\n+        (\n+            # same as above but with string value for settings policy\n+            {'REFERRER_POLICY': 'origin-when-cross-origin'},\n+            {},\n+            {'referrer_policy': 'ssscrapy-default'},\n+            OriginWhenCrossOriginPolicy,\n+            True\n+        ),\n+        (\n+            # request meta references a wrong policy but it is set,\n+            # so the Referrer-Policy header in response is not used,\n+            # and the settings' policy is applied\n+            {'REFERRER_POLICY': 'origin-when-cross-origin'},\n+            {'Referrer-Policy': 'unsafe-url'},\n+            {'referrer_policy': 'ssscrapy-default'},\n+            OriginWhenCrossOriginPolicy,\n+            True\n+        ),\n+        (\n+            # here, request meta does not set the policy\n+            # so response headers take precedence\n+            {'REFERRER_POLICY': 'origin-when-cross-origin'},\n+            {'Referrer-Policy': 'unsafe-url'},\n+            {},\n+            UnsafeUrlPolicy,\n+            False\n+        ),\n+        (\n+            # here, request meta does not set the policy,\n+            # but response headers also use an unknown policy,\n+            # so the settings' policy is used\n+            {'REFERRER_POLICY': 'origin-when-cross-origin'},\n+            {'Referrer-Policy': 'unknown'},\n+            {},\n+            OriginWhenCrossOriginPolicy,\n+            True\n+        )\n+    ]\n+\n+    def test(self):\n+\n+        origin = 'http://www.scrapy.org'\n+        target = 'http://www.example.com'\n+\n+        for settings, response_headers, request_meta, policy_class, check_warning in self.params[3:]:\n+            spider = Spider('foo')\n+            mw = RefererMiddleware(Settings(settings))\n+\n+            response = Response(origin, headers=response_headers)\n+            request = Request(target, meta=request_meta)\n+\n+            with warnings.catch_warnings(record=True) as w:\n+                policy = mw.policy(response, request)\n+                self.assertIsInstance(policy, policy_class)\n+\n+                if check_warning:\n+                    self.assertEqual(len(w), 1)\n+                    self.assertEqual(w[0].category, RuntimeWarning, w[0].message)\n+\n+\n class TestSettingsPolicyByName(TestCase):\n \n     def test_valid_name(self):\n@@ -436,7 +510,7 @@ class TestSettingsPolicyByName(TestCase):\n \n     def test_invalid_name(self):\n         settings = Settings({'REFERRER_POLICY': 'some-custom-unknown-policy'})\n-        with self.assertRaises(NotConfigured):\n+        with self.assertRaises(RuntimeError):\n             mw = RefererMiddleware(settings)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2d55d838ca1e2936e5729e67a6ab34a4bb58ba3b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 7 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 942 | Contributors (this commit): 6 | Commits (past 90d): 1 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -274,7 +274,6 @@ class StripUrl(unittest.TestCase):\n              'http://www.example.com/'),\n             ]:\n             self.assertEqual(strip_url(input_url, origin_only=origin), output_url)\n-            self.assertEqual(strip_url(urlparse(input_url), origin_only=origin), output_url)\n \n     def test_credentials(self):\n         for i, o in [\n@@ -288,7 +287,6 @@ class StripUrl(unittest.TestCase):\n              'ftp://www.example.com/index.html?somekey=somevalue'),\n             ]:\n             self.assertEqual(strip_url(i, strip_credentials=True), o)\n-            self.assertEqual(strip_url(urlparse(i), strip_credentials=True), o)\n \n     def test_credentials_encoded_delims(self):\n         for i, o in [\n@@ -308,7 +306,6 @@ class StripUrl(unittest.TestCase):\n              'ftp://www.example.com/index.html?somekey=somevalue'),\n             ]:\n             self.assertEqual(strip_url(i, strip_credentials=True), o)\n-            self.assertEqual(strip_url(urlparse(i), strip_credentials=True), o)\n \n     def test_default_ports_creds_off(self):\n         for i, o in [\n@@ -337,7 +334,6 @@ class StripUrl(unittest.TestCase):\n              'ftp://www.example.com:221/file.txt'),\n             ]:\n             self.assertEqual(strip_url(i), o)\n-            self.assertEqual(strip_url(urlparse(i)), o)\n \n     def test_default_ports(self):\n         for i, o in [\n@@ -366,7 +362,6 @@ class StripUrl(unittest.TestCase):\n              'ftp://username:password@www.example.com:221/file.txt'),\n             ]:\n             self.assertEqual(strip_url(i, strip_default_port=True, strip_credentials=False), o)\n-            self.assertEqual(strip_url(urlparse(i), strip_default_port=True, strip_credentials=False), o)\n \n     def test_default_ports_keep(self):\n         for i, o in [\n@@ -395,7 +390,6 @@ class StripUrl(unittest.TestCase):\n              'ftp://username:password@www.example.com:221/file.txt'),\n             ]:\n             self.assertEqual(strip_url(i, strip_default_port=False, strip_credentials=False), o)\n-            self.assertEqual(strip_url(urlparse(i), strip_default_port=False, strip_credentials=False), o)\n \n     def test_origin_only(self):\n         for i, o in [\n@@ -412,7 +406,6 @@ class StripUrl(unittest.TestCase):\n              'https://www.example.com/'),\n             ]:\n             self.assertEqual(strip_url(i, origin_only=True), o)\n-            self.assertEqual(strip_url(urlparse(i), origin_only=True), o)\n \n \n if __name__ == \"__main__\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a70ec30e19eff13d3bf18e5c392612db018d1b4f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 21 | Churn Cumulative: 247 | Contributors (this commit): 7 | Commits (past 90d): 1 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -40,7 +40,6 @@ class FileException(Exception):\n \n \n class FSFilesStore(object):\n-\n     def __init__(self, basedir):\n         if '://' in basedir:\n             basedir = basedir.split('://', 1)[1]\n@@ -79,9 +78,12 @@ class FSFilesStore(object):\n \n \n class S3FilesStore(object):\n-\n     AWS_ACCESS_KEY_ID = None\n     AWS_SECRET_ACCESS_KEY = None\n+    AWS_ENDPOINT_URL = None\n+    AWS_REGION_NAME = None\n+    AWS_USE_SSL = None\n+    AWS_VERIFY = None\n \n     POLICY = 'private'  # Overriden from settings.FILES_STORE_S3_ACL in\n                         # FilesPipeline.from_settings.\n@@ -95,8 +97,14 @@ class S3FilesStore(object):\n             import botocore.session\n             session = botocore.session.get_session()\n             self.s3_client = session.create_client(\n-                's3', aws_access_key_id=self.AWS_ACCESS_KEY_ID,\n-                aws_secret_access_key=self.AWS_SECRET_ACCESS_KEY)\n+                's3',\n+                aws_access_key_id=self.AWS_ACCESS_KEY_ID,\n+                aws_secret_access_key=self.AWS_SECRET_ACCESS_KEY,\n+                endpoint_url=self.AWS_ENDPOINT_URL,\n+                region_name=self.AWS_REGION_NAME,\n+                use_ssl=self.AWS_USE_SSL,\n+                verify=self.AWS_VERIFY\n+            )\n         else:\n             from boto.s3.connection import S3Connection\n             self.S3Connection = S3Connection\n@@ -256,6 +264,10 @@ class FilesPipeline(MediaPipeline):\n         s3store = cls.STORE_SCHEMES['s3']\n         s3store.AWS_ACCESS_KEY_ID = settings['AWS_ACCESS_KEY_ID']\n         s3store.AWS_SECRET_ACCESS_KEY = settings['AWS_SECRET_ACCESS_KEY']\n+        s3store.AWS_ENDPOINT_URL = settings['AWS_ENDPOINT_URL']\n+        s3store.AWS_REGION_NAME = settings['AWS_REGION_NAME']\n+        s3store.AWS_USE_SSL = settings['AWS_USE_SSL']\n+        s3store.AWS_VERIFY = settings['AWS_VERIFY']\n         s3store.POLICY = settings['FILES_STORE_S3_ACL']\n \n         store_uri = settings['FILES_STORE']\n@@ -423,4 +435,5 @@ class FilesPipeline(MediaPipeline):\n     # deprecated\n     def file_key(self, url):\n         return self.file_path(url)\n+\n     file_key._base = True\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#97d84d920bf97f3ec1becd291596b3f0ee966328", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 38 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,5 @@\n+import logging\n+\n from email.utils import formatdate\n from twisted.internet import defer\n from twisted.internet.error import TimeoutError, DNSLookupError, \\\n@@ -9,6 +11,9 @@ from scrapy.exceptions import NotConfigured, IgnoreRequest\n from scrapy.utils.misc import load_object\n \n \n+logger = logging.getLogger(__name__)\n+\n+\n class HttpCacheMiddleware(object):\n \n     DOWNLOAD_EXCEPTIONS = (defer.TimeoutError, TimeoutError, DNSLookupError,\n@@ -24,6 +29,8 @@ class HttpCacheMiddleware(object):\n         self.ignore_missing = settings.getbool('HTTPCACHE_IGNORE_MISSING')\n         self.stats = stats\n \n+        logger.debug(\"Using cache directory %(cachedir)s\" % {'cachedir': self.storage.cachedir})\n+\n     @classmethod\n     def from_crawler(cls, crawler):\n         o = cls(crawler.settings, crawler.stats)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f3b75c940d2fefc3ff5bb4435e507c5959ff903c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 5 | Churn Cumulative: 83 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -23,8 +23,13 @@ class SpiderLoader(object):\n \n     def _load_spiders(self, module):\n         for spcls in iter_spider_classes(module):\n+            if spcls.name in self._spiders.keys():\n+                import warnings\n+                warnings.warn(\"There are several spiders with the same name (\" + spcls.name +\n+                              \"), this can cause unexpected behavior\", UserWarning)\n         self._spiders[spcls.name] = spcls\n     \n+\n     def _load_all_spiders(self):\n         for name in self.spider_modules:\n             try:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5be5ef57f38b5d6f8ed01ccf9c9cac1e1e02fe03", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 84 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -29,7 +29,6 @@ class SpiderLoader(object):\n                               \"), this can cause unexpected behavior\", UserWarning)\n             self._spiders[spcls.name] = spcls\n \n-\n     def _load_all_spiders(self):\n         for name in self.spider_modules:\n             try:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e71803c833edd67400848a3b0d4dbb01e0ec80f7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 49 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 4 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 49 | Churn Cumulative: 165 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -101,3 +101,52 @@ class SpiderLoaderTest(unittest.TestCase):\n \n             spiders = spider_loader.list()\n             self.assertEqual(spiders, [])\n+\n+\n+class DuplicateSpiderNameLoaderTest(unittest.TestCase):\n+\n+    def setUp(self):\n+        orig_spiders_dir = os.path.join(module_dir, 'test_spiders')\n+        self.tmpdir = self.mktemp()\n+        os.mkdir(self.tmpdir)\n+        self.spiders_dir = os.path.join(self.tmpdir, 'test_spiders_xxx')\n+        shutil.copytree(orig_spiders_dir, self.spiders_dir)\n+        sys.path.append(self.tmpdir)\n+        self.settings = Settings({'SPIDER_MODULES': ['test_spiders_xxx']})\n+\n+    def tearDown(self):\n+        del sys.modules['test_spiders_xxx']\n+        sys.path.remove(self.tmpdir)\n+\n+    def test_dupename_warning(self):\n+        # copy 1 spider module so as to have duplicate spider name\n+        shutil.copyfile(os.path.join(self.tmpdir, 'test_spiders_xxx/spider3.py'),\n+                        os.path.join(self.tmpdir, 'test_spiders_xxx/spider3dupe.py'))\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            spider_loader = SpiderLoader.from_settings(self.settings)\n+\n+            self.assertEqual(len(w), 1)\n+            self.assertIn(\"several spiders with the same name (spider3)\", str(w[0].message))\n+\n+            spiders = set(spider_loader.list())\n+            self.assertEqual(spiders, set(['spider1', 'spider2', 'spider3']))\n+\n+    def test_multiple_dupename_warning(self):\n+        # copy 2 spider modules so as to have duplicate spider name\n+        # This should issue 2 warning, 1 for each duplicate spider name\n+        shutil.copyfile(os.path.join(self.tmpdir, 'test_spiders_xxx/spider1.py'),\n+                        os.path.join(self.tmpdir, 'test_spiders_xxx/spider1dupe.py'))\n+        shutil.copyfile(os.path.join(self.tmpdir, 'test_spiders_xxx/spider2.py'),\n+                        os.path.join(self.tmpdir, 'test_spiders_xxx/spider2dupe.py'))\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            spider_loader = SpiderLoader.from_settings(self.settings)\n+\n+            self.assertEqual(len(w), 2)\n+            msgs = sorted(str(wrn.message) for wrn in w)\n+            self.assertIn(\"several spiders with the same name (spider1)\", msgs[0])\n+            self.assertIn(\"several spiders with the same name (spider2)\", msgs[1])\n+\n+            spiders = set(spider_loader.list())\n+            self.assertEqual(spiders, set(['spider1', 'spider2', 'spider3']))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#12a8ddecab8c64923d1789571955699270255211", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 169 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -130,7 +130,7 @@ class DuplicateSpiderNameLoaderTest(unittest.TestCase):\n             self.assertIn(\"several spiders with the same name (spider3)\", str(w[0].message))\n \n             spiders = set(spider_loader.list())\n-            self.assertEqual(spiders, set(['spider1', 'spider2', 'spider3']))\n+            self.assertEqual(spiders, set(['spider1', 'spider2', 'spider3', 'spider4']))\n \n     def test_multiple_dupename_warning(self):\n         # copy 2 spider modules so as to have duplicate spider name\n@@ -149,4 +149,4 @@ class DuplicateSpiderNameLoaderTest(unittest.TestCase):\n             self.assertIn(\"several spiders with the same name (spider2)\", msgs[1])\n \n             spiders = set(spider_loader.list())\n-            self.assertEqual(spiders, set(['spider1', 'spider2', 'spider3']))\n+            self.assertEqual(spiders, set(['spider1', 'spider2', 'spider3', 'spider4']))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f96490df2ce532e3b8757b2464f5f3cc54181ce5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 8 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 194 | Contributors (this commit): 7 | Commits (past 90d): 3 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,3 @@\n-import logging\n-\n from email.utils import formatdate\n from twisted.internet import defer\n from twisted.internet.error import TimeoutError, DNSLookupError, \\\n@@ -11,9 +9,6 @@ from scrapy.exceptions import NotConfigured, IgnoreRequest\n from scrapy.utils.misc import load_object\n \n \n-logger = logging.getLogger(__name__)\n-\n-\n class HttpCacheMiddleware(object):\n \n     DOWNLOAD_EXCEPTIONS = (defer.TimeoutError, TimeoutError, DNSLookupError,\n@@ -29,8 +24,6 @@ class HttpCacheMiddleware(object):\n         self.ignore_missing = settings.getbool('HTTPCACHE_IGNORE_MISSING')\n         self.stats = stats\n \n-        logger.debug(\"Using cache directory %(cachedir)s\" % {'cachedir': self.storage.cachedir})\n-\n     @classmethod\n     def from_crawler(cls, crawler):\n         o = cls(crawler.settings, crawler.stats)\n\n@@ -1,6 +1,7 @@\n from __future__ import print_function\n import os\n import gzip\n+import logging\n from six.moves import cPickle as pickle\n from importlib import import_module\n from time import time\n@@ -15,6 +16,9 @@ from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.python import to_bytes, to_unicode\n \n \n+logger = logging.getLogger(__name__)\n+\n+\n class DummyPolicy(object):\n \n     def __init__(self, settings):\n@@ -216,6 +220,8 @@ class DbmCacheStorage(object):\n         self.dbmodule = import_module(settings['HTTPCACHE_DBM_MODULE'])\n         self.db = None\n \n+        logger.debug(\"Using DBM cache storage in %(cachedir)s\" % {'cachedir': self.cachedir})\n+\n     def open_spider(self, spider):\n         dbpath = os.path.join(self.cachedir, '%s.db' % spider.name)\n         self.db = self.dbmodule.open(dbpath, 'c')\n@@ -271,6 +277,8 @@ class FilesystemCacheStorage(object):\n         self.use_gzip = settings.getbool('HTTPCACHE_GZIP')\n         self._open = gzip.open if self.use_gzip else open\n \n+        logger.debug(\"Using filesystem cache storage in %(cachedir)s\" % {'cachedir': self.cachedir})\n+\n     def open_spider(self, spider):\n         pass\n \n@@ -344,6 +352,8 @@ class LeveldbCacheStorage(object):\n         self.expiration_secs = settings.getint('HTTPCACHE_EXPIRATION_SECS')\n         self.db = None\n \n+        logger.debug(\"Using LevelDB cache storage in %(cachedir)s\" % {'cachedir': self.cachedir})\n+\n     def open_spider(self, spider):\n         dbpath = os.path.join(self.cachedir, '%s.leveldb' % spider.name)\n         self.db = self._leveldb.LevelDB(dbpath)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b50d0370f4ebdb485f4625ef6d71398ac8538c79", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): -6/0 | Churn Δ: 10 | Churn Cumulative: 912 | Contributors (this commit): 12 | Commits (past 90d): 8 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -838,6 +838,16 @@ class DataURITestCase(unittest.TestCase):\n         self.download_request = self.download_handler.download_request\n         self.spider = Spider('foo')\n \n+    def test_response_attrs(self):\n+        uri = \"data:,A%20brief%20note\"\n+\n+        def _test(response):\n+            self.assertEquals(response.url, uri)\n+            self.assertFalse(response.headers)\n+\n+        request = Request(uri)\n+        return self.download_request(request, self.spider).addCallback(_test)\n+\n     def test_default_mediatype_encoding(self):\n         def _test(response):\n             self.assertEquals(response.text, 'A brief note')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c2c503192f24a198b4515ab006e3174e28062731", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 5 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 836 | Contributors (this commit): 2 | Commits (past 90d): 11 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -36,13 +36,13 @@ class ReferrerPolicy(object):\n     def referrer(self, response, request):\n         raise NotImplementedError()\n \n-    def stripped_referrer(self, r):\n-        if urlparse(r).scheme not in self.NOREFERRER_SCHEMES:\n-            return self.strip_url(r)\n+    def stripped_referrer(self, url):\n+        if urlparse(url).scheme not in self.NOREFERRER_SCHEMES:\n+            return self.strip_url(url)\n \n-    def origin_referrer(self, r):\n-        if urlparse(r).scheme not in self.NOREFERRER_SCHEMES:\n-            return self.origin(r)\n+    def origin_referrer(self, url):\n+        if urlparse(url).scheme not in self.NOREFERRER_SCHEMES:\n+            return self.origin(url)\n \n     def strip_url(self, url, origin_only=False):\n         \"\"\"\n@@ -66,9 +66,9 @@ class ReferrerPolicy(object):\n                          strip_default_port=True,\n                          origin_only=origin_only)\n \n-    def origin(self, r):\n+    def origin(self, url):\n         \"\"\"Return serialized origin (scheme, host, path) for a request or response URL.\"\"\"\n-        return self.strip_url(r, origin_only=True)\n+        return self.strip_url(url, origin_only=True)\n \n     def potentially_trustworthy(self, url):\n         # Note: this does not follow https://w3c.github.io/webappsec-secure-contexts/#is-url-trustworthy\n@@ -241,7 +241,7 @@ class UnsafeUrlPolicy(ReferrerPolicy):\n \n class LegacyPolicy(ReferrerPolicy):\n     def referrer(self, response, request):\n-        return response.url\n+        return response\n \n \n class DefaultReferrerPolicy(NoReferrerWhenDowngradePolicy):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#db176f872b91c4f51f10ea1e732875dc40a9e7bb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 5 | Churn Cumulative: 841 | Contributors (this commit): 2 | Commits (past 90d): 12 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -239,11 +239,6 @@ class UnsafeUrlPolicy(ReferrerPolicy):\n         return self.stripped_referrer(response)\n \n \n-class LegacyPolicy(ReferrerPolicy):\n-    def referrer(self, response, request):\n-        return response\n-\n-\n class DefaultReferrerPolicy(NoReferrerWhenDowngradePolicy):\n     \"\"\"\n     A variant of \"no-referrer-when-downgrade\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fad499ab602e7f12bf56a514f5aae1a2f80670f7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 29 | Files Changed: 1 | Hunks: 9 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 58 | Churn Cumulative: 899 | Contributors (this commit): 2 | Commits (past 90d): 13 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -33,7 +33,7 @@ class ReferrerPolicy(object):\n \n     NOREFERRER_SCHEMES = LOCAL_SCHEMES\n \n-    def referrer(self, response, request):\n+    def referrer(self, response_url, request_url):\n         raise NotImplementedError()\n \n     def stripped_referrer(self, url):\n@@ -91,7 +91,7 @@ class NoReferrerPolicy(ReferrerPolicy):\n     \"\"\"\n     name = POLICY_NO_REFERRER\n \n-    def referrer(self, response, request):\n+    def referrer(self, response_url, request_url):\n         return None\n \n \n@@ -111,9 +111,9 @@ class NoReferrerWhenDowngradePolicy(ReferrerPolicy):\n     \"\"\"\n     name = POLICY_NO_REFERRER_WHEN_DOWNGRADE\n \n-    def referrer(self, response, request):\n-        if not self.tls_protected(response) or self.tls_protected(request):\n-            return self.stripped_referrer(response)\n+    def referrer(self, response_url, request_url):\n+        if not self.tls_protected(response_url) or self.tls_protected(request_url):\n+            return self.stripped_referrer(response_url)\n \n \n class SameOriginPolicy(ReferrerPolicy):\n@@ -128,9 +128,9 @@ class SameOriginPolicy(ReferrerPolicy):\n     \"\"\"\n     name = POLICY_SAME_ORIGIN\n \n-    def referrer(self, response, request):\n-        if self.origin(response) == self.origin(request):\n-            return self.stripped_referrer(response)\n+    def referrer(self, response_url, request_url):\n+        if self.origin(response_url) == self.origin(request_url):\n+            return self.stripped_referrer(response_url)\n \n \n class OriginPolicy(ReferrerPolicy):\n@@ -144,8 +144,8 @@ class OriginPolicy(ReferrerPolicy):\n     \"\"\"\n     name = POLICY_ORIGIN\n \n-    def referrer(self, response, request):\n-        return self.origin_referrer(response)\n+    def referrer(self, response_url, request_url):\n+        return self.origin_referrer(response_url)\n \n \n class StrictOriginPolicy(ReferrerPolicy):\n@@ -163,11 +163,11 @@ class StrictOriginPolicy(ReferrerPolicy):\n     \"\"\"\n     name = POLICY_STRICT_ORIGIN\n \n-    def referrer(self, response, request):\n-        if ((self.tls_protected(response) and\n-             self.potentially_trustworthy(request))\n-            or not self.tls_protected(response)):\n-            return self.origin_referrer(response)\n+    def referrer(self, response_url, request_url):\n+        if ((self.tls_protected(response_url) and\n+             self.potentially_trustworthy(request_url))\n+            or not self.tls_protected(response_url)):\n+            return self.origin_referrer(response_url)\n \n \n class OriginWhenCrossOriginPolicy(ReferrerPolicy):\n@@ -183,10 +183,10 @@ class OriginWhenCrossOriginPolicy(ReferrerPolicy):\n     \"\"\"\n     name = POLICY_ORIGIN_WHEN_CROSS_ORIGIN\n \n-    def referrer(self, response, request):\n-        origin = self.origin(response)\n-        if origin == self.origin(request):\n-            return self.stripped_referrer(response)\n+    def referrer(self, response_url, request_url):\n+        origin = self.origin(response_url)\n+        if origin == self.origin(request_url):\n+            return self.stripped_referrer(response_url)\n         else:\n             return origin\n \n@@ -210,14 +210,14 @@ class StrictOriginWhenCrossOriginPolicy(ReferrerPolicy):\n     \"\"\"\n     name = POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN\n \n-    def referrer(self, response, request):\n-        origin = self.origin(response)\n-        if origin == self.origin(request):\n-            return self.stripped_referrer(response)\n-        elif ((self.tls_protected(response) and\n-               self.potentially_trustworthy(request))\n-              or not self.tls_protected(response)):\n-            return self.origin_referrer(response)\n+    def referrer(self, response_url, request_url):\n+        origin = self.origin(response_url)\n+        if origin == self.origin(request_url):\n+            return self.stripped_referrer(response_url)\n+        elif ((self.tls_protected(response_url) and\n+               self.potentially_trustworthy(request_url))\n+              or not self.tls_protected(response_url)):\n+            return self.origin_referrer(response_url)\n \n \n class UnsafeUrlPolicy(ReferrerPolicy):\n@@ -235,8 +235,8 @@ class UnsafeUrlPolicy(ReferrerPolicy):\n     \"\"\"\n     name = POLICY_UNSAFE_URL\n \n-    def referrer(self, response, request):\n-        return self.stripped_referrer(response)\n+    def referrer(self, response_url, request_url):\n+        return self.stripped_referrer(response_url)\n \n \n class DefaultReferrerPolicy(NoReferrerWhenDowngradePolicy):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#42b429dc37e17a3a24d9f5f6d9365b6d78479630", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 157 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -220,12 +220,12 @@ class DbmCacheStorage(object):\n         self.dbmodule = import_module(settings['HTTPCACHE_DBM_MODULE'])\n         self.db = None\n \n-        logger.debug(\"Using DBM cache storage in %(cachedir)s\" % {'cachedir': self.cachedir})\n-\n     def open_spider(self, spider):\n         dbpath = os.path.join(self.cachedir, '%s.db' % spider.name)\n         self.db = self.dbmodule.open(dbpath, 'c')\n \n+        logger.debug(\"Using DBM cache storage in %(cachepath)s\" % {'cachepath': dbpath}, extra={'spider': spider})\n+\n     def close_spider(self, spider):\n         self.db.close()\n \n@@ -352,12 +352,12 @@ class LeveldbCacheStorage(object):\n         self.expiration_secs = settings.getint('HTTPCACHE_EXPIRATION_SECS')\n         self.db = None\n \n-        logger.debug(\"Using LevelDB cache storage in %(cachedir)s\" % {'cachedir': self.cachedir})\n-\n     def open_spider(self, spider):\n         dbpath = os.path.join(self.cachedir, '%s.leveldb' % spider.name)\n         self.db = self._leveldb.LevelDB(dbpath)\n \n+        logger.debug(\"Using LevelDB cache storage in %(cachepath)s\" % {'cachepath': dbpath}, extra={'spider': spider})\n+\n     def close_spider(self, spider):\n         # Do compactation each time to save space and also recreate files to\n         # avoid them being removed in storages with timestamp-based autoremoval.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5e89db548419c4114ff71bd0504155c9a83868fa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 162 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -277,10 +277,9 @@ class FilesystemCacheStorage(object):\n         self.use_gzip = settings.getbool('HTTPCACHE_GZIP')\n         self._open = gzip.open if self.use_gzip else open\n \n-        logger.debug(\"Using filesystem cache storage in %(cachedir)s\" % {'cachedir': self.cachedir})\n-\n     def open_spider(self, spider):\n-        pass\n+        logger.debug(\"Using filesystem cache storage in %(cachedir)s\" % {'cachedir': self.cachedir},\n+                     extra={'spider': spider})\n \n     def close_spider(self, spider):\n         pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5d0058492c07d3f89f46ccfd2b8e33849f8bd30e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 36 | Churn Cumulative: 339 | Contributors (this commit): 8 | Commits (past 90d): 11 | Contributors (cumulative): 11 | DMM Complexity: 0.3076923076923077\n\nDIFF:\n@@ -249,7 +249,7 @@ class FilesPipeline(MediaPipeline):\n             resolve('FILES_RESULT_FIELD'), self.FILES_RESULT_FIELD\n         )\n \n-        super(FilesPipeline, self).__init__(download_func=download_func)\n+        super(FilesPipeline, self).__init__(download_func=download_func, settings=settings)\n \n     @classmethod\n     def from_settings(cls, settings):\n\n@@ -1,5 +1,6 @@\n from __future__ import print_function\n \n+import functools\n import logging\n from collections import defaultdict\n from twisted.internet.defer import Deferred, DeferredList\n@@ -16,6 +17,7 @@ logger = logging.getLogger(__name__)\n class MediaPipeline(object):\n \n     LOG_FAILED_RESULTS = True\n+    ALLOW_REDIRECTS = False\n \n     class SpiderInfo(object):\n         def __init__(self, spider):\n@@ -24,9 +26,16 @@ class MediaPipeline(object):\n             self.downloaded = {}\n             self.waiting = defaultdict(list)\n \n-    def __init__(self, download_func=None):\n+    def __init__(self, download_func=None, settings=None):\n         self.download_func = download_func\n-\n+        resolve = functools.partial(self._key_for_pipe,\n+                                    base_class_name=\"MediaPipeline\")\n+        self.allow_redirects = settings.getbool(\n+            resolve('MEDIA_ALLOW_REDIRECTS'), self.ALLOW_REDIRECTS\n+        )\n+        self.allow_httpstatus_list = settings.getlist(\n+            resolve('MEDIA_HTTPSTATUS_LIST'), []\n+        )\n \n     def _key_for_pipe(self, key, base_class_name=None,\n                       settings=None):\n@@ -93,6 +102,25 @@ class MediaPipeline(object):\n         )\n         return dfd.addBoth(lambda _: wad)  # it must return wad at last\n \n+    def _modify_media_request(self, request):\n+        httpstatus_list = []\n+        if self.allow_httpstatus_list:\n+            httpstatus_list = self.allow_httpstatus_list\n+        elif self.allow_redirects:\n+            if not httpstatus_list:\n+                httpstatus_list = list(range(0, 300)) + list(range(400, 1000))\n+            else:\n+                for i in range(300, 400):\n+                    try:\n+                        httpstatus_list.remove(i)\n+                    except ValueError:\n+                        pass\n+        if httpstatus_list:\n+            request.meta['handle_httpstatus_list'] = httpstatus_list\n+        else:\n+            request.meta['handle_httpstatus_all'] = True\n+        return request\n+\n     def _check_media_to_download(self, result, request, info):\n         if result is not None:\n             return result\n@@ -103,7 +131,7 @@ class MediaPipeline(object):\n                 callback=self.media_downloaded, callbackArgs=(request, info),\n                 errback=self.media_failed, errbackArgs=(request, info))\n         else:\n-            request.meta['handle_httpstatus_all'] = True\n+            request = self._modify_media_request(request)\n             dfd = self.crawler.engine.download(request, info.spider)\n             dfd.addCallbacks(\n                 callback=self.media_downloaded, callbackArgs=(request, info),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6a4221471610b7f3a5d44d93306294c21d550406", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 19 | Churn Cumulative: 50 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,6 +6,7 @@ from twisted.internet import reactor\n from twisted.internet.defer import Deferred, inlineCallbacks\n \n from scrapy.http import Request, Response\n+from scrapy.settings import Settings\n from scrapy.spiders import Spider\n from scrapy.utils.request import request_fingerprint\n from scrapy.pipelines.media import MediaPipeline\n@@ -25,7 +26,8 @@ class BaseMediaPipelineTestCase(unittest.TestCase):\n \n     def setUp(self):\n         self.spider = Spider('media.com')\n-        self.pipe = self.pipeline_class(download_func=_mocked_download_func)\n+        self.pipe = self.pipeline_class(download_func=_mocked_download_func,\n+                                        settings=Settings())\n         self.pipe.open_spider(self.spider)\n         self.info = self.pipe.spiderinfo\n \n@@ -82,6 +84,21 @@ class BaseMediaPipelineTestCase(unittest.TestCase):\n         new_item = yield self.pipe.process_item(item, self.spider)\n         assert new_item is item\n \n+    def test_modify_media_request(self):\n+        request = Request('http://url')\n+        assert self.pipe._modify_media_request(request).meta == {'handle_httpstatus_all': True}\n+\n+        request = Request('http://url')\n+        self.pipe.allow_httpstatus_list = list(range(100))\n+        assert self.pipe._modify_media_request(request).meta == {'handle_httpstatus_list': list(range(100))}\n+        self.pipe.allow_httpstatus_list = None\n+\n+        request = Request('http://url')\n+        self.pipe.allow_redirects = True\n+        correct = {'handle_httpstatus_list': list(range(300)) + list(range(400,1000))}\n+        assert self.pipe._modify_media_request(request).meta == correct\n+        self.pipe.allow_redirects = False\n+\n \n class MockedMediaPipeline(MediaPipeline):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2e052c86150502c7441c8f92cd1488de6232ec7f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 3 | Churn Cumulative: 93 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,6 +6,7 @@ from collections import defaultdict\n from twisted.internet.defer import Deferred, DeferredList\n from twisted.python.failure import Failure\n \n+from scrapy.settings import Settings\n from scrapy.utils.defer import mustbe_deferred, defer_result\n from scrapy.utils.request import request_fingerprint\n from scrapy.utils.misc import arg_to_iter\n@@ -28,6 +29,8 @@ class MediaPipeline(object):\n \n     def __init__(self, download_func=None, settings=None):\n         self.download_func = download_func\n+        if isinstance(settings, dict) or settings is None:\n+            settings = Settings(settings)\n         resolve = functools.partial(self._key_for_pipe,\n                                     base_class_name=\"MediaPipeline\")\n         self.allow_redirects = settings.getbool(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f0b4077f812619640df078a2485f8a460fafa58a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 64 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -57,6 +57,8 @@ class RedirectMiddleware(BaseRedirectMiddleware):\n     Handle redirection of requests based on response status\n     and meta-refresh html tag.\n     \"\"\"\n+    allowed_status = (301, 302, 303, 307)\n+\n     def process_response(self, request, response, spider):\n         if (request.meta.get('dont_redirect', False) or\n                 response.status in getattr(spider, 'handle_httpstatus_list', []) or\n@@ -64,8 +66,7 @@ class RedirectMiddleware(BaseRedirectMiddleware):\n                 request.meta.get('handle_httpstatus_all', False)):\n             return response\n \n-        allowed_status = (301, 302, 303, 307)\n-        if 'Location' not in response.headers or response.status not in allowed_status:\n+        if 'Location' not in response.headers or response.status not in self.allowed_status:\n             return response\n \n         location = safe_url_string(response.headers['location'])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3cef1cd451c8f28df8075aa4e3b65cedf5ecedfa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 12 | Files Changed: 2 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 23 | Churn Cumulative: 166 | Contributors (this commit): 5 | Commits (past 90d): 9 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,6 +6,7 @@ from collections import defaultdict\n from twisted.internet.defer import Deferred, DeferredList\n from twisted.python.failure import Failure\n \n+from scrapy.downloadermiddlewares.redirect import RedirectMiddleware\n from scrapy.settings import Settings\n from scrapy.utils.defer import mustbe_deferred, defer_result\n from scrapy.utils.request import request_fingerprint\n@@ -36,7 +37,7 @@ class MediaPipeline(object):\n         self.allow_redirects = settings.getbool(\n             resolve('MEDIA_ALLOW_REDIRECTS'), self.ALLOW_REDIRECTS\n         )\n-        self.allow_httpstatus_list = settings.getlist(\n+        self.handle_httpstatus_list = settings.getlist(\n             resolve('MEDIA_HTTPSTATUS_LIST'), []\n         )\n \n@@ -107,17 +108,15 @@ class MediaPipeline(object):\n \n     def _modify_media_request(self, request):\n         httpstatus_list = []\n-        if self.allow_httpstatus_list:\n-            httpstatus_list = self.allow_httpstatus_list\n-        elif self.allow_redirects:\n+        if self.handle_httpstatus_list:\n+            httpstatus_list = self.handle_httpstatus_list\n+        if self.allow_redirects:\n             if not httpstatus_list:\n-                httpstatus_list = list(range(0, 300)) + list(range(400, 1000))\n+                httpstatus_list = [i for i in range(1000)\n+                                   if i not in RedirectMiddleware.allowed_status]\n             else:\n-                for i in range(300, 400):\n-                    try:\n-                        httpstatus_list.remove(i)\n-                    except ValueError:\n-                        pass\n+                httpstatus_list = [i for i in httpstatus_list\n+                                   if i not in RedirectMiddleware.allowed_status]\n         if httpstatus_list:\n             request.meta['handle_httpstatus_list'] = httpstatus_list\n         else:\n\n@@ -89,9 +89,9 @@ class BaseMediaPipelineTestCase(unittest.TestCase):\n         assert self.pipe._modify_media_request(request).meta == {'handle_httpstatus_all': True}\n \n         request = Request('http://url')\n-        self.pipe.allow_httpstatus_list = list(range(100))\n+        self.pipe.handle_httpstatus_list = list(range(100))\n         assert self.pipe._modify_media_request(request).meta == {'handle_httpstatus_list': list(range(100))}\n-        self.pipe.allow_httpstatus_list = None\n+        self.pipe.handle_httpstatus_list = None\n \n         request = Request('http://url')\n         self.pipe.allow_redirects = True\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#11b31c9fbddd75fe7fec60152a1bec57acccc039", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 4 | Churn Cumulative: 58 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,6 +5,7 @@ from twisted.python.failure import Failure\n from twisted.internet import reactor\n from twisted.internet.defer import Deferred, inlineCallbacks\n \n+from scrapy.downloadermiddlewares.redirect import RedirectMiddleware\n from scrapy.http import Request, Response\n from scrapy.settings import Settings\n from scrapy.spiders import Spider\n@@ -95,7 +96,8 @@ class BaseMediaPipelineTestCase(unittest.TestCase):\n \n         request = Request('http://url')\n         self.pipe.allow_redirects = True\n-        correct = {'handle_httpstatus_list': list(range(300)) + list(range(400,1000))}\n+        correct = {'handle_httpstatus_list': [i for i in range(1000)\n+                                              if i not in RedirectMiddleware.allowed_status]}\n         assert self.pipe._modify_media_request(request).meta == correct\n         self.pipe.allow_redirects = False\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c64ebee06253ef385564c83806bd2422bff6f6d6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 14 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 28 | Churn Cumulative: 140 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -19,7 +19,6 @@ logger = logging.getLogger(__name__)\n class MediaPipeline(object):\n \n     LOG_FAILED_RESULTS = True\n-    ALLOW_REDIRECTS = False\n \n     class SpiderInfo(object):\n         def __init__(self, spider):\n@@ -35,12 +34,23 @@ class MediaPipeline(object):\n         resolve = functools.partial(self._key_for_pipe,\n                                     base_class_name=\"MediaPipeline\")\n         self.allow_redirects = settings.getbool(\n-            resolve('MEDIA_ALLOW_REDIRECTS'), self.ALLOW_REDIRECTS\n+            resolve('MEDIA_ALLOW_REDIRECTS'), False\n         )\n         self.handle_httpstatus_list = settings.getlist(\n             resolve('MEDIA_HTTPSTATUS_LIST'), []\n         )\n \n+        self.httpstatus_list = []\n+        if self.handle_httpstatus_list:\n+            self.httpstatus_list = self.handle_httpstatus_list\n+        if self.allow_redirects:\n+            if not self.httpstatus_list:\n+                self.httpstatus_list = [i for i in range(1000)\n+                                   if i not in RedirectMiddleware.allowed_status]\n+            else:\n+                self.httpstatus_list = [i for i in self.httpstatus_list\n+                                   if i not in RedirectMiddleware.allowed_status]\n+\n     def _key_for_pipe(self, key, base_class_name=None,\n                       settings=None):\n         \"\"\"\n@@ -107,18 +117,8 @@ class MediaPipeline(object):\n         return dfd.addBoth(lambda _: wad)  # it must return wad at last\n \n     def _modify_media_request(self, request):\n-        httpstatus_list = []\n-        if self.handle_httpstatus_list:\n-            httpstatus_list = self.handle_httpstatus_list\n-        if self.allow_redirects:\n-            if not httpstatus_list:\n-                httpstatus_list = [i for i in range(1000)\n-                                   if i not in RedirectMiddleware.allowed_status]\n-            else:\n-                httpstatus_list = [i for i in httpstatus_list\n-                                   if i not in RedirectMiddleware.allowed_status]\n-        if httpstatus_list:\n-            request.meta['handle_httpstatus_list'] = httpstatus_list\n+        if self.httpstatus_list:\n+            request.meta['handle_httpstatus_list'] = self.httpstatus_list\n         else:\n             request.meta['handle_httpstatus_all'] = True\n         return request\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
