{"custom_id": "scrapy#c2c6ea376ca2a1a0634946f95690c03fefb9990b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 25 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 25 | Churn Cumulative: 761 | Contributors (this commit): 19 | Commits (past 90d): 6 | Contributors (cumulative): 21 | DMM Complexity: 0.125\n\nDIFF:\n@@ -12,7 +12,6 @@ from scrapy.exceptions import UsageError\n from scrapy.utils.misc import walk_modules\n from scrapy.utils.project import inside_project, get_project_settings\n from scrapy.utils.python import garbage_collect\n-from scrapy.settings.deprecated import check_deprecated_settings\n \n \n def _iter_command_classes(module_name):\n@@ -118,7 +117,6 @@ def execute(argv=None, settings=None):\n             pass\n         else:\n             settings['EDITOR'] = editor\n-    check_deprecated_settings(settings)\n \n     inproject = inside_project()\n     cmds = _get_commands_dict(settings, inproject)\n\n@@ -1,23 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-\n-DEPRECATED_SETTINGS = [\n-    ('TRACK_REFS', 'no longer needed (trackref is always enabled)'),\n-    ('RESPONSE_CLASSES', 'no longer supported'),\n-    ('DEFAULT_RESPONSE_ENCODING', 'no longer supported'),\n-    ('BOT_VERSION', 'no longer used (user agent defaults to Scrapy now)'),\n-    ('ENCODING_ALIASES', 'no longer needed (encoding discovery uses w3lib now)'),\n-    ('STATS_ENABLED', 'no longer supported (change STATS_CLASS instead)'),\n-    ('SQLITE_DB', 'no longer supported'),\n-    ('AUTOTHROTTLE_MIN_DOWNLOAD_DELAY', 'use DOWNLOAD_DELAY instead'),\n-    ('AUTOTHROTTLE_MAX_CONCURRENCY', 'use CONCURRENT_REQUESTS_PER_DOMAIN instead'),\n-]\n-\n-\n-def check_deprecated_settings(settings):\n-    deprecated = [x for x in DEPRECATED_SETTINGS if settings[x[0]] is not None]\n-    if deprecated:\n-        msg = \"You are using the following settings which are deprecated or obsolete\"\n-        msg += \" (ask scrapy-users@googlegroups.com for alternatives):\"\n-        msg = msg + \"\\n    \" + \"\\n    \".join(\"%s: %s\" % x for x in deprecated)\n-        warnings.warn(msg, ScrapyDeprecationWarning)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#915e363db5cb208de9043b61015b79c91ed0a6bb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 423 | Contributors (this commit): 9 | Commits (past 90d): 2 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -9,6 +9,11 @@ import OpenSSL.SSL\n from twisted.trial import unittest\n from twisted.web import server, static, util, resource\n from twisted.internet import reactor, defer\n+try:\n+    from twisted.internet.testing import StringTransport\n+except ImportError:\n+    # deprecated in Twisted 19.7.0\n+    # (remove once we bump our requirement past that version)\n     from twisted.test.proto_helpers import StringTransport\n from twisted.python.filepath import FilePath\n from twisted.protocols.policies import WrappingFactory\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9d9dea0d69709ef0f7aef67ddba1bd7bda25d273", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 14 | Churn Cumulative: 131 | Contributors (this commit): 11 | Commits (past 90d): 2 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -54,12 +54,16 @@ class OffsiteMiddleware(object):\n         if not allowed_domains:\n             return re.compile('')  # allow all by default\n         url_pattern = re.compile(\"^https?://.*$\")\n+        domains = []\n         for domain in allowed_domains:\n-            if url_pattern.match(domain):\n+            if domain is None:\n+                continue\n+            elif url_pattern.match(domain):\n                 message = (\"allowed_domains accepts only domains, not URLs. \"\n                            \"Ignoring URL entry %s in allowed_domains.\" % domain)\n                 warnings.warn(message, URLWarning)\n-        domains = [re.escape(d) for d in allowed_domains if d is not None]\n+            else:\n+                domains.append(re.escape(domain))\n         regex = r'^(.*\\.)?(%s)$' % '|'.join(domains)\n         return re.compile(regex)\n \n\n@@ -55,13 +55,13 @@ class TestOffsiteMiddleware2(TestOffsiteMiddleware):\n \n class TestOffsiteMiddleware3(TestOffsiteMiddleware2):\n \n-    def _get_spider(self):\n-        return Spider('foo')\n+    def _get_spiderargs(self):\n+        return dict(name='foo')\n \n \n class TestOffsiteMiddleware4(TestOffsiteMiddleware3):\n \n-    def _get_spider(self):\n+    def _get_spiderargs(self):\n         bad_hostname = urlparse('http:////scrapytest.org').hostname\n         return dict(name='foo', allowed_domains=['scrapytest.org', None, bad_hostname])\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#91a78eef3ee9de033e66db55c49321b2cc43740e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 40 | Lines Deleted: 16 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 56 | Churn Cumulative: 2008 | Contributors (this commit): 22 | Commits (past 90d): 10 | Contributors (cumulative): 22 | DMM Complexity: 0.25\n\nDIFF:\n@@ -384,7 +384,13 @@ class ScrapyAgent(object):\n     def _cb_bodyready(self, txresponse, request):\n         # deliverBody hangs for responses without body\n         if txresponse.length == 0:\n-            return txresponse, b'', None, None\n+            return {\n+                \"txresponse\": txresponse,\n+                \"body\": b\"\",\n+                \"flags\": None,\n+                \"certificate\": None,\n+                \"ip_address\": None,\n+            }\n \n         maxsize = request.meta.get('download_maxsize', self._maxsize)\n         warnsize = request.meta.get('download_warnsize', self._warnsize)\n@@ -420,12 +426,18 @@ class ScrapyAgent(object):\n         return d\n \n     def _cb_bodydone(self, result, request, url):\n-        txresponse, body, flags, certificate, ip_address = result\n-        status = int(txresponse.code)\n-        headers = Headers(txresponse.headers.getAllRawHeaders())\n-        respcls = responsetypes.from_args(headers=headers, url=url, body=body)\n-        return respcls(url=url, status=status, headers=headers, body=body,\n-                       flags=flags, certificate=certificate, ip_address=ip_address)\n+        status = int(result[\"txresponse\"].code)\n+        headers = Headers(result[\"txresponse\"].headers.getAllRawHeaders())\n+        respcls = responsetypes.from_args(headers=headers, url=url, body=result[\"body\"])\n+        return respcls(\n+            url=url,\n+            status=status,\n+            headers=headers,\n+            body=result[\"body\"],\n+            flags=result[\"flags\"],\n+            certificate=result[\"certificate\"],\n+            ip_address=result[\"ip_address\"],\n+        )\n \n \n @implementer(IBodyProducer)\n@@ -501,22 +513,34 @@ class _ResponseReader(protocol.Protocol):\n \n         body = self._bodybuf.getvalue()\n         if reason.check(ResponseDone):\n-            self._finished.callback(\n-                (self._txresponse, body, None, self._certificate, self._ip_address)\n-            )\n+            self._finished.callback({\n+                \"txresponse\": self._txresponse,\n+                \"body\": body,\n+                \"flags\": None,\n+                \"certificate\": self._certificate,\n+                \"ip_address\": self._ip_address,\n+            })\n             return\n \n         if reason.check(PotentialDataLoss):\n-            self._finished.callback(\n-                (self._txresponse, body, ['partial'], self._certificate, self._ip_address)\n-            )\n+            self._finished.callback({\n+                \"txresponse\": self._txresponse,\n+                \"body\": body,\n+                \"flags\": [\"partial\"],\n+                \"certificate\": self._certificate,\n+                \"ip_address\": self._ip_address,\n+            })\n             return\n \n         if reason.check(ResponseFailed) and any(r.check(_DataLoss) for r in reason.value.reasons):\n             if not self._fail_on_dataloss:\n-                self._finished.callback(\n-                    (self._txresponse, body, ['dataloss'], self._certificate, self._ip_address)\n-                )\n+                self._finished.callback({\n+                    \"txresponse\": self._txresponse,\n+                    \"body\": body,\n+                    \"flags\": [\"dataloss\"],\n+                    \"certificate\": self._certificate,\n+                    \"ip_address\": self._ip_address,\n+                })\n                 return\n \n             elif not self._fail_on_dataloss_warned:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1785095707dec53647c835c0b0861b220e8495af", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 2011 | Contributors (this commit): 22 | Commits (past 90d): 11 | Contributors (cumulative): 22 | DMM Complexity: 0.0\n\nDIFF:\n@@ -426,12 +426,11 @@ class ScrapyAgent(object):\n         return d\n \n     def _cb_bodydone(self, result, request, url):\n-        status = int(result[\"txresponse\"].code)\n         headers = Headers(result[\"txresponse\"].headers.getAllRawHeaders())\n         respcls = responsetypes.from_args(headers=headers, url=url, body=result[\"body\"])\n         return respcls(\n             url=url,\n-            status=status,\n+            status=int(result[\"txresponse\"].code),\n             headers=headers,\n             body=result[\"body\"],\n             flags=result[\"flags\"],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f3bab819ab92cc0750c9b73141abdcbc8da7c4ac", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 45 | Churn Cumulative: 243 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,7 +3,12 @@ import warnings\n \n from scrapy.exceptions import UsageError, ScrapyDeprecationWarning\n from scrapy.settings import BaseSettings, Settings\n-from scrapy.utils.conf import build_component_list, arglist_to_dict, feed_process_params_from_cli\n+from scrapy.utils.conf import (\n+    arglist_to_dict,\n+    build_component_list,\n+    feed_complete_default_values_from_settings,\n+    feed_process_params_from_cli\n+)\n \n \n class BuildComponentListTest(unittest.TestCase):\n@@ -135,6 +140,44 @@ class FeedExportConfigTestCase(unittest.TestCase):\n             feed_process_params_from_cli(settings, ['-:pickle'])\n         )\n \n+    def test_feed_complete_default_values_from_settings_empty(self):\n+        feed = {}\n+        settings = Settings({\n+            \"FEED_EXPORT_ENCODING\": \"custom encoding\",\n+            \"FEED_EXPORT_FIELDS\": [\"f1\", \"f2\", \"f3\"],\n+            \"FEED_EXPORT_INDENT\": 42,\n+            \"FEED_STORE_EMPTY\": True,\n+            \"FEED_URI_PARAMS\": (1, 2, 3, 4),\n+        })\n+        new_feed = feed_complete_default_values_from_settings(feed, settings)\n+        self.assertEqual(new_feed, {\n+            \"encoding\": \"custom encoding\",\n+            \"fields\": [\"f1\", \"f2\", \"f3\"],\n+            \"indent\": 42,\n+            \"store_empty\": True,\n+            \"uri_params\": (1, 2, 3, 4),\n+        })\n+\n+    def test_feed_complete_default_values_from_settings_non_empty(self):\n+        feed = {\n+            \"encoding\": \"other encoding\",\n+            \"fields\": None,\n+        }\n+        settings = Settings({\n+            \"FEED_EXPORT_ENCODING\": \"custom encoding\",\n+            \"FEED_EXPORT_FIELDS\": [\"f1\", \"f2\", \"f3\"],\n+            \"FEED_EXPORT_INDENT\": 42,\n+            \"FEED_STORE_EMPTY\": True,\n+        })\n+        new_feed = feed_complete_default_values_from_settings(feed, settings)\n+        self.assertEqual(new_feed, {\n+            \"encoding\": \"other encoding\",\n+            \"fields\": None,\n+            \"indent\": 42,\n+            \"store_empty\": True,\n+            \"uri_params\": None,\n+        })\n+\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c886a70eae35e628040da0504af3fd9aaa6aea75", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 18 | Churn Cumulative: 347 | Contributors (this commit): 10 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -113,16 +113,14 @@ def get_sources(use_closest=True):\n \n def feed_complete_default_values_from_settings(feed, settings):\n     out = feed.copy()\n-    if 'encoding' not in out:\n-        out['encoding'] = settings['FEED_EXPORT_ENCODING']\n-    if 'fields' not in out:\n-        out['fields'] = settings.getlist('FEED_EXPORT_FIELDS') or None\n-    if 'indent' not in out:\n-        out['indent'] = None if settings['FEED_EXPORT_INDENT'] is None else settings.getint('FEED_EXPORT_INDENT')\n-    if 'store_empty' not in out:\n-        out['store_empty'] = settings.getbool('FEED_STORE_EMPTY')\n-    if 'uri_params' not in out:\n-        out['uri_params'] = settings['FEED_URI_PARAMS']\n+    out.setdefault(\"encoding\", settings[\"FEED_EXPORT_ENCODING\"])\n+    out.setdefault(\"fields\", settings.getlist(\"FEED_EXPORT_FIELDS\") or None)\n+    out.setdefault(\"store_empty\", settings.getbool(\"FEED_STORE_EMPTY\"))\n+    out.setdefault(\"uri_params\", settings[\"FEED_URI_PARAMS\"])\n+    if settings[\"FEED_EXPORT_INDENT\"] is None:\n+        out.setdefault(\"indent\", None)\n+    else:\n+        out.setdefault(\"indent\", settings.getint(\"FEED_EXPORT_INDENT\"))\n     return out\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8d30dc08882e3b97dbaa17b3254de708c358d0d2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 12 | Churn Cumulative: 1550 | Contributors (this commit): 16 | Commits (past 90d): 8 | Contributors (cumulative): 24 | DMM Complexity: 0.6\n\nDIFF:\n@@ -188,9 +188,11 @@ class TextResponse(Response):\n         selectors from which links cannot be obtained (for instance, anchor tags without an\n         ``href`` attribute)\n         \"\"\"\n-        arg_count = len(list(filter(None, (urls, css, xpath))))\n-        if arg_count != 1:\n-            raise ValueError('Please supply exactly one of the following arguments: urls, css, xpath')\n+        arguments = [x for x in (urls, css, xpath) if x is not None]\n+        if len(arguments) != 1:\n+            raise ValueError(\n+                \"Please supply exactly one of the following arguments: urls, css, xpath\"\n+            )\n         if not urls:\n             if css:\n                 urls = self.css(css)\n\n@@ -215,6 +215,10 @@ class BaseResponseTest(unittest.TestCase):\n         links = map(Link, absolute)\n         self._assert_followed_all_urls(links, absolute)\n \n+    def test_follow_all_empty(self):\n+        r = self.response_class(\"http://example.com\")\n+        self.assertEqual([], list(r.follow_all([])))\n+\n     def test_follow_all_invalid(self):\n         r = self.response_class(\"http://example.com\")\n         if self.response_class == Response:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3b0820d747e11a1a7722f0777baf923607fc0485", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 14 | Churn Cumulative: 716 | Contributors (this commit): 17 | Commits (past 90d): 3 | Contributors (cumulative): 26 | DMM Complexity: 1.0\n\nDIFF:\n@@ -78,6 +78,12 @@ class Spider(object_ref):\n \n     def make_requests_from_url(self, url):\n         \"\"\" This method is deprecated. \"\"\"\n+        warnings.warn(\n+            \"Spider.make_requests_from_url method is deprecated: \"\n+            \"it will be removed and not be called by the default \"\n+            \"Spider.start_requests method in future Scrapy releases. \"\n+            \"Please override Spider.start_requests method instead.\"\n+        )\n         return Request(url, dont_filter=True)\n \n     def parse(self, response):\n\n@@ -602,13 +602,19 @@ class DeprecationTest(unittest.TestCase):\n             self.assertEqual(len(list(spider1.start_requests())), 1)\n             self.assertEqual(len(w), 0)\n \n+            # spider without overridden make_requests_from_url method\n+            # should issue a warning when called directly\n+            request = spider1.make_requests_from_url(\"http://www.example.com\")\n+            self.assertTrue(isinstance(request, Request))\n+            self.assertEqual(len(w), 1)\n+\n             # spider with overridden make_requests_from_url issues a warning,\n             # but the method still works\n             spider2 = MySpider5()\n             requests = list(spider2.start_requests())\n             self.assertEqual(len(requests), 1)\n             self.assertEqual(requests[0].url, 'http://example.com/foo')\n-            self.assertEqual(len(w), 1)\n+            self.assertEqual(len(w), 2)\n \n \n class NoParseMethodSpiderTest(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ccc4d88779cf2827431ef9e73f976f755c04fe0e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 26 | Churn Cumulative: 157 | Contributors (this commit): 11 | Commits (past 90d): 4 | Contributors (cumulative): 16 | DMM Complexity: 0.0\n\nDIFF:\n@@ -53,7 +53,8 @@ class OffsiteMiddleware(object):\n         allowed_domains = getattr(spider, 'allowed_domains', None)\n         if not allowed_domains:\n             return re.compile('')  # allow all by default\n-        url_pattern = re.compile(\"^https?://.*$\")\n+        url_pattern = re.compile(r\"^https?://.*$\")\n+        port_pattern = re.compile(r\":\\d+$\")\n         domains = []\n         for domain in allowed_domains:\n             if domain is None:\n@@ -62,6 +63,10 @@ class OffsiteMiddleware(object):\n                 message = (\"allowed_domains accepts only domains, not URLs. \"\n                            \"Ignoring URL entry %s in allowed_domains.\" % domain)\n                 warnings.warn(message, URLWarning)\n+            elif port_pattern.search(domain):\n+                message = (\"allowed_domains accepts only domains without ports. \"\n+                           \"Ignoring entry %s in allowed_domains.\" % domain)\n+                warnings.warn(message, PortWarning)\n             else:\n                 domains.append(re.escape(domain))\n         regex = r'^(.*\\.)?(%s)$' % '|'.join(domains)\n@@ -74,3 +79,7 @@ class OffsiteMiddleware(object):\n \n class URLWarning(Warning):\n     pass\n+\n+\n+class PortWarning(Warning):\n+    pass\n\n@@ -4,7 +4,7 @@ import warnings\n \n from scrapy.http import Response, Request\n from scrapy.spiders import Spider\n-from scrapy.spidermiddlewares.offsite import OffsiteMiddleware, URLWarning\n+from scrapy.spidermiddlewares.offsite import OffsiteMiddleware, URLWarning, PortWarning\n from scrapy.utils.test import get_crawler\n \n \n@@ -26,7 +26,8 @@ class TestOffsiteMiddleware(TestCase):\n                        Request('http://scrapy.org/1'),\n                        Request('http://sub.scrapy.org/1'),\n                        Request('http://offsite.tld/letmepass', dont_filter=True),\n-                       Request('http://scrapy.test.org/')]\n+                       Request('http://scrapy.test.org/'),\n+                       Request('http://scrapy.test.org:8000/')]\n         offsite_reqs = [Request('http://scrapy2.org'),\n                        Request('http://offsite.tld/'),\n                        Request('http://offsite.tld/scrapytest.org'),\n@@ -80,3 +81,13 @@ class TestOffsiteMiddleware5(TestOffsiteMiddleware4):\n             warnings.simplefilter(\"always\")\n             self.mw.get_host_regex(self.spider)\n             assert issubclass(w[-1].category, URLWarning)\n+\n+\n+class TestOffsiteMiddleware6(TestOffsiteMiddleware4):\n+\n+    def test_get_host_regex(self):\n+        self.spider.allowed_domains = ['scrapytest.org:8000', 'scrapy.org', 'scrapy.test.org']\n+        with warnings.catch_warnings(record=True) as w:\n+            warnings.simplefilter(\"always\")\n+            self.mw.get_host_regex(self.spider)\n+            assert issubclass(w[-1].category, PortWarning)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f9bf4b8d4dd64a1d65e949927b8ea7ad34e756d3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 12 | Files Changed: 9 | Hunks: 24 | Methods Changed: 12 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 30 | Churn Cumulative: 3611 | Contributors (this commit): 37 | Commits (past 90d): 30 | Contributors (cumulative): 84 | DMM Complexity: 0.75\n\nDIFF:\n@@ -3,7 +3,7 @@ from time import time\n from datetime import datetime\n from collections import deque\n \n-from twisted.internet import reactor, defer, task\n+from twisted.internet import defer, task\n \n from scrapy.utils.defer import mustbe_deferred\n from scrapy.utils.httpobj import urlparse_cached\n@@ -133,6 +133,7 @@ class Downloader(object):\n         return deferred\n \n     def _process_queue(self, spider, slot):\n+        from twisted.internet import reactor\n         if slot.latercall and slot.latercall.active():\n             return\n \n\n@@ -32,7 +32,6 @@ import re\n from io import BytesIO\n from urllib.parse import unquote\n \n-from twisted.internet import reactor\n from twisted.internet.protocol import ClientCreator, Protocol\n from twisted.protocols.ftp import CommandFailed, FTPClient\n \n@@ -81,6 +80,7 @@ class FTPDownloadHandler:\n         return cls(crawler.settings)\n \n     def download_request(self, request, spider):\n+        from twisted.internet import reactor\n         parsed_url = urlparse_cached(request)\n         user = request.meta.get(\"ftp_user\", self.default_user)\n         password = request.meta.get(\"ftp_password\", self.default_password)\n\n@@ -1,7 +1,5 @@\n \"\"\"Download handlers for http and https schemes\n \"\"\"\n-from twisted.internet import reactor\n-\n from scrapy.utils.misc import create_instance, load_object\n from scrapy.utils.python import to_unicode\n \n@@ -26,6 +24,7 @@ class HTTP10DownloadHandler:\n         return factory.deferred\n \n     def _connect(self, factory):\n+        from twisted.internet import reactor\n         host, port = to_unicode(factory.host), factory.port\n         if factory.scheme == b'https':\n             client_context_factory = create_instance(\n\n@@ -8,7 +8,7 @@ from io import BytesIO\n from time import time\n from urllib.parse import urldefrag\n \n-from twisted.internet import defer, protocol, reactor, ssl\n+from twisted.internet import defer, protocol, ssl\n from twisted.internet.endpoints import TCP4ClientEndpoint\n from twisted.internet.error import TimeoutError\n from twisted.web.client import Agent, HTTPConnectionPool, ResponseDone, ResponseFailed, URI\n@@ -33,6 +33,7 @@ class HTTP11DownloadHandler:\n     lazy = False\n \n     def __init__(self, settings, crawler=None):\n+        from twisted.internet import reactor\n         self._pool = HTTPConnectionPool(reactor, persistent=True)\n         self._pool.maxPersistentPerHost = settings.getint('CONCURRENT_REQUESTS_PER_DOMAIN')\n         self._pool._factory.noisy = False\n@@ -81,6 +82,7 @@ class HTTP11DownloadHandler:\n         return agent.download_request(request)\n \n     def close(self):\n+        from twisted.internet import reactor\n         d = self._pool.closeCachedConnections()\n         # closeCachedConnections will hang on network or server issues, so\n         # we'll manually timeout the deferred.\n@@ -284,6 +286,7 @@ class ScrapyAgent(object):\n         self._txresponse = None\n \n     def _get_agent(self, request, timeout):\n+        from twisted.internet import reactor\n         bindaddress = request.meta.get('bindaddress') or self._bindAddress\n         proxy = request.meta.get('proxy')\n         if proxy:\n@@ -326,6 +329,7 @@ class ScrapyAgent(object):\n         )\n \n     def download_request(self, request):\n+        from twisted.internet import reactor\n         timeout = request.meta.get('download_timeout') or self._connectTimeout\n         agent = self._get_agent(request, timeout)\n \n\n@@ -6,8 +6,6 @@ See documentation in docs/topics/extensions.rst\n \n from collections import defaultdict\n \n-from twisted.internet import reactor\n-\n from scrapy import signals\n from scrapy.exceptions import NotConfigured\n \n@@ -54,6 +52,7 @@ class CloseSpider(object):\n             self.crawler.engine.close_spider(spider, 'closespider_pagecount')\n \n     def spider_opened(self, spider):\n+        from twisted.internet import reactor\n         self.task = reactor.callLater(self.close_on['timeout'],\n                                       self.crawler.engine.close_spider, spider,\n                                       reason='closespider_timeout')\n\n@@ -12,7 +12,7 @@ from email.mime.text import MIMEText\n from email.utils import COMMASPACE, formatdate\n from io import BytesIO\n \n-from twisted.internet import defer, reactor, ssl\n+from twisted.internet import defer, ssl\n \n from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.python import to_bytes\n@@ -28,7 +28,6 @@ def _to_bytes_or_none(text):\n \n \n class MailSender(object):\n-\n     def __init__(self, smtphost='localhost', mailfrom='scrapy@localhost',\n             smtpuser=None, smtppass=None, smtpport=25, smtptls=False, smtpssl=False, debug=False):\n         self.smtphost = smtphost\n@@ -47,6 +46,7 @@ class MailSender(object):\n             settings.getbool('MAIL_TLS'), settings.getbool('MAIL_SSL'))\n \n     def send(self, to, subject, body, cc=None, attachs=(), mimetype='text/plain', charset=None, _callback=None):\n+        from twisted.internet import reactor\n         if attachs:\n             msg = MIMEMultipart()\n         else:\n@@ -111,6 +111,7 @@ class MailSender(object):\n \n     def _sendmail(self, to_addrs, msg):\n         # Import twisted.mail here because it is not available in python3\n+        from twisted.internet import reactor\n         from twisted.mail.smtp import ESMTPSenderFactory\n         msg = BytesIO(msg)\n         d = defer.Deferred()\n\n@@ -3,7 +3,6 @@ from urllib.parse import urlencode\n \n from twisted.web.server import Site\n from twisted.web.resource import Resource\n-from twisted.internet import reactor\n \n \n class Root(Resource):\n@@ -34,6 +33,7 @@ def _getarg(request, name, default=None, type=str):\n \n \n if __name__ == '__main__':\n+    from twisted.internet import reactor\n     root = Root()\n     factory = Site(root)\n     httpPort = reactor.listenTCP(8998, Site(root))\n\n@@ -1,7 +1,7 @@\n import sys\n import os\n \n-from twisted.internet import reactor, defer, protocol\n+from twisted.internet import defer, protocol\n \n \n class ProcessTest(object):\n@@ -11,6 +11,7 @@ class ProcessTest(object):\n     cwd = os.getcwd()  # trial chdirs to temp dir\n \n     def execute(self, args, check_code=True, settings=None):\n+        from twisted.internet import reactor\n         env = os.environ.copy()\n         if settings is not None:\n             env['SCRAPY_SETTINGS_MODULE'] = settings\n\n@@ -1,12 +1,12 @@\n from urllib.parse import urljoin\n \n-from twisted.internet import reactor\n from twisted.web import server, resource, static, util\n \n \n class SiteTest(object):\n \n     def setUp(self):\n+        from twisted.internet import reactor\n         super(SiteTest, self).setUp()\n         self.site = reactor.listenTCP(0, test_site(), interface=\"127.0.0.1\")\n         self.baseurl = \"http://localhost:%d/\" % self.site.getHost().port\n@@ -38,6 +38,7 @@ def test_site():\n \n \n if __name__ == '__main__':\n+    from twisted.internet import reactor\n     port = reactor.listenTCP(0, test_site(), interface=\"127.0.0.1\")\n     print(\"http://localhost:%d/\" % port.getHost().port)\n     reactor.run()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e5711127b162ce78e6c1b60ef209792cd6eef4e6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 21 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 21 | Churn Cumulative: 35 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,21 +0,0 @@\n-import warnings\n-\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-from scrapy.utils.http import decode_chunked_transfer\n-\n-\n-warnings.warn(\"Module `scrapy.downloadermiddlewares.chunked` is deprecated, \"\n-              \"chunked transfers are supported by default.\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-\n-class ChunkedTransferMiddleware(object):\n-    \"\"\"This middleware adds support for chunked transfer encoding, as\n-    documented in: https://en.wikipedia.org/wiki/Chunked_transfer_encoding\n-    \"\"\"\n-\n-    def process_response(self, request, response, spider):\n-        if response.headers.get('Transfer-Encoding') == 'chunked':\n-            body = decode_chunked_transfer(response.body)\n-            return response.replace(body=body)\n-        return response\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dfbe1d95071acfbba159cea051530749b1684460", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 142 | Lines Deleted: 143 | Files Changed: 90 | Hunks: 140 | Methods Changed: 11 | Complexity Δ (Sum/Max): 15/10 | Churn Δ: 285 | Churn Cumulative: 26719 | Contributors (this commit): 150 | Commits (past 90d): 178 | Contributors (cumulative): 778 | DMM Complexity: None\n\nDIFF:\n@@ -9,7 +9,7 @@ from scrapy.utils.conf import arglist_to_dict\n from scrapy.exceptions import UsageError\n \n \n-class ScrapyCommand(object):\n+class ScrapyCommand:\n \n     requires_project = False\n     crawler_process = None\n\n@@ -25,7 +25,7 @@ class Command(ScrapyCommand):\n             self.crawler_process.start()\n \n \n-class _BenchServer(object):\n+class _BenchServer:\n \n     def __enter__(self):\n         from scrapy.utils.test import get_testenv\n\n@@ -9,7 +9,7 @@ from scrapy.utils.spider import iterate_spider_output\n from scrapy.utils.python import get_spec\n \n \n-class ContractsManager(object):\n+class ContractsManager:\n     contracts = {}\n \n     def __init__(self, contracts):\n@@ -107,7 +107,7 @@ class ContractsManager(object):\n         request.errback = eb_wrapper\n \n \n-class Contract(object):\n+class Contract:\n     \"\"\" Abstract class for contracts \"\"\"\n     request_cls = None\n \n\n@@ -13,7 +13,7 @@ from scrapy.core.downloader.middleware import DownloaderMiddlewareManager\n from scrapy.core.downloader.handlers import DownloadHandlers\n \n \n-class Slot(object):\n+class Slot:\n     \"\"\"Downloader slot\"\"\"\n \n     def __init__(self, concurrency, delay, randomize_delay):\n@@ -66,7 +66,7 @@ def _get_concurrency_delay(concurrency, spider, settings):\n     return concurrency, delay\n \n \n-class Downloader(object):\n+class Downloader:\n \n     DOWNLOAD_SLOT = 'download_slot'\n \n\n@@ -268,7 +268,7 @@ class ScrapyProxyAgent(Agent):\n         )\n \n \n-class ScrapyAgent(object):\n+class ScrapyAgent:\n \n     _Agent = Agent\n     _ProxyAgent = ScrapyProxyAgent\n@@ -432,7 +432,7 @@ class ScrapyAgent(object):\n \n \n @implementer(IBodyProducer)\n-class _RequestBodyProducer(object):\n+class _RequestBodyProducer:\n \n     def __init__(self, body):\n         self.body = body\n\n@@ -21,7 +21,7 @@ from scrapy.utils.log import logformatter_adapter, failure_to_exc_info\n logger = logging.getLogger(__name__)\n \n \n-class Slot(object):\n+class Slot:\n \n     def __init__(self, start_requests, close_if_idle, nextcall, scheduler):\n         self.closing = False\n@@ -53,7 +53,7 @@ class Slot(object):\n             self.closing.callback(None)\n \n \n-class ExecutionEngine(object):\n+class ExecutionEngine:\n \n     def __init__(self, crawler, spider_closed_callback):\n         self.crawler = crawler\n\n@@ -14,7 +14,7 @@ from scrapy.utils.deprecate import ScrapyDeprecationWarning\n logger = logging.getLogger(__name__)\n \n \n-class Scheduler(object):\n+class Scheduler:\n     \"\"\"\n     Scrapy Scheduler. It allows to enqueue requests and then get\n     a next request to download. Scheduler is also handling duplication\n\n@@ -21,7 +21,7 @@ from scrapy.core.spidermw import SpiderMiddlewareManager\n logger = logging.getLogger(__name__)\n \n \n-class Slot(object):\n+class Slot:\n     \"\"\"Scraper slot (one per running spider)\"\"\"\n \n     MIN_RESPONSE_SIZE = 1024\n@@ -62,7 +62,7 @@ class Slot(object):\n         return self.active_size > self.max_active_size\n \n \n-class Scraper(object):\n+class Scraper:\n \n     def __init__(self, crawler):\n         self.slot = None\n\n@@ -11,7 +11,7 @@ from scrapy.http import HtmlResponse\n logger = logging.getLogger(__name__)\n \n \n-class AjaxCrawlMiddleware(object):\n+class AjaxCrawlMiddleware:\n     \"\"\"\n     Handle 'AJAX crawlable' pages marked as crawlable via meta tag.\n     For more info see https://developers.google.com/webmasters/ajax-crawling/docs/getting-started.\n\n@@ -10,7 +10,7 @@ from scrapy.utils.python import to_unicode\n logger = logging.getLogger(__name__)\n \n \n-class CookiesMiddleware(object):\n+class CookiesMiddleware:\n     \"\"\"This middleware enables working with sites that need cookies\"\"\"\n \n     def __init__(self, debug=False):\n\n@@ -16,7 +16,7 @@ from scrapy.responsetypes import responsetypes\n logger = logging.getLogger(__name__)\n \n \n-class DecompressionMiddleware(object):\n+class DecompressionMiddleware:\n     \"\"\" This middleware tries to recognise and extract the possibly compressed\n     responses that may arrive. \"\"\"\n \n\n@@ -7,7 +7,7 @@ See documentation in docs/topics/downloader-middleware.rst\n from scrapy.utils.python import without_none_values\n \n \n-class DefaultHeadersMiddleware(object):\n+class DefaultHeadersMiddleware:\n \n     def __init__(self, headers):\n         self._headers = headers\n\n@@ -7,7 +7,7 @@ See documentation in docs/topics/downloader-middleware.rst\n from scrapy import signals\n \n \n-class DownloadTimeoutMiddleware(object):\n+class DownloadTimeoutMiddleware:\n \n     def __init__(self, timeout=180):\n         self._timeout = timeout\n\n@@ -9,7 +9,7 @@ from w3lib.http import basic_auth_header\n from scrapy import signals\n \n \n-class HttpAuthMiddleware(object):\n+class HttpAuthMiddleware:\n     \"\"\"Set Basic HTTP Authorization header\n     (http_user and http_pass spider class attributes)\"\"\"\n \n\n@@ -17,7 +17,7 @@ from scrapy.exceptions import IgnoreRequest, NotConfigured\n from scrapy.utils.misc import load_object\n \n \n-class HttpCacheMiddleware(object):\n+class HttpCacheMiddleware:\n \n     DOWNLOAD_EXCEPTIONS = (defer.TimeoutError, TimeoutError, DNSLookupError,\n                            ConnectionRefusedError, ConnectionDone, ConnectError,\n\n@@ -15,7 +15,7 @@ except ImportError:\n     pass\n \n \n-class HttpCompressionMiddleware(object):\n+class HttpCompressionMiddleware:\n     \"\"\"This middleware allows compressed (gzip, deflate) traffic to be\n     sent/received from web sites\"\"\"\n     @classmethod\n\n@@ -7,7 +7,7 @@ from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.python import to_bytes\n \n \n-class HttpProxyMiddleware(object):\n+class HttpProxyMiddleware:\n \n     def __init__(self, auth_encoding='latin-1'):\n         self.auth_encoding = auth_encoding\n\n@@ -11,7 +11,7 @@ from scrapy.exceptions import IgnoreRequest, NotConfigured\n logger = logging.getLogger(__name__)\n \n \n-class BaseRedirectMiddleware(object):\n+class BaseRedirectMiddleware:\n \n     enabled_setting = 'REDIRECT_ENABLED'\n \n\n@@ -25,7 +25,7 @@ from scrapy.utils.python import global_object_name\n logger = logging.getLogger(__name__)\n \n \n-class RetryMiddleware(object):\n+class RetryMiddleware:\n \n     # IOError is raised by the HttpCompression middleware when trying to\n     # decompress an empty response\n\n@@ -16,7 +16,7 @@ from scrapy.utils.misc import load_object\n logger = logging.getLogger(__name__)\n \n \n-class RobotsTxtMiddleware(object):\n+class RobotsTxtMiddleware:\n     DOWNLOAD_PRIORITY = 1000\n \n     def __init__(self, crawler):\n\n@@ -4,7 +4,7 @@ from scrapy.utils.response import response_httprepr\n from scrapy.utils.python import global_object_name\n \n \n-class DownloaderStats(object):\n+class DownloaderStats:\n \n     def __init__(self, stats):\n         self.stats = stats\n\n@@ -3,7 +3,7 @@\n from scrapy import signals\n \n \n-class UserAgentMiddleware(object):\n+class UserAgentMiddleware:\n     \"\"\"This middleware allows spiders to override the user_agent\"\"\"\n \n     def __init__(self, user_agent='Scrapy'):\n\n@@ -5,7 +5,7 @@ from scrapy.utils.job import job_dir\n from scrapy.utils.request import referer_str, request_fingerprint\n \n \n-class BaseDupeFilter(object):\n+class BaseDupeFilter:\n \n     @classmethod\n     def from_settings(cls, settings):\n\n@@ -21,7 +21,7 @@ __all__ = ['BaseItemExporter', 'PprintItemExporter', 'PickleItemExporter',\n            'JsonItemExporter', 'MarshalItemExporter']\n \n \n-class BaseItemExporter(object):\n+class BaseItemExporter:\n \n     def __init__(self, *, dont_fail=False, **kwargs):\n         self._kwargs = kwargs\n\n@@ -10,7 +10,7 @@ from scrapy import signals\n from scrapy.exceptions import NotConfigured\n \n \n-class CloseSpider(object):\n+class CloseSpider:\n \n     def __init__(self, crawler):\n         self.crawler = crawler\n\n@@ -6,7 +6,7 @@ from datetime import datetime\n from scrapy import signals\n \n \n-class CoreStats(object):\n+class CoreStats:\n \n     def __init__(self, stats):\n         self.stats = stats\n\n@@ -17,7 +17,7 @@ from scrapy.utils.trackref import format_live_refs\n logger = logging.getLogger(__name__)\n \n \n-class StackTraceDump(object):\n+class StackTraceDump:\n \n     def __init__(self, crawler=None):\n         self.crawler = crawler\n@@ -52,7 +52,7 @@ class StackTraceDump(object):\n         return dumps\n \n \n-class Debugger(object):\n+class Debugger:\n     def __init__(self):\n         try:\n             signal.signal(signal.SIGUSR2, self._enter_debugger)\n\n@@ -44,7 +44,7 @@ class IFeedStorage(Interface):\n \n \n @implementer(IFeedStorage)\n-class BlockingFeedStorage(object):\n+class BlockingFeedStorage:\n \n     def open(self, spider):\n         path = spider.crawler.settings['FEED_TEMPDIR']\n@@ -61,7 +61,7 @@ class BlockingFeedStorage(object):\n \n \n @implementer(IFeedStorage)\n-class StdoutFeedStorage(object):\n+class StdoutFeedStorage:\n \n     def __init__(self, uri, _stdout=None):\n         if not _stdout:\n@@ -76,7 +76,7 @@ class StdoutFeedStorage(object):\n \n \n @implementer(IFeedStorage)\n-class FileFeedStorage(object):\n+class FileFeedStorage:\n \n     def __init__(self, uri):\n         self.path = file_uri_to_path(uri)\n@@ -179,7 +179,7 @@ class FTPFeedStorage(BlockingFeedStorage):\n         )\n \n \n-class _FeedSlot(object):\n+class _FeedSlot:\n     def __init__(self, file, exporter, storage, uri, format, store_empty):\n         self.file = file\n         self.exporter = exporter\n@@ -203,7 +203,7 @@ class _FeedSlot(object):\n             self._exporting = False\n \n \n-class FeedExporter(object):\n+class FeedExporter:\n \n     @classmethod\n     def from_crawler(cls, crawler):\n\n@@ -20,7 +20,7 @@ from scrapy.utils.request import request_fingerprint\n logger = logging.getLogger(__name__)\n \n \n-class DummyPolicy(object):\n+class DummyPolicy:\n \n     def __init__(self, settings):\n         self.ignore_schemes = settings.getlist('HTTPCACHE_IGNORE_SCHEMES')\n@@ -39,7 +39,7 @@ class DummyPolicy(object):\n         return True\n \n \n-class RFC2616Policy(object):\n+class RFC2616Policy:\n \n     MAXAGE = 3600 * 24 * 365  # one year\n \n@@ -213,7 +213,7 @@ class RFC2616Policy(object):\n         return currentage\n \n \n-class DbmCacheStorage(object):\n+class DbmCacheStorage:\n \n     def __init__(self, settings):\n         self.cachedir = data_path(settings['HTTPCACHE_DIR'], createdir=True)\n@@ -270,7 +270,7 @@ class DbmCacheStorage(object):\n         return request_fingerprint(request)\n \n \n-class FilesystemCacheStorage(object):\n+class FilesystemCacheStorage:\n \n     def __init__(self, settings):\n         self.cachedir = data_path(settings['HTTPCACHE_DIR'])\n\n@@ -8,7 +8,7 @@ from scrapy import signals\n logger = logging.getLogger(__name__)\n \n \n-class LogStats(object):\n+class LogStats:\n     \"\"\"Log basic scraping stats periodically\"\"\"\n \n     def __init__(self, stats, interval=60.0):\n\n@@ -11,7 +11,7 @@ from scrapy.exceptions import NotConfigured\n from scrapy.utils.trackref import live_refs\n \n \n-class MemoryDebugger(object):\n+class MemoryDebugger:\n \n     def __init__(self, stats):\n         self.stats = stats\n\n@@ -19,7 +19,7 @@ from scrapy.utils.engine import get_engine_status\n logger = logging.getLogger(__name__)\n \n \n-class MemoryUsage(object):\n+class MemoryUsage:\n \n     def __init__(self, crawler):\n         if not crawler.settings.getbool('MEMUSAGE_ENABLED'):\n\n@@ -6,7 +6,7 @@ from scrapy.exceptions import NotConfigured\n from scrapy.utils.job import job_dir\n \n \n-class SpiderState(object):\n+class SpiderState:\n     \"\"\"Store and load spider state during a scraping job\"\"\"\n \n     def __init__(self, jobdir=None):\n\n@@ -8,7 +8,7 @@ from scrapy import signals\n from scrapy.mail import MailSender\n from scrapy.exceptions import NotConfigured\n \n-class StatsMailer(object):\n+class StatsMailer:\n \n     def __init__(self, stats, recipients, mail):\n         self.stats = stats\n\n@@ -6,7 +6,7 @@ from scrapy import signals\n logger = logging.getLogger(__name__)\n \n \n-class AutoThrottle(object):\n+class AutoThrottle:\n \n     def __init__(self, crawler):\n         self.crawler = crawler\n\n@@ -5,7 +5,7 @@ from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.python import to_unicode\n \n \n-class CookieJar(object):\n+class CookieJar:\n     def __init__(self, policy=None, check_expired_frequency=10000):\n         self.policy = policy or DefaultCookiePolicy()\n         self.jar = _CookieJar(self.policy)\n@@ -100,7 +100,7 @@ def potential_domain_matches(domain):\n     return matches + ['.' + d for d in matches]\n \n \n-class _DummyLock(object):\n+class _DummyLock:\n     def acquire(self):\n         pass\n \n@@ -108,7 +108,7 @@ class _DummyLock(object):\n         pass\n \n \n-class WrappedRequest(object):\n+class WrappedRequest:\n     \"\"\"Wraps a scrapy Request class with methods defined by urllib2.Request class to interact with CookieJar class\n \n     see http://docs.python.org/library/urllib2.html#urllib2.Request\n@@ -178,7 +178,7 @@ class WrappedRequest(object):\n         self.request.headers.appendlist(name, value)\n \n \n-class WrappedResponse(object):\n+class WrappedResponse:\n \n     def __init__(self, response):\n         self.response = response\n\n@@ -6,7 +6,7 @@ its documentation in: docs/topics/link-extractors.rst\n \"\"\"\n \n \n-class Link(object):\n+class Link:\n     \"\"\"Link objects represent an extracted link by the LinkExtractor.\"\"\"\n \n     __slots__ = ['url', 'text', 'fragment', 'nofollow']\n\n@@ -49,7 +49,7 @@ _matches = lambda url, regexs: any(r.search(url) for r in regexs)\n _is_valid_url = lambda url: url.split('://', 1)[0] in {'http', 'https', 'file', 'ftp'}\n \n \n-class FilteringLinkExtractor(object):\n+class FilteringLinkExtractor:\n \n     _csstranslator = HTMLTranslator()\n \n\n@@ -27,7 +27,7 @@ def _nons(tag):\n     return tag\n \n \n-class LxmlParserLinkExtractor(object):\n+class LxmlParserLinkExtractor:\n     def __init__(self, tag=\"a\", attr=\"href\", process=None, unique=False,\n                  strip=True, canonicalized=False):\n         self.scan_tag = tag if callable(tag) else lambda t: t == tag\n\n@@ -25,7 +25,7 @@ def unbound_method(method):\n     return method\n \n \n-class ItemLoader(object):\n+class ItemLoader:\n \n     default_item_class = Item\n     default_input_processor = Identity()\n\n@@ -9,7 +9,7 @@ from scrapy.utils.misc import arg_to_iter\n from scrapy.loader.common import wrap_loader_context\n \n \n-class MapCompose(object):\n+class MapCompose:\n \n     def __init__(self, *functions, **default_loader_context):\n         self.functions = functions\n@@ -36,7 +36,7 @@ class MapCompose(object):\n         return values\n \n \n-class Compose(object):\n+class Compose:\n \n     def __init__(self, *functions, **default_loader_context):\n         self.functions = functions\n@@ -61,7 +61,7 @@ class Compose(object):\n         return value\n \n \n-class TakeFirst(object):\n+class TakeFirst:\n \n     def __call__(self, values):\n         for value in values:\n@@ -69,13 +69,13 @@ class TakeFirst(object):\n                 return value\n \n \n-class Identity(object):\n+class Identity:\n \n     def __call__(self, values):\n         return values\n \n \n-class SelectJmes(object):\n+class SelectJmes:\n     \"\"\"\n         Query the input string for the jmespath (given at instantiation),\n         and return the answer\n@@ -95,7 +95,7 @@ class SelectJmes(object):\n         return self.compiled_path.search(value)\n \n \n-class Join(object):\n+class Join:\n \n     def __init__(self, separator=u' '):\n         self.separator = separator\n\n@@ -14,7 +14,7 @@ DOWNLOADERRORMSG_SHORT = \"Error downloading %(request)s\"\n DOWNLOADERRORMSG_LONG = \"Error downloading %(request)s: %(errmsg)s\"\n \n \n-class LogFormatter(object):\n+class LogFormatter:\n     \"\"\"Class for generating log messages for different actions.\n \n     All methods must return a dictionary listing the parameters ``level``, ``msg``\n\n@@ -27,7 +27,7 @@ def _to_bytes_or_none(text):\n     return to_bytes(text)\n \n \n-class MailSender(object):\n+class MailSender:\n     def __init__(self, smtphost='localhost', mailfrom='scrapy@localhost',\n             smtpuser=None, smtppass=None, smtpport=25, smtptls=False, smtpssl=False, debug=False):\n         self.smtphost = smtphost\n\n@@ -9,7 +9,7 @@ from scrapy.utils.defer import process_parallel, process_chain, process_chain_bo\n logger = logging.getLogger(__name__)\n \n \n-class MiddlewareManager(object):\n+class MiddlewareManager:\n     \"\"\"Base class for implementing middleware managers\"\"\"\n \n     component_name = 'foo middleware'\n\n@@ -37,7 +37,7 @@ class FileException(Exception):\n     \"\"\"General media error exception\"\"\"\n \n \n-class FSFilesStore(object):\n+class FSFilesStore:\n     def __init__(self, basedir):\n         if '://' in basedir:\n             basedir = basedir.split('://', 1)[1]\n@@ -75,7 +75,7 @@ class FSFilesStore(object):\n             seen.add(dirname)\n \n \n-class S3FilesStore(object):\n+class S3FilesStore:\n     AWS_ACCESS_KEY_ID = None\n     AWS_SECRET_ACCESS_KEY = None\n     AWS_ENDPOINT_URL = None\n@@ -213,7 +213,7 @@ class S3FilesStore(object):\n         return extra\n \n \n-class GCSFilesStore(object):\n+class GCSFilesStore:\n \n     GCS_PROJECT_ID = None\n \n@@ -259,7 +259,7 @@ class GCSFilesStore(object):\n         )\n \n \n-class FTPFilesStore(object):\n+class FTPFilesStore:\n \n     FTP_USERNAME = None\n     FTP_PASSWORD = None\n\n@@ -14,11 +14,11 @@ from scrapy.utils.log import failure_to_exc_info\n logger = logging.getLogger(__name__)\n \n \n-class MediaPipeline(object):\n+class MediaPipeline:\n \n     LOG_FAILED_RESULTS = True\n \n-    class SpiderInfo(object):\n+    class SpiderInfo:\n         def __init__(self, spider):\n             self.spider = spider\n             self.downloading = set()\n\n@@ -110,7 +110,7 @@ class ScrapyPriorityQueue:\n         return sum(len(x) for x in self.queues.values()) if self.queues else 0\n \n \n-class DownloaderInterface(object):\n+class DownloaderInterface:\n \n     def __init__(self, crawler):\n         self.downloader = crawler.engine.downloader\n@@ -129,8 +129,8 @@ class DownloaderInterface(object):\n         return len(self.downloader.slots[slot].active)\n \n \n-class DownloaderAwarePriorityQueue(object):\n-    \"\"\" PriorityQueue which takes Downloader activity in account:\n+class DownloaderAwarePriorityQueue:\n+    \"\"\" PriorityQueue which takes Downloader activity into account:\n     domains (slots) with the least amount of active downloads are dequeued\n     first.\n     \"\"\"\n\n@@ -11,7 +11,7 @@ from scrapy.utils.misc import load_object\n from scrapy.utils.python import binary_is_text, to_bytes, to_unicode\n \n \n-class ResponseTypes(object):\n+class ResponseTypes:\n \n     CLASSES = {\n         'text/html': 'scrapy.http.HtmlResponse',\n\n@@ -28,7 +28,7 @@ def get_settings_priority(priority):\n         return priority\n \n \n-class SettingsAttribute(object):\n+class SettingsAttribute:\n \n     \"\"\"Class for storing data related to settings attributes.\n \n\n@@ -24,7 +24,7 @@ from scrapy.utils.conf import get_config\n from scrapy.utils.console import DEFAULT_PYTHON_SHELLS\n \n \n-class Shell(object):\n+class Shell:\n \n     relevant_classes = (Crawler, Spider, Request, Response, BaseItem,\n                         Settings)\n\n@@ -2,7 +2,7 @@ from pydispatch import dispatcher\n from scrapy.utils import signal as _signal\n \n \n-class SignalManager(object):\n+class SignalManager:\n \n     def __init__(self, sender=dispatcher.Anonymous):\n         self.sender = sender\n\n@@ -11,7 +11,7 @@ from scrapy.utils.spider import iter_spider_classes\n \n \n @implementer(ISpiderLoader)\n-class SpiderLoader(object):\n+class SpiderLoader:\n     \"\"\"\n     SpiderLoader is a class which locates and loads spiders\n     in a Scrapy project.\n\n@@ -11,7 +11,7 @@ from scrapy.http import Request\n logger = logging.getLogger(__name__)\n \n \n-class DepthMiddleware(object):\n+class DepthMiddleware:\n \n     def __init__(self, maxdepth, stats, verbose_stats=False, prio=1):\n         self.maxdepth = maxdepth\n\n@@ -18,7 +18,7 @@ class HttpError(IgnoreRequest):\n         super(HttpError, self).__init__(*args, **kwargs)\n \n \n-class HttpErrorMiddleware(object):\n+class HttpErrorMiddleware:\n \n     @classmethod\n     def from_crawler(cls, crawler):\n\n@@ -14,7 +14,7 @@ from scrapy.utils.httpobj import urlparse_cached\n logger = logging.getLogger(__name__)\n \n \n-class OffsiteMiddleware(object):\n+class OffsiteMiddleware:\n \n     def __init__(self, stats):\n         self.stats = stats\n\n@@ -28,7 +28,7 @@ POLICY_UNSAFE_URL = \"unsafe-url\"\n POLICY_SCRAPY_DEFAULT = \"scrapy-default\"\n \n \n-class ReferrerPolicy(object):\n+class ReferrerPolicy:\n \n     NOREFERRER_SCHEMES = LOCAL_SCHEMES\n \n@@ -284,7 +284,7 @@ def _load_policy_class(policy, warning_only=False):\n                 return None\n \n \n-class RefererMiddleware(object):\n+class RefererMiddleware:\n \n     def __init__(self, settings=None):\n         self.default_policy = DefaultReferrerPolicy\n\n@@ -12,7 +12,7 @@ from scrapy.exceptions import NotConfigured\n logger = logging.getLogger(__name__)\n \n \n-class UrlLengthMiddleware(object):\n+class UrlLengthMiddleware:\n \n     def __init__(self, maxlength):\n         self.maxlength = maxlength\n\n@@ -34,7 +34,7 @@ def _get_method(method, spider):\n _default_link_extractor = LinkExtractor()\n \n \n-class Rule(object):\n+class Rule:\n \n     def __init__(self, link_extractor=None, callback=None, cb_kwargs=None, follow=None,\n                  process_links=None, process_request=None, errback=None):\n\n@@ -7,7 +7,7 @@ import logging\n logger = logging.getLogger(__name__)\n \n \n-class StatsCollector(object):\n+class StatsCollector:\n \n     def __init__(self, crawler):\n         self._dump = crawler.settings.getbool('STATS_DUMP')\n\n@@ -109,7 +109,7 @@ class LocalWeakReferencedCache(weakref.WeakKeyDictionary):\n             return None  # key is not weak-referenceable, it's not cached\n \n \n-class SequenceExclude(object):\n+class SequenceExclude:\n     \"\"\"Object to test if an item is NOT within some sequence.\"\"\"\n \n     def __init__(self, seq):\n\n@@ -144,7 +144,7 @@ def method_is_overridden(subclass, base_class, method_name):\n     Return True if a method named ``method_name`` of a ``base_class``\n     is overridden in a ``subclass``.\n \n-    >>> class Base(object):\n+    >>> class Base:\n     ...     def foo(self):\n     ...         pass\n     >>> class Sub1(Base):\n\n@@ -52,7 +52,7 @@ def xmliter_lxml(obj, nodename, namespace=None, prefix='x'):\n         yield xs.xpath(selxpath)[0]\n \n \n-class _StreamReader(object):\n+class _StreamReader:\n \n     def __init__(self, obj):\n         self._ptr = 0\n\n@@ -152,7 +152,7 @@ def log_scrapy_info(settings):\n     logger.debug(\"Using reactor: %s.%s\", reactor.__module__, reactor.__class__.__name__)\n \n \n-class StreamLogger(object):\n+class StreamLogger:\n     \"\"\"Fake file-like stream object that redirects writes to a logger instance\n \n     Taken from:\n\n@@ -223,7 +223,7 @@ def get_spec(func):\n     >>> get_spec(re.match)\n     (['pattern', 'string'], {'flags': 0})\n \n-    >>> class Test(object):\n+    >>> class Test:\n     ...     def __call__(self, val):\n     ...         pass\n     ...     def method(self, val, flags=0):\n@@ -272,7 +272,7 @@ def equal_attributes(obj1, obj2, attributes):\n     return True\n \n \n-class WeakKeyCache(object):\n+class WeakKeyCache:\n \n     def __init__(self, default_factory):\n         self.default_factory = default_factory\n\n@@ -24,7 +24,7 @@ def listen_tcp(portrange, host, factory):\n                 raise\n \n \n-class CallLaterOnce(object):\n+class CallLaterOnce:\n     \"\"\"Schedule a function to be called in the next reactor loop, but only if\n     it hasn't been already scheduled since the last time it ran.\n     \"\"\"\n\n@@ -10,7 +10,7 @@ from urllib.parse import urljoin\n import lxml.etree\n \n \n-class Sitemap(object):\n+class Sitemap:\n     \"\"\"Class to parse Sitemap (type=urlset) and Sitemap Index\n     (type=sitemapindex) files\"\"\"\n \n\n@@ -4,7 +4,7 @@ import os\n from twisted.internet import defer, protocol\n \n \n-class ProcessTest(object):\n+class ProcessTest:\n \n     command = None\n     prefix = [sys.executable, '-m', 'scrapy.cmdline']\n\n@@ -3,7 +3,7 @@ from urllib.parse import urljoin\n from twisted.web import server, resource, static, util\n \n \n-class SiteTest(object):\n+class SiteTest:\n \n     def setUp(self):\n         from twisted.internet import reactor\n\n@@ -19,9 +19,8 @@ NoneType = type(None)\n live_refs = defaultdict(weakref.WeakKeyDictionary)\n \n \n-class object_ref(object):\n-    \"\"\"Inherit from this class (instead of object) to a keep a record of live\n-    instances\"\"\"\n+class object_ref:\n+    \"\"\"Inherit from this class to a keep a record of live instances\"\"\"\n \n     __slots__ = ()\n \n\n@@ -3,7 +3,7 @@ Some pipelines used for testing\n \"\"\"\n \n \n-class ZeroDivisionErrorPipeline(object):\n+class ZeroDivisionErrorPipeline:\n \n     def open_spider(self, spider):\n         a = 1 / 0\n@@ -12,7 +12,7 @@ class ZeroDivisionErrorPipeline(object):\n         return item\n \n \n-class ProcessWithZeroDivisionErrorPipiline(object):\n+class ProcessWithZeroDivisionErrorPipiline:\n \n     def process_item(self, item, spider):\n         1 / 0\n\n@@ -1,7 +1,7 @@\n \"\"\"A test extension used to check the settings loading order\"\"\"\n \n \n-class TestExtension(object):\n+class TestExtension:\n \n     def __init__(self, settings):\n         settings.set('TEST1', \"%s + %s\" % (settings['TEST1'], 'started'))\n@@ -11,5 +11,5 @@ class TestExtension(object):\n         return cls(crawler.settings)\n \n \n-class DummyExtension(object):\n+class DummyExtension:\n     pass\n\n@@ -1,4 +1,4 @@\n-class TestSpiderPipeline(object):\n+class TestSpiderPipeline:\n \n     def open_spider(self, spider):\n         pass\n@@ -7,7 +7,7 @@ class TestSpiderPipeline(object):\n         return item\n \n \n-class TestSpiderExceptionPipeline(object):\n+class TestSpiderExceptionPipeline:\n \n     def open_spider(self, spider):\n         raise Exception('exception')\n\n@@ -89,7 +89,7 @@ class MyBadCrawlSpider(CrawlSpider):\n             f.write(\"\"\"\n import logging\n \n-class MyPipeline(object):\n+class MyPipeline:\n     component_name = 'my_pipeline'\n \n     def process_item(self, item, spider):\n\n@@ -25,7 +25,7 @@ class TestItem(Item):\n     url = Field()\n \n \n-class ResponseMock(object):\n+class ResponseMock:\n     url = 'http://scrapy.org'\n \n \n\n@@ -126,7 +126,7 @@ class CrawlerLoggingTestCase(unittest.TestCase):\n         self.assertEqual(crawler.stats.get_value('log_count/DEBUG', 0), 0)\n \n \n-class SpiderLoaderWithWrongInterface(object):\n+class SpiderLoaderWithWrongInterface:\n \n     def unneeded_method(self):\n         pass\n\n@@ -106,7 +106,7 @@ class ResponseFromProcessRequestTest(ManagerTestCase):\n     def test_download_func_not_called(self):\n         resp = Response('http://example.com/index.html')\n \n-        class ResponseMiddleware(object):\n+        class ResponseMiddleware:\n             def process_request(self, request, spider):\n                 return resp\n \n\n@@ -35,7 +35,7 @@ class FromSettingsRFPDupeFilter(RFPDupeFilter):\n         return df\n \n \n-class DirectDupeFilter(object):\n+class DirectDupeFilter:\n     method = 'n/a'\n \n \n\n@@ -96,7 +96,7 @@ def start_test_site(debug=False):\n     return port\n \n \n-class CrawlerRun(object):\n+class CrawlerRun:\n     \"\"\"A class to run the crawler and keep track of events occurred\"\"\"\n \n     def __init__(self, spider_class):\n\n@@ -373,7 +373,7 @@ class StdoutFeedStorageTest(unittest.TestCase):\n         self.assertEqual(out.getvalue(), b\"content\")\n \n \n-class FromCrawlerMixin(object):\n+class FromCrawlerMixin:\n     init_with_crawler = False\n \n     @classmethod\n\n@@ -213,7 +213,7 @@ class ItemTest(unittest.TestCase):\n         class B(A):\n             pass\n \n-        class C(object):\n+        class C:\n             fields = {'load': Field(default='C')}\n             not_allowed = Field(default='not_allowed')\n             save = Field(default='C')\n\n@@ -456,7 +456,7 @@ class BasicItemLoaderTest(unittest.TestCase):\n                           [u'marta', u'other'], Compose(float))\n \n \n-class InitializationTestMixin(object):\n+class InitializationTestMixin:\n \n     item_class = None\n \n\n@@ -170,7 +170,7 @@ class SkipMessagesLogFormatter(LogFormatter):\n         return None\n \n \n-class DropSomeItemsPipeline(object):\n+class DropSomeItemsPipeline:\n     drop = True\n \n     def process_item(self, item, spider):\n\n@@ -5,7 +5,7 @@ from scrapy.exceptions import NotConfigured\n from scrapy.middleware import MiddlewareManager\n \n \n-class M1(object):\n+class M1:\n \n     def open_spider(self, spider):\n         pass\n@@ -17,7 +17,7 @@ class M1(object):\n         pass\n \n \n-class M2(object):\n+class M2:\n \n     def open_spider(self, spider):\n         pass\n@@ -28,13 +28,13 @@ class M2(object):\n     pass\n \n \n-class M3(object):\n+class M3:\n \n     def process(self, response, request, spider):\n         pass\n \n \n-class MOff(object):\n+class MOff:\n \n     def open_spider(self, spider):\n         pass\n\n@@ -8,7 +8,7 @@ from tests.spiders import MockServerSpider\n from tests.mockserver import MockServer\n \n \n-class InjectArgumentsDownloaderMiddleware(object):\n+class InjectArgumentsDownloaderMiddleware:\n     \"\"\"\n     Make sure downloader middlewares are able to update the keyword arguments\n     \"\"\"\n@@ -23,7 +23,7 @@ class InjectArgumentsDownloaderMiddleware(object):\n         return response\n \n \n-class InjectArgumentsSpiderMiddleware(object):\n+class InjectArgumentsSpiderMiddleware:\n     \"\"\"\n     Make sure spider middlewares are able to update the keyword arguments\n     \"\"\"\n\n@@ -20,7 +20,7 @@ MockEngine = collections.namedtuple('MockEngine', ['downloader'])\n MockSlot = collections.namedtuple('MockSlot', ['active'])\n \n \n-class MockDownloader(object):\n+class MockDownloader:\n     def __init__(self):\n         self.slots = dict()\n \n@@ -57,7 +57,7 @@ class MockCrawler(Crawler):\n         self.engine = MockEngine(downloader=MockDownloader())\n \n \n-class SchedulerHandler(object):\n+class SchedulerHandler:\n     priority_queue_cls = None\n     jobdir = None\n \n@@ -245,7 +245,7 @@ def _is_scheduling_fair(enqueued_slots, dequeued_slots):\n     return True\n \n \n-class DownloaderAwareSchedulerTestMixin(object):\n+class DownloaderAwareSchedulerTestMixin:\n     priority_queue_cls = 'scrapy.pqueues.DownloaderAwarePriorityQueue'\n     reopen = False\n \n\n@@ -47,7 +47,7 @@ class TestRefererMiddleware(TestCase):\n             self.assertEqual(out[0].headers.get('Referer'), referrer)\n \n \n-class MixinDefault(object):\n+class MixinDefault:\n     \"\"\"\n     Based on https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade\n \n@@ -72,7 +72,7 @@ class MixinDefault(object):\n     ]\n \n \n-class MixinNoReferrer(object):\n+class MixinNoReferrer:\n     scenarii = [\n         ('https://example.com/page.html',       'https://example.com/', None),\n         ('http://www.example.com/',             'https://scrapy.org/',  None),\n@@ -82,7 +82,7 @@ class MixinNoReferrer(object):\n     ]\n \n \n-class MixinNoReferrerWhenDowngrade(object):\n+class MixinNoReferrerWhenDowngrade:\n     scenarii = [\n         # TLS to TLS: send non-empty referrer\n         ('https://example.com/page.html',       'https://not.example.com/', b'https://example.com/page.html'),\n@@ -111,7 +111,7 @@ class MixinNoReferrerWhenDowngrade(object):\n     ]\n \n \n-class MixinSameOrigin(object):\n+class MixinSameOrigin:\n     scenarii = [\n         # Same origin (protocol, host, port): send referrer\n         ('https://example.com/page.html',       'https://example.com/not-page.html',        b'https://example.com/page.html'),\n@@ -144,7 +144,7 @@ class MixinSameOrigin(object):\n     ]\n \n \n-class MixinOrigin(object):\n+class MixinOrigin:\n     scenarii = [\n         # TLS or non-TLS to TLS or non-TLS: referrer origin is sent (yes, even for downgrades)\n         ('https://example.com/page.html',   'https://example.com/not-page.html',    b'https://example.com/'),\n@@ -157,7 +157,7 @@ class MixinOrigin(object):\n     ]\n \n \n-class MixinStrictOrigin(object):\n+class MixinStrictOrigin:\n     scenarii = [\n         # TLS or non-TLS to TLS or non-TLS: referrer origin is sent but not for downgrades\n         ('https://example.com/page.html',   'https://example.com/not-page.html',    b'https://example.com/'),\n@@ -176,7 +176,7 @@ class MixinStrictOrigin(object):\n     ]\n \n \n-class MixinOriginWhenCrossOrigin(object):\n+class MixinOriginWhenCrossOrigin:\n     scenarii = [\n         # Same origin (protocol, host, port): send referrer\n         ('https://example.com/page.html',       'https://example.com/not-page.html',        b'https://example.com/page.html'),\n@@ -211,7 +211,7 @@ class MixinOriginWhenCrossOrigin(object):\n     ]\n \n \n-class MixinStrictOriginWhenCrossOrigin(object):\n+class MixinStrictOriginWhenCrossOrigin:\n     scenarii = [\n         # Same origin (protocol, host, port): send referrer\n         ('https://example.com/page.html',       'https://example.com/not-page.html',        b'https://example.com/page.html'),\n@@ -255,7 +255,7 @@ class MixinStrictOriginWhenCrossOrigin(object):\n     ]\n \n \n-class MixinUnsafeUrl(object):\n+class MixinUnsafeUrl:\n     scenarii = [\n         # TLS to TLS: send referrer\n         ('https://example.com/sekrit.html',     'http://not.example.com/',      b'https://example.com/sekrit.html'),\n\n@@ -36,7 +36,7 @@ def nonserializable_object_test(self):\n         self.assertRaises(ValueError, q.push, lambda x: x)\n     else:\n         # Use a different unpickleable object\n-        class A(object):\n+        class A:\n             pass\n \n         a = A()\n\n@@ -11,7 +11,7 @@ class MyWarning(UserWarning):\n     pass\n \n \n-class SomeBaseClass(object):\n+class SomeBaseClass:\n     pass\n \n \n@@ -155,7 +155,7 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n             class OutdatedUserClass1a(DeprecatedName):\n                 pass\n \n-            class UnrelatedClass(object):\n+            class UnrelatedClass:\n                 pass\n \n             class OldStyleClass:\n@@ -191,7 +191,7 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n             class OutdatedUserClass2a(DeprecatedName):\n                 pass\n \n-            class UnrelatedClass(object):\n+            class UnrelatedClass:\n                 pass\n \n             class OldStyleClass:\n\n@@ -73,7 +73,7 @@ class ToBytesTest(unittest.TestCase):\n \n class MemoizedMethodTest(unittest.TestCase):\n     def test_memoizemethod_noargs(self):\n-        class A(object):\n+        class A:\n \n             @memoizemethod_noargs\n             def cached(self):\n@@ -153,7 +153,7 @@ class UtilsPythonTestCase(unittest.TestCase):\n         self.assertFalse(equal_attributes(a, b, [compare_z, 'x']))\n \n     def test_weakkeycache(self):\n-        class _Weakme(object):\n+        class _Weakme:\n             pass\n \n         _values = count()\n@@ -176,14 +176,14 @@ class UtilsPythonTestCase(unittest.TestCase):\n         def f2(a, b=None, c=None):\n             pass\n \n-        class A(object):\n+        class A:\n             def __init__(self, a, b, c):\n                 pass\n \n             def method(self, a, b, c):\n                 pass\n \n-        class Callable(object):\n+        class Callable:\n \n             def __call__(self, a, b, c):\n                 pass\n\n@@ -126,7 +126,7 @@ class RequestSerializationTest(unittest.TestCase):\n         self.assertRaises(ValueError, request_to_dict, r)\n \n \n-class TestSpiderMixin(object):\n+class TestSpiderMixin:\n     def __mixin_callback(self, response):\n         pass\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
