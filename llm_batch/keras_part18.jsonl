{"custom_id": "keras#ba11d9ca75e586b42dbc6f796c2b72c13836015c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 11 | Methods Changed: 0 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 11 | Churn Cumulative: 1585 | Contributors (this commit): 17 | Commits (past 90d): 77 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -9,11 +9,13 @@ from . import optimizers\n from . import objectives\n from . import regularizers\n from . import constraints\n+from . import callbacks as cbks\n import time, copy\n from .utils.generic_utils import Progbar, printv\n from .layers import containers\n from six.moves import range\n \n+\n def standardize_y(y):\n     if not hasattr(y, 'shape'):\n         y = np.asarray(y)\n@@ -21,16 +23,19 @@ def standardize_y(y):\n         y = np.reshape(y, (len(y), 1))\n     return y\n \n+\n def make_batches(size, batch_size):\n     nb_batch = int(np.ceil(size/float(batch_size)))\n     return [(i*batch_size, min(size, (i+1)*batch_size)) for i in range(0, nb_batch)]\n \n+\n def standardize_X(X):\n     if type(X) == list:\n         return X\n     else:\n         return [X]\n \n+\n def slice_X(X, start=None, stop=None):\n     if type(X) == list:\n         if hasattr(start, '__len__'):\n@@ -43,6 +48,7 @@ def slice_X(X, start=None, stop=None):\n         else:\n             return X[start:stop]\n \n+\n def calculate_loss_weights(Y, sample_weight=None, class_weight=None):\n     if sample_weight is not None:\n         if isinstance(sample_weight, list):\n@@ -61,8 +67,8 @@ def calculate_loss_weights(Y, sample_weight=None, class_weight=None):\n         w = np.ones((Y.shape[0]))\n     return w\n \n-class Model(object):\n \n+class Model(object):\n     def compile(self, optimizer, loss, class_mode=\"categorical\", theano_mode=None):\n         self.optimizer = optimizers.get(optimizer)\n         self.loss = objectives.get(loss)\n@@ -232,6 +238,7 @@ class Model(object):\n         callbacks.on_train_end()\n         return callbacks.callbacks[0] # return history\n \n+\n     def predict(self, X, batch_size=128, verbose=1):\n         X = standardize_X(X)\n         batches = make_batches(len(X[0]), batch_size)\n@@ -251,6 +258,7 @@ class Model(object):\n \n         return preds\n \n+\n     def predict_proba(self, X, batch_size=128, verbose=1):\n         preds = self.predict(X, batch_size, verbose)\n         if preds.min() < 0 or preds.max() > 1:\n@@ -369,6 +377,7 @@ class Sequential(Model, containers.Sequential):\n         f.flush()\n         f.close()\n \n+\n     def load_weights(self, filepath):\n         # Loads weights from HDF5 file\n         import h5py\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#752037c1404be5cc277435e3d9bef2177b6a0eb6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 13 | Files Changed: 3 | Hunks: 8 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/3 | Churn Δ: 17 | Churn Cumulative: 2748 | Contributors (this commit): 20 | Commits (past 90d): 138 | Contributors (cumulative): 30 | DMM Complexity: 1.0\n\nDIFF:\n@@ -20,7 +20,6 @@ class Sequential(Layer):\n         self.params = []\n         self.regularizers = []\n         self.constraints = []\n-        self.loss_updates = []\n \n         for layer in layers:\n             self.add(layer)\n@@ -38,9 +37,6 @@ class Sequential(Layer):\n         self.regularizers += regularizers\n         self.constraints += constraints\n \n-        if hasattr(layer, 'cost_update'):\n-            self.loss_updates.append(layer.loss_update)\n-\n     def get_output(self, train=False):\n         return self.layers[-1].get_output(train)\n \n\n@@ -55,22 +55,22 @@ class Layer(object):\n                 if r:\n                     regs.append(r)\n                 else:\n-                    regs.append(regularizers.identity)\n+                    regs.append(regularizers.identity())\n         elif hasattr(self, 'regularizer') and self.regularizer:\n             regs += [self.regularizer for _ in range(len(self.params))]\n         else:\n-            regs += [regularizers.identity for _ in range(len(self.params))]\n+            regs += [regularizers.identity() for _ in range(len(self.params))]\n \n         if hasattr(self, 'constraints') and len(self.constraints) == len(self.params):\n             for c in self.constraints:\n                 if c:\n                     consts.append(c)\n                 else:\n-                    consts.append(constraints.identity)\n+                    consts.append(constraints.identity())\n         elif hasattr(self, 'constraint') and self.constraint:\n             consts += [self.constraint for _ in range(len(self.params))]\n         else:\n-            consts += [constraints.identity for _ in range(len(self.params))]\n+            consts += [constraints.identity() for _ in range(len(self.params))]\n \n         return self.params, regs, consts\n \n\n@@ -97,10 +97,6 @@ class Model(object):\n             raise Exception(\"Invalid class mode:\" + str(class_mode))\n         self.class_mode = class_mode\n \n-        if hasattr(self, 'loss_update'):\n-            for u in self.loss_updates:\n-                train_loss = u.update_loss(train_loss)\n-\n         updates = self.optimizer.get_updates(self.params, self.regularizers, self.constraints,  train_loss)\n \n         if type(self.X_train) == list:\n@@ -334,7 +330,6 @@ class Sequential(Model, containers.Sequential):\n         self.params = [] # learnable\n         self.regularizers = [] # same size as params\n         self.constraints = [] # same size as params\n-        self.loss_updates = [] # size can vary, no 1-to-1 mapping to params\n \n \n     def get_config(self, verbose=0):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f04fdec9e61eabe9750af00f56c18bdde8972045", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 110 | Lines Deleted: 90 | Files Changed: 6 | Hunks: 40 | Methods Changed: 29 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 200 | Churn Cumulative: 3738 | Contributors (this commit): 21 | Commits (past 90d): 177 | Contributors (cumulative): 40 | DMM Complexity: 1.0\n\nDIFF:\n@@ -40,13 +40,16 @@ class Sequential(Layer):\n     def get_output(self, train=False):\n         return self.layers[-1].get_output(train)\n \n-    def get_input(self, train=False):\n-        if not hasattr(self.layers[0], 'input'):\n+    def set_input(self):\n         for l in self.layers:\n             if hasattr(l, 'input'):\n-                    break\n                 ndim = l.input.ndim \n                 self.layers[0].input = ndim_tensor(ndim)\n+                break\n+\n+    def get_input(self, train=False):\n+        if not hasattr(self.layers[0], 'input'):\n+            self.set_input()\n         return self.layers[0].get_input(train)\n \n     @property\n\n@@ -47,19 +47,12 @@ class Layer(object):\n         return {\"name\":self.__class__.__name__}\n \n     def get_params(self):\n-        regs = []\n         consts = []\n \n-        if hasattr(self, 'regularizers') and len(self.regularizers) == len(self.params):\n-            for r in self.regularizers:\n-                if r:\n-                    regs.append(r)\n+        if hasattr(self, 'regularizers'):\n+            regularizers = self.regularizers\n         else:\n-                    regs.append(regularizers.identity())\n-        elif hasattr(self, 'regularizer') and self.regularizer:\n-            regs += [self.regularizer for _ in range(len(self.params))]\n-        else:\n-            regs += [regularizers.identity() for _ in range(len(self.params))]\n+            regularizers = []\n \n         if hasattr(self, 'constraints') and len(self.constraints) == len(self.params):\n             for c in self.constraints:\n@@ -72,7 +65,7 @@ class Layer(object):\n         else:\n             consts += [constraints.identity() for _ in range(len(self.params))]\n \n-        return self.params, regs, consts\n+        return self.params, regularizers, consts\n \n \n class Merge(object): \n@@ -88,10 +81,10 @@ class Merge(object):\n         self.regularizers = []\n         self.constraints = []\n         for m in self.models:\n+            self.regularizers += m.regularizers\n             for i in range(len(m.params)):\n                 if not m.params[i] in self.params:\n                     self.params.append(m.params[i])\n-                    self.regularizers.append(m.regularizers[i])\n                     self.constraints.append(m.constraints[i])\n \n     def get_params(self):\n@@ -248,7 +241,7 @@ class Dense(Layer):\n         Just your regular fully connected NN layer.\n     '''\n     def __init__(self, input_dim, output_dim, init='glorot_uniform', activation='linear', weights=None, \n-        W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None):\n+        W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None):\n \n         super(Dense,self).__init__()\n         self.init = initializations.get(init)\n@@ -262,7 +255,17 @@ class Dense(Layer):\n \n         self.params = [self.W, self.b]\n \n-        self.regularizers = [W_regularizer, b_regularizer]\n+        self.regularizers = []\n+        if W_regularizer:\n+            W_regularizer.set_param(self.W)\n+            self.regularizers.append(W_regularizer)\n+        if b_regularizer:\n+            b_regularizer.set_param(self.b)\n+            self.regularizers.append(b_regularizer)\n+        if activity_regularizer:\n+            activity_regularizer.set_layer(self)\n+            self.regularizers.append(activity_regularizer)\n+\n         self.constraints = [W_constraint, b_constraint]\n \n         if weights is not None:\n@@ -280,6 +283,7 @@ class Dense(Layer):\n             \"init\":self.init.__name__,\n             \"activation\":self.activation.__name__}\n \n+\n class ActivityRegularization(Layer):\n     '''\n         Layer that passes through its input unchanged, but applies an update\n@@ -287,13 +291,20 @@ class ActivityRegularization(Layer):\n     '''\n     def __init__(self, activity_regularizer=None):\n         super(ActivityRegularization, self).__init__()\n+\n+        self.activity_regularizer = activity_regularizer\n         if activity_regularizer is not None:\n             activity_regularizer.set_layer(self)\n-            self.loss_update = activity_regularizer\n+            self.regularizers = [activity_regularizer]\n \n     def get_output(self, train):\n         return self.get_input(train)\n \n+    def get_config(self):\n+        return {\"name\":self.__class__.__name__,\n+            \"activity_regularizer\":self.activity_regularizer.__name__}\n+\n+\n class TimeDistributedDense(Layer):\n     '''\n        Apply a same DenseLayer for each dimension[1] (shared_dimension) input\n@@ -303,7 +314,7 @@ class TimeDistributedDense(Layer):\n \n     '''\n     def __init__(self, input_dim, output_dim, init='glorot_uniform', activation='linear', weights=None, \n-        W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None):\n+        W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None):\n \n         super(TimeDistributedDense,self).__init__()\n         self.init = initializations.get(init)\n@@ -317,7 +328,17 @@ class TimeDistributedDense(Layer):\n \n         self.params = [self.W, self.b]\n \n-        self.regularizers = [W_regularizer, b_regularizer]\n+        self.regularizers = []\n+        if W_regularizer:\n+            W_regularizer.set_param(self.W)\n+            self.regularizers.append(W_regularizer)\n+        if b_regularizer:\n+            b_regularizer.set_param(self.b)\n+            self.regularizers.append(b_regularizer)\n+        if activity_regularizer:\n+            activity_regularizer.set_layer(self)\n+            self.regularizers.append(activity_regularizer)\n+\n         self.constraints = [W_constraint, b_constraint]\n \n         if weights is not None:\n@@ -447,7 +468,7 @@ class MaxoutDense(Layer):\n         Refer to http://arxiv.org/pdf/1302.4389.pdf\n     '''\n     def __init__(self, input_dim, output_dim, nb_feature=4, init='glorot_uniform', weights=None, \n-        W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None):\n+        W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None):\n \n         super(MaxoutDense,self).__init__()\n         self.init = initializations.get(init)\n@@ -461,7 +482,17 @@ class MaxoutDense(Layer):\n \n         self.params = [self.W, self.b]\n \n-        self.regularizers = [W_regularizer, b_regularizer]\n+        self.regularizers = []\n+        if W_regularizer:\n+            W_regularizer.set_param(self.W)\n+            self.regularizers.append(W_regularizer)\n+        if b_regularizer:\n+            b_regularizer.set_param(self.b)\n+            self.regularizers.append(b_regularizer)\n+        if activity_regularizer:\n+            activity_regularizer.set_layer(self)\n+            self.regularizers.append(activity_regularizer)\n+\n         self.constraints = [W_constraint, b_constraint]\n \n         if weights is not None:\n\n@@ -97,7 +97,9 @@ class Model(object):\n             raise Exception(\"Invalid class mode:\" + str(class_mode))\n         self.class_mode = class_mode\n \n-        updates = self.optimizer.get_updates(self.params, self.regularizers, self.constraints,  train_loss)\n+        for r in self.regularizers:\n+            train_loss = r(train_loss)\n+        updates = self.optimizer.get_updates(self.params, self.constraints, train_loss)\n \n         if type(self.X_train) == list:\n             train_ins = self.X_train + [self.y]\n\n@@ -19,7 +19,7 @@ class Optimizer(object):\n     def get_updates(self, params, regularizers, constraints,  loss):\n         raise NotImplementedError\n \n-    def get_gradients(self, loss, params, regularizers):\n+    def get_gradients(self, loss, params):\n \n         grads = T.grad(loss, params)\n \n@@ -27,12 +27,7 @@ class Optimizer(object):\n             norm = T.sqrt(sum([T.sum(g**2) for g in grads]))\n             grads = [clip_norm(g, self.clipnorm, norm) for g in grads]\n \n-        new_grads = []\n-        for p, g, r in zip(params, grads, regularizers):\n-            g = r.update_gradient(g, p)\n-            new_grads.append(g)\n-\n-        return new_grads\n+        return grads\n \n \n class SGD(Optimizer):\n@@ -42,8 +37,8 @@ class SGD(Optimizer):\n         self.__dict__.update(locals())\n         self.iterations = shared_scalar(0)\n \n-    def get_updates(self, params, regularizers, constraints, loss):\n-        grads = self.get_gradients(loss, params, regularizers)\n+    def get_updates(self, params, constraints, loss):\n+        grads = self.get_gradients(loss, params)\n         lr = self.lr * (1.0 / (1.0 + self.decay * self.iterations))\n         updates = [(self.iterations, self.iterations+1.)]\n \n@@ -67,8 +62,8 @@ class RMSprop(Optimizer):\n         self.__dict__.update(kwargs)\n         self.__dict__.update(locals())\n \n-    def get_updates(self, params, regularizers, constraints, loss):\n-        grads = self.get_gradients(loss, params, regularizers)\n+    def get_updates(self, params, constraints, loss):\n+        grads = self.get_gradients(loss, params)\n         accumulators = [shared_zeros(p.get_value().shape) for p in params]\n         updates = []\n \n@@ -88,8 +83,8 @@ class Adagrad(Optimizer):\n         self.__dict__.update(kwargs)\n         self.__dict__.update(locals())\n \n-    def get_updates(self, params, regularizers, constraints, loss):\n-        grads = self.get_gradients(loss, params, regularizers)\n+    def get_updates(self, params, constraints, loss):\n+        grads = self.get_gradients(loss, params)\n         accumulators = [shared_zeros(p.get_value().shape) for p in params]\n         updates = []\n \n@@ -110,8 +105,8 @@ class Adadelta(Optimizer):\n         self.__dict__.update(kwargs)\n         self.__dict__.update(locals())\n \n-    def get_updates(self, params, regularizers, constraints, loss):\n-        grads = self.get_gradients(loss, params, regularizers)\n+    def get_updates(self, params, constraints, loss):\n+        grads = self.get_gradients(loss, params)\n         accumulators = [shared_zeros(p.get_value().shape) for p in params]\n         delta_accumulators = [shared_zeros(p.get_value().shape) for p in params]\n         updates = []\n@@ -145,8 +140,8 @@ class Adam(Optimizer):\n         self.__dict__.update(locals())\n         self.iterations = shared_scalar(0)\n \n-    def get_updates(self, params, regularizers, constraints, loss):\n-        grads = self.get_gradients(loss, params, regularizers)\n+    def get_updates(self, params, constraints, loss):\n+        grads = self.get_gradients(loss, params)\n         updates = [(self.iterations, self.iterations+1.)]\n \n         i = self.iterations\n\n@@ -2,71 +2,60 @@ from __future__ import absolute_import\n import theano.tensor as T\n \n class Regularizer(object):\n-    def update_gradient(self, gradient, params):\n-        return gradient\n+    def set_param(self, p):\n+        self.p = p\n \n-    def update_loss(self, loss):\n+    def set_layer(self, layer):\n+        self.layer = layer\n+\n+    def __call__(self, loss):\n         return loss\n \n \n-class WeightsL1(Regularizer):\n-    def __init__(self, l=0.01):\n-        self.l = l\n-\n-    def update_gradient(self, gradient, params):\n-        gradient += T.sgn(params) * self.l\n-        return gradient\n-\n-\n-class WeightsL2(Regularizer):\n-    def __init__(self, l=0.01):\n-        self.l = l\n-\n-    def update_gradient(self, gradient, params):\n-        gradient += params * self.l\n-        return gradient\n-\n-\n-class WeightsL1L2(Regularizer):\n-    def __init__(self, l1=0.01, l2=0.01):\n+class WeightRegularizer(Regularizer):\n+    def __init__(self, l1=0., l2=0.):\n         self.l1 = l1\n         self.l2 = l2\n \n-    def update_gradient(self, gradient, params):\n-        gradient += params * self.l2\n-        gradient += T.sgn(params) * self.l1\n-        return gradient\n+    def set_param(self, p):\n+        self.p = p\n+\n+    def __call__(self, loss):\n+        loss += T.sum(abs(self.p)) * self.l1\n+        loss += T.sum(self.p ** 2) * self.l2\n+        return loss\n \n \n-class ActivityL1(Regularizer):\n-    def __init__(self, l=0.01):\n-        self.l = l\n-        self.layer = None\n+class ActivityRegularizer(Regularizer):\n+    def __init__(self, l1=0., l2=0.):\n+        self.l1 = l1\n+        self.l2 = l2\n \n     def set_layer(self, layer):\n         self.layer = layer\n \n-    def update_loss(self, loss):\n-        return loss + self.l * T.sum(T.mean(abs(self.layer.get_output(True)), axis=0))\n+    def __call__(self, loss):\n+        loss += self.l1 * T.sum(T.mean(abs(self.layer.get_output(True)), axis=0))\n+        loss += self.l2 * T.sum(T.mean(self.layer.get_output(True) ** 2, axis=0))\n+        return loss\n \n \n-class ActivityL2(Regularizer):\n-    def __init__(self, l=0.01):\n-        self.l = l\n-        self.layer = None\n+def l1(l=0.01):\n+    return WeightRegularizer(l1=l)\n \n-    def set_layer(self, layer):\n-        self.layer = layer\n+def l2(l=0.01):\n+    return WeightRegularizer(l2=l)\n \n-    def update_loss(self, loss):\n-        return loss + self.l * T.sum(T.mean(self.layer.get_output(True) ** 2, axis=0))\n+def l1l2(l1=0.01, l2=0.01):\n+    return WeightRegularizer(l1=l1, l2=l2)\n \n+def activity_l1(l=0.01):\n+    return ActivityRegularizer(l1=l)\n+\n+def activity_l2(l=0.01):\n+    return ActivityRegularizer(l2=l)\n+\n+def activity_l1l2(l1=0.01, l2=0.01):\n+    return ActivityRegularizer(l1=l1, l2=l2)\n \n identity = Regularizer\n-\n-l1 = weights_l1 = WeightsL1\n-l2 = weights_l2 = WeightsL2\n-l1l2 = weights_l1l2 = WeightsL1L2\n-\n-activity_l1 = ActivityL1\n-activity_l2 = ActivityL2\n\n@@ -1,5 +1,7 @@\n import unittest\n import numpy as np\n+np.random.seed(1337) # for reproducibility\n+\n from keras.models import Sequential\n from keras.layers.core import Merge, Dense, Activation, Flatten, ActivityRegularization\n from keras.layers.embeddings import Embedding\n@@ -16,8 +18,6 @@ high_weight = 5\n max_train_samples = 5000\n max_test_samples = 1000\n \n-np.random.seed(1337) # for reproducibility\n-\n # the data, shuffled and split between tran and test sets\n (X_train, y_train), (X_test, y_test) = mnist.load_data()\n X_train = X_train.reshape(60000, 784)[:max_train_samples]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8ef90a03b37b03a3d4b88b958462fa200be99d7b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 28 | Churn Cumulative: 171 | Contributors (this commit): 4 | Commits (past 90d): 11 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,7 +14,7 @@ class Convolution1D(Layer):\n     def __init__(self, nb_filter, stack_size, filter_length,\n         init='uniform', activation='linear', weights=None,\n         image_shape=None, border_mode='valid', subsample_length=1,\n-        W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None):\n+        W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None):\n \n         nb_row = 1\n         nb_col = filter_length\n@@ -36,7 +36,17 @@ class Convolution1D(Layer):\n \n         self.params = [self.W, self.b]\n \n-        self.regularizers = [W_regularizer, b_regularizer]\n+        self.regularizers = []\n+        if W_regularizer:\n+            W_regularizer.set_param(self.W)\n+            self.regularizers.append(W_regularizer)\n+        if b_regularizer:\n+            b_regularizer.set_param(self.b)\n+            self.regularizers.append(b_regularizer)\n+        if activity_regularizer:\n+            activity_regularizer.set_layer(self)\n+            self.regularizers.append(activity_regularizer)\n+\n         self.constraints = [W_constraint, b_constraint]\n \n         if weights is not None:\n@@ -87,7 +97,7 @@ class Convolution2D(Layer):\n     def __init__(self, nb_filter, stack_size, nb_row, nb_col, \n         init='glorot_uniform', activation='linear', weights=None, \n         image_shape=None, border_mode='valid', subsample=(1,1),\n-        W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None):\n+        W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None):\n         super(Convolution2D,self).__init__()\n \n         self.init = initializations.get(init)\n@@ -107,7 +117,17 @@ class Convolution2D(Layer):\n \n         self.params = [self.W, self.b]\n \n-        self.regularizers = [W_regularizer, b_regularizer]\n+        self.regularizers = []\n+        if W_regularizer:\n+            W_regularizer.set_param(self.W)\n+            self.regularizers.append(W_regularizer)\n+        if b_regularizer:\n+            b_regularizer.set_param(self.b)\n+            self.regularizers.append(b_regularizer)\n+        if activity_regularizer:\n+            activity_regularizer.set_layer(self)\n+            self.regularizers.append(activity_regularizer)\n+            \n         self.constraints = [W_constraint, b_constraint]\n \n         if weights is not None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#76b28524d15cf05f935d7063687b119b5db59858", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 8 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 183 | Contributors (this commit): 4 | Commits (past 90d): 11 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -13,7 +13,7 @@ from ..layers.core import Layer\n class Convolution1D(Layer):\n     def __init__(self, nb_filter, stack_size, filter_length,\n         init='uniform', activation='linear', weights=None,\n-        image_shape=None, border_mode='valid', subsample_length=1,\n+        border_mode='valid', subsample_length=1,\n         W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None):\n \n         nb_row = 1\n@@ -27,7 +27,6 @@ class Convolution1D(Layer):\n         self.activation = activations.get(activation)\n         self.subsample = (1, subsample_length)\n         self.border_mode = border_mode\n-        self.image_shape = image_shape\n \n         self.input = T.tensor4()\n         self.W_shape = (nb_filter, stack_size, nb_row, nb_col)\n@@ -56,7 +55,7 @@ class Convolution1D(Layer):\n         X = self.get_input(train)\n \n         conv_out = theano.tensor.nnet.conv.conv2d(X, self.W,\n-            border_mode=self.border_mode, subsample=self.subsample, image_shape=self.image_shape)\n+            border_mode=self.border_mode, subsample=self.subsample)\n         output = self.activation(conv_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n         return output\n \n@@ -67,7 +66,6 @@ class Convolution1D(Layer):\n             \"filter_length\":self.filter_length,\n             \"init\":self.init.__name__,\n             \"activation\":self.activation.__name__,\n-            \"image_shape\":self.image_shape,\n             \"border_mode\":self.border_mode,\n             \"subsample_length\":self.subsample_length}\n \n@@ -96,7 +94,7 @@ class MaxPooling1D(Layer):\n class Convolution2D(Layer):\n     def __init__(self, nb_filter, stack_size, nb_row, nb_col, \n         init='glorot_uniform', activation='linear', weights=None, \n-        image_shape=None, border_mode='valid', subsample=(1,1),\n+        border_mode='valid', subsample=(1, 1),\n         W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None):\n         super(Convolution2D,self).__init__()\n \n@@ -104,7 +102,6 @@ class Convolution2D(Layer):\n         self.activation = activations.get(activation)\n         self.subsample = subsample\n         self.border_mode = border_mode\n-        self.image_shape = image_shape\n         self.nb_filter = nb_filter\n         self.stack_size = stack_size\n         self.nb_row = nb_row\n@@ -137,7 +134,7 @@ class Convolution2D(Layer):\n         X = self.get_input(train)\n \n         conv_out = theano.tensor.nnet.conv.conv2d(X, self.W, \n-            border_mode=self.border_mode, subsample=self.subsample, image_shape=self.image_shape)\n+            border_mode=self.border_mode, subsample=self.subsample)\n         output = self.activation(conv_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n         return output\n \n@@ -149,7 +146,6 @@ class Convolution2D(Layer):\n             \"nb_col\":self.nb_col,\n             \"init\":self.init.__name__,\n             \"activation\":self.activation.__name__,\n-            \"image_shape\":self.image_shape,\n             \"border_mode\":self.border_mode,\n             \"subsample\":self.subsample}\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cbde35fdf5fa72cbc725eb1103c2063cdc81add2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 12 | Churn Cumulative: 1153 | Contributors (this commit): 11 | Commits (past 90d): 55 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,7 +8,7 @@ import numpy as np\n from .. import activations, initializations\n from ..utils.theano_utils import shared_zeros, floatX\n from ..utils.generic_utils import make_tuple\n-from .. import regularizers\n+from ..regularizers import ActivityRegularizer\n from .. import constraints\n \n from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n@@ -289,11 +289,12 @@ class ActivityRegularization(Layer):\n         Layer that passes through its input unchanged, but applies an update\n         to the cost function based on the activity.\n     '''\n-    def __init__(self, activity_regularizer=None):\n+    def __init__(self, l1=0., l2=0.):\n         super(ActivityRegularization, self).__init__()\n+        self.l1 = l1\n+        self.l2 = l2\n \n-        self.activity_regularizer = activity_regularizer\n-        if activity_regularizer is not None:\n+        activity_regularizer = ActivityRegularizer(l1=l1, l2=l2)\n         activity_regularizer.set_layer(self)\n         self.regularizers = [activity_regularizer]\n \n@@ -302,7 +303,8 @@ class ActivityRegularization(Layer):\n \n     def get_config(self):\n         return {\"name\":self.__class__.__name__,\n-            \"activity_regularizer\":self.activity_regularizer.__name__}\n+            \"l1\":self.l1,\n+            \"l2\":self.l2}\n \n \n class TimeDistributedDense(Layer):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d9357646e2c2da5c25f79a5850288bb07ec7b417", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 301 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -53,6 +53,8 @@ load_model = False\n load_tokenizer = False\n train_model = True\n save_dir = os.path.expanduser(\"~/.keras/models\")\n+if not os.path.exists(save_dir):\n+    os.makedirs(save_dir)\n model_load_fname = \"HN_skipgram_model.pkl\"\n model_save_fname = \"HN_skipgram_model.pkl\"\n tokenizer_fname = \"HN_tokenizer.pkl\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5e5bff05109c7165d2b4342fe75abd0b4cb48815", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 146 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -48,8 +48,8 @@ print('X_test shape:', X_test.shape)\n \n print('Build model...')\n model = Sequential()\n-model.add(Embedding(max_features, 256))\n-model.add(LSTM(256, 128)) # try using a GRU instead, for fun\n+model.add(Embedding(max_features, 128))\n+model.add(LSTM(128, 128)) # try using a GRU instead, for fun\n model.add(Dropout(0.5))\n model.add(Dense(128, 1))\n model.add(Activation('sigmoid'))\n@@ -58,11 +58,7 @@ model.add(Activation('sigmoid'))\n model.compile(loss='binary_crossentropy', optimizer='adam', class_mode=\"binary\")\n \n print(\"Train...\")\n-model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=4, validation_split=0.1, show_accuracy=True)\n-score = model.evaluate(X_test, y_test, batch_size=batch_size)\n+model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=4, validation_data=(X_test, y_test), show_accuracy=True)\n+score, acc = model.evaluate(X_test, y_test, batch_size=batch_size, show_accuracy=True)\n print('Test score:', score)\n-\n-classes = model.predict_classes(X_test, batch_size=batch_size)\n-acc = np_utils.accuracy(classes, y_test)\n print('Test accuracy:', acc)\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e8a6ae298d7fdbd5ad4e5fe25c2d5ff82d0178a3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 15 | Churn Cumulative: 1764 | Contributors (this commit): 18 | Commits (past 90d): 91 | Contributors (cumulative): 21 | DMM Complexity: 1.0\n\nDIFF:\n@@ -17,7 +17,10 @@ class Embedding(Layer):\n         @input_dim: size of vocabulary (highest input integer + 1)\n         @out_dim: size of dense representation\n     '''\n-    def __init__(self, input_dim, output_dim, init='uniform', weights=None, W_regularizer=None, W_constraint=None, mask_zero=False):\n+    def __init__(self, input_dim, output_dim, init='uniform',\n+        W_regularizer=None, activity_regularizer=None, W_constraint=None,\n+        mask_zero=False, weights=None):\n+\n         super(Embedding,self).__init__()\n         self.init = initializations.get(init)\n         self.input_dim = input_dim\n@@ -29,7 +32,14 @@ class Embedding(Layer):\n \n         self.params = [self.W]\n         self.constraints = [W_constraint]\n-        self.regularizers = [W_regularizer]\n+\n+        self.regularizers = []\n+        if W_regularizer:\n+            W_regularizer.set_param(self.W)\n+            self.regularizers.append(W_regularizer)\n+        if activity_regularizer:\n+            activity_regularizer.set_layer(self)\n+            self.regularizers.append(activity_regularizer)\n \n         if weights is not None:\n             self.set_weights(weights)\n\n@@ -97,7 +97,6 @@ class Model(object):\n         train_loss = self.loss(self.y, self.y_train, self.weights)\n         test_score = self.loss(self.y, self.y_test, self.weights)\n         \n-        \n         if class_mode == \"categorical\":\n             train_accuracy = T.mean(T.eq(T.argmax(self.y, axis=-1), T.argmax(self.y_train, axis=-1)))\n             test_accuracy = T.mean(T.eq(T.argmax(self.y, axis=-1), T.argmax(self.y_test, axis=-1)))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ce659e568be435116981663d8743b406340c8e77", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 2 | Churn Cumulative: 1155 | Contributors (this commit): 11 | Commits (past 90d): 55 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -20,7 +20,7 @@ class Layer(object):\n         self.params = []\n \n     def connect(self, layer):\n-        if layer.get_output_mask() is not None and not self.supports_masked_input():\n+        if not hasattr(self, \"get_output_mask\") and layer.get_output_mask() is not None:\n             raise Exception(\"Attached non-masking layer to layer with masked output\")\n         self.previous = layer\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7e069956787750724d30b59bd5c0db50c9eed43e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 23 | Churn Cumulative: 151 | Contributors (this commit): 5 | Commits (past 90d): 12 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -4,15 +4,17 @@ import numpy as np\n import random\n from six.moves import range\n \n-def pad_sequences(sequences, maxlen=None, dtype='int32', padding='pre', value=0.):\n+def pad_sequences(sequences, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.):\n     \"\"\"\n         Pad each sequence to the same length: \n         the length of the longuest sequence.\n \n         If maxlen is provided, any sequence longer\n-        than maxlen is truncated to maxlen.\n+        than maxlen is truncated to maxlen. Truncation happens off either the beginning (default) or\n+        the end of the sequence.\n+\n+        Supports post-padding and pre-padding (default).\n \n-        Support post-padding and pre-padding (default).\n     \"\"\"\n     lengths = [len(s) for s in sequences]\n \n@@ -22,10 +24,19 @@ def pad_sequences(sequences, maxlen=None, dtype='int32', padding='pre', value=0.\n \n     x = (np.ones((nb_samples, maxlen)) * value).astype(dtype)\n     for idx, s in enumerate(sequences):\n-        if padding == 'post':\n-            x[idx, :lengths[idx]] = s[:maxlen]\n+        if truncating == 'pre':\n+            trunc = s[-maxlen:]\n+        elif truncating == 'post':\n+            trunc = s[:maxlen]\n         else:\n-            x[idx, -min(maxlen, lengths[idx]):] = s[:maxlen]\n+            raise ValueError(\"Truncating type '%s' not understood\" % padding)\n+\n+        if padding == 'post':\n+            x[idx, :len(trunc)] = trunc\n+        elif padding == 'pre':\n+            x[idx, -len(trunc):] = trunc\n+        else:\n+            raise ValueError(\"Padding type '%s' not understood\" % padding)\n     return x\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c7aac3ce39d9cae52fe864a8aeb722b87c8f0dff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 11 | Churn Cumulative: 891 | Contributors (this commit): 7 | Commits (past 90d): 40 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,7 +3,7 @@ import theano\n import theano.tensor as T\n import numpy as np\n \n-from .utils.theano_utils import sharedX, shared_zeros\n+from .utils.theano_utils import sharedX, shared_zeros, shared_ones\n \n def get_fans(shape):\n     fan_in = shape[0] if len(shape) == 2 else np.prod(shape[1:])\n@@ -68,6 +68,9 @@ def identity(shape, scale=1):\n def zero(shape):\n     return shared_zeros(shape)\n \n+def one(shape):\n+    return shared_ones(shape)\n+\n \n from .utils.generic_utils import get_from_module\n def get(identifier):\n\n@@ -322,7 +322,7 @@ class LSTM(Recurrent):\n                 http://www.cs.toronto.edu/~graves/preprint.pdf\n     '''\n     def __init__(self, input_dim, output_dim=128, \n-        init='glorot_uniform', inner_init='orthogonal', \n+        init='glorot_uniform', inner_init='orthogonal', forget_bias_init='one',\n         activation='tanh', inner_activation='hard_sigmoid',\n         weights=None, truncate_gradient=-1, return_sequences=False):\n     \n@@ -334,6 +334,7 @@ class LSTM(Recurrent):\n \n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n+        self.forget_bias_init = initializations.get(forget_bias_init)\n         self.activation = activations.get(activation)\n         self.inner_activation = activations.get(inner_activation)\n         self.input = T.tensor3()\n@@ -344,7 +345,7 @@ class LSTM(Recurrent):\n \n         self.W_f = self.init((self.input_dim, self.output_dim))\n         self.U_f = self.inner_init((self.output_dim, self.output_dim))\n-        self.b_f = shared_zeros((self.output_dim))\n+        self.b_f = self.forget_bias_init((self.output_dim))\n \n         self.W_c = self.init((self.input_dim, self.output_dim))\n         self.U_c = self.inner_init((self.output_dim, self.output_dim))\n@@ -409,6 +410,7 @@ class LSTM(Recurrent):\n             \"output_dim\":self.output_dim,\n             \"init\":self.init.__name__,\n             \"inner_init\":self.inner_init.__name__,\n+            \"forget_bias_init\":self.forget_bias_init.__name__,\n             \"activation\":self.activation.__name__,\n             \"inner_activation\":self.inner_activation.__name__,\n             \"truncate_gradient\":self.truncate_gradient,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d148ff11c2df308c0167a126bb75f530814568aa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 123 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 11 | Complexity Δ (Sum/Max): 29/29 | Churn Δ: 124 | Churn Cumulative: 1374 | Contributors (this commit): 11 | Commits (past 90d): 64 | Contributors (cumulative): 13 | DMM Complexity: 0.44871794871794873\n\nDIFF:\n@@ -15,6 +15,15 @@ def ndim_tensor(ndim):\n     return T.matrix()\n \n class Sequential(Layer):\n+    '''\n+        Simple linear stack of layers.\n+\n+        inherited from Layer:\n+        - get_params\n+        - get_output_mask\n+        - supports_masked_input\n+    '''\n+\n     def __init__(self, layers=[]):\n         self.layers = []\n         self.params = []\n@@ -71,3 +80,115 @@ class Sequential(Layer):\n     def get_config(self):\n         return {\"name\":self.__class__.__name__,\n             \"layers\":[layer.get_config() for layer in self.layers]}\n+\n+\n+class Graph(Layer):\n+    '''\n+        Implement a NN graph with arbitrary layer connections.\n+\n+        Small difference with 'classical' layer API:\n+        input, get_input, get_output are lists of tensors instead of single tensors.\n+\n+        inherited from Layer:\n+            - get_params\n+            - get_output_mask\n+            - supports_masked_input\n+\n+        not implemented:\n+            - connect\n+    '''\n+    def __init__(self):\n+        self.namespace = set() # strings\n+        self.nodes = {} # layer-like\n+        self.inputs = {} # layer-like\n+        self.input_order = [] # strings\n+        self.outputs = {} # layer-like\n+        self.output_order = [] # strings\n+        self.params = []\n+        self.regularizers = []\n+        self.constraints = []\n+\n+    def connect(self):\n+        raise Exception('The Graph container does not implement the connect method.')\n+\n+    def add_input(self, name, ndim=2):\n+        if name in self.namespace:\n+            raise Exception('Duplicate node identifier: ' + name)\n+        self.namespace.add(name)\n+        layer = Layer() # empty layer \n+        layer.input = ndim_tensor(ndim)\n+        self.inputs[name] = layer\n+\n+    def get_input(self, train=False):\n+        # return list of tensors\n+        inputs = []\n+        for name in self.input_order:\n+            inputs.append(self.inputs[name].get_output(train))\n+        return inputs\n+\n+    @property\n+    def input(self):\n+        return self.get_input()\n+\n+    def add_node(self, layer, name, input=None, inputs=[], merge_mode='concat'):\n+        if name in self.namespace:\n+            raise Exception('Duplicate node identifier: ' + name)\n+        if input:\n+            if input not in self.namespace:\n+                raise Exception('Unknown identifier: ' + input)\n+            if input in self.nodes:\n+                layer.connect(self.nodes(input))\n+            elif input in self.inputs:\n+                layer.input = self.inputs[input]\n+        if inputs:\n+            to_merge = []\n+            for n in inputs:\n+                if n not in self.nodes:\n+                    raise Exception('Unknown identifier: ' + n)\n+                to_merge.append(self.nodes[n])\n+            merge = Merge(to_merge, merge_mode=merge_mode)\n+            layer.connect(merge)\n+\n+        self.nodes[name] = layer\n+        params, regularizers, constraints = layer.get_params()\n+        self.params += params\n+        self.regularizers += regularizers\n+        self.constraints += constraints\n+\n+    def add_output(self, name, input=None, inputs=[], merge_mode='concat'):\n+        if name in self.namespace:\n+            raise Exception('Duplicate node identifier: ' + name)\n+        if input:\n+            if input not in self.namespace:\n+                raise Exception('Unknown identifier: ' + input)\n+            if input in self.nodes:\n+                self.outputs[name] = self.nodes[inputs]\n+            elif input in self.inputs:\n+                layer = Layer()\n+                layer.input = self.inputs[input]\n+                self.ouputs[name] = layer\n+            \n+        if inputs:\n+            to_merge = []\n+            for n in inputs:\n+                if n not in self.nodes:\n+                    raise Exception('Unknown identifier: ' + n)\n+                to_merge.append(self.nodes[n])\n+            merge = Merge(to_merge, merge_mode=merge_mode)\n+            self.outputs[name] = merge.get_output()\n+\n+    def get_ouput(self, train=False):\n+        # return list of tensors\n+        outputs = []\n+        for name in self.output_order:\n+            outputs.append(self.outputs[name].get_output(train))\n+        return ouputs\n+\n+    def get_weights(self):\n+        pass\n+\n+    def set_weights(self):\n+        pass\n+\n+\n+\n\n@@ -25,7 +25,7 @@ class Layer(object):\n         self.previous = layer\n \n     def get_output(self, train):\n-        raise NotImplementedError\n+        return self.input\n \n     def get_input(self, train):\n         if hasattr(self, 'previous'):\n@@ -75,6 +75,7 @@ class Layer(object):\n \n         return self.params, regularizers, consts\n \n+\n class MaskedLayer(Layer):\n     def supports_masked_input(self):\n         return True\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2ab9f0ef616c6d05124bdf9c81eb542d73f8e5b6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 361 | Lines Deleted: 172 | Files Changed: 4 | Hunks: 55 | Methods Changed: 40 | Complexity Δ (Sum/Max): 25/20 | Churn Δ: 533 | Churn Cumulative: 3502 | Contributors (this commit): 20 | Commits (past 90d): 144 | Contributors (cumulative): 31 | DMM Complexity: 0.328125\n\nDIFF:\n@@ -1,8 +1,9 @@\n # -*- coding: utf-8 -*-\n from __future__ import absolute_import\n+from __future__ import print_function\n \n import theano.tensor as T\n-from ..layers.core import Layer\n+from ..layers.core import Layer, Merge\n from six.moves import range\n \n def ndim_tensor(ndim):\n@@ -33,13 +34,13 @@ class Sequential(Layer):\n         for layer in layers:\n             self.add(layer)\n \n-    def connect(self, layer):\n+    def set_previous(self, layer):\n         self.layers[0].previous = layer\n \n     def add(self, layer):\n         self.layers.append(layer)\n         if len(self.layers) > 1:\n-            self.layers[-1].connect(self.layers[-2])\n+            self.layers[-1].set_previous(self.layers[-2])\n         \n         params, regularizers, constraints = layer.get_params()\n         self.params += params\n@@ -84,18 +85,19 @@ class Sequential(Layer):\n \n class Graph(Layer):\n     '''\n-        Implement a NN graph with arbitrary layer connections.\n+        Implement a NN graph with arbitrary layer connections,\n+        arbitrary number of inputs and arbitrary number of outputs.\n \n-        Small difference with 'classical' layer API:\n-        input, get_input, get_output are lists of tensors instead of single tensors.\n+        Note: Graph can only be used as a layer\n+        (connect, input, get_input, get_output) \n+        when it has exactly one input and one output.\n \n         inherited from Layer:\n             - get_params\n             - get_output_mask\n             - supports_masked_input\n-\n-        not implemented:\n-            - connect\n+            - get_weights\n+            - set_weights\n     '''\n     def __init__(self):\n         self.namespace = set() # strings\n@@ -104,51 +106,64 @@ class Graph(Layer):\n         self.input_order = [] # strings\n         self.outputs = {} # layer-like\n         self.output_order = [] # strings\n+\n         self.params = []\n         self.regularizers = []\n         self.constraints = []\n \n-    def connect(self):\n-        raise Exception('The Graph container does not implement the connect method.')\n-\n-    def add_input(self, name, ndim=2):\n-        if name in self.namespace:\n-            raise Exception('Duplicate node identifier: ' + name)\n-        self.namespace.add(name)\n-        layer = Layer() # empty layer \n-        layer.input = ndim_tensor(ndim)\n-        self.inputs[name] = layer\n+    def set_previous(self, layer):\n+        if len(self.inputs) != 1 or len(self.outputs) != 1:\n+            raise Exception('The Graph container can only be used as a layer \\\n+                when it has exactly one input and one output.')\n+        self.inputs[self.input_order[0]].set_previous(layer)\n \n     def get_input(self, train=False):\n-        # return list of tensors\n-        inputs = []\n-        for name in self.input_order:\n-            inputs.append(self.inputs[name].get_output(train))\n-        return inputs\n+        if len(self.inputs) != 1 or len(self.outputs) != 1:\n+            raise Exception('The Graph container can only be used as a layer \\\n+                when it has exactly one input and one output.')\n+        return self.inputs[self.input_order[0]].get_input(train)\n \n     @property\n     def input(self):\n         return self.get_input()\n \n+    def get_output(self, train=False):\n+        if len(self.inputs) != 1 or len(self.outputs) != 1:\n+            raise Exception('The Graph container can only be used as a layer \\\n+                when it has exactly one input and one output.')\n+        return self.outputs[self.output_order[0]].get_output(train)\n+\n+    def add_input(self, name, ndim=2):\n+        if name in self.namespace:\n+            raise Exception('Duplicate node identifier: ' + name)\n+        self.namespace.add(name)\n+        self.input_order.append(name)\n+        layer = Layer() # empty layer \n+        layer.input = ndim_tensor(ndim)\n+        layer.input.name = name\n+        self.inputs[name] = layer\n+\n     def add_node(self, layer, name, input=None, inputs=[], merge_mode='concat'):\n+        layer.set_name(name)\n         if name in self.namespace:\n             raise Exception('Duplicate node identifier: ' + name)\n         if input:\n             if input not in self.namespace:\n                 raise Exception('Unknown identifier: ' + input)\n             if input in self.nodes:\n-                layer.connect(self.nodes(input))\n+                layer.set_previous(self.nodes[input])\n             elif input in self.inputs:\n-                layer.input = self.inputs[input]\n+                layer.set_previous(self.inputs[input])\n         if inputs:\n             to_merge = []\n             for n in inputs:\n                 if n not in self.nodes:\n                     raise Exception('Unknown identifier: ' + n)\n                 to_merge.append(self.nodes[n])\n-            merge = Merge(to_merge, merge_mode=merge_mode)\n-            layer.connect(merge)\n+            merge = Merge(to_merge, mode=merge_mode)\n+            layer.set_previous(merge)\n \n+        self.namespace.add(name)\n         self.nodes[name] = layer\n         params, regularizers, constraints = layer.get_params()\n         self.params += params\n@@ -162,32 +177,21 @@ class Graph(Layer):\n             if input not in self.namespace:\n                 raise Exception('Unknown identifier: ' + input)\n             if input in self.nodes:\n-                self.outputs[name] = self.nodes[inputs]\n+                self.outputs[name] = self.nodes[input]\n             elif input in self.inputs:\n-                layer = Layer()\n-                layer.input = self.inputs[input]\n-                self.ouputs[name] = layer\n-            \n+                self.ouputs[name] = self.inputs[input]\n         if inputs:\n             to_merge = []\n             for n in inputs:\n                 if n not in self.nodes:\n                     raise Exception('Unknown identifier: ' + n)\n                 to_merge.append(self.nodes[n])\n-            merge = Merge(to_merge, merge_mode=merge_mode)\n-            self.outputs[name] = merge.get_output()\n+            merge = Merge(to_merge, mode=merge_mode)\n+            self.outputs[name] = merge\n+        self.namespace.add(name)\n+        self.output_order.append(name)\n \n-    def get_ouput(self, train=False):\n-        # return list of tensors\n-        outputs = []\n-        for name in self.output_order:\n-            outputs.append(self.outputs[name].get_output(train))\n-        return ouputs\n-\n-    def get_weights(self):\n-        pass\n-\n-    def set_weights(self):\n+    def get_config(self):\n         pass\n \n \n\n@@ -19,15 +19,15 @@ class Layer(object):\n     def __init__(self):\n         self.params = []\n \n-    def connect(self, layer):\n+    def set_previous(self, layer):\n         if not hasattr(self, \"get_output_mask\") and layer.get_output_mask() is not None:\n             raise Exception(\"Attached non-masking layer to layer with masked output\")\n         self.previous = layer\n \n-    def get_output(self, train):\n-        return self.input\n+    def get_output(self, train=False):\n+        return self.get_input(train)\n \n-    def get_input(self, train):\n+    def get_input(self, train=False):\n         if hasattr(self, 'previous'):\n             return self.previous.get_output(train=train)\n         else:\n@@ -80,13 +80,13 @@ class MaskedLayer(Layer):\n     def supports_masked_input(self):\n         return True\n \n-    def get_input_mask(self, train=None):\n+    def get_input_mask(self, train=False):\n         if hasattr(self, 'previous'):\n             return self.previous.get_output_mask(train)\n         else:\n             return None\n \n-    def get_output_mask(self, train=None):\n+    def get_output_mask(self, train=False):\n         ''' The default output mask is just the input mask unchanged. Override this in your own\n         implementations if, for instance, you are reshaping the input'''\n         return self.get_input_mask(train)\n@@ -167,7 +167,7 @@ class Dropout(MaskedLayer):\n         super(Dropout, self).__init__()\n         self.p = p\n \n-    def get_output(self, train):\n+    def get_output(self, train=False):\n         X = self.get_input(train)\n         if self.p > 0.:\n             retain_prob = 1. - self.p\n@@ -192,7 +192,7 @@ class Activation(MaskedLayer):\n         self.target = target\n         self.beta = beta\n \n-    def get_output(self, train):\n+    def get_output(self, train=False):\n         X = self.get_input(train)\n         return self.activation(X)\n \n@@ -213,7 +213,7 @@ class Reshape(Layer):\n         super(Reshape, self).__init__()\n         self.dims = dims\n \n-    def get_output(self, train):\n+    def get_output(self, train=False):\n         X = self.get_input(train)\n         nshape = make_tuple(X.shape[0], *self.dims)\n         return theano.tensor.reshape(X, nshape)\n@@ -231,7 +231,7 @@ class Flatten(Layer):\n     def __init__(self):\n         super(Flatten, self).__init__()\n \n-    def get_output(self, train):\n+    def get_output(self, train=False):\n         X = self.get_input(train)\n         size = theano.tensor.prod(X.shape) // X.shape[0]\n         nshape = (X.shape[0], size)\n@@ -249,7 +249,7 @@ class RepeatVector(Layer):\n         super(RepeatVector, self).__init__()\n         self.n = n\n \n-    def get_output(self, train):\n+    def get_output(self, train=False):\n         X = self.get_input(train)\n         tensors = [X]*self.n\n         stacked = theano.tensor.stack(*tensors)\n@@ -264,7 +264,7 @@ class Dense(Layer):\n     '''\n         Just your regular fully connected NN layer.\n     '''\n-    def __init__(self, input_dim, output_dim, init='glorot_uniform', activation='linear', weights=None, \n+    def __init__(self, input_dim, output_dim, init='glorot_uniform', activation='linear', weights=None, name=None,\n         W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None):\n \n         super(Dense, self).__init__()\n@@ -295,7 +295,14 @@ class Dense(Layer):\n         if weights is not None:\n             self.set_weights(weights)\n \n-    def get_output(self, train):\n+        if name is not None:\n+            self.set_name(name)\n+\n+    def set_name(self, name):\n+        self.W.name = '%s_W' % name\n+        self.b.name = '%s_b' % name\n+\n+    def get_output(self, train=False):\n         X = self.get_input(train)\n         output = self.activation(T.dot(X, self.W) + self.b)\n         return output\n@@ -322,7 +329,7 @@ class ActivityRegularization(Layer):\n         activity_regularizer.set_layer(self)\n         self.regularizers = [activity_regularizer]\n \n-    def get_output(self, train):\n+    def get_output(self, train=False):\n         return self.get_input(train)\n \n     def get_config(self):\n@@ -370,7 +377,7 @@ class TimeDistributedDense(MaskedLayer):\n         if weights is not None:\n             self.set_weights(weights)\n \n-    def get_output(self, train):\n+    def get_output(self, train=False):\n         X = self.get_input(train)\n         output = self.activation(T.dot(X.dimshuffle(1, 0, 2), self.W) + self.b)\n         return output.dimshuffle(1, 0, 2)\n@@ -398,7 +405,7 @@ class AutoEncoder(Layer):\n         self.encoder = encoder\n         self.decoder = decoder\n \n-        self.decoder.connect(self.encoder)\n+        self.decoder.set_previous(self.encoder)\n \n         self.params = []\n         self.regularizers = []\n@@ -413,8 +420,8 @@ class AutoEncoder(Layer):\n         if weights is not None:\n             self.set_weights(weights)\n \n-    def connect(self, node):\n-        self.encoder.connect(node)\n+    def set_previous(self, node):\n+        self.encoder.set_previous(node)\n \n     def get_weights(self):\n         weights = []\n@@ -434,10 +441,10 @@ class AutoEncoder(Layer):\n     def input(self):\n         return self.encoder.input\n \n-    def _get_hidden(self, train):\n+    def _get_hidden(self, train=False):\n         return self.encoder.get_output(train)\n \n-    def get_output(self, train):\n+    def get_output(self, train=False):\n         if not train and not self.output_reconstruction:\n             return self.encoder.get_output(train)\n \n@@ -525,7 +532,7 @@ class MaxoutDense(Layer):\n         if weights is not None:\n             self.set_weights(weights)\n \n-    def get_output(self, train):\n+    def get_output(self, train=False):\n         X = self.get_input(train)\n         # -- don't need activation since it's just linear.\n         output = T.max(T.dot(X, self.W) + self.b, axis=1)\n\n@@ -78,6 +78,103 @@ def standardize_weights(y, sample_weight=None, class_weight=None):\n \n \n class Model(object):\n+    def _fit(self, f, ins, out_labels=[], batch_size=128, nb_epoch=100, verbose=1, callbacks=[], \\\n+        validation_split=0., val_f=None, val_ins=None, shuffle=True):\n+        '''\n+            Abstract fit function for f(*ins). Assume that f returns a list, labelled by out_labels.\n+        '''\n+\n+        do_validation = False\n+        if val_f and val_ins:\n+            do_validation = True\n+            if verbose:\n+                print(\"Train on %d samples, validate on %d samples\" % (len(ins[0]), len(val_ins[0])))\n+        else:\n+            if 0 < validation_split < 1:\n+                do_validation = True\n+                split_at = int(len(ins[0]) * (1 - validation_split))\n+                (ins, ins_val) = (slice_X(ins, 0, split_at), slice_X(ins, split_at))\n+                if verbose:\n+                    print(\"Train on %d samples, validate on %d samples\" % (len(ins[0]), len(val_ins[0])))\n+\n+        nb_train_sample = len(ins[0])\n+        index_array = np.arange(nb_train_sample)\n+\n+        history = cbks.History()\n+        if verbose:\n+            callbacks = [history, cbks.BaseLogger()] + callbacks\n+        else:\n+            callbacks = [history] + callbacks\n+        callbacks = cbks.CallbackList(callbacks)\n+\n+        callbacks._set_model(self)\n+        callbacks._set_params({\n+            'batch_size': batch_size,\n+            'nb_epoch': nb_epoch,\n+            'nb_sample': nb_train_sample,\n+            'verbose': verbose,\n+            'do_validation': do_validation,\n+        })\n+        callbacks.on_train_begin()\n+\n+        self.stop_training = False\n+        for epoch in range(nb_epoch):\n+            callbacks.on_epoch_begin(epoch)\n+            if shuffle:\n+                np.random.shuffle(index_array)\n+\n+            batches = make_batches(nb_train_sample, batch_size)\n+            for batch_index, (batch_start, batch_end) in enumerate(batches):\n+                batch_ids = index_array[batch_start:batch_end]\n+                ins_batch = slice_X(ins, batch_ids)\n+\n+                batch_logs = {}\n+                batch_logs['batch'] = batch_index\n+                batch_logs['size'] = len(batch_ids)\n+                callbacks.on_batch_begin(batch_index, batch_logs)\n+                outs = f(*ins_batch)\n+                if type(outs) != list:\n+                    outs = [outs]\n+                for l, o in zip(out_labels, outs):\n+                    batch_logs[l] = o\n+\n+                callbacks.on_batch_end(batch_index, batch_logs)\n+                \n+                if batch_index == len(batches) - 1: # last batch\n+                    # validation\n+                    epoch_logs = {}\n+                    if do_validation:\n+                        # replace with self._evaluate\n+                        val_outs = val_f(*val_ins)\n+                        if type(val_outs) != list:\n+                            val_outs = [val_outs]\n+                        # same labels assumed\n+                        for l, o in zip(out_labels, val_outs):\n+                            epoch_logs['val_' + l] = o\n+\n+            callbacks.on_epoch_end(epoch, epoch_logs)\n+            if self.stop_training:\n+                break\n+\n+        callbacks.on_train_end()\n+        return history\n+\n+\n+class Sequential(Model, containers.Sequential):\n+    '''\n+        Inherits from Model the following methods:\n+            - _fit\n+            - _predict\n+            - _evaluate\n+        Inherits from containers.Sequential the following methods:\n+            - __init__\n+            - add \n+            - get_output\n+            - get_input\n+            - get_weights\n+            - set_weights\n+    '''\n+\n     def compile(self, optimizer, loss, class_mode=\"categorical\", theano_mode=None):\n         self.optimizer = optimizers.get(optimizer)\n         self.loss = weighted_objective(objectives.get(loss))\n@@ -95,7 +192,11 @@ class Model(object):\n         self.weights = T.ones_like(self.y_train)\n \n         train_loss = self.loss(self.y, self.y_train, self.weights)\n-        test_score = self.loss(self.y, self.y_test, self.weights)\n+        test_loss = self.loss(self.y, self.y_test, self.weights)\n+\n+        train_loss.name = 'train_loss'\n+        test_loss.name = 'test_loss'\n+        self.y.name = 'y'\n         \n         if class_mode == \"categorical\":\n             train_accuracy = T.mean(T.eq(T.argmax(self.y, axis=-1), T.argmax(self.y_train, axis=-1)))\n@@ -127,9 +228,9 @@ class Model(object):\n             updates=updates, allow_input_downcast=True, mode=theano_mode)\n         self._predict = theano.function(predict_ins, self.y_test, \n             allow_input_downcast=True, mode=theano_mode)\n-        self._test = theano.function(test_ins, test_score, \n+        self._test = theano.function(test_ins, test_loss, \n             allow_input_downcast=True, mode=theano_mode)\n-        self._test_with_acc = theano.function(test_ins, [test_score, test_accuracy], \n+        self._test_with_acc = theano.function(test_ins, [test_loss, test_accuracy], \n             allow_input_downcast=True, mode=theano_mode)\n \n \n@@ -158,7 +259,6 @@ class Model(object):\n         else:\n             return self._test(*ins)\n \n-\n     def fit(self, X, y, batch_size=128, nb_epoch=100, verbose=1, callbacks=[],\n             validation_split=0., validation_data=None, shuffle=True, show_accuracy=False, \n             class_weight=None, sample_weight=None):\n@@ -167,95 +267,34 @@ class Model(object):\n         y = standardize_y(y)\n         sample_weight = standardize_weights(y, class_weight=class_weight, sample_weight=sample_weight)\n \n-        do_validation = False\n+        val_f = None\n+        val_ins = None\n+        if validation_data or validation_split:\n+            if show_accuracy:\n+                val_f = self._test_with_acc\n+            else:\n+                val_f = self._test\n         if validation_data:\n             try:\n                 X_val, y_val = validation_data\n             except:\n                 raise Exception(\"Invalid format for validation data; provide a tuple (X_val, y_val). \\\n                     X_val may be a numpy array or a list of numpy arrays depending on your model input.\")\n-            do_validation = True\n             X_val = standardize_X(X_val)\n             y_val = standardize_y(y_val)\n+            val_ins = X_val + [y_val, np.ones(y_val.shape[:-1] + (1,))]\n \n-            if verbose:\n-                print(\"Train on %d samples, validate on %d samples\" % (len(y), len(y_val)))\n-        else:\n-            if 0 < validation_split < 1:\n-                # If a validation split size is given (e.g. validation_split=0.2)\n-                # then split X into smaller X and X_val,\n-                # and split y into smaller y and y_val.\n-                do_validation = True\n-                split_at = int(len(y) * (1 - validation_split))\n-                (X, X_val) = (slice_X(X, 0, split_at), slice_X(X, split_at))\n-                (y, y_val) = (y[:split_at], y[split_at:])\n-                sample_weight = sample_weight[:split_at]\n-                if verbose:\n-                    print(\"Train on %d samples, validate on %d samples\" % (len(y), len(y_val)))\n-\n-        index_array = np.arange(len(y))\n-\n-        if verbose:\n-            callbacks = [cbks.BaseLogger()] + callbacks\n-        callbacks = cbks.CallbackList([cbks.History()] + callbacks)\n-\n-        callbacks._set_model(self)\n-        callbacks._set_params({\n-            'batch_size': batch_size,\n-            'nb_epoch': nb_epoch,\n-            'nb_sample': len(y),\n-            'verbose': verbose,\n-            'do_validation': do_validation,\n-            'show_accuracy': show_accuracy\n-        })\n-        callbacks.on_train_begin()\n-\n-        self.stop_training = False\n-        for epoch in range(nb_epoch):\n-            callbacks.on_epoch_begin(epoch)\n-            if shuffle:\n-                np.random.shuffle(index_array)\n-\n-            batches = make_batches(len(y), batch_size)\n-            for batch_index, (batch_start, batch_end) in enumerate(batches):\n-                batch_ids = index_array[batch_start:batch_end]\n-                X_batch = slice_X(X, batch_ids)\n-                y_batch = y[batch_ids]\n-                weight_batch = sample_weight[batch_ids]\n-\n-                batch_logs = {}\n-                batch_logs['batch'] = batch_index\n-                batch_logs['size'] = len(batch_ids)\n-                callbacks.on_batch_begin(batch_index, batch_logs)\n-\n-                ins = X_batch + [y_batch, weight_batch]\n         if show_accuracy:\n-                    loss, acc = self._train_with_acc(*ins)\n-                    batch_logs['accuracy'] = acc\n+            f = self._train_with_acc\n+            out_labels = ['loss', 'acc']\n         else:\n-                    loss = self._train(*ins)\n-                batch_logs['loss'] = loss\n+            f = self._train\n+            out_labels = ['loss']\n \n-                callbacks.on_batch_end(batch_index, batch_logs)\n+        ins = X + [y, sample_weight]\n \n-                if batch_index == len(batches) - 1: # last batch\n-                    # validation\n-                    epoch_logs = {}\n-                    if do_validation:\n-                        if show_accuracy:\n-                            val_loss, val_acc = self.evaluate(X_val, y_val, batch_size=batch_size, \\\n-                                verbose=0, show_accuracy=True)\n-                            epoch_logs['val_accuracy'] = val_acc\n-                        else:\n-                            val_loss = self.evaluate(X_val, y_val, batch_size=batch_size, verbose=0)\n-                        epoch_logs['val_loss'] = val_loss\n-\n-            callbacks.on_epoch_end(epoch, epoch_logs)\n-            if self.stop_training:\n-                break\n-\n-        callbacks.on_train_end()\n-        return callbacks.callbacks[0] # return history\n+        return self._fit(f, ins, out_labels=out_labels, batch_size=batch_size, nb_epoch=nb_epoch, verbose=verbose, callbacks=callbacks, \\\n+            validation_split=validation_split, val_f=val_f, val_ins=val_ins, shuffle=shuffle)\n \n \n     def predict(self, X, batch_size=128, verbose=1):\n@@ -332,31 +371,6 @@ class Model(object):\n             return tot_score / seen\n \n \n-class Sequential(Model, containers.Sequential):\n-    '''\n-        Inherits from Model the following methods:\n-            - compile\n-            - train\n-            - test\n-            - evaluate\n-            - fit\n-            - predict\n-            - predict_proba\n-            - predict_classes\n-        Inherits from containers.Sequential the following methods:\n-            - add \n-            - get_output\n-            - get_input\n-            - get_weights\n-            - set_weights\n-    '''\n-    def __init__(self):\n-        self.layers = []\n-        self.params = [] # learnable\n-        self.regularizers = [] # same size as params\n-        self.constraints = [] # same size as params\n-\n-\n     def get_config(self, verbose=0):\n         layers = []\n         for i, l in enumerate(self.layers):\n@@ -407,3 +421,61 @@ class Sequential(Model, containers.Sequential):\n             weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n             self.layers[k].set_weights(weights)\n         f.close()\n+\n+\n+class Graph(containers.Graph):\n+    def compile(self, optimizer, loss, theano_mode='DebugMode'):\n+        # loss is a dictionary mapping output name to loss functions\n+        ys = []\n+        ys_train = []\n+        ys_test = []\n+        train_loss = 0.\n+        test_loss = 0.\n+        for output_name in self.output_order:\n+            loss_fn = loss[output_name]\n+            output = self.outputs[output_name]\n+            y_train = output.get_output(True)\n+            y_test = output.get_output(False)\n+            y = T.zeros_like(y_test)\n+            ys.append(y)\n+            ys_train.append(y_train)\n+            ys_test.append(y_test)\n+            \n+            train_loss += objectives.get(loss_fn)(y, y_train).mean()\n+            test_loss += objectives.get(loss_fn)(y, y_test).mean()\n+\n+        train_loss.name = 'train_loss'\n+        test_loss.name = 'test_loss'\n+\n+        ins = [self.inputs[name].input for name in self.input_order]\n+        train_ins = ins + ys\n+        test_ins = ins + ys\n+\n+        for r in self.regularizers:\n+            train_loss = r(train_loss)\n+        self.optimizer = optimizers.get(optimizer)\n+        updates = self.optimizer.get_updates(self.params, self.constraints, train_loss)\n+\n+        self._train = theano.function(train_ins, train_loss, \n+            updates=updates, allow_input_downcast=True, mode=theano_mode)\n+        self._test = theano.function(test_ins, test_loss, \n+            allow_input_downcast=True, mode=theano_mode)\n+        self._predict = theano.function(inputs=ins, outputs=ys_test, \n+            allow_input_downcast=True, mode=theano_mode)\n+\n+    def train(self, data):\n+        # data is a dictionary mapping output and input names to arrays\n+        ins = [data[name] for name in self.input_order] + [data[name] for name in self.output_order]\n+        return self._train(*ins)\n+\n+    def test(self, data):\n+        # data is a dictionary mapping input names to arrays\n+        ins = [data[name] for name in self.input_order] + [data[name] for name in self.output_order]\n+        return self._test(*ins)\n+\n+    def predict(self, data):\n+        # data is a dictionary mapping input names to arrays\n+        ins = [data[name] for name in self.input_order]\n+        return self._predict(*ins)\n+\n+\n\n@@ -0,0 +1,106 @@\n+from keras.models import Graph, Sequential\n+from keras.layers import containers\n+from keras.layers.core import Dense\n+import numpy as np\n+\n+X = np.random.random((100, 32))\n+X2 = np.random.random((100, 32))\n+y = np.random.random((100, 4))\n+y2 = np.random.random((100, 4))\n+\n+print 'test a non-sequential graph with 1 input and 1 output'\n+graph = Graph()\n+graph.add_input(name='input1', ndim=2)\n+\n+graph.add_node(Dense(32, 16), name='dense1', input='input1')\n+graph.add_node(Dense(32, 4), name='dense2', input='input1')\n+graph.add_node(Dense(16, 4), name='dense3', input='dense1')\n+\n+graph.add_output(name='output1', inputs=['dense2', 'dense3'], merge_mode='sum')\n+graph.compile('rmsprop', {'output1':'mse'})\n+\n+out = graph.predict({'input1':X})\n+loss = graph.test({'input1':X, 'output1':y})\n+loss = graph.train({'input1':X, 'output1':y})\n+print loss\n+\n+\n+print 'test a more complex non-sequential graph with 1 input and 1 output'\n+graph = Graph()\n+graph.add_input(name='input1', ndim=2)\n+\n+graph.add_node(Dense(32, 16), name='dense1', input='input1')\n+graph.add_node(Dense(32, 4), name='dense2', input='input1')\n+\n+graph.add_node(Dense(4, 16), name='dense3', input='dense2')\n+graph.add_node(Dense(16, 4), name='dense4', inputs=['dense1', 'dense3'], merge_mode='sum')\n+\n+graph.add_output(name='output1', inputs=['dense2', 'dense4'], merge_mode='sum')\n+graph.compile('rmsprop', {'output1':'mse'})\n+\n+out = graph.predict({'input1':X})\n+loss = graph.test({'input1':X, 'output1':y})\n+loss = graph.train({'input1':X, 'output1':y})\n+print loss\n+\n+\n+print 'test a non-sequential graph with 2 inputs and 1 output'\n+graph = Graph()\n+graph.add_input(name='input1', ndim=2)\n+graph.add_input(name='input2', ndim=2)\n+\n+graph.add_node(Dense(32, 16), name='dense1', input='input1')\n+graph.add_node(Dense(32, 4), name='dense2', input='input2')\n+graph.add_node(Dense(16, 4), name='dense3', input='dense1')\n+\n+graph.add_output(name='output1', inputs=['dense2', 'dense3'], merge_mode='sum')\n+graph.compile('rmsprop', {'output1':'mse'})\n+\n+out = graph.predict({'input1':X, 'input2':X2})\n+loss = graph.test({'input1':X, 'input2':X2, 'output1':y})\n+loss = graph.train({'input1':X, 'input2':X2, 'output1':y})\n+print loss\n+\n+\n+print 'test a non-sequential graph with 1 input and 2 outputs'\n+graph = Graph()\n+graph.add_input(name='input1', ndim=2)\n+\n+graph.add_node(Dense(32, 16), name='dense1', input='input1')\n+graph.add_node(Dense(32, 4), name='dense2', input='input1')\n+graph.add_node(Dense(16, 4), name='dense3', input='dense1')\n+\n+graph.add_output(name='output1', input='dense2')\n+graph.add_output(name='output2', input='dense3')\n+graph.compile('rmsprop', {'output1':'mse', 'output2':'mse'})\n+\n+out = graph.predict({'input1':X})\n+loss = graph.test({'input1':X, 'output1':y, 'output2':y2})\n+loss = graph.train({'input1':X, 'output1':y, 'output2':y2})\n+print loss\n+\n+\n+print 'test layer-like API'\n+\n+graph = containers.Graph()\n+graph.add_input(name='input1', ndim=2)\n+graph.add_node(Dense(32, 16), name='dense1', input='input1')\n+graph.add_node(Dense(32, 4), name='dense2', input='input1')\n+graph.add_node(Dense(16, 4), name='dense3', input='dense1')\n+graph.add_output(name='output1', inputs=['dense2', 'dense3'], merge_mode='sum')\n+\n+seq = Sequential()\n+seq.add(Dense(32, 32, name='first_seq_dense'))\n+seq.add(graph)\n+seq.add(Dense(4, 4, name='last_seq_dense'))\n+\n+print seq.params\n+print seq.layers\n+print 'input:'\n+print seq.get_input()\n+print 'output:'\n+print seq.get_output()\n+seq.compile('rmsprop', 'mse')\n+\n+loss = seq.fit(X, y, batch_size=10, nb_epoch=1)\n+print loss\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
