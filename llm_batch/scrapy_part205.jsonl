{"custom_id": "scrapy#9bd7af8a625d63a3372346bf6c69c80a2a9832a8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 660 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -2,7 +2,7 @@ import six\n import json\n import copy\n import warnings\n-from collections import Mapping, MutableMapping\n+from collections import MutableMapping\n from importlib import import_module\n \n from scrapy.utils.deprecate import create_deprecated_class\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9eb3597d159a8556259946b7acf39d3c368113dc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 666 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -19,6 +19,7 @@ SETTINGS_PRIORITIES = {\n     'cmdline': 40,\n }\n \n+\n def get_settings_priority(priority):\n     \"\"\"\n     Small helper function that looks up a given string priority in the\n@@ -196,8 +197,8 @@ class BaseSettings(MutableMapping):\n \n     def _getcomposite(self, name):\n         # DO NOT USE THIS FUNCTION IN YOUR CUSTOM PROJECTS\n-        # It's for internal use in the transition away from the _BASE settings and\n-        # will be removed along with _BASE support in a future release\n+        # It's for internal use in the transition away from the _BASE settings\n+        # and will be removed along with _BASE support in a future release\n         basename = name + \"_BASE\"\n         if basename in self:\n             warnings.warn('_BASE settings are deprecated.',\n@@ -482,6 +483,7 @@ def iter_default_settings():\n         if name.isupper():\n             yield name, getattr(default_settings, name)\n \n+\n def overridden_settings(settings):\n     \"\"\"Return a dict of the settings that have been overridden\"\"\"\n     for name, defvalue in iter_default_settings():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#90198e5324f3172ec83c457049a7a66a2805d875", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 669 | Contributors (this commit): 5 | Commits (past 90d): 6 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -378,7 +378,8 @@ class BaseSettings(MutableMapping):\n     def __str__(self):\n         return str(self.attributes)\n \n-    __repr__ = __str__\n+    def __repr__(self):\n+        return \"<%s %s>\" % (self.__class__.__name__, self.attributes)\n \n     @property\n     def overrides(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f249b309ab779b5ab518f54f309d7a4ac6661ec7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 26 | Files Changed: 9 | Hunks: 21 | Methods Changed: 10 | Complexity Δ (Sum/Max): 3/6 | Churn Δ: 62 | Churn Cumulative: 1576 | Contributors (this commit): 17 | Commits (past 90d): 33 | Contributors (cumulative): 53 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,6 +1,7 @@\n import os\n from scrapy.commands import ScrapyCommand\n-from scrapy.utils.conf import arglist_to_dict, remove_none_values\n+from scrapy.utils.conf import arglist_to_dict\n+from scrapy.utils.python import without_none_values\n from scrapy.exceptions import UsageError\n \n \n@@ -34,7 +35,7 @@ class Command(ScrapyCommand):\n                 self.settings.set('FEED_URI', 'stdout:', priority='cmdline')\n             else:\n                 self.settings.set('FEED_URI', opts.output, priority='cmdline')\n-            feed_exporters = remove_none_values(self.settings._getcomposite('FEED_EXPORTERS'))\n+            feed_exporters = without_none_values(self.settings._getcomposite('FEED_EXPORTERS'))\n             valid_output_formats = feed_exporters.keys()\n             if not opts.output_format:\n                 opts.output_format = os.path.splitext(opts.output)[1].replace(\".\", \"\")\n\n@@ -5,7 +5,8 @@ from importlib import import_module\n from scrapy.utils.spider import iter_spider_classes\n from scrapy.commands import ScrapyCommand\n from scrapy.exceptions import UsageError\n-from scrapy.utils.conf import arglist_to_dict, remove_none_values\n+from scrapy.utils.conf import arglist_to_dict\n+from scrapy.utils.python import without_none_values\n \n \n def _import_file(filepath):\n@@ -57,7 +58,7 @@ class Command(ScrapyCommand):\n                 self.settings.set('FEED_URI', 'stdout:', priority='cmdline')\n             else:\n                 self.settings.set('FEED_URI', opts.output, priority='cmdline')\n-            feed_exporters = remove_none_values(self.settings._getcomposite('FEED_EXPORTERS'))\n+            feed_exporters = without_none_values(self.settings._getcomposite('FEED_EXPORTERS'))\n             valid_output_formats = feed_exporters.keys()\n             if not opts.output_format:\n                 opts.output_format = os.path.splitext(opts.output)[1].replace(\".\", \"\")\n\n@@ -4,9 +4,9 @@ import logging\n from twisted.internet import defer\n import six\n from scrapy.exceptions import NotSupported, NotConfigured\n-from scrapy.utils.conf import remove_none_values\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.misc import load_object\n+from scrapy.utils.python import without_none_values\n from scrapy import signals\n \n \n@@ -20,7 +20,7 @@ class DownloadHandlers(object):\n         self._schemes = {}  # stores acceptable schemes on instancing\n         self._handlers = {}  # stores instanced handlers for schemes\n         self._notconfigured = {}  # remembers failed handlers\n-        handlers = remove_none_values(crawler.settings._getcomposite('DOWNLOAD_HANDLERS'))\n+        handlers = without_none_values(crawler.settings._getcomposite('DOWNLOAD_HANDLERS'))\n         for scheme, clspath in six.iteritems(handlers):\n             self._schemes[scheme] = clspath\n \n\n@@ -4,7 +4,7 @@ DefaultHeaders downloader middleware\n See documentation in docs/topics/downloader-middleware.rst\n \"\"\"\n \n-from scrapy.utils.conf import remove_none_values\n+from scrapy.utils.python import without_none_values\n \n \n class DefaultHeadersMiddleware(object):\n@@ -14,7 +14,7 @@ class DefaultHeadersMiddleware(object):\n \n     @classmethod\n     def from_crawler(cls, crawler):\n-        headers = remove_none_values(crawler.settings['DEFAULT_REQUEST_HEADERS'])\n+        headers = without_none_values(crawler.settings['DEFAULT_REQUEST_HEADERS'])\n         return cls(headers.items())\n \n     def process_request(self, request, spider):\n\n@@ -18,11 +18,11 @@ from twisted.internet import defer, threads\n from w3lib.url import file_uri_to_path\n \n from scrapy import signals\n-from scrapy.utils.conf import remove_none_values\n from scrapy.utils.ftp import ftp_makedirs_cwd\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.misc import load_object\n from scrapy.utils.log import failure_to_exc_info\n+from scrapy.utils.python import without_none_values\n \n logger = logging.getLogger(__name__)\n \n@@ -196,7 +196,7 @@ class FeedExporter(object):\n         return item\n \n     def _load_components(self, setting_prefix):\n-        conf = remove_none_values(self.settings._getcomposite(setting_prefix))\n+        conf = without_none_values(self.settings._getcomposite(setting_prefix))\n         d = {}\n         for k, v in conf.items():\n             try:\n\n@@ -1,6 +1,5 @@\n import os\n import sys\n-import warnings\n from operator import itemgetter\n \n import six\n@@ -8,6 +7,7 @@ from six.moves.configparser import SafeConfigParser\n \n from scrapy.settings import BaseSettings\n from scrapy.utils.deprecate import update_classpath\n+from scrapy.utils.python import without_none_values\n \n \n def build_component_list(compdict, convert=update_classpath):\n@@ -37,15 +37,10 @@ def build_component_list(compdict, convert=update_classpath):\n     if isinstance(compdict, (list, tuple)):\n         _check_components(compdict)\n         return type(compdict)(convert(c) for c in compdict)\n-    compdict = remove_none_values(_map_keys(compdict))\n+    compdict = without_none_values(_map_keys(compdict))\n     return [k for k, v in sorted(six.iteritems(compdict), key=itemgetter(1))]\n \n \n-def remove_none_values(compdict):\n-    \"\"\"Return dict with all pairs that have value 'None' removed\"\"\"\n-    return {k: v for k, v in six.iteritems(compdict) if v is not None}\n-\n-\n def arglist_to_dict(arglist):\n     \"\"\"Convert a list of arguments like ['arg1=val1', 'arg2=val2', ...] to a\n     dict\n\n@@ -324,3 +324,15 @@ def retry_on_eintr(function, *args, **kw):\n         except IOError as e:\n             if e.errno != errno.EINTR:\n                 raise\n+\n+\n+def without_none_values(iterable):\n+    \"\"\"Return a copy of `iterable` with all `None` entries removed.\n+\n+    If `iterable` is a mapping, return a dictionary where all pairs that have\n+    value `None` have been removed.\n+    \"\"\"\n+    try:\n+        return {k: v for k, v in six.iteritems(iterable) if v is not None}\n+    except AttributeError:\n+        return type(iterable)((v for v in iterable if v is not None))\n\n@@ -1,8 +1,7 @@\n import unittest\n \n from scrapy.settings import BaseSettings\n-from scrapy.utils.conf import (build_component_list, arglist_to_dict,\n-                               remove_none_values)\n+from scrapy.utils.conf import build_component_list, arglist_to_dict\n \n \n class BuildComponentListTest(unittest.TestCase):\n@@ -53,12 +52,6 @@ class BuildComponentListTest(unittest.TestCase):\n \n class UtilsConfTestCase(unittest.TestCase):\n \n-    def test_remove_none_values(self):\n-        comps = {'one': 1, 'none': None, 'three': 3, 'four': 4}\n-        compscopy = dict(comps)\n-        del compscopy['none']\n-        self.assertEqual(remove_none_values(comps), compscopy)\n-\n     def test_arglist_to_dict(self):\n         self.assertEqual(arglist_to_dict(['arg1=val1', 'arg2=val2']),\n             {'arg1': 'val1', 'arg2': 'val2'})\n\n@@ -6,7 +6,8 @@ import six\n \n from scrapy.utils.python import (\n     memoizemethod_noargs, isbinarytext, equal_attributes,\n-    WeakKeyCache, stringify_dict, get_func_args, to_bytes, to_unicode)\n+    WeakKeyCache, stringify_dict, get_func_args, to_bytes, to_unicode,\n+    without_none_values)\n \n __doctests__ = ['scrapy.utils.python']\n \n@@ -212,5 +213,12 @@ class UtilsPythonTestCase(unittest.TestCase):\n         self.assertEqual(get_func_args(\" \".join), [])\n         self.assertEqual(get_func_args(operator.itemgetter(2)), [])\n \n+    def test_without_none_values(self):\n+        self.assertEqual(without_none_values([1, None, 3, 4]), [1, 3, 4])\n+        self.assertEqual(without_none_values((1, None, 3, 4)), (1, 3, 4))\n+        self.assertEqual(\n+            without_none_values({'one': 1, 'none': None, 'three': 3, 'four': 4}),\n+            {'one': 1, 'three': 3, 'four': 4})\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#03f1720afb4a437314659a306286f440df664a0b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 25 | Churn Cumulative: 959 | Contributors (this commit): 6 | Commits (past 90d): 5 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -203,10 +203,16 @@ class BaseSettings(MutableMapping):\n         if basename in self:\n             warnings.warn('_BASE settings are deprecated.',\n                           category=ScrapyDeprecationWarning)\n-            compsett = BaseSettings(self[name + \"_BASE\"], priority='default')\n-            compsett.update(self[name])\n+            # When users defined a _BASE setting, they explicitly don't want to\n+            # use any of Scrapy's defaults. Therefore, we only use these entries\n+            # from self[name] (where the defaults now live) that have a priority\n+            # higher than 'default'\n+            compsett = BaseSettings(self[basename], priority='default')\n+            for k in self[name]:\n+                prio = self[name].getpriority(k)\n+                if prio > get_settings_priority('default'):\n+                    compsett.set(k, self[name][k], prio)\n             return compsett\n-        else:\n         return self[name]\n \n     def getpriority(self, name):\n\n@@ -252,12 +252,17 @@ class BaseSettingsTest(unittest.TestCase):\n \n     def test_getcomposite(self):\n         s = BaseSettings({'TEST_BASE': {1: 1, 2: 2},\n-                          'TEST': BaseSettings({1: 10}),\n-                          'HASNOBASE': BaseSettings({1: 1})})\n+                          'TEST': BaseSettings({1: 10, 3: 30}, 'default'),\n+                          'HASNOBASE': BaseSettings({1: 1}, 'default')})\n+        s['TEST'].set(4, 4, priority='project')\n+        # When users specify a _BASE setting they explicitly don't want to use\n+        # Scrapy's defaults, so we don't want to see anything that has a\n+        # 'default' priority from TEST\n         cs = s._getcomposite('TEST')\n-        self.assertEqual(len(cs), 2)\n-        self.assertEqual(cs[1], 10)\n+        self.assertEqual(len(cs), 3)\n+        self.assertEqual(cs[1], 1)\n         self.assertEqual(cs[2], 2)\n+        self.assertEqual(cs[4], 4)\n         cs = s._getcomposite('HASNOBASE')\n         self.assertEqual(len(cs), 1)\n         self.assertEqual(cs[1], 1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c34dbe955d1a72ddef97afa1f0fb92becd9f4ca3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 5 | Churn Cumulative: 724 | Contributors (this commit): 17 | Commits (past 90d): 3 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -11,6 +11,7 @@ from parsel.selector import create_root_node\n import six\n from scrapy.http.request import Request\n from scrapy.utils.python import to_bytes, is_listlike\n+from scrapy.utils.response import get_base_url\n \n \n class FormRequest(Request):\n@@ -44,7 +45,7 @@ class FormRequest(Request):\n \n def _get_form_url(form, url):\n     if url is None:\n-        return form.action or form.base_url\n+        return urljoin(form.base_url, form.action)\n     return urljoin(form.base_url, url)\n \n \n@@ -58,7 +59,7 @@ def _urlencode(seq, enc):\n def _get_form(response, formname, formid, formnumber, formxpath):\n     \"\"\"Find the form element \"\"\"\n     text = response.body_as_unicode()\n-    root = create_root_node(text, lxml.html.HTMLParser, base_url=response.url)\n+    root = create_root_node(text, lxml.html.HTMLParser, base_url=get_base_url(response))\n     forms = root.xpath('//form')\n     if not forms:\n         raise ValueError(\"No <form> element found in %s\" % response)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e379f58cad0289d725a5241606542d0e20ecd73d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 4 | Churn Cumulative: 19 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,7 @@\n import re\n from six.moves.urllib.parse import urljoin\n \n-from w3lib.html import remove_tags, replace_entities, replace_escape_chars\n+from w3lib.html import remove_tags, replace_entities, replace_escape_chars, get_base_url\n \n from scrapy.link import Link\n from .sgml import SgmlLinkExtractor\n@@ -31,7 +31,7 @@ class RegexLinkExtractor(SgmlLinkExtractor):\n             return clean_url\n \n         if base_url is None:\n-            base_url = urljoin(response_url, self.base_url) if self.base_url else response_url\n+            base_url = get_base_url(response_text, response_url, response_encoding)\n \n         links_text = linkre.findall(response_text)\n         return [Link(clean_url(url).encode(response_encoding),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8307c1212f48c83faf1f331f0bcafb439dbeb89c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 64 | Lines Deleted: 1 | Files Changed: 4 | Hunks: 6 | Methods Changed: 7 | Complexity Δ (Sum/Max): 9/3 | Churn Δ: 65 | Churn Cumulative: 3834 | Contributors (this commit): 21 | Commits (past 90d): 17 | Contributors (cumulative): 36 | DMM Complexity: 1.0\n\nDIFF:\n@@ -84,6 +84,21 @@ class ExecutionEngine(object):\n         dfd = self._close_all_spiders()\n         return dfd.addBoth(lambda _: self._finish_stopping_engine())\n \n+    def close(self):\n+        \"\"\"Close the execution engine gracefully.\n+\n+        If it has already been started, stop it. In all cases, close all spiders\n+        and the downloader.\n+        \"\"\"\n+        if self.running:\n+            # Will also close spiders and downloader\n+            return self.stop()\n+        elif self.open_spiders:\n+            # Will also close downloader\n+            return self._close_all_spiders()\n+        else:\n+            return defer.succeed(self.downloader.close())\n+\n     def pause(self):\n         \"\"\"Pause the execution engine\"\"\"\n         self.paused = True\n\n@@ -73,8 +73,11 @@ class Crawler(object):\n             yield self.engine.open_spider(self.spider, start_requests)\n             yield defer.maybeDeferred(self.engine.start)\n         except Exception:\n+            exc = defer.fail()\n             self.crawling = False\n-            raise\n+            if self.engine is not None:\n+                yield self.engine.close()\n+            yield exc\n \n     def _create_spider(self, *args, **kwargs):\n         return self.spidercls.from_crawler(self, *args, **kwargs)\n\n@@ -226,3 +226,24 @@ with multiples lines\n         s = dict(est[0])\n         self.assertEqual(s['engine.spider.name'], crawler.spider.name)\n         self.assertEqual(s['len(engine.scraper.slot.active)'], 1)\n+\n+    @defer.inlineCallbacks\n+    def test_graceful_crawl_error_handling(self):\n+        \"\"\"\n+        Test whether errors happening anywhere in Crawler.crawl() are properly\n+        reported (and not somehow swallowed) after a graceful engine shutdown.\n+        The errors should not come from within Scrapy's core but from within\n+        spiders/middlewares/etc., e.g. raised in Spider.start_requests(),\n+        SpiderMiddleware.process_start_requests(), etc.\n+        \"\"\"\n+\n+        class TestError(Exception):\n+            pass\n+\n+        class FaultySpider(SimpleSpider):\n+            def start_requests(self):\n+                raise TestError\n+\n+        crawler = get_crawler(FaultySpider)\n+        yield self.assertFailure(crawler.crawl(), TestError)\n+        self.assertFalse(crawler.crawling)\n\n@@ -19,6 +19,7 @@ from twisted.web import server, static, util\n from twisted.trial import unittest\n \n from scrapy import signals\n+from scrapy.core.engine import ExecutionEngine\n from scrapy.utils.test import get_crawler\n from pydispatch import dispatcher\n from tests import tests_datadir\n@@ -234,6 +235,29 @@ class EngineTest(unittest.TestCase):\n         self.assertEqual({'spider': self.run.spider, 'reason': 'finished'},\n                          self.run.signals_catched[signals.spider_closed])\n \n+    @defer.inlineCallbacks\n+    def test_close_downloader(self):\n+        e = ExecutionEngine(get_crawler(TestSpider), lambda: None)\n+        yield e.close()\n+\n+    @defer.inlineCallbacks\n+    def test_close_spiders_downloader(self):\n+        e = ExecutionEngine(get_crawler(TestSpider), lambda: None)\n+        yield e.open_spider(TestSpider(), [])\n+        self.assertEqual(len(e.open_spiders), 1)\n+        yield e.close()\n+        self.assertEqual(len(e.open_spiders), 0)\n+\n+    @defer.inlineCallbacks\n+    def test_close_engine_spiders_downloader(self):\n+        e = ExecutionEngine(get_crawler(TestSpider), lambda: None)\n+        yield e.open_spider(TestSpider(), [])\n+        e.start()\n+        self.assertTrue(e.running)\n+        yield e.close()\n+        self.assertFalse(e.running)\n+        self.assertEqual(len(e.open_spiders), 0)\n+\n \n if __name__ == \"__main__\":\n     if len(sys.argv) > 1 and sys.argv[1] == 'runserver':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#51ca84c9b4278e92c9e08e38adfd8819c964e3be", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 24 | Churn Cumulative: 302 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,11 +3,20 @@ import unittest\n import warnings\n \n from scrapy.settings import (BaseSettings, Settings, SettingsAttribute,\n-                             CrawlerSettings)\n+                             CrawlerSettings, SETTINGS_PRIORITIES,\n+                             get_settings_priority)\n from tests import mock\n from . import default_settings\n \n \n+class SettingsGlobalFuncsTest(unittest.TestCase):\n+\n+    def test_get_settings_priority(self):\n+        for prio_str, prio_num in six.iteritems(SETTINGS_PRIORITIES):\n+            self.assertEqual(get_settings_priority(prio_str), prio_num)\n+        self.assertEqual(get_settings_priority(99), 99)\n+\n+\n class SettingsAttributeTest(unittest.TestCase):\n \n     def setUp(self):\n@@ -45,6 +54,10 @@ class SettingsAttributeTest(unittest.TestCase):\n         self.assertEqual(attribute.value['one'], 12)\n         self.assertEqual(attribute.value['two'], 21)\n \n+    def test_repr(self):\n+        self.assertEqual(repr(self.attribute),\n+                         \"<SettingsAttribute value='value' priority=10>\")\n+\n \n class BaseSettingsTest(unittest.TestCase):\n \n@@ -329,7 +342,6 @@ class BaseSettingsTest(unittest.TestCase):\n             self.assertEqual(self.settings.get('FOO'), 'fez')\n             self.assertEqual(self.settings.overrides.get('FOO'), 'fez')\n \n-\n     def test_deprecated_attribute_defaults(self):\n         self.settings.set('BAR', 'fuz', priority='default')\n         with warnings.catch_warnings(record=True) as w:\n@@ -339,6 +351,14 @@ class BaseSettingsTest(unittest.TestCase):\n             self.assertEqual(self.settings.defaults.get('BAR'), 'foo')\n             self.assertIn('BAR', self.settings.defaults)\n \n+    def test_repr(self):\n+        settings = BaseSettings()\n+        self.assertEqual(repr(settings), \"<BaseSettings {}>\")\n+        attr = SettingsAttribute('testval', 15)\n+        settings['testkey'] = attr\n+        self.assertEqual(repr(settings),\n+                         \"<BaseSettings {'testkey': %s}>\" % repr(attr))\n+\n \n class SettingsTest(unittest.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#32ff4cc1d7e2c4fcb3427bd37ffbbe74ec95e174", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 308 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -39,8 +39,7 @@ class SettingsAttributeTest(unittest.TestCase):\n \n     def test_set_per_key_priorities(self):\n         attribute = SettingsAttribute(\n-                        BaseSettings({'one': 10, 'two': 20}, 0),\n-                        0)\n+            BaseSettings({'one': 10, 'two': 20}, 0), 0)\n \n         new_dict = {'one': 11, 'two': 21}\n         attribute.set(new_dict, 10)\n@@ -165,7 +164,8 @@ class BaseSettingsTest(unittest.TestCase):\n     def test_update(self):\n         settings = BaseSettings({'key_lowprio': 0}, priority=0)\n         settings.set('key_highprio', 10, priority=50)\n-        custom_settings = BaseSettings({'key_lowprio': 1, 'key_highprio': 11}, priority=30)\n+        custom_settings = BaseSettings({'key_lowprio': 1, 'key_highprio': 11},\n+                                       priority=30)\n         custom_settings.set('newkey_one', None, priority=50)\n         custom_dict = {'key_lowprio': 2, 'key_highprio': 12, 'newkey_two': None}\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#11b11c9803fa7f1ed41c0c34076336d701676235", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 19 | Churn Cumulative: 1370 | Contributors (this commit): 10 | Commits (past 90d): 3 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -148,9 +148,7 @@ class CrawlerRunner(object):\n \n         :param dict kwargs: keyword arguments to initialize the spider\n         \"\"\"\n-        crawler = crawler_or_spidercls\n-        if not isinstance(crawler_or_spidercls, Crawler):\n-            crawler = self._create_crawler(crawler_or_spidercls)\n+        crawler = self.create_crawler(crawler_or_spidercls)\n         return self._crawl(crawler, *args, **kwargs)\n \n     def _crawl(self, crawler, *args, **kwargs):\n@@ -165,6 +163,21 @@ class CrawlerRunner(object):\n \n         return d.addBoth(_done)\n \n+    def create_crawler(self, crawler_or_spidercls):\n+        \"\"\"\n+        Return a :class:`~scrapy.crawler.Crawler` object.\n+\n+        * If `crawler_or_spidercls` is a Crawler, it is returned as-is.\n+        * If `crawler_or_spidercls` is a Spider subclass, a new Crawler\n+          is constructed for it.\n+        * If `crawler_or_spidercls` is a string, this function finds\n+          a spider with this name in a Scrapy project (using spider loader),\n+          then creates a Crawler instance for it.\n+        \"\"\"\n+        if isinstance(crawler_or_spidercls, Crawler):\n+            return crawler_or_spidercls\n+        return self._create_crawler(crawler_or_spidercls)\n+\n     def _create_crawler(self, spidercls):\n         if isinstance(spidercls, six.string_types):\n             spidercls = self.spider_loader.load(spidercls)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a49c82ad625ee96b50489d0b05c323b7a8107830", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 52 | Lines Deleted: 20 | Files Changed: 3 | Hunks: 25 | Methods Changed: 22 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 72 | Churn Cumulative: 540 | Contributors (this commit): 9 | Commits (past 90d): 5 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -30,7 +30,7 @@ def get_crawler(spidercls=None, settings_dict=None):\n     from scrapy.spiders import Spider\n \n     runner = CrawlerRunner(Settings(settings_dict))\n-    return runner._create_crawler(spidercls or Spider)\n+    return runner.create_crawler(spidercls or Spider)\n \n def get_pythonpath():\n     \"\"\"Return a PYTHONPATH suitable to use in processes so that they find this\n\n@@ -6,12 +6,12 @@ from testfixtures import LogCapture\n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n \n-from scrapy.utils.test import get_crawler\n+from scrapy.http import Request\n+from scrapy.crawler import CrawlerRunner\n from tests import mock\n from tests.spiders import FollowAllSpider, DelaySpider, SimpleSpider, \\\n     BrokenStartRequestsSpider, SingleRequestSpider, DuplicateStartRequestsSpider\n from tests.mockserver import MockServer\n-from scrapy.http import Request\n \n \n class CrawlTestCase(TestCase):\n@@ -19,13 +19,14 @@ class CrawlTestCase(TestCase):\n     def setUp(self):\n         self.mockserver = MockServer()\n         self.mockserver.__enter__()\n+        self.runner = CrawlerRunner()\n \n     def tearDown(self):\n         self.mockserver.__exit__(None, None, None)\n \n     @defer.inlineCallbacks\n     def test_follow_all(self):\n-        crawler = get_crawler(FollowAllSpider)\n+        crawler = self.runner.create_crawler(FollowAllSpider)\n         yield crawler.crawl()\n         self.assertEqual(len(crawler.spider.urls_visited), 11)  # 10 + start_url\n \n@@ -41,7 +42,7 @@ class CrawlTestCase(TestCase):\n     @defer.inlineCallbacks\n     def _test_delay(self, delay, randomize):\n         settings = {\"DOWNLOAD_DELAY\": delay, 'RANDOMIZE_DOWNLOAD_DELAY': randomize}\n-        crawler = get_crawler(FollowAllSpider, settings)\n+        crawler = CrawlerRunner(settings).create_crawler(FollowAllSpider)\n         yield crawler.crawl(maxlatency=delay * 2)\n         t = crawler.spider.times\n         totaltime = t[-1] - t[0]\n@@ -52,7 +53,7 @@ class CrawlTestCase(TestCase):\n \n     @defer.inlineCallbacks\n     def test_timeout_success(self):\n-        crawler = get_crawler(DelaySpider)\n+        crawler = self.runner.create_crawler(DelaySpider)\n         yield crawler.crawl(n=0.5)\n         self.assertTrue(crawler.spider.t1 > 0)\n         self.assertTrue(crawler.spider.t2 > 0)\n@@ -60,7 +61,7 @@ class CrawlTestCase(TestCase):\n \n     @defer.inlineCallbacks\n     def test_timeout_failure(self):\n-        crawler = get_crawler(DelaySpider, {\"DOWNLOAD_TIMEOUT\": 0.35})\n+        crawler = CrawlerRunner({\"DOWNLOAD_TIMEOUT\": 0.35}).create_crawler(DelaySpider)\n         yield crawler.crawl(n=0.5)\n         self.assertTrue(crawler.spider.t1 > 0)\n         self.assertTrue(crawler.spider.t2 == 0)\n@@ -75,14 +76,14 @@ class CrawlTestCase(TestCase):\n \n     @defer.inlineCallbacks\n     def test_retry_503(self):\n-        crawler = get_crawler(SimpleSpider)\n+        crawler = self.runner.create_crawler(SimpleSpider)\n         with LogCapture() as l:\n             yield crawler.crawl(\"http://localhost:8998/status?n=503\")\n         self._assert_retried(l)\n \n     @defer.inlineCallbacks\n     def test_retry_conn_failed(self):\n-        crawler = get_crawler(SimpleSpider)\n+        crawler = self.runner.create_crawler(SimpleSpider)\n         with LogCapture() as l:\n             yield crawler.crawl(\"http://localhost:65432/status?n=503\")\n         self._assert_retried(l)\n@@ -91,7 +92,7 @@ class CrawlTestCase(TestCase):\n     def test_retry_dns_error(self):\n         with mock.patch('socket.gethostbyname',\n                         side_effect=socket.gaierror(-5, 'No address associated with hostname')):\n-            crawler = get_crawler(SimpleSpider)\n+            crawler = self.runner.create_crawler(SimpleSpider)\n             with LogCapture() as l:\n                 yield crawler.crawl(\"http://example.com/\")\n             self._assert_retried(l)\n@@ -99,7 +100,7 @@ class CrawlTestCase(TestCase):\n     @defer.inlineCallbacks\n     def test_start_requests_bug_before_yield(self):\n         with LogCapture('scrapy', level=logging.ERROR) as l:\n-            crawler = get_crawler(BrokenStartRequestsSpider)\n+            crawler = self.runner.create_crawler(BrokenStartRequestsSpider)\n             yield crawler.crawl(fail_before_yield=1)\n \n         self.assertEqual(len(l.records), 1)\n@@ -110,7 +111,7 @@ class CrawlTestCase(TestCase):\n     @defer.inlineCallbacks\n     def test_start_requests_bug_yielding(self):\n         with LogCapture('scrapy', level=logging.ERROR) as l:\n-            crawler = get_crawler(BrokenStartRequestsSpider)\n+            crawler = self.runner.create_crawler(BrokenStartRequestsSpider)\n             yield crawler.crawl(fail_yielding=1)\n \n         self.assertEqual(len(l.records), 1)\n@@ -121,7 +122,7 @@ class CrawlTestCase(TestCase):\n     @defer.inlineCallbacks\n     def test_start_requests_lazyness(self):\n         settings = {\"CONCURRENT_REQUESTS\": 1}\n-        crawler = get_crawler(BrokenStartRequestsSpider, settings)\n+        crawler = CrawlerRunner(settings).create_crawler(BrokenStartRequestsSpider)\n         yield crawler.crawl()\n         #self.assertTrue(False, crawler.spider.seedsseen)\n         #self.assertTrue(crawler.spider.seedsseen.index(None) < crawler.spider.seedsseen.index(99),\n@@ -130,7 +131,7 @@ class CrawlTestCase(TestCase):\n     @defer.inlineCallbacks\n     def test_start_requests_dupes(self):\n         settings = {\"CONCURRENT_REQUESTS\": 1}\n-        crawler = get_crawler(DuplicateStartRequestsSpider, settings)\n+        crawler = CrawlerRunner(settings).create_crawler(DuplicateStartRequestsSpider)\n         yield crawler.crawl(dont_filter=True, distinct_urls=2, dupe_factor=3)\n         self.assertEqual(crawler.spider.visited, 6)\n \n@@ -159,7 +160,7 @@ Connection: close\n foo body\n with multiples lines\n '''})\n-        crawler = get_crawler(SimpleSpider)\n+        crawler = self.runner.create_crawler(SimpleSpider)\n         with LogCapture() as l:\n             yield crawler.crawl(\"http://localhost:8998/raw?{0}\".format(query))\n         self.assertEqual(str(l).count(\"Got response 200\"), 1)\n@@ -167,7 +168,7 @@ with multiples lines\n     @defer.inlineCallbacks\n     def test_retry_conn_lost(self):\n         # connection lost after receiving data\n-        crawler = get_crawler(SimpleSpider)\n+        crawler = self.runner.create_crawler(SimpleSpider)\n         with LogCapture() as l:\n             yield crawler.crawl(\"http://localhost:8998/drop?abort=0\")\n         self._assert_retried(l)\n@@ -175,7 +176,7 @@ with multiples lines\n     @defer.inlineCallbacks\n     def test_retry_conn_aborted(self):\n         # connection lost before receiving data\n-        crawler = get_crawler(SimpleSpider)\n+        crawler = self.runner.create_crawler(SimpleSpider)\n         with LogCapture() as l:\n             yield crawler.crawl(\"http://localhost:8998/drop?abort=1\")\n         self._assert_retried(l)\n@@ -194,7 +195,7 @@ with multiples lines\n         req0.meta['next'] = req1\n         req1.meta['next'] = req2\n         req2.meta['next'] = req3\n-        crawler = get_crawler(SingleRequestSpider)\n+        crawler = self.runner.create_crawler(SingleRequestSpider)\n         yield crawler.crawl(seed=req0)\n         # basic asserts in case of weird communication errors\n         self.assertIn('responses', crawler.spider.meta)\n@@ -220,7 +221,7 @@ with multiples lines\n         def cb(response):\n             est.append(get_engine_status(crawler.engine))\n \n-        crawler = get_crawler(SingleRequestSpider)\n+        crawler = self.runner.create_crawler(SingleRequestSpider)\n         yield crawler.crawl(seed='http://localhost:8998/', callback_func=cb)\n         self.assertEqual(len(est), 1, est)\n         s = dict(est[0])\n@@ -244,6 +245,24 @@ with multiples lines\n             def start_requests(self):\n                 raise TestError\n \n-        crawler = get_crawler(FaultySpider)\n+        crawler = self.runner.create_crawler(FaultySpider)\n         yield self.assertFailure(crawler.crawl(), TestError)\n         self.assertFalse(crawler.crawling)\n+\n+    @defer.inlineCallbacks\n+    def test_crawlerrunner_accepts_crawler(self):\n+        crawler = self.runner.create_crawler(SimpleSpider)\n+        with LogCapture() as log:\n+            yield self.runner.crawl(crawler, \"http://localhost:8998/status?n=200\")\n+        self.assertIn(\"Got response 200\", str(log))\n+\n+    @defer.inlineCallbacks\n+    def test_crawl_multiple(self):\n+        self.runner.crawl(SimpleSpider, \"http://localhost:8998/status?n=200\")\n+        self.runner.crawl(SimpleSpider, \"http://localhost:8998/status?n=503\")\n+\n+        with LogCapture() as log:\n+            yield self.runner.join()\n+\n+        self._assert_retried(log)\n+        self.assertIn(\"Got response 200\", str(log))\n\n@@ -8,10 +8,12 @@ from twisted.trial import unittest\n \n # ugly hack to avoid cyclic imports of scrapy.spiders when running this test\n # alone\n+import scrapy\n from scrapy.interfaces import ISpiderLoader\n from scrapy.spiderloader import SpiderLoader\n from scrapy.settings import Settings\n from scrapy.http import Request\n+from scrapy.crawler import CrawlerRunner\n \n module_dir = os.path.dirname(os.path.abspath(__file__))\n \n@@ -76,3 +78,14 @@ class SpiderLoaderTest(unittest.TestCase):\n         settings = Settings({'SPIDER_MODULES': [module]})\n         self.spider_loader = SpiderLoader.from_settings(settings)\n         assert len(self.spider_loader._spiders) == 0\n+\n+    def test_crawler_runner_loading(self):\n+        module = 'tests.test_spiderloader.test_spiders.spider1'\n+        runner = CrawlerRunner({'SPIDER_MODULES': [module]})\n+\n+        self.assertRaisesRegexp(KeyError, 'Spider not found',\n+                                runner.create_crawler, 'spider2')\n+\n+        crawler = runner.create_crawler('spider1')\n+        self.assertTrue(issubclass(crawler.spidercls, scrapy.Spider))\n+        self.assertEqual(crawler.spidercls.name, 'spider1')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0000b6e9efc45d90b4ecd6be8c41d39a7f8b0578", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 185 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -26,10 +26,9 @@ def get_crawler(spidercls=None, settings_dict=None):\n     priority.\n     \"\"\"\n     from scrapy.crawler import CrawlerRunner\n-    from scrapy.settings import Settings\n     from scrapy.spiders import Spider\n \n-    runner = CrawlerRunner(Settings(settings_dict))\n+    runner = CrawlerRunner(settings_dict)\n     return runner.create_crawler(spidercls or Spider)\n \n def get_pythonpath():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#98a2e77a75f0b203ce9841801e342a11f15ad419", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 164 | Contributors (this commit): 9 | Commits (past 90d): 3 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -5,11 +5,11 @@ See documentation in docs/topics/shell.rst\n \"\"\"\n \n from threading import Thread\n+from w3lib.url import any_to_uri\n \n from scrapy.commands import ScrapyCommand\n from scrapy.shell import Shell\n from scrapy.http import Request\n-from scrapy.utils.url import add_http_if_no_scheme\n from scrapy.utils.spider import spidercls_for_request, DefaultSpider\n \n \n@@ -43,7 +43,8 @@ class Command(ScrapyCommand):\n     def run(self, args, opts):\n         url = args[0] if args else None\n         if url:\n-            url = add_http_if_no_scheme(url)\n+            url = any_to_uri(url)\n+\n         spider_loader = self.crawler_process.spider_loader\n \n         spidercls = DefaultSpider\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a41c64bfb97a7bba8ec138345d48b94d8734d27e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 21 | Churn Cumulative: 508 | Contributors (this commit): 15 | Commits (past 90d): 4 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,6 +6,7 @@ Some of the functions that used to be imported from this module have been moved\n to the w3lib.url module. Always import those from there instead.\n \"\"\"\n import posixpath\n+from urlparse import urlsplit, urlunsplit\n from six.moves.urllib.parse import (ParseResult, urlunparse, urldefrag,\n                                     urlparse, parse_qsl, urlencode,\n                                     unquote)\n@@ -114,10 +115,16 @@ def escape_ajax(url):\n \n def add_http_if_no_scheme(url):\n     \"\"\"Add http as the default scheme if it is missing from the url.\"\"\"\n-    if url.startswith('//'):\n-        url = 'http:' + url\n-        return url\n-    parser = parse_url(url)\n-    if not parser.scheme or not parser.netloc:\n-        url = 'http://' + url\n-    return url\n+    parts = urlsplit(url)\n+    scheme = parts.scheme or \"http\"\n+    if parts.netloc:\n+        netloc = parts.netloc\n+        path = parts.path\n+    else:\n+        path_parts = url.split(\"/\", 1)\n+        netloc = path_parts[0]\n+        path = path_parts[1] if len(path_parts) > 1 else \"/\"\n+\n+    return urlunsplit((\n+        scheme, netloc, path, parts.query, parts.fragment\n+    ))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bc9db65358bcae3fc228d4b8099d319cba479ff2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 11 | Churn Cumulative: 175 | Contributors (this commit): 9 | Commits (past 90d): 4 | Contributors (cumulative): 9 | DMM Complexity: 0.0\n\nDIFF:\n@@ -5,11 +5,13 @@ See documentation in docs/topics/shell.rst\n \"\"\"\n \n from threading import Thread\n+import urlparse\n from w3lib.url import any_to_uri\n \n from scrapy.commands import ScrapyCommand\n from scrapy.shell import Shell\n from scrapy.http import Request\n+from scrapy.utils.url import add_http_if_no_scheme\n from scrapy.utils.spider import spidercls_for_request, DefaultSpider\n \n \n@@ -43,8 +45,17 @@ class Command(ScrapyCommand):\n     def run(self, args, opts):\n         url = args[0] if args else None\n         if url:\n+            parts = urlparse.urlsplit(url)\n+            if not parts.scheme:\n+                if \".\" not in parts.path.split(\"/\", 1)[0]:\n                     url = any_to_uri(url)\n \n+                for pattern in [\"/\", \"./\", \"../\"]:\n+                    if url.startswith(pattern):\n+                        url = any_to_uri(url)\n+                        break\n+                url = add_http_if_no_scheme(url)\n+\n         spider_loader = self.crawler_process.spider_loader\n \n         spidercls = DefaultSpider\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e19bf4aecc9027fd4023282a371c13f77fadd510", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 19 | Churn Cumulative: 256 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -801,6 +801,25 @@ class FormRequestTest(RequestTest):\n         self.assertEqual(fs[b'test2'], [b'val2'])\n         self.assertEqual(fs[b'button1'], [b''])\n \n+    def test_html_base_form_action(self):\n+        response = _buildresponse(\n+            \"\"\"\n+            <html>\n+                <head>\n+                    <base href=\"http://b.com/\">\n+                </head>\n+                <body>\n+                    <form action=\"test_form\">\n+                    </form>\n+                </body>\n+            </html>\n+            \"\"\",\n+            url='http://a.com/'\n+        )\n+        req = self.request_class.from_response(response)\n+        self.assertEqual(req.url, 'http://b.com/test_form')\n+\n+\n def _buildresponse(body, **kwargs):\n     kwargs.setdefault('body', body)\n     kwargs.setdefault('url', 'http://example.com')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#94486bb294a6e765efe14affddb1df355a6c298b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 17 | Churn Cumulative: 207 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -190,3 +190,20 @@ class RegexLinkExtractorTestCase(unittest.TestCase):\n             Link(url='http://example.org/item1.html', text=u'Item 1', nofollow=False),\n             Link(url='http://example.org/item3.html', text=u'Item 3', nofollow=False),\n         ])\n+\n+    def test_html_base_href(self):\n+        html = \"\"\"\n+        <html>\n+            <head>\n+                <base href=\"http://b.com/\">\n+            </head>\n+            <body>\n+                <a href=\"test.html\"></a>\n+            </body>\n+        </html>\n+        \"\"\"\n+        response = HtmlResponse(\"http://a.com/\", body=html)\n+        lx = RegexLinkExtractor()\n+        self.assertEqual([link for link in lx.extract_links(response)], [\n+            Link(url='http://b.com/test.html', text=u'', nofollow=False),\n+        ])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c0566b2b07514897e78a820c8aecae43809eafe5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 1375 | Contributors (this commit): 10 | Commits (past 90d): 4 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -43,9 +43,7 @@ class Crawler(object):\n \n         lf_cls = load_object(self.settings['LOG_FORMATTER'])\n         self.logformatter = lf_cls.from_crawler(self)\n-        self.extensions = ExtensionManager.from_crawler(self)\n \n-        self.settings.freeze()\n         self.crawling = False\n         self.spider = None\n         self.engine = None\n@@ -67,6 +65,9 @@ class Crawler(object):\n         self.crawling = True\n \n         try:\n+            self.settings.freeze()\n+            self.extensions = ExtensionManager.from_crawler(self)\n+\n             self.spider = self._create_spider(*args, **kwargs)\n             self.engine = self._create_engine()\n             start_requests = iter(self.spider.start_requests())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d67f292d92a23c94f731596e4a9462583f176740", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1377 | Contributors (this commit): 10 | Commits (past 90d): 5 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -29,7 +29,6 @@ class Crawler(object):\n \n         self.spidercls = spidercls\n         self.settings = settings.copy()\n-        self.spidercls.update_settings(self.settings)\n \n         self.signals = SignalManager(self)\n         self.stats = load_object(self.settings['STATS_CLASS'])(self)\n@@ -65,6 +64,7 @@ class Crawler(object):\n         self.crawling = True\n \n         try:\n+            self.spidercls.update_settings(self.settings)\n             self.settings.freeze()\n             self.extensions = ExtensionManager.from_crawler(self)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
