{"custom_id": "scrapy#69cd2e247efe1823ab188eacb241b9e5596a879c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 11 | Files Changed: 7 | Hunks: 20 | Methods Changed: 13 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 28 | Churn Cumulative: 3737 | Contributors (this commit): 45 | Commits (past 90d): 24 | Contributors (cumulative): 89 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,6 +6,7 @@ import inspect\n import pkg_resources\n \n import scrapy\n+from scrapy.crawler import CrawlerProcess\n from scrapy.commands import ScrapyCommand\n from scrapy.exceptions import UsageError\n from scrapy.utils.misc import walk_modules\n@@ -140,9 +141,6 @@ def execute(argv=None, settings=None):\n     opts, args = parser.parse_args(args=argv[1:])\n     _run_print_help(parser, cmd.process_options, args, opts)\n \n-    # needs to be after cmd.process_options() as it imports twisted.internet.reactor\n-    # while commands may want to install the asyncio reactor\n-    from scrapy.crawler import CrawlerProcess\n     cmd.crawler_process = CrawlerProcess(settings)\n     _run_print_help(parser, _run_command, cmd, args, opts)\n     sys.exit(cmd.exitcode)\n\n@@ -7,6 +7,7 @@ from threading import Thread\n \n from scrapy.commands import ScrapyCommand\n from scrapy.http import Request\n+from scrapy.shell import Shell\n from scrapy.utils.spider import spidercls_for_request, DefaultSpider\n from scrapy.utils.url import guess_scheme\n \n@@ -69,8 +70,6 @@ class Command(ScrapyCommand):\n \n         self._start_crawler_thread()\n \n-        # moved from the top-level because it imports twisted.internet.reactor\n-        from scrapy.shell import Shell\n         shell = Shell(crawler, update_vars=self.update_vars, code=opts.code)\n         shell.start(url=url, redirect=not opts.no_redirect)\n \n\n@@ -3,7 +3,7 @@ import pprint\n import signal\n import warnings\n \n-from twisted.internet import reactor, defer\n+from twisted.internet import defer\n from zope.interface.verify import verifyClass, DoesNotImplement\n \n from scrapy import Spider\n@@ -261,6 +261,7 @@ class CrawlerProcess(CrawlerRunner):\n         log_scrapy_info(self.settings)\n \n     def _signal_shutdown(self, signum, _):\n+        from twisted.internet import reactor\n         install_shutdown_handlers(self._signal_kill)\n         signame = signal_names[signum]\n         logger.info(\"Received %(signame)s, shutting down gracefully. Send again to force \",\n@@ -268,6 +269,7 @@ class CrawlerProcess(CrawlerRunner):\n         reactor.callFromThread(self._graceful_stop_reactor)\n \n     def _signal_kill(self, signum, _):\n+        from twisted.internet import reactor\n         install_shutdown_handlers(signal.SIG_IGN)\n         signame = signal_names[signum]\n         logger.info('Received %(signame)s twice, forcing unclean shutdown',\n@@ -286,6 +288,7 @@ class CrawlerProcess(CrawlerRunner):\n         :param boolean stop_after_crawl: stop or not the reactor when all\n             crawlers have finished\n         \"\"\"\n+        from twisted.internet import reactor\n         if stop_after_crawl:\n             d = self.join()\n             # Don't start the reactor if the deferreds are already fired\n@@ -300,6 +303,7 @@ class CrawlerProcess(CrawlerRunner):\n         reactor.run(installSignalHandlers=False)  # blocking call\n \n     def _get_dns_resolver(self):\n+        from twisted.internet import reactor\n         if self.settings.getbool('DNSCACHE_ENABLED'):\n             cache_size = self.settings.getint('DNSCACHE_SIZE')\n         else:\n@@ -316,6 +320,7 @@ class CrawlerProcess(CrawlerRunner):\n         return d\n \n     def _stop_reactor(self, _=None):\n+        from twisted.internet import reactor\n         try:\n             reactor.stop()\n         except RuntimeError:  # raised if already stopped or in shutdown stage\n\n@@ -7,7 +7,7 @@ import os\n import signal\n import warnings\n \n-from twisted.internet import reactor, threads, defer\n+from twisted.internet import threads, defer\n from twisted.python import threadable\n from w3lib.url import any_to_uri\n \n@@ -98,6 +98,7 @@ class Shell(object):\n         return spider\n \n     def fetch(self, request_or_url, spider=None, redirect=True, **kwargs):\n+        from twisted.internet import reactor\n         if isinstance(request_or_url, Request):\n             request = request_or_url\n         else:\n\n@@ -5,7 +5,7 @@ import asyncio\n import asyncio.futures\n import inspect\n \n-from twisted.internet import defer, reactor, task\n+from twisted.internet import defer, task\n from twisted.python import failure\n \n from scrapy.exceptions import IgnoreRequest\n@@ -19,6 +19,7 @@ def defer_fail(_failure):\n     It delays by 100ms so reactor has a chance to go through readers and writers\n     before attending pending delayed calls, so do not set delay to zero.\n     \"\"\"\n+    from twisted.internet import reactor\n     d = defer.Deferred()\n     reactor.callLater(0.1, d.errback, _failure)\n     return d\n@@ -31,6 +32,7 @@ def defer_succeed(result):\n     It delays by 100ms so reactor has a chance to go trough readers and writers\n     before attending pending delayed calls, so do not set delay to zero.\n     \"\"\"\n+    from twisted.internet import reactor\n     d = defer.Deferred()\n     reactor.callLater(0.1, d.callback, result)\n     return d\n\n@@ -1,7 +1,5 @@\n import signal\n \n-from twisted.internet import reactor\n-\n \n signal_names = {}\n for signame in dir(signal):\n@@ -17,6 +15,7 @@ def install_shutdown_handlers(function, override_sigint=True):\n     SIGINT handler won't be install if there is already a handler in place\n     (e.g.  Pdb)\n     \"\"\"\n+    from twisted.internet import reactor\n     reactor._handleSignals()\n     signal.signal(signal.SIGTERM, function)\n     if signal.getsignal(signal.SIGINT) == signal.default_int_handler or \\\n\n@@ -1,8 +1,9 @@\n-from twisted.internet import reactor, error\n+from twisted.internet import error\n \n \n def listen_tcp(portrange, host, factory):\n     \"\"\"Like reactor.listenTCP but tries different ports in a range.\"\"\"\n+    from twisted.internet import reactor\n     assert len(portrange) <= 2, \"invalid portrange: %s\" % portrange\n     if not portrange:\n         return reactor.listenTCP(0, factory, interface=host)\n@@ -30,6 +31,7 @@ class CallLaterOnce(object):\n         self._call = None\n \n     def schedule(self, delay=0):\n+        from twisted.internet import reactor\n         if self._call is None:\n             self._call = reactor.callLater(delay, self)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#855bbebc8bb862aa02e48f65fd861b1ddf78b57a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 6 | Files Changed: 3 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): -1/1 | Churn Δ: 9 | Churn Cumulative: 2020 | Contributors (this commit): 27 | Commits (past 90d): 16 | Contributors (cumulative): 44 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,5 @@\n import os\n from scrapy.commands import ScrapyCommand\n-from scrapy.utils.asyncio import install_asyncio_reactor\n from scrapy.utils.conf import arglist_to_dict\n from scrapy.utils.python import without_none_values\n from scrapy.exceptions import UsageError\n@@ -27,8 +26,6 @@ class Command(ScrapyCommand):\n \n     def process_options(self, args, opts):\n         ScrapyCommand.process_options(self, args, opts)\n-        if self.settings.getbool('ASYNCIO_ENABLED'):\n-            install_asyncio_reactor()\n         try:\n             opts.spargs = arglist_to_dict(opts.spargs)\n         except ValueError:\n\n@@ -2,7 +2,6 @@ import sys\n import os\n from importlib import import_module\n \n-from scrapy.utils.asyncio import install_asyncio_reactor\n from scrapy.utils.spider import iter_spider_classes\n from scrapy.commands import ScrapyCommand\n from scrapy.exceptions import UsageError\n@@ -51,8 +50,6 @@ class Command(ScrapyCommand):\n \n     def process_options(self, args, opts):\n         ScrapyCommand.process_options(self, args, opts)\n-        if self.settings.getbool('ASYNCIO_ENABLED'):\n-            install_asyncio_reactor()\n         try:\n             opts.spargs = arglist_to_dict(opts.spargs)\n         except ValueError:\n\n@@ -14,6 +14,7 @@ from scrapy.extension import ExtensionManager\n from scrapy.settings import overridden_settings, Settings\n from scrapy.signalmanager import SignalManager\n from scrapy.exceptions import ScrapyDeprecationWarning\n+from scrapy.utils.asyncio import install_asyncio_reactor\n from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n from scrapy.utils.misc import load_object\n from scrapy.utils.log import (\n@@ -256,6 +257,8 @@ class CrawlerProcess(CrawlerRunner):\n \n     def __init__(self, settings=None, install_root_handler=True):\n         super(CrawlerProcess, self).__init__(settings)\n+        if self.settings.getbool('ASYNCIO_ENABLED'):\n+            install_asyncio_reactor()\n         install_shutdown_handlers(self._signal_shutdown)\n         configure_logging(self.settings, install_root_handler)\n         log_scrapy_info(self.settings)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bfb78b8dea44a5db3f4a3bca83ab58c7ca0e3ef3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 50 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 5 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 50 | Churn Cumulative: 593 | Contributors (this commit): 9 | Commits (past 90d): 6 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,17 @@\n+import scrapy\n+from scrapy.crawler import CrawlerProcess\n+\n+\n+class NoRequestsSpider(scrapy.Spider):\n+    name = 'no_request'\n+\n+    def start_requests(self):\n+        return []\n+\n+\n+process = CrawlerProcess(settings={\n+    'ASYNCIO_ENABLED': True,\n+})\n+\n+process.crawl(NoRequestsSpider)\n+process.start()\n\n@@ -0,0 +1,22 @@\n+import asyncio\n+\n+from twisted.internet import asyncioreactor\n+asyncioreactor.install(asyncio.get_event_loop())\n+\n+import scrapy\n+from scrapy.crawler import CrawlerProcess\n+\n+\n+class NoRequestsSpider(scrapy.Spider):\n+    name = 'no_request'\n+\n+    def start_requests(self):\n+        return []\n+\n+\n+process = CrawlerProcess(settings={\n+    'ASYNCIO_ENABLED': True,\n+})\n+\n+process.crawl(NoRequestsSpider)\n+process.start()\n\n@@ -301,3 +301,14 @@ class CrawlerProcessSubprocess(unittest.TestCase):\n     def test_simple(self):\n         log = self.run_script('simple.py')\n         self.assertIn('Spider closed (finished)', log)\n+        self.assertNotIn(\"DEBUG: Asyncio support enabled\", log)\n+\n+    def test_asyncio_enabled_no_reactor(self):\n+        log = self.run_script('asyncio_enabled_no_reactor.py')\n+        self.assertIn('Spider closed (finished)', log)\n+        self.assertIn(\"DEBUG: Asyncio support enabled\", log)\n+\n+    def test_asyncio_enabled_reactor(self):\n+        log = self.run_script('asyncio_enabled_reactor.py')\n+        self.assertIn('Spider closed (finished)', log)\n+        self.assertIn(\"DEBUG: Asyncio support enabled\", log)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b5c4c2cae89714479d6fbb5a497fe43d48d886bc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 11 | Files Changed: 3 | Hunks: 7 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 798 | Contributors (this commit): 52 | Commits (past 90d): 9 | Contributors (cumulative): 64 | DMM Complexity: 0.0\n\nDIFF:\n@@ -84,8 +84,8 @@ DOWNLOADER = 'scrapy.core.downloader.Downloader'\n DOWNLOADER_HTTPCLIENTFACTORY = 'scrapy.core.downloader.webclient.ScrapyHTTPClientFactory'\n DOWNLOADER_CLIENTCONTEXTFACTORY = 'scrapy.core.downloader.contextfactory.ScrapyClientContextFactory'\n DOWNLOADER_CLIENT_TLS_CIPHERS = 'DEFAULT'\n-DOWNLOADER_CLIENT_TLS_METHOD = 'TLS' # Use highest TLS/SSL protocol version supported by the platform,\n-                                     # also allowing negotiation\n+# Use highest TLS/SSL protocol version supported by the platform, also allowing negotiation:\n+DOWNLOADER_CLIENT_TLS_METHOD = 'TLS'\n DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING = False\n \n DOWNLOADER_MIDDLEWARES = {}\n\n@@ -271,7 +271,6 @@ class EngineTest(unittest.TestCase):\n                          self.run.signals_catched[signals.spider_opened])\n         self.assertEqual({'spider': self.run.spider},\n                          self.run.signals_catched[signals.spider_idle])\n-        self.run.signals_catched[signals.spider_closed].pop('spider_stats', None) # XXX: remove for scrapy 0.17\n         self.assertEqual({'spider': self.run.spider, 'reason': 'finished'},\n                          self.run.signals_catched[signals.spider_closed])\n \n\n@@ -1,20 +1,16 @@\n import unittest\n+\n+from scrapy import Spider\n from scrapy.http import Request\n from scrapy.item import BaseItem\n from scrapy.utils.spider import iterate_spider_output, iter_spider_classes\n \n-from scrapy.spiders import CrawlSpider\n \n-\n-class MyBaseSpider(CrawlSpider):\n-    pass # abstract spider\n-\n-\n-class MySpider1(MyBaseSpider):\n+class MySpider1(Spider):\n     name = 'myspider1'\n \n \n-class MySpider2(MyBaseSpider):\n+class MySpider2(Spider):\n     name = 'myspider2'\n \n \n@@ -35,5 +31,6 @@ class UtilsSpidersTestCase(unittest.TestCase):\n         it = iter_spider_classes(tests.test_utils_spider)\n         self.assertEqual(set(it), {MySpider1, MySpider2})\n \n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a4ef9750f9058fbe1041bad0fb28f39c693e5659", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 10 | Churn Cumulative: 521 | Contributors (this commit): 8 | Commits (past 90d): 3 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -514,11 +514,13 @@ class LxmlLinkExtractorTestCase(Base.LinkExtractorTestCase):\n         \"\"\"Make sure the FilteringLinkExtractor deprecation warning is not\n         issued for LxmlLinkExtractor\"\"\"\n         with catch_warnings(record=True) as warnings:\n-            extractor = LxmlLinkExtractor()\n+            LxmlLinkExtractor()\n             self.assertEqual(len(warnings), 0)\n+\n             class SubclassedItem(LxmlLinkExtractor):\n                 pass\n-            subclassed_extractor = SubclassedItem()\n+\n+            SubclassedItem()\n             self.assertEqual(len(warnings), 0)\n \n \n@@ -527,12 +529,12 @@ class FilteringLinkExtractorTest(unittest.TestCase):\n     def test_deprecation_warning(self):\n         args = [None] * 10\n         with catch_warnings(record=True) as warnings:\n-            extractor = FilteringLinkExtractor(*args)\n+            FilteringLinkExtractor(*args)\n             self.assertEqual(len(warnings), 1)\n             self.assertEqual(warnings[0].category, ScrapyDeprecationWarning)\n         with catch_warnings(record=True) as warnings:\n             class SubclassedFilteringLinkExtractor(FilteringLinkExtractor):\n                 pass\n-            subclassed_extractor = SubclassedFilteringLinkExtractor(*args)\n+            SubclassedFilteringLinkExtractor(*args)\n             self.assertEqual(len(warnings), 1)\n             self.assertEqual(warnings[0].category, ScrapyDeprecationWarning)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a1605cade6286dd5f7f1c9e4c9660d44ed15ed19", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 253 | Contributors (this commit): 14 | Commits (past 90d): 5 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -2,7 +2,6 @@\n Helper functions for dealing with Twisted deferreds\n \"\"\"\n import asyncio\n-import asyncio.futures\n import inspect\n \n from twisted.internet import defer, task\n@@ -121,18 +120,18 @@ def iter_errback(iterable, errback, *a, **kw):\n             errback(failure.Failure(), *a, **kw)\n \n \n-def isfuture(o):\n+def _isfuture(o):\n     # workaround for Python before 3.5.3 not having asyncio.isfuture\n     if hasattr(asyncio, 'isfuture'):\n         return asyncio.isfuture(o)\n-    return isinstance(o, asyncio.futures.Future)\n+    return isinstance(o, asyncio.Future)\n \n \n def deferred_from_coro(o, asyncio_enabled=False):\n     \"\"\"Converts a coroutine into a Deferred, or returns the object as is if it isn't a coroutine\"\"\"\n     if isinstance(o, defer.Deferred):\n         return o\n-    if isfuture(o) or inspect.isawaitable(o):\n+    if _isfuture(o) or inspect.isawaitable(o):\n         if not asyncio_enabled:\n             # wrapping the coroutine directly into a Deferred, this doesn't work correctly with coroutines\n             # that use asyncio, e.g. \"await asyncio.sleep(1)\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1aab20e1cec9e2e9aac4ce9efd37b07811b8f7d6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 189 | Contributors (this commit): 14 | Commits (past 90d): 13 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -50,7 +50,7 @@ master_doc = 'index'\n \n # General information about the project.\n project = u'Scrapy'\n-copyright = u'2008–2018, Scrapy developers'\n+copyright = u'2008–2020, Scrapy developers'\n \n # The version info for the project you're documenting, acts as replacement for\n # |version| and |release|, also used in various other places throughout the\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a59bb279d18966e6a47f71d21c690598fc69678e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 198 | Contributors (this commit): 14 | Commits (past 90d): 14 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -12,6 +12,7 @@\n # serve to show the default.\n \n import sys\n+from datetime import datetime\n from os import path\n \n # If your extensions are in another directory, add it here. If the directory\n@@ -49,8 +50,8 @@ source_suffix = '.rst'\n master_doc = 'index'\n \n # General information about the project.\n-project = u'Scrapy'\n-copyright = u'2008–2020, Scrapy developers'\n+project = 'Scrapy'\n+copyright = '2008–{}, Scrapy developers'.format(datetime.now().year)\n \n # The version info for the project you're documenting, acts as replacement for\n # |version| and |release|, also used in various other places throughout the\n@@ -194,8 +195,8 @@ htmlhelp_basename = 'Scrapydoc'\n # Grouping the document tree into LaTeX files. List of tuples\n # (source start file, target name, title, author, document class [howto/manual]).\n latex_documents = [\n-  ('index', 'Scrapy.tex', u'Scrapy Documentation',\n-   u'Scrapy developers', 'manual'),\n+  ('index', 'Scrapy.tex', 'Scrapy Documentation',\n+   'Scrapy developers', 'manual'),\n ]\n \n # The name of an image file (relative to this directory) to place at the top of\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2db7d453788f5c638d0921b0f7f8bab58e2a58bc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 8 | Churn Cumulative: 157 | Contributors (this commit): 10 | Commits (past 90d): 6 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -35,6 +35,14 @@ def pytest_collection_modifyitems(session, config, items):\n     except ImportError:\n         pass\n \n+\n @pytest.fixture()\n def reactor_pytest(request):\n     request.cls.reactor_pytest = request.config.getoption(\"--reactor\")\n+    return request.cls.reactor_pytest\n+\n+\n+@pytest.fixture(autouse=True)\n+def only_asyncio(request, reactor_pytest):\n+    if request.node.get_closest_marker('only_asyncio') and reactor_pytest != 'asyncio':\n+        pytest.skip('This test is only run with --reactor-asyncio')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#451e7a616e6d23e88528836296a9abdc887834c6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 190 | Lines Deleted: 9 | Files Changed: 5 | Hunks: 19 | Methods Changed: 21 | Complexity Δ (Sum/Max): 13/11 | Churn Δ: 199 | Churn Cumulative: 2266 | Contributors (this commit): 39 | Commits (past 90d): 40 | Contributors (cumulative): 59 | DMM Complexity: 0.905982905982906\n\nDIFF:\n@@ -9,7 +9,7 @@ from twisted.internet import defer\n \n from scrapy.utils.defer import defer_result, defer_succeed, parallel, iter_errback\n from scrapy.utils.spider import iterate_spider_output\n-from scrapy.utils.misc import load_object\n+from scrapy.utils.misc import load_object, warn_on_generator_with_return_value\n from scrapy.utils.log import logformatter_adapter, failure_to_exc_info\n from scrapy.exceptions import CloseSpider, DropItem, IgnoreRequest\n from scrapy import signals\n@@ -18,6 +18,7 @@ from scrapy.item import BaseItem\n from scrapy.core.spidermw import SpiderMiddlewareManager\n from scrapy.utils.request import referer_str\n \n+\n logger = logging.getLogger(__name__)\n \n \n@@ -99,11 +100,13 @@ class Scraper(object):\n     def enqueue_scrape(self, response, request, spider):\n         slot = self.slot\n         dfd = slot.add_response_request(response, request)\n+\n         def finish_scraping(_):\n             slot.finish_response(response, request)\n             self._check_if_closing(spider, slot)\n             self._scrape_next(spider, slot)\n             return _\n+\n         dfd.addBoth(finish_scraping)\n         dfd.addErrback(\n             lambda f: logger.error('Scraper bug processing %(request)s',\n@@ -123,7 +126,7 @@ class Scraper(object):\n         callback/errback\"\"\"\n         assert isinstance(response, (Response, Failure))\n \n-        dfd = self._scrape2(response, request, spider)  # returns spiders processed output\n+        dfd = self._scrape2(response, request, spider)  # returns spider's processed output\n         dfd.addErrback(self.handle_spider_error, request, response, spider)\n         dfd.addCallback(self.handle_spider_output, request, response, spider)\n         return dfd\n@@ -142,7 +145,10 @@ class Scraper(object):\n     def call_spider(self, result, request, spider):\n         result.request = request\n         dfd = defer_result(result)\n-        dfd.addCallbacks(callback=request.callback or spider.parse,\n+        callback = request.callback or spider.parse\n+        warn_on_generator_with_return_value(spider, callback)\n+        warn_on_generator_with_return_value(spider, request.errback)\n+        dfd.addCallbacks(callback=callback,\n                          errback=request.errback,\n                          callbackKeywords=request.cb_kwargs)\n         return dfd.addCallback(iterate_spider_output)\n@@ -172,8 +178,8 @@ class Scraper(object):\n         if not result:\n             return defer_succeed(None)\n         it = iter_errback(result, self.handle_spider_error, request, response, spider)\n-        dfd = parallel(it, self.concurrent_items,\n-            self._process_spidermw_output, request, response, spider)\n+        dfd = parallel(it, self.concurrent_items, self._process_spidermw_output,\n+                       request, response, spider)\n         return dfd\n \n     def _process_spidermw_output(self, output, request, response, spider):\n@@ -200,8 +206,7 @@ class Scraper(object):\n         \"\"\"Log and silence errors that come from the engine (typically download\n         errors that got propagated thru here)\n         \"\"\"\n-        if (isinstance(download_failure, Failure) and\n-                not download_failure.check(IgnoreRequest)):\n+        if isinstance(download_failure, Failure) and not download_failure.check(IgnoreRequest):\n             if download_failure.frames:\n                 logger.error('Error downloading %(request)s',\n                              {'request': request},\n\n@@ -8,6 +8,7 @@ This module must not depend on any module outside the Standard Library.\n import collections\n import copy\n import warnings\n+import weakref\n from collections.abc import Mapping\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n@@ -240,7 +241,6 @@ class LocalCache(collections.OrderedDict):\n     \"\"\"Dictionary with a finite number of keys.\n \n     Older items expires first.\n-\n     \"\"\"\n \n     def __init__(self, limit=None):\n@@ -254,6 +254,35 @@ class LocalCache(collections.OrderedDict):\n         super(LocalCache, self).__setitem__(key, value)\n \n \n+class LocalWeakReferencedCache(weakref.WeakKeyDictionary):\n+    \"\"\"\n+    A weakref.WeakKeyDictionary implementation that uses LocalCache as its\n+    underlying data structure, making it ordered and capable of being size-limited.\n+\n+    Useful for memoization, while avoiding keeping received\n+    arguments in memory only because of the cached references.\n+\n+    Note: like LocalCache and unlike weakref.WeakKeyDictionary,\n+    it cannot be instantiated with an initial dictionary.\n+    \"\"\"\n+\n+    def __init__(self, limit=None):\n+        super(LocalWeakReferencedCache, self).__init__()\n+        self.data = LocalCache(limit=limit)\n+\n+    def __setitem__(self, key, value):\n+        try:\n+            super(LocalWeakReferencedCache, self).__setitem__(key, value)\n+        except TypeError:\n+            pass  # key is not weak-referenceable, skip caching\n+\n+    def __getitem__(self, key):\n+        try:\n+            return super(LocalWeakReferencedCache, self).__getitem__(key)\n+        except TypeError:\n+            return None  # key is not weak-referenceable, it's not cached\n+\n+\n class SequenceExclude(object):\n     \"\"\"Object to test if an item is NOT within some sequence.\"\"\"\n \n\n@@ -1,13 +1,18 @@\n \"\"\"Helper functions which don't fit anywhere else\"\"\"\n+import ast\n+import inspect\n import os\n import re\n import hashlib\n+import warnings\n from contextlib import contextmanager\n from importlib import import_module\n from pkgutil import iter_modules\n+from textwrap import dedent\n \n from w3lib.html import replace_entities\n \n+from scrapy.utils.datatypes import LocalWeakReferencedCache\n from scrapy.utils.python import flatten, to_unicode\n from scrapy.item import BaseItem\n \n@@ -161,3 +166,44 @@ def set_environ(**kwargs):\n                 del os.environ[k]\n             else:\n                 os.environ[k] = v\n+\n+\n+_generator_callbacks_cache = LocalWeakReferencedCache(limit=128)\n+\n+\n+def is_generator_with_return_value(callable):\n+    \"\"\"\n+    Returns True if a callable is a generator function which includes a\n+    'return' statement with a value different than None, False otherwise\n+    \"\"\"\n+    if callable in _generator_callbacks_cache:\n+        return _generator_callbacks_cache[callable]\n+\n+    def returns_none(return_node):\n+        value = return_node.value\n+        return value is None or isinstance(value, ast.NameConstant) and value.value is None\n+\n+    if inspect.isgeneratorfunction(callable):\n+        tree = ast.parse(dedent(inspect.getsource(callable)))\n+        for node in ast.walk(tree):\n+            if isinstance(node, ast.Return) and not returns_none(node):\n+                _generator_callbacks_cache[callable] = True\n+                return _generator_callbacks_cache[callable]\n+\n+    _generator_callbacks_cache[callable] = False\n+    return _generator_callbacks_cache[callable]\n+\n+\n+def warn_on_generator_with_return_value(spider, callable):\n+    \"\"\"\n+    Logs a warning if a callable is a generator function and includes\n+    a 'return' statement with a value different than None\n+    \"\"\"\n+    if is_generator_with_return_value(callable):\n+        warnings.warn(\n+            'The \"{}.{}\" method is a generator and includes a \"return\" statement with a '\n+            'value different than None. This could lead to unexpected behaviour. Please see '\n+            'https://docs.python.org/3/reference/simple_stmts.html#the-return-statement '\n+            'for details about the semantics of the \"return\" statement within generators'\n+            .format(spider.__class__.__name__, callable.__name__), stacklevel=2,\n+        )\n\n@@ -2,7 +2,9 @@ import copy\n import unittest\n from collections.abc import Mapping, MutableMapping\n \n-from scrapy.utils.datatypes import CaselessDict, LocalCache, SequenceExclude\n+from scrapy.http import Request\n+from scrapy.utils.datatypes import CaselessDict, LocalCache, LocalWeakReferencedCache, SequenceExclude\n+from scrapy.utils.python import garbage_collect\n \n \n __doctests__ = ['scrapy.utils.datatypes']\n@@ -255,5 +257,67 @@ class LocalCacheTest(unittest.TestCase):\n             self.assertEqual(cache[str(x)], x)\n \n \n+class LocalWeakReferencedCacheTest(unittest.TestCase):\n+\n+    def test_cache_with_limit(self):\n+        cache = LocalWeakReferencedCache(limit=2)\n+        r1 = Request('https://example.org')\n+        r2 = Request('https://example.com')\n+        r3 = Request('https://example.net')\n+        cache[r1] = 1\n+        cache[r2] = 2\n+        cache[r3] = 3\n+        self.assertEqual(len(cache), 2)\n+        self.assertNotIn(r1, cache)\n+        self.assertIn(r2, cache)\n+        self.assertIn(r3, cache)\n+        self.assertEqual(cache[r2], 2)\n+        self.assertEqual(cache[r3], 3)\n+        del r2\n+\n+        # PyPy takes longer to collect dead references\n+        garbage_collect()\n+\n+        self.assertEqual(len(cache), 1)\n+\n+    def test_cache_non_weak_referenceable_objects(self):\n+        cache = LocalWeakReferencedCache()\n+        k1 = None\n+        k2 = 1\n+        k3 = [1, 2, 3]\n+        cache[k1] = 1\n+        cache[k2] = 2\n+        cache[k3] = 3\n+        self.assertNotIn(k1, cache)\n+        self.assertNotIn(k2, cache)\n+        self.assertNotIn(k3, cache)\n+        self.assertEqual(len(cache), 0)\n+\n+    def test_cache_without_limit(self):\n+        max = 10**4\n+        cache = LocalWeakReferencedCache()\n+        refs = []\n+        for x in range(max):\n+            refs.append(Request('https://example.org/{}'.format(x)))\n+            cache[refs[-1]] = x\n+        self.assertEqual(len(cache), max)\n+        for i, r in enumerate(refs):\n+            self.assertIn(r, cache)\n+            self.assertEqual(cache[r], i)\n+        del r  # delete reference to the last object in the list\n+\n+        # delete half of the objects, make sure that is reflected in the cache\n+        for _ in range(max // 2):\n+            refs.pop()\n+\n+        # PyPy takes longer to collect dead references\n+        garbage_collect()\n+\n+        self.assertEqual(len(cache), max // 2)\n+        for i, r in enumerate(refs):\n+            self.assertIn(r, cache)\n+            self.assertEqual(cache[r], i)\n+\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -0,0 +1,37 @@\n+import unittest\n+\n+from scrapy.utils.misc import is_generator_with_return_value\n+\n+\n+class UtilsMiscPy3TestCase(unittest.TestCase):\n+\n+    def test_generators_with_return_statements(self):\n+        def f():\n+            yield 1\n+            return 2\n+\n+        def g():\n+            yield 1\n+            return 'asdf'\n+\n+        def h():\n+            yield 1\n+            return None\n+\n+        def i():\n+            yield 1\n+            return\n+\n+        def j():\n+            yield 1\n+\n+        def k():\n+            yield 1\n+            yield from g()\n+\n+        assert is_generator_with_return_value(f)\n+        assert is_generator_with_return_value(g)\n+        assert not is_generator_with_return_value(h)\n+        assert not is_generator_with_return_value(i)\n+        assert not is_generator_with_return_value(j)\n+        assert not is_generator_with_return_value(k)  # not recursive\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#039e6fe6919341dbfd864c4a406d35389c8e2992", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 14 | Churn Cumulative: 45 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,3 +1,8 @@\n+from contextlib import suppress\n+\n+from twisted.internet.error import ReactorAlreadyInstalledError\n+\n+\n def install_asyncio_reactor():\n     \"\"\" Tries to install AsyncioSelectorReactor\n     \"\"\"\n@@ -5,13 +10,10 @@ def install_asyncio_reactor():\n         import asyncio\n         from twisted.internet import asyncioreactor\n     except ImportError:\n-        pass\n-    else:\n-        from twisted.internet.error import ReactorAlreadyInstalledError\n-        try:\n+        return\n+\n+    with suppress(ReactorAlreadyInstalledError):\n         asyncioreactor.install(asyncio.get_event_loop())\n-        except ReactorAlreadyInstalledError:\n-            pass\n \n \n def is_asyncio_reactor_installed():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#900de7c14607fbe2936fa682d03747916337f075", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 5 | Churn Cumulative: 162 | Contributors (this commit): 10 | Commits (past 90d): 7 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -36,8 +36,11 @@ def pytest_collection_modifyitems(session, config, items):\n         pass\n \n \n-@pytest.fixture()\n+@pytest.fixture(scope='class')\n def reactor_pytest(request):\n+    if not request.cls:\n+        # doctests\n+        return\n     request.cls.reactor_pytest = request.config.getoption(\"--reactor\")\n     return request.cls.reactor_pytest\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5980d3bbff67d060a1a1b15372293ced972dbe8b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 71 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 10 | Complexity Δ (Sum/Max): 11/11 | Churn Δ: 71 | Churn Cumulative: 71 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,71 @@\n+from twisted.internet import defer\n+from twisted.internet.defer import Deferred\n+from twisted.trial import unittest\n+\n+from scrapy import Spider, signals, Request\n+from scrapy.utils.test import get_crawler\n+\n+from tests.mockserver import MockServer\n+\n+\n+class SimplePipeline:\n+    def process_item(self, item, spider):\n+        item['pipeline_passed'] = True\n+        return item\n+\n+\n+class DeferredPipeline:\n+    def cb(self, item):\n+        item['pipeline_passed'] = True\n+        return item\n+\n+    def process_item(self, item, spider):\n+        d = Deferred()\n+        d.addCallback(self.cb)\n+        d.callback(item)\n+        return d\n+\n+\n+class ItemSpider(Spider):\n+    name = 'itemspider'\n+\n+    def start_requests(self):\n+        yield Request(self.mockserver.url('/status?n=200'))\n+\n+    def parse(self, response):\n+        return {'field': 42}\n+\n+\n+class PipelineTestCase(unittest.TestCase):\n+    def setUp(self):\n+        self.mockserver = MockServer()\n+        self.mockserver.__enter__()\n+\n+    def tearDown(self):\n+        self.mockserver.__exit__(None, None, None)\n+\n+    def _on_item_scraped(self, item):\n+        self.assertIsInstance(item, dict)\n+        self.assertTrue(item.get('pipeline_passed'))\n+        self.items.append(item)\n+\n+    def _create_crawler(self, pipeline_class):\n+        settings = {\n+            'ITEM_PIPELINES': {__name__ + '.' + pipeline_class.__name__: 1},\n+        }\n+        crawler = get_crawler(ItemSpider, settings)\n+        crawler.signals.connect(self._on_item_scraped, signals.item_scraped)\n+        self.items = []\n+        return crawler\n+\n+    @defer.inlineCallbacks\n+    def test_simple_pipeline(self):\n+        crawler = self._create_crawler(SimplePipeline)\n+        yield crawler.crawl(mockserver=self.mockserver)\n+        self.assertEqual(len(self.items), 1)\n+\n+    @defer.inlineCallbacks\n+    def test_deferred_pipeline(self):\n+        crawler = self._create_crawler(DeferredPipeline)\n+        yield crawler.crawl(mockserver=self.mockserver)\n+        self.assertEqual(len(self.items), 1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7d0096da6e37e508280e3af7388122bcdf0e3dcf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 316 | Contributors (this commit): 14 | Commits (past 90d): 2 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -73,8 +73,7 @@ class MailSender(object):\n                 part = MIMEBase(*mimetype.split('/'))\n                 part.set_payload(f.read())\n                 Encoders.encode_base64(part)\n-                part.add_header('Content-Disposition', 'attachment; filename=\"%s\"' \\\n-                    % attach_name)\n+                part.add_header('Content-Disposition', 'attachment', filename=attach_name)\n                 msg.attach(part)\n         else:\n             msg.set_payload(body)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#63cf5c75c850e48b6f267574e4a8f7ae3293deac", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 62 | Lines Deleted: 57 | Files Changed: 19 | Hunks: 40 | Methods Changed: 38 | Complexity Δ (Sum/Max): -37/1 | Churn Δ: 119 | Churn Cumulative: 6997 | Contributors (this commit): 69 | Commits (past 90d): 75 | Contributors (cumulative): 203 | DMM Complexity: 0.8571428571428571\n\nDIFF:\n@@ -67,7 +67,7 @@ def _pop_command_name(argv):\n \n def _print_header(settings, inproject):\n     if inproject:\n-        print(\"Scrapy %s - project: %s\\n\" % (scrapy.__version__, \\\n+        print(\"Scrapy %s - project: %s\\n\" % (scrapy.__version__,\n                                              settings['BOT_NAME']))\n     else:\n         print(\"Scrapy %s - no active project\\n\" % scrapy.__version__)\n@@ -123,7 +123,7 @@ def execute(argv=None, settings=None):\n     inproject = inside_project()\n     cmds = _get_commands_dict(settings, inproject)\n     cmdname = _pop_command_name(argv)\n-    parser = optparse.OptionParser(formatter=optparse.TitledHelpFormatter(), \\\n+    parser = optparse.OptionParser(formatter=optparse.TitledHelpFormatter(),\n                                    conflict_handler='resolve')\n     if not cmdname:\n         _print_commands(settings, inproject)\n\n@@ -24,11 +24,10 @@ class Command(ScrapyCommand):\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"--spider\", dest=\"spider\",\n-            help=\"use this spider\")\n-        parser.add_option(\"--headers\", dest=\"headers\", action=\"store_true\", \\\n+        parser.add_option(\"--spider\", dest=\"spider\", help=\"use this spider\")\n+        parser.add_option(\"--headers\", dest=\"headers\", action=\"store_true\",\n                           help=\"print response HTTP headers instead of body\")\n-        parser.add_option(\"--no-redirect\", dest=\"no_redirect\", action=\"store_true\", \\\n+        parser.add_option(\"--no-redirect\", dest=\"no_redirect\", action=\"store_true\",\n                           default=False, help=\"do not handle HTTP 3xx status codes and print response as-is\")\n \n     def _print_headers(self, headers, prefix):\n\n@@ -43,7 +43,7 @@ class Command(ScrapyCommand):\n                 return False\n \n         if not re.search(r'^[_a-zA-Z]\\w*$', project_name):\n-            print('Error: Project names must begin with a letter and contain'\\\n+            print('Error: Project names must begin with a letter and contain'\n                   ' only\\nletters, numbers and underscores')\n         elif _module_exists(project_name):\n             print('Error: Module %r already exists' % project_name)\n\n@@ -86,7 +86,7 @@ class ReturnsContract(Contract):\n             else:\n                 expected = '%s..%s' % (self.min_bound, self.max_bound)\n \n-            raise ContractFail(\"Returned %s %s, expected %s\" % \\\n+            raise ContractFail(\"Returned %s %s, expected %s\" %\n                                (occurrences, self.obj_name, expected))\n \n \n\n@@ -32,7 +32,7 @@ def _get_boto_connection():\n \n class S3DownloadHandler(object):\n \n-    def __init__(self, settings, aws_access_key_id=None, aws_secret_access_key=None, \\\n+    def __init__(self, settings, aws_access_key_id=None, aws_secret_access_key=None,\n                  httpdownloadhandler=HTTPDownloadHandler, **kw):\n \n         if not aws_access_key_id:\n\n@@ -88,8 +88,8 @@ class ScrapyHTTPPageGetter(HTTPClient):\n         if self.factory.url.startswith(b'https'):\n             self.transport.stopProducing()\n \n-        self.factory.noPage(\\\n-                defer.TimeoutError(\"Getting %s took longer than %s seconds.\" % \\\n+        self.factory.noPage(\n+                defer.TimeoutError(\"Getting %s took longer than %s seconds.\" %\n                                    (self.factory.url, self.factory.timeout)))\n \n \n\n@@ -44,7 +44,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                 try:\n                     result = method(response=response, spider=spider)\n                     if result is not None:\n-                        raise _InvalidOutput('Middleware {} must return None or raise an exception, got {}' \\\n+                        raise _InvalidOutput('Middleware {} must return None or raise an exception, got {}'\n                                              .format(fname(method), type(result)))\n                 except _InvalidOutput:\n                     raise\n@@ -69,7 +69,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                 elif result is None:\n                     continue\n                 else:\n-                    raise _InvalidOutput('Middleware {} must return None or an iterable, got {}' \\\n+                    raise _InvalidOutput('Middleware {} must return None or an iterable, got {}'\n                                          .format(fname(method), type(result)))\n             return _failure\n \n@@ -103,7 +103,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                 if _isiterable(result):\n                     result = evaluate_iterable(result, method_index)\n                 else:\n-                    raise _InvalidOutput('Middleware {} must return an iterable, got {}' \\\n+                    raise _InvalidOutput('Middleware {} must return an iterable, got {}'\n                                          .format(fname(method), type(result)))\n \n             return chain(result, recovered)\n\n@@ -37,8 +37,9 @@ class HttpCompressionMiddleware(object):\n             if content_encoding:\n                 encoding = content_encoding.pop()\n                 decoded_body = self._decode(response.body, encoding.lower())\n-                respcls = responsetypes.from_args(headers=response.headers, \\\n-                    url=response.url, body=decoded_body)\n+                respcls = responsetypes.from_args(\n+                    headers=response.headers, url=response.url, body=decoded_body\n+                )\n                 kwargs = dict(cls=respcls, body=decoded_body)\n                 if issubclass(respcls, TextResponse):\n                     # force recalculating the encoding until we make sure the\n\n@@ -54,8 +54,8 @@ class CloseSpider(object):\n             self.crawler.engine.close_spider(spider, 'closespider_pagecount')\n \n     def spider_opened(self, spider):\n-        self.task = reactor.callLater(self.close_on['timeout'], \\\n-            self.crawler.engine.close_spider, spider, \\\n+        self.task = reactor.callLater(self.close_on['timeout'],\n+                                      self.crawler.engine.close_spider, spider,\n                                       reason='closespider_timeout')\n \n     def item_scraped(self, item, spider):\n\n@@ -44,8 +44,7 @@ IGNORED_EXTENSIONS = [\n \n _re_type = type(re.compile(\"\", 0))\n _matches = lambda url, regexs: any(r.search(url) for r in regexs)\n-_is_valid_url = lambda url: url.split('://', 1)[0] in {'http', 'https', \\\n-                                                       'file', 'ftp'}\n+_is_valid_url = lambda url: url.split('://', 1)[0] in {'http', 'https', 'file', 'ftp'}\n \n \n class FilteringLinkExtractor(object):\n\n@@ -65,7 +65,7 @@ class MiddlewareManager(object):\n         return process_chain(self.methods[methodname], obj, *args)\n \n     def _process_chain_both(self, cb_methodname, eb_methodname, obj, *args):\n-        return process_chain_both(self.methods[cb_methodname], \\\n+        return process_chain_both(self.methods[cb_methodname],\n                                   self.methods[eb_methodname], obj, *args)\n \n     def open_spider(self, spider):\n\n@@ -37,7 +37,7 @@ def build_component_list(compdict, custom=None, convert=update_classpath):\n         \"\"\"Fail if a value in the components dict is not a real number or None.\"\"\"\n         for name, value in compdict.items():\n             if value is not None and not isinstance(value, numbers.Real):\n-                raise ValueError('Invalid value {} for component {}, please provide ' \\\n+                raise ValueError('Invalid value {} for component {}, please provide '\n                                  'a real number or None instead'.format(value, name))\n \n     # BEGIN Backward compatibility for old (base, custom) call signature\n\n@@ -25,17 +25,15 @@ class CmdlineTest(unittest.TestCase):\n         return comm.decode(encoding)\n \n     def test_default_settings(self):\n-        self.assertEqual(self._execute('settings', '--get', 'TEST1'), \\\n-                         'default')\n+        self.assertEqual(self._execute('settings', '--get', 'TEST1'), 'default')\n \n     def test_override_settings_using_set_arg(self):\n-        self.assertEqual(self._execute('settings', '--get', 'TEST1', '-s', 'TEST1=override'), \\\n-                         'override')\n+        self.assertEqual(self._execute('settings', '--get', 'TEST1', '-s',\n+                                       'TEST1=override'), 'override')\n \n     def test_override_settings_using_envvar(self):\n         self.env['SCRAPY_TEST1'] = 'override'\n-        self.assertEqual(self._execute('settings', '--get', 'TEST1'), \\\n-                         'override')\n+        self.assertEqual(self._execute('settings', '--get', 'TEST1'), 'override')\n \n     def test_profiling(self):\n         path = tempfile.mkdtemp()\n\n@@ -800,7 +800,7 @@ class S3TestCase(unittest.TestCase):\n         req = Request('s3://johnsmith/photos/puppy.jpg', headers={'Date': date})\n         with self._mocked_date(date):\n             httpreq = self.download_request(req, self.spider)\n-        self.assertEqual(httpreq.headers['Authorization'], \\\n+        self.assertEqual(httpreq.headers['Authorization'],\n                          b'AWS 0PN5J17HBGZHT7JJ3X82:xXjDGYUmKxnwqr5KXNPGldn5LbA=')\n \n     def test_request_signing2(self):\n@@ -813,20 +813,21 @@ class S3TestCase(unittest.TestCase):\n             })\n         with self._mocked_date(date):\n             httpreq = self.download_request(req, self.spider)\n-        self.assertEqual(httpreq.headers['Authorization'], \\\n+        self.assertEqual(httpreq.headers['Authorization'],\n                          b'AWS 0PN5J17HBGZHT7JJ3X82:hcicpDDvL9SsO6AkvxqmIWkmOuQ=')\n \n     def test_request_signing3(self):\n         # lists the content of the johnsmith bucket.\n         date = 'Tue, 27 Mar 2007 19:42:41 +0000'\n-        req = Request('s3://johnsmith/?prefix=photos&max-keys=50&marker=puppy', \\\n+        req = Request(\n+            's3://johnsmith/?prefix=photos&max-keys=50&marker=puppy',\n             method='GET', headers={\n                 'User-Agent': 'Mozilla/5.0',\n                 'Date': date,\n             })\n         with self._mocked_date(date):\n             httpreq = self.download_request(req, self.spider)\n-        self.assertEqual(httpreq.headers['Authorization'], \\\n+        self.assertEqual(httpreq.headers['Authorization'],\n                          b'AWS 0PN5J17HBGZHT7JJ3X82:jsRt/rhG+Vtp88HrYL706QhE4w4=')\n \n     def test_request_signing4(self):\n@@ -836,7 +837,7 @@ class S3TestCase(unittest.TestCase):\n             method='GET', headers={'Date': date})\n         with self._mocked_date(date):\n             httpreq = self.download_request(req, self.spider)\n-        self.assertEqual(httpreq.headers['Authorization'], \\\n+        self.assertEqual(httpreq.headers['Authorization'],\n                          b'AWS 0PN5J17HBGZHT7JJ3X82:thdUi9VAkzhkniLj96JIrOPGi0g=')\n \n     def test_request_signing5(self):\n@@ -850,8 +851,8 @@ class S3TestCase(unittest.TestCase):\n         # deletes an object from the 'johnsmith' bucket using the\n         # path-style and Date alternative.\n         date = 'Tue, 27 Mar 2007 21:20:27 +0000'\n-        req = Request('s3://johnsmith/photos/puppy.jpg', \\\n-                method='DELETE', headers={\n+        req = Request(\n+            's3://johnsmith/photos/puppy.jpg', method='DELETE', headers={\n                 'Date': date,\n                 'x-amz-date': 'Tue, 27 Mar 2007 21:20:26 +0000',\n             })\n@@ -864,7 +865,8 @@ class S3TestCase(unittest.TestCase):\n     def test_request_signing6(self):\n         # uploads an object to a CNAME style virtual hosted bucket with metadata.\n         date = 'Tue, 27 Mar 2007 21:06:08 +0000'\n-        req = Request('s3://static.johnsmith.net:8080/db-backup.dat.gz', \\\n+        req = Request(\n+            's3://static.johnsmith.net:8080/db-backup.dat.gz',\n             method='PUT', headers={\n                 'User-Agent': 'curl/7.15.5',\n                 'Host': 'static.johnsmith.net:8080',\n@@ -881,7 +883,7 @@ class S3TestCase(unittest.TestCase):\n             })\n         with self._mocked_date(date):\n             httpreq = self.download_request(req, self.spider)\n-        self.assertEqual(httpreq.headers['Authorization'], \\\n+        self.assertEqual(httpreq.headers['Authorization'],\n                          b'AWS 0PN5J17HBGZHT7JJ3X82:C0FlOtU8Ylb9KDTpZqYkZPX91iI=')\n \n     def test_request_signing7(self):\n\n@@ -165,12 +165,12 @@ class MaxRetryTimesTest(unittest.TestCase):\n         # SETTINGS: meta(max_retry_times) = 4\n         meta_max_retry_times = 4\n \n-        req = Request(self.invalid_url, meta= \\\n-            {'max_retry_times': meta_max_retry_times, 'dont_retry': True})\n+        req = Request(self.invalid_url, meta={\n+            'max_retry_times': meta_max_retry_times, 'dont_retry': True\n+        })\n \n         self._test_retry(req, DNSLookupError('foo'), 0)\n \n-\n     def _test_retry(self, req, exception, max_retry_times):\n         \n         for i in range(0, max_retry_times):\n\n@@ -91,7 +91,7 @@ def start_test_site(debug=False):\n \n     port = reactor.listenTCP(0, server.Site(r), interface=\"127.0.0.1\")\n     if debug:\n-        print(\"Test server running at http://localhost:%d/ - hit Ctrl-C to finish.\" \\\n+        print(\"Test server running at http://localhost:%d/ - hit Ctrl-C to finish.\"\n               % port.getHost().port)\n     return port\n \n\n@@ -622,8 +622,9 @@ class FormRequestTest(RequestTest):\n             <input type=\"hidden\" name=\"two\" value=\"3\">\n             <input type=\"submit\" name=\"clickable2\" value=\"clicked2\">\n             </form>\"\"\")\n-        req = self.request_class.from_response(response, formdata={'two': '2'}, \\\n-                                              clickdata={'name': 'clickable2'})\n+        req = self.request_class.from_response(\n+            response, formdata={'two': '2'}, clickdata={'name': 'clickable2'}\n+        )\n         fs = _qs(req)\n         self.assertEqual(fs[b'clickable2'], [b'clicked2'])\n         self.assertFalse(b'clickable1' in fs, fs)\n@@ -671,8 +672,9 @@ class FormRequestTest(RequestTest):\n             <input type=\"hidden\" name=\"one\" value=\"clicked1\">\n             <input type=\"hidden\" name=\"two\" value=\"clicked2\">\n             </form>\"\"\")\n-        req = self.request_class.from_response(response, \\\n-                clickdata={u'name': u'clickable', u'value': u'clicked2'})\n+        req = self.request_class.from_response(\n+            response, clickdata={u'name': u'clickable', u'value': u'clicked2'}\n+        )\n         fs = _qs(req)\n         self.assertEqual(fs[b'clickable'], [b'clicked2'])\n         self.assertEqual(fs[b'one'], [b'clicked1'])\n@@ -686,8 +688,9 @@ class FormRequestTest(RequestTest):\n             <input type=\"hidden\" name=\"poundsign\" value=\"\\u00a3\">\n             <input type=\"hidden\" name=\"eurosign\" value=\"\\u20ac\">\n             </form>\"\"\")\n-        req = self.request_class.from_response(response, \\\n-                clickdata={u'name': u'price in \\u00a3'})\n+        req = self.request_class.from_response(\n+            response, clickdata={u'name': u'price in \\u00a3'}\n+        )\n         fs = _qs(req, to_unicode=True)\n         self.assertTrue(fs[u'price in \\u00a3'])\n \n@@ -700,8 +703,9 @@ class FormRequestTest(RequestTest):\n             <input type=\"hidden\" name=\"yensign\" value=\"\\u00a5\">\n             </form>\"\"\",\n             encoding='latin1')\n-        req = self.request_class.from_response(response, \\\n-                clickdata={u'name': u'price in \\u00a5'})\n+        req = self.request_class.from_response(\n+            response, clickdata={u'name': u'price in \\u00a5'}\n+        )\n         fs = _qs(req, to_unicode=True, encoding='latin1')\n         self.assertTrue(fs[u'price in \\u00a5'])\n \n@@ -716,8 +720,9 @@ class FormRequestTest(RequestTest):\n             <input type=\"hidden\" name=\"field2\" value=\"value2\">\n             </form>\n             \"\"\")\n-        req = self.request_class.from_response(response, formname='form2', \\\n-                clickdata={u'name': u'clickable'})\n+        req = self.request_class.from_response(\n+            response, formname='form2', clickdata={u'name': u'clickable'}\n+        )\n         fs = _qs(req)\n         self.assertEqual(fs[b'clickable'], [b'clicked2'])\n         self.assertEqual(fs[b'field2'], [b'value2'])\n@@ -725,8 +730,9 @@ class FormRequestTest(RequestTest):\n \n     def test_from_response_override_clickable(self):\n         response = _buildresponse('''<form><input type=\"submit\" name=\"clickme\" value=\"one\"> </form>''')\n-        req = self.request_class.from_response(response, \\\n-                formdata={'clickme': 'two'}, clickdata={'name': 'clickme'})\n+        req = self.request_class.from_response(\n+            response, formdata={'clickme': 'two'}, clickdata={'name': 'clickme'}\n+        )\n         fs = _qs(req)\n         self.assertEqual(fs[b'clickme'], [b'two'])\n \n@@ -853,7 +859,7 @@ class FormRequestTest(RequestTest):\n             <form name=\"form2\" action=\"post.php\" method=\"POST\">\n             <input type=\"hidden\" name=\"two\" value=\"2\">\n             </form>\"\"\")\n-        self.assertRaises(IndexError, self.request_class.from_response, \\\n+        self.assertRaises(IndexError, self.request_class.from_response,\n                           response, formname=\"form3\", formnumber=2)\n \n     def test_from_response_formid_exists(self):\n@@ -907,7 +913,7 @@ class FormRequestTest(RequestTest):\n             <form id=\"form2\" name=\"form2\" action=\"post.php\" method=\"POST\">\n             <input type=\"hidden\" name=\"two\" value=\"2\">\n             </form>\"\"\")\n-        self.assertRaises(IndexError, self.request_class.from_response, \\\n+        self.assertRaises(IndexError, self.request_class.from_response,\n                           response, formid=\"form3\", formnumber=2)\n \n     def test_from_response_select(self):\n\n@@ -316,7 +316,7 @@ class TextResponseTest(BaseResponseTest):\n         assert u'SUFFIX' in r.text, repr(r.text)\n \n         # Do not destroy html tags due to encoding bugs\n-        r = self.response_class(\"http://example.com\", encoding='utf-8', \\\n+        r = self.response_class(\"http://example.com\", encoding='utf-8',\n                                 body=b'\\xf0<span>value</span>')\n         assert u'<span>value</span>' in r.text, repr(r.text)\n \n\n@@ -13,7 +13,7 @@ class ChunkedTest(unittest.TestCase):\n         chunked_body += \"8\\r\\n\" + \"sequence\\r\\n\"\n         chunked_body += \"0\\r\\n\\r\\n\"\n         body = decode_chunked_transfer(chunked_body)\n-        self.assertEqual(body, \\\n+        self.assertEqual(body,\n                          \"This is the data in the first chunk\\r\\n\" +\n                          \"and this is the second one\\r\\n\" +\n                          \"consequence\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
