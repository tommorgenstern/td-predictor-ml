{"custom_id": "scrapy#92068067703ecc269f210dd5268f0a533231432d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 136 | Contributors (this commit): 4 | Commits (past 90d): 8 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -4,6 +4,7 @@\n # some of its features, like package dependencies.\n from distutils.command.install_data import install_data\n from distutils.command.install import INSTALL_SCHEMES\n+from subprocess import Popen, PIPE\n import os\n import sys\n \n@@ -78,9 +79,14 @@ scripts = ['bin/scrapy']\n if os.name == 'nt':\n     scripts.append('extras/scrapy.bat')\n \n+version = __import__('scrapy').__version__\n+if os.environ.get('SCRAPY_VERSION_FROM_HG'):\n+    rev = Popen([\"hg\", \"tip\", \"--template\", \"{rev}\"], stdout=PIPE).communicate()[0]\n+    version = \"%s.r%s\" % (version, rev)\n+\n setup_args = {\n     'name': 'Scrapy',\n-    'version': __import__('scrapy').__version__,\n+    'version': version,\n     'url': 'http://scrapy.org',\n     'description': 'A high-level Python Screen Scraping framework',\n     'long_description': 'Scrapy is a high level scraping and web crawling framework for writing spiders to crawl and parse web pages for all kinds of purposes, from information retrieval to monitoring or testing web sites.',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#49ffe528a39e29a84246b31097e2e6aa90cbbb8f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 19 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2,9 +2,11 @@ from twisted.internet import reactor, error\n \n def listen_tcp(portrange, factory):\n     \"\"\"Like reactor.listenTCP but tries different ports in a range.\"\"\"\n-    assert len(portrange) in [1, 2], \"invalid portrange: %s\" % portrange\n+    assert len(portrange) <= 2, \"invalid portrange: %s\" % portrange\n     if not hasattr(portrange, '__iter__'):\n         return reactor.listenTCP(portrange, factory)\n+    if not portrange:\n+        return reactor.listenTCP(0, factory)\n     if len(portrange) == 1:\n         return reactor.listenTCP(portrange[0], factory)\n     for x in range(portrange[0], portrange[1]+1):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0bf9e4627cfcc477f23fb81edabbad85c24911bd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 10 | Churn Cumulative: 44 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -18,16 +18,25 @@ class CloseSpider(object):\n     def __init__(self):\n         self.timeout = settings.getint('CLOSESPIDER_TIMEOUT')\n         self.itempassed = settings.getint('CLOSESPIDER_ITEMPASSED')\n+        self.pagecount = settings.getint('CLOSESPIDER_PAGECOUNT')\n \n+        self.pagecounts = defaultdict(int)\n         self.counts = defaultdict(int)\n         self.tasks = {}\n \n+        if self.pagecount:\n+            dispatcher.connect(self.page_count, signal=signals.response_received)\n         if self.timeout:\n             dispatcher.connect(self.spider_opened, signal=signals.spider_opened)\n         if self.itempassed:\n             dispatcher.connect(self.item_passed, signal=signals.item_passed)\n         dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n \n+    def page_count(self, response, request, spider):\n+        self.pagecounts[spider] += 1\n+        if self.pagecounts[spider] == self.pagecount:\n+            crawler.engine.close_spider(spider, 'closespider_pagecount')\n+\n     def spider_opened(self, spider):\n         self.tasks[spider] = reactor.callLater(self.timeout, \\\n             crawler.engine.close_spider, spider=spider, \\\n\n@@ -19,6 +19,7 @@ BOT_NAME = 'scrapybot'\n BOT_VERSION = '1.0'\n \n CLOSESPIDER_TIMEOUT = 0\n+CLOSESPIDER_PAGECOUNT = 0\n CLOSESPIDER_ITEMPASSED = 0\n \n COMMANDS_MODULE = ''\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#61ab9b86b774f4ecfbd546b27547aeddbf750806", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 111 | Contributors (this commit): 2 | Commits (past 90d): 11 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -2,8 +2,8 @@\n Scrapy - a screen scraping framework written in Python\n \"\"\"\n \n-version_info = (0, 10, 3, 'dev')\n-__version__ = \"0.10.3\"\n+version_info = (0, 10, 4, 'dev')\n+__version__ = \"0.10.4\"\n \n import sys, os, warnings\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2d40705ea00bbc0ce1846dbe675c133cd546d9a7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 17 | Churn Cumulative: 57 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -7,9 +7,10 @@ See documentation in docs/topics/extensions.rst\n from collections import defaultdict\n \n from twisted.internet import reactor\n+from twisted.python import log as txlog\n from scrapy.xlib.pydispatch import dispatcher\n \n-from scrapy import signals\n+from scrapy import signals, log\n from scrapy.project import crawler\n from scrapy.conf import settings\n \n@@ -19,11 +20,15 @@ class CloseSpider(object):\n         self.timeout = settings.getint('CLOSESPIDER_TIMEOUT')\n         self.itempassed = settings.getint('CLOSESPIDER_ITEMPASSED')\n         self.pagecount = settings.getint('CLOSESPIDER_PAGECOUNT')\n+        self.errorcount = settings.getint('CLOSESPIDER_ERRORCOUNT')\n \n+        self.errorcounts = defaultdict(int)\n         self.pagecounts = defaultdict(int)\n         self.counts = defaultdict(int)\n         self.tasks = {}\n \n+        if self.errorcount:\n+            txlog.addObserver(self.catch_log)\n         if self.pagecount:\n             dispatcher.connect(self.page_count, signal=signals.response_received)\n         if self.timeout:\n@@ -32,6 +37,14 @@ class CloseSpider(object):\n             dispatcher.connect(self.item_passed, signal=signals.item_passed)\n         dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n \n+    def catch_log(self, event):\n+        if event.get('logLevel') == log.ERROR:\n+            spider = event.get('spider')\n+            if spider:\n+                self.errorcounts[spider] += 1\n+                if self.errorcounts[spider] == self.errorcount:\n+                    crawler.engine.close_spider(spider, 'closespider_errorcount')\n+\n     def page_count(self, response, request, spider):\n         self.pagecounts[spider] += 1\n         if self.pagecounts[spider] == self.pagecount:\n@@ -49,6 +62,8 @@ class CloseSpider(object):\n \n     def spider_closed(self, spider):\n         self.counts.pop(spider, None)\n+        self.pagecounts.pop(spider, None)\n+        self.errorcounts.pop(spider, None)\n         tsk = self.tasks.pop(spider, None)\n         if tsk and tsk.active():\n             tsk.cancel()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4bbcbd7b77748bb073841b627fa9de388ee72da6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 114 | Contributors (this commit): 2 | Commits (past 90d): 12 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -16,8 +16,11 @@ warnings.filterwarnings('ignore', category=DeprecationWarning, module='twisted')\n \n # prevents noisy (and innocent) dropin.cache errors when loading spiders from\n # egg files using the old Spider Manager. TODO: Remove for Scrapy 0.11\n+try:\n     from twisted.python.zippath import ZipPath\n     ZipPath.setContent = lambda x, y: None\n+except ImportError: # to avoid making setup.py depend on Twisted\n+    pass\n \n # monkey patches to fix external library issues\n from scrapy.xlib import twisted_250_monkeypatches, urlparse_monkeypatches\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fafaee51d558e74ff6cd223aabf7089b1f994168", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 49 | Lines Deleted: 44 | Files Changed: 3 | Hunks: 22 | Methods Changed: 11 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 93 | Churn Cumulative: 775 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2,15 +2,15 @@\n htmlpage.py tests\n \"\"\"\n import os\n-from gzip import GzipFile\n from unittest import TestCase\n \n from scrapy.utils.py26 import json\n from scrapy.tests.test_contrib_ibl import path\n from scrapy.contrib.ibl.htmlpage import parse_html, HtmlTag, HtmlDataFragment\n from scrapy.tests.test_contrib_ibl.test_htmlpage_data import *\n+from scrapy.utils.python import unicode_to_str, str_to_unicode\n \n-SAMPLES_FILE = \"samples_htmlpage.json.gz\"\n+SAMPLES_FILE_PREFIX = os.path.join(path, \"samples/samples_htmlpage\")\n \n def _encode_element(el):\n     \"\"\"\n@@ -39,24 +39,18 @@ def add_sample(source):\n     Method for adding samples to test samples file\n     (use from console)\n     \"\"\"\n-    samples = []\n-    if os.path.exists(SAMPLES_FILE):\n-        for line in GzipFile(os.path.join(path, SAMPLES_FILE), \"r\").readlines():\n-            samples.append(json.loads(line))\n+    count = 0\n+    while os.path.exists(\"%s_%d.json\" % (SAMPLES_FILE_PREFIX, count)):\n+        count += 1\n     \n-    new_sample = {\"source\": source}\n-    new_sample[\"parsed\"] = list(parse_html(source))\n-    samples.append(new_sample)\n-    samples_file = GzipFile(os.path.join(path, SAMPLES_FILE), \"wb\")\n-    for sample in samples:\n-        samples_file.write(json.dumps(sample, default=_encode_element) + \"\\n\")\n-    samples_file.close()\n+    open(\"%s_%d.html\" % (SAMPLES_FILE_PREFIX, count), \"w\").write(unicode_to_str(source))\n+    parsed = list(parse_html(source))\n+    open(\"%s_%d.json\" % (SAMPLES_FILE_PREFIX, count), \"w\")\\\n+        .write(json.dumps(parsed, default=_encode_element, indent=8))\n \n class TestParseHtml(TestCase):\n     \"\"\"Test for parse_html\"\"\"\n-    def _test_sample(self, sample):\n-        source = sample[\"source\"]\n-        expected_parsed = sample[\"parsed\"]\n+    def _test_sample(self, source, expected_parsed, samplecount=None):\n         parsed = parse_html(source)\n         count_element = 0\n         count_expected = 0\n@@ -69,59 +63,70 @@ class TestParseHtml(TestCase):\n             element_text = source[element.start:element.end]\n             expected_text = source[expected.start:expected.end]\n             if element.start != expected.start or element.end != expected.end:\n-                assert False, \"[%s,%s] %s != [%s,%s] %s\" % (element.start, \\\n+                errstring = \"[%s,%s] %s != [%s,%s] %s\" % (element.start, \\\n                     element.end, element_text, expected.start, \\\n                     expected.end, expected_text)\n+                if samplecount is not None:\n+                    errstring += \" (sample %d)\" % samplecount\n+                assert False, errstring\n             if type(element) != type(expected):\n-                assert False, \"(%s) %s != (%s) %s for text\\n%s\" % (count_element, \\\n+                errstring = \"(%s) %s != (%s) %s for text\\n%s\" % (count_element, \\\n                     repr(type(element)), count_expected, repr(type(expected)), element_text)\n+                if samplecount is not None:\n+                    errstring += \" (sample %d)\" % samplecount\n+                assert False, errstring\n             if type(element) == HtmlTag:\n                 self.assertEqual(element.tag, expected.tag)\n                 self.assertEqual(element.attributes, expected.attributes)\n+                self.assertEqual(element.tag_type, expected.tag_type)\n+        if expected_parsed:\n+            errstring = \"Expected %s\" % repr(expected_parsed)\n+            if samplecount is not None:\n+                errstring += \" (sample %d)\" % samplecount\n+            assert False, errstring\n \n     def test_parse(self):\n         \"\"\"simple parse_html test\"\"\"\n         parsed = [_decode_element(d) for d in PARSED]\n         sample = {\"source\": PAGE, \"parsed\": parsed}\n-        self._test_sample(sample)\n+        self._test_sample(PAGE, parsed)\n         \n     def test_site_samples(self):\n         \"\"\"test parse_html from real cases\"\"\"\n-        samples = []\n-        for line in GzipFile(os.path.join(path, SAMPLES_FILE), \"r\").readlines():\n-            samples.append(json.loads(line, object_hook=_decode_element))\n-        for sample in samples:\n-            self._test_sample(sample)\n+        count = 0\n+        fname = \"%s_%d.json\" % (SAMPLES_FILE_PREFIX, count)\n+        while os.path.exists(fname):\n+            source = str_to_unicode(open(\"%s_%d.html\" % (SAMPLES_FILE_PREFIX, count), \"r\").read())\n+            parsed = json.loads(str_to_unicode(open(fname, \"r\").read()),\\\n+                    object_hook=_decode_element)\n+            self._test_sample(source, parsed, count)\n+            count += 1\n+            fname = \"%s_%d.json\" % (SAMPLES_FILE_PREFIX, count)\n  \n     def test_bad(self):\n         \"\"\"test parsing of bad html layout\"\"\"\n         parsed = [_decode_element(d) for d in PARSED2]\n-        sample = {\"source\": PAGE2, \"parsed\": parsed}\n-        self._test_sample(sample)\n+        self._test_sample(PAGE2, parsed)\n \n     def test_comments(self):\n         \"\"\"test parsing of tags inside comments\"\"\"\n         parsed = [_decode_element(d) for d in PARSED3]\n-        sample = {\"source\": PAGE3, \"parsed\": parsed}\n-        self._test_sample(sample)\n+        self._test_sample(PAGE3, parsed)\n \n     def test_script_text(self):\n         \"\"\"test parsing of tags inside scripts\"\"\"\n         parsed = [_decode_element(d) for d in PARSED4]\n-        sample = {\"source\": PAGE4, \"parsed\": parsed}\n-        self._test_sample(sample)\n+        self._test_sample(PAGE4, parsed)\n         \n     def test_sucessive(self):\n         \"\"\"test parsing of sucesive cleaned elements\"\"\"\n         parsed = [_decode_element(d) for d in PARSED5]\n-        sample = {\"source\": PAGE5, \"parsed\": parsed}\n-        self._test_sample(sample)\n+        self._test_sample(PAGE5, parsed)\n         \n     def test_sucessive2(self):\n         \"\"\"test parsing of sucesive cleaned elements (variant 2)\"\"\"\n         parsed = [_decode_element(d) for d in PARSED6]\n-        sample = {\"source\": PAGE6, \"parsed\": parsed}\n-        self._test_sample(sample)\n+        self._test_sample(PAGE6, parsed)\n     \n     def test_special_cases(self):\n         \"\"\"some special cases tests\"\"\"\n\n@@ -137,7 +137,7 @@ PARSED2 = [\n  {'end': 8, 'start': 7},\n  {'attributes': {}, 'end': 14, 'start': 8, 'tag': u'body', 'tag_type': 1},\n  {'end': 15, 'start': 14},\n- {'attributes': {u'style': u'&#34;margin:', u'0pt&#34;': None, u'class': u'&#34;MsoNormal&#34;', u'0cm': None}, 'end': 80, 'start': 15, 'tag': u'p', 'tag_type': 2},\n+ {'attributes': {u'style': u'&#34;margin:', u'0pt&#34;': None, u'class': u'&#34;MsoNormal&#34;', u'0cm': None}, 'end': 80, 'start': 15, 'tag': u'p', 'tag_type': 1},\n  {'attributes': {u'lang': u'&#34;EN-GB&#34;'}, 'end': 107, 'start': 80, 'tag': u'span', 'tag_type': 1},\n  {'end': 121, 'start': 107},\n  {'attributes': {}, 'end': 128, 'start': 121, 'tag': u'span', 'tag_type': 2},\n\n@@ -3,7 +3,6 @@ Unit tests for pageparsing\n \"\"\"\n import os\n from cStringIO import StringIO                                                                                                    \n-from gzip import GzipFile\n \n from twisted.trial.unittest import TestCase, SkipTest\n from scrapy.utils.python import str_to_unicode\n@@ -274,14 +273,13 @@ class TestPageParsing(TestCase):\n         \"\"\"\n         Tests from real pages. More reliable and easy to build for more complicated structures\n         \"\"\"\n-        samples_file = open(os.path.join(path, \"samples_pageparsing.json.gz\"), \"rb\")\n-        samples = []\n-        for line in GzipFile(fileobj=StringIO(samples_file.read())).readlines():\n-            samples.append(json.loads(line))\n-        for sample in samples:\n-            source = sample[\"annotated\"]\n-            annotations = sample[\"annotations\"]\n-            template = HtmlPage(body=str_to_unicode(source))\n+        SAMPLES_FILE_PREFIX = os.path.join(path, \"samples/samples_pageparsing\")\n+        count = 0\n+        fname = \"%s_%d.json\" % (SAMPLES_FILE_PREFIX, count)\n+        while os.path.exists(fname):\n+            source = str_to_unicode(open(\"%s_%d.html\" % (SAMPLES_FILE_PREFIX, count), \"r\").read())\n+            annotations = json.loads(str_to_unicode(open(fname, \"r\").read()))\n+            template = HtmlPage(body=source)\n             parser = TemplatePageParser(TokenDict())\n             parser.feed(template)\n             for annotation in parser.annotations:\n@@ -293,3 +291,5 @@ class TestPageParsing(TestCase):\n                     else:\n                         self.assertEqual(getattr(annotation, s), test_annotation[s])\n             self.assertEqual(annotations, [])\n+            count += 1\n+            fname = \"%s_%d.json\" % (SAMPLES_FILE_PREFIX, count)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a67a981cca207221777b0755d0bd42473eff9a8f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 59 | Lines Deleted: 83 | Files Changed: 2 | Hunks: 11 | Methods Changed: 5 | Complexity Δ (Sum/Max): -9/0 | Churn Δ: 142 | Churn Cumulative: 583 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -83,93 +83,18 @@ class HtmlTag(HtmlDataFragment):\n _ATTR = \"((?:[^=/>\\s]|/(?!>))+)(?:\\s*=(?:\\s*\\\"(.*?)\\\"|\\s*'(.*?)'|([^>\\s]+))?)?\"\n _TAG = \"<(\\/?)(\\w+(?::\\w+)?)((?:\\s+\" + _ATTR + \")+\\s*|\\s*)(\\/?)>\"\n _DOCTYPE = r\"<!DOCTYPE.*?>\"\n+_SCRIPT = \"(<script.*?>)(.*?)(</script.*?>)\"\n+_COMMENT = \"(<!--.*?-->)\"\n \n _ATTR_REGEXP = re.compile(_ATTR, re.I | re.DOTALL)\n-_HTML_REGEXP = re.compile(_TAG, re.I | re.DOTALL)\n+_HTML_REGEXP = re.compile(\"%s|%s|%s\" % (_COMMENT, _SCRIPT, _TAG), re.I | re.DOTALL)\n _DOCTYPE_REGEXP = re.compile(\"(?:%s)\" % _DOCTYPE)\n-_COMMENT_RE = re.compile(\"(<!--.*?-->)\", re.DOTALL)\n-_SCRIPT_RE = re.compile(\"(<script.*?>).*?(</script.*?>)\", re.DOTALL | re.I)\n+_COMMENT_REGEXP = re.compile(_COMMENT, re.DOTALL)\n \n def parse_html(text):\n     \"\"\"Higher level html parser. Calls lower level parsers and joins sucesive\n     HtmlDataFragment elements in a single one.\n     \"\"\"\n-    script_layer = lambda x: _parse_clean_html(x, _SCRIPT_RE, HtmlTag, _simple_parse_html)\n-    comment_layer = lambda x: _parse_clean_html(x, _COMMENT_RE, HtmlDataFragment, script_layer)\n-    delayed_element = None\n-    for element in comment_layer(text):\n-        if isinstance(element, HtmlTag):\n-            if delayed_element is not None:\n-                yield delayed_element\n-                delayed_element = None\n-            yield element\n-        else:# element is HtmlDataFragment\n-            if delayed_element is not None:\n-                delayed_element.start = min(element.start, delayed_element.start)\n-                delayed_element.end = max(element.end, delayed_element.end)\n-            else:\n-                delayed_element = element\n-    if delayed_element is not None:\n-        yield delayed_element\n-\n-def _parse_clean_html(text, regex, htype, func):\n-    \"\"\"\n-    Removes regions from text, passes the cleaned text to the lower parse layer,\n-    and reinserts removed regions.\n-    regex - regular expression that defines regions to be removed/re inserted\n-    htype - the html parser type of the removed elements\n-    func - function that performs the lower parse layer\n-    \"\"\"\n-    removed = [[m.start(), m.end(), m.groups()] for m in regex.finditer(text)]\n-    \n-    cleaned = regex.sub(\"\", text)\n-    shift = 0\n-    for element in func(cleaned):\n-        element.start += shift\n-        element.end += shift\n-        while removed:\n-            if element.end <= removed[0][0]:\n-                yield element\n-                break\n-            else:\n-                start, end, groups = removed.pop(0)\n-                add = end - start\n-                element.end += add\n-                shift += add\n-                if element.start >= start:\n-                    element.start += add\n-                elif isinstance(element, HtmlTag):\n-                    yield element\n-                    break\n-                \n-                if element.start < start:\n-                    yield HtmlDataFragment(element.start, start)\n-                    element.start = end\n-                \n-                if htype == HtmlTag:\n-                    begintag = _parse_tag(_HTML_REGEXP.match(groups[0]))\n-                    endtag = _parse_tag(_HTML_REGEXP.match(groups[1]))\n-                    begintag.start = start\n-                    begintag.end += start\n-                    \n-                    endtag.start = end - endtag.end\n-                    endtag.end = end\n-                    content = None\n-                    if begintag.end < endtag.start:\n-                        content = HtmlDataFragment(begintag.end, endtag.start)\n-                    yield begintag\n-                    if content is not None:\n-                        yield content\n-                    yield endtag\n-                else:\n-                    yield htype(start, end)\n-        else:\n-            yield element\n-\n-def _simple_parse_html(text):\n-    \"\"\"Simple html parse. It returns a sequence of HtmlTag and HtmlDataFragment\n-    objects. Does not ignore any region.\n-    \"\"\"\n     # If have doctype remove it.\n     start_pos = 0\n     match = _DOCTYPE_REGEXP.match(text)\n@@ -183,18 +108,48 @@ def _simple_parse_html(text):\n         if start > prev_end:\n             yield HtmlDataFragment(prev_end, start)\n \n+        if match.groups()[0] is not None: # comment\n+            yield HtmlDataFragment(start, end)\n+        elif match.groups()[1] is not None: # <script>...</script>\n+            for e in _parse_script(match):\n+                yield e\n+        else: # tag\n             yield _parse_tag(match)\n         prev_end = end\n     textlen = len(text)\n     if prev_end < textlen:\n         yield HtmlDataFragment(prev_end, textlen)\n \n+def _parse_script(match):\n+    \"\"\"parse a <script>...</script> region matched by _HTML_REGEXP\"\"\"\n+    open_text, content, close_text = match.groups()[1:4]\n+\n+    open_tag = _parse_tag(_HTML_REGEXP.match(open_text))\n+    open_tag.start = match.start()\n+    open_tag.end = match.start() + len(open_text)\n+\n+    close_tag = _parse_tag(_HTML_REGEXP.match(close_text))\n+    close_tag.start = match.end() - len(close_text)\n+    close_tag.end = match.end()\n+    \n+    yield open_tag\n+    if open_tag.end < close_tag.start:\n+        start_pos = 0\n+        for m in _COMMENT_REGEXP.finditer(content):\n+            if m.start() > start_pos:\n+                yield HtmlDataFragment(open_tag.end + start_pos, open_tag.end + m.start())\n+            yield HtmlDataFragment(open_tag.end + m.start(), open_tag.end + m.end())\n+            start_pos = m.end()\n+        if open_tag.end + start_pos < close_tag.start:\n+            yield HtmlDataFragment(open_tag.end + start_pos, close_tag.start)\n+    yield close_tag\n+\n def _parse_tag(match):\n     \"\"\"\n     parse a tag matched by _HTML_REGEXP\n     \"\"\"\n     data = match.groups()\n-    closing, tag, attr_text = data[:3]\n+    closing, tag, attr_text = data[4:7]\n     # if tag is None then the match is a comment\n     if tag is not None:\n         unpaired = data[-1]\n\n@@ -165,7 +165,9 @@ PARSED3 = [\n  {'attributes': {}, 'end': 55, 'start': 51, 'tag': u'p', 'tag_type': 2},\n  {'end': 70, 'start': 55},\n  {'attributes': {u'type': u'text/javascript'}, 'end': 101, 'start': 70, 'tag': u'script', 'tag_type': 1},\n- {'end': 124, 'start': 101},\n+ {'end': 104, 'start': 101},\n+ {'end': 118, 'start': 104},\n+ {'end': 124, 'start': 118},\n  {'attributes': {}, 'end': 133, 'start': 124, 'tag': u'script', 'tag_type': 2},\n  {'attributes': {}, 'end': 140, 'start': 133, 'tag': u'body', 'tag_type': 2},\n  {'attributes': {}, 'end': 147, 'start': 140, 'tag': u'html', 'tag_type': 2}\n@@ -204,7 +206,8 @@ PARSED5 = [\n  {'end': 45, 'start': 42},\n  {'attributes': {}, 'end': 54, 'start': 45, 'tag': u'script', 'tag_type': 2},\n  {'attributes': {}, 'end': 61, 'start': 54, 'tag': u'body', 'tag_type': 2},\n- {'end': 91, 'start': 61},\n+ {'end': 76, 'start': 61},\n+ {'end': 91, 'start': 76},\n  {'attributes': {}, 'end': 98, 'start': 91, 'tag': u'html', 'tag_type': 2},\n ]\n  \n@@ -215,7 +218,9 @@ PARSED6 = [\n  {'attributes': {}, 'end': 6, 'start': 0, 'tag': u'html', 'tag_type': 1},\n  {'attributes': {}, 'end': 12, 'start': 6, 'tag': u'body', 'tag_type': 1},\n  {'attributes': {}, 'end': 20, 'start': 12, 'tag': u'script', 'tag_type': 1},\n- {'end': 40, 'start': 20},\n+ {'end': 23, 'start': 20},\n+ {'end': 37, 'start': 23},\n+ {'end': 40, 'start': 37},\n  {'attributes': {}, 'end': 49, 'start': 40, 'tag': u'script', 'tag_type': 2},\n  {'end': 52, 'start': 49},\n  {'attributes': {}, 'end': 60, 'start': 52, 'tag': u'script', 'tag_type': 1},\n@@ -225,3 +230,19 @@ PARSED6 = [\n  {'attributes': {}, 'end': 81, 'start': 74, 'tag': u'body', 'tag_type': 2},\n  {'attributes': {}, 'end': 88, 'start': 81, 'tag': u'html', 'tag_type': 2},\n ]\n+\n+# Test source without ending body nor html\n+PAGE7 = u\"\"\"<html><body><p>veris in temporibus sub aprilis idibus</p><script>script code</script><!--comment-->\"\"\"\n+\n+PARSED7 = [\n+    {'attributes' : {}, 'end': 6, 'start': 0, 'tag': u'html', 'tag_type': 1},\n+    {'attributes': {}, 'end': 12, 'start': 6, 'tag': u'body', 'tag_type': 1},\n+    {'attributes': {}, 'end': 15, 'start': 12, 'tag': u'p', 'tag_type': 1},\n+    {'end': 53, 'start': 15},\n+    {'attributes': {}, 'end': 57, 'start': 53, 'tag': u'p', 'tag_type': 2},\n+    {'attributes' : {}, 'end': 65, 'start': 57, 'tag': u'script', 'tag_type': 1},\n+    {'end': 76, 'start': 65},\n+    {'attributes' : {}, 'end': 85, 'start': 76, 'tag': u'script', 'tag_type': 2},\n+    {'end': 99, 'start': 85},\n+]\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a99bb0de9bdac4bae700a1cfce3e8d4b2e755b35", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 558 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -43,9 +43,9 @@ def add_sample(source):\n     while os.path.exists(\"%s_%d.json\" % (SAMPLES_FILE_PREFIX, count)):\n         count += 1\n     \n-    open(\"%s_%d.html\" % (SAMPLES_FILE_PREFIX, count), \"w\").write(unicode_to_str(source))\n+    open(\"%s_%d.html\" % (SAMPLES_FILE_PREFIX, count), \"wb\").write(unicode_to_str(source))\n     parsed = list(parse_html(source))\n-    open(\"%s_%d.json\" % (SAMPLES_FILE_PREFIX, count), \"w\")\\\n+    open(\"%s_%d.json\" % (SAMPLES_FILE_PREFIX, count), \"wb\")\\\n         .write(json.dumps(parsed, default=_encode_element, indent=8))\n \n class TestParseHtml(TestCase):\n@@ -96,8 +96,8 @@ class TestParseHtml(TestCase):\n         count = 0\n         fname = \"%s_%d.json\" % (SAMPLES_FILE_PREFIX, count)\n         while os.path.exists(fname):\n-            source = str_to_unicode(open(\"%s_%d.html\" % (SAMPLES_FILE_PREFIX, count), \"r\").read())\n-            parsed = json.loads(str_to_unicode(open(fname, \"r\").read()),\\\n+            source = str_to_unicode(open(\"%s_%d.html\" % (SAMPLES_FILE_PREFIX, count), \"rb\").read())\n+            parsed = json.loads(str_to_unicode(open(fname, \"rb\").read()),\\\n                     object_hook=_decode_element)\n             self._test_sample(source, parsed, count)\n             count += 1\n\n@@ -277,8 +277,8 @@ class TestPageParsing(TestCase):\n         count = 0\n         fname = \"%s_%d.json\" % (SAMPLES_FILE_PREFIX, count)\n         while os.path.exists(fname):\n-            source = str_to_unicode(open(\"%s_%d.html\" % (SAMPLES_FILE_PREFIX, count), \"r\").read())\n-            annotations = json.loads(str_to_unicode(open(fname, \"r\").read()))\n+            source = str_to_unicode(open(\"%s_%d.html\" % (SAMPLES_FILE_PREFIX, count), \"rb\").read())\n+            annotations = json.loads(str_to_unicode(open(fname, \"rb\").read()))\n             template = HtmlPage(body=source)\n             parser = TemplatePageParser(TokenDict())\n             parser.feed(template)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0b91c04007c8875f7d9a29cd5e151a91939c5160", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 19 | Churn Cumulative: 274 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -33,6 +33,8 @@ def _parse(url):\n \n class ScrapyHTTPPageGetter(HTTPClient):\n \n+    delimiter = '\\n'\n+\n     def connectionMade(self):\n         self.headers = Headers() # bucket for response headers\n \n@@ -47,6 +49,9 @@ class ScrapyHTTPPageGetter(HTTPClient):\n         if self.factory.body is not None:\n             self.transport.write(self.factory.body)\n \n+    def lineReceived(self, line):\n+        return HTTPClient.lineReceived(self, line.rstrip())\n+\n     def handleHeader(self, key, value):\n         self.headers.appendlist(key, value)\n \n\n@@ -172,6 +172,20 @@ class ScrapyHTTPPageGetterTests(unittest.TestCase):\n         self.assertEqual(transport.value(), testvalue)\n         return testvalue\n \n+    def test_non_standard_line_endings(self):\n+        # regression test for: http://dev.scrapy.org/ticket/258\n+        factory = client.ScrapyHTTPClientFactory(Request(\n+            url='http://foo/bar'))\n+        protocol = client.ScrapyHTTPPageGetter()\n+        protocol.factory = factory\n+        protocol.headers = Headers()\n+        protocol.dataReceived(\"HTTP/1.0 200 OK\\n\")\n+        protocol.dataReceived(\"Hello: World\\n\")\n+        protocol.dataReceived(\"Foo: Bar\\n\")\n+        protocol.dataReceived(\"\\n\")\n+        self.assertEqual(protocol.headers,\n+            Headers({'Hello': ['World'], 'Foo': ['Bar']}))\n+\n \n from twisted.web.test.test_webclient import ForeverTakingResource, \\\n         ErrorResource, NoLengthResource, HostHeaderResource, \\\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f5b188b17990ac5bce43da0aa05494c1fc8937dd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 10 | Churn Cumulative: 104 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -51,7 +51,8 @@ class RetryMiddleware(object):\n         return response\n \n     def process_exception(self, request, exception, spider):\n-        if isinstance(exception, self.EXCEPTIONS_TO_RETRY):\n+        if isinstance(exception, self.EXCEPTIONS_TO_RETRY) \\\n+                and 'dont_retry' not in request.meta:\n             return self._retry(request, exception, spider)\n \n     def _retry(self, request, reason, spider):\n\n@@ -35,6 +35,13 @@ class RetryTest(unittest.TestCase):\n         r = self.mw.process_response(req, rsp, self.spider)\n         assert r is rsp\n \n+    def test_dont_retry_exc(self):\n+        req = Request('http://www.scrapytest.org/503', meta={'dont_retry': True})\n+        rsp = Response('http://www.scrapytest.org/503', body='', status=503)\n+\n+        r = self.mw.process_exception(req, DNSLookupError(), self.spider)\n+        assert r is None\n+\n     def test_503(self):\n         req = Request('http://www.scrapytest.org/503')\n         rsp = Response('http://www.scrapytest.org/503', body='', status=503)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#deb9c7ef0484279003163f847abc7075f8fcc4a5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 92 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -55,7 +55,8 @@ def get_config(use_closest=True):\n     return cfg\n \n def get_sources(use_closest=True):\n-    sources = [os.path.expanduser('~/.scrapy.cfg'), '/etc/scrapy.cfg']\n+    sources = ['/etc/scrapy.cfg', r'c:\\scrapy\\scrapy.cfg', \\\n+        os.path.expanduser('~/.scrapy.cfg')]\n     if use_closest:\n-        sources.insert(0, closest_scrapy_cfg())\n+        sources.append(closest_scrapy_cfg())\n     return sources\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1d567cdce67320f8c22af0a5d2afa096edf282d7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 220 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 18 | Complexity Δ (Sum/Max): 43/38 | Churn Δ: 220 | Churn Cumulative: 220 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.9210526315789473\n\nDIFF:\n@@ -0,0 +1,186 @@\n+\n+import sys\n+import os\n+import glob\n+import tempfile\n+import shutil\n+import time\n+import urllib2\n+import netrc\n+from urlparse import urlparse, urljoin\n+from subprocess import Popen, PIPE, check_call\n+\n+from scrapy.command import ScrapyCommand\n+from scrapy.exceptions import UsageError\n+from scrapy.utils.py26 import json\n+from scrapy.utils.multipart import encode_multipart\n+from scrapy.utils.http import basic_auth_header\n+from scrapy.utils.conf import get_config, closest_scrapy_cfg\n+\n+_DEFAULT_TARGETS = {\n+    'scrapyd': {\n+        'url': 'http://localhost:6800/',\n+    },\n+}\n+\n+_SETUP_PY_TEMPLATE = \\\n+\"\"\"# Automatically created by: scrapy deploy\n+\n+from setuptools import setup, find_packages\n+\n+setup(\n+    name         = 'project',\n+    version      = '1.0',\n+    packages     = find_packages(),\n+    entry_points = {'scrapy': ['settings = %(settings)s']},\n+)\n+\"\"\"\n+\n+class Command(ScrapyCommand):\n+\n+    def syntax(self):\n+        return \"[options] [ <target:project> | -l <target> | -L ]\"\n+\n+    def short_desc(self):\n+        return \"Deploy project in Scrapyd server\"\n+\n+    def long_desc(self):\n+        return \"Deploy the current project into the given Scrapyd server \" \\\n+            \"(aka target) and project.\"\n+\n+    def add_options(self, parser):\n+        ScrapyCommand.add_options(self, parser)\n+        parser.add_option(\"-v\", \"--version\",\n+            help=\"the version to deploy. Defaults to current timestamp\")\n+        parser.add_option(\"-L\", \"--list-targets\", action=\"store_true\", \\\n+            help=\"list available targets\")\n+        parser.add_option(\"-l\", \"--list-projects\", metavar=\"TARGET\", \\\n+            help=\"list available projects on TARGET\")\n+\n+    def run(self, args, opts):\n+        try:\n+            import setuptools\n+        except ImportError:\n+            raise UsageError(\"setuptools not installed\")\n+        if opts.list_targets:\n+            for name, target in _get_targets().items():\n+                print \"%-20s %s\" % (name, target['url'])\n+            return\n+        if opts.list_projects:\n+            target = _get_target(opts.list_projects)\n+            req = urllib2.Request(_url(target, 'listprojects.json'))\n+            _add_auth_header(req, target)\n+            f = urllib2.urlopen(req)\n+            projects = json.loads(f.read())['projects']\n+            print os.linesep.join(projects)\n+            return\n+        target, project = _get_target_project(args)\n+        version = _get_version(opts)\n+        egg = _build_egg()\n+        _upload_egg(target, egg, project, version)\n+\n+def _log(message):\n+    sys.stderr.write(\"%s\\n\" % message)\n+\n+def _get_target_project(args):\n+    if len(args) >= 1 and ':' in args[0]:\n+        target_name, project = args[0].split(':', 1)\n+    elif len(args) < 1:\n+        target_name = _get_option('deploy', 'target')\n+        project = _get_option('deploy', 'project')\n+        if not target_name or not project:\n+            raise UsageError(\"<target:project> not given and defaults not found\")\n+    else:\n+        raise UsageError(\"%r is not a <target:project>\" % args[0])\n+    target = _get_target(target_name)\n+    return target, project\n+\n+def _get_option(section, option, default=None):\n+    cfg = get_config()\n+    return cfg.get(section, option) if cfg.has_option(section, option) \\\n+        else default\n+\n+def _get_targets():\n+    cfg = get_config()\n+    targets = _DEFAULT_TARGETS.copy()\n+    for x in cfg.sections():\n+        if x.startswith('deploy_'):\n+            targets[x[7:]] = dict(cfg.items(x))\n+    return targets\n+\n+def _get_target(name):\n+    try:\n+        return _get_targets()[name]\n+    except KeyError:\n+        raise UsageError(\"Unknown target: %s\" % name)\n+\n+def _url(target, action):\n+    return urljoin(target['url'], action)\n+\n+def _get_version(opts):\n+    if opts.version == 'HG':\n+        p = Popen(['hg', 'tip', '--template', '{rev}'], stdout=PIPE)\n+        return 'r%s' % p.communicate()[0]\n+    elif opts.version:\n+        return opts.version\n+    else:\n+        return str(int(time.time()))\n+\n+def _upload_egg(target, eggfile, project, version):\n+    data = {\n+        'project': project,\n+        'version': version,\n+        'egg': ('project.egg', eggfile.read()),\n+    }\n+    body, boundary = encode_multipart(data)\n+    url = _url(target, 'addversion.json')\n+    headers = {\n+        'Content-Type': 'multipart/form-data; boundary=%s' % boundary,\n+        'Content-Length': str(len(body)),\n+    }\n+    req = urllib2.Request(url, body, headers)\n+    _add_auth_header(req, target)\n+    _log(\"Deploying %s-%s to %s\" % (project, version, url))\n+    _http_post(req)\n+\n+def _add_auth_header(request, target):\n+    if 'username' in target:\n+        u, p = target.get('username'), target.get('password', '')\n+        request.add_header('Authorization', basic_auth_header(u, p))\n+    else: # try netrc\n+        try:\n+            host = urlparse(target['url']).hostname\n+            a = netrc.netrc().authenticators(host)\n+            request.add_header('Authorization', basic_auth_header(a[0], a[2]))\n+        except (netrc.NetrcParseError, TypeError):\n+            pass\n+\n+def _http_post(request):\n+    try:\n+        f = urllib2.urlopen(request)\n+        _log(\"Server response (%s):\" % f.getcode())\n+        print f.read()\n+    except urllib2.HTTPError, e:\n+        _log(\"Deploy failed (%s):\" % e.getcode())\n+        print e.read()\n+    except urllib2.URLError, e:\n+        _log(\"Deploy failed: %s\" % e)\n+\n+def _build_egg():\n+    closest = closest_scrapy_cfg()\n+    os.chdir(os.path.dirname(closest))\n+    if not os.path.exists('setup.py'):\n+        settings = get_config().get('settings', 'default')\n+        _create_default_setup_py(settings=settings)\n+    d = tempfile.mkdtemp()\n+    try:\n+        f = tempfile.TemporaryFile(dir=d)\n+        check_call([sys.executable, 'setup.py', 'bdist_egg', '-d', d], stdout=f)\n+        egg = glob.glob(os.path.join(d, '*.egg'))[0]\n+        return open(egg, 'rb')\n+    finally:\n+        shutil.rmtree(d)\n+\n+def _create_default_setup_py(**kwargs):\n+    with open('setup.py', 'w') as f:\n+        f.write(_SETUP_PY_TEMPLATE % kwargs)\n\n@@ -0,0 +1,34 @@\n+from cStringIO import StringIO\n+\n+def encode_multipart(data):\n+    \"\"\"Encode the given data to be used in a multipart HTTP POST. Data is a\n+    where keys are the field name, and values are either strings or tuples\n+    (filename, content) for file uploads.\n+\n+    This code is based on distutils.command.upload\n+    \"\"\"\n+\n+    # Build up the MIME payload for the POST data\n+    boundary = '--------------GHSKFJDLGDS7543FJKLFHRE75642756743254'\n+    sep_boundary = '\\r\\n--' + boundary\n+    end_boundary = sep_boundary + '--'\n+    body = StringIO()\n+    for key, value in data.items():\n+        # handle multiple entries for the same name\n+        if type(value) != type([]):\n+            value = [value]\n+        for value in value:\n+            if type(value) is tuple:\n+                fn = '; filename=\"%s\"' % value[0]\n+                value = value[1]\n+            else:\n+                fn = \"\"\n+\n+            body.write(sep_boundary)\n+            body.write('\\r\\nContent-Disposition: form-data; name=\"%s\"' % key)\n+            body.write(fn)\n+            body.write(\"\\r\\n\\r\\n\")\n+            body.write(value)\n+    body.write(end_boundary)\n+    body.write(\"\\r\\n\")\n+    return body.getvalue(), boundary\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f8b4d1dc5d6dd8f3f3f642cea342ca37cb6f2b4e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 187 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,3 +1,4 @@\n+from __future__ import with_statement\n \n import sys\n import os\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#992683ac5cca1840738cafa91a74170e72cea67f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 189 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -39,6 +39,8 @@ setup(\n \n class Command(ScrapyCommand):\n \n+    requires_project = True\n+\n     def syntax(self):\n         return \"[options] [ <target:project> | -l <target> | -L ]\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2873c4d9fedd2c6b58ba22924b5f880acd94e283", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 16 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -18,10 +18,13 @@ class SimpledbStatsCollector(StatsCollector):\n     def __init__(self):\n         super(SimpledbStatsCollector, self).__init__()\n         self._sdbdomain = settings['STATS_SDB_DOMAIN']\n+        self._access_key = settings['AWS_ACCESS_KEY_ID']\n+        self._secret_key = settings['AWS_SECRET_ACCESS_KEY']\n+\n         self._async = settings.getbool('STATS_SDB_ASYNC')\n         import boto\n         self.connect_sdb = boto.connect_sdb\n-        self.connect_sdb().create_domain(self._sdbdomain)\n+        self.connect_sdb(aws_access_key_id=self._access_key, aws_secret_access_key=self._secret_key).create_domain(self._sdbdomain)\n \n     def _persist_stats(self, stats, spider=None):\n         if spider is None: # only store spider-specific stats\n@@ -41,7 +44,7 @@ class SimpledbStatsCollector(StatsCollector):\n         sdb_item = dict((k, self._to_sdb_value(v, k)) for k, v in stats.iteritems())\n         sdb_item['spider'] = spider.name\n         sdb_item['timestamp'] = self._to_sdb_value(ts)\n-        self.connect_sdb().put_attributes(self._sdbdomain, sdb_item_id, sdb_item)\n+        self.connect_sdb(aws_access_key_id=self._access_key, aws_secret_access_key=self._secret_key).put_attributes(self._sdbdomain, sdb_item_id, sdb_item)\n \n     def _get_timestamp(self, spider):\n         return datetime.utcnow()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d17edd4a5972d252b1290b76108aa441bf155333", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 38 | Churn Cumulative: 1548 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -409,16 +409,16 @@ class RecordExtractor(object):\n                 _, _, nested_data = self._doextract(page, nested_regions, pindex, sindex)\n                 extracted_data += nested_data\n             if following_regions:\n-                _, _, following_data = self._doextract(page, following_regions, sindex or start_index, end_region)\n+                _, _, following_data = self._doextract(page, following_regions, sindex or start_index, end_index)\n                 extracted_data += following_data\n         \n         elif following_regions:\n-            end_index, _, following_data = self._doextract(page, following_regions, start_index, end_region)\n+            end_index, _, following_data = self._doextract(page, following_regions, start_index, end_index)\n             if end_index is not None:\n                 pindex, sindex, extracted_data = self._doextract(page, [first_region], start_index, end_index - 1, nested_regions, ignored_regions)\n                 extracted_data += following_data\n         elif nested_regions:\n-            _, _, nested_data = self._doextract(page, nested_regions, start_index, end_region)\n+            _, _, nested_data = self._doextract(page, nested_regions, start_index, end_index)\n             extracted_data += nested_data\n         return pindex, sindex, extracted_data\n                 \n\n@@ -607,6 +607,32 @@ EXTRACT_PAGE19b = u\"\"\"\n </body></html>\n \"\"\"\n \n+ANNOTATED_PAGE20 = u\"\"\"\n+<html><body>\n+<h1>Product Name</h1>\n+<img src=\"product.jpg\">\n+<br/>\n+<span><ins data-scrapy-annotate=\"{&quot;variant&quot;: 1, &quot;generated&quot;: true,                                              \n+&quot;annotations&quot;: {&quot;content&quot;: &quot;name&quot;}}\">Twin</ins>:</span> $<ins data-scrapy-annotate=\"{&quot;variant&quot;: 1, &quot;generated&quot;: true,\n+&quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}\">270</ins> - November 2010<br/>\n+<span><ins data-scrapy-annotate=\"{&quot;variant&quot;: 2, &quot;generated&quot;: true,\n+&quot;annotations&quot;: {&quot;content&quot;: &quot;name&quot;}}\">Queen</ins>:</span> $<ins data-scrapy-annotate=\"{&quot;variant&quot;: 2, &quot;generated&quot;: true,\n+&quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}\">330</ins> - In stock<br/>\n+<br/>\n+</body></html>\n+\"\"\"\n+\n+EXTRACT_PAGE20 = u\"\"\"\n+<html><body>\n+<h1>Product Name</h1>\n+<img src=\"product.jpg\">\n+<br/>\n+<span>Twin:</span> $270 - November 2010<br/>\n+<span>Queen:</span> $330 - Movember 2010<br/>\n+<br/>\n+</body></html>\n+\"\"\"\n+\n SAMPLE_DESCRIPTOR1 = ItemDescriptor('test', 'product test', [\n     A('name', \"Product name\", required=True),\n     A('price', \"Product price, including any discounts and tax or vat\", \n@@ -781,6 +807,12 @@ TEST_DATA = [\n          SAMPLE_DESCRIPTOR1,\n          None,\n     ),\n+    ('repeated partial annotations with variants', [ANNOTATED_PAGE20], EXTRACT_PAGE20, None,\n+            {u'variants': [\n+                {'price': ['270'], 'name': ['Twin']},\n+                {'price': ['330'], 'name': ['Queen']},\n+            ]},\n+    ),\n ]\n \n class TestExtraction(TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
