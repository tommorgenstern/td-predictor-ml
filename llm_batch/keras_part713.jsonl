{"custom_id": "keras#6b53ab21cdbf155c4c3b27e71097d3b75b82a616", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 16 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,2 +1,2 @@\n # Unique source of truth for the version number.\n-__version__ = \"0.1.4\"\n+__version__ = \"0.1.5\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#59321fc09a8b32d931b5cb111c4644bbd515c75b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 221 | Lines Deleted: 53 | Files Changed: 25 | Hunks: 60 | Methods Changed: 39 | Complexity Δ (Sum/Max): 27/14 | Churn Δ: 274 | Churn Cumulative: 15414 | Contributors (this commit): 24 | Commits (past 90d): 284 | Contributors (cumulative): 132 | DMM Complexity: 0.6307692307692307\n\nDIFF:\n@@ -9,6 +9,7 @@ if backend() == \"torch\":\n from keras_core.backend.common.keras_tensor import KerasTensor\n from keras_core.backend.common.keras_tensor import any_symbolic_tensors\n from keras_core.backend.common.keras_tensor import is_keras_tensor\n+from keras_core.backend.common.name_scope import name_scope\n from keras_core.backend.common.stateless_scope import StatelessScope\n from keras_core.backend.common.stateless_scope import get_stateless_scope\n from keras_core.backend.common.stateless_scope import in_stateless_scope\n\n@@ -0,0 +1,57 @@\n+from keras_core.backend.common import global_state\n+\n+\n+class name_scope:\n+    \"\"\"Creates a sub-namespace for variable paths.\n+\n+    Args:\n+        name: Name of the current scope (string).\n+        caller: Optional ID of a caller object (e.g. class instance).\n+        deduplicate: If `True`, if `caller` was passed,\n+            and the previous caller matches the current caller,\n+            and the previous name matches the current name,\n+            do not reenter a new namespace.\n+    \"\"\"\n+\n+    def __init__(self, name, caller=None, deduplicate=True):\n+        if not isinstance(name, str) or \"/\" in name:\n+            raise ValueError(\n+                \"Argument `name` must be a string and \"\n+                \"cannot contain character `/`. \"\n+                f\"Received: name={name}\"\n+            )\n+        self.name = name\n+        self.caller = caller\n+        self.deduplicate = deduplicate\n+        self._pop_on_exit = False\n+\n+    def __enter__(self):\n+        name_scope_stack = global_state.get_global_attribute(\n+            \"name_scope_stack\", default=[], set_to_default=True\n+        )\n+        if self.deduplicate and name_scope_stack:\n+            parent_caller = name_scope_stack[-1].caller\n+            parent_name = name_scope_stack[-1].name\n+            if (\n+                self.caller is not None\n+                and self.caller is parent_caller\n+                and self.name == parent_name\n+            ):\n+                return self\n+        name_scope_stack.append(self)\n+        self._pop_on_exit = True\n+        return self\n+\n+    def __exit__(self, *args, **kwargs):\n+        if self._pop_on_exit:\n+            name_scope_stack = global_state.get_global_attribute(\n+                \"name_scope_stack\"\n+            )\n+            name_scope_stack.pop()\n+\n+\n+def current_path():\n+    name_scope_stack = global_state.get_global_attribute(\"name_scope_stack\")\n+    if name_scope_stack is None:\n+        return \"\"\n+    return \"/\".join(x.name for x in name_scope_stack)\n\n@@ -0,0 +1,36 @@\n+from keras_core import testing\n+from keras_core.backend.common.name_scope import current_path\n+from keras_core.backend.common.name_scope import name_scope\n+\n+\n+class NameScopeTest(testing.TestCase):\n+    def test_stacking(self):\n+        self.assertEqual(current_path(), \"\")\n+        with name_scope(\"outer\") as outer:\n+            self.assertEqual(outer.name, \"outer\")\n+            self.assertEqual(current_path(), \"outer\")\n+            with name_scope(\"middle\") as middle:\n+                self.assertEqual(middle.name, \"middle\")\n+                self.assertEqual(current_path(), \"outer/middle\")\n+                with name_scope(\"inner\") as inner:\n+                    self.assertEqual(inner.name, \"inner\")\n+                    self.assertEqual(current_path(), \"outer/middle/inner\")\n+                self.assertEqual(current_path(), \"outer/middle\")\n+            self.assertEqual(current_path(), \"outer\")\n+        self.assertEqual(current_path(), \"\")\n+\n+    def test_deduplication(self):\n+        self.assertEqual(current_path(), \"\")\n+        with name_scope(\"name\", caller=1):\n+            with name_scope(\"name\", caller=1):\n+                self.assertEqual(current_path(), \"name\")\n+        self.assertEqual(current_path(), \"\")\n+        with name_scope(\"name\"):\n+            with name_scope(\"name\"):\n+                self.assertEqual(current_path(), \"name/name\")\n+\n+    def test_errors(self):\n+        with self.assertRaisesRegex(ValueError, \"must be a string\"):\n+            name_scope(\"foo/bar\")\n+        with self.assertRaisesRegex(ValueError, \"must be a string\"):\n+            name_scope(4)\n\n@@ -2,6 +2,7 @@ import numpy as np\n \n from keras_core.backend import config\n from keras_core.backend.common import global_state\n+from keras_core.backend.common.name_scope import current_path\n from keras_core.backend.common.stateless_scope import get_stateless_scope\n from keras_core.backend.common.stateless_scope import in_stateless_scope\n from keras_core.utils.naming import auto_name\n@@ -19,6 +20,11 @@ class KerasVariable:\n                 f\"Received: name={name}\"\n             )\n         self.name = name\n+        parent_path = current_path()\n+        if parent_path:\n+            self.path = current_path() + \"/\" + self.name\n+        else:\n+            self.path = self.name\n         dtype = standardize_dtype(dtype)\n         self._dtype = dtype\n         self._shape = None\n@@ -68,7 +74,7 @@ class KerasVariable:\n \n     def _deferred_initialize(self):\n         if self._value is not None:\n-            raise ValueError(f\"Variable {self.name} is already initialized.\")\n+            raise ValueError(f\"Variable {self.path} is already initialized.\")\n \n         if in_stateless_scope():\n             raise ValueError(\n@@ -147,7 +153,7 @@ class KerasVariable:\n     def __repr__(self):\n         return (\n             f\"<KerasVariable shape={self.shape}, dtype={self.dtype}, \"\n-            f\"name={self.name}>\"\n+            f\"path={self.path}>\"\n         )\n \n     def _initialize(self, value):\n\n@@ -3,18 +3,27 @@ from keras_core.api_export import keras_core_export\n \n if backend.backend() == \"tensorflow\":\n     BackendVariable = backend.tensorflow.core.Variable\n+    backend_name_scope = backend.tensorflow.core.name_scope\n elif backend.backend() == \"jax\":\n     BackendVariable = backend.jax.core.Variable\n+    backend_name_scope = backend.common.name_scope.name_scope\n elif backend.backend() == \"torch\":\n     BackendVariable = backend.torch.core.Variable\n+    backend_name_scope = backend.common.name_scope.name_scope\n elif backend.backend() == \"numpy\":\n     from keras_core.backend.numpy.core import Variable as NumpyVariable\n \n     BackendVariable = NumpyVariable\n+    backend_name_scope = backend.common.name_scope.name_scope\n else:\n     raise RuntimeError(f\"Invalid backend: {backend.backend()}\")\n \n \n-@keras_core_export(\"keras_core.backend.Variable\")\n+@keras_core_export(\"keras_core.Variable\")\n class Variable(BackendVariable):\n     pass\n+\n+\n+@keras_core_export(\"keras_core.name_scope\")\n+class name_scope(backend_name_scope):\n+    pass\n\n@@ -12,7 +12,6 @@ from keras_core.backend.jax.core import cond\n from keras_core.backend.jax.core import convert_to_numpy\n from keras_core.backend.jax.core import convert_to_tensor\n from keras_core.backend.jax.core import is_tensor\n-from keras_core.backend.jax.core import name_scope\n from keras_core.backend.jax.core import scatter\n from keras_core.backend.jax.core import shape\n from keras_core.backend.jax.core import stop_gradient\n\n@@ -60,10 +60,6 @@ def cast(x, dtype):\n     return convert_to_tensor(x, dtype=dtype)\n \n \n-def name_scope(name):\n-    return jax.named_scope(name)\n-\n-\n # Shape / dtype inference util\n def compute_output_spec(fn, *args, **kwargs):\n     with StatelessScope():\n\n@@ -12,7 +12,6 @@ from keras_core.backend.numpy.core import cond\n from keras_core.backend.numpy.core import convert_to_numpy\n from keras_core.backend.numpy.core import convert_to_tensor\n from keras_core.backend.numpy.core import is_tensor\n-from keras_core.backend.numpy.core import name_scope\n from keras_core.backend.numpy.core import shape\n from keras_core.backend.numpy.core import vectorized_map\n from keras_core.backend.numpy.rnn import cudnn_ok\n\n@@ -1,5 +1,3 @@\n-from contextlib import nullcontext\n-\n import numpy as np\n from tensorflow import nest\n \n@@ -60,11 +58,6 @@ def cond(pred, true_fn, false_fn):\n     return false_fn()\n \n \n-def name_scope(name):\n-    # There is no need for a named context for NumPy.\n-    return nullcontext()\n-\n-\n def vectorized_map(function, elements):\n     if len(elements) == 1:\n         return function(elements)\n\n@@ -5,8 +5,10 @@ import tensorflow as tf\n from tensorflow.compiler.tf2xla.python.xla import dynamic_update_slice\n \n from keras_core.backend.common import KerasVariable\n+from keras_core.backend.common import global_state\n from keras_core.backend.common import standardize_dtype\n from keras_core.backend.common.keras_tensor import KerasTensor\n+from keras_core.backend.common.name_scope import name_scope as base_name_scope\n from keras_core.backend.common.stateless_scope import StatelessScope\n from keras_core.utils.naming import auto_name\n \n@@ -111,10 +113,6 @@ def cast(x, dtype):\n     return tf.cast(x, dtype=dtype)\n \n \n-def name_scope(name):\n-    return tf.name_scope(name)\n-\n-\n def compute_output_spec(fn, *args, **kwargs):\n     with StatelessScope():\n         graph_name = auto_name(\"scratch_graph\")\n@@ -203,3 +201,32 @@ def stop_gradient(variable):\n \n def unstack(x, num=None, axis=0):\n     return tf.unstack(x, num=num, axis=axis)\n+\n+\n+class name_scope(base_name_scope):\n+    def __init__(self, name, **kwargs):\n+        super().__init__(name, **kwargs)\n+        self._tf_name_scope = tf.name_scope(name)\n+\n+    def __enter__(self):\n+        name_scope_stack = global_state.get_global_attribute(\n+            \"name_scope_stack\", default=[], set_to_default=True\n+        )\n+        if self.deduplicate and name_scope_stack:\n+            parent_caller = name_scope_stack[-1].caller\n+            parent_name = name_scope_stack[-1].name\n+            if (\n+                self.caller is not None\n+                and self.caller is parent_caller\n+                and self.name == parent_name\n+            ):\n+                return self\n+        name_scope_stack.append(self)\n+        self._pop_on_exit = True\n+        self._tf_name_scope.__enter__()\n+        return self\n+\n+    def __exit__(self, *args, **kwargs):\n+        super().__exit__(*args, **kwargs)\n+        if self._pop_on_exit:\n+            self._tf_name_scope.__exit__(*args, **kwargs)\n\n@@ -0,0 +1,37 @@\n+import tensorflow as tf\n+\n+from keras_core.backend.tensorflow.core import name_scope\n+from keras_core.testing import TestCase\n+\n+\n+class TFNameScopeTest(TestCase):\n+    def test_stacking(self):\n+        self.assertEqual(tf.Variable(0, name=\"x\").name, \"x:0\")\n+        with name_scope(\"outer\") as outer:\n+            self.assertEqual(outer.name, \"outer\")\n+            self.assertEqual(tf.Variable(0, name=\"x\").name, \"outer/x:0\")\n+            with name_scope(\"middle\") as middle:\n+                self.assertEqual(middle.name, \"middle\")\n+                self.assertEqual(\n+                    tf.Variable(0, name=\"x\").name, \"outer/middle/x:0\"\n+                )\n+                with name_scope(\"inner\") as inner:\n+                    self.assertEqual(inner.name, \"inner\")\n+                    self.assertEqual(\n+                        tf.Variable(0, name=\"x\").name, \"outer/middle/inner/x:0\"\n+                    )\n+                self.assertEqual(\n+                    tf.Variable(0, name=\"x\").name, \"outer/middle/x:0\"\n+                )\n+            self.assertEqual(tf.Variable(0, name=\"x\").name, \"outer/x:0\")\n+        self.assertEqual(tf.Variable(0, name=\"x\").name, \"x:0\")\n+\n+    def test_deduplicate(self):\n+        self.assertEqual(tf.Variable(0, name=\"x\").name, \"x:0\")\n+        with name_scope(\"name\", caller=1):\n+            with name_scope(\"name\", caller=1):\n+                self.assertEqual(tf.Variable(0, name=\"x\").name, \"name/x:0\")\n+        self.assertEqual(tf.Variable(0, name=\"x\").name, \"x:0\")\n+        with name_scope(\"name\"):\n+            with name_scope(\"name\"):\n+                self.assertEqual(tf.Variable(0, name=\"x\").name, \"name/name/x:0\")\n\n@@ -28,7 +28,6 @@ from keras_core.backend.torch.core import cond\n from keras_core.backend.torch.core import convert_to_numpy\n from keras_core.backend.torch.core import convert_to_tensor\n from keras_core.backend.torch.core import is_tensor\n-from keras_core.backend.torch.core import name_scope\n from keras_core.backend.torch.core import scatter\n from keras_core.backend.torch.core import shape\n from keras_core.backend.torch.core import stop_gradient\n\n@@ -174,10 +174,6 @@ def cast(x, dtype):\n     return convert_to_tensor(x, dtype)\n \n \n-def name_scope(name):\n-    return contextlib.nullcontext()\n-\n-\n # Shape / dtype inference util\n def compute_output_spec(fn, *args, **kwargs):\n     def has_none_shape(x):\n\n@@ -439,6 +439,7 @@ class Layer(BackendLayer, Operation):\n         # TODO: handle layout\n         self._check_super_called()\n         initializer = initializers.get(initializer)\n+        with backend.name_scope(self.name, caller=self):\n             variable = backend.Variable(\n                 initializer=initializer,\n                 shape=shape,\n@@ -683,6 +684,7 @@ class Layer(BackendLayer, Operation):\n \n         ################\n         # 4. Call build.\n+        with backend.name_scope(self.name, caller=self):\n             self._maybe_build(call_spec)\n \n         ##########################\n@@ -737,7 +739,7 @@ class Layer(BackendLayer, Operation):\n         ####################\n         # 7. Call the layer.\n         try:\n-            with backend.name_scope(self.name):\n+            with backend.name_scope(self.name, caller=self):\n                 if self.autocast and self.compute_dtype != self.variable_dtype:\n                     # For mixed precision, we automatically cast layer variables\n                     # (float ones only) to the compute dtype upon access.\n@@ -1083,7 +1085,6 @@ class Layer(BackendLayer, Operation):\n             shapes_dict = get_shapes_dict(call_spec)\n             self._build_shapes_dict = shapes_dict\n \n-            with backend.name_scope(self.name):\n             if not utils.is_default(self.build):\n                 shapes_dict = update_shapes_dict_for_target_fn(\n                     self.build,\n\n@@ -288,9 +288,13 @@ class RNN(Layer):\n \n     @tracking.no_automatic_dependency_tracking\n     def _create_state_variables(self, batch_size):\n+        with backend.name_scope(self.name, caller=self):\n             self.states = tree.map_structure(\n                 lambda value: backend.Variable(\n-                value, trainable=False, dtype=self.variable_dtype\n+                    value,\n+                    trainable=False,\n+                    dtype=self.variable_dtype,\n+                    name=\"rnn_state\",\n                 ),\n                 self.get_initial_state(batch_size),\n             )\n\n@@ -166,6 +166,7 @@ class Metric:\n \n     def add_variable(self, shape, initializer, dtype=None, name=None):\n         self._check_super_called()\n+        with backend.name_scope(self.name, caller=self):\n             initializer = initializers.get(initializer)\n             variable = backend.Variable(\n                 initializer=initializer,\n\n@@ -542,9 +542,7 @@ class Model(Trainer, Layer):\n \n     def _get_variable_map(self):\n         store = {}\n-        map_trackable_variables(\n-            self, store=store, inner_path=\"\", visited_trackables=set()\n-        )\n+        map_trackable_variables(self, store=store, visited_trackables=set())\n         return store\n \n \n\n@@ -2,10 +2,9 @@ from keras_core.layers.layer import Layer\n from keras_core.metrics.metric import Metric\n from keras_core.optimizers.optimizer import Optimizer\n from keras_core.saving import saving_lib\n-from keras_core.utils import file_utils\n \n \n-def map_trackable_variables(trackable, store, inner_path, visited_trackables):\n+def map_trackable_variables(trackable, store, visited_trackables):\n     # If the trackable has already been saved, skip it.\n     if id(trackable) in visited_trackables:\n         return\n@@ -22,7 +21,15 @@ def map_trackable_variables(trackable, store, inner_path, visited_trackables):\n     elif isinstance(trackable, Metric):\n         variables = trackable._variables\n     for v in variables:\n-        store[inner_path + \"/\" + v.name] = v\n+        if v.path in store:\n+            raise ValueError(\n+                \"The model contains two variables with a duplicate path: \"\n+                f\"path='{v.path}' appears at least twice. \"\n+                f\"This path is used for {v} and for {store[v.path]}. \"\n+                \"In order to get a variable map, make sure to use \"\n+                \"unique paths/names for each variable.\"\n+            )\n+        store[v.path] = v\n \n     # Recursively save state of children trackables (layers, optimizers, etc.)\n     for child_attr, child_obj in saving_lib._walk_trackable(trackable):\n@@ -30,28 +37,24 @@ def map_trackable_variables(trackable, store, inner_path, visited_trackables):\n             map_trackable_variables(\n                 child_obj,\n                 store,\n-                inner_path=file_utils.join(inner_path, child_obj.name),\n                 visited_trackables=visited_trackables,\n             )\n         elif isinstance(child_obj, (list, dict, tuple, set)):\n             map_container_variables(\n                 child_obj,\n                 store,\n-                inner_path=file_utils.join(inner_path, child_attr),\n                 visited_trackables=visited_trackables,\n             )\n \n \n-def map_container_variables(container, store, inner_path, visited_trackables):\n+def map_container_variables(container, store, visited_trackables):\n     if isinstance(container, dict):\n         container = list(container.values())\n \n     for trackable in container:\n         if saving_lib._is_keras_trackable(trackable):\n-            name = trackable.name\n             map_trackable_variables(\n                 trackable,\n                 store,\n-                inner_path=file_utils.join(inner_path, name),\n                 visited_trackables=visited_trackables,\n             )\n\n@@ -96,6 +96,7 @@ class Adafactor(optimizer.Optimizer):\n             if len(var.shape) < 2:\n                 # Don't factor if variable is of dimension < 2, but we still\n                 # need to create dummy variables as placeholder.\n+                with backend.name_scope(self.name, caller=self):\n                     self._r.append(\n                         backend.Variable(0, name=var.name, trainable=False)\n                     )\n@@ -122,7 +123,7 @@ class Adafactor(optimizer.Optimizer):\n                 )\n             self._v.append(\n                 self.add_variable_from_reference(\n-                    reference_variable=var, name=\"v\"\n+                    reference_variable=var, name=\"velocity\"\n                 )\n             )\n \n\n@@ -91,12 +91,12 @@ class Adam(optimizer.Optimizer):\n         for var in var_list:\n             self._momentums.append(\n                 self.add_variable_from_reference(\n-                    reference_variable=var, name=\"m\"\n+                    reference_variable=var, name=\"momentum\"\n                 )\n             )\n             self._velocities.append(\n                 self.add_variable_from_reference(\n-                    reference_variable=var, name=\"v\"\n+                    reference_variable=var, name=\"velocity\"\n                 )\n             )\n         if self.amsgrad:\n@@ -104,7 +104,7 @@ class Adam(optimizer.Optimizer):\n             for var in var_list:\n                 self._velocity_hats.append(\n                     self.add_variable_from_reference(\n-                        reference_variable=var, name=\"vhat\"\n+                        reference_variable=var, name=\"velocity_hat\"\n                     )\n                 )\n \n\n@@ -99,12 +99,12 @@ class Adamax(optimizer.Optimizer):\n         for var in var_list:\n             self._m.append(\n                 self.add_variable_from_reference(\n-                    reference_variable=var, name=\"m\"\n+                    reference_variable=var, name=\"momentum\"\n                 )\n             )\n             self._u.append(\n                 self.add_variable_from_reference(\n-                    reference_variable=var, name=\"u\"\n+                    reference_variable=var, name=\"norm\"\n                 )\n             )\n \n\n@@ -33,6 +33,8 @@ class BaseOptimizer:\n         if kwargs:\n             raise ValueError(f\"Argument(s) not recognized: {kwargs}\")\n \n+        if name is None:\n+            name = auto_name(self.__class__.__name__)\n         self.name = name\n         self.weight_decay = weight_decay\n         self.clipnorm = clipnorm\n@@ -83,6 +85,7 @@ class BaseOptimizer:\n         # Create iteration variable\n         # Note: dtype=\"int\" will resolve to int32 in JAX\n         # (since int64 is disallowed in JAX) and to int64 in TF.\n+        with backend.name_scope(self.name, caller=self):\n             iterations = backend.Variable(\n                 0, name=\"iteration\", dtype=\"int\", trainable=False\n             )\n@@ -105,6 +108,7 @@ class BaseOptimizer:\n                     \"and returns the corresponding learning rate value). \"\n                     f\"Received instead: learning_rate={learning_rate}\"\n                 )\n+            with backend.name_scope(self.name, caller=self):\n                 learning_rate = backend.Variable(\n                     learning_rate,\n                     name=\"learning_rate\",\n@@ -154,6 +158,7 @@ class BaseOptimizer:\n     ):\n         self._check_super_called()\n         initializer = initializers.get(initializer)\n+        with backend.name_scope(self.name, caller=self):\n             variable = backend.Variable(\n                 initializer=initializer,\n                 shape=shape,\n@@ -169,12 +174,12 @@ class BaseOptimizer:\n         variable.\n         \"\"\"\n         initializer = initializers.Zeros()\n-        name = name or auto_name(self.__class__.__name__)\n+        name = name or \"var\"\n         return self.add_variable(\n             shape=reference_variable.shape,\n             initializer=initializer,\n             dtype=reference_variable.dtype,\n-            name=name,\n+            name=reference_variable.path.replace(\"/\", \"_\") + \"_\" + name,\n         )\n \n     def _check_variables_are_known(self, variables):\n@@ -226,12 +231,12 @@ class BaseOptimizer:\n             trainable_variables = list(trainable_variables)\n             # Optionally build optimizer.\n             if not self.built:\n-                with ops.name_scope(self.name):\n+                with backend.name_scope(self.name, caller=self):\n                     self.build(trainable_variables)\n                 self.built = True\n             self._check_variables_are_known(trainable_variables)\n \n-        with ops.name_scope(self.name):\n+        with backend.name_scope(self.name, caller=self):\n             # Filter empty gradients.\n             grads, trainable_variables = self._filter_empty_gradients(\n                 grads, trainable_variables\n\n@@ -91,7 +91,7 @@ class Lion(optimizer.Optimizer):\n         for var in var_list:\n             self._momentums.append(\n                 self.add_variable_from_reference(\n-                    reference_variable=var, name=\"m\"\n+                    reference_variable=var, name=\"momentum\"\n                 )\n             )\n \n\n@@ -90,12 +90,12 @@ class Nadam(optimizer.Optimizer):\n         for var in var_list:\n             self._momentums.append(\n                 self.add_variable_from_reference(\n-                    reference_variable=var, name=\"m\"\n+                    reference_variable=var, name=\"momentum\"\n                 )\n             )\n             self._velocities.append(\n                 self.add_variable_from_reference(\n-                    reference_variable=var, name=\"v\"\n+                    reference_variable=var, name=\"velocity\"\n                 )\n             )\n \n\n@@ -89,7 +89,7 @@ class SGD(optimizer.Optimizer):\n             for variable in variables:\n                 self.momentums.append(\n                     self.add_variable_from_reference(\n-                        reference_variable=variable, name=\"m\"\n+                        reference_variable=variable, name=\"momentum\"\n                     )\n                 )\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#35f5cf865908578ea3cfadae83b838cf65d2301b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 8 | Churn Cumulative: 4267 | Contributors (this commit): 14 | Commits (past 90d): 41 | Contributors (cumulative): 19 | DMM Complexity: 1.0\n\nDIFF:\n@@ -63,7 +63,7 @@ class TensorFlowTrainer(base_trainer.Trainer):\n \n         # Compute gradients\n         if self.trainable_weights:\n-            trainable_weights = [v.value for v in self.trainable_weights]\n+            trainable_weights = self.trainable_weights\n             gradients = tape.gradient(loss, trainable_weights)\n \n             # Update weights\n\n@@ -175,11 +175,15 @@ class BaseOptimizer:\n         \"\"\"\n         initializer = initializers.Zeros()\n         name = name or \"var\"\n+        if hasattr(reference_variable, \"path\"):\n+            name = reference_variable.path.replace(\"/\", \"_\") + \"_\" + name\n+        else:\n+            name = reference_variable.name + \"_\" + name\n         return self.add_variable(\n             shape=reference_variable.shape,\n             initializer=initializer,\n             dtype=reference_variable.dtype,\n-            name=reference_variable.path.replace(\"/\", \"_\") + \"_\" + name,\n+            name=name,\n         )\n \n     def _check_variables_are_known(self, variables):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#924bee148259638d328432566ef05bd1a2343834", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 214 | Files Changed: 7 | Hunks: 25 | Methods Changed: 22 | Complexity Δ (Sum/Max): -29/4 | Churn Δ: 242 | Churn Cumulative: 3869 | Contributors (this commit): 2 | Commits (past 90d): 49 | Contributors (cumulative): 12 | DMM Complexity: 0.375\n\nDIFF:\n@@ -1,9 +1,7 @@\n from keras_core import backend\n from keras_core.api_export import keras_core_export\n from keras_core.layers.layer import Layer\n-from keras_core.utils import argument_validation\n from keras_core.utils import backend_utils\n-from keras_core.utils import tf_utils\n from keras_core.utils.module_utils import tensorflow as tf\n \n \n@@ -85,21 +83,20 @@ class HashedCrossing(Layer):\n \n         if output_mode == \"int\" and dtype is None:\n             dtype = \"int64\"\n-\n         super().__init__(name=name, dtype=dtype)\n         if sparse and backend.backend() != \"tensorflow\":\n             raise ValueError(\n                 \"`sparse` can only be set to True with the \"\n                 \"TensorFlow backend.\"\n             )\n-\n-        argument_validation.validate_string_arg(\n-            output_mode,\n-            allowable_strings=(\"int\", \"one_hot\"),\n-            caller_name=self.__class__.__name__,\n-            arg_name=\"output_mode\",\n+        self.layer = tf.keras.layers.HashedCrossing(\n+            num_bins=num_bins,\n+            output_mode=output_mode,\n+            sparse=sparse,\n+            name=name,\n+            dtype=dtype,\n+            **kwargs,\n         )\n-\n         self.num_bins = num_bins\n         self.output_mode = output_mode\n         self.sparse = sparse\n@@ -136,45 +133,12 @@ class HashedCrossing(Layer):\n         return output_shape\n \n     def call(self, inputs):\n-        self._check_at_least_two_inputs(inputs)\n-        inputs = [tf_utils.ensure_tensor(x) for x in inputs]\n-        self._check_input_shape_and_type(inputs)\n-\n-        # Uprank to rank 2 for the cross_hashed op.\n-        rank = inputs[0].shape.rank\n-        if rank < 2:\n-            inputs = [tf_utils.expand_dims(x, -1) for x in inputs]\n-        if rank < 1:\n-            inputs = [tf_utils.expand_dims(x, -1) for x in inputs]\n-\n-        # Perform the cross and convert to dense\n-        outputs = tf.sparse.cross_hashed(inputs, self.num_bins)\n-        outputs = tf.sparse.to_dense(outputs)\n-\n-        # Fix output shape and downrank to match input rank.\n-        if rank == 2:\n-            # tf.sparse.cross_hashed output shape will always be None on the\n-            # last dimension. Given our input shape restrictions, we want to\n-            # force shape 1 instead.\n-            outputs = tf.reshape(outputs, [-1, 1])\n-        elif rank == 1:\n-            outputs = tf.reshape(outputs, [-1])\n-        elif rank == 0:\n-            outputs = tf.reshape(outputs, [])\n-\n-        # Encode outputs.\n-        outputs = tf_utils.encode_categorical_inputs(\n-            outputs,\n-            output_mode=self.output_mode,\n-            depth=self.num_bins,\n-            sparse=self.sparse,\n-            dtype=self.compute_dtype,\n-        )\n+        outputs = self.layer.call(inputs)\n         if (\n             backend.backend() != \"tensorflow\"\n             and not backend_utils.in_tf_graph()\n         ):\n-            outputs = backend.convert_to_tensor(outputs, dtype=self.dtype)\n+            outputs = backend.convert_to_tensor(outputs)\n         return outputs\n \n     def get_config(self):\n@@ -185,42 +149,3 @@ class HashedCrossing(Layer):\n             \"name\": self.name,\n             \"dtype\": self.dtype,\n         }\n-\n-    def _check_at_least_two_inputs(self, inputs):\n-        if not isinstance(inputs, (list, tuple)):\n-            raise ValueError(\n-                \"`HashedCrossing` should be called on a list or tuple of \"\n-                f\"inputs. Received: inputs={inputs}\"\n-            )\n-        if len(inputs) < 2:\n-            raise ValueError(\n-                \"`HashedCrossing` should be called on at least two inputs. \"\n-                f\"Received: inputs={inputs}\"\n-            )\n-\n-    def _check_input_shape_and_type(self, inputs):\n-        first_shape = inputs[0].shape.as_list()\n-        rank = len(first_shape)\n-        if rank > 2 or (rank == 2 and first_shape[-1] != 1):\n-            raise ValueError(\n-                \"All `HashedCrossing` inputs should have shape `()`, \"\n-                \"`(batch_size)` or `(batch_size, 1)`. \"\n-                f\"Received: inputs={inputs}\"\n-            )\n-        if not all(x.shape.as_list() == first_shape for x in inputs[1:]):\n-            raise ValueError(\n-                \"All `HashedCrossing` inputs should have equal shape. \"\n-                f\"Received: inputs={inputs}\"\n-            )\n-        if any(\n-            isinstance(x, (tf.RaggedTensor, tf.SparseTensor)) for x in inputs\n-        ):\n-            raise ValueError(\n-                \"All `HashedCrossing` inputs should be dense tensors. \"\n-                f\"Received: inputs={inputs}\"\n-            )\n-        if not all(x.dtype.is_integer or x.dtype == tf.string for x in inputs):\n-            raise ValueError(\n-                \"All `HashedCrossing` inputs should have an integer or \"\n-                f\"string dtype. Received: inputs={inputs}\"\n-            )\n\n@@ -1,8 +1,6 @@\n import numpy as np\n-import pytest\n import tensorflow as tf\n \n-from keras_core import backend\n from keras_core import layers\n from keras_core import testing\n \n@@ -42,7 +40,7 @@ class HashedCrossingTest(testing.TestCase):\n         feat1 = np.array([\"A\", \"B\", \"A\", \"B\", \"A\"])\n         feat2 = np.array([101, 101, 101, 102, 102])\n         output = layer((feat1, feat2))\n-        self.assertAllClose(tf.constant([1, 4, 1, 1, 3]), output)\n+        self.assertAllClose(np.array([1, 4, 1, 1, 3]), output)\n \n         layer = layers.HashedCrossing(num_bins=5, output_mode=\"one_hot\")\n         feat1 = np.array([\"A\", \"B\", \"A\", \"B\", \"A\"])\n@@ -73,110 +71,3 @@ class HashedCrossingTest(testing.TestCase):\n         for output in ds.take(1):\n             output = output.numpy()\n         self.assertAllClose(np.array([1, 4, 1, 1, 3]), output)\n-\n-    def test_upsupported_shape_input_fails(self):\n-        with self.assertRaisesRegex(ValueError, \"inputs should have shape\"):\n-            layers.HashedCrossing(num_bins=10)(\n-                (np.array([[[1.0]]]), np.array([[[1.0]]]))\n-            )\n-\n-    @pytest.mark.xfail\n-    def test_cross_output_dtype(self):\n-        input_1, input_2 = np.array([1]), np.array([1])\n-\n-        layer = layers.HashedCrossing(num_bins=2)\n-        output_dtype = backend.standardize_dtype(\n-            layer((input_1, input_2)).dtype\n-        )\n-        self.assertEqual(output_dtype, \"int64\")\n-        layer = layers.HashedCrossing(num_bins=2, dtype=\"int32\")\n-        output_dtype = backend.standardize_dtype(\n-            layer((input_1, input_2)).dtype\n-        )\n-        self.assertEqual(output_dtype, \"int32\")\n-        layer = layers.HashedCrossing(num_bins=2, output_mode=\"one_hot\")\n-        output_dtype = backend.standardize_dtype(\n-            layer((input_1, input_2)).dtype\n-        )\n-        self.assertEqual(output_dtype, \"float32\")\n-        layer = layers.HashedCrossing(\n-            num_bins=2, output_mode=\"one_hot\", dtype=\"float64\"\n-        )\n-        output_dtype = backend.standardize_dtype(\n-            layer((input_1, input_2)).dtype\n-        )\n-        self.assertEqual(output_dtype, \"float64\")\n-\n-    def test_non_list_input_fails(self):\n-        with self.assertRaisesRegex(ValueError, \"should be called on a list\"):\n-            layers.HashedCrossing(num_bins=10)(np.array(1))\n-\n-    def test_single_input_fails(self):\n-        with self.assertRaisesRegex(ValueError, \"at least two inputs\"):\n-            layers.HashedCrossing(num_bins=10)([np.array(1)])\n-\n-    @pytest.mark.skipif(\n-        backend.backend() != \"tensorflow\",\n-        reason=\"Sparse tensor are only applicable with tensorflow backend\",\n-    )\n-    def test_sparse_input_fails(self):\n-        with self.assertRaisesRegex(\n-            ValueError, \"inputs should be dense tensors\"\n-        ):\n-            sparse_in = tf.sparse.from_dense(np.array([1]))\n-            layers.HashedCrossing(num_bins=10)((sparse_in, sparse_in))\n-\n-    def test_float_input_fails(self):\n-        with self.assertRaisesRegex(\n-            ValueError, \"should have an integer or string\"\n-        ):\n-            layers.HashedCrossing(num_bins=10)(\n-                (np.array([1.0]), np.array([1.0]))\n-            )\n-\n-    @pytest.mark.skipif(\n-        backend.backend() != \"tensorflow\",\n-        reason=\"String tensors are only applicable with tensorflow backend\",\n-    )\n-    def test_tf_string(self):\n-        layer = layers.HashedCrossing(num_bins=10)\n-        feat1 = tf.constant(\"A\")\n-        feat2 = tf.constant(101)\n-        outputs = layer((feat1, feat2))\n-        self.assertAllClose(outputs, 1)\n-\n-        layer = tf.keras.layers.HashedCrossing(\n-            num_bins=5, output_mode=\"one_hot\"\n-        )\n-        feat1 = tf.constant([\"A\", \"B\", \"A\", \"B\", \"A\"])\n-        feat2 = tf.constant([101, 101, 101, 102, 102])\n-        self.assertAllClose(\n-            tf.constant(\n-                [\n-                    [0.0, 1.0, 0.0, 0.0, 0.0],\n-                    [0.0, 0.0, 0.0, 0.0, 1.0],\n-                    [0.0, 1.0, 0.0, 0.0, 0.0],\n-                    [0.0, 1.0, 0.0, 0.0, 0.0],\n-                    [0.0, 0.0, 0.0, 1.0, 0.0],\n-                ]\n-            ),\n-            layer((feat1, feat2)),\n-        )\n-\n-        layer = tf.keras.layers.HashedCrossing(num_bins=5)\n-        feat1 = tf.constant([\"A\", \"B\", \"A\", \"B\", \"A\"])\n-        feat2 = tf.constant([101, 101, 101, 102, 102])\n-        self.assertAllClose(tf.constant([1, 4, 1, 1, 3]), layer((feat1, feat2)))\n-\n-        layer = layers.HashedCrossing(\n-            num_bins=5, output_mode=\"one_hot\", sparse=True\n-        )\n-        cloned_layer = layers.HashedCrossing.from_config(layer.get_config())\n-        feat1 = tf.constant([[\"A\"], [\"B\"], [\"A\"], [\"B\"], [\"A\"]])\n-        feat2 = tf.constant([[101], [101], [101], [102], [102]])\n-        original_outputs = layer((feat1, feat2))\n-        cloned_outputs = cloned_layer((feat1, feat2))\n-        self.assertAllClose(\n-            tf.sparse.to_dense(cloned_outputs),\n-            tf.sparse.to_dense(original_outputs),\n-        )\n\n@@ -572,7 +572,7 @@ class IndexLookup(Layer):\n             for batch in data:\n                 self.update_state(batch)\n         else:\n-            data = tf_utils.ensure_tensor(data, dtype=self.vocabulary_dtype)\n+            data = ensure_tensor(data, dtype=self.vocabulary_dtype)\n             if data.shape.rank == 1:\n                 # A plain list of strings\n                 # is treated as as many documents\n@@ -588,7 +588,7 @@ class IndexLookup(Layer):\n                 \"`set_vocabulary()` method.\"\n             )\n \n-        data = tf_utils.ensure_tensor(data, dtype=self.vocabulary_dtype)\n+        data = ensure_tensor(data, dtype=self.vocabulary_dtype)\n         if data.shape.rank == 0:\n             data = tf.expand_dims(data, 0)\n         if data.shape.rank == 1:\n@@ -700,7 +700,7 @@ class IndexLookup(Layer):\n     def call(self, inputs):\n         self._ensure_known_vocab_size()\n \n-        inputs = tf_utils.ensure_tensor(inputs, dtype=self._key_dtype)\n+        inputs = ensure_tensor(inputs, dtype=self._key_dtype)\n         original_shape = inputs.shape\n         # Some ops will not handle scalar input, so uprank to rank 1.\n         if inputs.shape.rank == 0:\n@@ -990,3 +990,12 @@ def listify_tensors(x):\n     if isinstance(x, np.ndarray):\n         x = x.tolist()\n     return x\n+\n+\n+def ensure_tensor(inputs, dtype=None):\n+    \"\"\"Ensures the input is a Tensor, SparseTensor or RaggedTensor.\"\"\"\n+    if not isinstance(inputs, (tf.Tensor, tf.RaggedTensor, tf.SparseTensor)):\n+        inputs = tf.convert_to_tensor(inputs, dtype)\n+    if dtype is not None and inputs.dtype != dtype:\n+        inputs = tf.cast(inputs, dtype)\n+    return inputs\n\n@@ -401,7 +401,7 @@ class IntegerLookup(IndexLookup):\n         if not isinstance(\n             inputs, (tf.Tensor, tf.RaggedTensor, np.ndarray, list, tuple)\n         ):\n-            inputs = tf.convert_to_tensor(np.array(inputs))\n+            inputs = tf.convert_to_tensor(backend.convert_to_numpy(inputs))\n         outputs = super().call(inputs)\n         if (\n             backend.backend() != \"tensorflow\"\n\n@@ -388,7 +388,7 @@ class StringLookup(IndexLookup):\n         else:\n             tf_inputs = False\n             if not isinstance(inputs, (np.ndarray, list, tuple)):\n-                inputs = tf.convert_to_tensor(np.array(inputs))\n+                inputs = tf.convert_to_tensor(backend.convert_to_numpy(inputs))\n         outputs = super().call(inputs)\n         if (\n             not tf_inputs\n\n@@ -3,12 +3,12 @@ import numpy as np\n from keras_core import backend\n from keras_core.api_export import keras_core_export\n from keras_core.layers.layer import Layer\n+from keras_core.layers.preprocessing.index_lookup import ensure_tensor\n from keras_core.layers.preprocessing.index_lookup import listify_tensors\n from keras_core.layers.preprocessing.string_lookup import StringLookup\n from keras_core.saving import serialization_lib\n from keras_core.utils import argument_validation\n from keras_core.utils import backend_utils\n-from keras_core.utils import tf_utils\n from keras_core.utils.module_utils import tensorflow as tf\n \n \n@@ -424,7 +424,7 @@ class TextVectorization(Layer):\n             for batch in data:\n                 self.update_state(batch)\n         else:\n-            data = tf_utils.ensure_tensor(data, dtype=\"string\")\n+            data = ensure_tensor(data, dtype=\"string\")\n             if data.shape.rank == 1:\n                 # A plain list of strings\n                 # is treated as as many documents\n@@ -519,7 +519,7 @@ class TextVectorization(Layer):\n         self._lookup_layer.set_vocabulary(vocabulary, idf_weights=idf_weights)\n \n     def _preprocess(self, inputs):\n-        inputs = tf_utils.ensure_tensor(inputs, dtype=tf.string)\n+        inputs = ensure_tensor(inputs, dtype=tf.string)\n         if self._standardize in (\"lower\", \"lower_and_strip_punctuation\"):\n             inputs = tf.strings.lower(inputs)\n         if self._standardize in (\n@@ -569,7 +569,7 @@ class TextVectorization(Layer):\n         if not isinstance(\n             inputs, (tf.Tensor, tf.RaggedTensor, np.ndarray, list, tuple)\n         ):\n-            inputs = tf.convert_to_tensor(np.array(inputs))\n+            inputs = tf.convert_to_tensor(backend.convert_to_numpy(inputs))\n \n         inputs = self._preprocess(inputs)\n \n\n@@ -1,5 +1,3 @@\n-import numpy as np\n-\n from keras_core.utils.module_utils import tensorflow as tf\n \n \n@@ -137,12 +135,3 @@ def get_tensor_spec(t, dynamic_batch=False, name=None):\n     shape = tf.TensorShape(shape_list)\n     spec._shape = shape\n     return spec\n-\n-\n-def ensure_tensor(inputs, dtype=None):\n-    \"\"\"Ensures the input is a Tensor, SparseTensor or RaggedTensor.\"\"\"\n-    if isinstance(inputs, (list, np.ndarray)):\n-        inputs = tf.convert_to_tensor(inputs, dtype)\n-    if dtype is not None and inputs.dtype != dtype:\n-        inputs = tf.cast(inputs, dtype)\n-    return inputs\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b9b43d22e30643234a5400eb1e6127ffe81e502f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 410 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -117,7 +117,7 @@ class HashedCrossingTest(testing.TestCase):\n \n     @pytest.mark.skipif(\n         backend.backend() != \"tensorflow\",\n-        reason=\"Sparse tensor are only applicable with tensorflow backend\",\n+        reason=\"Need sparse tensor support.\",\n     )\n     def test_sparse_input_fails(self):\n         with self.assertRaisesRegex(\n@@ -136,7 +136,7 @@ class HashedCrossingTest(testing.TestCase):\n \n     @pytest.mark.skipif(\n         backend.backend() != \"tensorflow\",\n-        reason=\"String tensors are only applicable with tensorflow backend\",\n+        reason=\"Need string tensor support.\",\n     )\n     def test_tf_string(self):\n         layer = layers.HashedCrossing(num_bins=10)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fb8ad4decd1c397b87387f9a70b14470d2cfaa34", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 33 | Churn Cumulative: 33 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,33 @@\n+import numpy as np\n+\n+from keras_core import testing\n+from keras_core.saving import saving_lib_test\n+\n+\n+class VariableMappingTest(testing.TestCase):\n+    def test_basics(self):\n+        model = saving_lib_test._get_basic_functional_model()\n+        model.optimizer.build(model.trainable_variables)\n+        variable_map = model._get_variable_map()\n+\n+        self.assertIn(\"first_dense/kernel\", variable_map)\n+        self.assertIn(\"second_dense/bias\", variable_map)\n+        self.assertIn(\"adam/learning_rate\", variable_map)\n+\n+        model = saving_lib_test._get_basic_sequential_model()\n+        model.build((None, 1))\n+        model.optimizer.build(model.trainable_variables)\n+        variable_map = model._get_variable_map()\n+        self.assertIn(\"dense_1/bias\", variable_map)\n+        self.assertIn(\"adam/learning_rate\", variable_map)\n+\n+        model = saving_lib_test._get_subclassed_model()\n+        model(np.ones((1, 1)))\n+        model.optimizer.build(model.trainable_variables)\n+        variable_map = model._get_variable_map()\n+        self.assertIn(\"custom_model_x/my_dense_1/dense/kernel\", variable_map)\n+        self.assertIn(\"custom_model_x/my_dense_1/my_dict_weight\", variable_map)\n+        self.assertIn(\n+            \"custom_model_x/my_dense_1/my_additional_weight\", variable_map\n+        )\n+        self.assertIn(\"adam/learning_rate\", variable_map)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8e99fcf49edfe3d13660fae0e5d5de6e497e5ea5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 10 | Churn Cumulative: 21 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,11 +4,17 @@ from keras_core.backend.common import global_state\n \n @keras_core_export(\"keras_core.StatelessScope\")\n class StatelessScope:\n-    def __init__(self, state_mapping=None, collect_losses=False):\n+    def __init__(\n+        self,\n+        state_mapping=None,\n+        collect_losses=False,\n+        initialize_variables=True,\n+    ):\n         from keras_core import backend\n         from keras_core.backend.common.variables import KerasVariable\n \n         self.collect_losses = collect_losses\n+        self.initialize_variables = initialize_variables\n         self.losses = []\n         self.state_mapping = {}\n         state_mapping = state_mapping or {}\n@@ -49,7 +55,7 @@ class StatelessScope:\n         global_state.set_global_attribute(\n             \"stateless_scope\", self.original_scope\n         )\n-        if self.original_scope is None:\n+        if self.original_scope is None and self.initialize_variables:\n             # We're back in eager scope;\n             # if any variables were created within the stateless\n             # scope, we initialize them here.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#43b6447475622c78ec777d8cf39f63815720cc31", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 16 | Churn Cumulative: 867 | Contributors (this commit): 10 | Commits (past 90d): 53 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,5 @@\n import contextlib\n+import os\n \n import numpy as np\n import torch\n@@ -12,7 +13,18 @@ from keras_core.backend.common.stateless_scope import StatelessScope\n from keras_core.utils.nest import pack_sequence_as\n \n DYNAMIC_SHAPES_OK = True\n-DEFAULT_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+# Some operators such as 'aten::_foreach_mul_.Scalar'\n+# are not currently implemented for the MPS device.\n+# check https://github.com/pytorch/pytorch/issues/77764.\n+if (\n+    torch.backends.mps.is_available()\n+    and os.getenv(\"PYTORCH_ENABLE_MPS_FALLBACK\") == \"1\"\n+):\n+    DEFAULT_DEVICE = \"mps\"\n+elif torch.cuda.is_available():\n+    DEFAULT_DEVICE = \"cuda\"\n+else:\n+    DEFAULT_DEVICE = \"cpu\"\n \n TORCH_DTYPES = {\n     \"float16\": torch.float16,\n@@ -145,7 +157,7 @@ def convert_to_numpy(x):\n             if x.requires_grad:\n                 x = x.detach()\n             # Tensor has to be moved to CPU before converting to numpy.\n-            if x.is_cuda:\n+            if x.is_cuda or x.is_mps:\n                 x = x.cpu()\n         return np.array(x)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
