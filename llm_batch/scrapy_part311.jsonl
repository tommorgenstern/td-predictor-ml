{"custom_id": "scrapy#cf50561b8696b645940ae33ba9b168ef97580477", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 75 | Lines Deleted: 22 | Files Changed: 9 | Hunks: 23 | Methods Changed: 19 | Complexity Δ (Sum/Max): 6/2 | Churn Δ: 97 | Churn Cumulative: 9097 | Contributors (this commit): 57 | Commits (past 90d): 48 | Contributors (cumulative): 142 | DMM Complexity: 1.0\n\nDIFF:\n@@ -135,7 +135,7 @@ DEPRECATION_RULES = [\n def update_classpath(path):\n     \"\"\"Update a deprecated path from an object with its new location\"\"\"\n     for prefix, replacement in DEPRECATION_RULES:\n-        if path.startswith(prefix):\n+        if isinstance(path, str) and path.startswith(prefix):\n             new_path = path.replace(prefix, replacement, 1)\n             warnings.warn(\"`{}` class is deprecated, use `{}` instead\".format(path, new_path),\n                           ScrapyDeprecationWarning)\n\n@@ -39,10 +39,20 @@ def arg_to_iter(arg):\n def load_object(path):\n     \"\"\"Load an object given its absolute object path, and return it.\n \n-    object can be the import path of a class, function, variable or an\n-    instance, e.g. 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'\n+    The object can be the import path of a class, function, variable or an\n+    instance, e.g. 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'.\n+\n+    If ``path`` is not a string, but is a callable object, such as a class or\n+    a function, then return it as is.\n     \"\"\"\n \n+    if not isinstance(path, str):\n+        if callable(path):\n+            return path\n+        else:\n+            raise TypeError(\"Unexpected argument type, expected string \"\n+                            \"or object, got: %s\" % type(path))\n+\n     try:\n         dot = path.rindex('.')\n     except ValueError:\n\n@@ -142,7 +142,7 @@ class CrawlerRunnerTestCase(BaseCrawlerTest):\n \n     def test_spider_manager_verify_interface(self):\n         settings = Settings({\n-            'SPIDER_LOADER_CLASS': 'tests.test_crawler.SpiderLoaderWithWrongInterface'\n+            'SPIDER_LOADER_CLASS': SpiderLoaderWithWrongInterface,\n         })\n         with warnings.catch_warnings(record=True) as w:\n             self.assertRaises(AttributeError, CrawlerRunner, settings)\n\n@@ -61,7 +61,7 @@ class OffDH:\n class LoadTestCase(unittest.TestCase):\n \n     def test_enabled_handler(self):\n-        handlers = {'scheme': 'tests.test_downloader_handlers.DummyDH'}\n+        handlers = {'scheme': DummyDH}\n         crawler = get_crawler(settings_dict={'DOWNLOAD_HANDLERS': handlers})\n         dh = DownloadHandlers(crawler)\n         self.assertIn('scheme', dh._schemes)\n@@ -69,7 +69,7 @@ class LoadTestCase(unittest.TestCase):\n         self.assertNotIn('scheme', dh._notconfigured)\n \n     def test_not_configured_handler(self):\n-        handlers = {'scheme': 'tests.test_downloader_handlers.OffDH'}\n+        handlers = {'scheme': OffDH}\n         crawler = get_crawler(settings_dict={'DOWNLOAD_HANDLERS': handlers})\n         dh = DownloadHandlers(crawler)\n         self.assertIn('scheme', dh._schemes)\n@@ -87,7 +87,7 @@ class LoadTestCase(unittest.TestCase):\n         self.assertIn('scheme', dh._notconfigured)\n \n     def test_lazy_handlers(self):\n-        handlers = {'scheme': 'tests.test_downloader_handlers.DummyLazyDH'}\n+        handlers = {'scheme': DummyLazyDH}\n         crawler = get_crawler(settings_dict={'DOWNLOAD_HANDLERS': handlers})\n         dh = DownloadHandlers(crawler)\n         self.assertIn('scheme', dh._schemes)\n\n@@ -845,7 +845,7 @@ class FeedExportTest(FeedExportTestBase):\n                 self._random_temp_filename(): {'format': 'xml'},\n                 self._random_temp_filename(): {'format': 'csv'},\n             },\n-            'FEED_STORAGES': {'file': 'tests.test_feedexport.LogOnStoreFileStorage'},\n+            'FEED_STORAGES': {'file': LogOnStoreFileStorage},\n             'FEED_STORE_EMPTY': False\n         }\n \n@@ -1189,8 +1189,8 @@ class FeedExportTest(FeedExportTestBase):\n     @defer.inlineCallbacks\n     def test_init_exporters_storages_with_crawler(self):\n         settings = {\n-            'FEED_EXPORTERS': {'csv': 'tests.test_feedexport.FromCrawlerCsvItemExporter'},\n-            'FEED_STORAGES': {'file': 'tests.test_feedexport.FromCrawlerFileFeedStorage'},\n+            'FEED_EXPORTERS': {'csv': FromCrawlerCsvItemExporter},\n+            'FEED_STORAGES': {'file': FromCrawlerFileFeedStorage},\n             'FEEDS': {\n                 self._random_temp_filename(): {'format': 'csv'},\n             },\n@@ -1219,7 +1219,7 @@ class FeedExportTest(FeedExportTestBase):\n                 self._random_temp_filename(): {'format': 'xml'},\n                 self._random_temp_filename(): {'format': 'csv'},\n             },\n-            'FEED_STORAGES': {'file': 'tests.test_feedexport.DummyBlockingFeedStorage'},\n+            'FEED_STORAGES': {'file': DummyBlockingFeedStorage},\n         }\n         items = [\n             {'foo': 'bar1', 'baz': ''},\n@@ -1240,7 +1240,7 @@ class FeedExportTest(FeedExportTestBase):\n                 self._random_temp_filename(): {'format': 'xml'},\n                 self._random_temp_filename(): {'format': 'csv'},\n             },\n-            'FEED_STORAGES': {'file': 'tests.test_feedexport.FailingBlockingFeedStorage'},\n+            'FEED_STORAGES': {'file': FailingBlockingFeedStorage},\n         }\n         items = [\n             {'foo': 'bar1', 'baz': ''},\n@@ -1667,7 +1667,7 @@ class StdoutFeedStoragePreFeedOptionsTest(unittest.TestCase):\n         settings_dict = {\n             'FEED_URI': 'file:///tmp/foobar',\n             'FEED_STORAGES': {\n-                'file': 'tests.test_feedexport.StdoutFeedStorageWithoutFeedOptions'\n+                'file': StdoutFeedStorageWithoutFeedOptions\n             },\n         }\n         crawler = get_crawler(settings_dict=settings_dict)\n@@ -1708,7 +1708,7 @@ class FileFeedStoragePreFeedOptionsTest(unittest.TestCase):\n         settings_dict = {\n             'FEED_URI': 'file:///tmp/foobar',\n             'FEED_STORAGES': {\n-                'file': 'tests.test_feedexport.FileFeedStorageWithoutFeedOptions'\n+                'file': FileFeedStorageWithoutFeedOptions\n             },\n         }\n         crawler = get_crawler(settings_dict=settings_dict)\n@@ -1756,7 +1756,7 @@ class S3FeedStoragePreFeedOptionsTest(unittest.TestCase):\n         settings_dict = {\n             'FEED_URI': 'file:///tmp/foobar',\n             'FEED_STORAGES': {\n-                'file': 'tests.test_feedexport.S3FeedStorageWithoutFeedOptions'\n+                'file': S3FeedStorageWithoutFeedOptions\n             },\n         }\n         crawler = get_crawler(settings_dict=settings_dict)\n@@ -1784,7 +1784,7 @@ class S3FeedStoragePreFeedOptionsTest(unittest.TestCase):\n         settings_dict = {\n             'FEED_URI': 'file:///tmp/foobar',\n             'FEED_STORAGES': {\n-                'file': 'tests.test_feedexport.S3FeedStorageWithoutFeedOptionsWithFromCrawler'\n+                'file': S3FeedStorageWithoutFeedOptionsWithFromCrawler\n             },\n         }\n         crawler = get_crawler(settings_dict=settings_dict)\n@@ -1833,7 +1833,7 @@ class FTPFeedStoragePreFeedOptionsTest(unittest.TestCase):\n         settings_dict = {\n             'FEED_URI': 'file:///tmp/foobar',\n             'FEED_STORAGES': {\n-                'file': 'tests.test_feedexport.FTPFeedStorageWithoutFeedOptions'\n+                'file': FTPFeedStorageWithoutFeedOptions\n             },\n         }\n         crawler = get_crawler(settings_dict=settings_dict)\n@@ -1861,7 +1861,7 @@ class FTPFeedStoragePreFeedOptionsTest(unittest.TestCase):\n         settings_dict = {\n             'FEED_URI': 'file:///tmp/foobar',\n             'FEED_STORAGES': {\n-                'file': 'tests.test_feedexport.FTPFeedStorageWithoutFeedOptionsWithFromCrawler'\n+                'file': FTPFeedStorageWithoutFeedOptionsWithFromCrawler\n             },\n         }\n         crawler = get_crawler(settings_dict=settings_dict)\n\n@@ -50,7 +50,7 @@ class TestMiddlewareManager(MiddlewareManager):\n \n     @classmethod\n     def _get_mwlist_from_settings(cls, settings):\n-        return ['tests.test_middleware.%s' % x for x in ['M1', 'MOff', 'M3']]\n+        return [M1, MOff, M3]\n \n     def _add_middleware(self, mw):\n         super()._add_middleware(mw)\n\n@@ -385,6 +385,38 @@ class SettingsTest(unittest.TestCase):\n         self.assertIn('key', mydict)\n         self.assertEqual(mydict['key'], 'val')\n \n+    def test_passing_objects_as_values(self):\n+        from scrapy.core.downloader.handlers.file import FileDownloadHandler\n+        from scrapy.utils.misc import create_instance\n+        from scrapy.utils.test import get_crawler\n+\n+        class TestPipeline():\n+            def process_item(self, i, s):\n+                return i\n+\n+        settings = Settings({\n+            'ITEM_PIPELINES': {\n+                TestPipeline: 800,\n+            },\n+            'DOWNLOAD_HANDLERS': {\n+                'ftp': FileDownloadHandler,\n+            },\n+        })\n+\n+        self.assertIn('ITEM_PIPELINES', settings.attributes)\n+\n+        mypipeline, priority = settings.getdict('ITEM_PIPELINES').popitem()\n+        self.assertEqual(priority, 800)\n+        self.assertEqual(mypipeline, TestPipeline)\n+        self.assertIsInstance(mypipeline(), TestPipeline)\n+        self.assertEqual(mypipeline().process_item('item', None), 'item')\n+\n+        myhandler = settings.getdict('DOWNLOAD_HANDLERS').pop('ftp')\n+        self.assertEqual(myhandler, FileDownloadHandler)\n+        myhandler_instance = create_instance(myhandler, None, get_crawler())\n+        self.assertIsInstance(myhandler_instance, FileDownloadHandler)\n+        self.assertTrue(hasattr(myhandler_instance, 'download_request'))\n+\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -385,7 +385,7 @@ class CustomPythonOrgPolicy(ReferrerPolicy):\n \n \n class TestSettingsCustomPolicy(TestRefererMiddleware):\n-    settings = {'REFERRER_POLICY': 'tests.test_spidermiddleware_referer.CustomPythonOrgPolicy'}\n+    settings = {'REFERRER_POLICY': CustomPythonOrgPolicy}\n     scenarii = [\n         ('https://example.com/', 'https://scrapy.org/', b'https://python.org/'),\n         ('http://example.com/', 'http://scrapy.org/', b'http://python.org/'),\n\n@@ -12,11 +12,22 @@ __doctests__ = ['scrapy.utils.misc']\n \n class UtilsMiscTestCase(unittest.TestCase):\n \n-    def test_load_object(self):\n+    def test_load_object_class(self):\n+        obj = load_object(Field)\n+        self.assertIs(obj, Field)\n+        obj = load_object('scrapy.item.Field')\n+        self.assertIs(obj, Field)\n+\n+    def test_load_object_function(self):\n+        obj = load_object(load_object)\n+        self.assertIs(obj, load_object)\n         obj = load_object('scrapy.utils.misc.load_object')\n-        assert obj is load_object\n+        self.assertIs(obj, load_object)\n+\n+    def test_load_object_exceptions(self):\n         self.assertRaises(ImportError, load_object, 'nomodule999.mod.function')\n         self.assertRaises(NameError, load_object, 'scrapy.utils.misc.load_object999')\n+        self.assertRaises(TypeError, load_object, dict())\n \n     def test_walk_modules(self):\n         mods = walk_modules('tests.test_utils_misc.test_walk_modules')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c77450990d9f78e68380d724b4a8c15e7b99c995", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 151 | Contributors (this commit): 9 | Commits (past 90d): 2 | Contributors (cumulative): 9 | DMM Complexity: 0.0\n\nDIFF:\n@@ -23,8 +23,7 @@ class Command(ScrapyCommand):\n         if opts.verbose:\n             versions = scrapy_components_versions()\n             width = max(len(n) for (n, _) in versions)\n-            patt = f\"%-{width}s : %s\"\n             for name, version in versions:\n-                print(patt % (name, version))\n+                print(f\"{name:<{width}} : {version}\")\n         else:\n             print(f\"Scrapy {scrapy.__version__}\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#92dfa7176dbadcb92e4aae9d3f2c2b02c6e52a4a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 14 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -28,7 +28,7 @@ class StatsMailer:\n     def spider_closed(self, spider):\n         spider_stats = self.stats.get_stats(spider)\n         body = \"Global stats\\n\\n\"\n-        body += \"\\n\".join(f\"{i:<50} : {self.stats.get_stats()[i]}\" for i in self.stats.get_stats())\n+        body += \"\\n\".join(f\"{k:<50} : {v}\" for k, v in self.stats.get_stats().items())\n         body += f\"\\n\\n{spider.name} stats\\n\\n\"\n-        body += \"\\n\".join(f\"{i:<50} : {spider_stats[i]}\" for i in spider_stats)\n+        body += \"\\n\".join(f\"{k:<50} : {v}\" for k, v in spider_stats.items())\n         return self.mail.send(self.recipients, f\"Scrapy stats for: {spider.name}\", body)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ea03e4254fbaff71ce584a461344eb0eb9aa7e09", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1036 | Contributors (this commit): 31 | Commits (past 90d): 5 | Contributors (cumulative): 31 | DMM Complexity: None\n\nDIFF:\n@@ -204,7 +204,7 @@ def _get_clickable(clickdata, form):\n \n     # We didn't find it, so now we build an XPath expression out of the other\n     # arguments, because they can be used as such\n-    xpath = './/*' + ''.join(f'[@{key}=\"{clickdata[key]}\"]' for key in clickdata)\n+    xpath = './/*' + ''.join(f'[@{k}=\"{v}\"]' for k, v in clickdata.items())\n     el = form.xpath(xpath)\n     if len(el) == 1:\n         return (el[0].get('name'), el[0].get('value') or '')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9aaddcde0af5910e33fbba253777149e04f021f8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 437 | Contributors (this commit): 13 | Commits (past 90d): 6 | Contributors (cumulative): 13 | DMM Complexity: 0.0\n\nDIFF:\n@@ -42,8 +42,7 @@ def build_component_list(compdict, custom=None, convert=update_classpath):\n         for name, value in compdict.items():\n             if value is not None and not isinstance(value, numbers.Real):\n                 raise ValueError(f'Invalid value {value} for component {name}, '\n-                                 'please provide a real number or None instead'\n-                                 )\n+                                 'please provide a real number or None instead')\n \n     # BEGIN Backward compatibility for old (base, custom) call signature\n     if isinstance(custom, (list, tuple)):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#450ba6b51f7cc3cd89a92e8ff4d9e105865eb862", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1252 | Contributors (this commit): 2 | Commits (past 90d): 30 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -90,7 +90,7 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n         # all requests in a pool and send them as the connection is made\n         self._pending_request_stream_pool: deque = deque()\n \n-        # Counter to keep track of opened stream. This counter\n+        # Counter to keep track of opened streams. This counter\n         # is used to make sure that not more than MAX_CONCURRENT_STREAMS\n         # streams are opened which leads to ProtocolError\n         # We use simple FIFO policy to handle pending requests\n@@ -218,7 +218,7 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n         \"\"\"We close the connection with InvalidNegotiatedProtocol exception\n         when the connection was not made via h2 protocol\"\"\"\n         negotiated_protocol = self.transport.negotiatedProtocol\n-        if type(negotiated_protocol) is bytes:\n+        if isinstance(negotiated_protocol, bytes):\n             negotiated_protocol = str(self.transport.negotiatedProtocol, 'utf-8')\n         if negotiated_protocol != 'h2':\n             # Here we have not initiated the connection yet\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a8114d3731cfd1ef8fdb808b1563a7288b8a2e90", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 5 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 738 | Contributors (this commit): 18 | Commits (past 90d): 8 | Contributors (cumulative): 29 | DMM Complexity: None\n\nDIFF:\n@@ -5,6 +5,7 @@ See documentation in docs/topics/spiders.rst\n \"\"\"\n import logging\n import warnings\n+from typing import Optional\n \n from scrapy import signals\n from scrapy.http import Request\n@@ -18,8 +19,8 @@ class Spider(object_ref):\n     class.\n     \"\"\"\n \n-    name = None\n-    custom_settings = None\n+    name: Optional[str] = None\n+    custom_settings: Optional[dict] = None\n \n     def __init__(self, name=None, **kwargs):\n         if name is not None:\n\n@@ -7,6 +7,7 @@ See documentation in docs/topics/spiders.rst\n \n import copy\n import warnings\n+from typing import Sequence\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Request, HtmlResponse\n@@ -72,7 +73,7 @@ class Rule:\n \n class CrawlSpider(Spider):\n \n-    rules = ()\n+    rules: Sequence[Rule] = ()\n \n     def __init__(self, *a, **kw):\n         super().__init__(*a, **kw)\n\n@@ -255,7 +255,7 @@ class CrawlSpiderWithParseMethod(MockServerSpider, CrawlSpider):\n     A CrawlSpider which overrides the 'parse' method\n     \"\"\"\n     name = 'crawl_spider_with_parse_method'\n-    custom_settings = {\n+    custom_settings: dict = {\n         'RETRY_HTTP_CODES': [],  # no need to retry\n     }\n     rules = (\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5ab1a318e8a8245cad694edc297d94ac80a90760", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 714 | Contributors (this commit): 13 | Commits (past 90d): 11 | Contributors (cumulative): 21 | DMM Complexity: 1.0\n\nDIFF:\n@@ -17,7 +17,7 @@ def build_component_list(compdict, custom=None, convert=update_classpath):\n \n     def _check_components(complist):\n         if len({convert(c) for c in complist}) != len(complist):\n-            raise ValueError('Some paths in {complist!r} convert to the same object, '\n+            raise ValueError(f'Some paths in {complist!r} convert to the same object, '\n                              'please update your settings')\n \n     def _map_keys(compdict):\n\n@@ -50,8 +50,9 @@ class BuildComponentListTest(unittest.TestCase):\n \n     def test_duplicate_components_in_list(self):\n         duplicate_list = ['a', 'b', 'a']\n-        self.assertRaises(ValueError, build_component_list, None,\n-                          duplicate_list, convert=lambda x: x)\n+        with self.assertRaises(ValueError) as cm:\n+            build_component_list(None, duplicate_list, convert=lambda x: x)\n+        self.assertIn(str(duplicate_list), str(cm.exception))\n \n     def test_duplicate_components_in_basesettings(self):\n         # Higher priority takes precedence\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dd378b4bb1499b9e8da7b193f19ae6a8d81b5a2a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 69 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 69 | Churn Cumulative: 240 | Contributors (this commit): 11 | Commits (past 90d): 2 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,6 +2,8 @@ from pathlib import Path\n \n import pytest\n \n+from tests.keys import generate_keys\n+\n \n def _py_files(folder):\n     return (str(p) for p in Path(folder).rglob('*.py'))\n@@ -53,3 +55,7 @@ def reactor_pytest(request):\n def only_asyncio(request, reactor_pytest):\n     if request.node.get_closest_marker('only_asyncio') and reactor_pytest != 'asyncio':\n         pytest.skip('This test is only run with --reactor=asyncio')\n+\n+\n+# Generate localhost certificate files, needed by some tests\n+generate_keys()\n\n@@ -0,0 +1,63 @@\n+import os\n+from datetime import datetime, timedelta\n+\n+from cryptography.hazmat.backends import default_backend\n+from cryptography.hazmat.primitives.asymmetric import rsa\n+from cryptography.hazmat.primitives.hashes import SHA256\n+from cryptography.hazmat.primitives.serialization import (\n+    Encoding,\n+    NoEncryption,\n+    PrivateFormat,\n+)\n+from cryptography.x509 import (\n+    CertificateBuilder,\n+    DNSName,\n+    Name,\n+    NameAttribute,\n+    random_serial_number,\n+    SubjectAlternativeName,\n+)\n+from cryptography.x509.oid import NameOID\n+\n+\n+# https://cryptography.io/en/latest/x509/tutorial/#creating-a-self-signed-certificate\n+def generate_keys():\n+    folder = os.path.dirname(__file__)\n+\n+    key = rsa.generate_private_key(\n+        public_exponent=65537,\n+        key_size=2048,\n+        backend=default_backend(),\n+    )\n+    with open(os.path.join(folder, 'localhost.key'), \"wb\") as f:\n+        f.write(\n+            key.private_bytes(\n+                encoding=Encoding.PEM,\n+                format=PrivateFormat.TraditionalOpenSSL,\n+                encryption_algorithm=NoEncryption(),\n+            )\n+        )\n+\n+    subject = issuer = Name(\n+        [\n+            NameAttribute(NameOID.COUNTRY_NAME, u\"IE\"),\n+            NameAttribute(NameOID.ORGANIZATION_NAME, u\"Scrapy\"),\n+            NameAttribute(NameOID.COMMON_NAME, u\"localhost\"),\n+        ]\n+    )\n+    cert = (\n+        CertificateBuilder()\n+        .subject_name(subject)\n+        .issuer_name(issuer)\n+        .public_key(key.public_key())\n+        .serial_number(random_serial_number())\n+        .not_valid_before(datetime.utcnow())\n+        .not_valid_after(datetime.utcnow() + timedelta(days=10))\n+        .add_extension(\n+            SubjectAlternativeName([DNSName(u\"localhost\")]),\n+            critical=False,\n+        )\n+        .sign(key, SHA256(), default_backend())\n+    )\n+    with open(os.path.join(folder, 'localhost.crt'), \"wb\") as f:\n+        f.write(cert.public_bytes(Encoding.PEM))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5e36f539e28631625862b84d8ca1c7c0134584b0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 55 | Lines Deleted: 92 | Files Changed: 4 | Hunks: 30 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 147 | Churn Cumulative: 3501 | Contributors (this commit): 26 | Commits (past 90d): 87 | Contributors (cumulative): 31 | DMM Complexity: None\n\nDIFF:\n@@ -3,7 +3,6 @@ import itertools\n import logging\n from collections import deque\n from ipaddress import IPv4Address, IPv6Address\n-from typing import Dict, List, Optional, Union\n \n from h2.config import H2Configuration\n from h2.connection import H2Connection\n@@ -22,10 +21,10 @@ from twisted.internet.ssl import Certificate\n from twisted.protocols.policies import TimeoutMixin\n from twisted.python.failure import Failure\n from twisted.web.client import URI\n+from typing import Dict, List, Optional, Union\n from zope.interface import implementer\n \n from scrapy.core.http2.stream import Stream, StreamCloseReason\n-from scrapy.core.http2.types import H2ConnectionMetadataDict\n from scrapy.http import Request\n from scrapy.settings import Settings\n from scrapy.spiders import Spider\n@@ -90,26 +89,39 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n         # all requests in a pool and send them as the connection is made\n         self._pending_request_stream_pool: deque = deque()\n \n-        # Counter to keep track of opened streams. This counter\n-        # is used to make sure that not more than MAX_CONCURRENT_STREAMS\n-        # streams are opened which leads to ProtocolError\n-        # We use simple FIFO policy to handle pending requests\n-        self._active_streams = 0\n-\n-        # Flag to keep track if settings were acknowledged by the remote\n-        # This ensures that we have established a HTTP/2 connection\n-        self._settings_acknowledged = False\n-\n         # Save an instance of errors raised which lead to losing the connection\n         # We pass these instances to the streams ResponseFailed() failure\n         self._conn_lost_errors: List[BaseException] = []\n \n-        self.metadata: H2ConnectionMetadataDict = {\n+        # Some meta data of this connection\n+        # initialized when connection is successfully made\n+        self.metadata: Dict = {\n+            # Peer certificate instance\n             'certificate': None,\n+\n+            # Address of the server we are connected to which\n+            # is updated when HTTP/2 connection is  made successfully\n             'ip_address': None,\n+\n+            # URI of the peer HTTP/2 connection is made\n             'uri': uri,\n+\n+            # Both ip_address and uri are used by the Stream before\n+            # initiating the request to verify that the base address\n+\n+            # Variables taken from Project Settings\n             'default_download_maxsize': settings.getint('DOWNLOAD_MAXSIZE'),\n             'default_download_warnsize': settings.getint('DOWNLOAD_WARNSIZE'),\n+\n+            # Counter to keep track of opened streams. This counter\n+            # is used to make sure that not more than MAX_CONCURRENT_STREAMS\n+            # streams are opened which leads to ProtocolError\n+            # We use simple FIFO policy to handle pending requests\n+            'active_streams': 0,\n+\n+            # Flag to keep track if settings were acknowledged by the remote\n+            # This ensures that we have established a HTTP/2 connection\n+            'settings_acknowledged': False,\n         }\n \n     @property\n@@ -118,7 +130,7 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n         This is used while initiating pending streams to make sure\n         that we initiate stream only during active HTTP/2 Connection\n         \"\"\"\n-        return bool(self.transport.connected) and self._settings_acknowledged\n+        return bool(self.transport.connected) and self.metadata['settings_acknowledged']\n \n     @property\n     def allowed_max_concurrent_streams(self) -> int:\n@@ -139,10 +151,10 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n         \"\"\"\n         while (\n             self._pending_request_stream_pool\n-            and self._active_streams < self.allowed_max_concurrent_streams\n+            and self.metadata['active_streams'] < self.allowed_max_concurrent_streams\n             and self.h2_connected\n         ):\n-            self._active_streams += 1\n+            self.metadata['active_streams'] += 1\n             stream = self._pending_request_stream_pool.popleft()\n             stream.initiate_request()\n             self._write_to_transport()\n@@ -151,7 +163,7 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n         \"\"\"Perform cleanup when a stream is closed\n         \"\"\"\n         stream = self.streams.pop(stream_id)\n-        self._active_streams -= 1\n+        self.metadata['active_streams'] -= 1\n         self._send_pending_requests()\n         return stream\n \n@@ -261,7 +273,7 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n         if (\n             self.conn.open_outbound_streams > 0\n             or self.conn.open_inbound_streams > 0\n-            or self._active_streams > 0\n+            or self.metadata['active_streams'] > 0\n         ):\n             error_code = ErrorCodes.PROTOCOL_ERROR\n         else:\n@@ -295,7 +307,7 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n                 close_reason = StreamCloseReason.INACTIVE\n             stream.close(close_reason, self._conn_lost_errors, from_protocol=True)\n \n-        self._active_streams -= len(self.streams)\n+        self.metadata['active_streams'] -= len(self.streams)\n         self.streams.clear()\n         self._pending_request_stream_pool.clear()\n         self.conn.close_connection()\n@@ -338,7 +350,7 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n         self.streams[event.stream_id].receive_headers(event.headers)\n \n     def settings_acknowledged(self, event: SettingsAcknowledged) -> None:\n-        self._settings_acknowledged = True\n+        self.metadata['settings_acknowledged'] = True\n \n         # Send off all the pending requests as now we have\n         # established a proper HTTP/2 connection\n\n@@ -1,7 +1,6 @@\n import logging\n from enum import Enum\n from io import BytesIO\n-from typing import List, Optional, Tuple, TYPE_CHECKING\n from urllib.parse import urlparse\n \n from h2.errors import ErrorCodes\n@@ -11,8 +10,9 @@ from twisted.internet.defer import Deferred, CancelledError\n from twisted.internet.error import ConnectionClosed\n from twisted.python.failure import Failure\n from twisted.web.client import ResponseFailed\n+from typing import Dict\n+from typing import List, Optional, Tuple, TYPE_CHECKING\n \n-from scrapy.core.http2.types import H2ResponseDict, H2StreamMetadataDict\n from scrapy.http import Request\n from scrapy.http.headers import Headers\n from scrapy.responsetypes import responsetypes\n@@ -103,21 +103,40 @@ class Stream:\n         self._download_maxsize = self._request.meta.get('download_maxsize', download_maxsize)\n         self._download_warnsize = self._request.meta.get('download_warnsize', download_warnsize)\n \n-        self.metadata: H2StreamMetadataDict = {\n+        # Metadata of an HTTP/2 connection stream\n+        # initialized when stream is instantiated\n+        self.metadata: Dict = {\n             'request_content_length': 0 if self._request.body is None else len(self._request.body),\n+\n+            # Flag to keep track whether the stream has initiated the request\n             'request_sent': False,\n+\n+            # Flag to track whether we have logged about exceeding download warnsize\n             'reached_warnsize': False,\n+\n+            # Each time we send a data frame, we will decrease value by the amount send.\n             'remaining_content_length': 0 if self._request.body is None else len(self._request.body),\n+\n+            # Flag to keep track whether client (self) have closed this stream\n             'stream_closed_local': False,\n+\n+            # Flag to keep track whether the server has closed the stream\n             'stream_closed_server': False,\n         }\n \n         # Private variable used to build the response\n         # this response is then converted to appropriate Response class\n         # passed to the response deferred callback\n-        self._response: H2ResponseDict = {\n+        self._response: Dict = {\n+            # Data received frame by frame from the server is appended\n+            # and passed to the response Deferred when completely received.\n             'body': BytesIO(),\n+\n+            # The amount of data received that counts against the\n+            # flow control window\n             'flow_controlled_size': 0,\n+\n+            # Headers received after sending the request\n             'headers': Headers({}),\n         }\n \n\n@@ -1,67 +0,0 @@\n-from io import BytesIO\n-from ipaddress import IPv4Address, IPv6Address\n-from typing import Union, Optional\n-\n-from twisted.internet.ssl import Certificate\n-from twisted.web.client import URI\n-# for python < 3.8 -- typing.TypedDict is undefined\n-from typing_extensions import TypedDict\n-\n-from scrapy.http.headers import Headers\n-\n-\n-class H2ConnectionMetadataDict(TypedDict):\n-    \"\"\"Some meta data of this connection\n-    initialized when connection is successfully made\n-    \"\"\"\n-    certificate: Optional[Certificate]\n-\n-    # Address of the server we are connected to which\n-    # is updated when HTTP/2 connection is  made successfully\n-    ip_address: Optional[Union[IPv4Address, IPv6Address]]\n-\n-    # URI of the peer HTTP/2 connection is made\n-    uri: URI\n-\n-    # Both ip_address and uri are used by the Stream before\n-    # initiating the request to verify that the base address\n-\n-    # Variables taken from Project Settings\n-    default_download_maxsize: int\n-    default_download_warnsize: int\n-\n-\n-class H2StreamMetadataDict(TypedDict):\n-    \"\"\"Metadata of an HTTP/2 connection stream\n-    initialized when stream is instantiated\n-    \"\"\"\n-\n-    request_content_length: int\n-\n-    # Flag to keep track whether the stream has initiated the request\n-    request_sent: bool\n-\n-    # Flag to track whether we have logged about exceeding download warnsize\n-    reached_warnsize: bool\n-\n-    # Each time we send a data frame, we will decrease value by the amount send.\n-    remaining_content_length: int\n-\n-    # Flag to keep track whether we have closed this stream\n-    stream_closed_local: bool\n-\n-    # Flag to keep track whether the server has closed the stream\n-    stream_closed_server: bool\n-\n-\n-class H2ResponseDict(TypedDict):\n-    # Data received frame by frame from the server is appended\n-    # and passed to the response Deferred when completely received.\n-    body: BytesIO\n-\n-    # The amount of data received that counts against the flow control\n-    # window\n-    flow_controlled_size: int\n-\n-    # Headers received after sending the request\n-    headers: Headers\n\n@@ -33,7 +33,6 @@ install_requires = [\n     'protego>=0.1.15',\n     'itemadapter>=0.1.0',\n     'h2>=3.2.0',\n-    'typing-extensions>=3.7.4',\n ]\n extras_require = {}\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#195f738bbaa7c090df795161390056c3c6c7f1f7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 44 | Files Changed: 7 | Hunks: 18 | Methods Changed: 5 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 52 | Churn Cumulative: 3671 | Contributors (this commit): 47 | Commits (past 90d): 30 | Contributors (cumulative): 100 | DMM Complexity: 0.0\n\nDIFF:\n@@ -28,8 +28,8 @@ twisted_version = (_txv.major, _txv.minor, _txv.micro)\n \n \n # Check minimum required Python version\n-if sys.version_info < (3, 5, 2):\n-    print(\"Scrapy %s requires Python 3.5.2\" % __version__)\n+if sys.version_info < (3, 6):\n+    print(\"Scrapy %s requires Python 3.6+\" % __version__)\n     sys.exit(1)\n \n \n\n@@ -124,18 +124,11 @@ def iter_errback(iterable, errback, *a, **kw):\n             errback(failure.Failure(), *a, **kw)\n \n \n-def _isfuture(o):\n-    # workaround for Python before 3.5.3 not having asyncio.isfuture\n-    if hasattr(asyncio, 'isfuture'):\n-        return asyncio.isfuture(o)\n-    return isinstance(o, asyncio.Future)\n-\n-\n def deferred_from_coro(o):\n     \"\"\"Converts a coroutine into a Deferred, or returns the object as is if it isn't a coroutine\"\"\"\n     if isinstance(o, defer.Deferred):\n         return o\n-    if _isfuture(o) or inspect.isawaitable(o):\n+    if asyncio.isfuture(o) or inspect.isawaitable(o):\n         if not is_asyncio_reactor_installed():\n             # wrapping the coroutine directly into a Deferred, this doesn't work correctly with coroutines\n             # that use asyncio, e.g. \"await asyncio.sleep(1)\"\n@@ -167,7 +160,7 @@ def maybeDeferred_coro(f, *args, **kw):\n \n     if isinstance(result, defer.Deferred):\n         return result\n-    elif _isfuture(result) or inspect.isawaitable(result):\n+    elif asyncio.isfuture(result) or inspect.isawaitable(result):\n         return deferred_from_coro(result)\n     elif isinstance(result, failure.Failure):\n         return defer.fail(result)\n\n@@ -6,11 +6,10 @@ import struct\n from scrapy.utils.decorators import deprecated\n \n \n-# - Python>=3.5 GzipFile's read() has issues returning leftover\n-#   uncompressed data when input is corrupted\n-#   (regression or bug-fix compared to Python 3.4)\n+# - GzipFile's read() has issues returning leftover uncompressed data when\n+#   input is corrupted\n # - read1(), which fetches data before raising EOFError on next call\n-#   works here but is only available from Python>=3.3\n+#   works here\n @deprecated('GzipFile.read1')\n def read1(gzf, size=-1):\n     return gzf.read1(size)\n\n@@ -82,7 +82,6 @@ setup(\n         'Operating System :: OS Independent',\n         'Programming Language :: Python',\n         'Programming Language :: Python :: 3',\n-        'Programming Language :: Python :: 3.5',\n         'Programming Language :: Python :: 3.6',\n         'Programming Language :: Python :: 3.7',\n         'Programming Language :: Python :: 3.8',\n@@ -92,7 +91,7 @@ setup(\n         'Topic :: Software Development :: Libraries :: Application Frameworks',\n         'Topic :: Software Development :: Libraries :: Python Modules',\n     ],\n-    python_requires='>=3.5.2',\n+    python_requires='>=3.6',\n     install_requires=install_requires,\n     extras_require=extras_require,\n )\n\n@@ -1,6 +1,5 @@\n import json\n import logging\n-import sys\n from ipaddress import IPv4Address\n from socket import gethostbyname\n from urllib.parse import urlparse\n@@ -405,7 +404,6 @@ class CrawlSpiderTestCase(TestCase):\n         self.assertIn(\"Got response 200\", str(log))\n         self.assertIn({\"foo\": 42}, items)\n \n-    @mark.skipif(sys.version_info < (3, 6), reason=\"Async generators require Python 3.6 or higher\")\n     @mark.only_asyncio()\n     @defer.inlineCallbacks\n     def test_async_def_asyncgen_parse(self):\n@@ -417,7 +415,6 @@ class CrawlSpiderTestCase(TestCase):\n         itemcount = crawler.stats.get_value('item_scraped_count')\n         self.assertEqual(itemcount, 1)\n \n-    @mark.skipif(sys.version_info < (3, 6), reason=\"Async generators require Python 3.6 or higher\")\n     @mark.only_asyncio()\n     @defer.inlineCallbacks\n     def test_async_def_asyncgen_parse_loop(self):\n@@ -437,7 +434,6 @@ class CrawlSpiderTestCase(TestCase):\n         for i in range(10):\n             self.assertIn({'foo': i}, items)\n \n-    @mark.skipif(sys.version_info < (3, 6), reason=\"Async generators require Python 3.6 or higher\")\n     @mark.only_asyncio()\n     @defer.inlineCallbacks\n     def test_async_def_asyncgen_parse_complex(self):\n\n@@ -1,4 +1,3 @@\n-import sys\n import unittest\n from unittest import mock\n from warnings import catch_warnings\n@@ -7,9 +6,6 @@ from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.item import ABCMeta, _BaseItem, BaseItem, DictItem, Field, Item, ItemMeta\n \n \n-PY36_PLUS = (sys.version_info.major >= 3) and (sys.version_info.minor >= 6)\n-\n-\n class ItemTest(unittest.TestCase):\n \n     def assertSortedEqual(self, first, second, msg=None):\n@@ -280,14 +276,6 @@ class ItemMetaTest(unittest.TestCase):\n         with mock.patch.object(base, '__new__', new_mock):\n \n             class MyItem(Item):\n-                if not PY36_PLUS:\n-                    # This attribute is an internal attribute in Python 3.6+\n-                    # and must be propagated properly. See\n-                    # https://docs.python.org/3.6/reference/datamodel.html#creating-the-class-object\n-                    # In <3.6, we add a dummy attribute just to ensure the\n-                    # __new__ method propagates it correctly.\n-                    __classcell__ = object()\n-\n                 def f(self):\n                     # For rationale of this see:\n                     # https://github.com/python/cpython/blob/ee1a81b77444c6715cbe610e951c655b6adab88b/Lib/test/test_super.py#L222\n\n@@ -7,7 +7,6 @@ from subprocess import Popen, PIPE\n from urllib.parse import urlsplit, urlunsplit\n from unittest import skipIf\n \n-import pytest\n from testfixtures import LogCapture\n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n@@ -58,8 +57,6 @@ def _wrong_credentials(proxy_url):\n     return urlunsplit(bad_auth_proxy)\n \n \n-@skipIf(sys.version_info < (3, 5, 4),\n-        \"requires mitmproxy < 3.0.0, which these tests do not support\")\n @skipIf(\"pypy\" in sys.executable,\n         \"mitmproxy does not support PyPy\")\n @skipIf(platform.system() == 'Windows' and sys.version_info < (3, 7),\n@@ -88,14 +85,6 @@ class ProxyConnectTestCase(TestCase):\n             yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n         self._assert_got_response_code(200, log)\n \n-    @pytest.mark.xfail(reason='Python 3.6+ fails this earlier', condition=sys.version_info >= (3, 6))\n-    @defer.inlineCallbacks\n-    def test_https_connect_tunnel_error(self):\n-        crawler = get_crawler(SimpleSpider)\n-        with LogCapture() as log:\n-            yield crawler.crawl(\"https://localhost:99999/status?n=200\")\n-        self._assert_got_tunnel_error(log)\n-\n     @defer.inlineCallbacks\n     def test_https_tunnel_auth_error(self):\n         os.environ['https_proxy'] = _wrong_credentials(os.environ['https_proxy'])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0e579182319504ba7bfd0c09333fe92f70c6d312", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 12 | Churn Cumulative: 825 | Contributors (this commit): 12 | Commits (past 90d): 3 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -43,7 +43,7 @@ class Slot:\n         cls_name = self.__class__.__name__\n         return (f\"{cls_name}(concurrency={self.concurrency!r}, \"\n                 f\"delay={self.delay:.2f}, \"\n-                f\"randomize_delay={self.randomize_delay!r}\")\n+                f\"randomize_delay={self.randomize_delay!r})\")\n \n     def __str__(self):\n         return (\n\n@@ -0,0 +1,10 @@\n+from twisted.trial import unittest\n+\n+from scrapy.core.downloader import Slot\n+\n+\n+class SlotTest(unittest.TestCase):\n+\n+    def test_repr(self):\n+        slot = Slot(concurrency=8, delay=0.1, randomize_delay=True)\n+        self.assertEqual(repr(slot), 'Slot(concurrency=8, delay=0.10, randomize_delay=True)')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#de640f41ecfa1a87b39d9e9268c396fc97353fc8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 56 | Lines Deleted: 62 | Files Changed: 4 | Hunks: 7 | Methods Changed: 11 | Complexity Δ (Sum/Max): 0/10 | Churn Δ: 118 | Churn Cumulative: 1642 | Contributors (this commit): 20 | Commits (past 90d): 13 | Contributors (cumulative): 41 | DMM Complexity: 0.0\n\nDIFF:\n@@ -14,8 +14,6 @@ collect_ignore = [\n     *_py_files(\"tests/CrawlerProcess\"),\n     # contains scripts to be run by tests/test_crawler.py::CrawlerRunnerSubprocess\n     *_py_files(\"tests/CrawlerRunner\"),\n-    # Py36-only parts of respective tests\n-    *_py_files(\"tests/py36\"),\n ]\n \n for line in open('tests/ignores.txt'):\n\n@@ -1,57 +0,0 @@\n-import asyncio\n-\n-from scrapy import Request\n-from tests.spiders import SimpleSpider\n-\n-\n-class AsyncDefAsyncioGenSpider(SimpleSpider):\n-\n-    name = 'asyncdef_asyncio_gen'\n-\n-    async def parse(self, response):\n-        await asyncio.sleep(0.2)\n-        yield {'foo': 42}\n-        self.logger.info(\"Got response %d\" % response.status)\n-\n-\n-class AsyncDefAsyncioGenLoopSpider(SimpleSpider):\n-\n-    name = 'asyncdef_asyncio_gen_loop'\n-\n-    async def parse(self, response):\n-        for i in range(10):\n-            await asyncio.sleep(0.1)\n-            yield {'foo': i}\n-        self.logger.info(\"Got response %d\" % response.status)\n-\n-\n-class AsyncDefAsyncioGenComplexSpider(SimpleSpider):\n-\n-    name = 'asyncdef_asyncio_gen_complex'\n-    initial_reqs = 4\n-    following_reqs = 3\n-    depth = 2\n-\n-    def _get_req(self, index, cb=None):\n-        return Request(self.mockserver.url(\"/status?n=200&request=%d\" % index),\n-                       meta={'index': index},\n-                       dont_filter=True,\n-                       callback=cb)\n-\n-    def start_requests(self):\n-        for i in range(1, self.initial_reqs + 1):\n-            yield self._get_req(i)\n-\n-    async def parse(self, response):\n-        index = response.meta['index']\n-        yield {'index': index}\n-        if index < 10 ** self.depth:\n-            for new_index in range(10 * index, 10 * index + self.following_reqs):\n-                yield self._get_req(new_index)\n-        yield self._get_req(index, cb=self.parse2)\n-        await asyncio.sleep(0.1)\n-        yield {'index': index + 5}\n-\n-    async def parse2(self, response):\n-        await asyncio.sleep(0.1)\n-        yield {'index2': response.meta['index']}\n\n@@ -148,6 +148,59 @@ class AsyncDefAsyncioReqsReturnSpider(SimpleSpider):\n         return reqs\n \n \n+class AsyncDefAsyncioGenSpider(SimpleSpider):\n+\n+    name = 'asyncdef_asyncio_gen'\n+\n+    async def parse(self, response):\n+        await asyncio.sleep(0.2)\n+        yield {'foo': 42}\n+        self.logger.info(\"Got response %d\" % response.status)\n+\n+\n+class AsyncDefAsyncioGenLoopSpider(SimpleSpider):\n+\n+    name = 'asyncdef_asyncio_gen_loop'\n+\n+    async def parse(self, response):\n+        for i in range(10):\n+            await asyncio.sleep(0.1)\n+            yield {'foo': i}\n+        self.logger.info(\"Got response %d\" % response.status)\n+\n+\n+class AsyncDefAsyncioGenComplexSpider(SimpleSpider):\n+\n+    name = 'asyncdef_asyncio_gen_complex'\n+    initial_reqs = 4\n+    following_reqs = 3\n+    depth = 2\n+\n+    def _get_req(self, index, cb=None):\n+        return Request(self.mockserver.url(\"/status?n=200&request=%d\" % index),\n+                       meta={'index': index},\n+                       dont_filter=True,\n+                       callback=cb)\n+\n+    def start_requests(self):\n+        for i in range(1, self.initial_reqs + 1):\n+            yield self._get_req(i)\n+\n+    async def parse(self, response):\n+        index = response.meta['index']\n+        yield {'index': index}\n+        if index < 10 ** self.depth:\n+            for new_index in range(10 * index, 10 * index + self.following_reqs):\n+                yield self._get_req(new_index)\n+        yield self._get_req(index, cb=self.parse2)\n+        await asyncio.sleep(0.1)\n+        yield {'index': index + 5}\n+\n+    async def parse2(self, response):\n+        await asyncio.sleep(0.1)\n+        yield {'index2': response.meta['index']}\n+\n+\n class ItemSpider(FollowAllSpider):\n \n     name = 'item'\n\n@@ -19,6 +19,9 @@ from scrapy.http.response import Response\n from scrapy.utils.python import to_unicode\n from tests.mockserver import MockServer\n from tests.spiders import (\n+    AsyncDefAsyncioGenComplexSpider,\n+    AsyncDefAsyncioGenLoopSpider,\n+    AsyncDefAsyncioGenSpider,\n     AsyncDefAsyncioReqsReturnSpider,\n     AsyncDefAsyncioReturnSingleElementSpider,\n     AsyncDefAsyncioReturnSpider,\n@@ -407,7 +410,6 @@ class CrawlSpiderTestCase(TestCase):\n     @mark.only_asyncio()\n     @defer.inlineCallbacks\n     def test_async_def_asyncgen_parse(self):\n-        from tests.py36._test_crawl import AsyncDefAsyncioGenSpider\n         crawler = self.runner.create_crawler(AsyncDefAsyncioGenSpider)\n         with LogCapture() as log:\n             yield crawler.crawl(self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver)\n@@ -423,7 +425,6 @@ class CrawlSpiderTestCase(TestCase):\n         def _on_item_scraped(item):\n             items.append(item)\n \n-        from tests.py36._test_crawl import AsyncDefAsyncioGenLoopSpider\n         crawler = self.runner.create_crawler(AsyncDefAsyncioGenLoopSpider)\n         crawler.signals.connect(_on_item_scraped, signals.item_scraped)\n         with LogCapture() as log:\n@@ -442,7 +443,6 @@ class CrawlSpiderTestCase(TestCase):\n         def _on_item_scraped(item):\n             items.append(item)\n \n-        from tests.py36._test_crawl import AsyncDefAsyncioGenComplexSpider\n         crawler = self.runner.create_crawler(AsyncDefAsyncioGenComplexSpider)\n         crawler.signals.connect(_on_item_scraped, signals.item_scraped)\n         yield crawler.crawl(mockserver=self.mockserver)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
