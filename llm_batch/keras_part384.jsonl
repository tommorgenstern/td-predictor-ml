{"custom_id": "keras#6b3deaaa511cbe3c8686c8c2ded80d54eb7f1689", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 9313 | Contributors (this commit): 26 | Commits (past 90d): 7 | Contributors (cumulative): 26 | DMM Complexity: None\n\nDIFF:\n@@ -121,8 +121,8 @@ class Metric(base_layer.Layer, metaclass=abc.ABCMeta):\n     calling `self.add_weight()` like: `self.var = self.add_weight(...)`\n   * `update_state()`: Has all updates to the state variables like:\n     self.var.assign_add(...).\n-  * `result()`: Computes and returns a scalar value for the metric\n-    from the state variables.\n+  * `result()`: Computes and returns a scalar value or a dict of scalar values\n+    for the metric from the state variables.\n \n   Example subclass implementation:\n \n@@ -323,7 +323,7 @@ class Metric(base_layer.Layer, metaclass=abc.ABCMeta):\n \n   @abc.abstractmethod\n   def result(self):\n-    \"\"\"Computes and returns the scalar metric value tensor.\n+    \"\"\"Computes and returns the scalar metric value tensor or a dict of scalars.\n \n     Result computation is an idempotent operation that simply calculates the\n     metric value using the state variables.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b0ae4a5be1bc47715cae5ffe1893074ab9f3840e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 23 | Files Changed: 6 | Hunks: 23 | Methods Changed: 15 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 46 | Churn Cumulative: 10119 | Contributors (this commit): 102 | Commits (past 90d): 28 | Contributors (cumulative): 125 | DMM Complexity: None\n\nDIFF:\n@@ -1205,7 +1205,7 @@ class ModelCheckpoint(Callback):\n \n   Args:\n       filepath: string or `PathLike`, path to save the model file. e.g.\n-        filepath = os.path.join(working_dir, 'ckpt', file_name). `filepath`\n+        filepath = tf.io.gfile.join(working_dir, 'ckpt', file_name). `filepath`\n         can contain named formatting options, which will be filled the value of\n         `epoch` and keys in `logs` (passed in `on_epoch_end`). For example: if\n         `filepath` is `weights.{epoch:02d}-{val_loss:.2f}.hdf5`, then the model\n@@ -1515,9 +1515,9 @@ class ModelCheckpoint(Callback):\n     ```python\n     file_pattern = 'f.batch{batch:02d}epoch{epoch:02d}.h5'\n     test_dir = self.get_temp_dir()\n-    path_pattern = os.path.join(test_dir, file_pattern)\n+    path_pattern = tf.io.gfile.join(test_dir, file_pattern)\n     file_paths = [\n-        os.path.join(test_dir, file_name) for file_name in\n+        tf.io.gfile.join(test_dir, file_name) for file_name in\n         ['f.batch03epoch02.h5', 'f.batch02epoch02.h5', 'f.batch01epoch01.h5']\n     ]\n     for file_path in file_paths:\n@@ -1557,7 +1557,7 @@ class ModelCheckpoint(Callback):\n       for file_name in os.listdir(dir_name):\n         # Only consider if `file_name` matches the pattern.\n         if re.match(base_name_regex, file_name):\n-          file_path = os.path.join(dir_name, file_name)\n+          file_path = tf.io.gfile.join(dir_name, file_name)\n           mod_time = os.path.getmtime(file_path)\n           if (file_path_with_largest_file_name is None or\n               file_path > file_path_with_largest_file_name):\n@@ -1642,7 +1642,7 @@ class BackupAndRestore(Callback):\n \n   Args:\n       backup_dir: String, path to store the checkpoint.\n-        e.g. backup_dir = os.path.join(working_dir, 'backup')\n+        e.g. backup_dir = tf.io.gfile.join(working_dir, 'backup')\n         This is the directory in which the system stores temporary files to\n         recover the model from jobs terminated unexpectedly. The directory\n         cannot be reused elsewhere to store other files, e.g. by\n@@ -2061,7 +2061,7 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n \n   Args:\n       log_dir: the path of the directory where to save the log files to be\n-        parsed by TensorBoard. e.g. log_dir = os.path.join(working_dir, 'logs')\n+        parsed by TensorBoard. e.g. log_dir = tf.io.gfile.join(working_dir, 'logs')\n         This directory should not be reused by any other callbacks.\n       histogram_freq: frequency (in epochs) at which to compute activation and\n         weight histograms for the layers of the model. If set to 0, histograms\n@@ -2229,10 +2229,10 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n     self.model = model\n     self._log_write_dir = self._get_log_write_dir()\n \n-    self._train_dir = os.path.join(self._log_write_dir, 'train')\n+    self._train_dir = tf.io.gfile.join(self._log_write_dir, 'train')\n     self._train_step = self.model._train_counter  # pylint: disable=protected-access\n \n-    self._val_dir = os.path.join(self._log_write_dir, 'validation')\n+    self._val_dir = tf.io.gfile.join(self._log_write_dir, 'validation')\n     self._val_step = self.model._test_counter  # pylint: disable=protected-access\n \n     self._writers = {}  # Resets writers.\n@@ -2316,7 +2316,7 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n                        f'argument: {self.embeddings_metadata.keys()}')\n \n     config_pbtxt = text_format.MessageToString(config)\n-    path = os.path.join(self._log_write_dir, 'projector_config.pbtxt')\n+    path = tf.io.gfile.join(self._log_write_dir, 'projector_config.pbtxt')\n     with tf.io.gfile.GFile(path, 'w') as f:\n       f.write(config_pbtxt)\n \n@@ -2563,7 +2563,7 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n       tf.summary.image(weight_name, w_img, step=epoch)\n \n   def _log_embeddings(self, epoch):\n-    embeddings_ckpt = os.path.join(self._log_write_dir, 'train',\n+    embeddings_ckpt = tf.io.gfile.join(self._log_write_dir, 'train',\n                                    'keras_embedding.ckpt-{}'.format(epoch))\n     self.model.save_weights(embeddings_ckpt)\n \n\n@@ -39,7 +39,7 @@ def deserialize_model_from_bytecode(serialized_model):\n   b = io.BytesIO(serialized_model)\n   with tarfile.open(fileobj=b, mode=\"r\") as archive:\n     for name in archive.getnames():\n-      dest_path = os.path.join(temp_dir, name)\n+      dest_path = tf.io.gfile.join(temp_dir, name)\n       member = archive.getmember(name)\n       tf.io.gfile.makedirs(os.path.dirname(dest_path))\n       if member.isfile():\n@@ -66,12 +66,12 @@ def serialize_model_as_bytecode(model):\n   with tarfile.open(fileobj=b, mode=\"w\") as archive:\n     for root, dirs, filenames in tf.io.gfile.walk(temp_dir):\n       for dirname in dirs:\n-        dest_path = os.path.join(root, dirname)\n+        dest_path = tf.io.gfile.join(root, dirname)\n         t = tarfile.TarInfo(dest_path)\n         t.type = tarfile.DIRTYPE\n         archive.addfile(t)\n       for filename in filenames:\n-        dest_path = os.path.join(root, filename)\n+        dest_path = tf.io.gfile.join(root, filename)\n         with tf.io.gfile.GFile(dest_path, \"rb\") as f:\n           info = tarfile.TarInfo(name=os.path.relpath(dest_path, temp_dir))\n           info.size = f.size()\n\n@@ -108,7 +108,7 @@ def load(path, compile=True, options=None):  # pylint: disable=redefined-builtin\n   meta_graph_def = tf.__internal__.saved_model.parse_saved_model(\n       path).meta_graphs[0]\n   object_graph_def = meta_graph_def.object_graph_def\n-  path_to_metadata_pb = os.path.join(path, constants.SAVED_METADATA_PATH)\n+  path_to_metadata_pb = tf.io.gfile.join(path, constants.SAVED_METADATA_PATH)\n   if tf.compat.v1.gfile.Exists(path_to_metadata_pb):\n     try:\n       with tf.io.gfile.GFile(path_to_metadata_pb, 'rb') as f:\n\n@@ -97,7 +97,7 @@ def save(model, filepath, overwrite, include_optimizer, signatures=None,\n     metadata = generate_keras_metadata(saved_nodes, node_paths)\n \n   with tf.io.gfile.GFile(\n-      os.path.join(filepath, constants.SAVED_METADATA_PATH), \"wb\") as w:\n+      tf.io.gfile.join(filepath, constants.SAVED_METADATA_PATH), \"wb\") as w:\n     w.write(metadata.SerializeToString(deterministic=True))\n \n   if not include_optimizer:\n\n@@ -139,7 +139,7 @@ def export_saved_model(model,\n def _export_model_json(model, saved_model_path):\n   \"\"\"Saves model configuration as a json string under assets folder.\"\"\"\n   model_json = model.to_json()\n-  model_json_filepath = os.path.join(\n+  model_json_filepath = tf.io.gfile.join(\n       _get_or_create_assets_dir(saved_model_path),\n       tf.compat.as_text(SAVED_MODEL_FILENAME_JSON))\n   with tf.io.gfile.GFile(model_json_filepath, 'w') as f:\n@@ -406,7 +406,7 @@ def load_from_saved_model(saved_model_path, custom_objects=None):\n       'Please switch to `tf.keras.models.load_model`.',\n       stacklevel=2)\n   # restore model topology from json string\n-  model_json_filepath = os.path.join(\n+  model_json_filepath = tf.io.gfile.join(\n       tf.compat.as_bytes(saved_model_path),\n       tf.compat.as_bytes(tf.saved_model.ASSETS_DIRECTORY),\n       tf.compat.as_bytes(SAVED_MODEL_FILENAME_JSON))\n@@ -416,7 +416,7 @@ def load_from_saved_model(saved_model_path, custom_objects=None):\n       model_json, custom_objects=custom_objects)\n \n   # restore model weights\n-  checkpoint_prefix = os.path.join(\n+  checkpoint_prefix = tf.io.gfile.join(\n       tf.compat.as_text(saved_model_path),\n       tf.compat.as_text(tf.saved_model.VARIABLES_DIRECTORY),\n       tf.compat.as_text(tf.saved_model.VARIABLES_FILENAME))\n@@ -436,14 +436,14 @@ def _get_or_create_variables_dir(export_dir):\n \n def _get_variables_dir(export_dir):\n   \"\"\"Return variables sub-directory in the SavedModel.\"\"\"\n-  return os.path.join(\n+  return tf.io.gfile.join(\n       tf.compat.as_text(export_dir),\n       tf.compat.as_text(tf.saved_model.VARIABLES_DIRECTORY))\n \n \n def _get_variables_path(export_dir):\n   \"\"\"Return the variables path, used as the prefix for checkpoint files.\"\"\"\n-  return os.path.join(\n+  return tf.io.gfile.join(\n       tf.compat.as_text(_get_variables_dir(export_dir)),\n       tf.compat.as_text(tf.saved_model.VARIABLES_FILENAME))\n \n@@ -459,6 +459,6 @@ def _get_or_create_assets_dir(export_dir):\n \n def _get_assets_dir(export_dir):\n   \"\"\"Return path to asset directory in the SavedModel.\"\"\"\n-  return os.path.join(\n+  return tf.io.gfile.join(\n       tf.compat.as_text(export_dir),\n       tf.compat.as_text(tf.saved_model.ASSETS_DIRECTORY))\n\n@@ -206,7 +206,7 @@ def get_timestamped_export_dir(export_dir_base):\n   while attempts < MAX_DIRECTORY_CREATION_ATTEMPTS:\n     timestamp = int(time.time())\n \n-    result_dir = os.path.join(\n+    result_dir = tf.io.gfile.join(\n         tf.compat.as_bytes(export_dir_base), tf.compat.as_bytes(str(timestamp)))\n     if not tf.compat.v1.gfile.Exists(result_dir):\n       # Collisions are still possible (though extremely unlikely): this\n@@ -241,7 +241,7 @@ def get_temp_export_dir(timestamped_export_dir):\n     str_name = basename.decode('utf-8')\n   else:\n     str_name = str(basename)\n-  temp_export_dir = os.path.join(\n+  temp_export_dir = tf.io.gfile.join(\n       tf.compat.as_bytes(dirname),\n       tf.compat.as_bytes('temp-{}'.format(str_name)))\n   return temp_export_dir\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#21204201a99958bd9d787d4d04f664a718c0dc70", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 300 | Lines Deleted: 16 | Files Changed: 6 | Hunks: 26 | Methods Changed: 20 | Complexity Δ (Sum/Max): 24/10 | Churn Δ: 316 | Churn Cumulative: 5532 | Contributors (this commit): 43 | Commits (past 90d): 14 | Contributors (cumulative): 48 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,133 @@\n+# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Adadelta optimizer implementation.\"\"\"\n+\n+from keras.optimizer_experimental import optimizer\n+import tensorflow.compat.v2 as tf\n+\n+\n+class Adadelta(optimizer.Optimizer):\n+  r\"\"\"Optimizer that implements the Adadelta algorithm.\n+\n+  Adadelta optimization is a stochastic gradient descent method that is based on\n+  adaptive learning rate per dimension to address two drawbacks:\n+\n+  - The continual decay of learning rates throughout training.\n+  - The need for a manually selected global learning rate.\n+\n+  Adadelta is a more robust extension of Adagrad that adapts learning rates\n+  based on a moving window of gradient updates, instead of accumulating all\n+  past gradients. This way, Adadelta continues learning even when many updates\n+  have been done. Compared to Adagrad, in the original version of Adadelta you\n+  don't have to set an initial learning rate. In this version, the initial\n+  learning rate can be set, as in most other Keras optimizers.\n+\n+  Attributes:\n+    learning_rate: Initial value for the learning rate:\n+      either a floating point value,\n+      or a `tf.keras.optimizers.schedules.LearningRateSchedule` instance.\n+      Defaults to 0.001.\n+      Note that `Adadelta` tends to benefit from higher initial learning rate\n+      values compared to other optimizers.\n+      To match the exact form in the original paper, use 1.0.\n+    rho: A `Tensor` or a floating point value. The decay rate. Defaults to 0.95.\n+    epsilon: Small floating point value used to maintain numerical stability.\n+      Defaults to 1e-7.\n+    clipnorm: float. If set, the gradient of each weight is individually\n+      clipped so that its norm is no higher than this value.\n+    clipvalue: float. If set, the gradient of each weight is clipped to be\n+      no higher than this value.\n+    global_clipnorm: float. If set, the gradient of all weights is clipped\n+      so that their global norm is no higher than this value.\n+    name: Optional name prefix for the operations created when applying\n+      gradients.  Defaults to `\"Adadelta\"`.\n+\n+  Reference:\n+    - [Zeiler, 2012](http://arxiv.org/abs/1212.5701)\n+  \"\"\"\n+\n+  def __init__(self,\n+               learning_rate=0.001,\n+               rho=0.95,\n+               epsilon=1e-7,\n+               clipnorm=None,\n+               clipvalue=None,\n+               global_clipnorm=None,\n+               name='Adadelta'):\n+    super(Adadelta, self).__init__(\n+        clipnorm=clipnorm,\n+        clipvalue=clipvalue,\n+        global_clipnorm=global_clipnorm,\n+        name=name)\n+    self._learning_rate = self._build_learning_rate(learning_rate)\n+    self.rho = rho\n+    self.epsilon = epsilon\n+\n+  def build(self, var_list):\n+    super().build(var_list)\n+    if hasattr(self, '_built') and self._built:\n+      return\n+    self._built = True\n+    self._accumulated_grads = []\n+    self._accumulated_delta_vars = []\n+    for var in var_list:\n+      self._accumulated_grads.append(\n+          self.add_variable_from_reference(var, 'accumulated_grad'))\n+      self._accumulated_delta_vars.append(\n+          self.add_variable_from_reference(var, 'accumulated_delta_var'))\n+\n+  def update_step(self, grad, variable):\n+    \"\"\"Update step given gradient and the associated model variable.\"\"\"\n+    if self._var_key(variable) not in self._index_dict:\n+      raise KeyError(f'Optimizer cannot recognize variable {variable.name}, '\n+                     f'this usually means you are calling an optimizer '\n+                     f'previously used on a different model. Please try '\n+                     f'creating a new optimizer instance.')\n+    lr = tf.cast(self.learning_rate, variable.dtype)\n+\n+    var_key = self._var_key(variable)\n+    rho = self.rho\n+    accumulated_grad = self._accumulated_grads[self._index_dict[var_key]]\n+    accumulated_delta_var = self._accumulated_delta_vars[\n+        self._index_dict[var_key]]\n+\n+    def rms(x):\n+      return tf.sqrt(x + self.epsilon)\n+\n+    if isinstance(grad, tf.IndexedSlices):\n+      # Sparse gradients.\n+      accumulated_grad.assign_add((rho - 1) * accumulated_grad)\n+      accumulated_grad.scatter_add(tf.IndexedSlices(\n+          (1 - rho) * tf.square(grad.values), grad.indices))\n+      delta_var = -rms(accumulated_delta_var) * grad / rms(accumulated_grad)\n+      accumulated_delta_var.assign(rho * accumulated_delta_var +\n+                                   (1 - rho) * delta_var * delta_var)\n+    else:\n+      # Dense gradients.\n+      accumulated_grad.assign(rho * accumulated_grad + (1 - rho) * grad * grad)\n+      delta_var = -rms(accumulated_delta_var) * grad / rms(accumulated_grad)\n+      accumulated_delta_var.assign(rho * accumulated_delta_var +\n+                                   (1 - rho) * delta_var * delta_var)\n+    variable.assign_add(lr * delta_var)\n+\n+  def get_config(self):\n+    config = super(Adadelta, self).get_config()\n+\n+    config.update({\n+        'learning_rate': self._serialize_hyperparameter(self._learning_rate),\n+        'rho': self.rho,\n+        'epsilon': self.epsilon,\n+    })\n+    return config\n\n@@ -0,0 +1,115 @@\n+# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Adagrad optimizer implementation.\"\"\"\n+\n+from keras import initializers\n+from keras.optimizer_experimental import optimizer\n+import tensorflow.compat.v2 as tf\n+\n+\n+class Adagrad(optimizer.Optimizer):\n+  r\"\"\"Optimizer that implements the Adagrad algorithm.\n+\n+  Adagrad is an optimizer with parameter-specific learning rates,\n+  which are adapted relative to how frequently a parameter gets\n+  updated during training. The more updates a parameter receives,\n+  the smaller the updates.\n+\n+  Attributes:\n+    learning_rate: Initial value for the learning rate:\n+      either a floating point value,\n+      or a `tf.keras.optimizers.schedules.LearningRateSchedule` instance.\n+      Defaults to 0.001.\n+      Note that `Adagrad` tends to benefit from higher initial learning rate\n+      values compared to other optimizers.\n+      To match the exact form in the original paper, use 1.0.\n+    initial_accumulator_value: Floating point value.\n+      Starting value for the accumulators (per-parameter momentum values).\n+      Must be non-negative.\n+    epsilon: Small floating point value used to maintain numerical stability.\n+    clipnorm: float. If set, the gradient of each weight is individually\n+      clipped so that its norm is no higher than this value.\n+    clipvalue: float. If set, the gradient of each weight is clipped to be\n+      no higher than this value.\n+    global_clipnorm: float. If set, the gradient of all weights is clipped\n+      so that their global norm is no higher than this value.\n+    name: Optional name prefix for the operations created when applying\n+      gradients.  Defaults to `\"Adagrad\"`.\n+\n+  Reference:\n+    - [Duchi et al., 2011](\n+      http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf).\n+  \"\"\"\n+\n+  def __init__(self,\n+               learning_rate=0.001,\n+               initial_accumulator_value=0.1,\n+               epsilon=1e-7,\n+               clipnorm=None,\n+               clipvalue=None,\n+               global_clipnorm=None,\n+               name='Adagrad'):\n+    super(Adagrad, self).__init__(\n+        clipnorm=clipnorm,\n+        clipvalue=clipvalue,\n+        global_clipnorm=global_clipnorm,\n+        name=name)\n+    self._learning_rate = self._build_learning_rate(learning_rate)\n+    self.initial_accumulator_value = initial_accumulator_value\n+    self.epsilon = 1e-7\n+\n+  def build(self, var_list):\n+    super().build(var_list)\n+    if hasattr(self, '_built') and self._built:\n+      return\n+    self._built = True\n+    self._accumulators = []\n+    initializer = initializers.Constant(self.initial_accumulator_value)\n+    for var in var_list:\n+      self._accumulators.append(\n+          self.add_variable_from_reference(\n+              var, 'accumulator', initializer(shape=var.shape,\n+                                              dtype=var.dtype)))\n+\n+  def update_step(self, grad, variable, params=None):\n+    \"\"\"Update step given gradient and the associated model variable.\"\"\"\n+    if self._var_key(variable) not in self._index_dict:\n+      raise KeyError(f'Optimizer cannot recognize variable {variable.name}, '\n+                     f'this usually means you are calling an optimizer '\n+                     f'previously used on a different model. Please try '\n+                     f'creating a new optimizer instance.')\n+    lr = tf.cast(self.learning_rate, variable.dtype)\n+\n+    var_key = self._var_key(variable)\n+    accumulator = self._accumulators[self._index_dict[var_key]]\n+\n+    if isinstance(grad, tf.IndexedSlices):\n+      # Sparse gradients.\n+      accumulator.scatter_add(\n+          tf.IndexedSlices(grad.values * grad.values, grad.indices))\n+    else:\n+      # Dense gradients.\n+      accumulator.assign_add(grad * grad)\n+    variable.assign_sub(lr * grad / tf.sqrt(accumulator + self.epsilon))\n+\n+  def get_config(self):\n+    config = super(Adagrad, self).get_config()\n+\n+    config.update({\n+        'learning_rate': self._serialize_hyperparameter(self._learning_rate),\n+        'initial_accumulator_value': self.initial_accumulator_value,\n+        'epsilon': self.epsilon,\n+    })\n+    return config\n\n@@ -48,14 +48,14 @@ class Adam(optimizer.Optimizer):\n       1e-7.\n     amsgrad: Boolean. Whether to apply AMSGrad variant of this algorithm from\n       the paper \"On the Convergence of Adam and beyond\". Defaults to `False`.\n-    name: Optional name for the operations created when applying gradients.\n-      Defaults to `\"Adam\"`.\n     clipnorm: float. If set, the gradient of each weight is individually\n       clipped so that its norm is no higher than this value.\n     clipvalue: float. If set, the gradient of each weight is clipped to be\n       no higher than this value.\n     global_clipnorm: float. If set, the gradient of all weights is clipped\n       so that their global norm is no higher than this value.\n+    name: Optional name for the operations created when applying gradients.\n+      Defaults to `\"Adam\"`.\n \n   Reference:\n     - [Kingma et al., 2014](http://arxiv.org/abs/1412.6980)\n@@ -112,8 +112,9 @@ class Adam(optimizer.Optimizer):\n       var_list: list of model variables to build Adam variables on.\n     \"\"\"\n     super().build(var_list)\n-    if hasattr(self, '_momentums'):\n+    if hasattr(self, '_built') and self._built:\n       return\n+    self._built = True\n     self._momentums = []\n     self._velocities = []\n     for var in var_list:\n@@ -133,7 +134,10 @@ class Adam(optimizer.Optimizer):\n   def update_step(self, gradient, variable):\n     \"\"\"Update step given gradient and the associated model variable.\"\"\"\n     if self._var_key(variable) not in self._index_dict:\n-      return\n+      raise KeyError(f'Optimizer cannot recognize variable {variable.name}, '\n+                     f'this usually means you are calling an optimizer '\n+                     f'previously used on a different model. Please try '\n+                     f'creating a new optimizer instance.')\n     beta_1_power = None\n     beta_2_power = None\n     lr = tf.cast(self.learning_rate, variable.dtype)\n@@ -141,10 +145,6 @@ class Adam(optimizer.Optimizer):\n     beta_1_power = tf.pow(tf.cast(self.beta_1, variable.dtype), local_step)\n     beta_2_power = tf.pow(tf.cast(self.beta_2, variable.dtype), local_step)\n \n-    if gradient is None:\n-      # Skip updating if gradient is none.\n-      return\n-\n     var_key = self._var_key(variable)\n     m = self._momentums[self._index_dict[var_key]]\n     v = self._velocities[self._index_dict[var_key]]\n@@ -167,7 +167,7 @@ class Adam(optimizer.Optimizer):\n         v = v_hat\n       variable.assign_sub((m * alpha) / (tf.sqrt(v) + self.epsilon))\n     else:\n-      # Dense gradients\n+      # Dense gradients.\n       m.assign_add((gradient - m) * (1 - self.beta_1))\n       v.assign_add((tf.square(gradient) - v) * (1 - self.beta_2))\n       if self.amsgrad:\n\n@@ -274,6 +274,7 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n     Raises:\n       TypeError: If `grads_and_vars` is malformed.\n     \"\"\"\n+    grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)\n     grads, trainable_variables = zip(*grads_and_vars)\n     scope_name = self._name or \"optimizer\"\n     with tf.name_scope(scope_name):\n@@ -324,7 +325,7 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n     if hasattr(self, \"_clipnorm\"):\n       config[\"clipnorm\"] = self._clipnorm\n     if hasattr(self, \"_global_clipnorm\"):\n-      config[\"clipnorm\"] = self._global_clipnorm\n+      config[\"global_clipnorm\"] = self._global_clipnorm\n     if hasattr(self, \"_clipvalue\"):\n       config[\"clipvalue\"] = self._clipvalue\n     return config\n\n@@ -7,7 +7,11 @@ import os\n \n from absl.testing import parameterized\n import keras\n+from keras.optimizer_experimental import adadelta as adadelta_new\n+from keras.optimizer_experimental import adagrad as adagrad_new\n from keras.optimizer_experimental import adam as adam_new\n+from keras.optimizer_v2 import adadelta as adadelta_old\n+from keras.optimizer_v2 import adagrad as adagrad_old\n from keras.optimizer_v2 import adam as adam_old\n from keras.optimizer_v2 import learning_rate_schedule\n from keras.utils import losses_utils\n@@ -28,15 +32,21 @@ STRATEGIES = [\n     ds_combinations.central_storage_strategy_with_two_gpus,\n ]\n \n+adadelta_new_fn = tf.__internal__.test.combinations.NamedObject(\n+    \"experimentaladadelta\", lambda: adadelta_new.Adadelta(0.002))\n+adagrad_new_fn = tf.__internal__.test.combinations.NamedObject(\n+    \"experimentaladagrad\", lambda: adagrad_new.Adagrad(0.002))\n adam_new_fn = tf.__internal__.test.combinations.NamedObject(\n-    \"AdamExperimental\", adam_new.Adam)\n+    \"experimentaladam\", lambda: adam_new.Adam(0.002))\n \n OPTIMIZER_FN = [\n+    adadelta_new_fn,\n+    adagrad_new_fn,\n     adam_new_fn,\n ]\n \n \n-class OptimizerFuntionalityTest(tf.test.TestCase):\n+class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n   \"\"\"Test the functionality of optimizer.\"\"\"\n \n   def testAddVariableFromReference(self):\n@@ -106,6 +116,7 @@ class OptimizerFuntionalityTest(tf.test.TestCase):\n             \"epsilon\": 0.001,\n             \"amsgrad\": True,\n             \"clipnorm\": None,\n+            \"global_clipnorm\": None,\n             \"clipvalue\": None,\n         })\n \n@@ -138,12 +149,13 @@ class OptimizerFuntionalityTest(tf.test.TestCase):\n         self.evaluate(optimizer_1._iterations),\n         self.evaluate(optimizer_2._iterations))\n \n-  def testSaveAndLoadOptimizerWithModel(self):\n+  @parameterized.product(optimizer_fn=OPTIMIZER_FN)\n+  def testSaveAndLoadOptimizerWithModel(self, optimizer_fn):\n     model = keras.Sequential(\n         [keras.layers.Input(shape=(1,)),\n          keras.layers.Dense(1)])\n-    optimizer = adam_new.Adam(\n-        learning_rate=0.02, beta_1=0.8, beta_2=0.888)\n+    optimizer = optimizer_fn()\n+    optimizer._clipnorm = 0.1\n     x = tf.expand_dims(tf.convert_to_tensor([1, 1, 1, 0, 0, 0]), axis=1)\n     y = tf.expand_dims(tf.convert_to_tensor([1, 1, 1, 0, 0, 0]), axis=1)\n     model.compile(loss=\"mse\", optimizer=optimizer)\n@@ -152,7 +164,9 @@ class OptimizerFuntionalityTest(tf.test.TestCase):\n     model.save(path)\n     loaded_model = keras.models.load_model(path)\n     loaded_model.load_weights(path)\n-    self.assertEqual(loaded_model.optimizer.learning_rate, 0.02)\n+    self.assertEqual(loaded_model.optimizer.learning_rate, 0.002)\n+    self.assertEqual(loaded_model.optimizer._clipnorm, 0.1)\n+    model.fit(x, y)\n \n \n class OptimizerRegressionTest(tf.test.TestCase, parameterized.TestCase):\n@@ -181,6 +195,14 @@ class OptimizerRegressionTest(tf.test.TestCase, parameterized.TestCase):\n     self._compare_numerical(\n         adam_old.Adam(amsgrad=True), adam_new.Adam(amsgrad=True))\n \n+  def testAdadelta(self):\n+    self._compare_numerical(\n+        adadelta_old.Adadelta(), adadelta_new.Adadelta())\n+\n+  def testAdagrad(self):\n+    self._compare_numerical(\n+        adagrad_old.Adagrad(), adagrad_new.Adagrad())\n+\n \n class DistributedTrainingTest(tf.test.TestCase, parameterized.TestCase):\n \n@@ -198,8 +220,17 @@ class DistributedTrainingTest(tf.test.TestCase, parameterized.TestCase):\n       y = tf.expand_dims(tf.convert_to_tensor([1, 1, 1, 0, 0, 0]), axis=1)\n       model.compile(loss=\"mse\", optimizer=optimizer)\n     model.fit(x, y, epochs=1, steps_per_epoch=5)\n+    if optimizer._name == \"Adam\":\n       # Assert the momentum variable is not 0.\n       self.assertNotEqual(self.evaluate(optimizer._momentums._storage[0]), 0)\n+    elif optimizer._name == \"Adadelta\":\n+      # Assert the accumulated variable is not 0.\n+      self.assertNotEqual(\n+          self.evaluate(optimizer._accumulated_grads._storage[0]), 0)\n+    elif optimizer._name == \"Adagrad\":\n+      # Assert the accumulated variable is not 0.\n+      self.assertNotEqual(\n+          self.evaluate(optimizer._accumulators._storage[0]), 0)\n \n   @ds_combinations.generate(\n       tf.__internal__.test.combinations.combine(\n\n@@ -22,6 +22,8 @@ import tensorflow.compat.v2 as tf\n \n from keras import backend\n from keras.optimizer_experimental import optimizer as optimizer_experimental\n+from keras.optimizer_experimental import adadelta as adadelta_experimental\n+from keras.optimizer_experimental import adagrad as adagrad_experimental\n from keras.optimizer_experimental import adam as adam_experimental\n from keras.optimizer_v1 import Optimizer\n from keras.optimizer_v1 import TFOptimizer\n@@ -80,6 +82,8 @@ def deserialize(config, custom_objects=None):\n       'adagrad': adagrad_v2.Adagrad,\n       'adam': adam_v2.Adam,\n       'adamax': adamax_v2.Adamax,\n+      'experimentaladadelta': adadelta_experimental.Adadelta,\n+      'experimentaladagrad': adagrad_experimental.Adagrad,\n       'experimentaladam': adam_experimental.Adam,\n       'nadam': nadam_v2.Nadam,\n       'rmsprop': rmsprop_v2.RMSprop,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3f66015fc48136d665f6d36cb966eaff2b8efc68", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 2101 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -938,8 +938,10 @@ def EfficientNetV2(\n   x = img_input\n \n   if include_preprocessing:\n-    # V1 preprocessing for Bx variants\n-    if model_name.split('-')[-1].startswith('b'):\n+    # Apply original V1 preprocessing for Bx variants\n+    # if number of channels allows it\n+    num_channels = input_shape[bn_axis-1]\n+    if model_name.split('-')[-1].startswith('b') and num_channels == 3:\n       x = layers.Rescaling(scale=1./255)(x)\n       x = layers.Normalization(\n         mean=[0.485, 0.456, 0.406],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dd58ff02605cbdf285cfb98bbd446128abbbf134", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 300 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -209,8 +209,9 @@ def load_model(filepath, custom_objects=None, compile=True, options=None):  # py\n             if h5py is None:\n               raise ImportError(\n                   'Filepath looks like a hdf5 file but h5py is not available.'\n-                  f' filepath={filepath}')\n-            return hdf5_format.load_model_from_hdf5(filepath, custom_objects,\n+                  f' filepath={filepath_str}')\n+            return hdf5_format.load_model_from_hdf5(\n+                tf.io.gfile.GFile(filepath_str, mode='rb'), custom_objects,\n                 compile)\n         elif h5py is not None and isinstance(filepath, h5py.File):\n           return hdf5_format.load_model_from_hdf5(filepath, custom_objects,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e63b89c8dddb2a51099c146b0732967a0149fa25", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 55 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -15,18 +15,19 @@\n # pylint: disable=g-classes-have-attributes\n \"\"\"Tests for layers.__init__.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n from keras import layers\n+import tensorflow.compat.v2 as tf\n \n \n class LayersTest(tf.test.TestCase):\n \n   def test_keras_private_symbol(self):\n+    normalization_parent = layers.BatchNormalization.__module__.split('.')[-1]\n     if tf.__internal__.tf2.enabled():\n-      normalization_parent = layers.Normalization.__module__.split('.')[-1]\n-      self.assertEqual('normalization', normalization_parent)\n+      self.assertEqual('batch_normalization', normalization_parent)\n       self.assertTrue(layers.BatchNormalization._USE_V2_BEHAVIOR)\n     else:\n+      self.assertEqual('batch_normalization_v1', normalization_parent)\n       self.assertFalse(layers.BatchNormalization._USE_V2_BEHAVIOR)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9559fb5302f437bce83b6d9092602b407f155193", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 92 | Lines Deleted: 105 | Files Changed: 8 | Hunks: 23 | Methods Changed: 18 | Complexity Δ (Sum/Max): 3/4 | Churn Δ: 197 | Churn Cumulative: 13809 | Contributors (this commit): 11 | Commits (past 90d): 53 | Contributors (cumulative): 38 | DMM Complexity: 0.0\n\nDIFF:\n@@ -528,7 +528,7 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n           \"Cannot adapt {} layer after setting a static vocabulary via init \"\n           \"argument or `set_vocabulary`.\".format(self.__class__.__name__))\n \n-    data = self._standardize_inputs(data, self.vocabulary_dtype)\n+    data = utils.ensure_tensor(data, dtype=self.vocabulary_dtype)\n     if data.shape.rank == 0:\n       data = tf.expand_dims(data, 0)\n     if data.shape.rank == 1:\n@@ -612,7 +612,7 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n   def call(self, inputs):\n     self._maybe_freeze_vocab_size()\n \n-    inputs = self._standardize_inputs(inputs, self._key_dtype)\n+    inputs = utils.ensure_tensor(inputs, dtype=self._key_dtype)\n     original_shape = inputs.shape\n     # Some ops will not handle scalar input, so uprank to rank 1.\n     if inputs.shape.rank == 0:\n@@ -722,13 +722,6 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n           value_index_offset=self._token_start_index())\n       return tf.lookup.StaticHashTable(initializer, self._default_value)\n \n-  def _standardize_inputs(self, inputs, dtype):\n-    if not isinstance(inputs, (tf.Tensor, tf.RaggedTensor, tf.SparseTensor)):\n-      inputs = tf.convert_to_tensor(inputs, dtype)\n-    elif inputs.dtype != dtype:\n-      inputs = tf.cast(inputs, dtype)\n-    return inputs\n-\n   def _convert_to_ndarray(self, x):\n     return np.array(x) if isinstance(x, (list, tuple)) else x\n \n\n@@ -676,15 +676,6 @@ class CategoricalEncodingAdaptTest(\n     layer.adapt(batched_ds)\n \n \n-class ArrayLike:\n-\n-  def __init__(self, values):\n-    self.values = values\n-\n-  def __array__(self):\n-    return np.array(self.values)\n-\n-\n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class IndexLookupOutputTest(keras_parameterized.TestCase,\n                             preprocessing_test_utils.PreprocessingLayerTest):\n@@ -698,50 +689,40 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n       writer.close()\n     return vocab_path\n \n-  @parameterized.named_parameters(\n-      (\"array_2d\", [None],\n-       np.array([[\"earth\", \"wind\", \"and\", \"fire\"],\n-                 [\"fire\", \"and\", \"earth\", \"michigan\"]]),\n-       [[2, 3, 4, 5],\n-        [5, 4, 2, 1]]),\n-      (\"array_1d\", [],\n-       np.array([\"earth\", \"wind\", \"and\", \"fire\"]),\n-       [2, 3, 4, 5]),\n-      (\"array_0\", [],\n-       lambda: tf.constant(\"earth\"),\n-       2),\n-      (\"str\", [],\n-       \"earth\",\n-       2),\n-      (\"list\", [None],\n-       [\"earth\", \"wind\", \"and\", \"fire\"],\n-       [2, 3, 4, 5]),\n-      (\"array_like\", [None],\n-       ArrayLike([\"earth\", \"wind\", \"and\", \"fire\"]),\n-       [2, 3, 4, 5]),\n-  )  # pyformat: disable\n-  def test_int_output(self, shape, input_array, expected_output):\n-    vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n-    if callable(input_array):\n-      input_array = input_array()\n+  @parameterized.product(\n+      rank=[0, 1, 2],\n+      # Check lists, numpy arrays, tensors, and objects convertable to tensor.\n+      data_fn=[None, np.array, tf.constant, preprocessing_test_utils.ArrayLike]\n+  )\n+  def test_input_types(self, rank, data_fn):\n+    input_data = vocab = [\"earth\", \"wind\", \"and\", \"fire\"]\n+    expected_output = [2, 3, 4, 5]\n+    if rank == 0:\n+      input_data = input_data[0]\n+      expected_output = expected_output[0]\n+    elif rank == 2:\n+      input_data = [input_data]\n+      expected_output = [expected_output]\n+    if data_fn is not None:\n+      input_data = data_fn(input_data)\n+    input_shape = [] if rank == 0 else [None]\n \n     layer = index_lookup.IndexLookup(\n         max_tokens=None,\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n+        vocabulary=vocab,\n         vocabulary_dtype=tf.string)\n-    layer.set_vocabulary(vocab_data)\n-    output_dataset = layer(input_array)\n-    self.assertAllEqual(expected_output, output_dataset)\n+    output_data = layer(input_data)\n+    self.assertAllEqual(expected_output, output_data)\n \n     # Again in a keras.Model\n-    inputs = keras.Input(shape=shape, dtype=tf.string)\n+    inputs = keras.Input(shape=input_shape, dtype=tf.string)\n     outputs = layer(inputs)\n     model = keras.Model(inputs=inputs, outputs=outputs)\n-    output_dataset = model(tf.constant(input_array))\n-\n-    self.assertAllEqual(expected_output, output_dataset)\n+    output_data = model(tf.constant(input_data))\n+    self.assertAllEqual(expected_output, output_data)\n \n   def test_int_output_shape(self):\n     input_data = keras.Input(batch_size=16, shape=(4,), dtype=tf.string)\n@@ -2260,4 +2241,6 @@ class EagerExecutionDisabled(keras_parameterized.TestCase,\n \n \n if __name__ == \"__main__\":\n+  # IndexLookup is only exported as a TF2 API.\n+  tf.compat.v1.enable_v2_behavior()\n   tf.test.main()\n\n@@ -615,4 +615,6 @@ class IntegerLookupSavingTest(keras_parameterized.TestCase,\n \n \n if __name__ == \"__main__\":\n+  # IntegerLookup is only exported as a TF2 API.\n+  tf.compat.v1.enable_v2_behavior()\n   tf.test.main()\n\n@@ -19,6 +19,15 @@ import numpy as np\n import tensorflow.compat.v2 as tf\n \n \n+class ArrayLike:\n+\n+  def __init__(self, values):\n+    self.values = values\n+\n+  def __array__(self):\n+    return np.array(self.values)\n+\n+\n class PreprocessingLayerTest(tf.test.TestCase):\n   \"\"\"Base test class for preprocessing layer API validation.\"\"\"\n   # TODO(b/137303934): Consider incorporating something like this Close vs All\n\n@@ -25,6 +25,15 @@ COUNT = \"count\"\n TF_IDF = \"tf_idf\"\n \n \n+def ensure_tensor(inputs, dtype=None):\n+  \"\"\"Ensures the input is a Tensor, SparseTensor or RaggedTensor.\"\"\"\n+  if not isinstance(inputs, (tf.Tensor, tf.RaggedTensor, tf.SparseTensor)):\n+    inputs = tf.convert_to_tensor(inputs, dtype)\n+  if dtype is not None and inputs.dtype != dtype:\n+    inputs = tf.cast(inputs, dtype)\n+  return inputs\n+\n+\n def listify_tensors(x):\n   \"\"\"Convert any tensors or numpy arrays to lists for config serialization.\"\"\"\n   if tf.is_tensor(x):\n\n@@ -388,4 +388,6 @@ class StringLookupVocabularyTest(keras_parameterized.TestCase,\n       fn()\n \n if __name__ == \"__main__\":\n+  # StringLookup is only exported as a TF2 API.\n+  tf.compat.v1.enable_v2_behavior()\n   tf.test.main()\n\n@@ -436,21 +436,8 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n     \"\"\"\n     self._lookup_layer.set_vocabulary(vocabulary, idf_weights=idf_weights)\n \n-  def build(self, input_shape):\n-    # We have to use 'and not ==' here, because input_shape[1] !/== 1 can result\n-    # in None for undefined shape axes. If using 'and !=', this causes the\n-    # expression to evaluate to False instead of True if the shape is undefined;\n-    # the expression needs to evaluate to True in that case.\n-    if self._split is not None:\n-      if input_shape.ndims > 1 and not input_shape[-1] == 1:  # pylint: disable=g-comparison-negation\n-        raise RuntimeError(\n-            \"When using TextVectorization to tokenize strings, the innermost \"\n-            \"dimension of the input array must be 1, got shape \"\n-            \"{}\".format(input_shape))\n-\n-    super(TextVectorization, self).build(input_shape)\n-\n   def _preprocess(self, inputs):\n+    inputs = utils.ensure_tensor(inputs, dtype=tf.string)\n     if self._standardize == LOWER_AND_STRIP_PUNCTUATION:\n       if tf_utils.is_ragged(inputs):\n         lowercase_inputs = tf.ragged.map_flat_values(\n@@ -477,7 +464,13 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n       # If we are splitting, we validate that the 1st axis is of dimension 1 and\n       # so can be squeezed out. We do this here instead of after splitting for\n       # performance reasons - it's more expensive to squeeze a ragged tensor.\n-      if inputs.shape.ndims > 1:\n+      if inputs.shape.rank > 1:\n+        if inputs.shape[-1] != 1:\n+          raise ValueError(\n+              \"When using `TextVectorization` to tokenize strings, the input \"\n+              \"rank must be 1 or the last shape dimension must be 1. Received: \"\n+              f\"inputs.shape={inputs.shape} with rank={inputs.shape.rank}\")\n+        else:\n           inputs = tf.squeeze(inputs, axis=-1)\n       if self._split == SPLIT_ON_WHITESPACE:\n         # This treats multiple whitespaces as one whitespace, and strips leading\n\n@@ -310,6 +310,36 @@ class TextVectorizationLayerTest(keras_parameterized.TestCase,\n         adapt_data=vocab_data)\n     self.assertAllClose(expected_output, output_data)\n \n+  @parameterized.product(\n+      rank=[0, 1, 2],\n+      # Check lists, numpy arrays, tensors, and objects convertable to tensor.\n+      data_fn=[None, np.array, tf.constant, preprocessing_test_utils.ArrayLike]\n+  )\n+  def test_input_types(self, rank, data_fn):\n+    input_data = \"earth wind and fire\"\n+    expected_output = [2, 3, 4, 5]\n+    if rank == 1:\n+      input_data = [input_data]\n+      expected_output = [expected_output]\n+    elif rank == 2:\n+      input_data = [[input_data]]\n+      expected_output = [expected_output]\n+    if data_fn is not None:\n+      input_data = data_fn(input_data)\n+    input_shape = [] if rank == 0 else [1]\n+\n+    layer = text_vectorization.TextVectorization(\n+        vocabulary=[\"earth\", \"wind\", \"and\", \"fire\"])\n+    output_data = layer(input_data)\n+    self.assertAllEqual(expected_output, output_data)\n+\n+    # Again in a keras.Model\n+    inputs = keras.Input(shape=input_shape, dtype=tf.string)\n+    outputs = layer(inputs)\n+    model = keras.Model(inputs=inputs, outputs=outputs)\n+    output_data = model(tf.constant(input_data))\n+    self.assertAllEqual(expected_output, output_data)\n+\n   def test_scalar_input_int_mode_no_len_limit(self):\n     vocab_data = [\n         \"fire earth earth\", \"earth earth\", \"wind wind\", \"and wind and\"\n@@ -349,41 +379,6 @@ class TextVectorizationLayerTest(keras_parameterized.TestCase,\n     out = layer(input_data)\n     self.assertAllClose(out.numpy(), [2, 3, 4, 5, 5, 4, 2, 1, 0, 0])\n \n-  def test_list_inputs_1d(self):\n-    vocab_data = [\"two two two\", \"two three three\", \"three four four five\"]\n-    input_data = [\"two three\", \"four five\"]\n-    layer = text_vectorization.TextVectorization()\n-    layer.adapt(vocab_data)\n-    out = layer(input_data)\n-    self.assertAllClose(out.numpy(), [[2, 3], [4, 5]])\n-    layer.set_vocabulary([\"two\", \"three\", \"four\", \"five\"])\n-    out = layer(input_data)\n-    self.assertAllClose(out.numpy(), [[2, 3], [4, 5]])\n-\n-  def test_tensor_inputs(self):\n-    vocab_data = tf.constant(\n-        [\"two two two\", \"two three three\", \"three four four five\"])\n-    input_data = tf.constant([\"two three\", \"four five\"])\n-    layer = text_vectorization.TextVectorization()\n-    layer.adapt(vocab_data)\n-    out = layer(input_data)\n-    self.assertAllClose(out.numpy(), [[2, 3], [4, 5]])\n-    layer.set_vocabulary([\"two\", \"three\", \"four\", \"five\"])\n-    out = layer(input_data)\n-    self.assertAllClose(out.numpy(), [[2, 3], [4, 5]])\n-\n-  def test_list_inputs_2d(self):\n-    vocab_data = [\n-        [\"two two two\"], [\"two three three\"], [\"three four four five\"]]\n-    input_data = [[\"two three\"], [\"four five\"]]\n-    layer = text_vectorization.TextVectorization()\n-    layer.adapt(vocab_data)\n-    out = layer(input_data)\n-    self.assertAllClose(out.numpy(), [[2, 3], [4, 5]])\n-    layer.set_vocabulary([\"two\", \"three\", \"four\", \"five\"])\n-    out = layer(input_data)\n-    self.assertAllClose(out.numpy(), [[2, 3], [4, 5]])\n-\n   def test_dataset_of_single_strings(self):\n     vocab_data = [\"two two two\", \"two three three\", \"three four four five\"]\n     input_data = [\"two three\", \"four five\"]\n@@ -687,12 +682,12 @@ class TextVectorizationPreprocessingTest(\n   def test_string_splitting_with_non_1d_array_fails(self):\n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n     layer = text_vectorization.TextVectorization(\n+        vocabulary=[\"a\"],\n         max_tokens=None,\n         standardize=None,\n         split=text_vectorization.SPLIT_ON_WHITESPACE,\n         output_mode=None)\n-    with self.assertRaisesRegex(RuntimeError,\n-                                \".*tokenize strings, the innermost dime.*\"):\n+    with self.assertRaisesRegex(ValueError, \"last shape dimension must be 1\"):\n       _ = layer(input_data)\n \n   def test_string_splitting_with_non_1d_raggedarray_fails(self):\n@@ -703,8 +698,7 @@ class TextVectorizationPreprocessingTest(\n         standardize=None,\n         split=text_vectorization.SPLIT_ON_WHITESPACE,\n         output_mode=None)\n-    with self.assertRaisesRegex(RuntimeError,\n-                                \".*tokenize strings, the innermost dime.*\"):\n+    with self.assertRaisesRegex(ValueError, \"last shape dimension must be 1\"):\n       _ = layer(input_data)\n \n   def test_standardization_with_invalid_standardize_arg(self):\n@@ -1849,4 +1843,6 @@ class TextVectorizationE2ETest(keras_parameterized.TestCase,\n \n \n if __name__ == \"__main__\":\n+  # TextVectorization is only exported as a TF2 API.\n+  tf.compat.v1.enable_v2_behavior()\n   tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
