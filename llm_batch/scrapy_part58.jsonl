{"custom_id": "scrapy#51029e37a326e683bfb9aaa8551148b4daeea0f2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 19 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 20 | Churn Cumulative: 525 | Contributors (this commit): 4 | Commits (past 90d): 29 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -28,21 +28,6 @@ from scrapy.utils.misc import load_object\n from scrapy.utils.defer import mustbe_deferred\n \n class ExecutionEngine(object):\n-    \"\"\"\n-    The Execution Engine controls execution of the scraping process.\n-\n-    The process begins with the _mainloop() method, which is called\n-    periodically to add more domains to scrape. It adds the first page for a\n-    domain to the scheduler by calling _schedule_page and calls\n-    process_scheduled_requests, which starts the scraping process for a domain.\n-\n-    The process_scheduled_requests method asks the scheduler for the next\n-    available page for a given domain and requests it from the downloader. The\n-    downloader will execute a callback function that was added in the\n-    _schedule_page method. This callback with process the output from the\n-    spider and then call process_scheduled_requests to continue the scraping\n-    process for that domain.\n-    \"\"\"\n \n     def __init__(self):\n         self.configured = False\n@@ -175,16 +160,13 @@ class ExecutionEngine(object):\n             self._next_request_called.discard(domain)\n         elif domain not in self._next_request_called:\n             self._next_request_called.add(domain)\n+            return reactor.callLater(0, self.next_request, domain, now=True)\n         else:\n             return\n \n         if self.paused:\n             return reactor.callLater(5, self.next_request, domain)\n \n-        # call next_request in next reactor loop by default\n-        if not now:\n-            return reactor.callLater(0, self.next_request, domain, now=True)\n-\n         if not self.running or \\\n                 self.domain_is_closed(domain) or \\\n                 self.downloader.sites[domain].needs_backout():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#87df33ce0a0e4b65846afd682efe24a4f958c168", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 533 | Contributors (this commit): 4 | Commits (past 90d): 30 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -38,7 +38,7 @@ class ExecutionEngine(object):\n         self.running = False\n         self.paused = False\n         self.control_reactor = True\n-        self._next_request_called = set()\n+        self._next_request_pending = set()\n \n     def configure(self, scheduler=None, downloader=None):\n         \"\"\"\n@@ -157,9 +157,9 @@ class ExecutionEngine(object):\n         The domain is closed if there are no more pages to scrape.\n         \"\"\"\n         if now:\n-            self._next_request_called.discard(domain)\n-        elif domain not in self._next_request_called:\n-            self._next_request_called.add(domain)\n+            self._next_request_pending.discard(domain)\n+        elif domain not in self._next_request_pending:\n+            self._next_request_pending.add(domain)\n             return reactor.callLater(0, self.next_request, domain, now=True)\n         else:\n             return\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#830cd4f19f71a5171c1f0fd309e86e7d0b2bf157", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 4 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 760 | Contributors (this commit): 4 | Commits (past 90d): 44 | Contributors (cumulative): 7 | DMM Complexity: 0.3333333333333333\n\nDIFF:\n@@ -109,7 +109,7 @@ class Downloader(object):\n         # Process requests in queue if there are free slots to transfer for this site\n         while site.queue and site.free_transfer_slots() > 0:\n             request, deferred = site.queue.pop(0)\n-            self._download(site, request, spider).chainDeferred(deferred)\n+            self._download(site, request, spider, deferred)\n \n         self._close_if_idle(domain)\n \n@@ -118,13 +118,15 @@ class Downloader(object):\n         if site and site.closing and not site.active:\n             del self.sites[domain]\n \n-    def _download(self, site, request, spider):\n+    def _download(self, site, request, spider, deferred):\n         site.transferring.add(request)\n         def _transferred(_):\n             site.transferring.remove(request)\n             self.process_queue(spider)\n-            return _\n-        return mustbe_deferred(download_any, request, spider).addBoth(_transferred)\n+\n+        dfd = mustbe_deferred(download_any, request, spider)\n+        dfd.chainDeferred(deferred).addBoth(lambda _: deferred)\n+        dfd.addBoth(_transferred) # request next download after finish processing current response\n \n     def open_domain(self, domain):\n         \"\"\"Allocate resources to begin processing a domain\"\"\"\n\n@@ -176,7 +176,9 @@ class ExecutionEngine(object):\n         request, deferred = self.scheduler.next_request(domain)\n         if request:\n             spider = spiders.fromdomain(domain)\n-            mustbe_deferred(self.download, request, spider).chainDeferred(deferred)\n+            dwld = mustbe_deferred(self.download, request, spider)\n+            dwld.chainDeferred(deferred).addBoth(lambda _: deferred)\n+            dwld.addErrback(log.err)\n         elif self.domain_is_idle(domain):\n             self._domain_idle(domain)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8de09fe4ddd39b0fbfb7910080a66342c2e3c133", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 27 | Churn Cumulative: 250 | Contributors (this commit): 3 | Commits (past 90d): 14 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -119,14 +119,31 @@ class Downloader(object):\n             del self.sites[domain]\n \n     def _download(self, site, request, spider, deferred):\n-        site.transferring.add(request)\n-        def _transferred(_):\n-            site.transferring.remove(request)\n-            self.process_queue(spider)\n+        # The order is very important for the following deferreds. Do not\n+        # change!\n \n+        # 1. Create the download deferred\n         dfd = mustbe_deferred(download_any, request, spider)\n+\n+        # 2. After response arrives,  remove the request from transferring\n+        # state to free up the transferring slot so it can be used by the\n+        # following requests (perhaps those which came from the downloader\n+        # middleware itself)\n+        site.transferring.add(request)\n+        def finish_transferring(_):\n+            site.transferring.remove(request)\n+            return _\n+        dfd.addBoth(finish_transferring)\n+\n+        # 3. Wait until deferred returned to engine completes\n         dfd.chainDeferred(deferred).addBoth(lambda _: deferred)\n-        dfd.addBoth(_transferred) # request next download after finish processing current response\n+\n+        # 4. Finally, continue processing the next request from the queue. This\n+        # callback must be placed at the end to make sure processing is\n+        # performed first, otherwise memory could increase severely\n+        def next_request(_):\n+            self.process_queue(spider)\n+        dfd.addBoth(next_request)\n \n     def open_domain(self, domain):\n         \"\"\"Allocate resources to begin processing a domain\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c22d2b158798e2a9d9242a29e3ce14d8a4cecd91", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 7 | Churn Cumulative: 544 | Contributors (this commit): 4 | Commits (past 90d): 32 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -190,8 +190,15 @@ class ExecutionEngine(object):\n         return not (pending or downloading or haspipe or scraping)\n \n     def domain_is_closed(self, domain):\n+        \"\"\"Return True if the domain is fully closed (ie. not even in the\n+        closing stage)\"\"\"\n         return domain not in self.downloader.sites\n \n+    def domain_is_open(self, domain):\n+        \"\"\"Return True if the domain is fully opened (ie. not in closing\n+        stage)\"\"\"\n+        return domain in self.downloader.sites and domain not in self.closing\n+\n     @property\n     def open_domains(self):\n         return self.downloader.sites.keys()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7933e00ebd95b95db37781492b7ac5fade8bbcf4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 11 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -18,7 +18,7 @@ from scrapy.core.exceptions import IgnoreRequest\n from scrapy.conf import settings\n \n class RobotsTxtMiddleware(object):\n-    DOWNLOAD_PRIORITY = -1\n+    DOWNLOAD_PRIORITY = 1000\n \n     def __init__(self):\n         if not settings.getbool('ROBOTSTXT_OBEY'):\n\n@@ -10,7 +10,7 @@ from scrapy.spider import spiders\n \n \n class MediaPipeline(object):\n-    DOWNLOAD_PRIORITY = -1\n+    DOWNLOAD_PRIORITY = 1000\n \n     class DomainInfo(object):\n         def __init__(self, domain):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#18301b7e66c6ab837dd24d6f14aa25a84a432019", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 11 | Churn Cumulative: 261 | Contributors (this commit): 3 | Commits (past 90d): 15 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,7 +2,7 @@\n Download web pages using asynchronous IO\n \"\"\"\n \n-import datetime\n+from time import time\n \n from twisted.internet import reactor, defer\n \n@@ -35,7 +35,7 @@ class SiteInfo(object):\n         self.active = set()\n         self.transferring = set()\n         self.closing = False\n-        self.lastseen = None\n+        self.lastseen = 0\n \n     def free_transfer_slots(self):\n         return self.max_concurrent_requests - len(self.transferring)\n@@ -97,10 +97,9 @@ class Downloader(object):\n             return\n \n         # Delay queue processing if a download_delay is configured\n-        now = datetime.datetime.now()\n-        if site.download_delay and site.lastseen:\n-            delta = now - site.lastseen\n-            penalty = site.download_delay - delta.seconds\n+        now = time()\n+        if site.download_delay:\n+            penalty = site.download_delay - now + site.lastseen\n             if penalty > 0:\n                 reactor.callLater(penalty, self.process_queue, spider=spider)\n                 return\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#80cd534f920f51ab42756cc8ee970b8392695d8c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 41 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -51,10 +51,10 @@ def start(logfile=None, loglevel=None, log_stdout=None):\n \n def msg(message, level=INFO, component=BOT_NAME, domain=None):\n     \"\"\"Log message according to the level\"\"\"\n-    component = \"%s/%s\" % (component, domain) if domain else component\n+    system = domain if domain else component\n     if level <= log_level:\n         msg_txt = unicode_to_str(\"%s: %s\" % (level_names[level], message))\n-        log.msg(msg_txt, system=component)\n+        log.msg(msg_txt, system=system)\n \n def exc(message, level=ERROR, component=BOT_NAME, domain=None):\n     message = message + '\\n' + format_exc()\n@@ -63,5 +63,5 @@ def exc(message, level=ERROR, component=BOT_NAME, domain=None):\n def err(*args, **kwargs):\n     domain = kwargs.pop('domain', None)\n     component = kwargs.pop('component', BOT_NAME)\n-    kwargs['system'] = \"%s/%s\" % (component, domain) if domain else component\n+    kwargs['system'] = domain if domain else component\n     log.err(*args, **kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#af6db1691eb5cb8bf1b18475d4a6f7a3fe70010f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 48 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -7,6 +7,7 @@ import sys\n from traceback import format_exc\n \n from twisted.python import log\n+from pydispatch import dispatcher\n \n from scrapy.conf import settings\n from scrapy.utils.python import unicode_to_str\n@@ -25,6 +26,10 @@ level_names = {\n \n BOT_NAME = settings['BOT_NAME']\n \n+# signal sent when log message is received\n+# args: message, level, domain\n+logmessage_received = object()\n+\n # default logging level\n log_level = DEBUG\n \n@@ -51,6 +56,8 @@ def start(logfile=None, loglevel=None, log_stdout=None):\n \n def msg(message, level=INFO, component=BOT_NAME, domain=None):\n     \"\"\"Log message according to the level\"\"\"\n+    dispatcher.send(signal=logmessage_received, message=message, level=level, \\\n+        domain=domain)\n     system = domain if domain else component\n     if level <= log_level:\n         msg_txt = unicode_to_str(\"%s: %s\" % (level_names[level], message))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0c4c1538196b8a7c5c820f47b866fe2e0f53f922", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 10 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -11,7 +11,7 @@\n # All configuration values have a default; values that are commented out\n # serve to show the default.\n \n-import sys, os\n+import sys\n from os import path\n \n # If your extensions are in another directory, add it here. If the directory\n@@ -29,7 +29,7 @@ sys.path.append(path.join(path.dirname(path.dirname(__file__)), \"scrapy\"))\n extensions = ['scrapydocs', 'sphinx.ext.autodoc']\n \n # Add any paths that contain templates here, relative to this directory.\n-templates_path = ['../templates/docs']\n+templates_path = ['_templates']\n \n # The suffix of source filenames.\n source_suffix = '.rst'\n@@ -38,7 +38,7 @@ source_suffix = '.rst'\n #source_encoding = 'utf-8'\n \n # The master toctree document.\n-master_doc = 'index'\n+master_doc = 'contents'\n \n # General information about the project.\n project = u'Scrapy'\n@@ -130,7 +130,9 @@ html_use_smartypants = True\n \n # Additional templates that should be rendered to pages, maps page names to\n # template names.\n-#html_additional_pages = {}\n+html_additional_pages = {\n+    'index': 'index.html'\n+}\n \n # If false, no module index is generated.\n #html_use_modindex = True\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f467c233b2bab852eb10e8d0dd01ca7c3ee68b6f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 15 | Files Changed: 1 | Hunks: 4 | Methods Changed: 5 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 20 | Churn Cumulative: 281 | Contributors (this commit): 3 | Commits (past 90d): 16 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -108,7 +108,7 @@ class Downloader(object):\n         # Process requests in queue if there are free slots to transfer for this site\n         while site.queue and site.free_transfer_slots() > 0:\n             request, deferred = site.queue.pop(0)\n-            self._download(site, request, spider, deferred)\n+            self._download(site, request, spider).chainDeferred(deferred)\n \n         self._close_if_idle(domain)\n \n@@ -117,9 +117,8 @@ class Downloader(object):\n         if site and site.closing and not site.active:\n             del self.sites[domain]\n \n-    def _download(self, site, request, spider, deferred):\n-        # The order is very important for the following deferreds. Do not\n-        # change!\n+    def _download(self, site, request, spider):\n+        # The order is very important for the following deferreds. Do not change!\n \n         # 1. Create the download deferred\n         dfd = mustbe_deferred(download_any, request, spider)\n@@ -131,18 +130,9 @@ class Downloader(object):\n         site.transferring.add(request)\n         def finish_transferring(_):\n             site.transferring.remove(request)\n-            return _\n-        dfd.addBoth(finish_transferring)\n-\n-        # 3. Wait until deferred returned to engine completes\n-        dfd.chainDeferred(deferred).addBoth(lambda _: deferred)\n-\n-        # 4. Finally, continue processing the next request from the queue. This\n-        # callback must be placed at the end to make sure processing is\n-        # performed first, otherwise memory could increase severely\n-        def next_request(_):\n             self.process_queue(spider)\n-        dfd.addBoth(next_request)\n+            return _\n+        return dfd.addBoth(finish_transferring)\n \n     def open_domain(self, domain):\n         \"\"\"Allocate resources to begin processing a domain\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2ce43ebbec5b16ea3d56ece8c6e34e2b47de8c22", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 25 | Files Changed: 3 | Hunks: 22 | Methods Changed: 6 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 61 | Churn Cumulative: 196 | Contributors (this commit): 4 | Commits (past 90d): 19 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -19,6 +19,8 @@ class DownloaderMiddlewareManager(object):\n \n     def __init__(self):\n         self.loaded = False\n+        self.enabled = {}\n+        self.disabled = {}\n         self.request_middleware = []\n         self.response_middleware = []\n         self.exception_middleware = []\n@@ -33,21 +35,22 @@ class DownloaderMiddlewareManager(object):\n             self.exception_middleware.insert(0, mw.process_exception)\n \n     def load(self):\n-        \"\"\"Load middleware defined in settings module\n-        \"\"\"\n-        mws = []\n+        \"\"\"Load middleware defined in settings module\"\"\"\n         mwlist = build_middleware_list(settings['DOWNLOADER_MIDDLEWARES_BASE'],\n                                        settings['DOWNLOADER_MIDDLEWARES'])\n+        self.enabled.clear()\n+        self.disabled.clear()\n         for mwpath in mwlist:\n-            cls = load_object(mwpath)\n-            if cls:\n             try:\n+                cls = load_object(mwpath)\n                 mw = cls()\n+                self.enabled[cls.__name__] = mw\n                 self._add_middleware(mw)\n-                    mws.append(mw)\n-                except NotConfigured:\n-                    pass\n-        log.msg(\"Enabled downloader middlewares: %s\" % \", \".join([type(m).__name__ for m in mws]),\n+            except NotConfigured, e:\n+                self.disabled[cls.__name__] = mwpath\n+                if e.args:\n+                    log.msg(e)\n+        log.msg(\"Enabled downloader middlewares: %s\" % \", \".join(self.enabled.keys()), \\\n             level=log.DEBUG)\n         self.loaded = True\n \n\n@@ -19,27 +19,31 @@ from scrapy.conf import settings\n class SchedulerMiddlewareManager(object):\n \n     def __init__(self):\n+        self.loaded = False\n+        self.enabled = {}\n+        self.disabled = {}\n         self.mw_cbs = defaultdict(list)\n         self.load()\n \n     def load(self):\n         \"\"\"Load middleware defined in settings module\"\"\"\n-        self.mws = []\n         mwlist = build_middleware_list(settings['SCHEDULER_MIDDLEWARES_BASE'],\n                                        settings['SCHEDULER_MIDDLEWARES'])\n+        self.enabled.clear()\n+        self.disabled.clear()\n         for mwpath in mwlist:\n-            cls = load_object(mwpath)\n-            if cls:\n             try:\n+                cls = load_object(mwpath)\n                 mw = cls()\n-                except NotConfigured:\n-                    pass\n-                else:\n+                self.enabled[cls.__name__] = mw\n                 self._add_middleware(mw)\n-                    self.mws.append(mw)\n-\n-        log.msg(\"Enabled scheduler middlewares: %s\" % \", \".join([type(m).__name__ for m in self.mws]),\n+            except NotConfigured, e:\n+                self.disabled[cls.__name__] = mwpath\n+                if e.args:\n+                    log.msg(e)\n+        log.msg(\"Enabled scheduler middlewares: %s\" % \", \".join(self.enabled.keys()), \\\n             level=log.DEBUG)\n+        self.loaded = True\n \n     def _add_middleware(self, mw):\n         for name in ('enqueue_request', 'open_domain', 'close_domain'):\n\n@@ -20,6 +20,8 @@ def _isiterable(possible_iterator):\n class SpiderMiddlewareManager(object):\n     def __init__(self):\n         self.loaded = False\n+        self.enabled = {}\n+        self.disabled = {}\n         self.spider_middleware = []\n         self.result_middleware = []\n         self.exception_middleware = []\n@@ -35,19 +37,21 @@ class SpiderMiddlewareManager(object):\n \n     def load(self):\n         \"\"\"Load middleware defined in settings module\"\"\"\n-        mws = []\n         mwlist = build_middleware_list(settings['SPIDER_MIDDLEWARES_BASE'],\n                                        settings['SPIDER_MIDDLEWARES'])\n+        self.enabled.clear()\n+        self.disabled.clear()\n         for mwpath in mwlist:\n-            cls = load_object(mwpath)\n-            if cls:\n             try:\n+                cls = load_object(mwpath)\n                 mw = cls()\n+                self.enabled[cls.__name__] = mw\n                 self._add_middleware(mw)\n-                    mws.append(mw)\n-                except NotConfigured:\n-                    pass\n-        log.msg(\"Enabled spider middlewares: %s\" % \", \".join([type(m).__name__ for m in mws]),\n+            except NotConfigured, e:\n+                self.disabled[cls.__name__] = mwpath\n+                if e.args:\n+                    log.msg(e)\n+        log.msg(\"Enabled spider middlewares: %s\" % \", \".join(self.enabled.keys()), \\\n             level=log.DEBUG)\n         self.loaded = True\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2e520058473f3e7e860e035552d73c5aa3013e3d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 71 | Lines Deleted: 43 | Files Changed: 3 | Hunks: 3 | Methods Changed: 8 | Complexity Δ (Sum/Max): 11/11 | Churn Δ: 114 | Churn Cumulative: 227 | Contributors (this commit): 3 | Commits (past 90d): 20 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -175,7 +175,7 @@ SPIDER_MIDDLEWARES_BASE = {\n     'scrapy.contrib.spidermiddleware.itempipeline.ItemPipelineMiddleware': 30,\n     'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n     'scrapy.contrib.itemsampler.ItemSamplerMiddleware': 100,\n-    'scrapy.contrib.spidermiddleware.limit.RequestLimitMiddleware': 200,\n+    'scrapy.contrib.spidermiddleware.requestlimit.RequestLimitMiddleware': 200,\n     'scrapy.contrib.spidermiddleware.restrict.RestrictMiddleware': 300,\n     'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n     'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n\n@@ -1,42 +0,0 @@\n-\"\"\"\n-RequestLimitMiddleware: Limits the scheduler request queue size. When spiders\n-try to schedule more than the allowed amount of requests the new requests\n-(returned by the spider) will be dropped.\n-\n-The limit can be set using the spider attribue `requests_queue_size` or the\n-setting \"REQUESTS_QUEUE_SIZE\". If not specified (or 0), no limit will be\n-applied. \n-\"\"\"\n-\n-from scrapy.core.engine import scrapyengine\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.conf import settings\n-from scrapy.http import Request\n-from scrapy import log\n-\n-class RequestLimitMiddleware(object):\n-\n-    def __init__(self):\n-        self.max_queue_size = settings.getint(\"REQUESTS_QUEUE_SIZE\")\n-        if not self.max_queue_size:\n-            raise NotConfigured\n-\n-    def process_spider_output(self, response, result, spider):\n-        requests = []\n-        items = []\n-        for r in result:\n-            if isinstance(r, Request):\n-                requests.append(r)\n-            else:\n-                items.append(r)\n-\n-        max_pending = getattr(spider, 'requests_queue_size', self.max_queue_size)\n-        if max_pending:\n-            pending_count = len(scrapyengine.scheduler.pending_requests.get(spider.domain_name, []))\n-            free_slots = max_pending - pending_count\n-            dropped_count = len(requests) - free_slots\n-            if dropped_count > 0:\n-                requests = requests[:free_slots]\n-                log.msg(\"Dropping %d request(s) because the maximum schedule size (%d) has been exceeded\" % \\\n-                        (dropped_count, max_pending), level=log.DEBUG, domain=spider.domain_name)\n-        return requests + items\n\n@@ -0,0 +1,70 @@\n+\"\"\"\n+RequestLimitMiddleware: Limits the scheduler request queue size. When spiders\n+try to schedule more than the allowed amount of requests the new requests\n+(returned by the spider) will be dropped.\n+\n+The limit can be set using the spider attribue `requests_queue_size` or the\n+setting \"REQUESTS_QUEUE_SIZE\". If not specified (or 0), no limit will be\n+applied. \n+\"\"\"\n+from itertools import imap\n+from pydispatch import dispatcher\n+\n+from scrapy.core import signals\n+from scrapy.core.engine import scrapyengine\n+from scrapy.core.exceptions import NotConfigured\n+from scrapy.conf import settings\n+from scrapy.http import Request\n+from scrapy import log\n+\n+class RequestLimitMiddleware(object):\n+\n+    def __init__(self):\n+        self.max_queue_size = settings.getint(\"REQUESTS_QUEUE_SIZE\")\n+        if not self.max_queue_size:\n+            raise NotConfigured\n+\n+        self.max_pending = {}\n+        self.dropped_count = {}\n+\n+        dispatcher.connect(self.domain_opened, signal=signals.domain_opened)\n+        dispatcher.connect(self.domain_closed, signal=signals.domain_closed)\n+\n+    def domain_opened(self, domain, spider):\n+        self.max_pending[domain] = getattr(spider, 'requests_queue_size', self.max_queue_size)\n+        self.dropped_count[domain] = 0\n+\n+    def domain_closed(self, domain):\n+        dropped_count = self.dropped_count[domain]\n+        if dropped_count:\n+            max_pending = self.max_pending[domain]\n+            log.msg('Dropped %d request(s) because the scheduler queue size limit (%d requests) was exceeded' % \\\n+                    (dropped_count, max_pending), level=log.DEBUG, domain=domain)\n+        del self.dropped_count[domain]\n+        del self.max_pending[domain]\n+\n+    def process_spider_output(self, response, result, spider):\n+        domain = spider.domain_name\n+        max_pending = self.max_pending.get(domain, 0)\n+        if max_pending:\n+            return imap(lambda v: self._limit_requests(v, domain, max_pending), result)\n+        else:\n+            return result\n+\n+    def _limit_requests(self, request_or_other, domain, max_pending):\n+        if isinstance(request_or_other, Request):\n+            free_slots = max_pending - self._pending_count(domain)\n+            if free_slots > 0:\n+                # Scheduler isn't saturated and it is fine to schedule more requests.\n+                return request_or_other\n+            else:\n+                # Skip the request and give engine time to handle other tasks.\n+                self.dropped_count[domain] += 1\n+                return None\n+        else:\n+            # Return others (non-requests) as is.\n+            return request_or_other\n+\n+    def _pending_count(self, domain):\n+        pending = scrapyengine.scheduler.pending_requests.get(domain, [])\n+        return len(pending)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a15dc9434091337532b037bddcef4d9b929a8230", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 5 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -94,7 +94,7 @@ class S3ImagesPipeline(BaseImagesPipeline):\n \n         \"\"\"\n         if self.AmazonS3Spider:\n-            return scrapyengine.download(request, self.AmazonS3Spider)\n+            return scrapyengine.schedule(request, self.AmazonS3Spider)\n         return self.download(request, info)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c205f7d8e5258a1248d03bde0706d46dd8d28fb5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 6 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -94,6 +94,7 @@ class S3ImagesPipeline(BaseImagesPipeline):\n \n         \"\"\"\n         if self.AmazonS3Spider:\n+            # need to use schedule to auto-open domain\n             return scrapyengine.schedule(request, self.AmazonS3Spider)\n         return self.download(request, info)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#460f690c5cc85e3ed0657b5287e8fd14b07efe6c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 36/28 | Churn Δ: 13 | Churn Cumulative: 13 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -102,6 +102,12 @@ class UrlUtilsTest(unittest.TestCase):\n                          'http://domain/test?arg1=v1;arg2=v2')\n         self.assertEqual(add_or_replace_parameter(\"http://domain/moreInfo.asp?prodID=\", 'prodID', '20'),\n                          'http://domain/moreInfo.asp?prodID=20')\n+        url = 'http://rmc-offers.co.uk/productlist.asp?BCat=2%2C60&CatID=60'\n+        self.assertEqual(add_or_replace_parameter(url, 'BCat', 'newvalue', is_quoted=True),\n+                         'http://rmc-offers.co.uk/productlist.asp?BCat=newvalue&CatID=60')\n+        url = 'http://rmc-offers.co.uk/productlist.asp?BCat=2,60&CatID=60'\n+        self.assertEqual(add_or_replace_parameter(url, 'BCat', 'newvalue'),\n+                         'http://rmc-offers.co.uk/productlist.asp?BCat=newvalue&CatID=60')\n \n     def test_url_query_cleaner(self):\n         self.assertEqual(url_query_cleaner(\"product.html?id=200&foo=bar&name=wired\", 'id'),\n\n@@ -85,7 +85,8 @@ def is_url(text):\n def url_query_parameter(url, parameter, default=None, keep_blank_values=0):\n     \"\"\"Return the value of a url parameter, given the url and parameter name\"\"\"\n     queryparams = cgi.parse_qs(urlparse.urlsplit(str(url))[3], keep_blank_values=keep_blank_values)\n-    return queryparams.get(parameter, [default])[0] \n+    result = queryparams.get(parameter, [default])[0]\n+    return result\n \n def url_query_cleaner(url, parameterlist=(), sep='&', kvsep='='):\n     \"\"\"Clean url arguments leaving only those passed in the parameterlist\"\"\"\n@@ -110,13 +111,15 @@ def url_query_cleaner(url, parameterlist=(), sep='&', kvsep='='):\n     query = sep.join([kvsep.join(pair) for pair in querylist if pair[0] in parameterlist])\n     return '?'.join([base, query])\n \n-def add_or_replace_parameter(url, name, new_value, sep='&'):\n+def add_or_replace_parameter(url, name, new_value, sep='&', url_is_quoted=False):\n     \"\"\"Add or remove a parameter to a given url\"\"\"\n     def has_querystring(url):\n         _, _, _, query, _ = urlparse.urlsplit(url)\n         return bool(query)\n \n     parameter = url_query_parameter(url, name, keep_blank_values=1)\n+    if url_is_quoted:\n+        parameter = urllib.quote(parameter)\n     if parameter is None:\n         if has_querystring(url):\n             next_url = url + sep + name + '=' + new_value\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1ba98606c226005e3205d9459e05c0e334b94e24", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 18 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -103,7 +103,7 @@ class UrlUtilsTest(unittest.TestCase):\n         self.assertEqual(add_or_replace_parameter(\"http://domain/moreInfo.asp?prodID=\", 'prodID', '20'),\n                          'http://domain/moreInfo.asp?prodID=20')\n         url = 'http://rmc-offers.co.uk/productlist.asp?BCat=2%2C60&CatID=60'\n-        self.assertEqual(add_or_replace_parameter(url, 'BCat', 'newvalue', is_quoted=True),\n+        self.assertEqual(add_or_replace_parameter(url, 'BCat', 'newvalue', url_is_quoted=True),\n                          'http://rmc-offers.co.uk/productlist.asp?BCat=newvalue&CatID=60')\n         url = 'http://rmc-offers.co.uk/productlist.asp?BCat=2,60&CatID=60'\n         self.assertEqual(add_or_replace_parameter(url, 'BCat', 'newvalue'),\n\n@@ -85,8 +85,7 @@ def is_url(text):\n def url_query_parameter(url, parameter, default=None, keep_blank_values=0):\n     \"\"\"Return the value of a url parameter, given the url and parameter name\"\"\"\n     queryparams = cgi.parse_qs(urlparse.urlsplit(str(url))[3], keep_blank_values=keep_blank_values)\n-    result = queryparams.get(parameter, [default])[0]\n-    return result\n+    return queryparams.get(parameter, [default])[0]\n \n def url_query_cleaner(url, parameterlist=(), sep='&', kvsep='='):\n     \"\"\"Clean url arguments leaving only those passed in the parameterlist\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8e5ede717945921a0302fdbefd9561a3e0f476d7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 22/22 | Churn Δ: 16 | Churn Cumulative: 16 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -34,12 +34,8 @@ class Command(ScrapyCommand):\n         \"\"\" You can use this function to update the Scrapy objects that will be available in the shell\"\"\"\n         pass\n \n-    def _get_response(self, response):\n-        self.response = response\n-        return []\n-\n     def get_url(self, url):\n-        self.response = None\n+        response = None\n \n         url = url.strip()\n         if url:\n@@ -52,15 +48,15 @@ class Command(ScrapyCommand):\n             if u.scheme not in ('http', 'https', 'file'):\n                 print \"Unsupported scheme '%s' in URL: <%s>\" % (u.scheme, url)\n                 return\n-            request = Request(url, callback=self._get_response)\n+            request = Request(url)\n         else:\n-            request = self.user_ns['request'].replace(callback=self._get_response)\n+            request = self.user_ns['request']\n \n         spider = get_or_create_spider(url)\n         print \"Fetching %s...\" % request\n-        threads.blockingCallFromThread(reactor, scrapyengine.crawl, request, spider)\n-        if self.response:\n-            self.generate_vars(url, self.response, request)\n+        response = threads.blockingCallFromThread(reactor, scrapyengine.schedule, request, spider)\n+        if response:\n+            self.generate_vars(url, response, request)\n             return True\n \n     def generate_vars(self, url=None, response=None, request=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b83851dcc3b676e98d5343c4f909e266f8fab230", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 18 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -35,8 +35,6 @@ class Command(ScrapyCommand):\n         pass\n \n     def get_url(self, url):\n-        response = None\n-\n         url = url.strip()\n         if url:\n             u = urlparse.urlparse(url)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5cbafaea7f6044ad8608617fa61e78e4650c2030", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 2 | Churn Cumulative: 2 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -9,7 +9,7 @@ import traceback\n class StackTraceDump(object):\n     def __init__(self):\n         try:\n-            signal.signal(signal.SIGUSR1, self.dump_stacktrace)\n+            signal.signal(signal.SIGUSR2, self.dump_stacktrace)\n         except AttributeError:\n             # win32 platforms don't support SIGUSR signals\n             pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#42b86a385fa17fe3f42aa029cc4f48f61456e11b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 72/72 | Churn Δ: 2 | Churn Cumulative: 2 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -3,8 +3,6 @@\n import re, os, time\n from unittest import TestCase\n \n-from test import test_support\n-\n from scrapy.http import Request, Response, Headers\n from scrapy.http.cookies import WrappedRequest, WrappedResponse, CookieJar, DefaultCookiePolicy\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8b26e49636aadcefd80103e009a1cf55d516a739", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 113 | Lines Deleted: 186 | Files Changed: 8 | Hunks: 54 | Methods Changed: 47 | Complexity Δ (Sum/Max): -13/3 | Churn Δ: 299 | Churn Cumulative: 1488 | Contributors (this commit): 4 | Commits (past 90d): 80 | Contributors (cumulative): 21 | DMM Complexity: 0.0\n\nDIFF:\n@@ -38,7 +38,9 @@ CLUSTER_WORKER_PORT = 8789\n COMMANDS_MODULE = ''\n COMMANDS_SETTINGS_MODULE = ''\n \n-CONCURRENT_DOMAINS = 8    # number of domains to scrape in parallel\n+CONCURRENT_DOMAINS = 8\n+\n+CONCURRENT_ITEMS = 100\n \n COOKIES_DEBUG = False\n \n@@ -111,6 +113,8 @@ HTTPCACHE_IGNORE_MISSING = False\n HTTPCACHE_SECTORIZE = True\n HTTPCACHE_EXPIRATION_SECS = 0\n \n+ITEM_PROCESSOR = 'scrapy.item.pipeline.ItemPipelineManager'\n+\n # Item pipelines are typically set in specific commands settings\n ITEM_PIPELINES = []\n \n@@ -172,7 +176,6 @@ SPIDER_MIDDLEWARES = {}\n \n SPIDER_MIDDLEWARES_BASE = {\n     # Engine side\n-    'scrapy.contrib.spidermiddleware.itempipeline.ItemPipelineMiddleware': 30,\n     'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n     'scrapy.contrib.itemsampler.ItemSamplerMiddleware': 100,\n     'scrapy.contrib.spidermiddleware.requestlimit.RequestLimitMiddleware': 200,\n\n@@ -1,75 +0,0 @@\n-\"\"\"\n-ItemPipelineMiddleware: feed item pipeline with scraped items\n-\"\"\"\n-from pydispatch import dispatcher\n-\n-from twisted.python.failure import Failure\n-\n-from scrapy.core import signals\n-from scrapy.core.exceptions import DontCloseDomain\n-from scrapy.item.pipeline import ItemPipelineManager\n-from scrapy.item import ScrapedItem\n-from scrapy.conf import settings\n-from scrapy import log\n-\n-class ItemPipelineMiddleware(object):\n-    \"\"\"SpiderMiddleware that sends items through a pipeline\"\"\"\n-\n-    # The type of items to process by pipeline \n-    ScrapedItem = ScrapedItem\n-\n-    # The Pipeline Manager to use for processing these item\n-    ItemPipelineManager = ItemPipelineManager\n-\n-    # Maximum number of items to process in parallel by this pipeline\n-    concurrent_limit = settings.getint('ITEMPIPELINE_CONCURRENTLIMIT', 0)\n-\n-    def __init__(self):\n-        self.pipeline = self.ItemPipelineManager()\n-        dispatcher.connect(self.domain_opened, signal=signals.domain_opened)\n-        dispatcher.connect(self.domain_closed, signal=signals.domain_closed)\n-        dispatcher.connect(self.domain_idle, signal=signals.domain_idle)\n-\n-    def domain_opened(self, domain):\n-        self.pipeline.open_domain(domain)\n-\n-    def domain_closed(self, domain):\n-        self.pipeline.close_domain(domain)\n-\n-    def domain_idle(self, domain):\n-        if not self.pipeline.domain_is_idle(domain):\n-            raise DontCloseDomain\n-\n-    def process_spider_output(self, response, result, spider):\n-        domain = spider.domain_name\n-        info = self.pipeline.domaininfo[domain]\n-\n-        for item_or_request in result:\n-            # return to engine until pipeline frees up some slots\n-            # TODO: this is ugly, a proper flow control mechanism should be\n-            # added instead\n-            while 0 < self.concurrent_limit <= len(info):\n-                yield None\n-\n-            if isinstance(item_or_request, self.ScrapedItem):\n-                log.msg(\"Scraped %s in <%s>\" % (item_or_request, response.request.url), \\\n-                    domain=domain)\n-                signals.send_catch_log(signal=signals.item_scraped, sender=self.__class__, \\\n-                    item=item_or_request, spider=spider, response=response)\n-                self.pipeline.pipe(item_or_request, spider).addBoth(self._pipeline_finished, \\\n-                    item_or_request, spider)\n-                # yielding here breaks the loop and allows the engine to run\n-                # other tasks, such as attending IO (very important)\n-                yield None \n-            else:\n-                yield item_or_request\n-\n-    def _pipeline_finished(self, pipe_result, item, spider):\n-        # exception can only be of DropItem type here, since other exceptions\n-        # are caught in the Item Pipeline (item/pipeline.py)\n-        if isinstance(pipe_result, Failure):\n-            signals.send_catch_log(signal=signals.item_dropped, \\\n-                sender=self.__class__, item=item, spider=spider, exception=pipe_result.value)\n-        else:\n-            signals.send_catch_log(signal=signals.item_passed, \\\n-                sender=self.__class__, item=item, spider=spider, pipe_output=pipe_result)\n\n@@ -36,13 +36,13 @@ class ExecutionEngine(object):\n         self.control_reactor = True\n         self._next_request_pending = set()\n \n-    def configure(self, scheduler=None, downloader=None):\n+    def configure(self):\n         \"\"\"\n         Configure execution engine with the given scheduling policy and downloader.\n         \"\"\"\n-        self.scheduler = scheduler or Scheduler()\n+        self.scheduler = load_object(settings['SCHEDULER'])()\n         self.domain_scheduler = load_object(settings['DOMAIN_SCHEDULER'])()\n-        self.downloader = downloader or Downloader()\n+        self.downloader = Downloader()\n         self.scraper = Scraper(self)\n         self.configured = True\n \n@@ -161,8 +161,7 @@ class ExecutionEngine(object):\n         if self.paused:\n             return reactor.callLater(5, self.next_request, domain)\n \n-        if not self.running or \\\n-                self.domain_is_closed(domain) or \\\n+        if not self.running or self.domain_is_closed(domain) or \\\n                 self.downloader.sites[domain].needs_backout() or  \\\n                 self.scraper.sites[domain].needs_backout():\n             return\n@@ -222,7 +221,6 @@ class ExecutionEngine(object):\n         if not self.running or self.paused:\n             return\n \n-        # main domain starter loop\n         while self.running and self.downloader.has_capacity():\n             if not self.next_domain():\n                 return self._stop_if_idle()\n@@ -349,8 +347,10 @@ class ExecutionEngine(object):\n             \"self.downloader.sites[domain].closing\",\n             \"self.downloader.sites[domain].lastseen\",\n             \"len(self.scraper.sites[domain].queue)\",\n-            \"len(self.scraper.sites[domain].processing)\",\n-            \"self.scraper.sites[domain].backlog_size\",\n+            \"len(self.scraper.sites[domain].active)\",\n+            \"self.scraper.sites[domain].active_size\",\n+            \"self.scraper.sites[domain].itemproc_size\",\n+            \"self.scraper.sites[domain].needs_backout()\",\n             ]\n \n         for test in global_tests:\n\n@@ -7,7 +7,7 @@ from scrapy import log\n from scrapy.http import Request\n from scrapy.core.engine import scrapyengine\n from scrapy.spider import spiders\n-from scrapy.utils.misc import load_object, arg_to_iter\n+from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.url import is_url\n from scrapy.conf import settings\n \n@@ -39,8 +39,7 @@ class ExecutionManager(object):\n         log.msg(\"Enabled extensions: %s\" % \", \".join(extensions.enabled.iterkeys()),\n             level=log.DEBUG)\n \n-        scheduler = load_object(settings['SCHEDULER'])()\n-        scrapyengine.configure(scheduler=scheduler)\n+        scrapyengine.configure()\n         \n     def crawl(self, *args):\n         \"\"\"Schedule the given args for crawling. args is a list of urls or domains\"\"\"\n\n@@ -1,64 +1,66 @@\n \"\"\"This module implements the Scraper component which parses responses and\n extracts information from them\"\"\"\n \n-from itertools import imap\n-\n-from twisted.internet import task\n from twisted.python.failure import Failure\n from twisted.internet import defer\n \n-from scrapy.utils.defer import defer_result\n-from scrapy.utils.misc import arg_to_iter\n-from scrapy.core.exceptions import IgnoreRequest\n+from scrapy.utils.defer import defer_result, defer_succeed, parallel\n+from scrapy.utils.misc import arg_to_iter, load_object\n+from scrapy.core.exceptions import IgnoreRequest, DropItem\n from scrapy.core import signals\n from scrapy.http import Request, Response\n+from scrapy.item import ScrapedItem\n from scrapy.spider.middleware import SpiderMiddlewareManager\n from scrapy import log\n from scrapy.stats import stats\n+from scrapy.conf import settings\n \n class SiteInfo(object):\n     \"\"\"Object for holding data of the responses being scraped\"\"\"\n \n     FAILURE_SIZE = 1024 # make failures equivalent to 1K responses in size\n \n-    def __init__(self, max_backlog_size=5000000):\n+    def __init__(self, max_active_size=5000000):\n+        self.max_active_size = max_active_size\n         self.queue = []\n-        self.processing = set()\n-        self.backlog_size = 0\n-        self.max_backlog_size = max_backlog_size\n+        self.active = set()\n+        self.active_size = 0\n+        self.itemproc_size = 0\n \n     def add_response_request(self, response, request):\n         deferred = defer.Deferred()\n         self.queue.append((response, request, deferred))\n         if isinstance(response, Response):\n-            self.backlog_size += len(response.body)\n+            self.active_size += len(response.body)\n         else:\n-            self.backlog_size += self.FAILURE_SIZE\n+            self.active_size += self.FAILURE_SIZE\n         return deferred\n \n     def next_response_request_deferred(self):\n         response, request, deferred = self.queue.pop(0)\n-        self.processing.add(response)\n+        self.active.add(response)\n         return response, request, deferred\n \n     def finish_response(self, response):\n-        self.processing.remove(response)\n+        self.active.remove(response)\n         if isinstance(response, Response):\n-            self.backlog_size -= len(response.body)\n+            self.active_size -= len(response.body)\n         else:\n-            self.backlog_size -= self.FAILURE_SIZE\n+            self.active_size -= self.FAILURE_SIZE\n \n     def is_idle(self):\n-        return not (self.queue or self.processing)\n+        return not (self.queue or self.active)\n \n     def needs_backout(self):\n-        return self.backlog_size > self.max_backlog_size\n+        return self.active_size > self.max_active_size\n \n class Scraper(object):\n \n     def __init__(self, engine):\n         self.sites = {}\n-        self.middleware = SpiderMiddlewareManager()\n+        self.spidermw = SpiderMiddlewareManager()\n+        self.itemproc = load_object(settings['ITEM_PROCESSOR'])()\n+        self.concurrent_items = settings.getint('CONCURRENT_ITEMS')\n         self.engine = engine\n \n     def open_domain(self, domain):\n@@ -66,12 +68,14 @@ class Scraper(object):\n         if domain in self.sites:\n             raise RuntimeError('Scraper domain already opened: %s' % domain)\n         self.sites[domain] = SiteInfo()\n+        self.itemproc.open_domain(domain)\n \n     def close_domain(self, domain):\n         \"\"\"Close a domain being scraped and release its resources\"\"\"\n         if domain not in self.sites:\n             raise RuntimeError('Scraper domain already closed: %s' % domain)\n-        del self.sites[domain]\n+        self.sites.pop(domain)\n+        self.itemproc.open_domain(domain)\n \n     def is_idle(self):\n         \"\"\"Return True if there isn't any more spiders to process\"\"\"\n@@ -82,19 +86,15 @@ class Scraper(object):\n         dfd = site.add_response_request(response, request)\n         def finish_scraping(_):\n             site.finish_response(response)\n+            self._scrape_next(spider, site)\n             return _\n         dfd.addBoth(finish_scraping)\n         dfd.addErrback(log.err, 'Scraper bug processing %s' % request, \\\n             domain=spider.domain_name)\n-        self.scrape_next(spider)\n+        self._scrape_next(spider, site)\n         return dfd\n \n-    def scrape_next(self, spider):\n-        site = self.sites.get(spider.domain_name)\n-        if not site:\n-            return\n-\n-        # Process responses in queue\n+    def _scrape_next(self, spider, site):\n         while site.queue:\n             response, request, deferred = site.next_response_request_deferred()\n             self._scrape(response, request, spider).chainDeferred(deferred)\n@@ -106,14 +106,14 @@ class Scraper(object):\n \n         dfd = self._scrape2(response, request, spider) # returns spiders processed output\n         dfd.addErrback(self.handle_spider_error, request, spider)\n-        dfd.addCallback(self.handle_spider_output, request, spider)\n+        dfd.addCallback(self.handle_spider_output, request, response, spider)\n         return dfd\n \n     def _scrape2(self, request_result, request, spider):\n         \"\"\"Handle the diferent cases of request's result been a Response or a\n         Failure\"\"\"\n         if not isinstance(request_result, Failure):\n-            return self.middleware.scrape_response(self.call_spider, \\\n+            return self.spidermw.scrape_response(self.call_spider, \\\n                 request_result, request, spider)\n         else:\n             # FIXME: don't ignore errors in spider middleware\n@@ -132,22 +132,39 @@ class Scraper(object):\n         stats.incpath(\"%s/spider_exceptions/%s\" % (spider.domain_name, \\\n             _failure.value.__class__.__name__))\n \n-    def handle_spider_output(self, result, request, spider):\n-        func = lambda o: self.process_spider_output(o, request, spider)\n-        return task.coiterate(imap(func, result or []))\n+    def handle_spider_output(self, result, request, response, spider):\n+        domain = spider.domain_name\n+        if not result:\n+            return defer_succeed(None)\n+        dfd = parallel(iter(result), self.concurrent_items,\n+            self._process_spidermw_output, request, response, spider)\n+        return dfd\n \n-    def process_spider_output(self, output, request, spider):\n+    def _process_spidermw_output(self, output, request, response, spider):\n+        \"\"\"Process each Request/Item (given in the output parameter) returned\n+        from the given spider\n+        \"\"\"\n         # TODO: keep closing state internally instead of checking engine\n-        if spider.domain_name in self.engine.closing:\n+        domain = spider.domain_name\n+        if domain in self.engine.closing:\n             return\n         elif isinstance(output, Request):\n-            signals.send_catch_log(signal=signals.request_received, request=output, spider=spider)\n+            signals.send_catch_log(signal=signals.request_received, request=output, \\\n+                spider=spider)\n             self.engine.crawl(request=output, spider=spider)\n+        elif isinstance(output, ScrapedItem):\n+            log.msg(\"Scraped %s in <%s>\" % (output, request.url), domain=domain)\n+            signals.send_catch_log(signal=signals.item_scraped, sender=self.__class__, \\\n+                item=output, spider=spider, response=response)\n+            self.sites[domain].itemproc_size += 1\n+            dfd = self.itemproc.process_item(output, spider)\n+            dfd.addBoth(self._itemproc_finished, output, response, spider)\n+            return dfd\n         elif output is None:\n-            pass # may be next time.\n+            pass\n         else:\n-            log.msg(\"Spider must return Request, ScrapedItem or None, got '%s' while processing %s\" \\\n-                    % (type(output).__name__, request), log.WARNING, domain=spider.domain_name)\n+            log.msg(\"Spider must return Request, ScrapedItem or None, got '%s' in %s\" % \\\n+                (type(output).__name__, request), log.ERROR, domain=domain)\n \n     def _check_propagated_failure(self, spider_failure, propagated_failure, request, spider):\n         \"\"\"Log and silence the bugs raised outside of spiders, but still allow\n@@ -162,3 +179,22 @@ class Scraper(object):\n             return # stop propagating this error\n         else:\n             return spider_failure # exceptions raised in the spider code\n+\n+    def _itemproc_finished(self, output, item, response, spider):\n+        \"\"\"ItemProcessor finished for the given ``item`` and returned ``output``\n+        \"\"\"\n+        domain = spider.domain_name\n+        self.sites[domain].itemproc_size -= 1\n+        if isinstance(output, Failure):\n+            ex = output.value\n+            if isinstance(ex, DropItem):\n+                log.msg(\"Dropped %s - %s\" % (item, str(ex)), log.DEBUG, domain=domain)\n+                signals.send_catch_log(signal=signals.item_dropped, \\\n+                    sender=self.__class__, item=item, spider=spider, exception=output.value)\n+            else:\n+                log.msg('Error processing %s - %s' % (item, output), \\\n+                    log.ERROR, domain=domain)\n+        else:\n+            signals.send_catch_log(signal=signals.item_passed, \\\n+                sender=self.__class__, item=item, spider=spider, output=output)\n+\n\n@@ -1,5 +1,5 @@\n from scrapy import log\n-from scrapy.core.exceptions import DropItem, NotConfigured\n+from scrapy.core.exceptions import NotConfigured\n from scrapy.item import ScrapedItem\n from scrapy.utils.misc import load_object\n from scrapy.utils.defer import defer_succeed, mustbe_deferred\n@@ -10,7 +10,6 @@ class ItemPipelineManager(object):\n     def __init__(self):\n         self.loaded = False\n         self.pipeline = []\n-        self.domaininfo = {}\n         self.load()\n \n     def load(self):\n@@ -30,69 +29,24 @@ class ItemPipelineManager(object):\n         self.loaded = True\n \n     def open_domain(self, domain):\n-        self.domaininfo[domain] = set()\n+        pass\n \n     def close_domain(self, domain):\n-        del self.domaininfo[domain]\n+        pass\n \n-    def is_idle(self):\n-        return not self.domaininfo\n-\n-    def domain_is_idle(self, domain):\n-        return not self.domaininfo.get(domain)\n-\n-    def pipe(self, item, spider):\n-        \"\"\"\n-        item pipelines are instanceable classes that defines a `pipeline` method\n-        that takes ScrapedItem as input and returns ScrapedItem.\n-\n-        The output from one stage is the input of the next.\n-\n-        Raising DropItem stops pipeline.\n-\n-        This pipeline is configurable with the ITEM_PIPELINES setting\n-        \"\"\"\n-        domain = spider.domain_name\n-        if not self.pipeline or domain not in self.domaininfo:\n+    def process_item(self, item, spider):\n+        if not self.pipeline:\n             return defer_succeed(item)\n \n-        pipeline = self.pipeline[:]\n-        current_stage = pipeline[0]\n-        info = self.domaininfo[domain]\n-        info.add(item)\n-\n-        def _next_stage(item):\n+        def next_stage(item, stages_left):\n             assert isinstance(item, ScrapedItem), \\\n-                    'Pipeline stages must return a ScrapedItem or raise DropItem, got %s' % type(item).__name__\n-\n-            if not pipeline:\n+                'Item pipelines must return a ScrapedItem, got %s' % type(item).__name__\n+            if not stages_left:\n                 return item\n-\n-            current_stage = pipeline.pop(0)\n-            log.msg(\"_%s_ Pipeline stage: %s\" % (item, type(current_stage).__name__), log.TRACE, domain=domain)\n-\n-            d = mustbe_deferred(current_stage.process_item, domain, item)\n-            d.addCallback(_next_stage)\n+            current_stage = stages_left.pop(0)\n+            d = mustbe_deferred(current_stage.process_item, spider.domain_name, item)\n+            d.addCallback(next_stage, stages_left)\n             return d\n \n-        def _ondrop(_failure):\n-            ex = _failure.value\n-            if isinstance(ex, DropItem):\n-                # TODO: current_stage is not working, check why\n-                #log.msg(\"%s: Dropped %s - %s\" % (type(current_stage).__name__, item, str(ex)), log.DEBUG, domain=domain)\n-                log.msg(\"Dropped %s - %s\" % (item, str(ex)), log.DEBUG, domain=domain)\n-                return _failure\n-            else:\n-                # TODO: current_stage is not working, check why\n-                #log.msg('%s: Error processing %s - %s' % (type(current_stage).__name__, item, _failure), log.ERROR, domain=domain)\n-                log.msg('Error processing %s - %s' % (item, _failure), log.ERROR, domain=domain)\n-\n-        def _pipeline_finished(_):\n-            log.msg(\"_%s_ Pipeline finished\" % item, log.TRACE, domain=domain)\n-            info.remove(item)\n-            return _\n-\n-        deferred = mustbe_deferred(_next_stage, item)\n-        deferred.addErrback(_ondrop)\n-        deferred.addBoth(_pipeline_finished)\n+        deferred = mustbe_deferred(next_stage, item, self.pipeline[:])\n         return deferred\n\n@@ -41,7 +41,7 @@ class CoreStats(object):\n         stats.incpath('%s/item_scraped_count' % spider.domain_name)\n         stats.incpath('_global/item_scraped_count')\n \n-    def item_passed(self, item, spider, pipe_output):\n+    def item_passed(self, item, spider):\n         stats.incpath('%s/item_passed_count' % spider.domain_name)\n         stats.incpath('_global/item_passed_count')\n \n\n@@ -2,7 +2,7 @@\n Helper functions for dealing with Twisted deferreds\n \"\"\"\n \n-from twisted.internet import defer, reactor\n+from twisted.internet import defer, reactor, task\n from twisted.python import failure\n \n def defer_fail(_failure):\n@@ -37,3 +37,13 @@ def mustbe_deferred(f, *args, **kw):\n def chain_deferred(d1, d2):\n     return d1.chainDeferred(d2).addBoth(lambda _:d2)\n \n+def parallel(iterable, count, callable, *args, **named):\n+    \"\"\"Execute a callable over the objects in the given iterable, in parallel,\n+    using no more than ``count`` concurrent calls.\n+\n+    Taken from: http://jcalderone.livejournal.com/24285.html\n+    \"\"\"\n+    coop = task.Cooperator()\n+    work = (callable(elem, *args, **named) for elem in iterable)\n+    return defer.DeferredList([coop.coiterate(work) for i in xrange(count)])\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
