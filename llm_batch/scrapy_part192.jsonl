{"custom_id": "scrapy#157cec7f64f3eab47814e3865ccfc867cb0e0bd0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 9 | Files Changed: 5 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 19 | Churn Cumulative: 2789 | Contributors (this commit): 15 | Commits (past 90d): 18 | Contributors (cumulative): 26 | DMM Complexity: 1.0\n\nDIFF:\n@@ -112,8 +112,8 @@ class ExecutionEngine(object):\n                 slot.start_requests = None\n             except Exception:\n                 slot.start_requests = None\n-                logger.exception('Error while obtaining start requests',\n-                                 extra={'spider': spider})\n+                logger.error('Error while obtaining start requests',\n+                             exc_info=True, extra={'spider': spider})\n             else:\n                 self.crawl(request, spider)\n \n\n@@ -83,9 +83,9 @@ class Scheduler(object):\n             self.dqs.push(reqd, -request.priority)\n         except ValueError as e: # non serializable request\n             if self.logunser:\n-                logger.exception(\"Unable to serialize request: %(request)s - reason: %(reason)s\",\n+                logger.error(\"Unable to serialize request: %(request)s - reason: %(reason)s\",\n                              {'request': request, 'reason': e},\n-                                 extra={'spider': self.spider})\n+                             exc_info=True, extra={'spider': self.spider})\n             return\n         else:\n             return True\n\n@@ -272,11 +272,11 @@ class FilesPipeline(MediaPipeline):\n             )\n             raise\n         except Exception as exc:\n-            logger.exception(\n+            logger.error(\n                 'File (unknown-error): Error processing file from %(request)s '\n                 'referred in <%(referer)s>',\n                 {'request': request, 'referer': referer},\n-                extra={'spider': info.spider}\n+                exc_info=True, extra={'spider': info.spider}\n             )\n             raise FileException(str(exc))\n \n\n@@ -30,8 +30,9 @@ def send_catch_log(signal=Any, sender=Anonymous, *arguments, **named):\n             result = Failure()\n         except Exception:\n             result = Failure()\n-            logger.exception(\"Error caught on signal handler: %(receiver)s\",\n-                             {'receiver': receiver}, extra={'spider': spider})\n+            logger.error(\"Error caught on signal handler: %(receiver)s\",\n+                         {'receiver': receiver},\n+                         exc_info=True, extra={'spider': spider})\n         else:\n             result = response\n         responses.append((receiver, result))\n\n@@ -28,7 +28,7 @@ class FailureFormatterTest(unittest.TestCase):\n             try:\n                 0/0\n             except ZeroDivisionError:\n-                self.logger.exception('test log msg')\n+                self.logger.error('test log msg', exc_info=True)\n                 failure = Failure()\n \n             self.logger.error('test log msg', extra={'failure': failure})\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6dccb3a9b320a8d0808764ac8e6e88e663e2d52c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 71 | Files Changed: 11 | Hunks: 38 | Methods Changed: 26 | Complexity Δ (Sum/Max): 80/46 | Churn Δ: 124 | Churn Cumulative: 4458 | Contributors (this commit): 23 | Commits (past 90d): 51 | Contributors (cumulative): 46 | DMM Complexity: 0.0\n\nDIFF:\n@@ -16,7 +16,7 @@ from scrapy.exceptions import DontCloseSpider\n from scrapy.http import Response, Request\n from scrapy.utils.misc import load_object\n from scrapy.utils.reactor import CallLaterOnce\n-from scrapy.utils.log import logformatter_adapter\n+from scrapy.utils.log import logformatter_adapter, failure_to_exc_info\n \n logger = logging.getLogger(__name__)\n \n@@ -135,13 +135,16 @@ class ExecutionEngine(object):\n         d = self._download(request, spider)\n         d.addBoth(self._handle_downloader_output, request, spider)\n         d.addErrback(lambda f: logger.info('Error while handling downloader output',\n-                                           extra={'spider': spider, 'failure': f}))\n+                                           exc_info=failure_to_exc_info(f),\n+                                           extra={'spider': spider}))\n         d.addBoth(lambda _: slot.remove_request(request))\n         d.addErrback(lambda f: logger.info('Error while removing request from slot',\n-                                           extra={'spider': spider, 'failure': f}))\n+                                           exc_info=failure_to_exc_info(f),\n+                                           extra={'spider': spider}))\n         d.addBoth(lambda _: slot.nextcall.schedule())\n         d.addErrback(lambda f: logger.info('Error while scheduling new request',\n-                                           extra={'spider': spider, 'failure': f}))\n+                                           exc_info=failure_to_exc_info(f),\n+                                           extra={'spider': spider}))\n         return d\n \n     def _handle_downloader_output(self, response, request, spider):\n@@ -153,7 +156,8 @@ class ExecutionEngine(object):\n         # response is a Response or Failure\n         d = self.scraper.enqueue_scrape(response, request, spider)\n         d.addErrback(lambda f: logger.error('Error while enqueuing downloader output',\n-                                            extra={'spider': spider, 'failure': f}))\n+                                            exc_info=failure_to_exc_info(f),\n+                                            extra={'spider': spider}))\n         return d\n \n     def spider_is_idle(self, spider):\n@@ -268,7 +272,11 @@ class ExecutionEngine(object):\n \n         def log_failure(msg):\n             def errback(failure):\n-                logger.error(msg, extra={'spider': spider, 'failure': failure})\n+                logger.error(\n+                    msg,\n+                    exc_info=failure_to_exc_info(failure),\n+                    extra={'spider': spider}\n+                )\n             return errback\n \n         dfd.addBoth(lambda _: self.downloader.close())\n\n@@ -10,7 +10,7 @@ from twisted.internet import defer\n from scrapy.utils.defer import defer_result, defer_succeed, parallel, iter_errback\n from scrapy.utils.spider import iterate_spider_output\n from scrapy.utils.misc import load_object\n-from scrapy.utils.log import logformatter_adapter\n+from scrapy.utils.log import logformatter_adapter, failure_to_exc_info\n from scrapy.exceptions import CloseSpider, DropItem, IgnoreRequest\n from scrapy import signals\n from scrapy.http import Request, Response\n@@ -107,7 +107,8 @@ class Scraper(object):\n         dfd.addErrback(\n             lambda f: logger.error('Scraper bug processing %(request)s',\n                                    {'request': request},\n-                                   extra={'spider': spider, 'failure': f}))\n+                                   exc_info=failure_to_exc_info(f),\n+                                   extra={'spider': spider}))\n         self._scrape_next(spider, slot)\n         return dfd\n \n@@ -153,7 +154,8 @@ class Scraper(object):\n         logger.error(\n             \"Spider error processing %(request)s (referer: %(referer)s)\",\n             {'request': request, 'referer': referer},\n-            extra={'spider': spider, 'failure': _failure}\n+            exc_info=failure_to_exc_info(_failure),\n+            extra={'spider': spider}\n         )\n         self.signals.send_catch_log(\n             signal=signals.spider_error,\n@@ -202,7 +204,8 @@ class Scraper(object):\n             if download_failure.frames:\n                 logger.error('Error downloading %(request)s',\n                              {'request': request},\n-                             extra={'spider': spider, 'failure': download_failure})\n+                             exc_info=failure_to_exc_info(download_failure),\n+                             extra={'spider': spider})\n             else:\n                 errmsg = download_failure.getErrorMessage()\n                 if errmsg:\n@@ -227,7 +230,8 @@ class Scraper(object):\n                     spider=spider, exception=output.value)\n             else:\n                 logger.error('Error processing %(item)s', {'item': item},\n-                             extra={'spider': spider, 'failure': output})\n+                             exc_info=failure_to_exc_info(output),\n+                             extra={'spider': spider})\n         else:\n             logkws = self.logformatter.scraped(output, response, spider)\n             logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n\n@@ -11,6 +11,7 @@ from six.moves.urllib import robotparser\n from scrapy.exceptions import NotConfigured, IgnoreRequest\n from scrapy.http import Request\n from scrapy.utils.httpobj import urlparse_cached\n+from scrapy.utils.log import failure_to_exc_info\n \n logger = logging.getLogger(__name__)\n \n@@ -59,7 +60,8 @@ class RobotsTxtMiddleware(object):\n         if failure.type is not IgnoreRequest:\n             logger.error(\"Error downloading %(request)s: %(f_exception)s\",\n                          {'request': request, 'f_exception': failure.value},\n-                         extra={'spider': spider, 'failure': failure})\n+                         exc_info=failure_to_exc_info(failure),\n+                         extra={'spider': spider})\n \n     def _parse_robots(self, response):\n         rp = robotparser.RobotFileParser(response.url)\n\n@@ -22,6 +22,7 @@ from scrapy.utils.ftp import ftp_makedirs_cwd\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.misc import load_object\n from scrapy.utils.python import get_func_args\n+from scrapy.utils.log import failure_to_exc_info\n \n logger = logging.getLogger(__name__)\n \n@@ -184,7 +185,8 @@ class FeedExporter(object):\n         d.addCallback(lambda _: logger.info(logfmt % \"Stored\", log_args,\n                                             extra={'spider': spider}))\n         d.addErrback(lambda f: logger.error(logfmt % \"Error storing\", log_args,\n-                                            extra={'spider': spider, 'failure': f}))\n+                                            exc_info=failure_to_exc_info(f),\n+                                            extra={'spider': spider}))\n         return d\n \n     def item_scraped(self, item, spider):\n\n@@ -8,6 +8,7 @@ import warnings\n from twisted.python.failure import Failure\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n+from scrapy.utils.log import failure_to_exc_info\n \n logger = logging.getLogger(__name__)\n \n@@ -48,4 +49,4 @@ def err(_stuff=None, _why=None, **kw):\n     level = kw.pop('level', logging.ERROR)\n     failure = kw.pop('failure', _stuff) or Failure()\n     message = kw.pop('why', _why) or failure.value\n-    logger.log(level, message, *[kw] if kw else [], extra={'failure': failure})\n+    logger.log(level, message, *[kw] if kw else [], exc_info=failure_to_exc_info(failure))\n\n@@ -25,6 +25,7 @@ from scrapy.pipelines.media import MediaPipeline\n from scrapy.exceptions import NotConfigured, IgnoreRequest\n from scrapy.http import Request\n from scrapy.utils.misc import md5sum\n+from scrapy.utils.log import failure_to_exc_info\n \n logger = logging.getLogger(__name__)\n \n@@ -212,7 +213,8 @@ class FilesPipeline(MediaPipeline):\n         dfd.addErrback(\n             lambda f:\n             logger.error(self.__class__.__name__ + '.store.stat_file',\n-                         extra={'spider': info.spider, 'failure': f})\n+                         exc_info=failure_to_exc_info(f),\n+                         extra={'spider': info.spider})\n         )\n         return dfd\n \n\n@@ -8,6 +8,7 @@ from twisted.python.failure import Failure\n from scrapy.utils.defer import mustbe_deferred, defer_result\n from scrapy.utils.request import request_fingerprint\n from scrapy.utils.misc import arg_to_iter\n+from scrapy.utils.log import failure_to_exc_info\n \n logger = logging.getLogger(__name__)\n \n@@ -70,7 +71,7 @@ class MediaPipeline(object):\n         dfd.addCallback(self._check_media_to_download, request, info)\n         dfd.addBoth(self._cache_result_and_execute_waiters, fp, info)\n         dfd.addErrback(lambda f: logger.error(\n-            f.value, extra={'spider': info.spider, 'failure': f})\n+            f.value, exc_info=failure_to_exc_info(f), extra={'spider': info.spider})\n         )\n         return dfd.addBoth(lambda _: wad)  # it must return wad at last\n \n@@ -127,6 +128,7 @@ class MediaPipeline(object):\n                     logger.error(\n                         '%(class)s found errors processing %(item)s',\n                         {'class': self.__class__.__name__, 'item': item},\n-                        extra={'spider': info.spider, 'failure': value}\n+                        exc_info=failure_to_exc_info(value),\n+                        extra={'spider': info.spider}\n                     )\n         return item\n\n@@ -1,6 +1,5 @@\n # -*- coding: utf-8 -*-\n \n-import os\n import sys\n import logging\n import warnings\n@@ -16,22 +15,10 @@ from scrapy.exceptions import ScrapyDeprecationWarning\n logger = logging.getLogger(__name__)\n \n \n-class FailureFormatter(logging.Filter):\n-    \"\"\"Extract exc_info from Failure instances provided as contextual data\n-\n-    This filter mimics Twisted log.err formatting for its first `_stuff`\n-    argument, which means that reprs of non Failure objects are appended to the\n-    log messages.\n-    \"\"\"\n-\n-    def filter(self, record):\n-        failure = record.__dict__.get('failure')\n-        if failure:\n+def failure_to_exc_info(failure):\n+    \"\"\"Extract exc_info from Failure instances\"\"\"\n     if isinstance(failure, Failure):\n-                record.exc_info = (failure.type, failure.value, failure.tb)\n-            else:\n-                record.msg += os.linesep + repr(failure)\n-        return True\n+        return (failure.type, failure.value, failure.tb)\n \n \n class TopLevelFormatter(logging.Filter):\n@@ -58,15 +45,9 @@ class TopLevelFormatter(logging.Filter):\n DEFAULT_LOGGING = {\n     'version': 1,\n     'disable_existing_loggers': False,\n-    'filters': {\n-        'failure_formatter': {\n-            '()': 'scrapy.utils.log.FailureFormatter',\n-        },\n-    },\n     'loggers': {\n         'scrapy': {\n             'level': 'DEBUG',\n-            'filters': ['failure_formatter'],\n         },\n         'twisted': {\n             'level': 'ERROR',\n\n@@ -8,6 +8,7 @@ from twisted.python.failure import Failure\n from scrapy.xlib.pydispatch.dispatcher import Any, Anonymous, liveReceivers, \\\n     getAllReceivers, disconnect\n from scrapy.xlib.pydispatch.robustapply import robustApply\n+from scrapy.utils.log import failure_to_exc_info\n \n logger = logging.getLogger(__name__)\n \n@@ -47,7 +48,8 @@ def send_catch_log_deferred(signal=Any, sender=Anonymous, *arguments, **named):\n         if dont_log is None or not isinstance(failure.value, dont_log):\n             logger.error(\"Error caught on signal handler: %(receiver)s\",\n                          {'receiver': recv},\n-                         extra={'spider': spider, 'failure': failure})\n+                         exc_info=failure_to_exc_info(failure),\n+                         extra={'spider': spider})\n         return failure\n \n     dont_log = named.pop('dont_log', None)\n\n@@ -9,6 +9,7 @@ from scrapy.http import Request, Response\n from scrapy.spiders import Spider\n from scrapy.utils.request import request_fingerprint\n from scrapy.pipelines.media import MediaPipeline\n+from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.signal import disconnect_all\n from scrapy import signals\n \n@@ -66,7 +67,7 @@ class BaseMediaPipelineTestCase(unittest.TestCase):\n         assert len(l.records) == 1\n         record = l.records[0]\n         assert record.levelname == 'ERROR'\n-        assert record.failure is fail\n+        self.assertTupleEqual(record.exc_info, failure_to_exc_info(fail))\n \n         # disable failure logging and check again\n         self.pipe.LOG_FAILED_RESULTS = False\n\n@@ -1,6 +1,5 @@\n # -*- coding: utf-8 -*-\n from __future__ import print_function\n-import os\n import sys\n import logging\n import unittest\n@@ -8,46 +7,24 @@ import unittest\n from testfixtures import LogCapture\n from twisted.python.failure import Failure\n \n-from scrapy.utils.log import (FailureFormatter, TopLevelFormatter,\n+from scrapy.utils.log import (failure_to_exc_info, TopLevelFormatter,\n                               LogCounterHandler, StreamLogger)\n from scrapy.utils.test import get_crawler\n \n \n-class FailureFormatterTest(unittest.TestCase):\n+class FailureToExcInfoTest(unittest.TestCase):\n \n-    def setUp(self):\n-        self.logger = logging.getLogger('test')\n-        self.filter = FailureFormatter()\n-        self.logger.addFilter(self.filter)\n-\n-    def tearDown(self):\n-        self.logger.removeFilter(self.filter)\n-\n-    def test_failure_format(self):\n-        with LogCapture() as l:\n+    def test_failure(self):\n         try:\n             0/0\n         except ZeroDivisionError:\n-                self.logger.error('test log msg', exc_info=True)\n+            exc_info = sys.exc_info()\n             failure = Failure()\n \n-            self.logger.error('test log msg', extra={'failure': failure})\n+        self.assertTupleEqual(exc_info, failure_to_exc_info(failure))\n \n-        self.assertEqual(len(l.records), 2)\n-        exc_record, failure_record = l.records\n-        self.assertTupleEqual(failure_record.exc_info, exc_record.exc_info)\n-\n-        formatter = logging.Formatter()\n-        self.assertMultiLineEqual(formatter.format(failure_record),\n-                                  formatter.format(exc_record))\n-\n-    def test_non_failure_format(self):\n-        with LogCapture() as l:\n-            self.logger.error('test log msg', extra={'failure': 3})\n-\n-        self.assertEqual(len(l.records), 1)\n-        self.assertMultiLineEqual(l.records[0].getMessage(),\n-                                  'test log msg' + os.linesep + '3')\n+    def test_non_failure(self):\n+        self.assertIsNone(failure_to_exc_info('test'))\n \n \n class TopLevelFormatterTest(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e7574a809e382120f424de18796d7d2e61e71625", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 220 | Contributors (this commit): 1 | Commits (past 90d): 12 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -18,7 +18,7 @@ logger = logging.getLogger(__name__)\n def failure_to_exc_info(failure):\n     \"\"\"Extract exc_info from Failure instances\"\"\"\n     if isinstance(failure, Failure):\n-        return (failure.type, failure.value, failure.tb)\n+        return (failure.type, failure.value, failure.getTracebackObject())\n \n \n class TopLevelFormatter(logging.Filter):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8d9e3b7e8da201223b353f43789efc7088817c32", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 12 | Methods Changed: 10 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 35 | Churn Cumulative: 37 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,7 @@\n from __future__ import absolute_import\n import os\n import csv\n+import json\n from io import BytesIO\n import tempfile\n import shutil\n@@ -126,7 +127,6 @@ class FeedExportTest(unittest.TestCase):\n         egg = scrapy.Field()\n         baz = scrapy.Field()\n \n-\n     @defer.inlineCallbacks\n     def run_and_export(self, spider_cls, settings=None):\n         \"\"\" Run spider with specified settings; return exported data. \"\"\"\n@@ -180,7 +180,20 @@ class FeedExportTest(unittest.TestCase):\n         self.assertEqual(rows, got_rows)\n \n     @defer.inlineCallbacks\n-    def test_export_csv_items(self):\n+    def assertExportedJsonLines(self, items, rows, settings=None):\n+        settings = settings or {}\n+        settings.update({'FEED_FORMAT': 'jl'})\n+        data = yield self.exported_data(items, settings)\n+        parsed = [json.loads(line) for line in data.splitlines()]\n+        self.assertEqual(rows, parsed)\n+\n+    @defer.inlineCallbacks\n+    def assertExported(self, items, header, rows, settings=None, ordered=True):\n+        yield self.assertExportedCsv(items, header, rows, settings, ordered)\n+        yield self.assertExportedJsonLines(items, rows, settings)\n+\n+    @defer.inlineCallbacks\n+    def test_export_items(self):\n         # feed exporters use field names from Item\n         items = [\n             self.MyItem({'foo': 'bar1', 'egg': 'spam1'}),\n@@ -191,10 +204,10 @@ class FeedExportTest(unittest.TestCase):\n             {'egg': 'spam2', 'foo': 'bar2', 'baz': 'quux2'}\n         ]\n         header = self.MyItem.fields.keys()\n-        yield self.assertExportedCsv(items, header, rows, ordered=False)\n+        yield self.assertExported(items, header, rows, ordered=False)\n \n     @defer.inlineCallbacks\n-    def test_export_csv_multiple_item_classes(self):\n+    def test_export_multiple_item_classes(self):\n \n         class MyItem2(scrapy.Item):\n             foo = scrapy.Field()\n@@ -215,7 +228,7 @@ class FeedExportTest(unittest.TestCase):\n             {'egg': 'spam3', 'foo': 'bar3', 'baz': 'quux3'},\n             {'egg': 'spam4', 'foo': '',     'baz': ''},\n         ]\n-        yield self.assertExportedCsv(items, header, rows, ordered=False)\n+        yield self.assertExported(items, header, rows, ordered=False)\n \n         # but it is possible to override fields using FEED_EXPORT_FIELDS\n         header = [\"foo\", \"baz\", \"hello\"]\n@@ -226,11 +239,11 @@ class FeedExportTest(unittest.TestCase):\n             {'foo': 'bar3', 'baz': 'quux3', 'hello': ''},\n             {'foo': '',     'baz': '',      'hello': 'world4'},\n         ]\n-        yield self.assertExportedCsv(items, header, rows,\n+        yield self.assertExported(items, header, rows,\n                                   settings=settings, ordered=True)\n \n     @defer.inlineCallbacks\n-    def test_export_csv_dicts(self):\n+    def test_export_dicts(self):\n         # When dicts are used, only keys from the first row are used as\n         # a header.\n         items = [\n@@ -241,10 +254,10 @@ class FeedExportTest(unittest.TestCase):\n             {'egg': 'spam', 'foo': 'bar'},\n             {'egg': 'spam', 'foo': 'bar'}\n         ]\n-        yield self.assertExportedCsv(items, ['egg', 'foo'], rows, ordered=False)\n+        yield self.assertExported(items, ['egg', 'foo'], rows, ordered=False)\n \n     @defer.inlineCallbacks\n-    def test_export_csv_feed_export_fields(self):\n+    def test_export_feed_export_fields(self):\n         # FEED_EXPORT_FIELDS option allows to order export fields\n         # and to select a subset of fields to export, both for Items and dicts.\n \n@@ -260,7 +273,7 @@ class FeedExportTest(unittest.TestCase):\n                 {'egg': 'spam1', 'foo': 'bar1', 'baz': ''},\n                 {'egg': 'spam2', 'foo': 'bar2', 'baz': 'quux2'}\n             ]\n-            yield self.assertExportedCsv(items, ['foo', 'baz', 'egg'], rows,\n+            yield self.assertExported(items, ['foo', 'baz', 'egg'], rows,\n                                       settings=settings, ordered=True)\n \n             # export a subset of columns\n@@ -269,5 +282,5 @@ class FeedExportTest(unittest.TestCase):\n                 {'egg': 'spam1', 'baz': ''},\n                 {'egg': 'spam2', 'baz': 'quux2'}\n             ]\n-            yield self.assertExportedCsv(items, ['egg', 'baz'], rows,\n+            yield self.assertExported(items, ['egg', 'baz'], rows,\n                                       settings=settings, ordered=True)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e1efd1917564c654627db0d3ae39c2d1f7be3e3c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 18 | Churn Cumulative: 55 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -185,6 +185,7 @@ class FeedExportTest(unittest.TestCase):\n         settings.update({'FEED_FORMAT': 'jl'})\n         data = yield self.exported_data(items, settings)\n         parsed = [json.loads(line) for line in data.splitlines()]\n+        rows = [{k: v for k, v in row.items() if v} for row in rows]\n         self.assertEqual(rows, parsed)\n \n     @defer.inlineCallbacks\n@@ -220,15 +221,18 @@ class FeedExportTest(unittest.TestCase):\n             {'hello': 'world4', 'egg': 'spam4'},\n         ]\n \n-        # by default, Scrapy uses fields of the first Item\n+        # by default, Scrapy uses fields of the first Item for CSV and\n+        # all fields for JSON Lines\n         header = self.MyItem.fields.keys()\n-        rows = [\n+        rows_csv = [\n             {'egg': 'spam1', 'foo': 'bar1', 'baz': ''},\n             {'egg': '',      'foo': 'bar2', 'baz': ''},\n             {'egg': 'spam3', 'foo': 'bar3', 'baz': 'quux3'},\n             {'egg': 'spam4', 'foo': '',     'baz': ''},\n         ]\n-        yield self.assertExported(items, header, rows, ordered=False)\n+        rows_jl = [dict(row) for row in items]\n+        yield self.assertExportedCsv(items, header, rows_csv, ordered=False)\n+        yield self.assertExportedJsonLines(items, rows_jl)\n \n         # but it is possible to override fields using FEED_EXPORT_FIELDS\n         header = [\"foo\", \"baz\", \"hello\"]\n@@ -245,16 +249,18 @@ class FeedExportTest(unittest.TestCase):\n     @defer.inlineCallbacks\n     def test_export_dicts(self):\n         # When dicts are used, only keys from the first row are used as\n-        # a header.\n+        # a header for CSV, and all fields are used for JSON Lines.\n         items = [\n             {'foo': 'bar', 'egg': 'spam'},\n             {'foo': 'bar', 'egg': 'spam', 'baz': 'quux'},\n         ]\n-        rows = [\n+        rows_csv = [\n             {'egg': 'spam', 'foo': 'bar'},\n             {'egg': 'spam', 'foo': 'bar'}\n         ]\n-        yield self.assertExported(items, ['egg', 'foo'], rows, ordered=False)\n+        rows_jl = items\n+        yield self.assertExportedCsv(items, ['egg', 'foo'], rows_csv, ordered=False)\n+        yield self.assertExportedJsonLines(items, rows_jl)\n \n     @defer.inlineCallbacks\n     def test_export_feed_export_fields(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c41203486452203968e1879fba4d8760646ee86d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 7 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -21,7 +21,6 @@ from scrapy import signals\n from scrapy.utils.ftp import ftp_makedirs_cwd\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.misc import load_object\n-from scrapy.utils.python import get_func_args\n from scrapy.utils.log import failure_to_exc_info\n \n logger = logging.getLogger(__name__)\n@@ -152,7 +151,7 @@ class FeedExporter(object):\n         if not self._exporter_supported(self.format):\n             raise NotConfigured\n         self.store_empty = settings.getbool('FEED_STORE_EMPTY')\n-        self.export_fields = settings.getlist('FEED_EXPORT_FIELDS')\n+        self.export_fields = settings.getlist('FEED_EXPORT_FIELDS') or None\n         uripar = settings['FEED_URI_PARAMS']\n         self._uripar = load_object(uripar) if uripar else lambda x, y: None\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9fb318338bb499ff899ee8d1b096cc9eceb5f6cc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 73 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 4 | DMM Complexity: 0.4\n\nDIFF:\n@@ -151,7 +151,12 @@ class FeedExporter(object):\n         if not self._exporter_supported(self.format):\n             raise NotConfigured\n         self.store_empty = settings.getbool('FEED_STORE_EMPTY')\n-        self.export_fields = settings.getlist('FEED_EXPORT_FIELDS') or None\n+\n+        if settings.get('FEED_EXPORT_FIELDS') is None:\n+            self.export_fields = None  # don't promote None to []\n+        else:\n+            self.export_fields = settings.getlist('FEED_EXPORT_FIELDS')\n+\n         uripar = settings['FEED_URI_PARAMS']\n         self._uripar = load_object(uripar) if uripar else lambda x, y: None\n \n\n@@ -246,6 +246,10 @@ class FeedExportTest(unittest.TestCase):\n         yield self.assertExported(items, header, rows,\n                                   settings=settings, ordered=True)\n \n+        # edge case: FEED_EXPORT_FIELDS==[] means nothing is exported\n+        settings = {'FEED_EXPORT_FIELDS': []}\n+        yield self.assertExportedJsonLines(items, [{},{},{},{}], settings)\n+\n     @defer.inlineCallbacks\n     def test_export_dicts(self):\n         # When dicts are used, only keys from the first row are used as\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9b0ca1b7a0184532a8c8909d2b88c279e3eaf6cb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 11 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 91 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -151,12 +151,7 @@ class FeedExporter(object):\n         if not self._exporter_supported(self.format):\n             raise NotConfigured\n         self.store_empty = settings.getbool('FEED_STORE_EMPTY')\n-\n-        if settings.get('FEED_EXPORT_FIELDS') is None:\n-            self.export_fields = None  # don't promote None to []\n-        else:\n-            self.export_fields = settings.getlist('FEED_EXPORT_FIELDS')\n-\n+        self.export_fields = settings.getlist('FEED_EXPORT_FIELDS') or None\n         uripar = settings['FEED_URI_PARAMS']\n         self._uripar = load_object(uripar) if uripar else lambda x, y: None\n \n\n@@ -234,7 +234,12 @@ class FeedExportTest(unittest.TestCase):\n         yield self.assertExportedCsv(items, header, rows_csv, ordered=False)\n         yield self.assertExportedJsonLines(items, rows_jl)\n \n-        # but it is possible to override fields using FEED_EXPORT_FIELDS\n+        # edge case: FEED_EXPORT_FIELDS==[] means the same as default None\n+        settings = {'FEED_EXPORT_FIELDS': []}\n+        yield self.assertExportedCsv(items, header, rows_csv, ordered=False)\n+        yield self.assertExportedJsonLines(items, rows_jl, settings)\n+\n+        # it is possible to override fields using FEED_EXPORT_FIELDS\n         header = [\"foo\", \"baz\", \"hello\"]\n         settings = {'FEED_EXPORT_FIELDS': header}\n         rows = [\n@@ -246,10 +251,6 @@ class FeedExportTest(unittest.TestCase):\n         yield self.assertExported(items, header, rows,\n                                   settings=settings, ordered=True)\n \n-        # edge case: FEED_EXPORT_FIELDS==[] means nothing is exported\n-        settings = {'FEED_EXPORT_FIELDS': []}\n-        yield self.assertExportedJsonLines(items, [{},{},{},{}], settings)\n-\n     @defer.inlineCallbacks\n     def test_export_dicts(self):\n         # When dicts are used, only keys from the first row are used as\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#60e79db3eebc321547f09284082516344030fad8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 2 | Churn Cumulative: 23 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -177,7 +177,7 @@ class FeedExporter(object):\n         if not slot.itemcount and not self.store_empty:\n             return\n         slot.exporter.finish_exporting()\n-        logfmt = \"%%s %(format)s feed (%(itemcount)d items) in: %(uri)s\"\n+        logfmt = \"%s %%(format)s feed (%%(itemcount)d items) in: %%(uri)s\"\n         log_args = {'format': self.format,\n                     'itemcount': slot.itemcount,\n                     'uri': slot.uri}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#af0c8f82f4e4242079d9e4545c1000686be2d4a6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#795ca3945fcc3c0394ac9f691722ddf786b5ca53", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 745 | Contributors (this commit): 9 | Commits (past 90d): 6 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,6 @@\n \"\"\"\n This module is kept to provide a helpful warning about its removal.\n \"\"\"\n-\n import logging\n import warnings\n \n@@ -18,7 +17,7 @@ warnings.warn(\"Module `scrapy.log` has been deprecated, Scrapy now relies on \"\n               ScrapyDeprecationWarning, stacklevel=2)\n \n \n-# Imports kept for backwards-compatibility\n+# Imports and level_names variable kept for backwards-compatibility\n \n DEBUG = logging.DEBUG\n INFO = logging.INFO\n@@ -27,6 +26,15 @@ ERROR = logging.ERROR\n CRITICAL = logging.CRITICAL\n SILENT = CRITICAL + 1\n \n+level_names = {\n+    logging.DEBUG: \"DEBUG\",\n+    logging.INFO: \"INFO\",\n+    logging.WARNING: \"WARNING\",\n+    logging.ERROR: \"ERROR\",\n+    logging.CRITICAL: \"CRITICAL\",\n+    SILENT: \"SILENT\",\n+}\n+\n \n def msg(message=None, _level=logging.INFO, **kw):\n     warnings.warn('log.msg has been deprecated, create a python logger and '\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#27ce3225bdf843567e037c2cdba387d0b75e24ef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 167 | Contributors (this commit): 6 | Commits (past 90d): 1 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -12,7 +12,7 @@ def defer_fail(_failure):\n     next reactor loop\n     \"\"\"\n     d = defer.Deferred()\n-    reactor.callLater(0, d.errback, _failure)\n+    reactor.callLater(0.1, d.errback, _failure)\n     return d\n \n def defer_succeed(result):\n@@ -20,7 +20,7 @@ def defer_succeed(result):\n     next reactor loop\n     \"\"\"\n     d = defer.Deferred()\n-    reactor.callLater(0, d.callback, result)\n+    reactor.callLater(0.1, d.callback, result)\n     return d\n \n def defer_result(result):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d439c26d76468d1732128f037a646f72409ea135", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 177 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -8,16 +8,22 @@ from twisted.python import failure\n from scrapy.exceptions import IgnoreRequest\n \n def defer_fail(_failure):\n-    \"\"\"Same as twisted.internet.defer.fail, but delay calling errback until\n+    \"\"\"Same as twisted.internet.defer.fail but delay calling errback until\n     next reactor loop\n+\n+    It delays by 100ms so reactor has a chance to go trough readers and writers\n+    before attending pending delayed calls, so do not set delay to zero.\n     \"\"\"\n     d = defer.Deferred()\n     reactor.callLater(0.1, d.errback, _failure)\n     return d\n \n def defer_succeed(result):\n-    \"\"\"Same as twsited.internet.defer.succed, but delay calling callback until\n+    \"\"\"Same as twisted.internet.defer.succeed but delay calling callback until\n     next reactor loop\n+\n+    It delays by 100ms so reactor has a chance to go trough readers and writers\n+    before attending pending delayed calls, so do not set delay to zero.\n     \"\"\"\n     d = defer.Deferred()\n     reactor.callLater(0.1, d.callback, result)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ebe889a6635ae4ee33186155912a8931bac1692d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 21 | Churn Cumulative: 271 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -30,7 +30,7 @@ In case of status 200 request, response.headers will come with two keys:\n \n import re\n from io import BytesIO\n-from six.moves.urllib.parse import urlparse\n+from six.moves.urllib.parse import urlparse, unquote\n \n from twisted.internet import reactor\n from twisted.protocols.ftp import FTPClient, CommandFailed\n@@ -73,7 +73,7 @@ class FTPDownloadHandler(object):\n                                     request.meta[\"ftp_password\"],\n                                     passive=request.meta.get(\"ftp_passive\", 1))\n         return creator.connectTCP(parsed_url.hostname, parsed_url.port or 21).addCallback(self.gotClient,\n-                                request, parsed_url.path)\n+                                request, unquote(parsed_url.path))\n \n     def gotClient(self, client, request, filepath):\n         self.client = client\n\n@@ -511,7 +511,9 @@ class FTPTestCase(unittest.TestCase):\n         os.mkdir(self.directory)\n         userdir = os.path.join(self.directory, self.username)\n         os.mkdir(userdir)\n-        FilePath(userdir).child('file.txt').setContent(\"I have the power!\")\n+        fp = FilePath(userdir)\n+        fp.child('file.txt').setContent(\"I have the power!\")\n+        fp.child('file with spaces.txt').setContent(\"Moooooooooo power!\")\n \n         # setup server\n         realm = FTPRealm(anonymousRoot=self.directory, userHome=self.directory)\n@@ -547,6 +549,19 @@ class FTPTestCase(unittest.TestCase):\n             self.assertEqual(r.headers, {'Local Filename': [''], 'Size': ['17']})\n         return self._add_test_callbacks(d, _test)\n \n+    def test_ftp_download_path_with_spaces(self):\n+        request = Request(\n+            url=\"ftp://127.0.0.1:%s/file with spaces.txt\" % self.portNum,\n+            meta={\"ftp_user\": self.username, \"ftp_password\": self.password}\n+        )\n+        d = self.download_handler.download_request(request, None)\n+\n+        def _test(r):\n+            self.assertEqual(r.status, 200)\n+            self.assertEqual(r.body, 'Moooooooooo power!')\n+            self.assertEqual(r.headers, {'Local Filename': [''], 'Size': ['18']})\n+        return self._add_test_callbacks(d, _test)\n+\n     def test_ftp_download_notexist(self):\n         request = Request(url=\"ftp://127.0.0.1:%s/notexist.txt\" % self.portNum,\n                 meta={\"ftp_user\": self.username, \"ftp_password\": self.password})\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b0ea3e38d1a045561409ea76c5d267edb24e6f33", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 1295 | Contributors (this commit): 8 | Commits (past 90d): 20 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -207,7 +207,6 @@ class CrawlerProcess(CrawlerRunner):\n     def __init__(self, settings):\n         super(CrawlerProcess, self).__init__(settings)\n         install_shutdown_handlers(self._signal_shutdown)\n-        self.stopping = False\n         configure_logging(settings)\n         log_scrapy_info(settings)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b2765aabd817f26ea40b4373aae1fc84ac86cc2b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1301 | Contributors (this commit): 8 | Commits (past 90d): 21 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -35,8 +35,10 @@ class Crawler(object):\n \n         handler = LogCounterHandler(self, level=settings.get('LOG_LEVEL'))\n         logging.root.addHandler(handler)\n-        self.signals.connect(lambda: logging.root.removeHandler(handler),\n-                             signals.engine_stopped)\n+        # lambda is assigned to Crawler attribute because this way it is not\n+        # garbage collected after leaving __init__ scope\n+        self.__remove_handler = lambda: logging.root.removeHandler(handler)\n+        self.signals.connect(self.__remove_handler, signals.engine_stopped)\n \n         lf_cls = load_object(self.settings['LOG_FORMATTER'])\n         self.logformatter = lf_cls.from_crawler(self)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#aa6a72707daabfb6217f52e4774f2ff038f83dcc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 16 | Churn Cumulative: 1479 | Contributors (this commit): 8 | Commits (past 90d): 27 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -209,8 +209,8 @@ class CrawlerProcess(CrawlerRunner):\n     def __init__(self, settings):\n         super(CrawlerProcess, self).__init__(settings)\n         install_shutdown_handlers(self._signal_shutdown)\n-        configure_logging(settings)\n-        log_scrapy_info(settings)\n+        configure_logging(self.settings)\n+        log_scrapy_info(self.settings)\n \n     def _signal_shutdown(self, signum, _):\n         install_shutdown_handlers(self._signal_kill)\n\n@@ -1,7 +1,7 @@\n import warnings\n import unittest\n \n-from scrapy.crawler import Crawler, CrawlerRunner\n+from scrapy.crawler import Crawler, CrawlerRunner, CrawlerProcess\n from scrapy.settings import Settings, default_settings\n from scrapy.spiderloader import SpiderLoader\n from scrapy.utils.spider import DefaultSpider\n@@ -104,3 +104,13 @@ class CrawlerRunnerTestCase(unittest.TestCase):\n             self.assertEqual(len(w), 1)\n             self.assertIn('Please use SPIDER_LOADER_CLASS', str(w[0].message))\n \n+\n+class CrawlerProcessTest(unittest.TestCase):\n+    def test_crawler_process_accepts_dict(self):\n+        runner = CrawlerProcess({'foo': 'bar'})\n+        self.assertEqual(runner.settings['foo'], 'bar')\n+        self.assertEqual(\n+            runner.settings['RETRY_ENABLED'],\n+            default_settings.RETRY_ENABLED\n+        )\n+        self.assertIsInstance(runner.settings, Settings)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4446baae33b8654ec505df06cf95528eff5ccaf1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 86/83 | Churn Δ: 24 | Churn Cumulative: 37 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,11 +1,21 @@\n from email.utils import formatdate\n+from twisted.internet import defer\n+from twisted.internet.error import TimeoutError, DNSLookupError, \\\n+        ConnectionRefusedError, ConnectionDone, ConnectError, \\\n+        ConnectionLost, TCPTimedOutError\n from scrapy import signals\n from scrapy.exceptions import NotConfigured, IgnoreRequest\n from scrapy.utils.misc import load_object\n+from scrapy.xlib.tx import ResponseFailed\n \n \n class HttpCacheMiddleware(object):\n \n+    DOWNLOAD_EXCEPTIONS = (defer.TimeoutError, TimeoutError, DNSLookupError,\n+                           ConnectionRefusedError, ConnectionDone, ConnectError,\n+                           ConnectionLost, TCPTimedOutError, ResponseFailed,\n+                           IOError)\n+\n     def __init__(self, settings, stats):\n         if not settings.getbool('HTTPCACHE_ENABLED'):\n             raise NotConfigured\n@@ -84,6 +94,12 @@ class HttpCacheMiddleware(object):\n         self._cache_response(spider, response, request, cachedresponse)\n         return response\n \n+    def process_exception(self, request, exception, spider):\n+        cachedresponse = request.meta.pop('cached_response', None)\n+        if cachedresponse is not None and isinstance(exception, self.DOWNLOAD_EXCEPTIONS):\n+            self.stats.inc_value('httpcache/errorrecovery', spider=spider)\n+            return cachedresponse\n+\n     def _cache_response(self, spider, response, request, cachedresponse):\n         if self.policy.should_cache_response(response, request):\n             self.stats.inc_value('httpcache/store', spider=spider)\n\n@@ -99,6 +99,14 @@ class RFC2616Policy(object):\n         return False\n \n     def is_cached_response_valid(self, cachedresponse, response, request):\n+        # Use the cached response if the new response is a server error,\n+        # as long as the old response didn't specify must-revalidate.\n+        if response.status >= 500:\n+            cc = self._parse_cachecontrol(cachedresponse)\n+            if 'must-revalidate' not in cc:\n+                return True\n+\n+        # Use the cached response if the server says it hasn't changed.\n         return response.status == 304\n \n     def _set_conditional_validators(self, request, cachedresponse):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
