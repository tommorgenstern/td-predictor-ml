{"custom_id": "keras#860b1ca4da441f49ea749eb9e73408113e45e6de", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 88 | Lines Deleted: 20 | Files Changed: 6 | Hunks: 14 | Methods Changed: 9 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 108 | Churn Cumulative: 1168 | Contributors (this commit): 9 | Commits (past 90d): 36 | Contributors (cumulative): 20 | DMM Complexity: 0.0625\n\nDIFF:\n@@ -0,0 +1,58 @@\n+import torch\n+\n+from keras_core import ops\n+from keras_core import optimizers\n+from keras_core.backend.torch.optimizers import torch_parallel_optimizer\n+\n+\n+class Adam(torch_parallel_optimizer.TorchParallelOptimizer, optimizers.Adam):\n+    def _parallel_update_step(\n+        self,\n+        grads,\n+        variables,\n+        learning_rate,\n+    ):\n+        keras_variables = variables\n+        variables = [v.value for v in variables]\n+\n+        dtype = variables[0].dtype\n+        lr = ops.cast(learning_rate, dtype)\n+        local_step = ops.cast(self.iterations + 1, dtype)\n+\n+        beta_1_power = ops.power(ops.cast(self.beta_1, dtype), local_step)\n+        beta_2_power = ops.power(ops.cast(self.beta_2, dtype), local_step)\n+        alpha = lr * ops.sqrt(1 - beta_2_power) / (1 - beta_1_power)\n+\n+        m_list = [\n+            self._momentums[self._get_variable_index(variable)].value\n+            for variable in keras_variables\n+        ]\n+        v_list = [\n+            self._velocities[self._get_variable_index(variable)].value\n+            for variable in keras_variables\n+        ]\n+\n+        torch._foreach_mul_(m_list, self.beta_1)\n+        torch._foreach_add_(m_list, grads, alpha=1 - self.beta_1)\n+\n+        torch._foreach_mul_(v_list, self.beta_2)\n+        torch._foreach_add_(\n+            v_list, torch._foreach_mul(grads, grads), alpha=1 - self.beta_2\n+        )\n+\n+        if self.amsgrad:\n+            v_hat_list = [\n+                self._velocity_hats[self._get_variable_index(variable)].value\n+                for variable in keras_variables\n+            ]\n+            torch._foreach_maximum_(v_hat_list, v_list)\n+            v_list = v_hat_list\n+\n+        torch._foreach_add_(\n+            variables,\n+            torch._foreach_div(\n+                torch._foreach_mul(m_list, alpha),\n+                torch._foreach_add(torch._foreach_sqrt(v_list), self.epsilon),\n+            ),\n+            alpha=-1,\n+        )\n\n@@ -1,15 +1,19 @@\n import torch\n \n+from keras_core import optimizers\n from keras_core.optimizers.base_optimizer import BaseOptimizer\n \n \n class TorchOptimizer(BaseOptimizer):\n     def __new__(cls, *args, **kwargs):\n         # Import locally to avoid circular imports.\n-        from keras_core import optimizers\n+        from keras_core.backend.torch.optimizers import torch_adam\n         from keras_core.backend.torch.optimizers import torch_sgd\n \n-        OPTIMIZERS = {optimizers.SGD: torch_sgd.SGD}\n+        OPTIMIZERS = {\n+            optimizers.SGD: torch_sgd.SGD,\n+            optimizers.Adam: torch_adam.Adam,\n+        }\n         if cls in OPTIMIZERS:\n             return OPTIMIZERS[cls](*args, **kwargs)\n         return super().__new__(cls)\n\n@@ -0,0 +1,13 @@\n+from keras_core.optimizers.base_optimizer import BaseOptimizer\n+\n+\n+class TorchParallelOptimizer(BaseOptimizer):\n+    def _internal_apply_gradients(self, grads_and_vars):\n+        grads, trainable_variables = zip(*grads_and_vars)\n+\n+        self._parallel_update_step(\n+            grads,\n+            trainable_variables,\n+            self._get_current_learning_rate(),\n+        )\n+        self.iterations.assign(self.iterations + 1)\n\n@@ -1,29 +1,22 @@\n import torch\n \n from keras_core import optimizers\n+from keras_core.backend.torch.optimizers import torch_parallel_optimizer\n \n \n-class SGD(optimizers.SGD):\n-    def _internal_apply_gradients(self, grads_and_vars):\n-        grads, trainable_variables = zip(*grads_and_vars)\n-\n-        self._parallel_update_step(\n-            grads,\n-            [v.value for v in trainable_variables],\n-            self._get_current_learning_rate(),\n-        )\n-        self.iterations.assign(self.iterations + 1)\n-\n+class SGD(torch_parallel_optimizer.TorchParallelOptimizer, optimizers.SGD):\n     def _parallel_update_step(\n         self,\n         grads,\n         variables,\n         learning_rate,\n     ):\n+        keras_variables = variables\n+        variables = [v.value for v in variables]\n         if self.momentum != 0:\n             bufs = [\n-                self.momentums[self._get_variable_index(variable.value)]\n-                for variable in variables\n+                self.momentums[self._get_variable_index(variable.value)].value\n+                for variable in keras_variables\n             ]\n \n             for i in range(len(bufs)):\n\n@@ -5,7 +5,6 @@ from keras_core.backend import KerasTensor\n \n class SymbolicArguments:\n     def __init__(self, *args, **kwargs):\n-\n         self.args = tree.map_structure(lambda x: x, args)\n         self.kwargs = tree.map_structure(lambda x: x, kwargs)\n         self._flat_arguments = tree.flatten((self.args, self.kwargs))\n\n@@ -2,6 +2,7 @@ import numpy as np\n \n import keras_core\n from keras_core import backend\n+from keras_core import ops\n from keras_core import testing\n from keras_core.optimizers.adam import Adam\n \n@@ -19,14 +20,14 @@ class AdamTest(testing.TestCase):\n \n     def test_single_step(self):\n         optimizer = Adam(learning_rate=0.5)\n-        grads = np.array([1.0, 6.0, 7.0, 2.0])\n+        grads = ops.array([1.0, 6.0, 7.0, 2.0])\n         vars = backend.Variable([1.0, 2.0, 3.0, 4.0])\n         optimizer.apply_gradients(zip([grads], [vars]))\n         self.assertAllClose(vars, [0.5, 1.5, 2.5, 3.5], rtol=1e-4, atol=1e-4)\n \n     def test_weight_decay(self):\n         grads, var1, var2, var3 = (\n-            np.zeros(()),\n+            ops.zeros(()),\n             backend.Variable(2.0),\n             backend.Variable(2.0, name=\"exclude\"),\n             backend.Variable(2.0),\n@@ -50,8 +51,8 @@ class AdamTest(testing.TestCase):\n         optimizer = Adam(amsgrad=True)\n \n         x = backend.Variable(np.ones([10]))\n-        grads = np.arange(0.1, 1.1, 0.1)\n-        first_grads = np.full((10,), 0.01)\n+        grads = ops.arange(0.1, 1.1, 0.1)\n+        first_grads = ops.full((10,), 0.01)\n \n         golden = np.tile(\n             [[0.999], [0.9982], [0.9974], [0.9965], [0.9955]], (1, 10)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a0d777658573c55860f890ddbc514441f6d6cb60", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 84 | Files Changed: 4 | Hunks: 14 | Methods Changed: 8 | Complexity Δ (Sum/Max): -8/0 | Churn Δ: 104 | Churn Cumulative: 599 | Contributors (this commit): 9 | Commits (past 90d): 22 | Contributors (cumulative): 14 | DMM Complexity: 0.0\n\nDIFF:\n@@ -0,0 +1,6 @@\n+from keras_core import optimizers\n+from keras_core.backend.torch.optimizers import torch_adam\n+\n+\n+class AdamW(torch_adam.Adam, optimizers.AdamW):\n+    pass\n\n@@ -8,11 +8,13 @@ class TorchOptimizer(BaseOptimizer):\n     def __new__(cls, *args, **kwargs):\n         # Import locally to avoid circular imports.\n         from keras_core.backend.torch.optimizers import torch_adam\n+        from keras_core.backend.torch.optimizers import torch_adamw\n         from keras_core.backend.torch.optimizers import torch_sgd\n \n         OPTIMIZERS = {\n             optimizers.SGD: torch_sgd.SGD,\n             optimizers.Adam: torch_adam.Adam,\n+            optimizers.AdamW: torch_adamw.AdamW,\n         }\n         if cls in OPTIMIZERS:\n             return OPTIMIZERS[cls](*args, **kwargs)\n\n@@ -1,10 +1,10 @@\n-from keras_core import ops\n from keras_core.api_export import keras_core_export\n+from keras_core.optimizers import adam\n from keras_core.optimizers import optimizer\n \n \n @keras_core_export([\"keras_core.optimizers.AdamW\"])\n-class AdamW(optimizer.Optimizer):\n+class AdamW(adam.Adam):\n     \"\"\"Optimizer that implements the AdamW algorithm.\n \n     AdamW optimization is a stochastic gradient descent method that is based on\n@@ -68,7 +68,12 @@ class AdamW(optimizer.Optimizer):\n     ):\n         super().__init__(\n             learning_rate=learning_rate,\n+            beta_1=beta_1,\n+            beta_2=beta_2,\n+            epsilon=epsilon,\n+            amsgrad=amsgrad,\n             name=name,\n+            weight_decay=weight_decay,\n             clipnorm=clipnorm,\n             clipvalue=clipvalue,\n             global_clipnorm=global_clipnorm,\n@@ -76,11 +81,6 @@ class AdamW(optimizer.Optimizer):\n             ema_momentum=ema_momentum,\n             ema_overwrite_frequency=ema_overwrite_frequency,\n         )\n-        self.weight_decay = weight_decay\n-        self.beta_1 = beta_1\n-        self.beta_2 = beta_2\n-        self.epsilon = epsilon\n-        self.amsgrad = amsgrad\n \n         if self.weight_decay is None:\n             raise ValueError(\n@@ -88,79 +88,6 @@ class AdamW(optimizer.Optimizer):\n                 \"weight_decay=None\"\n             )\n \n-    def build(self, var_list):\n-        \"\"\"Initialize optimizer variables.\n-\n-        AdamW optimizer has 3 types of variables: momentums, velocities and\n-        velocity_hat (only set when amsgrad is applied),\n-\n-        Args:\n-            var_list: list of model variables to build AdamW variables on.\n-        \"\"\"\n-        if self.built:\n-            return\n-        super().build(var_list)\n-        self._momentums = []\n-        self._velocities = []\n-        for var in var_list:\n-            self._momentums.append(\n-                self.add_variable_from_reference(\n-                    reference_variable=var, name=\"m\"\n-                )\n-            )\n-            self._velocities.append(\n-                self.add_variable_from_reference(\n-                    reference_variable=var, name=\"v\"\n-                )\n-            )\n-        if self.amsgrad:\n-            self._velocity_hats = []\n-            for var in var_list:\n-                self._velocity_hats.append(\n-                    self.add_variable_from_reference(\n-                        reference_variable=var, name=\"vhat\"\n-                    )\n-                )\n-\n-    def update_step(self, gradient, variable, learning_rate):\n-        \"\"\"Update step given gradient and the associated model variable.\"\"\"\n-        lr = ops.cast(learning_rate, variable.dtype)\n-        gradient = ops.cast(gradient, variable.dtype)\n-        local_step = ops.cast(self.iterations + 1, variable.dtype)\n-        beta_1_power = ops.power(\n-            ops.cast(self.beta_1, variable.dtype), local_step\n-        )\n-        beta_2_power = ops.power(\n-            ops.cast(self.beta_2, variable.dtype), local_step\n-        )\n-\n-        m = self._momentums[self._get_variable_index(variable)]\n-        v = self._velocities[self._get_variable_index(variable)]\n-\n-        alpha = lr * ops.sqrt(1 - beta_2_power) / (1 - beta_1_power)\n-\n-        m.assign(m + (gradient - m) * (1 - self.beta_1))\n-        v.assign(v + (ops.square(gradient) - v) * (1 - self.beta_2))\n-        if self.amsgrad:\n-            v_hat = self._velocity_hats[self._get_variable_index(variable)]\n-            v_hat.assign(ops.maximum(v_hat, v))\n-            v = v_hat\n-        variable.assign(variable - (m * alpha) / (ops.sqrt(v) + self.epsilon))\n-\n-    def get_config(self):\n-        config = super().get_config()\n-\n-        config.update(\n-            {\n-                \"weight_decay\": self.weight_decay,\n-                \"beta_1\": self.beta_1,\n-                \"beta_2\": self.beta_2,\n-                \"epsilon\": self.epsilon,\n-                \"amsgrad\": self.amsgrad,\n-            }\n-        )\n-        return config\n-\n \n AdamW.__doc__ = AdamW.__doc__.replace(\n     \"{{base_optimizer_keyword_args}}\", optimizer.base_optimizer_keyword_args\n\n@@ -4,6 +4,7 @@\n import numpy as np\n \n from keras_core import backend\n+from keras_core import ops\n from keras_core import testing\n from keras_core.optimizers.adamw import AdamW\n \n@@ -22,7 +23,7 @@ class AdamWTest(testing.TestCase):\n \n     def test_single_step(self):\n         optimizer = AdamW(learning_rate=0.5)\n-        grads = np.array([1.0, 6.0, 7.0, 2.0])\n+        grads = ops.array([1.0, 6.0, 7.0, 2.0])\n         vars = backend.Variable([1.0, 2.0, 3.0, 4.0])\n         optimizer.apply_gradients(zip([grads], [vars]))\n         self.assertAllClose(\n@@ -31,7 +32,7 @@ class AdamWTest(testing.TestCase):\n \n     def test_weight_decay(self):\n         grads, var1, var2, var3 = (\n-            np.zeros(()),\n+            ops.zeros(()),\n             backend.Variable(2.0),\n             backend.Variable(2.0, name=\"exclude\"),\n             backend.Variable(2.0),\n@@ -55,8 +56,8 @@ class AdamWTest(testing.TestCase):\n         optimizer = AdamW(learning_rate=1.0, weight_decay=0.5, epsilon=2)\n \n         x = backend.Variable(np.ones([10]))\n-        grads = np.arange(0.1, 1.1, 0.1)\n-        first_grads = np.full((10,), 0.01)\n+        grads = ops.arange(0.1, 1.1, 0.1)\n+        first_grads = ops.full((10,), 0.01)\n \n         # fmt: off\n         golden = np.array(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#60943630150782972468c9e03d7bcddfce197136", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 169 | Lines Deleted: 2 | Files Changed: 6 | Hunks: 9 | Methods Changed: 11 | Complexity Δ (Sum/Max): 13/6 | Churn Δ: 171 | Churn Cumulative: 3178 | Contributors (this commit): 16 | Commits (past 90d): 114 | Contributors (cumulative): 31 | DMM Complexity: 0.6055045871559633\n\nDIFF:\n@@ -6,6 +6,25 @@ class TFLayer(tf.__internal__.tracking.AutoTrackable):\n         \"\"\"Can be overriden to perform post-build actions.\"\"\"\n         pass\n \n+    def _trackable_children(self, save_type=\"checkpoint\", **kwargs):\n+        if save_type == \"savedmodel\":\n+            # SavedModel needs to ignore the execution functions.\n+            train_function = getattr(self, \"train_function\", None)\n+            test_function = getattr(self, \"test_function\", None)\n+            predict_function = getattr(self, \"predict_function\", None)\n+            self.train_function = None\n+            self.test_function = None\n+            self.predict_function = None\n+\n+        children = super()._trackable_children(save_type, **kwargs)\n+\n+        if save_type == \"savedmodel\":\n+            self.train_function = train_function\n+            self.test_function = test_function\n+            self.predict_function = predict_function\n+\n+        return children\n+\n     @property\n     def _default_save_signature(self):\n         \"\"\"For SavedModel support: returns the default serving signature.\"\"\"\n\n@@ -0,0 +1,99 @@\n+\"\"\"Tests for SavedModel functionality under tf implementation.\"\"\"\n+\n+import os\n+\n+import numpy as np\n+import pytest\n+import tensorflow as tf\n+\n+from keras_core import backend\n+from keras_core import layers\n+from keras_core import metrics\n+from keras_core import models\n+from keras_core import testing\n+from keras_core.saving import object_registration\n+\n+\n+@object_registration.register_keras_serializable(package=\"my_package\")\n+class CustomModelX(models.Model):\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.dense1 = layers.Dense(1)\n+        self.dense2 = layers.Dense(1)\n+\n+    def call(self, inputs):\n+        out = self.dense1(inputs)\n+        return self.dense2(out)\n+\n+    def one(self):\n+        return 1\n+\n+\n+@pytest.mark.skipif(\n+    backend.backend() != \"tensorflow\",\n+    reason=\"The SavedModel test can only run with TF backend.\",\n+)\n+class SavedModelTest(testing.TestCase):\n+    def test_sequential(self):\n+        model = models.Sequential([layers.Dense(1)])\n+        model.compile(loss=\"mse\", optimizer=\"adam\")\n+        X_train = np.random.rand(100, 3)\n+        y_train = np.random.rand(100, 1)\n+        model.fit(X_train, y_train)\n+        path = os.path.join(self.get_temp_dir(), \"my_keras_core_model\")\n+        tf.saved_model.save(model, path)\n+        restored_model = tf.saved_model.load(path)\n+        self.assertAllClose(\n+            model(X_train),\n+            restored_model.signatures[\"serving_default\"](\n+                tf.convert_to_tensor(X_train, dtype=tf.float32)\n+            )[\"output_0\"],\n+            rtol=1e-4,\n+            atol=1e-4,\n+        )\n+\n+    def test_functional(self):\n+        inputs = layers.Input(shape=(3,))\n+        x = layers.Dense(1, name=\"first_dense\")(inputs)\n+        outputs = layers.Dense(1, name=\"second_dense\")(x)\n+        model = models.Model(inputs, outputs)\n+        model.compile(\n+            optimizer=\"adam\",\n+            loss=\"mse\",\n+        )\n+        X_train = np.random.rand(100, 3)\n+        y_train = np.random.rand(100, 1)\n+        model.fit(X_train, y_train)\n+        path = os.path.join(self.get_temp_dir(), \"my_keras_core_model\")\n+        tf.saved_model.save(model, path)\n+        restored_model = tf.saved_model.load(path)\n+        self.assertAllClose(\n+            model(X_train),\n+            restored_model.signatures[\"serving_default\"](\n+                tf.convert_to_tensor(X_train, dtype=tf.float32)\n+            )[\"output_0\"],\n+            rtol=1e-4,\n+            atol=1e-4,\n+        )\n+\n+    def test_subclassed(self):\n+        model = CustomModelX()\n+        model.compile(\n+            optimizer=\"adam\",\n+            loss=\"mse\",\n+            metrics=[metrics.Hinge(), \"mse\"],\n+        )\n+        X_train = np.random.rand(100, 3)\n+        y_train = np.random.rand(100, 1)\n+        model.fit(X_train, y_train)\n+        path = os.path.join(self.get_temp_dir(), \"my_keras_core_model\")\n+        tf.saved_model.save(model, path)\n+        restored_model = tf.saved_model.load(path)\n+        self.assertAllClose(\n+            model(X_train),\n+            restored_model.signatures[\"serving_default\"](\n+                tf.convert_to_tensor(X_train, dtype=tf.float32)\n+            )[\"output_0\"],\n+            rtol=1e-4,\n+            atol=1e-4,\n+        )\n\n@@ -52,8 +52,9 @@ class LearningRateSchedulerTest(testing.TestCase):\n             lambda step: 1.0 / (1.0 + step), verbose=1\n         )\n         io_utils.disable_interactive_logging()\n+        io_utils.set_logging_verbosity(\"INFO\")\n \n-        with self.assertLogs(level=\"INFO\") as logs:\n+        with self.assertLogs() as logs:\n             self.model.fit(\n                 self.x_train,\n                 self.y_train,\n\n@@ -83,8 +83,9 @@ class ReduceLROnPlateauTest(testing.TestCase):\n             patience=1, factor=0.1, monitor=\"val_loss\", min_delta=100, verbose=1\n         )\n         io_utils.disable_interactive_logging()\n+        io_utils.set_logging_verbosity(\"INFO\")\n \n-        with self.assertLogs(level=\"INFO\") as logs:\n+        with self.assertLogs() as logs:\n             self.model.fit(\n                 self.x_train,\n                 self.y_train,\n\n@@ -295,12 +295,23 @@ class Layer(BackendLayer, Operation):\n                 ),\n             }\n         )\n+        if backend.backend() == \"tensorflow\":\n+            # Remove attribute tracking for lists (TF-specific attribute)\n+            _self_setattr_tracking = getattr(\n+                self, \"_self_setattr_tracking\", True\n+            )\n+            self._self_setattr_tracking = False\n+\n         self._trainable_variables = trainable_variables\n         self._non_trainable_variables = non_trainable_variables\n         self._layers = layers\n         self._metrics = metrics\n         self._seed_generators = seed_generators\n \n+        if backend.backend() == \"tensorflow\":\n+            # Reset attribute tracking (TF-specific)\n+            self._self_setattr_tracking = _self_setattr_tracking\n+\n     @property\n     def input_spec(self):\n         return self._input_spec\n\n@@ -58,6 +58,42 @@ def is_interactive_logging_enabled():\n     return global_state.get_global_setting(\"interactive_logging\", True)\n \n \n+@keras_core_export(\n+    [\n+        \"keras_core.config.set_logging_verbosity\",\n+        \"keras_core.utils.set_logging_verbosity\",\n+    ]\n+)\n+def set_logging_verbosity(level):\n+    \"\"\"Sets the verbosity level for logging.\n+    The log levels are as follows:\n+\n+        - \"FATAL\" (least verbose)\n+        - \"ERROR\"\n+        - \"WARNING\"\n+        - \"INFO\"\n+        - \"DEBUG\" (most verbose)\n+\n+    Args:\n+        level: A string corresponding to the level of verbosity for logging.\n+    \"\"\"\n+    valid_levels = {\n+        \"FATAL\": logging.FATAL,\n+        \"ERROR\": logging.ERROR,\n+        \"WARNING\": logging.WARNING,\n+        \"INFO\": logging.INFO,\n+        \"DEBUG\": logging.DEBUG,\n+    }\n+    verbosity = valid_levels.get(level)\n+    if verbosity is None:\n+        raise ValueError(\n+            \"Please pass a valid level for logging verbosity. \"\n+            f\"The valid levels are {valid_levels.keys()}. \"\n+            f\"Received: {level}\"\n+        )\n+    logging.set_verbosity(verbosity)\n+\n+\n def print_msg(message, line_break=True):\n     \"\"\"Print the message to absl logging or stdout.\"\"\"\n     if is_interactive_logging_enabled():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e7050054330d7c571ebbf49d045dbf0e20d92fd0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 21 | Churn Cumulative: 249 | Contributors (this commit): 5 | Commits (past 90d): 9 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -58,21 +58,16 @@ def is_interactive_logging_enabled():\n     return global_state.get_global_setting(\"interactive_logging\", True)\n \n \n-@keras_core_export(\n-    [\n-        \"keras_core.config.set_logging_verbosity\",\n-        \"keras_core.utils.set_logging_verbosity\",\n-    ]\n-)\n def set_logging_verbosity(level):\n     \"\"\"Sets the verbosity level for logging.\n-    The log levels are as follows:\n \n-        - \"FATAL\" (least verbose)\n-        - \"ERROR\"\n-        - \"WARNING\"\n-        - \"INFO\"\n-        - \"DEBUG\" (most verbose)\n+    Supported log levels are as follows:\n+\n+    - `\"FATAL\"` (least verbose)\n+    - `\"ERROR\"`\n+    - `\"WARNING\"`\n+    - `\"INFO\"`\n+    - `\"DEBUG\"` (most verbose)\n \n     Args:\n         level: A string corresponding to the level of verbosity for logging.\n@@ -88,7 +83,7 @@ def set_logging_verbosity(level):\n     if verbosity is None:\n         raise ValueError(\n             \"Please pass a valid level for logging verbosity. \"\n-            f\"The valid levels are {valid_levels.keys()}. \"\n+            f\"Expected one of: {set(valid_levels.keys())}. \"\n             f\"Received: {level}\"\n         )\n     logging.set_verbosity(verbosity)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5937df8275a8168eb3d91b3bc1718b77a32fe4ed", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 74 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 9 | Methods Changed: 5 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 79 | Churn Cumulative: 361 | Contributors (this commit): 7 | Commits (past 90d): 14 | Contributors (cumulative): 9 | DMM Complexity: 0.0392156862745098\n\nDIFF:\n@@ -9,13 +9,16 @@ class TorchOptimizer(BaseOptimizer):\n         # Import locally to avoid circular imports.\n         from keras_core.backend.torch.optimizers import torch_adam\n         from keras_core.backend.torch.optimizers import torch_adamw\n+        from keras_core.backend.torch.optimizers import torch_rmsprop\n         from keras_core.backend.torch.optimizers import torch_sgd\n \n         OPTIMIZERS = {\n-            optimizers.SGD: torch_sgd.SGD,\n             optimizers.Adam: torch_adam.Adam,\n             optimizers.AdamW: torch_adamw.AdamW,\n+            optimizers.RMSprop: torch_rmsprop.RMSprop,\n+            optimizers.SGD: torch_sgd.SGD,\n         }\n+\n         if cls in OPTIMIZERS:\n             return OPTIMIZERS[cls](*args, **kwargs)\n         return super().__new__(cls)\n\n@@ -0,0 +1,65 @@\n+import torch\n+\n+from keras_core import ops\n+from keras_core import optimizers\n+from keras_core.backend.torch.optimizers import torch_parallel_optimizer\n+\n+\n+class RMSprop(\n+    torch_parallel_optimizer.TorchParallelOptimizer, optimizers.RMSprop\n+):\n+    def _parallel_update_step(\n+        self,\n+        grads,\n+        variables,\n+        learning_rate,\n+    ):\n+        keras_variables = variables\n+        variables = [v.value for v in variables]\n+\n+        dtype = variables[0].dtype\n+        lr = ops.cast(learning_rate, dtype)\n+\n+        velocities = [\n+            self._velocities[self._get_variable_index(variable)].value\n+            for variable in keras_variables\n+        ]\n+\n+        rho = self.rho\n+\n+        torch._foreach_mul_(velocities, rho)\n+        torch._foreach_add_(\n+            velocities, torch._foreach_mul(grads, grads), alpha=1 - rho\n+        )\n+\n+        denominators = torch._foreach_add(velocities, self.epsilon)\n+        if self.centered:\n+            average_grads = [\n+                self._average_gradients[\n+                    self._get_variable_index(variable)\n+                ].value\n+                for variable in keras_variables\n+            ]\n+            torch._foreach_mul_(average_grads, rho)\n+            torch._foreach_add_(average_grads, grads, alpha=1 - rho)\n+            torch._foreach_add_(\n+                denominators,\n+                torch._foreach_mul(average_grads, average_grads),\n+                alpha=-1,\n+            )\n+        torch._foreach_sqrt_(denominators)\n+        increments = torch._foreach_div(\n+            torch._foreach_mul(grads, lr), denominators\n+        )\n+\n+        if self.momentum > 0:\n+            momentum_list = [\n+                self._momentums[self._get_variable_index(variable)].value\n+                for variable in keras_variables\n+            ]\n+            momentum_list = torch._foreach_add(\n+                increments, momentum_list, alpha=self.momentum\n+            )\n+            torch._foreach_add_(variables, momentum_list, alpha=-1)\n+        else:\n+            torch._foreach_add_(variables, increments, alpha=-1)\n\n@@ -1,6 +1,7 @@\n import numpy as np\n \n from keras_core import backend\n+from keras_core import ops\n from keras_core import testing\n from keras_core.optimizers.rmsprop import RMSprop\n \n@@ -18,7 +19,7 @@ class RMSpropTest(testing.TestCase):\n \n     def test_single_step(self):\n         optimizer = RMSprop(learning_rate=0.5)\n-        grads = np.array([1.0, 6.0, 7.0, 2.0])\n+        grads = ops.array([1.0, 6.0, 7.0, 2.0])\n         vars = backend.Variable([1.0, 2.0, 3.0, 4.0])\n         optimizer.apply_gradients(zip([grads], [vars]))\n         self.assertAllClose(\n@@ -27,7 +28,7 @@ class RMSpropTest(testing.TestCase):\n \n     def test_weight_decay(self):\n         grads, var1, var2, var3 = (\n-            np.zeros(()),\n+            ops.zeros(()),\n             backend.Variable(2.0),\n             backend.Variable(2.0, name=\"exclude\"),\n             backend.Variable(2.0),\n@@ -51,8 +52,8 @@ class RMSpropTest(testing.TestCase):\n         optimizer = RMSprop(centered=True)\n \n         x = backend.Variable(np.ones([10]))\n-        grads = np.arange(0.1, 1.1, 0.1)\n-        first_grads = np.full((10,), 0.01)\n+        grads = ops.arange(0.1, 1.1, 0.1)\n+        first_grads = ops.full((10,), 0.01)\n \n         golden = np.tile(\n             [[0.9967], [0.9933], [0.9908], [0.9885], [0.9864]], (1, 10)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bacd49c4d27e2aadc7b90651c42f8f8eb00b41b5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 8 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,2 +1,2 @@\n # Unique source of truth for the version number.\n-__version__ = \"0.1.0\"\n+__version__ = \"0.1.1\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2e9ed2ddba6cb6855e3244bc2482f28c410d913d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 66 | Lines Deleted: 8 | Files Changed: 5 | Hunks: 9 | Methods Changed: 8 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 74 | Churn Cumulative: 716 | Contributors (this commit): 6 | Commits (past 90d): 25 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,56 @@\n+import torch\n+\n+from keras_core import ops\n+from keras_core import optimizers\n+from keras_core.backend.torch.optimizers import torch_parallel_optimizer\n+\n+\n+class Adadelta(\n+    torch_parallel_optimizer.TorchParallelOptimizer, optimizers.Adadelta\n+):\n+    def _parallel_update_step(\n+        self,\n+        grads,\n+        variables,\n+        learning_rate,\n+    ):\n+        keras_variables = variables\n+        variables = [v.value for v in variables]\n+\n+        dtype = variables[0].dtype\n+        lr = ops.cast(learning_rate, dtype)\n+        rho = self.rho\n+\n+        accumulated_grads = [\n+            self._accumulated_grads[self._get_variable_index(variable)].value\n+            for variable in keras_variables\n+        ]\n+        accumulated_delta_vars = [\n+            self._accumulated_delta_vars[\n+                self._get_variable_index(variable)\n+            ].value\n+            for variable in keras_variables\n+        ]\n+        torch._foreach_mul_(accumulated_grads, rho)\n+        torch._foreach_add_(\n+            accumulated_grads, torch._foreach_mul(grads, grads), alpha=1 - rho\n+        )\n+\n+        def rms(x):\n+            return torch._foreach_sqrt(torch._foreach_add(x, self.epsilon))\n+\n+        delta_vars = torch._foreach_mul(\n+            torch._foreach_div(\n+                torch._foreach_mul(rms(accumulated_delta_vars), grads),\n+                rms(accumulated_grads),\n+            ),\n+            -1,\n+        )\n+        torch._foreach_mul_(accumulated_delta_vars, rho)\n+        torch._foreach_add_(\n+            accumulated_delta_vars,\n+            torch._foreach_mul(delta_vars, delta_vars),\n+            alpha=1 - rho,\n+        )\n+\n+        torch._foreach_add_(variables, delta_vars, alpha=lr)\n\n@@ -7,12 +7,14 @@ from keras_core.optimizers.base_optimizer import BaseOptimizer\n class TorchOptimizer(BaseOptimizer):\n     def __new__(cls, *args, **kwargs):\n         # Import locally to avoid circular imports.\n+        from keras_core.backend.torch.optimizers import torch_adadelta\n         from keras_core.backend.torch.optimizers import torch_adam\n         from keras_core.backend.torch.optimizers import torch_adamw\n         from keras_core.backend.torch.optimizers import torch_rmsprop\n         from keras_core.backend.torch.optimizers import torch_sgd\n \n         OPTIMIZERS = {\n+            optimizers.Adadelta: torch_adadelta.Adadelta,\n             optimizers.Adam: torch_adam.Adam,\n             optimizers.AdamW: torch_adamw.AdamW,\n             optimizers.RMSprop: torch_rmsprop.RMSprop,\n\n@@ -57,9 +57,8 @@ class RMSprop(\n                 self._momentums[self._get_variable_index(variable)].value\n                 for variable in keras_variables\n             ]\n-            momentum_list = torch._foreach_add(\n-                increments, momentum_list, alpha=self.momentum\n-            )\n+            torch._foreach_mul_(momentum_list, self.momentum)\n+            torch._foreach_add_(momentum_list, increments)\n             torch._foreach_add_(variables, momentum_list, alpha=-1)\n         else:\n             torch._foreach_add_(variables, increments, alpha=-1)\n\n@@ -15,7 +15,7 @@ class SGD(torch_parallel_optimizer.TorchParallelOptimizer, optimizers.SGD):\n         variables = [v.value for v in variables]\n         if self.momentum != 0:\n             bufs = [\n-                self.momentums[self._get_variable_index(variable.value)].value\n+                self.momentums[self._get_variable_index(variable)].value\n                 for variable in keras_variables\n             ]\n \n\n@@ -1,6 +1,7 @@\n import numpy as np\n \n from keras_core import backend\n+from keras_core import ops\n from keras_core import testing\n from keras_core.optimizers.adadelta import Adadelta\n \n@@ -16,7 +17,7 @@ class AdadeltaTest(testing.TestCase):\n \n     def test_single_step(self):\n         optimizer = Adadelta(learning_rate=0.5)\n-        grads = np.array([1.0, 6.0, 7.0, 2.0])\n+        grads = ops.array([1.0, 6.0, 7.0, 2.0])\n         vars = backend.Variable([1.0, 2.0, 3.0, 4.0])\n         optimizer.apply_gradients(zip([grads], [vars]))\n         self.assertAllClose(\n@@ -25,7 +26,7 @@ class AdadeltaTest(testing.TestCase):\n \n     def test_weight_decay(self):\n         grads, var1, var2, var3 = (\n-            np.zeros(()),\n+            ops.zeros(()),\n             backend.Variable(2.0),\n             backend.Variable(2.0, name=\"exclude\"),\n             backend.Variable(2.0),\n@@ -49,8 +50,8 @@ class AdadeltaTest(testing.TestCase):\n         optimizer = Adadelta(learning_rate=1.0, rho=0.8, epsilon=1e-6)\n \n         x = backend.Variable(np.ones([10]))\n-        grads = np.arange(0.1, 1.1, 0.1)\n-        first_grads = np.full((10,), 0.01)\n+        grads = ops.arange(0.1, 1.1, 0.1)\n+        first_grads = ops.full((10,), 0.01)\n \n         golden = np.tile(\n             [[0.9978], [0.9947], [0.9915], [0.9882], [0.9849]], (1, 10)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1c796ee476af0d5b396f509311d783020a6d936e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 118 | Lines Deleted: 30 | Files Changed: 1 | Hunks: 11 | Methods Changed: 8 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 148 | Churn Cumulative: 1890 | Contributors (this commit): 11 | Commits (past 90d): 44 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,14 +1,15 @@\n-import numpy as np\n-\n from keras_core import backend\n+from keras_core import ops\n from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n+from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n+from keras_core.random.seed_generator import SeedGenerator\n from keras_core.utils import backend_utils\n-from keras_core.utils.module_utils import tensorflow as tf\n+from keras_core.utils import image_utils\n+from keras_core.utils import dtype_utils\n \n \n @keras_core_export(\"keras_core.layers.RandomCrop\")\n-class RandomCrop(Layer):\n+class RandomCrop(TFDataLayer):\n     \"\"\"A preprocessing layer which randomly crops images during training.\n \n     During training, this layer will randomly choose a location to crop images\n@@ -52,41 +53,128 @@ class RandomCrop(Layer):\n             `name` and `dtype`.\n     \"\"\"\n \n-    def __init__(self, height, width, seed=None, name=None, **kwargs):\n-        if not tf.available:\n-            raise ImportError(\n-                \"Layer RandomCrop requires TensorFlow. \"\n-                \"Install it via `pip install tensorflow`.\"\n-            )\n-\n+    def __init__(\n+        self, height, width, seed=None, data_format=None, name=None, **kwargs\n+    ):\n         super().__init__(name=name, **kwargs)\n+        self.height = height\n+        self.width = width\n         self.seed = seed or backend.random.make_default_seed()\n-        self.layer = tf.keras.layers.RandomCrop(\n-            height=height,\n-            width=width,\n-            seed=self.seed,\n-            name=name,\n-        )\n+        self.seed_generator = SeedGenerator(seed)\n+        self.data_format = backend.standardize_data_format(data_format)\n+\n+        if self.data_format == \"channels_first\":\n+            self.heigh_axis = -2\n+            self.width_axis = -1\n+        elif self.data_format == \"channels_last\":\n+            self.height_axis = -3\n+            self.width_axis = -2\n+\n         self.supports_masking = False\n         self.supports_jit = False\n         self._convert_input_args = False\n         self._allow_non_tensor_positional_args = True\n \n     def call(self, inputs, training=True):\n-        if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n-            inputs = tf.convert_to_tensor(backend.convert_to_numpy(inputs))\n-        outputs = self.layer.call(inputs, training=training)\n-        if (\n-            backend.backend() != \"tensorflow\"\n-            and not backend_utils.in_tf_graph()\n-        ):\n-            outputs = backend.convert_to_tensor(outputs)\n+        inputs = self.backend.cast(inputs, self.compute_dtype)\n+        input_shape = self.backend.shape(inputs)\n+        is_batched = len(input_shape) > 3\n+        inputs = (\n+            self.backend.numpy.expand_dims(inputs, axis=0)\n+            if not is_batched\n+            else inputs\n+        )\n+\n+        h_diff = input_shape[self.height_axis] - self.height\n+        w_diff = input_shape[self.width_axis] - self.width\n+\n+        def random_crop():\n+            # input_dtype_max = (2 ** dtype_utils.dtype_size(inputs.dtype)) - 1\n+            input_height, input_width = (\n+                input_shape[self.height_axis],\n+                input_shape[self.width_axis],\n+            )\n+\n+            h_start = self.backend.cast(\n+                ops.random.uniform(\n+                    (),\n+                    0,\n+                    maxval=float(input_height - self.height + 1),\n+                    dtype=inputs.dtype,\n+                    seed=self.seed_generator,\n+                ),\n+                h_diff.dtype,\n+            )\n+            w_start = self.backend.cast(\n+                ops.random.uniform(\n+                    (),\n+                    0,\n+                    maxval=float(input_width - self.width + 1),\n+                    dtype=inputs.dtype,\n+                    seed=self.seed_generator,\n+                ),\n+                h_diff.dtype,\n+            )\n+            # rands = ops.random.uniform(\n+            #     [2], 0, input_dtype_max, inputs.dtype, seed=self.seed_generator\n+            # )\n+            # original_dtype = h_diff.dtype\n+            # h_start = self.backend.cast(\n+            #     rands[0] % self.backend.cast((h_diff + 1), self.compute_dtype),\n+            #     original_dtype,\n+            # )\n+            # w_start = self.backend.cast(\n+            #     rands[1] % self.backend.cast((w_diff + 1), self.compute_dtype),\n+            #     original_dtype,\n+            # )\n+            if self.data_format == \"channels_last\":\n+                return inputs[\n+                    :,\n+                    h_start : h_start + self.height,\n+                    w_start : w_start + self.width,\n+                ]\n+            else:\n+                return inputs[\n+                    :,\n+                    :,\n+                    h_start : h_start + self.height,\n+                    w_start : w_start + self.width,\n+                ]\n+\n+        def resize():\n+            outputs = image_utils.smart_resize(\n+                inputs,\n+                [self.height, self.width],\n+                data_format=self.data_format,\n+                backend_module=self.backend,\n+            )\n+            # smart_resize will always output float32, so we need to re-cast.\n+            return self.backend.cast(outputs, self.compute_dtype)\n+\n+        outputs = self.backend.cond(\n+            self.backend.numpy.all((training, h_diff >= 0, w_diff >= 0)),\n+            random_crop,\n+            resize,\n+        )\n+\n+        if self.backend != \"tensorflow\" and not backend_utils.in_tf_graph():\n+            outputs = self.backend.convert_to_tensor(outputs)\n         return outputs\n \n-    def compute_output_shape(self, input_shape):\n-        return tuple(self.layer.compute_output_shape(input_shape))\n+    def compute_output_shape(self, input_shape, *args, **kwargs):\n+        input_shape = list(input_shape)\n+        input_shape[self.height_axis] = self.height\n+        input_shape[self.width_axis] = self.width\n+        return tuple(input_shape)\n \n     def get_config(self):\n-        config = self.layer.get_config()\n-        config.update({\"seed\": self.seed})\n+        config = super().get_config()\n+        config.update(\n+            {\n+                \"height\": self.height,\n+                \"width\": self.width,\n+                \"seed\": self.seed,\n+                \"data_format\": self.data_format,\n+            }\n+        )\n         return config\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#97135b796642329611df5a35dfd4c66975f71445", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 119 | Files Changed: 3 | Hunks: 13 | Methods Changed: 9 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 151 | Churn Cumulative: 2147 | Contributors (this commit): 13 | Commits (past 90d): 61 | Contributors (cumulative): 17 | DMM Complexity: 0.0\n\nDIFF:\n@@ -10,6 +10,7 @@ from keras_core import ops\n from keras_core import optimizers\n from keras_core import regularizers\n from keras_core import utils\n+from keras_core.backend import KerasTensor\n from keras_core.layers import Input\n from keras_core.layers import Layer\n from keras_core.models import Functional\n\n@@ -1,15 +1,14 @@\n+import numpy as np\n+\n from keras_core import backend\n-from keras_core import ops\n from keras_core.api_export import keras_core_export\n-from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n-from keras_core.random.seed_generator import SeedGenerator\n+from keras_core.layers.layer import Layer\n from keras_core.utils import backend_utils\n-from keras_core.utils import image_utils\n-from keras_core.utils import dtype_utils\n+from keras_core.utils.module_utils import tensorflow as tf\n \n \n @keras_core_export(\"keras_core.layers.RandomCrop\")\n-class RandomCrop(TFDataLayer):\n+class RandomCrop(Layer):\n     \"\"\"A preprocessing layer which randomly crops images during training.\n \n     During training, this layer will randomly choose a location to crop images\n@@ -53,128 +52,41 @@ class RandomCrop(TFDataLayer):\n             `name` and `dtype`.\n     \"\"\"\n \n-    def __init__(\n-        self, height, width, seed=None, data_format=None, name=None, **kwargs\n-    ):\n+    def __init__(self, height, width, seed=None, name=None, **kwargs):\n+        if not tf.available:\n+            raise ImportError(\n+                \"Layer RandomCrop requires TensorFlow. \"\n+                \"Install it via `pip install tensorflow`.\"\n+            )\n+\n         super().__init__(name=name, **kwargs)\n-        self.height = height\n-        self.width = width\n         self.seed = seed or backend.random.make_default_seed()\n-        self.seed_generator = SeedGenerator(seed)\n-        self.data_format = backend.standardize_data_format(data_format)\n-\n-        if self.data_format == \"channels_first\":\n-            self.heigh_axis = -2\n-            self.width_axis = -1\n-        elif self.data_format == \"channels_last\":\n-            self.height_axis = -3\n-            self.width_axis = -2\n-\n+        self.layer = tf.keras.layers.RandomCrop(\n+            height=height,\n+            width=width,\n+            seed=self.seed,\n+            name=name,\n+        )\n         self.supports_masking = False\n         self.supports_jit = False\n         self._convert_input_args = False\n         self._allow_non_tensor_positional_args = True\n \n     def call(self, inputs, training=True):\n-        inputs = self.backend.cast(inputs, self.compute_dtype)\n-        input_shape = self.backend.shape(inputs)\n-        is_batched = len(input_shape) > 3\n-        inputs = (\n-            self.backend.numpy.expand_dims(inputs, axis=0)\n-            if not is_batched\n-            else inputs\n-        )\n-\n-        h_diff = input_shape[self.height_axis] - self.height\n-        w_diff = input_shape[self.width_axis] - self.width\n-\n-        def random_crop():\n-            # input_dtype_max = (2 ** dtype_utils.dtype_size(inputs.dtype)) - 1\n-            input_height, input_width = (\n-                input_shape[self.height_axis],\n-                input_shape[self.width_axis],\n-            )\n-\n-            h_start = self.backend.cast(\n-                ops.random.uniform(\n-                    (),\n-                    0,\n-                    maxval=float(input_height - self.height + 1),\n-                    dtype=inputs.dtype,\n-                    seed=self.seed_generator,\n-                ),\n-                h_diff.dtype,\n-            )\n-            w_start = self.backend.cast(\n-                ops.random.uniform(\n-                    (),\n-                    0,\n-                    maxval=float(input_width - self.width + 1),\n-                    dtype=inputs.dtype,\n-                    seed=self.seed_generator,\n-                ),\n-                h_diff.dtype,\n-            )\n-            # rands = ops.random.uniform(\n-            #     [2], 0, input_dtype_max, inputs.dtype, seed=self.seed_generator\n-            # )\n-            # original_dtype = h_diff.dtype\n-            # h_start = self.backend.cast(\n-            #     rands[0] % self.backend.cast((h_diff + 1), self.compute_dtype),\n-            #     original_dtype,\n-            # )\n-            # w_start = self.backend.cast(\n-            #     rands[1] % self.backend.cast((w_diff + 1), self.compute_dtype),\n-            #     original_dtype,\n-            # )\n-            if self.data_format == \"channels_last\":\n-                return inputs[\n-                    :,\n-                    h_start : h_start + self.height,\n-                    w_start : w_start + self.width,\n-                ]\n-            else:\n-                return inputs[\n-                    :,\n-                    :,\n-                    h_start : h_start + self.height,\n-                    w_start : w_start + self.width,\n-                ]\n-\n-        def resize():\n-            outputs = image_utils.smart_resize(\n-                inputs,\n-                [self.height, self.width],\n-                data_format=self.data_format,\n-                backend_module=self.backend,\n-            )\n-            # smart_resize will always output float32, so we need to re-cast.\n-            return self.backend.cast(outputs, self.compute_dtype)\n-\n-        outputs = self.backend.cond(\n-            self.backend.numpy.all((training, h_diff >= 0, w_diff >= 0)),\n-            random_crop,\n-            resize,\n-        )\n-\n-        if self.backend != \"tensorflow\" and not backend_utils.in_tf_graph():\n-            outputs = self.backend.convert_to_tensor(outputs)\n+        if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n+            inputs = tf.convert_to_tensor(backend.convert_to_numpy(inputs))\n+        outputs = self.layer.call(inputs, training=training)\n+        if (\n+            backend.backend() != \"tensorflow\"\n+            and not backend_utils.in_tf_graph()\n+        ):\n+            outputs = backend.convert_to_tensor(outputs)\n         return outputs\n \n-    def compute_output_shape(self, input_shape, *args, **kwargs):\n-        input_shape = list(input_shape)\n-        input_shape[self.height_axis] = self.height\n-        input_shape[self.width_axis] = self.width\n-        return tuple(input_shape)\n+    def compute_output_shape(self, input_shape):\n+        return tuple(self.layer.compute_output_shape(input_shape))\n \n     def get_config(self):\n-        config = super().get_config()\n-        config.update(\n-            {\n-                \"height\": self.height,\n-                \"width\": self.width,\n-                \"seed\": self.seed,\n-                \"data_format\": self.data_format,\n-            }\n-        )\n+        config = self.layer.get_config()\n+        config.update({\"seed\": self.seed})\n         return config\n\n@@ -22,7 +22,7 @@ class TFDataLayer(Layer):\n \n     def __call__(self, inputs, **kwargs):\n         if backend_utils.in_tf_graph() and not isinstance(\n-            inputs, keras_core.backend.KerasTensor\n+            inputs, keras_core.KerasTensor\n         ):\n             # We're in a TF graph, e.g. a tf.data pipeline.\n             self.backend.set_backend(\"tensorflow\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
