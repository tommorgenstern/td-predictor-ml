{"custom_id": "keras#36a06289b201562ca0700328a4ba2f403b066e75", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 87 | Files Changed: 4 | Hunks: 27 | Methods Changed: 15 | Complexity Δ (Sum/Max): -10/0 | Churn Δ: 117 | Churn Cumulative: 770 | Contributors (this commit): 12 | Commits (past 90d): 22 | Contributors (cumulative): 24 | DMM Complexity: 0.05128205128205128\n\nDIFF:\n@@ -116,18 +116,18 @@ class TFOptimizer(KerasAutoTrackable, base_optimizer.BaseOptimizer):\n             v.value if isinstance(v, backend.Variable) else v\n             for v in trainable_variables\n         ]\n-        grads_and_vars = list(zip(grads, trainable_variables))\n-        grads_and_vars = self._all_reduce_sum_gradients(grads_and_vars)\n         tf.__internal__.distribute.interim.maybe_merge_call(\n             self._distributed_tf_update_step,\n             self._distribution_strategy,\n-            grads_and_vars,\n+            list(zip(grads, trainable_variables)),\n             learning_rate,\n         )\n \n     def _distributed_tf_update_step(\n         self, distribution, grads_and_vars, learning_rate\n     ):\n+        grads_and_vars = self._all_reduce_sum_gradients(grads_and_vars)\n+\n         def apply_grad_to_update_var(var, grad, learning_rate):\n             return self.update_step(grad, var, learning_rate)\n \n\n@@ -3,7 +3,6 @@\n import numpy as np\n import pytest\n import tensorflow as tf\n-from absl.testing import parameterized\n from tensorflow.python.eager import context\n \n from keras.src import backend\n@@ -15,7 +14,7 @@ from keras.src.optimizers.sgd import SGD\n     backend.backend() != \"tensorflow\",\n     reason=\"The distribute test can only run with TF backend.\",\n )\n-class OptimizerDistributeTest(testing.TestCase, parameterized.TestCase):\n+class OptimizerDistributeTest(testing.TestCase):\n     def setUp(self):\n         super().setUp()\n         # Need at least 2 devices for distribution related tests.\n@@ -40,32 +39,20 @@ class OptimizerDistributeTest(testing.TestCase, parameterized.TestCase):\n             )\n         self.run_class_serialization_test(optimizer)\n \n-    @parameterized.parameters([(\"keras_sgd\",), (\"tf_keras_sgd\",)])\n-    def test_single_step(self, optimizer_type):\n-        if optimizer_type == \"tf_keras_sgd\":\n-            try:\n-                import tf_keras\n-\n-                optimizer_fn = tf_keras.optimizers.SGD\n-            except (ImportError, AttributeError):\n-                self.skipTest(\"tf_keras not installed\")\n-        else:\n-            optimizer_fn = SGD\n+    def test_single_step(self):\n         with self.strategy.scope():\n-            optimizer = optimizer_fn(\n+            optimizer = SGD(\n                 learning_rate=0.5,\n                 momentum=0.06,\n             )\n-            # use tf variable to work both in k2 & k3.\n-            vars = tf.Variable([1.0, 2.0, 3.0, 4.0])\n-\n-            def update():\n             grads = tf.constant([1.0, 6.0, 7.0, 2.0])\n-                optimizer.apply_gradients(zip([grads], [vars]))\n+            vars = backend.Variable([1.0, 2.0, 3.0, 4.0])\n \n-            self.strategy.run(update)\n+            self.strategy.run(\n+                lambda: optimizer.apply_gradients(zip([grads], [vars]))\n+            )\n             self.assertAllClose(\n-                vars, [0.0, -4.0, -4.0, 2.0], rtol=1e-4, atol=1e-4\n+                vars, [0.5, -1.0, -0.5, 3.0], rtol=1e-4, atol=1e-4\n             )\n \n     def test_weight_decay(self):\n@@ -104,32 +91,31 @@ class OptimizerDistributeTest(testing.TestCase, parameterized.TestCase):\n     def test_correctness_with_golden(self):\n         with self.strategy.scope():\n             optimizer = SGD(nesterov=True)\n+\n             x = backend.Variable(np.ones([10]))\n-\n-            def update_grads():\n-                grads = backend.convert_to_tensor(np.arange(0.1, 1.1, 0.1))\n-                optimizer.apply_gradients(zip([grads], [x]))\n-\n-            def update_first_grads():\n-                first_grads = backend.convert_to_tensor(np.full((10,), 0.01))\n-                optimizer.apply_gradients(zip([first_grads], [x]))\n+            grads = np.arange(0.1, 1.1, 0.1)\n+            first_grads = np.full((10,), 0.01)\n \n         # fmt: off\n         golden = np.array(\n-            [\n-                [0.9980, 0.9960, 0.9940, 0.9920, 0.9900, 0.9880, 0.9860, 0.9840, 0.9820, 0.9800],\n-                [0.9978, 0.9958, 0.9938, 0.9918, 0.9898, 0.9878, 0.9858, 0.9838, 0.9818, 0.9798],\n-                [0.9976, 0.9956, 0.9936, 0.9916, 0.9896, 0.9876, 0.9856, 0.9836, 0.9816, 0.9796],\n-                [0.9974, 0.9954, 0.9934, 0.9914, 0.9894, 0.9874, 0.9854, 0.9834, 0.9814, 0.9794],\n-                [0.9972, 0.9952, 0.9932, 0.9912, 0.9892, 0.9872, 0.9852, 0.9832, 0.9812, 0.9792],\n-            ]\n+            [[0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n+            0.9999, 0.9999], [0.9989, 0.9979, 0.9969, 0.9959, 0.9949, 0.9939,\n+            0.9929, 0.9919, 0.9909, 0.9899], [0.9979, 0.9959, 0.9939, 0.9919,\n+            0.9899, 0.9879, 0.9859, 0.9839, 0.9819, 0.9799], [0.9969, 0.9939,\n+            0.9909, 0.9879, 0.9849, 0.9819, 0.9789, 0.9759, 0.9729, 0.9699],\n+            [0.9959, 0.9919, 0.9879, 0.9839, 0.9799, 0.9759, 0.9719, 0.9679,\n+            0.9639, 0.9599]]\n         )\n         # fmt: on\n \n-        self.strategy.run(update_grads)\n+        self.strategy.run(\n+            lambda: optimizer.apply_gradients(zip([first_grads], [x]))\n+        )\n         for i in range(5):\n             self.assertAllClose(x, golden[i], rtol=5e-4, atol=5e-4)\n-            self.strategy.run(update_first_grads)\n+            self.strategy.run(\n+                lambda: optimizer.apply_gradients(zip([grads], [x]))\n+            )\n \n     def test_clip_norm(self):\n         with self.strategy.scope():\n@@ -204,7 +190,7 @@ class OptimizerDistributeTest(testing.TestCase, parameterized.TestCase):\n             self.assertAllClose(optimizer._iterations, 2)\n             self.assertAllClose(optimizer.iterations, 0)\n             self.strategy.run(lambda: optimizer.apply_gradients([(grads, v)]))\n-            self.assertAllClose(v, [[-1.0, 0.0], [-1.0, 0.0]])\n+            self.assertAllClose(v, [[0.0, 1.0], [1.0, 2.0]])\n             self.assertAllClose(\n                 optimizer._accumulated_gradients[0], [[0.0, 0.0], [0.0, 0.0]]\n             )\n\n@@ -291,8 +291,6 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             steps_per_execution=self.steps_per_execution,\n         )\n \n-        self._symbolic_build(iterator=epoch_iterator)\n-\n         # Container that configures and calls callbacks.\n         if not isinstance(callbacks, callbacks_module.CallbackList):\n             callbacks = callbacks_module.CallbackList(\n@@ -408,8 +406,6 @@ class TensorFlowTrainer(base_trainer.Trainer):\n                 steps_per_execution=self.steps_per_execution,\n             )\n \n-        self._symbolic_build(iterator=epoch_iterator)\n-\n         # Container that configures and calls callbacks.\n         if not isinstance(callbacks, callbacks_module.CallbackList):\n             callbacks = callbacks_module.CallbackList(\n@@ -529,6 +525,7 @@ class TensorFlowTrainer(base_trainer.Trainer):\n         return_dict=False,\n     ):\n         self._assert_compile_called(\"train_on_batch\")\n+        self.make_train_function()\n         if class_weight is not None:\n             if sample_weight is not None:\n                 raise ValueError(\n@@ -541,10 +538,6 @@ class TensorFlowTrainer(base_trainer.Trainer):\n                 y, class_weight\n             )\n \n-        # Maybe build model\n-        self._symbolic_build(data_batch=(x, y, sample_weight))\n-        self.make_train_function()\n-\n         def data():\n             yield (x, y, sample_weight)\n \n@@ -562,14 +555,11 @@ class TensorFlowTrainer(base_trainer.Trainer):\n         return_dict=False,\n     ):\n         self._assert_compile_called(\"test_on_batch\")\n+        self.make_test_function()\n \n         def data():\n             yield (x, y, sample_weight)\n \n-        # Maybe build model\n-        self._symbolic_build(data_batch=(x, y, sample_weight))\n-        self.make_test_function()\n-\n         logs = self.test_function(data())\n         logs = tree.map_structure(lambda x: np.array(x), logs)\n         if return_dict:\n@@ -631,33 +621,6 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             x=None, y=y, y_pred=y_pred, sample_weight=sample_weight\n         )\n \n-    def _symbolic_build(self, iterator=None, data_batch=None):\n-        if self._distribute_strategy is None:\n-            # When no distribution strategy is set, defer building\n-            # to when the train/test/predict function gets traced.\n-            # This maximizes backwards compatibility.\n-            return\n-\n-        # Unlike jax/torch iterator, tf iterator returns an iterator instead\n-        # of data batch in `iterator.enumerate_epoch()`.\n-        if iterator is not None:\n-            for _, it in iterator.enumerate_epoch():\n-                maybe_distributed_data_batch = next(it)\n-                has_distributed_values = tree.map_structure(\n-                    lambda x: isinstance(x, tf.distribute.DistributedValues),\n-                    maybe_distributed_data_batch,\n-                )\n-                if all(tree.flatten(has_distributed_values)):\n-                    data_batch = self.distribute_strategy.reduce(\n-                        \"MEAN\",\n-                        maybe_distributed_data_batch,\n-                        axis=None,\n-                    )\n-                else:\n-                    data_batch = maybe_distributed_data_batch\n-                break\n-        super()._symbolic_build(data_batch=data_batch)\n-\n \n class TFEpochIterator(EpochIterator):\n     def __init__(self, distribute_strategy=None, *args, **kwargs):\n\n@@ -1891,12 +1891,8 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         reason=\"synchronized=True only implemented for TF backend\",\n     )\n     def test_moments_sync_with_distribution_strategy(self, dtype):\n-        from tensorflow.python.eager import context\n-\n         from keras.src.utils.module_utils import tensorflow as tf\n \n-        context._reset_context()\n-\n         # Config 2 CPUs for testing.\n         logical_cpus = tf.config.list_logical_devices(\"CPU\")\n         if len(logical_cpus) == 1:\n@@ -1926,8 +1922,6 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             self.assertEqual(variance.values[0], 8.75)\n             self.assertEqual(variance.values[0], 8.75)\n \n-        context._reset_context()\n-\n     def test_batch_normalization(self):\n         x = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\n         mean = np.array([0.2, 0.3, 0.4])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#93786e3a2ad12a59e5ada0224c079f75546c8191", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 17 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 40 | Churn Cumulative: 778 | Contributors (this commit): 12 | Commits (past 90d): 17 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -552,23 +552,19 @@ def softmax(x, axis=-1):\n     if any_symbolic_tensors((x,)):\n         return Softmax(axis).symbolic_call(x)\n     if isinstance(axis, tuple):\n-        original_shape = x.shape\n-        new_shape = []\n-        skip_dims = set(axis)\n-        i = 0\n-        while i < len(original_shape):\n-            if i in skip_dims:\n-                size = 1\n-                while i in skip_dims:\n-                    size *= original_shape[i]\n-                    i += 1\n-                new_shape.append(size)\n-            else:\n-                new_shape.append(original_shape[i])\n-                i += 1\n-        x = backend.numpy.reshape(x, new_shape)\n-        x = backend.nn.softmax(x, axis=-1)\n-        x = backend.numpy.reshape(x, original_shape)\n+        axis_to_keep = [v for v in range(len(x.shape)) if v not in axis]\n+\n+        x_transposed = backend.numpy.transpose(x, axes=(*axis_to_keep, *axis))\n+        x_reshaped = backend.numpy.reshape(\n+            x_transposed, (*[x.shape[v] for v in axis_to_keep], -1)\n+        )\n+\n+        x = backend.nn.softmax(x_reshaped, axis=-1)\n+\n+        x = backend.numpy.reshape(x, x_transposed.shape)\n+        x = backend.numpy.transpose(\n+            x, axes=list(backend.numpy.argsort([*axis_to_keep, *axis]))\n+        )\n         return x\n     else:\n         return backend.nn.softmax(x, axis=axis)\n\n@@ -1,3 +1,5 @@\n+from itertools import combinations\n+\n import numpy as np\n import pytest\n from absl.testing import parameterized\n@@ -1248,6 +1250,14 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             ],\n         )\n \n+    def test_softmax_correctness_with_axis_tuple(self):\n+        input = np.array([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n+        combination = combinations(range(3), 2)\n+        for axis in list(combination):\n+            result = keras.ops.nn.softmax(input, axis=axis)\n+            normalized_sum_by_axis = np.sum(np.asarray(result), axis=axis)\n+            self.assertAllClose(normalized_sum_by_axis, 1.0)\n+\n     def test_log_softmax(self):\n         x = np.array([[1, 2, 3], [1, 2, 3]], dtype=np.float32)\n         self.assertAllClose(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f9501f53fa15fe6286ca834618a6bcf10f86f7e8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 474 | Contributors (this commit): 10 | Commits (past 90d): 12 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1255,7 +1255,9 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         combination = combinations(range(3), 2)\n         for axis in list(combination):\n             result = keras.ops.nn.softmax(input, axis=axis)\n-            normalized_sum_by_axis = np.sum(np.asarray(result), axis=axis)\n+            normalized_sum_by_axis = np.sum(\n+                ops.convert_to_numpy(result), axis=axis\n+            )\n             self.assertAllClose(normalized_sum_by_axis, 1.0)\n \n     def test_log_softmax(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7a6d539ad6ddc82c8fe31674d5d8edb4f76f492f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 1394 | Contributors (this commit): 8 | Commits (past 90d): 10 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -338,7 +338,7 @@ class CategoricalTruePositives(keras.metrics.Metric):\n \n     def reset_state(self):\n         # The state of the metric will be reset at the start of each epoch.\n-        self.true_positives.assign(0.0)\n+        self.true_positives.assign(0)\n \n \n model = get_uncompiled_model()\n\n@@ -39,7 +39,7 @@ class ExampleMetric(Metric):\n         return _sum / (_total + _epsilon)\n \n     def reset_state(self):\n-        self.sum.assign(0.0)\n+        self.sum.assign(0)\n         self.total.assign(0)\n \n \n\n@@ -87,7 +87,7 @@ class Sum(Metric):\n         self.total.assign_add(ops.sum(values))\n \n     def reset_state(self):\n-        self.total.assign(0.0)\n+        self.total.assign(0)\n \n     def result(self):\n         return ops.cast(self.total, self.dtype)\n@@ -150,7 +150,7 @@ class Mean(Metric):\n         self.count.assign_add(ops.cast(num_samples, dtype=self.dtype))\n \n     def reset_state(self):\n-        self.total.assign(0.0)\n+        self.total.assign(0)\n         self.count.assign(0)\n \n     def result(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7c6d501935f1c9ce7bca5d235dd6620154d9caa0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 40 | Churn Cumulative: 454 | Contributors (this commit): 2 | Commits (past 90d): 13 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -708,6 +708,24 @@ class EinsumDense(Layer):\n             if self.lora_enabled:\n                 # Dequantize & quantize to merge lora weights into int8 kernel\n                 # Note that this is a lossy compression\n+                if self._kernel_squeeze_axes:\n+                    kernel_scale = ops.expand_dims(\n+                        kernel_scale, axis=self._kernel_squeeze_axes\n+                    )\n+                if self._kernel_expand_axes:\n+                    kernel_scale = ops.squeeze(\n+                        kernel_scale, axis=self._kernel_expand_axes\n+                    )\n+                if self._kernel_transpose_axes:\n+\n+                    def _argsort(seq):\n+                        # Ref: https://stackoverflow.com/a/3382369\n+                        return sorted(range(len(seq)), key=seq.__getitem__)\n+\n+                    reverse_transpose = _argsort(self._kernel_transpose_axes)\n+                    kernel_scale = ops.transpose(\n+                        kernel_scale, axes=reverse_transpose\n+                    )\n                 kernel_value = ops.divide(kernel_value, kernel_scale)\n                 kernel_value = ops.add(\n                     kernel_value,\n\n@@ -625,15 +625,19 @@ class EinsumDenseTest(testing.TestCase, parameterized.TestCase):\n             supports_masking=False,\n         )\n \n+    @parameterized.named_parameters(\n+        (\"ab,bcd->acd\", \"ab,bcd->acd\", (64, 3), (64, 8, 32)),\n+        (\"btd,ndh->btnh\", \"btd,ndh->btnh\", (1, 4, 32), (1, 4, 8, 16)),\n+    )\n     @pytest.mark.requires_trainable_backend\n-    def test_quantize_int8_when_lora_enabled(self):\n+    def test_quantize_int8_when_lora_enabled(\n+        self, equation, input_shape, output_shape\n+    ):\n         config = dict(\n-            equation=\"ab,bcd->acd\",\n-            output_shape=(8, 32),\n-            bias_axes=None,\n+            equation=equation, output_shape=output_shape[1:], bias_axes=None\n         )\n         layer = layers.EinsumDense(**config)\n-        layer.build((None, 3))\n+        layer.build(input_shape)\n         layer.enable_lora(2)\n         layer.quantize(\"int8\")\n         self.assertLen(layer.trainable_weights, 2)\n@@ -644,8 +648,8 @@ class EinsumDenseTest(testing.TestCase, parameterized.TestCase):\n         # Try calling fit()\n         init_lora_a_kernel_value = layer.lora_kernel_a.numpy()\n         init_lora_b_kernel_value = layer.lora_kernel_b.numpy()\n-        x = np.random.random((64, 3))\n-        y = np.random.random((64, 8, 32))\n+        x = np.random.random(input_shape)\n+        y = np.random.random(output_shape)\n         model = models.Sequential([layer])\n         model.compile(optimizer=\"sgd\", loss=\"mse\")\n         model.fit(x, y, epochs=2)\n@@ -676,7 +680,7 @@ class EinsumDenseTest(testing.TestCase, parameterized.TestCase):\n         )\n         model.save_weights(temp_filepath)\n         new_model = models.Sequential([layers.EinsumDense(**config)])\n-        new_model.build((None, 3))\n+        new_model.build(input_shape)\n         new_model.quantize(\"int8\")\n         new_model.load_weights(temp_filepath)\n         self.assertFalse(new_model.layers[0].lora_enabled)\n@@ -692,7 +696,7 @@ class EinsumDenseTest(testing.TestCase, parameterized.TestCase):\n             import tensorflow as tf\n \n             temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n-            ref_input = tf.random.normal((32, 3))\n+            ref_input = tf.random.normal(input_shape)\n             ref_output = model(ref_input)\n             export_lib.export_model(model, temp_filepath)\n             reloaded_layer = export_lib.TFSMLayer(temp_filepath)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7b516d09d895bcc56feddde6410ec645cd7955aa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 9 | Methods Changed: 8 | Complexity Δ (Sum/Max): 5/4 | Churn Δ: 40 | Churn Cumulative: 147 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -967,25 +967,44 @@ class JAXTrainer(base_trainer.Trainer):\n         return tuple(state)\n \n \n-def _distribute_data(data):\n+def _distribute_data(data, layouts=None):\n     distribution = distribution_lib.distribution()\n     if distribution is not None:\n+        if layouts is None:\n+            layouts = tree.map_structure(\n+                lambda d: distribution.get_data_layout(d.shape),\n+                data,\n+            )\n+        return tree.map_structure(\n+            jax_distribution_lib.distribute_data_input, data, layouts\n+        )\n \n-        def distribute_single_value(d):\n-            layout = distribution.get_data_layout(d.shape)\n-            return jax_distribution_lib.distribute_data_input(d, layout)\n-\n-        return tree.map_structure(distribute_single_value, data)\n-    else:\n     return tree.map_structure(jax.device_put, data)\n \n \n class JAXEpochIterator(EpochIterator):\n     def _get_iterator(self):\n+        distribution = distribution_lib.distribution()\n+        if distribution is not None:\n+            return self._get_distributed_iterator(distribution)\n+\n         return self._prefetch_numpy_iterator(\n             self.data_adapter.get_jax_iterator()\n         )\n \n+    def _get_distributed_iterator(self, distribution):\n+        \"\"\"Lazily compute layouts to reduce host to device transfer latency.\"\"\"\n+        layouts = None\n+        for data in self.data_adapter.get_jax_iterator():\n+            if layouts is None:\n+                layouts = tree.map_structure(\n+                    lambda d: jax_distribution_lib._to_jax_layout(\n+                        distribution.get_data_layout(d.shape)\n+                    ),\n+                    data,\n+                )\n+            yield _distribute_data(data, layouts)\n+\n     def _prefetch_numpy_iterator(self, numpy_iterator):\n         \"\"\"Shard and prefetch batches on device.\n \n\n@@ -385,9 +385,11 @@ class DataParallel(Distribution):\n     Args:\n         device_mesh: Optional `DeviceMesh` instance.\n         devices: Optional list of devices.\n+        auto_shard_dataset: Automatically shard the dataset amongst processes.\n+            Defaults to true.\n     \"\"\"\n \n-    def __init__(self, device_mesh=None, devices=None):\n+    def __init__(self, device_mesh=None, devices=None, auto_shard_dataset=True):\n         if device_mesh:\n             self._initialize_with_device_mesh(device_mesh)\n         elif devices:\n@@ -400,6 +402,7 @@ class DataParallel(Distribution):\n         self._num_process = distribution_lib.num_processes()\n         self._process_id = distribution_lib.process_id()\n         self._is_multi_process = self._num_process > 1\n+        self._auto_shard_dataset = auto_shard_dataset\n \n     def _initialize_with_device_mesh(self, device_mesh):\n         if not isinstance(device_mesh, DeviceMesh):\n@@ -459,7 +462,7 @@ class DataParallel(Distribution):\n                 \"Only `tf.data.Dataset` is supported for \"\n                 f\"sharding, got {type(dataset)}\"\n             )\n-        if not self._is_multi_process:\n+        if not self._is_multi_process or not self._auto_shard_dataset:\n             return dataset\n \n         batch_size = tf_data_distribute.compute_batch_size(dataset)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9ebc65ad8fd6c856ff0d6f782d285ac7e72aab45", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 12 | Files Changed: 3 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 27 | Churn Cumulative: 610 | Contributors (this commit): 5 | Commits (past 90d): 8 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -43,6 +43,10 @@ def in_top_k(targets, predictions, k):\n     preds_at_label = jnp.take_along_axis(\n         predictions, jnp.expand_dims(targets, axis=-1), axis=-1\n     )\n+    # `nan` shouldn't be considered as large probability.\n+    preds_at_label = jnp.where(\n+        jnp.isnan(preds_at_label), -jnp.inf, preds_at_label\n+    )\n     rank = 1 + jnp.sum(jnp.greater(predictions, preds_at_label), axis=-1)\n     return jnp.less_equal(rank, k)\n \n\n@@ -53,26 +53,17 @@ def segment_max(data, segment_ids, num_segments=None, sorted=False):\n \n \n def top_k(x, k, sorted=False):\n-    sorted_indices = np.argsort(x, axis=-1)[..., ::-1]\n-    sorted_values = np.sort(x, axis=-1)[..., ::-1]\n-\n     if sorted:\n         # Take the k largest values.\n+        sorted_indices = np.argsort(x, axis=-1)[..., ::-1]\n+        sorted_values = np.take_along_axis(x, sorted_indices, axis=-1)\n         top_k_values = sorted_values[..., :k]\n         top_k_indices = sorted_indices[..., :k]\n     else:\n         # Partition the array such that all values larger than the k-th\n         # largest value are to the right of it.\n-        top_k_values = np.partition(x, -k, axis=-1)[..., -k:]\n         top_k_indices = np.argpartition(x, -k, axis=-1)[..., -k:]\n-\n-        # Get the indices in sorted order.\n-        idx = np.argsort(-top_k_values, axis=-1)\n-\n-        # Get the top k values and their indices.\n-        top_k_values = np.take_along_axis(top_k_values, idx, axis=-1)\n-        top_k_indices = np.take_along_axis(top_k_indices, idx, axis=-1)\n-\n+        top_k_values = np.take_along_axis(x, top_k_indices, axis=-1)\n     return top_k_values, top_k_indices\n \n \n\n@@ -632,6 +632,14 @@ class MathOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             kmath.in_top_k(targets, predictions, k=3), [True, True, True]\n         )\n \n+        # Test `nan` in predictions\n+        # https://github.com/keras-team/keras/issues/19995\n+        targets = np.array([1, 0])\n+        predictions = np.array([[0.1, np.nan, 0.5], [0.3, 0.2, 0.5]])\n+        self.assertAllEqual(\n+            kmath.in_top_k(targets, predictions, k=2), [False, True]\n+        )\n+\n     def test_logsumexp(self):\n         x = np.random.rand(5, 5)\n         outputs = kmath.logsumexp(x)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#da72acdfb54c57701d831feb3631ed3b80032057", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 17 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 40 | Churn Cumulative: 822 | Contributors (this commit): 12 | Commits (past 90d): 20 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -613,23 +613,19 @@ def log_softmax(x, axis=-1):\n     if any_symbolic_tensors((x,)):\n         return LogSoftmax(axis).symbolic_call(x)\n     if isinstance(axis, tuple):\n-        original_shape = x.shape\n-        new_shape = []\n-        skip_dims = set(axis)\n-        i = 0\n-        while i < len(original_shape):\n-            if i in skip_dims:\n-                size = 1\n-                while i in skip_dims:\n-                    size *= original_shape[i]\n-                    i += 1\n-                new_shape.append(size)\n-            else:\n-                new_shape.append(original_shape[i])\n-                i += 1\n-        x = backend.numpy.reshape(x, new_shape)\n-        x = backend.nn.log_softmax(x, axis=-1)\n-        x = backend.numpy.reshape(x, original_shape)\n+        axis_to_keep = [v for v in range(len(x.shape)) if v not in axis]\n+\n+        x_transposed = backend.numpy.transpose(x, axes=(*axis_to_keep, *axis))\n+        x_reshaped = backend.numpy.reshape(\n+            x_transposed, (*[x.shape[v] for v in axis_to_keep], -1)\n+        )\n+\n+        x = backend.nn.log_softmax(x_reshaped, axis=-1)\n+\n+        x = backend.numpy.reshape(x, x_transposed.shape)\n+        x = backend.numpy.transpose(\n+            x, axes=list(backend.numpy.argsort([*axis_to_keep, *axis]))\n+        )\n         return x\n     else:\n         return backend.nn.log_softmax(x, axis=axis)\n\n@@ -1291,6 +1291,16 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             ],\n         )\n \n+    def test_log_softmax_correctness_with_axis_tuple(self):\n+        input = np.array([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n+        combination = combinations(range(3), 2)\n+        for axis in list(combination):\n+            result = keras.ops.nn.log_softmax(input, axis=axis)\n+            normalized_sum_by_axis = np.sum(\n+                np.exp(ops.convert_to_numpy(result)), axis=axis\n+            )\n+            self.assertAllClose(normalized_sum_by_axis, 1.0)\n+\n     def test_max_pool(self):\n         data_format = backend.config.image_data_format()\n         # Test 1D max pooling.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3ac43b1800c94c269d95538aa20a2cc602cf240e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 162 | Lines Deleted: 105 | Files Changed: 9 | Hunks: 56 | Methods Changed: 41 | Complexity Δ (Sum/Max): 9/13 | Churn Δ: 267 | Churn Cumulative: 1735 | Contributors (this commit): 10 | Commits (past 90d): 51 | Contributors (cumulative): 34 | DMM Complexity: 1.0\n\nDIFF:\n@@ -58,4 +58,5 @@ class TorchLayer(torch.nn.Module):\n \n     def _post_untrack_variable(self, variable):\n         if hasattr(self, \"torch_params\"):\n+            if variable.path in self.torch_params:\n                 self.torch_params.pop(variable.path)\n\n@@ -300,13 +300,6 @@ class Dense(Layer):\n \n     # Quantization-related (int8 and float8) methods\n \n-    def _quantization_mode_error(self, mode):\n-        return NotImplementedError(\n-            \"Invalid quantization mode. Expected one of \"\n-            f\"{dtype_policies.QUANTIZATION_MODES}. \"\n-            f\"Received: quantization_mode={mode}\"\n-        )\n-\n     def quantized_build(self, input_shape, mode):\n         if mode == \"int8\":\n             input_dim = input_shape[-1]\n@@ -390,15 +383,7 @@ class Dense(Layer):\n         self.outputs_grad_amax_history.overwrite_with_gradient = True\n         self._is_quantized = True\n \n-    def quantized_call(self, inputs, training=None):\n-        if self.quantization_mode == \"int8\":\n-            return self._int8_call(inputs)\n-        elif self.quantization_mode == \"float8\":\n-            return self._float8_call(inputs, training=training)\n-        else:\n-            raise self._quantization_mode_error(self.quantization_mode)\n-\n-    def _int8_call(self, inputs):\n+    def _int8_call(self, inputs, training=None):\n         @ops.custom_gradient\n         def matmul_with_inputs_gradient(inputs, kernel, kernel_scale):\n             def grad_fn(*args, upstream=None):\n@@ -525,22 +510,17 @@ class Dense(Layer):\n         return x\n \n     def quantize(self, mode, type_check=True):\n-        import gc\n-\n         # Prevent quantization of the subclasses\n         if type_check and (type(self) is not Dense):\n-            raise self._quantize_not_implemented_error()\n-        self._check_quantize_args(mode, self.compute_dtype)\n+            raise self._not_implemented_error(self.quantize)\n \n-        self._tracker.unlock()\n         if mode == \"int8\":\n             # Quantize `self._kernel` to int8 and compute corresponding scale\n             kernel_value, kernel_scale = quantizers.abs_max_quantize(\n-                self._kernel, axis=0\n+                self._kernel, axis=0, to_numpy=True\n             )\n             kernel_scale = ops.squeeze(kernel_scale, axis=0)\n-            self._untrack_variable(self._kernel)\n-            kernel_shape = self._kernel.shape\n+            kernel_shape = tuple(self._kernel.shape)\n             del self._kernel\n             # Utilize a lambda expression as an initializer to prevent adding a\n             # large constant to the computation graph.\n@@ -553,16 +533,12 @@ class Dense(Layer):\n             self._float8_build()\n         else:\n             raise self._quantization_mode_error(mode)\n-        self._tracker.lock()\n \n         # Set new dtype policy\n         if self.dtype_policy.quantization_mode is None:\n             policy = dtype_policies.get(f\"{mode}_from_{self.dtype_policy.name}\")\n             self.dtype_policy = policy\n \n-        # Release memory manually because sometimes the backend doesn't\n-        gc.collect()\n-\n     def _get_kernel_with_merged_lora(self):\n         if self.dtype_policy.quantization_mode is not None:\n             kernel_value = self._kernel\n@@ -576,7 +552,7 @@ class Dense(Layer):\n                     ops.matmul(self.lora_kernel_a, self.lora_kernel_b),\n                 )\n                 kernel_value, kernel_scale = quantizers.abs_max_quantize(\n-                    kernel_value, axis=0\n+                    kernel_value, axis=0, to_numpy=True\n                 )\n                 kernel_scale = ops.squeeze(kernel_scale, axis=0)\n             return kernel_value, kernel_scale\n\n@@ -360,13 +360,6 @@ class EinsumDense(Layer):\n \n     # Quantization-related (int8 and float8) methods\n \n-    def _quantization_mode_error(self, mode):\n-        return NotImplementedError(\n-            \"Invalid quantization mode. Expected one of \"\n-            f\"{dtype_policies.QUANTIZATION_MODES}. \"\n-            f\"Received: quantization_mode={mode}\"\n-        )\n-\n     def quantized_build(self, input_shape, mode):\n         if mode == \"int8\":\n             shape_data = _analyze_einsum_string(\n@@ -477,15 +470,7 @@ class EinsumDense(Layer):\n         self.outputs_grad_amax_history.overwrite_with_gradient = True\n         self._is_quantized = True\n \n-    def quantized_call(self, inputs, training=None):\n-        if self.quantization_mode == \"int8\":\n-            return self._int8_call(inputs)\n-        elif self.quantization_mode == \"float8\":\n-            return self._float8_call(inputs, training=training)\n-        else:\n-            raise self._quantization_mode_error(self.quantization_mode)\n-\n-    def _int8_call(self, inputs):\n+    def _int8_call(self, inputs, training=None):\n         @ops.custom_gradient\n         def einsum_with_inputs_gradient(inputs, kernel, kernel_scale):\n             def grad_fn(*args, upstream=None):\n@@ -641,14 +626,10 @@ class EinsumDense(Layer):\n         return x\n \n     def quantize(self, mode, type_check=True):\n-        import gc\n-\n         # Prevent quantization of the subclasses\n         if type_check and (type(self) is not EinsumDense):\n-            raise self._quantize_not_implemented_error()\n-        self._check_quantize_args(mode, self.compute_dtype)\n+            raise self._not_implemented_error(self.quantize)\n \n-        self._tracker.unlock()\n         if mode == \"int8\":\n             (\n                 self._input_reduced_axes,\n@@ -664,7 +645,7 @@ class EinsumDense(Layer):\n             ) = _analyze_quantization_info(self.equation, self.input_spec.ndim)\n             # Quantize `self._kernel` to int8 and compute corresponding scale\n             kernel_value, kernel_scale = quantizers.abs_max_quantize(\n-                self._kernel, axis=self._kernel_reduced_axes\n+                self._kernel, axis=self._kernel_reduced_axes, to_numpy=True\n             )\n             kernel_scale = ops.transpose(\n                 kernel_scale, self._kernel_transpose_axes\n@@ -677,8 +658,7 @@ class EinsumDense(Layer):\n                 kernel_scale = ops.squeeze(\n                     kernel_scale, axis=self._kernel_squeeze_axes\n                 )\n-            self._untrack_variable(self._kernel)\n-            kernel_shape = self._kernel.shape\n+            kernel_shape = tuple(self._kernel.shape)\n             del self._kernel\n             # Utilize a lambda expression as an initializer to prevent adding a\n             # large constant to the computation graph.\n@@ -691,16 +671,12 @@ class EinsumDense(Layer):\n             self._float8_build()\n         else:\n             raise self._quantization_mode_error(mode)\n-        self._tracker.lock()\n \n         # Set new dtype policy\n         if self.dtype_policy.quantization_mode is None:\n             policy = dtype_policies.get(f\"{mode}_from_{self.dtype_policy.name}\")\n             self.dtype_policy = policy\n \n-        # Release memory manually because sometimes the backend doesn't\n-        gc.collect()\n-\n     def _get_kernel_with_merged_lora(self):\n         if self.dtype_policy.quantization_mode is not None:\n             kernel_value = self._kernel\n@@ -732,7 +708,7 @@ class EinsumDense(Layer):\n                     ops.matmul(self.lora_kernel_a, self.lora_kernel_b),\n                 )\n                 kernel_value, kernel_scale = quantizers.abs_max_quantize(\n-                    kernel_value, axis=self._kernel_reduced_axes\n+                    kernel_value, axis=self._kernel_reduced_axes, to_numpy=True\n                 )\n                 kernel_scale = ops.transpose(\n                     kernel_scale, self._kernel_transpose_axes\n\n@@ -320,13 +320,12 @@ class Embedding(Layer):\n         )\n         self._is_quantized = True\n \n-    def quantized_call(self, inputs):\n-        if self.quantization_mode == \"int8\":\n-            return self._int8_call(inputs)\n-        else:\n+    def quantized_call(self, *args, **kwargs):\n+        if self.quantization_mode != \"int8\":\n             raise self._quantization_mode_error(self.quantization_mode)\n+        return super().quantized_call(*args, **kwargs)\n \n-    def _int8_call(self, inputs):\n+    def _int8_call(self, inputs, training=None):\n         # We cannot update quantized self._embeddings, so the custom gradient is\n         # not needed\n         if backend.standardize_dtype(inputs.dtype) not in (\"int32\", \"int64\"):\n@@ -345,22 +344,17 @@ class Embedding(Layer):\n         return outputs\n \n     def quantize(self, mode, type_check=True):\n-        import gc\n-\n         # Prevent quantization of the subclasses\n         if type_check and (type(self) is not Embedding):\n-            raise self._quantize_not_implemented_error()\n-        self._check_quantize_args(mode, self.compute_dtype)\n+            raise self._not_implemented_error(self.quantize)\n \n-        self._tracker.unlock()\n         if mode == \"int8\":\n             # Quantize `self._embeddings` to int8 and compute corresponding\n             # scale\n             embeddings_value, embeddings_scale = quantizers.abs_max_quantize(\n-                self._embeddings, axis=-1\n+                self._embeddings, axis=-1, to_numpy=True\n             )\n             embeddings_scale = ops.squeeze(embeddings_scale, axis=-1)\n-            self._untrack_variable(self._embeddings)\n             del self._embeddings\n             # Utilize a lambda expression as an initializer to prevent adding a\n             # large constant to the computation graph.\n@@ -370,16 +364,12 @@ class Embedding(Layer):\n             )\n         else:\n             raise self._quantization_mode_error(mode)\n-        self._tracker.lock()\n \n         # Set new dtype policy\n         if self.dtype_policy.quantization_mode is None:\n             policy = dtype_policies.get(f\"{mode}_from_{self.dtype_policy.name}\")\n             self.dtype_policy = policy\n \n-        # Release memory manually because sometimes the backend doesn't\n-        gc.collect()\n-\n     def _get_embeddings_with_merged_lora(self):\n         if self.dtype_policy.quantization_mode is not None:\n             embeddings_value = self._embeddings\n@@ -395,7 +385,9 @@ class Embedding(Layer):\n                     ops.matmul(self.lora_embeddings_a, self.lora_embeddings_b),\n                 )\n                 embeddings_value, embeddings_scale = (\n-                    quantizers.abs_max_quantize(embeddings_value, axis=-1)\n+                    quantizers.abs_max_quantize(\n+                        embeddings_value, axis=-1, to_numpy=True\n+                    )\n                 )\n                 embeddings_scale = ops.squeeze(embeddings_scale, axis=-1)\n             return embeddings_value, embeddings_scale\n\n@@ -212,10 +212,10 @@ class Layer(BackendLayer, Operation, KerasSaveable):\n     \"\"\"\n \n     def __new__(cls, *args, **kwargs):\n-        # Wrap the user-provided build method in the build_decorator\n-        # to add name scope support and serialization support.\n         obj = super().__new__(cls, *args, **kwargs)\n \n+        # Wrap the user-provided `build` method in the `build_wrapper`\n+        # to add name scope support and serialization support.\n         original_build_method = obj.build\n \n         @wraps(original_build_method)\n@@ -232,6 +232,24 @@ class Layer(BackendLayer, Operation, KerasSaveable):\n             obj._lock_state()\n \n         obj.build = build_wrapper\n+\n+        # Wrap the user-provided `quantize` method in the `quantize_wrapper`\n+        # to add tracker support.\n+        original_quantize_method = obj.quantize\n+\n+        @wraps(original_quantize_method)\n+        def quantize_wrapper(mode, **kwargs):\n+            obj._check_quantize_args(mode, obj.compute_dtype)\n+            obj._tracker.unlock()\n+            try:\n+                original_quantize_method(mode, **kwargs)\n+            except Exception:\n+                raise\n+            finally:\n+                obj._tracker.lock()\n+\n+        obj.quantize = quantize_wrapper\n+\n         return obj\n \n     def __init__(\n@@ -925,16 +943,7 @@ class Layer(BackendLayer, Operation, KerasSaveable):\n         return outputs\n \n     def call(self, *args, **kwargs):\n-        raise NotImplementedError(\n-            f\"Layer {self.__class__.__name__} does not have a `call()` \"\n-            \"method implemented.\"\n-        )\n-\n-    def quantized_call(self, *args, **kwargs):\n-        raise NotImplementedError(\n-            f\"Layer {self.__class__.__name__} does not have a \"\n-            \"`quantized_call()` method implemented.\"\n-        )\n+        raise self._not_implemented_error(self.call)\n \n     @traceback_utils.filter_traceback\n     def stateless_call(\n@@ -1088,9 +1097,9 @@ class Layer(BackendLayer, Operation, KerasSaveable):\n \n     @utils.default\n     def compute_output_shape(self, *args, **kwargs):\n-        raise NotImplementedError(\n-            f\"Layer {self.__class__.__name__} should implement \"\n-            \"`def compute_output_shape(self, input_shape)`.\"\n+        raise self._not_implemented_error(\n+            self.compute_output_shape,\n+            \"Should implement `def compute_output_shape(self, input_shape)`.\",\n         )\n \n     def add_loss(self, loss):\n@@ -1170,14 +1179,13 @@ class Layer(BackendLayer, Operation, KerasSaveable):\n         for layer in self._layers:\n             layer._clear_losses()\n \n-    def quantize(self, mode):\n-        raise self._quantize_not_implemented_error()\n+    # Quantization-related (int8 and float8) methods\n \n-    def _quantize_not_implemented_error(self):\n-        return NotImplementedError(\n-            f\"Layer {self.__class__.__name__} does not have a `quantize()` \"\n-            \"method implemented.\"\n-        )\n+    def quantized_build(self, input_shape, mode):\n+        raise self._not_implemented_error(self.quantized_build)\n+\n+    def quantize(self, mode, type_check=True):\n+        raise self._not_implemented_error(self.quantize)\n \n     def _check_quantize_args(self, mode, compute_dtype):\n         if not self.built:\n@@ -1206,6 +1214,40 @@ class Layer(BackendLayer, Operation, KerasSaveable):\n                 \"'mixed_float16' before calling `quantize()`.\"\n             )\n \n+    def quantized_call(self, *args, **kwargs):\n+        if self.quantization_mode == \"int8\":\n+            return self._int8_call(*args, **kwargs)\n+        elif self.quantization_mode == \"float8\":\n+            return self._float8_call(*args, **kwargs)\n+        else:\n+            raise self._quantization_mode_error(self.quantization_mode)\n+\n+    def _int8_call(self, *args, **kwargs):\n+        raise self._not_implemented_error(self._int8_call)\n+\n+    def _float8_call(self, *args, **kwargs):\n+        raise self._not_implemented_error(self._float8_call)\n+\n+    def _not_implemented_error(self, attr, msg=None):\n+        if callable(attr):\n+            attr_name = attr.__name__\n+            attr_type = \"method\"\n+        else:\n+            attr_name = str(attr)\n+            attr_type = \"attribute\"\n+        msg = \" \" + msg if msg is not None else \"\"\n+        return NotImplementedError(\n+            f\"Layer {self.__class__.__name__} does not have a `{attr_name}` \"\n+            f\"{attr_type} implemented.{msg}\"\n+        )\n+\n+    def _quantization_mode_error(self, mode):\n+        return NotImplementedError(\n+            \"Invalid quantization mode. Expected one of \"\n+            f\"{dtype_policies.QUANTIZATION_MODES}. \"\n+            f\"Received: quantization_mode={mode}\"\n+        )\n+\n     def save_own_variables(self, store):\n         \"\"\"Saves the state of the layer.\n \n@@ -1280,9 +1322,9 @@ class Layer(BackendLayer, Operation, KerasSaveable):\n             self._tracker.lock()\n         self._post_untrack_variable(variable)\n \n-    def add_metric(self):\n+    def add_metric(self, *args, **kwargs):\n         # Permanently disabled\n-        raise NotImplementedError\n+        raise self._not_implemented_error(self.add_metric)\n \n     def count_params(self):\n         \"\"\"Count the total number of scalars composing the weights.\n@@ -1395,6 +1437,20 @@ class Layer(BackendLayer, Operation, KerasSaveable):\n             value = self._tracker.track(value)\n         return super().__setattr__(name, value)\n \n+    def __delattr__(self, name):\n+        obj = getattr(self, name)\n+        if isinstance(obj, backend.Variable):\n+            import gc\n+\n+            # It will take a short amount of time for the corresponding buffer\n+            # to be actually removed from the device.\n+            # https://stackoverflow.com/a/74631949\n+            self._untrack_variable(obj)\n+            super().__delattr__(name)\n+            gc.collect()\n+        else:\n+            super().__delattr__(name)\n+\n     def _check_super_called(self):\n         if getattr(self, \"_lock\", True):\n             raise RuntimeError(\n\n@@ -2,6 +2,7 @@ import pickle\n \n import numpy as np\n import pytest\n+from absl.testing import parameterized\n \n from keras.src import backend\n from keras.src import dtype_policies\n@@ -13,7 +14,7 @@ from keras.src import testing\n from keras.src.backend.common import global_state\n \n \n-class LayerTest(testing.TestCase):\n+class LayerTest(testing.TestCase, parameterized.TestCase):\n \n     def test_compute_output_spec(self):\n         # Test that implementing compute_output_shape\n@@ -140,6 +141,32 @@ class LayerTest(testing.TestCase):\n         # This works\n         SomeLayer()(x, bool_arg=True)\n \n+    @parameterized.named_parameters(\n+        (\"call\", \"call\", None),\n+        (\"compute_output_shape\", \"compute_output_shape\", None),\n+        (\"add_metric\", \"add_metric\", None),\n+        (\n+            \"quantized_build\",\n+            \"quantized_build\",\n+            {\"input_shape\": None, \"mode\": None},\n+        ),\n+        (\"quantize\", \"quantize\", {\"mode\": \"int8\"}),\n+        (\"_int8_call\", \"_int8_call\", None),\n+        (\"_float8_call\", \"_float8_call\", None),\n+    )\n+    def test_not_implemented_error(self, method, args):\n+        layer = layers.Layer()\n+        layer.built = True\n+\n+        with self.assertRaisesRegex(\n+            NotImplementedError,\n+            f\"does not have a `{method}` method implemented.\",\n+        ):\n+            if isinstance(args, dict):\n+                getattr(layer, method)(**args)\n+            else:\n+                getattr(layer, method)(args)\n+\n     def test_rng_seed_tracking(self):\n         class RNGLayer(layers.Layer):\n             def __init__(self):\n\n@@ -354,7 +354,7 @@ class Model(Trainer, base_trainer.Trainer, Layer):\n             self, filepath, skip_mismatch=skip_mismatch, **kwargs\n         )\n \n-    def quantize(self, mode):\n+    def quantize(self, mode, **kwargs):\n         \"\"\"Quantize the weights of the model.\n \n         Note that the model must be built first before calling this method.\n@@ -367,9 +367,11 @@ class Model(Trainer, base_trainer.Trainer, Layer):\n         \"\"\"\n         from keras.src.dtype_policies import QUANTIZATION_MODES\n \n-        if not self.built:\n+        type_check = kwargs.pop(\"type_check\", True)\n+        if kwargs:\n             raise ValueError(\n-                \"The model must be built first before calling `quantize()`.\"\n+                \"Unrecognized keyword arguments \"\n+                f\"passed to {self.__class__.__name__}: {kwargs}\"\n             )\n         if mode not in QUANTIZATION_MODES:\n             raise ValueError(\n@@ -381,7 +383,7 @@ class Model(Trainer, base_trainer.Trainer, Layer):\n             list_of_sublayers = list(layer._flatten_layers())\n             if len(list_of_sublayers) == 1:  # leaves of the model\n                 try:\n-                    layer.quantize(mode)\n+                    layer.quantize(mode, type_check=type_check)\n                     mode_changed = True\n                 except NotImplementedError as e:\n                     warnings.warn(str(e))\n\n@@ -682,7 +682,7 @@ class ModelTest(testing.TestCase, parameterized.TestCase):\n \n         model = MyModel()\n         with self.assertRaisesRegex(\n-            ValueError, \"The model must be built first before calling\"\n+            ValueError, \"Cannot quantize a layer that isn't yet built.\"\n         ):\n             model.quantize(mode)\n \n@@ -697,6 +697,14 @@ class ModelTest(testing.TestCase, parameterized.TestCase):\n         ):\n             model.quantize(\"abc\")\n \n+        with self.assertRaisesRegex(\n+            ValueError, \"Unrecognized keyword arguments\"\n+        ):\n+            model.quantize(\"int8\", unrecognized_kwargs=None)\n+\n+        with self.assertRaisesRegex(ValueError, \"Invalid quantization mode\"):\n+            model.quantize(\"int7\")\n+\n     @parameterized.named_parameters(\n         (\"int8\", \"int8\"),\n         (\"float8\", \"float8\"),\n\n@@ -1,8 +1,10 @@\n import ml_dtypes\n+import numpy as np\n \n from keras.src import backend\n from keras.src import ops\n from keras.src.api_export import keras_export\n+from keras.src.backend.common.backend_utils import standardize_axis_for_numpy\n \n \"\"\"Int8-related classes and methods\"\"\"\n \n@@ -64,7 +66,24 @@ def abs_max_quantize(\n     value_range=(-127, 127),\n     dtype=\"int8\",\n     epsilon=backend.epsilon(),\n+    to_numpy=False,\n ):\n+    if to_numpy:\n+        # Save memory on the device using numpy\n+        original_dtype = backend.standardize_dtype(inputs.dtype)\n+        inputs = ops.convert_to_numpy(inputs)\n+        axis = standardize_axis_for_numpy(axis)\n+        scale = np.divide(\n+            value_range[1],\n+            np.add(np.max(np.abs(inputs), axis=axis, keepdims=True), epsilon),\n+        )\n+        outputs = np.multiply(inputs, scale)\n+        outputs = np.clip(np.round(outputs), value_range[0], value_range[1])\n+        outputs = outputs.astype(dtype)\n+        return ops.convert_to_tensor(outputs), ops.convert_to_tensor(\n+            scale, dtype=original_dtype\n+        )\n+\n     scale = ops.divide(\n         value_range[1],\n         ops.add(ops.max(ops.abs(inputs), axis=axis, keepdims=True), epsilon),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
