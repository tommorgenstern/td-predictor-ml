{"custom_id": "keras#a6c02ead9fa475d20a7245e7f1dfa67e2f281b8d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 2820 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -1109,6 +1109,7 @@ class RecallAtPrecisionTest(testing.TestCase, parameterized.TestCase):\n         ):\n             metrics.RecallAtPrecision(0.4, num_thresholds=-1)\n \n+    @pytest.mark.requires_trainable_backend\n     def test_end_to_end(self):\n         # Test for https://github.com/keras-team/keras-core/issues/718\n         model = models.Sequential(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#50b5b2f2ec363abfa691e276f20680fc0dceeedd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 97 | Lines Deleted: 19 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 116 | Churn Cumulative: 613 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 0.029411764705882353\n\nDIFF:\n@@ -1,7 +1,10 @@\n+import math\n+\n import numpy as np\n import pytest\n import tensorflow as tf\n from absl.testing import parameterized\n+from numpy.lib.stride_tricks import as_strided\n \n from keras_core import layers\n from keras_core import testing\n@@ -245,6 +248,85 @@ class ConvBasicTest(testing.TestCase, parameterized.TestCase):\n \n \n class ConvCorrectnessTest(testing.TestCase, parameterized.TestCase):\n+    def _same_padding(self, input_size, kernel_size, stride):\n+        # P = ((S-1)*W-S+K)/2, with K = kernel size, S = stride, W = input size\n+        padding = int(\n+            math.ceil(((stride - 1) * input_size - stride + kernel_size) / 2)\n+        )\n+        return padding\n+\n+    def _np_conv1d(\n+        self,\n+        x,\n+        kernel_weights,\n+        bias_weights,\n+        strides,\n+        padding,\n+        data_format,\n+        dilation_rate,\n+        groups,\n+    ):\n+        if data_format == \"channels_first\":\n+            x = x.swapaxes(1, 2)\n+        if isinstance(strides, (tuple, list)):\n+            h_stride = strides[0]\n+        else:\n+            h_stride = strides\n+        if isinstance(dilation_rate, (tuple, list)):\n+            dilation_rate = dilation_rate[0]\n+        kernel_size, ch_in, ch_out = kernel_weights.shape\n+\n+        if dilation_rate > 1:\n+            new_kernel_size = kernel_size + (dilation_rate - 1) * (\n+                kernel_size - 1\n+            )\n+            new_kernel_weights = np.zeros(\n+                (new_kernel_size, ch_in, ch_out), dtype=kernel_weights.dtype\n+            )\n+            new_kernel_weights[::dilation_rate] = kernel_weights\n+            kernel_weights = new_kernel_weights\n+            kernel_size = kernel_weights.shape[0]\n+\n+        if padding != \"valid\":\n+            n_batch, h_x, _ = x.shape\n+            h_pad = self._same_padding(h_x, kernel_size, h_stride)\n+            npad = [(0, 0)] * x.ndim\n+            if padding == \"causal\":\n+                npad[1] = (h_pad * 2, 0)\n+            else:\n+                npad[1] = (h_pad, h_pad)\n+            x = np.pad(x, pad_width=npad, mode=\"constant\", constant_values=0)\n+\n+        n_batch, h_x, _ = x.shape\n+        h_out = int((h_x - kernel_size) / h_stride) + 1\n+\n+        kernel_weights = kernel_weights.reshape(-1, ch_out)\n+        bias_weights = bias_weights.reshape(1, ch_out)\n+\n+        out_grps = []\n+        for grp in range(1, groups + 1):\n+            x_in = x[..., (grp - 1) * ch_in : grp * ch_in]\n+            stride_shape = (n_batch, h_out, kernel_size, ch_in)\n+            strides = (\n+                x_in.strides[0],\n+                h_stride * x_in.strides[1],\n+                x_in.strides[1],\n+                x_in.strides[2],\n+            )\n+            inner_dim = kernel_size * ch_in\n+            x_strided = as_strided(\n+                x_in, shape=stride_shape, strides=strides\n+            ).reshape(n_batch, h_out, inner_dim)\n+            ch_out_groups = ch_out // groups\n+            kernel_weights_grp = kernel_weights[\n+                ..., (grp - 1) * ch_out_groups : grp * ch_out_groups\n+            ]\n+            bias_weights_grp = bias_weights[\n+                ..., (grp - 1) * ch_out_groups : grp * ch_out_groups\n+            ]\n+            out_grps.append(x_strided @ kernel_weights_grp + bias_weights_grp)\n+        return np.concatenate(out_grps, axis=-1)\n+\n     @parameterized.parameters(\n         {\n             \"filters\": 5,\n@@ -302,31 +384,27 @@ class ConvCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             dilation_rate=dilation_rate,\n             groups=groups,\n         )\n-        tf_keras_layer = tf.keras.layers.Conv1D(\n-            filters=filters,\n-            kernel_size=kernel_size,\n+\n+        inputs = np.random.normal(size=[2, 8, 4])\n+        layer.build(input_shape=inputs.shape)\n+\n+        kernel_shape = layer.kernel.shape\n+        kernel_weights = np.random.normal(size=kernel_shape)\n+        bias_weights = np.random.normal(size=(filters,))\n+        layer.kernel.assign(kernel_weights)\n+        layer.bias.assign(bias_weights)\n+\n+        outputs = layer(inputs)\n+        expected = self._np_conv1d(\n+            inputs,\n+            kernel_weights,\n+            bias_weights,\n             strides=strides,\n             padding=padding,\n             data_format=data_format,\n             dilation_rate=dilation_rate,\n             groups=groups,\n         )\n-\n-        inputs = np.random.normal(size=[2, 8, 4])\n-        layer.build(input_shape=inputs.shape)\n-        tf_keras_layer.build(input_shape=inputs.shape)\n-\n-        kernel_shape = layer.kernel.shape\n-        kernel_weights = np.random.normal(size=kernel_shape)\n-        bias_weights = np.random.normal(size=(filters,))\n-        layer.kernel.assign(kernel_weights)\n-        tf_keras_layer.kernel.assign(kernel_weights)\n-\n-        layer.bias.assign(bias_weights)\n-        tf_keras_layer.bias.assign(bias_weights)\n-\n-        outputs = layer(inputs)\n-        expected = tf_keras_layer(inputs)\n         self.assertAllClose(outputs, expected)\n \n     @parameterized.parameters(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#897f2c2542f0a615068c79676988a1f55d1d42f6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 140 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 12 | Complexity Δ (Sum/Max): 12/12 | Churn Δ: 141 | Churn Cumulative: 861 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 0.9913793103448276\n\nDIFF:\n@@ -7,6 +7,7 @@ from absl import logging\n \n from keras_core import backend\n from keras_core import optimizers\n+from keras_core.backend.common import global_state\n from keras_core.legacy.saving import json_utils\n from keras_core.legacy.saving import saving_options\n from keras_core.legacy.saving import saving_utils\n@@ -108,7 +109,8 @@ def load_model_from_hdf5(filepath, custom_objects=None, compile=True):\n         custom_objects = {}\n \n     gco = object_registration.GLOBAL_CUSTOM_OBJECTS\n-    custom_objects = {**custom_objects, **gco}\n+    tlco = global_state.get_global_attribute(\"custom_objects_scope_dict\", {})\n+    custom_objects = {**custom_objects, **gco, **tlco}\n \n     opened_new_file = not isinstance(filepath, h5py.File)\n     if opened_new_file:\n\n@@ -9,6 +9,8 @@ from keras_core import layers\n from keras_core import models\n from keras_core import testing\n from keras_core.legacy.saving import legacy_h5_format\n+from keras_core.saving import object_registration\n+from keras_core.saving import serialization_lib\n \n # TODO: more thorough testing. Correctness depends\n # on exact weight ordering for each layer, so we need\n@@ -122,6 +124,141 @@ class LegacyH5WholeModelTest(testing.TestCase):\n         ref_input = np.random.random((1, 3))\n         self._check_reloading_model(ref_input, model)\n \n+    def test_saving_lambda(self):\n+        mean = np.random.random((4, 2, 3))\n+        std = np.abs(np.random.random((4, 2, 3))) + 1e-5\n+        inputs = layers.Input(shape=(4, 2, 3))\n+        output = layers.Lambda(\n+            lambda image, mu, std: (image - mu) / std,\n+            arguments={\"mu\": mean, \"std\": std},\n+        )(inputs)\n+        model = models.Model(inputs, output)\n+        model.compile(loss=\"mse\", optimizer=\"sgd\", metrics=[\"acc\"])\n+\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"lambda_model.h5\")\n+        legacy_h5_format.save_model_to_hdf5(model, temp_filepath)\n+        loaded = legacy_h5_format.load_model_from_hdf5(temp_filepath)\n+\n+        self.assertAllClose(mean, loaded.layers[1].arguments[\"mu\"])\n+        self.assertAllClose(std, loaded.layers[1].arguments[\"std\"])\n+\n+    def test_saving_include_optimizer_false(self):\n+        model = models.Sequential()\n+        model.add(layers.Dense(1))\n+        model.compile(\"adam\", loss=\"mse\")\n+        x, y = np.ones((10, 10)), np.ones((10, 1))\n+        model.fit(x, y)\n+        ref_output = model(x)\n+\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"model.h5\")\n+        legacy_h5_format.save_model_to_hdf5(\n+            model, temp_filepath, include_optimizer=False\n+        )\n+        loaded = legacy_h5_format.load_model_from_hdf5(temp_filepath)\n+        output = loaded(x)\n+\n+        # Assert that optimizer does not exist in loaded model\n+        with self.assertRaises(AttributeError):\n+            _ = loaded.optimizer\n+\n+        # Compare output\n+        self.assertAllClose(ref_output, output, atol=1e-5)\n+\n+    def test_custom_sequential_registered_no_scope(self):\n+        @object_registration.register_keras_serializable(package=\"my_package\")\n+        class MyDense(layers.Dense):\n+            def __init__(self, units, **kwargs):\n+                super().__init__(units, **kwargs)\n+\n+        input_shape = [1]\n+        inputs = layers.Input(shape=input_shape)\n+        custom_layer = MyDense(1)\n+        model = models.Sequential(layers=[inputs, custom_layer])\n+\n+        ref_input = np.array([5])\n+        self._check_reloading_model(ref_input, model)\n+\n+    def test_custom_functional_registered_no_scope(self):\n+        @object_registration.register_keras_serializable(package=\"my_package\")\n+        class MyDense(layers.Dense):\n+            def __init__(self, units, **kwargs):\n+                super().__init__(units, **kwargs)\n+\n+        inputs = layers.Input(shape=[1])\n+        outputs = MyDense(1)(inputs)\n+        model = models.Model(inputs, outputs)\n+\n+        ref_input = np.array([5])\n+        self._check_reloading_model(ref_input, model)\n+\n+    def test_nested_layers(self):\n+        class MyLayer(layers.Layer):\n+            def __init__(self, sublayers, **kwargs):\n+                super().__init__(**kwargs)\n+                self.sublayers = sublayers\n+\n+            def call(self, x):\n+                prev_input = x\n+                for layer in self.sublayers:\n+                    prev_input = layer(prev_input)\n+                return prev_input\n+\n+            def get_config(self):\n+                config = super().get_config()\n+                config[\"sublayers\"] = serialization_lib.serialize_keras_object(\n+                    self.sublayers\n+                )\n+                return config\n+\n+            @classmethod\n+            def from_config(cls, config):\n+                config[\n+                    \"sublayers\"\n+                ] = serialization_lib.deserialize_keras_object(\n+                    config[\"sublayers\"]\n+                )\n+                return cls(**config)\n+\n+        @object_registration.register_keras_serializable(package=\"Foo\")\n+        class RegisteredSubLayer(layers.Layer):\n+            pass\n+\n+        layer = MyLayer(\n+            [\n+                layers.Dense(2, name=\"MyDense\"),\n+                RegisteredSubLayer(name=\"MySubLayer\"),\n+            ]\n+        )\n+        model = models.Sequential([layer])\n+        with self.subTest(\"test_JSON\"):\n+            from keras_core.models.model import model_from_json\n+\n+            model_json = model.to_json()\n+            self.assertIn(\"Foo>RegisteredSubLayer\", model_json)\n+\n+            loaded_model = model_from_json(\n+                model_json, custom_objects={\"MyLayer\": MyLayer}\n+            )\n+            loaded_layer = loaded_model.layers[0]\n+\n+            self.assertIsInstance(loaded_layer.sublayers[0], layers.Dense)\n+            self.assertEqual(loaded_layer.sublayers[0].name, \"MyDense\")\n+            self.assertIsInstance(loaded_layer.sublayers[1], RegisteredSubLayer)\n+            self.assertEqual(loaded_layer.sublayers[1].name, \"MySubLayer\")\n+\n+        with self.subTest(\"test_H5\"):\n+            temp_filepath = os.path.join(self.get_temp_dir(), \"model.h5\")\n+            legacy_h5_format.save_model_to_hdf5(model, temp_filepath)\n+            loaded_model = legacy_h5_format.load_model_from_hdf5(\n+                temp_filepath, custom_objects={\"MyLayer\": MyLayer}\n+            )\n+            loaded_layer = loaded_model.layers[0]\n+\n+            self.assertIsInstance(loaded_layer.sublayers[0], layers.Dense)\n+            self.assertEqual(loaded_layer.sublayers[0].name, \"MyDense\")\n+            self.assertIsInstance(loaded_layer.sublayers[1], RegisteredSubLayer)\n+            self.assertEqual(loaded_layer.sublayers[1].name, \"MySubLayer\")\n+\n \n @pytest.mark.requires_trainable_backend\n class LegacyH5BackwardsCompatTest(testing.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#33ed910130250f047f11d09f3f3cda6d89c34d27", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 22 | Churn Cumulative: 1014 | Contributors (this commit): 13 | Commits (past 90d): 40 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -87,7 +87,23 @@ def is_tensor(x):\n \n \n def shape(x):\n-    return tf.shape(x)\n+    \"\"\"Always return a tuple shape.\n+\n+    `tf.shape` will return a `tf.Tensor`, which differs from the tuple return\n+    type on the torch and jax backends. We write our own method instead which\n+    always returns a tuple, with integer values when the shape is known, and\n+    tensor values when the shape is unknown (this is tf specific, as dynamic\n+    shapes do not apply in other backends).\n+    \"\"\"\n+    x = tf.convert_to_tensor(x)\n+    dynamic = tf.shape(x)\n+    if x.shape == tf.TensorShape(None):\n+        raise ValueError(\n+            \"All tensors passed to `ops.shape` must have a statically known \"\n+            f\"rank. Received: x={x} with unknown rank.\"\n+        )\n+    static = x.shape.as_list()\n+    return tuple(dynamic[i] if s is None else s for i, s in enumerate(static))\n \n \n def cast(x, dtype):\n\n@@ -402,6 +402,10 @@ def unstack(x, num=None, axis=0):\n def shape(x):\n     \"\"\"Gets the shape of the tensor input.\n \n+    Note: On the tensorflow backend, when `x` is a `tf.Tensor` with dynamic\n+    shape, dimensions which are dynamic in the context of a compiled function\n+    will have a `tf.Tensor` value instead of a static integer value.\n+\n     Args:\n         x: A tensor. This function will try to access the `shape` attribute of\n             the input tensor.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#26e453d259dcb63fddeabdfd59c4f4b340dd0b34", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 1030 | Contributors (this commit): 6 | Commits (past 90d): 12 | Contributors (cumulative): 9 | DMM Complexity: 0.7777777777777778\n\nDIFF:\n@@ -198,7 +198,10 @@ class LayerNormalization(Layer):\n \n         # Calculate the mean and variance last axis (layer activations).\n         mean = ops.mean(inputs, axis=self.axis, keepdims=True)\n-        variance = ops.var(inputs, axis=self.axis, keepdims=True)\n+        # Don't use ops.var, as that would re-compute the mean from scratch.\n+        variance = ops.mean(\n+            ops.power(inputs - mean, 2), axis=self.axis, keepdims=True\n+        )\n \n         scale, offset = _broadcast(self.gamma), _broadcast(self.beta)\n \n\n@@ -85,3 +85,11 @@ class LayerNormalizationTest(testing.TestCase):\n \n         self.assertAllClose(ops.mean(out), 0.0, atol=1e-1)\n         self.assertAllClose(ops.std(out), 1.0, atol=1e-1)\n+\n+    def test_output(self):\n+        layer = layers.LayerNormalization(\n+            dtype=\"float32\", beta_initializer=\"ones\", gamma_initializer=\"ones\",\n+        )\n+        inputs = np.arange(5).astype(\"float32\")[None, :]\n+        out = layer(inputs)\n+        self.assertAllClose(out, [[-0.41386, 0.29307, 1., 1.70693, 2.41386]])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b0563e216dc8ff45308cbd84e87d0abf57dcc9e3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 67 | Lines Deleted: 270 | Files Changed: 11 | Hunks: 17 | Methods Changed: 27 | Complexity Δ (Sum/Max): -19/6 | Churn Δ: 337 | Churn Cumulative: 10926 | Contributors (this commit): 26 | Commits (past 90d): 182 | Contributors (cumulative): 75 | DMM Complexity: 0.38650306748466257\n\nDIFF:\n@@ -416,6 +416,8 @@ class JAXTrainer(base_trainer.Trainer):\n                         sample_weight=val_sample_weight,\n                         batch_size=validation_batch_size or batch_size,\n                         steps_per_execution=self.steps_per_execution,\n+                        steps_per_epoch=validation_steps,\n+                        shuffle=False,\n                     )\n                 val_logs = self.evaluate(\n                     x=val_x,\n\n@@ -87,23 +87,7 @@ def is_tensor(x):\n \n \n def shape(x):\n-    \"\"\"Always return a tuple shape.\n-\n-    `tf.shape` will return a `tf.Tensor`, which differs from the tuple return\n-    type on the torch and jax backends. We write our own method instead which\n-    always returns a tuple, with integer values when the shape is known, and\n-    tensor values when the shape is unknown (this is tf specific, as dynamic\n-    shapes do not apply in other backends).\n-    \"\"\"\n-    x = tf.convert_to_tensor(x)\n-    dynamic = tf.shape(x)\n-    if x.shape == tf.TensorShape(None):\n-        raise ValueError(\n-            \"All tensors passed to `ops.shape` must have a statically known \"\n-            f\"rank. Received: x={x} with unknown rank.\"\n-        )\n-    static = x.shape.as_list()\n-    return tuple(dynamic[i] if s is None else s for i, s in enumerate(static))\n+    return tf.shape(x)\n \n \n def cast(x, dtype):\n\n@@ -338,6 +338,8 @@ class TensorFlowTrainer(base_trainer.Trainer):\n                         batch_size=validation_batch_size or batch_size,\n                         distribute_strategy=self.distribute_strategy,\n                         steps_per_execution=self.steps_per_execution,\n+                        steps_per_epoch=validation_steps,\n+                        shuffle=False,\n                     )\n                 val_logs = self.evaluate(\n                     x=val_x,\n\n@@ -313,6 +313,8 @@ class TorchTrainer(base_trainer.Trainer):\n                         sample_weight=val_sample_weight,\n                         batch_size=validation_batch_size or batch_size,\n                         steps_per_execution=self.steps_per_execution,\n+                        steps_per_epoch=validation_steps,\n+                        shuffle=False,\n                     )\n                 val_logs = self.evaluate(\n                     x=val_x,\n\n@@ -1,10 +1,7 @@\n-import math\n-\n import numpy as np\n import pytest\n import tensorflow as tf\n from absl.testing import parameterized\n-from numpy.lib.stride_tricks import as_strided\n \n from keras_core import layers\n from keras_core import testing\n@@ -248,85 +245,6 @@ class ConvBasicTest(testing.TestCase, parameterized.TestCase):\n \n \n class ConvCorrectnessTest(testing.TestCase, parameterized.TestCase):\n-    def _same_padding(self, input_size, kernel_size, stride):\n-        # P = ((S-1)*W-S+K)/2, with K = kernel size, S = stride, W = input size\n-        padding = int(\n-            math.ceil(((stride - 1) * input_size - stride + kernel_size) / 2)\n-        )\n-        return padding\n-\n-    def _np_conv1d(\n-        self,\n-        x,\n-        kernel_weights,\n-        bias_weights,\n-        strides,\n-        padding,\n-        data_format,\n-        dilation_rate,\n-        groups,\n-    ):\n-        if data_format == \"channels_first\":\n-            x = x.swapaxes(1, 2)\n-        if isinstance(strides, (tuple, list)):\n-            h_stride = strides[0]\n-        else:\n-            h_stride = strides\n-        if isinstance(dilation_rate, (tuple, list)):\n-            dilation_rate = dilation_rate[0]\n-        kernel_size, ch_in, ch_out = kernel_weights.shape\n-\n-        if dilation_rate > 1:\n-            new_kernel_size = kernel_size + (dilation_rate - 1) * (\n-                kernel_size - 1\n-            )\n-            new_kernel_weights = np.zeros(\n-                (new_kernel_size, ch_in, ch_out), dtype=kernel_weights.dtype\n-            )\n-            new_kernel_weights[::dilation_rate] = kernel_weights\n-            kernel_weights = new_kernel_weights\n-            kernel_size = kernel_weights.shape[0]\n-\n-        if padding != \"valid\":\n-            n_batch, h_x, _ = x.shape\n-            h_pad = self._same_padding(h_x, kernel_size, h_stride)\n-            npad = [(0, 0)] * x.ndim\n-            if padding == \"causal\":\n-                npad[1] = (h_pad * 2, 0)\n-            else:\n-                npad[1] = (h_pad, h_pad)\n-            x = np.pad(x, pad_width=npad, mode=\"constant\", constant_values=0)\n-\n-        n_batch, h_x, _ = x.shape\n-        h_out = int((h_x - kernel_size) / h_stride) + 1\n-\n-        kernel_weights = kernel_weights.reshape(-1, ch_out)\n-        bias_weights = bias_weights.reshape(1, ch_out)\n-\n-        out_grps = []\n-        for grp in range(1, groups + 1):\n-            x_in = x[..., (grp - 1) * ch_in : grp * ch_in]\n-            stride_shape = (n_batch, h_out, kernel_size, ch_in)\n-            strides = (\n-                x_in.strides[0],\n-                h_stride * x_in.strides[1],\n-                x_in.strides[1],\n-                x_in.strides[2],\n-            )\n-            inner_dim = kernel_size * ch_in\n-            x_strided = as_strided(\n-                x_in, shape=stride_shape, strides=strides\n-            ).reshape(n_batch, h_out, inner_dim)\n-            ch_out_groups = ch_out // groups\n-            kernel_weights_grp = kernel_weights[\n-                ..., (grp - 1) * ch_out_groups : grp * ch_out_groups\n-            ]\n-            bias_weights_grp = bias_weights[\n-                ..., (grp - 1) * ch_out_groups : grp * ch_out_groups\n-            ]\n-            out_grps.append(x_strided @ kernel_weights_grp + bias_weights_grp)\n-        return np.concatenate(out_grps, axis=-1)\n-\n     @parameterized.parameters(\n         {\n             \"filters\": 5,\n@@ -384,27 +302,31 @@ class ConvCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             dilation_rate=dilation_rate,\n             groups=groups,\n         )\n-\n-        inputs = np.random.normal(size=[2, 8, 4])\n-        layer.build(input_shape=inputs.shape)\n-\n-        kernel_shape = layer.kernel.shape\n-        kernel_weights = np.random.normal(size=kernel_shape)\n-        bias_weights = np.random.normal(size=(filters,))\n-        layer.kernel.assign(kernel_weights)\n-        layer.bias.assign(bias_weights)\n-\n-        outputs = layer(inputs)\n-        expected = self._np_conv1d(\n-            inputs,\n-            kernel_weights,\n-            bias_weights,\n+        tf_keras_layer = tf.keras.layers.Conv1D(\n+            filters=filters,\n+            kernel_size=kernel_size,\n             strides=strides,\n             padding=padding,\n             data_format=data_format,\n             dilation_rate=dilation_rate,\n             groups=groups,\n         )\n+\n+        inputs = np.random.normal(size=[2, 8, 4])\n+        layer.build(input_shape=inputs.shape)\n+        tf_keras_layer.build(input_shape=inputs.shape)\n+\n+        kernel_shape = layer.kernel.shape\n+        kernel_weights = np.random.normal(size=kernel_shape)\n+        bias_weights = np.random.normal(size=(filters,))\n+        layer.kernel.assign(kernel_weights)\n+        tf_keras_layer.kernel.assign(kernel_weights)\n+\n+        layer.bias.assign(bias_weights)\n+        tf_keras_layer.bias.assign(bias_weights)\n+\n+        outputs = layer(inputs)\n+        expected = tf_keras_layer(inputs)\n         self.assertAllClose(outputs, expected)\n \n     @parameterized.parameters(\n\n@@ -198,10 +198,7 @@ class LayerNormalization(Layer):\n \n         # Calculate the mean and variance last axis (layer activations).\n         mean = ops.mean(inputs, axis=self.axis, keepdims=True)\n-        # Don't use ops.var, as that would re-compute the mean from scratch.\n-        variance = ops.mean(\n-            ops.power(inputs - mean, 2), axis=self.axis, keepdims=True\n-        )\n+        variance = ops.var(inputs, axis=self.axis, keepdims=True)\n \n         scale, offset = _broadcast(self.gamma), _broadcast(self.beta)\n \n\n@@ -85,11 +85,3 @@ class LayerNormalizationTest(testing.TestCase):\n \n         self.assertAllClose(ops.mean(out), 0.0, atol=1e-1)\n         self.assertAllClose(ops.std(out), 1.0, atol=1e-1)\n-\n-    def test_output(self):\n-        layer = layers.LayerNormalization(\n-            dtype=\"float32\", beta_initializer=\"ones\", gamma_initializer=\"ones\",\n-        )\n-        inputs = np.arange(5).astype(\"float32\")[None, :]\n-        out = layer(inputs)\n-        self.assertAllClose(out, [[-0.41386, 0.29307, 1., 1.70693, 2.41386]])\n\n@@ -7,7 +7,6 @@ from absl import logging\n \n from keras_core import backend\n from keras_core import optimizers\n-from keras_core.backend.common import global_state\n from keras_core.legacy.saving import json_utils\n from keras_core.legacy.saving import saving_options\n from keras_core.legacy.saving import saving_utils\n@@ -109,8 +108,7 @@ def load_model_from_hdf5(filepath, custom_objects=None, compile=True):\n         custom_objects = {}\n \n     gco = object_registration.GLOBAL_CUSTOM_OBJECTS\n-    tlco = global_state.get_global_attribute(\"custom_objects_scope_dict\", {})\n-    custom_objects = {**custom_objects, **gco, **tlco}\n+    custom_objects = {**custom_objects, **gco}\n \n     opened_new_file = not isinstance(filepath, h5py.File)\n     if opened_new_file:\n\n@@ -9,8 +9,6 @@ from keras_core import layers\n from keras_core import models\n from keras_core import testing\n from keras_core.legacy.saving import legacy_h5_format\n-from keras_core.saving import object_registration\n-from keras_core.saving import serialization_lib\n \n # TODO: more thorough testing. Correctness depends\n # on exact weight ordering for each layer, so we need\n@@ -124,141 +122,6 @@ class LegacyH5WholeModelTest(testing.TestCase):\n         ref_input = np.random.random((1, 3))\n         self._check_reloading_model(ref_input, model)\n \n-    def test_saving_lambda(self):\n-        mean = np.random.random((4, 2, 3))\n-        std = np.abs(np.random.random((4, 2, 3))) + 1e-5\n-        inputs = layers.Input(shape=(4, 2, 3))\n-        output = layers.Lambda(\n-            lambda image, mu, std: (image - mu) / std,\n-            arguments={\"mu\": mean, \"std\": std},\n-        )(inputs)\n-        model = models.Model(inputs, output)\n-        model.compile(loss=\"mse\", optimizer=\"sgd\", metrics=[\"acc\"])\n-\n-        temp_filepath = os.path.join(self.get_temp_dir(), \"lambda_model.h5\")\n-        legacy_h5_format.save_model_to_hdf5(model, temp_filepath)\n-        loaded = legacy_h5_format.load_model_from_hdf5(temp_filepath)\n-\n-        self.assertAllClose(mean, loaded.layers[1].arguments[\"mu\"])\n-        self.assertAllClose(std, loaded.layers[1].arguments[\"std\"])\n-\n-    def test_saving_include_optimizer_false(self):\n-        model = models.Sequential()\n-        model.add(layers.Dense(1))\n-        model.compile(\"adam\", loss=\"mse\")\n-        x, y = np.ones((10, 10)), np.ones((10, 1))\n-        model.fit(x, y)\n-        ref_output = model(x)\n-\n-        temp_filepath = os.path.join(self.get_temp_dir(), \"model.h5\")\n-        legacy_h5_format.save_model_to_hdf5(\n-            model, temp_filepath, include_optimizer=False\n-        )\n-        loaded = legacy_h5_format.load_model_from_hdf5(temp_filepath)\n-        output = loaded(x)\n-\n-        # Assert that optimizer does not exist in loaded model\n-        with self.assertRaises(AttributeError):\n-            _ = loaded.optimizer\n-\n-        # Compare output\n-        self.assertAllClose(ref_output, output, atol=1e-5)\n-\n-    def test_custom_sequential_registered_no_scope(self):\n-        @object_registration.register_keras_serializable(package=\"my_package\")\n-        class MyDense(layers.Dense):\n-            def __init__(self, units, **kwargs):\n-                super().__init__(units, **kwargs)\n-\n-        input_shape = [1]\n-        inputs = layers.Input(shape=input_shape)\n-        custom_layer = MyDense(1)\n-        model = models.Sequential(layers=[inputs, custom_layer])\n-\n-        ref_input = np.array([5])\n-        self._check_reloading_model(ref_input, model)\n-\n-    def test_custom_functional_registered_no_scope(self):\n-        @object_registration.register_keras_serializable(package=\"my_package\")\n-        class MyDense(layers.Dense):\n-            def __init__(self, units, **kwargs):\n-                super().__init__(units, **kwargs)\n-\n-        inputs = layers.Input(shape=[1])\n-        outputs = MyDense(1)(inputs)\n-        model = models.Model(inputs, outputs)\n-\n-        ref_input = np.array([5])\n-        self._check_reloading_model(ref_input, model)\n-\n-    def test_nested_layers(self):\n-        class MyLayer(layers.Layer):\n-            def __init__(self, sublayers, **kwargs):\n-                super().__init__(**kwargs)\n-                self.sublayers = sublayers\n-\n-            def call(self, x):\n-                prev_input = x\n-                for layer in self.sublayers:\n-                    prev_input = layer(prev_input)\n-                return prev_input\n-\n-            def get_config(self):\n-                config = super().get_config()\n-                config[\"sublayers\"] = serialization_lib.serialize_keras_object(\n-                    self.sublayers\n-                )\n-                return config\n-\n-            @classmethod\n-            def from_config(cls, config):\n-                config[\n-                    \"sublayers\"\n-                ] = serialization_lib.deserialize_keras_object(\n-                    config[\"sublayers\"]\n-                )\n-                return cls(**config)\n-\n-        @object_registration.register_keras_serializable(package=\"Foo\")\n-        class RegisteredSubLayer(layers.Layer):\n-            pass\n-\n-        layer = MyLayer(\n-            [\n-                layers.Dense(2, name=\"MyDense\"),\n-                RegisteredSubLayer(name=\"MySubLayer\"),\n-            ]\n-        )\n-        model = models.Sequential([layer])\n-        with self.subTest(\"test_JSON\"):\n-            from keras_core.models.model import model_from_json\n-\n-            model_json = model.to_json()\n-            self.assertIn(\"Foo>RegisteredSubLayer\", model_json)\n-\n-            loaded_model = model_from_json(\n-                model_json, custom_objects={\"MyLayer\": MyLayer}\n-            )\n-            loaded_layer = loaded_model.layers[0]\n-\n-            self.assertIsInstance(loaded_layer.sublayers[0], layers.Dense)\n-            self.assertEqual(loaded_layer.sublayers[0].name, \"MyDense\")\n-            self.assertIsInstance(loaded_layer.sublayers[1], RegisteredSubLayer)\n-            self.assertEqual(loaded_layer.sublayers[1].name, \"MySubLayer\")\n-\n-        with self.subTest(\"test_H5\"):\n-            temp_filepath = os.path.join(self.get_temp_dir(), \"model.h5\")\n-            legacy_h5_format.save_model_to_hdf5(model, temp_filepath)\n-            loaded_model = legacy_h5_format.load_model_from_hdf5(\n-                temp_filepath, custom_objects={\"MyLayer\": MyLayer}\n-            )\n-            loaded_layer = loaded_model.layers[0]\n-\n-            self.assertIsInstance(loaded_layer.sublayers[0], layers.Dense)\n-            self.assertEqual(loaded_layer.sublayers[0].name, \"MyDense\")\n-            self.assertIsInstance(loaded_layer.sublayers[1], RegisteredSubLayer)\n-            self.assertEqual(loaded_layer.sublayers[1].name, \"MySubLayer\")\n-\n \n @pytest.mark.requires_trainable_backend\n class LegacyH5BackwardsCompatTest(testing.TestCase):\n\n@@ -402,10 +402,6 @@ def unstack(x, num=None, axis=0):\n def shape(x):\n     \"\"\"Gets the shape of the tensor input.\n \n-    Note: On the tensorflow backend, when `x` is a `tf.Tensor` with dynamic\n-    shape, dimensions which are dynamic in the context of a compiled function\n-    will have a `tf.Tensor` value instead of a static integer value.\n-\n     Args:\n         x: A tensor. This function will try to access the `shape` attribute of\n             the input tensor.\n\n@@ -700,3 +700,42 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n         self.assertEqual(predict_out.shape, (3, 2))\n         predict_out = model.predict_on_batch([np.ones((3, 2)), np.ones((3, 3))])\n         self.assertEqual(predict_out.shape, (3, 2))\n+\n+    @pytest.mark.requires_trainable_backend\n+    def test_validation_data_infinite_generator(self):\n+        # Test that you can pass an infinite generator to `validation_data`\n+        # arg of fit() as well as a `validation_steps` argument and that\n+        # validation only runs for the correct number of steps.\n+        inputs = layers.Input((2,))\n+        outputs = layers.Dense(3)(inputs)\n+        model = keras_core.Model(inputs, outputs)\n+        model.compile(optimizer=\"sgd\", loss=\"mse\", metrics=[\"mse\"])\n+\n+        class Recorder(keras_core.callbacks.Callback):\n+            def __init__(self):\n+                self.train_counter = 0\n+                self.val_counter = 0\n+\n+            def on_train_batch_end(self, *args, **kwargs):\n+                self.train_counter += 1\n+\n+            def on_test_batch_end(self, *args, **kwargs):\n+                self.val_counter += 1\n+\n+        def infinite_gen():\n+            while True:\n+                yield np.ones((2, 2)), np.ones((2, 3))\n+\n+        recorder = Recorder()\n+\n+        model.fit(\n+            infinite_gen(),\n+            validation_data=infinite_gen(),\n+            steps_per_epoch=3,\n+            validation_steps=4,\n+            epochs=1,\n+            shuffle=False,\n+            callbacks=[recorder],\n+        )\n+        self.assertEqual(recorder.train_counter, 3)\n+        self.assertEqual(recorder.val_counter, 4)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d56e7c68213bd9049c3987e8232fa752d256b70a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 270 | Lines Deleted: 22 | Files Changed: 7 | Hunks: 13 | Methods Changed: 19 | Complexity Δ (Sum/Max): 25/12 | Churn Δ: 292 | Churn Cumulative: 4102 | Contributors (this commit): 16 | Commits (past 90d): 75 | Contributors (cumulative): 35 | DMM Complexity: 0.6567164179104478\n\nDIFF:\n@@ -87,7 +87,23 @@ def is_tensor(x):\n \n \n def shape(x):\n-    return tf.shape(x)\n+    \"\"\"Always return a tuple shape.\n+\n+    `tf.shape` will return a `tf.Tensor`, which differs from the tuple return\n+    type on the torch and jax backends. We write our own method instead which\n+    always returns a tuple, with integer values when the shape is known, and\n+    tensor values when the shape is unknown (this is tf specific, as dynamic\n+    shapes do not apply in other backends).\n+    \"\"\"\n+    x = tf.convert_to_tensor(x)\n+    dynamic = tf.shape(x)\n+    if x.shape == tf.TensorShape(None):\n+        raise ValueError(\n+            \"All tensors passed to `ops.shape` must have a statically known \"\n+            f\"rank. Received: x={x} with unknown rank.\"\n+        )\n+    static = x.shape.as_list()\n+    return tuple(dynamic[i] if s is None else s for i, s in enumerate(static))\n \n \n def cast(x, dtype):\n\n@@ -1,7 +1,10 @@\n+import math\n+\n import numpy as np\n import pytest\n import tensorflow as tf\n from absl.testing import parameterized\n+from numpy.lib.stride_tricks import as_strided\n \n from keras_core import layers\n from keras_core import testing\n@@ -245,6 +248,85 @@ class ConvBasicTest(testing.TestCase, parameterized.TestCase):\n \n \n class ConvCorrectnessTest(testing.TestCase, parameterized.TestCase):\n+    def _same_padding(self, input_size, kernel_size, stride):\n+        # P = ((S-1)*W-S+K)/2, with K = kernel size, S = stride, W = input size\n+        padding = int(\n+            math.ceil(((stride - 1) * input_size - stride + kernel_size) / 2)\n+        )\n+        return padding\n+\n+    def _np_conv1d(\n+        self,\n+        x,\n+        kernel_weights,\n+        bias_weights,\n+        strides,\n+        padding,\n+        data_format,\n+        dilation_rate,\n+        groups,\n+    ):\n+        if data_format == \"channels_first\":\n+            x = x.swapaxes(1, 2)\n+        if isinstance(strides, (tuple, list)):\n+            h_stride = strides[0]\n+        else:\n+            h_stride = strides\n+        if isinstance(dilation_rate, (tuple, list)):\n+            dilation_rate = dilation_rate[0]\n+        kernel_size, ch_in, ch_out = kernel_weights.shape\n+\n+        if dilation_rate > 1:\n+            new_kernel_size = kernel_size + (dilation_rate - 1) * (\n+                kernel_size - 1\n+            )\n+            new_kernel_weights = np.zeros(\n+                (new_kernel_size, ch_in, ch_out), dtype=kernel_weights.dtype\n+            )\n+            new_kernel_weights[::dilation_rate] = kernel_weights\n+            kernel_weights = new_kernel_weights\n+            kernel_size = kernel_weights.shape[0]\n+\n+        if padding != \"valid\":\n+            n_batch, h_x, _ = x.shape\n+            h_pad = self._same_padding(h_x, kernel_size, h_stride)\n+            npad = [(0, 0)] * x.ndim\n+            if padding == \"causal\":\n+                npad[1] = (h_pad * 2, 0)\n+            else:\n+                npad[1] = (h_pad, h_pad)\n+            x = np.pad(x, pad_width=npad, mode=\"constant\", constant_values=0)\n+\n+        n_batch, h_x, _ = x.shape\n+        h_out = int((h_x - kernel_size) / h_stride) + 1\n+\n+        kernel_weights = kernel_weights.reshape(-1, ch_out)\n+        bias_weights = bias_weights.reshape(1, ch_out)\n+\n+        out_grps = []\n+        for grp in range(1, groups + 1):\n+            x_in = x[..., (grp - 1) * ch_in : grp * ch_in]\n+            stride_shape = (n_batch, h_out, kernel_size, ch_in)\n+            strides = (\n+                x_in.strides[0],\n+                h_stride * x_in.strides[1],\n+                x_in.strides[1],\n+                x_in.strides[2],\n+            )\n+            inner_dim = kernel_size * ch_in\n+            x_strided = as_strided(\n+                x_in, shape=stride_shape, strides=strides\n+            ).reshape(n_batch, h_out, inner_dim)\n+            ch_out_groups = ch_out // groups\n+            kernel_weights_grp = kernel_weights[\n+                ..., (grp - 1) * ch_out_groups : grp * ch_out_groups\n+            ]\n+            bias_weights_grp = bias_weights[\n+                ..., (grp - 1) * ch_out_groups : grp * ch_out_groups\n+            ]\n+            out_grps.append(x_strided @ kernel_weights_grp + bias_weights_grp)\n+        return np.concatenate(out_grps, axis=-1)\n+\n     @parameterized.parameters(\n         {\n             \"filters\": 5,\n@@ -302,31 +384,27 @@ class ConvCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             dilation_rate=dilation_rate,\n             groups=groups,\n         )\n-        tf_keras_layer = tf.keras.layers.Conv1D(\n-            filters=filters,\n-            kernel_size=kernel_size,\n+\n+        inputs = np.random.normal(size=[2, 8, 4])\n+        layer.build(input_shape=inputs.shape)\n+\n+        kernel_shape = layer.kernel.shape\n+        kernel_weights = np.random.normal(size=kernel_shape)\n+        bias_weights = np.random.normal(size=(filters,))\n+        layer.kernel.assign(kernel_weights)\n+        layer.bias.assign(bias_weights)\n+\n+        outputs = layer(inputs)\n+        expected = self._np_conv1d(\n+            inputs,\n+            kernel_weights,\n+            bias_weights,\n             strides=strides,\n             padding=padding,\n             data_format=data_format,\n             dilation_rate=dilation_rate,\n             groups=groups,\n         )\n-\n-        inputs = np.random.normal(size=[2, 8, 4])\n-        layer.build(input_shape=inputs.shape)\n-        tf_keras_layer.build(input_shape=inputs.shape)\n-\n-        kernel_shape = layer.kernel.shape\n-        kernel_weights = np.random.normal(size=kernel_shape)\n-        bias_weights = np.random.normal(size=(filters,))\n-        layer.kernel.assign(kernel_weights)\n-        tf_keras_layer.kernel.assign(kernel_weights)\n-\n-        layer.bias.assign(bias_weights)\n-        tf_keras_layer.bias.assign(bias_weights)\n-\n-        outputs = layer(inputs)\n-        expected = tf_keras_layer(inputs)\n         self.assertAllClose(outputs, expected)\n \n     @parameterized.parameters(\n\n@@ -198,7 +198,10 @@ class LayerNormalization(Layer):\n \n         # Calculate the mean and variance last axis (layer activations).\n         mean = ops.mean(inputs, axis=self.axis, keepdims=True)\n-        variance = ops.var(inputs, axis=self.axis, keepdims=True)\n+        # Don't use ops.var, as that would re-compute the mean from scratch.\n+        variance = ops.mean(\n+            ops.power(inputs - mean, 2), axis=self.axis, keepdims=True\n+        )\n \n         scale, offset = _broadcast(self.gamma), _broadcast(self.beta)\n \n\n@@ -85,3 +85,11 @@ class LayerNormalizationTest(testing.TestCase):\n \n         self.assertAllClose(ops.mean(out), 0.0, atol=1e-1)\n         self.assertAllClose(ops.std(out), 1.0, atol=1e-1)\n+\n+    def test_output(self):\n+        layer = layers.LayerNormalization(\n+            dtype=\"float32\", beta_initializer=\"ones\", gamma_initializer=\"ones\",\n+        )\n+        inputs = np.arange(5).astype(\"float32\")[None, :]\n+        out = layer(inputs)\n+        self.assertAllClose(out, [[-0.41386, 0.29307, 1., 1.70693, 2.41386]])\n\n@@ -7,6 +7,7 @@ from absl import logging\n \n from keras_core import backend\n from keras_core import optimizers\n+from keras_core.backend.common import global_state\n from keras_core.legacy.saving import json_utils\n from keras_core.legacy.saving import saving_options\n from keras_core.legacy.saving import saving_utils\n@@ -108,7 +109,8 @@ def load_model_from_hdf5(filepath, custom_objects=None, compile=True):\n         custom_objects = {}\n \n     gco = object_registration.GLOBAL_CUSTOM_OBJECTS\n-    custom_objects = {**custom_objects, **gco}\n+    tlco = global_state.get_global_attribute(\"custom_objects_scope_dict\", {})\n+    custom_objects = {**custom_objects, **gco, **tlco}\n \n     opened_new_file = not isinstance(filepath, h5py.File)\n     if opened_new_file:\n\n@@ -9,6 +9,8 @@ from keras_core import layers\n from keras_core import models\n from keras_core import testing\n from keras_core.legacy.saving import legacy_h5_format\n+from keras_core.saving import object_registration\n+from keras_core.saving import serialization_lib\n \n # TODO: more thorough testing. Correctness depends\n # on exact weight ordering for each layer, so we need\n@@ -122,6 +124,141 @@ class LegacyH5WholeModelTest(testing.TestCase):\n         ref_input = np.random.random((1, 3))\n         self._check_reloading_model(ref_input, model)\n \n+    def test_saving_lambda(self):\n+        mean = np.random.random((4, 2, 3))\n+        std = np.abs(np.random.random((4, 2, 3))) + 1e-5\n+        inputs = layers.Input(shape=(4, 2, 3))\n+        output = layers.Lambda(\n+            lambda image, mu, std: (image - mu) / std,\n+            arguments={\"mu\": mean, \"std\": std},\n+        )(inputs)\n+        model = models.Model(inputs, output)\n+        model.compile(loss=\"mse\", optimizer=\"sgd\", metrics=[\"acc\"])\n+\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"lambda_model.h5\")\n+        legacy_h5_format.save_model_to_hdf5(model, temp_filepath)\n+        loaded = legacy_h5_format.load_model_from_hdf5(temp_filepath)\n+\n+        self.assertAllClose(mean, loaded.layers[1].arguments[\"mu\"])\n+        self.assertAllClose(std, loaded.layers[1].arguments[\"std\"])\n+\n+    def test_saving_include_optimizer_false(self):\n+        model = models.Sequential()\n+        model.add(layers.Dense(1))\n+        model.compile(\"adam\", loss=\"mse\")\n+        x, y = np.ones((10, 10)), np.ones((10, 1))\n+        model.fit(x, y)\n+        ref_output = model(x)\n+\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"model.h5\")\n+        legacy_h5_format.save_model_to_hdf5(\n+            model, temp_filepath, include_optimizer=False\n+        )\n+        loaded = legacy_h5_format.load_model_from_hdf5(temp_filepath)\n+        output = loaded(x)\n+\n+        # Assert that optimizer does not exist in loaded model\n+        with self.assertRaises(AttributeError):\n+            _ = loaded.optimizer\n+\n+        # Compare output\n+        self.assertAllClose(ref_output, output, atol=1e-5)\n+\n+    def test_custom_sequential_registered_no_scope(self):\n+        @object_registration.register_keras_serializable(package=\"my_package\")\n+        class MyDense(layers.Dense):\n+            def __init__(self, units, **kwargs):\n+                super().__init__(units, **kwargs)\n+\n+        input_shape = [1]\n+        inputs = layers.Input(shape=input_shape)\n+        custom_layer = MyDense(1)\n+        model = models.Sequential(layers=[inputs, custom_layer])\n+\n+        ref_input = np.array([5])\n+        self._check_reloading_model(ref_input, model)\n+\n+    def test_custom_functional_registered_no_scope(self):\n+        @object_registration.register_keras_serializable(package=\"my_package\")\n+        class MyDense(layers.Dense):\n+            def __init__(self, units, **kwargs):\n+                super().__init__(units, **kwargs)\n+\n+        inputs = layers.Input(shape=[1])\n+        outputs = MyDense(1)(inputs)\n+        model = models.Model(inputs, outputs)\n+\n+        ref_input = np.array([5])\n+        self._check_reloading_model(ref_input, model)\n+\n+    def test_nested_layers(self):\n+        class MyLayer(layers.Layer):\n+            def __init__(self, sublayers, **kwargs):\n+                super().__init__(**kwargs)\n+                self.sublayers = sublayers\n+\n+            def call(self, x):\n+                prev_input = x\n+                for layer in self.sublayers:\n+                    prev_input = layer(prev_input)\n+                return prev_input\n+\n+            def get_config(self):\n+                config = super().get_config()\n+                config[\"sublayers\"] = serialization_lib.serialize_keras_object(\n+                    self.sublayers\n+                )\n+                return config\n+\n+            @classmethod\n+            def from_config(cls, config):\n+                config[\n+                    \"sublayers\"\n+                ] = serialization_lib.deserialize_keras_object(\n+                    config[\"sublayers\"]\n+                )\n+                return cls(**config)\n+\n+        @object_registration.register_keras_serializable(package=\"Foo\")\n+        class RegisteredSubLayer(layers.Layer):\n+            pass\n+\n+        layer = MyLayer(\n+            [\n+                layers.Dense(2, name=\"MyDense\"),\n+                RegisteredSubLayer(name=\"MySubLayer\"),\n+            ]\n+        )\n+        model = models.Sequential([layer])\n+        with self.subTest(\"test_JSON\"):\n+            from keras_core.models.model import model_from_json\n+\n+            model_json = model.to_json()\n+            self.assertIn(\"Foo>RegisteredSubLayer\", model_json)\n+\n+            loaded_model = model_from_json(\n+                model_json, custom_objects={\"MyLayer\": MyLayer}\n+            )\n+            loaded_layer = loaded_model.layers[0]\n+\n+            self.assertIsInstance(loaded_layer.sublayers[0], layers.Dense)\n+            self.assertEqual(loaded_layer.sublayers[0].name, \"MyDense\")\n+            self.assertIsInstance(loaded_layer.sublayers[1], RegisteredSubLayer)\n+            self.assertEqual(loaded_layer.sublayers[1].name, \"MySubLayer\")\n+\n+        with self.subTest(\"test_H5\"):\n+            temp_filepath = os.path.join(self.get_temp_dir(), \"model.h5\")\n+            legacy_h5_format.save_model_to_hdf5(model, temp_filepath)\n+            loaded_model = legacy_h5_format.load_model_from_hdf5(\n+                temp_filepath, custom_objects={\"MyLayer\": MyLayer}\n+            )\n+            loaded_layer = loaded_model.layers[0]\n+\n+            self.assertIsInstance(loaded_layer.sublayers[0], layers.Dense)\n+            self.assertEqual(loaded_layer.sublayers[0].name, \"MyDense\")\n+            self.assertIsInstance(loaded_layer.sublayers[1], RegisteredSubLayer)\n+            self.assertEqual(loaded_layer.sublayers[1].name, \"MySubLayer\")\n+\n \n @pytest.mark.requires_trainable_backend\n class LegacyH5BackwardsCompatTest(testing.TestCase):\n\n@@ -402,6 +402,10 @@ def unstack(x, num=None, axis=0):\n def shape(x):\n     \"\"\"Gets the shape of the tensor input.\n \n+    Note: On the tensorflow backend, when `x` is a `tf.Tensor` with dynamic\n+    shape, dimensions which are dynamic in the context of a compiled function\n+    will have a `tf.Tensor` value instead of a static integer value.\n+\n     Args:\n         x: A tensor. This function will try to access the `shape` attribute of\n             the input tensor.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
