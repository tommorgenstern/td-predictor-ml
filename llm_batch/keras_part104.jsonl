{"custom_id": "keras#2da66ed00943d0c5682b694828cd6f5469e10529", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 2209 | Contributors (this commit): 24 | Commits (past 90d): 31 | Contributors (cumulative): 24 | DMM Complexity: None\n\nDIFF:\n@@ -201,7 +201,7 @@ def eval(x):\n def zeros(shape, dtype=_FLOATX, name=None):\n     '''Instantiates an all-zeros tensor variable.\n     '''\n-    shape = map(int, shape)\n+    shape = tuple(map(int, shape))\n     tf_dtype = _convert_string_dtype(dtype)\n     return variable(tf.constant_initializer(0., dtype=tf_dtype)(shape), dtype, name)\n \n@@ -209,7 +209,7 @@ def zeros(shape, dtype=_FLOATX, name=None):\n def ones(shape, dtype=_FLOATX, name=None):\n     '''Instantiates an all-ones tensor variable.\n     '''\n-    shape = map(int, shape)\n+    shape = tuple(map(int, shape))\n     tf_dtype = _convert_string_dtype(dtype)\n     return variable(tf.constant_initializer(1., dtype=tf_dtype)(shape), dtype, name)\n \n@@ -235,14 +235,14 @@ def ones_like(x, name=None):\n \n \n def random_uniform_variable(shape, low, high, dtype=_FLOATX, name=None):\n-    shape = map(int, shape)\n+    shape = tuple(map(int, shape))\n     tf_dtype = _convert_string_dtype(dtype)\n     value = tf.random_uniform_initializer(low, high, dtype=tf_dtype)(shape)\n     return variable(value, dtype=dtype, name=name)\n \n \n def random_normal_variable(shape, mean, scale, dtype=_FLOATX, name=None):\n-    shape = map(int, shape)\n+    shape = tuple(map(int, shape))\n     tf_dtype = _convert_string_dtype(dtype)\n     value = tf.random_normal_initializer(mean, scale, dtype=tf_dtype)(shape)\n     return variable(value, dtype=dtype, name=name)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#efa5b04797ae0e881eef2d4c72f618fae066dd12", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 2212 | Contributors (this commit): 24 | Commits (past 90d): 32 | Contributors (cumulative): 24 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,4 @@\n import tensorflow as tf\n-from tensorflow.python.framework.ops import reset_default_graph\n import numpy as np\n import os\n import copy\n@@ -16,7 +15,7 @@ _MANUAL_VAR_INIT = False\n def clear_session():\n     global _SESSION\n     global _LEARNING_PHASE\n-    reset_default_graph()\n+    tf.reset_default_graph()\n     reset_uids()\n     _SESSION = None\n     _LEARNING_PHASE = tf.placeholder(dtype='uint8', name='keras_learning_phase')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3c8f91ee3d32222633c125dd97b669dbf740a1eb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2214 | Contributors (this commit): 24 | Commits (past 90d): 33 | Contributors (cumulative): 24 | DMM Complexity: None\n\nDIFF:\n@@ -99,7 +99,7 @@ def _convert_string_dtype(dtype):\n \n \n def _to_tensor(x, dtype):\n-    x = tf.python.framework.ops.convert_to_tensor(x)\n+    x = tf.convert_to_tensor(x)\n     if x.dtype != dtype:\n         x = tf.cast(x, dtype)\n     return x\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0a3939883ad60db9a9ab85d7602cc3b471612799", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 2217 | Contributors (this commit): 24 | Commits (past 90d): 34 | Contributors (cumulative): 24 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,5 @@\n import tensorflow as tf\n+from tensorflow.python.training import moving_averages\n import numpy as np\n import os\n import copy\n@@ -276,7 +277,7 @@ def update_sub(x, decrement):\n \n \n def moving_average_update(variable, value, momentum):\n-    return tf.python.training.moving_averages.assign_moving_average(\n+    return moving_averages.assign_moving_average(\n         variable, value, momentum)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#23e1ad2df7c81bcf89b20225ccf90f815d5e978a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 14 | Files Changed: 3 | Hunks: 19 | Methods Changed: 17 | Complexity Δ (Sum/Max): 22/11 | Churn Δ: 56 | Churn Cumulative: 6236 | Contributors (this commit): 48 | Commits (past 90d): 80 | Contributors (cumulative): 70 | DMM Complexity: 0.2\n\nDIFF:\n@@ -44,7 +44,10 @@ def learning_phase():\n \n def set_learning_phase(value):\n     global _LEARNING_PHASE\n-    _LEARNING_PHASE = tf.constant(value, name='keras_learning_phase')\n+    if value not in {0, 1}:\n+        raise ValueError('Expected learning phase to be '\n+                         '0 or 1.')\n+    _LEARNING_PHASE = value\n \n \n def get_session():\n@@ -819,7 +822,7 @@ class Function(object):\n \n     def __call__(self, inputs):\n         assert type(inputs) in {list, tuple}\n-        names = [v.name for v in self.inputs]\n+        names = [getattr(v, 'name', None) for v in self.inputs]\n         feed_dict = dict(zip(names, inputs))\n         session = get_session()\n         updated = session.run(self.outputs + [self.updates_op], feed_dict=feed_dict)\n@@ -989,6 +992,11 @@ def in_train_phase(x, alt):\n     '''Selects `x` in train phase, and `alt` otherwise.\n     Note that `alt` should have the *same shape* as `x`.\n     '''\n+    if _LEARNING_PHASE is 1:\n+        return x\n+    elif _LEARNING_PHASE is 0:\n+        return alt\n+    # else: assume learning phase is a placeholder.\n     x_shape = copy.copy(x.get_shape())\n     x = tf.python.control_flow_ops.cond(tf.cast(_LEARNING_PHASE, 'bool'),\n                                         lambda: x,\n@@ -1002,6 +1010,10 @@ def in_test_phase(x, alt):\n     '''Selects `x` in test phase, and `alt` otherwise.\n     Note that `alt` should have the *same shape* as `x`.\n     '''\n+    if _LEARNING_PHASE is 1:\n+        return alt\n+    elif _LEARNING_PHASE is 0:\n+        return x\n     x_shape = copy.copy(x.get_shape())\n     x = tf.python.control_flow_ops.cond(tf.cast(_LEARNING_PHASE, 'bool'),\n                                         lambda: alt,\n\n@@ -22,6 +22,14 @@ def learning_phase():\n     return _LEARNING_PHASE\n \n \n+def set_learning_phase(value):\n+    global _LEARNING_PHASE\n+    if value not in {0, 1}:\n+        raise ValueError('Expected learning phase to be '\n+                         '0 or 1.')\n+    _LEARNING_PHASE = value\n+\n+\n # VARIABLE MANIPULATION\n \n def variable(value, dtype=_FLOATX, name=None):\n@@ -602,7 +610,7 @@ class Function(object):\n     def __init__(self, inputs, outputs, updates=[], **kwargs):\n         self.function = theano.function(inputs, outputs, updates=updates,\n                                         allow_input_downcast=True,\n-                                        on_unused_input='warn',\n+                                        on_unused_input='ignore',\n                                         **kwargs)\n \n     def __call__(self, inputs):\n@@ -803,12 +811,20 @@ def switch(condition, then_expression, else_expression):\n \n \n def in_train_phase(x, alt):\n+    if _LEARNING_PHASE is 1:\n+        return x\n+    elif _LEARNING_PHASE is 0:\n+        return alt\n     x = T.switch(_LEARNING_PHASE, x, alt)\n     x._uses_learning_phase = True\n     return x\n \n \n def in_test_phase(x, alt):\n+    if _LEARNING_PHASE is 1:\n+        return alt\n+    elif _LEARNING_PHASE is 0:\n+        return x\n     x = T.switch(_LEARNING_PHASE, alt, x)\n     x._uses_learning_phase = True\n     return x\n\n@@ -686,7 +686,7 @@ class Model(Container):\n         if not hasattr(self, 'train_function'):\n             raise Exception('You must compile your model before using it.')\n         if self.train_function is None:\n-            if self.uses_learning_phase:\n+            if self.uses_learning_phase and type(K.learning_phase()) is not int:\n                 inputs = self.inputs + self.targets + self.sample_weights + [K.learning_phase()]\n             else:\n                 inputs = self.inputs + self.targets + self.sample_weights\n@@ -706,7 +706,7 @@ class Model(Container):\n         if not hasattr(self, 'test_function'):\n             raise Exception('You must compile your model before using it.')\n         if self.test_function is None:\n-            if self.uses_learning_phase:\n+            if self.uses_learning_phase and type(K.learning_phase()) is not int:\n                 inputs = self.inputs + self.targets + self.sample_weights + [K.learning_phase()]\n             else:\n                 inputs = self.inputs + self.targets + self.sample_weights\n@@ -721,7 +721,7 @@ class Model(Container):\n         if not hasattr(self, 'predict_function'):\n             self.predict_function = None\n         if self.predict_function is None:\n-            if self.uses_learning_phase:\n+            if self.uses_learning_phase and type(K.learning_phase()) is not int:\n                 inputs = self.inputs + [K.learning_phase()]\n             else:\n                 inputs = self.inputs\n@@ -1048,7 +1048,7 @@ class Model(Container):\n                                                                            batch_size=batch_size)\n             self._make_test_function()\n             val_f = self.test_function\n-            if self.uses_learning_phase:\n+            if self.uses_learning_phase and type(K.learning_phase()) is not int:\n                 val_ins = val_x + val_y + val_sample_weights + [0.]\n             else:\n                 val_ins = val_x + val_y + val_sample_weights\n@@ -1062,7 +1062,7 @@ class Model(Container):\n                 slice_X(sample_weights, 0, split_at), slice_X(sample_weights, split_at))\n             self._make_test_function()\n             val_f = self.test_function\n-            if self.uses_learning_phase:\n+            if self.uses_learning_phase and type(K.learning_phase()) is not int:\n                 val_ins = val_x + val_y + val_sample_weights + [0.]\n             else:\n                 val_ins = val_x + val_y + val_sample_weights\n@@ -1072,7 +1072,7 @@ class Model(Container):\n             val_ins = None\n \n         # prepare input arrays and training function\n-        if self.uses_learning_phase:\n+        if self.uses_learning_phase and type(K.learning_phase()) is not int:\n             ins = x + y + sample_weights + [1.]\n         else:\n             ins = x + y + sample_weights\n@@ -1132,7 +1132,7 @@ class Model(Container):\n                                                            check_batch_dim=False,\n                                                            batch_size=batch_size)\n         # prepare inputs, delegate logic to _test_loop\n-        if self.uses_learning_phase:\n+        if self.uses_learning_phase and type(K.learning_phase()) is not int:\n             ins = x + y + sample_weights + [0.]\n         else:\n             ins = x + y + sample_weights\n@@ -1169,7 +1169,7 @@ class Model(Container):\n                                 'Batch size: ' + str(batch_size) + '.')\n \n         # prepare inputs, delegate logic to _predict_loop\n-        if self.uses_learning_phase:\n+        if self.uses_learning_phase and type(K.learning_phase()) is not int:\n             ins = x + [0.]\n         else:\n             ins = x\n@@ -1213,7 +1213,7 @@ class Model(Container):\n                                                            sample_weight=sample_weight,\n                                                            class_weight=class_weight,\n                                                            check_batch_dim=True)\n-        if self.uses_learning_phase:\n+        if self.uses_learning_phase and type(K.learning_phase()) is not int:\n             ins = x + y + sample_weights + [1.]\n         else:\n             ins = x + y + sample_weights\n@@ -1251,7 +1251,7 @@ class Model(Container):\n         x, y, sample_weights = self._standardize_user_data(x, y,\n                                                            sample_weight=sample_weight,\n                                                            check_batch_dim=True)\n-        if self.uses_learning_phase:\n+        if self.uses_learning_phase and type(K.learning_phase()) is not int:\n             ins = x + y + sample_weights + [0.]\n         else:\n             ins = x + y + sample_weights\n@@ -1266,7 +1266,7 @@ class Model(Container):\n         '''\n         x = standardize_input_data(x, self.input_names,\n                                    self.internal_input_shapes)\n-        if self.uses_learning_phase:\n+        if self.uses_learning_phase and type(K.learning_phase()) is not int:\n             ins = x + [0.]\n         else:\n             ins = x\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b7edcf6eeaaccea61cec23c10f3b99f81c0f8bc1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 23 | Churn Cumulative: 585 | Contributors (this commit): 8 | Commits (past 90d): 5 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -43,15 +43,15 @@ class EigenvalueRegularizer(Regularizer):\n \n         # power method for approximating the dominant eigenvector:\n         o = K.ones([dim1, 1])  # initial values for the dominant eigenvector\n-        domin_eigenvect = K.dot(WW, o)\n+        main_eigenvect = K.dot(WW, o)\n         for n in range(power - 1):\n-            domin_eigenvect = K.dot(WW, domin_eigenvect)\n+            main_eigenvect = K.dot(WW, main_eigenvect)\n \n-        WWd = K.dot(WW, domin_eigenvect)\n+        WWd = K.dot(WW, main_eigenvect)\n \n         # the corresponding dominant eigenvalue:\n-        domin_eigenval = K.dot(K.transpose(WWd), domin_eigenvect) / K.dot(K.transpose(domin_eigenvect), domin_eigenvect)\n-        regularized_loss = loss + (domin_eigenval ** 0.5) * self.k  # multiplied by the given regularization gain\n+        main_eigenval = K.dot(K.transpose(WWd), main_eigenvect) / K.dot(K.transpose(main_eigenvect), main_eigenvect)\n+        regularized_loss = loss + (main_eigenval ** 0.5) * self.k  # multiplied by the given regularization gain\n \n         return K.in_train_phase(regularized_loss[0, 0], loss)\n \n@@ -75,8 +75,11 @@ class WeightRegularizer(Regularizer):\n                             'ActivityRegularizer '\n                             '(i.e. activity_regularizer=\"l2\" instead '\n                             'of activity_regularizer=\"activity_l2\".')\n-        regularized_loss = loss + K.sum(K.abs(self.p)) * self.l1\n-        regularized_loss += K.sum(K.square(self.p)) * self.l2\n+        regularized_loss = loss\n+        if self.l1:\n+            regularized_loss += K.mean(K.abs(self.p)) * self.l1\n+        if self.l2:\n+            regularized_loss += K.mean(K.square(self.p)) * self.l2\n         return K.in_train_phase(regularized_loss, loss)\n \n     def get_config(self):\n@@ -102,8 +105,10 @@ class ActivityRegularizer(Regularizer):\n         regularized_loss = loss\n         for i in range(len(self.layer.inbound_nodes)):\n             output = self.layer.get_output_at(i)\n-            regularized_loss += self.l1 * K.sum(K.mean(K.abs(output), axis=0))\n-            regularized_loss += self.l2 * K.sum(K.mean(K.square(output), axis=0))\n+            if self.l1:\n+                regularized_loss += self.l1 * K.mean(K.abs(output))\n+            if self.l2:\n+                regularized_loss += self.l2 * K.mean(K.square(output))\n         return K.in_train_phase(regularized_loss, loss)\n \n     def get_config(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c034262b7857a0738339b1747b052e51ae60dce0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 74 | Files Changed: 1 | Hunks: 5 | Methods Changed: 6 | Complexity Δ (Sum/Max): -8/0 | Churn Δ: 74 | Churn Cumulative: 443 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -61,15 +61,6 @@ def create_sequential_model():\n     return model\n \n \n-def create_graph_model():\n-    model = Graph()\n-    model.add_input(name='input', input_shape=(input_dim,))\n-    model.add_node(Dense(32, activation='relu'), name='d1', input='input')\n-    model.add_node(Dense(nb_classes, activation='softmax'), name='d2', input='d1')\n-    model.add_output(name='output', input='d2')\n-    return model\n-\n-\n def create_temporal_sequential_model():\n     model = Sequential()\n     model.add(GRU(32, input_shape=(timesteps, input_dim), return_sequences=True))\n@@ -78,17 +69,6 @@ def create_temporal_sequential_model():\n     return model\n \n \n-def create_temporal_graph_model():\n-    model = Graph()\n-    model.add_input(name='input', input_shape=(timesteps, input_dim))\n-    model.add_node(GRU(32, return_sequences=True),\n-                   name='d1', input='input')\n-    model.add_node(TimeDistributedDense(nb_classes, activation='softmax'),\n-                   name='d2', input='d1')\n-    model.add_output(name='output', input='d2')\n-    return model\n-\n-\n def _test_weights_sequential(model, class_weight=None, sample_weight=None,\n                              X_train=X_train, Y_train=Y_train,\n                              X_test=X_test, Y_test=Y_test):\n@@ -122,38 +102,11 @@ def _test_weights_sequential(model, class_weight=None, sample_weight=None,\n     return score\n \n \n-def _test_weights_graph(model, class_weight=None, sample_weight=None,\n-                        X_train=X_train, Y_train=Y_train,\n-                        X_test=X_test, Y_test=Y_test):\n-    model.fit({'input': X_train, 'output': Y_train},\n-              batch_size=batch_size, nb_epoch=nb_epoch // 2, verbose=0,\n-              class_weight={'output': class_weight},\n-              sample_weight={'output': sample_weight})\n-    model.fit({'input': X_train, 'output': Y_train},\n-              batch_size=batch_size, nb_epoch=nb_epoch // 2, verbose=0,\n-              class_weight={'output': class_weight},\n-              sample_weight={'output': sample_weight}, validation_split=0.1)\n-\n-    model.train_on_batch({'input': X_train[:32], 'output': Y_train[:32]},\n-                         class_weight={'output': class_weight},\n-                         sample_weight={'output': sample_weight[:32] if sample_weight is not None else None})\n-    model.test_on_batch({'input': X_train[:32], 'output': Y_train[:32]},\n-                        sample_weight={'output': sample_weight[:32] if sample_weight is not None else None})\n-    score = model.evaluate({'input': X_test[test_ids, :],\n-                            'output': Y_test[test_ids, :]},\n-                           verbose=0)\n-    return score\n-\n-\n # no weights: reference point\n model = create_sequential_model()\n model.compile(loss=loss, optimizer='rmsprop')\n standard_score_sequential = _test_weights_sequential(model)\n \n-model = create_graph_model()\n-model.compile(loss={'output': loss}, optimizer='rmsprop')\n-standard_score_graph = _test_weights_graph(model)\n-\n \n def test_sequential_class_weights():\n     model = create_sequential_model()\n@@ -194,32 +147,5 @@ def test_sequential_temporal_sample_weights():\n     assert(score < standard_score_sequential)\n \n \n-def test_graph_class_weights():\n-    model = create_graph_model()\n-    model.compile(loss={'output': loss}, optimizer='rmsprop')\n-    score = _test_weights_graph(model, class_weight=class_weight)\n-    assert(score < standard_score_graph)\n-\n-\n-def test_graph_sample_weights():\n-    model = create_graph_model()\n-    model.compile(loss={'output': loss}, optimizer='rmsprop')\n-    score = _test_weights_graph(model, sample_weight=sample_weight)\n-    assert(score < standard_score_graph)\n-\n-\n-def test_graph_temporal_sample_weight():\n-    model = create_temporal_graph_model()\n-    model.compile(loss={'output': loss}, optimizer='rmsprop',\n-                  sample_weight_modes={'output': 'temporal'})\n-    score = _test_weights_graph(model,\n-                                sample_weight=temporal_sample_weight,\n-                                X_train=temporal_X_train,\n-                                X_test=temporal_X_test,\n-                                Y_train=temporal_Y_train,\n-                                Y_test=temporal_Y_test)\n-    assert(score < standard_score_graph)\n-\n-\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2788b60fe67bbcfd243cd0915cf249c568984996", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 593 | Contributors (this commit): 8 | Commits (past 90d): 6 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -77,9 +77,9 @@ class WeightRegularizer(Regularizer):\n                             'of activity_regularizer=\"activity_l2\".')\n         regularized_loss = loss\n         if self.l1:\n-            regularized_loss += K.mean(K.abs(self.p)) * self.l1\n+            regularized_loss += K.sum(self.l1 * K.abs(self.p))\n         if self.l2:\n-            regularized_loss += K.mean(K.square(self.p)) * self.l2\n+            regularized_loss += K.sum(self.l2 * K.square(self.p))\n         return K.in_train_phase(regularized_loss, loss)\n \n     def get_config(self):\n@@ -106,9 +106,9 @@ class ActivityRegularizer(Regularizer):\n         for i in range(len(self.layer.inbound_nodes)):\n             output = self.layer.get_output_at(i)\n             if self.l1:\n-                regularized_loss += self.l1 * K.mean(K.abs(output))\n+                regularized_loss += K.sum(self.l1 * K.abs(output))\n             if self.l2:\n-                regularized_loss += self.l2 * K.mean(K.square(output))\n+                regularized_loss += K.sum(self.l2 * K.square(output))\n         return K.in_train_phase(regularized_loss, loss)\n \n     def get_config(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#defa1283c44653c29b81403f9d9f19fb3df0a27d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 31 | Files Changed: 1 | Hunks: 16 | Methods Changed: 3 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 66 | Churn Cumulative: 1389 | Contributors (this commit): 14 | Commits (past 90d): 9 | Contributors (cumulative): 14 | DMM Complexity: 0.0\n\nDIFF:\n@@ -72,26 +72,22 @@ class Optimizer(object):\n                 output of `get_weights`).\n         '''\n         params = self.weights\n-        if len(params) != len(weights):\n-            raise Exception('Provided weight array does not match  weights (' +\n-                            str(len(params)) + ' optimizer params vs. ' +\n-                            str(len(weights)) + ' provided weights)')\n-        for p, w in zip(params, weights):\n-            if K.get_value(p).shape != w.shape:\n+        weight_value_tuples = []\n+        param_values = K.batch_get_value(params)\n+        for pv, p, w in zip(param_values, params, weights):\n+            if pv.shape != w.shape:\n                 raise Exception('Optimizer weight shape ' +\n-                                str(K.get_value(p).shape) +\n+                                str(pv.shape) +\n                                 ' not compatible with '\n                                 'provided weight shape ' + str(w.shape))\n-            K.set_value(p, w)\n+            weight_value_tuples.append((p, w))\n+        K.batch_set_value(weight_value_tuples)\n \n     def get_weights(self):\n         '''Returns the current weights of the optimizer,\n         as a list of numpy arrays.\n         '''\n-        weights = []\n-        for p in self.weights:\n-            weights.append(K.get_value(p))\n-        return weights\n+        return K.batch_get_value(self.weights)\n \n     def get_config(self):\n         config = {'name': self.__class__.__name__}\n@@ -127,8 +123,10 @@ class SGD(Optimizer):\n         self.updates = [K.update_add(self.iterations, 1)]\n \n         # momentum\n-        self.weights = [K.variable(np.zeros(K.get_value(p).shape)) for p in params]\n-        for p, g, m in zip(params, grads, self.weights):\n+        shapes = [x.shape for x in K.batch_get_value(params)]\n+        moments = [K.zeros(shape) for shape in shapes]\n+        self.weights = [self.iterations] + moments\n+        for p, g, m in zip(params, grads, moments):\n             v = self.momentum * m - lr * g  # velocity\n             self.updates.append(K.update(m, v))\n \n@@ -177,11 +175,12 @@ class RMSprop(Optimizer):\n \n     def get_updates(self, params, constraints, loss):\n         grads = self.get_gradients(loss, params)\n-        # accumulators\n-        self.weights = [K.variable(np.zeros(K.get_value(p).shape)) for p in params]\n+        shapes = [x.shape for x in K.batch_get_value(params)]\n+        accumulators = [K.zeros(shape) for shape in shapes]\n+        self.weights = accumulators\n         self.updates = []\n \n-        for p, g, a in zip(params, grads, self.weights):\n+        for p, g, a in zip(params, grads, accumulators):\n             # update accumulator\n             new_a = self.rho * a + (1. - self.rho) * K.square(g)\n             self.updates.append(K.update(a, new_a))\n@@ -219,11 +218,12 @@ class Adagrad(Optimizer):\n \n     def get_updates(self, params, constraints, loss):\n         grads = self.get_gradients(loss, params)\n-        # accumulators\n-        self.weights = [K.variable(np.zeros(K.get_value(p).shape)) for p in params]\n+        shapes = [x.shape for x in K.batch_get_value(params)]\n+        accumulators = [K.zeros(shape) for shape in shapes]\n+        self.weights = accumulators\n         self.updates = []\n \n-        for p, g, a in zip(params, grads, self.weights):\n+        for p, g, a in zip(params, grads, accumulators):\n             new_a = a + K.square(g)  # update accumulator\n             self.updates.append(K.update(a, new_a))\n             new_p = p - self.lr * g / (K.sqrt(new_a) + self.epsilon)\n@@ -263,8 +263,9 @@ class Adadelta(Optimizer):\n \n     def get_updates(self, params, constraints, loss):\n         grads = self.get_gradients(loss, params)\n-        accumulators = [K.variable(np.zeros(K.get_value(p).shape)) for p in params]\n-        delta_accumulators = [K.variable(np.zeros(K.get_value(p).shape)) for p in params]\n+        shapes = [x.shape for x in K.batch_get_value(params)]\n+        accumulators = [K.zeros(shape) for shape in shapes]\n+        delta_accumulators = [K.zeros(shape) for shape in shapes]\n         self.weights = accumulators + delta_accumulators\n         self.updates = []\n \n@@ -325,9 +326,10 @@ class Adam(Optimizer):\n         t = self.iterations + 1\n         lr_t = self.lr * K.sqrt(1. - K.pow(self.beta_2, t)) / (1. - K.pow(self.beta_1, t))\n \n-        ms = [K.variable(np.zeros(K.get_value(p).shape)) for p in params]\n-        vs = [K.variable(np.zeros(K.get_value(p).shape)) for p in params]\n-        self.weights = ms + vs\n+        shapes = [x.shape for x in K.batch_get_value(params)]\n+        ms = [K.zeros(shape) for shape in shapes]\n+        vs = [K.zeros(shape) for shape in shapes]\n+        self.weights = [self.iterations] + ms + vs\n \n         for p, g, m, v in zip(params, grads, ms, vs):\n             m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n@@ -384,11 +386,12 @@ class Adamax(Optimizer):\n         t = self.iterations + 1\n         lr_t = self.lr / (1. - K.pow(self.beta_1, t))\n \n+        shapes = [x.shape for x in K.batch_get_value(params)]\n         # zero init of 1st moment\n-        ms = [K.variable(np.zeros(K.get_value(p).shape)) for p in params]\n+        ms = [K.zeros(shape) for shape in shapes]\n         # zero init of exponentially weighted infinity norm\n-        us = [K.variable(np.zeros(K.get_value(p).shape)) for p in params]\n-        self.weights = ms + us\n+        us = [K.zeros(shape) for shape in shapes]\n+        self.weights = [self.iterations] + ms + us\n \n         for p, g, m, u in zip(params, grads, ms, us):\n \n@@ -459,10 +462,11 @@ class Nadam(Optimizer):\n         m_schedule_next = self.m_schedule * momentum_cache_t * momentum_cache_t_1\n         self.updates.append((self.m_schedule, m_schedule_new))\n \n-        ms = [K.variable(np.zeros(K.get_value(p).shape)) for p in params]\n-        vs = [K.variable(np.zeros(K.get_value(p).shape)) for p in params]\n+        shapes = [x.shape for x in K.batch_get_value(params)]\n+        ms = [K.zeros(shape) for shape in shapes]\n+        vs = [K.zeros(shape) for shape in shapes]\n \n-        self.weights = ms + vs\n+        self.weights = [self.iterations] + ms + vs\n \n         for p, g, m, v in zip(params, grads, ms, vs):\n             # the following equations given in [1]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f9ef72c38af0e8452fef91343e2ae6a444122c4b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 49 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 9 | Complexity Δ (Sum/Max): 8/4 | Churn Δ: 49 | Churn Cumulative: 4977 | Contributors (this commit): 36 | Commits (past 90d): 64 | Contributors (cumulative): 66 | DMM Complexity: 1.0\n\nDIFF:\n@@ -562,6 +562,34 @@ def not_equal(x, y):\n     return tf.not_equal(x, y)\n \n \n+def greater(x, y):\n+    '''Element-wise truth value of (x > y).\n+    Returns a bool tensor.\n+    '''\n+    return tf.greater(x, y)\n+\n+\n+def greater_equal(x, y):\n+    '''Element-wise truth value of (x >= y).\n+    Returns a bool tensor.\n+    '''\n+    return tf.greater_equal(x, y)\n+\n+\n+def lesser(x, y):\n+    '''Element-wise truth value of (x < y).\n+    Returns a bool tensor.\n+    '''\n+    return tf.less(x, y)\n+\n+\n+def lesser_equal(x, y):\n+    '''Element-wise truth value of (x <= y).\n+    Returns a bool tensor.\n+    '''\n+    return tf.less_equal(x, y)\n+\n+\n def maximum(x, y):\n     '''Element-wise maximum of two tensors.\n     '''\n\n@@ -325,6 +325,22 @@ def not_equal(x, y):\n     return T.neq(x, y)\n \n \n+def greater(x, y):\n+    return T.gt(x, y)\n+\n+\n+def greater_equal(x, y):\n+    return T.ge(x, y)\n+\n+\n+def lesser(x, y):\n+    return T.lt(x, y)\n+\n+\n+def lesser_equal(x, y):\n+    return T.le(x, y)\n+\n+\n def maximum(x, y):\n     return T.maximum(x, y)\n \n\n@@ -198,6 +198,11 @@ class TestBackend(object):\n \n         # two-tensor ops\n         check_two_tensor_operation('equal', (4, 2), (4, 2))\n+        check_two_tensor_operation('not_equal', (4, 2), (4, 2))\n+        check_two_tensor_operation('greater', (4, 2), (4, 2))\n+        check_two_tensor_operation('greater_equal', (4, 2), (4, 2))\n+        check_two_tensor_operation('lesser', (4, 2), (4, 2))\n+        check_two_tensor_operation('lesser_equal', (4, 2), (4, 2))\n         check_two_tensor_operation('maximum', (4, 2), (4, 2))\n         check_two_tensor_operation('minimum', (4, 2), (4, 2))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3aab55d29fc3981f7574dd243b2ec7e09ce295ae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 47 | Lines Deleted: 12 | Files Changed: 3 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/5 | Churn Δ: 59 | Churn Cumulative: 7697 | Contributors (this commit): 38 | Commits (past 90d): 54 | Contributors (cumulative): 52 | DMM Complexity: 1.0\n\nDIFF:\n@@ -706,6 +706,27 @@ def resize_images(X, height_factor, width_factor, dim_ordering):\n         raise Exception('Invalid dim_ordering: ' + dim_ordering)\n \n \n+def resize_volumes(X, depth_factor, height_factor, width_factor, dim_ordering):\n+    '''Resize the volume contained in a 5D tensor of shape\n+    - [batch, channels, depth, height, width] (for 'th' dim_ordering)\n+    - [batch, depth, height, width, channels] (for 'tf' dim_ordering)\n+    by a factor of (depth_factor, height_factor, width_factor).\n+    All three factors should be positive integers.\n+    '''\n+    if dim_ordering == 'th':\n+        output = repeat_elements(X, depth_factor, axis=2)\n+        output = repeat_elements(output, height_factor, axis=3)\n+        output = repeat_elements(output, width_factor, axis=4)\n+        return output\n+    elif dim_ordering == 'tf':\n+        output = repeat_elements(X, depth_factor, axis=1)\n+        output = repeat_elements(output, height_factor, axis=2)\n+        output = repeat_elements(output, width_factor, axis=3)\n+        return output\n+    else:\n+        raise Exception('Invalid dim_ordering: ' + dim_ordering)\n+\n+\n def repeat_elements(x, rep, axis):\n     '''Repeats the elements of a tensor along an axis, like np.repeat\n \n@@ -784,6 +805,32 @@ def spatial_2d_padding(x, padding=(1, 1), dim_ordering='th'):\n     return tf.pad(x, pattern)\n \n \n+def spatial_3d_padding(x, padding=(1, 1, 1), dim_ordering='th'):\n+    '''Pads 5D tensor with zeros for the depth, height, width dimension with\n+    \"padding[0]\", \"padding[1]\" and \"padding[2]\" (resp.) zeros left and right\n+\n+    For 'tf' dim_ordering, the 2nd, 3rd and 4th dimension will be padded.\n+    For 'th' dim_ordering, the 3rd, 4th and 5th dimension will be padded.\n+    '''\n+    if dim_ordering == 'th':\n+        pattern = [\n+            [0, 0],\n+            [0, 0],\n+            [padding[0], padding[0]],\n+            [padding[1], padding[1]],\n+            [padding[2], padding[2]]\n+        ]\n+    else:\n+        pattern = [\n+            [0, 0],\n+            [padding[0], padding[0]],\n+            [padding[1], padding[1]],\n+            [padding[2], padding[2]],\n+            [0, 0]\n+        ]\n+    return tf.pad(x, pattern)\n+\n+\n def pack(x):\n     return tf.pack(x)\n \n\n@@ -1036,8 +1036,6 @@ class UpSampling3D(Layer):\n     '''Repeat the first, second and third dimension of the data\n     by size[0], size[1] and size[2] respectively.\n \n-    Note: this layer will only work with Theano for the time being.\n-\n     # Arguments\n         size: tuple of 3 integers. The upsampling factors for dim1, dim2 and dim3.\n         dim_ordering: 'th' or 'tf'.\n@@ -1061,9 +1059,6 @@ class UpSampling3D(Layer):\n     '''\n \n     def __init__(self, size=(2, 2, 2), dim_ordering=K.image_dim_ordering(), **kwargs):\n-        if K._BACKEND != 'theano':\n-            raise Exception(self.__class__.__name__ +\n-                            ' is currently only working with Theano backend.')\n         self.size = tuple(size)\n         assert dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf, th}'\n         self.dim_ordering = dim_ordering\n@@ -1192,8 +1187,6 @@ class ZeroPadding2D(Layer):\n class ZeroPadding3D(Layer):\n     '''Zero-padding layer for 3D data (spatial or spatio-temporal).\n \n-    Note: this layer will only work with Theano for the time being.\n-\n     # Arguments\n         padding: tuple of int (length 3)\n             How many zeros to add at the beginning and end of\n@@ -1215,9 +1208,6 @@ class ZeroPadding3D(Layer):\n     '''\n \n     def __init__(self, padding=(1, 1, 1), dim_ordering=K.image_dim_ordering(), **kwargs):\n-        if K._BACKEND != 'theano':\n-            raise Exception(self.__class__.__name__ +\n-                            ' is currently only working with Theano backend.')\n         super(ZeroPadding3D, self).__init__(**kwargs)\n         self.padding = tuple(padding)\n         assert dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf, th}'\n\n@@ -284,7 +284,6 @@ def test_zero_padding_2d():\n     layer.get_config()\n \n \n-@pytest.mark.skipif(K._BACKEND != 'theano', reason=\"Requires Theano backend\")\n def test_zero_padding_3d():\n     nb_samples = 2\n     stack_size = 2\n@@ -360,7 +359,6 @@ def test_upsampling_2d():\n                 assert_allclose(out, expected_out)\n \n \n-@pytest.mark.skipif(K._BACKEND != 'theano', reason=\"Requires Theano backend\")\n def test_upsampling_3d():\n     nb_samples = 2\n     stack_size = 2\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d3c75e1d342a6d11c7bf21b55eb9a664ed1e3e14", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 22 | Churn Cumulative: 5046 | Contributors (this commit): 38 | Commits (past 90d): 68 | Contributors (cumulative): 70 | DMM Complexity: 1.0\n\nDIFF:\n@@ -874,6 +874,13 @@ def batch_set_value(tuples):\n         get_session().run(ops)\n \n \n+def print_tensor(x, message=''):\n+    '''Print the message and the tensor when evaluated and return the same\n+    tensor.\n+    '''\n+    return tf.Print(x, [x], message)\n+\n+\n # GRAPH MANIPULATION\n \n class Function(object):\n\n@@ -3,6 +3,7 @@ from theano import tensor as T\n from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n from theano.tensor.signal import pool\n from theano.tensor.nnet import conv3d2d\n+from theano.printing import Print\n try:\n     from theano.tensor.nnet.nnet import softsign as T_softsign\n except ImportError:\n@@ -619,6 +620,14 @@ def batch_set_value(tuples):\n         x.set_value(np.asarray(value, dtype=x.dtype))\n \n \n+def print_tensor(x, message=''):\n+    '''Print the message and the tensor when evaluated and return the same\n+    tensor.\n+    '''\n+    p_op = Print(message)\n+    return p_op(x)\n+\n+\n # GRAPH MANIPULATION\n \n class Function(object):\n\n@@ -151,6 +151,12 @@ class TestBackend(object):\n         # count_params\n         assert KTH.count_params(xth) == KTF.count_params(xtf)\n \n+        # print_tensor\n+        check_single_tensor_operation('print_tensor', ())\n+        check_single_tensor_operation('print_tensor', (2,))\n+        check_single_tensor_operation('print_tensor', (4, 3))\n+        check_single_tensor_operation('print_tensor', (1, 2, 3))\n+\n     def test_elementwise_operations(self):\n         check_single_tensor_operation('max', (4, 2))\n         check_single_tensor_operation('max', (4, 2), axis=1, keepdims=True)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f221ef952f597e5d11df776d683e2e3e0b0d444f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 144 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -97,8 +97,11 @@ y_val = labels[-nb_validation_samples:]\n print('Preparing embedding matrix.')\n \n # prepare embedding matrix\n-embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n+nb_words = min(MAX_NB_WORDS, len(word_index))\n+embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\n for word, i in word_index.items():\n+    if i > MAX_NB_WORDS:\n+        continue\n     embedding_vector = embeddings_index.get(word)\n     if embedding_vector is not None:\n         # words not found in embedding index will be all-zeros.\n@@ -106,7 +109,7 @@ for word, i in word_index.items():\n \n # load pre-trained word embeddings into an Embedding layer\n # note that we set trainable = False so as to keep the embeddings fixed\n-embedding_layer = Embedding(len(word_index) + 1,\n+embedding_layer = Embedding(nb_words + 1,\n                             EMBEDDING_DIM,\n                             weights=[embedding_matrix],\n                             input_length=MAX_SEQUENCE_LENGTH,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#59bd2476038fa59760cdc4cb9c37e181762c2f19", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 15 | Files Changed: 1 | Hunks: 8 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 27 | Churn Cumulative: 137 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -5,33 +5,30 @@ Reference: \"Auto-Encoding Variational Bayes\" https://arxiv.org/abs/1312.6114\n import numpy as np\n import matplotlib.pyplot as plt\n \n-from keras.layers import Input, Dense, Lambda\n+from keras.layers import Input, Dense, merge\n from keras.models import Model\n from keras import backend as K\n from keras import objectives\n from keras.datasets import mnist\n \n-batch_size = 16\n+batch_size = 100\n original_dim = 784\n latent_dim = 2\n-intermediate_dim = 128\n-epsilon_std = 0.01\n-nb_epoch = 40\n+intermediate_dim = 500\n+nb_epoch = 100\n \n x = Input(batch_shape=(batch_size, original_dim))\n h = Dense(intermediate_dim, activation='relu')(x)\n z_mean = Dense(latent_dim)(h)\n-z_log_std = Dense(latent_dim)(h)\n+z_log_var = Dense(latent_dim)(h)\n \n def sampling(args):\n-    z_mean, z_log_std = args\n-    epsilon = K.random_normal(shape=(batch_size, latent_dim),\n-                              mean=0., std=epsilon_std)\n-    return z_mean + K.exp(z_log_std) * epsilon\n+    z_mean, z_log_var = args\n+    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.)\n+    return z_mean + K.exp(z_log_var/2) * epsilon\n \n # note that \"output_shape\" isn't necessary with the TensorFlow backend\n-# so you could write `Lambda(sampling)([z_mean, z_log_std])`\n-z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_std])\n+z = merge([z_mean, z_log_var], mode=sampling, output_shape=(latent_dim,))\n \n # we instantiate these layers separately so as to reuse them later\n decoder_h = Dense(intermediate_dim, activation='relu')\n@@ -40,8 +37,8 @@ h_decoded = decoder_h(z)\n x_decoded_mean = decoder_mean(h_decoded)\n \n def vae_loss(x, x_decoded_mean):\n-    xent_loss = objectives.binary_crossentropy(x, x_decoded_mean)\n-    kl_loss = - 0.5 * K.mean(1 + z_log_std - K.square(z_mean) - K.exp(z_log_std), axis=-1)\n+    xent_loss = original_dim * objectives.binary_crossentropy(x, x_decoded_mean)\n+    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n     return xent_loss + kl_loss\n \n vae = Model(x, x_decoded_mean)\n@@ -87,7 +84,7 @@ grid_y = np.linspace(-15, 15, n)\n \n for i, yi in enumerate(grid_x):\n     for j, xi in enumerate(grid_y):\n-        z_sample = np.array([[xi, yi]]) * epsilon_std\n+        z_sample = np.array([[xi, yi]])\n         x_decoded = generator.predict(z_sample)\n         digit = x_decoded[0].reshape(digit_size, digit_size)\n         figure[i * digit_size: (i + 1) * digit_size,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#09d75a434714e0ef8dcc14c88749dbb76b02b8df", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 5 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 147 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -5,7 +5,7 @@ Reference: \"Auto-Encoding Variational Bayes\" https://arxiv.org/abs/1312.6114\n import numpy as np\n import matplotlib.pyplot as plt\n \n-from keras.layers import Input, Dense, merge\n+from keras.layers import Input, Dense, Lambda\n from keras.models import Model\n from keras import backend as K\n from keras import objectives\n@@ -14,21 +14,22 @@ from keras.datasets import mnist\n batch_size = 100\n original_dim = 784\n latent_dim = 2\n-intermediate_dim = 500\n-nb_epoch = 100\n+intermediate_dim = 256\n+nb_epoch = 50\n \n x = Input(batch_shape=(batch_size, original_dim))\n h = Dense(intermediate_dim, activation='relu')(x)\n z_mean = Dense(latent_dim)(h)\n z_log_var = Dense(latent_dim)(h)\n \n+\n def sampling(args):\n     z_mean, z_log_var = args\n     epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.)\n     return z_mean + K.exp(z_log_var / 2) * epsilon\n \n # note that \"output_shape\" isn't necessary with the TensorFlow backend\n-z = merge([z_mean, z_log_var], mode=sampling, output_shape=(latent_dim,))\n+z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n \n # we instantiate these layers separately so as to reuse them later\n decoder_h = Dense(intermediate_dim, activation='relu')\n@@ -36,6 +37,7 @@ decoder_mean = Dense(original_dim, activation='sigmoid')\n h_decoded = decoder_h(z)\n x_decoded_mean = decoder_mean(h_decoded)\n \n+\n def vae_loss(x, x_decoded_mean):\n     xent_loss = original_dim * objectives.binary_crossentropy(x, x_decoded_mean)\n     kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c689b52dd1ecb4c4b260714e826353b4e6b2003d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 371 | Lines Deleted: 47 | Files Changed: 7 | Hunks: 18 | Methods Changed: 20 | Complexity Δ (Sum/Max): 49/24 | Churn Δ: 418 | Churn Cumulative: 10399 | Contributors (this commit): 51 | Commits (past 90d): 83 | Contributors (cumulative): 96 | DMM Complexity: 0.9803921568627451\n\nDIFF:\n@@ -0,0 +1,122 @@\n+'''This script demonstrates how to build a variational autoencoder with Keras and deconvolution layers.\n+\n+Reference: \"Auto-Encoding Variational Bayes\" https://arxiv.org/abs/1312.6114\n+'''\n+import numpy as np\n+import matplotlib.pyplot as plt\n+\n+from keras.layers import Input, Dense, Lambda, Flatten, Reshape\n+from keras.layers import Convolution2D, Deconvolution2D, MaxPooling2D\n+from keras.models import Model\n+from keras import backend as K\n+from keras import objectives\n+from keras.datasets import mnist\n+\n+# input image dimensions\n+img_rows, img_cols, img_chns = 28, 28, 1\n+# number of convolutional filters to use\n+nb_filters = 32\n+# convolution kernel size\n+nb_conv = 3\n+\n+batch_size = 16\n+original_dim = (img_chns, img_rows, img_cols)\n+latent_dim = 2\n+intermediate_dim = 128\n+epsilon_std = 0.01\n+nb_epoch = 5\n+\n+\n+x = Input(batch_shape=(batch_size,) + original_dim)\n+c = Convolution2D(nb_filters, nb_conv, nb_conv, border_mode='same', activation='relu')(x)\n+f = Flatten()(c)\n+h = Dense(intermediate_dim, activation='relu')(f)\n+\n+z_mean = Dense(latent_dim)(h)\n+z_log_std = Dense(latent_dim)(h)\n+\n+def sampling(args):\n+    z_mean, z_log_std = args\n+    epsilon = K.random_normal(shape=(batch_size, latent_dim),\n+                              mean=0., std=epsilon_std)\n+    return z_mean + K.exp(z_log_std) * epsilon\n+\n+# note that \"output_shape\" isn't necessary with the TensorFlow backend\n+# so you could write `Lambda(sampling)([z_mean, z_log_std])`\n+z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_std])\n+\n+# we instantiate these layers separately so as to reuse them later\n+decoder_h = Dense(intermediate_dim, activation='relu')\n+decoder_f = Dense(nb_filters*img_rows*img_cols, activation='relu')\n+decoder_c = Reshape((nb_filters, img_rows, img_cols))\n+decoder_mean = Deconvolution2D(img_chns, nb_conv, nb_conv,\n+                               (batch_size, img_chns, img_rows, img_cols), border_mode='same')\n+\n+h_decoded = decoder_h(z)\n+f_decoded = decoder_f(h_decoded)\n+c_decoded = decoder_c(f_decoded)\n+x_decoded_mean = decoder_mean(c_decoded)\n+\n+\n+def vae_loss(x, x_decoded_mean):\n+    # NOTE: binary_crossentropy expects a batch_size by dim for x and x_decoded_mean, so we MUST flatten these!\n+    x = K.flatten(x)\n+    x_decoded_mean = K.flatten(x_decoded_mean)\n+    xent_loss = objectives.binary_crossentropy(x, x_decoded_mean)\n+    kl_loss = - 0.5 * K.mean(1 + z_log_std - K.square(z_mean) - K.exp(z_log_std), axis=-1)\n+    return xent_loss + kl_loss\n+\n+vae = Model(x, x_decoded_mean)\n+vae.compile(optimizer='rmsprop', loss=vae_loss)\n+vae.summary()\n+\n+# train the VAE on MNIST digits\n+(x_train, y_train), (x_test, y_test) = mnist.load_data()\n+\n+x_train = x_train.astype('float32')[:, None, :, :] / 255.\n+x_test = x_test.astype('float32')[:, None, :, :] / 255.\n+\n+vae.fit(x_train, x_train,\n+        shuffle=True,\n+        nb_epoch=nb_epoch,\n+        batch_size=batch_size,\n+        validation_data=(x_test, x_test))\n+\n+\n+# build a model to project inputs on the latent space\n+encoder = Model(x, z_mean)\n+\n+# display a 2D plot of the digit classes in the latent space\n+x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n+plt.figure(figsize=(6, 6))\n+plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\n+plt.colorbar()\n+plt.show()\n+\n+# build a digit generator that can sample from the learned distribution\n+decoder_input = Input(shape=(latent_dim,))\n+_h_decoded = decoder_h(decoder_input)\n+_f_decoded = decoder_f(_h_decoded)\n+_c_decoded = decoder_c(_f_decoded)\n+_x_decoded_mean = decoder_mean(_c_decoded)\n+generator = Model(decoder_input, _x_decoded_mean)\n+\n+# display a 2D manifold of the digits\n+n = 15  # figure with 15x15 digits\n+digit_size = 28\n+figure = np.zeros((digit_size * n, digit_size * n))\n+# we will sample n points within [-15, 15] standard deviations\n+grid_x = np.linspace(-15, 15, n)\n+grid_y = np.linspace(-15, 15, n)\n+\n+for i, yi in enumerate(grid_x):\n+    for j, xi in enumerate(grid_y):\n+        z_sample = np.array([[xi, yi]]) * epsilon_std\n+        x_decoded = generator.predict(z_sample)\n+        digit = x_decoded[0].reshape(digit_size, digit_size)\n+        figure[i * digit_size: (i + 1) * digit_size,\n+               j * digit_size: (j + 1) * digit_size] = digit\n+\n+plt.figure(figsize=(10, 10))\n+plt.imshow(figure)\n+plt.show()\n\n@@ -1250,6 +1250,12 @@ def l2_normalize(x, axis):\n \n # CONVOLUTIONS\n \n+def _preprocess_deconv_output_shape(sh, dim_ordering):\n+    if dim_ordering == \"th\":\n+        sh = (sh[0], sh[2], sh[3], sh[1])\n+    return sh\n+\n+\n def _preprocess_conv2d_input(x, dim_ordering):\n     if _FLOATX == 'float64':\n         x = tf.cast(x, 'float32')\n@@ -1375,11 +1381,12 @@ def deconv2d(x, kernel, output_shape, strides=(1, 1),\n         raise Exception('Unknown dim_ordering ' + str(dim_ordering))\n \n     x = _preprocess_conv2d_input(x, dim_ordering)\n+    output_shape = _preprocess_deconv_output_shape(output_shape, dim_ordering)\n     kernel = _preprocess_conv2d_kernel(kernel, dim_ordering)\n+    kernel = tf.transpose(kernel, (0, 1, 3, 2))  # tranpose kernel chanels\n     padding = _preprocess_border_mode(border_mode)\n     strides = (1,) + strides + (1,)\n \n-    # TODO: pre-process output_shape if dim_ordering == th\n     x = tf.nn.conv2d_transpose(x, kernel, output_shape, strides,\n                                padding=padding)\n     return _postprocess_conv2d_output(x, dim_ordering)\n\n@@ -937,6 +937,79 @@ def l2_normalize(x, axis):\n \n # CONVOLUTIONS\n \n+def _preprocess_conv2d_input(x, dim_ordering):\n+    if dim_ordering == 'tf':\n+        # TF uses the last dimension as channel dimension,\n+        # instead of the 2nd one.\n+        # TH input shape: (samples, input_depth, rows, cols)\n+        # TF input shape: (samples, rows, cols, input_depth)\n+        x = x.dimshuffle((0, 3, 1, 2))\n+    return x\n+\n+\n+def _preprocess_conv2d_kernel(kernel, dim_ordering):\n+    if dim_ordering == 'tf':\n+        # TF uses the last dimension as channel dimension,\n+        # instead of the 2nd one.\n+        # TH kernel shape: (depth, input_depth, rows, cols)\n+        # TF kernel shape: (rows, cols, input_depth, depth)\n+        kernel = kernel.dimshuffle((3, 2, 0, 1))\n+    return kernel\n+\n+\n+def _preprocess_border_mode(border_mode):\n+    if border_mode == 'same':\n+        th_border_mode = 'half'\n+    elif border_mode == 'valid':\n+        th_border_mode = 'valid'\n+    else:\n+        raise Exception('Border mode not supported: ' + str(border_mode))\n+    return th_border_mode\n+\n+\n+def _preprocess_image_shape(dim_ordering, image_shape):\n+    # Theano might not accept long type\n+    def int_or_none(value):\n+        try:\n+            return int(value)\n+        except TypeError:\n+            return None\n+    if dim_ordering == 'tf':\n+        if image_shape:\n+            image_shape = (image_shape[0], image_shape[3],\n+                           image_shape[1], image_shape[2])\n+    if image_shape is not None:\n+        image_shape = tuple(int_or_none(v) for v in image_shape)\n+    return image_shape\n+\n+\n+def _preprocess_filter_shape(dim_ordering, filter_shape):\n+    # Theano might not accept long type\n+    def int_or_none(value):\n+        try:\n+            return int(value)\n+        except TypeError:\n+            return None\n+    if dim_ordering == 'tf':\n+        if filter_shape:\n+            filter_shape = (filter_shape[3], filter_shape[2],\n+                            filter_shape[0], filter_shape[1])\n+    if filter_shape is not None:\n+        filter_shape = tuple(int_or_none(v) for v in filter_shape)\n+    return filter_shape\n+\n+\n+def _postprocess_conv2d_output(conv_out, x, border_mode, np_kernel, strides, dim_ordering):\n+    if border_mode == 'same':\n+        if np_kernel.shape[2] % 2 == 0:\n+            conv_out = conv_out[:, :, :(x.shape[2] + strides[0] - 1) // strides[0], :]\n+        if np_kernel.shape[3] % 2 == 0:\n+            conv_out = conv_out[:, :, :, :(x.shape[3] + strides[1] - 1) // strides[1]]\n+    if dim_ordering == 'tf':\n+        conv_out = conv_out.dimshuffle((0, 2, 3, 1))\n+    return conv_out\n+\n+\n def conv2d(x, kernel, strides=(1, 1), border_mode='valid',\n            dim_ordering=_IMAGE_DIM_ORDERING, image_shape=None,\n            filter_shape=None, filter_dilation=(1, 1)):\n@@ -953,42 +1026,12 @@ def conv2d(x, kernel, strides=(1, 1), border_mode='valid',\n     if dim_ordering not in {'th', 'tf'}:\n         raise Exception('Unknown dim_ordering ' + str(dim_ordering))\n \n-    if dim_ordering == 'tf':\n-        # TF uses the last dimension as channel dimension,\n-        # instead of the 2nd one.\n-        # TH input shape: (samples, input_depth, rows, cols)\n-        # TF input shape: (samples, rows, cols, input_depth)\n-        # TH kernel shape: (depth, input_depth, rows, cols)\n-        # TF kernel shape: (rows, cols, input_depth, depth)\n-        x = x.dimshuffle((0, 3, 1, 2))\n-        kernel = kernel.dimshuffle((3, 2, 0, 1))\n-        if image_shape:\n-            image_shape = (image_shape[0], image_shape[3],\n-                           image_shape[1], image_shape[2])\n-        if filter_shape:\n-            filter_shape = (filter_shape[3], filter_shape[2],\n-                            filter_shape[0], filter_shape[1])\n-\n-    if border_mode == 'same':\n-        th_border_mode = 'half'\n+    x = _preprocess_conv2d_input(x, dim_ordering)\n+    kernel = _preprocess_conv2d_kernel(kernel, dim_ordering)\n+    th_border_mode = _preprocess_border_mode(border_mode)\n     np_kernel = kernel.eval()\n-    elif border_mode == 'valid':\n-        th_border_mode = 'valid'\n-    else:\n-        raise Exception('Border mode not supported: ' + str(border_mode))\n-\n-    # Theano might not accept long type\n-    def int_or_none(value):\n-        try:\n-            return int(value)\n-        except TypeError:\n-            return None\n-\n-    if image_shape is not None:\n-        image_shape = tuple(int_or_none(v) for v in image_shape)\n-\n-    if filter_shape is not None:\n-        filter_shape = tuple(int_or_none(v) for v in filter_shape)\n+    image_shape = _preprocess_image_shape(dim_ordering, image_shape)\n+    filter_shape = _preprocess_filter_shape(dim_ordering, filter_shape)\n \n     # TODO: remove the if statement when theano with no filter dilation is deprecated.\n     if filter_dilation == (1, 1):\n@@ -1005,14 +1048,8 @@ def conv2d(x, kernel, strides=(1, 1), border_mode='valid',\n                                  filter_shape=filter_shape,\n                                  filter_dilation=filter_dilation)\n \n-    if border_mode == 'same':\n-        if np_kernel.shape[2] % 2 == 0:\n-            conv_out = conv_out[:, :, :(x.shape[2] + strides[0] - 1) // strides[0], :]\n-        if np_kernel.shape[3] % 2 == 0:\n-            conv_out = conv_out[:, :, :, :(x.shape[3] + strides[1] - 1) // strides[1]]\n-\n-    if dim_ordering == 'tf':\n-        conv_out = conv_out.dimshuffle((0, 2, 3, 1))\n+    conv_out = _postprocess_conv2d_output(conv_out, x, border_mode, np_kernel,\n+                                          strides, dim_ordering)\n     return conv_out\n \n \n@@ -1020,7 +1057,38 @@ def deconv2d(x, kernel, output_shape, strides=(1, 1),\n              border_mode='valid',\n              dim_ordering=_IMAGE_DIM_ORDERING,\n              image_shape=None, filter_shape=None):\n-    raise NotImplementedError\n+    '''2D deconvolution (transposed convolution).\n+\n+    # Arguments\n+        kernel: kernel tensor.\n+        output_shape: desired dimensions of output.\n+        strides: strides tuple.\n+        border_mode: string, \"same\" or \"valid\".\n+        dim_ordering: \"tf\" or \"th\".\n+            Whether to use Theano or TensorFlow dimension ordering\n+        in inputs/kernels/ouputs.\n+    '''\n+    flip_filters = False\n+    if dim_ordering not in {'th', 'tf'}:\n+        raise Exception('Unknown dim_ordering ' + str(dim_ordering))\n+\n+    x = _preprocess_conv2d_input(x, dim_ordering)\n+    kernel = _preprocess_conv2d_kernel(kernel, dim_ordering)\n+    kernel = kernel.dimshuffle((1, 0, 2, 3))\n+    th_border_mode = _preprocess_border_mode(border_mode)\n+    np_kernel = kernel.eval()\n+    filter_shape = _preprocess_filter_shape(dim_ordering, filter_shape)\n+\n+    op = T.nnet.abstract_conv.AbstractConv2d_gradInputs(imshp=output_shape,\n+                                                        kshp=filter_shape,\n+                                                        subsample=strides,\n+                                                        border_mode=th_border_mode,\n+                                                        filter_flip=not flip_filters)\n+    conv_out = op(kernel, x, output_shape[2:])\n+\n+    conv_out = _postprocess_conv2d_output(conv_out, x, border_mode, np_kernel,\n+                                          strides, dim_ordering)\n+    return conv_out\n \n \n def atrous_conv2d(x, kernel, rate=1,\n\n@@ -4,7 +4,7 @@ from __future__ import absolute_import\n from .. import backend as K\n from .. import activations, initializations, regularizers, constraints\n from ..engine import Layer, InputSpec\n-from ..utils.np_utils import conv_output_length\n+from ..utils.np_utils import conv_output_length, conv_input_length\n \n # imports for backwards namespace compatibility\n from .pooling import AveragePooling1D, AveragePooling2D, AveragePooling3D\n@@ -379,6 +379,79 @@ class Convolution2D(Layer):\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n+class Deconvolution2D(Convolution2D):\n+    '''Transposed convolution operator for filtering windows of two-dimensional inputs.\n+    When using this layer as the first layer in a model,\n+    provide the keyword argument `input_shape`\n+    (tuple of integers, does not include the sample axis),\n+    e.g. `input_shape=(3, 128, 128)` for 128x128 RGB pictures.\n+    '''\n+    def __init__(self, nb_filter, nb_row, nb_col, output_shape,\n+                 init='glorot_uniform', activation='linear', weights=None,\n+                 border_mode='valid', subsample=(1, 1),\n+                 dim_ordering=K.image_dim_ordering(),\n+                 W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n+                 W_constraint=None, b_constraint=None,\n+                 bias=True, **kwargs):\n+\n+        if border_mode not in {'valid', 'same'}:\n+            raise Exception('Invalid border mode for AtrousConv2D:', border_mode)\n+\n+        self.output_shape_ = output_shape\n+\n+        super(Deconvolution2D, self).__init__(nb_filter, nb_row, nb_col,\n+                                              init=init, activation=activation,\n+                                              weights=weights, border_mode=border_mode,\n+                                              subsample=subsample, dim_ordering=dim_ordering,\n+                                              W_regularizer=W_regularizer, b_regularizer=b_regularizer,\n+                                              activity_regularizer=activity_regularizer,\n+                                              W_constraint=W_constraint, b_constraint=b_constraint,\n+                                              bias=bias, **kwargs)\n+\n+    def get_output_shape_for(self, input_shape):\n+        if self.dim_ordering == 'th':\n+            rows = input_shape[2]\n+            cols = input_shape[3]\n+        elif self.dim_ordering == 'tf':\n+            rows = input_shape[1]\n+            cols = input_shape[2]\n+        else:\n+            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+\n+        rows = conv_input_length(rows, self.nb_row,\n+                                 self.border_mode, self.subsample[0])\n+        cols = conv_input_length(cols, self.nb_col,\n+                                 self.border_mode, self.subsample[1])\n+\n+        if self.dim_ordering == 'th':\n+            return (input_shape[0], self.nb_filter, rows, cols)\n+        elif self.dim_ordering == 'tf':\n+            return (input_shape[0], rows, cols, self.nb_filter)\n+        else:\n+            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+\n+    def call(self, x, mask=None):\n+        output = K.deconv2d(x, self.W, self.output_shape_, \n+                            strides=self.subsample,\n+                            border_mode=self.border_mode,\n+                            dim_ordering=self.dim_ordering,\n+                            filter_shape=self.W_shape)\n+        if self.bias:\n+            if self.dim_ordering == 'th':\n+                output += K.reshape(self.b, (1, self.nb_filter, 1, 1))\n+            elif self.dim_ordering == 'tf':\n+                output += K.reshape(self.b, (1, 1, 1, self.nb_filter))\n+            else:\n+                raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+        output = self.activation(output)\n+        return output\n+\n+    def get_config(self):\n+        config = {'output_shape': self.output_shape}\n+        base_config = super(Deconvolution2D, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n class AtrousConvolution2D(Convolution2D):\n     '''Atrous Convolution operator for filtering windows of two-dimensional inputs.\n     A.k.a dilated convolution or convolution with holes.\n@@ -1251,5 +1324,6 @@ class ZeroPadding3D(Layer):\n Conv1D = Convolution1D\n Conv2D = Convolution2D\n Conv3D = Convolution3D\n+Deconv2D = Deconvolution2D\n AtrousConv2D = AtrousConvolution2D\n SeparableConv2D = SeparableConvolution2D\n\n@@ -120,3 +120,13 @@ def conv_output_length(input_length, filter_size, border_mode, stride, dilation=\n     elif border_mode == 'valid':\n         output_length = input_length - dilated_filter_size + 1\n     return (output_length + stride - 1) // stride\n+\n+def conv_input_length(output_length, filter_size, border_mode, stride):\n+    if output_length is None:\n+        return None\n+    assert border_mode in {'same', 'valid'}\n+    if border_mode == 'same':\n+        pad = filter_size // 2\n+    elif border_mode == 'valid':\n+        pad = 0\n+    return (output_length - 1) * stride - 2 * pad + filter_size\n\n@@ -36,7 +36,7 @@ def get_test_data(nb_train=1000, nb_test=500, input_shape=(10,),\n \n \n def layer_test(layer_cls, kwargs={}, input_shape=None, input_dtype=None,\n-               input_data=None, expected_output=None, expected_output_dtype=None):\n+               input_data=None, expected_output=None, expected_output_dtype=None, fixed_batch_size=False):\n     '''Test routine for a layer with a single input tensor\n     and single output tensor.\n     '''\n@@ -64,6 +64,9 @@ def layer_test(layer_cls, kwargs={}, input_shape=None, input_dtype=None,\n         layer = layer_cls(**kwargs)\n \n     # test in functional API\n+    if fixed_batch_size:\n+        x = Input(batch_shape=input_shape, dtype=input_dtype)\n+    else:\n         x = Input(shape=input_shape[1:], dtype=input_dtype)\n     y = layer(x)\n     assert K.dtype(y) == expected_output_dtype\n\n@@ -3,6 +3,7 @@ import numpy as np\n from numpy.testing import assert_allclose\n \n from keras.utils.test_utils import layer_test, keras_test\n+from keras.utils.np_utils import conv_input_length\n from keras import backend as K\n from keras.layers import convolutional\n \n@@ -88,6 +89,45 @@ def test_convolution_2d():\n                        input_shape=(nb_samples, stack_size, nb_row, nb_col))\n \n \n+@keras_test\n+def test_deconvolution_2d():\n+    nb_samples = 2\n+    nb_filter = 2\n+    stack_size = 3\n+    nb_row = 10\n+    nb_col = 6\n+\n+    for border_mode in ['valid', 'same']:\n+        for subsample in [(1, 1), (2, 2)]:\n+            if border_mode == 'same' and subsample != (1, 1):\n+                continue\n+\n+            rows = conv_input_length(nb_row, 3, border_mode, subsample[0])\n+            cols = conv_input_length(nb_col, 3, border_mode, subsample[1])\n+            layer_test(convolutional.Deconvolution2D,\n+                       kwargs={'nb_filter': nb_filter,\n+                               'nb_row': 3,\n+                               'nb_col': 3,\n+                               'output_shape': (nb_samples, nb_filter, rows, cols),\n+                               'border_mode': border_mode,\n+                               'subsample': subsample},\n+                       input_shape=(nb_samples, stack_size, nb_row, nb_col),\n+                       fixed_batch_size=True)\n+\n+            layer_test(convolutional.Deconvolution2D,\n+                       kwargs={'nb_filter': nb_filter,\n+                               'nb_row': 3,\n+                               'nb_col': 3,\n+                               'output_shape': (nb_samples, nb_filter, rows, cols),\n+                               'border_mode': border_mode,\n+                               'W_regularizer': 'l2',\n+                               'b_regularizer': 'l2',\n+                               'activity_regularizer': 'activity_l2',\n+                               'subsample': subsample},\n+                       input_shape=(nb_samples, stack_size, nb_row, nb_col),\n+                       fixed_batch_size=True)\n+\n+\n @keras_test\n def test_atrous_conv_2d():\n     nb_samples = 2\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
