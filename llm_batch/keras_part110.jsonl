{"custom_id": "keras#65e4c8e76e8df8a030af0e3a8815618caff6a404", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 224 | Contributors (this commit): 7 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -48,7 +48,7 @@ for i, sentence in enumerate(sentences):\n     y[i, char_indices[next_chars[i]]] = 1\n \n \n-# build the model: 2 stacked LSTM\n+# build the model: a single LSTM\n print('Build model...')\n model = Sequential()\n model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#acc5c45feba49de09ee6f46dfb5fad64f4eeb28f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 2087 | Contributors (this commit): 20 | Commits (past 90d): 22 | Contributors (cumulative): 20 | DMM Complexity: 0.5\n\nDIFF:\n@@ -413,20 +413,16 @@ def generator_queue(generator, max_q_size=10,\n     if pickle_safe:\n         q = multiprocessing.Queue(maxsize=max_q_size)\n         _stop = multiprocessing.Event()\n-        lock = multiprocessing.Lock()\n     else:\n         q = queue.Queue()\n         _stop = threading.Event()\n-        lock = threading.Lock()\n \n     try:\n         def data_generator_task():\n             while not _stop.is_set():\n                 try:\n                     if pickle_safe or q.qsize() < max_q_size:\n-                        lock.acquire()\n                         generator_output = next(generator)\n-                        lock.release()\n                         q.put(generator_output)\n                     else:\n                         time.sleep(wait_time)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2f4eed1f0fb01e58791f02fbe1640869f13f876b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 2091 | Contributors (this commit): 20 | Commits (past 90d): 22 | Contributors (cumulative): 20 | DMM Complexity: 0.5\n\nDIFF:\n@@ -413,20 +413,16 @@ def generator_queue(generator, max_q_size=10,\n     if pickle_safe:\n         q = multiprocessing.Queue(maxsize=max_q_size)\n         _stop = multiprocessing.Event()\n-        lock = multiprocessing.Lock()\n     else:\n         q = queue.Queue()\n         _stop = threading.Event()\n-        lock = threading.Lock()\n \n     try:\n         def data_generator_task():\n             while not _stop.is_set():\n                 try:\n                     if pickle_safe or q.qsize() < max_q_size:\n-                        lock.acquire()\n                         generator_output = next(generator)\n-                        lock.release()\n                         q.put(generator_output)\n                     else:\n                         time.sleep(wait_time)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c6daa24e3cac10e3488d5a2eadfd6134de55e677", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 12 | Churn Cumulative: 3034 | Contributors (this commit): 41 | Commits (past 90d): 37 | Contributors (cumulative): 47 | DMM Complexity: 1.0\n\nDIFF:\n@@ -386,10 +386,14 @@ def normalize_batch_in_training(x, gamma, beta,\n def batch_normalization(x, mean, var, beta, gamma, epsilon=0.0001):\n     '''Apply batch normalization on x given mean, var, beta and gamma.\n     '''\n-    normed = T.nnet.bn.batch_normalization(x, gamma, beta, mean,\n-                                           sqrt(var) + epsilon,\n+    if theano.config.device.startswith('cuda') or theano.config.device.startswith('gpu'):\n+        try:\n+            return theano.sandbox.cuda.dnn.dnn_batch_normalization_test(x, gamma, beta, mean, var,\n+                                                                        'spatial', epsilon)\n+        except AttributeError:\n+            pass\n+    return T.nnet.bn.batch_normalization(x, gamma, beta, mean, sqrt(var) + epsilon,\n                                          mode='high_mem')\n-    return normed\n \n \n # SHAPE OPERATIONS\n\n@@ -56,7 +56,7 @@ class BatchNormalization(Layer):\n     # References\n         - [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://jmlr.org/proceedings/papers/v37/ioffe15.html)\n     '''\n-    def __init__(self, epsilon=1e-6, mode=0, axis=-1, momentum=0.99,\n+    def __init__(self, epsilon=1e-5, mode=0, axis=-1, momentum=0.99,\n                  weights=None, beta_init='zero', gamma_init='one', **kwargs):\n         self.supports_masking = True\n         self.beta_init = initializations.get(beta_init)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f2786d9d800325cd4032af4041614f7a0d900efb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2323 | Contributors (this commit): 34 | Commits (past 90d): 27 | Contributors (cumulative): 34 | DMM Complexity: None\n\nDIFF:\n@@ -392,7 +392,7 @@ def batch_normalization(x, mean, var, beta, gamma, epsilon=0.0001):\n                                                                         'spatial', epsilon)\n         except AttributeError:\n             pass\n-    return T.nnet.bn.batch_normalization(x, gamma, beta, mean, sqrt(var) + epsilon,\n+    return T.nnet.bn.batch_normalization(x, gamma, beta, mean, sqrt(var + epsilon),\n                                          mode='high_mem')\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e4dda27de186f6f98a61aee3328aa21d7ef7f7ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 99 | Lines Deleted: 57 | Files Changed: 2 | Hunks: 12 | Methods Changed: 15 | Complexity Δ (Sum/Max): 14/11 | Churn Δ: 156 | Churn Cumulative: 1296 | Contributors (this commit): 9 | Commits (past 90d): 5 | Contributors (cumulative): 12 | DMM Complexity: 0.7333333333333333\n\nDIFF:\n@@ -234,6 +234,13 @@ class KerasClassifier(BaseWrapper):\n                 Mean accuracy of predictions on X wrt. y.\n         '''\n         kwargs = self.filter_sk_params(Sequential.evaluate, kwargs)\n+\n+        loss_name = self.model.loss\n+        if hasattr(loss_name, '__name__'):\n+            loss_name = loss_name.__name__\n+        if loss_name == 'categorical_crossentropy' and len(y.shape) != 2:\n+            y = to_categorical(y)\n+\n         outputs = self.model.evaluate(X, y, **kwargs)\n         if type(outputs) is not list:\n             outputs = [outputs]\n@@ -263,7 +270,7 @@ class KerasRegressor(BaseWrapper):\n                 Predictions.\n         '''\n         kwargs = self.filter_sk_params(Sequential.predict, kwargs)\n-        return self.model.predict(X, **kwargs)\n+        return np.squeeze(self.model.predict(X, **kwargs))\n \n     def score(self, X, y, **kwargs):\n         '''Returns the mean loss on the given test data and labels.\n\n@@ -9,9 +9,10 @@ from keras.models import Sequential\n from keras.layers.core import Dense, Activation\n from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n \n-np.random.seed(1337)\n-\n-input_dim = 10\n+input_dim = 5\n+hidden_dims = 5\n+nb_train = 100\n+nb_test = 50\n nb_class = 3\n batch_size = 32\n nb_epoch = 1\n@@ -19,25 +20,13 @@ verbosity = 0\n optim = 'adam'\n loss = 'categorical_crossentropy'\n \n-\n-(X_train, y_train), (X_test, y_test) = get_test_data(nb_train=400,\n-                                                     nb_test=200,\n-                                                     input_shape=(input_dim,),\n-                                                     classification=True,\n-                                                     nb_class=nb_class)\n-y_train = np_utils.to_categorical(y_train, nb_classes=nb_class)\n-y_test = np_utils.to_categorical(y_test, nb_classes=nb_class)\n+np.random.seed(42)\n+(X_train, y_train), (X_test, y_test) = get_test_data(\n+    nb_train=nb_train, nb_test=nb_test, input_shape=(input_dim,),\n+    classification=True, nb_class=nb_class)\n \n \n-(X_train_reg, y_train_reg), (X_test_reg, y_test_reg) = get_test_data(nb_train=400,\n-                                                                     nb_test=200,\n-                                                                     input_shape=(input_dim,),\n-                                                                     classification=False,\n-                                                                     nb_class=1,\n-                                                                     output_shape=(1,))\n-\n-\n-def build_fn_clf(hidden_dims=50):\n+def build_fn_clf(hidden_dims):\n     model = Sequential()\n     model.add(Dense(input_dim, input_shape=(input_dim,)))\n     model.add(Activation('relu'))\n@@ -50,15 +39,53 @@ def build_fn_clf(hidden_dims=50):\n     return model\n \n \n-class Class_build_fn_clf(object):\n+def test_clasify_build_fn():\n+    clf = KerasClassifier(\n+        build_fn=build_fn_clf, hidden_dims=hidden_dims,\n+        batch_size=batch_size, nb_epoch=nb_epoch)\n+\n+    assert_classification_works(clf)\n+\n+\n+def test_clasify_class_build_fn():\n+    class ClassBuildFnClf(object):\n         def __call__(self, hidden_dims):\n             return build_fn_clf(hidden_dims)\n \n+    clf = KerasClassifier(\n+        build_fn=ClassBuildFnClf(), hidden_dims=hidden_dims,\n+        batch_size=batch_size, nb_epoch=nb_epoch)\n \n-class Inherit_class_build_fn_clf(KerasClassifier):\n+    assert_classification_works(clf)\n+\n+\n+def test_clasify_inherit_class_build_fn():\n+    class InheritClassBuildFnClf(KerasClassifier):\n         def __call__(self, hidden_dims):\n             return build_fn_clf(hidden_dims)\n \n+    clf = InheritClassBuildFnClf(\n+        build_fn=None, hidden_dims=hidden_dims,\n+        batch_size=batch_size, nb_epoch=nb_epoch)\n+\n+    assert_classification_works(clf)\n+\n+\n+def assert_classification_works(clf):\n+    clf.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch)\n+\n+    score = clf.score(X_train, y_train, batch_size=batch_size)\n+    assert np.isscalar(score) and np.isfinite(score)\n+\n+    preds = clf.predict(X_test, batch_size=batch_size)\n+    assert preds.shape == (nb_test, )\n+    for prediction in np.unique(preds):\n+        assert prediction in range(nb_class)\n+\n+    proba = clf.predict_proba(X_test, batch_size=batch_size)\n+    assert proba.shape == (nb_test, nb_class)\n+    assert np.allclose(np.sum(proba, axis=1), np.ones(nb_test))\n+\n \n def build_fn_reg(hidden_dims=50):\n     model = Sequential()\n@@ -73,42 +100,50 @@ def build_fn_reg(hidden_dims=50):\n     return model\n \n \n-class Class_build_fn_reg(object):\n-    def __call__(self, hidden_dims):\n-        return build_fn_reg(hidden_dims)\n-\n-\n-class Inherit_class_build_fn_reg(KerasRegressor):\n-    def __call__(self, hidden_dims):\n-        return build_fn_reg(hidden_dims)\n-\n-for fn in [build_fn_clf, Class_build_fn_clf(), Inherit_class_build_fn_clf]:\n-    if fn is Inherit_class_build_fn_clf:\n-        classifier = Inherit_class_build_fn_clf(\n-            build_fn=None, hidden_dims=50, batch_size=batch_size, nb_epoch=nb_epoch)\n-    else:\n-        classifier = KerasClassifier(\n-            build_fn=fn, hidden_dims=50, batch_size=batch_size, nb_epoch=nb_epoch)\n-\n-    classifier.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch)\n-    score = classifier.score(X_train, y_train, batch_size=batch_size)\n-    preds = classifier.predict(X_test, batch_size=batch_size)\n-    proba = classifier.predict_proba(X_test, batch_size=batch_size)\n-\n-\n-for fn in [build_fn_reg, Class_build_fn_reg(), Inherit_class_build_fn_reg]:\n-    if fn is Inherit_class_build_fn_reg:\n-        regressor = Inherit_class_build_fn_reg(\n-            build_fn=None, hidden_dims=50, batch_size=batch_size, nb_epoch=nb_epoch)\n-    else:\n-        regressor = KerasRegressor(\n-            build_fn=fn, hidden_dims=50, batch_size=batch_size, nb_epoch=nb_epoch)\n-\n-    regressor.fit(X_train_reg, y_train_reg,\n+def test_regression_build_fn():\n+    reg = KerasRegressor(\n+        build_fn=build_fn_reg, hidden_dims=hidden_dims,\n         batch_size=batch_size, nb_epoch=nb_epoch)\n-    score = regressor.score(X_train_reg, y_train_reg, batch_size=batch_size)\n-    preds = regressor.predict(X_test, batch_size=batch_size)\n \n+    assert_regression_works(reg)\n+\n+\n+def test_regression_class_build_fn():\n+    class ClassBuildFnReg(object):\n+        def __call__(self, hidden_dims):\n+            return build_fn_reg(hidden_dims)\n+\n+    reg = KerasRegressor(\n+        build_fn=ClassBuildFnReg(), hidden_dims=hidden_dims,\n+        batch_size=batch_size, nb_epoch=nb_epoch)\n+\n+    assert_regression_works(reg)\n+\n+\n+def test_regression_inherit_class_build_fn():\n+    class InheritClassBuildFnReg(KerasRegressor):\n+        def __call__(self, hidden_dims):\n+            return build_fn_reg(hidden_dims)\n+\n+    reg = InheritClassBuildFnReg(\n+        build_fn=None, hidden_dims=hidden_dims,\n+        batch_size=batch_size, nb_epoch=nb_epoch)\n+\n+    assert_regression_works(reg)\n+\n+\n+def assert_regression_works(reg):\n+    reg.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch)\n+\n+    score = reg.score(X_train, y_train, batch_size=batch_size)\n+    assert np.isscalar(score) and np.isfinite(score)\n+\n+    preds = reg.predict(X_test, batch_size=batch_size)\n+    assert preds.shape == (nb_test, )\n+\n+\n+if __name__ == '__main__':\n+    pytest.main([__file__])\n \n # Usage of sklearn's grid_search\n # from sklearn import grid_search\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#090b8b7f9902c2a358a26c3af09403a86937fe42", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 297 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 10 | Complexity Δ (Sum/Max): 40/32 | Churn Δ: 297 | Churn Cumulative: 5870 | Contributors (this commit): 25 | Commits (past 90d): 22 | Contributors (cumulative): 32 | DMM Complexity: 0.8864864864864865\n\nDIFF:\n@@ -1346,6 +1346,213 @@ class ZeroPadding3D(Layer):\n         base_config = super(ZeroPadding3D, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n+class Cropping1D(Layer):\n+    '''Cropping layer for 1D input (e.g. temporal sequence).\n+    It crops along the time dimension (axis 1).\n+\n+    # Arguments\n+        cropping: tuple of int (length 2)\n+            How many units should be trimmed off at the beginning and end of\n+            the cropping dimension (axis 1).\n+\n+    # Input shape\n+        3D tensor with shape (samples, axis_to_crop, features)\n+\n+    # Output shape\n+        3D tensor with shape (samples, cropped_axis, features)\n+    '''\n+\n+    def __init__(self, cropping=(1, 1), **kwargs):\n+        super(Cropping1D, self).__init__(**kwargs)\n+        self.cropping = tuple(cropping)\n+        assert len(self.cropping) == 2, 'cropping must be a tuple length of 2'\n+        self.input_spec = [InputSpec(ndim=3)] # redundant due to build()?       \n+\n+    def build(self, input_shape):\n+        self.input_spec = [InputSpec(shape=input_shape)]\n+\n+    def get_output_shape_for(self, input_shape):\n+        length = input_shape[1] - self.cropping[0] - self.cropping[1] if input_shape[1] is not None else None\n+        return (input_shape[0],\n+                length,\n+                input_shape[2])\n+\n+    def call(self, x, mask=None):\n+        input_shape = self.input_spec[0].shape\n+        return x[:, self.cropping[0]:input_shape[1]-self.cropping[1], :]\n+\n+    def get_config(self):\n+        config = {'cropping': self.cropping}\n+        base_config = super(Cropping1D, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+class Cropping2D(Layer):\n+    '''Cropping layer for 2D input (e.g. picture).\n+    It crops along spatial dimensions, i.e. width and height.\n+\n+    # Arguments\n+        cropping: tuple of tuple of int (length 2)\n+            How many units should be trimmed off at the beginning and end of\n+            the 2 cropping dimensions (width, height).\n+        dim_ordering: 'th' or 'tf'.\n+            In 'th' mode, the channels dimension (the depth)\n+            is at index 1, in 'tf' mode is it at index 3.\n+            It defaults to the `image_dim_ordering` value found in your\n+            Keras config file at `~/.keras/keras.json`.\n+            If you never set it, then it will be \"th\".\n+\n+    # Input shape\n+        4D tensor with shape:\n+        (samples, depth, first_axis_to_crop, second_axis_to_crop)\n+\n+    # Output shape\n+        4D tensor with shape:\n+        (samples, depth, first_cropped_axis, second_cropped_axis)\n+\n+    # Examples\n+\n+    ```python\n+        # Crop the input 2D images or feature maps\n+        model = Sequential()\n+        model.add(Cropping2D(cropping=((2, 2), (4, 4)), input_shape=(3, 28, 28)))\n+        # now model.output_shape == (None, 3, 24, 20)\n+        model.add(Convolution2D(64, 3, 3, border_mode='same))\n+        model.add(Cropping2D(cropping=((2, 2), (2, 2))))\n+        # now model.output_shape == (None, 64, 20, 16)\n+\n+    ```\n+\n+    '''\n+\n+    def __init__(self, cropping=((0, 0), (0, 0)), dim_ordering='default', **kwargs):\n+        super(Cropping2D, self).__init__(**kwargs)\n+        if dim_ordering == 'default':\n+            dim_ordering = K.image_dim_ordering()\n+        self.cropping = tuple(cropping)\n+        assert len(self.cropping) == 2, 'cropping must be a tuple length of 2'\n+        assert len(self.cropping[0]) == 2, 'cropping[0] must be a tuple length of 2'\n+        assert len(self.cropping[1]) == 2, 'cropping[1] must be a tuple length of 2'\n+        assert dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf, th}'\n+        self.dim_ordering = dim_ordering\n+        self.input_spec = [InputSpec(ndim=4)]        \n+\n+    def build(self, input_shape):\n+        self.input_spec = [InputSpec(shape=input_shape)]\n+\n+    def get_output_shape_for(self, input_shape):\n+        if self.dim_ordering == 'th':\n+            return (input_shape[0],\n+                    input_shape[1],\n+                    input_shape[2] - self.cropping[0][0] - self.cropping[0][1],\n+                    input_shape[3] - self.cropping[1][0] - self.cropping[1][1])\n+        elif self.dim_ordering == 'tf':\n+            return (input_shape[0],\n+                    input_shape[1] - self.cropping[0][0] - self.cropping[0][1],\n+                    input_shape[2] - self.cropping[1][0] - self.cropping[1][1],\n+                    input_shape[3])\n+        else:\n+            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+\n+    def call(self, x, mask=None):\n+        input_shape = self.input_spec[0].shape\n+        if self.dim_ordering == 'th':\n+            return x[:, \n+                     :, \n+                     self.cropping[0][0]:input_shape[2]-self.cropping[0][1],\n+                     self.cropping[1][0]:input_shape[3]-self.cropping[1][1]]\n+        elif self.dim_ordering == 'tf':\n+            return x[:, \n+                     self.cropping[0][0]:input_shape[1]-self.cropping[0][1], \n+                     self.cropping[1][0]:input_shape[2]-self.cropping[1][1],\n+                     :]\n+\n+    def get_config(self):\n+        config = {'cropping': self.cropping}\n+        base_config = super(Cropping2D, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+class Cropping3D(Layer):\n+    '''Cropping layer for 2D input (e.g. picture).\n+\n+    # Arguments\n+        cropping: tuple of tuple of int (length 3)\n+            How many units should be trimmed off at the beginning and end of\n+            the 3 cropping dimensions (kernel_dim1, kernel_dim2, kernerl_dim3).\n+        dim_ordering: 'th' or 'tf'.\n+            In 'th' mode, the channels dimension (the depth)\n+            is at index 1, in 'tf' mode is it at index 4.\n+            It defaults to the `image_dim_ordering` value found in your\n+            Keras config file at `~/.keras/keras.json`.\n+            If you never set it, then it will be \"th\".\n+\n+    # Input shape\n+        5D tensor with shape:\n+        (samples, depth, first_axis_to_crop, second_axis_to_crop, third_axis_to_crop)\n+\n+    # Output shape\n+        5D tensor with shape:\n+        (samples, depth, first_cropped_axis, second_cropped_axis, third_cropped_axis)\n+    \n+    '''\n+\n+    def __init__(self, cropping=((1, 1), (1, 1), (1, 1)), dim_ordering='default', **kwargs):\n+        super(Cropping3D, self).__init__(**kwargs)\n+        if dim_ordering == 'default':\n+            dim_ordering = K.image_dim_ordering()\n+        self.cropping = tuple(cropping)\n+        assert len(self.cropping) == 3, 'cropping must be a tuple length of 3'\n+        assert len(self.cropping[0]) == 2, 'cropping[0] must be a tuple length of 2'\n+        assert len(self.cropping[1]) == 2, 'cropping[1] must be a tuple length of 2'\n+        assert len(self.cropping[2]) == 2, 'cropping[2] must be a tuple length of 2'\n+        assert dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf, th}'\n+        self.dim_ordering = dim_ordering\n+        self.input_spec = [InputSpec(ndim=4)]        \n+\n+    def build(self, input_shape):\n+        self.input_spec = [InputSpec(shape=input_shape)]\n+\n+    def get_output_shape_for(self, input_shape):\n+        if self.dim_ordering == 'th':\n+            dim1 = input_shape[2] - self.cropping[0][0] - self.cropping[0][1] if input_shape[2] is not None else None\n+            dim2 = input_shape[3] - self.cropping[1][0] - self.cropping[1][1] if input_shape[3] is not None else None\n+            dim3 = input_shape[4] - self.cropping[2][0] - self.cropping[2][1] if input_shape[4] is not None else None\n+            return (input_shape[0],\n+                    input_shape[1],\n+                    dim1,\n+                    dim2,\n+                    dim3)\n+        elif self.dim_ordering == 'tf':\n+            dim1 = input_shape[1] - self.cropping[0][0] - self.cropping[0][1] if input_shape[1] is not None else None\n+            dim2 = input_shape[2] - self.cropping[1][0] - self.cropping[1][1] if input_shape[2] is not None else None\n+            dim3 = input_shape[3] - self.cropping[2][0] - self.cropping[2][1] if input_shape[3] is not None else None\n+            return (input_shape[0],\n+                    dim1,\n+                    dim2,\n+                    dim3,\n+                    input_shape[4])\n+        else:\n+            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+\n+    def call(self, x, mask=None):\n+        input_shape = self.input_spec[0].shape\n+        if self.dim_ordering == 'th':\n+            return x[:, \n+                     :, \n+                     self.cropping[0][0]:input_shape[2]-self.cropping[0][1], \n+                     self.cropping[1][0]:input_shape[3]-self.cropping[1][1], \n+                     self.cropping[2][0]:input_shape[4]-self.cropping[2][1]]\n+        elif self.dim_ordering == 'tf':\n+            return x[:, \n+                     self.cropping[0][0]:input_shape[1]-self.cropping[0][1], \n+                     self.cropping[1][0]:input_shape[2]-self.cropping[1][1], \n+                     self.cropping[2][0]:input_shape[3]-self.cropping[2][1], \n+                     :]\n+\n+    def get_config(self):\n+        config = {'cropping': self.cropping}\n+        base_config = super(Cropping3D, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n \n # Aliases\n \n\n@@ -444,5 +444,95 @@ def test_upsampling_3d():\n                     assert_allclose(out, expected_out)\n \n \n+@keras_test\n+def test_cropping_1d():\n+    nb_samples = 2\n+    time_length = 10\n+    input_len_dim1 = 2\n+    input = np.random.rand(nb_samples, time_length, input_len_dim1)\n+\n+    layer_test(convolutional.Cropping1D,\n+               kwargs={'cropping': (2, 2)},\n+               input_shape=input.shape)\n+\n+def test_cropping_2d():\n+    nb_samples = 2\n+    stack_size = 2\n+    input_len_dim1 = 10\n+    input_len_dim2 = 20\n+    cropping = ((2, 2), (3, 3))\n+    dim_ordering = K.image_dim_ordering()\n+    \n+    if dim_ordering == 'th':\n+        input = np.random.rand(nb_samples, stack_size, input_len_dim1, input_len_dim2)\n+    else:\n+        input = np.random.rand(nb_samples, input_len_dim1, input_len_dim2, stack_size)\n+    # basic test        \n+    layer_test(convolutional.Cropping2D,\n+               kwargs={'cropping': cropping,\n+                       'dim_ordering': dim_ordering},\n+               input_shape=input.shape)\n+    # correctness test\n+    layer = convolutional.Cropping2D(cropping=cropping, dim_ordering=dim_ordering)\n+    layer.set_input(K.variable(input), shape=input.shape)\n+\n+    out = K.eval(layer.output)\n+    # compare with numpy\n+    if dim_ordering == 'th':\n+        expected_out = input[:, \n+                             :, \n+                             cropping[0][0]:-cropping[0][1], \n+                             cropping[1][0]:-cropping[1][1]]\n+    else:\n+        expected_out = input[:, \n+                             cropping[0][0]:-cropping[0][1], \n+                             cropping[1][0]:-cropping[1][1], \n+                             :]\n+\n+    assert_allclose(out, expected_out)\n+\n+\n+def test_cropping_3d():\n+    nb_samples = 2\n+    stack_size = 2\n+    input_len_dim1 = 10\n+    input_len_dim2 = 20\n+    input_len_dim3 = 30\n+    cropping = ((2, 2), (3, 3), (2, 3))\n+    dim_ordering = K.image_dim_ordering()\n+    \n+    if dim_ordering == 'th':\n+        input = np.random.rand(nb_samples, stack_size, input_len_dim1, input_len_dim2, input_len_dim3)\n+    else:\n+        input = np.random.rand(nb_samples, input_len_dim1, input_len_dim2, input_len_dim3, stack_size)\n+    # basic test        \n+    layer_test(convolutional.Cropping3D,\n+               kwargs={'cropping': cropping,\n+                       'dim_ordering': dim_ordering},\n+               input_shape=input.shape)\n+    # correctness test\n+    layer = convolutional.Cropping3D(cropping=cropping, dim_ordering=dim_ordering)\n+    layer.set_input(K.variable(input), shape=input.shape)\n+\n+    out = K.eval(layer.output)\n+    # compare with numpy\n+    if dim_ordering == 'th':\n+        expected_out = input[:, \n+                             :, \n+                             cropping[0][0]:-cropping[0][1], \n+                             cropping[1][0]:-cropping[1][1], \n+                             cropping[2][0]:-cropping[2][1]]\n+    else:\n+        expected_out = input[:, \n+                             cropping[0][0]:-cropping[0][1], \n+                             cropping[1][0]:-cropping[1][1], \n+                             cropping[2][0]:-cropping[2][1], \n+                             :]\n+\n+    assert_allclose(out, expected_out)\n+\n+\n+def test_cropping_3d():\n+    pass\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6a6d939dea195ab94dfad2050cd2a0b3555a2568", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 444 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -389,7 +389,7 @@ img_gen = TextImageGenerator(monogram_file=os.path.join(fdir, 'wordlist_mono_cle\n act = 'relu'\n input_data = Input(name='the_input', shape=(1, img_h, img_w), dtype='float32')\n inner = Convolution2D(conv_num_filters, filter_size, filter_size, border_mode='same',\n-                      activation=act, input_shape=(1, img_h, img_w), name='conv1')(input_data)\n+                      activation=act, name='conv1')(input_data)\n inner = MaxPooling2D(pool_size=(pool_size_1, pool_size_1), name='max1')(inner)\n inner = Convolution2D(conv_num_filters, filter_size, filter_size, border_mode='same',\n                       activation=act, name='conv2')(inner)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7d2f0b1ba8c1ebd4213f748878953d576626fe84", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 62 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 8 | Methods Changed: 7 | Complexity Δ (Sum/Max): 6/3 | Churn Δ: 64 | Churn Cumulative: 11900 | Contributors (this commit): 63 | Commits (past 90d): 46 | Contributors (cumulative): 70 | DMM Complexity: 0.9777777777777777\n\nDIFF:\n@@ -1586,6 +1586,9 @@ class Container(Layer):\n             name = prefix + '_' + str(K.get_uid(prefix))\n         self.name = name\n \n+        # whether container weights are trainable\n+        self.trainable = True\n+\n         # Container-specific properties\n         if type(input) in {list, tuple}:\n             self.inputs = list(input)  # tensor or list of tensors\n@@ -1962,6 +1965,8 @@ class Container(Layer):\n \n     @property\n     def trainable_weights(self):\n+        if not self.trainable:\n+            return []\n         weights = []\n         for layer in self.layers:\n             weights += layer.trainable_weights\n@@ -1972,6 +1977,11 @@ class Container(Layer):\n         weights = []\n         for layer in self.layers:\n             weights += layer.non_trainable_weights\n+        if not self.trainable:\n+            trainable_weights = []\n+            for layer in self.layers:\n+                trainable_weights += layer.trainable_weights\n+            return trainable_weights + weights\n         return weights\n \n     def get_weights(self):\n\n@@ -238,6 +238,7 @@ class Sequential(Model):\n         self.model = None  # internal Model instance\n         self.inputs = []  # tensors\n         self.outputs = []  # tensors (length 1)\n+        self.trainable = True\n \n         # model attributes\n         self.inbound_nodes = []\n@@ -438,13 +439,19 @@ class Sequential(Model):\n \n     @property\n     def trainable_weights(self):\n+        if not self.trainable:\n+            return []\n         # support for legacy behavior\n         return self._gather_list_attr('trainable_weights')\n \n     @property\n     def non_trainable_weights(self):\n         # support for legacy behavior\n-        return self._gather_list_attr('non_trainable_weights')\n+        weights = self._gather_list_attr('non_trainable_weights')\n+        if not self.trainable:\n+            trainable_weights = self._gather_list_attr('trainable_weights')\n+            return trainable_weights + weights\n+        return weights\n \n     @property\n     def updates(self):\n\n@@ -4,12 +4,55 @@ import numpy as np\n \n from keras.layers import Dense, Dropout, InputLayer\n from keras.engine import merge, Input, get_source_inputs\n-from keras.models import Model\n+from keras.models import Model, Sequential\n from keras import backend as K\n from keras.models import model_from_json, model_from_yaml\n from keras.utils.test_utils import keras_test\n \n \n+@keras_test\n+def test_trainable_weights():\n+    a = Input(shape=(2,))\n+    b = Dense(1)(a)\n+    model = Model(a, b)\n+\n+    weights = model.weights\n+    assert model.trainable_weights == weights\n+    assert model.non_trainable_weights == []\n+\n+    model.trainable = False\n+    assert model.trainable_weights == []\n+    assert model.non_trainable_weights == weights\n+\n+    model.trainable = True\n+    assert model.trainable_weights == weights\n+    assert model.non_trainable_weights == []\n+\n+    model.layers[1].trainable = False\n+    assert model.trainable_weights == []\n+    assert model.non_trainable_weights == weights\n+\n+    # sequential model\n+    model = Sequential()\n+    model.add(Dense(1, input_dim=2))\n+    weights = model.weights\n+\n+    assert model.trainable_weights == weights\n+    assert model.non_trainable_weights == []\n+\n+    model.trainable = False\n+    assert model.trainable_weights == []\n+    assert model.non_trainable_weights == weights\n+\n+    model.trainable = True\n+    assert model.trainable_weights == weights\n+    assert model.non_trainable_weights == []\n+\n+    model.layers[0].trainable = False\n+    assert model.trainable_weights == []\n+    assert model.non_trainable_weights == weights\n+\n+\n @keras_test\n def test_learning_phase():\n     a = Input(shape=(32,), name='input_a')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f59736a06c34ae8ee4e6dd710f1f31eb1208a188", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 4320 | Contributors (this commit): 28 | Commits (past 90d): 7 | Contributors (cumulative): 28 | DMM Complexity: None\n\nDIFF:\n@@ -375,7 +375,7 @@ class SimpleRNN(Recurrent):\n         constants = []\n         if 0 < self.dropout_U < 1:\n             ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n-            ones = K.concatenate([ones] * self.output_dim, 1)\n+            ones = K.tile(ones, (1, self.output_dim))\n             B_U = K.in_train_phase(K.dropout(ones, self.dropout_U), ones)\n             constants.append(B_U)\n         else:\n@@ -384,7 +384,7 @@ class SimpleRNN(Recurrent):\n             input_shape = self.input_spec[0].shape\n             input_dim = input_shape[-1]\n             ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n-            ones = K.concatenate([ones] * input_dim, 1)\n+            ones = K.tile(ones, (1, input_dim))\n             B_W = K.in_train_phase(K.dropout(ones, self.dropout_W), ones)\n             constants.append(B_W)\n         else:\n@@ -588,7 +588,7 @@ class GRU(Recurrent):\n         constants = []\n         if 0 < self.dropout_U < 1:\n             ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n-            ones = K.concatenate([ones] * self.output_dim, 1)\n+            ones = K.tile(ones, (1, self.output_dim))\n             B_U = [K.in_train_phase(K.dropout(ones, self.dropout_U), ones) for _ in range(3)]\n             constants.append(B_U)\n         else:\n@@ -598,7 +598,7 @@ class GRU(Recurrent):\n             input_shape = self.input_spec[0].shape\n             input_dim = input_shape[-1]\n             ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n-            ones = K.concatenate([ones] * input_dim, 1)\n+            ones = K.tile(ones, (1, input_dim))\n             B_W = [K.in_train_phase(K.dropout(ones, self.dropout_W), ones) for _ in range(3)]\n             constants.append(B_W)\n         else:\n@@ -828,7 +828,7 @@ class LSTM(Recurrent):\n         constants = []\n         if 0 < self.dropout_U < 1:\n             ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n-            ones = K.concatenate([ones] * self.output_dim, 1)\n+            ones = K.tile(ones, (1, self.output_dim))\n             B_U = [K.in_train_phase(K.dropout(ones, self.dropout_U), ones) for _ in range(4)]\n             constants.append(B_U)\n         else:\n@@ -838,7 +838,7 @@ class LSTM(Recurrent):\n             input_shape = self.input_spec[0].shape\n             input_dim = input_shape[-1]\n             ones = K.ones_like(K.reshape(x[:, 0, 0], (-1, 1)))\n-            ones = K.concatenate([ones] * input_dim, 1)\n+            ones = K.tile(ones, (1, input_dim))\n             B_W = [K.in_train_phase(K.dropout(ones, self.dropout_W), ones) for _ in range(4)]\n             constants.append(B_W)\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a1e9a8addd18c1dd14a8d4867b78cdc109767517", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 136 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -22,9 +22,10 @@ if sys.version_info[0] == 2:\n             count = 0\n             while 1:\n                 chunk = response.read(chunk_size)\n-                if not chunk:\n-                    break\n                 count += 1\n+                if not chunk:\n+                    reporthook(count, total_size, total_size)\n+                    break\n                 if reporthook:\n                     reporthook(count, chunk_size, total_size)\n                 yield chunk\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#519d1e742011258742af664bb2a6db7a805add38", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2553 | Contributors (this commit): 29 | Commits (past 90d): 44 | Contributors (cumulative): 29 | DMM Complexity: None\n\nDIFF:\n@@ -783,7 +783,7 @@ def batch_flatten(x):\n     '''Turn a n-D tensor into a 2D tensor where\n     the first dimension is conserved.\n     '''\n-    x = tf.reshape(x, [-1, prod(shape(x)[1:])])\n+    x = tf.reshape(x, tf.pack([-1, prod(shape(x)[1:])]))\n     return x\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2370f9f1dbf04aeb89e49e4f6a4b09e2e087e84d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 86 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 87 | Churn Cumulative: 4811 | Contributors (this commit): 24 | Commits (past 90d): 14 | Contributors (cumulative): 24 | DMM Complexity: None\n\nDIFF:\n@@ -382,10 +382,95 @@ class Convolution2D(Layer):\n \n class Deconvolution2D(Convolution2D):\n     '''Transposed convolution operator for filtering windows of two-dimensional inputs.\n+    The need for transposed convolutions generally arises from the desire\n+    to use a transformation going in the opposite direction of a normal convolution,\n+    i.e., from something that has the shape of the output of some convolution\n+    to something that has the shape of its input\n+    while maintaining a connectivity pattern that is compatible with said convolution. [1]\n+\n     When using this layer as the first layer in a model,\n     provide the keyword argument `input_shape`\n     (tuple of integers, does not include the sample axis),\n     e.g. `input_shape=(3, 128, 128)` for 128x128 RGB pictures.\n+\n+    # Examples\n+\n+    ```python\n+    # apply a 3x3 transposed convolution with stride 1x1 and 3 output filters on a 12x12 image:\n+    model = Sequential()\n+    model.add(Deconvolution2D(3, 3, 3, output_shape=(None, 3, 14, 14), border_mode='valid', input_shape=(3, 12, 12)))\n+    # output_shape will be (None, 3, 14, 14)\n+\n+    # apply a 3x3 transposed convolution with stride 2x2 and 3 output filters on a 12x12 image:\n+    model = Sequential()\n+    model.add(Deconvolution2D(3, 3, 3, output_shape=(None, 3, 25, 25), subsample=(2, 2), border_mode='valid', input_shape=(3, 12, 12)))\n+    model.summary()\n+    # output_shape will be (None, 3, 25, 25)\n+    ```\n+\n+    # Arguments\n+        nb_filter: Number of transposed convolution filters to use.\n+        nb_row: Number of rows in the transposed convolution kernel.\n+        nb_col: Number of columns in the transposed convolution kernel.\n+        output_shape: Output shape of the transposed convolution operation.\n+            tuple of integers (nb_samples, nb_filter, nb_output_rows, nb_output_cols)\n+            Formula for calculation of the output shape [1], [2]:\n+                o = s (i - 1) + a + k - 2p, \\quad a \\in \\{0, \\ldots, s - 1\\}\n+                where:\n+                    i - input size (rows or cols),\n+                    k - kernel size (nb_filter),\n+                    s - stride (subsample for rows or cols respectively),\n+                    p - padding size,\n+                    a - user-specified quantity used to distinguish between\n+                        the s different possible output sizes.\n+        init: name of initialization function for the weights of the layer\n+            (see [initializations](../initializations.md)), or alternatively,\n+            Theano function to use for weights initialization.\n+            This parameter is only relevant if you don't pass\n+            a `weights` argument.\n+        activation: name of activation function to use\n+            (see [activations](../activations.md)),\n+            or alternatively, elementwise Theano/TensorFlow function.\n+            If you don't specify anything, no activation is applied\n+            (ie. \"linear\" activation: a(x) = x).\n+        weights: list of numpy arrays to set as initial weights.\n+        border_mode: 'valid' or 'same'.\n+        subsample: tuple of length 2. Factor by which to oversample output.\n+            Also called strides elsewhere.\n+        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n+            (eg. L1 or L2 regularization), applied to the main weights matrix.\n+        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n+            applied to the bias.\n+        activity_regularizer: instance of [ActivityRegularizer](../regularizers.md),\n+            applied to the network output.\n+        W_constraint: instance of the [constraints](../constraints.md) module\n+            (eg. maxnorm, nonneg), applied to the main weights matrix.\n+        b_constraint: instance of the [constraints](../constraints.md) module,\n+            applied to the bias.\n+        dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n+            (the depth) is at index 1, in 'tf' mode is it at index 3.\n+            It defaults to the `image_dim_ordering` value found in your\n+            Keras config file at `~/.keras/keras.json`.\n+            If you never set it, then it will be \"th\".\n+        bias: whether to include a bias (i.e. make the layer affine rather than linear).\n+        \n+    # Input shape\n+        4D tensor with shape:\n+        `(samples, channels, rows, cols)` if dim_ordering='th'\n+        or 4D tensor with shape:\n+        `(samples, rows, cols, channels)` if dim_ordering='tf'.\n+        \n+    # Output shape\n+        4D tensor with shape:\n+        `(samples, nb_filter, new_rows, new_cols)` if dim_ordering='th'\n+        or 4D tensor with shape:\n+        `(samples, new_rows, new_cols, nb_filter)` if dim_ordering='tf'.\n+        `rows` and `cols` values might have changed due to padding.\n+\n+    # References\n+        [1] [A guide to convolution arithmetic for deep learning](https://arxiv.org/abs/1603.07285 \"arXiv:1603.07285v1 [stat.ML]\")\n+        [2] [Transposed convolution arithmetic](http://deeplearning.net/software/theano_versions/dev/tutorial/conv_arithmetic.html#transposed-convolution-arithmetic)\n+        [3] [Deconvolutional Networks](http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf)\n     '''\n     def __init__(self, nb_filter, nb_row, nb_col, output_shape,\n                  init='glorot_uniform', activation='linear', weights=None,\n@@ -396,7 +481,7 @@ class Deconvolution2D(Convolution2D):\n                  bias=True, **kwargs):\n \n         if border_mode not in {'valid', 'same'}:\n-            raise Exception('Invalid border mode for AtrousConv2D:', border_mode)\n+            raise Exception('Invalid border mode for Deconvolution2D:', border_mode)\n \n         self.output_shape_ = output_shape\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#045d442fcdd52e923fc3ef68f552eac83d6216a1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 989 | Contributors (this commit): 9 | Commits (past 90d): 5 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -78,7 +78,7 @@ class BaseWrapper(object):\n \n         for params_name in params:\n             if params_name not in legal_params:\n-                assert False, '{} is not a legal parameter'.format(params_name)\n+                raise ValueError('{} is not a legal parameter'.format(params_name))\n \n     def get_params(self, deep=True):\n         '''Get parameters for this estimator.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e418fc6937d96ccf81e021fd35d00fec68c40610", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 7 | Files Changed: 4 | Hunks: 10 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 27 | Churn Cumulative: 7138 | Contributors (this commit): 54 | Commits (past 90d): 93 | Contributors (cumulative): 94 | DMM Complexity: 1.0\n\nDIFF:\n@@ -931,6 +931,10 @@ def batch_set_value(tuples):\n         get_session().run(assign_ops, feed_dict=feed_dict)\n \n \n+def get_variable_shape(x):\n+    return int_shape(x)\n+\n+\n def print_tensor(x, message=''):\n     '''Print the message and the tensor when evaluated and return the same\n     tensor.\n\n@@ -645,6 +645,10 @@ def batch_set_value(tuples):\n         x.set_value(np.asarray(value, dtype=x.dtype))\n \n \n+def get_variable_shape(x):\n+    return x.get_value().shape\n+\n+\n def print_tensor(x, message=''):\n     '''Print the message and the tensor when evaluated and return the same\n     tensor.\n\n@@ -142,7 +142,7 @@ class SGD(Optimizer):\n         self.updates = [K.update_add(self.iterations, 1)]\n \n         # momentum\n-        shapes = [x.shape for x in K.batch_get_value(params)]\n+        shapes = [K.get_variable_shape(p) for p in params]\n         moments = [K.zeros(shape) for shape in shapes]\n         self.weights = [self.iterations] + moments\n         for p, g, m in zip(params, grads, moments):\n@@ -194,7 +194,7 @@ class RMSprop(Optimizer):\n \n     def get_updates(self, params, constraints, loss):\n         grads = self.get_gradients(loss, params)\n-        shapes = [x.shape for x in K.batch_get_value(params)]\n+        shapes = [K.get_variable_shape(p) for p in params]\n         accumulators = [K.zeros(shape) for shape in shapes]\n         self.weights = accumulators\n         self.updates = []\n@@ -240,7 +240,7 @@ class Adagrad(Optimizer):\n \n     def get_updates(self, params, constraints, loss):\n         grads = self.get_gradients(loss, params)\n-        shapes = [x.shape for x in K.batch_get_value(params)]\n+        shapes = [K.get_variable_shape(p) for p in params]\n         accumulators = [K.zeros(shape) for shape in shapes]\n         self.weights = accumulators\n         self.updates = []\n@@ -285,7 +285,7 @@ class Adadelta(Optimizer):\n \n     def get_updates(self, params, constraints, loss):\n         grads = self.get_gradients(loss, params)\n-        shapes = [x.shape for x in K.batch_get_value(params)]\n+        shapes = [K.get_variable_shape(p) for p in params]\n         accumulators = [K.zeros(shape) for shape in shapes]\n         delta_accumulators = [K.zeros(shape) for shape in shapes]\n         self.weights = accumulators + delta_accumulators\n@@ -348,7 +348,7 @@ class Adam(Optimizer):\n         t = self.iterations + 1\n         lr_t = self.lr * K.sqrt(1. - K.pow(self.beta_2, t)) / (1. - K.pow(self.beta_1, t))\n \n-        shapes = [x.shape for x in K.batch_get_value(params)]\n+        shapes = [K.get_variable_shape(p) for p in params]\n         ms = [K.zeros(shape) for shape in shapes]\n         vs = [K.zeros(shape) for shape in shapes]\n         self.weights = [self.iterations] + ms + vs\n@@ -408,7 +408,7 @@ class Adamax(Optimizer):\n         t = self.iterations + 1\n         lr_t = self.lr / (1. - K.pow(self.beta_1, t))\n \n-        shapes = [x.shape for x in K.batch_get_value(params)]\n+        shapes = [K.get_variable_shape(p) for p in params]\n         # zero init of 1st moment\n         ms = [K.zeros(shape) for shape in shapes]\n         # zero init of exponentially weighted infinity norm\n@@ -483,7 +483,7 @@ class Nadam(Optimizer):\n         m_schedule_next = self.m_schedule * momentum_cache_t * momentum_cache_t_1\n         self.updates.append((self.m_schedule, m_schedule_new))\n \n-        shapes = [x.shape for x in K.batch_get_value(params)]\n+        shapes = [K.get_variable_shape(p) for p in params]\n         ms = [K.zeros(shape) for shape in shapes]\n         vs = [K.zeros(shape) for shape in shapes]\n \n\n@@ -160,6 +160,11 @@ class TestBackend(object):\n         check_single_tensor_operation('print_tensor', (4, 3))\n         check_single_tensor_operation('print_tensor', (1, 2, 3))\n \n+        val = np.random.random((3, 2))\n+        xth = KTH.variable(val)\n+        xtf = KTF.variable(val)\n+        assert KTH.get_variable_shape(xth) == KTF.get_variable_shape(xtf)\n+\n     def test_elementwise_operations(self):\n         check_single_tensor_operation('max', (4, 2))\n         check_single_tensor_operation('max', (4, 2), axis=1, keepdims=True)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
