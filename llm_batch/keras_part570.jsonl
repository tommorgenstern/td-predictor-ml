{"custom_id": "keras#50e106b56cd0ce7b94e991d1c24bb48b9b833c03", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 650 | Lines Deleted: 0 | Files Changed: 6 | Hunks: 6 | Methods Changed: 24 | Complexity Δ (Sum/Max): 28/16 | Churn Δ: 650 | Churn Cumulative: 885 | Contributors (this commit): 4 | Commits (past 90d): 24 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,6 +6,14 @@ from keras_core.layers.core.input_layer import InputLayer\n from keras_core.layers.layer import Layer\n from keras_core.layers.merging.add import Add\n from keras_core.layers.merging.add import add\n+from keras_core.layers.merging.average import Average\n+from keras_core.layers.merging.average import average\n+from keras_core.layers.merging.maximum import Maximum\n+from keras_core.layers.merging.maximum import maximum\n+from keras_core.layers.merging.minimum import Minimum\n+from keras_core.layers.merging.minimum import minimum\n+from keras_core.layers.merging.multiply import Multiply\n+from keras_core.layers.merging.multiply import multiply\n from keras_core.layers.merging.subtract import Subtract\n from keras_core.layers.merging.subtract import subtract\n from keras_core.layers.pooling.average_pooling1d import AveragePooling1D\n\n@@ -0,0 +1,69 @@\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.merging.base_merge import Merge\n+\n+\n+@keras_core_export(\"keras_core.layers.Average\")\n+class Average(Merge):\n+    \"\"\"Averages a list of inputs element-wise..\n+\n+    It takes as input a list of tensors, all of the same shape,\n+    and returns a single tensor (also of the same shape).\n+\n+    Examples:\n+\n+    >>> input_shape = (2, 3, 4)\n+    >>> x1 = np.random.rand(*input_shape)\n+    >>> x2 = np.random.rand(*input_shape)\n+    >>> y = keras_core.layers.Average()([x1, x2])\n+\n+    Usage in a Keras model:\n+\n+    >>> input1 = keras_core.layers.Input(shape=(16,))\n+    >>> x1 = keras_core.layers.Dense(8, activation='relu')(input1)\n+    >>> input2 = keras_core.layers.Input(shape=(32,))\n+    >>> x2 = keras_core.layers.Dense(8, activation='relu')(input2)\n+    >>> # equivalent to `y = keras_core.layers.average([x1, x2])`\n+    >>> y = keras_core.layers.Average()([x1, x2])\n+    >>> out = keras_core.layers.Dense(4)(y)\n+    >>> model = keras_core.models.Model(inputs=[input1, input2], outputs=out)\n+\n+    \"\"\"\n+\n+    def _merge_function(self, inputs):\n+        output = inputs[0]\n+        for i in range(1, len(inputs)):\n+            output += inputs[i]\n+        return output / len(inputs)\n+\n+\n+@keras_core_export(\"keras_core.layers.average\")\n+def average(inputs, **kwargs):\n+    \"\"\"Functional interface to the `keras_core.layers.Average` layer.\n+\n+    Args:\n+        inputs: A list of input tensors , all of the same shape.\n+        **kwargs: Standard layer keyword arguments.\n+\n+    Returns:\n+        A tensor as the element-wise product of the inputs with the same\n+        shape as the inputs.\n+\n+    Examples:\n+\n+    >>> input_shape = (2, 3, 4)\n+    >>> x1 = np.random.rand(*input_shape)\n+    >>> x2 = np.random.rand(*input_shape)\n+    >>> y = keras_core.layers.average([x1, x2])\n+\n+    Usage in a Keras model:\n+\n+    >>> input1 = keras_core.layers.Input(shape=(16,))\n+    >>> x1 = keras_core.layers.Dense(8, activation='relu')(input1)\n+    >>> input2 = keras_core.layers.Input(shape=(32,))\n+    >>> x2 = keras_core.layers.Dense(8, activation='relu')(input2)\n+    >>> y = keras_core.layers.average([x1, x2])\n+    >>> out = keras_core.layers.Dense(4)(y)\n+    >>> model = keras_core.models.Model(inputs=[input1, input2], outputs=out)\n+\n+    \"\"\"\n+    return Average(**kwargs)(inputs)\n\n@@ -0,0 +1,70 @@\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.merging.base_merge import Merge\n+\n+\n+@keras_core_export(\"keras_core.layers.Maximum\")\n+class Maximum(Merge):\n+    \"\"\"Computes element-wise maximum on a list of inputs.\n+\n+    It takes as input a list of tensors, all of the same shape,\n+    and returns a single tensor (also of the same shape).\n+\n+    Examples:\n+\n+    >>> input_shape = (2, 3, 4)\n+    >>> x1 = np.random.rand(*input_shape)\n+    >>> x2 = np.random.rand(*input_shape)\n+    >>> y = keras_core.layers.Maximum()([x1, x2])\n+\n+    Usage in a Keras model:\n+\n+    >>> input1 = keras_core.layers.Input(shape=(16,))\n+    >>> x1 = keras_core.layers.Dense(8, activation='relu')(input1)\n+    >>> input2 = keras_core.layers.Input(shape=(32,))\n+    >>> x2 = keras_core.layers.Dense(8, activation='relu')(input2)\n+    >>> # equivalent to `y = keras_core.layers.maximum([x1, x2])`\n+    >>> y = keras_core.layers.Maximum()([x1, x2])\n+    >>> out = keras_core.layers.Dense(4)(y)\n+    >>> model = keras_core.models.Model(inputs=[input1, input2], outputs=out)\n+\n+    \"\"\"\n+\n+    def _merge_function(self, inputs):\n+        output = inputs[0]\n+        for i in range(1, len(inputs)):\n+            output = ops.maximum(output, inputs[i])\n+        return output\n+\n+\n+@keras_core_export(\"keras_core.layers.maximum\")\n+def maximum(inputs, **kwargs):\n+    \"\"\"Functional interface to the `keras_core.layers.Maximum` layer.\n+\n+    Args:\n+        inputs: A list of input tensors , all of the same shape.\n+        **kwargs: Standard layer keyword arguments.\n+\n+    Returns:\n+        A tensor as the element-wise product of the inputs with the same\n+        shape as the inputs.\n+\n+    Examples:\n+\n+    >>> input_shape = (2, 3, 4)\n+    >>> x1 = np.random.rand(*input_shape)\n+    >>> x2 = np.random.rand(*input_shape)\n+    >>> y = keras_core.layers.maximum([x1, x2])\n+\n+    Usage in a Keras model:\n+\n+    >>> input1 = keras_core.layers.Input(shape=(16,))\n+    >>> x1 = keras_core.layers.Dense(8, activation='relu')(input1)\n+    >>> input2 = keras_core.layers.Input(shape=(32,))\n+    >>> x2 = keras_core.layers.Dense(8, activation='relu')(input2)\n+    >>> y = keras_core.layers.maximum([x1, x2])\n+    >>> out = keras_core.layers.Dense(4)(y)\n+    >>> model = keras_core.models.Model(inputs=[input1, input2], outputs=out)\n+\n+    \"\"\"\n+    return Maximum(**kwargs)(inputs)\n\n@@ -199,3 +199,367 @@ class MergingLayersTest(testing.TestCase):\n             ValueError, \"layer should be called on exactly 2 inputs\"\n         ):\n             layers.Subtract()([input_1])\n+\n+    def test_minimum_basic(self):\n+        self.run_layer_test(\n+            layers.Minimum,\n+            init_kwargs={},\n+            input_shape=[(2, 3), (2, 3)],\n+            expected_output_shape=(2, 3),\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=True,\n+        )\n+\n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes.\",\n+    )\n+    def test_minimum_correctness_dynamic(self):\n+        x1 = np.random.rand(2, 4, 5)\n+        x2 = np.random.rand(2, 4, 5)\n+        x3 = ops.minimum(x1, x2)\n+\n+        input_1 = layers.Input(shape=(4, 5))\n+        input_2 = layers.Input(shape=(4, 5))\n+        merge_layer = layers.Minimum()\n+        out = merge_layer([input_1, input_2])\n+        model = models.Model([input_1, input_2], out)\n+        res = model([x1, x2])\n+\n+        self.assertEqual(res.shape, (2, 4, 5))\n+        self.assertAllClose(res, x3, atol=1e-4)\n+        self.assertIsNone(\n+            merge_layer.compute_mask([input_1, input_2], [None, None])\n+        )\n+        self.assertTrue(\n+            np.all(\n+                merge_layer.compute_mask(\n+                    [input_1, input_2],\n+                    [backend.Variable(x1), backend.Variable(x2)],\n+                )\n+            )\n+        )\n+\n+    def test_minimum_correctness_static(self):\n+        batch_size = 2\n+        shape = (4, 5)\n+        x1 = np.random.rand(batch_size, *shape)\n+        x2 = np.random.rand(batch_size, *shape)\n+        x3 = ops.minimum(x1, x2)\n+\n+        input_1 = layers.Input(shape=shape, batch_size=batch_size)\n+        input_2 = layers.Input(shape=shape, batch_size=batch_size)\n+        merge_layer = layers.Minimum()\n+        out = merge_layer([input_1, input_2])\n+        model = models.Model([input_1, input_2], out)\n+        res = model([x1, x2])\n+\n+        self.assertEqual(res.shape, (batch_size, *shape))\n+        self.assertAllClose(res, x3, atol=1e-4)\n+        self.assertIsNone(\n+            merge_layer.compute_mask([input_1, input_2], [None, None])\n+        )\n+        self.assertTrue(\n+            np.all(\n+                merge_layer.compute_mask(\n+                    [input_1, input_2],\n+                    [backend.Variable(x1), backend.Variable(x2)],\n+                )\n+            )\n+        )\n+\n+    def test_minimum_errors(self):\n+        batch_size = 2\n+        shape = (4, 5)\n+        x1 = np.random.rand(batch_size, *shape)\n+\n+        input_1 = layers.Input(shape=shape, batch_size=batch_size)\n+        input_2 = layers.Input(shape=shape, batch_size=batch_size)\n+        merge_layer = layers.Minimum()\n+\n+        with self.assertRaisesRegex(ValueError, \"`mask` should be a list.\"):\n+            merge_layer.compute_mask([input_1, input_2], x1)\n+\n+        with self.assertRaisesRegex(ValueError, \"`inputs` should be a list.\"):\n+            merge_layer.compute_mask(input_1, [None, None])\n+\n+        with self.assertRaisesRegex(\n+            ValueError, \" should have the same length.\"\n+        ):\n+            merge_layer.compute_mask([input_1, input_2], [None])\n+\n+    def test_maximum_basic(self):\n+        self.run_layer_test(\n+            layers.Maximum,\n+            init_kwargs={},\n+            input_shape=[(2, 3), (2, 3)],\n+            expected_output_shape=(2, 3),\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=True,\n+        )\n+\n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes.\",\n+    )\n+    def test_maximum_correctness_dynamic(self):\n+        x1 = np.random.rand(2, 4, 5)\n+        x2 = np.random.rand(2, 4, 5)\n+        x3 = ops.maximum(x1, x2)\n+\n+        input_1 = layers.Input(shape=(4, 5))\n+        input_2 = layers.Input(shape=(4, 5))\n+        merge_layer = layers.Maximum()\n+        out = merge_layer([input_1, input_2])\n+        model = models.Model([input_1, input_2], out)\n+        res = model([x1, x2])\n+\n+        self.assertEqual(res.shape, (2, 4, 5))\n+        self.assertAllClose(res, x3, atol=1e-4)\n+        self.assertIsNone(\n+            merge_layer.compute_mask([input_1, input_2], [None, None])\n+        )\n+        self.assertTrue(\n+            np.all(\n+                merge_layer.compute_mask(\n+                    [input_1, input_2],\n+                    [backend.Variable(x1), backend.Variable(x2)],\n+                )\n+            )\n+        )\n+\n+    def test_maximum_correctness_static(self):\n+        batch_size = 2\n+        shape = (4, 5)\n+        x1 = np.random.rand(batch_size, *shape)\n+        x2 = np.random.rand(batch_size, *shape)\n+        x3 = ops.maximum(x1, x2)\n+\n+        input_1 = layers.Input(shape=shape, batch_size=batch_size)\n+        input_2 = layers.Input(shape=shape, batch_size=batch_size)\n+        merge_layer = layers.Maximum()\n+        out = merge_layer([input_1, input_2])\n+        model = models.Model([input_1, input_2], out)\n+        res = model([x1, x2])\n+\n+        self.assertEqual(res.shape, (batch_size, *shape))\n+        self.assertAllClose(res, x3, atol=1e-4)\n+        self.assertIsNone(\n+            merge_layer.compute_mask([input_1, input_2], [None, None])\n+        )\n+        self.assertTrue(\n+            np.all(\n+                merge_layer.compute_mask(\n+                    [input_1, input_2],\n+                    [backend.Variable(x1), backend.Variable(x2)],\n+                )\n+            )\n+        )\n+\n+    def test_maximum_errors(self):\n+        batch_size = 2\n+        shape = (4, 5)\n+        x1 = np.random.rand(batch_size, *shape)\n+\n+        input_1 = layers.Input(shape=shape, batch_size=batch_size)\n+        input_2 = layers.Input(shape=shape, batch_size=batch_size)\n+        merge_layer = layers.Maximum()\n+\n+        with self.assertRaisesRegex(ValueError, \"`mask` should be a list.\"):\n+            merge_layer.compute_mask([input_1, input_2], x1)\n+\n+        with self.assertRaisesRegex(ValueError, \"`inputs` should be a list.\"):\n+            merge_layer.compute_mask(input_1, [None, None])\n+\n+        with self.assertRaisesRegex(\n+            ValueError, \" should have the same length.\"\n+        ):\n+            merge_layer.compute_mask([input_1, input_2], [None])\n+\n+    def test_multiply_basic(self):\n+        self.run_layer_test(\n+            layers.Multiply,\n+            init_kwargs={},\n+            input_shape=[(2, 3), (2, 3)],\n+            expected_output_shape=(2, 3),\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=True,\n+        )\n+\n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes.\",\n+    )\n+    def test_multiply_correctness_dynamic(self):\n+        x1 = np.random.rand(2, 4, 5)\n+        x2 = np.random.rand(2, 4, 5)\n+        x3 = ops.convert_to_tensor(x1 * x2)\n+\n+        input_1 = layers.Input(shape=(4, 5))\n+        input_2 = layers.Input(shape=(4, 5))\n+        merge_layer = layers.Multiply()\n+        out = merge_layer([input_1, input_2])\n+        model = models.Model([input_1, input_2], out)\n+        res = model([x1, x2])\n+\n+        self.assertEqual(res.shape, (2, 4, 5))\n+        self.assertAllClose(res, x3, atol=1e-4)\n+        self.assertIsNone(\n+            merge_layer.compute_mask([input_1, input_2], [None, None])\n+        )\n+        self.assertTrue(\n+            np.all(\n+                merge_layer.compute_mask(\n+                    [input_1, input_2],\n+                    [backend.Variable(x1), backend.Variable(x2)],\n+                )\n+            )\n+        )\n+\n+    def test_multiply_correctness_static(self):\n+        batch_size = 2\n+        shape = (4, 5)\n+        x1 = np.random.rand(batch_size, *shape)\n+        x2 = np.random.rand(batch_size, *shape)\n+        x3 = ops.convert_to_tensor(x1 * x2)\n+\n+        input_1 = layers.Input(shape=shape, batch_size=batch_size)\n+        input_2 = layers.Input(shape=shape, batch_size=batch_size)\n+        merge_layer = layers.Multiply()\n+        out = merge_layer([input_1, input_2])\n+        model = models.Model([input_1, input_2], out)\n+        res = model([x1, x2])\n+\n+        self.assertEqual(res.shape, (batch_size, *shape))\n+        self.assertAllClose(res, x3, atol=1e-4)\n+        self.assertIsNone(\n+            merge_layer.compute_mask([input_1, input_2], [None, None])\n+        )\n+        self.assertTrue(\n+            np.all(\n+                merge_layer.compute_mask(\n+                    [input_1, input_2],\n+                    [backend.Variable(x1), backend.Variable(x2)],\n+                )\n+            )\n+        )\n+\n+    def test_multiply_errors(self):\n+        batch_size = 2\n+        shape = (4, 5)\n+        x1 = np.random.rand(batch_size, *shape)\n+\n+        input_1 = layers.Input(shape=shape, batch_size=batch_size)\n+        input_2 = layers.Input(shape=shape, batch_size=batch_size)\n+        merge_layer = layers.Multiply()\n+\n+        with self.assertRaisesRegex(ValueError, \"`mask` should be a list.\"):\n+            merge_layer.compute_mask([input_1, input_2], x1)\n+\n+        with self.assertRaisesRegex(ValueError, \"`inputs` should be a list.\"):\n+            merge_layer.compute_mask(input_1, [None, None])\n+\n+        with self.assertRaisesRegex(\n+            ValueError, \" should have the same length.\"\n+        ):\n+            merge_layer.compute_mask([input_1, input_2], [None])\n+\n+    def test_average_basic(self):\n+        self.run_layer_test(\n+            layers.Average,\n+            init_kwargs={},\n+            input_shape=[(2, 3), (2, 3)],\n+            expected_output_shape=(2, 3),\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=True,\n+        )\n+\n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes.\",\n+    )\n+    def test_average_correctness_dynamic(self):\n+        x1 = np.random.rand(2, 4, 5)\n+        x2 = np.random.rand(2, 4, 5)\n+        x3 = ops.average(np.array([x1, x2]), axis=0)\n+\n+        input_1 = layers.Input(shape=(4, 5))\n+        input_2 = layers.Input(shape=(4, 5))\n+        merge_layer = layers.Average()\n+        out = merge_layer([input_1, input_2])\n+        model = models.Model([input_1, input_2], out)\n+        res = model([x1, x2])\n+\n+        self.assertEqual(res.shape, (2, 4, 5))\n+        self.assertAllClose(res, x3, atol=1e-4)\n+        self.assertIsNone(\n+            merge_layer.compute_mask([input_1, input_2], [None, None])\n+        )\n+        self.assertTrue(\n+            np.all(\n+                merge_layer.compute_mask(\n+                    [input_1, input_2],\n+                    [backend.Variable(x1), backend.Variable(x2)],\n+                )\n+            )\n+        )\n+\n+    def test_average_correctness_static(self):\n+        batch_size = 2\n+        shape = (4, 5)\n+        x1 = np.random.rand(batch_size, *shape)\n+        x2 = np.random.rand(batch_size, *shape)\n+        x3 = ops.average(np.array([x1, x2]), axis=0)\n+\n+        input_1 = layers.Input(shape=shape, batch_size=batch_size)\n+        input_2 = layers.Input(shape=shape, batch_size=batch_size)\n+        merge_layer = layers.Average()\n+        out = merge_layer([input_1, input_2])\n+        model = models.Model([input_1, input_2], out)\n+        res = model([x1, x2])\n+\n+        self.assertEqual(res.shape, (batch_size, *shape))\n+        self.assertAllClose(res, x3, atol=1e-4)\n+        self.assertIsNone(\n+            merge_layer.compute_mask([input_1, input_2], [None, None])\n+        )\n+        self.assertTrue(\n+            np.all(\n+                merge_layer.compute_mask(\n+                    [input_1, input_2],\n+                    [backend.Variable(x1), backend.Variable(x2)],\n+                )\n+            )\n+        )\n+\n+    def test_average_errors(self):\n+        batch_size = 2\n+        shape = (4, 5)\n+        x1 = np.random.rand(batch_size, *shape)\n+\n+        input_1 = layers.Input(shape=shape, batch_size=batch_size)\n+        input_2 = layers.Input(shape=shape, batch_size=batch_size)\n+        merge_layer = layers.Average()\n+\n+        with self.assertRaisesRegex(ValueError, \"`mask` should be a list.\"):\n+            merge_layer.compute_mask([input_1, input_2], x1)\n+\n+        with self.assertRaisesRegex(ValueError, \"`inputs` should be a list.\"):\n+            merge_layer.compute_mask(input_1, [None, None])\n+\n+        with self.assertRaisesRegex(\n+            ValueError, \" should have the same length.\"\n+        ):\n+            merge_layer.compute_mask([input_1, input_2], [None])\n\n@@ -0,0 +1,70 @@\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.merging.base_merge import Merge\n+\n+\n+@keras_core_export(\"keras_core.layers.Minimum\")\n+class Minimum(Merge):\n+    \"\"\"Computes elementwise minimum on a list of inputs.\n+\n+    It takes as input a list of tensors, all of the same shape,\n+    and returns a single tensor (also of the same shape).\n+\n+    Examples:\n+\n+    >>> input_shape = (2, 3, 4)\n+    >>> x1 = np.random.rand(*input_shape)\n+    >>> x2 = np.random.rand(*input_shape)\n+    >>> y = keras_core.layers.Minimum()([x1, x2])\n+\n+    Usage in a Keras model:\n+\n+    >>> input1 = keras_core.layers.Input(shape=(16,))\n+    >>> x1 = keras_core.layers.Dense(8, activation='relu')(input1)\n+    >>> input2 = keras_core.layers.Input(shape=(32,))\n+    >>> x2 = keras_core.layers.Dense(8, activation='relu')(input2)\n+    >>> # equivalent to `y = keras_core.layers.minimum([x1, x2])`\n+    >>> y = keras_core.layers.Minimum()([x1, x2])\n+    >>> out = keras_core.layers.Dense(4)(y)\n+    >>> model = keras_core.models.Model(inputs=[input1, input2], outputs=out)\n+\n+    \"\"\"\n+\n+    def _merge_function(self, inputs):\n+        output = inputs[0]\n+        for i in range(1, len(inputs)):\n+            output = ops.minimum(output, inputs[i])\n+        return output\n+\n+\n+@keras_core_export(\"keras_core.layers.minimum\")\n+def minimum(inputs, **kwargs):\n+    \"\"\"Functional interface to the `keras_core.layers.Minimum` layer.\n+\n+    Args:\n+        inputs: A list of input tensors , all of the same shape.\n+        **kwargs: Standard layer keyword arguments.\n+\n+    Returns:\n+        A tensor as the elementwise product of the inputs with the same\n+        shape as the inputs.\n+\n+    Examples:\n+\n+    >>> input_shape = (2, 3, 4)\n+    >>> x1 = np.random.rand(*input_shape)\n+    >>> x2 = np.random.rand(*input_shape)\n+    >>> y = keras_core.layers.minimum([x1, x2])\n+\n+    Usage in a Keras model:\n+\n+    >>> input1 = keras_core.layers.Input(shape=(16,))\n+    >>> x1 = keras_core.layers.Dense(8, activation='relu')(input1)\n+    >>> input2 = keras_core.layers.Input(shape=(32,))\n+    >>> x2 = keras_core.layers.Dense(8, activation='relu')(input2)\n+    >>> y = keras_core.layers.minimum([x1, x2])\n+    >>> out = keras_core.layers.Dense(4)(y)\n+    >>> model = keras_core.models.Model(inputs=[input1, input2], outputs=out)\n+\n+    \"\"\"\n+    return Minimum(**kwargs)(inputs)\n\n@@ -0,0 +1,69 @@\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.merging.base_merge import Merge\n+\n+\n+@keras_core_export(\"keras_core.layers.Multiply\")\n+class Multiply(Merge):\n+    \"\"\"Performs elementwise multiplication.\n+\n+    It takes as input a list of tensors, all of the same shape,\n+    and returns a single tensor (also of the same shape).\n+\n+    Examples:\n+\n+    >>> input_shape = (2, 3, 4)\n+    >>> x1 = np.random.rand(*input_shape)\n+    >>> x2 = np.random.rand(*input_shape)\n+    >>> y = keras_core.layers.Multiply()([x1, x2])\n+\n+    Usage in a Keras model:\n+\n+    >>> input1 = keras_core.layers.Input(shape=(16,))\n+    >>> x1 = keras_core.layers.Dense(8, activation='relu')(input1)\n+    >>> input2 = keras_core.layers.Input(shape=(32,))\n+    >>> x2 = keras_core.layers.Dense(8, activation='relu')(input2)\n+    >>> # equivalent to `y = keras_core.layers.multiply([x1, x2])`\n+    >>> y = keras_core.layers.Multiply()([x1, x2])\n+    >>> out = keras_core.layers.Dense(4)(y)\n+    >>> model = keras_core.models.Model(inputs=[input1, input2], outputs=out)\n+\n+    \"\"\"\n+\n+    def _merge_function(self, inputs):\n+        output = inputs[0]\n+        for i in range(1, len(inputs)):\n+            output = output * inputs[i]\n+        return output\n+\n+\n+@keras_core_export(\"keras_core.layers.multiply\")\n+def multiply(inputs, **kwargs):\n+    \"\"\"Functional interface to the `keras_core.layers.Multiply` layer.\n+\n+    Args:\n+        inputs: A list of input tensors , all of the same shape.\n+        **kwargs: Standard layer keyword arguments.\n+\n+    Returns:\n+        A tensor as the elementwise product of the inputs with the same\n+        shape as the inputs.\n+\n+    Examples:\n+\n+    >>> input_shape = (2, 3, 4)\n+    >>> x1 = np.random.rand(*input_shape)\n+    >>> x2 = np.random.rand(*input_shape)\n+    >>> y = keras_core.layers.multiply([x1, x2])\n+\n+    Usage in a Keras model:\n+\n+    >>> input1 = keras_core.layers.Input(shape=(16,))\n+    >>> x1 = keras_core.layers.Dense(8, activation='relu')(input1)\n+    >>> input2 = keras_core.layers.Input(shape=(32,))\n+    >>> x2 = keras_core.layers.Dense(8, activation='relu')(input2)\n+    >>> y = keras_core.layers.multiply([x1, x2])\n+    >>> out = keras_core.layers.Dense(4)(y)\n+    >>> model = keras_core.models.Model(inputs=[input1, input2], outputs=out)\n+\n+    \"\"\"\n+    return Multiply(**kwargs)(inputs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a85647b3ec2841acdc461141c6e2ce0fa94b33ab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 788 | Lines Deleted: 0 | Files Changed: 10 | Hunks: 10 | Methods Changed: 29 | Complexity Δ (Sum/Max): 42/8 | Churn Δ: 788 | Churn Cumulative: 828 | Contributors (this commit): 4 | Commits (past 90d): 27 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -19,6 +19,18 @@ from keras_core.layers.merging.subtract import subtract\n from keras_core.layers.pooling.average_pooling1d import AveragePooling1D\n from keras_core.layers.pooling.average_pooling2d import AveragePooling2D\n from keras_core.layers.pooling.average_pooling3d import AveragePooling3D\n+from keras_core.layers.pooling.global_average_pooling1d import (\n+    GlobalAveragePooling1D,\n+)\n+from keras_core.layers.pooling.global_average_pooling2d import (\n+    GlobalAveragePooling2D,\n+)\n+from keras_core.layers.pooling.global_average_pooling3d import (\n+    GlobalAveragePooling3D,\n+)\n+from keras_core.layers.pooling.global_max_pooling1d import GlobalMaxPooling1D\n+from keras_core.layers.pooling.global_max_pooling2d import GlobalMaxPooling2D\n+from keras_core.layers.pooling.global_max_pooling3d import GlobalMaxPooling3D\n from keras_core.layers.pooling.max_pooling1d import MaxPooling1D\n from keras_core.layers.pooling.max_pooling2d import MaxPooling2D\n from keras_core.layers.pooling.max_pooling3d import MaxPooling3D\n\n@@ -0,0 +1,50 @@\n+from keras_core.backend import image_data_format\n+from keras_core.layers.input_spec import InputSpec\n+from keras_core.layers.layer import Layer\n+\n+\n+class BaseGlobalPooling(Layer):\n+    \"\"\"Base global pooling layer.\"\"\"\n+\n+    def __init__(\n+        self, pool_dimensions, data_format=None, keepdims=False, **kwargs\n+    ):\n+        super().__init__(**kwargs)\n+\n+        self.data_format = (\n+            image_data_format() if data_format is None else data_format\n+        )\n+        self.keepdims = keepdims\n+        self.input_spec = InputSpec(ndim=pool_dimensions + 2)\n+\n+    def call(self, inputs):\n+        raise NotImplementedError\n+\n+    def compute_output_shape(self, input_shape):\n+        num_spatial_dims = len(input_shape) - 2\n+        if self.data_format == \"channels_last\":\n+            if self.keepdims:\n+                return (\n+                    (input_shape[0],)\n+                    + (1,) * num_spatial_dims\n+                    + (input_shape[-1],)\n+                )\n+            else:\n+                return (input_shape[0],) + (input_shape[-1],)\n+        else:\n+            if self.keepdims:\n+                return (input_shape[0], input_shape[1]) + (\n+                    1,\n+                ) * num_spatial_dims\n+            else:\n+                return (input_shape[0], input_shape[1])\n+\n+    def get_config(self):\n+        config = super().get_config()\n+        config.update(\n+            {\n+                \"data_format\": self.data_format,\n+                \"keepdims\": self.keepdims,\n+            }\n+        )\n+        return config\n\n@@ -0,0 +1,84 @@\n+from keras_core import operations as ops\n+from keras_core import backend\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.pooling.base_global_pooling import BaseGlobalPooling\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.layers.GlobalAveragePooling1D\",\n+        \"keras_core.layers.GlobalAvgPool1D\",\n+    ]\n+)\n+class GlobalAveragePooling1D(BaseGlobalPooling):\n+    \"\"\"Global average pooling operation for temporal data.\n+\n+    Args:\n+        data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n+            The ordering of the dimensions in the inputs. `\"channels_last\"`\n+            corresponds to inputs with shape `(batch, steps, features)`\n+            while `\"channels_first\"` corresponds to inputs with shape\n+            `(batch, features, steps)`. It defaults to the `image_data_format`\n+            value found in your Keras config file at `~/.keras/keras.json`.\n+            If you never set it, then it will be `\"channels_last\"`.\n+        keepdims: A boolean, whether to keep the temporal dimension or not.\n+            If `keepdims` is `False` (default), the rank of the tensor is\n+            reduced for spatial dimensions. If `keepdims` is `True`, the\n+            temporal dimension are retained with length 1.\n+            The behavior is the same as for `tf.reduce_mean` or `np.mean`.\n+\n+    Call arguments:\n+        inputs: A 3D tensor.\n+        mask: Binary tensor of shape `(batch_size, steps)` indicating whether\n+            a given step should be masked (excluded from the average).\n+\n+    Input shape:\n+    - If `data_format='channels_last'`:\n+        3D tensor with shape:\n+        `(batch_size, steps, features)`\n+    - If `data_format='channels_first'`:\n+        3D tensor with shape:\n+        `(batch_size, features, steps)`\n+\n+    Output shape:\n+    - If `keepdims`=False:\n+        2D tensor with shape `(batch_size, features)`.\n+    - If `keepdims`=True:\n+        - If `data_format=\"channels_last\"`:\n+            3D tensor with shape `(batch_size, 1, features)`\n+        - If `data_format=\"channels_first\"`:\n+            3D tensor with shape `(batch_size, features, 1)`\n+\n+    Examples:\n+\n+    >>> x = np.random.rand(2, 3, 4)\n+    >>> y = keras_core.layers.GlobalAveragePooling1D()(x)\n+    >>> print(y.shape)\n+    (2, 4)\n+    \"\"\"\n+\n+    def __init__(self, data_format=None, keepdims=False, **kwargs):\n+        super().__init__(\n+            pool_dimensions=1,\n+            data_format=data_format,\n+            keepdims=keepdims,\n+            **kwargs,\n+        )\n+        self.supports_masking = True\n+\n+    def call(self, inputs, mask=None):\n+        steps_axis = 1 if self.data_format == \"channels_last\" else 2\n+        if mask is not None:\n+            mask = backend.cast(mask, inputs[0].dtype)\n+            mask = ops.expand_dims(\n+                mask, 2 if self.data_format == \"channels_last\" else 1\n+            )\n+            inputs *= mask\n+            return ops.sum(\n+                inputs, axis=steps_axis, keepdims=self.keepdims\n+            ) / ops.sum(mask, axis=steps_axis, keepdims=self.keepdims)\n+        else:\n+            return ops.mean(inputs, axis=steps_axis, keepdims=self.keepdims)\n+        \n+    def compute_mask(self, inputs, mask=None):\n+        return None\n\n@@ -0,0 +1,66 @@\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.pooling.base_global_pooling import BaseGlobalPooling\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.layers.GlobalAveragePooling2D\",\n+        \"keras_core.layers.GlobalAvgPool2D\",\n+    ]\n+)\n+class GlobalAveragePooling2D(BaseGlobalPooling):\n+    \"\"\"Global average pooling operation for 2D data.\n+\n+    Args:\n+        data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n+            The ordering of the dimensions in the inputs. `\"channels_last\"`\n+            corresponds to inputs with shape `(batch, height, width, channels)`\n+            while `\"channels_first\"` corresponds to inputs with shape\n+            `(batch, features, height, weight)`. It defaults to the\n+            `image_data_format` value found in your Keras config file at\n+            `~/.keras/keras.json`. If you never set it, then it will be\n+            `\"channels_last\"`.\n+        keepdims: A boolean, whether to keep the temporal dimension or not.\n+            If `keepdims` is `False` (default), the rank of the tensor is\n+            reduced for spatial dimensions. If `keepdims` is `True`, the\n+            spatial dimension are retained with length 1.\n+            The behavior is the same as for `tf.reduce_mean` or `np.mean`.\n+\n+    Input shape:\n+    - If `data_format='channels_last'`:\n+        4D tensor with shape:\n+        `(batch_size, height, width, channels)`\n+    - If `data_format='channels_first'`:\n+        4D tensor with shape:\n+        `(batch_size, channels, height, width)`\n+\n+    Output shape:\n+    - If `keepdims`=False:\n+        2D tensor with shape `(batch_size, channels)`.\n+    - If `keepdims`=True:\n+        - If `data_format=\"channels_last\"`:\n+            4D tensor with shape `(batch_size, 1, 1, channels)`\n+        - If `data_format=\"channels_first\"`:\n+            4D tensor with shape `(batch_size, channels, 1, 1)`\n+\n+    Examples:\n+\n+    >>> x = np.random.rand(2, 4, 5, 3)\n+    >>> y = keras_core.layers.GlobalAveragePooling2D()(x)\n+    >>> print(y.shape)\n+    (2, 3)\n+    \"\"\"\n+\n+    def __init__(self, data_format=None, keepdims=False, **kwargs):\n+        super().__init__(\n+            pool_dimensions=2,\n+            data_format=data_format,\n+            keepdims=keepdims,\n+            **kwargs,\n+        )\n+\n+    def call(self, inputs):\n+        if self.data_format == \"channels_last\":\n+            return ops.mean(inputs, axis=[1, 2], keepdims=self.keepdims)\n+        return ops.mean(inputs, axis=[2, 3], keepdims=self.keepdims)\n\n@@ -0,0 +1,67 @@\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.pooling.base_global_pooling import BaseGlobalPooling\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.layers.GlobalAveragePooling3D\",\n+        \"keras_core.layers.GlobalAvgPool3D\",\n+    ]\n+)\n+class GlobalAveragePooling3D(BaseGlobalPooling):\n+    \"\"\"Global average pooling operation for 3D data.\n+\n+    Args:\n+        data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n+            The ordering of the dimensions in the inputs. `\"channels_last\"`\n+            corresponds to inputs with shape\n+            `(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n+            while `\"channels_first\"` corresponds to inputs with shape\n+            `(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n+            It defaults to the `image_data_format` value found in your Keras\n+            config file at `~/.keras/keras.json`. If you never set it, then it\n+            will be `\"channels_last\"`.\n+        keepdims: A boolean, whether to keep the temporal dimension or not.\n+            If `keepdims` is `False` (default), the rank of the tensor is\n+            reduced for spatial dimensions. If `keepdims` is `True`, the\n+            spatial dimension are retained with length 1.\n+            The behavior is the same as for `tf.reduce_mean` or `np.mean`.\n+\n+    Input shape:\n+    - If `data_format='channels_last'`:\n+        5D tensor with shape:\n+        `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n+    - If `data_format='channels_first'`:\n+        5D tensor with shape:\n+        `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`\n+\n+    Output shape:\n+    - If `keepdims`=False:\n+        2D tensor with shape `(batch_size, channels)`.\n+    - If `keepdims`=True:\n+        - If `data_format=\"channels_last\"`:\n+            5D tensor with shape `(batch_size, 1, 1, 1, channels)`\n+        - If `data_format=\"channels_first\"`:\n+            5D tensor with shape `(batch_size, channels, 1, 1, 1)`\n+\n+    Examples:\n+\n+    >>> x = np.random.rand(2, 4, 5, 4, 3)\n+    >>> y = keras_core.layers.GlobalAveragePooling3D()(x)\n+    >>> print(y.shape)\n+    (2, 3)\n+    \"\"\"\n+\n+    def __init__(self, data_format=None, keepdims=False, **kwargs):\n+        super().__init__(\n+            pool_dimensions=3,\n+            data_format=data_format,\n+            keepdims=keepdims,\n+            **kwargs,\n+        )\n+\n+    def call(self, inputs):\n+        if self.data_format == \"channels_last\":\n+            return ops.mean(inputs, axis=[1, 2, 3], keepdims=self.keepdims)\n+        return ops.mean(inputs, axis=[2, 3, 4], keepdims=self.keepdims)\n\n@@ -0,0 +1,161 @@\n+import numpy as np\n+import tensorflow as tf\n+from absl.testing import parameterized\n+\n+from keras_core import layers\n+from keras_core import testing\n+\n+\n+class GlobalAveragePoolingBasicTest(testing.TestCase, parameterized.TestCase):\n+    @parameterized.parameters(\n+        (\"channels_last\", False, (3, 5, 4), (3, 4)),\n+        (\"channels_last\", True, (3, 5, 4), (3, 1, 4)),\n+        (\"channels_first\", False, (3, 5, 4), (3, 5)),\n+    )\n+    def test_global_average_pooling1d(\n+        self,\n+        data_format,\n+        keepdims,\n+        input_shape,\n+        output_shape,\n+    ):\n+        self.run_layer_test(\n+            layers.GlobalAveragePooling1D,\n+            init_kwargs={\n+                \"data_format\": data_format,\n+                \"keepdims\": keepdims,\n+            },\n+            input_shape=input_shape,\n+            expected_output_shape=output_shape,\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_losses=0,\n+            supports_masking=True,\n+        )\n+\n+    @parameterized.parameters(\n+        (\"channels_last\", False, (3, 5, 6, 4), (3, 4)),\n+        (\"channels_last\", True, (3, 5, 6, 4), (3, 1, 1, 4)),\n+        (\"channels_first\", False, (3, 5, 6, 4), (3, 5)),\n+    )\n+    def test_global_average_pooling2d(\n+        self,\n+        data_format,\n+        keepdims,\n+        input_shape,\n+        output_shape,\n+    ):\n+        self.run_layer_test(\n+            layers.GlobalAveragePooling2D,\n+            init_kwargs={\n+                \"data_format\": data_format,\n+                \"keepdims\": keepdims,\n+            },\n+            input_shape=input_shape,\n+            expected_output_shape=output_shape,\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_losses=0,\n+            supports_masking=False,\n+        )\n+\n+    @parameterized.parameters(\n+        (\"channels_last\", False, (3, 5, 6, 5, 4), (3, 4)),\n+        (\"channels_last\", True, (3, 5, 6, 5, 4), (3, 1, 1, 1, 4)),\n+        (\"channels_first\", False, (3, 5, 6, 5, 4), (3, 5)),\n+    )\n+    def test_global_average_pooling3d(\n+        self,\n+        data_format,\n+        keepdims,\n+        input_shape,\n+        output_shape,\n+    ):\n+        self.run_layer_test(\n+            layers.GlobalAveragePooling3D,\n+            init_kwargs={\n+                \"data_format\": data_format,\n+                \"keepdims\": keepdims,\n+            },\n+            input_shape=input_shape,\n+            expected_output_shape=output_shape,\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_losses=0,\n+            supports_masking=False,\n+        )\n+\n+\n+class GlobalAveragePoolingCorrectnessTest(\n+    testing.TestCase, parameterized.TestCase\n+):\n+    @parameterized.parameters(\n+        (\"channels_last\", False),\n+        (\"channels_last\", True),\n+        (\"channels_first\", False),\n+    )\n+    def test_global_average_pooling1d(self, data_format, keepdims):\n+        inputs = np.arange(24, dtype=np.float).reshape((2, 3, 4))\n+\n+        layer = layers.GlobalAveragePooling1D(\n+            data_format=data_format,\n+            keepdims=keepdims,\n+        )\n+        tf_keras_layer = tf.keras.layers.GlobalAveragePooling1D(\n+            data_format=data_format,\n+            keepdims=keepdims,\n+        )\n+\n+        outputs = layer(inputs)\n+        expected = tf_keras_layer(inputs)\n+        self.assertAllClose(outputs, expected)\n+        \n+        if data_format == \"channels_last\":\n+            mask = np.array([[1, 1, 0], [0, 1, 0]], dtype=np.int)\n+        else:\n+            mask = np.array([[1, 1, 0, 0], [0, 1, 0, 1]], dtype=np.int)\n+        outputs = layer(inputs, mask)\n+        expected = tf_keras_layer(inputs, mask)\n+        self.assertAllClose(outputs, expected)\n+\n+    @parameterized.parameters(\n+        (\"channels_last\", False),\n+        (\"channels_last\", True),\n+        (\"channels_first\", False),\n+    )\n+    def test_global_average_pooling2d(self, data_format, keepdims):\n+        inputs = np.arange(96, dtype=np.float).reshape((2, 3, 4, 4))\n+\n+        layer = layers.GlobalAveragePooling2D(\n+            data_format=data_format,\n+            keepdims=keepdims,\n+        )\n+        tf_keras_layer = tf.keras.layers.GlobalAveragePooling2D(\n+            data_format=data_format,\n+            keepdims=keepdims,\n+        )\n+\n+        outputs = layer(inputs)\n+        expected = tf_keras_layer(inputs)\n+        self.assertAllClose(outputs, expected)\n+\n+    @parameterized.parameters(\n+        (\"channels_last\", False),\n+        (\"channels_last\", True),\n+        (\"channels_first\", False),\n+    )\n+    def test_global_average_pooling3d(self, data_format, keepdims):\n+        inputs = np.arange(360, dtype=np.float).reshape((2, 3, 3, 5, 4))\n+\n+        layer = layers.GlobalAveragePooling3D(\n+            data_format=data_format,\n+            keepdims=keepdims,\n+        )\n+        tf_keras_layer = tf.keras.layers.GlobalAveragePooling3D(\n+            data_format=data_format,\n+            keepdims=keepdims,\n+        )\n+\n+        outputs = layer(inputs)\n+        expected = tf_keras_layer(inputs)\n+        self.assertAllClose(outputs, expected)\n\n@@ -0,0 +1,64 @@\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.pooling.base_global_pooling import BaseGlobalPooling\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.layers.GlobalMaxPooling1D\",\n+        \"keras_core.layers.GlobalMaxPool1D\",\n+    ]\n+)\n+class GlobalMaxPooling1D(BaseGlobalPooling):\n+    \"\"\"Global max pooling operation for temporal data.\n+\n+    Args:\n+        data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n+            The ordering of the dimensions in the inputs. `\"channels_last\"`\n+            corresponds to inputs with shape `(batch, steps, features)`\n+            while `\"channels_first\"` corresponds to inputs with shape\n+            `(batch, features, steps)`. It defaults to the `image_data_format`\n+            value found in your Keras config file at `~/.keras/keras.json`.\n+            If you never set it, then it will be `\"channels_last\"`.\n+        keepdims: A boolean, whether to keep the temporal dimension or not.\n+            If `keepdims` is `False` (default), the rank of the tensor is\n+            reduced for spatial dimensions. If `keepdims` is `True`, the\n+            temporal dimension are retained with length 1.\n+            The behavior is the same as for `tf.reduce_mean` or `np.mean`.\n+\n+    Input shape:\n+    - If `data_format='channels_last'`:\n+        3D tensor with shape:\n+        `(batch_size, steps, features)`\n+    - If `data_format='channels_first'`:\n+        3D tensor with shape:\n+        `(batch_size, features, steps)`\n+\n+    Output shape:\n+    - If `keepdims`=False:\n+        2D tensor with shape `(batch_size, features)`.\n+    - If `keepdims`=True:\n+        - If `data_format=\"channels_last\"`:\n+            3D tensor with shape `(batch_size, 1, features)`\n+        - If `data_format=\"channels_first\"`:\n+            3D tensor with shape `(batch_size, features, 1)`\n+\n+    Examples:\n+\n+    >>> x = np.random.rand(2, 3, 4)\n+    >>> y = keras_core.layers.GlobalMaxPooling1D()(x)\n+    >>> print(y.shape)\n+    (2, 4)\n+    \"\"\"\n+\n+    def __init__(self, data_format=None, keepdims=False, **kwargs):\n+        super().__init__(\n+            pool_dimensions=1,\n+            data_format=data_format,\n+            keepdims=keepdims,\n+            **kwargs,\n+        )\n+\n+    def call(self, inputs):\n+        steps_axis = 1 if self.data_format == \"channels_last\" else 2\n+        return ops.max(inputs, axis=steps_axis, keepdims=self.keepdims)\n\n@@ -0,0 +1,66 @@\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.pooling.base_global_pooling import BaseGlobalPooling\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.layers.GlobalMaxPooling1D\",\n+        \"keras_core.layers.GlobalMaxPool1D\",\n+    ]\n+)\n+class GlobalMaxPooling2D(BaseGlobalPooling):\n+    \"\"\"Global max pooling operation for 2D data.\n+\n+    Args:\n+        data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n+            The ordering of the dimensions in the inputs. `\"channels_last\"`\n+            corresponds to inputs with shape `(batch, height, width, channels)`\n+            while `\"channels_first\"` corresponds to inputs with shape\n+            `(batch, features, height, weight)`. It defaults to the\n+            `image_data_format` value found in your Keras config file at\n+            `~/.keras/keras.json`. If you never set it, then it will be\n+            `\"channels_last\"`.\n+        keepdims: A boolean, whether to keep the temporal dimension or not.\n+            If `keepdims` is `False` (default), the rank of the tensor is\n+            reduced for spatial dimensions. If `keepdims` is `True`, the\n+            spatial dimension are retained with length 1.\n+            The behavior is the same as for `tf.reduce_mean` or `np.mean`.\n+\n+    Input shape:\n+    - If `data_format='channels_last'`:\n+        4D tensor with shape:\n+        `(batch_size, height, width, channels)`\n+    - If `data_format='channels_first'`:\n+        4D tensor with shape:\n+        `(batch_size, channels, height, width)`\n+\n+    Output shape:\n+    - If `keepdims`=False:\n+        2D tensor with shape `(batch_size, channels)`.\n+    - If `keepdims`=True:\n+        - If `data_format=\"channels_last\"`:\n+            4D tensor with shape `(batch_size, 1, 1, channels)`\n+        - If `data_format=\"channels_first\"`:\n+            4D tensor with shape `(batch_size, channels, 1, 1)`\n+\n+    Examples:\n+\n+    >>> x = np.random.rand(2, 4, 5, 3)\n+    >>> y = keras_core.layers.GlobalMaxPooling2D()(x)\n+    >>> print(y.shape)\n+    (2, 3)\n+    \"\"\"\n+\n+    def __init__(self, data_format=None, keepdims=False, **kwargs):\n+        super().__init__(\n+            pool_dimensions=2,\n+            data_format=data_format,\n+            keepdims=keepdims,\n+            **kwargs,\n+        )\n+\n+    def call(self, inputs):\n+        if self.data_format == \"channels_last\":\n+            return ops.max(inputs, axis=[1, 2], keepdims=self.keepdims)\n+        return ops.max(inputs, axis=[2, 3], keepdims=self.keepdims)\n\n@@ -0,0 +1,67 @@\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.pooling.base_global_pooling import BaseGlobalPooling\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.layers.GlobalMaxPooling3D\",\n+        \"keras_core.layers.GlobalMaxPool3D\",\n+    ]\n+)\n+class GlobalMaxPooling3D(BaseGlobalPooling):\n+    \"\"\"Global max pooling operation for 3D data.\n+\n+    Args:\n+        data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n+            The ordering of the dimensions in the inputs. `\"channels_last\"`\n+            corresponds to inputs with shape\n+            `(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n+            while `\"channels_first\"` corresponds to inputs with shape\n+            `(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n+            It defaults to the `image_data_format` value found in your Keras\n+            config file at `~/.keras/keras.json`. If you never set it, then it\n+            will be `\"channels_last\"`.\n+        keepdims: A boolean, whether to keep the temporal dimension or not.\n+            If `keepdims` is `False` (default), the rank of the tensor is\n+            reduced for spatial dimensions. If `keepdims` is `True`, the\n+            spatial dimension are retained with length 1.\n+            The behavior is the same as for `tf.reduce_mean` or `np.mean`.\n+\n+    Input shape:\n+    - If `data_format='channels_last'`:\n+        5D tensor with shape:\n+        `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n+    - If `data_format='channels_first'`:\n+        5D tensor with shape:\n+        `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`\n+\n+    Output shape:\n+    - If `keepdims`=False:\n+        2D tensor with shape `(batch_size, channels)`.\n+    - If `keepdims`=True:\n+        - If `data_format=\"channels_last\"`:\n+            5D tensor with shape `(batch_size, 1, 1, 1, channels)`\n+        - If `data_format=\"channels_first\"`:\n+            5D tensor with shape `(batch_size, channels, 1, 1, 1)`\n+\n+    Examples:\n+\n+    >>> x = np.random.rand(2, 4, 5, 4, 3)\n+    >>> y = keras_core.layers.GlobalMaxPooling3D()(x)\n+    >>> print(y.shape)\n+    (2, 3)\n+    \"\"\"\n+\n+    def __init__(self, data_format=None, keepdims=False, **kwargs):\n+        super().__init__(\n+            pool_dimensions=3,\n+            data_format=data_format,\n+            keepdims=keepdims,\n+            **kwargs,\n+        )\n+\n+    def call(self, inputs):\n+        if self.data_format == \"channels_last\":\n+            return ops.max(inputs, axis=[1, 2, 3], keepdims=self.keepdims)\n+        return ops.max(inputs, axis=[2, 3, 4], keepdims=self.keepdims)\n\n@@ -0,0 +1,151 @@\n+import numpy as np\n+import tensorflow as tf\n+from absl.testing import parameterized\n+\n+from keras_core import layers\n+from keras_core import testing\n+\n+\n+class GlobalMaxPoolingBasicTest(testing.TestCase, parameterized.TestCase):\n+    @parameterized.parameters(\n+        (\"channels_last\", False, (3, 5, 4), (3, 4)),\n+        (\"channels_last\", True, (3, 5, 4), (3, 1, 4)),\n+        (\"channels_first\", False, (3, 5, 4), (3, 5)),\n+    )\n+    def test_global_max_pooling1d(\n+        self,\n+        data_format,\n+        keepdims,\n+        input_shape,\n+        output_shape,\n+    ):\n+        self.run_layer_test(\n+            layers.GlobalMaxPooling1D,\n+            init_kwargs={\n+                \"data_format\": data_format,\n+                \"keepdims\": keepdims,\n+            },\n+            input_shape=input_shape,\n+            expected_output_shape=output_shape,\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_losses=0,\n+            supports_masking=False,\n+        )\n+\n+    @parameterized.parameters(\n+        (\"channels_last\", False, (3, 5, 6, 4), (3, 4)),\n+        (\"channels_last\", True, (3, 5, 6, 4), (3, 1, 1, 4)),\n+        (\"channels_first\", False, (3, 5, 6, 4), (3, 5)),\n+    )\n+    def test_global_max_pooling2d(\n+        self,\n+        data_format,\n+        keepdims,\n+        input_shape,\n+        output_shape,\n+    ):\n+        self.run_layer_test(\n+            layers.GlobalMaxPooling2D,\n+            init_kwargs={\n+                \"data_format\": data_format,\n+                \"keepdims\": keepdims,\n+            },\n+            input_shape=input_shape,\n+            expected_output_shape=output_shape,\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_losses=0,\n+            supports_masking=False,\n+        )\n+\n+    @parameterized.parameters(\n+        (\"channels_last\", False, (3, 5, 6, 5, 4), (3, 4)),\n+        (\"channels_last\", True, (3, 5, 6, 5, 4), (3, 1, 1, 1, 4)),\n+        (\"channels_first\", False, (3, 5, 6, 5, 4), (3, 5)),\n+    )\n+    def test_global_max_pooling3d(\n+        self,\n+        data_format,\n+        keepdims,\n+        input_shape,\n+        output_shape,\n+    ):\n+        self.run_layer_test(\n+            layers.GlobalMaxPooling3D,\n+            init_kwargs={\n+                \"data_format\": data_format,\n+                \"keepdims\": keepdims,\n+            },\n+            input_shape=input_shape,\n+            expected_output_shape=output_shape,\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_losses=0,\n+            supports_masking=False,\n+        )\n+\n+\n+class GlobalMaxPoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n+    @parameterized.parameters(\n+        (\"channels_last\", False),\n+        (\"channels_last\", True),\n+        (\"channels_first\", False),\n+    )\n+    def test_global_max_pooling1d(self, data_format, keepdims):\n+        inputs = np.arange(24, dtype=np.float).reshape((2, 3, 4))\n+\n+        layer = layers.GlobalMaxPooling1D(\n+            data_format=data_format,\n+            keepdims=keepdims,\n+        )\n+        tf_keras_layer = tf.keras.layers.GlobalMaxPooling1D(\n+            data_format=data_format,\n+            keepdims=keepdims,\n+        )\n+\n+        outputs = layer(inputs)\n+        expected = tf_keras_layer(inputs)\n+        self.assertAllClose(outputs, expected)\n+\n+    @parameterized.parameters(\n+        (\"channels_last\", False),\n+        (\"channels_last\", True),\n+        (\"channels_first\", False),\n+    )\n+    def test_global_max_pooling2d(self, data_format, keepdims):\n+        inputs = np.arange(96, dtype=np.float).reshape((2, 3, 4, 4))\n+\n+        layer = layers.GlobalMaxPooling2D(\n+            data_format=data_format,\n+            keepdims=keepdims,\n+        )\n+        tf_keras_layer = tf.keras.layers.GlobalMaxPooling2D(\n+            data_format=data_format,\n+            keepdims=keepdims,\n+        )\n+\n+        outputs = layer(inputs)\n+        expected = tf_keras_layer(inputs)\n+        self.assertAllClose(outputs, expected)\n+\n+    @parameterized.parameters(\n+        (\"channels_last\", False),\n+        (\"channels_last\", True),\n+        (\"channels_first\", False),\n+    )\n+    def test_global_max_pooling3d(self, data_format, keepdims):\n+        inputs = np.arange(360, dtype=np.float).reshape((2, 3, 3, 5, 4))\n+\n+        layer = layers.GlobalMaxPooling3D(\n+            data_format=data_format,\n+            keepdims=keepdims,\n+        )\n+        tf_keras_layer = tf.keras.layers.GlobalMaxPooling3D(\n+            data_format=data_format,\n+            keepdims=keepdims,\n+        )\n+\n+        outputs = layer(inputs)\n+        expected = tf_keras_layer(inputs)\n+        self.assertAllClose(outputs, expected)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
