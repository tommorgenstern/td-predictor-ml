{"custom_id": "scrapy#3b6fae1e617030c6ee4f3a9e6bb0e2e6b1066acc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 55 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 5 | Methods Changed: 8 | Complexity Δ (Sum/Max): 7/4 | Churn Δ: 59 | Churn Cumulative: 509 | Contributors (this commit): 4 | Commits (past 90d): 11 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -126,14 +126,28 @@ class Settings(object):\n             self._overrides = o = _DictProxy()\n         return o\n \n+    @property\n+    def defaults(self):\n+        warnings.warn(\"`Settings.defaults` attribute is deprecated and won't \"\n+                      \"be supported in Scrapy 0.26, use \"\n+                      \"`Settings.set(name, value, priority='default')` instead\",\n+                      category=ScrapyDeprecationWarning, stacklevel=2)\n+        try:\n+            o = self._defaults\n+        except AttributeError:\n+            class _DictProxy(dict):\n+                def __setitem__(this, key, value):\n+                    super(_DictProxy, this).__setitem__(key, value)\n+                    self.set(key, value, priority='default')\n+            self._defaults = o = _DictProxy()\n+        return o\n+\n \n class CrawlerSettings(Settings):\n \n     def __init__(self, settings_module=None, **kw):\n         Settings.__init__(self, **kw)\n         self.settings_module = settings_module\n-        self.overrides = {}\n-        self.defaults = {}\n \n     def __getitem__(self, opt_name):\n         if opt_name in self.overrides:\n\n@@ -6,7 +6,7 @@ try:\n except ImportError:\n     import mock\n \n-from scrapy.settings import Settings, SettingsAttribute\n+from scrapy.settings import Settings, SettingsAttribute, CrawlerSettings\n from . import default_settings\n \n \n@@ -190,7 +190,7 @@ class SettingsTest(unittest.TestCase):\n         self.assertEqual(settings.getdict('TEST_DICT3', {'key1': 5}), {'key1': 5})\n         self.assertRaises(ValueError, settings.getdict, 'TEST_LIST1')\n \n-    def test_deprecated_attribute(self):\n+    def test_deprecated_attribute_overrides(self):\n         self.settings.set('BAR', 'fuz', priority='cmdline')\n         with warnings.catch_warnings(record=True) as w:\n             self.settings.overrides['BAR'] = 'foo'\n@@ -199,6 +199,43 @@ class SettingsTest(unittest.TestCase):\n             self.assertEqual(self.settings.overrides.get('BAR'), 'foo')\n             self.assertIn('BAR', self.settings.overrides)\n \n+    def test_deprecated_attribute_defaults(self):\n+        self.settings.set('BAR', 'fuz', priority='default')\n+        with warnings.catch_warnings(record=True) as w:\n+            self.settings.defaults['BAR'] = 'foo'\n+            self.assertIn(\"Settings.defaults\", str(w[0].message))\n+            self.assertEqual(self.settings.get('BAR'), 'foo')\n+            self.assertEqual(self.settings.defaults.get('BAR'), 'foo')\n+            self.assertIn('BAR', self.settings.defaults)\n+\n+\n+class CrawlerSettingsTest(unittest.TestCase):\n+\n+    def test_deprecated_crawlersettings(self):\n+        def _get_settings(settings_dict=None):\n+            settings_module = type('SettingsModuleMock', (object,), settings_dict or {})\n+            return CrawlerSettings(settings_module)\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            settings = _get_settings()\n+            self.assertIn(\"CrawlerSettings is deprecated\", str(w[0].message))\n+\n+            # test_global_defaults\n+            self.assertEqual(settings.getint('DOWNLOAD_TIMEOUT'), 180)\n+\n+            # test_defaults\n+            settings.defaults['DOWNLOAD_TIMEOUT'] = '99'\n+            self.assertEqual(settings.getint('DOWNLOAD_TIMEOUT'), 99)\n+\n+            # test_settings_module\n+            settings = _get_settings({'DOWNLOAD_TIMEOUT': '3'})\n+            self.assertEqual(settings.getint('DOWNLOAD_TIMEOUT'), 3)\n+\n+            # test_overrides\n+            settings = _get_settings({'DOWNLOAD_TIMEOUT': '3'})\n+            settings.overrides['DOWNLOAD_TIMEOUT'] = '15'\n+            self.assertEqual(settings.getint('DOWNLOAD_TIMEOUT'), 15)\n+\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9d3aa945f4508afbe6935bd662cdf21ee65a2210", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 13 | Files Changed: 3 | Hunks: 9 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 26 | Churn Cumulative: 1489 | Contributors (this commit): 8 | Commits (past 90d): 3 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -135,12 +135,12 @@ class CrawlerProcess(object):\n \n         name, crawler = self.crawlers.popitem()\n         self._active_crawler = crawler\n-        sflo = log.start_from_crawler(crawler)\n+        log_observer = log.start_from_crawler(crawler)\n         crawler.configure()\n         crawler.install()\n         crawler.signals.connect(crawler.uninstall, signals.engine_stopped)\n-        if sflo:\n-            crawler.signals.connect(sflo.stop, signals.engine_stopped)\n+        if log_observer:\n+            crawler.signals.connect(log_observer.stop, signals.engine_stopped)\n         crawler.signals.connect(self._check_done, signals.engine_stopped)\n         crawler.start()\n         return name, crawler\n\n@@ -114,12 +114,12 @@ def _get_log_level(level_name_or_id):\n def start(logfile=None, loglevel='INFO', logstdout=True, logencoding='utf-8', crawler=None):\n     loglevel = _get_log_level(loglevel)\n     file = open(logfile, 'a') if logfile else sys.stderr\n-    sflo = ScrapyFileLogObserver(file, loglevel, logencoding, crawler)\n+    log_observer = ScrapyFileLogObserver(file, loglevel, logencoding, crawler)\n     _oldshowwarning = warnings.showwarning\n-    log.startLoggingWithObserver(sflo.emit, setStdout=logstdout)\n+    log.startLoggingWithObserver(log_observer.emit, setStdout=logstdout)\n     # restore warnings, wrongly silenced by Twisted\n     warnings.showwarning = _oldshowwarning\n-    return sflo\n+    return log_observer\n \n def msg(message=None, _level=INFO, **kw):\n     kw['logLevel'] = kw.pop('level', _level)\n@@ -140,9 +140,9 @@ def start_from_settings(settings, crawler=None):\n             settings['LOG_ENCODING'], crawler)\n \n def scrapy_info(settings):\n-    sflo = start_from_settings(settings)\n-    if sflo:\n-        msg(\"Scrapy %s started (bot: %s)\" % (scrapy.__version__, \\\n+    log_observer = start_from_settings(settings)\n+    if log_observer:\n+        msg(\"Scrapy %s started (bot: %s)\" % (scrapy.__version__,\n             settings['BOT_NAME']))\n \n         msg(\"Optional features available: %s\" % \", \".join(scrapy.optional_features),\n@@ -151,7 +151,7 @@ def scrapy_info(settings):\n         d = dict(overridden_settings(settings))\n         msg(format=\"Overridden settings: %(settings)r\", settings=d, level=INFO)\n \n-        sflo.stop()\n+        log_observer.stop()\n \n def start_from_crawler(crawler):\n     return start_from_settings(crawler.settings, crawler)\n\n@@ -22,12 +22,12 @@ class ScrapyFileLogObserverTest(unittest.TestCase):\n \n     def setUp(self):\n         self.f = StringIO()\n-        self.sflo = log.ScrapyFileLogObserver(self.f, self.level, self.encoding)\n-        self.sflo.start()\n+        self.log_observer = log.ScrapyFileLogObserver(self.f, self.level, self.encoding)\n+        self.log_observer.start()\n \n     def tearDown(self):\n         self.flushLoggedErrors()\n-        self.sflo.stop()\n+        self.log_observer.stop()\n \n     def logged(self):\n         return self.f.getvalue().strip()[25:]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#daabc718a9e63556b5aa9ae7e92bb0fc4cb7604f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 10 | Churn Cumulative: 1047 | Contributors (this commit): 8 | Commits (past 90d): 4 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -36,6 +36,16 @@ class SelectorTestCase(unittest.TestCase):\n         self.assertEqual([x.extract() for x in sel.xpath(\"concat(//input[@name='a']/@value, //input[@name='b']/@value)\")],\n                          [u'12'])\n \n+    def test_representation(self):\n+        body = u\"<p><input name='{}' value='\\xa9'/></p>\".format(50 * 'b')\n+        response = TextResponse(url=\"http://example.com\", body=body, encoding='utf8')\n+        sel = self.sscls(response)\n+\n+        self.assertEqual(\n+            map(repr, sel.xpath('//input/@name')),\n+            [u\"<Selector xpath='//input/@name' data=u'{}'>\".format(40 * 'b')]\n+        )\n+\n     def test_select_unicode_query(self):\n         body = u\"<p><input name='\\xa9' value='1'/></p>\"\n         response = TextResponse(url=\"http://example.com\", body=body, encoding='utf8')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e38e3d27528a5dc2cffc9cba6745f537f0d029df", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 257 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -17,7 +17,6 @@ import sys\n if sys.version_info < (2, 7):\n     print(\"Scrapy %s requires Python 2.7\" % __version__)\n     sys.exit(1)\n-del sys\n \n # Ignore noisy twisted deprecation warnings\n import warnings\n@@ -25,7 +24,7 @@ warnings.filterwarnings('ignore', category=DeprecationWarning, module='twisted')\n del warnings\n \n # Apply monkey patches to fix issues in external libraries\n-import _monkeypatches\n+from . import _monkeypatches\n del _monkeypatches\n \n # WARNING: optional_features set is deprecated and will be removed soon. Do not use.\n@@ -53,7 +52,13 @@ if twisted_version >= (11, 1, 0):\n     optional_features.add('http11')\n \n # Declare top-level shortcuts\n+if sys.version_info[0] == 2:\n+    # Top-level shortcuts are not ready for Python 3 (like most of Scrapy);\n+    # skip them here to make at least some parts of Scrapy\n+    # importable in Python 3.\n     from scrapy.spider import Spider\n     from scrapy.http import Request, FormRequest\n     from scrapy.selector import Selector\n     from scrapy.item import Item, Field\n+\n+del sys\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2b749668bad623cd5d7de19eb3044da623f8b26b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 21 | Churn Cumulative: 319 | Contributors (this commit): 7 | Commits (past 90d): 1 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,13 +10,30 @@ import re\n import inspect\n import weakref\n import errno\n+import warnings\n from functools import partial, wraps\n-from sgmllib import SGMLParser\n \n+import six\n+\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+\n+\n+if six.PY2:\n+    from sgmllib import SGMLParser\n \n     class FixedSGMLParser(SGMLParser):\n         \"\"\"The SGMLParser that comes with Python has a bug in the convert_charref()\n-    method. This is the same class with the bug fixed\"\"\"\n+        method. This is the same class with the bug fixed.\n+\n+        Warning: this class is deprecated and will be removed in future releases.\n+        \"\"\"\n+\n+        def __init__(self, *args, **kwargs):\n+            warnings.warn(\n+                \"FixedSGMLParser is deprecated and will be removed in future releases.\",\n+                ScrapyDeprecationWarning\n+            )\n+            SGMLParser.__init__(self, *args, **kwargs)\n \n         def convert_charref(self, name):\n             \"\"\"This method fixes a bug in Python's SGMLParser.\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#00cd4f0fa598fb1aa55084e3326ee302bbd88b38", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 22 | Churn Cumulative: 382 | Contributors (this commit): 10 | Commits (past 90d): 1 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,8 +1,9 @@\n \"\"\"\n SGMLParser-based Link extractors\n \"\"\"\n-import re\n-from urlparse import urlparse, urljoin\n+from urlparse import urljoin\n+import warnings\n+\n from w3lib.url import safe_url_string\n from scrapy.selector import Selector\n from scrapy.link import Link\n@@ -10,11 +11,18 @@ from scrapy.linkextractor import FilteringLinkExtractor\n from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.python import FixedSGMLParser, unique as unique_list, str_to_unicode\n from scrapy.utils.response import get_base_url\n+from scrapy.exceptions import ScrapyDeprecationWarning\n \n \n class BaseSgmlLinkExtractor(FixedSGMLParser):\n \n     def __init__(self, tag=\"a\", attr=\"href\", unique=False, process_value=None):\n+        warnings.warn(\n+            \"BaseSgmlLinkExtractor is deprecated and will be removed in future releases!!! \"\n+            \"Please use scrapy.contrib.linkextractors.LinkExtractor\",\n+            ScrapyDeprecationWarning\n+        )\n+        with warnings.catch_warnings(record=True):\n             FixedSGMLParser.__init__(self)\n         self.scan_tag = tag if callable(tag) else lambda t: t == tag\n         self.scan_attr = attr if callable(attr) else lambda a: a == attr\n@@ -91,11 +99,21 @@ class SgmlLinkExtractor(FilteringLinkExtractor):\n     def __init__(self, allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(),\n                  tags=('a', 'area'), attrs=('href',), canonicalize=True, unique=True, process_value=None,\n                  deny_extensions=None):\n+\n+        warnings.warn(\n+            \"SgmlLinkExtractor is deprecated and will be removed in future releases. \"\n+            \"Please use scrapy.contrib.linkextractors.LinkExtractor\",\n+            ScrapyDeprecationWarning\n+        )\n+\n         tags, attrs = set(arg_to_iter(tags)), set(arg_to_iter(attrs))\n         tag_func = lambda x: x in tags\n         attr_func = lambda x: x in attrs\n+\n+        with warnings.catch_warnings(record=True):\n             lx = BaseSgmlLinkExtractor(tag=tag_func, attr=attr_func,\n                 unique=unique, process_value=process_value)\n+\n         super(SgmlLinkExtractor, self).__init__(lx, allow, deny,\n             allow_domains, deny_domains, restrict_xpaths, canonicalize,\n             deny_extensions)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7316a37635cc30e56635c0090e969fb17d274faf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 20 | Files Changed: 2 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 28 | Churn Cumulative: 729 | Contributors (this commit): 14 | Commits (past 90d): 4 | Contributors (cumulative): 17 | DMM Complexity: 0.0\n\nDIFF:\n@@ -3,18 +3,19 @@ SGMLParser-based Link extractors\n \"\"\"\n from urlparse import urljoin\n import warnings\n+from sgmllib import SGMLParser\n \n from w3lib.url import safe_url_string\n from scrapy.selector import Selector\n from scrapy.link import Link\n from scrapy.linkextractor import FilteringLinkExtractor\n from scrapy.utils.misc import arg_to_iter\n-from scrapy.utils.python import FixedSGMLParser, unique as unique_list, str_to_unicode\n+from scrapy.utils.python import unique as unique_list, str_to_unicode\n from scrapy.utils.response import get_base_url\n from scrapy.exceptions import ScrapyDeprecationWarning\n \n \n-class BaseSgmlLinkExtractor(FixedSGMLParser):\n+class BaseSgmlLinkExtractor(SGMLParser):\n \n     def __init__(self, tag=\"a\", attr=\"href\", unique=False, process_value=None):\n         warnings.warn(\n@@ -22,8 +23,7 @@ class BaseSgmlLinkExtractor(FixedSGMLParser):\n             \"Please use scrapy.contrib.linkextractors.LinkExtractor\",\n             ScrapyDeprecationWarning\n         )\n-        with warnings.catch_warnings(record=True):\n-            FixedSGMLParser.__init__(self)\n+        SGMLParser.__init__(self)\n         self.scan_tag = tag if callable(tag) else lambda t: t == tag\n         self.scan_attr = attr if callable(attr) else lambda a: a == attr\n         self.process_value = (lambda v: v) if process_value is None else process_value\n@@ -64,7 +64,7 @@ class BaseSgmlLinkExtractor(FixedSGMLParser):\n         return links\n \n     def reset(self):\n-        FixedSGMLParser.reset(self)\n+        SGMLParser.reset(self)\n         self.links = []\n         self.base_url = None\n \n\n@@ -22,12 +22,10 @@ if six.PY2:\n     from sgmllib import SGMLParser\n \n     class FixedSGMLParser(SGMLParser):\n-        \"\"\"The SGMLParser that comes with Python has a bug in the convert_charref()\n-        method. This is the same class with the bug fixed.\n-\n-        Warning: this class is deprecated and will be removed in future releases.\n         \"\"\"\n-\n+        Warning: this class is deprecated and will be removed in future\n+        releases. Please use standard `sgmllib.SGMLParser`.\n+        \"\"\"\n         def __init__(self, *args, **kwargs):\n             warnings.warn(\n                 \"FixedSGMLParser is deprecated and will be removed in future releases.\",\n@@ -35,16 +33,6 @@ if six.PY2:\n             )\n             SGMLParser.__init__(self, *args, **kwargs)\n \n-        def convert_charref(self, name):\n-            \"\"\"This method fixes a bug in Python's SGMLParser.\"\"\"\n-            try:\n-                n = int(name)\n-            except ValueError:\n-                return\n-            if not 0 <= n <= 127 : # ASCII ends at 127, not 255\n-                return\n-            return self.convert_codepoint(n)\n-\n \n def flatten(x):\n     \"\"\"flatten(sequence) -> list\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#19228c6584f791a6d3fda681fe271d1176859a46", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 21 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 21 | Churn Cumulative: 358 | Contributors (this commit): 7 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -10,29 +10,8 @@ import re\n import inspect\n import weakref\n import errno\n-import warnings\n from functools import partial, wraps\n \n-import six\n-\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-\n-\n-if six.PY2:\n-    from sgmllib import SGMLParser\n-\n-    class FixedSGMLParser(SGMLParser):\n-        \"\"\"\n-        Warning: this class is deprecated and will be removed in future\n-        releases. Please use standard `sgmllib.SGMLParser`.\n-        \"\"\"\n-        def __init__(self, *args, **kwargs):\n-            warnings.warn(\n-                \"FixedSGMLParser is deprecated and will be removed in future releases.\",\n-                ScrapyDeprecationWarning\n-            )\n-            SGMLParser.__init__(self, *args, **kwargs)\n-\n \n def flatten(x):\n     \"\"\"flatten(sequence) -> list\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d2d014a4e90640b5bef8e3ba239d445fa502028b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1049 | Contributors (this commit): 8 | Commits (past 90d): 5 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -43,7 +43,7 @@ class SelectorTestCase(unittest.TestCase):\n \n         self.assertEqual(\n             map(repr, sel.xpath('//input/@name')),\n-            [u\"<Selector xpath='//input/@name' data=u'{}'>\".format(40 * 'b')]\n+            [\"<Selector xpath='//input/@name' data=u'{}'>\".format(40 * 'b')]\n         )\n \n     def test_select_unicode_query(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9ac639abd323793999bae41bf43fbe1ba08bb2c4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1051 | Contributors (this commit): 8 | Commits (past 90d): 6 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -36,7 +36,7 @@ class SelectorTestCase(unittest.TestCase):\n         self.assertEqual([x.extract() for x in sel.xpath(\"concat(//input[@name='a']/@value, //input[@name='b']/@value)\")],\n                          [u'12'])\n \n-    def test_representation(self):\n+    def test_representation_slice(self):\n         body = u\"<p><input name='{}' value='\\xa9'/></p>\".format(50 * 'b')\n         response = TextResponse(url=\"http://example.com\", body=body, encoding='utf8')\n         sel = self.sscls(response)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#aacf1d05e17888c9f0017d7b5d6ee74fe01aed4c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 1060 | Contributors (this commit): 8 | Commits (past 90d): 7 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -46,6 +46,15 @@ class SelectorTestCase(unittest.TestCase):\n             [\"<Selector xpath='//input/@name' data=u'{}'>\".format(40 * 'b')]\n         )\n \n+    def test_representation_unicode_query(self):\n+        body = u\"<p><input name='{}' value='\\xa9'/></p>\".format(50 * 'b')\n+        response = TextResponse(url=\"http://example.com\", body=body, encoding='utf8')\n+        sel = self.sscls(response)\n+        self.assertEqual(\n+            map(repr, sel.xpath(u'//input[@value=\"\\xa9\"]/@value')),\n+            [\"<Selector xpath=u'//input[@value=\\\"\\\\xa9\\\"]/@value' data=u'\\\\xa9'>\"]\n+        )\n+\n     def test_select_unicode_query(self):\n         body = u\"<p><input name='\\xa9' value='1'/></p>\"\n         response = TextResponse(url=\"http://example.com\", body=body, encoding='utf8')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a9c7f8b26d10aead103d9b65f5f58eb0217989d5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 1040 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -66,12 +66,11 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n \n     def requestTunnel(self, protocol):\n         \"\"\"Asks the proxy to open a tunnel.\"\"\"\n-        tunnelReq = 'CONNECT %s:%s HTTP/1.1\\n' % (self._tunneledHost,\n+        tunnelReq = 'CONNECT %s:%s HTTP/1.1\\r\\n' % (self._tunneledHost,\n                                                   self._tunneledPort)\n         if self._proxyAuthHeader:\n-            tunnelReq += 'Proxy-Authorization: %s \\n\\n' % self._proxyAuthHeader\n-        else:\n-            tunnelReq += '\\n'\n+            tunnelReq += 'Proxy-Authorization: %s\\r\\n' % self._proxyAuthHeader\n+        tunnelReq += '\\r\\n'\n         protocol.transport.write(tunnelReq)\n         self._protocolDataReceived = protocol.dataReceived\n         protocol.dataReceived = self.processProxyResponse\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#acbde40fe7e1e2db5859d6a2ec94414f38c0f30a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 5 | Methods Changed: 11 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 46 | Churn Cumulative: 555 | Contributors (this commit): 4 | Commits (past 90d): 13 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,7 @@\n import six\n import json\n import warnings\n+from collections import MutableMapping\n from importlib import import_module\n \n from scrapy.utils.deprecate import create_deprecated_class\n@@ -119,11 +120,7 @@ class Settings(object):\n         try:\n             o = self._overrides\n         except AttributeError:\n-            class _DictProxy(dict):\n-                def __setitem__(this, key, value):\n-                    super(_DictProxy, this).__setitem__(key, value)\n-                    self.set(key, value, priority='cmdline')\n-            self._overrides = o = _DictProxy()\n+            self._overrides = o = _DictProxy(self, 'cmdline')\n         return o\n \n     @property\n@@ -135,14 +132,34 @@ class Settings(object):\n         try:\n             o = self._defaults\n         except AttributeError:\n-            class _DictProxy(dict):\n-                def __setitem__(this, key, value):\n-                    super(_DictProxy, this).__setitem__(key, value)\n-                    self.set(key, value, priority='default')\n-            self._defaults = o = _DictProxy()\n+            self._defaults = o = _DictProxy(self, 'default')\n         return o\n \n \n+class _DictProxy(MutableMapping):\n+\n+    def __init__(self, settings, priority):\n+        self.o = {}\n+        self.settings = settings\n+        self.priority = priority\n+\n+    def __len__(self):\n+        return len(self.o)\n+\n+    def __getitem__(self, k):\n+        return self.o[k]\n+\n+    def __setitem__(self, k, v):\n+        self.settings.set(k, v, priority=self.priority)\n+        self.o[k] = v\n+\n+    def __delitem__(self, k):\n+        del self.o[k]\n+\n+    def __iter__(self, k, v):\n+        return iter(self.o)\n+\n+\n class CrawlerSettings(Settings):\n \n     def __init__(self, settings_module=None, **kw):\n\n@@ -199,6 +199,15 @@ class SettingsTest(unittest.TestCase):\n             self.assertEqual(self.settings.overrides.get('BAR'), 'foo')\n             self.assertIn('BAR', self.settings.overrides)\n \n+            self.settings.overrides.update(BAR='bus')\n+            self.assertEqual(self.settings.get('BAR'), 'bus')\n+            self.assertEqual(self.settings.overrides.get('BAR'), 'bus')\n+\n+            self.settings.overrides.setdefault('FOO', 'fez')\n+            self.assertEqual(self.settings.get('FOO'), 'fez')\n+            self.assertEqual(self.settings.overrides.get('FOO'), 'fez')\n+\n+\n     def test_deprecated_attribute_defaults(self):\n         self.settings.set('BAR', 'fuz', priority='default')\n         with warnings.catch_warnings(record=True) as w:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ea8e444bd6483386018613f96770fa4eb69fa12c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 259 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -203,6 +203,9 @@ class SettingsTest(unittest.TestCase):\n             self.assertEqual(self.settings.get('BAR'), 'bus')\n             self.assertEqual(self.settings.overrides.get('BAR'), 'bus')\n \n+            self.settings.overrides.setdefault('BAR', 'fez')\n+            self.assertEqual(self.settings.get('BAR'), 'bus')\n+\n             self.settings.overrides.setdefault('FOO', 'fez')\n             self.assertEqual(self.settings.get('FOO'), 'fez')\n             self.assertEqual(self.settings.overrides.get('FOO'), 'fez')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#54d2b110f103770edd0c3199c4319f2e1223cbd4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 50 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 55 | Churn Cumulative: 808 | Contributors (this commit): 9 | Commits (past 90d): 3 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -36,19 +36,19 @@ class LxmlParserLinkExtractor(object):\n \n     def _iter_links(self, document):\n         for el in document.iter(etree.Element):\n-            tag = _nons(el.tag)\n-            if not self.scan_tag(el.tag):\n+            if not self.scan_tag(_nons(el.tag)):\n                 continue\n             attribs = el.attrib\n             for attrib in attribs:\n+                if not self.scan_attr(attrib):\n+                    continue\n                 yield (el, attrib, attribs[attrib])\n \n     def _extract_links(self, selector, response_url, response_encoding, base_url):\n         links = []\n         # hacky way to get the underlying lxml parsed document\n         for el, attr, attr_val in self._iter_links(selector._root):\n-            if self.scan_tag(el.tag) and self.scan_attr(attr):\n-                # pseudo _root.make_links_absolute(base_url)\n+            # pseudo lxml.html.HtmlElement.make_links_absolute(base_url)\n             attr_val = urljoin(base_url, attr_val)\n             url = self.process_attr(attr_val)\n             if url is None:\n\n@@ -1,7 +1,7 @@\n import re\n import unittest\n from scrapy.contrib.linkextractors.regex import RegexLinkExtractor\n-from scrapy.http import HtmlResponse\n+from scrapy.http import HtmlResponse, XmlResponse\n from scrapy.link import Link\n from scrapy.contrib.linkextractors.htmlparser import HtmlParserLinkExtractor\n from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor, BaseSgmlLinkExtractor\n@@ -419,6 +419,51 @@ class SgmlLinkExtractorTestCase(unittest.TestCase):\n             Link(url='http://example.com/get?id=2', text=u'Item 2', fragment='', nofollow=False)\n         ])\n \n+    def test_xhtml(self):\n+        xhtml = \"\"\"\n+<?xml version=\"1.0\"?>\n+<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\"\n+    \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\">\n+<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\" lang=\"en\">\n+<head>\n+    <title>XHTML document title</title>\n+</head>\n+<body>\n+    <div class='links'>\n+    <p><a href=\"/about.html\">About us</a></p>\n+    </div>\n+    <div>\n+    <p><a href=\"/follow.html\">Follow this link</a></p>\n+    </div>\n+    <div>\n+    <p><a href=\"/nofollow.html\" rel=\"nofollow\">Dont follow this one</a></p>\n+    </div>\n+    <div>\n+    <p><a href=\"/nofollow2.html\" rel=\"blah\">Choose to follow or not</a></p>\n+    </div>\n+</body>\n+</html>\n+        \"\"\"\n+\n+        response = HtmlResponse(\"http://example.com/index.xhtml\", body=xhtml)\n+\n+        lx = self.extractor_cls()\n+        self.assertEqual(lx.extract_links(response),\n+                         [Link(url='http://example.com/about.html', text=u'About us', fragment='', nofollow=False),\n+                          Link(url='http://example.com/follow.html', text=u'Follow this link', fragment='', nofollow=False),\n+                          Link(url='http://example.com/nofollow.html', text=u'Dont follow this one', fragment='', nofollow=True),\n+                          Link(url='http://example.com/nofollow2.html', text=u'Choose to follow or not', fragment='', nofollow=False)]\n+                        )\n+\n+        response = XmlResponse(\"http://example.com/index.xhtml\", body=xhtml)\n+\n+        lx = self.extractor_cls()\n+        self.assertEqual(lx.extract_links(response),\n+                         [Link(url='http://example.com/about.html', text=u'About us', fragment='', nofollow=False),\n+                          Link(url='http://example.com/follow.html', text=u'Follow this link', fragment='', nofollow=False),\n+                          Link(url='http://example.com/nofollow.html', text=u'Dont follow this one', fragment='', nofollow=True),\n+                          Link(url='http://example.com/nofollow2.html', text=u'Choose to follow or not', fragment='', nofollow=False)]\n+                        )\n \n \n class LxmlLinkExtractorTestCase(SgmlLinkExtractorTestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#71685fa2d042aaea97e431ce74cebe931a3d7952", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 394 | Contributors (this commit): 10 | Commits (past 90d): 3 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -19,7 +19,7 @@ class BaseSgmlLinkExtractor(SGMLParser):\n \n     def __init__(self, tag=\"a\", attr=\"href\", unique=False, process_value=None):\n         warnings.warn(\n-            \"BaseSgmlLinkExtractor is deprecated and will be removed in future releases!!! \"\n+            \"BaseSgmlLinkExtractor is deprecated and will be removed in future releases. \"\n             \"Please use scrapy.contrib.linkextractors.LinkExtractor\",\n             ScrapyDeprecationWarning\n         )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a2dd5e5ad9d647c5971c416bc9a610a8d8639c42", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 17 | Files Changed: 1 | Hunks: 8 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 47 | Churn Cumulative: 909 | Contributors (this commit): 10 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -57,6 +57,7 @@ class Slot(object):\n     def needs_backout(self):\n         return self.active_size > self.max_active_size\n \n+\n class Scraper(object):\n \n     def __init__(self, crawler):\n@@ -100,8 +101,8 @@ class Scraper(object):\n             self._scrape_next(spider, slot)\n             return _\n         dfd.addBoth(finish_scraping)\n-        dfd.addErrback(log.err, 'Scraper bug processing %s' % request, \\\n-            spider=spider)\n+        dfd.addErrback(\n+            log.err, 'Scraper bug processing %s' % request, spider=spider)\n         self._scrape_next(spider, slot)\n         return dfd\n \n@@ -124,13 +125,13 @@ class Scraper(object):\n         \"\"\"Handle the different cases of request's result been a Response or a\n         Failure\"\"\"\n         if not isinstance(request_result, Failure):\n-            return self.spidermw.scrape_response(self.call_spider, \\\n-                request_result, request, spider)\n+            return self.spidermw.scrape_response(\n+                self.call_spider, request_result, request, spider)\n         else:\n             # FIXME: don't ignore errors in spider middleware\n             dfd = self.call_spider(request_result, request, spider)\n-            return dfd.addErrback(self._log_download_errors, \\\n-                request_result, request, spider)\n+            return dfd.addErrback(\n+                self._log_download_errors, request_result, request, spider)\n \n     def call_spider(self, result, request, spider):\n         result.request = request\n@@ -143,11 +144,21 @@ class Scraper(object):\n         if isinstance(exc, CloseSpider):\n             self.crawler.engine.close_spider(spider, exc.reason or 'cancelled')\n             return\n-        log.err(_failure, \"Spider error processing %s\" % request, spider=spider)\n-        self.signals.send_catch_log(signal=signals.spider_error, failure=_failure, response=response, \\\n-            spider=spider)\n-        self.crawler.stats.inc_value(\"spider_exceptions/%s\" % _failure.value.__class__.__name__, \\\n-            spider=spider)\n+        referer = request.headers.get('Referer')\n+        log.err(\n+            _failure,\n+            \"Spider error processing %s (referer: %s)\" % (request, referer),\n+            spider=spider\n+        )\n+        self.signals.send_catch_log(\n+            signal=signals.spider_error,\n+            failure=_failure, response=response,\n+            spider=spider\n+        )\n+        self.crawler.stats.inc_value(\n+            \"spider_exceptions/%s\" % _failure.value.__class__.__name__,\n+            spider=spider\n+        )\n \n     def handle_spider_output(self, result, request, response, spider):\n         if not result:\n@@ -180,8 +191,8 @@ class Scraper(object):\n         \"\"\"Log and silence errors that come from the engine (typically download\n         errors that got propagated thru here)\n         \"\"\"\n-        if isinstance(download_failure, Failure) \\\n-                and not download_failure.check(IgnoreRequest):\n+        if (isinstance(download_failure, Failure) and\n+                not download_failure.check(IgnoreRequest)):\n             if download_failure.frames:\n                 log.err(download_failure, 'Error downloading %s' % request,\n                         spider=spider)\n@@ -204,13 +215,15 @@ class Scraper(object):\n             if isinstance(ex, DropItem):\n                 logkws = self.logformatter.dropped(item, ex, response, spider)\n                 log.msg(spider=spider, **logkws)\n-                return self.signals.send_catch_log_deferred(signal=signals.item_dropped, \\\n-                    item=item, response=response, spider=spider, exception=output.value)\n+                return self.signals.send_catch_log_deferred(\n+                    signal=signals.item_dropped, item=item, response=response,\n+                    spider=spider, exception=output.value)\n             else:\n                 log.err(output, 'Error processing %s' % item, spider=spider)\n         else:\n             logkws = self.logformatter.scraped(output, response, spider)\n             log.msg(spider=spider, **logkws)\n-            return self.signals.send_catch_log_deferred(signal=signals.item_scraped, \\\n-                item=output, response=response, spider=spider)\n+            return self.signals.send_catch_log_deferred(\n+                signal=signals.item_scraped, item=output, response=response,\n+                spider=spider)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#389882eb131250a1eb8418580b165625dd854bf0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 6 | Files Changed: 6 | Hunks: 6 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 2392 | Contributors (this commit): 14 | Commits (past 90d): 8 | Contributors (cumulative): 31 | DMM Complexity: None\n\nDIFF:\n@@ -6,7 +6,7 @@ import csv\n import sys\n import pprint\n import marshal\n-import cPickle as pickle\n+from six.moves import cPickle as pickle\n from xml.sax.saxutils import XMLGenerator\n from scrapy.utils.serialize import ScrapyJSONEncoder\n from scrapy.item import BaseItem\n\n@@ -1,6 +1,6 @@\n from __future__ import print_function\n import os\n-import cPickle as pickle\n+from six.moves import cPickle as pickle\n from importlib import import_module\n from time import time\n from weakref import WeakKeyDictionary\n\n@@ -1,4 +1,5 @@\n-import os, cPickle as pickle\n+import os\n+from six.moves import cPickle as pickle\n \n from scrapy import signals\n \n\n@@ -2,7 +2,8 @@\n Scheduler queues\n \"\"\"\n \n-import marshal, cPickle as pickle\n+import marshal\n+from six.moves import cPickle as pickle\n \n from queuelib import queue\n \n\n@@ -1,4 +1,5 @@\n-import unittest, json, cPickle as pickle\n+import unittest, json\n+from six.moves import cPickle as pickle\n from cStringIO import StringIO\n import lxml.etree\n import re\n\n@@ -1,5 +1,5 @@\n import os\n-import cPickle as pickle\n+from six.moves import cPickle as pickle\n import warnings\n \n from importlib import import_module\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3067d6d71087c56a63c05ad869b8a7e94e0d55e1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 6 | Files Changed: 4 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 219 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 13 | DMM Complexity: 0.0\n\nDIFF:\n@@ -75,7 +75,6 @@ class CaselessDictTest(unittest.TestCase):\n         d = CaselessDict()\n         d['a'] = 1\n         assert 'a' in d\n-        assert d.has_key('a')\n \n     def test_pop(self):\n         d = CaselessDict()\n\n@@ -1,6 +1,6 @@\n import sys\n import os\n-from ConfigParser import SafeConfigParser\n+from six.moves.configparser import SafeConfigParser\n from operator import itemgetter\n \n def build_component_list(base, custom):\n\n@@ -134,7 +134,7 @@ def connect(receiver, signal=Any, sender=Any, weak=True):\n \tif weak:\n \t\treceiver = saferef.safeRef(receiver, onDelete=_removeReceiver)\n \tsenderkey = id(sender)\n-\tif connections.has_key(senderkey):\n+\tif senderkey in connections:\n \t\tsignals = connections[senderkey]\n \telse:\n \t\tconnections[senderkey] = signals = {}\n@@ -154,7 +154,7 @@ def connect(receiver, signal=Any, sender=Any, weak=True):\n \treceiverID = id(receiver)\n \t# get current set, remove any current references to\n \t# this receiver in the set, including back-references\n-\tif signals.has_key(signal):\n+\tif signal in signals:\n \t\treceivers = signals[signal]\n \t\t_removeOldBackRefs(senderkey, signal, receiver, receivers)\n \telse:\n@@ -290,7 +290,7 @@ def getAllReceivers( sender = Any, signal = Any ):\n \t\tfor receiver in set:\n \t\t\tif receiver: # filter out dead instance-method weakrefs\n \t\t\t\ttry:\n-\t\t\t\t\tif not receivers.has_key( receiver ):\n+\t\t\t\t\tif receiver not in receivers:\n \t\t\t\t\t\treceivers[receiver] = 1\n \t\t\t\t\t\tyield receiver\n \t\t\t\texcept TypeError:\n\n@@ -37,7 +37,7 @@ def robustApply(receiver, *arguments, **named):\n     receiver, codeObject, startIndex = function(receiver)\n     acceptable = codeObject.co_varnames[startIndex+len(arguments):codeObject.co_argcount]\n     for name in codeObject.co_varnames[startIndex:startIndex+len(arguments)]:\n-        if named.has_key(name):\n+        if name in named:\n             raise TypeError(\n                 \"\"\"Argument %r specified both positionally and as a keyword for calling %r\"\"\"% (\n                     name, receiver,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4f24e724a3767946e0911009878c708285ff297f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 50 | Lines Deleted: 34 | Files Changed: 21 | Hunks: 50 | Methods Changed: 29 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 84 | Churn Cumulative: 6555 | Contributors (this commit): 33 | Commits (past 90d): 32 | Contributors (cumulative): 123 | DMM Complexity: None\n\nDIFF:\n@@ -71,7 +71,7 @@ def _print_commands(settings, inproject):\n     print(\"  scrapy <command> [options] [args]\\n\")\n     print(\"Available commands:\")\n     cmds = _get_commands_dict(settings, inproject)\n-    for cmdname, cmdclass in sorted(cmds.iteritems()):\n+    for cmdname, cmdclass in sorted(cmds.items()):\n         print(\"  %-13s %s\" % (cmdname, cmdclass.short_desc()))\n     if not inproject:\n         print()\n\n@@ -86,7 +86,7 @@ class Command(ScrapyCommand):\n \n         # start checks\n         if opts.list:\n-            for spider, methods in sorted(contract_reqs.iteritems()):\n+            for spider, methods in sorted(contract_reqs.items()):\n                 if not methods and not opts.verbose:\n                     continue\n                 print(spider)\n\n@@ -1,4 +1,5 @@\n import os\n+import six\n from collections import defaultdict\n \n from scrapy.exceptions import NotConfigured\n@@ -77,7 +78,7 @@ class CookiesMiddleware(object):\n     def _get_request_cookies(self, jar, request):\n         if isinstance(request.cookies, dict):\n             cookie_list = [{'name': k, 'value': v} for k, v in \\\n-                    request.cookies.iteritems()]\n+                    six.iteritems(request.cookies)]\n         else:\n             cookie_list = request.cookies\n \n\n@@ -6,6 +6,7 @@ import csv\n import sys\n import pprint\n import marshal\n+import six\n from six.moves import cPickle as pickle\n from xml.sax.saxutils import XMLGenerator\n from scrapy.utils.serialize import ScrapyJSONEncoder\n@@ -56,9 +57,9 @@ class BaseItemExporter(object):\n             include_empty = self.export_empty_fields\n         if self.fields_to_export is None:\n             if include_empty:\n-                field_iter = item.fields.iterkeys()\n+                field_iter = six.iterkeys(item.fields)\n             else:\n-                field_iter = item.iterkeys()\n+                field_iter = six.iterkeys(item)\n         else:\n             if include_empty:\n                 field_iter = self.fields_to_export\n@@ -248,7 +249,7 @@ class PythonItemExporter(BaseItemExporter):\n         return self._to_str_if_unicode(value)\n \n     def _serialize_dict(self, value):\n-        for key, val in value.iteritems():\n+        for key, val in six.iteritems(value):\n             yield key, self._serialize_value(val)\n \n     def export_item(self, item):\n\n@@ -4,6 +4,7 @@ See documentation in docs/topics/loaders.rst\n \n \"\"\"\n from collections import defaultdict\n+import six\n \n from scrapy.item import Item\n from scrapy.selector import Selector\n@@ -39,7 +40,7 @@ class ItemLoader(object):\n         if value is None:\n             return\n         if not field_name:\n-            for k,v in value.iteritems():\n+            for k, v in six.iteritems(value):\n                 self._add_value(k, v)\n         else:\n             self._add_value(field_name, value)\n@@ -49,7 +50,7 @@ class ItemLoader(object):\n         if value is None:\n             return\n         if not field_name:\n-            for k,v in value.iteritems():\n+            for k, v in six.iteritems(value):\n                 self._replace_value(k, v)\n         else:\n             self._replace_value(field_name, value)\n\n@@ -5,6 +5,7 @@ See documentation in docs/topics/extensions.rst\n \"\"\"\n \n import gc\n+import six\n \n from scrapy import signals\n from scrapy.exceptions import NotConfigured\n@@ -27,7 +28,7 @@ class MemoryDebugger(object):\n     def spider_closed(self, spider, reason):\n         gc.collect()\n         self.stats.set_value('memdebug/gc_garbage_count', len(gc.garbage), spider=spider)\n-        for cls, wdict in live_refs.iteritems():\n+        for cls, wdict in six.iteritems(live_refs):\n             if not wdict:\n                 continue\n             self.stats.set_value('memdebug/live_refs/%s' % cls.__name__, len(wdict), spider=spider)\n\n@@ -10,6 +10,7 @@ import time\n import urlparse\n from collections import defaultdict\n from cStringIO import StringIO\n+import six\n \n from twisted.internet import defer, threads\n \n@@ -105,7 +106,7 @@ class S3FilesStore(object):\n         key_name = '%s%s' % (self.prefix, path)\n         k = b.new_key(key_name)\n         if meta:\n-            for metakey, metavalue in meta.iteritems():\n+            for metakey, metavalue in six.iteritems(meta):\n                 k.set_metadata(metakey, str(metavalue))\n         h = self.HEADERS.copy()\n         if headers:\n\n@@ -6,6 +6,7 @@ See documentation in topics/images.rst\n \n import hashlib\n from cStringIO import StringIO\n+import six\n \n from PIL import Image\n \n@@ -79,7 +80,7 @@ class ImagesPipeline(FilesPipeline):\n         image, buf = self.convert_image(orig_image)\n         yield path, image, buf\n \n-        for thumb_id, size in self.THUMBS.iteritems():\n+        for thumb_id, size in six.iteritems(self.THUMBS):\n             thumb_path = self.thumb_path(request, thumb_id, response=response, info=info)\n             thumb_image, thumb_buf = self.convert_image(image, size)\n             yield thumb_path, thumb_image, thumb_buf\n\n@@ -8,6 +8,7 @@ import zipfile\n import tarfile\n from cStringIO import StringIO\n from tempfile import mktemp\n+import six\n \n from scrapy import log\n from scrapy.responsetypes import responsetypes\n@@ -71,7 +72,7 @@ class DecompressionMiddleware(object):\n         if not response.body:\n             return response\n \n-        for fmt, func in self._formats.iteritems():\n+        for fmt, func in six.iteritems(self._formats):\n             new_response = func(response)\n             if new_response:\n                 log.msg(format='Decompressed response with format: %(responsefmt)s',\n\n@@ -1,6 +1,7 @@\n \"\"\"Download handlers for different schemes\"\"\"\n \n from twisted.internet import defer\n+import six\n from scrapy.exceptions import NotSupported, NotConfigured\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.misc import load_object\n@@ -14,7 +15,7 @@ class DownloadHandlers(object):\n         self._notconfigured = {}\n         handlers = crawler.settings.get('DOWNLOAD_HANDLERS_BASE')\n         handlers.update(crawler.settings.get('DOWNLOAD_HANDLERS', {}))\n-        for scheme, clspath in handlers.iteritems():\n+        for scheme, clspath in six.iteritems(handlers):\n             # Allow to disable a handler just like any other\n             # component (extension, middleware, etc).\n             if clspath is None:\n\n@@ -7,6 +7,7 @@ See documentation in docs/topics/request-response.rst\n \n import urllib, urlparse\n import lxml.html\n+import six\n from scrapy.http.request import Request\n from scrapy.utils.python import unicode_to_str\n \n@@ -21,7 +22,7 @@ class FormRequest(Request):\n         super(FormRequest, self).__init__(*args, **kwargs)\n \n         if formdata:\n-            items = formdata.iteritems() if isinstance(formdata, dict) else formdata\n+            items = formdata.items() if isinstance(formdata, dict) else formdata\n             querystr = _urlencode(items, self.encoding)\n             if self.method == 'POST':\n                 self.headers.setdefault('Content-Type', 'application/x-www-form-urlencoded')\n@@ -106,7 +107,7 @@ def _get_inputs(form, formdata, dont_click, clickdata, response):\n         if clickable and clickable[0] not in formdata and not clickable[0] is None:\n             values.append(clickable)\n \n-    values.extend(formdata.iteritems())\n+    values.extend(formdata.items())\n     return values\n \n def _value(ele):\n@@ -161,7 +162,7 @@ def _get_clickable(clickdata, form):\n     # We didn't find it, so now we build an XPath expression out of the other\n     # arguments, because they can be used as such\n     xpath = u'.//*' + \\\n-            u''.join(u'[@%s=\"%s\"]' % c for c in clickdata.iteritems())\n+            u''.join(u'[@%s=\"%s\"]' % c for c in six.iteritems(clickdata))\n     el = form.xpath(xpath)\n     if len(el) == 1:\n         return (el[0].name, el[0].value)\n\n@@ -6,6 +6,7 @@ See documentation in docs/topics/item.rst\n \n from pprint import pformat\n from UserDict import DictMixin\n+import six\n \n from scrapy.utils.trackref import object_ref\n \n@@ -24,7 +25,7 @@ class ItemMeta(type):\n     def __new__(mcs, class_name, bases, attrs):\n         fields = {}\n         new_attrs = {}\n-        for n, v in attrs.iteritems():\n+        for n, v in six.iteritems(attrs):\n             if isinstance(v, Field):\n                 fields[n] = v\n             else:\n@@ -43,7 +44,7 @@ class DictItem(DictMixin, BaseItem):\n     def __init__(self, *args, **kwargs):\n         self._values = {}\n         if args or kwargs:  # avoid creating dict for most common case\n-            for k, v in dict(*args, **kwargs).iteritems():\n+            for k, v in six.iteritems(dict(*args, **kwargs)):\n                 self[k] = v\n \n     def __getitem__(self, key):\n\n@@ -7,6 +7,7 @@ based on different criteria.\n from mimetypes import MimeTypes\n from pkgutil import get_data\n from cStringIO import StringIO\n+import six\n \n from scrapy.http import Response\n from scrapy.utils.misc import load_object\n@@ -34,7 +35,7 @@ class ResponseTypes(object):\n         self.mimetypes = MimeTypes()\n         mimedata = get_data('scrapy', 'mime.types')\n         self.mimetypes.readfp(StringIO(mimedata))\n-        for mimetype, cls in self.CLASSES.iteritems():\n+        for mimetype, cls in six.iteritems(self.CLASSES):\n             self.classes[mimetype] = load_object(cls)\n \n     def from_mimetype(self, mimetype):\n\n@@ -110,7 +110,7 @@ class Shell(object):\n \n     def print_help(self):\n         self.p(\"Available Scrapy objects:\")\n-        for k, v in sorted(self.vars.iteritems()):\n+        for k, v in sorted(self.vars.items()):\n             if self._is_relevant(v):\n                 self.p(\"  %-10s %s\" % (k, v))\n         self.p(\"Useful shortcuts:\")\n\n@@ -4,6 +4,7 @@ spiders\n \"\"\"\n \n from zope.interface import implements\n+import six\n \n from scrapy import signals\n from scrapy.interfaces import ISpiderManager\n@@ -48,7 +49,7 @@ class SpiderManager(object):\n             return spcls(**spider_kwargs)\n \n     def find_by_request(self, request):\n-        return [name for name, cls in self._spiders.iteritems()\n+        return [name for name, cls in six.iteritems(self._spiders)\n             if cls.handles_request(request)]\n \n     def list(self):\n\n@@ -1,6 +1,7 @@\n import os\n import sys\n import subprocess\n+import tempfile\n from time import sleep\n from os.path import exists, join, abspath\n from shutil import rmtree\n@@ -25,7 +26,7 @@ class ProjectTest(unittest.TestCase):\n         rmtree(self.temp_path)\n \n     def call(self, *new_args, **kwargs):\n-        out = os.tmpfile()\n+        out = tempfile.TemporaryFile()\n         args = (sys.executable, '-m', 'scrapy.cmdline') + new_args\n         return subprocess.call(args, stdout=out, stderr=out, cwd=self.cwd, \\\n             env=self.env, **kwargs)\n\n@@ -1,4 +1,5 @@\n from unittest import TestCase\n+import six\n \n from scrapy.contrib.downloadermiddleware.defaultheaders import DefaultHeadersMiddleware\n from scrapy.http import Request\n@@ -13,7 +14,7 @@ class TestDefaultHeadersMiddleware(TestCase):\n         spider = Spider('foo')\n         spider.set_crawler(crawler)\n         defaults = dict([(k, [v]) for k, v in \\\n-            crawler.settings.get('DEFAULT_REQUEST_HEADERS').iteritems()])\n+            six.iteritems(crawler.settings.get('DEFAULT_REQUEST_HEADERS'))])\n         return defaults, spider, DefaultHeadersMiddleware.from_crawler(crawler)\n \n     def test_process_request(self):\n\n@@ -6,6 +6,7 @@ This module must not depend on any module outside the Standard Library.\n \"\"\"\n \n import copy\n+import six\n from collections import OrderedDict\n \n \n@@ -133,7 +134,7 @@ class MultiValueDict(dict):\n                         self.setlistdefault(key, []).append(value)\n                 except TypeError:\n                     raise ValueError(\"MultiValueDict.update() takes either a MultiValueDict or dictionary\")\n-        for key, value in kwargs.iteritems():\n+        for key, value in six.iteritems(kwargs):\n             self.setlistdefault(key, []).append(value)\n \n class SiteNode(object):\n@@ -200,7 +201,7 @@ class CaselessDict(dict):\n         return dict.setdefault(self, self.normkey(key), self.normvalue(def_val))\n \n     def update(self, seq):\n-        seq = seq.iteritems() if isinstance(seq, dict) else seq\n+        seq = seq.items() if isinstance(seq, dict) else seq\n         iseq = ((self.normkey(k), self.normvalue(v)) for k, v in seq)\n         super(CaselessDict, self).update(iseq)\n \n\n@@ -10,6 +10,7 @@ import re\n import inspect\n import weakref\n import errno\n+import six\n from functools import partial, wraps\n \n \n@@ -232,7 +233,7 @@ def stringify_dict(dct_or_tuples, encoding='utf-8', keys_only=True):\n     dict or a list of tuples, like any dict constructor supports.\n     \"\"\"\n     d = {}\n-    for k, v in dict(dct_or_tuples).iteritems():\n+    for k, v in six.iteritems(dict(dct_or_tuples)):\n         k = k.encode(encoding) if isinstance(k, unicode) else k\n         if not keys_only:\n             v = v.encode(encoding) if isinstance(v, unicode) else v\n\n@@ -10,7 +10,7 @@ alias to object in that case).\n \"\"\"\n \n from __future__ import print_function\n-import weakref, os\n+import weakref, os, six\n from collections import defaultdict\n from time import time\n from operator import itemgetter\n@@ -33,7 +33,7 @@ class object_ref(object):\n def format_live_refs(ignore=NoneType):\n     s = \"Live References\" + os.linesep + os.linesep\n     now = time()\n-    for cls, wdict in live_refs.iteritems():\n+    for cls, wdict in six.iteritems(live_refs):\n         if not wdict:\n             continue\n         if issubclass(cls, ignore):\n@@ -47,12 +47,12 @@ def print_live_refs(*a, **kw):\n     print(format_live_refs(*a, **kw))\n \n def get_oldest(class_name):\n-    for cls, wdict in live_refs.iteritems():\n+    for cls, wdict in six.iteritems(live_refs):\n         if cls.__name__ == class_name:\n             if wdict:\n-                return min(wdict.iteritems(), key=itemgetter(1))[0]\n+                return min(six.iteritems(wdict), key=itemgetter(1))[0]\n \n def iter_all(class_name):\n-    for cls, wdict in live_refs.iteritems():\n+    for cls, wdict in six.iteritems(live_refs):\n         if cls.__name__ == class_name:\n-            return wdict.iterkeys()\n+            return six.iterkeys(wdict)\n\n@@ -26,7 +26,7 @@ Internal attributes:\n \t\tvs. the original code.)\n \"\"\"\n from __future__ import generators\n-import types, weakref\n+import types, weakref, six\n from scrapy.xlib.pydispatch import saferef, robustapply, errors\n \n __author__ = \"Patrick K. O'Brien <pobrien@orbtech.com>\"\n@@ -463,7 +463,7 @@ def _removeOldBackRefs(senderkey, signal, receiver, receivers):\n \t\tfound = 0\n \t\tsignals = connections.get(signal)\n \t\tif signals is not None:\n-\t\t\tfor sig,recs in connections.get(signal,{}).iteritems():\n+\t\t\tfor sig, recs in six.iteritems(connections.get(signal,{})):\n \t\t\t\tif sig != signal:\n \t\t\t\t\tfor rec in recs:\n \t\t\t\t\t\tif rec is oldReceiver:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
