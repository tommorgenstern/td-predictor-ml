{"custom_id": "scrapy#92c18d15b4dbed7c98dc3d5ac329c90abb23950f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 294 | Lines Deleted: 411 | Files Changed: 11 | Hunks: 173 | Methods Changed: 80 | Complexity Δ (Sum/Max): -11/2 | Churn Δ: 705 | Churn Cumulative: 8668 | Contributors (this commit): 46 | Commits (past 90d): 24 | Contributors (cumulative): 112 | DMM Complexity: 0.0\n\nDIFF:\n@@ -18,11 +18,12 @@ from twisted.internet.task import deferLater\n from twisted.names import dns, error\n from twisted.names.server import DNSServerFactory\n from twisted.web import resource, server\n-from twisted.web.server import NOT_DONE_YET, GzipEncoderFactory, Site\n-from twisted.web.static import File\n-from twisted.web.util import redirectTo\n+from twisted.web.server import NOT_DONE_YET, Site\n+from twisted.web.static import Data, File\n+from twisted.web.util import Redirect, redirectTo\n \n from scrapy.utils.python import to_bytes, to_unicode\n+from tests import tests_datadir\n \n if TYPE_CHECKING:\n     from twisted.internet.protocol import ServerFactory\n@@ -245,6 +246,14 @@ class ArbitraryLengthPayloadResource(LeafResource):\n         return request.content.read()\n \n \n+class NoMetaRefreshRedirect(Redirect):\n+    def render(self, request: server.Request) -> bytes:\n+        content = Redirect.render(self, request)\n+        return content.replace(\n+            b'http-equiv=\"refresh\"', b'http-no-equiv=\"do-not-refresh-me\"'\n+        )\n+\n+\n class Root(resource.Resource):\n     def __init__(self):\n         resource.Resource.__init__(self)\n@@ -256,18 +265,26 @@ class Root(resource.Resource):\n         self.putChild(b\"raw\", Raw())\n         self.putChild(b\"echo\", Echo())\n         self.putChild(b\"payload\", PayloadResource())\n-        self.putChild(\n-            b\"xpayload\",\n-            resource.EncodingResourceWrapper(PayloadResource(), [GzipEncoderFactory()]),\n-        )\n         self.putChild(b\"alpayload\", ArbitraryLengthPayloadResource())\n-        try:\n-            from tests import tests_datadir\n-\n         self.putChild(b\"files\", File(str(Path(tests_datadir, \"test_site/files/\"))))\n-        except Exception:\n-            pass\n         self.putChild(b\"redirect-to\", RedirectTo())\n+        self.putChild(b\"text\", Data(b\"Works\", \"text/plain\"))\n+        self.putChild(\n+            b\"html\",\n+            Data(\n+                b\"<body><p class='one'>Works</p><p class='two'>World</p></body>\",\n+                \"text/html\",\n+            ),\n+        )\n+        self.putChild(\n+            b\"enc-gb18030\",\n+            Data(b\"<p>gb18030 encoding</p>\", \"text/html; charset=gb18030\"),\n+        )\n+        self.putChild(b\"redirect\", Redirect(b\"/redirected\"))\n+        self.putChild(\n+            b\"redirect-no-meta-refresh\", NoMetaRefreshRedirect(b\"/redirected\")\n+        )\n+        self.putChild(b\"redirected\", Data(b\"Redirected here\", \"text/plain\"))\n \n     def getChild(self, name, request):\n         return self\n\n@@ -7,10 +7,8 @@ from tests.test_commands import TestCommandBase\n \n \n class TestCheckCommand(TestCommandBase):\n-    command = \"check\"\n-\n-    def setUp(self):\n-        super().setUp()\n+    def setup_method(self):\n+        super().setup_method()\n         self.spider_name = \"check_spider\"\n         self.spider = (self.proj_mod_path / \"spiders\" / \"checkspider.py\").resolve()\n \n\n@@ -1,35 +1,35 @@\n-from twisted.internet.defer import inlineCallbacks\n-from twisted.trial import unittest\n-\n-from tests.utils.testproc import ProcessTest\n-from tests.utils.testsite import SiteTest\n+from tests.mockserver import MockServer\n+from tests.test_commands import TestProjectBase\n \n \n-class TestFetchCommand(ProcessTest, SiteTest, unittest.TestCase):\n-    command = \"fetch\"\n+class TestFetchCommand(TestProjectBase):\n+    @classmethod\n+    def setup_class(cls):\n+        cls.mockserver = MockServer()\n+        cls.mockserver.__enter__()\n+\n+    @classmethod\n+    def teardown_class(cls):\n+        cls.mockserver.__exit__(None, None, None)\n \n-    @inlineCallbacks\n     def test_output(self):\n-        _, out, _ = yield self.execute([self.url(\"/text\")])\n-        assert out.strip() == b\"Works\"\n+        _, out, _ = self.proc(\"fetch\", self.mockserver.url(\"/text\"))\n+        assert out.strip() == \"Works\"\n \n-    @inlineCallbacks\n     def test_redirect_default(self):\n-        _, out, _ = yield self.execute([self.url(\"/redirect\")])\n-        assert out.strip() == b\"Redirected here\"\n+        _, out, _ = self.proc(\"fetch\", self.mockserver.url(\"/redirect\"))\n+        assert out.strip() == \"Redirected here\"\n \n-    @inlineCallbacks\n     def test_redirect_disabled(self):\n-        _, out, err = yield self.execute(\n-            [\"--no-redirect\", self.url(\"/redirect-no-meta-refresh\")]\n+        _, _, err = self.proc(\n+            \"fetch\", \"--no-redirect\", self.mockserver.url(\"/redirect-no-meta-refresh\")\n         )\n         err = err.strip()\n-        assert b\"downloader/response_status_count/302\" in err, err\n-        assert b\"downloader/response_status_count/200\" not in err, err\n+        assert \"downloader/response_status_count/302\" in err, err\n+        assert \"downloader/response_status_count/200\" not in err, err\n \n-    @inlineCallbacks\n     def test_headers(self):\n-        _, out, _ = yield self.execute([self.url(\"/text\"), \"--headers\"])\n-        out = out.replace(b\"\\r\", b\"\")  # required on win32\n-        assert b\"Server: TwistedWeb\" in out, out\n-        assert b\"Content-Type: text/plain\" in out\n+        _, out, _ = self.proc(\"fetch\", self.mockserver.url(\"/text\"), \"--headers\")\n+        out = out.replace(\"\\r\", \"\")  # required on win32\n+        assert \"Server: TwistedWeb\" in out, out\n+        assert \"Content-Type: text/plain\" in out\n\n@@ -1,29 +1,25 @@\n import argparse\n-import os\n import re\n from pathlib import Path\n \n-from twisted.internet.defer import inlineCallbacks\n-\n from scrapy.commands import parse\n from scrapy.settings import Settings\n-from scrapy.utils.python import to_unicode\n+from tests.mockserver import MockServer\n from tests.test_commands import TestCommandBase\n-from tests.utils.testproc import ProcessTest\n-from tests.utils.testsite import SiteTest\n \n \n-def _textmode(bstr: bytes) -> str:\n-    \"\"\"Normalize input the same as writing to a file\n-    and reading from it in text mode\"\"\"\n-    return to_unicode(bstr).replace(os.linesep, \"\\n\")\n+class TestParseCommand(TestCommandBase):\n+    @classmethod\n+    def setup_class(cls):\n+        cls.mockserver = MockServer()\n+        cls.mockserver.__enter__()\n \n+    @classmethod\n+    def teardown_class(cls):\n+        cls.mockserver.__exit__(None, None, None)\n \n-class TestParseCommand(ProcessTest, SiteTest, TestCommandBase):\n-    command = \"parse\"\n-\n-    def setUp(self):\n-        super().setUp()\n+    def setup_method(self):\n+        super().setup_method()\n         self.spider_name = \"parse_spider\"\n         (self.proj_mod_path / \"spiders\" / \"myspider.py\").write_text(\n             f\"\"\"\n@@ -171,10 +167,9 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n \"\"\"\n             )\n \n-    @inlineCallbacks\n     def test_spider_arguments(self):\n-        _, _, stderr = yield self.execute(\n-            [\n+        _, _, stderr = self.proc(\n+            \"parse\",\n             \"--spider\",\n             self.spider_name,\n             \"-a\",\n@@ -182,16 +177,14 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n             \"-c\",\n             \"parse\",\n             \"--verbose\",\n-                self.url(\"/html\"),\n-            ]\n+            self.mockserver.url(\"/html\"),\n         )\n-        assert \"DEBUG: It Works!\" in _textmode(stderr)\n+        assert \"DEBUG: It Works!\" in stderr\n \n-    @inlineCallbacks\n     def test_request_with_meta(self):\n         raw_json_string = '{\"foo\" : \"baz\"}'\n-        _, _, stderr = yield self.execute(\n-            [\n+        _, _, stderr = self.proc(\n+            \"parse\",\n             \"--spider\",\n             self.spider_name,\n             \"--meta\",\n@@ -199,13 +192,12 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n             \"-c\",\n             \"parse_request_with_meta\",\n             \"--verbose\",\n-                self.url(\"/html\"),\n-            ]\n+            self.mockserver.url(\"/html\"),\n         )\n-        assert \"DEBUG: It Works!\" in _textmode(stderr)\n+        assert \"DEBUG: It Works!\" in stderr\n \n-        _, _, stderr = yield self.execute(\n-            [\n+        _, _, stderr = self.proc(\n+            \"parse\",\n             \"--spider\",\n             self.spider_name,\n             \"-m\",\n@@ -213,16 +205,14 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n             \"-c\",\n             \"parse_request_with_meta\",\n             \"--verbose\",\n-                self.url(\"/html\"),\n-            ]\n+            self.mockserver.url(\"/html\"),\n         )\n-        assert \"DEBUG: It Works!\" in _textmode(stderr)\n+        assert \"DEBUG: It Works!\" in stderr\n \n-    @inlineCallbacks\n     def test_request_with_cb_kwargs(self):\n         raw_json_string = '{\"foo\" : \"bar\", \"key\": \"value\"}'\n-        _, _, stderr = yield self.execute(\n-            [\n+        _, _, stderr = self.proc(\n+            \"parse\",\n             \"--spider\",\n             self.spider_name,\n             \"--cbkwargs\",\n@@ -230,201 +220,200 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n             \"-c\",\n             \"parse_request_with_cb_kwargs\",\n             \"--verbose\",\n-                self.url(\"/html\"),\n-            ]\n+            self.mockserver.url(\"/html\"),\n         )\n-        log = _textmode(stderr)\n-        assert \"DEBUG: It Works!\" in log\n+        assert \"DEBUG: It Works!\" in stderr\n         assert (\n-            \"DEBUG: request.callback signature: (response, foo=None, key=None)\" in log\n+            \"DEBUG: request.callback signature: (response, foo=None, key=None)\"\n+            in stderr\n         )\n \n-    @inlineCallbacks\n     def test_request_without_meta(self):\n-        _, _, stderr = yield self.execute(\n-            [\n+        _, _, stderr = self.proc(\n+            \"parse\",\n             \"--spider\",\n             self.spider_name,\n             \"-c\",\n             \"parse_request_without_meta\",\n             \"--nolinks\",\n-                self.url(\"/html\"),\n-            ]\n+            self.mockserver.url(\"/html\"),\n         )\n-        assert \"DEBUG: It Works!\" in _textmode(stderr)\n+        assert \"DEBUG: It Works!\" in stderr\n \n-    @inlineCallbacks\n     def test_pipelines(self):\n-        _, _, stderr = yield self.execute(\n-            [\n+        _, _, stderr = self.proc(\n+            \"parse\",\n             \"--spider\",\n             self.spider_name,\n             \"--pipelines\",\n             \"-c\",\n             \"parse\",\n             \"--verbose\",\n-                self.url(\"/html\"),\n-            ]\n+            self.mockserver.url(\"/html\"),\n         )\n-        assert \"INFO: It Works!\" in _textmode(stderr)\n+        assert \"INFO: It Works!\" in stderr\n \n-    @inlineCallbacks\n     def test_async_def_asyncio_parse_items_list(self):\n-        status, out, stderr = yield self.execute(\n-            [\n+        _, out, stderr = self.proc(\n+            \"parse\",\n             \"--spider\",\n             \"asyncdef_asyncio_return\",\n             \"-c\",\n             \"parse\",\n-                self.url(\"/html\"),\n-            ]\n+            self.mockserver.url(\"/html\"),\n         )\n-        assert \"INFO: Got response 200\" in _textmode(stderr)\n-        assert \"{'id': 1}\" in _textmode(out)\n-        assert \"{'id': 2}\" in _textmode(out)\n+        assert \"INFO: Got response 200\" in stderr\n+        assert \"{'id': 1}\" in out\n+        assert \"{'id': 2}\" in out\n \n-    @inlineCallbacks\n     def test_async_def_asyncio_parse_items_single_element(self):\n-        status, out, stderr = yield self.execute(\n-            [\n+        _, out, stderr = self.proc(\n+            \"parse\",\n             \"--spider\",\n             \"asyncdef_asyncio_return_single_element\",\n             \"-c\",\n             \"parse\",\n-                self.url(\"/html\"),\n-            ]\n+            self.mockserver.url(\"/html\"),\n         )\n-        assert \"INFO: Got response 200\" in _textmode(stderr)\n-        assert \"{'foo': 42}\" in _textmode(out)\n+        assert \"INFO: Got response 200\" in stderr\n+        assert \"{'foo': 42}\" in out\n \n-    @inlineCallbacks\n     def test_async_def_asyncgen_parse_loop(self):\n-        status, out, stderr = yield self.execute(\n-            [\n+        _, out, stderr = self.proc(\n+            \"parse\",\n             \"--spider\",\n             \"asyncdef_asyncio_gen_loop\",\n             \"-c\",\n             \"parse\",\n-                self.url(\"/html\"),\n-            ]\n+            self.mockserver.url(\"/html\"),\n         )\n-        assert \"INFO: Got response 200\" in _textmode(stderr)\n+        assert \"INFO: Got response 200\" in stderr\n         for i in range(10):\n-            assert f\"{{'foo': {i}}}\" in _textmode(out)\n+            assert f\"{{'foo': {i}}}\" in out\n \n-    @inlineCallbacks\n     def test_async_def_asyncgen_parse_exc(self):\n-        status, out, stderr = yield self.execute(\n-            [\n+        _, out, stderr = self.proc(\n+            \"parse\",\n             \"--spider\",\n             \"asyncdef_asyncio_gen_exc\",\n             \"-c\",\n             \"parse\",\n-                self.url(\"/html\"),\n-            ]\n+            self.mockserver.url(\"/html\"),\n         )\n-        assert \"ValueError\" in _textmode(stderr)\n+        assert \"ValueError\" in stderr\n         for i in range(7):\n-            assert f\"{{'foo': {i}}}\" in _textmode(out)\n+            assert f\"{{'foo': {i}}}\" in out\n \n-    @inlineCallbacks\n     def test_async_def_asyncio_parse(self):\n-        _, _, stderr = yield self.execute(\n-            [\n+        _, _, stderr = self.proc(\n+            \"parse\",\n             \"--spider\",\n             \"asyncdef_asyncio\",\n             \"-c\",\n             \"parse\",\n-                self.url(\"/html\"),\n-            ]\n+            self.mockserver.url(\"/html\"),\n         )\n-        assert \"DEBUG: Got response 200\" in _textmode(stderr)\n+        assert \"DEBUG: Got response 200\" in stderr\n \n-    @inlineCallbacks\n     def test_parse_items(self):\n-        status, out, stderr = yield self.execute(\n-            [\"--spider\", self.spider_name, \"-c\", \"parse\", self.url(\"/html\")]\n+        _, out, _ = self.proc(\n+            \"parse\",\n+            \"--spider\",\n+            self.spider_name,\n+            \"-c\",\n+            \"parse\",\n+            self.mockserver.url(\"/html\"),\n         )\n-        assert \"[{}, {'foo': 'bar'}]\" in _textmode(out)\n+        assert \"[{}, {'foo': 'bar'}]\" in out\n \n-    @inlineCallbacks\n     def test_parse_items_no_callback_passed(self):\n-        status, out, stderr = yield self.execute(\n-            [\"--spider\", self.spider_name, self.url(\"/html\")]\n+        _, out, _ = self.proc(\n+            \"parse\", \"--spider\", self.spider_name, self.mockserver.url(\"/html\")\n         )\n-        assert \"[{}, {'foo': 'bar'}]\" in _textmode(out)\n+        assert \"[{}, {'foo': 'bar'}]\" in out\n \n-    @inlineCallbacks\n     def test_wrong_callback_passed(self):\n-        status, out, stderr = yield self.execute(\n-            [\"--spider\", self.spider_name, \"-c\", \"dummy\", self.url(\"/html\")]\n+        _, out, stderr = self.proc(\n+            \"parse\",\n+            \"--spider\",\n+            self.spider_name,\n+            \"-c\",\n+            \"dummy\",\n+            self.mockserver.url(\"/html\"),\n         )\n-        assert re.search(r\"# Scraped Items  -+\\n\\[\\]\", _textmode(out))\n-        assert \"Cannot find callback\" in _textmode(stderr)\n+        assert re.search(r\"# Scraped Items  -+\\r?\\n\\[\\]\", out)\n+        assert \"Cannot find callback\" in stderr\n \n-    @inlineCallbacks\n     def test_crawlspider_matching_rule_callback_set(self):\n         \"\"\"If a rule matches the URL, use it's defined callback.\"\"\"\n-        status, out, stderr = yield self.execute(\n-            [\"--spider\", \"goodcrawl\" + self.spider_name, \"-r\", self.url(\"/html\")]\n+        _, out, _ = self.proc(\n+            \"parse\",\n+            \"--spider\",\n+            \"goodcrawl\" + self.spider_name,\n+            \"-r\",\n+            self.mockserver.url(\"/html\"),\n         )\n-        assert \"[{}, {'foo': 'bar'}]\" in _textmode(out)\n+        assert \"[{}, {'foo': 'bar'}]\" in out\n \n-    @inlineCallbacks\n     def test_crawlspider_matching_rule_default_callback(self):\n         \"\"\"If a rule match but it has no callback set, use the 'parse' callback.\"\"\"\n-        status, out, stderr = yield self.execute(\n-            [\"--spider\", \"goodcrawl\" + self.spider_name, \"-r\", self.url(\"/text\")]\n+        _, out, _ = self.proc(\n+            \"parse\",\n+            \"--spider\",\n+            \"goodcrawl\" + self.spider_name,\n+            \"-r\",\n+            self.mockserver.url(\"/text\"),\n         )\n-        assert \"[{}, {'nomatch': 'default'}]\" in _textmode(out)\n+        assert \"[{}, {'nomatch': 'default'}]\" in out\n \n-    @inlineCallbacks\n     def test_spider_with_no_rules_attribute(self):\n         \"\"\"Using -r with a spider with no rule should not produce items.\"\"\"\n-        status, out, stderr = yield self.execute(\n-            [\"--spider\", self.spider_name, \"-r\", self.url(\"/html\")]\n+        _, out, stderr = self.proc(\n+            \"parse\", \"--spider\", self.spider_name, \"-r\", self.mockserver.url(\"/html\")\n         )\n-        assert re.search(r\"# Scraped Items  -+\\n\\[\\]\", _textmode(out))\n-        assert \"No CrawlSpider rules found\" in _textmode(stderr)\n+        assert re.search(r\"# Scraped Items  -+\\r?\\n\\[\\]\", out)\n+        assert \"No CrawlSpider rules found\" in stderr\n \n-    @inlineCallbacks\n     def test_crawlspider_missing_callback(self):\n-        status, out, stderr = yield self.execute(\n-            [\"--spider\", \"badcrawl\" + self.spider_name, \"-r\", self.url(\"/html\")]\n+        _, out, _ = self.proc(\n+            \"parse\",\n+            \"--spider\",\n+            \"badcrawl\" + self.spider_name,\n+            \"-r\",\n+            self.mockserver.url(\"/html\"),\n         )\n-        assert re.search(r\"# Scraped Items  -+\\n\\[\\]\", _textmode(out))\n+        assert re.search(r\"# Scraped Items  -+\\r?\\n\\[\\]\", out)\n \n-    @inlineCallbacks\n     def test_crawlspider_no_matching_rule(self):\n         \"\"\"The requested URL has no matching rule, so no items should be scraped\"\"\"\n-        status, out, stderr = yield self.execute(\n-            [\"--spider\", \"badcrawl\" + self.spider_name, \"-r\", self.url(\"/enc-gb18030\")]\n+        _, out, stderr = self.proc(\n+            \"parse\",\n+            \"--spider\",\n+            \"badcrawl\" + self.spider_name,\n+            \"-r\",\n+            self.mockserver.url(\"/enc-gb18030\"),\n         )\n-        assert re.search(r\"# Scraped Items  -+\\n\\[\\]\", _textmode(out))\n-        assert \"Cannot find a rule that matches\" in _textmode(stderr)\n+        assert re.search(r\"# Scraped Items  -+\\r?\\n\\[\\]\", out)\n+        assert \"Cannot find a rule that matches\" in stderr\n \n-    @inlineCallbacks\n     def test_crawlspider_not_exists_with_not_matched_url(self):\n-        status, out, stderr = yield self.execute([self.url(\"/invalid_url\")])\n-        assert status == 0\n+        assert self.call(\"parse\", self.mockserver.url(\"/invalid_url\")) == 0\n \n-    @inlineCallbacks\n     def test_output_flag(self):\n         \"\"\"Checks if a file was created successfully having\n         correct format containing correct data in it.\n         \"\"\"\n         file_name = \"data.json\"\n         file_path = Path(self.proj_path, file_name)\n-        yield self.execute(\n-            [\n+        self.proc(\n+            \"parse\",\n             \"--spider\",\n             self.spider_name,\n             \"-c\",\n             \"parse\",\n             \"-o\",\n             file_name,\n-                self.url(\"/html\"),\n-            ]\n+            self.mockserver.url(\"/html\"),\n         )\n \n         assert file_path.exists()\n\n@@ -7,7 +7,6 @@ from contextlib import contextmanager\n from pathlib import Path\n from tempfile import TemporaryDirectory, mkdtemp\n from typing import TYPE_CHECKING\n-from unittest import skipIf\n \n import pytest\n \n@@ -58,7 +57,7 @@ class BadSpider(scrapy.Spider):\n             return self.proc(\"runspider\", fname, *args)\n \n     def get_log(self, code, name=None, args=()):\n-        p, stdout, stderr = self.runspider(code, name, args=args)\n+        _, _, stderr = self.runspider(code, name, args=args)\n         return stderr\n \n     def test_runspider(self):\n@@ -288,7 +287,7 @@ class MySpider(scrapy.Spider):\n         log = self.get_log(spider_code, args=args)\n         assert \"[myspider] DEBUG: FEEDS: {'stdout:': {'format': 'json'}}\" in log\n \n-    @skipIf(platform.system() == \"Windows\", reason=\"Linux only\")\n+    @pytest.mark.skipif(platform.system() == \"Windows\", reason=\"Linux only\")\n     def test_absolute_path_linux(self):\n         spider_code = \"\"\"\n import scrapy\n@@ -317,7 +316,7 @@ class MySpider(scrapy.Spider):\n             in log\n         )\n \n-    @skipIf(platform.system() != \"Windows\", reason=\"Windows only\")\n+    @pytest.mark.skipif(platform.system() != \"Windows\", reason=\"Windows only\")\n     def test_absolute_path_windows(self):\n         spider_code = \"\"\"\n import scrapy\n@@ -370,14 +369,12 @@ class MySpider(scrapy.Spider):\n         assert \"The value of FOO is 42\" in log\n \n \n+@pytest.mark.skipif(\n+    platform.system() != \"Windows\", reason=\"Windows required for .pyw files\"\n+)\n class TestWindowsRunSpiderCommand(TestRunSpiderCommand):\n     spider_filename = \"myspider.pyw\"\n \n-    def setUp(self):\n-        if platform.system() != \"Windows\":\n-            pytest.skip(\"Windows required for .pyw files\")\n-        return super().setUp()\n-\n     def test_start_errors(self):\n         log = self.get_log(self.badspider, name=\"badspider.pyw\")\n         assert \"start\" in log\n\n@@ -5,140 +5,137 @@ from pathlib import Path\n \n import pytest\n from pexpect.popen_spawn import PopenSpawn\n-from twisted.internet.defer import inlineCallbacks\n-from twisted.trial import unittest\n \n from scrapy.utils.reactor import _asyncio_reactor_path\n from tests import NON_EXISTING_RESOLVABLE, tests_datadir\n from tests.mockserver import MockServer\n-from tests.utils.testproc import ProcessTest\n-from tests.utils.testsite import SiteTest\n+from tests.test_commands import TestProjectBase\n \n \n-class TestShellCommand(ProcessTest, SiteTest, unittest.TestCase):\n-    command = \"shell\"\n+class TestShellCommand(TestProjectBase):\n+    @classmethod\n+    def setup_class(cls):\n+        cls.mockserver = MockServer()\n+        cls.mockserver.__enter__()\n+\n+    @classmethod\n+    def teardown_class(cls):\n+        cls.mockserver.__exit__(None, None, None)\n \n-    @inlineCallbacks\n     def test_empty(self):\n-        _, out, _ = yield self.execute([\"-c\", \"item\"])\n-        assert b\"{}\" in out\n+        _, out, _ = self.proc(\"shell\", \"-c\", \"item\")\n+        assert \"{}\" in out\n \n-    @inlineCallbacks\n     def test_response_body(self):\n-        _, out, _ = yield self.execute([self.url(\"/text\"), \"-c\", \"response.body\"])\n-        assert b\"Works\" in out\n+        _, out, _ = self.proc(\n+            \"shell\", self.mockserver.url(\"/text\"), \"-c\", \"response.body\"\n+        )\n+        assert \"Works\" in out\n \n-    @inlineCallbacks\n     def test_response_type_text(self):\n-        _, out, _ = yield self.execute([self.url(\"/text\"), \"-c\", \"type(response)\"])\n-        assert b\"TextResponse\" in out\n+        _, out, _ = self.proc(\n+            \"shell\", self.mockserver.url(\"/text\"), \"-c\", \"type(response)\"\n+        )\n+        assert \"TextResponse\" in out\n \n-    @inlineCallbacks\n     def test_response_type_html(self):\n-        _, out, _ = yield self.execute([self.url(\"/html\"), \"-c\", \"type(response)\"])\n-        assert b\"HtmlResponse\" in out\n+        _, out, _ = self.proc(\n+            \"shell\", self.mockserver.url(\"/html\"), \"-c\", \"type(response)\"\n+        )\n+        assert \"HtmlResponse\" in out\n \n-    @inlineCallbacks\n     def test_response_selector_html(self):\n         xpath = \"response.xpath(\\\"//p[@class='one']/text()\\\").get()\"\n-        _, out, _ = yield self.execute([self.url(\"/html\"), \"-c\", xpath])\n-        assert out.strip() == b\"Works\"\n+        _, out, _ = self.proc(\"shell\", self.mockserver.url(\"/html\"), \"-c\", xpath)\n+        assert out.strip() == \"Works\"\n \n-    @inlineCallbacks\n     def test_response_encoding_gb18030(self):\n-        _, out, _ = yield self.execute(\n-            [self.url(\"/enc-gb18030\"), \"-c\", \"response.encoding\"]\n+        _, out, _ = self.proc(\n+            \"shell\", self.mockserver.url(\"/enc-gb18030\"), \"-c\", \"response.encoding\"\n         )\n-        assert out.strip() == b\"gb18030\"\n+        assert out.strip() == \"gb18030\"\n \n-    @inlineCallbacks\n     def test_redirect(self):\n-        _, out, _ = yield self.execute([self.url(\"/redirect\"), \"-c\", \"response.url\"])\n-        assert out.strip().endswith(b\"/redirected\")\n-\n-    @inlineCallbacks\n-    def test_redirect_follow_302(self):\n-        _, out, _ = yield self.execute(\n-            [self.url(\"/redirect-no-meta-refresh\"), \"-c\", \"response.status\"]\n+        _, out, _ = self.proc(\n+            \"shell\", self.mockserver.url(\"/redirect\"), \"-c\", \"response.url\"\n         )\n-        assert out.strip().endswith(b\"200\")\n+        assert out.strip().endswith(\"/redirected\")\n \n-    @inlineCallbacks\n-    def test_redirect_not_follow_302(self):\n-        _, out, _ = yield self.execute(\n-            [\n-                \"--no-redirect\",\n-                self.url(\"/redirect-no-meta-refresh\"),\n+    def test_redirect_follow_302(self):\n+        _, out, _ = self.proc(\n+            \"shell\",\n+            self.mockserver.url(\"/redirect-no-meta-refresh\"),\n             \"-c\",\n             \"response.status\",\n-            ]\n         )\n-        assert out.strip().endswith(b\"302\")\n+        assert out.strip().endswith(\"200\")\n+\n+    def test_redirect_not_follow_302(self):\n+        _, out, _ = self.proc(\n+            \"shell\",\n+            \"--no-redirect\",\n+            self.mockserver.url(\"/redirect-no-meta-refresh\"),\n+            \"-c\",\n+            \"response.status\",\n+        )\n+        assert out.strip().endswith(\"302\")\n \n-    @inlineCallbacks\n     def test_fetch_redirect_follow_302(self):\n         \"\"\"Test that calling ``fetch(url)`` follows HTTP redirects by default.\"\"\"\n-        url = self.url(\"/redirect-no-meta-refresh\")\n+        url = self.mockserver.url(\"/redirect-no-meta-refresh\")\n         code = f\"fetch('{url}')\"\n-        errcode, out, errout = yield self.execute([\"-c\", code])\n-        assert errcode == 0, out\n-        assert b\"Redirecting (302)\" in errout\n-        assert b\"Crawled (200)\" in errout\n+        p, out, errout = self.proc(\"shell\", \"-c\", code)\n+        assert p.returncode == 0, out\n+        assert \"Redirecting (302)\" in errout\n+        assert \"Crawled (200)\" in errout\n \n-    @inlineCallbacks\n     def test_fetch_redirect_not_follow_302(self):\n         \"\"\"Test that calling ``fetch(url, redirect=False)`` disables automatic redirects.\"\"\"\n-        url = self.url(\"/redirect-no-meta-refresh\")\n+        url = self.mockserver.url(\"/redirect-no-meta-refresh\")\n         code = f\"fetch('{url}', redirect=False)\"\n-        errcode, out, errout = yield self.execute([\"-c\", code])\n-        assert errcode == 0, out\n-        assert b\"Crawled (302)\" in errout\n+        p, out, errout = self.proc(\"shell\", \"-c\", code)\n+        assert p.returncode == 0, out\n+        assert \"Crawled (302)\" in errout\n \n-    @inlineCallbacks\n     def test_request_replace(self):\n-        url = self.url(\"/text\")\n+        url = self.mockserver.url(\"/text\")\n         code = f\"fetch('{url}') or fetch(response.request.replace(method='POST'))\"\n-        errcode, out, _ = yield self.execute([\"-c\", code])\n-        assert errcode == 0, out\n+        p, out, _ = self.proc(\"shell\", \"-c\", code)\n+        assert p.returncode == 0, out\n \n-    @inlineCallbacks\n     def test_scrapy_import(self):\n-        url = self.url(\"/text\")\n+        url = self.mockserver.url(\"/text\")\n         code = f\"fetch(scrapy.Request('{url}'))\"\n-        errcode, out, _ = yield self.execute([\"-c\", code])\n-        assert errcode == 0, out\n+        p, out, _ = self.proc(\"shell\", \"-c\", code)\n+        assert p.returncode == 0, out\n \n-    @inlineCallbacks\n     def test_local_file(self):\n         filepath = Path(tests_datadir, \"test_site\", \"index.html\")\n-        _, out, _ = yield self.execute([str(filepath), \"-c\", \"item\"])\n-        assert b\"{}\" in out\n+        _, out, _ = self.proc(\"shell\", str(filepath), \"-c\", \"item\")\n+        assert \"{}\" in out\n \n-    @inlineCallbacks\n     def test_local_nofile(self):\n         filepath = \"file:///tests/sample_data/test_site/nothinghere.html\"\n-        errcode, out, err = yield self.execute(\n-            [filepath, \"-c\", \"item\"], check_code=False\n-        )\n-        assert errcode == 1, out or err\n-        assert b\"No such file or directory\" in err\n+        p, out, err = self.proc(\"shell\", filepath, \"-c\", \"item\")\n+        assert p.returncode == 1, out or err\n+        assert \"No such file or directory\" in err\n \n-    @inlineCallbacks\n     def test_dns_failures(self):\n         if NON_EXISTING_RESOLVABLE:\n             pytest.skip(\"Non-existing hosts are resolvable\")\n         url = \"www.somedomainthatdoesntexi.st\"\n-        errcode, out, err = yield self.execute([url, \"-c\", \"item\"], check_code=False)\n-        assert errcode == 1, out or err\n-        assert b\"DNS lookup failed\" in err\n+        p, out, err = self.proc(\"shell\", url, \"-c\", \"item\")\n+        assert p.returncode == 1, out or err\n+        assert \"DNS lookup failed\" in err\n \n-    @inlineCallbacks\n     def test_shell_fetch_async(self):\n-        url = self.url(\"/html\")\n+        url = self.mockserver.url(\"/html\")\n         code = f\"fetch('{url}')\"\n-        args = [\"-c\", code, \"--set\", f\"TWISTED_REACTOR={_asyncio_reactor_path}\"]\n-        _, _, err = yield self.execute(args, check_code=True)\n-        assert b\"RuntimeError: There is no current event loop in thread\" not in err\n+        p, _, err = self.proc(\n+            \"shell\", \"-c\", code, \"--set\", f\"TWISTED_REACTOR={_asyncio_reactor_path}\"\n+        )\n+        assert p.returncode == 0, err\n+        assert \"RuntimeError: There is no current event loop in thread\" not in err\n \n \n class TestInteractiveShell:\n\n@@ -108,8 +108,8 @@ def get_permissions_dict(\n class TestStartprojectTemplates(TestProjectBase):\n     maxDiff = None\n \n-    def setUp(self):\n-        super().setUp()\n+    def setup_method(self):\n+        super().setup_method()\n         self.tmpl = str(Path(self.temp_path, \"templates\"))\n         self.tmpl_proj = str(Path(self.tmpl, \"project\"))\n \n\n@@ -1,29 +1,15 @@\n-import sys\n-\n-from twisted.internet.defer import inlineCallbacks\n-from twisted.trial import unittest\n-\n import scrapy\n-from tests.utils.testproc import ProcessTest\n+from tests.test_commands import TestProjectBase\n \n \n-class TestVersionCommand(ProcessTest, unittest.TestCase):\n-    command = \"version\"\n-\n-    @inlineCallbacks\n+class TestVersionCommand(TestProjectBase):\n     def test_output(self):\n-        encoding = sys.stdout.encoding or \"utf-8\"\n-        _, out, _ = yield self.execute([])\n-        assert out.strip().decode(encoding) == f\"Scrapy {scrapy.__version__}\"\n+        _, out, _ = self.proc(\"version\")\n+        assert out.strip() == f\"Scrapy {scrapy.__version__}\"\n \n-    @inlineCallbacks\n     def test_verbose_output(self):\n-        encoding = sys.stdout.encoding or \"utf-8\"\n-        _, out, _ = yield self.execute([\"-v\"])\n-        headers = [\n-            line.partition(\":\")[0].strip()\n-            for line in out.strip().decode(encoding).splitlines()\n-        ]\n+        _, out, _ = self.proc(\"version\", \"-v\")\n+        headers = [line.partition(\":\")[0].strip() for line in out.strip().splitlines()]\n         assert headers == [\n             \"Scrapy\",\n             \"lxml\",\n\n@@ -10,11 +10,9 @@ from pathlib import Path\n from shutil import rmtree\n from tempfile import TemporaryFile, mkdtemp\n from threading import Timer\n-from typing import TYPE_CHECKING\n+from typing import TYPE_CHECKING, Any\n from unittest import mock\n \n-from twisted.trial import unittest\n-\n import scrapy\n from scrapy.cmdline import _pop_command_name, _print_unknown_command_msg\n from scrapy.commands import ScrapyCommand, ScrapyHelpFormatter, view\n@@ -61,28 +59,30 @@ class TestCommandSettings:\n         )\n \n \n-class TestProjectBase(unittest.TestCase):\n+class TestProjectBase:\n     project_name = \"testproject\"\n \n-    def setUp(self):\n+    def setup_method(self):\n         self.temp_path = mkdtemp()\n         self.cwd = self.temp_path\n         self.proj_path = Path(self.temp_path, self.project_name)\n         self.proj_mod_path = self.proj_path / self.project_name\n         self.env = get_testenv()\n \n-    def tearDown(self):\n+    def teardown_method(self):\n         rmtree(self.temp_path)\n \n-    def call(self, *new_args, **kwargs):\n+    def call(self, *args: str, **popen_kwargs: Any) -> int:\n         with TemporaryFile() as out:\n-            args = (sys.executable, \"-m\", \"scrapy.cmdline\", *new_args)\n+            args = (sys.executable, \"-m\", \"scrapy.cmdline\", *args)\n             return subprocess.call(\n-                args, stdout=out, stderr=out, cwd=self.cwd, env=self.env, **kwargs\n+                args, stdout=out, stderr=out, cwd=self.cwd, env=self.env, **popen_kwargs\n             )\n \n-    def proc(self, *new_args, **popen_kwargs):\n-        args = (sys.executable, \"-m\", \"scrapy.cmdline\", *new_args)\n+    def proc(\n+        self, *args: str, **popen_kwargs: Any\n+    ) -> tuple[subprocess.Popen[bytes], str, str]:\n+        args = (sys.executable, \"-m\", \"scrapy.cmdline\", *args)\n         p = subprocess.Popen(\n             args,\n             cwd=popen_kwargs.pop(\"cwd\", self.cwd),\n@@ -118,10 +118,10 @@ class TestProjectBase(unittest.TestCase):\n \n \n class TestCommandBase(TestProjectBase):\n-    def setUp(self):\n-        super().setUp()\n+    def setup_method(self):\n+        super().setup_method()\n         self.call(\"startproject\", self.project_name)\n-        self.cwd = Path(self.temp_path, self.project_name)\n+        self.cwd = self.proj_path\n         self.env[\"SCRAPY_SETTINGS_MODULE\"] = f\"{self.project_name}.settings\"\n \n \n@@ -136,8 +136,8 @@ class TestCommandCrawlerProcess(TestCommandBase):\n         \"Type of self.crawler_process: <class 'scrapy.crawler.AsyncCrawlerProcess'>\"\n     )\n \n-    def setUp(self):\n-        super().setUp()\n+    def setup_method(self):\n+        super().setup_method()\n         (self.cwd / self.project_name / \"commands\").mkdir(exist_ok=True)\n         (self.cwd / self.project_name / \"commands\" / \"__init__.py\").touch()\n         (self.cwd / self.project_name / \"commands\" / f\"{self.name}.py\").write_text(\"\"\"\n@@ -363,6 +363,19 @@ Unknown command: abc\n                 assert out.getvalue().strip() == message.strip()\n \n \n+class TestProjectSubdir(TestProjectBase):\n+    \"\"\"Test that commands work in a subdirectory of the project.\"\"\"\n+\n+    def setup_method(self):\n+        super().setup_method()\n+        self.call(\"startproject\", self.project_name)\n+        self.cwd = self.proj_path / \"subdir\"\n+        self.cwd.mkdir(exist_ok=True)\n+\n+    def test_list(self):\n+        assert self.call(\"list\") == 0\n+\n+\n class TestBenchCommand(TestCommandBase):\n     def test_run(self):\n         _, _, log = self.proc(\n@@ -389,8 +402,8 @@ class TestViewCommand(TestCommandBase):\n \n \n class TestHelpMessage(TestCommandBase):\n-    def setUp(self):\n-        super().setUp()\n+    def setup_method(self):\n+        super().setup_method()\n         self.commands = [\n             \"parse\",\n             \"startproject\",\n\n@@ -1,67 +0,0 @@\n-from __future__ import annotations\n-\n-import os\n-import sys\n-from typing import TYPE_CHECKING, cast\n-\n-from twisted.internet.defer import Deferred\n-from twisted.internet.error import ProcessTerminated\n-from twisted.internet.protocol import ProcessProtocol\n-\n-if TYPE_CHECKING:\n-    from collections.abc import Iterable\n-\n-    from twisted.python.failure import Failure\n-\n-\n-class ProcessTest:\n-    command: str | None = None\n-    prefix = [sys.executable, \"-m\", \"scrapy.cmdline\"]\n-    cwd = os.getcwd()  # trial chdirs to temp dir  # noqa: PTH109\n-\n-    def execute(\n-        self,\n-        args: Iterable[str],\n-        check_code: bool = True,\n-        settings: str | None = None,\n-    ) -> Deferred[TestProcessProtocol]:\n-        from twisted.internet import reactor\n-\n-        env = os.environ.copy()\n-        if settings is not None:\n-            env[\"SCRAPY_SETTINGS_MODULE\"] = settings\n-        assert self.command\n-        cmd = [*self.prefix, self.command, *args]\n-        pp = TestProcessProtocol()\n-        pp.deferred.addCallback(self._process_finished, cmd, check_code)\n-        reactor.spawnProcess(pp, cmd[0], cmd, env=env, path=self.cwd)\n-        return pp.deferred\n-\n-    def _process_finished(\n-        self, pp: TestProcessProtocol, cmd: list[str], check_code: bool\n-    ) -> tuple[int, bytes, bytes]:\n-        if pp.exitcode and check_code:\n-            msg = f\"process {cmd} exit with code {pp.exitcode}\"\n-            msg += f\"\\n>>> stdout <<<\\n{pp.out.decode()}\"\n-            msg += \"\\n\"\n-            msg += f\"\\n>>> stderr <<<\\n{pp.err.decode()}\"\n-            raise RuntimeError(msg)\n-        return cast(int, pp.exitcode), pp.out, pp.err\n-\n-\n-class TestProcessProtocol(ProcessProtocol):\n-    def __init__(self) -> None:\n-        self.deferred: Deferred[TestProcessProtocol] = Deferred()\n-        self.out: bytes = b\"\"\n-        self.err: bytes = b\"\"\n-        self.exitcode: int | None = None\n-\n-    def outReceived(self, data: bytes) -> None:\n-        self.out += data\n-\n-    def errReceived(self, data: bytes) -> None:\n-        self.err += data\n-\n-    def processEnded(self, status: Failure) -> None:\n-        self.exitcode = cast(ProcessTerminated, status.value).exitCode\n-        self.deferred.callback(self)\n\n@@ -1,47 +0,0 @@\n-from urllib.parse import urljoin\n-\n-from twisted.web import resource, server, static, util\n-\n-\n-class SiteTest:\n-    def setUp(self):\n-        from twisted.internet import reactor\n-\n-        super().setUp()\n-        self.site = reactor.listenTCP(0, test_site(), interface=\"127.0.0.1\")\n-        self.baseurl = f\"http://localhost:{self.site.getHost().port}/\"\n-\n-    def tearDown(self):\n-        super().tearDown()\n-        self.site.stopListening()\n-\n-    def url(self, path: str) -> str:\n-        return urljoin(self.baseurl, path)\n-\n-\n-class NoMetaRefreshRedirect(util.Redirect):\n-    def render(self, request: server.Request) -> bytes:\n-        content = util.Redirect.render(self, request)\n-        return content.replace(\n-            b'http-equiv=\"refresh\"', b'http-no-equiv=\"do-not-refresh-me\"'\n-        )\n-\n-\n-def test_site():\n-    r = resource.Resource()\n-    r.putChild(b\"text\", static.Data(b\"Works\", \"text/plain\"))\n-    r.putChild(\n-        b\"html\",\n-        static.Data(\n-            b\"<body><p class='one'>Works</p><p class='two'>World</p></body>\",\n-            \"text/html\",\n-        ),\n-    )\n-    r.putChild(\n-        b\"enc-gb18030\",\n-        static.Data(b\"<p>gb18030 encoding</p>\", \"text/html; charset=gb18030\"),\n-    )\n-    r.putChild(b\"redirect\", util.Redirect(b\"/redirected\"))\n-    r.putChild(b\"redirect-no-meta-refresh\", NoMetaRefreshRedirect(b\"/redirected\"))\n-    r.putChild(b\"redirected\", static.Data(b\"Redirected here\", \"text/plain\"))\n-    return server.Site(r)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#daa1a7d0b6549f901a002bf4c8bb7c4aed23e068", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 54 | Lines Deleted: 65 | Files Changed: 4 | Hunks: 37 | Methods Changed: 30 | Complexity Δ (Sum/Max): -3/1 | Churn Δ: 119 | Churn Cumulative: 11809 | Contributors (this commit): 68 | Commits (past 90d): 12 | Contributors (cumulative): 100 | DMM Complexity: 0.0\n\nDIFF:\n@@ -12,6 +12,8 @@ def _py_files(folder):\n \n \n collect_ignore = [\n+    # may need extra deps\n+    \"docs/_ext\",\n     # not a test, but looks like a test\n     \"scrapy/utils/testproc.py\",\n     \"scrapy/utils/testsite.py\",\n@@ -46,12 +48,6 @@ if not H2_ENABLED:\n     )\n \n \n-@pytest.fixture\n-def chdir(tmpdir):\n-    \"\"\"Change to pytest-provided temporary directory\"\"\"\n-    tmpdir.chdir()\n-\n-\n def pytest_addoption(parser):\n     parser.addoption(\n         \"--reactor\",\n\n@@ -5,6 +5,7 @@ import csv\n import gzip\n import json\n import lzma\n+import os\n import random\n import shutil\n import string\n@@ -107,8 +108,13 @@ class TestFileFeedStorage:\n         self._assert_stores(FileFeedStorage(str(path)), path)\n \n     def test_store_direct_path_relative(self, tmp_path):\n-        path = (tmp_path / \"foo\" / \"bar\").relative_to(Path.cwd())\n+        old_cwd = Path.cwd()\n+        try:\n+            os.chdir(tmp_path)\n+            path = Path(\"foo\", \"bar\")\n             self._assert_stores(FileFeedStorage(str(path)), path)\n+        finally:\n+            os.chdir(old_cwd)\n \n     def test_interface(self, tmp_path):\n         path = tmp_path / \"file.txt\"\n@@ -236,24 +242,22 @@ class TestBlockingFeedStorage:\n     def test_default_temp_dir(self):\n         b = BlockingFeedStorage()\n \n-        tmp = b.open(self.get_test_spider())\n-        tmp_path = Path(tmp.name).parent\n-        assert str(tmp_path) == tempfile.gettempdir()\n+        storage_file = b.open(self.get_test_spider())\n+        storage_dir = Path(storage_file.name).parent\n+        assert str(storage_dir) == tempfile.gettempdir()\n \n-    def test_temp_file(self):\n+    def test_temp_file(self, tmp_path):\n         b = BlockingFeedStorage()\n \n-        tests_path = Path(__file__).resolve().parent\n-        spider = self.get_test_spider({\"FEED_TEMPDIR\": str(tests_path)})\n-        tmp = b.open(spider)\n-        tmp_path = Path(tmp.name).parent\n-        assert tmp_path == tests_path\n+        spider = self.get_test_spider({\"FEED_TEMPDIR\": str(tmp_path)})\n+        storage_file = b.open(spider)\n+        storage_dir = Path(storage_file.name).parent\n+        assert storage_dir == tmp_path\n \n-    def test_invalid_folder(self):\n+    def test_invalid_folder(self, tmp_path):\n         b = BlockingFeedStorage()\n \n-        tests_path = Path(__file__).resolve().parent\n-        invalid_path = tests_path / \"invalid_path\"\n+        invalid_path = tmp_path / \"invalid_path\"\n         spider = self.get_test_spider({\"FEED_TEMPDIR\": str(invalid_path)})\n \n         with pytest.raises(OSError, match=\"Not a Directory:\"):\n\n@@ -524,21 +524,20 @@ class TestFilesPipelineCustomSettings:\n             expected_value = settings.get(settings_attr)\n             assert getattr(pipeline_cls, pipe_inst_attr) == expected_value\n \n-    def test_file_pipeline_using_pathlike_objects(self):\n+    def test_file_pipeline_using_pathlike_objects(self, tmp_path):\n         class CustomFilesPipelineWithPathLikeDir(FilesPipeline):\n             def file_path(self, request, response=None, info=None, *, item=None):\n                 return Path(\"subdir\") / Path(request.url).name\n \n         pipeline = CustomFilesPipelineWithPathLikeDir.from_crawler(\n-            get_crawler(None, {\"FILES_STORE\": Path(\"./Temp\")})\n+            get_crawler(None, {\"FILES_STORE\": tmp_path})\n         )\n         request = Request(\"http://example.com/image01.jpg\")\n         assert pipeline.file_path(request) == Path(\"subdir/image01.jpg\")\n \n-    def test_files_store_constructor_with_pathlike_object(self):\n-        path = Path(\"./FileDir\")\n-        fs_store = FSFilesStore(path)\n-        assert fs_store.basedir == str(path)\n+    def test_files_store_constructor_with_pathlike_object(self, tmp_path):\n+        fs_store = FSFilesStore(tmp_path)\n+        assert fs_store.basedir == str(tmp_path)\n \n \n @pytest.mark.requires_botocore\n\n@@ -2,8 +2,7 @@\n Queues that handle requests\n \"\"\"\n \n-import shutil\n-import tempfile\n+from pathlib import Path\n \n import pytest\n import queuelib\n@@ -23,30 +22,17 @@ from scrapy.utils.test import get_crawler\n \n class TestBaseQueue:\n     def setup_method(self):\n-        self.tmpdir = tempfile.mkdtemp(prefix=\"scrapy-queue-tests-\")\n-        self.qpath = self.tempfilename()\n-        self.qdir = tempfile.mkdtemp()\n         self.crawler = get_crawler(Spider)\n \n-    def teardown_method(self):\n-        shutil.rmtree(self.tmpdir)\n-\n-    def tempfilename(self):\n-        with tempfile.NamedTemporaryFile(dir=self.tmpdir) as nf:\n-            return nf.name\n-\n-    def mkdtemp(self):\n-        return tempfile.mkdtemp(dir=self.tmpdir)\n-\n \n class RequestQueueTestMixin:\n-    def queue(self):\n+    def queue(self, base_path: Path):\n         raise NotImplementedError\n \n-    def test_one_element_with_peek(self):\n+    def test_one_element_with_peek(self, tmp_path):\n         if not hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n             pytest.skip(\"The queuelib queues do not define peek\")\n-        q = self.queue()\n+        q = self.queue(tmp_path)\n         assert len(q) == 0\n         assert q.peek() is None\n         assert q.pop() is None\n@@ -60,10 +46,10 @@ class RequestQueueTestMixin:\n         assert q.pop() is None\n         q.close()\n \n-    def test_one_element_without_peek(self):\n+    def test_one_element_without_peek(self, tmp_path):\n         if hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n             pytest.skip(\"The queuelib queues define peek\")\n-        q = self.queue()\n+        q = self.queue(tmp_path)\n         assert len(q) == 0\n         assert q.pop() is None\n         req = Request(\"http://www.example.com\")\n@@ -81,10 +67,10 @@ class RequestQueueTestMixin:\n \n \n class FifoQueueMixin(RequestQueueTestMixin):\n-    def test_fifo_with_peek(self):\n+    def test_fifo_with_peek(self, tmp_path):\n         if not hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n             pytest.skip(\"The queuelib queues do not define peek\")\n-        q = self.queue()\n+        q = self.queue(tmp_path)\n         assert len(q) == 0\n         assert q.peek() is None\n         assert q.pop() is None\n@@ -108,10 +94,10 @@ class FifoQueueMixin(RequestQueueTestMixin):\n         assert q.pop() is None\n         q.close()\n \n-    def test_fifo_without_peek(self):\n+    def test_fifo_without_peek(self, tmp_path):\n         if hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n-            pytest.skip(\"The queuelib queues do not define peek\")\n-        q = self.queue()\n+            pytest.skip(\"The queuelib queues define peek\")\n+        q = self.queue(tmp_path)\n         assert len(q) == 0\n         assert q.pop() is None\n         req1 = Request(\"http://www.example.com/1\")\n@@ -137,10 +123,10 @@ class FifoQueueMixin(RequestQueueTestMixin):\n \n \n class LifoQueueMixin(RequestQueueTestMixin):\n-    def test_lifo_with_peek(self):\n+    def test_lifo_with_peek(self, tmp_path):\n         if not hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n             pytest.skip(\"The queuelib queues do not define peek\")\n-        q = self.queue()\n+        q = self.queue(tmp_path)\n         assert len(q) == 0\n         assert q.peek() is None\n         assert q.pop() is None\n@@ -164,10 +150,10 @@ class LifoQueueMixin(RequestQueueTestMixin):\n         assert q.pop() is None\n         q.close()\n \n-    def test_lifo_without_peek(self):\n+    def test_lifo_without_peek(self, tmp_path):\n         if hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n-            pytest.skip(\"The queuelib queues do not define peek\")\n-        q = self.queue()\n+            pytest.skip(\"The queuelib queues define peek\")\n+        q = self.queue(tmp_path)\n         assert len(q) == 0\n         assert q.pop() is None\n         req1 = Request(\"http://www.example.com/1\")\n@@ -193,34 +179,38 @@ class LifoQueueMixin(RequestQueueTestMixin):\n \n \n class TestPickleFifoDiskQueueRequest(FifoQueueMixin, TestBaseQueue):\n-    def queue(self):\n-        return PickleFifoDiskQueue.from_crawler(crawler=self.crawler, key=\"pickle/fifo\")\n+    def queue(self, base_path):\n+        return PickleFifoDiskQueue.from_crawler(\n+            crawler=self.crawler, key=str(base_path / \"pickle\" / \"fifo\")\n+        )\n \n \n class TestPickleLifoDiskQueueRequest(LifoQueueMixin, TestBaseQueue):\n-    def queue(self):\n-        return PickleLifoDiskQueue.from_crawler(crawler=self.crawler, key=\"pickle/lifo\")\n+    def queue(self, base_path):\n+        return PickleLifoDiskQueue.from_crawler(\n+            crawler=self.crawler, key=str(base_path / \"pickle\" / \"lifo\")\n+        )\n \n \n class TestMarshalFifoDiskQueueRequest(FifoQueueMixin, TestBaseQueue):\n-    def queue(self):\n+    def queue(self, base_path):\n         return MarshalFifoDiskQueue.from_crawler(\n-            crawler=self.crawler, key=\"marshal/fifo\"\n+            crawler=self.crawler, key=str(base_path / \"marshal\" / \"fifo\")\n         )\n \n \n class TestMarshalLifoDiskQueueRequest(LifoQueueMixin, TestBaseQueue):\n-    def queue(self):\n+    def queue(self, base_path):\n         return MarshalLifoDiskQueue.from_crawler(\n-            crawler=self.crawler, key=\"marshal/lifo\"\n+            crawler=self.crawler, key=str(base_path / \"marshal\" / \"lifo\")\n         )\n \n \n class TestFifoMemoryQueueRequest(FifoQueueMixin, TestBaseQueue):\n-    def queue(self):\n+    def queue(self, base_path):\n         return FifoMemoryQueue.from_crawler(crawler=self.crawler)\n \n \n class TestLifoMemoryQueueRequest(LifoQueueMixin, TestBaseQueue):\n-    def queue(self):\n+    def queue(self, base_path):\n         return LifoMemoryQueue.from_crawler(crawler=self.crawler)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
