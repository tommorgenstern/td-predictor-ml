{"custom_id": "keras#6f607f6aa8dcb1de3a85e18c29b68abfea101bff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 8 | Files Changed: 3 | Hunks: 11 | Methods Changed: 3 | Complexity Δ (Sum/Max): 5/6 | Churn Δ: 16 | Churn Cumulative: 1189 | Contributors (this commit): 4 | Commits (past 90d): 14 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2,7 +2,7 @@ import unittest\n from numpy.testing import assert_allclose\n import numpy as np\n from keras import backend as K\n-\n+import pytest\n \n def get_standard_values():\n     '''\n@@ -88,6 +88,7 @@ class TestAdvancedActivations(unittest.TestCase):\n             config = layer.get_config()\n             assert config['alpha'] == alpha\n \n+    @pytest.mark.skipif(K._BACKEND=='tensorflow', reason=\"currently not working with TensorFlow\")\n     def test_parametric_softplus(self):\n         from keras.layers.advanced_activations import ParametricSoftplus\n         np.random.seed(1337)\n@@ -107,6 +108,7 @@ class TestAdvancedActivations(unittest.TestCase):\n                 assert config['alpha_init'] == alpha\n                 assert config['beta_init'] == beta\n \n+    @pytest.mark.skipif(K._BACKEND=='tensorflow', reason=\"currently not working with TensorFlow\")\n     def test_thresholded_linear(self):\n         from keras.layers.advanced_activations import ThresholdedLinear\n         np.random.seed(1337)\n@@ -126,6 +128,7 @@ class TestAdvancedActivations(unittest.TestCase):\n             config = layer.get_config()\n             assert config['theta'] == theta\n \n+    @pytest.mark.skipif(K._BACKEND=='tensorflow', reason=\"currently not working with TensorFlow\")\n     def test_thresholded_relu(self):\n         from keras.layers.advanced_activations import ThresholdedReLU\n         np.random.seed(1337)\n\n@@ -9,10 +9,8 @@ from keras import backend as K\n \n \n class TestMasking(unittest.TestCase):\n+    @pytest.mark.skipif(K._BACKEND=='tensorflow', reason=\"currently not working with TensorFlow\")\n     def test_masking(self):\n-        if K._BACKEND == 'tensorflow':\n-            # skip this test for TF: not supported yet\n-            return\n         X = np.array(\n             [[[1, 1], [2, 1], [3, 1], [5, 5]],\n              [[1, 5], [5, 0], [0, 0], [0, 0]]], dtype=np.int32)\n\n@@ -12,6 +12,7 @@ from keras.utils.test_utils import get_test_data\n import pickle\n import sys\n import os\n+import pytest\n \n input_dim = 32\n nb_hidden = 16\n@@ -137,9 +138,8 @@ class TestSequential(unittest.TestCase):\n         print(nloss)\n         assert(loss == nloss)\n \n+    @pytest.mark.skipif(K._BACKEND=='tensorflow', reason=\"currently not working with TensorFlow\")\n     def test_merge_dot1(self):\n-        if K._BACKEND == 'tensorflow':\n-            return\n \n         print('Test merge: dot')\n         left = Sequential()\n@@ -157,9 +157,8 @@ class TestSequential(unittest.TestCase):\n \n         model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n+    @pytest.mark.skipif(K._BACKEND=='tensorflow', reason=\"currently not working with TensorFlow\")\n     def test_merge_dot2(self):\n-        if K._BACKEND == 'tensorflow':\n-            return\n \n         print('Test merge: dot')\n         left = Sequential()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8ab149df8abb39dfd7f67b7fa06077e1b7c694f4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 13 | Churn Cumulative: 872 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -122,10 +122,9 @@ class TestBuildConfigParams(unittest.TestCase):\n class TestMasking(unittest.TestCase):\n     \"\"\"Test the Masking class\"\"\"\n \n+    @pytest.mark.skipif(K._BACKEND=='tensorflow', reason=\"currently not working with TensorFlow\")\n     def test_sequences(self):\n         \"\"\"Test masking sequences with zeroes as padding\"\"\"\n-        if K._BACKEND == \"tensorflow\":\n-            return\n         # integer inputs, one per timestep, like embeddings\n         layer = core.Masking()\n         func = K.function([layer.input], [layer.get_output_mask()])\n@@ -139,10 +138,9 @@ class TestMasking(unittest.TestCase):\n         output = func([input_data])[0]\n         self.assertTrue(np.all(output == expected))\n \n+    @pytest.mark.skipif(K._BACKEND=='tensorflow', reason=\"currently not working with TensorFlow\")\n     def test_non_zero(self):\n         \"\"\"Test masking with non-zero mask value\"\"\"\n-        if K._BACKEND == \"tensorflow\":\n-            return\n         layer = core.Masking(5)\n         func = K.function([layer.input], [layer.get_output_mask()])\n         input_data = np.array([[[1, 1], [2, 1], [3, 1], [5, 5]],\n@@ -152,10 +150,9 @@ class TestMasking(unittest.TestCase):\n         expected = np.array([[1, 1, 1, 0], [1, 1, 1, 1]])\n         self.assertTrue(np.all(output == expected))\n \n+    @pytest.mark.skipif(K._BACKEND=='tensorflow', reason=\"currently not working with TensorFlow\")\n     def test_non_zero_output(self):\n         \"\"\"Test output of masking layer with non-zero mask value\"\"\"\n-        if K._BACKEND == \"tensorflow\":\n-            return\n         layer = core.Masking(5)\n         func = K.function([layer.input], [layer.get_output()])\n \n\n@@ -135,10 +135,8 @@ def test_merge_sum():\n     assert(loss == nloss)\n \n \n+@pytest.mark.skipif(K._BACKEND=='tensorflow', reason=\"currently not working with TensorFlow\")\n def test_merge_dot():\n-    if K._BACKEND == 'tensorflow':\n-        return\n-\n     left = Sequential()\n     left.add(Dense(input_dim=input_dim, output_dim=nb_hidden))\n     left.add(Activation('relu'))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7dd3d062ff59dfcb11cb1df05098e4a0b80abd64", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 110 | Lines Deleted: 117 | Files Changed: 1 | Hunks: 6 | Methods Changed: 40 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 227 | Churn Cumulative: 430 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,5 @@\n import unittest\n+import pytest\n import numpy as np\n from numpy.testing import assert_allclose\n \n@@ -6,8 +7,7 @@ from keras import backend as K\n from keras.layers import core\n \n \n-class TestLayerBase(unittest.TestCase):\n-    def test_input_output(self):\n+def test_input_output():\n     nb_samples = 10\n     input_dim = 5\n     layer = core.Layer()\n@@ -20,7 +20,7 @@ class TestLayerBase(unittest.TestCase):\n         assert_allclose(K.eval(layer.get_input(train)), input)\n         assert_allclose(K.eval(layer.get_output(train)), input)\n \n-    def test_connections(self):\n+def test_connections():\n     nb_samples = 10\n     input_dim = 5\n     layer1 = core.Layer()\n@@ -36,12 +36,114 @@ class TestLayerBase(unittest.TestCase):\n         assert_allclose(K.eval(layer2.get_output(train)), input)\n \n \n-class TestBuildConfigParams(unittest.TestCase):\n-    \"\"\"\n-    Test the constructor, build, config and params functions of all layers in core.\n-    \"\"\"\n+def test_base():\n+    layer = core.Layer()\n+    _runner(layer)\n \n-    def _runner(self, layer):\n+def test_masked():\n+    layer = core.MaskedLayer()\n+    _runner(layer)\n+\n+def test_merge():\n+    layer_1 = core.Layer()\n+    layer_2 = core.Layer()\n+    layer_1.set_input_shape((None,))\n+    layer_2.set_input_shape((None,))\n+    layer = core.Merge([layer_1, layer_2])\n+    _runner(layer)\n+\n+def test_dropout():\n+    layer = core.Dropout(0.5)\n+    _runner(layer)\n+\n+def test_activation():\n+    layer = core.Activation('linear')\n+    _runner(layer)\n+\n+def test_reshape():\n+    layer = core.Reshape(dims=(10, 10))\n+    _runner(layer)\n+\n+def test_flatten():\n+    layer = core.Flatten()\n+    _runner(layer)\n+\n+def test_repeat_vector():\n+    layer = core.RepeatVector(10)\n+    _runner(layer)\n+\n+def test_dense():\n+    layer = core.Dense(10, input_shape=(10,))\n+    _runner(layer)\n+\n+def test_act_reg():\n+    layer = core.ActivityRegularization(0.5, 0.5)\n+    _runner(layer)\n+\n+def test_time_dist_dense():\n+    layer = core.TimeDistributedDense(10, input_shape=(None, 10))\n+    _runner(layer)\n+\n+def test_time_dist_merge():\n+    layer = core.TimeDistributedMerge()\n+    _runner(layer)\n+\n+def test_autoencoder():\n+    layer_1 = core.Layer()\n+    layer_2 = core.Layer()\n+\n+    layer = core.AutoEncoder(layer_1, layer_2)\n+    _runner(layer)\n+\n+def test_maxout_dense():\n+    layer = core.MaxoutDense(10, 10, input_shape=(20,))\n+    _runner(layer)\n+\n+\n+@pytest.mark.skipif(K._BACKEND=='tensorflow', reason=\"currently not working with TensorFlow\")\n+def test_sequences():\n+    \"\"\"Test masking sequences with zeroes as padding\"\"\"\n+    # integer inputs, one per timestep, like embeddings\n+    layer = core.Masking()\n+    func = K.function([layer.input], [layer.get_output_mask()])\n+    input_data = np.array([[[1], [2], [3], [0]],\n+                          [[0], [4], [5], [0]]], dtype=np.int32)\n+\n+    # This is the expected output mask, one dimension less\n+    expected = np.array([[1, 1, 1, 0], [0, 1, 1, 0]])\n+\n+    # get mask for this input\n+    output = func([input_data])[0]\n+    assert np.all(output == expected) , \"Output not as expected\"\n+\n+@pytest.mark.skipif(K._BACKEND=='tensorflow', reason=\"currently not working with TensorFlow\")\n+def test_non_zero():\n+    \"\"\"Test masking with non-zero mask value\"\"\"\n+    layer = core.Masking(5)\n+    func = K.function([layer.input], [layer.get_output_mask()])\n+    input_data = np.array([[[1, 1], [2, 1], [3, 1], [5, 5]],\n+                          [[1, 5], [5, 0], [0, 0], [0, 0]]],\n+                          dtype=np.int32)\n+    output = func([input_data])[0]\n+    expected = np.array([[1, 1, 1, 0], [1, 1, 1, 1]])\n+    assert np.all(output == expected) , \"Output not as expected\"\n+\n+@pytest.mark.skipif(K._BACKEND=='tensorflow', reason=\"currently not working with TensorFlow\")\n+def test_non_zero_output():\n+    \"\"\"Test output of masking layer with non-zero mask value\"\"\"\n+    layer = core.Masking(5)\n+    func = K.function([layer.input], [layer.get_output()])\n+\n+    input_data = np.array([[[1, 1], [2, 1], [3, 1], [5, 5]],\n+                          [[1, 5], [5, 0], [0, 0], [0, 0]]],\n+                          dtype=np.int32)\n+    output = func([input_data])[0]\n+    expected = np.array([[[1, 1], [2, 1], [3, 1], [0, 0]],\n+                        [[1, 5], [5, 0], [0, 0], [0, 0]]])\n+    assert np.all(output == expected) , \"Output not as expected\"\n+\n+\n+def _runner(layer):\n     assert isinstance(layer, core.Layer)\n     layer.build()\n     conf = layer.get_config()\n@@ -55,115 +157,6 @@ class TestBuildConfigParams(unittest.TestCase):\n     layer.trainable = True\n     layer.trainable = False\n \n-    def test_base(self):\n-        layer = core.Layer()\n-        self._runner(layer)\n-\n-    def test_masked(self):\n-        layer = core.MaskedLayer()\n-        self._runner(layer)\n-\n-    def test_merge(self):\n-        layer_1 = core.Layer()\n-        layer_2 = core.Layer()\n-        layer_1.set_input_shape((None,))\n-        layer_2.set_input_shape((None,))\n-        layer = core.Merge([layer_1, layer_2])\n-        self._runner(layer)\n-\n-    def test_dropout(self):\n-        layer = core.Dropout(0.5)\n-        self._runner(layer)\n-\n-    def test_activation(self):\n-        layer = core.Activation('linear')\n-        self._runner(layer)\n-\n-    def test_reshape(self):\n-        layer = core.Reshape(dims=(10, 10))\n-        self._runner(layer)\n-\n-    def test_flatten(self):\n-        layer = core.Flatten()\n-        self._runner(layer)\n-\n-    def test_repeat_vector(self):\n-        layer = core.RepeatVector(10)\n-        self._runner(layer)\n-\n-    def test_dense(self):\n-        layer = core.Dense(10, input_shape=(10,))\n-        self._runner(layer)\n-\n-    def test_act_reg(self):\n-        layer = core.ActivityRegularization(0.5, 0.5)\n-        self._runner(layer)\n-\n-    def test_time_dist_dense(self):\n-        layer = core.TimeDistributedDense(10, input_shape=(None, 10))\n-        self._runner(layer)\n-\n-    def test_time_dist_merge(self):\n-        layer = core.TimeDistributedMerge()\n-        self._runner(layer)\n-\n-    def test_autoencoder(self):\n-        layer_1 = core.Layer()\n-        layer_2 = core.Layer()\n-\n-        layer = core.AutoEncoder(layer_1, layer_2)\n-        self._runner(layer)\n-\n-    def test_maxout_dense(self):\n-        layer = core.MaxoutDense(10, 10, input_shape=(20,))\n-        self._runner(layer)\n-\n-\n-class TestMasking(unittest.TestCase):\n-    \"\"\"Test the Masking class\"\"\"\n-\n-    @pytest.mark.skipif(K._BACKEND=='tensorflow', reason=\"currently not working with TensorFlow\")\n-    def test_sequences(self):\n-        \"\"\"Test masking sequences with zeroes as padding\"\"\"\n-        # integer inputs, one per timestep, like embeddings\n-        layer = core.Masking()\n-        func = K.function([layer.input], [layer.get_output_mask()])\n-        input_data = np.array([[[1], [2], [3], [0]],\n-                              [[0], [4], [5], [0]]], dtype=np.int32)\n-\n-        # This is the expected output mask, one dimension less\n-        expected = np.array([[1, 1, 1, 0], [0, 1, 1, 0]])\n-\n-        # get mask for this input\n-        output = func([input_data])[0]\n-        self.assertTrue(np.all(output == expected))\n-\n-    @pytest.mark.skipif(K._BACKEND=='tensorflow', reason=\"currently not working with TensorFlow\")\n-    def test_non_zero(self):\n-        \"\"\"Test masking with non-zero mask value\"\"\"\n-        layer = core.Masking(5)\n-        func = K.function([layer.input], [layer.get_output_mask()])\n-        input_data = np.array([[[1, 1], [2, 1], [3, 1], [5, 5]],\n-                              [[1, 5], [5, 0], [0, 0], [0, 0]]],\n-                              dtype=np.int32)\n-        output = func([input_data])[0]\n-        expected = np.array([[1, 1, 1, 0], [1, 1, 1, 1]])\n-        self.assertTrue(np.all(output == expected))\n-\n-    @pytest.mark.skipif(K._BACKEND=='tensorflow', reason=\"currently not working with TensorFlow\")\n-    def test_non_zero_output(self):\n-        \"\"\"Test output of masking layer with non-zero mask value\"\"\"\n-        layer = core.Masking(5)\n-        func = K.function([layer.input], [layer.get_output()])\n-\n-        input_data = np.array([[[1, 1], [2, 1], [3, 1], [5, 5]],\n-                              [[1, 5], [5, 0], [0, 0], [0, 0]]],\n-                              dtype=np.int32)\n-        output = func([input_data])[0]\n-        expected = np.array([[[1, 1], [2, 1], [3, 1], [0, 0]],\n-                            [[1, 5], [5, 0], [0, 0], [0, 0]]])\n-        self.assertTrue(np.all(output == expected))\n-\n \n if __name__ == '__main__':\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8c1c86baf0220164c355198708a1c613d27d4132", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 431 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,3 @@\n-import unittest\n import pytest\n import numpy as np\n from numpy.testing import assert_allclose\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bf3ef82fa5b989725d5b0235a7149f13bc171378", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 26 | Churn Cumulative: 1054 | Contributors (this commit): 14 | Commits (past 90d): 33 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -76,9 +76,15 @@ class Sequential(Layer):\n \n     @property\n     def state_updates(self):\n+        \"\"\"\n+        Returns the `updates` from all layers in the sequence that are\n+        stateful.  This is useful for separating _training_ updates and\n+        _prediction_ updates for when we need to update a layers internal state\n+        during a stateful prediction.\n+        \"\"\"\n         state_updates = []\n         for l in self.layers:\n-            if l.trainable and getattr(l, 'stateful', False):\n+            if getattr(l, 'stateful', False):\n                 state_updates += l.get_params()[3]\n         return state_updates\n \n@@ -202,9 +208,15 @@ class Graph(Layer):\n \n     @property\n     def state_updates(self):\n+        \"\"\"\n+        Returns the `updates` from all nodes in that graph for nodes that are\n+        stateful.  This is useful for separating _training_ updates and\n+        _prediction_ updates for when we need to update a layers internal state\n+        during a stateful prediction.\n+        \"\"\"\n         state_updates = []\n         for l in self.nodes.values():\n-            if l.trainable and getattr(l, 'stateful', False):\n+            if getattr(l, 'stateful', False):\n                 state_updates += l.get_params()[3]\n         return state_updates\n \n\n@@ -55,6 +55,16 @@ def _runner(layer_class):\n     out3 = model.predict(np.ones((nb_samples, timesteps, input_dim)))\n     assert(out2.max() != out3.max())\n \n+    # check that we stay stateful during predictions\n+    out4 = model.predict(np.ones((nb_samples, timesteps, input_dim)))\n+    assert(out3.max() != out4.max())\n+\n+    # check that container-level reset_states() works\n+    model.reset_states()\n+    out5 = model.predict(np.ones((nb_samples, timesteps, input_dim)))\n+    assert(out4.max() != out5.max())\n+\n+\n \n class TestRNNS(unittest.TestCase):\n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bf8dc2f61fa9957985484f458c3c6e8a917e71c6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 796 | Contributors (this commit): 3 | Commits (past 90d): 15 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -407,15 +407,12 @@ def rnn(step_function, inputs, initial_states,\n     '''\n     inputs = inputs.dimshuffle((1, 0, 2))\n \n-    def _step(*args):\n-        global single_result\n-        input = args[0]\n-        states = args[1:]\n-        output, new_states = step_function(input, states)\n+    def _step(ins, *states):\n+        output, new_states = step_function(ins, states)\n         if masking:\n             # if all-zero input timestep, return\n             # all-zero output and unchanged states\n-            switch = T.any(input)\n+            switch = T.any(ins)\n             output = T.switch(switch, output, 0. * output)\n             return_states = []\n             for state, new_state in zip(states, new_states):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a4fff5aba3679a7d98297c9cc74764080a268c18", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -14/0 | Churn Δ: 2 | Churn Cumulative: 934 | Contributors (this commit): 14 | Commits (past 90d): 28 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -84,7 +84,7 @@ class Sequential(Layer):\n     def set_input(self):\n         for l in self.layers:\n             if hasattr(l, 'input'):\n-                ndim = len(K.get_shape(l.input))\n+                ndim = K.ndim(l.input)\n                 self.layers[0].input = K.placeholder(ndim=ndim)\n                 break\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#45c5f36399c08e71b8d5e64997b5a40cec50b92b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 14 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 37 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -1,15 +1 @@\n-\r\n-\"\"\"\r\n-Keras: Theano-based Deep Learning library\r\n-==================================\r\n-Keras is a minimalist, highly modular neural network library in \r\n-the spirit of Torch, written in Python / Theano so as not to have \r\n-to deal with the dearth of ecosystem in Lua. It was developed with \r\n-a focus on enabling fast experimentation. Being able to go from \r\n-idea to result with the least possible delay is key to doing \r\n-good research.\r\n-\r\n-See http://keras.io/\r\n-\"\"\"\r\n-\r\n __version__ = '0.3.0'\r\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#67332afc4658e5ab6a54865f3ae36bff3fe8f906", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 385 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -3,7 +3,6 @@ import pytest\n from numpy.testing import assert_allclose\n import numpy as np\n \n-if sys.version_info.major == 2:\n from keras.backend import theano_backend as KTH\n from keras.backend import tensorflow_backend as KTF\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7401f47da4d438927b982c3dcecf07313c05ffd7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 6 | Churn Cumulative: 1959 | Contributors (this commit): 13 | Commits (past 90d): 33 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -216,8 +216,12 @@ class Convolution2D(Layer):\n                             dim_ordering=self.dim_ordering,\n                             image_shape=self.input_shape,\n                             filter_shape=self.W_shape)\n-\n+        if self.dim_ordering == 'th':\n             output = conv_out + K.reshape(self.b, (1, self.nb_filter, 1, 1))\n+        elif self.dim_ordering == 'tf':\n+            output = conv_out + K.reshape(self.b, (1, 1, 1, self.nb_filter))\n+        else:\n+            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n         output = self.activation(output)\n         return output\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f04272e6978662d954fc25c4d8cf9329004061f9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2489 | Contributors (this commit): 13 | Commits (past 90d): 22 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -65,7 +65,7 @@ class Recurrent(MaskedLayer):\n         mask = self.get_output_mask(train)\n         if mask:\n             # apply mask\n-            X *= K.expand_dims(mask)\n+            X *= K.expand_dims(mask).astype(X.dtype)\n             masking = True\n         else:\n             masking = False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c6f68f8311c8918276d0a4dbb655c68726ad8da1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2491 | Contributors (this commit): 13 | Commits (past 90d): 23 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -65,7 +65,7 @@ class Recurrent(MaskedLayer):\n         mask = self.get_output_mask(train)\n         if mask:\n             # apply mask\n-            X *= K.expand_dims(mask).astype(X.dtype)\n+            X *= K.cast(K.expand_dims(mask), X.dtype)\n             masking = True\n         else:\n             masking = False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#314ffececb154867c61e7ec5b2baaca1d73321ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 1086 | Contributors (this commit): 3 | Commits (past 90d): 22 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -391,12 +391,12 @@ def rnn(step_function, inputs, initial_states,\n             # if all-zero input timestep, return\n             # all-zero output and unchanged states\n             switch = tf.reduce_any(input)\n-            output = tf.control_flow_ops.cond(switch,\n+            output = tf.python.control_flow_ops.cond(switch,\n                                                      lambda: output,\n                                                      lambda: 0. * output)\n             return_states = []\n             for state, new_state in zip(states, new_states):\n-                return_states.append(tf.control_flow_ops.cond(switch,\n+                return_states.append(tf.python.control_flow_ops.cond(switch,\n                                                                      lambda: new_state,\n                                                                      lambda: state))\n             states = return_states\n@@ -416,7 +416,7 @@ def rnn(step_function, inputs, initial_states,\n def switch(condition, then_expression, else_expression):\n     '''condition: scalar tensor.\n     '''\n-    return tf.control_flow_ops.cond(condition,\n+    return tf.python.control_flow_ops.cond(condition,\n                                            lambda: then_expression,\n                                            lambda: else_expression)\n \n\n@@ -36,7 +36,6 @@ def check_two_tensor_operation(function_name, x_input_shape,\n     assert_allclose(zth, ztf, atol=1e-05)\n \n \n-@pytest.mark.skipif(sys.version_info.major != 2, reason=\"Requires Python 2.7\")\n class TestBackend(object):\n \n     def test_linear_operations(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#23afa3f3b6cc56be419ef1e8459565578ab5cc6f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 932 | Contributors (this commit): 3 | Commits (past 90d): 23 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -407,12 +407,12 @@ def rnn(step_function, inputs, initial_states,\n     '''\n     inputs = inputs.dimshuffle((1, 0, 2))\n \n-    def _step(ins, *states):\n-        output, new_states = step_function(ins, states)\n+    def _step(input, *states):\n+        output, new_states = step_function(input, states)\n         if masking:\n             # if all-zero input timestep, return\n             # all-zero output and unchanged states\n-            switch = T.any(ins)\n+            switch = T.any(input)\n             output = T.switch(switch, output, 0. * output)\n             return_states = []\n             for state, new_state in zip(states, new_states):\n\n@@ -55,12 +55,12 @@ def _runner(layer_class):\n     out3 = model.predict(np.ones((nb_samples, timesteps, input_dim)))\n     assert(out2.max() != out3.max())\n \n-    # check that we stay stateful during predictions\n-    out4 = model.predict(np.ones((nb_samples, timesteps, input_dim)))\n-    assert(out3.max() != out4.max())\n-\n     # check that container-level reset_states() works\n     model.reset_states()\n+    out4 = model.predict(np.ones((nb_samples, timesteps, input_dim)))\n+    assert_allclose(out3, out4, atol=1e-5)\n+\n+    # check that the call to `predict` updated the states\n     out5 = model.predict(np.ones((nb_samples, timesteps, input_dim)))\n     assert(out4.max() != out5.max())\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e8e8f7625be33b009e81ec6457526382ef00d929", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 17/14 | Churn Δ: 17 | Churn Cumulative: 6620 | Contributors (this commit): 35 | Commits (past 90d): 122 | Contributors (cumulative): 46 | DMM Complexity: 0.0\n\nDIFF:\n@@ -20,6 +20,7 @@ class Sequential(Layer):\n \n     def __init__(self, layers=[]):\n         self.layers = []\n+        self.layer_cache = {}\n         for layer in layers:\n             self.add(layer)\n \n@@ -36,6 +37,7 @@ class Sequential(Layer):\n         self.layers[0].previous = layer\n \n     def add(self, layer):\n+        layer.layer_cache = self.layer_cache\n         self.layers.append(layer)\n         if len(self.layers) > 1:\n             self.layers[-1].set_previous(self.layers[-2])\n@@ -165,6 +167,7 @@ class Graph(Layer):\n         self.input_config = []  # dicts\n         self.output_config = []  # dicts\n         self.node_config = []  # dicts\n+        self.layer_cache = {}\n \n     @property\n     def nb_input(self):\n@@ -314,6 +317,7 @@ class Graph(Layer):\n             layer.set_previous(merge)\n \n         self.namespace.add(name)\n+        layer.layer_cache = self.layer_cache\n         self.nodes[name] = layer\n         self.node_config.append({'name': name,\n                                  'input': input,\n@@ -344,6 +348,7 @@ class Graph(Layer):\n         create_output -  Similar to create_output argument of add_node().\n             Output will be created only if merge_mode is given\n         '''\n+        layer.layer_cache = self.layer_cache\n         if name in self.namespace:\n             raise Exception('Duplicate node identifier: ' + name)\n         for o in outputs:\n\n@@ -114,7 +114,17 @@ class Layer(object):\n \n     def get_input(self, train=False):\n         if hasattr(self, 'previous'):\n-            return self.previous.get_output(train=train)\n+            # to avoid redundant computations,\n+            # layer outputs are cached when possible.\n+            if hasattr(self, 'layer_cache'):\n+                previous_layer_id = id(self.previous)\n+                if previous_layer_id in self.layer_cache:\n+                    return self.layer_cache[previous_layer_id]\n+            previous_output = self.previous.get_output(train=train)\n+            if hasattr(self, 'layer_cache'):\n+                previous_layer_id = id(self.previous)\n+                self.layer_cache[previous_layer_id] = previous_output\n+            return previous_output\n         elif hasattr(self, 'input'):\n             return self.input\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#febf9b2164dbc12461f28cad5e6965420371f812", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 5685 | Contributors (this commit): 32 | Commits (past 90d): 94 | Contributors (cumulative): 32 | DMM Complexity: None\n\nDIFF:\n@@ -117,12 +117,12 @@ class Layer(object):\n             # to avoid redundant computations,\n             # layer outputs are cached when possible.\n             if hasattr(self, 'layer_cache'):\n-                previous_layer_id = id(self.previous)\n+                previous_layer_id = '%s_%s' % (id(self.previous), train)\n                 if previous_layer_id in self.layer_cache:\n                     return self.layer_cache[previous_layer_id]\n             previous_output = self.previous.get_output(train=train)\n             if hasattr(self, 'layer_cache'):\n-                previous_layer_id = id(self.previous)\n+                previous_layer_id = '%s_%s' % (id(self.previous), train)\n                 self.layer_cache[previous_layer_id] = previous_output\n             return previous_output\n         elif hasattr(self, 'input'):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#71952f29c7439cab7b363093a8fc2259d9ba387f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 75 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): -14/0 | Churn Δ: 75 | Churn Cumulative: 188 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,87 +1,12 @@\n # -*- coding: utf-8 -*-\n from __future__ import absolute_import\n-from __future__ import print_function\n from .data_utils import get_file\n-import string\n import random\n-import os\n from six.moves import cPickle\n from six.moves import zip\n import numpy as np\n \n \n-def make_reuters_dataset(path=os.path.join('datasets', 'temp', 'reuters21578'), min_samples_per_topic=15):\n-    import re\n-    from ..preprocessing.text import Tokenizer\n-\n-    wire_topics = []\n-    topic_counts = {}\n-    wire_bodies = []\n-\n-    for fname in os.listdir(path):\n-        if 'sgm' in fname:\n-            s = open(os.path.join(path, fname)).read()\n-            tag = '<TOPICS>'\n-            while tag in s:\n-                s = s[s.find(tag)+len(tag):]\n-                topics = s[:s.find('</')]\n-                if topics and '</D><D>' not in topics:\n-                    topic = topics.replace('<D>', '').replace('</D>', '')\n-                    wire_topics.append(topic)\n-                    topic_counts[topic] = topic_counts.get(topic, 0) + 1\n-                else:\n-                    continue\n-\n-                bodytag = '<BODY>'\n-                body = s[s.find(bodytag)+len(bodytag):]\n-                body = body[:body.find('</')]\n-                wire_bodies.append(body)\n-\n-    # only keep most common topics\n-    items = list(topic_counts.items())\n-    items.sort(key=lambda x: x[1])\n-    kept_topics = set()\n-    for x in items:\n-        print(x[0] + ': ' + str(x[1]))\n-        if x[1] >= min_samples_per_topic:\n-            kept_topics.add(x[0])\n-    print('-')\n-    print('Kept topics:', len(kept_topics))\n-\n-    # filter wires with rare topics\n-    kept_wires = []\n-    labels = []\n-    topic_indexes = {}\n-    for t, b in zip(wire_topics, wire_bodies):\n-        if t in kept_topics:\n-            if t not in topic_indexes:\n-                topic_index = len(topic_indexes)\n-                topic_indexes[t] = topic_index\n-            else:\n-                topic_index = topic_indexes[t]\n-\n-            labels.append(topic_index)\n-            kept_wires.append(b)\n-\n-    # vectorize wires\n-    tokenizer = Tokenizer()\n-    tokenizer.fit_on_texts(kept_wires)\n-    X = tokenizer.texts_to_sequences(kept_wires)\n-\n-    print('Sanity check:')\n-    for w in [\"banana\", \"oil\", \"chocolate\", \"the\", \"dsft\"]:\n-        print('...index of', w, ':', tokenizer.word_index.get(w))\n-    print('text reconstruction:')\n-    reverse_word_index = dict([(v, k) for k, v in tokenizer.word_index.items()])\n-    print(' '.join(reverse_word_index[i] for i in X[10]))\n-\n-    dataset = (X, labels)\n-    print('-')\n-    print('Saving...')\n-    cPickle.dump(dataset, open(os.path.join('datasets', 'data', 'reuters.pkl'), 'w'))\n-    cPickle.dump(tokenizer.word_index, open(os.path.join('datasets', 'data', 'reuters_word_index.pkl'), 'w'))\n-\n-\n def load_data(path=\"reuters.pkl\", nb_words=None, skip_top=0, maxlen=None, test_split=0.2, seed=113,\n               start_char=1, oov_char=2, index_from=3):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a5d8b6378e6d65a95a7bc62f784373eae7e0af3f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 69 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -3,6 +3,7 @@ import numpy as np\n \n from keras.utils.test_utils import get_test_data\n from keras.utils import np_utils\n+from keras import backend as K\n \n from keras.models import Sequential\n from keras.layers.core import Dense, Activation\n@@ -36,6 +37,7 @@ y_test = np_utils.to_categorical(y_test, nb_classes=nb_class)\n                                                                      output_shape=(1,))\n \n \n+@pytest.mark.skipif(K._BACKEND=='tensorflow', reason=\"currently not working with TensorFlow\")\n def test_keras_classifier():\n     model = Sequential()\n     model.add(Dense(input_dim, input_shape=(input_dim,)))\n@@ -51,6 +53,7 @@ def test_keras_classifier():\n     sklearn_clf.score(X_test, y_test)\n \n \n+@pytest.mark.skipif(K._BACKEND=='tensorflow', reason=\"currently not working with TensorFlow\")\n def test_keras_regressor():\n     model = Sequential()\n     model.add(Dense(input_dim, input_shape=(input_dim,)))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#40864d89974a2b0a2f97aaf7bfdd66659c8ceae2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1066 | Lines Deleted: 361 | Files Changed: 8 | Hunks: 139 | Methods Changed: 33 | Complexity Δ (Sum/Max): -17/0 | Churn Δ: 1427 | Churn Cumulative: 13622 | Contributors (this commit): 50 | Commits (past 90d): 223 | Contributors (cumulative): 96 | DMM Complexity: 0.14285714285714285\n\nDIFF:\n@@ -1,10 +1,25 @@\n from .. import initializations\n-from ..layers.core import Layer, MaskedLayer\n+from ..layers.core import MaskedLayer\n from .. import backend as K\n import numpy as np\n \n \n class LeakyReLU(MaskedLayer):\n+    '''Special version of a Rectified Linear Unit\n+    that allows a small gradient when the unit is not active\n+    (`f(x) = alpha*x for x < 0`).\n+\n+    # Input shape\n+        Arbitrary. Use the keyword argument `input_shape`\n+        (tuple of integers, does not include the samples axis)\n+        when using this layer as the first layer in a model.\n+\n+    # Output shape\n+        Same shape as the input.\n+\n+    # Arguments\n+        alpha: float >= 0. Negative slope coefficient.\n+    '''\n     def __init__(self, alpha=0.3, **kwargs):\n         super(LeakyReLU, self).__init__(**kwargs)\n         self.alpha = alpha\n@@ -22,10 +37,20 @@ class LeakyReLU(MaskedLayer):\n \n class PReLU(MaskedLayer):\n     '''\n-        Reference:\n-            Delving Deep into Rectifiers: Surpassing Human-Level\n-            Performance on ImageNet Classification\n-                http://arxiv.org/pdf/1502.01852v1.pdf\n+    # Input shape\n+        Arbitrary. Use the keyword argument `input_shape`\n+        (tuple of integers, does not include the samples axis)\n+        when using this layer as the first layer in a model.\n+\n+    # Output shape\n+        Same shape as the input.\n+\n+    # Arguments:\n+        init: initialization function for the weights.\n+        weights: initial weights, as a list of a single numpy array.\n+\n+    # References:\n+        - [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](http://arxiv.org/pdf/1502.01852v1.pdf)\n     '''\n     def __init__(self, init='zero', weights=None, **kwargs):\n         self.init = initializations.get(init)\n@@ -55,6 +80,21 @@ class PReLU(MaskedLayer):\n \n \n class ELU(MaskedLayer):\n+    '''\n+    # Input shape\n+        Arbitrary. Use the keyword argument `input_shape`\n+        (tuple of integers, does not include the samples axis)\n+        when using this layer as the first layer in a model.\n+\n+    # Output shape\n+        Same shape as the input.\n+\n+    # Arguments\n+        alpha: scale for the negative factor.\n+\n+    # References\n+        - [Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)](http://arxiv.org/pdf/1511.07289v1.pdf)\n+    '''\n     def __init__(self, alpha=1.0, **kwargs):\n         super(ELU, self).__init__(**kwargs)\n         self.alpha = alpha\n@@ -73,13 +113,23 @@ class ELU(MaskedLayer):\n \n \n class ParametricSoftplus(MaskedLayer):\n-    '''\n-        Parametric Softplus of the form: alpha * log(1 + exp(beta * X))\n+    '''Parametric Softplus of the form: alpha * log(1 + exp(beta * X))\n \n-        Reference:\n-            Inferring Nonlinear Neuronal Computation\n-            Based on Physiologically Plausible Inputs\n-            http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003143\n+    # Input shape\n+        Arbitrary. Use the keyword argument `input_shape`\n+        (tuple of integers, does not include the samples axis)\n+        when using this layer as the first layer in a model.\n+\n+    # Output shape\n+        Same shape as the input.\n+\n+    # Arguments\n+        alpha_init: float. Initial value of the alpha weights.\n+        beta_init: float. Initial values of the beta weights.\n+        weights: initial weights, as a list of 2 numpy arrays.\n+\n+    # References:\n+        - [Inferring Nonlinear Neuronal Computation Based on Physiologically Plausible Inputs](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003143)\n     '''\n     def __init__(self, alpha_init=0.2, beta_init=5.0,\n                  weights=None, **kwargs):\n@@ -111,12 +161,21 @@ class ParametricSoftplus(MaskedLayer):\n \n \n class ThresholdedLinear(MaskedLayer):\n-    '''\n-        Thresholded Linear Activation\n+    '''Thresholded Linear Activation.\n \n-        Reference:\n-            Zero-Bias Autoencoders and the Benefits of Co-Adapting Features\n-            http://arxiv.org/pdf/1402.3337.pdf\n+    # Input shape\n+        Arbitrary. Use the keyword argument `input_shape`\n+        (tuple of integers, does not include the samples axis)\n+        when using this layer as the first layer in a model.\n+\n+    # Output shape\n+        Same shape as the input.\n+\n+    # Arguments\n+        theta: float >= 0. Threshold location of activation.\n+\n+    # References\n+        [Zero-Bias Autoencoders and the Benefits of Co-Adapting Features](http://arxiv.org/pdf/1402.3337.pdf)\n     '''\n     def __init__(self, theta=1.0, **kwargs):\n         super(ThresholdedLinear, self).__init__(**kwargs)\n@@ -134,12 +193,21 @@ class ThresholdedLinear(MaskedLayer):\n \n \n class ThresholdedReLU(MaskedLayer):\n-    '''\n-        Thresholded Rectified Activation\n+    '''Thresholded Rectified Activation.\n \n-        Reference:\n-            Zero-Bias Autoencoders and the Benefits of Co-Adapting Features\n-            http://arxiv.org/pdf/1402.3337.pdf\n+    # Input shape\n+        Arbitrary. Use the keyword argument `input_shape`\n+        (tuple of integers, does not include the samples axis)\n+        when using this layer as the first layer in a model.\n+\n+    # Output shape\n+        Same shape as the input.\n+\n+    # Arguments\n+        theta: float >= 0. Threshold location of activation.\n+\n+    # References\n+        [Zero-Bias Autoencoders and the Benefits of Co-Adapting Features](http://arxiv.org/pdf/1402.3337.pdf)\n     '''\n     def __init__(self, theta=1.0, **kwargs):\n         super(ThresholdedReLU, self).__init__(**kwargs)\n\n@@ -9,15 +9,15 @@ from six.moves import range\n \n \n class Sequential(Layer):\n-    '''\n-        Simple linear stack of layers.\n+    '''The Sequential container is a linear stack of layers.\n+    Apart from the `add` methods and the `layers` constructor argument,\n+    the API is identical to that of the `Layer` class.\n \n-        inherited from Layer:\n-        - get_params\n-        - get_output_mask\n-        - supports_masked_input\n-    '''\n+    This class is also the basis for the `keras.models.Sequential` model.\n \n+    # Arguments\n+        layers: list of layers to be added to the container.\n+    '''\n     def __init__(self, layers=[]):\n         self.layers = []\n         for layer in layers:\n@@ -114,27 +114,22 @@ class Sequential(Layer):\n             weights = weights[nb_param:]\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n-                \"layers\": [layer.get_config() for layer in self.layers]}\n+        return {'name': self.__class__.__name__,\n+                'layers': [layer.get_config() for layer in self.layers]}\n \n     def count_params(self):\n         return sum([layer.count_params() for layer in self.layers])\n \n \n class Graph(Layer):\n-    '''\n-        Implement a NN graph with arbitrary layer connections,\n+    '''Implement a NN graph with arbitrary layer connections,\n     arbitrary number of inputs and arbitrary number of outputs.\n \n-        Note: Graph can only be used as a layer\n+    This class is also the basis for the `keras.models.Graph` model.\n+\n+    Note: `Graph` can only be used as a layer\n     (connect, input, get_input, get_output)\n     when it has exactly one input and one output.\n-\n-        inherited from Layer:\n-            - get_output_mask\n-            - supports_masked_input\n-            - get_weights\n-            - set_weights\n     '''\n     def __init__(self):\n         self.namespace = set()  # strings\n@@ -189,12 +184,14 @@ class Graph(Layer):\n \n     def set_previous(self, layer, connection_map={}):\n         if self.nb_input != layer.nb_output:\n-            raise Exception('Cannot connect layers: input count does not match output count.')\n+            raise Exception('Cannot connect layers: '\n+                            'input count does not match output count.')\n         if self.nb_input == 1:\n             self.inputs[self.input_order[0]].set_previous(layer)\n         else:\n             if not connection_map:\n-                raise Exception('Cannot attach multi-input layer: no connection_map provided.')\n+                raise Exception('Cannot attach multi-input layer: '\n+                                'no connection_map provided.')\n             for k, v in connection_map.items():\n                 if k in self.inputs and v in layer.outputs:\n                     self.inputs[k].set_previous(layer.outputs[v])\n@@ -226,7 +223,18 @@ class Graph(Layer):\n         else:\n             return dict([(k, v.get_output(train)) for k, v in self.outputs.items()])\n \n-    def add_input(self, name, input_shape=None, batch_input_shape=None, dtype='float'):\n+    def add_input(self, name, input_shape=None,\n+                  batch_input_shape=None, dtype='float'):\n+        '''Add an input to the graph.\n+\n+        # Arguments:\n+            name: string. The name of the new input. Must be unique in the graph.\n+            input_shape: a tuple of integers, the expected shape of the input samples.\n+                Does not include the batch size.\n+            batch_input_shape: a tuple of integers, the expected shape of the\n+                whole input batch, including the batch size.\n+            dtype: 'float' or 'int'.\n+        '''\n         if name in self.namespace:\n             raise Exception('Duplicate node identifier: ' + name)\n         self.namespace.add(name)\n@@ -253,6 +261,25 @@ class Graph(Layer):\n     def add_node(self, layer, name, input=None, inputs=[],\n                  merge_mode='concat', concat_axis=-1, dot_axes=-1,\n                  create_output=False):\n+        '''Add a node in the graph. It can be connected to multiple\n+        inputs, which will first be merged into one tensor\n+        according to the mode specified.\n+\n+        # Arguments\n+            layer: the layer at the node.\n+            name: name for the node.\n+            input: when connecting the layer to a single input,\n+                this is the name of the incoming node.\n+            inputs: when connecting the layer to multiple inputs,\n+                this is a list of names of incoming nodes.\n+            merge_mode: one of {concat, sum, dot, ave, mul}\n+            concat_axis: when `merge_mode=='concat'`, this is the\n+                input concatenation axis.\n+            dot_axes: when `merge_mode='dot'`, this is the contraction axes\n+                specification; see the `Merge layer for details.\n+            create_output: boolean. Set this to `True` if you want the output\n+                of your node to be an output of the graph.\n+        '''\n         if name in self.namespace:\n             raise Exception('Duplicate node identifier: ' + name)\n         if input:\n@@ -291,20 +318,22 @@ class Graph(Layer):\n     def add_shared_node(self, layer, name, inputs=[], merge_mode=None,\n                         concat_axis=-1, dot_axes=-1, outputs=[],\n                         create_output=False):\n-        '''\n-        Used to shared / multi input-multi output node\n+        '''Used to share a same layer across multiple nodes.\n \n-        Arguments\n-        ------------\n-        layer - The layer to be shared across multiple inputs\n-        name - Name of the shared layer\n-        inputs - List of names of input nodes\n-        merge_mode - Similar to merge_mode argument of add_node()\n-        concat_axis - Similar to concat_axis argument of add_node()\n-        dot_axes - Similar to dot_axes argument of add_node()\n-        outputs - Names for output nodes. Used when merge_mode = None\n-        create_output -  Similar to create_output argument of add_node().\n-            Output will be created only if merge_mode is given\n+        Supposed, for instance, that you want to apply one same `Dense`\n+        layer after to the output of two different nodes.\n+        You can then add the `Dense` layer as a shared node.\n+\n+        # Arguments\n+            layer: The layer to be shared across multiple inputs\n+            name: Name of the shared node\n+            inputs: List of names of input nodes\n+            merge_mode: Same meaning as `merge_mode` argument of `add_node()`\n+            concat_axis: Same meaning as `concat_axis` argument of `add_node()`\n+            dot_axes: Same meaning as `dot_axes` argument of `add_node()`\n+            outputs: Used when `merge_mode=None`. Names for the output nodes.\n+            create_output: Same meaning as `create_output` argument of `add_node()`.\n+                When creating an output, `merge_mode` must be specified.\n         '''\n         if name in self.namespace:\n             raise Exception('Duplicate node identifier: ' + name)\n@@ -313,7 +342,7 @@ class Graph(Layer):\n                 raise Exception('Duplicate node identifier: ' + o)\n         if merge_mode:\n             if merge_mode not in {'sum', 'ave', 'mul', 'dot', 'cos', 'concat', 'join'}:\n-                raise Exception(\"Invalid merge mode\")\n+                raise Exception('Invalid merge mode')\n         layers = []\n         for i in range(len(inputs)):\n             input = inputs[i]\n@@ -334,7 +363,9 @@ class Graph(Layer):\n                 layers.append(n)\n             else:\n                 raise Exception('Unknown identifier: ' + input)\n-        s = Siamese(layer, layers, merge_mode, concat_axis=concat_axis, dot_axes=dot_axes)\n+        s = Siamese(layer, layers, merge_mode,\n+                    concat_axis=concat_axis,\n+                    dot_axes=dot_axes)\n         self.namespace.add(name)\n         self.nodes[name] = s\n         self.node_config.append({'name': name,\n@@ -358,11 +389,27 @@ class Graph(Layer):\n \n         if create_output and merge_mode:\n             if merge_mode == 'join':\n-                raise Exception(\"Output can not be of type OrderedDict\")\n+                raise Exception('Output can not be of type OrderedDict')\n             self.add_output(name, input=name)\n \n     def add_output(self, name, input=None, inputs=[],\n                    merge_mode='concat', concat_axis=-1, dot_axes=-1):\n+        '''Add an output to the graph.\n+\n+        This output can merge several node outputs into a single output.\n+\n+        # Arguments\n+            name: name of the output.\n+            input: when connecting the layer to a single input,\n+                this is the name of the incoming node.\n+            inputs: when connecting the layer to multiple inputs,\n+                this is a list of names of incoming nodes.\n+            merge_mode: one of {concat, sum, dot, ave, mul}\n+            concat_axis: when `merge_mode=='concat'`, this is the\n+                input concatenation axis.\n+            dot_axes: when `merge_mode='dot'`, this is the contraction axes\n+                specification; see the `Merge layer for details.\n+        '''\n         if name in self.output_order:\n             raise Exception('Duplicate output identifier: ' + name)\n         if input:\n@@ -391,13 +438,13 @@ class Graph(Layer):\n                                    'dot_axes': dot_axes})\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n-                \"input_config\": self.input_config,\n-                \"node_config\": self.node_config,\n-                \"output_config\": self.output_config,\n-                \"input_order\": self.input_order,\n-                \"output_order\": self.output_order,\n-                \"nodes\": dict([(c[\"name\"], self.nodes[c[\"name\"]].get_config()) for c in self.node_config])}\n+        return {'name': self.__class__.__name__,\n+                'input_config': self.input_config,\n+                'node_config': self.node_config,\n+                'output_config': self.output_config,\n+                'input_order': self.input_order,\n+                'output_order': self.output_order,\n+                'nodes': dict([(c['name'], self.nodes[c['name']].get_config()) for c in self.node_config])}\n \n     def count_params(self):\n         return sum([layer.count_params() for layer in self.nodes.values()])\n\n@@ -18,6 +18,54 @@ def conv_output_length(input_length, filter_size, border_mode, stride):\n \n \n class Convolution1D(Layer):\n+    '''Convolution operator for filtering neighborhoods of one-dimensional inputs.\n+    When using this layer as the first layer in a model,\n+    either provide the keyword argument `input_dim`\n+    (int, e.g. 128 for sequences of 128-dimensional vectors),\n+    or `input_shape` (tuple of integers, e.g. (10, 128) for sequences\n+    of 10 vectors of 128-dimensional vectors).\n+\n+    # Input shape\n+        3D tensor with shape: `(samples, steps, input_dim)`.\n+\n+    # Output shape\n+        3D tensor with shape: `(samples, new_steps, nb_filter)`.\n+        `steps` value might have changed due to padding.\n+\n+    # Arguments\n+        nb_filter: Number of convolution kernels to use\n+            (dimensionality of the output).\n+        filter_length: The extension (spatial or temporal) of each filter.\n+        init: name of initialization function for the weights of the layer\n+            (see [initializations](../initializations.md)),\n+            or alternatively, Theano function to use for weights initialization.\n+            This parameter is only relevant if you don't pass a `weights` argument.\n+        activation: name of activation function to use\n+            (see [activations](../activations.md)),\n+            or alternatively, elementwise Theano function.\n+            If you don't specify anything, no activation is applied\n+            (ie. \"linear\" activation: a(x) = x).\n+        weights: list of numpy arrays to set as initial weights.\n+        border_mode: 'valid' or 'same'.\n+        subsample_length: factor by which to subsample output.\n+        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n+            (eg. L1 or L2 regularization), applied to the main weights matrix.\n+        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n+            applied to the bias.\n+        activity_regularizer: instance of [ActivityRegularizer](../regularizers.md),\n+            applied to the network output.\n+        W_constraint: instance of the [constraints](../constraints.md) module\n+            (eg. maxnorm, nonneg), applied to the main weights matrix.\n+        b_constraint: instance of the [constraints](../constraints.md) module,\n+            applied to the bias.\n+        input_dim: Number of channels/dimensions in the input.\n+            Either this argument or the keyword argument `input_shape`must be\n+            provided when using this layer as the first layer in a model.\n+        input_length: Length of input sequences, when it is constant.\n+            This argument is required if you are going to connect\n+            `Flatten` then `Dense` layers upstream\n+            (without it, the shape of the dense outputs cannot be computed).\n+    '''\n     input_ndim = 3\n \n     def __init__(self, nb_filter, filter_length,\n@@ -103,25 +151,75 @@ class Convolution1D(Layer):\n         return output\n \n     def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"nb_filter\": self.nb_filter,\n-                  \"filter_length\": self.filter_length,\n-                  \"init\": self.init.__name__,\n-                  \"activation\": self.activation.__name__,\n-                  \"border_mode\": self.border_mode,\n-                  \"subsample_length\": self.subsample_length,\n-                  \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n-                  \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n-                  \"activity_regularizer\": self.activity_regularizer.get_config() if self.activity_regularizer else None,\n-                  \"W_constraint\": self.W_constraint.get_config() if self.W_constraint else None,\n-                  \"b_constraint\": self.b_constraint.get_config() if self.b_constraint else None,\n-                  \"input_dim\": self.input_dim,\n-                  \"input_length\": self.input_length}\n+        config = {'name': self.__class__.__name__,\n+                  'nb_filter': self.nb_filter,\n+                  'filter_length': self.filter_length,\n+                  'init': self.init.__name__,\n+                  'activation': self.activation.__name__,\n+                  'border_mode': self.border_mode,\n+                  'subsample_length': self.subsample_length,\n+                  'W_regularizer': self.W_regularizer.get_config() if self.W_regularizer else None,\n+                  'b_regularizer': self.b_regularizer.get_config() if self.b_regularizer else None,\n+                  'activity_regularizer': self.activity_regularizer.get_config() if self.activity_regularizer else None,\n+                  'W_constraint': self.W_constraint.get_config() if self.W_constraint else None,\n+                  'b_constraint': self.b_constraint.get_config() if self.b_constraint else None,\n+                  'input_dim': self.input_dim,\n+                  'input_length': self.input_length}\n         base_config = super(Convolution1D, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n class Convolution2D(Layer):\n+    '''Convolution operator for filtering windows of two-dimensional inputs.\n+    When using this layer as the first layer in a model,\n+    provide the keyword argument `input_shape`\n+    (tuple of integers, does not include the sample axis),\n+    e.g. `input_shape=(3, 128, 128)` for 128x128 RGB pictures.\n+\n+    # Input shape\n+        4D tensor with shape:\n+        `(samples, channels, rows, cols)` if dim_ordering='th'\n+        or 4D tensor with shape:\n+        `(samples, rows, cols, channels)` if dim_ordering='tf'.\n+\n+    # Output shape\n+        4D tensor with shape:\n+        `(samples, nb_filter, nb_row, nb_col)` if dim_ordering='th'\n+        or 4D tensor with shape:\n+        `(samples, nb_row, nb_col, nb_filter)` if dim_ordering='tf'.\n+\n+\n+    # Arguments\n+        nb_filter: Number of convolution filters to use.\n+        nb_row: Number of rows in the convolution kernel.\n+        nb_col: Number of columns in the convolution kernel.\n+        init: name of initialization function for the weights of the layer\n+            (see [initializations](../initializations.md)), or alternatively,\n+            Theano function to use for weights initialization.\n+            This parameter is only relevant if you don't pass\n+            a `weights` argument.\n+        activation: name of activation function to use\n+            (see [activations](../activations.md)),\n+            or alternatively, elementwise Theano function.\n+            If you don't specify anything, no activation is applied\n+            (ie. \"linear\" activation: a(x) = x).\n+        weights: list of numpy arrays to set as initial weights.\n+        border_mode: 'valid' or 'same'.\n+        subsample: tuple of length 2. Factor by which to subsample output.\n+            Also called strides elsewhere.\n+        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n+            (eg. L1 or L2 regularization), applied to the main weights matrix.\n+        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n+            applied to the bias.\n+        activity_regularizer: instance of [ActivityRegularizer](../regularizers.md),\n+            applied to the network output.\n+        W_constraint: instance of the [constraints](../constraints.md) module\n+            (eg. maxnorm, nonneg), applied to the main weights matrix.\n+        b_constraint: instance of the [constraints](../constraints.md) module,\n+            applied to the bias.\n+        dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n+            (the depth) is at index 1, in 'tf' mode is it at index 3.\n+    '''\n     input_ndim = 4\n \n     def __init__(self, nb_filter, nb_row, nb_col,\n@@ -226,30 +324,32 @@ class Convolution2D(Layer):\n         return output\n \n     def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"nb_filter\": self.nb_filter,\n-                  \"nb_row\": self.nb_row,\n-                  \"nb_col\": self.nb_col,\n-                  \"init\": self.init.__name__,\n-                  \"activation\": self.activation.__name__,\n-                  \"border_mode\": self.border_mode,\n-                  \"subsample\": self.subsample,\n-                  \"dim_ordering\": self.dim_ordering,\n-                  \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n-                  \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n-                  \"activity_regularizer\": self.activity_regularizer.get_config() if self.activity_regularizer else None,\n-                  \"W_constraint\": self.W_constraint.get_config() if self.W_constraint else None,\n-                  \"b_constraint\": self.b_constraint.get_config() if self.b_constraint else None}\n+        config = {'name': self.__class__.__name__,\n+                  'nb_filter': self.nb_filter,\n+                  'nb_row': self.nb_row,\n+                  'nb_col': self.nb_col,\n+                  'init': self.init.__name__,\n+                  'activation': self.activation.__name__,\n+                  'border_mode': self.border_mode,\n+                  'subsample': self.subsample,\n+                  'dim_ordering': self.dim_ordering,\n+                  'W_regularizer': self.W_regularizer.get_config() if self.W_regularizer else None,\n+                  'b_regularizer': self.b_regularizer.get_config() if self.b_regularizer else None,\n+                  'activity_regularizer': self.activity_regularizer.get_config() if self.activity_regularizer else None,\n+                  'W_constraint': self.W_constraint.get_config() if self.W_constraint else None,\n+                  'b_constraint': self.b_constraint.get_config() if self.b_constraint else None}\n         base_config = super(Convolution2D, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n-class Pooling1D(Layer):\n+class _Pooling1D(Layer):\n+    '''Abstract class for different pooling 1D layers.\n+    '''\n     input_dim = 3\n \n     def __init__(self, pool_length=2, stride=None,\n                  border_mode='valid', **kwargs):\n-        super(Pooling1D, self).__init__(**kwargs)\n+        super(_Pooling1D, self).__init__(**kwargs)\n         if stride is None:\n             stride = pool_length\n         self.pool_length = pool_length\n@@ -267,7 +367,7 @@ class Pooling1D(Layer):\n                                     self.border_mode, self.stride)\n         return (input_shape[0], length, input_shape[2])\n \n-    def pooling_function(self, back_end, inputs, pool_size, strides,\n+    def _pooling_function(self, back_end, inputs, pool_size, strides,\n                           border_mode, dim_ordering):\n         raise NotImplementedError\n \n@@ -275,7 +375,7 @@ class Pooling1D(Layer):\n         X = self.get_input(train)\n         X = K.expand_dims(X, -1)   # add dummy last dimension\n         X = K.permute_dimensions(X, (0, 2, 1, 3))\n-        output = self.pooling_function(inputs=X, pool_size=self.pool_size,\n+        output = self._pooling_function(inputs=X, pool_size=self.pool_size,\n                                         strides=self.st,\n                                         border_mode=self.border_mode,\n                                         dim_ordering='th')\n@@ -283,42 +383,76 @@ class Pooling1D(Layer):\n         return K.squeeze(output, 3)  # remove dummy last dimension\n \n     def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"stride\": self.stride,\n-                  \"pool_length\": self.pool_length,\n-                  \"border_mode\": self.border_mode}\n-        base_config = super(Pooling1D, self).get_config()\n+        config = {'name': self.__class__.__name__,\n+                  'stride': self.stride,\n+                  'pool_length': self.pool_length,\n+                  'border_mode': self.border_mode}\n+        base_config = super(_Pooling1D, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n-class MaxPooling1D(Pooling1D):\n-    def __init__(self, *args, **kwargs):\n-        super(MaxPooling1D, self).__init__(*args, **kwargs)\n+class MaxPooling1D(_Pooling1D):\n+    '''Max pooling operation for temporal data.\n \n-    def pooling_function(self, inputs, pool_size, strides,\n+    # Input shape\n+        3D tensor with shape: `(samples, steps, features)`.\n+\n+    # Output shape\n+        3D tensor with shape: `(samples, downsampled_steps, features)`.\n+\n+    # Arguments\n+        pool_length: factor by which to downscale. 2 will halve the input.\n+        stride: integer or None. Stride value.\n+        border_mode: 'valid' or 'same'.\n+            Note: 'same' will only work with TensorFlow for the time being.\n+    '''\n+    def __init__(self, pool_length=2, stride=None,\n+                 border_mode='valid', **kwargs):\n+        super(MaxPooling1D, self).__init__(pool_length, stride,\n+                                           border_mode, **kwargs)\n+\n+    def _pooling_function(self, inputs, pool_size, strides,\n                           border_mode, dim_ordering):\n         output = K.pool2d(inputs, pool_size, strides,\n                           border_mode, dim_ordering, pool_mode='max')\n         return output\n \n \n-class AveragePooling1D(Pooling1D):\n-    def __init__(self, *args, **kwargs):\n-        super(AveragePooling1D, self).__init__(*args, **kwargs)\n+class AveragePooling1D(_Pooling1D):\n+    '''Average pooling for temporal data.\n \n-    def pooling_function(self, inputs, pool_size, strides,\n+        # Input shape\n+        3D tensor with shape: `(samples, steps, features)`.\n+\n+    # Output shape\n+        3D tensor with shape: `(samples, downsampled_steps, features)`.\n+\n+    # Arguments\n+        pool_length: factor by which to downscale. 2 will halve the input.\n+        stride: integer or None. Stride value.\n+        border_mode: 'valid' or 'same'.\n+            Note: 'same' will only work with TensorFlow for the time being.\n+    '''\n+    def __init__(self, pool_length=2, stride=None,\n+                 border_mode='valid', **kwargs):\n+        super(AveragePooling1D, self).__init__(pool_length, stride,\n+                                               border_mode, **kwargs)\n+\n+    def _pooling_function(self, inputs, pool_size, strides,\n                           border_mode, dim_ordering):\n         output = K.pool2d(inputs, pool_size, strides,\n                           border_mode, dim_ordering, pool_mode='avg')\n         return output\n \n \n-class Pooling2D(Layer):\n+class _Pooling2D(Layer):\n+    '''Abstract class for different pooling 2D layers.\n+    '''\n     input_ndim = 4\n \n     def __init__(self, pool_size=(2, 2), strides=None, border_mode='valid',\n                  dim_ordering='th', **kwargs):\n-        super(Pooling2D, self).__init__(**kwargs)\n+        super(_Pooling2D, self).__init__(**kwargs)\n         self.input = K.placeholder(ndim=4)\n         self.pool_size = tuple(pool_size)\n         if strides is None:\n@@ -353,44 +487,96 @@ class Pooling2D(Layer):\n         else:\n             raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n \n-    def pooling_function(self, inputs, pool_size, strides,\n+    def _pooling_function(self, inputs, pool_size, strides,\n                           border_mode, dim_ordering):\n         raise NotImplementedError\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n-        output = self.pooling_function(inputs=X, pool_size=self.pool_size,\n+        output = self._pooling_function(inputs=X, pool_size=self.pool_size,\n                                         strides=self.strides,\n                                         border_mode=self.border_mode,\n                                         dim_ordering=self.dim_ordering)\n         return output\n \n     def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"pool_size\": self.pool_size,\n-                  \"border_mode\": self.border_mode,\n-                  \"strides\": self.strides,\n-                  \"dim_ordering\": self.dim_ordering}\n-        base_config = super(Pooling2D, self).get_config()\n+        config = {'name': self.__class__.__name__,\n+                  'pool_size': self.pool_size,\n+                  'border_mode': self.border_mode,\n+                  'strides': self.strides,\n+                  'dim_ordering': self.dim_ordering}\n+        base_config = super(_Pooling2D, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n-class MaxPooling2D(Pooling2D):\n-    def __init__(self, *args, **kwargs):\n-        super(MaxPooling2D, self).__init__(*args, **kwargs)\n+class MaxPooling2D(_Pooling2D):\n+    '''Max pooling operation for spatial data.\n \n-    def pooling_function(self, inputs, pool_size, strides,\n+    # Input shape\n+        4D tensor with shape:\n+        `(samples, channels, rows, cols)` if dim_ordering='th'\n+        or 4D tensor with shape:\n+        `(samples, rows, cols, channels)` if dim_ordering='tf'.\n+\n+    # Output shape\n+        4D tensor with shape:\n+        `(nb_samples, channels, pooled_rows, pooled_cols)` if dim_ordering='th'\n+        or 4D tensor with shape:\n+        `(samples, pooled_rows, pooled_cols, channels)` if dim_ordering='tf'.\n+\n+    # Arguments\n+        pool_size: tuple of 2 integers,\n+            factors by which to downscale (vertical, horizontal).\n+            (2, 2) will halve the image in each dimension.\n+        strides: tuple of 2 integers, or None. Strides values.\n+        border_mode: 'valid' or 'same'.\n+            Note: 'same' will only work with TensorFlow for the time being.\n+        dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n+            (the depth) is at index 1, in 'tf' mode is it at index 3.\n+    '''\n+    def __init__(self, pool_size=(2, 2), strides=None, border_mode='valid',\n+                 dim_ordering='th', **kwargs):\n+        super(MaxPooling2D, self).__init__(pool_size, strides, border_mode,\n+                                           dim_ordering, **kwargs)\n+\n+    def _pooling_function(self, inputs, pool_size, strides,\n                           border_mode, dim_ordering):\n         output = K.pool2d(inputs, pool_size, strides,\n                           border_mode, dim_ordering, pool_mode='max')\n         return output\n \n \n-class AveragePooling2D(Pooling2D):\n-    def __init__(self, *args, **kwargs):\n-        super(AveragePooling2D, self).__init__(*args, **kwargs)\n+class AveragePooling2D(_Pooling2D):\n+    '''Average pooling operation for spatial data.\n \n-    def pooling_function(self, inputs, pool_size, strides,\n+    # Input shape\n+        4D tensor with shape:\n+        `(samples, channels, rows, cols)` if dim_ordering='th'\n+        or 4D tensor with shape:\n+        `(samples, rows, cols, channels)` if dim_ordering='tf'.\n+\n+    # Output shape\n+        4D tensor with shape:\n+        `(nb_samples, channels, pooled_rows, pooled_cols)` if dim_ordering='th'\n+        or 4D tensor with shape:\n+        `(samples, pooled_rows, pooled_cols, channels)` if dim_ordering='tf'.\n+\n+    # Arguments\n+        pool_size: tuple of 2 integers,\n+            factors by which to downscale (vertical, horizontal).\n+            (2, 2) will halve the image in each dimension.\n+        strides: tuple of 2 integers, or None. Strides values.\n+        border_mode: 'valid' or 'same'.\n+            Note: 'same' will only work with TensorFlow for the time being.\n+        dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n+            (the depth) is at index 1, in 'tf' mode is it at index 3.\n+    '''\n+    def __init__(self, pool_size=(2, 2), strides=None, border_mode='valid',\n+                 dim_ordering='th', **kwargs):\n+        super(AveragePooling2D, self).__init__(pool_size, strides, border_mode,\n+                                               dim_ordering, **kwargs)\n+\n+    def _pooling_function(self, inputs, pool_size, strides,\n                           border_mode, dim_ordering):\n         output = K.pool2d(inputs, pool_size, strides,\n                           border_mode, dim_ordering, pool_mode='avg')\n@@ -398,6 +584,17 @@ class AveragePooling2D(Pooling2D):\n \n \n class UpSampling1D(Layer):\n+    '''Repeats each temporal step `length` times along the time axis.\n+\n+    # Input shape\n+        3D tensor with shape: `(samples, steps, features)`.\n+\n+    # Output shape\n+        3D tensor with shape: `(samples, upsampled_steps, features)`.\n+\n+    # Arguments:\n+        length: integer. Upsampling factor.\n+    '''\n     input_ndim = 3\n \n     def __init__(self, length=2, **kwargs):\n@@ -416,13 +613,34 @@ class UpSampling1D(Layer):\n         return output\n \n     def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"length\": self.length}\n+        config = {'name': self.__class__.__name__,\n+                  'length': self.length}\n         base_config = super(UpSampling1D, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n class UpSampling2D(Layer):\n+    '''Repeats the rows and columns of the data\n+    by size[0] and size[1] respectively.\n+\n+    # Input shape\n+        4D tensor with shape:\n+        `(samples, channels, rows, cols)` if dim_ordering='th'\n+        or 4D tensor with shape:\n+        `(samples, rows, cols, channels)` if dim_ordering='tf'.\n+\n+    # Output shape\n+        4D tensor with shape:\n+        `(samples, channels, upsampled_rows, upsampled_cols)` if dim_ordering='th'\n+        or 4D tensor with shape:\n+        `(samples, upsampled_rows, upsampled_cols, channels)` if dim_ordering='tf'.\n+\n+    # Arguments\n+        size: tuple of 2 integers. The upsampling factors for rows and columns.\n+        dim_ordering: 'th' or 'tf'.\n+            In 'th' mode, the channels dimension (the depth)\n+            is at index 1, in 'tf' mode is it at index 3.\n+    '''\n     input_ndim = 4\n \n     def __init__(self, size=(2, 2), dim_ordering='th', **kwargs):\n@@ -461,29 +679,26 @@ class UpSampling2D(Layer):\n         return output\n \n     def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"size\": self.size}\n+        config = {'name': self.__class__.__name__,\n+                  'size': self.size}\n         base_config = super(UpSampling2D, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n class ZeroPadding1D(Layer):\n-    \"\"\"Zero-padding layer for 1D input (e.g. temporal sequence).\n+    '''Zero-padding layer for 1D input (e.g. temporal sequence).\n \n-    Input shape\n-    -----------\n+    # Input shape\n         3D tensor with shape (samples, axis_to_pad, features)\n \n-    Output shape\n-    ------------\n+    # Output shape\n         3D tensor with shape (samples, padded_axis, features)\n \n-    Arguments\n-    ---------\n+    # Arguments\n         padding: int\n             How many zeros to add at the beginning and end of\n             the padding dimension (axis 1).\n-    \"\"\"\n+    '''\n     input_ndim = 3\n \n     def __init__(self, padding=1, **kwargs):\n@@ -503,31 +718,28 @@ class ZeroPadding1D(Layer):\n         return K.temporal_padding(X, padding=self.padding)\n \n     def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"padding\": self.padding}\n+        config = {'name': self.__class__.__name__,\n+                  'padding': self.padding}\n         base_config = super(ZeroPadding1D, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n class ZeroPadding2D(Layer):\n-    \"\"\"Zero-padding layer for 2D input (e.g. picture).\n+    '''Zero-padding layer for 2D input (e.g. picture).\n \n-    Input shape\n-    -----------\n+    # Input shape\n         4D tensor with shape:\n         (samples, depth, first_axis_to_pad, second_axis_to_pad)\n \n-    Output shape\n-    ------------\n+    # Output shape\n         4D tensor with shape:\n         (samples, depth, first_padded_axis, second_padded_axis)\n \n-    Arguments\n-    ---------\n+    # Arguments\n         padding: tuple of int (length 2)\n             How many zeros to add at the beginning and end of\n             the 2 padding dimensions (axis 3 and 4).\n-    \"\"\"\n+    '''\n     input_ndim = 4\n \n     def __init__(self, padding=(1, 1), dim_ordering='th', **kwargs):\n@@ -559,7 +771,7 @@ class ZeroPadding2D(Layer):\n                                     dim_ordering=self.dim_ordering)\n \n     def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"padding\": self.padding}\n+        config = {'name': self.__class__.__name__,\n+                  'padding': self.padding}\n         base_config = super(ZeroPadding2D, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n\n@@ -18,6 +18,19 @@ import sys\n \n \n class Layer(object):\n+    '''Abstract base layer class.\n+\n+    All Keras layers accept certain keyword arguments:\n+        trainable: boolean. Set to \"False\" before model compilation\n+            to freeze layer weights (they won't be updated further\n+            during training).\n+        input_shape: a tuple of integers specifying the expected shape\n+            of the input samples. Does not includes the batch size.\n+            (e.g. `(100,)` for 100-dimensional inputs).\n+        batch_input_shape: a tuple of integers specifying the expected\n+            shape of a batch of input samples. Includes the batch size\n+            (e.g. `(32, 100)` for a batch of 32 100-dimensional inputs).\n+    '''\n     def __init__(self, **kwargs):\n         allowed_kwargs = {'input_shape',\n                           'trainable',\n@@ -43,6 +56,8 @@ class Layer(object):\n         return Y\n \n     def set_previous(self, layer, connection_map={}):\n+        '''Connect a layer to its parent in the computational graph.\n+        '''\n         assert self.nb_input == layer.nb_output == 1, \"Cannot connect layers: input count and output count should be 1.\"\n         if hasattr(self, 'input_ndim'):\n             assert self.input_ndim == len(layer.output_shape), \"Incompatible shapes: layer expected input with ndim=\" +\\\n@@ -122,28 +137,48 @@ class Layer(object):\n                             'and is not an input layer.')\n \n     def supports_masked_input(self):\n-        ''' Whether or not this layer respects the output mask of its previous layer in its calculations. If you try\n-        to attach a layer that does *not* support masked_input to a layer that gives a non-None output_mask() that is\n-        an error'''\n+        ''' Whether or not this layer respects the output mask of its previous\n+        layer in its calculations.\n+        If you try to attach a layer that does *not* support masked_input to\n+        a layer that gives a non-None output_mask(), an error will be raised.\n+        '''\n         return False\n \n     def get_output_mask(self, train=None):\n         '''\n-        For some models (such as RNNs) you want a way of being able to mark some output data-points as\n-        \"masked\", so they are not used in future calculations. In such a model, get_output_mask() should return a mask\n-        of one less dimension than get_output() (so if get_output is (nb_samples, nb_timesteps, nb_dimensions), then the mask\n-        is (nb_samples, nb_timesteps), with a one for every unmasked datapoint, and a zero for every masked one.\n+        For some models (such as RNNs) you want a way of being able to mark\n+        some output data-points as \"masked\",\n+        so they are not used in future calculations.\n+        In such a model, get_output_mask() should return a mask\n+        of one less dimension than get_output()\n+        (so if get_output is (nb_samples, nb_timesteps, nb_dimensions),\n+        then the mask is (nb_samples, nb_timesteps),\n+        with a one for every unmasked datapoint,\n+        and a zero for every masked one.\n \n-        If there is *no* masking then it shall return None. For instance if you attach an Activation layer (they support masking)\n-        to a layer with an output_mask, then that Activation shall also have an output_mask. If you attach it to a layer with no\n-        such mask, then the Activation's get_output_mask shall return None.\n+        If there is *no* masking then it shall return None.\n+        For instance if you attach an Activation layer (they support masking)\n+        to a layer with an output_mask, then that Activation shall\n+        also have an output_mask.\n+        If you attach it to a layer with no such mask,\n+        then the Activation's get_output_mask shall return None.\n \n-        Some layers have an output_mask even if their input is unmasked, notably Embedding which can turn the entry \"0\" into\n+        Some layers have an output_mask even if their input is unmasked,\n+        notably Embedding which can turn the entry \"0\" into\n         a mask.\n         '''\n         return None\n \n     def set_weights(self, weights):\n+        '''Set the weights of the layer.\n+\n+        # Arguments\n+            weights: a list of numpy arrays. The number\n+                of arrays and their shape must match\n+                number of the dimensions of the weights\n+                of the layer (i.e. it should match the\n+                output of `get_weights`).\n+        '''\n         assert len(self.params) == len(weights), 'Provided weight array does not match layer weights (' + \\\n             str(len(self.params)) + ' layer params vs. ' + str(len(weights)) + ' provided weights)'\n         for p, w in zip(self.params, weights):\n@@ -152,12 +187,21 @@ class Layer(object):\n             K.set_value(p, w)\n \n     def get_weights(self):\n+        '''Return the weights of the layer.\n+\n+        # Returns: list of numpy arrays.\n+        '''\n         weights = []\n         for p in self.params:\n             weights.append(K.get_value(p))\n         return weights\n \n     def get_config(self):\n+        '''Return the parameters of the layer.\n+\n+        # Returns:\n+            A dictionary mapping parameter names to their values.\n+        '''\n         config = {\"name\": self.__class__.__name__}\n         if hasattr(self, '_input_shape'):\n             config['input_shape'] = self._input_shape[1:]\n@@ -191,16 +235,18 @@ class Layer(object):\n         return self.params, regularizers, consts, updates\n \n     def count_params(self):\n+        '''Return the total number of floats (or ints)\n+        composing the weights of the layer.\n+        '''\n         return sum([K.count_params(p) for p in self.params])\n \n \n class MaskedLayer(Layer):\n-    '''\n-    If your layer trivially supports masking\n+    '''If your layer trivially supports masking\n     (by simply copying the input mask to the output),\n     then subclass MaskedLayer instead of Layer,\n     and make sure that you incorporate the input mask\n-    into your calculation of get_output()\n+    into your calculation of get_output().\n     '''\n     def supports_masked_input(self):\n         return True\n@@ -219,7 +265,7 @@ class MaskedLayer(Layer):\n \n \n class Masking(MaskedLayer):\n-    \"\"\"Mask an input sequence by using a mask value to identify padding.\n+    '''Mask an input sequence by using a mask value to identify padding.\n \n     This layer copies the input to the output layer with identified padding\n     replaced with 0s and creates an output mask in the process.\n@@ -227,8 +273,7 @@ class Masking(MaskedLayer):\n     At each timestep, if the values all equal `mask_value`,\n     then the corresponding mask value for the timestep is 0 (skipped),\n     otherwise it is 1.\n-\n-    \"\"\"\n+    '''\n     def __init__(self, mask_value=0., **kwargs):\n         super(Masking, self).__init__(**kwargs)\n         self.mask_value = mask_value\n@@ -255,9 +300,15 @@ class Masking(MaskedLayer):\n \n class TimeDistributedMerge(Layer):\n     '''Sum/multiply/average over the outputs of a TimeDistributed layer.\n-    mode: {'sum', 'mul', 'ave'}\n-    Tensor input dimensions:   (nb_sample, time, features)\n-    Tensor output dimensions:  (nb_sample, features)\n+\n+    # Input shape\n+        3D tensor with shape: `(samples, steps, features)`.\n+\n+    # Output shape\n+        2D tensor with shape: `(samples, features)`.\n+\n+    # Arguments\n+        mode: one of {'sum', 'mul', 'ave'}\n     '''\n     input_ndim = 3\n \n@@ -295,10 +346,45 @@ class TimeDistributedMerge(Layer):\n \n \n class Merge(Layer):\n-    def __init__(self, layers, mode='sum', concat_axis=-1, dot_axes=-1):\n     '''Merge the output of a list of layers or containers into a single tensor.\n-            mode: {'sum', 'mul', 'concat', 'ave', 'join'}\n+\n+    # Arguments\n+        mode: one of {sum, mul, concat, ave, dot}.\n+            sum: sum the outputs (shapes must match)\n+            mul: multiply the outputs element-wise (shapes must match)\n+            concat: concatenate the outputs along the axis specified by `concat_axis`\n+            ave: average the outputs (shapes must match)\n+        concat_axis: axis to use in `concat` mode.\n+        dot_axes: axis or axes to use in `dot` mode\n+            (see [the Numpy documentation](http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.tensordot.html) for more details).\n+\n+    # TensorFlow warning\n+        `dot` mode only works with Theano for the time being.\n+\n+    # Examples\n+\n+    ```python\n+    left = Sequential()\n+    left.add(Dense(50, input_shape=(784,)))\n+    left.add(Activation('relu'))\n+\n+    right = Sequential()\n+    right.add(Dense(50, input_shape=(784,)))\n+    right.add(Activation('relu'))\n+\n+    model = Sequential()\n+    model.add(Merge([left, right], mode='sum'))\n+\n+    model.add(Dense(10))\n+    model.add(Activation('softmax'))\n+\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+    model.fit([X_train, X_train], Y_train, batch_size=128, nb_epoch=20,\n+              validation_data=([X_test, X_test], Y_test))\n+    ```\n     '''\n+    def __init__(self, layers, mode='sum', concat_axis=-1, dot_axes=-1):\n         if len(layers) < 2:\n             raise Exception(\"Please specify two or more input layers (or containers) to merge\")\n \n@@ -348,7 +434,6 @@ class Merge(Layer):\n                 raise Exception(\"'concat' mode can only merge layers with matching \" +\n                                 \"output shapes except for the concat axis. \" +\n                                 \"Layer shapes: %s\" % ([l.output_shape for l in layers]))\n-\n         self.mode = mode\n         self.concat_axis = concat_axis\n         self.dot_axes = dot_axes\n@@ -415,7 +500,7 @@ class Merge(Layer):\n             for i in range(len(self.layers)):\n                 X = self.layers[i].get_output(train)\n                 if X.name is None:\n-                    raise ValueError(\"merge_mode='join' only works with named inputs\")\n+                    raise ValueError('merge_mode='join' only works with named inputs')\n                 else:\n                     inputs[X.name] = X\n             return inputs\n@@ -492,8 +577,15 @@ class Merge(Layer):\n \n \n class Dropout(MaskedLayer):\n-    '''\n-        Hinton's dropout.\n+    '''Apply Dropout to the input. Dropout consists in randomly setting\n+    a fraction `p` of input units to 0 at each update during training time,\n+    which helps prevent overfitting.\n+\n+    # Arguments\n+        p: float between 0 and 1. Fraction of the input units to drop.\n+\n+    # References\n+        - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n     '''\n     def __init__(self, p, **kwargs):\n         super(Dropout, self).__init__(**kwargs)\n@@ -514,14 +606,24 @@ class Dropout(MaskedLayer):\n \n \n class Activation(MaskedLayer):\n+    '''Apply an activation function to an output.\n+\n+    # Input shape\n+        Arbitrary. Use the keyword argument `input_shape`\n+        (tuple of integers, does not include the samples axis)\n+        when using this layer as the first layer in a model.\n+\n+    # Output shape\n+        Same shape as input.\n+\n+    # Arguments:\n+        activation: name of activation function to use\n+            (see: [activations](../activations.md)),\n+            or alternatively, a Theano or TensorFlow operation.\n     '''\n-        Apply an activation function to an output.\n-    '''\n-    def __init__(self, activation, target=0, beta=0.1, **kwargs):\n+    def __init__(self, activation, **kwargs):\n         super(Activation, self).__init__(**kwargs)\n         self.activation = activations.get(activation)\n-        self.target = target\n-        self.beta = beta\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n@@ -529,18 +631,26 @@ class Activation(MaskedLayer):\n \n     def get_config(self):\n         config = {\"name\": self.__class__.__name__,\n-                  \"activation\": self.activation.__name__,\n-                  \"target\": self.target,\n-                  \"beta\": self.beta}\n+                  \"activation\": self.activation.__name__}\n         base_config = super(Activation, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n class Reshape(Layer):\n-    '''\n-        Reshape an output to a certain shape.\n-        Can't be used as first layer in a model (no fixed input!)\n-        First dimension is assumed to be nb_samples.\n+    '''Reshape an output to a certain shape.\n+\n+    # Input shape\n+        Arbitrary, although all dimensions in the input shaped must be fixed.\n+        Use the keyword argument `input_shape`\n+        (tuple of integers, does not include the samples axis)\n+        when using this layer as the first layer in a model.\n+\n+    # Output shape\n+        `(batch_size,) + dims`\n+\n+    # Arguments\n+        dims: target shape. Tuple of integers,\n+            does not include the samples dimension (batch size).\n     '''\n     def __init__(self, dims, **kwargs):\n         super(Reshape, self).__init__(**kwargs)\n@@ -562,8 +672,24 @@ class Reshape(Layer):\n \n \n class Permute(Layer):\n-    '''\n-        Permute the dimensions of the input according to the given tuple.\n+    '''Permute the dimensions of the input according to a given pattern.\n+\n+    Useful for e.g. connecting RNNs and convnets together.\n+\n+    # Input shape\n+        Arbitrary. Use the keyword argument `input_shape`\n+        (tuple of integers, does not include the samples axis)\n+        when using this layer as the first layer in a model.\n+\n+    # Output shape\n+        Same as the input shape, but with the dimensions re-ordered according\n+        to the specified pattern.\n+\n+    # Arguments\n+        dims: Tuple of integers. Permutation pattern, does not include the\n+            samples dimension. Indexing starts at 1.\n+            For instance, `(2, 1)` permutes the first and second dimension\n+            of the input.\n     '''\n     def __init__(self, dims, **kwargs):\n         super(Permute, self).__init__(**kwargs)\n@@ -590,9 +716,16 @@ class Permute(Layer):\n \n \n class Flatten(Layer):\n-    '''\n-        Reshape input to flat shape.\n-        First dimension is assumed to be nb_samples.\n+    '''Flatten the input. Does not affect the batch size.\n+\n+    # Input shape\n+        Arbitrary, although all dimensions in the input shaped must be fixed.\n+        Use the keyword argument `input_shape`\n+        (tuple of integers, does not include the samples axis)\n+        when using this layer as the first layer in a model.\n+\n+    # Output shape\n+        `(batch_size,)`\n     '''\n     def __init__(self, **kwargs):\n         super(Flatten, self).__init__(**kwargs)\n@@ -608,11 +741,16 @@ class Flatten(Layer):\n \n \n class RepeatVector(Layer):\n-    '''\n-        Repeat input n times.\n+    '''Repeat the input n times.\n \n-        Dimensions of input are assumed to be (nb_samples, dim).\n-        Return tensor of shape (nb_samples, n, dim).\n+    # Input shape\n+        2D tensor of shape `(nb_samples, features)`.\n+\n+    # Output shape\n+        3D tensor of shape `(nb_samples, n, features)`.\n+\n+    # Arguments\n+        n: integer, repetition factor.\n     '''\n     def __init__(self, n, **kwargs):\n         super(RepeatVector, self).__init__(**kwargs)\n@@ -635,8 +773,41 @@ class RepeatVector(Layer):\n \n \n class Dense(Layer):\n-    '''\n-        Just your regular fully connected NN layer.\n+    '''Just your regular fully connected NN layer.\n+\n+    # Input shape\n+        2D tensor with shape: `(nb_samples, input_dim)`.\n+\n+    # Output shape\n+        2D tensor with shape: `(nb_samples, output_dim)`.\n+\n+    # Arguments\n+        output_dim: int > 0.\n+        init: name of initialization function for the weights of the layer\n+            (see [initializations](../initializations.md)),\n+            or alternatively, Theano function to use for weights\n+            initialization. This parameter is only relevant\n+            if you don't pass a `weights` argument.\n+        activation: name of activation function to use\n+            (see [activations](../activations.md)),\n+            or alternatively, elementwise Theano function.\n+            If you don't specify anything, no activation is applied\n+            (ie. \"linear\" activation: a(x) = x).\n+        weights: list of numpy arrays to set as initial weights.\n+            The list should have 1 element, of shape `(input_dim, output_dim)`.\n+        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n+            (eg. L1 or L2 regularization), applied to the main weights matrix.\n+        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n+            applied to the bias.\n+        activity_regularizer: instance of [ActivityRegularizer](../regularizers.md),\n+            applied to the network output.\n+        W_constraint: instance of the [constraints](../constraints.md) module\n+            (eg. maxnorm, nonneg), applied to the main weights matrix.\n+        b_constraint: instance of the [constraints](../constraints.md) module,\n+            applied to the bias.\n+        input_dim: dimensionality of the input (integer).\n+            This argument (or alternatively, the keyword argument `input_shape`)\n+            is required when using this layer as the first layer in a model.\n     '''\n     input_ndim = 2\n \n@@ -713,9 +884,20 @@ class Dense(Layer):\n \n \n class ActivityRegularization(Layer):\n-    '''\n-        Layer that passes through its input unchanged, but applies an update\n+    '''Layer that passes through its input unchanged, but applies an update\n     to the cost function based on the activity.\n+\n+    # Input shape\n+        Arbitrary. Use the keyword argument `input_shape`\n+        (tuple of integers, does not include the samples axis)\n+        when using this layer as the first layer in a model.\n+\n+    # Output shape\n+        Same shape as input.\n+\n+    # Arguments\n+        l1: L1 regularization factor.\n+        l2: L2 regularization factor.\n     '''\n     def __init__(self, l1=0., l2=0., **kwargs):\n         super(ActivityRegularization, self).__init__(**kwargs)\n@@ -738,12 +920,42 @@ class ActivityRegularization(Layer):\n \n \n class TimeDistributedDense(MaskedLayer):\n-    '''\n-       Apply a same Dense layer for each dimension[1] (time_dimension) input.\n+    '''Apply a same Dense layer for each dimension[1] (time_dimension) input.\n     Especially useful after a recurrent network with 'return_sequence=True'.\n-       Tensor input dimensions:   (nb_sample, time_dimension, input_dim)\n-       Tensor output dimensions:  (nb_sample, time_dimension, output_dim)\n \n+    # Input shape\n+        3D tensor with shape `(nb_sample, time_dimension, input_dim)`.\n+\n+    # Output shape\n+        3D tensor with shape `(nb_sample, time_dimension, output_dim)`.\n+\n+    # Arguments\n+        output_dim: int > 0.\n+        init: name of initialization function for the weights of the layer\n+            (see [initializations](../initializations.md)),\n+            or alternatively, Theano function to use for weights\n+            initialization. This parameter is only relevant\n+            if you don't pass a `weights` argument.\n+        activation: name of activation function to use\n+            (see [activations](../activations.md)),\n+            or alternatively, elementwise Theano function.\n+            If you don't specify anything, no activation is applied\n+            (ie. \"linear\" activation: a(x) = x).\n+        weights: list of numpy arrays to set as initial weights.\n+            The list should have 1 element, of shape `(input_dim, output_dim)`.\n+        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n+            (eg. L1 or L2 regularization), applied to the main weights matrix.\n+        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n+            applied to the bias.\n+        activity_regularizer: instance of [ActivityRegularizer](../regularizers.md),\n+            applied to the network output.\n+        W_constraint: instance of the [constraints](../constraints.md) module\n+            (eg. maxnorm, nonneg), applied to the main weights matrix.\n+        b_constraint: instance of the [constraints](../constraints.md) module,\n+            applied to the bias.\n+        input_dim: dimensionality of the input (integer).\n+            This argument (or alternatively, the keyword argument `input_shape`)\n+            is required when using this layer as the first layer in a model.\n     '''\n     input_ndim = 3\n \n@@ -833,14 +1045,37 @@ class TimeDistributedDense(MaskedLayer):\n class AutoEncoder(Layer):\n     '''A customizable autoencoder model.\n \n-    Tensor input dimensions: same as encoder input\n-    Tensor output dimensions:\n-        if output_reconstruction:\n-            same as encoder output\n-        else:\n-            same as decoder output\n+    # Input shape\n+        Same as encoder input.\n+\n+    # Output shape\n+        If `output_reconstruction = True` then dim(input) = dim(output)\n+        else dim(output) = dim(hidden).\n+\n+    # Arguments\n+        encoder: A [layer](./) or [layer container](./containers.md).\n+        decoder: A [layer](./) or [layer container](./containers.md).\n+        output_reconstruction: If this is `False`,\n+            the output of the autoencoder is the output of\n+            the deepest hidden layer.\n+            Otherwise, the output of the final decoder layer is returned.\n+        weights: list of numpy arrays to set as initial weights.\n+\n+    # Examples\n+    ```python\n+    from keras.layers import containers\n+\n+    # input shape: (nb_samples, 32)\n+    encoder = containers.Sequential([Dense(16, input_dim=32), Dense(8)])\n+    decoder = containers.Sequential([Dense(16, input_dim=8), Dense(32)])\n+\n+    autoencoder = Sequential()\n+    autoencoder.add(AutoEncoder(encoder=encoder, decoder=decoder,\n+                                output_reconstruction=False))\n+    ```\n     '''\n-    def __init__(self, encoder, decoder, output_reconstruction=True, weights=None, **kwargs):\n+    def __init__(self, encoder, decoder, output_reconstruction=True,\n+                 weights=None, **kwargs):\n         super(AutoEncoder, self).__init__(**kwargs)\n \n         self.output_reconstruction = output_reconstruction\n@@ -914,13 +1149,31 @@ class AutoEncoder(Layer):\n \n \n class MaxoutDense(Layer):\n-    '''\n-        Max-out layer, nb_feature is the number of pieces in the piecewise linear approx.\n-        Refer to http://arxiv.org/pdf/1302.4389.pdf\n+    '''A dense maxout layer.\n+\n+    A `MaxoutDense` layer takes the element-wise maximum of\n+    `nb_feature` `Dense(input_dim, output_dim)` linear layers.\n+    This allows the layer to learn a convex,\n+    piecewise linear activation function over the inputs.\n+\n+    Note that this is a *linear* layer;\n+    if you wish to apply activation function\n+    (you shouldn't need to --they are universal function approximators),\n+    an `Activation` layer must be added after.\n+\n+    # Input shape\n+        2D tensor with shape: `(nb_samples, input_dim)`.\n+\n+    # Output shape\n+        2D tensor with shape: `(nb_samples, output_dim)`.\n+\n+    # References\n+        - [Maxout Networks](http://arxiv.org/pdf/1302.4389.pdf)\n     '''\n     input_ndim = 2\n \n-    def __init__(self, output_dim, nb_feature=4, init='glorot_uniform', weights=None,\n+    def __init__(self, output_dim, nb_feature=4,\n+                 init='glorot_uniform', weights=None,\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n                  W_constraint=None, b_constraint=None, input_dim=None, **kwargs):\n         self.output_dim = output_dim\n@@ -978,37 +1231,38 @@ class MaxoutDense(Layer):\n         return output\n \n     def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"output_dim\": self.output_dim,\n-                  \"init\": self.init.__name__,\n-                  \"nb_feature\": self.nb_feature,\n-                  \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n-                  \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n-                  \"activity_regularizer\": self.activity_regularizer.get_config() if self.activity_regularizer else None,\n-                  \"W_constraint\": self.W_constraint.get_config() if self.W_constraint else None,\n-                  \"b_constraint\": self.b_constraint.get_config() if self.b_constraint else None,\n-                  \"input_dim\": self.input_dim}\n+        config = {'name': self.__class__.__name__,\n+                  'output_dim': self.output_dim,\n+                  'init': self.init.__name__,\n+                  'nb_feature': self.nb_feature,\n+                  'W_regularizer': self.W_regularizer.get_config() if self.W_regularizer else None,\n+                  'b_regularizer': self.b_regularizer.get_config() if self.b_regularizer else None,\n+                  'activity_regularizer': self.activity_regularizer.get_config() if self.activity_regularizer else None,\n+                  'W_constraint': self.W_constraint.get_config() if self.W_constraint else None,\n+                  'b_constraint': self.b_constraint.get_config() if self.b_constraint else None,\n+                  'input_dim': self.input_dim}\n         base_config = super(MaxoutDense, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n class Lambda(Layer):\n-    \"\"\"Lambda layer for evaluating arbitrary function\n+    '''Used for evaluating an arbitrary Theano / TensorFlow expression\n+    on the output of the previous layer.\n \n-    Input shape\n-    -----------\n-    output_shape of previous layer\n+    # Input shape\n+        Arbitrary. Use the keyword argument input_shape\n+        (tuple of integers, does not include the samples axis)\n+        when using this layer as the first layer in a model.\n \n-    Output shape\n-    ------------\n-    Specified by output_shape argument\n-\n-    Arguments\n-    ---------\n-    function - The function to be evaluated. Takes one argument : output of previous layer\n-    output_shape - Expected output shape from function. Could be a tuple or a function of the shape of the input\n-    \"\"\"\n+    # Output shape\n+        Specified by `output_shape` argument.\n \n+    # Arguments\n+        function: The function to be evaluated.\n+            Takes one argument: the output of previous layer\n+        output_shape: Expected output shape from function.\n+            Could be a tuple or a function of the shape of the input\n+    '''\n     def __init__(self, function, output_shape=None, **kwargs):\n         super(Lambda, self).__init__(**kwargs)\n         py3 = sys.version_info[0] == 3\n@@ -1054,27 +1308,23 @@ class MaskedLambda(MaskedLayer, Lambda):\n \n \n class LambdaMerge(Lambda):\n-    \"\"\"LambdaMerge layer for evaluating arbitrary function over multiple inputs\n+    '''LambdaMerge layer for evaluating an arbitrary Theano / TensorFlow\n+    function over multiple inputs.\n \n-    Input shape\n-    -----------\n-    None\n-\n-    Output shape\n-    ------------\n+    # Output shape\n         Specified by output_shape argument\n \n-    Arguments\n-    ---------\n+    # Arguments\n         layers - Input layers. Similar to layers argument of Merge\n         function - The function to be evaluated. Takes one argument:\n             list of outputs from input layers\n         output_shape - Expected output shape from function.\n             Could be a tuple or a function of list of input shapes\n-    \"\"\"\n+    '''\n     def __init__(self, layers, function, output_shape=None):\n         if len(layers) < 2:\n-            raise Exception(\"Please specify two or more input layers (or containers) to merge\")\n+            raise Exception('Please specify two or more input layers '\n+                            '(or containers) to merge')\n         self.layers = layers\n         self.params = []\n         self.regularizers = []\n@@ -1116,7 +1366,7 @@ class LambdaMerge(Lambda):\n             output_shape_func = types.FunctionType(output_shape_func, globals())\n             shape = output_shape_func(input_shapes)\n             if type(shape) not in {list, tuple}:\n-                raise Exception(\"output_shape function must return a tuple\")\n+                raise Exception('output_shape function must return a tuple')\n             return tuple(shape)\n \n     def get_params(self):\n@@ -1162,39 +1412,40 @@ class LambdaMerge(Lambda):\n             weights = weights[nb_param:]\n \n     def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"layers\": [l.get_config() for l in self.layers],\n-                  \"function\": self.function,\n-                  \"output_shape\": self._output_shape\n-                  }\n+        config = {'name': self.__class__.__name__,\n+                  'layers': [l.get_config() for l in self.layers],\n+                  'function': self.function,\n+                  'output_shape': self._output_shape}\n         base_config = super(LambdaMerge, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n class Siamese(Layer):\n-    '''Shared layer with multiple inputs\n+    '''Share a layer accross multiple inputs.\n \n-    Output shape\n-    ------------\n+    For instance, this allows you to applied e.g.\n+    a same `Dense` layer to the output of two\n+    different layers in a graph.\n+\n+    # Output shape\n         Depends on merge_mode argument\n \n-    Arguments\n-    ---------\n-    layer - The layer to be shared across multiple inputs\n-    inputs - inputs to the shared layer\n-    merge_mode - Similar to mode argument of Merge layer\n-    concat_axis - Similar to concat_axis argument of Merge layer\n-    dot_axes - Similar to dot_axes argument of Merge layer\n+    # Arguments\n+        layer: The layer to be shared across multiple inputs\n+        inputs: Inputs to the shared layer\n+        merge_mode: Same meaning as `mode` argument of Merge layer\n+        concat_axis: Same meaning as `concat_axis` argument of Merge layer\n+        dot_axes: Same meaning as `dot_axes` argument of Merge layer\n     '''\n     def __init__(self, layer, inputs, merge_mode='concat',\n                  concat_axis=1, dot_axes=-1):\n         if merge_mode not in ['sum', 'mul', 'concat', 'ave',\n                               'join', 'cos', 'dot', None]:\n-            raise Exception(\"Invalid merge mode: \" + str(merge_mode))\n+            raise Exception('Invalid merge mode: ' + str(merge_mode))\n \n         if merge_mode in {'cos', 'dot'}:\n             if len(inputs) > 2:\n-                raise Exception(merge_mode + \" merge takes exactly 2 layers\")\n+                raise Exception(merge_mode + ' merge takes exactly 2 layers')\n \n         self.layer = layer\n         self.inputs = inputs\n@@ -1274,7 +1525,8 @@ class Siamese(Layer):\n         for i in range(len(self.inputs)):\n             X = self.get_output_at(i, train)\n             if X.name is None:\n-                raise ValueError(\"merge_mode='join' only works with named inputs\")\n+                raise ValueError('merge_mode=\"join\" '\n+                                 'only works with named inputs')\n             o[X.name] = X\n         return o\n \n@@ -1380,14 +1632,12 @@ class Siamese(Layer):\n                 weights = weights[nb_param:]\n \n     def get_config(self):\n-\n-        config = {\"name\": self.__class__.__name__,\n-                  \"layer\": self.layer.get_config(),\n-                  \"inputs\": [m.get_config() for m in self.inputs],\n-                  \"merge_mode\": self.merge_mode,\n-                  \"concat_axis\": self.concat_axis,\n-                  \"dot_axes\": self.dot_axes\n-                  }\n+        config = {'name': self.__class__.__name__,\n+                  'layer': self.layer.get_config(),\n+                  'inputs': [m.get_config() for m in self.inputs],\n+                  'merge_mode': self.merge_mode,\n+                  'concat_axis': self.concat_axis,\n+                  'dot_axes': self.dot_axes}\n         base_config = super(Siamese, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -1399,9 +1649,8 @@ class SiameseHead(Layer):\n     Outputs the output of the Siamese layer at a given index,\n     specified by the head argument\n \n-    Arguments\n-    ---------\n-    head - The index at which the output of the Siamese layer\n+    # Arguments\n+        head: The index at which the output of the Siamese layer\n             should be obtained\n     '''\n     def __init__(self, head):\n@@ -1420,8 +1669,8 @@ class SiameseHead(Layer):\n \n     def get_config(self):\n \n-        config = {\"name\": self.__class__.__name__,\n-                  \"head\": self.head}\n+        config = {'name': self.__class__.__name__,\n+                  'head': self.head}\n \n         base_config = super(SiameseHead, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n@@ -1431,9 +1680,8 @@ class SiameseHead(Layer):\n \n \n def add_shared_layer(layer, inputs):\n-    '''\n-    Use this function to add a shared layer across multiple Sequential models\n-    without merging the outputs\n+    '''Use this function to add a shared layer across\n+    multiple Sequential models without merging the outputs\n     '''\n     input_layers = [l.layers[-1] for l in inputs]\n     s = Siamese(layer, input_layers, merge_mode=None)\n\n@@ -8,12 +8,40 @@ from ..constraints import unitnorm\n \n \n class Embedding(Layer):\n-    '''\n-        Turn positive integers (indexes) into denses vectors of fixed size.\n+    '''Turn positive integers (indexes) into denses vectors of fixed size.\n     eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n \n-        @input_dim: size of vocabulary (highest input integer + 1)\n-        @out_dim: size of dense representation\n+    This layer can only be used as the first layer in  a model.\n+\n+    # Input shape\n+        2D tensor with shape: `(nb_samples, sequence_length)`.\n+\n+    # Output shape\n+        3D tensor with shape: `(nb_samples, sequence_length, output_dim)`.\n+\n+    # Arguments\n+      input_dim: int >= 0. Size of the vocabulary, ie.\n+          1 + maximum integer index occurring in the input data.\n+      output_dim: int >= 0. Dimension of the dense embedding.\n+      init: name of initialization function for the weights\n+          of the layer (see: [initializations](../initializations.md)),\n+          or alternatively, Theano function to use for weights initialization.\n+          This parameter is only relevant if you don't pass a `weights` argument.\n+      weights: list of numpy arrays to set as initial weights.\n+          The list should have 1 element, of shape `(input_dim, output_dim)`.\n+      W_regularizer: instance of the [regularizers](../regularizers.md) module\n+        (eg. L1 or L2 regularization), applied to the embedding matrix.\n+      W_constraint: instance of the [constraints](../constraints.md) module\n+          (eg. maxnorm, nonneg), applied to the embedding matrix.\n+      mask_zero: Whether or not the input value 0 is a special \"padding\"\n+          value that should be masked out.\n+          This is useful for [recurrent layers](recurrent.md) which may take\n+          variable length input. If this is `True` then all subsequent layers\n+          in the model need to support masking or an exception will be raised.\n+      input_length: Length of input sequences, when it is constantself.\n+          This argument is required if you are going to connect\n+          `Flatten` then `Dense` layers upstream\n+          (without it, the shape of the dense outputs cannot be computed).\n     '''\n     input_ndim = 2\n \n\n@@ -4,8 +4,24 @@ from .. import backend as K\n \n \n class GaussianNoise(MaskedLayer):\n-    '''\n-        Corruption process with GaussianNoise\n+    '''Apply to the input an additive zero-centred gaussian noise with\n+    standard deviation `sigma`. This is useful to mitigate overfitting\n+    (you could see it as a kind of random data augmentation).\n+    Gaussian Noise (GS) is a natural choice as corruption process\n+    for real valued inputs.\n+\n+    As it is a regularization layer, it is only active at training time.\n+\n+    # Input shape\n+        Arbitrary. Use the keyword argument `input_shape`\n+        (tuple of integers, does not include the samples axis)\n+        when using this layer as the first layer in a model.\n+\n+    # Output shape\n+        Same shape as input.\n+\n+    # Arguments\n+        sigma: float, standard deviation of the noise distribution.\n     '''\n     def __init__(self, sigma, **kwargs):\n         super(GaussianNoise, self).__init__(**kwargs)\n@@ -28,12 +44,16 @@ class GaussianNoise(MaskedLayer):\n \n \n class GaussianDropout(MaskedLayer):\n-    '''\n-        Multiplicative Gaussian Noise\n-        Reference:\n-            Dropout: A Simple Way to Prevent Neural Networks from Overfitting\n-            Srivastava, Hinton, et al. 2014\n-            http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf\n+    '''Apply to the input an multiplicative one-centred gaussian noise\n+    with standard deviation `sqrt(p/(1-p))`.\n+\n+    As it is a regularization layer, it is only active at training time.\n+\n+    # Arguments\n+        p: float, drop probability (as with `Dropout`).\n+\n+    # References:\n+        [Dropout: A Simple Way to Prevent Neural Networks from Overfitting Srivastava, Hinton, et al. 2014](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n     '''\n     def __init__(self, p, **kwargs):\n         super(GaussianDropout, self).__init__(**kwargs)\n\n@@ -4,18 +4,30 @@ from .. import backend as K\n \n \n class BatchNormalization(Layer):\n-    '''\n-        Reference:\n-            Batch Normalization: Accelerating Deep Network Training\n-            by Reducing Internal Covariate Shift\n-                http://arxiv.org/pdf/1502.03167v3.pdf\n+    '''Normalize the activations of the previous layer at each batch.\n \n-            mode: 0 -> featurewise normalization\n-                  1 -> samplewise normalization\n-                       (may sometimes outperform featurewise mode)\n+    # Input shape\n+        Arbitrary. Use the keyword argument `input_shape`\n+        (tuple of integers, does not include the samples axis)\n+        when using this layer as the first layer in a model.\n \n-            momentum: momentum term in the computation\n-            of a running estimate of the mean and std of the data\n+    # Output shape\n+        Same shape as input.\n+\n+    # Arguments\n+        epsilon: small float > 0. Fuzz parameter.\n+        mode: integer, 0 or 1.\n+            - 0: feature-wise normalization.\n+            - 1: sample-wise normalization.\n+        momentum: momentum in the computation of the\n+            exponential average of the mean and standard deviation\n+            of the data, for feature-wise normalization.\n+        weights: Initialization weights.\n+            List of 2 numpy arrays, with shapes:\n+            `[(input_shape,), (input_shape,)]`\n+\n+    # References\n+        - [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://arxiv.org/pdf/1502.03167v3.pdf)\n     '''\n     def __init__(self, epsilon=1e-6, mode=0, momentum=0.9,\n                  weights=None, **kwargs):\n\n@@ -8,6 +8,78 @@ from ..layers.core import MaskedLayer\n \n \n class Recurrent(MaskedLayer):\n+    '''Abstract base class for recurrent layers.\n+    Do not use in a model -- it's not a functional layer!\n+\n+    All recurrent layers (GRU, LSTM, SimpleRNN) also\n+    follow the specifications of this class and accept\n+    the keyword arguments listed below.\n+\n+    # Input shape\n+        3D tensor with shape `(nb_samples, timesteps, input_dim)`.\n+\n+    # Output shape\n+        - if `return_sequences`: 3D tensor with shape\n+            `(nb_samples, timesteps, output_dim)`.\n+        - else, 2D tensor with shape `(nb_samples, output_dim)`.\n+\n+    # Arguments\n+        weights: list of numpy arrays to set as initial weights.\n+            The list should have 3 elements, of shapes:\n+            `[(input_dim, output_dim), (output_dim, output_dim), (output_dim,)]`.\n+        return_sequences: Boolean. Whether to return the last output\n+            in the output sequence, or the full sequence.\n+        go_backwards: Boolean (default False).\n+            If True, rocess the input sequence backwards.\n+        stateful: Boolean (default False). If True, the last state\n+            for each sample at index i in a batch will be used as initial\n+            state for the sample of index i in the following batch.\n+        input_dim: dimensionality of the input (integer).\n+            This argument (or alternatively, the keyword argument `input_shape`)\n+            is required when using this layer as the first layer in a model.\n+        input_length: Length of input sequences, to be specified\n+            when it is constant.\n+            This argument is required if you are going to connect\n+            `Flatten` then `Dense` layers upstream\n+            (without it, the shape of the dense outputs cannot be computed).\n+            Note that if the recurrent layer is not the first layer\n+            in your model, you would need to specify the input Length\n+            at the level of the first layer\n+            (e.g. via the `input_shape` argument)\n+\n+    # Masking\n+        This layer supports masking for input data with a variable number\n+        of timesteps. To introduce masks to your data,\n+        use an [Embedding](embeddings.md) layer with the `mask_zero` parameter\n+        set to `True`.\n+        **Note:** for the time being, masking is only supported with Theano.\n+\n+    # TensorFlow warning\n+        For the time being, when using the TensorFlow backend,\n+        the number of timesteps used must be specified in your model.\n+        Make sure to pass an `input_length` int argument to your\n+        recurrent layer (if it comes first in your model),\n+        or to pass a complete `input_shape` argument to the first layer\n+        in your model otherwise.\n+\n+\n+    # Note on using statefulness in RNNs\n+        You can set RNN layers to be 'stateful', which means that the states\n+        computed for the samples in one batch will be reused as initial states\n+        for the samples in the next batch.\n+        This assumes a one-to-one mapping between\n+        samples in different successive batches.\n+\n+        To enable statefulness:\n+            - specify `stateful=True` in the layer constructor.\n+            - specify a fixed batch size for your model, by passing\n+                a `batch_input_size=(...)` to the first layer in your model.\n+                This is the expected shape of your inputs *including the batch size*.\n+                It should be a tuple of integers, e.g. `(32, 10, 100)`.\n+\n+        To reset the states of your model, call `.reset_states()` on either\n+        a specific layer, or on your entire model.\n+    '''\n     input_ndim = 3\n \n     def __init__(self, weights=None,\n@@ -100,17 +172,17 @@ class Recurrent(MaskedLayer):\n \n \n class SimpleRNN(Recurrent):\n-    '''\n-        Fully-connected RNN where the output is to fed back to input.\n-        Takes inputs with shape:\n-        (nb_samples, max_sample_length, input_dim)\n-        (samples shorter than `max_sample_length`\n-         are padded with zeros at the end)\n-        and returns outputs with shape:\n-        if not return_sequences:\n-            (nb_samples, output_dim)\n-        if return_sequences:\n-            (nb_samples, max_sample_length, output_dim)\n+    '''Fully-connected RNN where the output is to fed back to input.\n+\n+    # Arguments\n+        output_dim: dimension of the internal projections and the final output.\n+        init: weight initialization function.\n+            Can be the name of an existing function (str),\n+            or a Theano function (see: [initializations](../initializations.md)).\n+        inner_init: initialization function of the inner cells.\n+        activation: activation function.\n+            Can be the name of an existing function (str),\n+            or a Theano function (see: [activations](../activations.md)).\n     '''\n     def __init__(self, output_dim,\n                  init='glorot_uniform', inner_init='orthogonal',\n@@ -171,26 +243,22 @@ class SimpleRNN(Recurrent):\n \n \n class GRU(Recurrent):\n-    '''\n-        Gated Recurrent Unit - Cho et al. 2014\n-        Acts as a spatiotemporal projection,\n-        turning a sequence of vectors into a single vector.\n-        Takes inputs with shape:\n-        (nb_samples, max_sample_length, input_dim)\n-        (samples shorter than `max_sample_length`\n-         are padded with zeros at the end)\n-        and returns outputs with shape:\n-        if not return_sequences:\n-            (nb_samples, output_dim)\n-        if return_sequences:\n-            (nb_samples, max_sample_length, output_dim)\n-        References:\n-            On the Properties of Neural Machine Translation:\n-            Encoder–Decoder Approaches\n-                http://www.aclweb.org/anthology/W14-4012\n-            Empirical Evaluation of Gated Recurrent Neural Networks\n-            on Sequence Modeling\n-                http://arxiv.org/pdf/1412.3555v1.pdf\n+    '''Gated Recurrent Unit - Cho et al. 2014.\n+\n+    # Arguments\n+        output_dim: dimension of the internal projections and the final output.\n+        init: weight initialization function.\n+            Can be the name of an existing function (str),\n+            or a Theano function (see: [initializations](../initializations.md)).\n+        inner_init: initialization function of the inner cells.\n+        activation: activation function.\n+            Can be the name of an existing function (str),\n+            or a Theano function (see: [activations](../activations.md)).\n+        inner_activation: activation function for the inner cells.\n+\n+    # References\n+        - [On the Properties of Neural Machine Translation: Encoder–Decoder Approaches](http://www.aclweb.org/anthology/W14-4012)\n+        - [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](http://arxiv.org/pdf/1412.3555v1.pdf)\n     '''\n     def __init__(self, output_dim,\n                  init='glorot_uniform', inner_init='orthogonal',\n@@ -273,27 +341,29 @@ class GRU(Recurrent):\n \n \n class LSTM(Recurrent):\n-    '''\n-        Acts as a spatiotemporal projection,\n-        turning a sequence of vectors into a single vector.\n-        Takes inputs with shape:\n-        (nb_samples, max_sample_length, input_dim)\n-        (samples shorter than `max_sample_length`\n-         are padded with zeros at the end)\n-        and returns outputs with shape:\n-        if not return_sequences:\n-            (nb_samples, output_dim)\n-        if return_sequences:\n-            (nb_samples, max_sample_length, output_dim)\n-        For a step-by-step description of the algorithm, see:\n-        http://deeplearning.net/tutorial/lstm.html\n-        References:\n-            Long short-term memory (original 97 paper)\n-                http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\n-            Learning to forget: Continual prediction with LSTM\n-                http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015\n-            Supervised sequence labelling with recurrent neural networks\n-                http://www.cs.toronto.edu/~graves/preprint.pdf\n+    '''Long-Short Term Memory unit - Hochreiter 1997.\n+\n+    For a step-by-step description of the algorithm, see\n+    [this tutorial](http://deeplearning.net/tutorial/lstm.html).\n+\n+    # Arguments\n+        output_dim: dimension of the internal projections and the final output.\n+        init: weight initialization function.\n+            Can be the name of an existing function (str),\n+            or a Theano function (see: [initializations](../initializations.md)).\n+        inner_init: initialization function of the inner cells.\n+        forget_bias_init: initialization function for the bias of the forget gate.\n+            [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n+            recommend initializing with ones.\n+        activation: activation function.\n+            Can be the name of an existing function (str),\n+            or a Theano function (see: [activations](../activations.md)).\n+        inner_activation: activation function for the inner cells.\n+\n+    # References\n+        - [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) (original 1997 paper)\n+        - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n+        - [Supervised sequence labelling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n     '''\n     def __init__(self, output_dim,\n                  init='glorot_uniform', inner_init='orthogonal',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
