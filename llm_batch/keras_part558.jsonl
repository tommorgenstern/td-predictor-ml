{"custom_id": "keras#6822d5f39887896c30a4de672cee0b63700f1dfd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 5 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 186 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -1,11 +1,11 @@\n-from absl import logging\n import numpy as np\n+from absl import logging\n \n-from keras_core import testing\n-from keras_core import optimizers\n+from keras_core import callbacks\n from keras_core import layers\n from keras_core import losses\n-from keras_core import callbacks\n+from keras_core import optimizers\n+from keras_core import testing\n from keras_core.models.sequential import Sequential\n \n \n\n@@ -41,11 +41,13 @@ class Embedding(Layer):\n         embeddings_constraint: Constraint function applied to\n             the `embeddings` matrix (see `keras.constraints`).\n         mask_zero: Boolean, whether or not the input value 0 is a special\n-            \"padding\" value that should be masked out. This is useful when using\n-            recurrent layers which may take variable length input. If this is\n-            `True`, then all subsequent layers in the model need to support masking\n-            or an exception will be raised. If mask_zero is set to True, as a\n-            consequence, index 0 cannot be used in the vocabulary (input_dim should\n+            \"padding\" value that should be masked out.\n+            This is useful when using recurrent layers which\n+            may take variable length input. If this is `True`,\n+            then all subsequent layers in the model need\n+            to support masking or an exception will be raised.\n+            If mask_zero is set to True, as a consequence,\n+            index 0 cannot be used in the vocabulary (input_dim should\n             equal size of vocabulary + 1).\n \n     Input shape:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2aa4e4887a088efca295b6f922c1eec8e36cd099", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 230 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -1,3 +1,9 @@\n+# flake8: noqa\n+import os\n+\n+# Set backend env to JAX\n+os.environ[\"KERAS_BACKEND\"] = \"jax\"\n+\n import jax\n import numpy as np\n \n\n@@ -1,3 +1,9 @@\n+# flake8: noqa\n+import os\n+\n+# Set backend env to tensorflow\n+os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n+\n import numpy as np\n import tensorflow as tf\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8addc40cccb9758c062c8c5fff5904301fbf8036", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 10000 | Contributors (this commit): 35 | Commits (past 90d): 16 | Contributors (cumulative): 35 | DMM Complexity: None\n\nDIFF:\n@@ -2873,7 +2873,7 @@ def serialize(loss, use_legacy_format=False):\n     Args:\n         loss: A Keras `Loss` instance or a loss function.\n         use_legacy_format: Boolean, whether to use the legacy serialization\n-            format.\n+            format. Defaults to `False`.\n \n     Returns:\n         Loss configuration dictionary.\n@@ -2893,7 +2893,7 @@ def deserialize(name, custom_objects=None, use_legacy_format=False):\n             objects (classes and functions) to be considered during\n             deserialization.\n         use_legacy_format: Boolean, whether to use the legacy serialization\n-            format.\n+            format. Defaults to `False`.\n \n     Returns:\n         A Keras `Loss` instance or a loss function.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8c0bd197469bc00067653341b8ea3626a446497f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 366/366 | Churn Δ: 4 | Churn Cumulative: 13929 | Contributors (this commit): 25 | Commits (past 90d): 6 | Contributors (cumulative): 25 | DMM Complexity: None\n\nDIFF:\n@@ -94,7 +94,7 @@ class TrainingTest(test_combinations.TestCase):\n         model = sequential.Sequential([layers_module.Dense(1)])\n         model.compile(\"sgd\", \"mse\", run_eagerly=test_utils.should_run_eagerly())\n         with self.assertRaisesRegex(\n-            ValueError, \"Unexpected result of `train_function`.*\"\n+            ValueError, \"Expected input data to be non-empty.\"\n         ):\n             model.fit(x=np.array([]), y=np.array([]))\n \n@@ -2448,7 +2448,7 @@ class TestExceptionsAndWarnings(test_combinations.TestCase):\n         model.compile(loss=\"mse\")\n \n         with self.assertRaisesRegex(\n-            ValueError, \"Unexpected result of `predict_function`.*\"\n+            ValueError, \"Expected input data to be non-empty.\"\n         ):\n             model.predict(np.array([]))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2c8e6e19b5dc74aeb52f913d582b5631d4d389fb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 51 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 51 | Churn Cumulative: 301 | Contributors (this commit): 3 | Commits (past 90d): 17 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -176,3 +176,54 @@ class LayerTest(testing.TestCase):\n         model(np.ones((1,)))\n         self.assertLen(model.losses, 1)\n         self.assertAllClose(model.losses[0], 1.0)\n+\n+    def test_training_arg_value_resolution(self):\n+        # Check that even if `training` is not passed\n+        # to an inner layer, the outer value gets propagated\n+        # in __call__.\n+        class TrainingLayer(layers.Layer):\n+            def __init__(self):\n+                super().__init__()\n+                self.dp = layers.Dropout(0.9)\n+\n+            def call(self, x, training=False):\n+                return self.dp(x)\n+\n+        layer = TrainingLayer()\n+        x = np.ones((4, 4))\n+        y = layer(x)\n+        self.assertEqual(ops.min(y), 1)\n+        y = layer(x, training=True)\n+        self.assertEqual(ops.min(y), 0)\n+\n+        # Check that it still works one level deeper.\n+        class WrappedTrainingLayer(layers.Layer):\n+            def __init__(self):\n+                super().__init__()\n+                self.dp = TrainingLayer()\n+\n+            def call(self, x, training=False):\n+                return self.dp(x)\n+\n+        layer = WrappedTrainingLayer()\n+        x = np.ones((4, 4))\n+        y = layer(x)\n+        self.assertEqual(ops.min(y), 1)\n+        y = layer(x, training=True)\n+        self.assertEqual(ops.min(y), 0)\n+\n+        # Check that if `training` is passed\n+        # to an inner layer in call(), the explicitly\n+        # passed value is what the layer sees.\n+        class TrainingLayerExplicit(layers.Layer):\n+            def __init__(self):\n+                super().__init__()\n+                self.dp = layers.Dropout(0.9)\n+\n+            def call(self, x, training=False):\n+                return self.dp(x, training=True)\n+\n+        layer = TrainingLayerExplicit()\n+        x = np.ones((4, 4))\n+        y = layer(x, training=False)\n+        self.assertEqual(ops.min(y), 0)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4d18257aed25f8e38e1ccf678c912389a4881d52", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1250 | Lines Deleted: 272 | Files Changed: 6 | Hunks: 231 | Methods Changed: 356 | Complexity Δ (Sum/Max): 227/112 | Churn Δ: 1522 | Churn Cumulative: 11049 | Contributors (this commit): 6 | Commits (past 90d): 100 | Contributors (cumulative): 20 | DMM Complexity: 0.9923809523809524\n\nDIFF:\n@@ -160,14 +160,6 @@ def compute_output_spec(fn, *args, **kwargs):\n         return nest.map_structure(convert_jax_spec_to_keras_tensor, jax_out)\n \n \n-# NumPy op delegation\n-def execute(op_name, *args, **kwargs):\n-    if hasattr(jnp, op_name):\n-        op = getattr(jnp, op_name)\n-        return op(*args, **kwargs)\n-    raise AttributeError(f\"The JAX backend does not support op '{op_name}'\")\n-\n-\n def traceable_tensor(shape, dtype=None):\n     \"\"\"Create a \"traceable tensor\".\n \n\n@@ -35,3 +35,485 @@ def ones(shape, dtype=\"float32\"):\n \n def zeros(shape, dtype=\"float32\"):\n     return jnp.zeros(shape, dtype=dtype)\n+\n+\n+def absolute(x):\n+    return jnp.absolute(x)\n+\n+\n+def abs(x):\n+    return absolute(x)\n+\n+\n+def all(x, axis=None, keepdims=False):\n+    return jnp.all(x, axis=axis, keepdims=keepdims)\n+\n+\n+def amax(x, axis=None, keepdims=False):\n+    return jnp.amax(x, axis=axis, keepdims=keepdims)\n+\n+\n+def amin(x, axis=None, keepdims=False):\n+    return jnp.amin(x, axis=axis, keepdims=keepdims)\n+\n+\n+def append(\n+    x1,\n+    x2,\n+    axis=None,\n+):\n+    return jnp.append(x1, x2, axis=axis)\n+\n+\n+def arange(start, stop=None, step=None, dtype=None):\n+    return jnp.arange(start, stop, step=step, dtype=dtype)\n+\n+\n+def arccos(x):\n+    return jnp.arccos(x)\n+\n+\n+def arcsin(x):\n+    return jnp.arcsin(x)\n+\n+\n+def arctan(x):\n+    return jnp.arctan(x)\n+\n+\n+def arctan2(x1, x2):\n+    return jnp.arctan2(x1, x2)\n+\n+\n+def argmax(x, axis=None):\n+    return jnp.argmax(x, axis=axis)\n+\n+\n+def argmin(x, axis=None):\n+    return jnp.argmin(x, axis=axis)\n+\n+\n+def argsort(x, axis=-1):\n+    return jnp.argsort(x, axis=axis)\n+\n+\n+def array(x, dtype=None):\n+    return jnp.array(x, dtype=dtype)\n+\n+\n+def average(x, axis=None, weights=None):\n+    return jnp.average(x, weights=weights, axis=axis)\n+\n+\n+def broadcast_to(x, shape):\n+    return jnp.broadcast_to(x, shape)\n+\n+\n+def ceil(x):\n+    return jnp.ceil(x)\n+\n+\n+def clip(x, x_min, x_max):\n+    return jnp.clip(x, x_min, x_max)\n+\n+\n+def concatenate(xs, axis=0):\n+    return jnp.concatenate(xs, axis=axis)\n+\n+\n+def conjugate(x):\n+    return jnp.conjugate(x)\n+\n+\n+def conj(x):\n+    return conjugate(x)\n+\n+\n+def copy(x):\n+    return jnp.copy(x)\n+\n+\n+def cos(x):\n+    return jnp.cos(x)\n+\n+\n+def count_nonzero(x, axis=None):\n+    return jnp.count_nonzero(x, axis=axis)\n+\n+\n+def cross(x1, x2, axisa=-1, axisb=-1, axisc=-1, axis=None):\n+    return jnp.cross(\n+        x1,\n+        x2,\n+        axisa=axisa,\n+        axisb=axisb,\n+        axisc=axisc,\n+        axis=axis,\n+    )\n+\n+\n+def cumprod(x, axis=None):\n+    return jnp.cumprod(x, axis=axis)\n+\n+\n+def cumsum(x, axis=None):\n+    return jnp.cumsum(x, axis=axis)\n+\n+\n+def diag(x, k=0):\n+    return jnp.diag(x, k=k)\n+\n+\n+def diagonal(x, offset=0, axis1=0, axis2=1):\n+    return jnp.diagonal(\n+        x,\n+        offset=offset,\n+        axis1=axis1,\n+        axis2=axis2,\n+    )\n+\n+\n+def dot(x, y):\n+    return jnp.dot(x, y)\n+\n+\n+def empty(shape, dtype=\"float32\"):\n+    return jnp.empty(shape, dtype=dtype)\n+\n+\n+def equal(x1, x2):\n+    return jnp.equal(x1, x2)\n+\n+\n+def exp(x):\n+    return jnp.exp(x)\n+\n+\n+def expand_dims(x, axis):\n+    return jnp.expand_dims(x, axis)\n+\n+\n+def expm1(x):\n+    return jnp.expm1(x)\n+\n+\n+def flip(x, axis=None):\n+    return jnp.flip(x, axis=axis)\n+\n+\n+def floor(x):\n+    return jnp.floor(x)\n+\n+\n+def full(shape, fill_value, dtype=None):\n+    return jnp.full(shape, fill_value, dtype=dtype)\n+\n+\n+def full_like(x, fill_value, dtype=None):\n+    return jnp.full_like(x, fill_value, dtype=dtype)\n+\n+\n+def greater(x1, x2):\n+    return jnp.greater(x1, x2)\n+\n+\n+def greater_equal(x1, x2):\n+    return jnp.greater_equal(x1, x2)\n+\n+\n+def hstack(xs):\n+    return jnp.hstack(xs)\n+\n+\n+def identity(n, dtype=\"float32\"):\n+    return jnp.identity(n, dtype=dtype)\n+\n+\n+def imag(x):\n+    return jnp.imag(x)\n+\n+\n+def isclose(x1, x2):\n+    return jnp.isclose(x1, x2)\n+\n+\n+def isfinite(x):\n+    return jnp.isfinite(x)\n+\n+\n+def isinf(x):\n+    return jnp.isinf(x)\n+\n+\n+def isnan(x):\n+    return jnp.isnan(x)\n+\n+\n+def less(x1, x2):\n+    return jnp.less(x1, x2)\n+\n+\n+def less_equal(x1, x2):\n+    return jnp.less_equal(x1, x2)\n+\n+\n+def linspace(\n+    start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0\n+):\n+    return jnp.linspace(\n+        start,\n+        stop,\n+        num=num,\n+        endpoint=endpoint,\n+        retstep=retstep,\n+        dtype=dtype,\n+        axis=axis,\n+    )\n+\n+\n+def log(x):\n+    return jnp.log(x)\n+\n+\n+def log10(x):\n+    return jnp.log10(x)\n+\n+\n+def log1p(x):\n+    return jnp.log1p(x)\n+\n+\n+def log2(x):\n+    return jnp.log2(x)\n+\n+\n+def logaddexp(x1, x2):\n+    return jnp.logaddexp(x1, x2)\n+\n+\n+def logical_and(x1, x2):\n+    return jnp.logical_and(x1, x2)\n+\n+\n+def logical_not(x):\n+    return jnp.logical_not(x)\n+\n+\n+def logical_or(x1, x2):\n+    return jnp.logical_or(x1, x2)\n+\n+\n+def logspace(start, stop, num=50, endpoint=True, base=10, dtype=None, axis=0):\n+    return jnp.logspace(\n+        start,\n+        stop,\n+        num=num,\n+        endpoint=endpoint,\n+        base=base,\n+        dtype=dtype,\n+        axis=axis,\n+    )\n+\n+\n+def maximum(x1, x2):\n+    return jnp.maximum(x1, x2)\n+\n+\n+def meshgrid(*x, indexing=\"xy\"):\n+    return jnp.meshgrid(*x, indexing=indexing)\n+\n+\n+def min(x, axis=None, keepdims=False):\n+    return jnp.min(x, axis=axis, keepdims=keepdims)\n+\n+\n+def minimum(x1, x2):\n+    return jnp.minimum(x1, x2)\n+\n+\n+def mod(x1, x2):\n+    return jnp.mod(x1, x2)\n+\n+\n+def moveaxis(x, source, destination):\n+    return jnp.moveaxis(x, source=source, destination=destination)\n+\n+\n+def ndim(x):\n+    return jnp.ndim(x)\n+\n+\n+def nonzero(x):\n+    return jnp.nonzero(x)\n+\n+\n+def not_equal(x1, x2):\n+    return jnp.not_equal(x1, x2)\n+\n+\n+def ones_like(x, dtype=None):\n+    return jnp.ones_like(x, dtype=dtype)\n+\n+\n+def outer(x1, x2):\n+    return jnp.outer(x1, x2)\n+\n+\n+def pad(x, pad_width, mode=\"constant\"):\n+    return jnp.pad(x, pad_width, mode=mode)\n+\n+\n+def prod(x, axis=None, keepdims=False, dtype=None):\n+    return jnp.prod(x, axis=axis, keepdims=keepdims, dtype=dtype)\n+\n+\n+def ravel(x):\n+    return jnp.ravel(x)\n+\n+\n+def real(x):\n+    return jnp.real(x)\n+\n+\n+def reciprocal(x):\n+    return jnp.reciprocal(x)\n+\n+\n+def repeat(x, repeats, axis=None):\n+    return jnp.repeat(x, repeats, axis=axis)\n+\n+\n+def reshape(x, new_shape):\n+    return jnp.reshape(x, new_shape)\n+\n+\n+def roll(x, shift, axis=None):\n+    return jnp.roll(x, shift, axis=axis)\n+\n+\n+def sign(x):\n+    return jnp.sign(x)\n+\n+\n+def sin(x):\n+    return jnp.sin(x)\n+\n+\n+def size(x):\n+    return jnp.size(x)\n+\n+\n+def sort(x, axis=-1):\n+    return jnp.sort(x, axis=axis)\n+\n+\n+def split(x, indices_or_sections, axis=0):\n+    return jnp.split(x, indices_or_sections, axis=axis)\n+\n+\n+def stack(x, axis=0):\n+    return jnp.stack(x, axis=axis)\n+\n+\n+def std(x, axis=None, keepdims=False):\n+    return jnp.std(x, axis=axis, keepdims=keepdims)\n+\n+\n+def swapaxes(x, axis1, axis2):\n+    return jnp.swapaxes(x, axis1=axis1, axis2=axis2)\n+\n+\n+def take(x, indices, axis=None):\n+    return jnp.take(x, indices, axis=axis)\n+\n+\n+def take_along_axis(x, indices, axis=None):\n+    return jnp.take_along_axis(x, indices, axis=axis)\n+\n+\n+def tan(x):\n+    return jnp.tan(x)\n+\n+\n+def tensordot(x1, x2, axes=2):\n+    return jnp.tensordot(x1, x2, axes=axes)\n+\n+\n+def round(x, decimals=0):\n+    return jnp.round(x, decimals=decimals)\n+\n+\n+def tile(x, repeats):\n+    return jnp.tile(x, repeats)\n+\n+\n+def trace(x, offset=0, axis1=0, axis2=1):\n+    return jnp.trace(x, offset=offset, axis1=axis1, axis2=axis2)\n+\n+\n+def tri(N, M=None, k=0, dtype=\"float32\"):\n+    return jnp.tri(N, M=M, k=k, dtype=dtype)\n+\n+\n+def tril(x, k=0):\n+    return jnp.tril(x, k=k)\n+\n+\n+def triu(x, k=0):\n+    return jnp.triu(x, k=k)\n+\n+\n+def vdot(x1, x2):\n+    return jnp.vdot(x1, x2)\n+\n+\n+def vstack(xs):\n+    return jnp.vstack(xs)\n+\n+\n+def where(condition, x1, x2):\n+    return jnp.where(condition, x1, x2)\n+\n+\n+def divide(x1, x2):\n+    return jnp.divide(x1, x2)\n+\n+\n+def true_divide(x1, x2):\n+    return jnp.true_divide(x1, x2)\n+\n+\n+def power(x1, x2):\n+    return jnp.power(x1, x2)\n+\n+\n+def negative(x):\n+    return jnp.negative(x)\n+\n+\n+def square(x):\n+    return jnp.square(x)\n+\n+\n+def sqrt(x):\n+    return jnp.sqrt(x)\n+\n+\n+def squeeze(x, axis=None):\n+    return jnp.squeeze(x, axis=axis)\n+\n+\n+def transpose(x, axes=None):\n+    return jnp.transpose(x, axes=axes)\n+\n+\n+def var(x, axis=None, keepdims=False):\n+    return jnp.var(x, axis=axis, keepdims=keepdims)\n+\n+\n+def sum(x, axis=None, keepdims=False):\n+    return jnp.sum(x, axis=axis, keepdims=keepdims)\n+\n+\n+def eye(N, M=None, k=0, dtype=\"float32\"):\n+    return jnp.eye(N, M=M, k=k, dtype=dtype)\n\n@@ -128,15 +128,6 @@ def compute_output_spec(fn, *args, **kwargs):\n             return tf.nest.map_structure(convert_tf_to_keras_tensor, tf_out)\n \n \n-def execute(op_name, *args, **kwargs):\n-    if hasattr(tfnp, op_name):\n-        op = getattr(tfnp, op_name)\n-        return op(*args, **kwargs)\n-    raise AttributeError(\n-        f\"The TensorFlow backend does not support op '{op_name}'\"\n-    )\n-\n-\n def traceable_tensor(shape, dtype=None):\n     \"\"\"Create a \"traceable tensor\".\n \n\n@@ -36,3 +36,485 @@ def ones(shape, dtype=\"float32\"):\n \n def zeros(shape, dtype=\"float32\"):\n     return tf.zeros(shape, dtype=dtype)\n+\n+\n+def absolute(x):\n+    return tfnp.absolute(x)\n+\n+\n+def abs(x):\n+    return absolute(x)\n+\n+\n+def all(x, axis=None, keepdims=False):\n+    return tfnp.all(x, axis=axis, keepdims=keepdims)\n+\n+\n+def amax(x, axis=None, keepdims=False):\n+    return tfnp.amax(x, axis=axis, keepdims=keepdims)\n+\n+\n+def amin(x, axis=None, keepdims=False):\n+    return tfnp.amin(x, axis=axis, keepdims=keepdims)\n+\n+\n+def append(\n+    x1,\n+    x2,\n+    axis=None,\n+):\n+    return tfnp.append(x1, x2, axis=axis)\n+\n+\n+def arange(start, stop=None, step=None, dtype=None):\n+    return tfnp.arange(start, stop, step=step, dtype=dtype)\n+\n+\n+def arccos(x):\n+    return tfnp.arccos(x)\n+\n+\n+def arcsin(x):\n+    return tfnp.arcsin(x)\n+\n+\n+def arctan(x):\n+    return tfnp.arctan(x)\n+\n+\n+def arctan2(x1, x2):\n+    return tfnp.arctan2(x1, x2)\n+\n+\n+def argmax(x, axis=None):\n+    return tfnp.argmax(x, axis=axis)\n+\n+\n+def argmin(x, axis=None):\n+    return tfnp.argmin(x, axis=axis)\n+\n+\n+def argsort(x, axis=-1):\n+    return tfnp.argsort(x, axis=axis)\n+\n+\n+def array(x, dtype=None):\n+    return tfnp.array(x, dtype=dtype)\n+\n+\n+def average(x, axis=None, weights=None):\n+    return tfnp.average(x, weights=weights, axis=axis)\n+\n+\n+def broadcast_to(x, shape):\n+    return tfnp.broadcast_to(x, shape)\n+\n+\n+def ceil(x):\n+    return tfnp.ceil(x)\n+\n+\n+def clip(x, x_min, x_max):\n+    return tfnp.clip(x, x_min, x_max)\n+\n+\n+def concatenate(xs, axis=0):\n+    return tfnp.concatenate(xs, axis=axis)\n+\n+\n+def conjugate(x):\n+    return tfnp.conjugate(x)\n+\n+\n+def conj(x):\n+    return conjugate(x)\n+\n+\n+def copy(x):\n+    return tfnp.copy(x)\n+\n+\n+def cos(x):\n+    return tfnp.cos(x)\n+\n+\n+def count_nonzero(x, axis=None):\n+    return tfnp.count_nonzero(x, axis=axis)\n+\n+\n+def cross(x1, x2, axisa=-1, axisb=-1, axisc=-1, axis=None):\n+    return tfnp.cross(\n+        x1,\n+        x2,\n+        axisa=axisa,\n+        axisb=axisb,\n+        axisc=axisc,\n+        axis=axis,\n+    )\n+\n+\n+def cumprod(x, axis=None):\n+    return tfnp.cumprod(x, axis=axis)\n+\n+\n+def cumsum(x, axis=None):\n+    return tfnp.cumsum(x, axis=axis)\n+\n+\n+def diag(x, k=0):\n+    return tfnp.diag(x, k=k)\n+\n+\n+def diagonal(x, offset=0, axis1=0, axis2=1):\n+    return tfnp.diagonal(\n+        x,\n+        offset=offset,\n+        axis1=axis1,\n+        axis2=axis2,\n+    )\n+\n+\n+def dot(x, y):\n+    return tfnp.dot(x, y)\n+\n+\n+def empty(shape, dtype=\"float32\"):\n+    return tfnp.empty(shape, dtype=dtype)\n+\n+\n+def equal(x1, x2):\n+    return tfnp.equal(x1, x2)\n+\n+\n+def exp(x):\n+    return tfnp.exp(x)\n+\n+\n+def expand_dims(x, axis):\n+    return tfnp.expand_dims(x, axis)\n+\n+\n+def expm1(x):\n+    return tfnp.expm1(x)\n+\n+\n+def flip(x, axis=None):\n+    return tfnp.flip(x, axis=axis)\n+\n+\n+def floor(x):\n+    return tfnp.floor(x)\n+\n+\n+def full(shape, fill_value, dtype=None):\n+    return tfnp.full(shape, fill_value, dtype=dtype)\n+\n+\n+def full_like(x, fill_value, dtype=None):\n+    return tfnp.full_like(x, fill_value, dtype=dtype)\n+\n+\n+def greater(x1, x2):\n+    return tfnp.greater(x1, x2)\n+\n+\n+def greater_equal(x1, x2):\n+    return tfnp.greater_equal(x1, x2)\n+\n+\n+def hstack(xs):\n+    return tfnp.hstack(xs)\n+\n+\n+def identity(n, dtype=\"float32\"):\n+    return tfnp.identity(n, dtype=dtype)\n+\n+\n+def imag(x):\n+    return tfnp.imag(x)\n+\n+\n+def isclose(x1, x2):\n+    return tfnp.isclose(x1, x2)\n+\n+\n+def isfinite(x):\n+    return tfnp.isfinite(x)\n+\n+\n+def isinf(x):\n+    return tfnp.isinf(x)\n+\n+\n+def isnan(x):\n+    return tfnp.isnan(x)\n+\n+\n+def less(x1, x2):\n+    return tfnp.less(x1, x2)\n+\n+\n+def less_equal(x1, x2):\n+    return tfnp.less_equal(x1, x2)\n+\n+\n+def linspace(\n+    start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0\n+):\n+    return tfnp.linspace(\n+        start,\n+        stop,\n+        num=num,\n+        endpoint=endpoint,\n+        retstep=retstep,\n+        dtype=dtype,\n+        axis=axis,\n+    )\n+\n+\n+def log(x):\n+    return tfnp.log(x)\n+\n+\n+def log10(x):\n+    return tfnp.log10(x)\n+\n+\n+def log1p(x):\n+    return tfnp.log1p(x)\n+\n+\n+def log2(x):\n+    return tfnp.log2(x)\n+\n+\n+def logaddexp(x1, x2):\n+    return tfnp.logaddexp(x1, x2)\n+\n+\n+def logical_and(x1, x2):\n+    return tfnp.logical_and(x1, x2)\n+\n+\n+def logical_not(x):\n+    return tfnp.logical_not(x)\n+\n+\n+def logical_or(x1, x2):\n+    return tfnp.logical_or(x1, x2)\n+\n+\n+def logspace(start, stop, num=50, endpoint=True, base=10, dtype=None, axis=0):\n+    return tfnp.logspace(\n+        start,\n+        stop,\n+        num=num,\n+        endpoint=endpoint,\n+        base=base,\n+        dtype=dtype,\n+        axis=axis,\n+    )\n+\n+\n+def maximum(x1, x2):\n+    return tfnp.maximum(x1, x2)\n+\n+\n+def meshgrid(*x, indexing=\"xy\"):\n+    return tfnp.meshgrid(*x, indexing=indexing)\n+\n+\n+def min(x, axis=None, keepdims=False):\n+    return tfnp.min(x, axis=axis, keepdims=keepdims)\n+\n+\n+def minimum(x1, x2):\n+    return tfnp.minimum(x1, x2)\n+\n+\n+def mod(x1, x2):\n+    return tfnp.mod(x1, x2)\n+\n+\n+def moveaxis(x, source, destination):\n+    return tfnp.moveaxis(x, source=source, destination=destination)\n+\n+\n+def ndim(x):\n+    return tfnp.ndim(x)\n+\n+\n+def nonzero(x):\n+    return tfnp.nonzero(x)\n+\n+\n+def not_equal(x1, x2):\n+    return tfnp.not_equal(x1, x2)\n+\n+\n+def ones_like(x, dtype=None):\n+    return tfnp.ones_like(x, dtype=dtype)\n+\n+\n+def outer(x1, x2):\n+    return tfnp.outer(x1, x2)\n+\n+\n+def pad(x, pad_width, mode=\"constant\"):\n+    return tfnp.pad(x, pad_width, mode=mode)\n+\n+\n+def prod(x, axis=None, keepdims=False, dtype=None):\n+    return tfnp.prod(x, axis=axis, keepdims=keepdims, dtype=dtype)\n+\n+\n+def ravel(x):\n+    return tfnp.ravel(x)\n+\n+\n+def real(x):\n+    return tfnp.real(x)\n+\n+\n+def reciprocal(x):\n+    return tfnp.reciprocal(x)\n+\n+\n+def repeat(x, repeats, axis=None):\n+    return tfnp.repeat(x, repeats, axis=axis)\n+\n+\n+def reshape(x, new_shape):\n+    return tfnp.reshape(x, new_shape)\n+\n+\n+def roll(x, shift, axis=None):\n+    return tfnp.roll(x, shift, axis=axis)\n+\n+\n+def sign(x):\n+    return tfnp.sign(x)\n+\n+\n+def sin(x):\n+    return tfnp.sin(x)\n+\n+\n+def size(x):\n+    return tfnp.size(x)\n+\n+\n+def sort(x, axis=-1):\n+    return tfnp.sort(x, axis=axis)\n+\n+\n+def split(x, indices_or_sections, axis=0):\n+    return tfnp.split(x, indices_or_sections, axis=axis)\n+\n+\n+def stack(x, axis=0):\n+    return tfnp.stack(x, axis=axis)\n+\n+\n+def std(x, axis=None, keepdims=False):\n+    return tfnp.std(x, axis=axis, keepdims=keepdims)\n+\n+\n+def swapaxes(x, axis1, axis2):\n+    return tfnp.swapaxes(x, axis1=axis1, axis2=axis2)\n+\n+\n+def take(x, indices, axis=None):\n+    return tfnp.take(x, indices, axis=axis)\n+\n+\n+def take_along_axis(x, indices, axis=None):\n+    return tfnp.take_along_axis(x, indices, axis=axis)\n+\n+\n+def tan(x):\n+    return tfnp.tan(x)\n+\n+\n+def tensordot(x1, x2, axes=2):\n+    return tfnp.tensordot(x1, x2, axes=axes)\n+\n+\n+def round(x, decimals=0):\n+    return tfnp.round(x, decimals=decimals)\n+\n+\n+def tile(x, repeats):\n+    return tfnp.tile(x, repeats)\n+\n+\n+def trace(x, offset=0, axis1=0, axis2=1):\n+    return tfnp.trace(x, offset=offset, axis1=axis1, axis2=axis2)\n+\n+\n+def tri(N, M=None, k=0, dtype=\"float32\"):\n+    return tfnp.tri(N, M=M, k=k, dtype=dtype)\n+\n+\n+def tril(x, k=0):\n+    return tfnp.tril(x, k=k)\n+\n+\n+def triu(x, k=0):\n+    return tfnp.triu(x, k=k)\n+\n+\n+def vdot(x1, x2):\n+    return tfnp.vdot(x1, x2)\n+\n+\n+def vstack(xs):\n+    return tfnp.vstack(xs)\n+\n+\n+def where(condition, x1, x2):\n+    return tfnp.where(condition, x1, x2)\n+\n+\n+def divide(x1, x2):\n+    return tfnp.divide(x1, x2)\n+\n+\n+def true_divide(x1, x2):\n+    return tfnp.true_divide(x1, x2)\n+\n+\n+def power(x1, x2):\n+    return tfnp.power(x1, x2)\n+\n+\n+def negative(x):\n+    return tfnp.negative(x)\n+\n+\n+def square(x):\n+    return tfnp.square(x)\n+\n+\n+def sqrt(x):\n+    return tfnp.sqrt(x)\n+\n+\n+def squeeze(x, axis=None):\n+    return tfnp.squeeze(x, axis=axis)\n+\n+\n+def transpose(x, axes=None):\n+    return tfnp.transpose(x, axes=axes)\n+\n+\n+def var(x, axis=None, keepdims=False):\n+    return tfnp.var(x, axis=axis, keepdims=keepdims)\n+\n+\n+def sum(x, axis=None, keepdims=False):\n+    return tfnp.sum(x, axis=axis, keepdims=keepdims)\n+\n+\n+def eye(N, M=None, k=0, dtype=\"float32\"):\n+    return tfnp.eye(N, M=M, k=k, dtype=dtype)\n\n@@ -223,7 +223,7 @@ def shape_equal(shape1, shape2, axis=None, allow_none=True):\n \n class Absolute(Operation):\n     def call(self, x):\n-        return backend.execute(\"absolute\", x)\n+        return backend.numpy.absolute(x)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape, dtype=x.dtype)\n@@ -232,7 +232,7 @@ class Absolute(Operation):\n def absolute(x):\n     if any_symbolic_tensors((x,)):\n         return Absolute().symbolic_call(x)\n-    return backend.execute(\"absolute\", x)\n+    return backend.numpy.absolute(x)\n \n \n class Abs(Absolute):\n@@ -270,8 +270,7 @@ class All(Operation):\n         self.keepdims = keepdims\n \n     def call(self, x):\n-        return backend.execute(\n-            \"all\",\n+        return backend.numpy.all(\n             x,\n             axis=self.axis,\n             keepdims=self.keepdims,\n@@ -291,7 +290,7 @@ class All(Operation):\n def all(x, axis=None, keepdims=False):\n     if any_symbolic_tensors((x,)):\n         return All(axis=axis, keepdims=keepdims).symbolic_call(x)\n-    return backend.execute(\"all\", x, axis=axis, keepdims=keepdims)\n+    return backend.numpy.all(x, axis=axis, keepdims=keepdims)\n \n \n class Amax(Operation):\n@@ -303,8 +302,7 @@ class Amax(Operation):\n         self.keepdims = keepdims\n \n     def call(self, x):\n-        return backend.execute(\n-            \"amax\",\n+        return backend.numpy.amax(\n             x,\n             axis=self.axis,\n             keepdims=self.keepdims,\n@@ -320,7 +318,7 @@ class Amax(Operation):\n def amax(x, axis=None, keepdims=False):\n     if any_symbolic_tensors((x,)):\n         return All(axis=axis, keepdims=keepdims).symbolic_call(x)\n-    return backend.execute(\"amax\", x, axis=axis, keepdims=keepdims)\n+    return backend.numpy.amax(x, axis=axis, keepdims=keepdims)\n \n \n class Amin(Operation):\n@@ -332,9 +330,7 @@ class Amin(Operation):\n         self.keepdims = keepdims\n \n     def call(self, x):\n-        return backend.execute(\n-            \"amin\", x, axis=self.axis, keepdims=self.keepdims\n-        )\n+        return backend.numpy.amin(x, axis=self.axis, keepdims=self.keepdims)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(\n@@ -346,7 +342,7 @@ class Amin(Operation):\n def amin(x, axis=None, keepdims=False):\n     if any_symbolic_tensors((x,)):\n         return All(axis=axis, keepdims=keepdims).symbolic_call(x)\n-    return backend.execute(\"amin\", x, axis=axis, keepdims=keepdims)\n+    return backend.numpy.amin(x, axis=axis, keepdims=keepdims)\n \n \n class Append(Operation):\n@@ -355,7 +351,7 @@ class Append(Operation):\n         self.axis = axis\n \n     def call(self, x1, x2):\n-        return backend.execute(\"append\", x1, x2, axis=self.axis)\n+        return backend.numpy.append(x1, x2, axis=self.axis)\n \n     def compute_output_spec(self, x1, x2):\n         x1_shape = x1.shape\n@@ -386,7 +382,7 @@ def append(\n ):\n     if any_symbolic_tensors((x1, x2)):\n         return Append(axis=axis).symbolic_call(x1, x2)\n-    return backend.execute(\"append\", x1, x2, axis=axis)\n+    return backend.numpy.append(x1, x2, axis=axis)\n \n \n class Arange(Operation):\n@@ -395,7 +391,7 @@ class Arange(Operation):\n             start, stop = 0, start\n         if step is None:\n             step = 1\n-        return backend.execute(\"arange\", start, stop, step=step, dtype=dtype)\n+        return backend.numpy.arange(start, stop, step=step, dtype=dtype)\n \n     def compute_output_spec(self, start, stop=None, step=None, dtype=None):\n         if stop is None:\n@@ -411,12 +407,12 @@ def arange(start, stop=None, step=None, dtype=None):\n         start, stop = 0, start\n     if step is None:\n         step = 1\n-    return backend.execute(\"arange\", start, stop, step=step, dtype=dtype)\n+    return backend.numpy.arange(start, stop, step=step, dtype=dtype)\n \n \n class Arccos(Operation):\n     def call(self, x):\n-        return backend.execute(\"arccos\", x)\n+        return backend.numpy.arccos(x)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape, dtype=x.dtype)\n@@ -425,12 +421,12 @@ class Arccos(Operation):\n def arccos(x):\n     if any_symbolic_tensors((x,)):\n         return Arccos().symbolic_call(x)\n-    return backend.execute(\"arccos\", x)\n+    return backend.numpy.arccos(x)\n \n \n class Arcsin(Operation):\n     def call(self, x):\n-        return backend.execute(\"arcsin\", x)\n+        return backend.numpy.arcsin(x)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape, dtype=x.dtype)\n@@ -439,12 +435,12 @@ class Arcsin(Operation):\n def arcsin(x):\n     if any_symbolic_tensors((x,)):\n         return Arcsin().symbolic_call(x)\n-    return backend.execute(\"arcsin\", x)\n+    return backend.numpy.arcsin(x)\n \n \n class Arctan(Operation):\n     def call(self, x):\n-        return backend.execute(\"arctan\", x)\n+        return backend.numpy.arctan(x)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape, dtype=x.dtype)\n@@ -453,12 +449,12 @@ class Arctan(Operation):\n def arctan(x):\n     if any_symbolic_tensors((x,)):\n         return Arctan().symbolic_call(x)\n-    return backend.execute(\"arctan\", x)\n+    return backend.numpy.arctan(x)\n \n \n class Arctan2(Operation):\n     def call(self, x1, x2):\n-        return backend.execute(\"arctan2\", x1, x2)\n+        return backend.numpy.arctan2(x1, x2)\n \n     def compute_output_spec(self, x1, x2):\n         x1_shape = getattr(x1, \"shape\", [])\n@@ -470,7 +466,7 @@ class Arctan2(Operation):\n def arctan2(x1, x2):\n     if any_symbolic_tensors((x1, x2)):\n         return Arctan2().symbolic_call(x1, x2)\n-    return backend.execute(\"arctan2\", x1, x2)\n+    return backend.numpy.arctan2(x1, x2)\n \n \n class Argmax(Operation):\n@@ -479,7 +475,7 @@ class Argmax(Operation):\n         self.axis = axis\n \n     def call(self, x):\n-        return backend.execute(\"argmax\", x, axis=self.axis)\n+        return backend.numpy.argmax(x, axis=self.axis)\n \n     def compute_output_spec(self, x):\n         if self.axis is None:\n@@ -492,7 +488,7 @@ class Argmax(Operation):\n def argmax(x, axis=None):\n     if any_symbolic_tensors((x,)):\n         return Argmax(axis=axis).symbolic_call(x)\n-    return backend.execute(\"argmax\", x, axis=axis)\n+    return backend.numpy.argmax(x, axis=axis)\n \n \n class Argmin(Operation):\n@@ -501,7 +497,7 @@ class Argmin(Operation):\n         self.axis = axis\n \n     def call(self, x):\n-        return backend.execute(\"argmin\", x, axis=self.axis)\n+        return backend.numpy.argmin(x, axis=self.axis)\n \n     def compute_output_spec(self, x):\n         if self.axis is None:\n@@ -514,7 +510,7 @@ class Argmin(Operation):\n def argmin(x, axis=None):\n     if any_symbolic_tensors((x,)):\n         return Argmin(axis=axis).symbolic_call(x)\n-    return backend.execute(\"argmin\", x, axis=axis)\n+    return backend.numpy.argmin(x, axis=axis)\n \n \n class Argsort(Operation):\n@@ -523,7 +519,7 @@ class Argsort(Operation):\n         self.axis = axis\n \n     def call(self, x):\n-        return backend.execute(\"argsort\", x, axis=self.axis)\n+        return backend.numpy.argsort(x, axis=self.axis)\n \n     def compute_output_spec(self, x):\n         if self.axis is None:\n@@ -534,12 +530,12 @@ class Argsort(Operation):\n def argsort(x, axis=-1):\n     if any_symbolic_tensors((x,)):\n         return Argsort(axis=axis).symbolic_call(x)\n-    return backend.execute(\"argsort\", x, axis=axis)\n+    return backend.numpy.argsort(x, axis=axis)\n \n \n class Array(Operation):\n     def call(self, x, dtype=None):\n-        return backend.execute(\"array\", x, dtype=dtype)\n+        return backend.numpy.array(x, dtype=dtype)\n \n     def compute_output_spec(self, x, dtype=None):\n         return KerasTensor(x.shape, dtype=dtype)\n@@ -548,7 +544,7 @@ class Array(Operation):\n def array(x, dtype=None):\n     if any_symbolic_tensors((x,)):\n         return Array().symbolic_call(x, dtype=dtype)\n-    return backend.execute(\"array\", x, dtype=dtype)\n+    return backend.numpy.array(x, dtype=dtype)\n \n \n class Average(Operation):\n@@ -559,7 +555,7 @@ class Average(Operation):\n         self.axis = axis\n \n     def call(self, x, weights=None):\n-        return backend.execute(\"average\", x, weights=weights, axis=self.axis)\n+        return backend.numpy.average(x, weights=weights, axis=self.axis)\n \n     def compute_output_spec(self, x, weights=None):\n         if weights is not None:\n@@ -600,7 +596,7 @@ class Average(Operation):\n def average(x, axis=None, weights=None):\n     if any_symbolic_tensors((x,)):\n         return Average(axis=axis).symbolic_call(x, weights=weights)\n-    return backend.execute(\"average\", x, weights=weights, axis=axis)\n+    return backend.numpy.average(x, weights=weights, axis=axis)\n \n \n class BroadcastTo(Operation):\n@@ -609,7 +605,7 @@ class BroadcastTo(Operation):\n         self.shape = shape\n \n     def call(self, x):\n-        return backend.execute(\"broadcast_to\", x, self.shape)\n+        return backend.numpy.broadcast_to(x, self.shape)\n \n     def compute_output_spec(self, x):\n         # Catch broadcasting errors for clear error messages.\n@@ -620,12 +616,12 @@ class BroadcastTo(Operation):\n def broadcast_to(x, shape):\n     if any_symbolic_tensors((x,)):\n         return BroadcastTo(shape=shape).symbolic_call(x)\n-    return backend.execute(\"broadcast_to\", x, shape)\n+    return backend.numpy.broadcast_to(x, shape)\n \n \n class Ceil(Operation):\n     def call(self, x):\n-        return backend.execute(\"ceil\", x)\n+        return backend.numpy.ceil(x)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape, dtype=x.dtype)\n@@ -634,7 +630,7 @@ class Ceil(Operation):\n def ceil(x):\n     if any_symbolic_tensors((x,)):\n         return Ceil().symbolic_call(x)\n-    return backend.execute(\"ceil\", x)\n+    return backend.numpy.ceil(x)\n \n \n class Clip(Operation):\n@@ -644,7 +640,7 @@ class Clip(Operation):\n         self.x_max = x_max\n \n     def call(self, x):\n-        return backend.execute(\"clip\", x, self.x_min, self.x_max)\n+        return backend.numpy.clip(x, self.x_min, self.x_max)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape, dtype=x.dtype)\n@@ -653,7 +649,7 @@ class Clip(Operation):\n def clip(x, x_min, x_max):\n     if any_symbolic_tensors((x,)):\n         return Clip(x_min, x_max).symbolic_call(x)\n-    return backend.execute(\"clip\", x, x_min, x_max)\n+    return backend.numpy.clip(x, x_min, x_max)\n \n \n class Concatenate(Operation):\n@@ -664,7 +660,7 @@ class Concatenate(Operation):\n         self.axis = axis\n \n     def call(self, xs):\n-        return backend.execute(\"concatenate\", xs, axis=self.axis)\n+        return backend.numpy.concatenate(xs, axis=self.axis)\n \n     def compute_output_spec(self, xs):\n         first_shape = xs[0].shape\n@@ -691,12 +687,12 @@ class Concatenate(Operation):\n def concatenate(xs, axis=0):\n     if any_symbolic_tensors(xs):\n         return Concatenate(axis=axis).symbolic_call(xs)\n-    return backend.execute(\"concatenate\", xs, axis=axis)\n+    return backend.numpy.concatenate(xs, axis=axis)\n \n \n class Conjugate(Operation):\n     def call(self, x):\n-        return backend.execute(\"conjugate\", x)\n+        return backend.numpy.conjugate(x)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape, dtype=x.dtype)\n@@ -705,7 +701,7 @@ class Conjugate(Operation):\n def conjugate(x):\n     if any_symbolic_tensors((x,)):\n         return Conjugate().symbolic_call(x)\n-    return backend.execute(\"conjugate\", x)\n+    return backend.numpy.conjugate(x)\n \n \n class Conj(Conjugate):\n@@ -718,7 +714,7 @@ def conj(x):\n \n class Copy(Operation):\n     def call(self, x):\n-        return backend.execute(\"copy\", x)\n+        return backend.numpy.copy(x)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape, dtype=x.dtype)\n@@ -727,12 +723,12 @@ class Copy(Operation):\n def copy(x):\n     if any_symbolic_tensors((x,)):\n         return Copy().symbolic_call(x)\n-    return backend.execute(\"copy\", x)\n+    return backend.numpy.copy(x)\n \n \n class Cos(Operation):\n     def call(self, x):\n-        return backend.execute(\"cos\", x)\n+        return backend.numpy.cos(x)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape, dtype=x.dtype)\n@@ -741,7 +737,7 @@ class Cos(Operation):\n def cos(x):\n     if any_symbolic_tensors((x,)):\n         return Cos().symbolic_call(x)\n-    return backend.execute(\"cos\", x)\n+    return backend.numpy.cos(x)\n \n \n class CountNonzero(Operation):\n@@ -753,7 +749,7 @@ class CountNonzero(Operation):\n             self.axis = axis\n \n     def call(self, x):\n-        return backend.execute(\"count_nonzero\", x, axis=self.axis)\n+        return backend.numpy.count_nonzero(x, axis=self.axis)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(\n@@ -765,7 +761,7 @@ class CountNonzero(Operation):\n def count_nonzero(x, axis=None):\n     if any_symbolic_tensors((x,)):\n         return CountNonzero(axis=axis).symbolic_call(x)\n-    return backend.execute(\"count_nonzero\", x, axis=axis)\n+    return backend.numpy.count_nonzero(x, axis=axis)\n \n \n class Cross(Operation):\n@@ -781,9 +777,7 @@ class Cross(Operation):\n             self.axisc = axisc\n \n     def call(self, x1, x2):\n-        return backend.execute(\n-            \"cross\", x1, x2, self.axisa, self.axisb, self.axisc\n-        )\n+        return backend.numpy.cross(x1, x2, self.axisa, self.axisb, self.axisc)\n \n     def compute_output_spec(self, x1, x2):\n         x1_shape = list(x1.shape)\n@@ -822,8 +816,7 @@ def cross(x1, x2, axisa=-1, axisb=-1, axisc=-1, axis=None):\n         return Cross(\n             axisa=axisa, axisb=axisb, axisc=axisc, axis=axis\n         ).symbolic_call(x1, x2)\n-    return backend.execute(\n-        \"cross\",\n+    return backend.numpy.cross(\n         x1,\n         x2,\n         axisa=axisa,\n@@ -839,7 +832,7 @@ class Cumprod(Operation):\n         self.axis = axis\n \n     def call(self, x):\n-        return backend.execute(\"cumprod\", x, axis=self.axis)\n+        return backend.numpy.cumprod(x, axis=self.axis)\n \n     def compute_output_spec(self, x):\n         if self.axis is None:\n@@ -854,7 +847,7 @@ class Cumprod(Operation):\n def cumprod(x, axis=None):\n     if any_symbolic_tensors((x,)):\n         return Cumprod(axis=axis).symbolic_call(x)\n-    return backend.execute(\"cumprod\", x, axis=axis)\n+    return backend.numpy.cumprod(x, axis=axis)\n \n \n class Cumsum(Operation):\n@@ -863,7 +856,7 @@ class Cumsum(Operation):\n         self.axis = axis\n \n     def call(self, x):\n-        return backend.execute(\"cumsum\", x, axis=self.axis)\n+        return backend.numpy.cumsum(x, axis=self.axis)\n \n     def compute_output_spec(self, x):\n         if self.axis is None:\n@@ -878,7 +871,7 @@ class Cumsum(Operation):\n def cumsum(x, axis=None):\n     if any_symbolic_tensors((x,)):\n         return Cumsum(axis=axis).symbolic_call(x)\n-    return backend.execute(\"cumsum\", x, axis=axis)\n+    return backend.numpy.cumsum(x, axis=axis)\n \n \n class Diag(Operation):\n@@ -887,7 +880,7 @@ class Diag(Operation):\n         self.k = k\n \n     def call(self, x):\n-        return backend.execute(\"diag\", x, k=self.k)\n+        return backend.numpy.diag(x, k=self.k)\n \n     def compute_output_spec(self, x):\n         x_shape = x.shape\n@@ -921,7 +914,7 @@ class Diag(Operation):\n def diag(x, k=0):\n     if any_symbolic_tensors((x,)):\n         return Diag(k=k).symbolic_call(x)\n-    return backend.execute(\"diag\", x, k=k)\n+    return backend.numpy.diag(x, k=k)\n \n \n class Diagonal(Operation):\n@@ -932,8 +925,7 @@ class Diagonal(Operation):\n         self.axis2 = axis2\n \n     def call(self, x):\n-        return backend.execute(\n-            \"diagonal\",\n+        return backend.numpy.diagonal(\n             x,\n             offset=self.offset,\n             axis1=self.axis1,\n@@ -974,8 +966,7 @@ def diagonal(x, offset=0, axis1=0, axis2=1):\n             axis1=axis1,\n             axis2=axis2,\n         ).symbolic_call(x)\n-    return backend.execute(\n-        \"diagonal\",\n+    return backend.numpy.diagonal(\n         x,\n         offset=offset,\n         axis1=axis1,\n@@ -985,11 +976,11 @@ def diagonal(x, offset=0, axis1=0, axis2=1):\n \n class Dot(Operation):\n     def call(self, x1, x2):\n-        return backend.execute(\"dot\", x1, x2)\n+        return backend.numpy.dot(x1, x2)\n \n     def compute_output_spec(self, x1, x2):\n-        x1_shape = getattr(x1, \"shape\", [])\n-        x2_shape = getattr(x2, \"shape\", [])\n+        x1_shape = list(getattr(x1, \"shape\", []))\n+        x2_shape = list(getattr(x2, \"shape\", []))\n         if x1_shape == [] or x2_shape == []:\n             return multiply(x1, x2)\n         if len(x1_shape) == 1 and len(x2_shape) == 1:\n@@ -1003,15 +994,26 @@ class Dot(Operation):\n                 )\n             return KerasTensor(x1_shape[:-1], dtype=x1.dtype)\n \n-        if x1_shape[-1] != x2_shape[-2]:\n+        if (\n+            x1_shape[-1] is None\n+            or x2_shape[-2] is None\n+            or x1_shape[-1] == x2_shape[-2]\n+        ):\n+            del x1_shape[-1]\n+            del x2_shape[-2]\n+            return KerasTensor(x1_shape + x2_shape, dtype=x1.dtype)\n+\n         raise ValueError(\n             \"Shape must match on the last axis of `x1` and second last \"\n             \"axis of `x2` when `x1` is N-d array while `x2` is M-D, but \"\n             f\"received `x1.shape={x1.shape}` and x2.shape=`{x2.shape}`.\"\n         )\n-        del x1_shape[-1]\n-        del x2_shape[-2]\n-        return KerasTensor(x1_shape + x2_shape, dtype=x1.dtype)\n+\n+\n+def dot(x1, x2):\n+    if any_symbolic_tensors((x1, x2)):\n+        return Dot().symbolic_call(x1, x2)\n+    return backend.numpy.dot(x1, x2)\n \n \n class Einsum(Operation):\n@@ -1195,19 +1197,19 @@ def einsum(subscripts, *operands):\n \n class Empty(Operation):\n     def call(self, shape, dtype=\"float32\"):\n-        return backend.execute(\"empty\", shape, dtype=dtype)\n+        return backend.numpy.empty(shape, dtype=dtype)\n \n     def compute_output_spec(self, shape, dtype=\"float32\"):\n         return KerasTensor(shape, dtype=dtype)\n \n \n def empty(shape, dtype=\"float32\"):\n-    return backend.execute(\"empty\", shape, dtype=dtype)\n+    return backend.numpy.empty(shape, dtype=dtype)\n \n \n class Equal(Operation):\n     def call(self, x1, x2):\n-        return backend.execute(\"equal\", x1, x2)\n+        return backend.numpy.equal(x1, x2)\n \n     def compute_output_spec(self, x1, x2):\n         x1_shape = getattr(x1, \"shape\", [])\n@@ -1219,12 +1221,12 @@ class Equal(Operation):\n def equal(x1, x2):\n     if any_symbolic_tensors((x1, x2)):\n         return Equal().symbolic_call(x1, x2)\n-    return backend.execute(\"equal\", x1, x2)\n+    return backend.numpy.equal(x1, x2)\n \n \n class Exp(Operation):\n     def call(self, x):\n-        return backend.execute(\"exp\", x)\n+        return backend.numpy.exp(x)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape, dtype=x.dtype)\n@@ -1233,7 +1235,7 @@ class Exp(Operation):\n def exp(x):\n     if any_symbolic_tensors((x,)):\n         return Exp().symbolic_call(x)\n-    return backend.execute(\"exp\", x)\n+    return backend.numpy.exp(x)\n \n \n class ExpandDims(Operation):\n@@ -1247,7 +1249,7 @@ class ExpandDims(Operation):\n         self.axis = axis\n \n     def call(self, x):\n-        return backend.execute(\"expand_dims\", x, self.axis)\n+        return backend.numpy.expand_dims(x, self.axis)\n \n     def compute_output_spec(self, x):\n         x_shape = list(x.shape)\n@@ -1262,12 +1264,12 @@ class ExpandDims(Operation):\n def expand_dims(x, axis):\n     if any_symbolic_tensors((x,)):\n         return ExpandDims(axis=axis).symbolic_call(x)\n-    return backend.execute(\"expand_dims\", x, axis)\n+    return backend.numpy.expand_dims(x, axis)\n \n \n class Expm1(Operation):\n     def call(self, x):\n-        return backend.execute(\"expm1\", x)\n+        return backend.numpy.expm1(x)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape, dtype=x.dtype)\n@@ -1276,7 +1278,7 @@ class Expm1(Operation):\n def expm1(x):\n     if any_symbolic_tensors((x,)):\n         return Expm1().symbolic_call(x)\n-    return backend.execute(\"expm1\", x)\n+    return backend.numpy.expm1(x)\n \n \n class Flip(Operation):\n@@ -1285,7 +1287,7 @@ class Flip(Operation):\n         self.axis = axis\n \n     def call(self, x):\n-        return backend.execute(\"flip\", x, axis=self.axis)\n+        return backend.numpy.flip(x, axis=self.axis)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape, dtype=x.dtype)\n@@ -1294,12 +1296,12 @@ class Flip(Operation):\n def flip(x, axis=None):\n     if any_symbolic_tensors((x,)):\n         return Flip(axis=axis).symbolic_call(x)\n-    return backend.execute(\"flip\", x, axis=axis)\n+    return backend.numpy.flip(x, axis=axis)\n \n \n class Floor(Operation):\n     def call(self, x):\n-        return backend.execute(\"floor\", x)\n+        return backend.numpy.floor(x)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape, dtype=x.dtype)\n@@ -1308,24 +1310,24 @@ class Floor(Operation):\n def floor(x):\n     if any_symbolic_tensors((x,)):\n         return Floor().symbolic_call(x)\n-    return backend.execute(\"floor\", x)\n+    return backend.numpy.floor(x)\n \n \n class Full(Operation):\n     def call(self, shape, fill_value, dtype=None):\n-        return backend.execute(\"full\", shape, fill_value, dtype=dtype)\n+        return backend.numpy.full(shape, fill_value, dtype=dtype)\n \n     def compute_output_spec(self, shape, fill_value, dtype=None):\n         return KerasTensor(shape, dtype=dtype)\n \n \n def full(shape, fill_value, dtype=None):\n-    return backend.execute(\"full\", shape, fill_value, dtype=dtype)\n+    return backend.numpy.full(shape, fill_value, dtype=dtype)\n \n \n class FullLike(Operation):\n     def call(self, x, fill_value, dtype=None):\n-        return backend.execute(\"full_like\", x, fill_value, dtype=dtype)\n+        return backend.numpy.full_like(x, fill_value, dtype=dtype)\n \n     def compute_output_spec(self, x, fill_value, dtype=None):\n         return KerasTensor(x.shape, dtype=dtype)\n@@ -1334,7 +1336,7 @@ class FullLike(Operation):\n def full_like(x, fill_value, dtype=None):\n     if any_symbolic_tensors((x,)):\n         return FullLike().symbolic_call(x, fill_value, dtype=dtype)\n-    return backend.execute(\"full_like\", x, fill_value, dtype=dtype)\n+    return backend.numpy.full_like(x, fill_value, dtype=dtype)\n \n \n class GetItem(Operation):\n@@ -1382,7 +1384,7 @@ def get_item(x, key):\n \n class Greater(Operation):\n     def call(self, x1, x2):\n-        return backend.execute(\"greater\", x1, x2)\n+        return backend.numpy.greater(x1, x2)\n \n     def compute_output_spec(self, x1, x2):\n         x1_shape = getattr(x1, \"shape\", [])\n@@ -1394,12 +1396,12 @@ class Greater(Operation):\n def greater(x1, x2):\n     if any_symbolic_tensors((x1, x2)):\n         return Greater().symbolic_call(x1, x2)\n-    return backend.execute(\"greater\", x1, x2)\n+    return backend.numpy.greater(x1, x2)\n \n \n class GreaterEqual(Operation):\n     def call(self, x1, x2):\n-        return backend.execute(\"greater_equal\", x1, x2)\n+        return backend.numpy.greater_equal(x1, x2)\n \n     def compute_output_spec(self, x1, x2):\n         x1_shape = getattr(x1, \"shape\", [])\n@@ -1411,12 +1413,12 @@ class GreaterEqual(Operation):\n def greater_equal(x1, x2):\n     if any_symbolic_tensors((x1, x2)):\n         return GreaterEqual().symbolic_call(x1, x2)\n-    return backend.execute(\"greater_equal\", x1, x2)\n+    return backend.numpy.greater_equal(x1, x2)\n \n \n class Hstack(Operation):\n     def call(self, xs):\n-        return backend.execute(\"hstack\", xs)\n+        return backend.numpy.hstack(xs)\n \n     def compute_output_spec(self, xs):\n         first_shape = xs[0].shape\n@@ -1441,24 +1443,24 @@ class Hstack(Operation):\n def hstack(xs):\n     if any_symbolic_tensors((xs,)):\n         return Hstack().symbolic_call(xs)\n-    return backend.execute(\"hstack\", xs)\n+    return backend.numpy.hstack(xs)\n \n \n class Identity(Operation):\n     def call(self, n, dtype=\"float32\"):\n-        return backend.execute(\"identity\", n, dtype=dtype)\n+        return backend.numpy.identity(n, dtype=dtype)\n \n     def compute_output_spec(self, n, dtype=\"float32\"):\n         return KerasTensor([n, n], dtype=dtype)\n \n \n def identity(n, dtype=\"float32\"):\n-    return backend.execute(\"identity\", n, dtype=dtype)\n+    return backend.numpy.identity(n, dtype=dtype)\n \n \n class Imag(Operation):\n     def call(self, x):\n-        return backend.execute(\"imag\", x)\n+        return backend.numpy.imag(x)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape, dtype=x.dtype)\n@@ -1467,12 +1469,12 @@ class Imag(Operation):\n def imag(x):\n     if any_symbolic_tensors((x,)):\n         return Imag().symbolic_call(x)\n-    return backend.execute(\"imag\", x)\n+    return backend.numpy.imag(x)\n \n \n class Isclose(Operation):\n     def call(self, x1, x2):\n-        return backend.execute(\"isclose\", x1, x2)\n+        return backend.numpy.isclose(x1, x2)\n \n     def compute_output_spec(self, x1, x2):\n         x1_shape = getattr(x1, \"shape\", [])\n@@ -1484,12 +1486,12 @@ class Isclose(Operation):\n def isclose(x1, x2):\n     if any_symbolic_tensors((x1, x2)):\n         return Isclose().symbolic_call(x1, x2)\n-    return backend.execute(\"isclose\", x1, x2)\n+    return backend.numpy.isclose(x1, x2)\n \n \n class Isfinite(Operation):\n     def call(self, x):\n-        return backend.execute(\"isfinite\", x)\n+        return backend.numpy.isfinite(x)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape, dtype=\"bool\")\n@@ -1498,12 +1500,12 @@ class Isfinite(Operation):\n def isfinite(x):\n     if any_symbolic_tensors((x,)):\n         return Isfinite().symbolic_call(x)\n-    return backend.execute(\"isfinite\", x)\n+    return backend.numpy.isfinite(x)\n \n \n class Isinf(Operation):\n     def call(self, x):\n-        return backend.execute(\"isinf\", x)\n+        return backend.numpy.isinf(x)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape, dtype=\"bool\")\n@@ -1512,12 +1514,12 @@ class Isinf(Operation):\n def isinf(x):\n     if any_symbolic_tensors((x,)):\n         return Isinf().symbolic_call(x)\n-    return backend.execute(\"isinf\", x)\n+    return backend.numpy.isinf(x)\n \n \n class Isnan(Operation):\n     def call(self, x):\n-        return backend.execute(\"isnan\", x)\n+        return backend.numpy.isnan(x)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape, dtype=\"bool\")\n@@ -1526,12 +1528,12 @@ class Isnan(Operation):\n def isnan(x):\n     if any_symbolic_tensors((x,)):\n         return Isnan().symbolic_call(x)\n-    return backend.execute(\"isnan\", x)\n+    return backend.numpy.isnan(x)\n \n \n class Less(Operation):\n     def call(self, x1, x2):\n-        return backend.execute(\"less\", x1, x2)\n+        return backend.numpy.less(x1, x2)\n \n     def compute_output_spec(self, x1, x2):\n         x1_shape = getattr(x1, \"shape\", [])\n@@ -1543,12 +1545,12 @@ class Less(Operation):\n def less(x1, x2):\n     if any_symbolic_tensors((x1, x2)):\n         return Less().symbolic_call(x1, x2)\n-    return backend.execute(\"less\", x1, x2)\n+    return backend.numpy.less(x1, x2)\n \n \n class LessEqual(Operation):\n     def call(self, x1, x2):\n-        return backend.execute(\"less_equal\", x1, x2)\n+        return backend.numpy.less_equal(x1, x2)\n \n     def compute_output_spec(self, x1, x2):\n         x1_shape = getattr(x1, \"shape\", [])\n@@ -1560,7 +1562,7 @@ class LessEqual(Operation):\n def less_equal(x1, x2):\n     if any_symbolic_tensors((x1, x2)):\n         return LessEqual().symbolic_call(x1, x2)\n-    return backend.execute(\"less_equal\", x1, x2)\n+    return backend.numpy.less_equal(x1, x2)\n \n \n class Linspace(Operation):\n@@ -1575,8 +1577,7 @@ class Linspace(Operation):\n         self.axis = axis\n \n     def call(self, start, stop):\n-        return backend.execute(\n-            \"linspace\",\n+        return backend.numpy.linspace(\n             start,\n             stop,\n             num=self.num,\n@@ -1616,8 +1617,7 @@ def linspace(\n ):\n     if any_symbolic_tensors((start, stop)):\n         return Linspace(num, endpoint, retstep, dtype, axis)(start, stop)\n-    return backend.execute(\n-        \"linspace\",\n+    return backend.numpy.linspace(\n         start,\n         stop,\n         num=num,\n@@ -1630,7 +1630,7 @@ def linspace(\n \n class Log(Operation):\n     def call(self, x):\n-        return backend.execute(\"log\", x)\n+        return backend.numpy.log(x)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape, dtype=x.dtype)\n@@ -1639,12 +1639,12 @@ class Log(Operation):\n def log(x):\n     if any_symbolic_tensors((x,)):\n         return Log().symbolic_call(x)\n-    return backend.execute(\"log\", x)\n+    return backend.numpy.log(x)\n \n \n class Log10(Operation):\n     def call(self, x):\n-        return backend.execute(\"log10\", x)\n+        return backend.numpy.log10(x)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape, dtype=x.dtype)\n@@ -1653,12 +1653,12 @@ class Log10(Operation):\n def log10(x):\n     if any_symbolic_tensors((x,)):\n         return Log10().symbolic_call(x)\n-    return backend.execute(\"log10\", x)\n+    return backend.numpy.log10(x)\n \n \n class Log1p(Operation):\n     def call(self, x):\n-        return backend.execute(\"log1p\", x)\n+        return backend.numpy.log1p(x)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape, dtype=x.dtype)\n@@ -1666,13 +1666,13 @@ class Log1p(Operation):\n \n def log1p(x):\n     if any_symbolic_tensors((x,)):\n-        return Log10().symbolic_call(x)\n-    return backend.execute(\"log1p\", x)\n+        return Log1p().symbolic_call(x)\n+    return backend.numpy.log1p(x)\n \n \n class Log2(Operation):\n     def call(self, x):\n-        return backend.execute(\"log2\", x)\n+        return backend.numpy.log2(x)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape, dtype=x.dtype)\n@@ -1681,12 +1681,12 @@ class Log2(Operation):\n def log2(x):\n     if any_symbolic_tensors((x,)):\n         return Log2().symbolic_call(x)\n-    return backend.execute(\"log2\", x)\n+    return backend.numpy.log2(x)\n \n \n class Logaddexp(Operation):\n     def call(self, x1, x2):\n-        return backend.execute(\"logaddexp\", x1, x2)\n+        return backend.numpy.logaddexp(x1, x2)\n \n     def compute_output_spec(self, x1, x2):\n         x1_shape = getattr(x1, \"shape\", [])\n@@ -1698,12 +1698,12 @@ class Logaddexp(Operation):\n def logaddexp(x1, x2):\n     if any_symbolic_tensors((x1, x2)):\n         return Logaddexp().symbolic_call(x1, x2)\n-    return backend.execute(\"logaddexp\", x1, x2)\n+    return backend.numpy.logaddexp(x1, x2)\n \n \n class LogicalAnd(Operation):\n     def call(self, x1, x2):\n-        return backend.execute(\"logical_and\", x1, x2)\n+        return backend.numpy.logical_and(x1, x2)\n \n     def compute_output_spec(self, x1, x2):\n         x1_shape = getattr(x1, \"shape\", [])\n@@ -1715,12 +1715,12 @@ class LogicalAnd(Operation):\n def logical_and(x1, x2):\n     if any_symbolic_tensors((x1, x2)):\n         return LogicalAnd().symbolic_call(x1, x2)\n-    return backend.execute(\"logical_and\", x1, x2)\n+    return backend.numpy.logical_and(x1, x2)\n \n \n class LogicalNot(Operation):\n     def call(self, x):\n-        return backend.execute(\"logical_not\", x)\n+        return backend.numpy.logical_not(x)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape, dtype=x.dtype)\n@@ -1729,12 +1729,12 @@ class LogicalNot(Operation):\n def logical_not(x):\n     if any_symbolic_tensors((x,)):\n         return LogicalNot().symbolic_call(x)\n-    return backend.execute(\"logical_not\", x)\n+    return backend.numpy.logical_not(x)\n \n \n class LogicalOr(Operation):\n     def call(self, x1, x2):\n-        return backend.execute(\"logical_or\", x1, x2)\n+        return backend.numpy.logical_or(x1, x2)\n \n     def compute_output_spec(self, x1, x2):\n         x1_shape = getattr(x1, \"shape\", [])\n@@ -1746,7 +1746,7 @@ class LogicalOr(Operation):\n def logical_or(x1, x2):\n     if any_symbolic_tensors((x1, x2)):\n         return LogicalOr().symbolic_call(x1, x2)\n-    return backend.execute(\"logical_or\", x1, x2)\n+    return backend.numpy.logical_or(x1, x2)\n \n \n class Logspace(Operation):\n@@ -1759,8 +1759,7 @@ class Logspace(Operation):\n         self.axis = axis\n \n     def call(self, start, stop):\n-        return backend.execute(\n-            \"logspace\",\n+        return backend.numpy.logspace(\n             start,\n             stop,\n             num=self.num,\n@@ -1796,8 +1795,7 @@ class Logspace(Operation):\n def logspace(start, stop, num=50, endpoint=True, base=10, dtype=None, axis=0):\n     if any_symbolic_tensors((start, stop)):\n         return Logspace(num, endpoint, base, dtype, axis)(start, stop)\n-    return backend.execute(\n-        \"logspace\",\n+    return backend.numpy.logspace(\n         start,\n         stop,\n         num=num,\n@@ -1876,7 +1874,7 @@ def max(x, axis=None, keepdims=False):\n \n class Maximum(Operation):\n     def call(self, x1, x2):\n-        return backend.execute(\"maximum\", x1, x2)\n+        return backend.numpy.maximum(x1, x2)\n \n     def compute_output_spec(self, x1, x2):\n         x1_shape = getattr(x1, \"shape\", [])\n@@ -1888,7 +1886,7 @@ class Maximum(Operation):\n def maximum(x1, x2):\n     if any_symbolic_tensors((x1, x2)):\n         return Maximum().symbolic_call(x1, x2)\n-    return backend.execute(\"maximum\", x1, x2)\n+    return backend.numpy.maximum(x1, x2)\n \n \n class Meshgrid(Operation):\n@@ -1902,7 +1900,7 @@ class Meshgrid(Operation):\n         self.indexing = indexing\n \n     def call(self, *x):\n-        return backend.execute(\"meshgrid\", *x, indexing=self.indexing)\n+        return backend.numpy.meshgrid(*x, indexing=self.indexing)\n \n     def compute_output_spec(self, *x):\n         output_shape = []\n@@ -1926,7 +1924,7 @@ class Meshgrid(Operation):\n def meshgrid(*x, indexing=\"xy\"):\n     if any_symbolic_tensors(x):\n         return Meshgrid(indexing=indexing).symbolic_call(*x)\n-    return backend.execute(\"meshgrid\", *x, indexing=indexing)\n+    return backend.numpy.meshgrid(*x, indexing=indexing)\n \n \n class Min(Operation):\n@@ -1938,7 +1936,7 @@ class Min(Operation):\n         self.keepdims = keepdims\n \n     def call(self, x):\n-        return backend.execute(\"min\", x, axis=self.axis, keepdims=self.keepdims)\n+        return backend.numpy.min(x, axis=self.axis, keepdims=self.keepdims)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(\n@@ -1950,12 +1948,12 @@ class Min(Operation):\n def min(x, axis=None, keepdims=False):\n     if any_symbolic_tensors((x,)):\n         return Min(axis=axis, keepdims=keepdims).symbolic_call(x)\n-    return backend.execute(\"min\", x, axis=axis, keepdims=keepdims)\n+    return backend.numpy.min(x, axis=axis, keepdims=keepdims)\n \n \n class Minimum(Operation):\n     def call(self, x1, x2):\n-        return backend.execute(\"minimum\", x1, x2)\n+        return backend.numpy.minimum(x1, x2)\n \n     def compute_output_spec(self, x1, x2):\n         x1_shape = getattr(x1, \"shape\", [])\n@@ -1967,12 +1965,12 @@ class Minimum(Operation):\n def minimum(x1, x2):\n     if any_symbolic_tensors((x1, x2)):\n         return Minimum().symbolic_call(x1, x2)\n-    return backend.execute(\"minimum\", x1, x2)\n+    return backend.numpy.minimum(x1, x2)\n \n \n class Mod(Operation):\n     def call(self, x1, x2):\n-        return backend.execute(\"mod\", x1, x2)\n+        return backend.numpy.mod(x1, x2)\n \n     def compute_output_spec(self, x1, x2):\n         x1_shape = getattr(x1, \"shape\", [])\n@@ -1984,7 +1982,7 @@ class Mod(Operation):\n def mod(x1, x2):\n     if any_symbolic_tensors((x1, x2)):\n         return Mod().symbolic_call(x1, x2)\n-    return backend.execute(\"mod\", x1, x2)\n+    return backend.numpy.mod(x1, x2)\n \n \n class Moveaxis(Operation):\n@@ -2007,7 +2005,7 @@ class Moveaxis(Operation):\n             )\n \n     def call(self, x):\n-        return backend.execute(\"moveaxis\", x, self.source, self.destination)\n+        return backend.numpy.moveaxis(x, self.source, self.destination)\n \n     def compute_output_spec(self, x):\n         x_shape = list(x.shape)\n@@ -2034,15 +2032,12 @@ class Moveaxis(Operation):\n def moveaxis(x, source, destination):\n     if any_symbolic_tensors((x,)):\n         return Moveaxis(source, destination).symbolic_call(x)\n-    return backend.execute(\n-        \"moveaxis\", x, source=source, destination=destination\n-    )\n+    return backend.numpy.moveaxis(x, source=source, destination=destination)\n \n \n class Ndim(Operation):\n     def call(self, x):\n-        return backend.execute(\n-            \"ndim\",\n+        return backend.numpy.ndim(\n             x,\n         )\n \n@@ -2053,21 +2048,21 @@ class Ndim(Operation):\n def ndim(x):\n     if any_symbolic_tensors((x,)):\n         return Ndim().symbolic_call(x)\n-    return backend.execute(\"ndim\", x)\n+    return backend.numpy.ndim(x)\n \n \n class Nonzero(Operation):\n     def call(self, x):\n-        return backend.execute(\"nonzero\", x)\n+        return backend.numpy.nonzero(x)\n \n \n def nonzero(x):\n-    return backend.execute(\"nonzero\", x)\n+    return backend.numpy.nonzero(x)\n \n \n class NotEqual(Operation):\n     def call(self, x1, x2):\n-        return backend.execute(\"not_equal\", x1, x2)\n+        return backend.numpy.not_equal(x1, x2)\n \n     def compute_output_spec(self, x1, x2):\n         x1_shape = getattr(x1, \"shape\", [])\n@@ -2079,12 +2074,12 @@ class NotEqual(Operation):\n def not_equal(x1, x2):\n     if any_symbolic_tensors((x1, x2)):\n         return NotEqual().symbolic_call(x1, x2)\n-    return backend.execute(\"not_equal\", x1, x2)\n+    return backend.numpy.not_equal(x1, x2)\n \n \n class OnesLike(Operation):\n     def call(self, x, dtype=None):\n-        return backend.execute(\"ones_like\", x, dtype=dtype)\n+        return backend.numpy.ones_like(x, dtype=dtype)\n \n     def compute_output_spec(self, x, dtype=None):\n         return KerasTensor(x.shape, dtype=dtype)\n@@ -2093,12 +2088,12 @@ class OnesLike(Operation):\n def ones_like(x, dtype=None):\n     if any_symbolic_tensors((x,)):\n         return OnesLike().symbolic_call(x, dtype=dtype)\n-    return backend.execute(\"ones_like\", x, dtype=dtype)\n+    return backend.numpy.ones_like(x, dtype=dtype)\n \n \n class Outer(Operation):\n     def call(self, x1, x2):\n-        return backend.execute(\"outer\", x1, x2)\n+        return backend.numpy.outer(x1, x2)\n \n     def compute_output_spec(self, x1, x2):\n         x1_shape = getattr(x1, \"shape\", [1])\n@@ -2118,7 +2113,7 @@ class Outer(Operation):\n def outer(x1, x2):\n     if any_symbolic_tensors((x1, x2)):\n         return Outer().symbolic_call(x1, x2)\n-    return backend.execute(\"outer\", x1, x2)\n+    return backend.numpy.outer(x1, x2)\n \n \n class Pad(Operation):\n@@ -2146,9 +2141,7 @@ class Pad(Operation):\n         return pad_width\n \n     def call(self, x):\n-        return backend.execute(\n-            \"pad\", x, pad_width=self.pad_width, mode=self.mode\n-        )\n+        return backend.numpy.pad(x, pad_width=self.pad_width, mode=self.mode)\n \n     def compute_output_spec(self, x):\n         output_shape = list(x.shape)\n@@ -2173,7 +2166,7 @@ class Pad(Operation):\n def pad(x, pad_width, mode=\"constant\"):\n     if any_symbolic_tensors((x,)):\n         return Pad(pad_width, mode=mode).symbolic_call(x)\n-    return backend.execute(\"pad\", x, pad_width, mode=mode)\n+    return backend.numpy.pad(x, pad_width, mode=mode)\n \n \n class Prod(Operation):\n@@ -2187,8 +2180,7 @@ class Prod(Operation):\n         self.dtype = dtype\n \n     def call(self, x):\n-        return backend.execute(\n-            \"prod\",\n+        return backend.numpy.prod(\n             x,\n             axis=self.axis,\n             keepdims=self.keepdims,\n@@ -2205,12 +2197,12 @@ class Prod(Operation):\n def prod(x, axis=None, keepdims=False, dtype=None):\n     if any_symbolic_tensors((x,)):\n         return Prod(axis=axis, keepdims=keepdims, dtype=dtype).symbolic_call(x)\n-    return backend.execute(\"prod\", x, axis=axis, keepdims=keepdims, dtype=dtype)\n+    return backend.numpy.prod(x, axis=axis, keepdims=keepdims, dtype=dtype)\n \n \n class Ravel(Operation):\n     def call(self, x):\n-        return backend.execute(\"ravel\", x)\n+        return backend.numpy.ravel(x)\n \n     def compute_output_spec(self, x):\n         if None in x.shape:\n@@ -2225,12 +2217,12 @@ class Ravel(Operation):\n def ravel(x):\n     if any_symbolic_tensors((x,)):\n         return Ravel().symbolic_call(x)\n-    return backend.execute(\"ravel\", x)\n+    return backend.numpy.ravel(x)\n \n \n class Real(Operation):\n     def call(self, x):\n-        return backend.execute(\"real\", x)\n+        return backend.numpy.real(x)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape)\n@@ -2239,12 +2231,12 @@ class Real(Operation):\n def real(x):\n     if any_symbolic_tensors((x,)):\n         return Real().symbolic_call(x)\n-    return backend.execute(\"real\", x)\n+    return backend.numpy.real(x)\n \n \n class Reciprocal(Operation):\n     def call(self, x):\n-        return backend.execute(\"reciprocal\", x)\n+        return backend.numpy.reciprocal(x)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape)\n@@ -2253,7 +2245,7 @@ class Reciprocal(Operation):\n def reciprocal(x):\n     if any_symbolic_tensors((x,)):\n         return Reciprocal().symbolic_call(x)\n-    return backend.execute(\"reciprocal\", x)\n+    return backend.numpy.reciprocal(x)\n \n \n class Repeat(Operation):\n@@ -2263,7 +2255,7 @@ class Repeat(Operation):\n         self.repeats = repeats\n \n     def call(self, x):\n-        return backend.execute(\"repeat\", x, self.repeats, axis=self.axis)\n+        return backend.numpy.repeat(x, self.repeats, axis=self.axis)\n \n     def compute_output_spec(self, x):\n         x_shape = list(x.shape)\n@@ -2290,7 +2282,7 @@ class Repeat(Operation):\n def repeat(x, repeats, axis=None):\n     if any_symbolic_tensors((x,)):\n         return Repeat(repeats, axis=axis).symbolic_call(x)\n-    return backend.execute(\"repeat\", x, repeats, axis=axis)\n+    return backend.numpy.repeat(x, repeats, axis=axis)\n \n \n class Reshape(Operation):\n@@ -2299,7 +2291,7 @@ class Reshape(Operation):\n         self.new_shape = new_shape\n \n     def call(self, x):\n-        return backend.execute(\"reshape\", x, self.new_shape)\n+        return backend.numpy.reshape(x, self.new_shape)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(self.new_shape, dtype=x.dtype)\n@@ -2308,7 +2300,7 @@ class Reshape(Operation):\n def reshape(x, new_shape):\n     if any_symbolic_tensors((x,)):\n         return Reshape(new_shape).symbolic_call(x)\n-    return backend.execute(\"reshape\", x, new_shape)\n+    return backend.numpy.reshape(x, new_shape)\n \n \n class Roll(Operation):\n@@ -2318,7 +2310,7 @@ class Roll(Operation):\n         self.axis = axis\n \n     def call(self, x):\n-        return backend.execute(\"roll\", x, self.shift, self.axis)\n+        return backend.numpy.roll(x, self.shift, self.axis)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape, dtype=x.dtype)\n@@ -2327,24 +2319,30 @@ class Roll(Operation):\n def roll(x, shift, axis=None):\n     if any_symbolic_tensors((x,)):\n         return Roll(shift, axis=axis).symbolic_call(x)\n-    return backend.execute(\"roll\", x, shift, axis=axis)\n+    return backend.numpy.roll(x, shift, axis=axis)\n \n \n class Round(Operation):\n-    def __init__(self, decimal=0):\n+    def __init__(self, decimals=0):\n         super().__init__()\n-        self.decimal = decimal\n+        self.decimals = decimals\n \n     def call(self, x):\n-        return backend.execute(\"round\", x, self.decimal)\n+        return backend.numpy.round(x, self.decimals)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n+def round(x, decimals=0):\n+    if any_symbolic_tensors((x,)):\n+        return Round(decimals).symbolic_call(x)\n+    return backend.numpy.round(x, decimals)\n+\n+\n class Sign(Operation):\n     def call(self, x):\n-        return backend.execute(\"sign\", x)\n+        return backend.numpy.sign(x)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape, dtype=\"int32\")\n@@ -2353,12 +2351,12 @@ class Sign(Operation):\n def sign(x):\n     if any_symbolic_tensors((x,)):\n         return Sign().symbolic_call(x)\n-    return backend.execute(\"sign\", x)\n+    return backend.numpy.sign(x)\n \n \n class Sin(Operation):\n     def call(self, x):\n-        return backend.execute(\"sin\", x)\n+        return backend.numpy.sin(x)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape)\n@@ -2367,12 +2365,12 @@ class Sin(Operation):\n def sin(x):\n     if any_symbolic_tensors((x,)):\n         return Sin().symbolic_call(x)\n-    return backend.execute(\"sin\", x)\n+    return backend.numpy.sin(x)\n \n \n class Size(Operation):\n     def call(self, x):\n-        return backend.execute(\"size\", x)\n+        return backend.numpy.size(x)\n \n     def compute_output_spec(self, x):\n         return KerasTensor([], dtype=\"int32\")\n@@ -2381,7 +2379,7 @@ class Size(Operation):\n def size(x):\n     if any_symbolic_tensors((x,)):\n         return Size().symbolic_call(x)\n-    return backend.execute(\"size\", x)\n+    return backend.numpy.size(x)\n \n \n class Sort(Operation):\n@@ -2390,7 +2388,7 @@ class Sort(Operation):\n         self.axis = axis\n \n     def call(self, x):\n-        return backend.execute(\"sort\", x, axis=self.axis)\n+        return backend.numpy.sort(x, axis=self.axis)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape, x.dtype)\n@@ -2399,7 +2397,7 @@ class Sort(Operation):\n def sort(x, axis=-1):\n     if any_symbolic_tensors((x,)):\n         return Sort(axis=axis).symbolic_call(x)\n-    return backend.execute(\"sort\", x, axis=axis)\n+    return backend.numpy.sort(x, axis=axis)\n \n \n class Split(Operation):\n@@ -2409,9 +2407,7 @@ class Split(Operation):\n         self.axis = axis\n \n     def call(self, x):\n-        return backend.execute(\n-            \"split\", x, self.indices_or_sections, axis=self.axis\n-        )\n+        return backend.numpy.split(x, self.indices_or_sections, axis=self.axis)\n \n     def compute_output_spec(self, x):\n         x_shape = list(x.shape)\n@@ -2450,7 +2446,7 @@ class Split(Operation):\n def split(x, indices_or_sections, axis=0):\n     if any_symbolic_tensors((x,)):\n         return Split(indices_or_sections, axis=axis).symbolic_call(x)\n-    return backend.execute(\"split\", x, indices_or_sections, axis=axis)\n+    return backend.numpy.split(x, indices_or_sections, axis=axis)\n \n \n class Stack(Operation):\n@@ -2459,7 +2455,7 @@ class Stack(Operation):\n         self.axis = axis\n \n     def call(self, xs):\n-        return backend.execute(\"stack\", xs, axis=self.axis)\n+        return backend.numpy.stack(xs, axis=self.axis)\n \n     def compute_output_spec(self, xs):\n         first_shape = xs[0].shape\n@@ -2485,7 +2481,7 @@ class Stack(Operation):\n def stack(x, axis=0):\n     if any_symbolic_tensors((x,)):\n         return Stack(axis=axis).symbolic_call(x)\n-    return backend.execute(\"stack\", x, axis=axis)\n+    return backend.numpy.stack(x, axis=axis)\n \n \n class Std(Operation):\n@@ -2498,7 +2494,7 @@ class Std(Operation):\n         self.keepdims = keepdims\n \n     def call(self, x):\n-        return backend.execute(\"std\", x, axis=self.axis, keepdims=self.keepdims)\n+        return backend.numpy.std(x, axis=self.axis, keepdims=self.keepdims)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(\n@@ -2509,7 +2505,7 @@ class Std(Operation):\n def std(x, axis=None, keepdims=False):\n     if any_symbolic_tensors((x,)):\n         return Std(axis=axis, keepdims=keepdims).symbolic_call(x)\n-    return backend.execute(\"std\", x, axis=axis, keepdims=keepdims)\n+    return backend.numpy.std(x, axis=axis, keepdims=keepdims)\n \n \n class Swapaxes(Operation):\n@@ -2520,7 +2516,7 @@ class Swapaxes(Operation):\n         self.axis2 = axis2\n \n     def call(self, x):\n-        return backend.execute(\"swapaxes\", x, self.axis1, self.axis2)\n+        return backend.numpy.swapaxes(x, self.axis1, self.axis2)\n \n     def compute_output_spec(self, x):\n         x_shape = list(x.shape)\n@@ -2533,7 +2529,7 @@ class Swapaxes(Operation):\n def swapaxes(x, axis1, axis2):\n     if any_symbolic_tensors((x,)):\n         return Swapaxes(axis1, axis2).symbolic_call(x)\n-    return backend.execute(\"swapaxes\", x, axis1=axis1, axis2=axis2)\n+    return backend.numpy.swapaxes(x, axis1=axis1, axis2=axis2)\n \n \n class Take(Operation):\n@@ -2542,7 +2538,7 @@ class Take(Operation):\n         self.axis = axis\n \n     def call(self, x, indices):\n-        return backend.execute(\"take\", x, indices, axis=self.axis)\n+        return backend.numpy.take(x, indices, axis=self.axis)\n \n     def compute_output_spec(self, x, indices):\n         x_shape = list(x.shape)\n@@ -2562,7 +2558,7 @@ class Take(Operation):\n def take(x, indices, axis=None):\n     if any_symbolic_tensors((x, indices)):\n         return Take(axis=axis).symbolic_call(x, indices)\n-    return backend.execute(\"take\", x, indices, axis=axis)\n+    return backend.numpy.take(x, indices, axis=axis)\n \n \n class TakeAlongAxis(Operation):\n@@ -2571,7 +2567,7 @@ class TakeAlongAxis(Operation):\n         self.axis = axis\n \n     def call(self, x, indices):\n-        return backend.execute(\"take_along_axis\", x, indices, axis=self.axis)\n+        return backend.numpy.take_along_axis(x, indices, axis=self.axis)\n \n     def compute_output_spec(self, x, indices):\n         x_shape = list(x.shape)\n@@ -2602,12 +2598,12 @@ class TakeAlongAxis(Operation):\n def take_along_axis(x, indices, axis=None):\n     if any_symbolic_tensors((x, indices)):\n         return TakeAlongAxis(axis=axis).symbolic_call(x, indices)\n-    return backend.execute(\"take_along_axis\", x, indices, axis=axis)\n+    return backend.numpy.take_along_axis(x, indices, axis=axis)\n \n \n class Tan(Operation):\n     def call(self, x):\n-        return backend.execute(\"tan\", x)\n+        return backend.numpy.tan(x)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape)\n@@ -2616,7 +2612,7 @@ class Tan(Operation):\n def tan(x):\n     if any_symbolic_tensors((x,)):\n         return Tan().symbolic_call(x)\n-    return backend.execute(\"tan\", x)\n+    return backend.numpy.tan(x)\n \n \n class Tensordot(Operation):\n@@ -2625,7 +2621,7 @@ class Tensordot(Operation):\n         self.axes = axes\n \n     def call(self, x1, x2):\n-        return backend.execute(\"tensordot\", x1, x2, axes=self.axes)\n+        return backend.numpy.tensordot(x1, x2, axes=self.axes)\n \n     def compute_output_spec(self, x1, x2):\n         x1_shape = list(getattr(x1, \"shape\", []))\n@@ -2663,13 +2659,7 @@ class Tensordot(Operation):\n def tensordot(x1, x2, axes=2):\n     if any_symbolic_tensors((x1, x2)):\n         return Tensordot(axes=axes).symbolic_call(x1, x2)\n-    return backend.execute(\"tensordot\", x1, x2, axes=axes)\n-\n-\n-def round(x):\n-    if any_symbolic_tensors((x,)):\n-        return Round().symbolic_call(x)\n-    return backend.execute(\"round\", x)\n+    return backend.numpy.tensordot(x1, x2, axes=axes)\n \n \n class Tile(Operation):\n@@ -2678,7 +2668,7 @@ class Tile(Operation):\n         self.repeats = repeats\n \n     def call(self, x):\n-        return backend.execute(\"tile\", x, self.repeats)\n+        return backend.numpy.tile(x, self.repeats)\n \n     def compute_output_spec(self, x):\n         x_shape = list(x.shape)\n@@ -2702,7 +2692,7 @@ def tile(x, repeats):\n         return Tile(\n             repeats,\n         ).symbolic_call(x)\n-    return backend.execute(\"tile\", x, repeats)\n+    return backend.numpy.tile(x, repeats)\n \n \n class Trace(Operation):\n@@ -2713,8 +2703,8 @@ class Trace(Operation):\n         self.axis2 = axis2\n \n     def call(self, x):\n-        return backend.execute(\n-            \"trace\", x, offset=self.offset, axis1=self.axis1, axis2=self.axis2\n+        return backend.numpy.trace(\n+            x, offset=self.offset, axis1=self.axis1, axis2=self.axis2\n         )\n \n     def compute_output_spec(self, x):\n@@ -2728,12 +2718,12 @@ class Trace(Operation):\n def trace(x, offset=0, axis1=0, axis2=1):\n     if any_symbolic_tensors((x,)):\n         return Trace(offset, axis1, axis2).symbolic_call(x)\n-    return backend.execute(\"trace\", x, offset=offset, axis1=axis1, axis2=axis2)\n+    return backend.numpy.trace(x, offset=offset, axis1=axis1, axis2=axis2)\n \n \n class Tri(Operation):\n     def call(self, N, M=None, k=0, dtype=\"float32\"):\n-        return backend.execute(\"tri\", N, M=M, k=k, dtype=dtype)\n+        return backend.numpy.tri(N, M=M, k=k, dtype=dtype)\n \n     def compute_output_spec(self, N, M=None, k=0, dtype=\"float32\"):\n         if M is None:\n@@ -2742,7 +2732,7 @@ class Tri(Operation):\n \n \n def tri(N, M=None, k=0, dtype=\"float32\"):\n-    return backend.execute(\"tri\", N, M=M, k=k, dtype=dtype)\n+    return backend.numpy.tri(N, M=M, k=k, dtype=dtype)\n \n \n class Tril(Operation):\n@@ -2751,7 +2741,7 @@ class Tril(Operation):\n         self.k = k\n \n     def call(self, x):\n-        return backend.execute(\"tril\", x, k=self.k)\n+        return backend.numpy.tril(x, k=self.k)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape, dtype=x.dtype)\n@@ -2760,7 +2750,7 @@ class Tril(Operation):\n def tril(x, k=0):\n     if any_symbolic_tensors((x,)):\n         return Tril(k=k).symbolic_call(x)\n-    return backend.execute(\"tril\", x, k=k)\n+    return backend.numpy.tril(x, k=k)\n \n \n class Triu(Operation):\n@@ -2769,7 +2759,7 @@ class Triu(Operation):\n         self.k = k\n \n     def call(self, x):\n-        return backend.execute(\"triu\", x, k=self.k)\n+        return backend.numpy.triu(x, k=self.k)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape, dtype=x.dtype)\n@@ -2778,12 +2768,12 @@ class Triu(Operation):\n def triu(x, k=0):\n     if any_symbolic_tensors((x,)):\n         return Triu(k=k).symbolic_call(x)\n-    return backend.execute(\"triu\", x, k=k)\n+    return backend.numpy.triu(x, k=k)\n \n \n class Vdot(Operation):\n     def call(self, x1, x2):\n-        return backend.execute(\"vdot\", x1, x2)\n+        return backend.numpy.vdot(x1, x2)\n \n     def compute_output_spec(self, x1, x2):\n         return KerasTensor([], dtype=x1.dtype)\n@@ -2792,12 +2782,12 @@ class Vdot(Operation):\n def vdot(x1, x2):\n     if any_symbolic_tensors((x1, x2)):\n         return Vdot().symbolic_call(x1, x2)\n-    return backend.execute(\"vdot\", x1, x2)\n+    return backend.numpy.vdot(x1, x2)\n \n \n class Vstack(Operation):\n     def call(self, xs):\n-        return backend.execute(\"vstack\", xs)\n+        return backend.numpy.vstack(xs)\n \n     def compute_output_spec(self, xs):\n         first_shape = xs[0].shape\n@@ -2822,12 +2812,12 @@ class Vstack(Operation):\n def vstack(xs):\n     if any_symbolic_tensors((xs,)):\n         return Vstack().symbolic_call(xs)\n-    return backend.execute(\"vstack\", xs)\n+    return backend.numpy.vstack(xs)\n \n \n class Where(Operation):\n     def call(self, condition, x1, x2):\n-        return backend.execute(\"where\", condition, x1, x2)\n+        return backend.numpy.where(condition, x1, x2)\n \n     def compute_output_spec(self, condition, x1, x2):\n         condition_shape = getattr(condition, \"shape\", [])\n@@ -2841,7 +2831,7 @@ class Where(Operation):\n def where(condition, x1, x2):\n     if any_symbolic_tensors((condition, x1, x2)):\n         return Where().symbolic_call(condition, x1, x2)\n-    return backend.execute(\"where\", condition, x1, x2)\n+    return backend.numpy.where(condition, x1, x2)\n \n \n class Subtract(Operation):\n@@ -2880,7 +2870,7 @@ def multiply(x1, x2):\n \n class Divide(Operation):\n     def call(self, x1, x2):\n-        return backend.execute(\"divide\", x1, x2)\n+        return backend.numpy.divide(x1, x2)\n \n     def compute_output_spec(self, x1, x2):\n         x1_shape = getattr(x1, \"shape\", [])\n@@ -2892,12 +2882,12 @@ class Divide(Operation):\n def divide(x1, x2):\n     if any_symbolic_tensors((x1, x2)):\n         return Divide().symbolic_call(x1, x2)\n-    return backend.execute(\"divide\", x1, x2)\n+    return backend.numpy.divide(x1, x2)\n \n \n class TrueDivide(Operation):\n     def call(self, x1, x2):\n-        return backend.execute(\"true_divide\", x1, x2)\n+        return backend.numpy.true_divide(x1, x2)\n \n     def compute_output_spec(self, x1, x2):\n         x1_shape = getattr(x1, \"shape\", [])\n@@ -2909,12 +2899,12 @@ class TrueDivide(Operation):\n def true_divide(x1, x2):\n     if any_symbolic_tensors((x1, x2)):\n         return TrueDivide().symbolic_call(x1, x2)\n-    return backend.execute(\"true_divide\", x1, x2)\n+    return backend.numpy.true_divide(x1, x2)\n \n \n class Power(Operation):\n     def call(self, x1, x2):\n-        return backend.execute(\"power\", x1, x2)\n+        return backend.numpy.power(x1, x2)\n \n     def compute_output_spec(self, x1, x2):\n         x1_shape = getattr(x1, \"shape\", [])\n@@ -2926,12 +2916,12 @@ class Power(Operation):\n def power(x1, x2):\n     if any_symbolic_tensors((x1, x2)):\n         return Power().symbolic_call(x1, x2)\n-    return backend.execute(\"power\", x1, x2)\n+    return backend.numpy.power(x1, x2)\n \n \n class Negative(Operation):\n     def call(self, x):\n-        return backend.execute(\"negative\", x)\n+        return backend.numpy.negative(x)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape, dtype=x.dtype)\n@@ -2940,12 +2930,12 @@ class Negative(Operation):\n def negative(x):\n     if any_symbolic_tensors((x,)):\n         return Negative().symbolic_call(x)\n-    return backend.execute(\"negative\", x)\n+    return backend.numpy.negative(x)\n \n \n class Square(Operation):\n     def call(self, x):\n-        return backend.execute(\"square\", x)\n+        return backend.numpy.square(x)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape, dtype=x.dtype)\n@@ -2954,13 +2944,13 @@ class Square(Operation):\n def square(x):\n     if any_symbolic_tensors((x,)):\n         return Square().symbolic_call(x)\n-    return backend.execute(\"square\", x)\n+    return backend.numpy.square(x)\n \n \n class Sqrt(Operation):\n     def call(self, x):\n         x = backend.convert_to_tensor(x)\n-        return backend.execute(\"sqrt\", x)\n+        return backend.numpy.sqrt(x)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape, dtype=x.dtype)\n@@ -2970,7 +2960,7 @@ def sqrt(x):\n     if any_symbolic_tensors((x,)):\n         return Sqrt().symbolic_call(x)\n     x = backend.convert_to_tensor(x)\n-    return backend.execute(\"sqrt\", x)\n+    return backend.numpy.sqrt(x)\n \n \n class Squeeze(Operation):\n@@ -2979,7 +2969,7 @@ class Squeeze(Operation):\n         self.axis = axis\n \n     def call(self, x):\n-        return backend.execute(\"squeeze\", x, axis=self.axis)\n+        return backend.numpy.squeeze(x, axis=self.axis)\n \n     def compute_output_spec(self, x, axis=None):\n         input_shape = list(x.shape)\n@@ -2999,7 +2989,7 @@ class Squeeze(Operation):\n def squeeze(x, axis=None):\n     if any_symbolic_tensors((x,)):\n         return Squeeze().symbolic_call(x, axis=axis)\n-    return backend.execute(\"squeeze\", x, axis=axis)\n+    return backend.numpy.squeeze(x, axis=axis)\n \n \n class Transpose(Operation):\n@@ -3008,7 +2998,7 @@ class Transpose(Operation):\n         self.axes = axes\n \n     def call(self, x):\n-        return backend.execute(\"transpose\", x, axes=self.axes)\n+        return backend.numpy.transpose(x, axes=self.axes)\n \n     def compute_output_spec(self, x):\n         x_shape = x.shape\n@@ -3029,7 +3019,7 @@ class Transpose(Operation):\n def transpose(x, axes=None):\n     if any_symbolic_tensors((x,)):\n         return Transpose(axes=axes).symbolic_call(x)\n-    return backend.execute(\"transpose\", x, axes=axes)\n+    return backend.numpy.transpose(x, axes=axes)\n \n \n class Mean(Operation):\n@@ -3065,7 +3055,7 @@ class Var(Operation):\n         self.keepdims = keepdims\n \n     def call(self, x):\n-        return backend.execute(\"var\", x, axis=self.axis, keepdims=self.keepdims)\n+        return backend.numpy.var(x, axis=self.axis, keepdims=self.keepdims)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(\n@@ -3077,7 +3067,7 @@ class Var(Operation):\n def var(x, axis=None, keepdims=False):\n     if any_symbolic_tensors((x,)):\n         return Var(axis=axis, keepdims=keepdims).symbolic_call(x)\n-    return backend.execute(\"var\", x, axis=axis, keepdims=keepdims)\n+    return backend.numpy.var(x, axis=axis, keepdims=keepdims)\n \n \n class Sum(Operation):\n@@ -3089,7 +3079,7 @@ class Sum(Operation):\n         self.keepdims = keepdims\n \n     def call(self, x):\n-        return backend.execute(\"sum\", x, axis=self.axis, keepdims=self.keepdims)\n+        return backend.numpy.sum(x, axis=self.axis, keepdims=self.keepdims)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(\n@@ -3101,7 +3091,7 @@ class Sum(Operation):\n def sum(x, axis=None, keepdims=False):\n     if any_symbolic_tensors((x,)):\n         return Sum(axis=axis, keepdims=keepdims).symbolic_call(x)\n-    return backend.execute(\"sum\", x, axis=axis, keepdims=keepdims)\n+    return backend.numpy.sum(x, axis=axis, keepdims=keepdims)\n \n \n class Zeros(Operation):\n@@ -3130,7 +3120,7 @@ def ones(shape, dtype=\"float32\"):\n \n class Eye(Operation):\n     def call(self, N, M=None, k=0, dtype=\"float32\"):\n-        return backend.execute(\"eye\", N, M=M, k=k, dtype=dtype)\n+        return backend.numpy.eye(N, M=M, k=k, dtype=dtype)\n \n     def compute_output_spec(self, N, M=None, k=0, dtype=\"float32\"):\n         if M is None:\n@@ -3139,4 +3129,4 @@ class Eye(Operation):\n \n \n def eye(N, M=None, k=0, dtype=\"float32\"):\n-    return backend.execute(\"eye\", N, M=M, k=k, dtype=dtype)\n+    return backend.numpy.eye(N, M=M, k=k, dtype=dtype)\n\n@@ -847,6 +847,18 @@ class NumpyOneInputOpsDynamicShapeTest(testing.TestCase):\n         x = KerasTensor([None, 3, 3])\n         self.assertEqual(knp.diagonal(x).shape, (3, None))\n \n+    def test_dot(self):\n+        x = KerasTensor([None, 3])\n+        y = KerasTensor([3, 2])\n+        z = KerasTensor([None, None, 2])\n+        self.assertEqual(knp.dot(x, y).shape, (None, 2))\n+        self.assertEqual(knp.dot(x, 2).shape, (None, 3))\n+        self.assertEqual(knp.dot(x, z).shape, (None, None, 2))\n+\n+        x = KerasTensor([None])\n+        y = KerasTensor([5])\n+        self.assertEqual(knp.dot(x, y).shape, ())\n+\n     def test_exp(self):\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.exp(x).shape, (None, 3))\n@@ -1252,6 +1264,23 @@ class NumpyOneInputOpsStaticShapeTest(testing.TestCase):\n             x = KerasTensor([3])\n             knp.diagonal(x)\n \n+    def test_dot(self):\n+        x = KerasTensor([2, 3])\n+        y = KerasTensor([3, 2])\n+        z = KerasTensor([4, 3, 2])\n+        self.assertEqual(knp.dot(x, y).shape, (2, 2))\n+        self.assertEqual(knp.dot(x, 2).shape, (2, 3))\n+        self.assertEqual(knp.dot(x, z).shape, (2, 4, 2))\n+\n+        x = KerasTensor([5])\n+        y = KerasTensor([5])\n+        self.assertEqual(knp.dot(x, y).shape, ())\n+\n+        with self.assertRaises(ValueError):\n+            x = KerasTensor([2, 3])\n+            y = KerasTensor([2, 3])\n+            knp.dot(x, y)\n+\n     def test_exp(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.exp(x).shape, (2, 3))\n@@ -2446,6 +2475,18 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n             np.diagonal(x, offset=-1, axis1=2, axis2=3),\n         )\n \n+    def test_dot(self):\n+        x = np.arange(24).reshape([2, 3, 4])\n+        y = np.arange(12).reshape([4, 3])\n+        z = np.arange(4)\n+        self.assertAllClose(np.array(knp.dot(x, y)), np.dot(x, y))\n+        self.assertAllClose(np.array(knp.dot(x, z)), np.dot(x, z))\n+        self.assertAllClose(np.array(knp.dot(x, 2)), np.dot(x, 2))\n+\n+        self.assertAllClose(np.array(knp.Dot()(x, y)), np.dot(x, y))\n+        self.assertAllClose(np.array(knp.Dot()(x, z)), np.dot(x, z))\n+        self.assertAllClose(np.array(knp.Dot()(x, 2)), np.dot(x, 2))\n+\n     def test_exp(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n         self.assertAllClose(np.array(knp.exp(x)), np.exp(x))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
