{"custom_id": "keras#af2b59ac3cb080f937598dab15545ec2459724eb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 12 | Churn Cumulative: 189 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,6 +3,7 @@ import theano\n import theano.tensor as T\n import numpy as np\n import warnings\n+import time\n \n from .utils.generic_utils import Progbar\n \n@@ -25,18 +26,29 @@ class CallbackList(object):\n     def on_epoch_begin(self, epoch):\n         for callback in self.callbacks:\n             callback.on_epoch_begin(epoch)\n+        self._delta_t_batch = 0.\n \n     def on_epoch_end(self, epoch, val_loss, val_acc):\n         for callback in self.callbacks:\n             callback.on_epoch_end(epoch, val_loss, val_acc)\n \n     def on_batch_begin(self, batch):\n+        t_before_callbacks = time.time()\n         for callback in self.callbacks:\n             callback.on_batch_begin(batch)\n+        delta_t_callbacks = time.time() - t_before_callbacks\n+        if self._delta_t_batch > 0. and delta_t_callbacks > 2 * self._delta_t_batch:\n+            warnings.warn('Warning!!')\n+        self._t_enter_batch = time.time()\n \n     def on_batch_end(self, batch, indices, loss, accuracy):\n+        self._delta_t_batch = time.time() - self._t_enter_batch\n+        t_before_callbacks = time.time()\n         for callback in self.callbacks:\n             callback.on_batch_end(batch, indices, loss, accuracy)\n+        delta_t_callbacks = time.time() - t_before_callbacks\n+        if self._delta_t_batch > 0. and delta_t_callbacks > 2 * self._delta_t_batch:\n+            warnings.warn('Warning!!')\n \n     def on_train_begin(self):\n         for callback in self.callbacks:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2b1bb90bf0b076b2e1d11de0c3b2be6e2a4bd612", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 191 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -104,8 +104,6 @@ class History(Callback):\n         self.seen = 0\n         self.cum_loss = 0.\n         self.cum_accuracy = 0.\n-        self.val_loss = 0.\n-        self.val_accuracy = 0.\n \n     def on_batch_end(self, batch, indices, loss, accuracy):\n         batch_length = len(indices)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dced18442622da8bc143e52a9eb55d3bd2ad5306", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 914 | Contributors (this commit): 15 | Commits (past 90d): 40 | Contributors (cumulative): 15 | DMM Complexity: 0.0\n\nDIFF:\n@@ -239,6 +239,7 @@ class Sequential(Model):\n             for batch_index, (batch_start, batch_end) in enumerate(batches):\n                 callbacks.on_batch_begin(batch_index)\n \n+                try:\n                     batch_ids = index_array[batch_start:batch_end]\n                     X_batch = slice_X(X, batch_ids)\n                     y_batch = y[batch_ids]\n@@ -249,6 +250,12 @@ class Sequential(Model):\n                     else:\n                         loss = self._train(*ins)\n                         acc = None\n+                except KeyboardInterrupt:\n+                    # If training is aborted, call the callbacks anyway before terminating\n+                    callbacks.on_batch_end(batch_index, [], 0., 0.)\n+                    callbacks.on_epoch_end(epoch, 0., 0.)\n+                    callbacks.on_train_end()\n+                    raise KeyboardInterrupt # TODO: Raise a more explicit Excpetion (?)\n \n                 callbacks.on_batch_end(batch_index, batch_ids, loss, acc)\n             \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b85d9646ef4ec9baad36e1d157d0026db845ba95", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 77 | Lines Deleted: 19 | Files Changed: 2 | Hunks: 6 | Methods Changed: 15 | Complexity Δ (Sum/Max): 17/17 | Churn Δ: 96 | Churn Cumulative: 1117 | Contributors (this commit): 15 | Commits (past 90d): 44 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -255,7 +255,7 @@ class Sequential(Model):\n                     callbacks.on_batch_end(batch_index, [], 0., 0.)\n                     callbacks.on_epoch_end(epoch, 0., 0.)\n                     callbacks.on_train_end()\n-                    raise KeyboardInterrupt # TODO: Raise a more explicit Excpetion (?)\n+                    raise KeyboardInterrupt # TODO: Raise a more explicit Exception (?)\n \n                 callbacks.on_batch_end(batch_index, batch_ids, loss, acc)\n             \n\n@@ -35,30 +35,88 @@ X_test /= 255\n # convert class vectors to binary class matrices\n Y_train = np_utils.to_categorical(y_train, nb_classes)[:max_train_samples]\n \n-class DrawWeight(Callback):\n-    def __init__(self):\n-        self.fig = plt.figure()\n-        self.ax = self.fig.add_subplot(1, 1, 1)\n+class Frames(object):\n+    def __init__(self, n_plots=16):\n+        self._n_frames = 0\n+        self._framedata = []\n+        self._titles = []\n+        for i in range(n_plots):\n+            self._framedata.append([])\n+\n+    def add_frame(self, i, frame):\n+        self._framedata[i].append(frame)\n+\n+    def set_title(self, title):\n+        self._titles.append(title)\n+\n+class SubplotTimedAnimation(animation.TimedAnimation):\n+\n+    def __init__(self, fig, frames, grid=(4, 4), interval=10, blit=False, **kwargs):\n+        self.n_plots = grid[0] * grid[1]\n+        self.axes = [fig.add_subplot(grid[0], grid[1], i + 1) for i in range(self.n_plots)]\n+        for axis in self.axes:\n+            axis.get_xaxis().set_ticks([])\n+            axis.get_yaxis().set_ticks([])\n+        self.frames = frames\n+        self.imgs = [self.axes[i].imshow(frames._framedata[i][0], interpolation='nearest') for i in range(self.n_plots)]\n+        self.title = fig.suptitle('')\n+        super(SubplotTimedAnimation, self).__init__(fig, interval=interval, blit=blit, **kwargs)\n+\n+    def _draw_frame(self, j):\n+        for i in range(self.n_plots):\n+            self.imgs[i].set_data(self.frames._framedata[i][j])\n+        if len(self.frames._titles) > j:\n+            self.title.set_text(self.frames._titles[j])\n+        self._drawn_artists = self.imgs\n+\n+    def new_frame_seq(self):\n+        return iter(range(len(self.frames._framedata[0])))\n+\n+    def _init_draw(self):\n+        for img in self.imgs:\n+            img.set_data([[]])\n+\n+def combine_imgs(imgs, grid=(1,1)):\n+    n_imgs, img_h, img_w = imgs.shape\n+    if n_imgs != grid[0] * grid[1]:\n+        raise ValueError()\n+    combined = np.zeros((grid[0] * img_h, grid[1] * img_w))\n+    for i in range(grid[0]):\n+        for j in range(grid[1]):\n+            combined[img_h*i:img_h*(i+1),img_w*j:img_w*(j+1)] = imgs[grid[0] * i + j]\n+    return combined\n+\n+class DrawActivations(Callback):\n+    def __init__(self, figsize):\n+        self.fig = plt.figure(figsize=figsize)\n \n     def on_train_begin(self):\n-        self.imgs = []\n-        self.activations = np.zeros((4 * 30, 4 * 30))\n-        self.indices = np.random.choice(32, 16, replace=False)\n-        self.test = theano.function([self.model.get_input()], self.model.layers[1].get_output(train=False)[0, self.indices])\n-        # self.test = theano.function([self.model.get_input()], self.model.layers[10].get_output(train=False))\n+        self.imgs = Frames(n_plots=5)\n+\n+        layers_0_ids = np.random.choice(32, 16, replace=False)\n+        self.test_layer0 = theano.function([self.model.get_input()], self.model.layers[1].get_output(train=False)[0, layers_0_ids])\n+\n+        layers_1_ids = np.random.choice(64, 36, replace=False)\n+        self.test_layer1 = theano.function([self.model.get_input()], self.model.layers[5].get_output(train=False)[0, layers_1_ids])\n+\n+        self.test_layer2 = theano.function([self.model.get_input()], self.model.layers[10].get_output(train=False)[0])\n+\n+    def on_epoch_begin(self, epoch):\n+        self.epoch = epoch\n \n     def on_batch_end(self, batch, indices, loss, accuracy):\n         if batch % 10 == 0:\n-            activated = self.test(X_test)\n-            for i in range(4):\n-                for j in range(4):\n-                    self.activations[30*i:30*(i+1), 30*j:30*(j+1)] = activated[4 * i + j]\n-            img = self.ax.imshow(self.activations, interpolation='nearest')\n-            # img = self.ax.imshow(self.test(X_test)[0,:].reshape(16,16), interpolation='nearest')\n-            self.imgs.append([img])\n+            self.imgs.add_frame(0, X_test[0,0])\n+            self.imgs.add_frame(1, combine_imgs(self.test_layer0(X_test), grid=(4, 4)))\n+            self.imgs.add_frame(2, combine_imgs(self.test_layer1(X_test), grid=(6, 6)))\n+            self.imgs.add_frame(3, self.test_layer2(X_test).reshape((16,16)))\n+            self.imgs.add_frame(4, self.model._predict(X_test)[0].reshape((1,10)))\n+            self.imgs.set_title('Epoch #%d - Batch #%d' % (self.epoch, batch))\n \n     def on_train_end(self):\n-        anim = animation.ArtistAnimation(self.fig, self.imgs, interval=10, blit=False, repeat_delay=1000)\n+        # anim = animation.ArtistAnimation(self.fig, self.imgs, interval=10, blit=False, repeat_delay=1000)\n+        anim = SubplotTimedAnimation(self.fig, self.imgs, grid=(1,5), interval=10, blit=False, repeat_delay=1000)\n+        anim.save('test_gif.mp4', fps=15)\n         plt.show()\n \n # model = Sequential()\n@@ -89,5 +147,5 @@ model.add(Activation('softmax'))\n model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n # Fit the model\n-draw_weights = DrawWeight()\n+draw_weights = DrawActivations(figsize=(15, 3))\n model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1, callbacks=[draw_weights])\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0613ef9049a36af5c20102b08c60da0f9ccc116f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 214 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -5,6 +5,7 @@ import theano\n from keras.models import Sequential\n from keras.callbacks import Callback\n from keras.layers.core import Dense, Dropout, Activation, Flatten\n+from keras.regularizers import l2\n from keras.layers.convolutional import Convolution2D, MaxPooling2D\n from keras.utils import np_utils\n from keras.datasets import mnist\n@@ -14,7 +15,7 @@ from matplotlib import animation\n \n nb_classes = 10\n batch_size = 128\n-nb_epoch = 5\n+nb_epoch = 10\n \n max_train_samples = 5000\n max_test_samples = 1\n@@ -58,7 +59,7 @@ class SubplotTimedAnimation(animation.TimedAnimation):\n             axis.get_xaxis().set_ticks([])\n             axis.get_yaxis().set_ticks([])\n         self.frames = frames\n-        self.imgs = [self.axes[i].imshow(frames._framedata[i][0], interpolation='nearest') for i in range(self.n_plots)]\n+        self.imgs = [self.axes[i].imshow(frames._framedata[i][0], interpolation='nearest', cmap='bone') for i in range(self.n_plots)]\n         self.title = fig.suptitle('')\n         super(SubplotTimedAnimation, self).__init__(fig, interval=interval, blit=blit, **kwargs)\n \n@@ -105,7 +106,7 @@ class DrawActivations(Callback):\n         self.epoch = epoch\n \n     def on_batch_end(self, batch, indices, loss, accuracy):\n-        if batch % 10 == 0:\n+        if batch % 5 == 0:\n             self.imgs.add_frame(0, X_test[0,0])\n             self.imgs.add_frame(1, combine_imgs(self.test_layer0(X_test), grid=(4, 4)))\n             self.imgs.add_frame(2, combine_imgs(self.test_layer1(X_test), grid=(6, 6)))\n@@ -116,7 +117,7 @@ class DrawActivations(Callback):\n     def on_train_end(self):\n         # anim = animation.ArtistAnimation(self.fig, self.imgs, interval=10, blit=False, repeat_delay=1000)\n         anim = SubplotTimedAnimation(self.fig, self.imgs, grid=(1,5), interval=10, blit=False, repeat_delay=1000)\n-        anim.save('test_gif.mp4', fps=15)\n+        # anim.save('test_gif.gif', fps=15, writer='imagemagick')\n         plt.show()\n \n # model = Sequential()\n@@ -141,11 +142,11 @@ model.add(Dense(64*8*8, 256))\n model.add(Activation('relu'))\n model.add(Dropout(0.5))\n \n-model.add(Dense(256, 10))\n+model.add(Dense(256, 10, W_regularizer = l2(0.1)))\n model.add(Activation('softmax'))\n \n model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n # Fit the model\n-draw_weights = DrawActivations(figsize=(15, 3))\n+draw_weights = DrawActivations(figsize=(5.4, 1.35))\n model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1, callbacks=[draw_weights])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e75ff62dcbe2d028c71bdc9158899e410dc8067e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 215 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -115,7 +115,6 @@ class DrawActivations(Callback):\n             self.imgs.set_title('Epoch #%d - Batch #%d' % (self.epoch, batch))\n \n     def on_train_end(self):\n-        # anim = animation.ArtistAnimation(self.fig, self.imgs, interval=10, blit=False, repeat_delay=1000)\n         anim = SubplotTimedAnimation(self.fig, self.imgs, grid=(1,5), interval=10, blit=False, repeat_delay=1000)\n         # anim.save('test_gif.gif', fps=15, writer='imagemagick')\n         plt.show()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9a12865349134c8f34bda7c9d86db093af422805", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 924 | Contributors (this commit): 15 | Commits (past 90d): 42 | Contributors (cumulative): 15 | DMM Complexity: 0.0\n\nDIFF:\n@@ -262,7 +262,11 @@ class Sequential(Model):\n             # validation\n             val_loss, val_acc = None, None\n             if do_validation:\n-                val_loss, val_acc = self.evaluate(X_val, y_val, batch_size=batch_size, verbose=0, show_accuracy=True)\n+                if show_accuracy:\n+                    val_loss, val_acc = self.evaluate(X_val, y_val, batch_size=batch_size, \\\n+                        verbose=0, show_accuracy=True)\n+                else:\n+                    val_loss = self.evaluate(X_val, y_val, batch_size=batch_size, verbose=0)\n \n             callbacks.on_epoch_end(epoch, val_loss, val_acc)\n \n@@ -337,7 +341,7 @@ class Sequential(Model):\n         if show_accuracy:\n             return tot_score/len(batches), tot_acc/len(batches)\n         else:\n-            return tot_score/len(batches), None\n+            return tot_score/len(batches)\n \n     def get_config(self, verbose=0):\n         layers = []\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a9d135452ca81cf7d93c2475f8bf70465bc1a1a8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 12 | Churn Cumulative: 203 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -37,8 +37,10 @@ class CallbackList(object):\n         for callback in self.callbacks:\n             callback.on_batch_begin(batch)\n         delta_t_callbacks = time.time() - t_before_callbacks\n-        if self._delta_t_batch > 0. and delta_t_callbacks > 2 * self._delta_t_batch:\n-            warnings.warn('Warning!!')\n+        if self._delta_t_batch > 0. and delta_t_callbacks > 0.95 * self._delta_t_batch \\\n+            and delta_t_callbacks > 0.1:\n+            warnings.warn('Method on_batch_begin() is slow compared '\n+                'to the batch update. Check your callbacks.')\n         self._t_enter_batch = time.time()\n \n     def on_batch_end(self, batch, indices, loss, accuracy):\n@@ -47,8 +49,10 @@ class CallbackList(object):\n         for callback in self.callbacks:\n             callback.on_batch_end(batch, indices, loss, accuracy)\n         delta_t_callbacks = time.time() - t_before_callbacks\n-        if self._delta_t_batch > 0. and delta_t_callbacks > 2 * self._delta_t_batch:\n-            warnings.warn('Warning!!')\n+        if self._delta_t_batch > 0. and delta_t_callbacks > 0.95 * self._delta_t_batch \\\n+            and delta_t_callbacks > 0.1:\n+            warnings.warn('Method on_batch_end() is slow compared '\n+                'to the batch update. Check your callbacks.')\n \n     def on_train_begin(self):\n         for callback in self.callbacks:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ae7eb9c2f10406a847b2b4ac9e91b531355ebd55", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 68 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 5 | Complexity Δ (Sum/Max): 12/12 | Churn Δ: 68 | Churn Cumulative: 68 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,41 @@\n+import pydot\n+from keras.layers.core import Merge\n+from keras.models import Model\n+from collections import Counter\n+\n+class Grapher(object):\n+\n+    def __init__(self):\n+        self.names = {}\n+        self.class_counts = Counter()\n+\n+    def get_name(self, model):\n+        if hasattr(model, 'name'):\n+            return model.name\n+        cls = model.__class__.__name__\n+        if model not in self.names:\n+            self.class_counts[cls] += 1\n+            self.names[model] = cls + str(self.class_counts[cls])\n+        return self.names[model]\n+\n+    def add_edge(self, f, t, graph):\n+        if f: graph.add_edge(pydot.Edge(f, t))\n+        return t\n+\n+    def add_model(self, model, graph, parent=None):\n+        this = self.get_name(model)\n+        if isinstance(model, Model):\n+            parent = self.add_edge(parent, this, graph)\n+            for child in reversed(model.layers):\n+                parent = self.add_model(child, graph, parent)\n+        elif isinstance(model, Merge):\n+            for child in model.models:\n+                self.add_model(child, graph, this)\n+            return self.add_edge(parent, this, graph)\n+        else:\n+            return self.add_edge(parent, this, graph)\n+\n+    def plot(self, model, to_file):\n+        graph = pydot.Dot(graph_type='graph')\n+        self.add_model(model, graph)\n+        graph.write_png(to_file)\n\n@@ -0,0 +1,27 @@\n+from keras.utils.dot_utils import Grapher\n+\n+from keras.models import Sequential\n+from keras.layers.core import Dense, Activation, Merge, Flatten\n+from keras.layers.embeddings import Embedding\n+from keras.layers.recurrent import GRU\n+\n+ent_lookup = Sequential()\n+ent_lookup.add(Embedding(10, 2))\n+ent_lookup.add(Flatten())\n+\n+rel_lookup = Sequential()\n+rel_lookup.add(Embedding(20, 2))\n+rel_lookup.add(Flatten())\n+\n+word_sequence = Sequential()\n+word_sequence.add(Embedding(10, 5))\n+word_sequence.add(GRU(5, 2))\n+\n+model = Sequential()\n+model.add(Merge([word_sequence, ent_lookup, rel_lookup], mode='concat'))\n+model.add(Activation('relu'))\n+model.add(Dense(6, 2))\n+model.add(Activation('softmax'))\n+\n+g = Grapher()\n+g.plot(model, 'mymodel.png')\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#092b835753c1be7412d334d115c8e9ba5660a3f7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 9 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 28 | Churn Cumulative: 231 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,13 +4,15 @@ import theano.tensor as T\n import numpy as np\n import warnings\n import time\n+from collections import deque\n \n from .utils.generic_utils import Progbar\n \n class CallbackList(object):\n-    \"\"\"docstring for Callback\"\"\"\n-    def __init__(self, callbacks):\n+\n+    def __init__(self, callbacks, queue_length=10):\n         self.callbacks = callbacks\n+        self.queue_length = queue_length\n \n     def append(self, callback):\n         self.callbacks.append(callback)\n@@ -27,6 +29,8 @@ class CallbackList(object):\n         for callback in self.callbacks:\n             callback.on_epoch_begin(epoch)\n         self._delta_t_batch = 0.\n+        self._delta_ts_batch_begin = deque([], maxlen=self.queue_length)\n+        self._delta_ts_batch_end = deque([], maxlen=self.queue_length)\n \n     def on_epoch_end(self, epoch, val_loss, val_acc):\n         for callback in self.callbacks:\n@@ -36,11 +40,12 @@ class CallbackList(object):\n         t_before_callbacks = time.time()\n         for callback in self.callbacks:\n             callback.on_batch_begin(batch)\n-        delta_t_callbacks = time.time() - t_before_callbacks\n-        if self._delta_t_batch > 0. and delta_t_callbacks > 0.95 * self._delta_t_batch \\\n-            and delta_t_callbacks > 0.1:\n+        self._delta_ts_batch_begin.append(time.time() - t_before_callbacks)\n+        delta_t_median = np.median(self._delta_ts_batch_begin)\n+        if self._delta_t_batch > 0. and delta_t_median > 0.95 * self._delta_t_batch \\\n+            and delta_t_median > 0.1:\n             warnings.warn('Method on_batch_begin() is slow compared '\n-                'to the batch update. Check your callbacks.')\n+                'to the batch update (%f). Check your callbacks.' % delta_t_median)\n         self._t_enter_batch = time.time()\n \n     def on_batch_end(self, batch, indices, loss, accuracy):\n@@ -48,11 +53,12 @@ class CallbackList(object):\n         t_before_callbacks = time.time()\n         for callback in self.callbacks:\n             callback.on_batch_end(batch, indices, loss, accuracy)\n-        delta_t_callbacks = time.time() - t_before_callbacks\n-        if self._delta_t_batch > 0. and delta_t_callbacks > 0.95 * self._delta_t_batch \\\n-            and delta_t_callbacks > 0.1:\n+        self._delta_ts_batch_end.append(time.time() - t_before_callbacks)\n+        delta_t_median = np.median(self._delta_ts_batch_end)\n+        if self._delta_t_batch > 0. and delta_t_median > 0.95 * self._delta_t_batch \\\n+            and delta_t_median > 0.1:\n             warnings.warn('Method on_batch_end() is slow compared '\n-                'to the batch update. Check your callbacks.')\n+                'to the batch update (%f). Check your callbacks.' % delta_t_median)\n \n     def on_train_begin(self):\n         for callback in self.callbacks:\n@@ -64,7 +70,7 @@ class CallbackList(object):\n \n \n class Callback(object):\n-    \"\"\"docstring for Callback\"\"\"\n+\n     def __init__(self):\n         pass\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ecfcc85b7e973c5fa1525ac4cf6fe3f97ce80d70", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 47 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -12,10 +12,10 @@ class Grapher(object):\n     def get_name(self, model):\n         if hasattr(model, 'name'):\n             return model.name\n-        cls = model.__class__.__name__\n+        clz = model.__class__.__name__\n         if model not in self.names:\n-            self.class_counts[cls] += 1\n-            self.names[model] = cls + str(self.class_counts[cls])\n+            self.class_counts[clz] += 1\n+            self.names[model] = clz + str(self.class_counts[clz])\n         return self.names[model]\n \n     def add_edge(self, f, t, graph):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#912ab043238516773c804dbdf09fcd2fdde694b5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 57 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -10,6 +10,10 @@ class Grapher(object):\n         self.class_counts = Counter()\n \n     def get_name(self, model):\n+        \"\"\"\n+        returns the name of the model instance. If model does not have a `name` attribute, then it will be assigned\n+        a generic (and unique) identifier based on its class\n+        \"\"\"\n         if hasattr(model, 'name'):\n             return model.name\n         clz = model.__class__.__name__\n@@ -23,6 +27,9 @@ class Grapher(object):\n         return t\n \n     def add_model(self, model, graph, parent=None):\n+        \"\"\"\n+        Recursively adds `model` and its components to the pydot graph\n+        \"\"\"\n         this = self.get_name(model)\n         if isinstance(model, Model):\n             parent = self.add_edge(parent, this, graph)\n@@ -36,6 +43,9 @@ class Grapher(object):\n             return self.add_edge(parent, this, graph)\n \n     def plot(self, model, to_file):\n+        \"\"\"\n+        creates a graph visualizing the structure of `model` and writes it to `to_file`\n+        \"\"\"\n         graph = pydot.Dot(graph_type='graph')\n         self.add_model(model, graph)\n         graph.write_png(to_file)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#08547d2582ce3071495d2f992381a5dbba605882", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 10/10 | Churn Δ: 7 | Churn Cumulative: 931 | Contributors (this commit): 16 | Commits (past 90d): 43 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -362,10 +362,13 @@ class Sequential(Model):\n             self.layers[i].set_weights(weights[:nb_param])\n             weights = weights[nb_param:]\n \n-    def save_weights(self, filepath):\n+    def save_weights(self, filepath, overwrite=False):\n         # Save weights from all layers to HDF5\n         import h5py\n-        # FIXME: fail if file exists, or add option to overwrite!\n+        import os.path\n+        # if file exists and should not be overwritten\n+        if not overwrite and os.path.isfile(filepath):\n+            raise IOError('%s already exists' % (filepath))\n         f = h5py.File(filepath, 'w')\n         f.attrs['nb_layers'] = len(self.layers)\n         for k, l in enumerate(self.layers):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#49603c1594daf6267f8135937c11a23fe03112f6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 75 | Lines Deleted: 43 | Files Changed: 3 | Hunks: 37 | Methods Changed: 8 | Complexity Δ (Sum/Max): -6/2 | Churn Δ: 118 | Churn Cumulative: 1331 | Contributors (this commit): 19 | Commits (past 90d): 62 | Contributors (cumulative): 22 | DMM Complexity: 0.37142857142857144\n\nDIFF:\n@@ -32,9 +32,9 @@ class CallbackList(object):\n         self._delta_ts_batch_begin = deque([], maxlen=self.queue_length)\n         self._delta_ts_batch_end = deque([], maxlen=self.queue_length)\n \n-    def on_epoch_end(self, epoch, val_loss, val_acc):\n+    def on_epoch_end(self, epoch):\n         for callback in self.callbacks:\n-            callback.on_epoch_end(epoch, val_loss, val_acc)\n+            callback.on_epoch_end(epoch)\n \n     def on_batch_begin(self, batch):\n         t_before_callbacks = time.time()\n@@ -48,11 +48,11 @@ class CallbackList(object):\n                 'to the batch update (%f). Check your callbacks.' % delta_t_median)\n         self._t_enter_batch = time.time()\n \n-    def on_batch_end(self, batch, indices, loss, accuracy):\n+    def on_batch_end(self, batch):\n         self._delta_t_batch = time.time() - self._t_enter_batch\n         t_before_callbacks = time.time()\n         for callback in self.callbacks:\n-            callback.on_batch_end(batch, indices, loss, accuracy)\n+            callback.on_batch_end(batch)\n         self._delta_ts_batch_end.append(time.time() - t_before_callbacks)\n         delta_t_median = np.median(self._delta_ts_batch_end)\n         if self._delta_t_batch > 0. and delta_t_median > 0.95 * self._delta_t_batch \\\n@@ -83,13 +83,13 @@ class Callback(object):\n     def on_epoch_begin(self, epoch):\n         pass\n \n-    def on_epoch_end(self, epoch, val_loss, val_acc):\n+    def on_epoch_end(self, epoch):\n         pass\n \n     def on_batch_begin(self, batch):\n         pass\n \n-    def on_batch_end(self, batch, indices, loss, accuracy):\n+    def on_batch_end(self, batch):\n         pass\n \n     def on_train_begin(self):\n@@ -112,27 +112,29 @@ class History(Callback):\n \n     def on_epoch_begin(self, epoch):\n         self.seen = 0\n-        self.cum_loss = 0.\n-        self.cum_accuracy = 0.\n+        self.tot_loss = 0.\n+        self.tot_accuracy = 0.\n \n-    def on_batch_end(self, batch, indices, loss, accuracy):\n-        batch_length = len(indices)\n-        self.seen += batch_length\n-        self.cum_loss += loss * batch_length\n+    def on_batch_end(self, batch):\n+        batch_size = self.model.batch_history['batch_size'][-1]\n+        self.seen += batch_size\n+        self.tot_loss += self.model.batch_history['loss'][-1] * batch_size\n         if self.params['show_accuracy']:\n-            self.cum_accuracy += accuracy * batch_length\n+            self.tot_accuracy += self.model.batch_history['accuracy'][-1] * batch_size\n \n-    def on_epoch_end(self, epoch, val_loss, val_acc):\n+    def on_epoch_end(self, epoch):\n+        val_loss = self.model.epoch_history['loss'][-1]\n+        val_acc = self.model.epoch_history['accuracy'][-1]\n         self.epochs.append(epoch)\n-        self.losses.append(self.cum_loss / self.seen)\n+        self.losses.append(self.tot_loss / self.seen)\n         if self.params['show_accuracy']:\n-            self.accuracies.append(self.cum_accuracy / self.seen)\n+            self.accuracies.append(self.tot_accuracy / self.seen)\n         if self.params['do_validation']:\n             self.validation_losses.append(val_loss)\n             if self.params['show_accuracy']:\n                 self.validation_accuracies.append(val_acc)\n \n-class Logger(Callback):\n+class BaseLogger(Callback):\n \n     def on_train_begin(self):\n         self.verbose = self.params['verbose']\n@@ -147,17 +149,29 @@ class Logger(Callback):\n     def on_batch_begin(self, batch):\n         self.log_values = []\n \n-    def on_batch_end(self, batch, indices, loss, accuracy):\n+    def on_batch_end(self, batch_index):\n+        self.current += self.model.batch_history['batch_size'][-1]\n+        # skip progbar update for the last batch; will be handled by on_epoch_end\n+        if self.current < self.params['nb_sample']:\n+            loss = self.model.batch_history['loss'][-1]\n             self.log_values.append(('loss', loss))\n-        self.current += len(indices)\n             if self.params['show_accuracy']:\n-            self.log_values.append(('acc.', acc))\n+                accuracy = self.model.batch_history['accuracy'][-1]\n+                self.log_values.append(('acc.', accuracy))\n             if self.verbose:\n                 self.progbar.update(self.current, self.log_values)\n \n-    def on_epoch_end(self, epoch, val_loss, val_acc):\n-        # TODO: Show validation scores in the logger\n+    def on_epoch_end(self, epoch):\n+        loss = self.model.batch_history['loss'][-1]\n+        self.log_values.append(('loss', loss))\n+\n+        if self.params['show_accuracy']:\n+            accuracy = self.model.batch_history['accuracy'][-1]\n+            self.log_values.append(('acc.', accuracy))\n         if self.params['do_validation']:\n+            val_loss = self.model.epoch_history['val_loss'][-1]\n             self.log_values.append(('val. loss', val_loss))\n             if self.params['show_accuracy']:\n+                val_acc = self.model.epoch_history['val_accuracy'][-1]\n                 self.log_values.append(('val. acc.', val_acc))\n+        self.progbar.update(self.current, self.log_values)\n\n@@ -216,7 +216,8 @@ class Sequential(Model):\n         index_array = np.arange(len(y))\n \n         callbacks = cbks.CallbackList(callbacks)\n-        callbacks.append(cbks.Logger())\n+        if verbose:\n+            callbacks.append(cbks.BaseLogger())\n \n         callbacks._set_model(self)\n         callbacks._set_params({\n@@ -227,53 +228,70 @@ class Sequential(Model):\n             'do_validation': do_validation,\n             'show_accuracy': show_accuracy\n         })\n-\n+        self.batch_history = {\n+            'batch':[],\n+            'batch_size':[],\n+            'loss':[],\n+            'accuracy':[],\n+            'val_loss':[],\n+            'val_accuracy':[],\n+        }\n+        self.epoch_history = {\n+            'epoch':[],\n+            'epoch_size':[],\n+            'loss':[],\n+            'accuracy':[],\n+            'val_loss':[],\n+            'val_accuracy':[],\n+        }\n         callbacks.on_train_begin()\n \n         for epoch in range(nb_epoch):\n+            self.epoch_history['epoch'] = epoch\n             callbacks.on_epoch_begin(epoch)\n             if shuffle:\n                 np.random.shuffle(index_array)\n \n             batches = make_batches(len(y), batch_size)\n             for batch_index, (batch_start, batch_end) in enumerate(batches):\n-                callbacks.on_batch_begin(batch_index)\n-\n-                try:\n                 batch_ids = index_array[batch_start:batch_end]\n                 X_batch = slice_X(X, batch_ids)\n                 y_batch = y[batch_ids]\n \n+                self.batch_history['batch'].append(batch_index)\n+                self.batch_history['batch_size'].append(len(batch_ids))\n+                callbacks.on_batch_begin(batch_index)\n+\n                 ins = X_batch + [y_batch]\n                 if show_accuracy:\n                     loss, acc = self._train_with_acc(*ins)\n+                    self.batch_history['accuracy'].append(acc)\n                 else:\n                     loss = self._train(*ins)\n-                        acc = None\n-                except KeyboardInterrupt:\n-                    # If training is aborted, call the callbacks anyway before terminating\n-                    callbacks.on_batch_end(batch_index, [], 0., 0.)\n-                    callbacks.on_epoch_end(epoch, 0., 0.)\n-                    callbacks.on_train_end()\n-                    raise KeyboardInterrupt # TODO: Raise a more explicit Exception (?)\n+                self.batch_history['loss'].append(loss)\n \n-                callbacks.on_batch_end(batch_index, batch_ids, loss, acc)\n+                callbacks.on_batch_end(batch_index)\n                 \n+                if batch_index == len(batches) - 1: # last batch\n                     # validation\n-            val_loss, val_acc = None, None\n                     if do_validation:\n                         if show_accuracy:\n                             val_loss, val_acc = self.evaluate(X_val, y_val, batch_size=batch_size, \\\n                                 verbose=0, show_accuracy=True)\n+                            self.epoch_history['val_accuracy'].append(val_acc)\n                         else:\n                             val_loss = self.evaluate(X_val, y_val, batch_size=batch_size, verbose=0)\n+                        self.epoch_history['val_loss'].append(val_loss)\n \n-            callbacks.on_epoch_end(epoch, val_loss, val_acc)\n+            epoch_loss = sum(map(lambda x: x[0]*x[1], zip(self.batch_history['batch_size'], self.batch_history['loss']))) / len(y)\n+            self.epoch_history['loss'].append(epoch_loss)\n+            if show_accuracy:\n+                epoch_acc = sum(map(lambda x: x[0]*x[1], zip(self.batch_history['batch_size'], self.batch_history['accuracy']))) / len(y)\n+                self.epoch_history['accuracy'].append(epoch_acc)\n+            callbacks.on_epoch_end(epoch)\n \n         callbacks.on_train_end()\n-\n-        # return history\n-        return True\n+        return self.epoch_history\n \n     def predict(self, X, batch_size=128, verbose=1):\n         X = standardize_X(X)\n\n@@ -60,7 +60,7 @@ class Progbar(object):\n         '''\n         for k, v in values:\n             if k not in self.sum_values:\n-                self.sum_values[k] = [v * (current-self.seen_so_far), current-self.seen_so_far]\n+                self.sum_values[k] = [v * max(1, current-self.seen_so_far), current-self.seen_so_far]\n                 self.unique_values.append(k)\n             else:\n                 self.sum_values[k][0] += v * (current-self.seen_so_far)\n@@ -100,7 +100,7 @@ class Progbar(object):\n             else:\n                 info += ' - %ds' % (now - self.start)\n             for k in self.unique_values:\n-                info += ' - %s: %.4f' % (k, self.sum_values[k][0]/self.sum_values[k][1])\n+                info += ' - %s: %.4f' % (k, self.sum_values[k][0]/ max(1, self.sum_values[k][1]))\n \n             self.total_width += len(info)\n             if prev_total_width > self.total_width:\n@@ -116,7 +116,7 @@ class Progbar(object):\n             if current >= self.target:\n                 info = '%ds' % (now - self.start)\n                 for k in self.unique_values:\n-                    info += ' - %s: %.4f' % (k, self.sum_values[k][0]/self.sum_values[k][1])\n+                    info += ' - %s: %.4f' % (k, self.sum_values[k][0]/ max(1, self.sum_values[k][1]))\n                 sys.stdout.write(info + \"\\n\")\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5e49367ee623cb6dc2f9207cb6c24235473f5040", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 292 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,5 @@\n from __future__ import absolute_import\n+from __future__ import print_function\n import theano\n import theano.tensor as T\n import numpy as np\n@@ -141,7 +142,7 @@ class BaseLogger(Callback):\n \n     def on_epoch_begin(self, epoch):\n         if self.verbose:\n-            print 'Epoch %d' % epoch\n+            print('Epoch %d' % epoch)\n             self.progbar = Progbar(target=self.params['nb_sample'], \\\n                 verbose=self.verbose)\n         self.current = 0\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#28113b2010a86e9e1fac7f53cb115df3b6883a31", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 65 | Lines Deleted: 36 | Files Changed: 3 | Hunks: 11 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 101 | Churn Cumulative: 130 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,22 @@\n+# -*- coding: utf-8 -*-\n+from __future__ import absolute_import\n+import sys\n+import six.moves.cPickle\n+from six.moves import range\n+\n+def load_batch(fpath, label_key='labels'):\n+    f = open(fpath, 'rb')\n+    if sys.version_info < (3,):\n+        d = six.moves.cPickle.load(f)\n+    else:\n+        d = six.moves.cPickle.load(f, encoding=\"bytes\")\n+        # decode utf8\n+        for k, v in d.items():\n+            del(d[k])\n+            d[k.decode(\"utf8\")] = v\n+    f.close()\n+    data = d[\"data\"]\n+    labels = d[label_key]\n+\n+    data = data.reshape(data.shape[0], 3, 32, 32)\n+    return data, labels\n\n@@ -1,50 +1,30 @@\n-# -*- coding: utf-8 -*-\n from __future__ import absolute_import\n+from .cifar import load_batch\n from .data_utils import get_file\n-import random\n-import sys, os\n import numpy as np\n-import six.moves.cPickle\n-from six.moves import range\n+import os\n \n-def load_data(test_split=0.1, seed=113):\n+def load_data():\n     dirname = \"cifar-10-batches-py\"\n     origin = \"http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n     path = get_file(dirname, origin=origin, untar=True)\n \n-    nb_samples = 50000\n-    X = np.zeros((nb_samples, 3, 32, 32), dtype=\"uint8\")\n-    y = np.zeros((nb_samples,), dtype=\"uint8\")\n+    nb_test_samples = 10000\n+    nb_train_samples = 50000\n+\n+    X_train = np.zeros((nb_train_samples, 3, 32, 32), dtype=\"uint8\")\n+    y_train = np.zeros((nb_train_samples,), dtype=\"uint8\")\n+\n     for i in range(1, 6):\n         fpath = os.path.join(path, 'data_batch_' + str(i))\n-        f = open(fpath, 'rb')\n-        if sys.version_info < (3,):\n-            d = six.moves.cPickle.load(f)\n-        else:\n-            d = six.moves.cPickle.load(f, encoding=\"bytes\")\n-            # decode utf8\n-            for k, v in d.items():\n-                del(d[k])\n-                d[k.decode(\"utf8\")] = v\n-        f.close()\n-        data = d[\"data\"]\n-        labels = d[\"labels\"]\n+        data, labels = load_batch(fpath)\n+        X_train[(i-1)*10000:i*10000, :, :, :] = data\n+        y_train[(i-1)*10000:i*10000] = labels\n     \n-        data = data.reshape(data.shape[0], 3, 32, 32)\n-        X[(i-1)*10000:i*10000, :, :, :] = data\n-        y[(i-1)*10000:i*10000] = labels\n+    fpath = os.path.join(path, 'test_batch')\n+    X_test, y_test = load_batch(fpath)\n \n-    np.random.seed(seed)\n-    np.random.shuffle(X)\n-    np.random.seed(seed)\n-    np.random.shuffle(y)\n-\n-    y = np.reshape(y, (len(y), 1))\n-\n-    X_train = X[:int(len(X)*(1-test_split))]\n-    y_train = y[:int(len(X)*(1-test_split))]\n-\n-    X_test = X[int(len(X)*(1-test_split)):]\n-    y_test = y[int(len(X)*(1-test_split)):]\n+    y_train = np.reshape(y_train, (len(y_train), 1))\n+    y_test = np.reshape(y_test, (len(y_test), 1))\n \n     return (X_train, y_train), (X_test, y_test) \n\\ No newline at end of file\n\n@@ -0,0 +1,27 @@\n+from __future__ import absolute_import\n+from .cifar import load_batch\n+from .data_utils import get_file\n+import numpy as np\n+import os\n+\n+def load_data(label_mode='fine'):\n+    if label_mode not in ['fine', 'coarse']:\n+        raise Exception('label_mode must be one of \"fine\" \"coarse\".')\n+\n+    dirname = \"cifar-100-python\"\n+    origin = \"http://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\"\n+    path = get_file(dirname, origin=origin, untar=True)\n+\n+    nb_test_samples = 10000\n+    nb_train_samples = 50000\n+\n+    fpath = os.path.join(path, 'train')\n+    X_train, y_train = load_batch(fpath, label_key=label_mode+'_labels')\n+\n+    fpath = os.path.join(path, 'test')\n+    X_test, y_test = load_batch(fpath, label_key=label_mode+'_labels')\n+\n+    y_train = np.reshape(y_train, (len(y_train), 1))\n+    y_test = np.reshape(y_test, (len(y_test), 1))\n+\n+    return (X_train, y_train), (X_test, y_test) \n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#39a69b3f528a1a83bb1767135d6e57ba81e6e3be", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 169 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -29,7 +29,7 @@ nb_epoch = 200\n data_augmentation = True\n \n # the data, shuffled and split between tran and test sets\n-(X_train, y_train), (X_test, y_test) = cifar10.load_data(test_split=0.1)\n+(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n print(X_train.shape[0], 'train samples')\n print(X_test.shape[0], 'test samples')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4f9958452c84967eae7d0f2c833bb1b396f3ba48", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 36 | Files Changed: 1 | Hunks: 1 | Methods Changed: 4 | Complexity Δ (Sum/Max): -11/0 | Churn Δ: 36 | Churn Cumulative: 328 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -99,42 +99,6 @@ class Callback(object):\n     def on_train_end(self):\n         pass\n \n-class History(Callback):\n-\n-    def on_train_begin(self):\n-        self.epochs = []\n-        self.losses = []\n-        if self.params['show_accuracy']:\n-            self.accuracies = []\n-        if self.params['do_validation']:\n-            self.validation_losses = []\n-            if self.params['show_accuracy']:\n-                self.validation_accuracies = []\n-\n-    def on_epoch_begin(self, epoch):\n-        self.seen = 0\n-        self.tot_loss = 0.\n-        self.tot_accuracy = 0.\n-\n-    def on_batch_end(self, batch):\n-        batch_size = self.model.batch_history['batch_size'][-1]\n-        self.seen += batch_size\n-        self.tot_loss += self.model.batch_history['loss'][-1] * batch_size\n-        if self.params['show_accuracy']:\n-            self.tot_accuracy += self.model.batch_history['accuracy'][-1] * batch_size\n-\n-    def on_epoch_end(self, epoch):\n-        val_loss = self.model.epoch_history['loss'][-1]\n-        val_acc = self.model.epoch_history['accuracy'][-1]\n-        self.epochs.append(epoch)\n-        self.losses.append(self.tot_loss / self.seen)\n-        if self.params['show_accuracy']:\n-            self.accuracies.append(self.tot_accuracy / self.seen)\n-        if self.params['do_validation']:\n-            self.validation_losses.append(val_loss)\n-            if self.params['show_accuracy']:\n-                self.validation_accuracies.append(val_acc)\n-\n class BaseLogger(Callback):\n \n     def on_train_begin(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1d61d18b9e7f995ae37081e58711013177ad8406", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 217 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -105,7 +105,7 @@ class DrawActivations(Callback):\n     def on_epoch_begin(self, epoch):\n         self.epoch = epoch\n \n-    def on_batch_end(self, batch, indices, loss, accuracy):\n+    def on_batch_end(self, batch):\n         if batch % 5 == 0:\n             self.imgs.add_frame(0, X_test[0,0])\n             self.imgs.add_frame(1, combine_imgs(self.test_layer0(X_test), grid=(4, 4)))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
