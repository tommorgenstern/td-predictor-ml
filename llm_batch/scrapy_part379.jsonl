{"custom_id": "scrapy#b8e333c8ce78c195e265fa453e7bc0b11c5ca4af", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 172 | Lines Deleted: 87 | Files Changed: 6 | Hunks: 73 | Methods Changed: 72 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 259 | Churn Cumulative: 1227 | Contributors (this commit): 34 | Commits (past 90d): 14 | Contributors (cumulative): 72 | DMM Complexity: 0.5714285714285714\n\nDIFF:\n@@ -39,7 +39,7 @@ class Spider(object_ref):\n \n     def __init__(self, name: Optional[str] = None, **kwargs: Any):\n         if name is not None:\n-            self.name = name\n+            self.name: str = name\n         elif not getattr(self, \"name\", None):\n             raise ValueError(f\"{type(self).__name__} must have a name\")\n         self.__dict__.update(kwargs)\n@@ -67,8 +67,8 @@ class Spider(object_ref):\n         return spider\n \n     def _set_crawler(self, crawler: Crawler) -> None:\n-        self.crawler = crawler\n-        self.settings = crawler.settings\n+        self.crawler: Crawler = crawler\n+        self.settings: BaseSettings = crawler.settings\n         crawler.signals.connect(self.close, signals.spider_closed)\n \n     def start_requests(self) -> Iterable[Request]:\n\n@@ -8,9 +8,27 @@ See documentation in docs/topics/spiders.rst\n from __future__ import annotations\n \n import copy\n-from typing import TYPE_CHECKING, AsyncIterable, Awaitable, Sequence\n+from typing import (\n+    TYPE_CHECKING,\n+    Any,\n+    AsyncIterable,\n+    Awaitable,\n+    Callable,\n+    Dict,\n+    Iterable,\n+    List,\n+    Optional,\n+    Sequence,\n+    Set,\n+    TypeVar,\n+    Union,\n+    cast,\n+)\n+\n+from twisted.python.failure import Failure\n \n from scrapy.http import HtmlResponse, Request, Response\n+from scrapy.link import Link\n from scrapy.linkextractors import LinkExtractor\n from scrapy.spiders import Spider\n from scrapy.utils.asyncgen import collect_asyncgen\n@@ -20,20 +38,32 @@ if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy.crawler import Crawler\n \n-def _identity(x):\n+\n+_T = TypeVar(\"_T\")\n+ProcessLinksT = Callable[[List[Link]], List[Link]]\n+ProcessRequestT = Callable[[Request, Response], Optional[Request]]\n+\n+\n+def _identity(x: _T) -> _T:\n     return x\n \n \n-def _identity_process_request(request, response):\n+def _identity_process_request(\n+    request: Request, response: Response\n+) -> Optional[Request]:\n     return request\n \n \n-def _get_method(method, spider):\n+def _get_method(\n+    method: Union[Callable, str, None], spider: Spider\n+) -> Optional[Callable]:\n     if callable(method):\n         return method\n     if isinstance(method, str):\n         return getattr(spider, method, None)\n+    return None\n \n \n _default_link_extractor = LinkExtractor()\n@@ -42,37 +72,46 @@ _default_link_extractor = LinkExtractor()\n class Rule:\n     def __init__(\n         self,\n-        link_extractor=None,\n-        callback=None,\n-        cb_kwargs=None,\n-        follow=None,\n-        process_links=None,\n-        process_request=None,\n-        errback=None,\n+        link_extractor: Optional[LinkExtractor] = None,\n+        callback: Union[Callable, str, None] = None,\n+        cb_kwargs: Optional[Dict[str, Any]] = None,\n+        follow: Optional[bool] = None,\n+        process_links: Union[ProcessLinksT, str, None] = None,\n+        process_request: Union[ProcessRequestT, str, None] = None,\n+        errback: Union[Callable[[Failure], Any], str, None] = None,\n     ):\n-        self.link_extractor = link_extractor or _default_link_extractor\n-        self.callback = callback\n-        self.errback = errback\n-        self.cb_kwargs = cb_kwargs or {}\n-        self.process_links = process_links or _identity\n-        self.process_request = process_request or _identity_process_request\n-        self.follow = follow if follow is not None else not callback\n+        self.link_extractor: LinkExtractor = link_extractor or _default_link_extractor\n+        self.callback: Union[Callable, str, None] = callback\n+        self.errback: Union[Callable[[Failure], Any], str, None] = errback\n+        self.cb_kwargs: Dict[str, Any] = cb_kwargs or {}\n+        self.process_links: Union[ProcessLinksT, str] = process_links or _identity\n+        self.process_request: Union[ProcessRequestT, str] = (\n+            process_request or _identity_process_request\n+        )\n+        self.follow: bool = follow if follow is not None else not callback\n \n-    def _compile(self, spider):\n+    def _compile(self, spider: Spider) -> None:\n+        # this replaces method names with methods and we can't express this in type hints\n         self.callback = _get_method(self.callback, spider)\n-        self.errback = _get_method(self.errback, spider)\n-        self.process_links = _get_method(self.process_links, spider)\n-        self.process_request = _get_method(self.process_request, spider)\n+        self.errback = cast(Callable[[Failure], Any], _get_method(self.errback, spider))\n+        self.process_links = cast(\n+            ProcessLinksT, _get_method(self.process_links, spider)\n+        )\n+        self.process_request = cast(\n+            ProcessRequestT, _get_method(self.process_request, spider)\n+        )\n \n \n class CrawlSpider(Spider):\n     rules: Sequence[Rule] = ()\n+    _rules: List[Rule]\n+    _follow_links: bool\n \n-    def __init__(self, *a, **kw):\n+    def __init__(self, *a: Any, **kw: Any):\n         super().__init__(*a, **kw)\n         self._compile_rules()\n \n-    def _parse(self, response, **kwargs):\n+    def _parse(self, response: Response, **kwargs: Any) -> Any:\n         return self._parse_response(\n             response=response,\n             callback=self.parse_start_url,\n@@ -80,13 +119,13 @@ class CrawlSpider(Spider):\n             follow=True,\n         )\n \n-    def parse_start_url(self, response, **kwargs):\n+    def parse_start_url(self, response: Response, **kwargs: Any) -> Any:\n         return []\n \n-    def process_results(self, response: Response, results: list):\n+    def process_results(self, response: Response, results: Any) -> Any:\n         return results\n \n-    def _build_request(self, rule_index, link):\n+    def _build_request(self, rule_index: int, link: Link) -> Request:\n         return Request(\n             url=link.url,\n             callback=self._callback,\n@@ -94,32 +133,43 @@ class CrawlSpider(Spider):\n             meta={\"rule\": rule_index, \"link_text\": link.text},\n         )\n \n-    def _requests_to_follow(self, response):\n+    def _requests_to_follow(self, response: Response) -> Iterable[Optional[Request]]:\n         if not isinstance(response, HtmlResponse):\n             return\n-        seen = set()\n+        seen: Set[Link] = set()\n         for rule_index, rule in enumerate(self._rules):\n-            links = [\n+            links: List[Link] = [\n                 lnk\n                 for lnk in rule.link_extractor.extract_links(response)\n                 if lnk not in seen\n             ]\n-            for link in rule.process_links(links):\n+            for link in cast(ProcessLinksT, rule.process_links)(links):\n                 seen.add(link)\n                 request = self._build_request(rule_index, link)\n-                yield rule.process_request(request, response)\n+                yield cast(ProcessRequestT, rule.process_request)(request, response)\n \n-    def _callback(self, response, **cb_kwargs):\n-        rule = self._rules[response.meta[\"rule\"]]\n+    def _callback(self, response: Response, **cb_kwargs: Any) -> Any:\n+        rule = self._rules[cast(int, response.meta[\"rule\"])]\n         return self._parse_response(\n-            response, rule.callback, {**rule.cb_kwargs, **cb_kwargs}, rule.follow\n+            response,\n+            cast(Callable, rule.callback),\n+            {**rule.cb_kwargs, **cb_kwargs},\n+            rule.follow,\n         )\n \n-    def _errback(self, failure):\n-        rule = self._rules[failure.request.meta[\"rule\"]]\n-        return self._handle_failure(failure, rule.errback)\n+    def _errback(self, failure: Failure) -> Iterable[Any]:\n+        rule = self._rules[cast(int, failure.request.meta[\"rule\"])]  # type: ignore[attr-defined]\n+        return self._handle_failure(\n+            failure, cast(Callable[[Failure], Any], rule.errback)\n+        )\n \n-    async def _parse_response(self, response, callback, cb_kwargs, follow=True):\n+    async def _parse_response(\n+        self,\n+        response: Response,\n+        callback: Optional[Callable],\n+        cb_kwargs: Dict[str, Any],\n+        follow: bool = True,\n+    ) -> AsyncIterable[Any]:\n         if callback:\n             cb_res = callback(response, **cb_kwargs) or ()\n             if isinstance(cb_res, AsyncIterable):\n@@ -134,21 +184,23 @@ class CrawlSpider(Spider):\n             for request_or_item in self._requests_to_follow(response):\n                 yield request_or_item\n \n-    def _handle_failure(self, failure, errback):\n+    def _handle_failure(\n+        self, failure: Failure, errback: Optional[Callable[[Failure], Any]]\n+    ) -> Iterable[Any]:\n         if errback:\n             results = errback(failure) or ()\n             yield from iterate_spider_output(results)\n \n-    def _compile_rules(self):\n+    def _compile_rules(self) -> None:\n         self._rules = []\n         for rule in self.rules:\n             self._rules.append(copy.copy(rule))\n             self._rules[-1]._compile(self)\n \n     @classmethod\n-    def from_crawler(cls, crawler, *args, **kwargs) -> Self:\n+    def from_crawler(cls, crawler: Crawler, *args: Any, **kwargs: Any) -> Self:\n         spider = super().from_crawler(crawler, *args, **kwargs)\n-        spider._follow_links = crawler.settings.getbool(  # type: ignore[attr-defined]\n+        spider._follow_links = crawler.settings.getbool(\n             \"CRAWLSPIDER_FOLLOW_LINKS\", True\n         )\n         return spider\n\n@@ -5,7 +5,10 @@ for scraping from an XML feed.\n See documentation in docs/topics/spiders.rst\n \"\"\"\n \n+from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple\n+\n from scrapy.exceptions import NotConfigured, NotSupported\n+from scrapy.http import Response, TextResponse\n from scrapy.selector import Selector\n from scrapy.spiders import Spider\n from scrapy.utils.iterators import csviter, xmliter_lxml\n@@ -22,11 +25,13 @@ class XMLFeedSpider(Spider):\n     use iternodes, since it's a faster and cleaner.\n     \"\"\"\n \n-    iterator = \"iternodes\"\n-    itertag = \"item\"\n-    namespaces = ()\n+    iterator: str = \"iternodes\"\n+    itertag: str = \"item\"\n+    namespaces: Sequence[Tuple[str, str]] = ()\n \n-    def process_results(self, response, results):\n+    def process_results(\n+        self, response: Response, results: Iterable[Any]\n+    ) -> Iterable[Any]:\n         \"\"\"This overridable method is called for each result (item or request)\n         returned by the spider, and it's intended to perform any last time\n         processing required before returning the results to the framework core,\n@@ -36,20 +41,20 @@ class XMLFeedSpider(Spider):\n         \"\"\"\n         return results\n \n-    def adapt_response(self, response):\n+    def adapt_response(self, response: Response) -> Response:\n         \"\"\"You can override this function in order to make any changes you want\n         to into the feed before parsing it. This function must return a\n         response.\n         \"\"\"\n         return response\n \n-    def parse_node(self, response, selector):\n+    def parse_node(self, response: Response, selector: Selector) -> Any:\n         \"\"\"This method must be overridden with your custom spider functionality\"\"\"\n         if hasattr(self, \"parse_item\"):  # backward compatibility\n             return self.parse_item(response, selector)\n         raise NotImplementedError\n \n-    def parse_nodes(self, response, nodes):\n+    def parse_nodes(self, response: Response, nodes: Iterable[Selector]) -> Any:\n         \"\"\"This method is called for the nodes matching the provided tag name\n         (itertag). Receives the response and an Selector for each node.\n         Overriding this method is mandatory. Otherwise, you spider won't work.\n@@ -61,20 +66,25 @@ class XMLFeedSpider(Spider):\n             ret = iterate_spider_output(self.parse_node(response, selector))\n             yield from self.process_results(response, ret)\n \n-    def _parse(self, response, **kwargs):\n+    def _parse(self, response: Response, **kwargs: Any) -> Any:\n         if not hasattr(self, \"parse_node\"):\n             raise NotConfigured(\n                 \"You must define parse_node method in order to scrape this XML feed\"\n             )\n \n         response = self.adapt_response(response)\n+        nodes: Iterable[Selector]\n         if self.iterator == \"iternodes\":\n             nodes = self._iternodes(response)\n         elif self.iterator == \"xml\":\n+            if not isinstance(response, TextResponse):\n+                raise ValueError(\"Response content isn't text\")\n             selector = Selector(response, type=\"xml\")\n             self._register_namespaces(selector)\n             nodes = selector.xpath(f\"//{self.itertag}\")\n         elif self.iterator == \"html\":\n+            if not isinstance(response, TextResponse):\n+                raise ValueError(\"Response content isn't text\")\n             selector = Selector(response, type=\"html\")\n             self._register_namespaces(selector)\n             nodes = selector.xpath(f\"//{self.itertag}\")\n@@ -83,12 +93,12 @@ class XMLFeedSpider(Spider):\n \n         return self.parse_nodes(response, nodes)\n \n-    def _iternodes(self, response):\n+    def _iternodes(self, response: Response) -> Iterable[Selector]:\n         for node in xmliter_lxml(response, self.itertag):\n             self._register_namespaces(node)\n             yield node\n \n-    def _register_namespaces(self, selector):\n+    def _register_namespaces(self, selector: Selector) -> None:\n         for prefix, uri in self.namespaces:\n             selector.register_namespace(prefix, uri)\n \n@@ -102,27 +112,29 @@ class CSVFeedSpider(Spider):\n     and the file's headers.\n     \"\"\"\n \n-    delimiter = (\n+    delimiter: Optional[str] = (\n         None  # When this is None, python's csv module's default delimiter is used\n     )\n-    quotechar = (\n+    quotechar: Optional[str] = (\n         None  # When this is None, python's csv module's default quotechar is used\n     )\n-    headers = None\n+    headers: Optional[List[str]] = None\n \n-    def process_results(self, response, results):\n+    def process_results(\n+        self, response: Response, results: Iterable[Any]\n+    ) -> Iterable[Any]:\n         \"\"\"This method has the same purpose as the one in XMLFeedSpider\"\"\"\n         return results\n \n-    def adapt_response(self, response):\n+    def adapt_response(self, response: Response) -> Response:\n         \"\"\"This method has the same purpose as the one in XMLFeedSpider\"\"\"\n         return response\n \n-    def parse_row(self, response, row):\n+    def parse_row(self, response: Response, row: Dict[str, str]) -> Any:\n         \"\"\"This method must be overridden with your custom spider functionality\"\"\"\n         raise NotImplementedError\n \n-    def parse_rows(self, response):\n+    def parse_rows(self, response: Response) -> Any:\n         \"\"\"Receives a response and a dict (representing each row) with a key for\n         each provided (or detected) header of the CSV file.  This spider also\n         gives the opportunity to override adapt_response and\n@@ -135,7 +147,7 @@ class CSVFeedSpider(Spider):\n             ret = iterate_spider_output(self.parse_row(response, row))\n             yield from self.process_results(response, ret)\n \n-    def _parse(self, response, **kwargs):\n+    def _parse(self, response: Response, **kwargs: Any) -> Any:\n         if not hasattr(self, \"parse_row\"):\n             raise NotConfigured(\n                 \"You must define parse_row method in order to scrape this CSV feed\"\n\n@@ -1,3 +1,7 @@\n+from typing import Any, Iterable, Optional, cast\n+\n+from scrapy import Request\n+from scrapy.http import Response\n from scrapy.spiders import Spider\n from scrapy.utils.spider import iterate_spider_output\n \n@@ -5,17 +9,17 @@ from scrapy.utils.spider import iterate_spider_output\n class InitSpider(Spider):\n     \"\"\"Base Spider with initialization facilities\"\"\"\n \n-    def start_requests(self):\n-        self._postinit_reqs = super().start_requests()\n-        return iterate_spider_output(self.init_request())\n+    def start_requests(self) -> Iterable[Request]:\n+        self._postinit_reqs: Iterable[Request] = super().start_requests()\n+        return cast(Iterable[Request], iterate_spider_output(self.init_request()))\n \n-    def initialized(self, response=None):\n+    def initialized(self, response: Optional[Response] = None) -> Any:\n         \"\"\"This method must be set as the callback of your last initialization\n         request. See self.init_request() docstring for more info.\n         \"\"\"\n         return self.__dict__.pop(\"_postinit_reqs\")\n \n-    def init_request(self):\n+    def init_request(self) -> Any:\n         \"\"\"This function should return one initialization request, with the\n         self.initialized method as callback. When the self.initialized method\n         is called this spider is considered initialized. If you need to perform\n\n@@ -2,9 +2,21 @@ from __future__ import annotations\n \n import logging\n import re\n-from typing import TYPE_CHECKING, Any\n+from typing import (\n+    TYPE_CHECKING,\n+    Any,\n+    Callable,\n+    Dict,\n+    Iterable,\n+    List,\n+    Optional,\n+    Sequence,\n+    Tuple,\n+    Union,\n+    cast,\n+)\n \n-from scrapy.http import Request, XmlResponse\n+from scrapy.http import Request, Response, XmlResponse\n from scrapy.spiders import Spider\n from scrapy.utils._compression import _DecompressionMaxSizeExceeded\n from scrapy.utils.gz import gunzip, gzip_magic_number\n@@ -20,10 +32,12 @@ logger = logging.getLogger(__name__)\n \n \n class SitemapSpider(Spider):\n-    sitemap_urls = ()\n-    sitemap_rules = [(\"\", \"parse\")]\n-    sitemap_follow = [\"\"]\n-    sitemap_alternate_links = False\n+    sitemap_urls: Sequence[str] = ()\n+    sitemap_rules: Sequence[\n+        Tuple[Union[re.Pattern[str], str], Union[str, Callable]]\n+    ] = [(\"\", \"parse\")]\n+    sitemap_follow: Sequence[Union[re.Pattern[str], str]] = [\"\"]\n+    sitemap_alternate_links: bool = False\n     _max_size: int\n     _warn_size: int\n \n@@ -38,27 +52,29 @@ class SitemapSpider(Spider):\n         )\n         return spider\n \n-    def __init__(self, *a, **kw):\n+    def __init__(self, *a: Any, **kw: Any):\n         super().__init__(*a, **kw)\n-        self._cbs = []\n+        self._cbs: List[Tuple[re.Pattern[str], Callable]] = []\n         for r, c in self.sitemap_rules:\n             if isinstance(c, str):\n-                c = getattr(self, c)\n+                c = cast(Callable, getattr(self, c))\n             self._cbs.append((regex(r), c))\n-        self._follow = [regex(x) for x in self.sitemap_follow]\n+        self._follow: List[re.Pattern[str]] = [regex(x) for x in self.sitemap_follow]\n \n-    def start_requests(self):\n+    def start_requests(self) -> Iterable[Request]:\n         for url in self.sitemap_urls:\n             yield Request(url, self._parse_sitemap)\n \n-    def sitemap_filter(self, entries):\n+    def sitemap_filter(\n+        self, entries: Iterable[Dict[str, Any]]\n+    ) -> Iterable[Dict[str, Any]]:\n         \"\"\"This method can be used to filter sitemap entries by their\n         attributes, for example, you can filter locs with lastmod greater\n         than a given date (see docs).\n         \"\"\"\n         yield from entries\n \n-    def _parse_sitemap(self, response):\n+    def _parse_sitemap(self, response: Response) -> Iterable[Request]:\n         if response.url.endswith(\"/robots.txt\"):\n             for url in sitemap_urls_from_robots(response.text, base_url=response.url):\n                 yield Request(url, callback=self._parse_sitemap)\n@@ -86,7 +102,7 @@ class SitemapSpider(Spider):\n                             yield Request(loc, callback=c)\n                             break\n \n-    def _get_sitemap_body(self, response):\n+    def _get_sitemap_body(self, response: Response) -> Optional[bytes]:\n         \"\"\"Return the sitemap body contained in the given response,\n         or None if the response is not a sitemap.\n         \"\"\"\n@@ -117,15 +133,16 @@ class SitemapSpider(Spider):\n         # in other word, here, we have plain XML\n         if response.url.endswith(\".xml\") or response.url.endswith(\".xml.gz\"):\n             return response.body\n+        return None\n \n \n-def regex(x):\n+def regex(x: Union[re.Pattern[str], str]) -> re.Pattern[str]:\n     if isinstance(x, str):\n         return re.compile(x)\n     return x\n \n \n-def iterloc(it, alt=False):\n+def iterloc(it: Iterable[Dict[str, Any]], alt: bool = False) -> Iterable[str]:\n     for d in it:\n         yield d[\"loc\"]\n \n\n@@ -5,7 +5,7 @@ Note: The main purpose of this module is to provide support for the\n SitemapSpider, its API is subject to change without notice.\n \"\"\"\n \n-from typing import Any, Dict, Generator, Iterator, Optional\n+from typing import Any, Dict, Generator, Iterator, Optional, Union\n from urllib.parse import urljoin\n \n import lxml.etree  # nosec\n@@ -15,7 +15,7 @@ class Sitemap:\n     \"\"\"Class to parse Sitemap (type=urlset) and Sitemap Index\n     (type=sitemapindex) files\"\"\"\n \n-    def __init__(self, xmltext: str):\n+    def __init__(self, xmltext: Union[str, bytes]):\n         xmlp = lxml.etree.XMLParser(\n             recover=True, remove_comments=True, resolve_entities=False\n         )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b88f22c6c5de4ca8828b2abe860516c2468c4fe3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 237 | Lines Deleted: 3 | Files Changed: 4 | Hunks: 14 | Methods Changed: 17 | Complexity Δ (Sum/Max): 24/16 | Churn Δ: 240 | Churn Cumulative: 2227 | Contributors (this commit): 34 | Commits (past 90d): 8 | Contributors (cumulative): 63 | DMM Complexity: 1.0\n\nDIFF:\n@@ -25,6 +25,7 @@ if TYPE_CHECKING:\n \n     from scrapy.crawler import Crawler\n     from scrapy.settings import BaseSettings\n+    from scrapy.utils.log import SpiderLoggerAdapter\n \n     CallbackT = Callable[Concatenate[Response, ...], Any]\n \n@@ -47,9 +48,11 @@ class Spider(object_ref):\n             self.start_urls: List[str] = []\n \n     @property\n-    def logger(self) -> logging.LoggerAdapter:\n+    def logger(self) -> SpiderLoggerAdapter:\n+        from scrapy.utils.log import SpiderLoggerAdapter\n+\n         logger = logging.getLogger(self.name)\n-        return logging.LoggerAdapter(logger, {\"spider\": self})\n+        return SpiderLoggerAdapter(logger, {\"spider\": self})\n \n     def log(self, message: Any, level: int = logging.DEBUG, **kw: Any) -> None:\n         \"\"\"Log the given message at the given log level\n\n@@ -4,7 +4,17 @@ import logging\n import sys\n from logging.config import dictConfig\n from types import TracebackType\n-from typing import TYPE_CHECKING, Any, List, Optional, Tuple, Type, Union, cast\n+from typing import (\n+    TYPE_CHECKING,\n+    Any,\n+    List,\n+    MutableMapping,\n+    Optional,\n+    Tuple,\n+    Type,\n+    Union,\n+    cast,\n+)\n \n from twisted.python import log as twisted_log\n from twisted.python.failure import Failure\n@@ -238,3 +248,16 @@ def logformatter_adapter(logkws: dict) -> Tuple[int, str, dict]:\n     args = logkws if not logkws.get(\"args\") else logkws[\"args\"]\n \n     return (level, message, args)\n+\n+\n+class SpiderLoggerAdapter(logging.LoggerAdapter):\n+    def process(\n+        self, msg: str, kwargs: MutableMapping[str, Any]\n+    ) -> Tuple[str, MutableMapping[str, Any]]:\n+        \"\"\"Method that augments logging with additional 'extra' data\"\"\"\n+        if isinstance(kwargs.get(\"extra\"), MutableMapping):\n+            kwargs[\"extra\"].update(self.extra)\n+        else:\n+            kwargs[\"extra\"] = self.extra\n+\n+        return msg, kwargs\n\n@@ -4,6 +4,7 @@ Some spiders used for testing and benchmarking\n \n import asyncio\n import time\n+from typing import Optional\n from urllib.parse import urlencode\n \n from twisted.internet import defer\n@@ -78,6 +79,28 @@ class DelaySpider(MetaSpider):\n         self.t2_err = time.time()\n \n \n+class LogSpider(MetaSpider):\n+    name = \"log_spider\"\n+\n+    def log_debug(self, message: str, extra: Optional[dict] = None):\n+        self.logger.debug(message, extra=extra)\n+\n+    def log_info(self, message: str, extra: Optional[dict] = None):\n+        self.logger.info(message, extra=extra)\n+\n+    def log_warning(self, message: str, extra: Optional[dict] = None):\n+        self.logger.warning(message, extra=extra)\n+\n+    def log_error(self, message: str, extra: Optional[dict] = None):\n+        self.logger.error(message, extra=extra)\n+\n+    def log_critical(self, message: str, extra: Optional[dict] = None):\n+        self.logger.critical(message, extra=extra)\n+\n+    def parse(self, response):\n+        pass\n+\n+\n class SlowSpider(DelaySpider):\n     name = \"slow\"\n \n\n@@ -1,18 +1,26 @@\n+import json\n import logging\n+import re\n import sys\n import unittest\n+from io import StringIO\n+from typing import Any, Dict, Mapping, MutableMapping\n+from unittest import TestCase\n \n+import pytest\n from testfixtures import LogCapture\n from twisted.python.failure import Failure\n \n from scrapy.extensions import telnet\n from scrapy.utils.log import (\n     LogCounterHandler,\n+    SpiderLoggerAdapter,\n     StreamLogger,\n     TopLevelFormatter,\n     failure_to_exc_info,\n )\n from scrapy.utils.test import get_crawler\n+from tests.spiders import LogSpider\n \n \n class FailureToExcInfoTest(unittest.TestCase):\n@@ -106,3 +114,180 @@ class StreamLoggerTest(unittest.TestCase):\n         with LogCapture() as log:\n             print(\"test log msg\")\n         log.check((\"test\", \"ERROR\", \"test log msg\"))\n+\n+\n+@pytest.mark.parametrize(\n+    (\"base_extra\", \"log_extra\", \"expected_extra\"),\n+    (\n+        (\n+            {\"spider\": \"test\"},\n+            {\"extra\": {\"log_extra\": \"info\"}},\n+            {\"extra\": {\"log_extra\": \"info\", \"spider\": \"test\"}},\n+        ),\n+        (\n+            {\"spider\": \"test\"},\n+            {\"extra\": None},\n+            {\"extra\": {\"spider\": \"test\"}},\n+        ),\n+        (\n+            {\"spider\": \"test\"},\n+            {\"extra\": {\"spider\": \"test2\"}},\n+            {\"extra\": {\"spider\": \"test\"}},\n+        ),\n+    ),\n+)\n+def test_spider_logger_adapter_process(\n+    base_extra: Mapping[str, Any], log_extra: MutableMapping, expected_extra: Dict\n+):\n+    logger = logging.getLogger(\"test\")\n+    spider_logger_adapter = SpiderLoggerAdapter(logger, base_extra)\n+\n+    log_message = \"test_log_message\"\n+    result_message, result_kwargs = spider_logger_adapter.process(\n+        log_message, log_extra\n+    )\n+\n+    assert result_message == log_message\n+    assert result_kwargs == expected_extra\n+\n+\n+class LoggingTestCase(TestCase):\n+    def setUp(self):\n+        self.log_stream = StringIO()\n+        handler = logging.StreamHandler(self.log_stream)\n+        logger = logging.getLogger(\"log_spider\")\n+        logger.addHandler(handler)\n+        logger.setLevel(logging.DEBUG)\n+        self.handler = handler\n+        self.logger = logger\n+        self.spider = LogSpider()\n+\n+    def tearDown(self):\n+        self.logger.removeHandler(self.handler)\n+\n+    def test_debug_logging(self):\n+        log_message = \"Foo message\"\n+        self.spider.log_debug(log_message)\n+        log_contents = self.log_stream.getvalue()\n+\n+        assert log_contents == f\"{log_message}\\n\"\n+\n+    def test_info_logging(self):\n+        log_message = \"Bar message\"\n+        self.spider.log_info(log_message)\n+        log_contents = self.log_stream.getvalue()\n+\n+        assert log_contents == f\"{log_message}\\n\"\n+\n+    def test_warning_logging(self):\n+        log_message = \"Baz message\"\n+        self.spider.log_warning(log_message)\n+        log_contents = self.log_stream.getvalue()\n+\n+        assert log_contents == f\"{log_message}\\n\"\n+\n+    def test_error_logging(self):\n+        log_message = \"Foo bar message\"\n+        self.spider.log_error(log_message)\n+        log_contents = self.log_stream.getvalue()\n+\n+        assert log_contents == f\"{log_message}\\n\"\n+\n+    def test_critical_logging(self):\n+        log_message = \"Foo bar baz message\"\n+        self.spider.log_critical(log_message)\n+        log_contents = self.log_stream.getvalue()\n+\n+        assert log_contents == f\"{log_message}\\n\"\n+\n+\n+class LoggingWithExtraTestCase(TestCase):\n+    def setUp(self):\n+        self.log_stream = StringIO()\n+        handler = logging.StreamHandler(self.log_stream)\n+        formatter = logging.Formatter(\n+            '{\"levelname\": \"%(levelname)s\", \"message\": \"%(message)s\", \"spider\": \"%(spider)s\", \"important_info\": \"%(important_info)s\"}'\n+        )\n+        handler.setFormatter(formatter)\n+        logger = logging.getLogger(\"log_spider\")\n+        logger.addHandler(handler)\n+        logger.setLevel(logging.DEBUG)\n+        self.handler = handler\n+        self.logger = logger\n+        self.spider = LogSpider()\n+        self.regex_pattern = re.compile(r\"^<LogSpider\\s'log_spider'\\sat\\s[^>]+>$\")\n+\n+    def tearDown(self):\n+        self.logger.removeHandler(self.handler)\n+\n+    def test_debug_logging(self):\n+        log_message = \"Foo message\"\n+        extra = {\"important_info\": \"foo\"}\n+        self.spider.log_debug(log_message, extra)\n+        log_contents = self.log_stream.getvalue()\n+        log_contents = json.loads(log_contents)\n+\n+        assert log_contents[\"levelname\"] == \"DEBUG\"\n+        assert log_contents[\"message\"] == log_message\n+        assert self.regex_pattern.match(log_contents[\"spider\"])\n+        assert log_contents[\"important_info\"] == extra[\"important_info\"]\n+\n+    def test_info_logging(self):\n+        log_message = \"Bar message\"\n+        extra = {\"important_info\": \"bar\"}\n+        self.spider.log_info(log_message, extra)\n+        log_contents = self.log_stream.getvalue()\n+        log_contents = json.loads(log_contents)\n+\n+        assert log_contents[\"levelname\"] == \"INFO\"\n+        assert log_contents[\"message\"] == log_message\n+        assert self.regex_pattern.match(log_contents[\"spider\"])\n+        assert log_contents[\"important_info\"] == extra[\"important_info\"]\n+\n+    def test_warning_logging(self):\n+        log_message = \"Baz message\"\n+        extra = {\"important_info\": \"baz\"}\n+        self.spider.log_warning(log_message, extra)\n+        log_contents = self.log_stream.getvalue()\n+        log_contents = json.loads(log_contents)\n+\n+        assert log_contents[\"levelname\"] == \"WARNING\"\n+        assert log_contents[\"message\"] == log_message\n+        assert self.regex_pattern.match(log_contents[\"spider\"])\n+        assert log_contents[\"important_info\"] == extra[\"important_info\"]\n+\n+    def test_error_logging(self):\n+        log_message = \"Foo bar message\"\n+        extra = {\"important_info\": \"foo bar\"}\n+        self.spider.log_error(log_message, extra)\n+        log_contents = self.log_stream.getvalue()\n+        log_contents = json.loads(log_contents)\n+\n+        assert log_contents[\"levelname\"] == \"ERROR\"\n+        assert log_contents[\"message\"] == log_message\n+        assert self.regex_pattern.match(log_contents[\"spider\"])\n+        assert log_contents[\"important_info\"] == extra[\"important_info\"]\n+\n+    def test_critical_logging(self):\n+        log_message = \"Foo bar baz message\"\n+        extra = {\"important_info\": \"foo bar baz\"}\n+        self.spider.log_critical(log_message, extra)\n+        log_contents = self.log_stream.getvalue()\n+        log_contents = json.loads(log_contents)\n+\n+        assert log_contents[\"levelname\"] == \"CRITICAL\"\n+        assert log_contents[\"message\"] == log_message\n+        assert self.regex_pattern.match(log_contents[\"spider\"])\n+        assert log_contents[\"important_info\"] == extra[\"important_info\"]\n+\n+    def test_overwrite_spider_extra(self):\n+        log_message = \"Foo message\"\n+        extra = {\"important_info\": \"foo\", \"spider\": \"shouldn't change\"}\n+        self.spider.log_error(log_message, extra)\n+        log_contents = self.log_stream.getvalue()\n+        log_contents = json.loads(log_contents)\n+\n+        assert log_contents[\"levelname\"] == \"ERROR\"\n+        assert log_contents[\"message\"] == log_message\n+        assert self.regex_pattern.match(log_contents[\"spider\"])\n+        assert log_contents[\"important_info\"] == extra[\"important_info\"]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d2f1e00a6afa46c26a334a24217d5bf605bab9fb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1517 | Lines Deleted: 270 | Files Changed: 12 | Hunks: 60 | Methods Changed: 70 | Complexity Δ (Sum/Max): 70/21 | Churn Δ: 1787 | Churn Cumulative: 10476 | Contributors (this commit): 109 | Commits (past 90d): 23 | Contributors (cumulative): 242 | DMM Complexity: 0.9317073170731708\n\nDIFF:\n@@ -28,7 +28,7 @@ from twisted.python.failure import Failure\n from scrapy import signals\n from scrapy.core.downloader import Downloader\n from scrapy.core.scraper import Scraper\n-from scrapy.exceptions import CloseSpider, DontCloseSpider\n+from scrapy.exceptions import CloseSpider, DontCloseSpider, IgnoreRequest\n from scrapy.http import Request, Response\n from scrapy.logformatter import LogFormatter\n from scrapy.settings import BaseSettings, Settings\n@@ -36,6 +36,7 @@ from scrapy.signalmanager import SignalManager\n from scrapy.spiders import Spider\n from scrapy.utils.log import failure_to_exc_info, logformatter_adapter\n from scrapy.utils.misc import build_from_crawler, load_object\n+from scrapy.utils.python import global_object_name\n from scrapy.utils.reactor import CallLaterOnce\n \n if TYPE_CHECKING:\n@@ -292,9 +293,19 @@ class ExecutionEngine:\n         self.slot.nextcall.schedule()  # type: ignore[union-attr]\n \n     def _schedule_request(self, request: Request, spider: Spider) -> None:\n-        self.signals.send_catch_log(\n-            signals.request_scheduled, request=request, spider=spider\n+        request_scheduled_result = self.signals.send_catch_log(\n+            signals.request_scheduled,\n+            request=request,\n+            spider=spider,\n+            dont_log=IgnoreRequest,\n         )\n+        for handler, result in request_scheduled_result:\n+            if isinstance(result, Failure) and isinstance(result.value, IgnoreRequest):\n+                logger.debug(\n+                    f\"Signal handler {global_object_name(handler)} dropped \"\n+                    f\"request {request} before it reached the scheduler.\"\n+                )\n+                return\n         if not self.slot.scheduler.enqueue_request(request):  # type: ignore[union-attr]\n             self.signals.send_catch_log(\n                 signals.request_dropped, request=request, spider=spider\n\n@@ -60,26 +60,33 @@ class HttpProxyMiddleware:\n     def process_request(\n         self, request: Request, spider: Spider\n     ) -> Union[Request, Response, None]:\n-        creds, proxy_url = None, None\n+        creds, proxy_url, scheme = None, None, None\n         if \"proxy\" in request.meta:\n             if request.meta[\"proxy\"] is not None:\n                 creds, proxy_url = self._get_proxy(request.meta[\"proxy\"], \"\")\n         elif self.proxies:\n             parsed = urlparse_cached(request)\n-            scheme = parsed.scheme\n+            _scheme = parsed.scheme\n             if (\n                 # 'no_proxy' is only supported by http schemes\n-                scheme not in (\"http\", \"https\")\n+                _scheme not in (\"http\", \"https\")\n                 or (parsed.hostname and not proxy_bypass(parsed.hostname))\n-            ) and scheme in self.proxies:\n+            ) and _scheme in self.proxies:\n+                scheme = _scheme\n                 creds, proxy_url = self.proxies[scheme]\n \n-        self._set_proxy_and_creds(request, proxy_url, creds)\n+        self._set_proxy_and_creds(request, proxy_url, creds, scheme)\n         return None\n \n     def _set_proxy_and_creds(\n-        self, request: Request, proxy_url: Optional[str], creds: Optional[bytes]\n+        self,\n+        request: Request,\n+        proxy_url: Optional[str],\n+        creds: Optional[bytes],\n+        scheme: Optional[str],\n     ) -> None:\n+        if scheme:\n+            request.meta[\"_scheme_proxy\"] = True\n         if proxy_url:\n             request.meta[\"proxy\"] = proxy_url\n         elif request.meta.get(\"proxy\") is not None:\n\n@@ -0,0 +1,77 @@\n+import logging\n+import re\n+import warnings\n+\n+from scrapy import signals\n+from scrapy.exceptions import IgnoreRequest\n+from scrapy.utils.httpobj import urlparse_cached\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class OffsiteMiddleware:\n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        o = cls(crawler.stats)\n+        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n+        crawler.signals.connect(o.request_scheduled, signal=signals.request_scheduled)\n+        return o\n+\n+    def __init__(self, stats):\n+        self.stats = stats\n+        self.domains_seen = set()\n+\n+    def spider_opened(self, spider):\n+        self.host_regex = self.get_host_regex(spider)\n+\n+    def request_scheduled(self, request, spider):\n+        self.process_request(request, spider)\n+\n+    def process_request(self, request, spider):\n+        if request.dont_filter or self.should_follow(request, spider):\n+            return None\n+        domain = urlparse_cached(request).hostname\n+        if domain and domain not in self.domains_seen:\n+            self.domains_seen.add(domain)\n+            logger.debug(\n+                \"Filtered offsite request to %(domain)r: %(request)s\",\n+                {\"domain\": domain, \"request\": request},\n+                extra={\"spider\": spider},\n+            )\n+            self.stats.inc_value(\"offsite/domains\", spider=spider)\n+        self.stats.inc_value(\"offsite/filtered\", spider=spider)\n+        raise IgnoreRequest\n+\n+    def should_follow(self, request, spider):\n+        regex = self.host_regex\n+        # hostname can be None for wrong urls (like javascript links)\n+        host = urlparse_cached(request).hostname or \"\"\n+        return bool(regex.search(host))\n+\n+    def get_host_regex(self, spider):\n+        \"\"\"Override this method to implement a different offsite policy\"\"\"\n+        allowed_domains = getattr(spider, \"allowed_domains\", None)\n+        if not allowed_domains:\n+            return re.compile(\"\")  # allow all by default\n+        url_pattern = re.compile(r\"^https?://.*$\")\n+        port_pattern = re.compile(r\":\\d+$\")\n+        domains = []\n+        for domain in allowed_domains:\n+            if domain is None:\n+                continue\n+            if url_pattern.match(domain):\n+                message = (\n+                    \"allowed_domains accepts only domains, not URLs. \"\n+                    f\"Ignoring URL entry {domain} in allowed_domains.\"\n+                )\n+                warnings.warn(message)\n+            elif port_pattern.search(domain):\n+                message = (\n+                    \"allowed_domains accepts only domains without ports. \"\n+                    f\"Ignoring entry {domain} in allowed_domains.\"\n+                )\n+                warnings.warn(message)\n+            else:\n+                domains.append(re.escape(domain))\n+        regex = rf'^(.*\\.)?({\"|\".join(domains)})$'\n+        return re.compile(regex)\n\n@@ -29,17 +29,49 @@ def _build_redirect_request(\n         **kwargs,\n         cookies=None,\n     )\n+    if \"_scheme_proxy\" in redirect_request.meta:\n+        source_request_scheme = urlparse_cached(source_request).scheme\n+        redirect_request_scheme = urlparse_cached(redirect_request).scheme\n+        if source_request_scheme != redirect_request_scheme:\n+            redirect_request.meta.pop(\"_scheme_proxy\")\n+            redirect_request.meta.pop(\"proxy\", None)\n+            redirect_request.meta.pop(\"_auth_proxy\", None)\n+            redirect_request.headers.pop(b\"Proxy-Authorization\", None)\n     has_cookie_header = \"Cookie\" in redirect_request.headers\n     has_authorization_header = \"Authorization\" in redirect_request.headers\n     if has_cookie_header or has_authorization_header:\n-        source_request_netloc = urlparse_cached(source_request).netloc\n-        redirect_request_netloc = urlparse_cached(redirect_request).netloc\n-        if source_request_netloc != redirect_request_netloc:\n-            if has_cookie_header:\n+        default_ports = {\"http\": 80, \"https\": 443}\n+\n+        parsed_source_request = urlparse_cached(source_request)\n+        source_scheme, source_host, source_port = (\n+            parsed_source_request.scheme,\n+            parsed_source_request.hostname,\n+            parsed_source_request.port\n+            or default_ports.get(parsed_source_request.scheme),\n+        )\n+\n+        parsed_redirect_request = urlparse_cached(redirect_request)\n+        redirect_scheme, redirect_host, redirect_port = (\n+            parsed_redirect_request.scheme,\n+            parsed_redirect_request.hostname,\n+            parsed_redirect_request.port\n+            or default_ports.get(parsed_redirect_request.scheme),\n+        )\n+\n+        if has_cookie_header and (\n+            redirect_scheme not in {source_scheme, \"https\"}\n+            or source_host != redirect_host\n+        ):\n             del redirect_request.headers[\"Cookie\"]\n+\n         # https://fetch.spec.whatwg.org/#ref-for-cors-non-wildcard-request-header-name\n-            if has_authorization_header:\n+        if has_authorization_header and (\n+            source_scheme != redirect_scheme\n+            or source_host != redirect_host\n+            or source_port != redirect_port\n+        ):\n             del redirect_request.headers[\"Authorization\"]\n+\n     return redirect_request\n \n \n@@ -129,9 +161,11 @@ class RedirectMiddleware(BaseRedirectMiddleware):\n             location = request_scheme + \"://\" + location.lstrip(\"/\")\n \n         redirected_url = urljoin(request.url, location)\n+        redirected = _build_redirect_request(request, url=redirected_url)\n+        if urlparse_cached(redirected).scheme not in {\"http\", \"https\"}:\n+            return response\n \n         if response.status in (301, 307, 308) or request.method == \"HEAD\":\n-            redirected = _build_redirect_request(request, url=redirected_url)\n             return self._redirect(redirected, request, spider, response.status)\n \n         redirected = self._redirect_request_using_get(request, redirected_url)\n@@ -153,12 +187,16 @@ class MetaRefreshMiddleware(BaseRedirectMiddleware):\n             request.meta.get(\"dont_redirect\", False)\n             or request.method == \"HEAD\"\n             or not isinstance(response, HtmlResponse)\n+            or urlparse_cached(request).scheme not in {\"http\", \"https\"}\n         ):\n             return response\n \n         interval, url = get_meta_refresh(response, ignore_tags=self._ignore_tags)\n-        if url and cast(float, interval) < self._maxdelay:\n-            redirected = self._redirect_request_using_get(request, url)\n-            return self._redirect(redirected, request, spider, \"meta refresh\")\n-\n+        if not url:\n+            return response\n+        redirected = self._redirect_request_using_get(request, url)\n+        if urlparse_cached(redirected).scheme not in {\"http\", \"https\"}:\n+            return response\n+        if cast(float, interval) < self._maxdelay:\n+            return self._redirect(redirected, request, spider, \"meta refresh\")\n         return response\n\n@@ -101,6 +101,7 @@ DOWNLOADER_MIDDLEWARES = {}\n \n DOWNLOADER_MIDDLEWARES_BASE = {\n     # Engine side\n+    \"scrapy.downloadermiddlewares.offsite.OffsiteMiddleware\": 50,\n     \"scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware\": 100,\n     \"scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware\": 300,\n     \"scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware\": 350,\n@@ -301,7 +302,6 @@ SPIDER_MIDDLEWARES = {}\n SPIDER_MIDDLEWARES_BASE = {\n     # Engine side\n     \"scrapy.spidermiddlewares.httperror.HttpErrorMiddleware\": 50,\n-    \"scrapy.spidermiddlewares.offsite.OffsiteMiddleware\": 500,\n     \"scrapy.spidermiddlewares.referer.RefererMiddleware\": 700,\n     \"scrapy.spidermiddlewares.urllength.UrlLengthMiddleware\": 800,\n     \"scrapy.spidermiddlewares.depth.DepthMiddleware\": 900,\n\n@@ -13,15 +13,21 @@ from typing import TYPE_CHECKING, Any, AsyncIterable, Iterable, Set\n \n from scrapy import Spider, signals\n from scrapy.crawler import Crawler\n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Request, Response\n from scrapy.statscollectors import StatsCollector\n from scrapy.utils.httpobj import urlparse_cached\n \n+warnings.warn(\n+    \"The scrapy.spidermiddlewares.offsite module is deprecated, use \"\n+    \"scrapy.downloadermiddlewares.offsite instead.\",\n+    ScrapyDeprecationWarning,\n+)\n+\n if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n-\n logger = logging.getLogger(__name__)\n \n \n\n@@ -331,7 +331,7 @@ def global_object_name(obj: Any) -> str:\n     >>> global_object_name(Request)\n     'scrapy.http.request.Request'\n     \"\"\"\n-    return f\"{obj.__module__}.{obj.__name__}\"\n+    return f\"{obj.__module__}.{obj.__qualname__}\"\n \n \n if hasattr(sys, \"pypy_version_info\"):\n\n@@ -22,13 +22,11 @@ class ManagerTestCase(TestCase):\n         self.crawler = get_crawler(Spider, self.settings_dict)\n         self.spider = self.crawler._create_spider(\"foo\")\n         self.mwman = DownloaderMiddlewareManager.from_crawler(self.crawler)\n-        # some mw depends on stats collector\n-        self.crawler.stats.open_spider(self.spider)\n-        return self.mwman.open_spider(self.spider)\n+        self.crawler.engine = self.crawler._create_engine()\n+        return self.crawler.engine.open_spider(self.spider, start_requests=())\n \n     def tearDown(self):\n-        self.crawler.stats.close_spider(self.spider, \"\")\n-        return self.mwman.close_spider(self.spider)\n+        return self.crawler.engine.close_spider(self.spider)\n \n     def _download(self, request, response=None):\n         \"\"\"Executes downloader mw manager's download method and returns\n\n@@ -0,0 +1,184 @@\n+import pytest\n+\n+from scrapy import Request, Spider\n+from scrapy.downloadermiddlewares.offsite import OffsiteMiddleware\n+from scrapy.exceptions import IgnoreRequest\n+from scrapy.utils.test import get_crawler\n+\n+UNSET = object()\n+\n+\n+@pytest.mark.parametrize(\n+    (\"allowed_domain\", \"url\", \"allowed\"),\n+    (\n+        (\"example.com\", \"http://example.com/1\", True),\n+        (\"example.com\", \"http://example.org/1\", False),\n+        (\"example.com\", \"http://sub.example.com/1\", True),\n+        (\"sub.example.com\", \"http://sub.example.com/1\", True),\n+        (\"sub.example.com\", \"http://example.com/1\", False),\n+        (\"example.com\", \"http://example.com:8000/1\", True),\n+        (\"example.com\", \"http://example.org/example.com\", False),\n+        (\"example.com\", \"http://example.org/foo.example.com\", False),\n+        (\"example.com\", \"http://example.com.example\", False),\n+        (\"a.example\", \"http://nota.example\", False),\n+        (\"b.a.example\", \"http://notb.a.example\", False),\n+    ),\n+)\n+def test_process_request_domain_filtering(allowed_domain, url, allowed):\n+    crawler = get_crawler(Spider)\n+    spider = crawler._create_spider(name=\"a\", allowed_domains=[allowed_domain])\n+    mw = OffsiteMiddleware.from_crawler(crawler)\n+    mw.spider_opened(spider)\n+    request = Request(url)\n+    if allowed:\n+        assert mw.process_request(request, spider) is None\n+    else:\n+        with pytest.raises(IgnoreRequest):\n+            mw.process_request(request, spider)\n+\n+\n+@pytest.mark.parametrize(\n+    (\"value\", \"filtered\"),\n+    (\n+        (UNSET, True),\n+        (None, True),\n+        (False, True),\n+        (True, False),\n+    ),\n+)\n+def test_process_request_dont_filter(value, filtered):\n+    crawler = get_crawler(Spider)\n+    spider = crawler._create_spider(name=\"a\", allowed_domains=[\"a.example\"])\n+    mw = OffsiteMiddleware.from_crawler(crawler)\n+    mw.spider_opened(spider)\n+    kwargs = {}\n+    if value is not UNSET:\n+        kwargs[\"dont_filter\"] = value\n+    request = Request(\"https://b.example\", **kwargs)\n+    if filtered:\n+        with pytest.raises(IgnoreRequest):\n+            mw.process_request(request, spider)\n+    else:\n+        assert mw.process_request(request, spider) is None\n+\n+\n+@pytest.mark.parametrize(\n+    \"value\",\n+    (\n+        UNSET,\n+        None,\n+        [],\n+    ),\n+)\n+def test_process_request_no_allowed_domains(value):\n+    crawler = get_crawler(Spider)\n+    kwargs = {}\n+    if value is not UNSET:\n+        kwargs[\"allowed_domains\"] = value\n+    spider = crawler._create_spider(name=\"a\", **kwargs)\n+    mw = OffsiteMiddleware.from_crawler(crawler)\n+    mw.spider_opened(spider)\n+    request = Request(\"https://example.com\")\n+    assert mw.process_request(request, spider) is None\n+\n+\n+def test_process_request_invalid_domains():\n+    crawler = get_crawler(Spider)\n+    allowed_domains = [\"a.example\", None, \"http:////b.example\", \"//c.example\"]\n+    spider = crawler._create_spider(name=\"a\", allowed_domains=allowed_domains)\n+    mw = OffsiteMiddleware.from_crawler(crawler)\n+    mw.spider_opened(spider)\n+    request = Request(\"https://a.example\")\n+    assert mw.process_request(request, spider) is None\n+    for letter in (\"b\", \"c\"):\n+        request = Request(f\"https://{letter}.example\")\n+        with pytest.raises(IgnoreRequest):\n+            mw.process_request(request, spider)\n+\n+\n+@pytest.mark.parametrize(\n+    (\"allowed_domain\", \"url\", \"allowed\"),\n+    (\n+        (\"example.com\", \"http://example.com/1\", True),\n+        (\"example.com\", \"http://example.org/1\", False),\n+        (\"example.com\", \"http://sub.example.com/1\", True),\n+        (\"sub.example.com\", \"http://sub.example.com/1\", True),\n+        (\"sub.example.com\", \"http://example.com/1\", False),\n+        (\"example.com\", \"http://example.com:8000/1\", True),\n+        (\"example.com\", \"http://example.org/example.com\", False),\n+        (\"example.com\", \"http://example.org/foo.example.com\", False),\n+        (\"example.com\", \"http://example.com.example\", False),\n+        (\"a.example\", \"http://nota.example\", False),\n+        (\"b.a.example\", \"http://notb.a.example\", False),\n+    ),\n+)\n+def test_request_scheduled_domain_filtering(allowed_domain, url, allowed):\n+    crawler = get_crawler(Spider)\n+    spider = crawler._create_spider(name=\"a\", allowed_domains=[allowed_domain])\n+    mw = OffsiteMiddleware.from_crawler(crawler)\n+    mw.spider_opened(spider)\n+    request = Request(url)\n+    if allowed:\n+        assert mw.request_scheduled(request, spider) is None\n+    else:\n+        with pytest.raises(IgnoreRequest):\n+            mw.request_scheduled(request, spider)\n+\n+\n+@pytest.mark.parametrize(\n+    (\"value\", \"filtered\"),\n+    (\n+        (UNSET, True),\n+        (None, True),\n+        (False, True),\n+        (True, False),\n+    ),\n+)\n+def test_request_scheduled_dont_filter(value, filtered):\n+    crawler = get_crawler(Spider)\n+    spider = crawler._create_spider(name=\"a\", allowed_domains=[\"a.example\"])\n+    mw = OffsiteMiddleware.from_crawler(crawler)\n+    mw.spider_opened(spider)\n+    kwargs = {}\n+    if value is not UNSET:\n+        kwargs[\"dont_filter\"] = value\n+    request = Request(\"https://b.example\", **kwargs)\n+    if filtered:\n+        with pytest.raises(IgnoreRequest):\n+            mw.request_scheduled(request, spider)\n+    else:\n+        assert mw.request_scheduled(request, spider) is None\n+\n+\n+@pytest.mark.parametrize(\n+    \"value\",\n+    (\n+        UNSET,\n+        None,\n+        [],\n+    ),\n+)\n+def test_request_scheduled_no_allowed_domains(value):\n+    crawler = get_crawler(Spider)\n+    kwargs = {}\n+    if value is not UNSET:\n+        kwargs[\"allowed_domains\"] = value\n+    spider = crawler._create_spider(name=\"a\", **kwargs)\n+    mw = OffsiteMiddleware.from_crawler(crawler)\n+    mw.spider_opened(spider)\n+    request = Request(\"https://example.com\")\n+    assert mw.request_scheduled(request, spider) is None\n+\n+\n+def test_request_scheduled_invalid_domains():\n+    crawler = get_crawler(Spider)\n+    allowed_domains = [\"a.example\", None, \"http:////b.example\", \"//c.example\"]\n+    spider = crawler._create_spider(name=\"a\", allowed_domains=allowed_domains)\n+    mw = OffsiteMiddleware.from_crawler(crawler)\n+    mw.spider_opened(spider)\n+    request = Request(\"https://a.example\")\n+    assert mw.request_scheduled(request, spider) is None\n+    for letter in (\"b\", \"c\"):\n+        request = Request(f\"https://{letter}.example\")\n+        with pytest.raises(IgnoreRequest):\n+            mw.request_scheduled(request, spider)\n\n@@ -1,5 +1,9 @@\n import unittest\n+from itertools import chain, product\n \n+import pytest\n+\n+from scrapy.downloadermiddlewares.httpproxy import HttpProxyMiddleware\n from scrapy.downloadermiddlewares.redirect import (\n     MetaRefreshMiddleware,\n     RedirectMiddleware,\n@@ -7,22 +11,1030 @@ from scrapy.downloadermiddlewares.redirect import (\n from scrapy.exceptions import IgnoreRequest\n from scrapy.http import HtmlResponse, Request, Response\n from scrapy.spiders import Spider\n+from scrapy.utils.misc import set_environ\n from scrapy.utils.test import get_crawler\n \n \n-class RedirectMiddlewareTest(unittest.TestCase):\n+class Base:\n+    class Test(unittest.TestCase):\n+        def test_priority_adjust(self):\n+            req = Request(\"http://a.com\")\n+            rsp = self.get_response(req, \"http://a.com/redirected\")\n+            req2 = self.mw.process_response(req, rsp, self.spider)\n+            self.assertGreater(req2.priority, req.priority)\n+\n+        def test_dont_redirect(self):\n+            url = \"http://www.example.com/301\"\n+            url2 = \"http://www.example.com/redirected\"\n+            req = Request(url, meta={\"dont_redirect\": True})\n+            rsp = self.get_response(req, url2)\n+\n+            r = self.mw.process_response(req, rsp, self.spider)\n+            assert isinstance(r, Response)\n+            assert r is rsp\n+\n+            # Test that it redirects when dont_redirect is False\n+            req = Request(url, meta={\"dont_redirect\": False})\n+            rsp = self.get_response(req, url2)\n+\n+            r = self.mw.process_response(req, rsp, self.spider)\n+            assert isinstance(r, Request)\n+\n+        def test_post(self):\n+            url = \"http://www.example.com/302\"\n+            url2 = \"http://www.example.com/redirected2\"\n+            req = Request(\n+                url,\n+                method=\"POST\",\n+                body=\"test\",\n+                headers={\"Content-Type\": \"text/plain\", \"Content-length\": \"4\"},\n+            )\n+            rsp = self.get_response(req, url2)\n+\n+            req2 = self.mw.process_response(req, rsp, self.spider)\n+            assert isinstance(req2, Request)\n+            self.assertEqual(req2.url, url2)\n+            self.assertEqual(req2.method, \"GET\")\n+            assert (\n+                \"Content-Type\" not in req2.headers\n+            ), \"Content-Type header must not be present in redirected request\"\n+            assert (\n+                \"Content-Length\" not in req2.headers\n+            ), \"Content-Length header must not be present in redirected request\"\n+            assert not req2.body, f\"Redirected body must be empty, not '{req2.body}'\"\n+\n+        def test_max_redirect_times(self):\n+            self.mw.max_redirect_times = 1\n+            req = Request(\"http://scrapytest.org/302\")\n+            rsp = self.get_response(req, \"/redirected\")\n+\n+            req = self.mw.process_response(req, rsp, self.spider)\n+            assert isinstance(req, Request)\n+            assert \"redirect_times\" in req.meta\n+            self.assertEqual(req.meta[\"redirect_times\"], 1)\n+            self.assertRaises(\n+                IgnoreRequest, self.mw.process_response, req, rsp, self.spider\n+            )\n+\n+        def test_ttl(self):\n+            self.mw.max_redirect_times = 100\n+            req = Request(\"http://scrapytest.org/302\", meta={\"redirect_ttl\": 1})\n+            rsp = self.get_response(req, \"/a\")\n+\n+            req = self.mw.process_response(req, rsp, self.spider)\n+            assert isinstance(req, Request)\n+            self.assertRaises(\n+                IgnoreRequest, self.mw.process_response, req, rsp, self.spider\n+            )\n+\n+        def test_redirect_urls(self):\n+            req1 = Request(\"http://scrapytest.org/first\")\n+            rsp1 = self.get_response(req1, \"/redirected\")\n+            req2 = self.mw.process_response(req1, rsp1, self.spider)\n+            rsp2 = self.get_response(req1, \"/redirected2\")\n+            req3 = self.mw.process_response(req2, rsp2, self.spider)\n+\n+            self.assertEqual(req2.url, \"http://scrapytest.org/redirected\")\n+            self.assertEqual(\n+                req2.meta[\"redirect_urls\"], [\"http://scrapytest.org/first\"]\n+            )\n+            self.assertEqual(req3.url, \"http://scrapytest.org/redirected2\")\n+            self.assertEqual(\n+                req3.meta[\"redirect_urls\"],\n+                [\"http://scrapytest.org/first\", \"http://scrapytest.org/redirected\"],\n+            )\n+\n+        def test_redirect_reasons(self):\n+            req1 = Request(\"http://scrapytest.org/first\")\n+            rsp1 = self.get_response(req1, \"/redirected1\")\n+            req2 = self.mw.process_response(req1, rsp1, self.spider)\n+            rsp2 = self.get_response(req2, \"/redirected2\")\n+            req3 = self.mw.process_response(req2, rsp2, self.spider)\n+            self.assertEqual(req2.meta[\"redirect_reasons\"], [self.reason])\n+            self.assertEqual(req3.meta[\"redirect_reasons\"], [self.reason, self.reason])\n+\n+        def test_cross_origin_header_dropping(self):\n+            safe_headers = {\"A\": \"B\"}\n+            cookie_header = {\"Cookie\": \"a=b\"}\n+            authorization_header = {\"Authorization\": \"Bearer 123456\"}\n+\n+            original_request = Request(\n+                \"https://example.com\",\n+                headers={**safe_headers, **cookie_header, **authorization_header},\n+            )\n+\n+            # Redirects to the same origin (same scheme, same domain, same port)\n+            # keep all headers.\n+            internal_response = self.get_response(\n+                original_request, \"https://example.com/a\"\n+            )\n+            internal_redirect_request = self.mw.process_response(\n+                original_request, internal_response, self.spider\n+            )\n+            self.assertIsInstance(internal_redirect_request, Request)\n+            self.assertEqual(\n+                original_request.headers, internal_redirect_request.headers\n+            )\n+\n+            # Redirects to the same origin (same scheme, same domain, same port)\n+            # keep all headers also when the scheme is http.\n+            http_request = Request(\n+                \"http://example.com\",\n+                headers={**safe_headers, **cookie_header, **authorization_header},\n+            )\n+            http_response = self.get_response(http_request, \"http://example.com/a\")\n+            http_redirect_request = self.mw.process_response(\n+                http_request, http_response, self.spider\n+            )\n+            self.assertIsInstance(http_redirect_request, Request)\n+            self.assertEqual(http_request.headers, http_redirect_request.headers)\n+\n+            # For default ports, whether the port is explicit or implicit does not\n+            # affect the outcome, it is still the same origin.\n+            to_explicit_port_response = self.get_response(\n+                original_request, \"https://example.com:443/a\"\n+            )\n+            to_explicit_port_redirect_request = self.mw.process_response(\n+                original_request, to_explicit_port_response, self.spider\n+            )\n+            self.assertIsInstance(to_explicit_port_redirect_request, Request)\n+            self.assertEqual(\n+                original_request.headers, to_explicit_port_redirect_request.headers\n+            )\n+\n+            # For default ports, whether the port is explicit or implicit does not\n+            # affect the outcome, it is still the same origin.\n+            to_implicit_port_response = self.get_response(\n+                original_request, \"https://example.com/a\"\n+            )\n+            to_implicit_port_redirect_request = self.mw.process_response(\n+                original_request, to_implicit_port_response, self.spider\n+            )\n+            self.assertIsInstance(to_implicit_port_redirect_request, Request)\n+            self.assertEqual(\n+                original_request.headers, to_implicit_port_redirect_request.headers\n+            )\n+\n+            # A port change drops the Authorization header because the origin\n+            # changes, but keeps the Cookie header because the domain remains the\n+            # same.\n+            different_port_response = self.get_response(\n+                original_request, \"https://example.com:8080/a\"\n+            )\n+            different_port_redirect_request = self.mw.process_response(\n+                original_request, different_port_response, self.spider\n+            )\n+            self.assertIsInstance(different_port_redirect_request, Request)\n+            self.assertEqual(\n+                {**safe_headers, **cookie_header},\n+                different_port_redirect_request.headers.to_unicode_dict(),\n+            )\n+\n+            # A domain change drops both the Authorization and the Cookie header.\n+            external_response = self.get_response(\n+                original_request, \"https://example.org/a\"\n+            )\n+            external_redirect_request = self.mw.process_response(\n+                original_request, external_response, self.spider\n+            )\n+            self.assertIsInstance(external_redirect_request, Request)\n+            self.assertEqual(\n+                safe_headers, external_redirect_request.headers.to_unicode_dict()\n+            )\n+\n+            # A scheme upgrade (http → https) drops the Authorization header\n+            # because the origin changes, but keeps the Cookie header because the\n+            # domain remains the same.\n+            upgrade_response = self.get_response(http_request, \"https://example.com/a\")\n+            upgrade_redirect_request = self.mw.process_response(\n+                http_request, upgrade_response, self.spider\n+            )\n+            self.assertIsInstance(upgrade_redirect_request, Request)\n+            self.assertEqual(\n+                {**safe_headers, **cookie_header},\n+                upgrade_redirect_request.headers.to_unicode_dict(),\n+            )\n+\n+            # A scheme downgrade (https → http) drops the Authorization header\n+            # because the origin changes, and the Cookie header because its value\n+            # cannot indicate whether the cookies were secure (HTTPS-only) or not.\n+            #\n+            # Note: If the Cookie header is set by the cookie management\n+            # middleware, as recommended in the docs, the dropping of Cookie on\n+            # scheme downgrade is not an issue, because the cookie management\n+            # middleware will add again the Cookie header to the new request if\n+            # appropriate.\n+            downgrade_response = self.get_response(\n+                original_request, \"http://example.com/a\"\n+            )\n+            downgrade_redirect_request = self.mw.process_response(\n+                original_request, downgrade_response, self.spider\n+            )\n+            self.assertIsInstance(downgrade_redirect_request, Request)\n+            self.assertEqual(\n+                safe_headers,\n+                downgrade_redirect_request.headers.to_unicode_dict(),\n+            )\n+\n+        def test_meta_proxy_http_absolute(self):\n+            crawler = get_crawler()\n+            redirect_mw = self.mwcls.from_crawler(crawler)\n+            proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n+\n+            meta = {\"proxy\": \"https://a:@a.example\"}\n+            request1 = Request(\"http://example.com\", meta=meta)\n+            spider = None\n+            proxy_mw.process_request(request1, spider)\n+\n+            self.assertEqual(request1.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request1.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request1.meta[\"proxy\"], \"https://a.example\")\n+\n+            response1 = self.get_response(request1, \"http://example.com\")\n+            request2 = redirect_mw.process_response(request1, response1, spider)\n+\n+            self.assertIsInstance(request2, Request)\n+            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n+\n+            proxy_mw.process_request(request2, spider)\n+\n+            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n+\n+            response2 = self.get_response(request2, \"http://example.com\")\n+            request3 = redirect_mw.process_response(request2, response2, spider)\n+\n+            self.assertIsInstance(request3, Request)\n+            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n+\n+            proxy_mw.process_request(request3, spider)\n+\n+            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n+\n+        def test_meta_proxy_http_relative(self):\n+            crawler = get_crawler()\n+            redirect_mw = self.mwcls.from_crawler(crawler)\n+            proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n+\n+            meta = {\"proxy\": \"https://a:@a.example\"}\n+            request1 = Request(\"http://example.com\", meta=meta)\n+            spider = None\n+            proxy_mw.process_request(request1, spider)\n+\n+            self.assertEqual(request1.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request1.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request1.meta[\"proxy\"], \"https://a.example\")\n+\n+            response1 = self.get_response(request1, \"/a\")\n+            request2 = redirect_mw.process_response(request1, response1, spider)\n+\n+            self.assertIsInstance(request2, Request)\n+            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n+\n+            proxy_mw.process_request(request2, spider)\n+\n+            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n+\n+            response2 = self.get_response(request2, \"/a\")\n+            request3 = redirect_mw.process_response(request2, response2, spider)\n+\n+            self.assertIsInstance(request3, Request)\n+            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n+\n+            proxy_mw.process_request(request3, spider)\n+\n+            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n+\n+        def test_meta_proxy_https_absolute(self):\n+            crawler = get_crawler()\n+            redirect_mw = self.mwcls.from_crawler(crawler)\n+            proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n+\n+            meta = {\"proxy\": \"https://a:@a.example\"}\n+            request1 = Request(\"https://example.com\", meta=meta)\n+            spider = None\n+            proxy_mw.process_request(request1, spider)\n+\n+            self.assertEqual(request1.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request1.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request1.meta[\"proxy\"], \"https://a.example\")\n+\n+            response1 = self.get_response(request1, \"https://example.com\")\n+            request2 = redirect_mw.process_response(request1, response1, spider)\n+\n+            self.assertIsInstance(request2, Request)\n+            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n+\n+            proxy_mw.process_request(request2, spider)\n+\n+            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n+\n+            response2 = self.get_response(request2, \"https://example.com\")\n+            request3 = redirect_mw.process_response(request2, response2, spider)\n+\n+            self.assertIsInstance(request3, Request)\n+            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n+\n+            proxy_mw.process_request(request3, spider)\n+\n+            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n+\n+        def test_meta_proxy_https_relative(self):\n+            crawler = get_crawler()\n+            redirect_mw = self.mwcls.from_crawler(crawler)\n+            proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n+\n+            meta = {\"proxy\": \"https://a:@a.example\"}\n+            request1 = Request(\"https://example.com\", meta=meta)\n+            spider = None\n+            proxy_mw.process_request(request1, spider)\n+\n+            self.assertEqual(request1.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request1.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request1.meta[\"proxy\"], \"https://a.example\")\n+\n+            response1 = self.get_response(request1, \"/a\")\n+            request2 = redirect_mw.process_response(request1, response1, spider)\n+\n+            self.assertIsInstance(request2, Request)\n+            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n+\n+            proxy_mw.process_request(request2, spider)\n+\n+            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n+\n+            response2 = self.get_response(request2, \"/a\")\n+            request3 = redirect_mw.process_response(request2, response2, spider)\n+\n+            self.assertIsInstance(request3, Request)\n+            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n+\n+            proxy_mw.process_request(request3, spider)\n+\n+            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n+\n+        def test_meta_proxy_http_to_https(self):\n+            crawler = get_crawler()\n+            redirect_mw = self.mwcls.from_crawler(crawler)\n+            proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n+\n+            meta = {\"proxy\": \"https://a:@a.example\"}\n+            request1 = Request(\"http://example.com\", meta=meta)\n+            spider = None\n+            proxy_mw.process_request(request1, spider)\n+\n+            self.assertEqual(request1.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request1.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request1.meta[\"proxy\"], \"https://a.example\")\n+\n+            response1 = self.get_response(request1, \"https://example.com\")\n+            request2 = redirect_mw.process_response(request1, response1, spider)\n+\n+            self.assertIsInstance(request2, Request)\n+            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n+\n+            proxy_mw.process_request(request2, spider)\n+\n+            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n+\n+            response2 = self.get_response(request2, \"http://example.com\")\n+            request3 = redirect_mw.process_response(request2, response2, spider)\n+\n+            self.assertIsInstance(request3, Request)\n+            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n+\n+            proxy_mw.process_request(request3, spider)\n+\n+            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n+\n+        def test_meta_proxy_https_to_http(self):\n+            crawler = get_crawler()\n+            redirect_mw = self.mwcls.from_crawler(crawler)\n+            proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n+\n+            meta = {\"proxy\": \"https://a:@a.example\"}\n+            request1 = Request(\"https://example.com\", meta=meta)\n+            spider = None\n+            proxy_mw.process_request(request1, spider)\n+\n+            self.assertEqual(request1.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request1.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request1.meta[\"proxy\"], \"https://a.example\")\n+\n+            response1 = self.get_response(request1, \"http://example.com\")\n+            request2 = redirect_mw.process_response(request1, response1, spider)\n+\n+            self.assertIsInstance(request2, Request)\n+            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n+\n+            proxy_mw.process_request(request2, spider)\n+\n+            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n+\n+            response2 = self.get_response(request2, \"https://example.com\")\n+            request3 = redirect_mw.process_response(request2, response2, spider)\n+\n+            self.assertIsInstance(request3, Request)\n+            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n+\n+            proxy_mw.process_request(request3, spider)\n+\n+            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n+\n+        def test_system_proxy_http_absolute(self):\n+            crawler = get_crawler()\n+            redirect_mw = self.mwcls.from_crawler(crawler)\n+            env = {\n+                \"http_proxy\": \"https://a:@a.example\",\n+            }\n+            with set_environ(**env):\n+                proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n+\n+            request1 = Request(\"http://example.com\")\n+            spider = None\n+            proxy_mw.process_request(request1, spider)\n+\n+            self.assertEqual(request1.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request1.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request1.meta[\"proxy\"], \"https://a.example\")\n+\n+            response1 = self.get_response(request1, \"http://example.com\")\n+            request2 = redirect_mw.process_response(request1, response1, spider)\n+\n+            self.assertIsInstance(request2, Request)\n+            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n+\n+            proxy_mw.process_request(request2, spider)\n+\n+            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n+\n+            response2 = self.get_response(request2, \"http://example.com\")\n+            request3 = redirect_mw.process_response(request2, response2, spider)\n+\n+            self.assertIsInstance(request3, Request)\n+            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n+\n+            proxy_mw.process_request(request3, spider)\n+\n+            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n+\n+        def test_system_proxy_http_relative(self):\n+            crawler = get_crawler()\n+            redirect_mw = self.mwcls.from_crawler(crawler)\n+            env = {\n+                \"http_proxy\": \"https://a:@a.example\",\n+            }\n+            with set_environ(**env):\n+                proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n+\n+            request1 = Request(\"http://example.com\")\n+            spider = None\n+            proxy_mw.process_request(request1, spider)\n+\n+            self.assertEqual(request1.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request1.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request1.meta[\"proxy\"], \"https://a.example\")\n+\n+            response1 = self.get_response(request1, \"/a\")\n+            request2 = redirect_mw.process_response(request1, response1, spider)\n+\n+            self.assertIsInstance(request2, Request)\n+            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n+\n+            proxy_mw.process_request(request2, spider)\n+\n+            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n+\n+            response2 = self.get_response(request2, \"/a\")\n+            request3 = redirect_mw.process_response(request2, response2, spider)\n+\n+            self.assertIsInstance(request3, Request)\n+            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n+\n+            proxy_mw.process_request(request3, spider)\n+\n+            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n+\n+        def test_system_proxy_https_absolute(self):\n+            crawler = get_crawler()\n+            redirect_mw = self.mwcls.from_crawler(crawler)\n+            env = {\n+                \"https_proxy\": \"https://a:@a.example\",\n+            }\n+            with set_environ(**env):\n+                proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n+\n+            request1 = Request(\"https://example.com\")\n+            spider = None\n+            proxy_mw.process_request(request1, spider)\n+\n+            self.assertEqual(request1.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request1.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request1.meta[\"proxy\"], \"https://a.example\")\n+\n+            response1 = self.get_response(request1, \"https://example.com\")\n+            request2 = redirect_mw.process_response(request1, response1, spider)\n+\n+            self.assertIsInstance(request2, Request)\n+            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n+\n+            proxy_mw.process_request(request2, spider)\n+\n+            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n+\n+            response2 = self.get_response(request2, \"https://example.com\")\n+            request3 = redirect_mw.process_response(request2, response2, spider)\n+\n+            self.assertIsInstance(request3, Request)\n+            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n+\n+            proxy_mw.process_request(request3, spider)\n+\n+            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n+\n+        def test_system_proxy_https_relative(self):\n+            crawler = get_crawler()\n+            redirect_mw = self.mwcls.from_crawler(crawler)\n+            env = {\n+                \"https_proxy\": \"https://a:@a.example\",\n+            }\n+            with set_environ(**env):\n+                proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n+\n+            request1 = Request(\"https://example.com\")\n+            spider = None\n+            proxy_mw.process_request(request1, spider)\n+\n+            self.assertEqual(request1.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request1.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request1.meta[\"proxy\"], \"https://a.example\")\n+\n+            response1 = self.get_response(request1, \"/a\")\n+            request2 = redirect_mw.process_response(request1, response1, spider)\n+\n+            self.assertIsInstance(request2, Request)\n+            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n+\n+            proxy_mw.process_request(request2, spider)\n+\n+            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n+\n+            response2 = self.get_response(request2, \"/a\")\n+            request3 = redirect_mw.process_response(request2, response2, spider)\n+\n+            self.assertIsInstance(request3, Request)\n+            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n+\n+            proxy_mw.process_request(request3, spider)\n+\n+            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n+\n+        def test_system_proxy_proxied_http_to_proxied_https(self):\n+            crawler = get_crawler()\n+            redirect_mw = self.mwcls.from_crawler(crawler)\n+            env = {\n+                \"http_proxy\": \"https://a:@a.example\",\n+                \"https_proxy\": \"https://b:@b.example\",\n+            }\n+            with set_environ(**env):\n+                proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n+\n+            request1 = Request(\"http://example.com\")\n+            spider = None\n+            proxy_mw.process_request(request1, spider)\n+\n+            self.assertEqual(request1.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request1.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request1.meta[\"proxy\"], \"https://a.example\")\n+\n+            response1 = self.get_response(request1, \"https://example.com\")\n+            request2 = redirect_mw.process_response(request1, response1, spider)\n+\n+            self.assertIsInstance(request2, Request)\n+            self.assertNotIn(\"Proxy-Authorization\", request2.headers)\n+            self.assertNotIn(\"_auth_proxy\", request2.meta)\n+            self.assertNotIn(\"proxy\", request2.meta)\n+\n+            proxy_mw.process_request(request2, spider)\n+\n+            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic Yjo=\")\n+            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://b.example\")\n+            self.assertEqual(request2.meta[\"proxy\"], \"https://b.example\")\n+\n+            response2 = self.get_response(request2, \"http://example.com\")\n+            request3 = redirect_mw.process_response(request2, response2, spider)\n+\n+            self.assertIsInstance(request3, Request)\n+            self.assertNotIn(\"Proxy-Authorization\", request3.headers)\n+            self.assertNotIn(\"_auth_proxy\", request3.meta)\n+            self.assertNotIn(\"proxy\", request3.meta)\n+\n+            proxy_mw.process_request(request3, spider)\n+\n+            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n+\n+        def test_system_proxy_proxied_http_to_unproxied_https(self):\n+            crawler = get_crawler()\n+            redirect_mw = self.mwcls.from_crawler(crawler)\n+            env = {\n+                \"http_proxy\": \"https://a:@a.example\",\n+            }\n+            with set_environ(**env):\n+                proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n+\n+            request1 = Request(\"http://example.com\")\n+            spider = None\n+            proxy_mw.process_request(request1, spider)\n+\n+            self.assertEqual(request1.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request1.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request1.meta[\"proxy\"], \"https://a.example\")\n+\n+            response1 = self.get_response(request1, \"https://example.com\")\n+            request2 = redirect_mw.process_response(request1, response1, spider)\n+\n+            self.assertIsInstance(request2, Request)\n+            self.assertNotIn(\"Proxy-Authorization\", request2.headers)\n+            self.assertNotIn(\"_auth_proxy\", request2.meta)\n+            self.assertNotIn(\"proxy\", request2.meta)\n+\n+            proxy_mw.process_request(request2, spider)\n+\n+            self.assertNotIn(\"Proxy-Authorization\", request2.headers)\n+            self.assertNotIn(\"_auth_proxy\", request2.meta)\n+            self.assertNotIn(\"proxy\", request2.meta)\n+\n+            response2 = self.get_response(request2, \"http://example.com\")\n+            request3 = redirect_mw.process_response(request2, response2, spider)\n+\n+            self.assertIsInstance(request3, Request)\n+            self.assertNotIn(\"Proxy-Authorization\", request3.headers)\n+            self.assertNotIn(\"_auth_proxy\", request3.meta)\n+            self.assertNotIn(\"proxy\", request3.meta)\n+\n+            proxy_mw.process_request(request3, spider)\n+\n+            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request3.meta[\"proxy\"], \"https://a.example\")\n+\n+        def test_system_proxy_unproxied_http_to_proxied_https(self):\n+            crawler = get_crawler()\n+            redirect_mw = self.mwcls.from_crawler(crawler)\n+            env = {\n+                \"https_proxy\": \"https://b:@b.example\",\n+            }\n+            with set_environ(**env):\n+                proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n+\n+            request1 = Request(\"http://example.com\")\n+            spider = None\n+            proxy_mw.process_request(request1, spider)\n+\n+            self.assertNotIn(\"Proxy-Authorization\", request1.headers)\n+            self.assertNotIn(\"_auth_proxy\", request1.meta)\n+            self.assertNotIn(\"proxy\", request1.meta)\n+\n+            response1 = self.get_response(request1, \"https://example.com\")\n+            request2 = redirect_mw.process_response(request1, response1, spider)\n+\n+            self.assertIsInstance(request2, Request)\n+            self.assertNotIn(\"Proxy-Authorization\", request2.headers)\n+            self.assertNotIn(\"_auth_proxy\", request2.meta)\n+            self.assertNotIn(\"proxy\", request2.meta)\n+\n+            proxy_mw.process_request(request2, spider)\n+\n+            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic Yjo=\")\n+            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://b.example\")\n+            self.assertEqual(request2.meta[\"proxy\"], \"https://b.example\")\n+\n+            response2 = self.get_response(request2, \"http://example.com\")\n+            request3 = redirect_mw.process_response(request2, response2, spider)\n+\n+            self.assertIsInstance(request3, Request)\n+            self.assertNotIn(\"Proxy-Authorization\", request3.headers)\n+            self.assertNotIn(\"_auth_proxy\", request3.meta)\n+            self.assertNotIn(\"proxy\", request3.meta)\n+\n+            proxy_mw.process_request(request3, spider)\n+\n+            self.assertNotIn(\"Proxy-Authorization\", request3.headers)\n+            self.assertNotIn(\"_auth_proxy\", request3.meta)\n+            self.assertNotIn(\"proxy\", request3.meta)\n+\n+        def test_system_proxy_unproxied_http_to_unproxied_https(self):\n+            crawler = get_crawler()\n+            redirect_mw = self.mwcls.from_crawler(crawler)\n+            proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n+\n+            request1 = Request(\"http://example.com\")\n+            spider = None\n+            proxy_mw.process_request(request1, spider)\n+\n+            self.assertNotIn(\"Proxy-Authorization\", request1.headers)\n+            self.assertNotIn(\"_auth_proxy\", request1.meta)\n+            self.assertNotIn(\"proxy\", request1.meta)\n+\n+            response1 = self.get_response(request1, \"https://example.com\")\n+            request2 = redirect_mw.process_response(request1, response1, spider)\n+\n+            self.assertIsInstance(request2, Request)\n+            self.assertNotIn(\"Proxy-Authorization\", request2.headers)\n+            self.assertNotIn(\"_auth_proxy\", request2.meta)\n+            self.assertNotIn(\"proxy\", request2.meta)\n+\n+            proxy_mw.process_request(request2, spider)\n+\n+            self.assertNotIn(\"Proxy-Authorization\", request2.headers)\n+            self.assertNotIn(\"_auth_proxy\", request2.meta)\n+            self.assertNotIn(\"proxy\", request2.meta)\n+\n+            response2 = self.get_response(request2, \"http://example.com\")\n+            request3 = redirect_mw.process_response(request2, response2, spider)\n+\n+            self.assertIsInstance(request3, Request)\n+            self.assertNotIn(\"Proxy-Authorization\", request3.headers)\n+            self.assertNotIn(\"_auth_proxy\", request3.meta)\n+            self.assertNotIn(\"proxy\", request3.meta)\n+\n+            proxy_mw.process_request(request3, spider)\n+\n+            self.assertNotIn(\"Proxy-Authorization\", request3.headers)\n+            self.assertNotIn(\"_auth_proxy\", request3.meta)\n+            self.assertNotIn(\"proxy\", request3.meta)\n+\n+        def test_system_proxy_proxied_https_to_proxied_http(self):\n+            crawler = get_crawler()\n+            redirect_mw = self.mwcls.from_crawler(crawler)\n+            env = {\n+                \"http_proxy\": \"https://a:@a.example\",\n+                \"https_proxy\": \"https://b:@b.example\",\n+            }\n+            with set_environ(**env):\n+                proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n+\n+            request1 = Request(\"https://example.com\")\n+            spider = None\n+            proxy_mw.process_request(request1, spider)\n+\n+            self.assertEqual(request1.headers[\"Proxy-Authorization\"], b\"Basic Yjo=\")\n+            self.assertEqual(request1.meta[\"_auth_proxy\"], \"https://b.example\")\n+            self.assertEqual(request1.meta[\"proxy\"], \"https://b.example\")\n+\n+            response1 = self.get_response(request1, \"http://example.com\")\n+            request2 = redirect_mw.process_response(request1, response1, spider)\n+\n+            self.assertIsInstance(request2, Request)\n+            self.assertNotIn(\"Proxy-Authorization\", request2.headers)\n+            self.assertNotIn(\"_auth_proxy\", request2.meta)\n+            self.assertNotIn(\"proxy\", request2.meta)\n+\n+            proxy_mw.process_request(request2, spider)\n+\n+            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n+\n+            response2 = self.get_response(request2, \"https://example.com\")\n+            request3 = redirect_mw.process_response(request2, response2, spider)\n+\n+            self.assertIsInstance(request3, Request)\n+            self.assertNotIn(\"Proxy-Authorization\", request3.headers)\n+            self.assertNotIn(\"_auth_proxy\", request3.meta)\n+            self.assertNotIn(\"proxy\", request3.meta)\n+\n+            proxy_mw.process_request(request3, spider)\n+\n+            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic Yjo=\")\n+            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://b.example\")\n+            self.assertEqual(request3.meta[\"proxy\"], \"https://b.example\")\n+\n+        def test_system_proxy_proxied_https_to_unproxied_http(self):\n+            crawler = get_crawler()\n+            redirect_mw = self.mwcls.from_crawler(crawler)\n+            env = {\n+                \"https_proxy\": \"https://b:@b.example\",\n+            }\n+            with set_environ(**env):\n+                proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n+\n+            request1 = Request(\"https://example.com\")\n+            spider = None\n+            proxy_mw.process_request(request1, spider)\n+\n+            self.assertEqual(request1.headers[\"Proxy-Authorization\"], b\"Basic Yjo=\")\n+            self.assertEqual(request1.meta[\"_auth_proxy\"], \"https://b.example\")\n+            self.assertEqual(request1.meta[\"proxy\"], \"https://b.example\")\n+\n+            response1 = self.get_response(request1, \"http://example.com\")\n+            request2 = redirect_mw.process_response(request1, response1, spider)\n+\n+            self.assertIsInstance(request2, Request)\n+            self.assertNotIn(\"Proxy-Authorization\", request2.headers)\n+            self.assertNotIn(\"_auth_proxy\", request2.meta)\n+            self.assertNotIn(\"proxy\", request2.meta)\n+\n+            proxy_mw.process_request(request2, spider)\n+\n+            self.assertNotIn(\"Proxy-Authorization\", request2.headers)\n+            self.assertNotIn(\"_auth_proxy\", request2.meta)\n+            self.assertNotIn(\"proxy\", request2.meta)\n+\n+            response2 = self.get_response(request2, \"https://example.com\")\n+            request3 = redirect_mw.process_response(request2, response2, spider)\n+\n+            self.assertIsInstance(request3, Request)\n+            self.assertNotIn(\"Proxy-Authorization\", request3.headers)\n+            self.assertNotIn(\"_auth_proxy\", request3.meta)\n+            self.assertNotIn(\"proxy\", request3.meta)\n+\n+            proxy_mw.process_request(request3, spider)\n+\n+            self.assertEqual(request3.headers[\"Proxy-Authorization\"], b\"Basic Yjo=\")\n+            self.assertEqual(request3.meta[\"_auth_proxy\"], \"https://b.example\")\n+            self.assertEqual(request3.meta[\"proxy\"], \"https://b.example\")\n+\n+        def test_system_proxy_unproxied_https_to_proxied_http(self):\n+            crawler = get_crawler()\n+            redirect_mw = self.mwcls.from_crawler(crawler)\n+            env = {\n+                \"http_proxy\": \"https://a:@a.example\",\n+            }\n+            with set_environ(**env):\n+                proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n+\n+            request1 = Request(\"https://example.com\")\n+            spider = None\n+            proxy_mw.process_request(request1, spider)\n+\n+            self.assertNotIn(\"Proxy-Authorization\", request1.headers)\n+            self.assertNotIn(\"_auth_proxy\", request1.meta)\n+            self.assertNotIn(\"proxy\", request1.meta)\n+\n+            response1 = self.get_response(request1, \"http://example.com\")\n+            request2 = redirect_mw.process_response(request1, response1, spider)\n+\n+            self.assertIsInstance(request2, Request)\n+            self.assertNotIn(\"Proxy-Authorization\", request2.headers)\n+            self.assertNotIn(\"_auth_proxy\", request2.meta)\n+            self.assertNotIn(\"proxy\", request2.meta)\n+\n+            proxy_mw.process_request(request2, spider)\n+\n+            self.assertEqual(request2.headers[\"Proxy-Authorization\"], b\"Basic YTo=\")\n+            self.assertEqual(request2.meta[\"_auth_proxy\"], \"https://a.example\")\n+            self.assertEqual(request2.meta[\"proxy\"], \"https://a.example\")\n+\n+            response2 = self.get_response(request2, \"https://example.com\")\n+            request3 = redirect_mw.process_response(request2, response2, spider)\n+\n+            self.assertIsInstance(request3, Request)\n+            self.assertNotIn(\"Proxy-Authorization\", request3.headers)\n+            self.assertNotIn(\"_auth_proxy\", request3.meta)\n+            self.assertNotIn(\"proxy\", request3.meta)\n+\n+            proxy_mw.process_request(request3, spider)\n+\n+            self.assertNotIn(\"Proxy-Authorization\", request3.headers)\n+            self.assertNotIn(\"_auth_proxy\", request3.meta)\n+            self.assertNotIn(\"proxy\", request3.meta)\n+\n+        def test_system_proxy_unproxied_https_to_unproxied_http(self):\n+            crawler = get_crawler()\n+            redirect_mw = self.mwcls.from_crawler(crawler)\n+            proxy_mw = HttpProxyMiddleware.from_crawler(crawler)\n+\n+            request1 = Request(\"https://example.com\")\n+            spider = None\n+            proxy_mw.process_request(request1, spider)\n+\n+            self.assertNotIn(\"Proxy-Authorization\", request1.headers)\n+            self.assertNotIn(\"_auth_proxy\", request1.meta)\n+            self.assertNotIn(\"proxy\", request1.meta)\n+\n+            response1 = self.get_response(request1, \"http://example.com\")\n+            request2 = redirect_mw.process_response(request1, response1, spider)\n+\n+            self.assertIsInstance(request2, Request)\n+            self.assertNotIn(\"Proxy-Authorization\", request2.headers)\n+            self.assertNotIn(\"_auth_proxy\", request2.meta)\n+            self.assertNotIn(\"proxy\", request2.meta)\n+\n+            proxy_mw.process_request(request2, spider)\n+\n+            self.assertNotIn(\"Proxy-Authorization\", request2.headers)\n+            self.assertNotIn(\"_auth_proxy\", request2.meta)\n+            self.assertNotIn(\"proxy\", request2.meta)\n+\n+            response2 = self.get_response(request2, \"https://example.com\")\n+            request3 = redirect_mw.process_response(request2, response2, spider)\n+\n+            self.assertIsInstance(request3, Request)\n+            self.assertNotIn(\"Proxy-Authorization\", request3.headers)\n+            self.assertNotIn(\"_auth_proxy\", request3.meta)\n+            self.assertNotIn(\"proxy\", request3.meta)\n+\n+            proxy_mw.process_request(request3, spider)\n+\n+            self.assertNotIn(\"Proxy-Authorization\", request3.headers)\n+            self.assertNotIn(\"_auth_proxy\", request3.meta)\n+            self.assertNotIn(\"proxy\", request3.meta)\n+\n+\n+class RedirectMiddlewareTest(Base.Test):\n+    mwcls = RedirectMiddleware\n+    reason = 302\n+\n     def setUp(self):\n         self.crawler = get_crawler(Spider)\n         self.spider = self.crawler._create_spider(\"foo\")\n-        self.mw = RedirectMiddleware.from_crawler(self.crawler)\n+        self.mw = self.mwcls.from_crawler(self.crawler)\n \n-    def test_priority_adjust(self):\n-        req = Request(\"http://a.com\")\n-        rsp = Response(\n-            \"http://a.com\", headers={\"Location\": \"http://a.com/redirected\"}, status=301\n-        )\n-        req2 = self.mw.process_response(req, rsp, self.spider)\n-        assert req2.priority > req.priority\n+    def get_response(self, request, location, status=302):\n+        headers = {\"Location\": location}\n+        return Response(request.url, status=status, headers=headers)\n \n     def test_redirect_3xx_permanent(self):\n         def _test(method, status=301):\n@@ -52,51 +1064,6 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         _test(\"POST\", status=308)\n         _test(\"HEAD\", status=308)\n \n-    def test_dont_redirect(self):\n-        url = \"http://www.example.com/301\"\n-        url2 = \"http://www.example.com/redirected\"\n-        req = Request(url, meta={\"dont_redirect\": True})\n-        rsp = Response(url, headers={\"Location\": url2}, status=301)\n-\n-        r = self.mw.process_response(req, rsp, self.spider)\n-        assert isinstance(r, Response)\n-        assert r is rsp\n-\n-        # Test that it redirects when dont_redirect is False\n-        req = Request(url, meta={\"dont_redirect\": False})\n-        rsp = Response(url2, status=200)\n-\n-        r = self.mw.process_response(req, rsp, self.spider)\n-        assert isinstance(r, Response)\n-        assert r is rsp\n-\n-    def test_redirect_302(self):\n-        url = \"http://www.example.com/302\"\n-        url2 = \"http://www.example.com/redirected2\"\n-        req = Request(\n-            url,\n-            method=\"POST\",\n-            body=\"test\",\n-            headers={\"Content-Type\": \"text/plain\", \"Content-length\": \"4\"},\n-        )\n-        rsp = Response(url, headers={\"Location\": url2}, status=302)\n-\n-        req2 = self.mw.process_response(req, rsp, self.spider)\n-        assert isinstance(req2, Request)\n-        self.assertEqual(req2.url, url2)\n-        self.assertEqual(req2.method, \"GET\")\n-        assert (\n-            \"Content-Type\" not in req2.headers\n-        ), \"Content-Type header must not be present in redirected request\"\n-        assert (\n-            \"Content-Length\" not in req2.headers\n-        ), \"Content-Length header must not be present in redirected request\"\n-        assert not req2.body, f\"Redirected body must be empty, not '{req2.body}'\"\n-\n-        # response without Location header but with status code is 3XX should be ignored\n-        del rsp.headers[\"Location\"]\n-        assert self.mw.process_response(req, rsp, self.spider) is rsp\n-\n     def test_redirect_302_head(self):\n         url = \"http://www.example.com/302\"\n         url2 = \"http://www.example.com/redirected2\"\n@@ -108,10 +1075,6 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         self.assertEqual(req2.url, url2)\n         self.assertEqual(req2.method, \"HEAD\")\n \n-        # response without Location header but with status code is 3XX should be ignored\n-        del rsp.headers[\"Location\"]\n-        assert self.mw.process_response(req, rsp, self.spider) is rsp\n-\n     def test_redirect_302_relative(self):\n         url = \"http://www.example.com/302\"\n         url2 = \"///i8n.example2.com/302\"\n@@ -124,81 +1087,6 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         self.assertEqual(req2.url, url3)\n         self.assertEqual(req2.method, \"HEAD\")\n \n-        # response without Location header but with status code is 3XX should be ignored\n-        del rsp.headers[\"Location\"]\n-        assert self.mw.process_response(req, rsp, self.spider) is rsp\n-\n-    def test_max_redirect_times(self):\n-        self.mw.max_redirect_times = 1\n-        req = Request(\"http://scrapytest.org/302\")\n-        rsp = Response(\n-            \"http://scrapytest.org/302\", headers={\"Location\": \"/redirected\"}, status=302\n-        )\n-\n-        req = self.mw.process_response(req, rsp, self.spider)\n-        assert isinstance(req, Request)\n-        assert \"redirect_times\" in req.meta\n-        self.assertEqual(req.meta[\"redirect_times\"], 1)\n-        self.assertRaises(\n-            IgnoreRequest, self.mw.process_response, req, rsp, self.spider\n-        )\n-\n-    def test_ttl(self):\n-        self.mw.max_redirect_times = 100\n-        req = Request(\"http://scrapytest.org/302\", meta={\"redirect_ttl\": 1})\n-        rsp = Response(\n-            \"http://www.scrapytest.org/302\",\n-            headers={\"Location\": \"/redirected\"},\n-            status=302,\n-        )\n-\n-        req = self.mw.process_response(req, rsp, self.spider)\n-        assert isinstance(req, Request)\n-        self.assertRaises(\n-            IgnoreRequest, self.mw.process_response, req, rsp, self.spider\n-        )\n-\n-    def test_redirect_urls(self):\n-        req1 = Request(\"http://scrapytest.org/first\")\n-        rsp1 = Response(\n-            \"http://scrapytest.org/first\",\n-            headers={\"Location\": \"/redirected\"},\n-            status=302,\n-        )\n-        req2 = self.mw.process_response(req1, rsp1, self.spider)\n-        rsp2 = Response(\n-            \"http://scrapytest.org/redirected\",\n-            headers={\"Location\": \"/redirected2\"},\n-            status=302,\n-        )\n-        req3 = self.mw.process_response(req2, rsp2, self.spider)\n-\n-        self.assertEqual(req2.url, \"http://scrapytest.org/redirected\")\n-        self.assertEqual(req2.meta[\"redirect_urls\"], [\"http://scrapytest.org/first\"])\n-        self.assertEqual(req3.url, \"http://scrapytest.org/redirected2\")\n-        self.assertEqual(\n-            req3.meta[\"redirect_urls\"],\n-            [\"http://scrapytest.org/first\", \"http://scrapytest.org/redirected\"],\n-        )\n-\n-    def test_redirect_reasons(self):\n-        req1 = Request(\"http://scrapytest.org/first\")\n-        rsp1 = Response(\n-            \"http://scrapytest.org/first\",\n-            headers={\"Location\": \"/redirected1\"},\n-            status=301,\n-        )\n-        req2 = self.mw.process_response(req1, rsp1, self.spider)\n-        rsp2 = Response(\n-            \"http://scrapytest.org/redirected1\",\n-            headers={\"Location\": \"/redirected2\"},\n-            status=301,\n-        )\n-        req3 = self.mw.process_response(req2, rsp2, self.spider)\n-\n-        self.assertEqual(req2.meta[\"redirect_reasons\"], [301])\n-        self.assertEqual(req3.meta[\"redirect_reasons\"], [301, 301])\n-\n     def test_spider_handling(self):\n         smartspider = self.crawler._create_spider(\"smarty\")\n         smartspider.handle_httpstatus_list = [404, 301, 302]\n@@ -247,53 +1135,84 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         perc_encoded_utf8_url = \"http://scrapytest.org/a%C3%A7%C3%A3o\"\n         self.assertEqual(perc_encoded_utf8_url, req_result.url)\n \n-    def test_cross_domain_header_dropping(self):\n-        safe_headers = {\"A\": \"B\"}\n-        original_request = Request(\n-            \"https://example.com\",\n-            headers={\"Cookie\": \"a=b\", \"Authorization\": \"a\", **safe_headers},\n-        )\n+    def test_no_location(self):\n+        request = Request(\"https://example.com\")\n+        response = Response(request.url, status=302)\n+        assert self.mw.process_response(request, response, self.spider) is response\n \n-        internal_response = Response(\n-            \"https://example.com\",\n-            headers={\"Location\": \"https://example.com/a\"},\n-            status=301,\n-        )\n-        internal_redirect_request = self.mw.process_response(\n-            original_request, internal_response, self.spider\n-        )\n-        self.assertIsInstance(internal_redirect_request, Request)\n-        self.assertEqual(original_request.headers, internal_redirect_request.headers)\n \n-        external_response = Response(\n-            \"https://example.com\",\n-            headers={\"Location\": \"https://example.org/a\"},\n-            status=301,\n+SCHEME_PARAMS = (\"url\", \"location\", \"target\")\n+HTTP_SCHEMES = (\"http\", \"https\")\n+NON_HTTP_SCHEMES = (\"data\", \"file\", \"ftp\", \"s3\", \"foo\")\n+REDIRECT_SCHEME_CASES = (\n+    # http/https → http/https redirects\n+    *(\n+        (\n+            f\"{input_scheme}://example.com/a\",\n+            f\"{output_scheme}://example.com/b\",\n+            f\"{output_scheme}://example.com/b\",\n         )\n-        external_redirect_request = self.mw.process_response(\n-            original_request, external_response, self.spider\n+        for input_scheme, output_scheme in product(HTTP_SCHEMES, repeat=2)\n+    ),\n+    # http/https → data/file/ftp/s3/foo does not redirect\n+    *(\n+        (\n+            f\"{input_scheme}://example.com/a\",\n+            f\"{output_scheme}://example.com/b\",\n+            None,\n         )\n-        self.assertIsInstance(external_redirect_request, Request)\n-        self.assertEqual(\n-            safe_headers, external_redirect_request.headers.to_unicode_dict()\n+        for input_scheme in HTTP_SCHEMES\n+        for output_scheme in NON_HTTP_SCHEMES\n+    ),\n+    # http/https → relative redirects\n+    *(\n+        (\n+            f\"{scheme}://example.com/a\",\n+            location,\n+            f\"{scheme}://example.com/b\",\n+        )\n+        for scheme in HTTP_SCHEMES\n+        for location in (\"//example.com/b\", \"/b\")\n+    ),\n+    # Note: We do not test data/file/ftp/s3 schemes for the initial URL\n+    # because their download handlers cannot return a status code of 3xx.\n )\n \n \n-class MetaRefreshMiddlewareTest(unittest.TestCase):\n-    def setUp(self):\n+@pytest.mark.parametrize(SCHEME_PARAMS, REDIRECT_SCHEME_CASES)\n+def test_redirect_schemes(url, location, target):\n     crawler = get_crawler(Spider)\n-        self.spider = crawler._create_spider(\"foo\")\n-        self.mw = MetaRefreshMiddleware.from_crawler(crawler)\n+    spider = crawler._create_spider(\"foo\")\n+    mw = RedirectMiddleware.from_crawler(crawler)\n+    request = Request(url)\n+    response = Response(url, headers={\"Location\": location}, status=301)\n+    redirect = mw.process_response(request, response, spider)\n+    if target is None:\n+        assert redirect == response\n+    else:\n+        assert isinstance(redirect, Request)\n+        assert redirect.url == target\n \n-    def _body(self, interval=5, url=\"http://example.org/newpage\"):\n+\n+def meta_refresh_body(url, interval=5):\n     html = f\"\"\"<html><head><meta http-equiv=\"refresh\" content=\"{interval};url={url}\"/></head></html>\"\"\"\n     return html.encode(\"utf-8\")\n \n-    def test_priority_adjust(self):\n-        req = Request(\"http://a.com\")\n-        rsp = HtmlResponse(req.url, body=self._body())\n-        req2 = self.mw.process_response(req, rsp, self.spider)\n-        assert req2.priority > req.priority\n+\n+class MetaRefreshMiddlewareTest(Base.Test):\n+    mwcls = MetaRefreshMiddleware\n+    reason = \"meta refresh\"\n+\n+    def setUp(self):\n+        crawler = get_crawler(Spider)\n+        self.spider = crawler._create_spider(\"foo\")\n+        self.mw = self.mwcls.from_crawler(crawler)\n+\n+    def _body(self, interval=5, url=\"http://example.org/newpage\"):\n+        return meta_refresh_body(url, interval)\n+\n+    def get_response(self, request, location):\n+        return HtmlResponse(request.url, body=self._body(url=location))\n \n     def test_meta_refresh(self):\n         req = Request(url=\"http://example.org\")\n@@ -332,62 +1251,6 @@ class MetaRefreshMiddlewareTest(unittest.TestCase):\n         ), \"Content-Length header must not be present in redirected request\"\n         assert not req2.body, f\"Redirected body must be empty, not '{req2.body}'\"\n \n-    def test_max_redirect_times(self):\n-        self.mw.max_redirect_times = 1\n-        req = Request(\"http://scrapytest.org/max\")\n-        rsp = HtmlResponse(req.url, body=self._body())\n-\n-        req = self.mw.process_response(req, rsp, self.spider)\n-        assert isinstance(req, Request)\n-        assert \"redirect_times\" in req.meta\n-        self.assertEqual(req.meta[\"redirect_times\"], 1)\n-        self.assertRaises(\n-            IgnoreRequest, self.mw.process_response, req, rsp, self.spider\n-        )\n-\n-    def test_ttl(self):\n-        self.mw.max_redirect_times = 100\n-        req = Request(\"http://scrapytest.org/302\", meta={\"redirect_ttl\": 1})\n-        rsp = HtmlResponse(req.url, body=self._body())\n-\n-        req = self.mw.process_response(req, rsp, self.spider)\n-        assert isinstance(req, Request)\n-        self.assertRaises(\n-            IgnoreRequest, self.mw.process_response, req, rsp, self.spider\n-        )\n-\n-    def test_redirect_urls(self):\n-        req1 = Request(\"http://scrapytest.org/first\")\n-        rsp1 = HtmlResponse(req1.url, body=self._body(url=\"/redirected\"))\n-        req2 = self.mw.process_response(req1, rsp1, self.spider)\n-        assert isinstance(req2, Request), req2\n-        rsp2 = HtmlResponse(req2.url, body=self._body(url=\"/redirected2\"))\n-        req3 = self.mw.process_response(req2, rsp2, self.spider)\n-        assert isinstance(req3, Request), req3\n-        self.assertEqual(req2.url, \"http://scrapytest.org/redirected\")\n-        self.assertEqual(req2.meta[\"redirect_urls\"], [\"http://scrapytest.org/first\"])\n-        self.assertEqual(req3.url, \"http://scrapytest.org/redirected2\")\n-        self.assertEqual(\n-            req3.meta[\"redirect_urls\"],\n-            [\"http://scrapytest.org/first\", \"http://scrapytest.org/redirected\"],\n-        )\n-\n-    def test_redirect_reasons(self):\n-        req1 = Request(\"http://scrapytest.org/first\")\n-        rsp1 = HtmlResponse(\n-            \"http://scrapytest.org/first\", body=self._body(url=\"/redirected\")\n-        )\n-        req2 = self.mw.process_response(req1, rsp1, self.spider)\n-        rsp2 = HtmlResponse(\n-            \"http://scrapytest.org/redirected\", body=self._body(url=\"/redirected1\")\n-        )\n-        req3 = self.mw.process_response(req2, rsp2, self.spider)\n-\n-        self.assertEqual(req2.meta[\"redirect_reasons\"], [\"meta refresh\"])\n-        self.assertEqual(\n-            req3.meta[\"redirect_reasons\"], [\"meta refresh\", \"meta refresh\"]\n-        )\n-\n     def test_ignore_tags_default(self):\n         req = Request(url=\"http://example.org\")\n         body = (\n@@ -413,5 +1276,45 @@ class MetaRefreshMiddlewareTest(unittest.TestCase):\n         assert isinstance(response, Response)\n \n \n+@pytest.mark.parametrize(\n+    SCHEME_PARAMS,\n+    (\n+        *REDIRECT_SCHEME_CASES,\n+        # data/file/ftp/s3/foo → * does not redirect\n+        *(\n+            (\n+                f\"{input_scheme}://example.com/a\",\n+                f\"{output_scheme}://example.com/b\",\n+                None,\n+            )\n+            for input_scheme in NON_HTTP_SCHEMES\n+            for output_scheme in chain(HTTP_SCHEMES, NON_HTTP_SCHEMES)\n+        ),\n+        # data/file/ftp/s3/foo → relative does not redirect\n+        *(\n+            (\n+                f\"{scheme}://example.com/a\",\n+                location,\n+                None,\n+            )\n+            for scheme in NON_HTTP_SCHEMES\n+            for location in (\"//example.com/b\", \"/b\")\n+        ),\n+    ),\n+)\n+def test_meta_refresh_schemes(url, location, target):\n+    crawler = get_crawler(Spider)\n+    spider = crawler._create_spider(\"foo\")\n+    mw = MetaRefreshMiddleware.from_crawler(crawler)\n+    request = Request(url)\n+    response = HtmlResponse(url, body=meta_refresh_body(location))\n+    redirect = mw.process_response(request, response, spider)\n+    if target is None:\n+        assert redirect == response\n+    else:\n+        assert isinstance(redirect, Request)\n+        assert redirect.url == target\n+\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -15,8 +15,10 @@ import subprocess\n import sys\n from collections import defaultdict\n from dataclasses import dataclass\n+from logging import DEBUG\n from pathlib import Path\n from threading import Timer\n+from unittest.mock import Mock\n from urllib.parse import urlparse\n \n import attr\n@@ -27,11 +29,13 @@ from twisted.trial import unittest\n from twisted.web import server, static, util\n \n from scrapy import signals\n-from scrapy.core.engine import ExecutionEngine\n-from scrapy.exceptions import CloseSpider\n+from scrapy.core.engine import ExecutionEngine, Slot\n+from scrapy.core.scheduler import BaseScheduler\n+from scrapy.exceptions import CloseSpider, IgnoreRequest\n from scrapy.http import Request\n from scrapy.item import Field, Item\n from scrapy.linkextractors import LinkExtractor\n+from scrapy.signals import request_scheduled\n from scrapy.spiders import Spider\n from scrapy.utils.signal import disconnect_all\n from scrapy.utils.test import get_crawler\n@@ -467,6 +471,38 @@ class EngineTest(unittest.TestCase):\n         self.assertNotIn(b\"Traceback\", stderr)\n \n \n+def test_request_scheduled_signal(caplog):\n+    class TestScheduler(BaseScheduler):\n+        def __init__(self):\n+            self.enqueued = []\n+\n+        def enqueue_request(self, request: Request) -> bool:\n+            self.enqueued.append(request)\n+            return True\n+\n+    def signal_handler(request: Request, spider: Spider) -> None:\n+        if \"drop\" in request.url:\n+            raise IgnoreRequest\n+\n+    spider = TestSpider()\n+    crawler = get_crawler(spider.__class__)\n+    engine = ExecutionEngine(crawler, lambda _: None)\n+    engine.downloader._slot_gc_loop.stop()\n+    scheduler = TestScheduler()\n+    engine.slot = Slot((), None, Mock(), scheduler)\n+    crawler.signals.connect(signal_handler, request_scheduled)\n+    keep_request = Request(\"https://keep.example\")\n+    engine._schedule_request(keep_request, spider)\n+    drop_request = Request(\"https://drop.example\")\n+    caplog.set_level(DEBUG)\n+    engine._schedule_request(drop_request, spider)\n+    assert scheduler.enqueued == [\n+        keep_request\n+    ], f\"{scheduler.enqueued!r} != [{keep_request!r}]\"\n+    assert \"dropped request <GET https://drop.example>\" in caplog.text\n+    crawler.signals.disconnect(signal_handler, request_scheduled)\n+\n+\n if __name__ == \"__main__\":\n     if len(sys.argv) > 1 and sys.argv[1] == \"runserver\":\n         start_test_site(debug=True)\n\n@@ -6,6 +6,7 @@ import unittest\n import warnings\n from pathlib import Path\n \n+from scrapy.utils.misc import set_environ\n from scrapy.utils.project import data_path, get_project_settings\n \n \n@@ -38,20 +39,6 @@ class ProjectUtilsTest(unittest.TestCase):\n             self.assertEqual(abspath, data_path(abspath))\n \n \n-@contextlib.contextmanager\n-def set_env(**update):\n-    modified = set(update.keys()) & set(os.environ.keys())\n-    update_after = {k: os.environ[k] for k in modified}\n-    remove_after = frozenset(k for k in update if k not in os.environ)\n-    try:\n-        os.environ.update(update)\n-        yield\n-    finally:\n-        os.environ.update(update_after)\n-        for k in remove_after:\n-            os.environ.pop(k)\n-\n-\n class GetProjectSettingsTestCase(unittest.TestCase):\n     def test_valid_envvar(self):\n         value = \"tests.test_cmdline.settings\"\n@@ -60,7 +47,7 @@ class GetProjectSettingsTestCase(unittest.TestCase):\n         }\n         with warnings.catch_warnings():\n             warnings.simplefilter(\"error\")\n-            with set_env(**envvars):\n+            with set_environ(**envvars):\n                 settings = get_project_settings()\n \n         assert settings.get(\"SETTINGS_MODULE\") == value\n@@ -69,7 +56,7 @@ class GetProjectSettingsTestCase(unittest.TestCase):\n         envvars = {\n             \"SCRAPY_FOO\": \"bar\",\n         }\n-        with set_env(**envvars):\n+        with set_environ(**envvars):\n             settings = get_project_settings()\n \n         assert settings.get(\"SCRAPY_FOO\") is None\n@@ -80,7 +67,7 @@ class GetProjectSettingsTestCase(unittest.TestCase):\n             \"SCRAPY_FOO\": \"bar\",\n             \"SCRAPY_SETTINGS_MODULE\": value,\n         }\n-        with set_env(**envvars):\n+        with set_environ(**envvars):\n             settings = get_project_settings()\n         assert settings.get(\"SETTINGS_MODULE\") == value\n         assert settings.get(\"SCRAPY_FOO\") is None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
