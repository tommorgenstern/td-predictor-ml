{"custom_id": "scrapy#a611f8dd2dd9fb9f7fe81245a488c4493744014d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 263 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -57,16 +57,28 @@ DEFAULT_LOGGING = {\n \n \n def configure_logging(settings=None, install_root_handler=True):\n-    \"\"\"Initialize and configure default loggers\n+    \"\"\"\n+    Initialize logging defaults for Scrapy.\n+\n+    :param settings: settings used to create and configure a handler for the\n+        root logger (default: None).\n+    :type settings: dict, :class:`~scrapy.settings.Settings` object or ``None``\n+\n+    :param install_root_handler: whether to install root logging handler\n+        (default: True)\n+    :type install_root_handler: bool\n \n     This function does:\n+\n     - Route warnings and twisted logging through Python standard logging\n     - Assign DEBUG and ERROR level to Scrapy and Twisted loggers respectively\n     - Route stdout to log if LOG_STDOUT setting is True\n \n     When ``install_root_handler`` is True (default), this function also\n-      - Sets FailureFormatter filter on Scrapy logger\n-      - Creates a handler for the root logger according to given settings\n+    creates a handler for the root logger according to given settings\n+    (see :ref:`topics-logging-settings`). You can override default options\n+    using ``settings`` argument. When ``settings`` is empty or None, defaults\n+    are used.\n     \"\"\"\n     if not sys.warnoptions:\n         # Route warnings through python logging\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6c9daf3a952abbd53b653037b22a5e15b0b9e4cc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 111 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -46,10 +46,10 @@ class SignalManager(object):\n \n     def send_catch_log_deferred(self, signal, **kwargs):\n         \"\"\"\n-        Like :meth:`send_catch_log` but supports returning `deferred`_ from\n+        Like :meth:`send_catch_log` but supports returning `deferreds`_ from\n         signal handlers.\n \n-        Returns a `deferred`_ that gets fired once all signal handlers\n+        Returns a Deferred that gets fired once all signal handlers\n         deferreds were fired. Send a signal, catch exceptions and log them.\n \n         The keyword arguments are passed to the signal handlers (connected\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#877c25793a25bdf372a84d2c34bbed2754cd58f9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -12/0 | Churn Δ: 3 | Churn Cumulative: 62 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -302,6 +302,9 @@ class LeveldbCacheStorage(object):\n         self.db = self._leveldb.LevelDB(dbpath)\n \n     def close_spider(self, spider):\n+        # Do compactation each time to save space and also recreate files to\n+        # avoid them being removed in storages with timestamp-based autoremoval.\n+        self.db.CompactRange()\n         del self.db\n \n     def retrieve_response(self, spider, request):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#140f58251bf2292891e6bc6233589d3d0e944bb8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 40 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): -9/0 | Churn Δ: 40 | Churn Cumulative: 106 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,40 +0,0 @@\n-import sys, os, glob, shutil\n-from subprocess import check_call\n-from scrapy import version_info\n-\n-def build(suffix):\n-    for ifn in glob.glob(\"debian/scrapy.*\"):\n-        s = open(ifn).read()\n-        s = s.replace('SUFFIX', suffix)\n-        pre, suf = ifn.split('.', 1)\n-        ofn = \"%s-%s.%s\" % (pre, suffix, suf)\n-        with open(ofn, 'w') as of:\n-            of.write(s)\n-\n-    for ifn in ['debian/control', 'debian/changelog']:\n-        s = open(ifn).read()\n-        s = s.replace('SUFFIX', suffix)\n-        with open(ifn, 'w') as of:\n-            of.write(s)\n-\n-    check_call('debchange -m -D unstable --force-distribution -v $(python setup.py --version)+$(date +%s) \"Automatic build\"', \\\n-        shell=True)\n-    check_call('debuild -us -uc -b', shell=True)\n-\n-def clean(suffix):\n-    for f in glob.glob(\"debian/python-scrapy%s*\" % suffix):\n-        if os.path.isdir(f):\n-            shutil.rmtree(f)\n-        else:\n-            os.remove(f)\n-\n-def main():\n-    cmd = sys.argv[1]\n-    suffix = '%s.%s' % version_info[:2]\n-    if cmd == 'build':\n-        build(suffix)\n-    elif cmd == 'clean':\n-        clean(suffix)\n-\n-if __name__ == '__main__':\n-    main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f4dd8bcdc29cb5411d0fe2ed8f53e75883402ede", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 152 | Contributors (this commit): 7 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -16,7 +16,11 @@ from scrapy.utils.spider import spidercls_for_request, DefaultSpider\n class Command(ScrapyCommand):\n \n     requires_project = False\n-    default_settings = {'KEEP_ALIVE': True, 'LOGSTATS_INTERVAL': 0}\n+    default_settings = {\n+        'KEEP_ALIVE': True,\n+        'LOGSTATS_INTERVAL': 0,\n+        'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',\n+    }\n \n     def syntax(self):\n         return \"[url|file]\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eb8fed4ecd8fbb51822553534b8b2f3cc459de60", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 14/13 | Churn Δ: 10 | Churn Cumulative: 14 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -25,7 +25,7 @@ class HttpProxyMiddleware(object):\n         proxy_type, user, password, hostport = _parse_proxy(url)\n         proxy_url = urlunparse((proxy_type or orig_type, hostport, '', '', '', ''))\n \n-        if user and password:\n+        if user:\n             user_pass = '%s:%s' % (unquote(user), unquote(password))\n             creds = base64.b64encode(user_pass).strip()\n         else:\n\n@@ -54,6 +54,14 @@ class TestDefaultHeadersMiddleware(TestCase):\n         self.assertEquals(req.meta, {'proxy': 'https://proxy:3128'})\n         self.assertEquals(req.headers.get('Proxy-Authorization'), 'Basic dXNlcjpwYXNz')\n \n+    def test_proxy_auth_empty_passwd(self):\n+        os.environ['http_proxy'] = 'https://user:@proxy:3128'\n+        mw = HttpProxyMiddleware()\n+        req = Request('http://scrapytest.org')\n+        assert mw.process_request(req, spider) is None\n+        self.assertEquals(req.meta, {'proxy': 'https://proxy:3128'})\n+        self.assertEquals(req.headers.get('Proxy-Authorization'), 'Basic dXNlcjo=')\n+\n     def test_proxy_already_seted(self):\n         os.environ['http_proxy'] = http_proxy = 'https://proxy.for.http:3128'\n         mw = HttpProxyMiddleware()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8da9836db3d6071eff6d0267ed47a7af81b830d3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 733 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -17,10 +17,11 @@ from .handlers import DownloadHandlers\n class Slot(object):\n     \"\"\"Downloader slot\"\"\"\n \n-    def __init__(self, concurrency, delay, settings):\n+    def __init__(self, concurrency, delay, randomize_delay):\n         self.concurrency = concurrency\n         self.delay = delay\n-        self.randomize_delay = settings.getbool('RANDOMIZE_DOWNLOAD_DELAY')\n+        self.randomize_delay = randomize_delay\n+\n         self.active = set()\n         self.queue = deque()\n         self.transferring = set()\n@@ -66,6 +67,7 @@ class Downloader(object):\n         self.total_concurrency = self.settings.getint('CONCURRENT_REQUESTS')\n         self.domain_concurrency = self.settings.getint('CONCURRENT_REQUESTS_PER_DOMAIN')\n         self.ip_concurrency = self.settings.getint('CONCURRENT_REQUESTS_PER_IP')\n+        self.randomize_delay = self.settings.getbool('RANDOMIZE_DOWNLOAD_DELAY')\n         self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)\n         self._slot_gc_loop = task.LoopingCall(self._slot_gc)\n         self._slot_gc_loop.start(60)\n@@ -87,7 +89,7 @@ class Downloader(object):\n         if key not in self.slots:\n             conc = self.ip_concurrency if self.ip_concurrency else self.domain_concurrency\n             conc, delay = _get_concurrency_delay(conc, spider, self.settings)\n-            self.slots[key] = Slot(conc, delay, self.settings)\n+            self.slots[key] = Slot(conc, delay, self.randomize_delay)\n \n         return key, self.slots[key]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cb0445e8b884d350786fb6df829c9e00ade5036f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 18 | Churn Cumulative: 751 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,8 @@\n+from __future__ import absolute_import\n import random\n import warnings\n from time import time\n+from datetime import datetime\n from collections import deque\n \n from twisted.internet import reactor, defer, task\n@@ -8,7 +10,6 @@ from twisted.internet import reactor, defer, task\n from scrapy.utils.defer import mustbe_deferred\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.resolver import dnscache\n-from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy import signals\n from .middleware import DownloaderMiddlewareManager\n from .handlers import DownloadHandlers\n@@ -40,6 +41,21 @@ class Slot(object):\n         if self.latercall and self.latercall.active():\n             self.latercall.cancel()\n \n+    def __repr__(self):\n+        cls_name = self.__class__.__name__\n+        return \"%s(concurrency=%r, delay=%0.2f, randomize_delay=%r)\" % (\n+            cls_name, self.concurrency, self.delay, self.randomize_delay)\n+\n+    def __str__(self):\n+        return (\n+            \"<downloader.Slot concurrency=%r delay=%0.2f randomize_delay=%r \"\n+            \"len(active)=%d len(queue)=%d len(transferring)=%d lastseen=%s>\" % (\n+                self.concurrency, self.delay, self.randomize_delay,\n+                len(self.active), len(self.queue), len(self.transferring),\n+                datetime.fromtimestamp(self.lastseen).isoformat()\n+            )\n+        )\n+\n \n def _get_concurrency_delay(concurrency, spider, settings):\n     delay = settings.getfloat('DOWNLOAD_DELAY')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#48582be9a6caefa1cdf9e6c852f17fce903b553b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 88 | Contributors (this commit): 6 | Commits (past 90d): 5 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -108,7 +108,10 @@ html_theme = 'sphinx_rtd_theme'\n #html_theme_options = {}\n \n # Add any paths that contain custom themes here, relative to this directory.\n-#html_theme_path = []\n+# Add path to the RTD explicitly to robustify builds (otherwise might\n+# fail in a clean Debian build env)\n+import sphinx_rtd_theme\n+html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n \n \n # The style sheet to use for HTML and HTML Help pages. A file of that name\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4cea1c1231da1583fb177e976f473fa52b9ec450", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 22/22 | Churn Δ: 2 | Churn Cumulative: 2 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -78,7 +78,7 @@ class SelectJmes(object):\n \n     def __call__(self, value):\n         \"\"\"Query value for the jmespath query and return answer\n-        :param str value: a string with JSON data to extract from\n+        :param value: a data structure (dict, list) to extract from\n         :return: Element extracted according to jmespath query\n         \"\"\"\n         return self.compiled_path.search(value)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e6272e5209de38e2632d63ccb39e645b048c97b3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 17/17 | Churn Δ: 15 | Churn Cumulative: 15 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -68,13 +68,22 @@ class AutoThrottle(object):\n \n     def _adjust_delay(self, slot, latency, response):\n         \"\"\"Define delay adjustment policy\"\"\"\n+\n+        # Adjust the delay to be closer to latency.\n+        new_delay = (slot.delay + latency) / 2.0\n+\n         # If latency is bigger than old delay, then use latency instead of mean.\n-        # It works better with problematic sites\n-        new_delay = min(max(self.mindelay, latency, (slot.delay + latency) / 2.0), self.maxdelay)\n+        # It works better with problematic sites.\n+        new_delay = max(latency, new_delay)\n+\n+        # Make sure self.mindelay <= new_delay <= self.max_delay\n+        new_delay = min(max(self.mindelay, new_delay), self.maxdelay)\n \n         # Dont adjust delay if response status != 200 and new delay is smaller\n         # than old one, as error pages (and redirections) are usually small and\n         # so tend to reduce latency, thus provoking a positive feedback by\n         # reducing delay instead of increase.\n-        if response.status == 200 or new_delay > slot.delay:\n+        if response.status != 200 and new_delay <= slot.delay:\n+            return\n+\n         slot.delay = new_delay\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#584252e8f2f8577d37d648042d0a78a41af8e913", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 392 | Contributors (this commit): 20 | Commits (past 90d): 19 | Contributors (cumulative): 22 | DMM Complexity: None\n\nDIFF:\n@@ -33,10 +33,10 @@ class AutoThrottle(object):\n             s.getfloat('DOWNLOAD_DELAY')\n \n     def _max_delay(self, spider):\n-        return self.crawler.settings.getfloat('AUTOTHROTTLE_MAX_DELAY', 60.0)\n+        return self.crawler.settings.getfloat('AUTOTHROTTLE_MAX_DELAY')\n \n     def _start_delay(self, spider):\n-        return max(self.mindelay, self.crawler.settings.getfloat('AUTOTHROTTLE_START_DELAY', 5.0))\n+        return max(self.mindelay, self.crawler.settings.getfloat('AUTOTHROTTLE_START_DELAY'))\n \n     def _response_downloaded(self, response, request, spider):\n         key, slot = self._get_slot(request, spider)\n\n@@ -20,6 +20,11 @@ from os.path import join, abspath, dirname\n \n AJAXCRAWL_ENABLED = False\n \n+AUTOTHROTTLE_ENABLED = False\n+AUTOTHROTTLE_DEBUG = False\n+AUTOTHROTTLE_MAX_DELAY = 60.0\n+AUTOTHROTTLE_START_DELAY = 5.0\n+\n BOT_NAME = 'scrapybot'\n \n CLOSESPIDER_TIMEOUT = 0\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#17cb51315b875157d397719c2834d1d02f2ca49f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 4 | Churn Cumulative: 23 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -28,9 +28,7 @@ class AutoThrottle(object):\n \n     def _min_delay(self, spider):\n         s = self.crawler.settings\n-        return getattr(spider, 'download_delay', 0.0) or \\\n-            s.getfloat('AUTOTHROTTLE_MIN_DOWNLOAD_DELAY') or \\\n-            s.getfloat('DOWNLOAD_DELAY')\n+        return getattr(spider, 'download_delay', 0.0) or s.getfloat('DOWNLOAD_DELAY')\n \n     def _max_delay(self, spider):\n         return self.crawler.settings.getfloat('AUTOTHROTTLE_MAX_DELAY')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1fef9f113237630664ef05645127ca50662d71b6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 2 | Churn Cumulative: 25 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -28,7 +28,7 @@ class AutoThrottle(object):\n \n     def _min_delay(self, spider):\n         s = self.crawler.settings\n-        return getattr(spider, 'download_delay', 0.0) or s.getfloat('DOWNLOAD_DELAY')\n+        return getattr(spider, 'download_delay', s.getfloat('DOWNLOAD_DELAY'))\n \n     def _max_delay(self, spider):\n         return self.crawler.settings.getfloat('AUTOTHROTTLE_MAX_DELAY')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d850238c2220e2d082ea9805fa356b62d975697f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 413 | Contributors (this commit): 20 | Commits (past 90d): 23 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,6 +14,7 @@ class AutoThrottle(object):\n             raise NotConfigured\n \n         self.debug = crawler.settings.getbool(\"AUTOTHROTTLE_DEBUG\")\n+        self.target_concurrency = crawler.settings.getfloat(\"AUTOTHROTTLE_TARGET_CONCURRENCY\")\n         crawler.signals.connect(self._spider_opened, signal=signals.spider_opened)\n         crawler.signals.connect(self._response_downloaded, signal=signals.response_downloaded)\n \n@@ -67,12 +68,17 @@ class AutoThrottle(object):\n     def _adjust_delay(self, slot, latency, response):\n         \"\"\"Define delay adjustment policy\"\"\"\n \n-        # Adjust the delay to be closer to latency.\n-        new_delay = (slot.delay + latency) / 2.0\n+        # If a server needs `latency` seconds to respond then\n+        # we should send a request each `latency/N` seconds\n+        # to have N requests processed in parallel\n+        target_delay = latency / self.target_concurrency\n \n-        # If latency is bigger than old delay, then use latency instead of mean.\n+        # Adjust the delay to make it closer to target_delay\n+        new_delay = (slot.delay + target_delay) / 2.0\n+\n+        # If target delay is bigger than old delay, then use it instead of mean.\n         # It works better with problematic sites.\n-        new_delay = max(latency, new_delay)\n+        new_delay = max(target_delay, new_delay)\n \n         # Make sure self.mindelay <= new_delay <= self.max_delay\n         new_delay = min(max(self.mindelay, new_delay), self.maxdelay)\n\n@@ -24,6 +24,7 @@ AUTOTHROTTLE_ENABLED = False\n AUTOTHROTTLE_DEBUG = False\n AUTOTHROTTLE_MAX_DELAY = 60.0\n AUTOTHROTTLE_START_DELAY = 5.0\n+AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n \n BOT_NAME = 'scrapybot'\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9a15fcf89a151811de8ac783419df0512c863d5e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 13/13 | Churn Δ: 2 | Churn Cumulative: 45 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -24,6 +24,7 @@ class ResponseTypes(object):\n         'application/vnd.wap.xhtml+xml': 'scrapy.http.HtmlResponse',\n         'application/xml': 'scrapy.http.XmlResponse',\n         'application/json': 'scrapy.http.TextResponse',\n+        'application/x-json': 'scrapy.http.TextResponse',\n         'application/javascript': 'scrapy.http.TextResponse',\n         'application/x-javascript': 'scrapy.http.TextResponse',\n         'text/xml': 'scrapy.http.XmlResponse',\n\n@@ -35,6 +35,7 @@ class ResponseTypesTest(unittest.TestCase):\n             ('application/vnd.wap.xhtml+xml; charset=utf-8', HtmlResponse),\n             ('application/xml; charset=UTF-8', XmlResponse),\n             ('application/octet-stream', Response),\n+            ('application/x-json; encoding=UTF8;charset=UTF-8', TextResponse),\n         ]\n         for source, cls in mappings:\n             retcls = responsetypes.from_content_type(source)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d438075a47fe928ee9bbec7ca2defe79646899b4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 89 | Contributors (this commit): 7 | Commits (past 90d): 6 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -19,6 +19,7 @@ from os import path\n # absolute, like shown here.\n sys.path.append(path.join(path.dirname(__file__), \"_ext\"))\n sys.path.append(path.join(path.dirname(path.dirname(__file__)), \"scrapy\"))\n+sys.path.insert(0, path.dirname(path.dirname(__file__)))\n \n \n # General configuration\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d3d61fde870bb8a1364579959458ad9eacc1a5de", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 57 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -219,12 +219,9 @@ class SettingsTest(unittest.TestCase):\n                              \"Trying to modify an immutable Settings object\")\n \n     def test_frozencopy(self):\n-        with mock.patch.object(self.settings, 'copy') as mock_copy:\n-            with mock.patch.object(mock_copy, 'freeze') as mock_freeze:\n-                mock_object = self.settings.frozencopy()\n-                mock_copy.assert_call_once()\n-                mock_freeze.assert_call_once()\n-                self.assertEqual(mock_object, mock_copy.return_value)\n+        frozencopy = self.settings.frozencopy()\n+        self.assertTrue(frozencopy.frozen)\n+        self.assertIsNot(frozencopy, self.settings)\n \n     def test_deprecated_attribute_overrides(self):\n         self.settings.set('BAR', 'fuz', priority='cmdline')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4f56c6c79bd29dbd2e139533179b2039d7054aa4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 20 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -6,6 +6,13 @@ see http://doc.scrapy.org/en/latest/contributing.html#running-tests\n \n import os\n \n+# ignore system-wide proxies for tests\n+# which would send requests to a totally unsuspecting server\n+# (e.g. because urllib does not fully understand the proxy spec)\n+os.environ['http_proxy'] = ''\n+os.environ['https_proxy'] = ''\n+os.environ['ftp_proxy'] = ''\n+\n try:\n     import unittest.mock as mock\n except ImportError:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c908d3166015a9014b3b4ece54f313614cc43032", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 26/25 | Churn Δ: 20 | Churn Cumulative: 41 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 6 | DMM Complexity: 0.9\n\nDIFF:\n@@ -54,7 +54,8 @@ class RedirectMiddleware(BaseRedirectMiddleware):\n     \"\"\"Handle redirection of requests based on response status and meta-refresh html tag\"\"\"\n \n     def process_response(self, request, response, spider):\n-        if request.meta.get('dont_redirect', False):\n+        if (request.meta.get('dont_redirect', False) or\n+               response.status in getattr(spider, 'handle_httpstatus_list', [])):\n             return response\n \n         if request.method == 'HEAD':\n\n@@ -10,9 +10,9 @@ from scrapy.utils.test import get_crawler\n class RedirectMiddlewareTest(unittest.TestCase):\n \n     def setUp(self):\n-        crawler = get_crawler(Spider)\n-        self.spider = crawler._create_spider('foo')\n-        self.mw = RedirectMiddleware.from_crawler(crawler)\n+        self.crawler = get_crawler(Spider)\n+        self.spider = self.crawler._create_spider('foo')\n+        self.mw = RedirectMiddleware.from_crawler(self.crawler)\n \n     def test_priority_adjust(self):\n         req = Request('http://a.com')\n@@ -129,6 +129,17 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         self.assertEqual(req3.url, 'http://scrapytest.org/redirected2')\n         self.assertEqual(req3.meta['redirect_urls'], ['http://scrapytest.org/first', 'http://scrapytest.org/redirected'])\n \n+    def test_spider_handling(self):\n+        smartspider = self.crawler._create_spider('smarty')\n+        smartspider.handle_httpstatus_list = [404, 301, 302]\n+        url = 'http://www.example.com/301'\n+        url2 = 'http://www.example.com/redirected'\n+        req = Request(url, meta={'dont_redirect': True})\n+        rsp = Response(url, headers={'Location': url2}, status=301)\n+        r = self.mw.process_response(req, rsp, smartspider)\n+        self.assertIs(r, rsp)\n+\n+\n class MetaRefreshMiddlewareTest(unittest.TestCase):\n \n     def setUp(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c153217bfbd9c4166bfec8c7eb3b57cf25b41c55", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 40 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -134,7 +134,7 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         smartspider.handle_httpstatus_list = [404, 301, 302]\n         url = 'http://www.example.com/301'\n         url2 = 'http://www.example.com/redirected'\n-        req = Request(url, meta={'dont_redirect': True})\n+        req = Request(url)\n         rsp = Response(url, headers={'Location': url2}, status=301)\n         r = self.mw.process_response(req, rsp, smartspider)\n         self.assertIs(r, rsp)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#61cd27e5c7b777a54bd5baebc22ff46a1512298c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 92 | Lines Deleted: 76 | Files Changed: 9 | Hunks: 33 | Methods Changed: 21 | Complexity Δ (Sum/Max): 9/8 | Churn Δ: 168 | Churn Cumulative: 2292 | Contributors (this commit): 31 | Commits (past 90d): 14 | Contributors (cumulative): 65 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,7 +9,7 @@ from six.moves.urllib.parse import urljoin, urlencode\n import lxml.html\n import six\n from scrapy.http.request import Request\n-from scrapy.utils.python import unicode_to_str\n+from scrapy.utils.python import to_bytes\n \n \n class FormRequest(Request):\n@@ -48,7 +48,7 @@ def _get_form_url(form, url):\n \n \n def _urlencode(seq, enc):\n-    values = [(unicode_to_str(k, enc), unicode_to_str(v, enc))\n+    values = [(to_bytes(k, enc), to_bytes(v, enc))\n               for k, vs in seq\n               for v in (vs if hasattr(vs, '__iter__') else [vs])]\n     return urlencode(values, doseq=1)\n\n@@ -10,7 +10,7 @@ import lxml.etree as etree\n from scrapy.selector import Selector\n from scrapy.link import Link\n from scrapy.utils.misc import arg_to_iter\n-from scrapy.utils.python import unique as unique_list, str_to_unicode\n+from scrapy.utils.python import unique as unique_list\n from scrapy.linkextractors import FilteringLinkExtractor\n from scrapy.utils.response import get_base_url\n \n\n@@ -10,7 +10,7 @@ from scrapy.selector import Selector\n from scrapy.link import Link\n from scrapy.linkextractors import FilteringLinkExtractor\n from scrapy.utils.misc import arg_to_iter\n-from scrapy.utils.python import unique as unique_list, str_to_unicode\n+from scrapy.utils.python import unique as unique_list, to_unicode\n from scrapy.utils.response import get_base_url\n from scrapy.exceptions import ScrapyDeprecationWarning\n \n@@ -44,7 +44,7 @@ class BaseSgmlLinkExtractor(SGMLParser):\n                 link.url = link.url.encode(response_encoding)\n             link.url = urljoin(base_url, link.url)\n             link.url = safe_url_string(link.url, response_encoding)\n-            link.text = str_to_unicode(link.text, response_encoding, errors='replace').strip()\n+            link.text = to_unicode(link.text, response_encoding, errors='replace').strip()\n             ret.append(link)\n \n         return ret\n\n@@ -7,7 +7,7 @@ import six\n \n from scrapy.utils.misc import extract_regex\n from scrapy.utils.trackref import object_ref\n-from scrapy.utils.python import unicode_to_str, flatten, iflatten\n+from scrapy.utils.python import to_bytes, flatten, iflatten\n from scrapy.utils.decorators import deprecated\n from scrapy.http import HtmlResponse, XmlResponse\n from .lxmldocument import LxmlDocument\n@@ -44,7 +44,7 @@ def _st(response, st):\n def _response_from_text(text, st):\n     rt = XmlResponse if st == 'xml' else HtmlResponse\n     return rt(url='about:blank', encoding='utf-8',\n-              body=unicode_to_str(text, 'utf-8'))\n+              body=to_bytes(text, 'utf-8'))\n \n \n class Selector(object_ref):\n\n@@ -11,7 +11,7 @@ import six\n \n from scrapy.http import TextResponse, Response\n from scrapy.selector import Selector\n-from scrapy.utils.python import re_rsearch, str_to_unicode\n+from scrapy.utils.python import re_rsearch, to_unicode\n \n logger = logging.getLogger(__name__)\n \n@@ -98,7 +98,7 @@ def csviter(obj, delimiter=None, headers=None, encoding=None, quotechar=None):\n \n     encoding = obj.encoding if isinstance(obj, TextResponse) else encoding or 'utf-8'\n     def _getrow(csv_r):\n-        return [str_to_unicode(field, encoding) for field in next(csv_r)]\n+        return [to_unicode(field, encoding) for field in next(csv_r)]\n \n     lines = BytesIO(_body_or_str(obj, unicode=False))\n \n\n@@ -1,9 +1,5 @@\n \"\"\"\n This module contains essential stuff that should've come with Python itself ;)\n-\n-It also contains functions (or functionality) which is in Python versions\n-higher than 2.5 which used to be the lowest version supported by Scrapy.\n-\n \"\"\"\n import os\n import re\n@@ -13,6 +9,8 @@ import errno\n import six\n from functools import partial, wraps\n \n+from scrapy.utils.decorators import deprecated\n+\n \n def flatten(x):\n     \"\"\"flatten(sequence) -> list\n@@ -56,37 +54,44 @@ def unique(list_, key=lambda x: x):\n     return result\n \n \n+@deprecated(\"scrapy.utils.python.to_unicode\")\n def str_to_unicode(text, encoding=None, errors='strict'):\n-    \"\"\"Return the unicode representation of text in the given encoding. Unlike\n-    .encode(encoding) this function can be applied directly to a unicode\n-    object without the risk of double-decoding problems (which can happen if\n-    you don't use the default 'ascii' encoding)\n-    \"\"\"\n+    \"\"\" This function is deprecated.\n+    Please use scrapy.utils.python.to_unicode. \"\"\"\n+    return to_unicode(text, encoding, errors)\n \n-    if encoding is None:\n-        encoding = 'utf-8'\n-    if isinstance(text, str):\n-        return text.decode(encoding, errors)\n-    elif isinstance(text, unicode):\n-        return text\n-    else:\n-        raise TypeError('str_to_unicode must receive a str or unicode object, got %s' % type(text).__name__)\n \n+@deprecated(\"scrapy.utils.python.to_bytes\")\n def unicode_to_str(text, encoding=None, errors='strict'):\n-    \"\"\"Return the str representation of text in the given encoding. Unlike\n-    .encode(encoding) this function can be applied directly to a str\n-    object without the risk of double-decoding problems (which can happen if\n-    you don't use the default 'ascii' encoding)\n-    \"\"\"\n+    \"\"\" This function is deprecated. Please use scrapy.utils.python.to_bytes \"\"\"\n+    return to_bytes(text, encoding, errors)\n \n+\n+def to_unicode(text, encoding=None, errors='strict'):\n+    \"\"\"Return the unicode representation of a bytes object `text`. If `text`\n+    is already an unicode object, return it as-is.\"\"\"\n+    if isinstance(text, six.text_type):\n+        return text\n+    if not isinstance(text, (bytes, six.text_type)):\n+        raise TypeError('to_unicode must receive a bytes, str or unicode '\n+                        'object, got %s' % type(text).__name__)\n     if encoding is None:\n         encoding = 'utf-8'\n-    if isinstance(text, unicode):\n-        return text.encode(encoding, errors)\n-    elif isinstance(text, str):\n+    return text.decode(encoding, errors)\n+\n+\n+def to_bytes(text, encoding=None, errors='strict'):\n+    \"\"\"Return the binary representation of `text`. If `text`\n+    is already a bytes object, return it as-is.\"\"\"\n+    if isinstance(text, bytes):\n         return text\n-    else:\n-        raise TypeError('unicode_to_str must receive a unicode or str object, got %s' % type(text).__name__)\n+    if not isinstance(text, six.string_types):\n+        raise TypeError('to_bytes must receive a unicode, str or bytes '\n+                        'object, got %s' % type(text).__name__)\n+    if encoding is None:\n+        encoding = 'utf-8'\n+    return text.encode(encoding, errors)\n+\n \n def re_rsearch(pattern, text, chunk_size=1024):\n     \"\"\"\n@@ -117,6 +122,7 @@ def re_rsearch(pattern, text, chunk_size=1024):\n             return (offset + matches[-1].span()[0], offset + matches[-1].span()[1])\n     return None\n \n+\n def memoizemethod_noargs(method):\n     \"\"\"Decorator to cache the result of a method (without arguments) using a\n     weak reference to its object\n@@ -131,6 +137,7 @@ def memoizemethod_noargs(method):\n \n _BINARYCHARS = set(map(chr, range(32))) - set([\"\\0\", \"\\t\", \"\\n\", \"\\r\"])\n \n+\n def isbinarytext(text):\n     \"\"\"Return True if the given text is considered binary, or false\n     otherwise, by looking for binary bytes at their chars\n@@ -138,6 +145,7 @@ def isbinarytext(text):\n     assert isinstance(text, str), \"text must be str, got '%s'\" % type(text).__name__\n     return any(c in _BINARYCHARS for c in text)\n \n+\n def get_func_args(func, stripself=False):\n     \"\"\"Return the argument name list of a callable\"\"\"\n     if inspect.isfunction(func):\n@@ -164,6 +172,7 @@ def get_func_args(func, stripself=False):\n         func_args.pop(0)\n     return func_args\n \n+\n def get_spec(func):\n     \"\"\"Returns (args, kwargs) tuple for a function\n     >>> import re\n@@ -200,6 +209,7 @@ def get_spec(func):\n     kwargs = dict(zip(spec.args[firstdefault:], defaults))\n     return args, kwargs\n \n+\n def equal_attributes(obj1, obj2, attributes):\n     \"\"\"Compare two objects attributes\"\"\"\n     # not attributes given return False by default\n@@ -249,6 +259,7 @@ def stringify_dict(dct_or_tuples, encoding='utf-8', keys_only=True):\n         d[k] = v\n     return d\n \n+\n def is_writable(path):\n     \"\"\"Return True if the given path can be written (if it exists) or created\n     (if it doesn't exist)\n@@ -258,6 +269,7 @@ def is_writable(path):\n     else:\n         return os.access(os.path.dirname(path), os.W_OK)\n \n+\n def setattr_default(obj, name, value):\n     \"\"\"Set attribute value, but only if it's not already set. Similar to\n     setdefault() for dicts.\n\n@@ -12,7 +12,7 @@ from six.moves.urllib.parse import (ParseResult, urlunparse, urldefrag,\n \n # scrapy.utils.url was moved to w3lib.url and import * ensures this move doesn't break old code\n from w3lib.url import *\n-from scrapy.utils.python import unicode_to_str\n+from scrapy.utils.python import to_bytes\n \n \n def url_is_from_any_domain(url, domains):\n@@ -72,8 +72,9 @@ def parse_url(url, encoding=None):\n     \"\"\"Return urlparsed url from the given argument (which could be an already\n     parsed url)\n     \"\"\"\n-    return url if isinstance(url, ParseResult) else \\\n-        urlparse(unicode_to_str(url, encoding))\n+    if isinstance(url, ParseResult):\n+        return url\n+    return urlparse(to_bytes(url, encoding))\n \n \n def escape_ajax(url):\n\n@@ -8,7 +8,7 @@ from six.moves import cPickle as pickle\n import lxml.etree\n \n from scrapy.item import Item, Field\n-from scrapy.utils.python import str_to_unicode\n+from scrapy.utils.python import to_unicode\n from scrapy.exporters import (\n     BaseItemExporter, PprintItemExporter, PickleItemExporter, CsvItemExporter,\n     XmlItemExporter, JsonLinesItemExporter, JsonItemExporter, PythonItemExporter\n@@ -35,7 +35,7 @@ class BaseItemExporterTest(unittest.TestCase):\n \n     def _assert_expected_item(self, exported_dict):\n         for k, v in exported_dict.items():\n-            exported_dict[k] = str_to_unicode(v)\n+            exported_dict[k] = to_unicode(v)\n         self.assertEqual(self.i, exported_dict)\n \n     def assertItemExportWorks(self, item):\n\n@@ -3,45 +3,48 @@ import operator\n import unittest\n from itertools import count\n \n-from scrapy.utils.python import str_to_unicode, unicode_to_str, \\\n-    memoizemethod_noargs, isbinarytext, equal_attributes, \\\n-    WeakKeyCache, stringify_dict, get_func_args\n+from scrapy.utils.python import (\n+    memoizemethod_noargs, isbinarytext, equal_attributes,\n+    WeakKeyCache, stringify_dict, get_func_args, to_bytes, to_unicode)\n \n __doctests__ = ['scrapy.utils.python']\n \n+\n+class ToUnicodeTest(unittest.TestCase):\n+    def test_converting_an_utf8_encoded_string_to_unicode(self):\n+        self.assertEqual(to_unicode('lel\\xc3\\xb1e'), u'lel\\xf1e')\n+\n+    def test_converting_a_latin_1_encoded_string_to_unicode(self):\n+        self.assertEqual(to_unicode('lel\\xf1e', 'latin-1'), u'lel\\xf1e')\n+\n+    def test_converting_a_unicode_to_unicode_should_return_the_same_object(self):\n+        self.assertEqual(to_unicode(u'\\xf1e\\xf1e\\xf1e'), u'\\xf1e\\xf1e\\xf1e')\n+\n+    def test_converting_a_strange_object_should_raise_TypeError(self):\n+        self.assertRaises(TypeError, to_unicode, 423)\n+\n+    def test_check_errors_argument_works(self):\n+        self.assertIn(u'\\ufffd', to_unicode('a\\xedb', 'utf-8', errors='replace'))\n+\n+\n+class ToBytesTest(unittest.TestCase):\n+    def test_converting_a_unicode_object_to_an_utf_8_encoded_string(self):\n+        self.assertEqual(to_bytes(u'\\xa3 49'), '\\xc2\\xa3 49')\n+\n+    def test_converting_a_unicode_object_to_a_latin_1_encoded_string(self):\n+        self.assertEqual(to_bytes(u'\\xa3 49', 'latin-1'), '\\xa3 49')\n+\n+    def test_converting_a_regular_string_to_string_should_return_the_same_object(self):\n+        self.assertEqual(to_bytes('lel\\xf1e'), 'lel\\xf1e')\n+\n+    def test_converting_a_strange_object_should_raise_TypeError(self):\n+        self.assertRaises(TypeError, to_bytes, unittest)\n+\n+    def test_check_errors_argument_works(self):\n+        self.assertIn('?', to_bytes(u'a\\ufffdb', 'latin-1', errors='replace'))\n+\n+\n class UtilsPythonTestCase(unittest.TestCase):\n-    def test_str_to_unicode(self):\n-        # converting an utf-8 encoded string to unicode\n-        self.assertEqual(str_to_unicode('lel\\xc3\\xb1e'), u'lel\\xf1e')\n-\n-        # converting a latin-1 encoded string to unicode\n-        self.assertEqual(str_to_unicode('lel\\xf1e', 'latin-1'), u'lel\\xf1e')\n-\n-        # converting a unicode to unicode should return the same object\n-        self.assertEqual(str_to_unicode(u'\\xf1e\\xf1e\\xf1e'), u'\\xf1e\\xf1e\\xf1e')\n-\n-        # converting a strange object should raise TypeError\n-        self.assertRaises(TypeError, str_to_unicode, 423)\n-\n-        # check errors argument works\n-        assert u'\\ufffd' in str_to_unicode('a\\xedb', 'utf-8', errors='replace')\n-\n-    def test_unicode_to_str(self):\n-        # converting a unicode object to an utf-8 encoded string\n-        self.assertEqual(unicode_to_str(u'\\xa3 49'), '\\xc2\\xa3 49')\n-\n-        # converting a unicode object to a latin-1 encoded string\n-        self.assertEqual(unicode_to_str(u'\\xa3 49', 'latin-1'), '\\xa3 49')\n-\n-        # converting a regular string to string should return the same object\n-        self.assertEqual(unicode_to_str('lel\\xf1e'), 'lel\\xf1e')\n-\n-        # converting a strange object should raise TypeError\n-        self.assertRaises(TypeError, unicode_to_str, unittest)\n-\n-        # check errors argument works\n-        assert '?' in unicode_to_str(u'a\\ufffdb', 'latin-1', errors='replace')\n-\n     def test_memoizemethod_noargs(self):\n         class A(object):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#887936ebf6d39a977a3558e249fdb875fc3f21ce", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 35 | Churn Cumulative: 479 | Contributors (this commit): 10 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -23,8 +23,12 @@ def flatten(x):\n     >>> [1, 2, [3,4], (5,6)]\n     [1, 2, [3, 4], (5, 6)]\n     >>> flatten([[[1,2,3], (42,None)], [4,5], [6], 7, (8,9,10)])\n-    [1, 2, 3, 42, None, 4, 5, 6, 7, 8, 9, 10]\"\"\"\n-\n+    [1, 2, 3, 42, None, 4, 5, 6, 7, 8, 9, 10]\n+    >>> flatten([\"foo\", \"bar\"])\n+    ['foo', 'bar']\n+    >>> flatten([\"foo\", [\"baz\", 42], \"bar\"])\n+    ['foo', 'baz', 42, 'bar']\n+    \"\"\"\n     return list(iflatten(x))\n \n \n@@ -32,15 +36,38 @@ def iflatten(x):\n     \"\"\"iflatten(sequence) -> iterator\n \n     Similar to ``.flatten()``, but returns iterator instead\"\"\"\n-\n     for el in x:\n-        if hasattr(el, \"__iter__\"):\n+        if is_listlike(el):\n             for el_ in flatten(el):\n                 yield el_\n         else:\n             yield el\n \n \n+def is_listlike(x):\n+    \"\"\"\n+    >>> is_listlike(\"foo\")\n+    False\n+    >>> is_listlike(5)\n+    False\n+    >>> is_listlike(b\"foo\")\n+    False\n+    >>> is_listlike([b\"foo\"])\n+    True\n+    >>> is_listlike((b\"foo\",))\n+    True\n+    >>> is_listlike({})\n+    True\n+    >>> is_listlike(set())\n+    True\n+    >>> is_listlike((x for x in range(3)))\n+    True\n+    >>> is_listlike(six.moves.xrange(5))\n+    True\n+    \"\"\"\n+    return hasattr(x, \"__iter__\") and not isinstance(x, (six.text_type, bytes))\n+\n+\n def unique(list_, key=lambda x: x):\n     \"\"\"efficient function to uniquify a list preserving item order\"\"\"\n     seen = set()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#41bcae5d00a3207fa15c7e2150a92cd103c7552b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 7 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 101 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -12,10 +12,10 @@ __doctests__ = ['scrapy.utils.python']\n \n class ToUnicodeTest(unittest.TestCase):\n     def test_converting_an_utf8_encoded_string_to_unicode(self):\n-        self.assertEqual(to_unicode('lel\\xc3\\xb1e'), u'lel\\xf1e')\n+        self.assertEqual(to_unicode(b'lel\\xc3\\xb1e'), u'lel\\xf1e')\n \n     def test_converting_a_latin_1_encoded_string_to_unicode(self):\n-        self.assertEqual(to_unicode('lel\\xf1e', 'latin-1'), u'lel\\xf1e')\n+        self.assertEqual(to_unicode(b'lel\\xf1e', 'latin-1'), u'lel\\xf1e')\n \n     def test_converting_a_unicode_to_unicode_should_return_the_same_object(self):\n         self.assertEqual(to_unicode(u'\\xf1e\\xf1e\\xf1e'), u'\\xf1e\\xf1e\\xf1e')\n@@ -24,24 +24,24 @@ class ToUnicodeTest(unittest.TestCase):\n         self.assertRaises(TypeError, to_unicode, 423)\n \n     def test_check_errors_argument_works(self):\n-        self.assertIn(u'\\ufffd', to_unicode('a\\xedb', 'utf-8', errors='replace'))\n+        self.assertIn(u'\\ufffd', to_unicode(b'a\\xedb', 'utf-8', errors='replace'))\n \n \n class ToBytesTest(unittest.TestCase):\n     def test_converting_a_unicode_object_to_an_utf_8_encoded_string(self):\n-        self.assertEqual(to_bytes(u'\\xa3 49'), '\\xc2\\xa3 49')\n+        self.assertEqual(to_bytes(u'\\xa3 49'), b'\\xc2\\xa3 49')\n \n     def test_converting_a_unicode_object_to_a_latin_1_encoded_string(self):\n-        self.assertEqual(to_bytes(u'\\xa3 49', 'latin-1'), '\\xa3 49')\n+        self.assertEqual(to_bytes(u'\\xa3 49', 'latin-1'), b'\\xa3 49')\n \n     def test_converting_a_regular_string_to_string_should_return_the_same_object(self):\n-        self.assertEqual(to_bytes('lel\\xf1e'), 'lel\\xf1e')\n+        self.assertEqual(to_bytes(b'lel\\xf1e'), b'lel\\xf1e')\n \n     def test_converting_a_strange_object_should_raise_TypeError(self):\n         self.assertRaises(TypeError, to_bytes, unittest)\n \n     def test_check_errors_argument_works(self):\n-        self.assertIn('?', to_bytes(u'a\\ufffdb', 'latin-1', errors='replace'))\n+        self.assertIn(b'?', to_bytes(u'a\\ufffdb', 'latin-1', errors='replace'))\n \n \n class UtilsPythonTestCase(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
