{"custom_id": "scrapy#7ddd4441e3a422207fecf385c8feac3495027118", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 74 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 9/8 | Churn Δ: 75 | Churn Cumulative: 155 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 0.6428571428571429\n\nDIFF:\n@@ -1,7 +1,8 @@\n+import operator\n import unittest\n \n from scrapy.utils.python import str_to_unicode, unicode_to_str, \\\n-    memoizemethod_noargs, isbinarytext\n+    memoizemethod_noargs, isbinarytext, equal_attributes\n \n class UtilsPythonTestCase(unittest.TestCase):\n     def test_str_to_unicode(self):\n@@ -61,5 +62,52 @@ class UtilsPythonTestCase(unittest.TestCase):\n         # finally some real binary bytes\n         assert isbinarytext(\"\\x02\\xa3\")\n \n+    def test_equal_attributes(self):\n+        class Obj:\n+            pass\n+\n+        a = Obj()\n+        b = Obj()\n+        # no attributes given return False\n+        self.failIf(equal_attributes(a, b, []))\n+        # not existent attributes\n+        self.failIf(equal_attributes(a, b, ['x', 'y']))\n+\n+        a.x = 1\n+        b.x = 1\n+        # equal attribute\n+        self.failUnless(equal_attributes(a, b, ['x']))\n+\n+        b.y = 2\n+        # obj1 has no attribute y\n+        self.failIf(equal_attributes(a, b, ['x', 'y']))\n+\n+        a.y = 2\n+        # equal attributes\n+        self.failUnless(equal_attributes(a, b, ['x', 'y']))\n+\n+        a.y = 1\n+        # differente attributes\n+        self.failIf(equal_attributes(a, b, ['x', 'y']))\n+\n+        # test callable\n+        a.meta = {}\n+        b.meta = {}\n+        self.failUnless(equal_attributes(a, b, ['meta']))\n+\n+        # compare ['meta']['a']\n+        a.meta['z'] = 1\n+        b.meta['z'] = 1\n+\n+        get_z = operator.itemgetter('z')\n+        get_meta = operator.attrgetter('meta')\n+        compare_z = lambda obj: get_z(get_meta(obj))\n+\n+        self.failUnless(equal_attributes(a, b, [compare_z, 'x']))\n+        # fail z equality\n+        a.meta['z'] = 2\n+        self.failIf(equal_attributes(a, b, [compare_z, 'x']))\n+\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -216,3 +216,28 @@ def get_func_args(func):\n     else:\n         raise TypeError('%s is not callable' % type(func))\n     return func_args\n+\n+\n+def equal_attributes(obj1, obj2, attributes):\n+    \"\"\"Compare two objects attributes\"\"\"\n+    # not attributes given return False by default\n+    if not attributes:\n+        return False\n+\n+    for attr in attributes:\n+        # support callables like itemgetter\n+        if callable(attr):\n+            if not attr(obj1) == attr(obj2):\n+                return False\n+        else:\n+            # check that objects has attribute\n+            if not hasattr(obj1, attr):\n+                return False\n+            if not hasattr(obj2, attr):\n+                return False\n+            # compare object attributes\n+            if not getattr(obj1, attr) == getattr(obj2, attr):\n+                return False\n+    # all attributes equal\n+    return True\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4a053a762fe176693a42d8741ebe8c96af78e309", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 115 | Lines Deleted: 0 | Files Changed: 7 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 6/3 | Churn Δ: 115 | Churn Cumulative: 293 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 21 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1 @@\n+# googledir project\n\n@@ -0,0 +1,16 @@\n+# Define here the models for your scraped items\n+#\n+# See documentation in:\n+# http://doc.scrapy.org/topics/items.html\n+\n+from scrapy.item import Item, Field\n+\n+class GoogledirItem(Item):\n+\n+    name = Field(default='')\n+    url = Field(default='')\n+    description = Field(default='')\n+\n+    def __str__(self):\n+        return \"Google Category: name=%s url=%s\" \\\n+                    % (self['name'], self['url'])\n\n@@ -0,0 +1,22 @@\n+# Define your item pipelines here\n+#\n+# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n+# See: http://doc.scrapy.org/topics/item-pipeline.html\n+\n+from scrapy.core.exceptions import DropItem\n+\n+class FilterWordsPipeline(object):\n+    \"\"\"\n+    A pipeline for filtering out items which contain certain \n+    words in their description\n+    \"\"\" \n+\n+    # put all words in lowercase\n+    words_to_filter = ['politics', 'religion']\n+\n+    def process_item(self, spider, item):\n+        for word in self.words_to_filter:\n+            if word in unicode(item['description']).lower():\n+                raise DropItem(\"Contains forbidden word: %s\" % word)\n+        else:\n+            return item\n\n@@ -0,0 +1,21 @@\n+# Scrapy settings for googledir project\n+#\n+# For simplicity, this file contains only the most important settings by\n+# default. All the other settings are documented here:\n+#\n+#     http://doc.scrapy.org/topics/settings.html\n+#\n+# Or you can copy and paste them from where they're defined in Scrapy:\n+# \n+#     scrapy/conf/default_settings.py\n+#\n+\n+BOT_NAME = 'googledir'\n+BOT_VERSION = '1.0'\n+\n+SPIDER_MODULES = ['googledir.spiders']\n+NEWSPIDER_MODULE = 'googledir.spiders'\n+DEFAULT_ITEM_CLASS = 'googledir.items.GoogledirItem'\n+USER_AGENT = '%s/%s' % (BOT_NAME, BOT_VERSION)\n+\n+ITEM_PIPELINES = ['googledir.pipelines.FilterWordsPipeline']\n\n@@ -0,0 +1,8 @@\n+# This package will contain the spiders of your Scrapy project\n+#\n+# To create the first spider for your project use this command:\n+#\n+#   scrapy-ctl.py genspider myspider myspider-domain.com\n+#\n+# For more info see:\n+# http://doc.scrapy.org/topics/spiders.html\n\n@@ -0,0 +1,40 @@\n+from scrapy.selector import HtmlXPathSelector\n+from scrapy.contrib.loader import XPathItemLoader\n+from scrapy.contrib_exp.crawlspider import CrawlSpider, Rule\n+\n+from googledir.items import GoogledirItem\n+\n+class GoogleDirectorySpider(CrawlSpider):\n+\n+    domain_name = 'directory.google.com'\n+    start_urls = ['http://directory.google.com/']\n+\n+    rules = (\n+        # search for categories pattern and follow links\n+        Rule(r'/[A-Z][a-zA-Z_/]+$', 'parse_category', follow=True),\n+        )\n+\n+    def parse_category(self, response):\n+        # The main selector we're using to extract data from the page\n+        main_selector = HtmlXPathSelector(response)\n+\n+        # The XPath to website links in the directory page\n+        xpath = '//td[descendant::a[contains(@href, \"#pagerank\")]]/following-sibling::td/font'\n+\n+        # Get a list of (sub) selectors to each website node pointed by the XPath\n+        sub_selectors = main_selector.select(xpath)\n+\n+        # Iterate over the sub-selectors to extract data for each website\n+        for selector in sub_selectors:\n+            item = GoogledirItem()\n+\n+            l = XPathItemLoader(item=item, selector=selector)\n+            l.add_xpath('name', 'a/text()')\n+            l.add_xpath('url', 'a/@href')\n+            l.add_xpath('description', 'font[2]/text()')\n+\n+            # Here we populate the item and yield it\n+            yield l.load_item()\n+\n+SPIDER = GoogleDirectorySpider()\n+\n\n@@ -0,0 +1,7 @@\n+#!/usr/bin/env python\n+\n+import os\n+os.environ.setdefault('SCRAPY_SETTINGS_MODULE', 'googledir.settings')\n+\n+from scrapy.command.cmdline import execute\n+execute()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c1f8198639eab1b8ac3adffd68e5881ad020e4da", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 26 | Churn Cumulative: 722 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -122,6 +122,8 @@ MYSQL_CONNECTION_SETTINGS = {}\n \n NEWSPIDER_MODULE = ''\n \n+RANDOMIZE_DOWNLOAD_DELAY = True\n+\n REDIRECT_MAX_METAREFRESH_DELAY = 100\n REDIRECT_MAX_TIMES = 20 # uses Firefox default setting\n REDIRECT_PRIORITY_ADJUST = +2\n\n@@ -2,6 +2,7 @@\n Download web pages using asynchronous IO\n \"\"\"\n \n+import random\n from time import time\n \n from twisted.internet import reactor, defer\n@@ -20,15 +21,21 @@ class SpiderInfo(object):\n \n     def __init__(self, download_delay=None, max_concurrent_requests=None):\n         if download_delay is None:\n-            self.download_delay = settings.getfloat('DOWNLOAD_DELAY')\n+            self._download_delay = settings.getfloat('DOWNLOAD_DELAY')\n         else:\n-            self.download_delay = download_delay\n-        if self.download_delay:\n+            self._download_delay = float(download_delay)\n+        if self._download_delay:\n             self.max_concurrent_requests = 1\n         elif max_concurrent_requests is None:\n             self.max_concurrent_requests = settings.getint('CONCURRENT_REQUESTS_PER_SPIDER')\n         else:\n             self.max_concurrent_requests =  max_concurrent_requests\n+        if self._download_delay and settings.getbool('RANDOMIZE_DOWNLOAD_DELAY'):\n+            # same policy as wget --random-wait\n+            self.random_delay_interval = (0.5*self._download_delay, \\\n+                1.5*self._download_delay)\n+        else:\n+            self.random_delay_interval = None\n \n         self.active = set()\n         self.queue = []\n@@ -44,6 +51,12 @@ class SpiderInfo(object):\n         # use self.active to include requests in the downloader middleware\n         return len(self.active) > 2 * self.max_concurrent_requests\n \n+    def download_delay(self):\n+        if self.random_delay_interval:\n+            return random.uniform(*self.random_delay_interval)\n+        else:\n+            return self._download_delay\n+\n     def cancel_request_calls(self):\n         for call in self.next_request_calls:\n             call.cancel()\n@@ -99,8 +112,9 @@ class Downloader(object):\n \n         # Delay queue processing if a download_delay is configured\n         now = time()\n-        if site.download_delay:\n-            penalty = site.download_delay - now + site.lastseen\n+        delay = site.download_delay()\n+        if delay:\n+            penalty = delay - now + site.lastseen\n             if penalty > 0:\n                 d = defer.Deferred()\n                 d.addCallback(self._process_queue)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7b1ad321e3beda87afd7fc8d66d3bdf812a8b763", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 196 | Lines Deleted: 0 | Files Changed: 7 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 8/7 | Churn Δ: 196 | Churn Cumulative: 196 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1 @@\n+# package\n\n@@ -0,0 +1,12 @@\n+# Define here the models for your scraped items\n+#\n+# See documentation in:\n+# http://doc.scrapy.org/topics/items.html\n+\n+from scrapy.item import Item, Field\n+\n+class ImdbItem(Item):\n+    # define the fields for your item here like:\n+    # name = Field()\n+    title = Field()\n+    url = Field()\n\n@@ -0,0 +1,8 @@\n+# Define your item pipelines here\n+#\n+# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n+# See: http://doc.scrapy.org/topics/item-pipeline.html\n+\n+class ImdbPipeline(object):\n+    def process_item(self, spider, item):\n+        return item\n\n@@ -0,0 +1,20 @@\n+# Scrapy settings for imdb project\n+#\n+# For simplicity, this file contains only the most important settings by\n+# default. All the other settings are documented here:\n+#\n+#     http://doc.scrapy.org/topics/settings.html\n+#\n+# Or you can copy and paste them from where they're defined in Scrapy:\n+# \n+#     scrapy/conf/default_settings.py\n+#\n+\n+BOT_NAME = 'imdb'\n+BOT_VERSION = '1.0'\n+\n+SPIDER_MODULES = ['imdb.spiders']\n+NEWSPIDER_MODULE = 'imdb.spiders'\n+DEFAULT_ITEM_CLASS = 'imdb.items.ImdbItem'\n+USER_AGENT = '%s/%s' % (BOT_NAME, BOT_VERSION)\n+\n\n@@ -0,0 +1,8 @@\n+# This package will contain the spiders of your Scrapy project\n+#\n+# To create the first spider for your project use this command:\n+#\n+#   scrapy-ctl.py genspider myspider myspider-domain.com\n+#\n+# For more info see:\n+# http://doc.scrapy.org/topics/spiders.html\n\n@@ -0,0 +1,140 @@\n+from scrapy.http import Request\n+from scrapy.selector import HtmlXPathSelector\n+from scrapy.contrib.loader import XPathItemLoader\n+from scrapy.contrib_exp.crawlspider import CrawlSpider, Rule\n+from scrapy.contrib_exp.crawlspider.reqext import SgmlRequestExtractor\n+from scrapy.contrib_exp.crawlspider.reqproc import Canonicalize, \\\n+        FilterDupes, FilterUrl\n+from scrapy.utils.url import urljoin_rfc\n+\n+from imdb.items import ImdbItem, Field\n+\n+from itertools import chain, imap, izip\n+\n+class UsaOpeningWeekMovie(ImdbItem):\n+    pass\n+\n+class UsaTopWeekMovie(ImdbItem):\n+    pass\n+\n+class Top250Movie(ImdbItem):\n+    rank = Field()\n+    rating = Field()\n+    year = Field()\n+    votes = Field()\n+\n+class MovieItem(ImdbItem):\n+    release_date = Field()\n+    tagline = Field()\n+\n+\n+class ImdbSiteSpider(CrawlSpider):\n+    domain_name = 'imdb.com'\n+    start_urls = ['http://www.imdb.com/']\n+\n+    # extract requests using this classes from urls matching 'follow' flag\n+    request_extractors = [\n+        SgmlRequestExtractor(tags=['a'], attrs=['href']),\n+        ]\n+\n+    # process requests using this classes from urls matching 'follow' flag\n+    request_processors = [\n+        Canonicalize(),\n+        FilterDupes(),\n+        FilterUrl(deny=r'/tt\\d+/$'), # deny movie url as we will dispatch\n+                                     # manually the movie requests\n+        ]\n+\n+    # include domain bit for demo purposes\n+    rules = (\n+        # these two rules expects requests from start url\n+        Rule(r'imdb.com/nowplaying/$', 'parse_now_playing'),\n+        Rule(r'imdb.com/chart/top$', 'parse_top_250'),\n+        # this rule will parse requests manually dispatched\n+        Rule(r'imdb.com/title/tt\\d+/$', 'parse_movie_info'),\n+    )\n+\n+    def parse_now_playing(self, response):\n+        \"\"\"Scrapes USA openings this week and top 10 in week\"\"\"\n+        self.log(\"Parsing USA Top Week\")\n+        hxs = HtmlXPathSelector(response)\n+\n+        _urljoin = lambda url: self._urljoin(response, url)\n+\n+        #\n+        # openings this week\n+        #\n+        openings = hxs.select('//table[@class=\"movies\"]//a[@class=\"title\"]')\n+        boxoffice = hxs.select('//table[@class=\"boxoffice movies\"]//a[@class=\"title\"]')\n+\n+        opening_titles = openings.select('text()').extract()\n+        opening_urls = imap(_urljoin, openings.select('@href').extract())\n+\n+        box_titles = boxoffice.select('text()').extract()\n+        box_urls = imap(_urljoin, boxoffice.select('@href').extract())\n+\n+        # items \n+        opening_items = (UsaOpeningWeekMovie(title=title, url=url)\n+                            for (title, url)\n+                            in izip(opening_titles, opening_urls))\n+\n+        box_items = (UsaTopWeekMovie(title=title, url=url) \n+                        for (title, url)\n+                        in izip(box_titles, box_urls))\n+\n+        # movie requests\n+        requests = imap(self.make_requests_from_url,\n+                        chain(opening_urls, box_urls))\n+\n+        return chain(opening_items, box_items, requests)\n+\n+    def parse_top_250(self, response):\n+        \"\"\"Scrapes movies from top 250 list\"\"\"\n+        self.log(\"Parsing Top 250\")\n+        hxs = HtmlXPathSelector(response)\n+\n+        # scrap each row in the table\n+        rows = hxs.select('//div[@id=\"main\"]/table/tr//a/ancestor::tr')\n+        for row in rows:\n+            fields = row.select('td//text()').extract()\n+            url, = row.select('td//a/@href').extract()\n+            url = self._urljoin(response, url)\n+\n+            item = Top250Movie()\n+            item['title'] = fields[2]\n+            item['url'] = url\n+            item['rank'] = fields[0]\n+            item['rating'] = fields[1]\n+            item['year'] = fields[3]\n+            item['votes'] = fields[4]\n+\n+            # scrapped top250 item\n+            yield item\n+            # fetch movie\n+            yield self.make_requests_from_url(url)\n+\n+    def parse_movie_info(self, response):\n+        \"\"\"Scrapes movie information\"\"\"\n+        self.log(\"Parsing Movie Info\")\n+        hxs = HtmlXPathSelector(response)\n+        selector = hxs.select('//div[@class=\"maindetails\"]')\n+\n+        item = MovieItem()\n+        # set url\n+        item['url'] = response.url\n+\n+        # use item loader for other attributes\n+        l = XPathItemLoader(item=item, selector=selector)\n+        l.add_xpath('title', './/h1/text()')\n+        l.add_xpath('release_date', './/h5[text()=\"Release Date:\"]'\n+                                    '/following-sibling::div/text()')\n+        l.add_xpath('tagline', './/h5[text()=\"Tagline:\"]'\n+                               '/following-sibling::div/text()')\n+\n+        yield l.load_item()\n+\n+    def _urljoin(self, response, url):\n+        \"\"\"Helper to convert relative urls to absolute\"\"\"\n+        return urljoin_rfc(response.url, url, response.encoding)\n+\n+SPIDER = ImdbSiteSpider()\n\n@@ -0,0 +1,7 @@\n+#!/usr/bin/env python\n+\n+import os\n+os.environ.setdefault('SCRAPY_SETTINGS_MODULE', 'imdb.settings')\n+\n+from scrapy.command.cmdline import execute\n+execute()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#180c091fb2a281ab1f79f9897b32687a877582e6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 12 | Churn Cumulative: 310 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,6 +5,7 @@ discovering (through HTTP headers) to base Response class.\n See documentation in docs/topics/request-response.rst\n \"\"\"\n \n+import codecs\n import re\n \n from scrapy.xlib.BeautifulSoup import UnicodeDammit\n@@ -64,7 +65,12 @@ class TextResponse(Response):\n         if content_type:\n             encoding = self._ENCODING_RE.search(content_type)\n             if encoding:\n-                return encoding.group(1)\n+                enc = encoding.group(1)\n+                try:\n+                    codecs.lookup(enc) # check if the encoding is valid\n+                    return enc\n+                except LookupError:\n+                    pass\n \n     @memoizemethod_noargs\n     def body_as_unicode(self):\n\n@@ -175,6 +175,8 @@ class TextResponseTest(BaseResponseTest):\n         r2 = self.response_class(\"http://www.example.com\", encoding='utf-8', body=u\"\\xa3\")\n         r3 = self.response_class(\"http://www.example.com\", headers={\"Content-type\": [\"text/html; charset=iso-8859-1\"]}, body=\"\\xa3\")\n         r4 = self.response_class(\"http://www.example.com\", body=\"\\xa2\\xa3\")\n+        r5 = self.response_class(\"http://www.example.com\",\n+        headers={\"Content-type\": [\"text/html; charset=None\"]}, body=\"\\xc2\\xa3\")\n \n         self.assertEqual(r1.headers_encoding(), \"utf-8\")\n         self.assertEqual(r2.headers_encoding(), None)\n@@ -182,6 +184,8 @@ class TextResponseTest(BaseResponseTest):\n         self.assertEqual(r3.headers_encoding(), \"iso-8859-1\")\n         self.assertEqual(r3.encoding, 'iso-8859-1')\n         self.assertEqual(r4.headers_encoding(), None)\n+        self.assertEqual(r5.headers_encoding(), None)\n+        self.assertEqual(r5.encoding, \"utf-8\")\n         assert r4.body_encoding() is not None and r4.body_encoding() != 'ascii'\n         self._assert_response_values(r1, 'utf-8', u\"\\xa3\")\n         self._assert_response_values(r2, 'utf-8', u\"\\xa3\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2322322ee67021fedd410242fc39de53a0aa87e9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 17 | Churn Cumulative: 368 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: 0.625\n\nDIFF:\n@@ -96,8 +96,9 @@ class Request(object_ref):\n         \"\"\"Return a copy of this Request\"\"\"\n         return self.replace()\n \n-    def replace(self, url=None, callback=None, method=None, headers=None, body=None, \n-                cookies=None, meta=None, encoding=None, dont_filter=None):\n+    def replace(self, url=None, callback=None, method=None, headers=None, body=None, \\\n+                cookies=None, meta=None, encoding=None, priority=None, \\\n+                dont_filter=None, errback=None):\n         \"\"\"Create a new Request with the same attributes except for those\n         given new values.\n         \"\"\"\n@@ -109,4 +110,6 @@ class Request(object_ref):\n                               cookies=self.cookies if cookies is None else cookies,\n                               meta=self.meta if meta is None else meta,\n                               encoding=self.encoding if encoding is None else encoding,\n-                              dont_filter=self.dont_filter if dont_filter is None else dont_filter)\n+                              priority=self.priority if priority is None else priority,\n+                              dont_filter=self.dont_filter if dont_filter is None else dont_filter,\n+                              errback=errback)\n\n@@ -2,6 +2,7 @@ import cgi\n import weakref\n import unittest\n import xmlrpclib\n+from inspect import getargspec\n from cStringIO import StringIO\n from urlparse import urlparse\n \n@@ -170,6 +171,13 @@ class RequestTest(unittest.TestCase):\n         self.assertEqual(r4.meta, {})\n         assert r4.dont_filter is False\n \n+        # __init__ and replace() signatures must be equal unles *args,**kwargs is used\n+        i_args, i_varargs, i_varkwargs, _ = getargspec(self.request_class.__init__)\n+        self.assertFalse(bool(i_varargs) ^ bool(i_varkwargs))\n+        if not i_varargs:\n+            r_args, _, _, _ = getargspec(self.request_class.replace)\n+            self.assertEqual(i_args, r_args)\n+\n     def test_weakref_slots(self):\n         \"\"\"Check that classes are using slots and are weak-referenceable\"\"\"\n         x = self.request_class('http://www.example.com')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d12cd22d5eacf6e09853db59708dc219e0d46486", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 267 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -152,7 +152,7 @@ SCHEDULER_MIDDLEWARES_BASE = {\n     'scrapy.contrib.schedulermiddleware.duplicatesfilter.DuplicatesFilterMiddleware': 500,\n }\n \n-SCHEDULER_ORDER = 'BFO'   # available orders: BFO (default), DFO\n+SCHEDULER_ORDER = 'DFO'\n \n SPIDER_MANAGER_CLASS = 'scrapy.contrib.spidermanager.TwistedPluginSpiderManager'\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#861f9691c7bde83d7352cca721f5069fb68abeb8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 26 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 26 | Churn Cumulative: 26 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,26 +0,0 @@\n-\"\"\"\n-Extensions to override scrapy settings with per-group settings according to the\n-group the spider belongs to. It only overrides the settings when running the\n-crawl command with *only one domain as argument*.\n-\"\"\"\n-\n-from scrapy.conf import settings\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.command.cmdline import command_executed\n-\n-class GroupSettings(object):\n-\n-    def __init__(self):\n-        if not settings.getbool(\"GROUPSETTINGS_ENABLED\"):\n-            raise NotConfigured\n-\n-        if command_executed and command_executed['name'] == 'crawl':\n-            mod = __import__(settings['GROUPSETTINGS_MODULE'], {}, {}, [''])\n-            args = command_executed['args']\n-            if len(args) == 1 and not args[0].startswith('http://'):\n-                domain = args[0]\n-                settings.overrides.update(mod.default_settings)\n-                for group, domains in mod.group_spiders.iteritems():\n-                    if domain in domains:\n-                        settings.overrides.update(mod.group_settings.get(group, {}))\n-        \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4c1ec0c97ec85a0c94b424b2f8543dcb017dfae1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 17 | Churn Cumulative: 338 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -7,20 +7,14 @@ import cProfile\n \n import scrapy\n from scrapy import log\n-from scrapy.spider import spiders\n from scrapy.xlib import lsprofcalltree\n from scrapy.conf import settings\n from scrapy.command.models import ScrapyCommand\n+from scrapy.utils.signal import send_catch_log\n \n-# This dict holds information about the executed command for later use\n-command_executed = {}\n-\n-def _save_command_executed(cmdname, cmd, args, opts):\n-    \"\"\"Save command executed info for later reference\"\"\"\n-    command_executed['name'] = cmdname\n-    command_executed['class'] = cmd\n-    command_executed['args'] = args[:]\n-    command_executed['opts'] = opts.__dict__.copy()\n+# Signal that carries information about the command which was executed\n+# args: cmdname, cmdobj, args, opts\n+command_executed = object()\n \n def _find_commands(dir):\n     try:\n@@ -127,7 +121,8 @@ def execute(argv=None):\n         sys.exit(2)\n \n     del args[0]  # remove command name from args\n-    _save_command_executed(cmdname, cmd, args, opts)\n+    send_catch_log(signal=command_executed, cmdname=cmdname, cmdobj=cmd, \\\n+        args=args, opts=opts)\n     from scrapy.core.manager import scrapymanager\n     scrapymanager.configure(control_reactor=True)\n     ret = _run_command(cmd, args, opts)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a505a9d490143b8aa745930f1977798eeb4d9c84", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 10 | Churn Cumulative: 348 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -131,6 +131,11 @@ def execute(argv=None):\n \n def _run_command(cmd, args, opts):\n     if opts.profile or opts.lsprof:\n+        return _run_command_profiled(cmd, args, opts)\n+    else:\n+        return cmd.run(args, opts)\n+\n+def _run_command_profiled(cmd, args, opts):\n     if opts.profile:\n         log.msg(\"writing cProfile stats to %r\" % opts.profile)\n     if opts.lsprof:\n@@ -144,10 +149,7 @@ def _run_command(cmd, args, opts):\n     if opts.lsprof:\n         with open(opts.lsprof, 'w') as f:\n             k.output(f)\n-        ret = loc['ret']\n-    else:\n-        ret = cmd.run(args, opts)\n-    return ret\n+    return loc['ret']\n \n if __name__ == '__main__':\n     execute()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#39e4df0cff0af45c9e7629fca611e67f9ceb8e48", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 55 | Files Changed: 1 | Hunks: 1 | Methods Changed: 5 | Complexity Δ (Sum/Max): -10/0 | Churn Δ: 55 | Churn Cumulative: 94 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,55 +0,0 @@\n-\"\"\"\n-A pipeline to persist objects using shove. \n-\n-Shove is a \"new generation\" shelve. For more information see: \n-http://pypi.python.org/pypi/shove\n-\"\"\"\n-\n-from string import Template\n-\n-from shove import Shove\n-from scrapy.xlib.pydispatch import dispatcher\n-\n-from scrapy import log\n-from scrapy.core import signals\n-from scrapy.conf import settings\n-from scrapy.core.exceptions import NotConfigured\n-\n-class ShoveItemPipeline(object):\n-\n-    def __init__(self):\n-        self.uritpl = settings['SHOVEITEM_STORE_URI']\n-        if not self.uritpl:\n-            raise NotConfigured\n-        self.opts = settings['SHOVEITEM_STORE_OPT'] or {}\n-        self.stores = {}\n-\n-        dispatcher.connect(self.spider_opened, signal=signals.spider_opened)\n-        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n-\n-    def process_item(self, spider, item):\n-        guid = str(item.guid)\n-\n-        if guid in self.stores[spider]:\n-            if self.stores[spider][guid] == item:\n-                status = 'old'\n-            else:\n-                status = 'upd'\n-        else:\n-            status = 'new'\n-\n-        if not status == 'old':\n-            self.stores[spider][guid] = item\n-        self.log(spider, item, status)\n-        return item\n-\n-    def spider_opened(self, spider):\n-        uri = Template(self.uritpl).substitute(domain=spider.domain_name)\n-        self.stores[spider] = Shove(uri, **self.opts)\n-\n-    def spider_closed(self, spider):\n-        self.stores[spider].sync()\n-\n-    def log(self, spider, item, status):\n-        log.msg(\"Shove (%s): Item guid=%s\" % (status, item.guid), level=log.DEBUG, \\\n-            spider=spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#38a296aa2c04adf9b136ba8000f4f0b7f2364e61", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 20 | Churn Cumulative: 221 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,9 +1,9 @@\n import unittest\n \n from scrapy.xlib.BeautifulSoup import BeautifulSoup\n-from scrapy.http import Response, TextResponse\n+from scrapy.http import Response, TextResponse, HtmlResponse\n from scrapy.utils.response import body_or_str, get_base_url, get_meta_refresh, \\\n-    response_httprepr, get_cached_beautifulsoup\n+    response_httprepr, get_cached_beautifulsoup, open_in_browser\n \n class ResponseUtilsTest(unittest.TestCase):\n     dummy_response = TextResponse(url='http://example.org/', body='dummy_response')\n@@ -131,5 +131,15 @@ class ResponseUtilsTest(unittest.TestCase):\n         assert soup1 is soup2\n         assert soup1 is not soup3\n \n+    def test_open_in_browser(self):\n+        url = \"http:///www.example.com/some/page.html\"\n+        body = \"<html> <head> <title>test page</title> </head> <body>test body</body> </html>\"\n+        response = HtmlResponse(url, body=body)\n+        newbody = open_in_browser(response, debug=True)\n+        assert '<base href=\"%s\">' % url in newbody\n+\n+        self.assertRaises(TypeError, open_in_browser, Response(url, body=body), \\\n+            debug=True)\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -92,7 +92,7 @@ def response_httprepr(response):\n     s += response.body\n     return s\n \n-def open_in_browser(response):\n+def open_in_browser(response, debug=False):\n     \"\"\"Open the given response in a local web browser, populating the <base>\n     tag for external links to work\n     \"\"\"\n@@ -106,4 +106,8 @@ def open_in_browser(response):\n     fd, fname = tempfile.mkstemp('.html')\n     os.write(fd, body)\n     os.close(fd)\n+    if debug: # for testing purposes only\n+        body = open(fname).read()\n+        os.remove(fname)\n+        return body\n     webbrowser.open(\"file://%s\" % fname)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#54ae2c36d025cfcdfd35ffb19d3f582ea2c67f95", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 18 | Churn Cumulative: 239 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,5 @@\n import unittest\n+import urlparse\n \n from scrapy.xlib.BeautifulSoup import BeautifulSoup\n from scrapy.http import Response, TextResponse, HtmlResponse\n@@ -134,10 +135,13 @@ class ResponseUtilsTest(unittest.TestCase):\n     def test_open_in_browser(self):\n         url = \"http:///www.example.com/some/page.html\"\n         body = \"<html> <head> <title>test page</title> </head> <body>test body</body> </html>\"\n+        def browser_open(burl):\n+            bbody = open(urlparse.urlparse(burl).path).read()\n+            assert '<base href=\"%s\">' % url in bbody, \"<base> tag not added\"\n+            return True\n         response = HtmlResponse(url, body=body)\n-        newbody = open_in_browser(response, debug=True)\n-        assert '<base href=\"%s\">' % url in newbody\n-\n+        assert open_in_browser(response, _openfunc=browser_open), \\\n+            \"Browser not called\"\n         self.assertRaises(TypeError, open_in_browser, Response(url, body=body), \\\n             debug=True)\n \n\n@@ -92,7 +92,7 @@ def response_httprepr(response):\n     s += response.body\n     return s\n \n-def open_in_browser(response, debug=False):\n+def open_in_browser(response, _openfunc=webbrowser.open):\n     \"\"\"Open the given response in a local web browser, populating the <base>\n     tag for external links to work\n     \"\"\"\n@@ -106,8 +106,4 @@ def open_in_browser(response, debug=False):\n     fd, fname = tempfile.mkstemp('.html')\n     os.write(fd, body)\n     os.close(fd)\n-    if debug: # for testing purposes only\n-        body = open(fname).read()\n-        os.remove(fname)\n-        return body\n-    webbrowser.open(\"file://%s\" % fname)\n+    return _openfunc(\"file://%s\" % fname)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#403a21ec743325eb289811c87b69ed453361c95b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 117 | Files Changed: 2 | Hunks: 2 | Methods Changed: 5 | Complexity Δ (Sum/Max): -13/0 | Churn Δ: 117 | Churn Cumulative: 236 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,51 +0,0 @@\n-\"\"\"\n-Simple script to follow links from a start url. The links are followed in no\n-particular order.\n-\n-Usage:\n-count_and_follow_links.py <start_url> <links_to_follow>\n-\n-Example:\n-count_and_follow_links.py http://scrapy.org/ 20\n-\n-For each page visisted, this script will print the page body size and the\n-number of links found.\n-\"\"\"\n-\n-import sys\n-from urlparse import urljoin\n-\n-from scrapy.crawler import Crawler\n-from scrapy.selector import HtmlXPathSelector\n-from scrapy.http import Request, HtmlResponse\n-\n-links_followed = 0\n-\n-def parse(response):\n-    global links_followed\n-    links_followed += 1\n-    if links_followed >= links_to_follow:\n-        crawler.stop()\n-\n-    # ignore non-HTML responses\n-    if not isinstance(response, HtmlResponse):\n-        return\n-\n-    links = HtmlXPathSelector(response).select('//a/@href').extract()\n-    abslinks = [urljoin(response.url, l) for l in links]\n-\n-    print \"page %2d/%d: %s\" % (links_followed, links_to_follow, response.url)\n-    print \"  size : %d bytes\" % len(response.body)\n-    print \"  links: %d\" % len(links)\n-    print\n-\n-    return [Request(l, callback=parse) for l in abslinks]\n-\n-if len(sys.argv) != 3:\n-    print __doc__\n-    sys.exit(2)\n-\n-start_url, links_to_follow = sys.argv[1], int(sys.argv[2])\n-request = Request(start_url, callback=parse)\n-crawler = Crawler()\n-crawler.crawl(request)\n\n@@ -1,66 +0,0 @@\n-\"\"\"\n-Crawler class\n-\n-The Crawler class can be used to crawl pages using the Scrapy crawler from\n-outside a Scrapy project, for example, from a standalone script. \n-\n-To use it, instantiate it and call the \"crawl\" method with one (or more)\n-requests. For example:\n-\n-    >>> from scrapy.crawler import Crawler\n-    >>> from scrapy.http import Request\n-    >>> def parse_response(response):\n-    ...     print \"Visited: %s\" % response.url\n-    ...\n-    >>> request = Request('http://scrapy.org', callback=parse_response)\n-    >>> crawler = Crawler()\n-    >>> crawler.crawl(request)\n-    Visited: http://scrapy.org \n-    >>>\n-\n-Request callbacks follow the same API of spiders callback, which means that all\n-requests returned from the callbacks will be followed.\n-\n-See examples/scripts/count_and_follow_links.py for a more detailed example.\n-\n-WARNING: The Crawler class currently has a big limitation - it cannot be used\n-more than once in the same Python process. This is due to the fact that Twisted\n-reactors cannot be restarted. Hopefully, this limitation will be removed in the\n-future.\n-\"\"\"\n-\n-from scrapy.xlib.pydispatch import dispatcher\n-from scrapy.core.manager import scrapymanager\n-from scrapy.core.engine import scrapyengine\n-from scrapy.conf import settings as scrapy_settings\n-from scrapy import log\n-\n-class Crawler(object):\n-\n-    def __init__(self, enable_log=False, stop_on_error=False, silence_errors=False, \\\n-            settings=None):\n-        self.stop_on_error = stop_on_error\n-        self.silence_errors = silence_errors\n-        # disable offsite middleware (by default) because it prevents free crawling\n-        if settings is not None:\n-            settings.overrides.update(settings)\n-        scrapy_settings.overrides['SPIDER_MIDDLEWARES'] = {\n-            'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': None}\n-        scrapy_settings.overrides['LOG_ENABLED'] = enable_log\n-        scrapymanager.configure()\n-        dispatcher.connect(self._logmessage_received, signal=log.logmessage_received)\n-\n-    def crawl(self, *args):\n-        scrapymanager.runonce(*args)\n-\n-    def stop(self):\n-        scrapyengine.stop()\n-        log.log_level = log.SILENT\n-        scrapyengine.kill()\n-\n-    def _logmessage_received(self, message, level):\n-        if level <= log.ERROR:\n-            if not self.silence_errors:\n-                print \"Crawler error: %s\" % message\n-            if self.stop_on_error:\n-                self.stop()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#87e68e74381b6961c59c16efd212cbcb2d9e0386", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 19 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 30 | Churn Cumulative: 32 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -47,34 +47,26 @@ class MailSender(object):\n                 part = MIMEBase(*mimetype.split('/'))\n                 part.set_payload(f.read())\n                 Encoders.encode_base64(part)\n-                part.add_header('Content-Disposition', 'attachment; filename=\"%s\"' % attach_name)\n+                part.add_header('Content-Disposition', 'attachment; filename=\"%s\"' \\\n+                    % attach_name)\n                 msg.attach(part)\n         else:\n             msg.set_payload(body)\n \n-        # FIXME ---------------------------------------------------------------------\n-        # There seems to be a problem with sending emails using deferreds when\n-        # the last thing left to do is sending the mail, cause the engine stops\n-        # the reactor and the email don't get send. we need to fix this. until\n-        # then, we'll revert to use Python standard (IO-blocking) smtplib.\n-\n-        #dfd = self._sendmail(self.smtphost, self.mailfrom, rcpts, msg.as_string())\n-        #dfd.addCallbacks(self._sent_ok, self._sent_failed,\n-        #    callbackArgs=[to, cc, subject, len(attachs)],\n-        #    errbackArgs=[to, cc, subject, len(attachs)])\n-        import smtplib\n-        smtp = smtplib.SMTP(self.smtphost)\n-        smtp.sendmail(self.mailfrom, rcpts, msg.as_string())\n-        log.msg('Mail sent: To=%s Cc=%s Subject=\"%s\"' % (to, cc, subject))\n-        smtp.close()\n-        # ---------------------------------------------------------------------------\n+        dfd = self._sendmail(self.smtphost, self.mailfrom, rcpts, msg.as_string())\n+        dfd.addCallbacks(self._sent_ok, self._sent_failed,\n+            callbackArgs=[to, cc, subject, len(attachs)],\n+            errbackArgs=[to, cc, subject, len(attachs)])\n+        reactor.addSystemEventTrigger('before', 'shutdown', lambda: dfd)\n \n     def _sent_ok(self, result, to, cc, subject, nattachs):\n-        log.msg('Mail sent OK: To=%s Cc=%s Subject=\"%s\" Attachs=%d' % (to, cc, subject, nattachs))\n+        log.msg('Mail sent OK: To=%s Cc=%s Subject=\"%s\" Attachs=%d' % \\\n+            (to, cc, subject, nattachs))\n \n     def _sent_failed(self, failure, to, cc, subject, nattachs):\n         errstr = str(failure.value)\n-        log.msg('Unable to send mail: To=%s Cc=%s Subject=\"%s\" Attachs=%d - %s' % (to, cc, subject, nattachs, errstr), level=log.ERROR)\n+        log.msg('Unable to send mail: To=%s Cc=%s Subject=\"%s\" Attachs=%d - %s' % \\\n+            (to, cc, subject, nattachs, errstr), level=log.ERROR)\n \n     def _sendmail(self, smtphost, from_addr, to_addrs, msg, port=25):\n         \"\"\" This is based on twisted.mail.smtp.sendmail except that it\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4fa833c849e98ff9d0fe1e063db575f23f1d929a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 123 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -29,8 +29,9 @@ BOT_NAME = settings['BOT_NAME']\n # args: message, level, spider\n logmessage_received = object()\n \n-# default logging level\n+# default values\n log_level = DEBUG\n+log_encoding = 'utf-8'\n \n started = False\n \n@@ -47,11 +48,12 @@ def _get_log_level(level_name_or_id=None):\n \n def start(logfile=None, loglevel=None, logstdout=None):\n     \"\"\"Initialize and start logging facility\"\"\"\n-    global log_level, started\n+    global log_level, log_encoding, started\n \n     if started or not settings.getbool('LOG_ENABLED'):\n         return\n     log_level = _get_log_level(loglevel)\n+    log_encoding = settings['LOG_ENCODING']\n     started = True\n \n     # set log observer\n@@ -74,7 +76,7 @@ def msg(message, level=INFO, component=BOT_NAME, domain=None, spider=None):\n     dispatcher.send(signal=logmessage_received, message=message, level=level, \\\n         spider=spider)\n     system = domain or (spider.domain_name if spider else component)\n-    msg_txt = unicode_to_str(\"%s: %s\" % (level_names[level], message))\n+    msg_txt = unicode_to_str(\"%s: %s\" % (level_names[level], message), log_encoding)\n     log.msg(msg_txt, system=system)\n \n def exc(message, level=ERROR, component=BOT_NAME, domain=None, spider=None):\n@@ -93,5 +95,5 @@ def err(_stuff=None, _why=None, **kwargs):\n             \"use 'spider' argument instead\", DeprecationWarning, stacklevel=2)\n     kwargs['system'] = domain or (spider.domain_name if spider else component)\n     if _why:\n-        _why = unicode_to_str(\"ERROR: %s\" % _why)\n+        _why = unicode_to_str(\"ERROR: %s\" % _why, log_encoding)\n     log.err(_stuff, _why, **kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#45411926b52c678e079ae3d5bb2c80bcfd9a6974", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 27 | Files Changed: 7 | Hunks: 28 | Methods Changed: 21 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 62 | Churn Cumulative: 750 | Contributors (this commit): 6 | Commits (past 90d): 11 | Contributors (cumulative): 17 | DMM Complexity: 0.8571428571428571\n\nDIFF:\n@@ -18,6 +18,7 @@ from scrapy.xlib import twisted_250_monkeypatches\n from scrapy.utils.encoding import add_encoding_alias\n add_encoding_alias('gb2312', 'zh-cn')\n add_encoding_alias('cp1251', 'win-1251')\n+add_encoding_alias('cp1252', 'iso8859-1', overwrite=True)\n \n # optional_features is a set containing Scrapy optional features\n optional_features = set()\n\n@@ -101,6 +101,7 @@ ITEM_PROCESSOR = 'scrapy.contrib.pipeline.ItemPipelineManager'\n ITEM_PIPELINES = []\n \n LOG_ENABLED = True\n+LOG_ENCODING = 'utf-8'\n LOG_FORMATTER_CRAWLED = 'scrapy.contrib.logformatter.crawled_logline'\n LOG_STDOUT = False\n LOG_LEVEL = 'DEBUG'\n\n@@ -3,7 +3,6 @@ This module implements the HtmlImageLinkExtractor for extracting\n image links only.\n \"\"\"\n \n-import urlparse\n \n from scrapy.link import Link\n from scrapy.utils.url import canonicalize_url, urljoin_rfc\n@@ -25,13 +24,13 @@ class HTMLImageLinkExtractor(object):\n         self.unique = unique\n         self.canonicalize = canonicalize\n \n-    def extract_from_selector(self, selector, parent=None):\n+    def extract_from_selector(self, selector, encoding, parent=None):\n         ret = []\n         def _add_link(url_sel, alt_sel=None):\n             url = flatten([url_sel.extract()])\n             alt = flatten([alt_sel.extract()]) if alt_sel else (u'', )\n             if url:\n-                ret.append(Link(unicode_to_str(url[0]), alt[0]))\n+                ret.append(Link(unicode_to_str(url[0], encoding), alt[0]))\n \n         if selector.xmlNode.type == 'element':\n             if selector.xmlNode.name == 'img':\n@@ -41,7 +40,7 @@ class HTMLImageLinkExtractor(object):\n                 children = selector.select('child::*')\n                 if len(children):\n                     for child in children:\n-                        ret.extend(self.extract_from_selector(child, parent=selector))\n+                        ret.extend(self.extract_from_selector(child, encoding, parent=selector))\n                 elif selector.xmlNode.name == 'a' and not parent:\n                     _add_link(selector.select('@href'), selector.select('@title'))\n         else:\n@@ -52,7 +51,8 @@ class HTMLImageLinkExtractor(object):\n     def extract_links(self, response):\n         xs = HtmlXPathSelector(response)\n         base_url = xs.select('//base/@href').extract()\n-        base_url = unicode_to_str(base_url[0]) if base_url else unicode_to_str(response.url)\n+        base_url = unicode_to_str(base_url[0], response.encoding) if base_url \\\n+            else unicode_to_str(response.url, response.encoding)\n \n         links = []\n         for location in self.locations:\n@@ -64,7 +64,7 @@ class HTMLImageLinkExtractor(object):\n                 continue\n \n             for selector in selectors:\n-                links.extend(self.extract_from_selector(selector))\n+                links.extend(self.extract_from_selector(selector, response.encoding))\n \n         seen, ret = set(), []\n         for link in links:\n\n@@ -29,8 +29,8 @@ class XPathSelector(object_ref):\n             self.doc = Libxml2Document(response, factory=self._get_libxml2_doc)\n             self.xmlNode = self.doc.xmlDoc\n         elif text:\n-            response = TextResponse(url='about:blank', body=unicode_to_str(text), \\\n-                encoding='utf-8')\n+            response = TextResponse(url='about:blank', \\\n+                body=unicode_to_str(text, 'utf-8'), encoding='utf-8')\n             self.doc = Libxml2Document(response, factory=self._get_libxml2_doc)\n             self.xmlNode = self.doc.xmlDoc\n         self.expr = expr\n\n@@ -60,10 +60,10 @@ def remove_entities(text, keep=(), remove_illegal=True, encoding='utf-8'):\n \n     return _ent_re.sub(convert_entity, str_to_unicode(text, encoding))\n \n-def has_entities(text):\n-    return bool(_ent_re.search(str_to_unicode(text)))\n+def has_entities(text, encoding=None):\n+    return bool(_ent_re.search(str_to_unicode(text, encoding)))\n \n-def replace_tags(text, token=''):\n+def replace_tags(text, token='', encoding=None):\n     \"\"\"Replace all markup tags found in the given text by the given token. By\n     default token is a null string so it just remove all tags.\n \n@@ -71,14 +71,14 @@ def replace_tags(text, token=''):\n \n     Always returns a unicode string.\n     \"\"\"\n-    return _tag_re.sub(token, str_to_unicode(text))\n+    return _tag_re.sub(token, str_to_unicode(text, encoding))\n \n \n-def remove_comments(text):\n+def remove_comments(text, encoding=None):\n     \"\"\" Remove HTML Comments. \"\"\"\n-    return re.sub('<!--.*?-->', u'', str_to_unicode(text), re.DOTALL)\n+    return re.sub('<!--.*?-->', u'', str_to_unicode(text, encoding), re.DOTALL)\n       \n-def remove_tags(text, which_ones=()):\n+def remove_tags(text, which_ones=(), encoding=None):\n     \"\"\" Remove HTML Tags only. \n \n         which_ones -- is a tuple of which tags we want to remove.\n@@ -91,15 +91,15 @@ def remove_tags(text, which_ones=()):\n         regex = '<.*?>'\n     retags = re.compile(regex, re.DOTALL | re.IGNORECASE)\n \n-    return retags.sub(u'', str_to_unicode(text))\n+    return retags.sub(u'', str_to_unicode(text, encoding))\n \n-def remove_tags_with_content(text, which_ones=()):\n+def remove_tags_with_content(text, which_ones=(), encoding=None):\n     \"\"\" Remove tags and its content.\n         \n         which_ones -- is a tuple of which tags with its content we want to remove.\n                       if is empty do nothing.\n     \"\"\"\n-    text = str_to_unicode(text)\n+    text = str_to_unicode(text, encoding)\n     if which_ones:\n         tags = '|'.join(['<%s.*?</%s>' % (tag, tag) for tag in which_ones])\n         retags = re.compile(tags, re.DOTALL | re.IGNORECASE)\n@@ -107,7 +107,8 @@ def remove_tags_with_content(text, which_ones=()):\n     return text\n     \n \n-def replace_escape_chars(text, which_ones=('\\n','\\t','\\r'), replace_by=u''):\n+def replace_escape_chars(text, which_ones=('\\n', '\\t', '\\r'), replace_by=u'', \\\n+        encoding=None):\n     \"\"\" Remove escape chars. Default : \\\\n, \\\\t, \\\\r\n \n         which_ones -- is a tuple of which escape chars we want to remove.\n@@ -117,10 +118,10 @@ def replace_escape_chars(text, which_ones=('\\n','\\t','\\r'), replace_by=u''):\n                       It defaults to '', so the escape chars are removed.\n     \"\"\"\n     for ec in which_ones:\n-        text = text.replace(ec, str_to_unicode(replace_by))\n-    return str_to_unicode(text)\n+        text = text.replace(ec, str_to_unicode(replace_by, encoding))\n+    return str_to_unicode(text, encoding)\n \n-def unquote_markup(text, keep=(), remove_illegal=True):\n+def unquote_markup(text, keep=(), remove_illegal=True, encoding=None):\n     \"\"\"\n     This function receives markup as a text (always a unicode string or a utf-8 encoded string) and does the following:\n      - removes entities (except the ones in 'keep') from any part of it that it's not inside a CDATA\n@@ -138,7 +139,7 @@ def unquote_markup(text, keep=(), remove_illegal=True):\n             offset = match_e\n         yield txt[offset:]\n \n-    text = str_to_unicode(text)\n+    text = str_to_unicode(text, encoding)\n     ret_text = u''\n     for fragment in _get_fragments(text, _cdata_re):\n         if isinstance(fragment, basestring):\n\n@@ -64,13 +64,15 @@ def unique(list_, key=lambda x: x):\n     return result\n \n \n-def str_to_unicode(text, encoding='utf-8'):\n+def str_to_unicode(text, encoding=None):\n     \"\"\"Return the unicode representation of text in the given encoding. Unlike\n     .encode(encoding) this function can be applied directly to a unicode\n     object without the risk of double-decoding problems (which can happen if\n     you don't use the default 'ascii' encoding)\n     \"\"\"\n     \n+    if encoding is None:\n+        encoding = 'utf-8'\n     if isinstance(text, str):\n         return text.decode(encoding)\n     elif isinstance(text, unicode):\n@@ -78,13 +80,15 @@ def str_to_unicode(text, encoding='utf-8'):\n     else:\n         raise TypeError('str_to_unicode must receive a str or unicode object, got %s' % type(text).__name__)\n \n-def unicode_to_str(text, encoding='utf-8'):\n+def unicode_to_str(text, encoding=None):\n     \"\"\"Return the str representation of text in the given encoding. Unlike\n     .encode(encoding) this function can be applied directly to a str\n     object without the risk of double-decoding problems (which can happen if\n     you don't use the default 'ascii' encoding)\n     \"\"\"\n \n+    if encoding is None:\n+        encoding = 'utf-8'\n     if isinstance(text, unicode):\n         return text.encode(encoding)\n     elif isinstance(text, str):\n\n@@ -130,7 +130,8 @@ def add_or_replace_parameter(url, name, new_value, sep='&', url_is_quoted=False)\n                                name+'='+new_value)\n     return next_url\n \n-def canonicalize_url(url, keep_blank_values=True, keep_fragments=False):\n+def canonicalize_url(url, keep_blank_values=True, keep_fragments=False, \\\n+        encoding=None):\n     \"\"\"Canonicalize the given url by applying the following procedures:\n \n     - sort query arguments, first by key, then by value\n@@ -147,7 +148,7 @@ def canonicalize_url(url, keep_blank_values=True, keep_fragments=False):\n     For examples see the tests in scrapy.tests.test_utils_url\n     \"\"\"\n \n-    url = unicode_to_str(url)\n+    url = unicode_to_str(url, encoding)\n     scheme, netloc, path, params, query, fragment = urlparse.urlparse(url)\n     keyvals = cgi.parse_qsl(query, keep_blank_values)\n     keyvals.sort()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
