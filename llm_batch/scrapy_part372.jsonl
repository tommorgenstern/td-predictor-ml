{"custom_id": "scrapy#d2c05d9d96394e111ead1aa097402cdbc18c0859", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 5 | Churn Cumulative: 495 | Contributors (this commit): 15 | Commits (past 90d): 3 | Contributors (cumulative): 19 | DMM Complexity: 0.0\n\nDIFF:\n@@ -34,8 +34,7 @@ _T = TypeVar(\"_T\")\n \n # https://stackoverflow.com/questions/60222982\n @overload\n-def iterate_spider_output(result: AsyncGenerator) -> AsyncGenerator:  # type: ignore[misc]\n-    ...\n+def iterate_spider_output(result: AsyncGenerator) -> AsyncGenerator: ...  # type: ignore[overload-overlap]\n \n \n @overload\n\n@@ -1,6 +1,6 @@\n from typing import Any, Optional\n \n-import OpenSSL._util as pyOpenSSLutil  # type: ignore[import-untyped]\n+import OpenSSL._util as pyOpenSSLutil\n import OpenSSL.SSL\n import OpenSSL.version\n from OpenSSL.crypto import X509Name\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#63acd0720970c87450fdbcb9aa6967118c9c1cf2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 6 | Churn Cumulative: 1887 | Contributors (this commit): 41 | Commits (past 90d): 9 | Contributors (cumulative): 60 | DMM Complexity: None\n\nDIFF:\n@@ -135,7 +135,7 @@ class HttpCompressionMiddleware:\n                 respcls = responsetypes.from_args(\n                     headers=response.headers, url=response.url, body=decoded_body\n                 )\n-                kwargs = dict(cls=respcls, body=decoded_body)\n+                kwargs = {\"cls\": respcls, \"body\": decoded_body}\n                 if issubclass(respcls, TextResponse):\n                     # force recalculating the encoding until we make sure the\n                     # responsetypes guessing is reliable\n\n@@ -85,7 +85,7 @@ class CrawlSpider(Spider):\n             url=link.url,\n             callback=self._callback,\n             errback=self._errback,\n-            meta=dict(rule=rule_index, link_text=link.text),\n+            meta={\"rule\": rule_index, \"link_text\": link.text},\n         )\n \n     def _requests_to_follow(self, response):\n\n@@ -162,7 +162,7 @@ def re_rsearch(\n         pattern = re.compile(pattern)\n \n     for chunk, offset in _chunk_iter():\n-        matches = [match for match in pattern.finditer(chunk)]\n+        matches = list(pattern.finditer(chunk))\n         if matches:\n             start, end = matches[-1].span()\n             return offset + start, offset + end\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#26a16f2c43dc96fe33d0b0fc8846402e9ae97e9a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 117 | Lines Deleted: 114 | Files Changed: 15 | Hunks: 82 | Methods Changed: 53 | Complexity Δ (Sum/Max): -28/0 | Churn Δ: 231 | Churn Cumulative: 14301 | Contributors (this commit): 73 | Commits (past 90d): 26 | Contributors (cumulative): 237 | DMM Complexity: 1.0\n\nDIFF:\n@@ -76,11 +76,11 @@ class CrawlTestCase(TestCase):\n \n     @defer.inlineCallbacks\n     def _test_delay(self, total, delay, randomize=False):\n-        crawl_kwargs = dict(\n-            maxlatency=delay * 2,\n-            mockserver=self.mockserver,\n-            total=total,\n-        )\n+        crawl_kwargs = {\n+            \"maxlatency\": delay * 2,\n+            \"mockserver\": self.mockserver,\n+            \"total\": total,\n+        }\n         tolerance = 1 - (0.6 if randomize else 0.2)\n \n         settings = {\"DOWNLOAD_DELAY\": delay, \"RANDOMIZE_DOWNLOAD_DELAY\": randomize}\n\n@@ -320,7 +320,7 @@ class CookiesMiddlewareTest(TestCase):\n \n     @pytest.mark.xfail(reason=\"Cookie header is not currently being processed\")\n     def test_keep_cookie_from_default_request_headers_middleware(self):\n-        DEFAULT_REQUEST_HEADERS = dict(Cookie=\"default=value; asdf=qwerty\")\n+        DEFAULT_REQUEST_HEADERS = {\"Cookie\": \"default=value; asdf=qwerty\"}\n         mw_default_headers = DefaultHeadersMiddleware(DEFAULT_REQUEST_HEADERS.items())\n         # overwrite with values from 'cookies' request argument\n         req1 = Request(\"http://example.org\", cookies={\"default\": \"something\"})\n\n@@ -59,7 +59,7 @@ class HttpAuthMiddlewareTest(unittest.TestCase):\n         self.assertEqual(req.headers[\"Authorization\"], basic_auth_header(\"foo\", \"bar\"))\n \n     def test_auth_already_set(self):\n-        req = Request(\"http://example.com/\", headers=dict(Authorization=\"Digest 123\"))\n+        req = Request(\"http://example.com/\", headers={\"Authorization\": \"Digest 123\"})\n         assert self.mw.process_request(req, self.spider) is None\n         self.assertEqual(req.headers[\"Authorization\"], b\"Digest 123\")\n \n@@ -79,6 +79,6 @@ class HttpAuthAnyMiddlewareTest(unittest.TestCase):\n         self.assertEqual(req.headers[\"Authorization\"], basic_auth_header(\"foo\", \"bar\"))\n \n     def test_auth_already_set(self):\n-        req = Request(\"http://example.com/\", headers=dict(Authorization=\"Digest 123\"))\n+        req = Request(\"http://example.com/\", headers={\"Authorization\": \"Digest 123\"})\n         assert self.mw.process_request(req, self.spider) is None\n         self.assertEqual(req.headers[\"Authorization\"], b\"Digest 123\")\n\n@@ -152,7 +152,7 @@ class PythonItemExporterTest(BaseItemExporterTest):\n \n     def test_nested_item(self):\n         i1 = self.item_class(name=\"Joseph\", age=\"22\")\n-        i2 = dict(name=\"Maria\", age=i1)\n+        i2 = {\"name\": \"Maria\", \"age\": i1}\n         i3 = self.item_class(name=\"Jesus\", age=i2)\n         ie = self._get_exporter()\n         exported = ie.export_item(i3)\n@@ -185,7 +185,7 @@ class PythonItemExporterTest(BaseItemExporterTest):\n \n     def test_export_item_dict_list(self):\n         i1 = self.item_class(name=\"Joseph\", age=\"22\")\n-        i2 = dict(name=\"Maria\", age=[i1])\n+        i2 = {\"name\": \"Maria\", \"age\": [i1]}\n         i3 = self.item_class(name=\"Jesus\", age=[i2])\n         ie = self._get_exporter()\n         exported = ie.export_item(i3)\n@@ -373,7 +373,7 @@ class CsvItemExporterTest(BaseItemExporterTest):\n \n     def test_join_multivalue_not_strings(self):\n         self.assertExportResult(\n-            item=dict(name=\"John\", friends=[4, 8]),\n+            item={\"name\": \"John\", \"friends\": [4, 8]},\n             include_headers_line=False,\n             expected='\"[4, 8]\",John\\r\\n',\n         )\n@@ -388,14 +388,14 @@ class CsvItemExporterTest(BaseItemExporterTest):\n     def test_errors_default(self):\n         with self.assertRaises(UnicodeEncodeError):\n             self.assertExportResult(\n-                item=dict(text=\"W\\u0275\\u200Brd\"),\n+                item={\"text\": \"W\\u0275\\u200Brd\"},\n                 expected=None,\n                 encoding=\"windows-1251\",\n             )\n \n     def test_errors_xmlcharrefreplace(self):\n         self.assertExportResult(\n-            item=dict(text=\"W\\u0275\\u200Brd\"),\n+            item={\"text\": \"W\\u0275\\u200Brd\"},\n             include_headers_line=False,\n             expected=\"W&#629;&#8203;rd\\r\\n\",\n             encoding=\"windows-1251\",\n@@ -455,8 +455,8 @@ class XmlItemExporterTest(BaseItemExporterTest):\n         )\n \n     def test_nested_item(self):\n-        i1 = dict(name=\"foo\\xa3hoo\", age=\"22\")\n-        i2 = dict(name=\"bar\", age=i1)\n+        i1 = {\"name\": \"foo\\xa3hoo\", \"age\": \"22\"}\n+        i2 = {\"name\": \"bar\", \"age\": i1}\n         i3 = self.item_class(name=\"buz\", age=i2)\n \n         self.assertExportResult(\n@@ -478,8 +478,8 @@ class XmlItemExporterTest(BaseItemExporterTest):\n         )\n \n     def test_nested_list_item(self):\n-        i1 = dict(name=\"foo\")\n-        i2 = dict(name=\"bar\", v2={\"egg\": [\"spam\"]})\n+        i1 = {\"name\": \"foo\"}\n+        i2 = {\"name\": \"bar\", \"v2\": {\"egg\": [\"spam\"]}}\n         i3 = self.item_class(name=\"buz\", age=[i1, i2])\n \n         self.assertExportResult(\n@@ -534,7 +534,7 @@ class JsonLinesItemExporterTest(BaseItemExporterTest):\n \n     def test_nested_item(self):\n         i1 = self.item_class(name=\"Joseph\", age=\"22\")\n-        i2 = dict(name=\"Maria\", age=i1)\n+        i2 = {\"name\": \"Maria\", \"age\": i1}\n         i3 = self.item_class(name=\"Jesus\", age=i2)\n         self.ie.start_exporting()\n         self.ie.export_item(i3)\n@@ -622,9 +622,9 @@ class JsonItemExporterTest(JsonLinesItemExporterTest):\n         self.assertEqual(exported, [expected])\n \n     def test_nested_dict_item(self):\n-        i1 = dict(name=\"Joseph\\xa3\", age=\"22\")\n+        i1 = {\"name\": \"Joseph\\xa3\", \"age\": \"22\"}\n         i2 = self.item_class(name=\"Maria\", age=i1)\n-        i3 = dict(name=\"Jesus\", age=i2)\n+        i3 = {\"name\": \"Jesus\", \"age\": i2}\n         self.ie.start_exporting()\n         self.ie.export_item(i3)\n         self.ie.finish_exporting()\n\n@@ -37,7 +37,7 @@ class Base:\n             page4_url = \"http://example.com/page%204.html\"\n \n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                     Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n@@ -55,7 +55,7 @@ class Base:\n         def test_extract_filter_allow(self):\n             lx = self.extractor_cls(allow=(\"sample\",))\n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                     Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n@@ -70,7 +70,7 @@ class Base:\n         def test_extract_filter_allow_with_duplicates(self):\n             lx = self.extractor_cls(allow=(\"sample\",), unique=False)\n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                     Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n@@ -93,7 +93,7 @@ class Base:\n         def test_extract_filter_allow_with_duplicates_canonicalize(self):\n             lx = self.extractor_cls(allow=(\"sample\",), unique=False, canonicalize=True)\n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                     Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n@@ -116,7 +116,7 @@ class Base:\n         def test_extract_filter_allow_no_duplicates_canonicalize(self):\n             lx = self.extractor_cls(allow=(\"sample\",), unique=True, canonicalize=True)\n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                     Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n@@ -127,7 +127,7 @@ class Base:\n         def test_extract_filter_allow_and_deny(self):\n             lx = self.extractor_cls(allow=(\"sample\",), deny=(\"3\",))\n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                     Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n@@ -137,7 +137,7 @@ class Base:\n         def test_extract_filter_allowed_domains(self):\n             lx = self.extractor_cls(allow_domains=(\"google.com\",))\n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://www.google.com/something\", text=\"\"),\n                 ],\n@@ -148,7 +148,7 @@ class Base:\n \n             lx = self.extractor_cls(allow=\"sample\")\n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                     Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n@@ -162,7 +162,7 @@ class Base:\n \n             lx = self.extractor_cls(allow=\"sample\", deny=\"3\")\n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                     Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n@@ -171,7 +171,7 @@ class Base:\n \n             lx = self.extractor_cls(allow_domains=\"google.com\")\n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://www.google.com/something\", text=\"\"),\n                 ],\n@@ -179,7 +179,7 @@ class Base:\n \n             lx = self.extractor_cls(deny_domains=\"example.com\")\n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://www.google.com/something\", text=\"\"),\n                 ],\n@@ -265,7 +265,7 @@ class Base:\n         def test_restrict_xpaths(self):\n             lx = self.extractor_cls(restrict_xpaths=('//div[@id=\"subwrapper\"]',))\n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                     Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n@@ -337,7 +337,7 @@ class Base:\n                 restrict_css=(\"#subwrapper + a\",),\n             )\n             self.assertEqual(\n-                [link for link in lx.extract_links(self.response)],\n+                list(lx.extract_links(self.response)),\n                 [\n                     Link(url=\"http://example.com/sample1.html\", text=\"\"),\n                     Link(url=\"http://example.com/sample2.html\", text=\"sample 2\"),\n@@ -705,7 +705,7 @@ class Base:\n             response = HtmlResponse(\"http://example.org/index.html\", body=html)\n             lx = self.extractor_cls()\n             self.assertEqual(\n-                [link for link in lx.extract_links(response)],\n+                list(lx.extract_links(response)),\n                 [\n                     Link(\n                         url=\"http://example.org/item1.html\",\n@@ -758,7 +758,7 @@ class LxmlLinkExtractorTestCase(Base.LinkExtractorTestCase):\n         response = HtmlResponse(\"http://example.org/index.html\", body=html)\n         lx = self.extractor_cls()\n         self.assertEqual(\n-            [link for link in lx.extract_links(response)],\n+            list(lx.extract_links(response)),\n             [\n                 Link(\n                     url=\"http://example.org/item1.html\", text=\"Item 1\", nofollow=False\n@@ -779,7 +779,7 @@ class LxmlLinkExtractorTestCase(Base.LinkExtractorTestCase):\n         # Simple text inclusion test\n         lx = self.extractor_cls(restrict_text=\"dog\")\n         self.assertEqual(\n-            [link for link in lx.extract_links(response)],\n+            list(lx.extract_links(response)),\n             [\n                 Link(\n                     url=\"http://example.org/item2.html\",\n@@ -791,7 +791,7 @@ class LxmlLinkExtractorTestCase(Base.LinkExtractorTestCase):\n         # Unique regex test\n         lx = self.extractor_cls(restrict_text=r\"of.*dog\")\n         self.assertEqual(\n-            [link for link in lx.extract_links(response)],\n+            list(lx.extract_links(response)),\n             [\n                 Link(\n                     url=\"http://example.org/item2.html\",\n@@ -803,7 +803,7 @@ class LxmlLinkExtractorTestCase(Base.LinkExtractorTestCase):\n         # Multiple regex test\n         lx = self.extractor_cls(restrict_text=[r\"of.*dog\", r\"of.*cat\"])\n         self.assertEqual(\n-            [link for link in lx.extract_links(response)],\n+            list(lx.extract_links(response)),\n             [\n                 Link(\n                     url=\"http://example.org/item1.html\",\n@@ -834,7 +834,7 @@ class LxmlLinkExtractorTestCase(Base.LinkExtractorTestCase):\n         response = HtmlResponse(\"http://example.org/index.html\", body=html)\n         lx = self.extractor_cls()\n         self.assertEqual(\n-            [link for link in lx.extract_links(response)],\n+            list(lx.extract_links(response)),\n             [\n                 Link(\n                     url=\"http://example.org/item2.html\",\n\n@@ -565,37 +565,37 @@ class NoInputReprocessingFromDictTest(unittest.TestCase):\n     \"\"\"\n \n     def test_avoid_reprocessing_with_initial_values_single(self):\n-        il = NoInputReprocessingDictLoader(item=dict(title=\"foo\"))\n+        il = NoInputReprocessingDictLoader(item={\"title\": \"foo\"})\n         il_loaded = il.load_item()\n-        self.assertEqual(il_loaded, dict(title=\"foo\"))\n+        self.assertEqual(il_loaded, {\"title\": \"foo\"})\n         self.assertEqual(\n-            NoInputReprocessingDictLoader(item=il_loaded).load_item(), dict(title=\"foo\")\n+            NoInputReprocessingDictLoader(item=il_loaded).load_item(), {\"title\": \"foo\"}\n         )\n \n     def test_avoid_reprocessing_with_initial_values_list(self):\n-        il = NoInputReprocessingDictLoader(item=dict(title=[\"foo\", \"bar\"]))\n+        il = NoInputReprocessingDictLoader(item={\"title\": [\"foo\", \"bar\"]})\n         il_loaded = il.load_item()\n-        self.assertEqual(il_loaded, dict(title=\"foo\"))\n+        self.assertEqual(il_loaded, {\"title\": \"foo\"})\n         self.assertEqual(\n-            NoInputReprocessingDictLoader(item=il_loaded).load_item(), dict(title=\"foo\")\n+            NoInputReprocessingDictLoader(item=il_loaded).load_item(), {\"title\": \"foo\"}\n         )\n \n     def test_avoid_reprocessing_without_initial_values_single(self):\n         il = NoInputReprocessingDictLoader()\n         il.add_value(\"title\", \"foo\")\n         il_loaded = il.load_item()\n-        self.assertEqual(il_loaded, dict(title=\"FOO\"))\n+        self.assertEqual(il_loaded, {\"title\": \"FOO\"})\n         self.assertEqual(\n-            NoInputReprocessingDictLoader(item=il_loaded).load_item(), dict(title=\"FOO\")\n+            NoInputReprocessingDictLoader(item=il_loaded).load_item(), {\"title\": \"FOO\"}\n         )\n \n     def test_avoid_reprocessing_without_initial_values_list(self):\n         il = NoInputReprocessingDictLoader()\n         il.add_value(\"title\", [\"foo\", \"bar\"])\n         il_loaded = il.load_item()\n-        self.assertEqual(il_loaded, dict(title=\"FOO\"))\n+        self.assertEqual(il_loaded, {\"title\": \"FOO\"})\n         self.assertEqual(\n-            NoInputReprocessingDictLoader(item=il_loaded).load_item(), dict(title=\"FOO\")\n+            NoInputReprocessingDictLoader(item=il_loaded).load_item(), {\"title\": \"FOO\"}\n         )\n \n \n\n@@ -91,7 +91,7 @@ class MailSenderTest(unittest.TestCase):\n         self.assertEqual(attach.get_payload(decode=True), b\"content\")\n \n     def _catch_mail_sent(self, **kwargs):\n-        self.catched_msg = dict(**kwargs)\n+        self.catched_msg = {**kwargs}\n \n     def test_send_utf8(self):\n         subject = \"sübjèçt\"\n\n@@ -140,7 +140,7 @@ class FileDownloadCrawlTestCase(TestCase):\n         self.assertEqual(logs.count(file_dl_failure), 3)\n \n         # check that no files were written to the media store\n-        self.assertEqual([x for x in self.tmpmediastore.iterdir()], [])\n+        self.assertEqual(list(self.tmpmediastore.iterdir()), [])\n \n     @defer.inlineCallbacks\n     def test_download_media(self):\n\n@@ -221,7 +221,7 @@ class FilesPipelineTestCase(unittest.TestCase):\n         file_path = CustomFilesPipeline.from_settings(\n             Settings({\"FILES_STORE\": self.tempdir})\n         ).file_path\n-        item = dict(path=\"path-to-store-file\")\n+        item = {\"path\": \"path-to-store-file\"}\n         request = Request(\"http://example.com\")\n         self.assertEqual(file_path(request, item=item), \"full/path-to-store-file\")\n \n\n@@ -132,7 +132,7 @@ class ImagesPipelineTestCase(unittest.TestCase):\n         thumb_path = CustomImagesPipeline.from_settings(\n             Settings({\"IMAGES_STORE\": self.tempdir})\n         ).thumb_path\n-        item = dict(path=\"path-to-store-file\")\n+        item = {\"path\": \"path-to-store-file\"}\n         request = Request(\"http://example.com\")\n         self.assertEqual(\n             thumb_path(request, \"small\", item=item), \"thumb/small/path-to-store-file\"\n@@ -433,14 +433,14 @@ class ImagesPipelineTestCaseCustomSettings(unittest.TestCase):\n     ]\n \n     # This should match what is defined in ImagesPipeline.\n-    default_pipeline_settings = dict(\n-        MIN_WIDTH=0,\n-        MIN_HEIGHT=0,\n-        EXPIRES=90,\n-        THUMBS={},\n-        IMAGES_URLS_FIELD=\"image_urls\",\n-        IMAGES_RESULT_FIELD=\"images\",\n-    )\n+    default_pipeline_settings = {\n+        \"MIN_WIDTH\": 0,\n+        \"MIN_HEIGHT\": 0,\n+        \"EXPIRES\": 90,\n+        \"THUMBS\": {},\n+        \"IMAGES_URLS_FIELD\": \"image_urls\",\n+        \"IMAGES_RESULT_FIELD\": \"images\",\n+    }\n \n     def setUp(self):\n         self.tempdir = mkdtemp()\n\n@@ -59,7 +59,7 @@ class BaseMediaPipelineTestCase(unittest.TestCase):\n         assert self.pipe.media_to_download(request, self.info) is None\n \n     def test_default_get_media_requests(self):\n-        item = dict(name=\"name\")\n+        item = {\"name\": \"name\"}\n         assert self.pipe.get_media_requests(item, self.info) is None\n \n     def test_default_media_downloaded(self):\n@@ -73,7 +73,7 @@ class BaseMediaPipelineTestCase(unittest.TestCase):\n         assert self.pipe.media_failed(fail, request, self.info) is fail\n \n     def test_default_item_completed(self):\n-        item = dict(name=\"name\")\n+        item = {\"name\": \"name\"}\n         assert self.pipe.item_completed([], item, self.info) is item\n \n         # Check that failures are logged by default\n@@ -98,7 +98,7 @@ class BaseMediaPipelineTestCase(unittest.TestCase):\n \n     @inlineCallbacks\n     def test_default_process_item(self):\n-        item = dict(name=\"name\")\n+        item = {\"name\": \"name\"}\n         new_item = yield self.pipe.process_item(item, self.spider)\n         assert new_item is item\n \n@@ -226,11 +226,11 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n         rsp = Response(\"http://url1\")\n         req = Request(\n             \"http://url1\",\n-            meta=dict(response=rsp),\n+            meta={\"response\": rsp},\n             callback=self._callback,\n             errback=self._errback,\n         )\n-        item = dict(requests=req)\n+        item = {\"requests\": req}\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertEqual(new_item[\"results\"], [(True, rsp)])\n         self.assertEqual(\n@@ -250,11 +250,11 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n         fail = Failure(Exception())\n         req = Request(\n             \"http://url1\",\n-            meta=dict(response=fail),\n+            meta={\"response\": fail},\n             callback=self._callback,\n             errback=self._errback,\n         )\n-        item = dict(requests=req)\n+        item = {\"requests\": req}\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertEqual(new_item[\"results\"], [(False, fail)])\n         self.assertEqual(\n@@ -272,10 +272,10 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n     def test_mix_of_success_and_failure(self):\n         self.pipe.LOG_FAILED_RESULTS = False\n         rsp1 = Response(\"http://url1\")\n-        req1 = Request(\"http://url1\", meta=dict(response=rsp1))\n+        req1 = Request(\"http://url1\", meta={\"response\": rsp1})\n         fail = Failure(Exception())\n-        req2 = Request(\"http://url2\", meta=dict(response=fail))\n-        item = dict(requests=[req1, req2])\n+        req2 = Request(\"http://url2\", meta={\"response\": fail})\n+        item = {\"requests\": [req1, req2]}\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertEqual(new_item[\"results\"], [(True, rsp1), (False, fail)])\n         m = self.pipe._mockcalled\n@@ -294,7 +294,7 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n     def test_get_media_requests(self):\n         # returns single Request (without callback)\n         req = Request(\"http://url\")\n-        item = dict(requests=req)  # pass a single item\n+        item = {\"requests\": req}  # pass a single item\n         new_item = yield self.pipe.process_item(item, self.spider)\n         assert new_item is item\n         self.assertIn(self.fingerprint(req), self.info.downloaded)\n@@ -302,7 +302,7 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n         # returns iterable of Requests\n         req1 = Request(\"http://url1\")\n         req2 = Request(\"http://url2\")\n-        item = dict(requests=iter([req1, req2]))\n+        item = {\"requests\": iter([req1, req2])}\n         new_item = yield self.pipe.process_item(item, self.spider)\n         assert new_item is item\n         assert self.fingerprint(req1) in self.info.downloaded\n@@ -311,17 +311,17 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n     @inlineCallbacks\n     def test_results_are_cached_across_multiple_items(self):\n         rsp1 = Response(\"http://url1\")\n-        req1 = Request(\"http://url1\", meta=dict(response=rsp1))\n-        item = dict(requests=req1)\n+        req1 = Request(\"http://url1\", meta={\"response\": rsp1})\n+        item = {\"requests\": req1}\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertTrue(new_item is item)\n         self.assertEqual(new_item[\"results\"], [(True, rsp1)])\n \n         # rsp2 is ignored, rsp1 must be in results because request fingerprints are the same\n         req2 = Request(\n-            req1.url, meta=dict(response=Response(\"http://donot.download.me\"))\n+            req1.url, meta={\"response\": Response(\"http://donot.download.me\")}\n         )\n-        item = dict(requests=req2)\n+        item = {\"requests\": req2}\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertTrue(new_item is item)\n         self.assertEqual(self.fingerprint(req1), self.fingerprint(req2))\n@@ -330,11 +330,11 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n     @inlineCallbacks\n     def test_results_are_cached_for_requests_of_single_item(self):\n         rsp1 = Response(\"http://url1\")\n-        req1 = Request(\"http://url1\", meta=dict(response=rsp1))\n+        req1 = Request(\"http://url1\", meta={\"response\": rsp1})\n         req2 = Request(\n-            req1.url, meta=dict(response=Response(\"http://donot.download.me\"))\n+            req1.url, meta={\"response\": Response(\"http://donot.download.me\")}\n         )\n-        item = dict(requests=[req1, req2])\n+        item = {\"requests\": [req1, req2]}\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertTrue(new_item is item)\n         self.assertEqual(new_item[\"results\"], [(True, rsp1), (True, rsp1)])\n@@ -359,16 +359,16 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n         def rsp2_func():\n             self.fail(\"it must cache rsp1 result and must not try to redownload\")\n \n-        req1 = Request(\"http://url\", meta=dict(response=rsp1_func))\n-        req2 = Request(req1.url, meta=dict(response=rsp2_func))\n-        item = dict(requests=[req1, req2])\n+        req1 = Request(\"http://url\", meta={\"response\": rsp1_func})\n+        req2 = Request(req1.url, meta={\"response\": rsp2_func})\n+        item = {\"requests\": [req1, req2]}\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertEqual(new_item[\"results\"], [(True, rsp1), (True, rsp1)])\n \n     @inlineCallbacks\n     def test_use_media_to_download_result(self):\n-        req = Request(\"http://url\", meta=dict(result=\"ITSME\", response=self.fail))\n-        item = dict(requests=req)\n+        req = Request(\"http://url\", meta={\"result\": \"ITSME\", \"response\": self.fail})\n+        item = {\"requests\": req}\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertEqual(new_item[\"results\"], [(True, \"ITSME\")])\n         self.assertEqual(\n\n@@ -45,15 +45,15 @@ class MockDownloader:\n \n class MockCrawler(Crawler):\n     def __init__(self, priority_queue_cls, jobdir):\n-        settings = dict(\n-            SCHEDULER_DEBUG=False,\n-            SCHEDULER_DISK_QUEUE=\"scrapy.squeues.PickleLifoDiskQueue\",\n-            SCHEDULER_MEMORY_QUEUE=\"scrapy.squeues.LifoMemoryQueue\",\n-            SCHEDULER_PRIORITY_QUEUE=priority_queue_cls,\n-            JOBDIR=jobdir,\n-            DUPEFILTER_CLASS=\"scrapy.dupefilters.BaseDupeFilter\",\n-            REQUEST_FINGERPRINTER_IMPLEMENTATION=\"2.7\",\n-        )\n+        settings = {\n+            \"SCHEDULER_DEBUG\": False,\n+            \"SCHEDULER_DISK_QUEUE\": \"scrapy.squeues.PickleLifoDiskQueue\",\n+            \"SCHEDULER_MEMORY_QUEUE\": \"scrapy.squeues.LifoMemoryQueue\",\n+            \"SCHEDULER_PRIORITY_QUEUE\": priority_queue_cls,\n+            \"JOBDIR\": jobdir,\n+            \"DUPEFILTER_CLASS\": \"scrapy.dupefilters.BaseDupeFilter\",\n+            \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n+        }\n         super().__init__(Spider, settings)\n         self.engine = MockEngine(downloader=MockDownloader())\n         self.stats = load_object(self.settings[\"STATS_CLASS\"])(self)\n@@ -338,10 +338,10 @@ class TestIntegrationWithDownloaderAwareInMemory(TestCase):\n \n class TestIncompatibility(unittest.TestCase):\n     def _incompatible(self):\n-        settings = dict(\n-            SCHEDULER_PRIORITY_QUEUE=\"scrapy.pqueues.DownloaderAwarePriorityQueue\",\n-            CONCURRENT_REQUESTS_PER_IP=1,\n-        )\n+        settings = {\n+            \"SCHEDULER_PRIORITY_QUEUE\": \"scrapy.pqueues.DownloaderAwarePriorityQueue\",\n+            \"CONCURRENT_REQUESTS_PER_IP\": 1,\n+        }\n         crawler = get_crawler(Spider, settings)\n         scheduler = Scheduler.from_crawler(crawler)\n         spider = Spider(name=\"spider\")\n\n@@ -16,10 +16,10 @@ class TestOffsiteMiddleware(TestCase):\n         self.mw.spider_opened(self.spider)\n \n     def _get_spiderargs(self):\n-        return dict(\n-            name=\"foo\",\n-            allowed_domains=[\"scrapytest.org\", \"scrapy.org\", \"scrapy.test.org\"],\n-        )\n+        return {\n+            \"name\": \"foo\",\n+            \"allowed_domains\": [\"scrapytest.org\", \"scrapy.org\", \"scrapy.test.org\"],\n+        }\n \n     def test_process_spider_output(self):\n         res = Response(\"http://scrapytest.org\")\n@@ -50,7 +50,7 @@ class TestOffsiteMiddleware(TestCase):\n \n class TestOffsiteMiddleware2(TestOffsiteMiddleware):\n     def _get_spiderargs(self):\n-        return dict(name=\"foo\", allowed_domains=None)\n+        return {\"name\": \"foo\", \"allowed_domains\": None}\n \n     def test_process_spider_output(self):\n         res = Response(\"http://scrapytest.org\")\n@@ -61,13 +61,16 @@ class TestOffsiteMiddleware2(TestOffsiteMiddleware):\n \n class TestOffsiteMiddleware3(TestOffsiteMiddleware2):\n     def _get_spiderargs(self):\n-        return dict(name=\"foo\")\n+        return {\"name\": \"foo\"}\n \n \n class TestOffsiteMiddleware4(TestOffsiteMiddleware3):\n     def _get_spiderargs(self):\n         bad_hostname = urlparse(\"http:////scrapytest.org\").hostname\n-        return dict(name=\"foo\", allowed_domains=[\"scrapytest.org\", None, bad_hostname])\n+        return {\n+            \"name\": \"foo\",\n+            \"allowed_domains\": [\"scrapytest.org\", None, bad_hostname],\n+        }\n \n     def test_process_spider_output(self):\n         res = Response(\"http://scrapytest.org\")\n\n@@ -355,7 +355,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n         response = TextResponse(url=\"http://example.com/\", body=body)\n         csv = csviter(response)\n \n-        result = [row for row in csv]\n+        result = list(csv)\n         self.assertEqual(\n             result,\n             [\n@@ -377,7 +377,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n         csv = csviter(response, delimiter=\"\\t\")\n \n         self.assertEqual(\n-            [row for row in csv],\n+            list(csv),\n             [\n                 {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n                 {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\n@@ -394,7 +394,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n         csv1 = csviter(response1, quotechar=\"'\")\n \n         self.assertEqual(\n-            [row for row in csv1],\n+            list(csv1),\n             [\n                 {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n                 {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\n@@ -407,7 +407,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n         csv2 = csviter(response2, delimiter=\"|\", quotechar=\"'\")\n \n         self.assertEqual(\n-            [row for row in csv2],\n+            list(csv2),\n             [\n                 {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n                 {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\n@@ -422,7 +422,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n         csv = csviter(response)\n \n         self.assertEqual(\n-            [row for row in csv],\n+            list(csv),\n             [\n                 {\"'id'\": \"1\", \"'name'\": \"'alpha'\", \"'value'\": \"'foobar'\"},\n                 {\n@@ -441,7 +441,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n         csv = csviter(response, delimiter=\"\\t\")\n \n         self.assertEqual(\n-            [row for row in csv],\n+            list(csv),\n             [\n                 {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n                 {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\n@@ -458,7 +458,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n         csv = csviter(response, headers=[h.decode(\"utf-8\") for h in headers])\n \n         self.assertEqual(\n-            [row for row in csv],\n+            list(csv),\n             [\n                 {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n                 {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\n@@ -475,7 +475,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n         csv = csviter(response)\n \n         self.assertEqual(\n-            [row for row in csv],\n+            list(csv),\n             [\n                 {\"id\": \"1\", \"name\": \"alpha\", \"value\": \"foobar\"},\n                 {\"id\": \"2\", \"name\": \"unicode\", \"value\": \"\\xfan\\xedc\\xf3d\\xe9\\u203d\"},\n\n@@ -16,7 +16,7 @@ class UtilsRenderTemplateFileTestCase(unittest.TestCase):\n         rmtree(self.tmp_path)\n \n     def test_simple_render(self):\n-        context = dict(project_name=\"proj\", name=\"spi\", classname=\"TheSpider\")\n+        context = {\"project_name\": \"proj\", \"name\": \"spi\", \"classname\": \"TheSpider\"}\n         template = \"from ${project_name}.spiders.${name} import ${classname}\"\n         rendered = \"from proj.spiders.spi import TheSpider\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#706eb8d4275be993867122e5e41c31321488309e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 369 | Contributors (this commit): 16 | Commits (past 90d): 5 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -135,7 +135,7 @@ class HttpCompressionMiddleware:\n                 respcls = responsetypes.from_args(\n                     headers=response.headers, url=response.url, body=decoded_body\n                 )\n-                kwargs: Dict[str, Any] = {\"cls\": respcls, \"body\": decoded_body}\n+                kwargs: Dict[str, Any] = {\"body\": decoded_body}\n                 if issubclass(respcls, TextResponse):\n                     # force recalculating the encoding until we make sure the\n                     # responsetypes guessing is reliable\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2169810414a700fcbfe33eafe1e85e46e7f62413", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 894 | Contributors (this commit): 13 | Commits (past 90d): 2 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -27,7 +27,7 @@ from mitmproxy.tools.main import mitmdump\n sys.argv[0] = \"mitmdump\"\n sys.exit(mitmdump())\n         \"\"\"\n-        cert_path = Path(__file__).parent.resolve() / \"keys\" / \"mitmproxy-ca.pem\"\n+        cert_path = Path(__file__).parent.resolve() / \"keys\"\n         self.proc = Popen(\n             [\n                 sys.executable,\n@@ -40,8 +40,8 @@ sys.exit(mitmdump())\n                 \"0\",\n                 \"--proxyauth\",\n                 f\"{self.auth_user}:{self.auth_pass}\",\n-                \"--certs\",\n-                str(cert_path),\n+                \"--set\",\n+                f\"confdir={cert_path}\",\n                 \"--ssl-insecure\",\n             ],\n             stdout=PIPE,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2bfd9a2257c79ae56955e95b46f2bc7b23e1eabd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 17 | Files Changed: 12 | Hunks: 17 | Methods Changed: 10 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 36 | Churn Cumulative: 5504 | Contributors (this commit): 99 | Commits (past 90d): 15 | Contributors (cumulative): 226 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,4 @@\n-import subprocess\n+import subprocess  # nosec\n import sys\n import time\n from urllib.parse import urlencode\n@@ -29,7 +29,9 @@ class _BenchServer:\n         from scrapy.utils.test import get_testenv\n \n         pargs = [sys.executable, \"-u\", \"-m\", \"scrapy.utils.benchserver\"]\n-        self.proc = subprocess.Popen(pargs, stdout=subprocess.PIPE, env=get_testenv())\n+        self.proc = subprocess.Popen(\n+            pargs, stdout=subprocess.PIPE, env=get_testenv()\n+        )  # nosec\n         self.proc.stdout.readline()\n \n     def __exit__(self, exc_type, exc_value, traceback):\n\n@@ -37,4 +37,4 @@ class Command(ScrapyCommand):\n \n         sfile = sys.modules[spidercls.__module__].__file__\n         sfile = sfile.replace(\".pyc\", \".py\")\n-        self.exitcode = os.system(f'{editor} \"{sfile}\"')\n+        self.exitcode = os.system(f'{editor} \"{sfile}\"')  # nosec\n\n@@ -113,7 +113,7 @@ class Command(ScrapyCommand):\n         if template_file:\n             self._genspider(module, name, url, opts.template, template_file)\n             if opts.edit:\n-                self.exitcode = os.system(f'scrapy edit \"{name}\"')\n+                self.exitcode = os.system(f'scrapy edit \"{name}\"')  # nosec\n \n     def _genspider(self, module, name, url, template_name, template_file):\n         \"\"\"Generate the spider module, based on the given template\"\"\"\n\n@@ -40,7 +40,7 @@ class Slot:\n \n     def download_delay(self) -> float:\n         if self.randomize_delay:\n-            return random.uniform(0.5 * self.delay, 1.5 * self.delay)\n+            return random.uniform(0.5 * self.delay, 1.5 * self.delay)  # nosec\n         return self.delay\n \n     def close(self) -> None:\n\n@@ -5,10 +5,10 @@ Item Exporters are used to export/serialize items into different formats.\n import csv\n import io\n import marshal\n-import pickle\n+import pickle  # nosec\n import pprint\n from collections.abc import Mapping\n-from xml.sax.saxutils import XMLGenerator\n+from xml.sax.saxutils import XMLGenerator  # nosec\n \n from itemadapter import ItemAdapter, is_item\n \n\n@@ -1,6 +1,6 @@\n import gzip\n import logging\n-import pickle\n+import pickle  # nosec\n from email.utils import mktime_tz, parsedate_tz\n from importlib import import_module\n from pathlib import Path\n@@ -274,7 +274,7 @@ class DbmCacheStorage:\n         if 0 < self.expiration_secs < time() - float(ts):\n             return  # expired\n \n-        return pickle.loads(db[f\"{key}_data\"])\n+        return pickle.loads(db[f\"{key}_data\"])  # nosec\n \n \n class FilesystemCacheStorage:\n@@ -352,7 +352,7 @@ class FilesystemCacheStorage:\n         if 0 < self.expiration_secs < time() - mtime:\n             return  # expired\n         with self._open(metapath, \"rb\") as f:\n-            return pickle.load(f)\n+            return pickle.load(f)  # nosec\n \n \n def parse_cachecontrol(header):\n\n@@ -1,4 +1,4 @@\n-import pickle\n+import pickle  # nosec\n from pathlib import Path\n \n from scrapy import signals\n@@ -31,7 +31,7 @@ class SpiderState:\n     def spider_opened(self, spider):\n         if self.jobdir and Path(self.statefn).exists():\n             with Path(self.statefn).open(\"rb\") as f:\n-                spider.state = pickle.load(f)\n+                spider.state = pickle.load(f)  # nosec\n         else:\n             spider.state = {}\n \n\n@@ -177,7 +177,7 @@ FILES_STORE_S3_ACL = \"private\"\n FILES_STORE_GCS_ACL = \"\"\n \n FTP_USER = \"anonymous\"\n-FTP_PASSWORD = \"guest\"\n+FTP_PASSWORD = \"guest\"  # nosec\n FTP_PASSIVE_MODE = True\n \n GCS_PROJECT_ID = None\n\n@@ -50,7 +50,7 @@ class Shell:\n         else:\n             self.populate_vars()\n         if self.code:\n-            print(eval(self.code, globals(), self.vars))\n+            print(eval(self.code, globals(), self.vars))  # nosec\n         else:\n             \"\"\"\n             Detect interactive shell setting in scrapy.cfg\n\n@@ -3,7 +3,7 @@ Scheduler queues\n \"\"\"\n \n import marshal\n-import pickle\n+import pickle  # nosec\n from os import PathLike\n from pathlib import Path\n from typing import Union\n\n@@ -14,7 +14,7 @@ class Root(Resource):\n     def render(self, request):\n         total = _getarg(request, b\"total\", 100, int)\n         show = _getarg(request, b\"show\", 10, int)\n-        nlist = [random.randint(1, total) for _ in range(show)]\n+        nlist = [random.randint(1, total) for _ in range(show)]  # nosec\n         request.write(b\"<html><head></head><body>\")\n         args = request.args.copy()\n         for nl in nlist:\n\n@@ -30,7 +30,7 @@ def get_engine_status(engine: \"ExecutionEngine\") -> List[Tuple[str, Any]]:\n     checks: List[Tuple[str, Any]] = []\n     for test in tests:\n         try:\n-            checks += [(test, eval(test))]\n+            checks += [(test, eval(test))]  # nosec\n         except Exception as e:\n             checks += [(test, f\"{type(e).__name__} (exception)\")]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#31cbbb57584fe2a7c42d30acf2aa4707039457b5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1108 | Contributors (this commit): 10 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -20,7 +20,7 @@ def _path_safe(text):\n     pathable_slot = \"\".join([c if c.isalnum() or c in \"-._\" else \"_\" for c in text])\n     # as we replace some letters we can get collision for different slots\n     # add we add unique part\n-    unique_slot = hashlib.md5(text.encode(\"utf8\")).hexdigest()\n+    unique_slot = hashlib.md5(text.encode(\"utf8\")).hexdigest()  # nosec\n     return \"-\".join([pathable_slot, unique_slot])\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#032e6a091a27b406aa48293f752d4782f8cac159", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 6 | Churn Cumulative: 217 | Contributors (this commit): 8 | Commits (past 90d): 1 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -53,12 +53,10 @@ class JsonRequest(Request):\n     @overload\n     def replace(\n         self, *args: Any, cls: Type[RequestTypeVar], **kwargs: Any\n-    ) -> RequestTypeVar:\n-        ...\n+    ) -> RequestTypeVar: ...\n \n     @overload\n-    def replace(self, *args: Any, cls: None = None, **kwargs: Any) -> Self:\n-        ...\n+    def replace(self, *args: Any, cls: None = None, **kwargs: Any) -> Self: ...\n \n     def replace(\n         self, *args: Any, cls: Optional[Type[Request]] = None, **kwargs: Any\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4cd94aa668c60f92b1d9f4e5cf27752e1fe9c9cd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 32 | Churn Cumulative: 158 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,10 +1,40 @@\n import zlib\n from io import BytesIO\n+from warnings import warn\n+\n+from scrapy.exceptions import ScrapyDeprecationWarning\n \n try:\n     import brotli\n except ImportError:\n     pass\n+else:\n+    try:\n+        brotli.Decompressor.process\n+    except AttributeError:\n+\n+        warn(\n+            (\n+                \"You have brotlipy installed, and Scrapy will use it, but \"\n+                \"Scrapy support for brotlipy is deprecated and will stop \"\n+                \"working in a future version of Scrapy. brotlipy itself is \"\n+                \"deprecated, it has been superseded by brotlicffi (not \"\n+                \"currently supported by Scrapy). Please, uninstall brotlipy \"\n+                \"and install brotli instead. brotlipy has the same import \"\n+                \"name as brotli, so keeping both installed is strongly \"\n+                \"discouraged.\"\n+            ),\n+            ScrapyDeprecationWarning,\n+        )\n+\n+        def _brotli_decompress(decompressor, data):\n+            return decompressor.decompress(data)\n+\n+    else:\n+\n+        def _brotli_decompress(decompressor, data):\n+            return decompressor.process(data)\n+\n \n try:\n     import zstandard\n@@ -61,7 +91,7 @@ def _unbrotli(data: bytes, *, max_size: int = 0) -> bytes:\n     decompressed_size = 0\n     while output_chunk:\n         input_chunk = input_stream.read(_CHUNK_SIZE)\n-        output_chunk = decompressor.process(input_chunk)\n+        output_chunk = _brotli_decompress(decompressor, input_chunk)\n         decompressed_size += len(output_chunk)\n         if max_size and decompressed_size > max_size:\n             raise _DecompressionMaxSizeExceeded(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
