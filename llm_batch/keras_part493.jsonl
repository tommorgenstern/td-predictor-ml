{"custom_id": "keras#2851235d5bc1c6603a97d7efffc7649b0a84b826", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 185 | Lines Deleted: 105 | Files Changed: 4 | Hunks: 47 | Methods Changed: 32 | Complexity Δ (Sum/Max): 13/23 | Churn Δ: 290 | Churn Cumulative: 14835 | Contributors (this commit): 38 | Commits (past 90d): 80 | Contributors (cumulative): 58 | DMM Complexity: 0.0\n\nDIFF:\n@@ -27,7 +27,6 @@ import weakref\n \n import numpy as np\n import tensorflow.compat.v2 as tf\n-from absl import logging\n \n from keras import backend\n from keras import constraints\n@@ -3402,36 +3401,25 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         # Bypass Trackable logic as `__dict__` already contains this info.\n         object.__setattr__(self, \"__dict__\", state)\n \n-    def _get_state(self):\n-        \"\"\"Experimental method for getting the state of this layer object.\"\"\"\n-        result = {}\n+    def _save_own_variables(self, store):\n+        \"\"\"Experimental method for saving the state of this layer object.\"\"\"\n         all_vars = self._trainable_weights + self._non_trainable_weights\n         for i, v in enumerate(all_vars):\n-            result[str(i)] = v\n-        return result\n+            store[f\"{i}\"] = v.numpy()\n \n-    def _set_state(self, state):\n-        \"\"\"Experimental method for setting the state of this layer object.\"\"\"\n+    def _load_own_variables(self, store):\n+        \"\"\"Experimental method for loading the state of this layer object.\"\"\"\n         all_vars = self._trainable_weights + self._non_trainable_weights\n-        for i, v in enumerate(all_vars):\n-            v.assign(state[str(i)])\n-\n-    def _save_state(self, dirpath):\n-        filepath = tf.io.gfile.join(dirpath, \"weights.npz\")\n-        weights = self._get_state()\n-        if weights:\n-            # Only save the state if that of the trackable is available.\n-            np.savez(filepath, **weights)\n-            logging.debug(f\"Saved state to {filepath}\")\n-\n-    def _load_state(self, dirpath):\n-        filepath = tf.io.gfile.join(dirpath, \"weights.npz\")\n-        if tf.io.gfile.exists(filepath):\n-            loaded_npz = np.load(filepath)\n-            self._set_state(\n-                {file: loaded_npz[file] for file in loaded_npz.files}\n+        if len(store.keys()) != len(all_vars):\n+            raise ValueError(\n+                f\"Layer '{self.name}' expected {len(all_vars)} variables, \"\n+                \"but received \"\n+                f\"{len(store.keys())} variables during loading. \"\n+                f\"Names of variables received: {list(store.keys())}\"\n             )\n-            logging.debug(f\"Loaded state from {filepath}\")\n+        for i, v in enumerate(all_vars):\n+            # TODO(rchao): check shapes and raise errors.\n+            v.assign(store[f\"{i}\"])\n \n \n class TensorFlowOpLayer(Layer):\n\n@@ -30,9 +30,6 @@ from tensorflow.python.util.tf_export import keras_export\n from tensorflow.tools.docs import doc_controls\n \n \n-import numpy as np\n-\n-\n class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n     \"\"\"Optimizer base class, which only supports non-distribute use case.\"\"\"\n \n@@ -794,34 +791,15 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n                 )\n             variable.assign(weight)\n \n-    def _get_state(self):\n+    def _save_own_variables(self, store):\n         \"\"\"Get the state of this optimizer object.\"\"\"\n-        result = {}\n-        for variable in self.variables():\n-            result[variable.name] = variable.numpy()\n-        return result\n+        for i, variable in enumerate(self.variables()):\n+            store[str(i)] = variable.numpy()\n \n-    def _set_state(self, state):\n+    def _load_own_variables(self, store):\n         \"\"\"Set the state of this optimizer object.\"\"\"\n-        for variable in self.variables():\n-            variable.assign(state[variable.name])\n-\n-    def _save_state(self, dir_path):\n-        file_path = tf.io.gfile.join(dir_path, \"state.npz\")\n-        weights = self._get_state()\n-        if weights:\n-            # Only save the state if that of the trackable is available.\n-            np.savez(file_path, **weights)\n-            logging.debug(f\"Saved state to {file_path}\")\n-\n-    def _load_state(self, dir_path):\n-        file_path = tf.io.gfile.join(dir_path, \"state.npz\")\n-        if tf.io.gfile.exists(file_path):\n-            loaded_npz = np.load(file_path)\n-            logging.debug(f\"Loaded state from {file_path}\")\n-            self._set_state(\n-                {file: loaded_npz[file] for file in loaded_npz.files}\n-            )\n+        for i, variable in enumerate(self.variables()):\n+            variable.assign(store[str(i)])\n \n \n base_optimizer_keyword_args = \"\"\"name: String. The name to use\n\n@@ -29,14 +29,21 @@ from keras.engine import base_layer\n from keras.optimizers.optimizer_experimental import optimizer\n from keras.saving.experimental.serialization_lib import deserialize_keras_object\n from keras.saving.experimental.serialization_lib import serialize_keras_object\n+from keras.utils import generic_utils\n from keras.utils import io_utils\n \n+try:\n+    import h5py\n+except ImportError:\n+    h5py = None\n+\n # isort: off\n \n _SELF_DIRNAME = \"self\"\n _CONFIG_FILENAME = \"config.json\"\n _METADATA_FILENAME = \"metadata.json\"\n-_STATES_ROOT_DIRNAME = \"model\"\n+_VARS_FNAME = \"variables.h5\"\n+_ASSETS_DIRNAME = \"assets\"\n \n # A temporary flag to enable the new idempotent saving framework.\n _SAVING_V3_ENABLED = threading.local()\n@@ -44,11 +51,14 @@ _SAVING_V3_ENABLED.value = False\n \n ATTR_SKIPLIST = frozenset(\n     {\n+        \"__dict__\",\n         \"_self_tracked_trackables\",\n         \"_layer_call_argspecs\",\n         \"_self_unconditional_dependency_names\",\n         \"_output_layers\",\n         \"_input_layers\",\n+        \"_trainable_weights\",\n+        \"_non_trainable_weights\",\n         \"submodules\",\n         \"weights\",\n         \"non_trainable_weights\",\n@@ -91,6 +101,8 @@ def save_model(model, filepath):\n             \"Invalid filename: expected a `.keras` extension. \"\n             f\"Received: filepath={filepath}\"\n         )\n+    if h5py is None:\n+        raise ImportError(\"h5py must be installed in order to save a model.\")\n     if not model.built:\n         warnings.warn(\n             \"You are saving a model that has not yet been built. \"\n@@ -120,14 +132,23 @@ def save_model(model, filepath):\n             f.write(metadata_json)\n         with open(tf.io.gfile.join(temp_path, _CONFIG_FILENAME), \"w\") as f:\n             f.write(config_json)\n+\n+        h5_file = h5py.File(tf.io.gfile.join(temp_path, _VARS_FNAME), \"w\")\n+        assets_dir = tf.io.gfile.join(temp_path, _ASSETS_DIRNAME)\n         _save_state(\n-            model, tf.io.gfile.join(temp_path, _STATES_ROOT_DIRNAME), set()\n+            model,\n+            weights_handler=H5IOHandler(h5_file),\n+            assets_handler=DiskIOHandler(assets_dir),\n+            inner_path=\"\",\n+            visited_trackables=set(),\n         )\n+        _print_h5_file(h5_file, action=\"saving\")\n+        h5_file.close()\n \n         # Zip local files into an archive.\n         with zipfile.ZipFile(filepath, \"w\") as zipfile_to_save:\n             _write_recursively(zipfile_to_save, temp_path, \"\")\n-        _print_archive(zipfile_to_save, \"saving\")\n+        _print_zip_file(zipfile_to_save, \"saving\")\n     except Exception as e:\n         raise e\n     finally:\n@@ -143,12 +164,14 @@ def load_model(filepath, custom_objects=None):\n             \"Invalid filename: expected a `.keras` extension. \"\n             f\"Received: filepath={filepath}\"\n         )\n+    if h5py is None:\n+        raise ImportError(\"h5py must be installed in order to load a model.\")\n     saving_v3_enabled_value = getattr(_SAVING_V3_ENABLED, \"value\", False)\n     _SAVING_V3_ENABLED.value = True\n     temp_path = _get_temp_dir()\n     try:\n         with zipfile.ZipFile(filepath, \"r\") as zipfile_to_load:\n-            _print_archive(zipfile_to_load, \"loading\")\n+            _print_zip_file(zipfile_to_load, \"loading\")\n             zipfile_to_load.extractall(temp_path)\n \n         with open(tf.io.gfile.join(temp_path, _CONFIG_FILENAME), \"r\") as f:\n@@ -158,7 +181,17 @@ def load_model(filepath, custom_objects=None):\n         config_dict = json.loads(config_json)\n         # Construct the model from the configuration file in the archive.\n         model = deserialize_keras_object(config_dict, custom_objects)\n-        _load_state(model, tf.io.gfile.join(temp_path, _STATES_ROOT_DIRNAME))\n+        h5_file = h5py.File(tf.io.gfile.join(temp_path, _VARS_FNAME), \"r\")\n+        _print_h5_file(h5_file, action=\"loading\")\n+        assets_dir = tf.io.gfile.join(temp_path, _ASSETS_DIRNAME)\n+        _load_state(\n+            model,\n+            weights_handler=H5IOHandler(h5_file),\n+            assets_handler=DiskIOHandler(assets_dir),\n+            inner_path=\"\",\n+            visited_trackables=set(),\n+        )\n+        h5_file.close()\n     except Exception as e:\n         raise e\n     else:\n@@ -179,17 +212,20 @@ def _write_recursively(zipfile_to_save, system_path, zip_path):\n             _write_recursively(zipfile_to_save, system_file_path, zip_file_path)\n \n \n-def _save_state(trackable, temp_path, saved_trackables):\n+def _save_state(\n+    trackable, weights_handler, assets_handler, inner_path, visited_trackables\n+):\n     # If the trackable has already been saved, skip it.\n-    if id(trackable) in saved_trackables:\n+    if id(trackable) in visited_trackables:\n         return\n \n-    # TODO(rchao): Make `.get_state()` and `.save_state()` exported methods.\n-    if hasattr(trackable, \"_save_state\"):\n-        if not tf.io.gfile.exists(temp_path):\n-            tf.io.gfile.makedirs(temp_path)\n-        trackable._save_state(temp_path)\n-        saved_trackables.add(id(trackable))\n+    # TODO(fchollet): better name?\n+    if hasattr(trackable, \"_save_own_variables\"):\n+        trackable._save_own_variables(weights_handler.make(inner_path))\n+    if hasattr(trackable, \"_save_assets\"):\n+        trackable._save_assets(assets_handler.make(inner_path))\n+\n+    visited_trackables.add(id(trackable))\n \n     # Recursively save state of children trackables (layers, optimizers, etc.)\n     for child_attr in dir(trackable):\n@@ -203,20 +239,33 @@ def _save_state(trackable, temp_path, saved_trackables):\n         if _is_keras_trackable(child_obj):\n             _save_state(\n                 child_obj,\n-                tf.io.gfile.join(temp_path, child_attr),\n-                saved_trackables,\n+                weights_handler,\n+                assets_handler,\n+                inner_path=tf.io.gfile.join(inner_path, child_attr),\n+                visited_trackables=visited_trackables,\n             )\n         elif isinstance(child_obj, (list, dict, tuple)):\n             _save_container_state(\n                 child_obj,\n-                tf.io.gfile.join(temp_path, child_attr),\n-                saved_trackables,\n+                weights_handler,\n+                assets_handler,\n+                inner_path=tf.io.gfile.join(inner_path, child_attr),\n+                visited_trackables=visited_trackables,\n             )\n \n \n-def _load_state(trackable, temp_path):\n-    if hasattr(trackable, \"_load_state\"):\n-        trackable._load_state(temp_path)\n+def _load_state(\n+    trackable, weights_handler, assets_handler, inner_path, visited_trackables\n+):\n+    if id(trackable) in visited_trackables:\n+        return\n+\n+    if hasattr(trackable, \"_load_own_variables\"):\n+        trackable._load_own_variables(weights_handler.get(inner_path))\n+    if hasattr(trackable, \"_load_assets\"):\n+        trackable._load_assets(assets_handler.get(inner_path))\n+\n+    visited_trackables.add(id(trackable))\n \n     # Recursively load states for Keras trackables such as layers/optimizers.\n     for child_attr in dir(trackable):\n@@ -230,46 +279,125 @@ def _load_state(trackable, temp_path):\n         if _is_keras_trackable(child_obj):\n             _load_state(\n                 child_obj,\n-                tf.io.gfile.join(temp_path, child_attr),\n+                weights_handler,\n+                assets_handler,\n+                inner_path=tf.io.gfile.join(inner_path, child_attr),\n+                visited_trackables=visited_trackables,\n             )\n         elif isinstance(child_obj, (list, dict, tuple)):\n             _load_container_state(\n                 child_obj,\n-                tf.io.gfile.join(temp_path, child_attr),\n+                weights_handler,\n+                assets_handler,\n+                inner_path=tf.io.gfile.join(inner_path, child_attr),\n+                visited_trackables=visited_trackables,\n             )\n \n \n-def _save_container_state(container, temp_path, saved_trackables):\n+def _save_container_state(\n+    container, weights_handler, assets_handler, inner_path, visited_trackables\n+):\n+    used_names = {}\n     for trackable in container:\n         if _is_keras_trackable(trackable):\n+            # Do NOT address the trackable via `trackable.name`, since\n+            # names are usually autogenerated and thus not reproducible\n+            # (i.e. they may vary across two instances of the same model).\n+            name = generic_utils.to_snake_case(trackable.__class__.__name__)\n+            if name in used_names:\n+                used_names[name] += 1\n+                name = f\"{name}_{used_names[name]}\"\n+            else:\n+                used_names[name] = 0\n             _save_state(\n                 trackable,\n-                tf.io.gfile.join(temp_path, trackable.name),\n-                saved_trackables,\n+                weights_handler,\n+                assets_handler,\n+                inner_path=tf.io.gfile.join(inner_path, name),\n+                visited_trackables=visited_trackables,\n             )\n \n \n-def _load_container_state(container, temp_path):\n+def _load_container_state(\n+    container, weights_handler, assets_handler, inner_path, visited_trackables\n+):\n+    used_names = {}\n     for trackable in container:\n         if _is_keras_trackable(trackable):\n+            name = generic_utils.to_snake_case(trackable.__class__.__name__)\n+            if name in used_names:\n+                used_names[name] += 1\n+                name = f\"{name}_{used_names[name]}\"\n+            else:\n+                used_names[name] = 0\n             _load_state(\n                 trackable,\n-                tf.io.gfile.join(temp_path, trackable.name),\n+                weights_handler,\n+                assets_handler,\n+                inner_path=tf.io.gfile.join(inner_path, name),\n+                visited_trackables=visited_trackables,\n             )\n \n \n+class DiskIOHandler:\n+    def __init__(self, base_directory):\n+        self.base_directory = base_directory\n+\n+    def make(self, path):\n+        if not path:\n+            return self.base_directory\n+        path = tf.io.gfile.join(self.base_directory, path)\n+        if not tf.io.gfile.exists(path):\n+            tf.io.gfile.makedirs(path)\n+        return path\n+\n+    def get(self, path):\n+        if not path:\n+            return self.base_directory\n+        path = tf.io.gfile.join(self.base_directory, path)\n+        if tf.io.gfile.exists(path):\n+            return path\n+        return None\n+\n+\n+class H5IOHandler:\n+    def __init__(self, h5_file):\n+        self.h5_file = h5_file\n+\n+    def make(self, path):\n+        if not path:\n+            return self.h5_file.create_group(\"vars\")\n+        return self.h5_file.create_group(path).create_group(\"vars\")\n+\n+    def get(self, path):\n+        if not path:\n+            return self.h5_file[\"vars\"]\n+        if path in self.h5_file:\n+            return self.h5_file[path][\"vars\"]\n+        print(f\"Warning: asset missing from file: {path}\")\n+        return {}\n+\n+\n def _get_temp_dir():\n     temp_dir = tempfile.mkdtemp()\n     testfile = tempfile.TemporaryFile(dir=temp_dir)\n     testfile.close()\n-    # TODO(fchollet): Fallback on RAM if disk is nonwritable or if less than 2GB\n-    # available.\n     return temp_dir\n \n \n-def _print_archive(zipfile, action):\n+def _print_h5_file(h5_file, prefix=\"\", action=None):\n+    if not prefix:\n+        print(f\"Keras weights file ({h5_file}) {action}:\")\n+    if not hasattr(h5_file, \"keys\"):\n+        return\n+    for key in h5_file.keys():\n+        print(f\"...{prefix}{key}\")\n+        _print_h5_file(h5_file[key], prefix=prefix + \"...\")\n+\n+\n+def _print_zip_file(zipfile, action):\n     # TODO(fchollet): move to debugging logs.\n-    io_utils.print_msg(f\"Keras model {action}:\")\n+    io_utils.print_msg(f\"Keras model archive {action}:\")\n     # Same as `ZipFile.printdir()` except for using Keras' printing utility.\n     io_utils.print_msg(\n         \"%-46s %19s %12s\" % (\"File Name\", \"Modified    \", \"Size\")\n\n@@ -291,19 +291,17 @@ class SavingV3Test(tf.test.TestCase, parameterized.TestCase):\n \n     def test_saving_preserve_built_state(self):\n         temp_filepath = os.path.join(self.get_temp_dir(), \"my_model.keras\")\n-        subclassed_model = self._get_subclassed_model()\n+        model = self._get_subclassed_model()\n         x = np.random.random((100, 32))\n         y = np.random.random((100, 1))\n-        subclassed_model.fit(x, y, epochs=1)\n-        subclassed_model._save_experimental(temp_filepath)\n+        model.fit(x, y, epochs=1)\n+        model._save_experimental(temp_filepath)\n         loaded_model = saving_lib.load_model(temp_filepath)\n-        self.assertEqual(\n-            subclassed_model._is_compiled, loaded_model._is_compiled\n-        )\n-        self.assertTrue(subclassed_model.built)\n+        self.assertEqual(model._is_compiled, loaded_model._is_compiled)\n+        self.assertTrue(model.built)\n         self.assertTrue(loaded_model.built)\n         self.assertEqual(\n-            subclassed_model._build_input_shape, loaded_model._build_input_shape\n+            model._build_input_shape, loaded_model._build_input_shape\n         )\n         self.assertEqual(\n             tf.TensorShape([None, 32]), loaded_model._build_input_shape\n@@ -389,18 +387,6 @@ class SavingV3Test(tf.test.TestCase, parameterized.TestCase):\n             functional_to_string.contents, loaded_to_string.contents\n         )\n \n-    def test_get_state(self):\n-        i = keras.Input((4,))\n-        o = keras.layers.Dense(2)(i)\n-        model = keras.Model(i, o)\n-        input_layer = model.layers[0]\n-        dense_layer = model.layers[1]\n-        self.assertEmpty(input_layer._get_state().keys())\n-        self.assertIn(\"0\", dense_layer._get_state().keys())\n-        self.assertIn(\"1\", dense_layer._get_state().keys())\n-        self.assertEqual(dense_layer._get_state()[\"0\"].shape, (4, 2))\n-        self.assertEqual(dense_layer._get_state()[\"1\"].shape, (2,))\n-\n     @tf.__internal__.distribute.combinations.generate(\n         tf.__internal__.test.combinations.combine(\n             model_type=[\"sequential\", \"functional\", \"subclassed\"],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d0bbe7fe9382a45a3b79a4e40578016f5f75299d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 119 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -115,8 +115,8 @@ class Adagrad(optimizer.Optimizer):\n             accumulator.scatter_add(\n                 tf.IndexedSlices(grad.values * grad.values, grad.indices)\n             )\n-            denominator = tf.sqrt(accumulator + self.epsilon)\n-            sparse_denominator = tf.gather(denominator, indices=grad.indices)\n+            sparse_accumulator = tf.gather(accumulator, indices=grad.indices)\n+            sparse_denominator = tf.sqrt(sparse_accumulator + self.epsilon)\n             variable.scatter_add(\n                 tf.IndexedSlices(\n                     -lr * grad.values / sparse_denominator, grad.indices\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#50d214604fd8f7bcdb1c6c5c97cfde437a7ae4da", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 38 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 41 | Churn Cumulative: 20667 | Contributors (this commit): 26 | Commits (past 90d): 18 | Contributors (cumulative): 44 | DMM Complexity: 1.0\n\nDIFF:\n@@ -347,9 +347,8 @@ def learning_phase():\n             # subgraph.\n             if context.executing_eagerly():\n                 if _DUMMY_EAGER_GRAPH.key not in _GRAPH_LEARNING_PHASES:\n-                    phase = _default_learning_phase()\n-                    _internal_set_learning_phase(_DUMMY_EAGER_GRAPH.key, phase)\n-                    _DUMMY_EAGER_GRAPH.learning_phase_is_set = True\n+                    return _default_learning_phase()\n+                else:\n                     return _internal_get_learning_phase(_DUMMY_EAGER_GRAPH.key)\n             else:\n                 learning_phase = symbolic_learning_phase()\n\n@@ -187,6 +187,22 @@ class BackendUtilsTest(tf.test.TestCase):\n                     self.evaluate(tf.compat.v1.global_variables_initializer())\n                     sess.run(y, feed_dict={x: np.random.random((2, 3))})\n \n+    def test_get_learning_phase_eager(self):\n+        if not tf.executing_eagerly():\n+            self.skipTest(\"Check for eager only.\")\n+        # see b/251520266 for more details.\n+        # By default the learning phase should be False\n+        self.assertFalse(backend.learning_phase())\n+        # Also make sure retrieving the learning phase doesn't set the default\n+        # value\n+        self.assertFalse(backend.global_learning_phase_is_set())\n+\n+        with backend.learning_phase_scope(1):\n+            self.assertTrue(backend.learning_phase())\n+            self.assertTrue(backend.global_learning_phase_is_set())\n+\n+        self.assertFalse(backend.global_learning_phase_is_set())\n+\n     def test_learning_phase_name(self):\n         with backend.name_scope(\"test_scope\"):\n             # Test that outer name scopes do not affect the learning phase's\n\n@@ -2578,6 +2578,14 @@ class VectorizeDisabledLayer(image_preprocessing.BaseImageAugmentationLayer):\n         super().__init__(**kwargs)\n \n \n+class FilterLayer(image_preprocessing.BaseImageAugmentationLayer):\n+    # Testing layer for check whether the training flag is set properly for KPL\n+\n+    def augment_image(self, image, transformation):\n+        # Returns zeros based on the original image\n+        return tf.zeros_like(image)\n+\n+\n @test_combinations.run_all_keras_modes(always_skip_v1=True)\n class BaseImageAugmentationLayerTest(test_combinations.TestCase):\n     def test_augment_single_image(self):\n@@ -2648,6 +2656,18 @@ class BaseImageAugmentationLayerTest(test_combinations.TestCase):\n         self.assertNotAllClose(image_diff[0], image_diff[1])\n         self.assertNotAllClose(label_diff[0], label_diff[1])\n \n+    def test_training_flag(self):\n+        # See b/251520266 for more details.\n+        inputs = tf.ones((10, 8, 8, 3), dtype=\"float32\")\n+        dropout = keras.layers.Dropout(rate=0.00001)\n+        filter = FilterLayer()\n+        output = dropout(inputs)\n+        output = filter(output)\n+\n+        # Make sure the outputs are all zeros, which the behavior for\n+        # FilterLayer when `training` is True\n+        self.assertAllClose(output, tf.zeros((10, 8, 8, 3), dtype=\"float32\"))\n+\n \n if __name__ == \"__main__\":\n     tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#585b28b0c2ef5063953d6e7388a5a612e33ca8ff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 12 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -207,7 +207,9 @@ class SidecarEvaluator:\n         if self.model.optimizer and isinstance(\n             self.model.optimizer, optimizer_experimental.Optimizer\n         ):\n-            checkpoint = tf.train.Checkpoint(model=self.model)\n+            checkpoint = tf.train.Checkpoint(\n+                model=self.model, optimizer=self.model.optimizer\n+            )\n         else:\n             optimizer_checkpoint = tf.train.Checkpoint(iter=self._iterations)\n             checkpoint = tf.train.Checkpoint(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#88407d467914d9d0348e886d717835f3f748d9d1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 50 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 60 | Churn Cumulative: 4930 | Contributors (this commit): 18 | Commits (past 90d): 13 | Contributors (cumulative): 25 | DMM Complexity: 1.0\n\nDIFF:\n@@ -427,11 +427,12 @@ class Sequential(functional.Functional):\n \n             outputs = layer(inputs, **kwargs)\n \n-            if len(tf.nest.flatten(outputs)) != 1:\n-                raise ValueError(SINGLE_LAYER_OUTPUT_ERROR_MSG)\n-            # `outputs` will be the inputs to the next layer.\n             inputs = outputs\n-            mask = getattr(outputs, \"_keras_mask\", None)\n+\n+            def _get_mask_from_keras_tensor(kt):\n+                return getattr(kt, \"_keras_mask\", None)\n+\n+            mask = tf.nest.map_structure(_get_mask_from_keras_tensor, outputs)\n         return outputs\n \n     def compute_output_shape(self, input_shape):\n\n@@ -19,6 +19,7 @@ import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n \n import keras\n+from keras.layers.preprocessing import image_preprocessing\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n \n@@ -408,12 +409,6 @@ class TestSequential(test_combinations.TestCase):\n                 [keras.layers.Dense(1, input_shape=(3,)), MultiOutputLayer()]\n             )\n \n-        # Should also raise error in a deferred build mode\n-        with self.assertRaisesRegex(\n-            ValueError, \"should have a single output tensor\"\n-        ):\n-            keras.Sequential([MultiOutputLayer()])(np.zeros((10, 10)))\n-\n     @test_combinations.run_all_keras_modes(always_skip_v1=True)\n     def test_layer_add_after_compile_deferred(self):\n         model = keras.Sequential([keras.layers.Dense(3)])\n@@ -544,6 +539,42 @@ class TestSequential(test_combinations.TestCase):\n         with self.assertRaisesRegex(ValueError, \"is not defined\"):\n             model.add(MyModule())\n \n+    @test_combinations.run_all_keras_modes(always_skip_v1=True)\n+    def test_multi_inputs_outputs(self):\n+        model = keras.Sequential(\n+            [\n+                ImageAugmentLayer(),\n+                ImageAugmentLayer(),\n+            ]\n+        )\n+\n+        image_inputs = tf.ones((2, 512, 512, 3))\n+        label_inputs = tf.ones((2, 2))\n+\n+        output = model({\"images\": image_inputs, \"labels\": label_inputs})\n+        self.assertAllClose(output[\"images\"], image_inputs)\n+        self.assertAllClose(output[\"labels\"], label_inputs)\n+\n+        model.compile(loss=\"mse\")\n+        model.fit(\n+            x={\"images\": image_inputs, \"labels\": label_inputs},\n+            y={\"images\": image_inputs, \"labels\": label_inputs},\n+            steps_per_epoch=1,\n+        )\n+        self.assertIsNone(model.inputs)\n+        self.assertIsNone(model.outputs)\n+\n+        # Use the same model with image input only\n+        model({\"images\": image_inputs})\n+        model.fit(\n+            x={\"images\": image_inputs},\n+            y={\"images\": image_inputs},\n+            steps_per_epoch=1,\n+        )\n+\n+        model(image_inputs)\n+        model.fit(x=image_inputs, y=image_inputs, steps_per_epoch=1)\n+\n \n class TestSequentialEagerIntegration(test_combinations.TestCase):\n     @test_combinations.run_all_keras_modes\n@@ -612,5 +643,13 @@ class TestSequentialEagerIntegration(test_combinations.TestCase):\n         self.assertTrue(model.built)\n \n \n+class ImageAugmentLayer(image_preprocessing.BaseImageAugmentationLayer):\n+    def augment_image(self, image, transformation=None):\n+        return image\n+\n+    def augment_label(self, label, transformation=None):\n+        return label\n+\n+\n if __name__ == \"__main__\":\n     tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#37cfdd811dce30d3d417fd76b4ef9155c051f845", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 6/3 | Churn Δ: 17 | Churn Cumulative: 2372 | Contributors (this commit): 13 | Commits (past 90d): 47 | Contributors (cumulative): 22 | DMM Complexity: 0.0\n\nDIFF:\n@@ -24,6 +24,7 @@ from keras import backend\n from keras import initializers\n from keras.optimizers.optimizer_v2 import utils as optimizer_utils\n from keras.optimizers.schedules import learning_rate_schedule\n+from keras.utils import tf_utils\n \n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n@@ -1072,6 +1073,14 @@ class Optimizer(_BaseOptimizer):\n         # TODO(b/197554203): replace _distributed_container() with a public api.\n         if hasattr(variable, \"_distributed_container\"):\n             variable = variable._distributed_container()\n+        elif (\n+            tf_utils.is_extension_type(variable)\n+            and hasattr(variable, \"handle\")\n+            and hasattr(variable.handle, \"_distributed_container\")\n+        ):\n+            # For ResourceVariables, the _distributed_container attribute\n+            # is added to their handle tensors.\n+            variable = variable.handle._distributed_container()\n         return super()._var_key(variable)\n \n     def aggregate_gradients(self, grads_and_vars):\n\n@@ -1654,6 +1654,14 @@ def _var_key(var):\n     # Get the distributed variable if it exists.\n     if hasattr(var, \"_distributed_container\"):\n         var = var._distributed_container()\n+    elif (\n+        tf_utils.is_extension_type(var)\n+        and hasattr(var, \"handle\")\n+        and hasattr(var.handle, \"_distributed_container\")\n+    ):\n+        # For ResourceVariables, the _distributed_container attribute\n+        # is added to their handle tensors.\n+        var = var.handle._distributed_container()\n     if getattr(var, \"_in_graph_mode\", False):\n         return var._shared_name\n     return var._unique_id\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#56de08b3048dac6398e6347eba3699fa560a0ba1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 9 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 41 | Churn Cumulative: 1991 | Contributors (this commit): 33 | Commits (past 90d): 3 | Contributors (cumulative): 43 | DMM Complexity: 0.7586206896551724\n\nDIFF:\n@@ -86,6 +86,7 @@ def model_to_dot(\n     subgraph=False,\n     layer_range=None,\n     show_layer_activations=False,\n+    show_trainable=False,\n ):\n     \"\"\"Convert a Keras model to dot format.\n \n@@ -112,6 +113,8 @@ def model_to_dot(\n           must be complete.\n       show_layer_activations: Display layer activations (only for layers that\n           have an `activation` property).\n+      show_trainable: whether to display if a layer is trainable. Displays 'T'\n+          when the layer is trainable and 'NT' when it is not trainable.\n \n     Returns:\n       A `pydot.Dot` instance representing the Keras model or\n@@ -209,6 +212,7 @@ def model_to_dot(\n                     rankdir,\n                     expand_nested,\n                     subgraph=True,\n+                    show_trainable=show_trainable,\n                 )\n                 # sub_w : submodel_wrapper\n                 sub_w_nodes = submodel_wrapper.get_nodes()\n@@ -229,6 +233,7 @@ def model_to_dot(\n                 rankdir,\n                 expand_nested,\n                 subgraph=True,\n+                show_trainable=show_trainable,\n             )\n             # sub_n : submodel_not_wrapper\n             sub_n_nodes = submodel_not_wrapper.get_nodes()\n@@ -296,6 +301,11 @@ def model_to_dot(\n                 inputlabels,\n                 outputlabels,\n             )\n+\n+        # Rebuild the label as a table including trainable status\n+        if show_trainable:\n+            label = f\"{'T' if layer.trainable else 'NT'}|{label}\"\n+\n         if not expand_nested or not isinstance(layer, functional.Functional):\n             node = pydot.Node(layer_id, label=label)\n             dot.add_node(node)\n@@ -375,6 +385,7 @@ def plot_model(\n     dpi=96,\n     layer_range=None,\n     show_layer_activations=False,\n+    show_trainable=False,\n ):\n     \"\"\"Converts a Keras model to dot format and save to a file.\n \n@@ -416,6 +427,8 @@ def plot_model(\n         complete.\n       show_layer_activations: Display layer activations (only for layers that\n         have an `activation` property).\n+      show_trainable: whether to display if a layer is trainable. Displays 'T'\n+        when the layer is trainable and 'NT' when it is not trainable.\n \n     Raises:\n       ImportError: if graphviz or pydot are not available.\n@@ -458,6 +471,7 @@ def plot_model(\n         dpi=dpi,\n         layer_range=layer_range,\n         show_layer_activations=show_layer_activations,\n+        show_trainable=show_trainable,\n     )\n     to_file = io_utils.path_to_string(to_file)\n     if dot is None:\n\n@@ -265,6 +265,33 @@ class ModelToDotFormatTest(tf.test.TestCase, parameterized.TestCase):\n         except ImportError:\n             pass\n \n+    def test_plot_model_with_show_trainable(self):\n+        model = keras.Sequential(name=\"trainable\")\n+\n+        untrained = keras.layers.Conv2D(\n+            filters=2, kernel_size=(2, 3), input_shape=(3, 5, 5), name=\"conv\"\n+        )\n+        model.add(untrained)\n+        model.add(keras.layers.Flatten(name=\"flat\"))\n+        model.add(keras.layers.Dense(5, name=\"dense\"))\n+\n+        # Should display as Non Trainable\n+        untrained.trainable = False\n+\n+        dot_img_file = \"model_trainable.png\"\n+        try:\n+            vis_utils.plot_model(\n+                model,\n+                to_file=dot_img_file,\n+                show_shapes=True,\n+                show_dtype=True,\n+                show_trainable=True,\n+            )\n+            self.assertTrue(tf.io.gfile.exists(dot_img_file))\n+            tf.io.gfile.remove(dot_img_file)\n+        except ImportError:\n+            pass\n+\n \n def get_layer_ids_from_model(model, layer_range):\n     layer_range = layer_utils.get_layer_index_bound_by_layer_name(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c6f8a64747b2c0812f7616eec36c4c91260f83c5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 524 | Contributors (this commit): 10 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -259,7 +259,7 @@ class ModelToDotFormatTest(tf.test.TestCase, parameterized.TestCase):\n             outputs=outputs,\n         )\n         try:\n-            vis_utils.plot_model(\n+            vis_utils.model_to_dot(\n                 model, show_shapes=True, show_dtype=True, show_layer_names=True\n             )\n         except ImportError:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#af1408d3255e3db9067522762e22a6c454c56654", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 102 | Lines Deleted: 60 | Files Changed: 5 | Hunks: 16 | Methods Changed: 13 | Complexity Δ (Sum/Max): 1/2 | Churn Δ: 162 | Churn Cumulative: 42009 | Contributors (this commit): 178 | Commits (past 90d): 67 | Contributors (cumulative): 245 | DMM Complexity: 1.0\n\nDIFF:\n@@ -749,6 +749,34 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n                 self._non_trainable_weights.append(variable)\n         return variable\n \n+    def __new__(cls, *args, **kwargs):\n+        # Generate a config to be returned by default by `get_config()`.\n+        arg_names = tf_inspect.getfullargspec(cls.__init__).args\n+        kwargs.update(dict(zip(arg_names[1 : len(args) + 1], args)))\n+        instance = super(Layer, cls).__new__(cls, *args, **kwargs)\n+        # For safety, we only rely on auto-configs for a small set of\n+        # serializable types.\n+        supported_types = (str, int, float, bool, type(None))\n+        try:\n+            flat_arg_values = tf.nest.flatten(kwargs)\n+            auto_get_config = True\n+            for value in flat_arg_values:\n+                if not isinstance(value, supported_types):\n+                    auto_get_config = False\n+                    break\n+        except TypeError:\n+            auto_get_config = False\n+        try:\n+            instance._auto_get_config = auto_get_config\n+            if auto_get_config:\n+                instance._auto_config = generic_utils.Config(**kwargs)\n+        except RecursionError:\n+            # Setting an instance attribute in __new__ has the potential\n+            # to trigger an infinite recursion if a subclass overrides\n+            # setattr in an unsafe way.\n+            pass\n+        return instance\n+\n     @generic_utils.default\n     def get_config(self):\n         \"\"\"Returns the config of the layer.\n@@ -769,32 +797,31 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         Returns:\n             Python dictionary.\n         \"\"\"\n-        all_args = tf_inspect.getfullargspec(self.__init__).args[1:]\n         config = {\n             \"name\": self.name,\n             \"trainable\": self.trainable,\n         }\n+        config[\"dtype\"] = policy.serialize(self._dtype_policy)\n         if hasattr(self, \"_batch_input_shape\"):\n             config[\"batch_input_shape\"] = self._batch_input_shape\n-        config[\"dtype\"] = policy.serialize(self._dtype_policy)\n-        if hasattr(self, \"dynamic\"):\n-            # Only include `dynamic` in the `config` if it is `True`\n-            if self.dynamic:\n-                config[\"dynamic\"] = self.dynamic\n-            elif \"dynamic\" in all_args:\n-                all_args.remove(\"dynamic\")\n-        expected_args = config.keys()\n-        # Finds all arguments in the `__init__` that are not in the config:\n-        extra_args = [arg for arg in all_args if arg not in expected_args]\n-        # Check that either the only argument in the `__init__` is  `self`,\n-        # or that `get_config` has been overridden:\n-        if extra_args and hasattr(self.get_config, \"_is_default\"):\n+\n+        if not generic_utils.is_default(self.get_config):\n+            # In this case the subclass implements get_config()\n+            return config\n+\n+        # In this case the subclass doesn't implement get_config():\n+        # Let's see if we can autogenerate it.\n+        if getattr(self, \"_auto_get_config\", False):\n+            config.update(self._auto_config.config)\n+            return config\n+        else:\n             raise NotImplementedError(\n                 textwrap.dedent(\n                     f\"\"\"\n-          Layer {self.__class__.__name__} has arguments {extra_args}\n-          in `__init__()` and therefore must override `get_config()` in\n-          order to be serializable.\n+        Layer {self.__class__.__name__} was created by passing\n+        non-serializable argument values in `__init__()`,\n+        and therefore the layer must override `get_config()` in\n+        order to be serializable. Please implement `get_config()`.\n \n         Example:\n \n@@ -814,8 +841,6 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n                 )\n             )\n \n-        return config\n-\n     @classmethod\n     def from_config(cls, config):\n         \"\"\"Creates a layer from its config.\n\n@@ -655,8 +655,12 @@ class BaseLayerTest(test_combinations.TestCase):\n \n         # `__init__` includes kwargs but `get_config` is not overridden, so\n         # an error should be thrown:\n-        with self.assertRaisesRegex(NotImplementedError, \"Layer MyLayer has\"):\n-            MyLayer(\"custom\").get_config()\n+        with self.assertRaisesRegex(\n+            NotImplementedError, \"Layer MyLayer was created by\"\n+        ):\n+            # We pass bytes because it's non-serializable and thus\n+            # will not be handled by the auto-get_config\n+            MyLayer(b\"custom\").get_config()\n \n         class MyLayerNew(base_layer.Layer):\n             def __init__(self, my_kwarg=\"default\", **kwargs):\n@@ -1075,6 +1079,27 @@ class BaseLayerTest(test_combinations.TestCase):\n         self.assertLen(layer.trainable_variables, 0)\n         self.assertLen(layer.non_trainable_variables, 3)\n \n+    def test_auto_get_config(self):\n+        class MyLayer(base_layer.Layer):\n+            def __init__(self, var1, var2, var3=None, **kwargs):\n+                super().__init__(**kwargs)\n+\n+        layer = MyLayer(\"a\", 2, var3=True, name=\"mylayer\")\n+        config = layer.get_config()\n+        self.assertLen(config, 6)\n+        self.assertEqual(config[\"var1\"], \"a\")\n+        self.assertEqual(config[\"var2\"], 2)\n+        self.assertEqual(config[\"var3\"], True)\n+        self.assertEqual(config[\"name\"], \"mylayer\")\n+        self.assertEqual(config[\"trainable\"], True)\n+        self.assertEqual(config[\"dtype\"], \"float32\")\n+        layer = MyLayer.from_config(config)\n+        self.assertDictEqual(layer.get_config(), config)\n+\n+        layer = MyLayer(\"a\", 2, var3=tf.nn.relu)\n+        with self.assertRaises(NotImplementedError):\n+            config = layer.get_config()\n+\n \n @test_utils.run_v2_only\n class SymbolicSupportTest(test_combinations.TestCase):\n\n@@ -702,7 +702,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             **kwargs: Arguments supported for backwards compatibility only.\n         \"\"\"\n         base_layer.keras_api_gauge.get_cell(\"compile\").set(True)\n-        self._compile_config = CompileConfig(\n+        self._compile_config = generic_utils.Config(\n             optimizer=optimizer,\n             loss=loss,\n             metrics=metrics,\n@@ -3078,11 +3078,11 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         Returns:\n             Python dictionary containing the configuration of this `Model`.\n         \"\"\"\n-\n-        # Return an empty dict here because otherwise subclass model developers\n-        # may see their model's `__init__()` be fed with unexpected keyword\n-        # argument, if their `__init__()` takes no argument for example, and\n-        # they don't override `from_config()`, which would use `cls(**config)`\n+        # Return an empty dict here because otherwise Model\n+        # subclass developers may see\n+        # their model's `__init__()` fed with unexpected keyword arguments,\n+        # if their `__init__()` takes no argument for example, and they\n+        # don't override `from_config()`, which would use `cls(**config)`\n         # as a result.\n         config = {}\n         if getattr(saving_lib._SAVING_V3_ENABLED, \"value\", False):\n@@ -4182,11 +4182,3 @@ def is_functional_model_init_params(args, kwargs):\n         or \"inputs\" in kwargs\n         and \"outputs\" in kwargs\n     )\n-\n-\n-class CompileConfig:\n-    def __init__(self, **config):\n-        self.config = config\n-\n-    def serialize(self):\n-        return saving_lib.serialize_keras_object(self.config)\n\n@@ -12,12 +12,9 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\n-\n \"\"\"Base Metric classes.\"\"\"\n \n import abc\n-import copy\n import types\n import warnings\n \n@@ -218,28 +215,22 @@ class Metric(base_layer.Layer, metaclass=abc.ABCMeta):\n         args = \",\".join(f\"{k}={v}\" for k, v in self.get_config().items())\n         return f\"{self.__class__.__name__}({args})\"\n \n-    def __deepcopy__(self, memo):\n-        result = type(self)(name=self.name, dtype=self.dtype)\n-        memo[id(self)] = result\n-\n-        for k, v in self.__dict__.items():\n-            if k in [\"update_state\", \"result\"]:\n-                # `update_state` keeps a closure of `update_state_fn`, and deep\n-                # copying it would result in copying that old reference. Avoid\n-                # that.  Likewise for `result`.\n-                continue\n-            if k in [\"_obj_reference_counts_dict\"]:\n-                # `Layer.__setattr__` attempts to flatten the\n-                # `ObjectIdentityDictionary`, which can't be done since it\n-                # stores heterogeneous instances.\n-                tf.Module.__setattr__(result, k, copy.deepcopy(v, memo))\n-            elif k in [\"_thread_local\", \"_metrics_lock\"]:\n-                # Can't pickle _thread.lock objects.\n-                setattr(result, k, v)\n-            else:\n-                setattr(result, k, copy.deepcopy(v, memo))\n-\n-        return result\n+    def __deepcopy__(self, memo=None):\n+        try:\n+            new_self = self.from_config(self.get_config())\n+        except NotImplementedError as e:\n+            raise NotImplementedError(\n+                \"Calling `__deepcopy__()` on a Keras metric \"\n+                \"requires the metric to be serializable,  \"\n+                \"i.e. it should implement `get_config()`.\\n\\n\"\n+                f\"Error encountered during serialization: [{e}]\"\n+            )\n+        # Note that metrics don't implement `build()` so their variables\n+        # are readily available after instantiation.\n+        if self.weights:\n+            new_self.set_weights(self.get_weights())\n+        memo[self] = new_self\n+        return new_self\n \n     @property\n     def dtype(self):\n\n@@ -27,6 +27,7 @@ import types as python_types\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n+from keras.saving.experimental import serialization_lib\n from keras.utils import io_utils\n from keras.utils import tf_inspect\n \n@@ -555,3 +556,11 @@ class LazyLoader(python_types.ModuleType):\n     def __getattr__(self, item):\n         module = self._load()\n         return getattr(module, item)\n+\n+\n+class Config:\n+    def __init__(self, **config):\n+        self.config = config\n+\n+    def serialize(self):\n+        return serialization_lib.serialize_keras_object(self.config)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b57b7d0f441ab197972ec55145907d520daf0487", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 10797 | Contributors (this commit): 23 | Commits (past 90d): 14 | Contributors (cumulative): 23 | DMM Complexity: None\n\nDIFF:\n@@ -2029,7 +2029,7 @@ class RandomGenerator(tf.__internal__.tracking.AutoTrackable):\n         elif getattr(_SEED_GENERATOR, \"generator\", None):\n             return _SEED_GENERATOR.generator.randint(1, 1e9)\n         else:\n-            return random.randint(1, 1e9)\n+            return random.randint(1, int(1e9))\n \n     def random_normal(\n         self, shape, mean=0.0, stddev=1.0, dtype=None, nonce=None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
