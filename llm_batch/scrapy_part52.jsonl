{"custom_id": "scrapy#426282bbc9d25fc6cfec25b442f9d924402fc961", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 42/26 | Churn Δ: 2 | Churn Cumulative: 2 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -30,6 +30,7 @@ class FormRequest(Request):\n             items = formdata.iteritems() if isinstance(formdata, dict) else formdata\n             query = [(unicode_to_str(k, self.encoding), _unicode_to_str(v, self.encoding))\n                     for k, v in items]\n+            self.method = 'POST'\n             self.body = urllib.urlencode(query, doseq=1)\n             self.headers['Content-Type'] = 'application/x-www-form-urlencoded'\n \n\n@@ -184,6 +184,7 @@ class FormRequestTest(unittest.TestCase):\n         # using default encoding (utf-8)\n         data = {'one': 'two', 'price': '\\xc2\\xa3 100'}\n         r2 = FormRequest(\"http://www.example.com\", formdata=data)\n+        self.assertEqual(r2.method, 'POST')\n         self.assertEqual(r2.encoding, 'utf-8')\n         self.assertEqual(r2.body, 'price=%C2%A3+100&one=two')\n         self.assertEqual(r2.headers['Content-Type'], 'application/x-www-form-urlencoded')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#77b25d3036a6b013886d048f5a282b0f7ebd2aef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 18 | Churn Cumulative: 18 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,18 @@\n+import simplejson\n+\n+from scrapy.utils.serialization import ScrapyJSONEncoder\n+\n+\n+def item_to_dict(item):\n+    \"\"\"Returns a dict representation of an item\"\"\"\n+    res = {}\n+    for field in item.fields:\n+        res[field] = getattr(item, field)\n+        \n+    return res\n+\n+\n+def item_to_json(item):\n+    \"\"\"Returns a json representation of an item\"\"\"\n+    return simplejson.dumps(item_to_dict(item), cls=ScrapyJSONEncoder)\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7eb79488aa1c1b773271f7d2fc47102e2fb76b5e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 34/25 | Churn Δ: 26 | Churn Cumulative: 26 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,5 @@\n from scrapy.contrib_exp.adaptors.extraction import extract, ExtractImageLinks\n-from scrapy.contrib_exp.adaptors.markup import remove_tags, remove_root, unquote\n+from scrapy.contrib_exp.adaptors.markup import remove_tags, remove_root, remove_escape, unquote\n from scrapy.contrib_exp.adaptors.misc import to_unicode, clean_spaces, strip, drop_empty, delist, Regex\n from scrapy.contrib_exp.adaptors.date import to_date\n \n\n@@ -1,5 +1,5 @@\n import re\n-from scrapy.utils.markup import replace_tags, unquote_markup\n+from scrapy.utils.markup import replace_tags, remove_escape_chars, unquote_markup\n from scrapy.utils.python import str_to_unicode\n from scrapy.item.adaptors import adaptize\n \n@@ -30,6 +30,24 @@ def remove_root(value):\n         value = m.group(1)\n     return str_to_unicode(value)\n \n+\n+def remove_escape(which_ones=('\\n','\\t','\\r'), replace_str=u''):\n+    \"\"\"\n+    Factory that returns an adaptor for removing/replacing each escape\n+    character in the `wich_ones` parameter found in the given value.\n+\n+    If `replace_str` is given, escape characters are replaced by that\n+    string, else they're removed.\n+\n+    Input: string/unicode\n+    Output: unicode\n+    \n+    \"\"\"\n+    def _remove_escape(value):\n+        return remove_escape_chars(value, which_ones, replace_str)\n+    return _remove_escape\n+\n+\n def unquote(keep=None):\n     \"\"\"\n     This factory returns an adaptor that\n\n@@ -105,14 +105,14 @@ def remove_tags_with_content(text, which_ones=()):\n     return text\n     \n \n-def remove_escape_chars(text, which_ones=('\\n','\\t','\\r')):\n+def remove_escape_chars(text, which_ones=('\\n','\\t','\\r'), replace_str=u''):\n     \"\"\" Remove escape chars. Default : \\\\n, \\\\t, \\\\r\n \n         which_ones -- is a tuple of which escape chars we want to remove.\n                       By default removes \\n, \\t, \\r.\n     \"\"\"\n     for ec in which_ones:\n-        text = text.replace(ec, u'')\n+        text = text.replace(ec, replace_str)\n     return str_to_unicode(text)\n \n def unquote_markup(text, keep=(), remove_illegal=True):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1b3d40e63902b7c6d5c84b248073c77de97641d6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 5 | Churn Cumulative: 9 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -112,6 +112,7 @@ class UtilsMarkupTest(unittest.TestCase):\n         self.assertEqual(remove_escape_chars(u'escape\\n\\n'), u'escape')\n         self.assertEqual(remove_escape_chars(u'escape\\n', which_ones=('\\t',)), u'escape\\n')\n         self.assertEqual(remove_escape_chars(u'escape\\tchars\\n', which_ones=('\\t')), 'escapechars\\n')\n+        self.assertEqual(remove_escape_chars(u'escape\\tchars\\n', replace_by=' '), 'escape chars ')\n \n     def test_unquote_markup(self):\n         sample_txt1 = u\"\"\"<node1>hi, this is sample text with entities: &amp; &copy;\n\n@@ -105,14 +105,14 @@ def remove_tags_with_content(text, which_ones=()):\n     return text\n     \n \n-def remove_escape_chars(text, which_ones=('\\n','\\t','\\r'), replace_str=u''):\n+def remove_escape_chars(text, which_ones=('\\n','\\t','\\r'), replace_by=u''):\n     \"\"\" Remove escape chars. Default : \\\\n, \\\\t, \\\\r\n \n         which_ones -- is a tuple of which escape chars we want to remove.\n                       By default removes \\n, \\t, \\r.\n     \"\"\"\n     for ec in which_ones:\n-        text = text.replace(ec, replace_str)\n+        text = text.replace(ec, str_to_unicode(replace_by))\n     return str_to_unicode(text)\n \n def unquote_markup(text, keep=(), remove_illegal=True):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#da7b9358a4973b44859433c11377c888d559361e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 39 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -31,20 +31,20 @@ def remove_root(value):\n     return str_to_unicode(value)\n \n \n-def remove_escape(which_ones=('\\n','\\t','\\r'), replace_str=u''):\n+def remove_escape(which_ones=('\\n','\\t','\\r'), replace_by=u''):\n     \"\"\"\n     Factory that returns an adaptor for removing/replacing each escape\n     character in the `wich_ones` parameter found in the given value.\n \n-    If `replace_str` is given, escape characters are replaced by that\n-    string, else they're removed.\n+    If `replace_by` is given, escape characters are replaced by that\n+    text, else they're removed.\n \n     Input: string/unicode\n     Output: unicode\n     \n     \"\"\"\n     def _remove_escape(value):\n-        return remove_escape_chars(value, which_ones, replace_str)\n+        return remove_escape_chars(value, which_ones, replace_by)\n     return _remove_escape\n \n \n\n@@ -110,6 +110,9 @@ def remove_escape_chars(text, which_ones=('\\n','\\t','\\r'), replace_by=u''):\n \n         which_ones -- is a tuple of which escape chars we want to remove.\n                       By default removes \\n, \\t, \\r.\n+\n+        replace_by -- text to replace the escape chars for.\n+                      It defaults to '', so the escape chars are removed.\n     \"\"\"\n     for ec in which_ones:\n         text = text.replace(ec, str_to_unicode(replace_by))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3dee4b6728359f7558329f045d82df4d2c7763c6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 20/13 | Churn Δ: 12 | Churn Cumulative: 12 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -21,14 +21,14 @@ class RedirectMiddleware(object):\n         status = exception.status\n         response = exception.response\n \n-        if status in [302, 303]:\n+        if status in [302, 303] and 'Location' in response.headers:\n             redirected_url = urljoin(request.url, response.headers['location'])\n             redirected = request.replace(url=redirected_url, method='GET', body='')\n             redirected.headers.pop('Content-Type', None)\n             redirected.headers.pop('Content-Length', None)\n             return self._redirect(redirected, request, spider, status)\n \n-        if status in [301, 307]:\n+        if status in [301, 307] and 'Location' in response.headers:\n             redirected_url = urljoin(request.url, response.headers['location'])\n             redirected = request.replace(url=redirected_url)\n             return self._redirect(redirected, request, spider, status)\n\n@@ -30,6 +30,10 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         assert isinstance(req2, Request)\n         self.assertEqual(req2.url, url2)\n \n+        # response without Location header but with status code is 3XX should be ignored\n+        del rsp.headers['Location']\n+        assert self.mw.process_exception(req, exc, self.spider) is None\n+\n     def test_redirect_302(self):\n         url = 'http://www.example.com/302'\n         url2 = 'http://www.example.com/redirected2'\n@@ -50,6 +54,10 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         assert not req2.body, \\\n             \"Redirected body must be empty, not '%s'\" % req2.body\n \n+        # response without Location header but with status code is 3XX should be ignored\n+        del rsp.headers['Location']\n+        assert self.mw.process_exception(req, exc, self.spider) is None\n+\n     def test_meta_refresh(self):\n         body = \"\"\"<html>\n             <head><meta http-equiv=\"refresh\" content=\"5;url=http://example.org/newpage\" /></head>\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#314a1c2bc2fbe1258405e2b95017b96e28da683f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 72 | Lines Deleted: 57 | Files Changed: 7 | Hunks: 16 | Methods Changed: 5 | Complexity Δ (Sum/Max): 65/24 | Churn Δ: 129 | Churn Cumulative: 129 | Contributors (this commit): 2 | Commits (past 90d): 12 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -52,34 +52,6 @@ EXTENSIONS = (\n     'scrapy.management.telnet.TelnetConsole',\n )\n \n-DOWNLOADER_MIDDLEWARES = (\n-    # Engine side\n-    'scrapy.contrib.downloadermiddleware.errorpages.ErrorPagesMiddleware',\n-    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware',\n-    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware',\n-    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware',\n-    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware',\n-    'scrapy.contrib.downloadermiddleware.common.CommonMiddleware',\n-    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware',\n-    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware',\n-    'scrapy.contrib.downloadermiddleware.debug.CrawlDebug',\n-    'scrapy.contrib.downloadermiddleware.cache.CacheMiddleware',\n-    # Downloader side\n-)\n-\n-SPIDER_MIDDLEWARES = (\n-    # Engine side\n-    'scrapy.contrib.spidermiddleware.limit.RequestLimitMiddleware',\n-    'scrapy.contrib.spidermiddleware.restrict.RestrictMiddleware',\n-    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware',\n-    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware',\n-    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware',\n-    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware',\n-    'scrapy.contrib.spidermiddleware.urlfilter.UrlFilterMiddleware',\n-    'scrapy.contrib.spidermiddleware.duplicatesfilter.DuplicatesFilterMiddleware',\n-    # Spider side\n-)\n-\n ITEM_PIPELINES = (\n     'googledir.pipelines.GoogledirPipeline',\n )\n\n@@ -59,22 +59,24 @@ DOWNLOAD_TIMEOUT = 180      # 3mins\n \n DOWNLOADER_DEBUG = False\n \n-DOWNLOADER_MIDDLEWARES = [\n+DOWNLOADER_MIDDLEWARES = {}\n+\n+DOWNLOADER_MIDDLEWARES_BASE = {\n     # Engine side\n-    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware',\n-    'scrapy.contrib.downloadermiddleware.errorpages.ErrorPagesMiddleware',\n-    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware',\n-    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware',\n-    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware',\n-    'scrapy.contrib.downloadermiddleware.common.CommonMiddleware',\n-    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware',\n-    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware',\n-    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware',\n-    'scrapy.contrib.downloadermiddleware.debug.CrawlDebug',\n-    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats',\n-    'scrapy.contrib.downloadermiddleware.cache.CacheMiddleware',\n+    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,\n+    'scrapy.contrib.downloadermiddleware.errorpages.ErrorPagesMiddleware': 200,\n+    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,\n+    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,\n+    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,\n+    'scrapy.contrib.downloadermiddleware.common.CommonMiddleware': 550,\n+    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,\n+    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,\n+    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 800,\n+    'scrapy.contrib.downloadermiddleware.debug.CrawlDebug': 840,\n+    'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,\n+    'scrapy.contrib.downloadermiddleware.cache.CacheMiddleware': 900,\n     # Downloader side\n-]\n+}\n \n DOWNLOADER_STATS = True\n \n@@ -151,9 +153,11 @@ ROBOTSTXT_OBEY = False\n \n SCHEDULER = 'scrapy.core.scheduler.Scheduler'\n \n-SCHEDULER_MIDDLEWARES = [\n-        'scrapy.contrib.schedulermiddleware.duplicatesfilter.DuplicatesFilterMiddleware',\n-        ]\n+SCHEDULER_MIDDLEWARES = {}\n+\n+SCHEDULER_MIDDLEWARES_BASE = {\n+    'scrapy.contrib.schedulermiddleware.duplicatesfilter.DuplicatesFilterMiddleware': 500,\n+}\n \n SCHEDULER_ORDER = 'BFO'   # available orders: BFO (default), DFO\n \n@@ -161,17 +165,19 @@ SPIDER_MODULES = []\n \n SPIDERPROFILER_ENABLED = False\n \n-SPIDER_MIDDLEWARES = [\n+SPIDER_MIDDLEWARES = {}\n+\n+SPIDER_MIDDLEWARES_BASE = {\n     # Engine side\n-    'scrapy.contrib.itemsampler.ItemSamplerMiddleware',\n-    'scrapy.contrib.spidermiddleware.limit.RequestLimitMiddleware',\n-    'scrapy.contrib.spidermiddleware.restrict.RestrictMiddleware',\n-    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware',\n-    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware',\n-    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware',\n-    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware',\n+    'scrapy.contrib.itemsampler.ItemSamplerMiddleware': 100,\n+    'scrapy.contrib.spidermiddleware.limit.RequestLimitMiddleware': 200,\n+    'scrapy.contrib.spidermiddleware.restrict.RestrictMiddleware': 300,\n+    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n+    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n+    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,\n+    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,\n     # Spider side\n-]\n+}\n \n STATS_ENABLED = True\n STATS_CLEANUP = False\n\n@@ -12,6 +12,7 @@ from scrapy.http import Request, Response\n from scrapy.core.exceptions import NotConfigured\n from scrapy.utils.misc import load_object\n from scrapy.utils.defer import mustbe_deferred\n+from scrapy.utils.middleware import build_middleware_list\n from scrapy.core.downloader.handlers import download_any\n from scrapy.conf import settings\n \n@@ -37,7 +38,9 @@ class DownloaderMiddlewareManager(object):\n         \"\"\"Load middleware defined in settings module\n         \"\"\"\n         mws = []\n-        for mwpath in settings.getlist('DOWNLOADER_MIDDLEWARES') or ():\n+        mwlist = build_middleware_list(settings['DOWNLOADER_MIDDLEWARES_BASE'],\n+                                       settings['DOWNLOADER_MIDDLEWARES'])\n+        for mwpath in mwlist:\n             cls = load_object(mwpath)\n             if cls:\n                 try:\n\n@@ -12,6 +12,7 @@ from scrapy.http import Response\n from scrapy.core.exceptions import NotConfigured\n from scrapy.utils.misc import load_object\n from scrapy.utils.defer import mustbe_deferred\n+from scrapy.utils.middleware import build_middleware_list\n from scrapy.conf import settings\n \n class SchedulerMiddlewareManager(object):\n@@ -29,7 +30,9 @@ class SchedulerMiddlewareManager(object):\n     def load(self):\n         \"\"\"Load middleware defined in settings module\"\"\"\n         mws = []\n-        for mwpath in settings.getlist('SCHEDULER_MIDDLEWARES') or ():\n+        mwlist = build_middleware_list(settings['SCHEDULER_MIDDLEWARES_BASE'],\n+                                       settings['SCHEDULER_MIDDLEWARES'])\n+        for mwpath in mwlist:\n             cls = load_object(mwpath)\n             if cls:\n                 try:\n\n@@ -10,6 +10,7 @@ from scrapy import log\n from scrapy.core.exceptions import NotConfigured\n from scrapy.utils.misc import load_object, arg_to_iter\n from scrapy.utils.defer import mustbe_deferred, defer_result\n+from scrapy.utils.middleware import build_middleware_list\n from scrapy.conf import settings\n \n def _isiterable(possible_iterator):\n@@ -37,7 +38,9 @@ class SpiderMiddlewareManager(object):\n     def load(self):\n         \"\"\"Load middleware defined in settings module\"\"\"\n         mws = []\n-        for mwpath in settings.getlist('SPIDER_MIDDLEWARES') or ():\n+        mwlist = build_middleware_list(settings['SPIDER_MIDDLEWARES_BASE'],\n+                                       settings['SPIDER_MIDDLEWARES'])\n+        for mwpath in mwlist:\n             cls = load_object(mwpath)\n             if cls:\n                 try:\n\n@@ -0,0 +1,17 @@\n+import unittest\n+\n+from scrapy.utils.middleware import build_middleware_list\n+\n+class UtilsMiddlewareTestCase(unittest.TestCase):\n+\n+    def test_build_middleware_list(self):\n+        base = {'one': 1, 'two': 2, 'three': 3, 'five': 5, 'six': None}\n+        custom = {'two': None, 'three': 8, 'four': 4}\n+        self.assertEqual(build_middleware_list(base, custom),\n+                         ['one', 'four', 'five', 'three'])\n+\n+        custom = ['a', 'b', 'c']\n+        self.assertEqual(build_middleware_list(base, custom), custom)\n+\n+if __name__ == \"__main__\":\n+    unittest.main()\n\n@@ -0,0 +1,11 @@\n+from operator import itemgetter\n+\n+def build_middleware_list(base, custom):\n+    \"\"\"Compose a middleware list based on a custom and base dict of\n+    middlewares, unless custom is already a list, in which case it's returned.\n+    \"\"\"\n+    if isinstance(custom, (list, tuple)):\n+        return custom\n+    mwdict = base.copy()\n+    mwdict.update(custom)\n+    return [k for k, v in sorted(mwdict.items(), key=itemgetter(1)) if v is not None]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7166f9f6f9468ab043d8a8710d1ab62368e46d55", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 18/18 | Churn Δ: 4 | Churn Cumulative: 4 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -9,7 +9,7 @@ from scrapy.http import Request\n from scrapy.item import ScrapedItem\n from scrapy.core.exceptions import NotConfigured\n from scrapy.conf import settings\n-from scrapy.utils.misc import memoize\n+from scrapy.utils.misc import memoize, arg_to_iter\n from lrucache import LRUCache\n \n from .site import WebSite, WebResource\n@@ -63,7 +63,7 @@ def url_to_guid(httprequest):\n         except Exception, ex:\n             return _response(message='Error processing url')\n \n-        guids = [i.guid for i in items if isinstance(i, ScrapedItem)]\n+        guids = [i.guid for i in arg_to_iter(items) if isinstance(i, ScrapedItem)]\n         return _response(guids=guids)\n \n     deferred = defer.Deferred().addCallbacks(_on_success, _on_error)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#85cd7ea140f3bd52ef3481f0996e9a432816c0e3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 12 | Files Changed: 2 | Hunks: 13 | Methods Changed: 21 | Complexity Δ (Sum/Max): 58/36 | Churn Δ: 32 | Churn Cumulative: 32 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,11 +5,11 @@ import libxml2\n from scrapy.utils.iterators import csviter, xmliter\n from scrapy.http import XmlResponse, TextResponse\n \n-class UtilsXmlTestCase(unittest.TestCase):\n+class UtilsIteratorsTestCase(unittest.TestCase):\n     ### NOTE: Encoding issues have been found with BeautifulSoup for utf-16 files, utf-16 test removed ###\n     # pablo: Tests shouldn't be removed, but commented with proper steps on how\n     # to reproduce the missing functionality\n-    def test_iterator(self):\n+    def test_xmliter(self):\n         body = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\\n             <products xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:noNamespaceSchemaLocation=\"someschmea.xsd\">\\\n               <product id=\"001\">\\\n@@ -30,13 +30,13 @@ class UtilsXmlTestCase(unittest.TestCase):\n         self.assertEqual(attrs, \n                          [(['001'], ['Name 1'], ['Type 1']), (['002'], ['Name 2'], ['Type 2'])])\n \n-    def test_iterator_text(self):\n+    def test_xmliter_text(self):\n         body = u\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?><products><product>one</product><product>two</product></products>\"\"\"\n         \n         self.assertEqual([x.x(\"text()\").extract() for x in xmliter(body, 'product')],\n                          [[u'one'], [u'two']])\n \n-    def test_iterator_namespaces(self):\n+    def test_xmliter_namespaces(self):\n         body = \"\"\"\\\n             <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n             <rss version=\"2.0\" xmlns:g=\"http://base.google.com/ns/1.0\">\n@@ -70,7 +70,7 @@ class UtilsXmlTestCase(unittest.TestCase):\n         self.assertEqual(node.x('id/text()').extract(), [])\n         self.assertEqual(node.x('price/text()').extract(), [])\n \n-    def test_iterator_exception(self):\n+    def test_xmliter_exception(self):\n         body = u\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?><products><product>one</product><product>two</product></products>\"\"\"\n         \n         iter = xmliter(body, 'product')\n@@ -79,13 +79,21 @@ class UtilsXmlTestCase(unittest.TestCase):\n \n         self.assertRaises(StopIteration, iter.next)\n \n+    def test_xmliter_encoding(self):\n+        body = '<?xml version=\"1.0\" encoding=\"ISO-8859-9\"?>\\n<xml>\\n    <item>Some Turkish Characters \\xd6\\xc7\\xde\\xdd\\xd0\\xdc \\xfc\\xf0\\xfd\\xfe\\xe7\\xf6</item>\\n</xml>\\n\\n'\n+        response = XmlResponse('http://www.example.com', body=body)\n+        self.assertEqual(\n+            xmliter(response, 'item').next().extract(),\n+            u'<item>Some Turkish Characters \\xd6\\xc7\\u015e\\u0130\\u011e\\xdc \\xfc\\u011f\\u0131\\u015f\\xe7\\xf6</item>'\n+        )\n+\n class UtilsCsvTestCase(unittest.TestCase):\n     sample_feeds_dir = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'sample_data', 'feeds')\n     sample_feed_path = os.path.join(sample_feeds_dir, 'feed-sample3.csv')\n     sample_feed2_path = os.path.join(sample_feeds_dir, 'feed-sample4.csv')\n     sample_feed3_path = os.path.join(sample_feeds_dir, 'feed-sample5.csv')\n \n-    def test_iterator_defaults(self):\n+    def test_csviter_defaults(self):\n         body = open(self.sample_feed_path).read()\n \n         response = TextResponse(url=\"http://example.com/\", body=body)\n@@ -103,7 +111,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n             self.assert_(all((isinstance(k, unicode) for k in result_row.keys())))\n             self.assert_(all((isinstance(v, unicode) for v in result_row.values())))\n \n-    def test_iterator_delimiter(self):\n+    def test_csviter_delimiter(self):\n         body = open(self.sample_feed_path).read().replace(',', '\\t')\n \n         response = TextResponse(url=\"http://example.com/\", body=body)\n@@ -115,7 +123,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n                           {u'id': u'3', u'name': u'multi',   u'value': u'foo\\nbar'},\n                           {u'id': u'4', u'name': u'empty',   u'value': u''}])\n \n-    def test_iterator_headers(self):\n+    def test_csviter_headers(self):\n         sample = open(self.sample_feed_path).read().splitlines()\n         headers, body = sample[0].split(','), '\\n'.join(sample[1:])\n \n@@ -128,7 +136,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n                           {u'id': u'3', u'name': u'multi',   u'value': u'foo\\nbar'},\n                           {u'id': u'4', u'name': u'empty',   u'value': u''}])\n \n-    def test_iterator_falserow(self):\n+    def test_csviter_falserow(self):\n         body = open(self.sample_feed_path).read()\n         body = '\\n'.join((body, 'a,b', 'a,b,c,d'))\n \n@@ -141,7 +149,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n                           {u'id': u'3', u'name': u'multi',   u'value': u'foo\\nbar'},\n                           {u'id': u'4', u'name': u'empty',   u'value': u''}])\n \n-    def test_iterator_exception(self):\n+    def test_csviter_exception(self):\n         body = open(self.sample_feed_path).read()\n \n         response = TextResponse(url=\"http://example.com/\", body=body)\n@@ -153,7 +161,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n \n         self.assertRaises(StopIteration, iter.next)\n \n-    def test_iterator_encoding(self):\n+    def test_csviter_encoding(self):\n         body1 = open(self.sample_feed2_path).read()\n         body2 = open(self.sample_feed3_path).read()\n \n\n@@ -34,7 +34,7 @@ class XPathSelector(object):\n                 self.doc = Libxml2Document(response, constructor=constructor)\n             self.xmlNode = self.doc.xmlDoc\n         elif text:\n-            response = TextResponse(url=None, body=unicode_to_str(text))\n+            response = TextResponse(url=None, body=unicode_to_str(text), encoding='utf-8')\n             self.doc = Libxml2Document(response, constructor=constructor)\n             self.xmlNode = self.doc.xmlDoc\n         self.expr = expr\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ba48a24bb76413d0e2f57090feeff66058c089a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 37 | Files Changed: 8 | Hunks: 30 | Methods Changed: 13 | Complexity Δ (Sum/Max): 96/31 | Churn Δ: 73 | Churn Cumulative: 103 | Contributors (this commit): 2 | Commits (past 90d): 17 | Contributors (cumulative): 16 | DMM Complexity: 0.0\n\nDIFF:\n@@ -8,3 +8,12 @@ To run all Scrapy unittests type:\n Keep in mind that some tests may be skipped if you don't have some (optional)\n modules available like MySQLdb or simplejson.\n \"\"\"\n+\n+import os\n+\n+tests_datadir = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'sample_data')\n+\n+def get_testdata(*paths):\n+    \"\"\"Return test data\"\"\"\n+    path = os.path.join(tests_datadir, *paths)\n+    return open(path).read()\n\n@@ -3,11 +3,13 @@ Module to run Scrapy tests - see scrapy.tests docstring\n \"\"\"\n \n if __name__ == '__main__':\n-    import os\n+    import os, sys\n     from twisted.trial import runner, reporter\n \n+    tests_to_run = sys.argv[1:] or ['scrapy']\n+\n     os.environ['SCRAPY_SETTINGS_DISABLED'] = '1'\n     loader = runner.TestLoader()\n     runner = runner.TrialRunner(reporter.TreeReporter)\n-    suite = loader.loadByNames(['scrapy'], recurse=True)\n+    suite = loader.loadByNames(tests_to_run, recurse=True)\n     runner.run(suite)\n\n@@ -1,5 +1,4 @@\n # -*- coding: utf8 -*-\n-import os\n import unittest\n import re\n \n@@ -7,7 +6,7 @@ from scrapy.item.adaptors import AdaptorPipe\n from scrapy.contrib_exp import adaptors\n from scrapy.http import HtmlResponse, Headers\n from scrapy.xpath.selector import HtmlXPathSelector, XmlXPathSelector\n-from scrapy.link import Link\n+from scrapy.tests import get_testdata\n \n class AdaptorPipeTestCase(unittest.TestCase):\n     def test_pipe_init(self):\n@@ -37,12 +36,9 @@ class AdaptorPipeTestCase(unittest.TestCase):\n         self.assertTrue(isinstance(pipe1 + sample_callable, AdaptorPipe))\n \n class AdaptorsTestCase(unittest.TestCase):\n-    def setUp(self):\n-        self.samplesdir = os.path.abspath(os.path.join(os.path.dirname(__file__), 'sample_data', 'adaptors'))\n \n     def get_selector(self, domain, url, sample_filename, headers=None, selector=HtmlXPathSelector):\n-        sample_filename = os.path.join(self.samplesdir, sample_filename)\n-        body = file(sample_filename).read()\n+        body = get_testdata('adaptors', sample_filename)\n         response = HtmlResponse(url=url, headers=Headers(headers), status=200, body=body)\n         return selector(response)\n \n\n@@ -1,21 +1,14 @@\n-import os\n from unittest import TestCase, main\n from scrapy.http import Response, XmlResponse\n from scrapy.contrib_exp.downloadermiddleware.decompression import DecompressionMiddleware\n+from scrapy.tests import get_testdata\n \n def setUp():\n-    datadir = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'sample_data', 'compressed')\n     formats = ['tar', 'xml.bz2', 'xml.gz', 'zip']\n-\n-    uncompressed_fd = open(os.path.join(datadir, 'feed-sample1.xml'), 'r')\n-    uncompressed_body = uncompressed_fd.read()\n-    uncompressed_fd.close()\n-\n+    uncompressed_body = get_testdata('compressed', 'feed-sample1.xml')\n     test_responses = {}\n     for format in formats:\n-        fd = open(os.path.join(datadir, 'feed-sample1.' + format), 'r')\n-        body = fd.read()\n-        fd.close()\n+        body = get_testdata('compressed', 'feed-sample1.' + format)\n         test_responses[format] = Response('http://foo.com/bar', body=body)\n     return uncompressed_body, test_responses\n \n\n@@ -6,9 +6,10 @@ from os.path import join, abspath, dirname\n from scrapy.spider import spiders\n from scrapy.http import Response, Request\n from scrapy.contrib.downloadermiddleware.httpcompression import HttpCompressionMiddleware\n+from scrapy.tests import tests_datadir\n \n \n-SAMPLEDIR = join(abspath(dirname(__file__)), 'sample_data/compressed')\n+SAMPLEDIR = join(tests_datadir, 'compressed')\n \n FORMAT = {\n         'gzip': ('html-gzip.bin', 'gzip'),\n\n@@ -10,6 +10,8 @@ import unittest\n from twisted.internet import reactor\n from twisted.web import server, resource, static, util\n \n+from scrapy.tests import tests_datadir\n+\n #class TestResource(resource.Resource):\n #    isLeaf = True\n #\n@@ -17,7 +19,7 @@ from twisted.web import server, resource, static, util\n #        return \"hello world!\"\n \n def start_test_site():\n-    root_dir = os.path.join(os.path.abspath(os.path.dirname(__file__)), \"sample_data\", \"test_site\")\n+    root_dir = os.path.join(tests_datadir, \"test_site\")\n     r = static.File(root_dir)\n #    r.putChild(\"test\", TestResource())\n     r.putChild(\"redirect\", util.Redirect(\"/redirected\"))\n\n@@ -1,4 +1,3 @@\n-import os\n import re\n import unittest\n \n@@ -6,6 +5,7 @@ from scrapy.http import HtmlResponse\n from scrapy.link import LinkExtractor, Link\n from scrapy.link.extractors import RegexLinkExtractor\n from scrapy.contrib.link_extractors import HTMLImageLinkExtractor\n+from scrapy.tests import get_testdata\n \n class LinkExtractorTestCase(unittest.TestCase):\n     def test_basic(self):\n@@ -36,11 +36,10 @@ class LinkExtractorTestCase(unittest.TestCase):\n                          [Link(url='http://otherdomain.com/base/item/12.html', text='Item 12')])\n \n     def test_extraction_encoding(self):\n-        base_path = os.path.join(os.path.dirname(__file__), 'sample_data', 'link_extractor')\n-        body = open(os.path.join(base_path, 'linkextractor_noenc.html'), 'r').read()\n+        body = get_testdata('link_extractor', 'linkextractor_noenc.html')\n         response_utf8 = HtmlResponse(url='http://example.com/utf8', body=body, headers={'Content-Type': ['text/html; charset=utf-8']})\n         response_noenc = HtmlResponse(url='http://example.com/noenc', body=body)\n-        body = open(os.path.join(base_path, 'linkextractor_latin1.html'), 'r').read()\n+        body = get_testdata('link_extractor', 'linkextractor_latin1.html')\n         response_latin1 = HtmlResponse(url='http://example.com/latin1', body=body)\n \n         lx = LinkExtractor()\n@@ -66,8 +65,7 @@ class LinkExtractorTestCase(unittest.TestCase):\n \n class RegexLinkExtractorTestCase(unittest.TestCase):\n     def setUp(self):\n-        base_path = os.path.join(os.path.dirname(__file__), 'sample_data', 'link_extractor')\n-        body = open(os.path.join(base_path, 'regex_linkextractor.html'), 'r').read()\n+        body = get_testdata('link_extractor', 'regex_linkextractor.html')\n         self.response = HtmlResponse(url='http://example.com/index', body=body)\n \n     def test_urls_type(self):\n@@ -202,8 +200,7 @@ class RegexLinkExtractorTestCase(unittest.TestCase):\n \n class HTMLImageLinkExtractorTestCase(unittest.TestCase):\n     def setUp(self):\n-        base_path = os.path.join(os.path.dirname(__file__), 'sample_data', 'link_extractor')\n-        body = open(os.path.join(base_path, 'image_linkextractor.html'), 'r').read()\n+        body = get_testdata('link_extractor', 'image_linkextractor.html')\n         self.response = HtmlResponse(url='http://example.com/index', body=body)\n \n     def tearDown(self):\n\n@@ -4,6 +4,7 @@ import libxml2\n \n from scrapy.utils.iterators import csviter, xmliter\n from scrapy.http import XmlResponse, TextResponse\n+from scrapy.tests import get_testdata\n \n class UtilsIteratorsTestCase(unittest.TestCase):\n     ### NOTE: Encoding issues have been found with BeautifulSoup for utf-16 files, utf-16 test removed ###\n@@ -94,8 +95,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n     sample_feed3_path = os.path.join(sample_feeds_dir, 'feed-sample5.csv')\n \n     def test_csviter_defaults(self):\n-        body = open(self.sample_feed_path).read()\n-\n+        body = get_testdata('feeds', 'feed-sample3.csv')\n         response = TextResponse(url=\"http://example.com/\", body=body)\n         csv = csviter(response)\n \n@@ -112,8 +112,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n             self.assert_(all((isinstance(v, unicode) for v in result_row.values())))\n \n     def test_csviter_delimiter(self):\n-        body = open(self.sample_feed_path).read().replace(',', '\\t')\n-\n+        body = get_testdata('feeds', 'feed-sample3.csv').replace(',', '\\t')\n         response = TextResponse(url=\"http://example.com/\", body=body)\n         csv = csviter(response, delimiter='\\t')\n \n@@ -124,7 +123,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n                           {u'id': u'4', u'name': u'empty',   u'value': u''}])\n \n     def test_csviter_headers(self):\n-        sample = open(self.sample_feed_path).read().splitlines()\n+        sample = get_testdata('feeds', 'feed-sample3.csv').splitlines()\n         headers, body = sample[0].split(','), '\\n'.join(sample[1:])\n \n         response = TextResponse(url=\"http://example.com/\", body=body)\n@@ -137,7 +136,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n                           {u'id': u'4', u'name': u'empty',   u'value': u''}])\n \n     def test_csviter_falserow(self):\n-        body = open(self.sample_feed_path).read()\n+        body = get_testdata('feeds', 'feed-sample3.csv')\n         body = '\\n'.join((body, 'a,b', 'a,b,c,d'))\n \n         response = TextResponse(url=\"http://example.com/\", body=body)\n@@ -150,7 +149,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n                           {u'id': u'4', u'name': u'empty',   u'value': u''}])\n \n     def test_csviter_exception(self):\n-        body = open(self.sample_feed_path).read()\n+        body = get_testdata('feeds', 'feed-sample3.csv')\n \n         response = TextResponse(url=\"http://example.com/\", body=body)\n         iter = csviter(response)\n@@ -162,8 +161,8 @@ class UtilsCsvTestCase(unittest.TestCase):\n         self.assertRaises(StopIteration, iter.next)\n \n     def test_csviter_encoding(self):\n-        body1 = open(self.sample_feed2_path).read()\n-        body2 = open(self.sample_feed3_path).read()\n+        body1 = get_testdata('feeds', 'feed-sample4.csv')\n+        body2 = get_testdata('feeds', 'feed-sample5.csv')\n \n         response = TextResponse(url=\"http://example.com/\", body=body1, encoding='latin1')\n         csv = csviter(response)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eb649d661d8326bb611ea2e8aefb7d7a862eba9e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 11 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -16,4 +16,4 @@ tests_datadir = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'sample\n def get_testdata(*paths):\n     \"\"\"Return test data\"\"\"\n     path = os.path.join(tests_datadir, *paths)\n-    return open(path).read()\n+    return open(path, 'rb').read()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7b34e08392b264c9c373dfaae7c334157dab46ca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 18 | Files Changed: 2 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 23 | Churn Cumulative: 40 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -1,12 +1,14 @@\n \"\"\"\n scrapy.tests: this package contains all Scrapy unittests\n \n-To run all Scrapy unittests type: \n+To run all Scrapy unittests go to Scrapy main dir and type: \n \n-    python -m scrapy.tests.run\n+    bin/runtests.sh\n+    \n+If you're in windows use runtests.bat instead.\n \n Keep in mind that some tests may be skipped if you don't have some (optional)\n-modules available like MySQLdb or simplejson.\n+modules available like MySQLdb or simplejson, but that's not a problem.\n \"\"\"\n \n import os\n\n@@ -1,15 +0,0 @@\n-\"\"\"\n-Module to run Scrapy tests - see scrapy.tests docstring\n-\"\"\"\n-\n-if __name__ == '__main__':\n-    import os, sys\n-    from twisted.trial import runner, reporter\n-\n-    tests_to_run = sys.argv[1:] or ['scrapy']\n-\n-    os.environ['SCRAPY_SETTINGS_DISABLED'] = '1'\n-    loader = runner.TestLoader()\n-    runner = runner.TrialRunner(reporter.TreeReporter)\n-    suite = loader.loadByNames(tests_to_run, recurse=True)\n-    runner.run(suite)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#59e504a003b034b157c8ed8297b9992c79d62769", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 22 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 22 | Churn Cumulative: 117 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -22,25 +22,3 @@ class Link(object):\n     def __repr__(self):\n         return '<Link url=%r text=%r >' % (self.url, self.text)\n \n-\n-# FIXME: code below is for backwards compatibility and should be removed before\n-# the 0.7 release\n-\n-import warnings\n-\n-from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor, BaseSgmlLinkExtractor\n-\n-class LinkExtractor(BaseSgmlLinkExtractor):\n-    \n-    def __init__(self, *args, **kwargs):\n-        warnings.warn(\"scrapy.link.LinkExtractor is deprecated, use scrapy.contrib.linkextractors.sgml.BaseSgmlLinkExtractor instead\",\n-            DeprecationWarning, stacklevel=2)\n-        BaseSgmlLinkExtractor.__init__(self, *args, **kwargs)\n-\n-class RegexLinkExtractor(SgmlLinkExtractor):\n-\n-    def __init__(self, *args, **kwargs):\n-        warnings.warn(\"scrapy.link.RegexLinkExtractor is deprecated, use scrapy.contrib.linkextractors.sgml.SgmlLinkExtractor instead\",\n-            DeprecationWarning, stacklevel=2)\n-        SgmlLinkExtractor.__init__(self, *args, **kwargs)\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b87734341dc3d7debd389ef52900d2c22a9e7e0b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 2 | Churn Cumulative: 2 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -30,8 +30,8 @@ def xmlDoc_from_html(response):\n     return lxdoc\n \n def xmlDoc_from_xml(response):\n-    utf8body = body_as_utf8(response)\n     \"\"\"Return libxml2 doc for XMLs\"\"\"\n+    utf8body = body_as_utf8(response)\n     try:\n         lxdoc = libxml2.readDoc(utf8body, response.url, 'utf-8', xml_parser_options)\n     except TypeError:  # libxml2 doesn't parse text with null bytes\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a8a3de17ef528f438f1d418c0143b57a0dddd004", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 133/133 | Churn Δ: 1 | Churn Cumulative: 1 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -338,7 +338,6 @@ class ExecutionEngine(object):\n                 log.msg(\"Requested %s\" % request_info(request), level=log.DEBUG, domain=domain)\n             if isinstance(response, Response):\n                 response.request = request # tie request to obtained response\n-                cached = 'cached' if response.cached else 'live'\n                 log.msg(\"Crawled %s from <%s>\" % (response, request.headers.get('referer')), level=log.DEBUG, domain=domain)\n                 return response\n             elif isinstance(response, Request):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c161c29e086376285e93ed65a1f1bb98216df5cc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 10 | Methods Changed: 4 | Complexity Δ (Sum/Max): 14/14 | Churn Δ: 25 | Churn Cumulative: 25 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -4,6 +4,7 @@ Scrapy logging facility\n See documentation in docs/ref/logging.rst\n \"\"\"\n import sys\n+from traceback import format_exc\n \n from twisted.python import log\n \n@@ -11,7 +12,8 @@ from scrapy.conf import settings\n from scrapy.utils.python import unicode_to_str\n  \n # Logging levels\n-levels = {\n+SILENT, CRITICAL, ERROR, WARNING, INFO, DEBUG, TRACE = range(7)\n+level_names = {\n     0: \"SILENT\",\n     1: \"CRITICAL\",\n     2: \"ERROR\",\n@@ -23,17 +25,15 @@ levels = {\n \n BOT_NAME = settings['BOT_NAME']\n \n-# Set logging level module attributes\n-for v, k in levels.items():\n-    setattr(sys.modules[__name__], k, v)\n-\n # default logging level\n log_level = DEBUG\n \n started = False\n \n def start(logfile=None, loglevel=None, log_stdout=None):\n-    \"\"\" Init logging \"\"\"\n+    \"\"\"Initialize and start logging facility\"\"\"\n+    global log_level, started\n+\n     if started or not settings.getbool('LOG_ENABLED'):\n         return\n \n@@ -44,23 +44,22 @@ def start(logfile=None, loglevel=None, log_stdout=None):\n     file = open(logfile, 'a') if logfile else sys.stderr\n     level = int(getattr(sys.modules[__name__], loglevel)) if loglevel else DEBUG\n     log.startLogging(file, setStdout=log_stdout)\n-    setattr(sys.modules[__name__], 'log_level', level)\n-    setattr(sys.modules[__name__], 'started', True)\n+    log_level = level\n+    started = True\n \n def msg(message, level=INFO, component=BOT_NAME, domain=None):\n     \"\"\"Log message according to the level\"\"\"\n-    component = \"%s/%s\" % (BOT_NAME, domain) if domain else component\n+    component = \"%s/%s\" % (component, domain) if domain else component\n     if level <= log_level:\n-        msg_txt = unicode_to_str(\"%s: %s\" % (levels[level], message))\n+        msg_txt = unicode_to_str(\"%s: %s\" % (level_names[level], message))\n         log.msg(msg_txt, system=component)\n \n def exc(message, level=ERROR, component=BOT_NAME, domain=None):\n-    from traceback import format_exc\n     message = message + '\\n' + format_exc()\n     msg(message, level, component, domain)\n \n def err(*args, **kwargs):\n-    domain = kwargs.pop('domain', '')\n+    domain = kwargs.pop('domain', None)\n     component = kwargs.pop('component', BOT_NAME)\n-    kwargs['system'] = \"%s/%s\" % (BOT_NAME, domain) if domain else component\n+    kwargs['system'] = \"%s/%s\" % (component, domain) if domain else component\n     log.err(*args, **kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#13bb9934f9bf419e4a221684466e54bfad59fb63", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 177 | Lines Deleted: 193 | Files Changed: 3 | Hunks: 3 | Methods Changed: 31 | Complexity Δ (Sum/Max): 50/30 | Churn Δ: 370 | Churn Cumulative: 370 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -0,0 +1,70 @@\n+\"\"\"\n+HTMLParser-based link extractor\n+\"\"\"\n+\n+from HTMLParser import HTMLParser\n+\n+from scrapy.link import Link\n+from scrapy.utils.python import unique as unique_list\n+from scrapy.utils.url import safe_url_string, urljoin_rfc as urljoin\n+\n+class HtmlParserLinkExtractor(HTMLParser):\n+\n+    def __init__(self, tag=\"a\", attr=\"href\", process=None, unique=False):\n+        HTMLParser.__init__(self)\n+\n+        self.scan_tag = tag if callable(tag) else lambda t: t == tag\n+        self.scan_attr = attr if callable(attr) else lambda a: a == attr\n+        self.process_attr = process if callable(process) else lambda v: v\n+        self.unique = unique\n+\n+    def _extract_links(self, response_text, response_url, response_encoding):\n+        self.reset()\n+        self.feed(response_text)\n+        self.close()\n+\n+        links = unique_list(self.links, key=lambda link: link.url) if self.unique else self.links\n+\n+        ret = []\n+        base_url = self.base_url if self.base_url else response_url\n+        for link in links:\n+            link.url = urljoin(base_url, link.url)\n+            link.url = safe_url_string(link.url, response_encoding)\n+            link.text = link.text.decode(response_encoding)\n+            ret.append(link)\n+\n+        return ret\n+\n+    def extract_links(self, response):\n+        # wrapper needed to allow to work directly with text\n+        return self._extract_links(response.body, response.url, response.encoding)\n+\n+    def reset(self):\n+        HTMLParser.reset(self)\n+\n+        self.base_url = None\n+        self.current_link = None\n+        self.links = []\n+\n+    def handle_starttag(self, tag, attrs):\n+        if tag == 'base':\n+            self.base_url = dict(attrs).get('href')\n+        if self.scan_tag(tag):\n+            for attr, value in attrs:\n+                if self.scan_attr(attr):\n+                    url = self.process_attr(value)\n+                    link = Link(url=url)\n+                    self.links.append(link)\n+                    self.current_link = link\n+\n+    def handle_endtag(self, tag):\n+        self.current_link = None\n+\n+    def handle_data(self, data):\n+        if self.current_link and not self.current_link.text:\n+            self.current_link.text = data.strip()\n+\n+    def matches(self, url):\n+        \"\"\"This extractor matches with any url, since\n+        it doesn't contain any patterns\"\"\"\n+        return True\n\n@@ -0,0 +1,107 @@\n+\"\"\"\n+lxml-based link extractors (experimental)\n+\n+NOTE: The ideal name for this module would be `lxml`, but that's not possible\n+because it collides with the lxml library module.\n+\"\"\"\n+\n+from lxml import etree\n+import lxml.html\n+\n+from scrapy.link import Link\n+from scrapy.utils.python import unique as unique_list\n+from scrapy.utils.url import safe_url_string, urljoin_rfc as urljoin\n+\n+class LxmlLinkExtractor(object):\n+    def __init__(self, tag=\"a\", attr=\"href\", process=None, unique=False):\n+        scan_tag = tag if callable(tag) else lambda t: t == tag\n+        scan_attr = attr if callable(attr) else lambda a: a == attr\n+        process_attr = process if callable(process) else lambda v: v\n+\n+        self.unique = unique\n+\n+        target = LinkTarget(scan_tag, scan_attr, process_attr)\n+        self.parser = etree.HTMLParser(target=target)\n+\n+    def _extract_links(self, response_text, response_url, response_encoding):\n+        self.base_url, self.links = etree.HTML(response_text, self.parser) \n+\n+        links = unique_list(self.links, key=lambda link: link.url) if self.unique else self.links\n+\n+        ret = []\n+        base_url = self.base_url if self.base_url else response_url\n+        for link in links:\n+            link.url = urljoin(base_url, link.url)\n+            link.url = safe_url_string(link.url, response_encoding)\n+            link.text = link.text.decode(response_encoding)\n+            ret.append(link)\n+\n+        return ret\n+\n+    def extract_links(self, response):\n+        # wrapper needed to allow to work directly with text\n+        return self._extract_links(response.body, response.url, \n+                                   response.encoding)\n+\n+\n+class LinkTarget(object):\n+    def __init__(self, scan_tag, scan_attr, process_attr):\n+        self.scan_tag = scan_tag\n+        self.scan_attr = scan_attr\n+        self.process_attr = process_attr\n+\n+        self.base_url = None\n+        self.links = []\n+\n+        self.current_link = None\n+\n+    def start(self, tag, attrs):\n+        if tag == 'base':\n+            self.base_url = dict(attrs).get('href')\n+        if self.scan_tag(tag):\n+            for attr, value in attrs.iteritems():\n+                if self.scan_attr(attr):\n+                    url = self.process_attr(value)\n+                    link = Link(url=url)\n+                    self.links.append(link)\n+                    self.current_link = link\n+\n+    def end(self, tag):\n+        self.current_link = None\n+\n+    def data(self, data):\n+        if self.current_link and not self.current_link.text:\n+            self.current_link.text = data.strip()\n+\n+    def close(self):\n+        return self.base_url, self.links\n+\n+\n+class LxmlParserLinkExtractor(object):\n+    def __init__(self, tag=\"a\", attr=\"href\", process=None, unique=False):\n+        self.scan_tag = tag if callable(tag) else lambda t: t == tag\n+        self.scan_attr = attr if callable(attr) else lambda a: a == attr\n+        self.process_attr = process if callable(process) else lambda v: v\n+        self.unique = unique\n+\n+        self.links = []\n+\n+    def _extract_links(self, response_text, response_url):\n+        html = lxml.html.fromstring(response_text)\n+        html.make_links_absolute(response_url)\n+        for e, a, l, p in html.iterlinks():\n+            if self.scan_tag(e.tag):\n+                if self.scan_attr(a):\n+                    link = Link(self.process_attr(l), text=e.text)\n+                    self.links.append(link)\n+\n+        links = unique_list(self.links, key=lambda link: link.url) \\\n+                if self.unique else self.links\n+\n+        return links\n+\n+    def extract_links(self, response):\n+        # wrapper needed to allow to work directly with text\n+        return self._extract_links(response.body, response.url)\n+\n+\n\n@@ -1,193 +0,0 @@\n-from HTMLParser import HTMLParser\n-from lxml import etree\n-import lxml.html\n-\n-from scrapy.link import Link\n-from scrapy.utils.python import unique as unique_list\n-from scrapy.utils.url import safe_url_string, urljoin_rfc as urljoin\n-\n-class HtmlParserLinkExtractor(HTMLParser):\n-\n-    \"\"\"LinkExtractor are used to extract links from web pages. They are\n-    instantiated and later \"applied\" to a Response using the extract_links\n-    method which must receive a Response object and return a list of Link objects\n-    containing the (absolute) urls to follow, and the links texts.\n-\n-    This is the base LinkExtractor class that provides enough basic\n-    functionality for extracting links to follow, but you could override this\n-    class or create a new one if you need some additional functionality. The\n-    only requisite is that the new (or overrided) class must provide a\n-    extract_links method that receives a Response and returns a list of Link objects.\n-\n-    This LinkExtractor always returns percent-encoded URLs, using the detected encoding\n-    from the response.\n-\n-    The constructor arguments are:\n-\n-    * tag (string or function)\n-      * a tag name which is used to search for links (defaults to \"a\")\n-      * a function which receives a tag name and returns whether to scan it\n-    * attr (string or function)\n-      * an attrsute name which is used to search for links (defaults to \"href\")\n-      * a function which receives an attrsute name and returns whether to scan it\n-    * process (funtion)\n-      * a function wich receives the attrsute value before assigning it\n-    * unique - if True the same urls won't be extracted twice, otherwise the\n-      same urls will be extracted multiple times (with potentially different link texts)\n-    \"\"\"\n-\n-    def __init__(self, tag=\"a\", attr=\"href\", process=None, unique=False):\n-        HTMLParser.__init__(self)\n-\n-        self.scan_tag = tag if callable(tag) else lambda t: t == tag\n-        self.scan_attr = attr if callable(attr) else lambda a: a == attr\n-        self.process_attr = process if callable(process) else lambda v: v\n-        self.unique = unique\n-\n-    def _extract_links(self, response_text, response_url, response_encoding):\n-        self.reset()\n-        self.feed(response_text)\n-        self.close()\n-\n-        links = unique_list(self.links, key=lambda link: link.url) if self.unique else self.links\n-\n-        ret = []\n-        base_url = self.base_url if self.base_url else response_url\n-        for link in links:\n-            link.url = urljoin(base_url, link.url)\n-            link.url = safe_url_string(link.url, response_encoding)\n-            link.text = link.text.decode(response_encoding)\n-            ret.append(link)\n-\n-        return ret\n-\n-    def extract_links(self, response):\n-        # wrapper needed to allow to work directly with text\n-        return self._extract_links(response.body, response.url, \n-                                   response.encoding)\n-\n-    def reset(self):\n-        HTMLParser.reset(self)\n-\n-        self.base_url = None\n-        self.current_link = None\n-        self.links = []\n-\n-    def handle_starttag(self, tag, attrs):\n-        if tag == 'base':\n-            self.base_url = dict(attrs).get('href')\n-        if self.scan_tag(tag):\n-            for attr, value in attrs:\n-                if self.scan_attr(attr):\n-                    url = self.process_attr(value)\n-                    link = Link(url=url)\n-                    self.links.append(link)\n-                    self.current_link = link\n-\n-    def handle_endtag(self, tag):\n-        self.current_link = None\n-\n-    def handle_data(self, data):\n-        if self.current_link and not self.current_link.text:\n-            self.current_link.text = data.strip()\n-\n-    def matches(self, url):\n-        \"\"\"This extractor matches with any url, since\n-        it doesn't contain any patterns\"\"\"\n-        return True\n-\n-\n-class LxmlLinkExtractor(object):\n-    def __init__(self, tag=\"a\", attr=\"href\", process=None, unique=False):\n-        scan_tag = tag if callable(tag) else lambda t: t == tag\n-        scan_attr = attr if callable(attr) else lambda a: a == attr\n-        process_attr = process if callable(process) else lambda v: v\n-\n-        self.unique = unique\n-\n-        target = LinkTarget(scan_tag, scan_attr, process_attr)\n-        self.parser = etree.HTMLParser(target=target)\n-\n-\n-    def _extract_links(self, response_text, response_url, response_encoding):\n-        self.base_url, self.links = etree.HTML(response_text, self.parser) \n-\n-        links = unique_list(self.links, key=lambda link: link.url) if self.unique else self.links\n-\n-        ret = []\n-        base_url = self.base_url if self.base_url else response_url\n-        for link in links:\n-            link.url = urljoin(base_url, link.url)\n-            link.url = safe_url_string(link.url, response_encoding)\n-            link.text = link.text.decode(response_encoding)\n-            ret.append(link)\n-\n-        return ret\n-\n-    def extract_links(self, response):\n-        # wrapper needed to allow to work directly with text\n-        return self._extract_links(response.body, response.url, \n-                                   response.encoding)\n-\n-\n-class LinkTarget(object):\n-    def __init__(self, scan_tag, scan_attr, process_attr):\n-        self.scan_tag = scan_tag\n-        self.scan_attr = scan_attr\n-        self.process_attr = process_attr\n-\n-        self.base_url = None\n-        self.links = []\n-\n-        self.current_link = None\n-\n-    def start(self, tag, attrs):\n-        if tag == 'base':\n-            self.base_url = dict(attrs).get('href')\n-        if self.scan_tag(tag):\n-            for attr, value in attrs.iteritems():\n-                if self.scan_attr(attr):\n-                    url = self.process_attr(value)\n-                    link = Link(url=url)\n-                    self.links.append(link)\n-                    self.current_link = link\n-\n-    def end(self, tag):\n-        self.current_link = None\n-\n-    def data(self, data):\n-        if self.current_link and not self.current_link.text:\n-            self.current_link.text = data.strip()\n-\n-    def close(self):\n-        return self.base_url, self.links\n-\n-\n-class LxmlParserLinkExtractor(object):\n-    def __init__(self, tag=\"a\", attr=\"href\", process=None, unique=False):\n-        self.scan_tag = tag if callable(tag) else lambda t: t == tag\n-        self.scan_attr = attr if callable(attr) else lambda a: a == attr\n-        self.process_attr = process if callable(process) else lambda v: v\n-        self.unique = unique\n-\n-        self.links = []\n-\n-    def _extract_links(self, response_text, response_url):\n-        html = lxml.html.fromstring(response_text)\n-        html.make_links_absolute(response_url)\n-        for e, a, l, p in html.iterlinks():\n-            if self.scan_tag(e.tag):\n-                if self.scan_attr(a):\n-                    link = Link(self.process_attr(l), text=e.text)\n-                    self.links.append(link)\n-\n-        links = unique_list(self.links, key=lambda link: link.url) \\\n-                if self.unique else self.links\n-\n-        return links\n-\n-    def extract_links(self, response):\n-        # wrapper needed to allow to work directly with text\n-        return self._extract_links(response.body, response.url)\n-\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
