{"custom_id": "scrapy#0e0d1ad64323033017c4a01893b5337a980cb5ec", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 6 | Churn Cumulative: 229 | Contributors (this commit): 13 | Commits (past 90d): 2 | Contributors (cumulative): 17 | DMM Complexity: 0.0\n\nDIFF:\n@@ -186,9 +186,6 @@ class WrappedResponse:\n     def info(self):\n         return self\n \n-    # python3 cookiejars calls get_all\n     def get_all(self, name, default=None):\n         return [to_unicode(v, errors='replace')\n                 for v in self.response.headers.getlist(name)]\n-    # python2 cookiejars calls getheaders\n-    getheaders = get_all\n\n@@ -64,9 +64,6 @@ class WrappedResponseTest(TestCase):\n     def test_info(self):\n         self.assertIs(self.wrapped.info(), self.wrapped)\n \n-    def test_getheaders(self):\n-        self.assertEqual(self.wrapped.getheaders('content-type'), ['text/html'])\n-\n     def test_get_all(self):\n         # get_all result must be native string\n         self.assertEqual(self.wrapped.get_all('content-type'), ['text/html'])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b97a39fda0b72d8a4dcd631599e5d9bf530a5bee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 815 | Contributors (this commit): 23 | Commits (past 90d): 3 | Contributors (cumulative): 23 | DMM Complexity: None\n\nDIFF:\n@@ -285,6 +285,7 @@ class WeakKeyCache:\n         return self._weakdict[key]\n \n \n+@deprecated\n def retry_on_eintr(function, *args, **kw):\n     \"\"\"Run a function and retry it while getting EINTR errors\"\"\"\n     while True:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d29bec60d795b13ecb6e5978cb9e4d8fbd298b08", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 19 | Files Changed: 4 | Hunks: 11 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 58 | Churn Cumulative: 2231 | Contributors (this commit): 31 | Commits (past 90d): 20 | Contributors (cumulative): 54 | DMM Complexity: 1.0\n\nDIFF:\n@@ -18,12 +18,39 @@ def has_environment_marker_platform_impl_support():\n     return parse_version(setuptools_version) >= parse_version('18.5')\n \n \n+install_requires = [\n+    'Twisted>=17.9.0',\n+    'cryptography>=2.0',\n+    'cssselect>=0.9.1',\n+    'itemloaders>=1.0.1',\n+    'lxml>=3.5.0',\n+    'parsel>=1.5.0',\n+    'PyDispatcher>=2.0.5',\n+    'pyOpenSSL>=16.2.0',\n+    'queuelib>=1.4.2',\n+    'service_identity>=16.0.0',\n+    'w3lib>=1.17.0',\n+    'zope.interface>=4.1.3',\n+    'protego>=0.1.15',\n+    'itemadapter>=0.1.0',\n+]\n extras_require = {}\n \n if has_environment_marker_platform_impl_support():\n+    extras_require[':platform_python_implementation == \"CPython\"'] = [\n+        'lxml>=3.5.0',\n+    ]\n     extras_require[':platform_python_implementation == \"PyPy\"'] = [\n+        # Earlier lxml versions are affected by\n+        # https://bitbucket.org/pypy/pypy/issues/2498/cython-on-pypy-3-dict-object-has-no,\n+        # which was fixed in Cython 0.26, released on 2017-06-19, and used to\n+        # generate the C headers of lxml release tarballs published since then, the\n+        # first of which was:\n+        'lxml>=4.0.0',\n         'PyPyDispatcher>=2.1.0',\n     ]\n+else:\n+    install_requires.append('lxml>=3.5.0')\n \n \n setup(\n@@ -67,21 +94,6 @@ setup(\n         'Topic :: Software Development :: Libraries :: Python Modules',\n     ],\n     python_requires='>=3.5.2',\n-    install_requires=[\n-        'Twisted>=17.9.0',\n-        'cryptography>=2.0',\n-        'cssselect>=0.9.1',\n-        'itemloaders>=1.0.1',\n-        'lxml>=3.5.0',\n-        'parsel>=1.5.0',\n-        'PyDispatcher>=2.0.5',\n-        'pyOpenSSL>=16.2.0',\n-        'queuelib>=1.4.2',\n-        'service_identity>=16.0.0',\n-        'w3lib>=1.17.0',\n-        'zope.interface>=4.1.3',\n-        'protego>=0.1.15',\n-        'itemadapter>=0.1.0',\n-    ],\n+    install_requires=install_requires,\n     extras_require=extras_require,\n )\n\n@@ -60,6 +60,8 @@ def _wrong_credentials(proxy_url):\n \n @skipIf(sys.version_info < (3, 5, 4),\n         \"requires mitmproxy < 3.0.0, which these tests do not support\")\n+@skipIf(\"pypy\" in sys.executable,\n+        \"mitmproxy does not support PyPy\")\n @skipIf(platform.system() == 'Windows' and sys.version_info < (3, 7),\n         \"mitmproxy does not support Windows when running Python < 3.7\")\n class ProxyConnectTestCase(TestCase):\n\n@@ -4,6 +4,7 @@ import operator\n import platform\n import unittest\n from itertools import count\n+from sys import version_info\n from warnings import catch_warnings\n \n from scrapy.utils.python import (\n@@ -214,9 +215,12 @@ class UtilsPythonTestCase(unittest.TestCase):\n         else:\n             self.assertEqual(\n                 get_func_args(str.split, stripself=True), ['sep', 'maxsplit'])\n-            self.assertEqual(get_func_args(\" \".join, stripself=True), ['list'])\n             self.assertEqual(\n                 get_func_args(operator.itemgetter(2), stripself=True), ['obj'])\n+            if version_info < (3, 6):\n+                self.assertEqual(get_func_args(\" \".join, stripself=True), ['list'])\n+            else:\n+                self.assertEqual(get_func_args(\" \".join, stripself=True), ['iterable'])\n \n     def test_without_none_values(self):\n         self.assertEqual(without_none_values([1, None, 3, 4]), [1, 3, 4])\n\n@@ -413,7 +413,9 @@ class WebClientCustomCiphersSSLTestCase(WebClientSSLTestCase):\n             self.getURL(\"payload\"), body=s, contextFactory=client_context_factory\n         ).addCallback(self.assertEqual, to_bytes(s))\n \n-    def testPayloadDefaultCiphers(self):\n+    def testPayloadDisabledCipher(self):\n         s = \"0123456789\" * 10\n-        d = getPage(self.getURL(\"payload\"), body=s, contextFactory=ScrapyClientContextFactory())\n+        settings = Settings({'DOWNLOADER_CLIENT_TLS_CIPHERS': 'ECDHE-RSA-AES256-GCM-SHA384'})\n+        client_context_factory = create_instance(ScrapyClientContextFactory, settings=settings, crawler=None)\n+        d = getPage(self.getURL(\"payload\"), body=s, contextFactory=client_context_factory)\n         return self.assertFailure(d, OpenSSL.SSL.Error)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#62a4ede5e995f83abd5a90f7dd6ac242f2f3870d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 192 | Files Changed: 2 | Hunks: 2 | Methods Changed: 14 | Complexity Δ (Sum/Max): -40/0 | Churn Δ: 192 | Churn Cumulative: 306 | Contributors (this commit): 12 | Commits (past 90d): 2 | Contributors (cumulative): 18 | DMM Complexity: 0.29245283018867924\n\nDIFF:\n@@ -1,41 +0,0 @@\n-import re\n-from urllib.parse import urljoin\n-\n-from w3lib.html import remove_tags, replace_entities, replace_escape_chars, get_base_url\n-\n-from scrapy.link import Link\n-from scrapy.linkextractors.sgml import SgmlLinkExtractor\n-\n-\n-linkre = re.compile(\n-        \"<a\\s.*?href=(\\\"[.#]+?\\\"|\\'[.#]+?\\'|[^\\s]+?)(>|\\s.*?>)(.*?)<[/ ]?a>\",\n-        re.DOTALL | re.IGNORECASE)\n-\n-\n-def clean_link(link_text):\n-    \"\"\"Remove leading and trailing whitespace and punctuation\"\"\"\n-    return link_text.strip(\"\\t\\r\\n '\\\"\\x0c\")\n-\n-\n-class RegexLinkExtractor(SgmlLinkExtractor):\n-    \"\"\"High performant link extractor\"\"\"\n-\n-    def _extract_links(self, response_text, response_url, response_encoding, base_url=None):\n-        def clean_text(text):\n-            return replace_escape_chars(remove_tags(text.decode(response_encoding))).strip()\n-\n-        def clean_url(url):\n-            clean_url = ''\n-            try:\n-                clean_url = urljoin(base_url, replace_entities(clean_link(url.decode(response_encoding))))\n-            except ValueError:\n-                pass\n-            return clean_url\n-\n-        if base_url is None:\n-            base_url = get_base_url(response_text, response_url, response_encoding)\n-\n-        links_text = linkre.findall(response_text)\n-        return [Link(clean_url(url).encode(response_encoding),\n-                     clean_text(text))\n-                for url, _, text in links_text]\n\n@@ -1,151 +0,0 @@\n-\"\"\"\n-SGMLParser-based Link extractors\n-\"\"\"\n-import warnings\n-from urllib.parse import urljoin\n-from sgmllib import SGMLParser\n-\n-from w3lib.url import safe_url_string, canonicalize_url\n-from w3lib.html import strip_html5_whitespace\n-\n-from scrapy.link import Link\n-from scrapy.linkextractors import FilteringLinkExtractor\n-from scrapy.utils.misc import arg_to_iter, rel_has_nofollow\n-from scrapy.utils.python import unique as unique_list, to_unicode\n-from scrapy.utils.response import get_base_url\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-\n-\n-class BaseSgmlLinkExtractor(SGMLParser):\n-\n-    def __init__(self, tag=\"a\", attr=\"href\", unique=False, process_value=None,\n-                 strip=True, canonicalized=False):\n-        warnings.warn(\n-            \"BaseSgmlLinkExtractor is deprecated and will be removed in future releases. \"\n-            \"Please use scrapy.linkextractors.LinkExtractor\",\n-            ScrapyDeprecationWarning, stacklevel=2,\n-        )\n-        SGMLParser.__init__(self)\n-        self.scan_tag = tag if callable(tag) else lambda t: t == tag\n-        self.scan_attr = attr if callable(attr) else lambda a: a == attr\n-        self.process_value = (lambda v: v) if process_value is None else process_value\n-        self.current_link = None\n-        self.unique = unique\n-        self.strip = strip\n-        if canonicalized:\n-            self.link_key = lambda link: link.url\n-        else:\n-            self.link_key = lambda link: canonicalize_url(link.url,\n-                                                          keep_fragments=True)\n-\n-    def _extract_links(self, response_text, response_url, response_encoding, base_url=None):\n-        \"\"\" Do the real extraction work \"\"\"\n-        self.reset()\n-        self.feed(response_text)\n-        self.close()\n-\n-        ret = []\n-        if base_url is None:\n-            base_url = urljoin(response_url, self.base_url) if self.base_url else response_url\n-        for link in self.links:\n-            if isinstance(link.url, str):\n-                link.url = link.url.encode(response_encoding)\n-            try:\n-                link.url = urljoin(base_url, link.url)\n-            except ValueError:\n-                continue\n-            link.url = safe_url_string(link.url, response_encoding)\n-            link.text = to_unicode(link.text, response_encoding, errors='replace').strip()\n-            ret.append(link)\n-\n-        return ret\n-\n-    def _process_links(self, links):\n-        \"\"\" Normalize and filter extracted links\n-\n-        The subclass should override it if necessary\n-        \"\"\"\n-        return unique_list(links, key=self.link_key) if self.unique else links\n-\n-    def extract_links(self, response):\n-        # wrapper needed to allow to work directly with text\n-        links = self._extract_links(response.body, response.url, response.encoding)\n-        links = self._process_links(links)\n-        return links\n-\n-    def reset(self):\n-        SGMLParser.reset(self)\n-        self.links = []\n-        self.base_url = None\n-        self.current_link = None\n-\n-    def unknown_starttag(self, tag, attrs):\n-        if tag == 'base':\n-            self.base_url = dict(attrs).get('href')\n-        if self.scan_tag(tag):\n-            for attr, value in attrs:\n-                if self.scan_attr(attr):\n-                    if self.strip and value is not None:\n-                        value = strip_html5_whitespace(value)\n-                    url = self.process_value(value)\n-                    if url is not None:\n-                        link = Link(url=url, nofollow=rel_has_nofollow(dict(attrs).get('rel')))\n-                        self.links.append(link)\n-                        self.current_link = link\n-\n-    def unknown_endtag(self, tag):\n-        if self.scan_tag(tag):\n-            self.current_link = None\n-\n-    def handle_data(self, data):\n-        if self.current_link:\n-            self.current_link.text = self.current_link.text + data\n-\n-    def matches(self, url):\n-        \"\"\"This extractor matches with any url, since\n-        it doesn't contain any patterns\"\"\"\n-        return True\n-\n-\n-class SgmlLinkExtractor(FilteringLinkExtractor):\n-\n-    def __init__(self, allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(),\n-                 tags=('a', 'area'), attrs=('href',), canonicalize=False, unique=True,\n-                 process_value=None, deny_extensions=None, restrict_css=(),\n-                 strip=True, restrict_text=()):\n-        warnings.warn(\n-            \"SgmlLinkExtractor is deprecated and will be removed in future releases. \"\n-            \"Please use scrapy.linkextractors.LinkExtractor\",\n-            ScrapyDeprecationWarning, stacklevel=2,\n-        )\n-\n-        tags, attrs = set(arg_to_iter(tags)), set(arg_to_iter(attrs))\n-        tag_func = lambda x: x in tags\n-        attr_func = lambda x: x in attrs\n-\n-        with warnings.catch_warnings():\n-            warnings.simplefilter('ignore', ScrapyDeprecationWarning)\n-            lx = BaseSgmlLinkExtractor(tag=tag_func, attr=attr_func,\n-                                       unique=unique, process_value=process_value, strip=strip,\n-                                       canonicalized=canonicalize)\n-\n-        super(SgmlLinkExtractor, self).__init__(lx, allow=allow, deny=deny,\n-                                                allow_domains=allow_domains, deny_domains=deny_domains,\n-                                                restrict_xpaths=restrict_xpaths, restrict_css=restrict_css,\n-                                                canonicalize=canonicalize, deny_extensions=deny_extensions,\n-                                                restrict_text=restrict_text)\n-\n-    def extract_links(self, response):\n-        base_url = None\n-        if self.restrict_xpaths:\n-            base_url = get_base_url(response)\n-            body = u''.join(f\n-                            for x in self.restrict_xpaths\n-                            for f in response.xpath(x).getall()\n-                            ).encode(response.encoding, errors='xmlcharrefreplace')\n-        else:\n-            body = response.body\n-\n-        links = self._extract_links(body, response.url, response.encoding, base_url)\n-        links = self._process_links(links)\n-        return links\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#86f7ac2f2b5d58e0b2588fa2aa4c777a8decf299", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 2 | Churn Cumulative: 2518 | Contributors (this commit): 22 | Commits (past 90d): 17 | Contributors (cumulative): 22 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1204,6 +1204,8 @@ class BatchDeliveriesTest(FeedExportTestBase):\n \n             for path, feed in FEEDS.items():\n                 dir_name = os.path.dirname(path)\n+                if not os.path.exists(str(dir_name)):\n+                    continue\n                 for file in sorted(os.listdir(dir_name)):\n                     with open(os.path.join(dir_name, file), 'rb') as f:\n                         data = f.read()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3e0492741d93b05c464457b3b128a2b0d24c994b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 2527 | Contributors (this commit): 22 | Commits (past 90d): 18 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1190,9 +1190,14 @@ class BatchDeliveriesTest(FeedExportTestBase):\n     def run_and_export(self, spider_cls, settings):\n         \"\"\" Run spider with specified settings; return exported data. \"\"\"\n \n+        def build_url(path):\n+            if path[0] != '/':\n+                path = '/' + path\n+            return urljoin('file:', path)\n+\n         FEEDS = settings.get('FEEDS') or {}\n         settings['FEEDS'] = {\n-            urljoin('file:', file_path): feed\n+            build_url(file_path): feed\n             for file_path, feed in FEEDS.items()\n         }\n         content = defaultdict(list)\n@@ -1204,8 +1209,6 @@ class BatchDeliveriesTest(FeedExportTestBase):\n \n             for path, feed in FEEDS.items():\n                 dir_name = os.path.dirname(path)\n-                if not os.path.exists(str(dir_name)):\n-                    continue\n                 for file in sorted(os.listdir(dir_name)):\n                     with open(os.path.join(dir_name, file), 'rb') as f:\n                         data = f.read()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#de297a3a167a6be3ac1ed94a891effefc12a6d00", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 103 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 7 | Methods Changed: 12 | Complexity Δ (Sum/Max): 15/10 | Churn Δ: 106 | Churn Cumulative: 169 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,20 +2,42 @@\n pprint and pformat wrappers with colorization support\n \"\"\"\n \n+import ctypes\n+import platform\n import sys\n+from distutils.version import LooseVersion as parse_version\n from pprint import pformat as pformat_\n \n \n+def _enable_windows_terminal_processing():\n+    # https://stackoverflow.com/a/36760881\n+    kernel32 = ctypes.windll.kernel32\n+    return bool(kernel32.SetConsoleMode(kernel32.GetStdHandle(-11), 7))\n+\n+\n+def _tty_supports_color():\n+    if sys.platform != \"win32\":\n+        return True\n+\n+    if parse_version(platform.version()) < parse_version(\"10.0.14393\"):\n+        return True\n+\n+    # Windows >= 10.0.14393 interprets ANSI escape sequences providing terminal\n+    # processing is enabled.\n+    return _enable_windows_terminal_processing()\n+\n+\n def _colorize(text, colorize=True):\n-    if not colorize or not sys.stdout.isatty():\n+    if not colorize or not sys.stdout.isatty() or not _tty_supports_color():\n         return text\n     try:\n         from pygments import highlight\n+    except ImportError:\n+        return text\n+    else:\n         from pygments.formatters import TerminalFormatter\n         from pygments.lexers import PythonLexer\n         return highlight(text, PythonLexer(), TerminalFormatter())\n-    except ImportError:\n-        return text\n \n \n def pformat(obj, *args, **kwargs):\n\n@@ -0,0 +1,78 @@\n+from io import StringIO\n+\n+from unittest import mock, TestCase\n+\n+from scrapy.utils.display import pformat, pprint\n+\n+\n+class TestDisplay(TestCase):\n+    object = {'a': 1}\n+    colorized_string = (\n+        \"{\\x1b[33m'\\x1b[39;49;00m\\x1b[33ma\\x1b[39;49;00m\\x1b[33m'\"\n+        \"\\x1b[39;49;00m: \\x1b[34m1\\x1b[39;49;00m}\\n\"\n+    )\n+    plain_string = \"{'a': 1}\"\n+\n+    @mock.patch('sys.platform', 'linux')\n+    @mock.patch(\"sys.stdout.isatty\")\n+    def test_pformat(self, isatty):\n+        isatty.return_value = True\n+        self.assertEqual(pformat(self.object), self.colorized_string)\n+\n+    @mock.patch(\"sys.stdout.isatty\")\n+    def test_pformat_dont_colorize(self, isatty):\n+        isatty.return_value = True\n+        self.assertEqual(pformat(self.object, colorize=False), self.plain_string)\n+\n+    def test_pformat_not_tty(self):\n+        self.assertEqual(pformat(self.object), self.plain_string)\n+\n+    @mock.patch('sys.platform', 'win32')\n+    @mock.patch('platform.version')\n+    @mock.patch(\"sys.stdout.isatty\")\n+    def test_pformat_old_windows(self, isatty, version):\n+        isatty.return_value = True\n+        version.return_value = '10.0.14392'\n+        self.assertEqual(pformat(self.object), self.colorized_string)\n+\n+    @mock.patch('sys.platform', 'win32')\n+    @mock.patch('scrapy.utils.display._enable_windows_terminal_processing')\n+    @mock.patch('platform.version')\n+    @mock.patch(\"sys.stdout.isatty\")\n+    def test_pformat_windows_no_terminal_processing(self, isatty, version, terminal_processing):\n+        isatty.return_value = True\n+        version.return_value = '10.0.14393'\n+        terminal_processing.return_value = False\n+        self.assertEqual(pformat(self.object), self.plain_string)\n+\n+    @mock.patch('sys.platform', 'win32')\n+    @mock.patch('scrapy.utils.display._enable_windows_terminal_processing')\n+    @mock.patch('platform.version')\n+    @mock.patch(\"sys.stdout.isatty\")\n+    def test_pformat_windows(self, isatty, version, terminal_processing):\n+        isatty.return_value = True\n+        version.return_value = '10.0.14393'\n+        terminal_processing.return_value = True\n+        self.assertEqual(pformat(self.object), self.colorized_string)\n+\n+    @mock.patch('sys.platform', 'linux')\n+    @mock.patch(\"sys.stdout.isatty\")\n+    def test_pformat_no_pygments(self, isatty):\n+        isatty.return_value = True\n+\n+        import builtins\n+        real_import = builtins.__import__\n+\n+        def mock_import(name, globals, locals, fromlist, level):\n+            if 'pygments' in name:\n+                raise ImportError\n+            return real_import(name, globals, locals, fromlist, level)\n+\n+        builtins.__import__ = mock_import\n+        self.assertEqual(pformat(self.object), self.plain_string)\n+        builtins.__import__ = real_import\n+\n+    def test_pprint(self):\n+        with mock.patch('sys.stdout', new=StringIO()) as mock_out:\n+            pprint(self.object)\n+            self.assertEqual(mock_out.getvalue(), \"{'a': 1}\\n\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ece4fa6c7cf7fdce9e5e77f56b209941ad401f53", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -70/0 | Churn Δ: 4 | Churn Cumulative: 2531 | Contributors (this commit): 22 | Commits (past 90d): 28 | Contributors (cumulative): 22 | DMM Complexity: None\n\nDIFF:\n@@ -182,9 +182,9 @@ class S3FeedStorageTest(unittest.TestCase):\n                 create=True)\n     def test_parse_credentials(self):\n         try:\n-            import boto  # noqa: F401\n+            import botocore  # noqa: F401\n         except ImportError:\n-            raise unittest.SkipTest(\"S3FeedStorage requires boto\")\n+            raise unittest.SkipTest(\"S3FeedStorage requires botocore\")\n         aws_credentials = {'AWS_ACCESS_KEY_ID': 'settings_key',\n                            'AWS_SECRET_ACCESS_KEY': 'settings_secret'}\n         crawler = get_crawler(settings_dict=aws_credentials)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#234c8b8c50a2db0d6d71455c382957099f4c0dcc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 17 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -8/0 | Churn Δ: 21 | Churn Cumulative: 894 | Contributors (this commit): 23 | Commits (past 90d): 27 | Contributors (cumulative): 23 | DMM Complexity: 1.0\n\nDIFF:\n@@ -94,23 +94,10 @@ class FileFeedStorage:\n class S3FeedStorage(BlockingFeedStorage):\n \n     def __init__(self, uri, access_key=None, secret_key=None, acl=None):\n-        # BEGIN Backward compatibility for initialising without keys (and\n-        # without using from_crawler)\n-        no_defaults = access_key is None and secret_key is None\n-        if no_defaults:\n-            from scrapy.utils.project import get_project_settings\n-            settings = get_project_settings()\n-            if 'AWS_ACCESS_KEY_ID' in settings or 'AWS_SECRET_ACCESS_KEY' in settings:\n-                warnings.warn(\n-                    \"Initialising `scrapy.extensions.feedexport.S3FeedStorage` \"\n-                    \"without AWS keys is deprecated. Please supply credentials or \"\n-                    \"use the `from_crawler()` constructor.\",\n-                    category=ScrapyDeprecationWarning,\n-                    stacklevel=2\n-                )\n-                access_key = settings['AWS_ACCESS_KEY_ID']\n-                secret_key = settings['AWS_SECRET_ACCESS_KEY']\n-        # END Backward compatibility\n+        no_keys = access_key is None and secret_key is None\n+        if no_keys:\n+            raise NotConfigured('%s is missing AWS credentials' %\n+                                self.__class__.__name__)\n         u = urlparse(uri)\n         self.bucketname = u.hostname\n         self.access_key = u.username or access_key\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#98e8086d1b3da8906d1395f453b1299b4e3b499e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 2545 | Contributors (this commit): 22 | Commits (past 90d): 29 | Contributors (cumulative): 22 | DMM Complexity: 0.0\n\nDIFF:\n@@ -24,6 +24,7 @@ from zope.interface.verify import verifyObject\n \n import scrapy\n from scrapy.crawler import CrawlerRunner\n+from scrapy.exceptions import NotConfigured\n from scrapy.exporters import CsvItemExporter\n from scrapy.extensions.feedexport import (\n     BlockingFeedStorage,\n@@ -176,10 +177,6 @@ class BlockingFeedStorageTest(unittest.TestCase):\n \n class S3FeedStorageTest(unittest.TestCase):\n \n-    @mock.patch('scrapy.utils.project.get_project_settings',\n-                new=mock.MagicMock(return_value={'AWS_ACCESS_KEY_ID': 'conf_key',\n-                                                 'AWS_SECRET_ACCESS_KEY': 'conf_secret'}),\n-                create=True)\n     def test_parse_credentials(self):\n         try:\n             import botocore  # noqa: F401\n@@ -205,12 +202,9 @@ class S3FeedStorageTest(unittest.TestCase):\n                                 aws_credentials['AWS_SECRET_ACCESS_KEY'])\n         self.assertEqual(storage.access_key, 'uri_key')\n         self.assertEqual(storage.secret_key, 'uri_secret')\n-        # Backward compatibility for initialising without settings\n-        with warnings.catch_warnings(record=True) as w:\n-            storage = S3FeedStorage('s3://mybucket/export.csv')\n-            self.assertEqual(storage.access_key, 'conf_key')\n-            self.assertEqual(storage.secret_key, 'conf_secret')\n-            self.assertTrue('without AWS keys' in str(w[-1].message))\n+        # Instantiate without credentials\n+        with self.assertRaises(NotConfigured):\n+            S3FeedStorage('s3://mybucket/export.csv')\n \n     @defer.inlineCallbacks\n     def test_store(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2829cd4268d22a328acd64a6816c479b1fe21f39", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 8 | Churn Cumulative: 3447 | Contributors (this commit): 27 | Commits (past 90d): 58 | Contributors (cumulative): 45 | DMM Complexity: 1.0\n\nDIFF:\n@@ -94,10 +94,6 @@ class FileFeedStorage:\n class S3FeedStorage(BlockingFeedStorage):\n \n     def __init__(self, uri, access_key=None, secret_key=None, acl=None):\n-        no_keys = access_key is None and secret_key is None\n-        if no_keys:\n-            raise NotConfigured('%s is missing AWS credentials' %\n-                                self.__class__.__name__)\n         u = urlparse(uri)\n         self.bucketname = u.hostname\n         self.access_key = u.username or access_key\n\n@@ -24,7 +24,6 @@ from zope.interface.verify import verifyObject\n \n import scrapy\n from scrapy.crawler import CrawlerRunner\n-from scrapy.exceptions import NotConfigured\n from scrapy.exporters import CsvItemExporter\n from scrapy.extensions.feedexport import (\n     BlockingFeedStorage,\n@@ -202,9 +201,6 @@ class S3FeedStorageTest(unittest.TestCase):\n                                 aws_credentials['AWS_SECRET_ACCESS_KEY'])\n         self.assertEqual(storage.access_key, 'uri_key')\n         self.assertEqual(storage.secret_key, 'uri_secret')\n-        # Instantiate without credentials\n-        with self.assertRaises(NotConfigured):\n-            S3FeedStorage('s3://mybucket/export.csv')\n \n     @defer.inlineCallbacks\n     def test_store(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#430d22e46ebb67aba39005365e2e68d834d8fb86", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 2550 | Contributors (this commit): 23 | Commits (past 90d): 19 | Contributors (cumulative): 23 | DMM Complexity: None\n\nDIFF:\n@@ -5,7 +5,6 @@ import random\n import shutil\n import string\n import tempfile\n-import warnings\n from io import BytesIO\n from logging import getLogger\n from pathlib import Path\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#316620b517207b1082dd9c5b4ebfc7fbe745e3bb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 55 | Lines Deleted: 38 | Files Changed: 3 | Hunks: 34 | Methods Changed: 28 | Complexity Δ (Sum/Max): 1/2 | Churn Δ: 93 | Churn Cumulative: 3479 | Contributors (this commit): 2 | Commits (past 90d): 68 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -28,6 +28,7 @@ from scrapy.core.http2.stream import Stream, StreamCloseReason\n from scrapy.core.http2.types import H2ConnectionMetadataDict\n from scrapy.http import Request\n from scrapy.settings import Settings\n+from scrapy.spiders import Spider\n \n logger = logging.getLogger(__name__)\n \n@@ -71,7 +72,7 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n \n         # ID of the next request stream\n         # Following the convention - 'Streams initiated by a client MUST\n-        # use odd-numbered stream identifiers' (RFC 7540)\n+        # use odd-numbered stream identifiers' (RFC 7540 - Section 5.1.1)\n         self._stream_id_generator = itertools.count(start=1, step=2)\n \n         # Streams are stored in a dictionary keyed off their stream IDs\n@@ -136,6 +137,7 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n             self._active_streams += 1\n             stream = self._pending_request_stream_pool.popleft()\n             stream.initiate_request()\n+            self._write_to_transport()\n \n     def pop_stream(self, stream_id: int) -> Stream:\n         \"\"\"Perform cleanup when a stream is closed\n@@ -145,13 +147,15 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n         self._send_pending_requests()\n         return stream\n \n-    def _new_stream(self, request: Request) -> Stream:\n+    def _new_stream(self, request: Request, spider: Spider) -> Stream:\n         \"\"\"Instantiates a new Stream object\n         \"\"\"\n         stream = Stream(\n             stream_id=next(self._stream_id_generator),\n             request=request,\n             protocol=self,\n+            download_maxsize=getattr(spider, 'download_maxsize', self.metadata['default_download_maxsize']),\n+            download_warnsize=getattr(spider, 'download_warnsize', self.metadata['default_download_warnsize']),\n         )\n         self.streams[stream.stream_id] = stream\n         return stream\n@@ -166,11 +170,11 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n         data = self.conn.data_to_send()\n         self.transport.write(data)\n \n-    def request(self, request: Request) -> Deferred:\n+    def request(self, request: Request, spider: Spider) -> Deferred:\n         if not isinstance(request, Request):\n             raise TypeError(f'Expected scrapy.http.Request, received {request.__class__.__qualname__}')\n \n-        stream = self._new_stream(request)\n+        stream = self._new_stream(request, spider)\n         d = stream.get_response()\n \n         # Add the stream to the request pool\n\n@@ -83,7 +83,9 @@ class Stream:\n         self,\n         stream_id: int,\n         request: Request,\n-        protocol: \"H2ClientProtocol\"\n+        protocol: \"H2ClientProtocol\",\n+        download_maxsize: int = 0,\n+        download_warnsize: int = 0\n     ) -> None:\n         \"\"\"\n         Arguments:\n@@ -95,14 +97,8 @@ class Stream:\n         self._request: Request = request\n         self._protocol: \"H2ClientProtocol\" = protocol\n \n-        self._download_maxsize = self._request.meta.get(\n-            'download_maxsize',\n-            self._protocol.metadata['default_download_maxsize']\n-        )\n-        self._download_warnsize = self._request.meta.get(\n-            'download_warnsize',\n-            self._protocol.metadata['default_download_warnsize']\n-        )\n+        self._download_maxsize = self._request.meta.get('download_maxsize', download_maxsize)\n+        self._download_warnsize = self._request.meta.get('download_warnsize', download_warnsize)\n \n         self.request_start_time = None\n \n@@ -346,26 +342,28 @@ class Stream:\n \n         self.stream_closed_server = True\n \n-        flags = None\n-        if b'Content-Length' not in self._response['headers']:\n-            # Missing Content-Length - {twisted.web.http.PotentialDataLoss}\n-            flags = ['partial']\n+        # We do not check for Content-Length or Transfer-Encoding in response headers\n+        # and add `partial` flag as in HTTP/1.1 as 'A request or response that includes\n+        # a payload body can include a content-length header field' (RFC 7540 - Section 8.1.2.6)\n \n         # NOTE: Order of handling the events is important here\n         # As we immediately cancel the request when maxsize is exceeded while\n         # receiving DATA_FRAME's when we have received the headers (not\n         # having Content-Length)\n         if reason is StreamCloseReason.MAXSIZE_EXCEEDED:\n-            expected_size = int(self._response['headers'].get(b'Content-Length', -1))\n+            expected_size = int(self._response['headers'].get(\n+                b'Content-Length',\n+                self._response['flow_controlled_size'])\n+            )\n             error_msg = (\n-                f'Cancelling download of {self._request.url}: expected response '\n-                f'size ({expected_size}) larger than download max size ({self._download_maxsize}).'\n+                f'Cancelling download of {self._request.url}: received response '\n+                f'size ({expected_size}) larger than download max size ({self._download_maxsize})'\n             )\n             logger.error(error_msg)\n             self._deferred_response.errback(CancelledError(error_msg))\n \n         elif reason is StreamCloseReason.ENDED:\n-            self._fire_response_deferred(flags)\n+            self._fire_response_deferred()\n \n         # Stream was abruptly ended here\n         elif reason is StreamCloseReason.CANCELLED:\n@@ -398,7 +396,7 @@ class Stream:\n                 f'{self._protocol.metadata[\"ip_address\"]}:{self._protocol.metadata[\"uri\"].port}'\n             ))\n \n-    def _fire_response_deferred(self, flags: Optional[List[str]] = None) -> None:\n+    def _fire_response_deferred(self) -> None:\n         \"\"\"Builds response from the self._response dict\n         and fires the response deferred callback with the\n         generated response instance\"\"\"\n@@ -416,7 +414,6 @@ class Stream:\n             headers=self._response['headers'],\n             body=body,\n             request=self._request,\n-            flags=flags,\n             certificate=self._protocol.metadata['certificate'],\n             ip_address=self._protocol.metadata['ip_address'],\n         )\n\n@@ -23,6 +23,7 @@ from scrapy.core.http2.protocol import H2ClientFactory\n from scrapy.core.http2.stream import InactiveStreamClosed, InvalidHostname\n from scrapy.http import Request, Response, JsonRequest\n from scrapy.settings import Settings\n+from scrapy.spiders import Spider\n from tests.mockserver import ssl_context_factory, LeafResource, Status\n \n \n@@ -41,6 +42,14 @@ def make_html_body(val):\n     return bytes(response, 'utf-8')\n \n \n+class DummySpider(Spider):\n+    name = 'dummy'\n+    start_urls = []\n+\n+    def parse(self, response):\n+        print(response)\n+\n+\n class Data:\n     SMALL_SIZE = 1024  # 1 KB\n     LARGE_SIZE = 1024 ** 2  # 1 MB\n@@ -210,6 +219,9 @@ class Https2ClientProtocolTestCase(TestCase):\n         assert len(path) > 0 and (path[0] == '/' or path[0] == '&')\n         return f'{self.scheme}://{self.hostname}:{self.port_number}{path}'\n \n+    def make_request(self, request: Request) -> Deferred:\n+        return self.client.request(request, DummySpider())\n+\n     @staticmethod\n     def _check_repeat(get_deferred, count):\n         d_list = []\n@@ -233,7 +245,7 @@ class Https2ClientProtocolTestCase(TestCase):\n             content_length = int(response.headers.get('Content-Length'))\n             self.assertEqual(len(response.body), content_length)\n \n-        d = self.client.request(request)\n+        d = self.make_request(request)\n         d.addCallback(check_response)\n         d.addErrback(self.fail)\n         return d\n@@ -273,7 +285,7 @@ class Https2ClientProtocolTestCase(TestCase):\n         expected_extra_data,\n         expected_status: int\n     ):\n-        d = self.client.request(request)\n+        d = self.make_request(request)\n \n         def assert_response(response: Response):\n             self.assertEqual(response.status, expected_status)\n@@ -355,7 +367,7 @@ class Https2ClientProtocolTestCase(TestCase):\n             self.assertEqual(response.status, 499)\n             self.assertEqual(response.request, request)\n \n-        d = self.client.request(request)\n+        d = self.make_request(request)\n         d.addCallback(assert_response)\n         d.addErrback(self.fail)\n         d.cancel()\n@@ -367,8 +379,13 @@ class Https2ClientProtocolTestCase(TestCase):\n \n         def assert_cancelled_error(failure):\n             self.assertIsInstance(failure.value, CancelledError)\n+            error_pattern = re.compile(\n+                rf'Cancelling download of {request.url}: received response '\n+                rf'size \\(\\d*\\) larger than download max size \\(1000\\)'\n+            )\n+            self.assertEqual(len(re.findall(error_pattern, str(failure.value))), 1)\n \n-        d = self.client.request(request)\n+        d = self.make_request(request)\n         d.addCallback(self.fail)\n         d.addErrback(assert_cancelled_error)\n         return d\n@@ -385,7 +402,7 @@ class Https2ClientProtocolTestCase(TestCase):\n                 for error in failure.value.reasons\n             ))\n \n-        d = self.client.request(request)\n+        d = self.make_request(request)\n         d.addCallback(self.fail)\n         d.addErrback(assert_failure)\n         return d\n@@ -397,10 +414,9 @@ class Https2ClientProtocolTestCase(TestCase):\n             self.assertEqual(response.status, 200)\n             self.assertEqual(response.body, Data.NO_CONTENT_LENGTH)\n             self.assertEqual(response.request, request)\n-            self.assertIn('partial', response.flags)\n             self.assertNotIn('Content-Length', response.headers)\n \n-        d = self.client.request(request)\n+        d = self.make_request(request)\n         d.addCallback(assert_content_length)\n         d.addErrback(self.fail)\n         return d\n@@ -413,7 +429,7 @@ class Https2ClientProtocolTestCase(TestCase):\n         expected_body\n     ):\n         with self.assertLogs('scrapy.core.http2.stream', level='WARNING') as cm:\n-            response = yield self.client.request(request)\n+            response = yield self.make_request(request)\n             self.assertEqual(response.status, 200)\n             self.assertEqual(response.request, request)\n             self.assertEqual(response.body, expected_body)\n@@ -469,13 +485,13 @@ class Https2ClientProtocolTestCase(TestCase):\n \n         # Send 100 request (we do not check the result)\n         for _ in range(100):\n-            d = self.client.request(Request(self.get_url('/get-data-html-small')))\n+            d = self.make_request(Request(self.get_url('/get-data-html-small')))\n             d.addBoth(lambda _: None)\n             d_list.append(d)\n \n         # Now send 10 extra request and save the response deferred in a list\n         for _ in range(10):\n-            d = self.client.request(Request(self.get_url('/get-data-html-small')))\n+            d = self.make_request(Request(self.get_url('/get-data-html-small')))\n             d.addCallback(self.fail)\n             d.addErrback(assert_inactive_stream)\n             d_list.append(d)\n@@ -488,7 +504,7 @@ class Https2ClientProtocolTestCase(TestCase):\n \n     def test_invalid_request_type(self):\n         with self.assertRaises(TypeError):\n-            self.client.request('https://InvalidDataTypePassed.com')\n+            self.make_request('https://InvalidDataTypePassed.com')\n \n     def test_query_parameters(self):\n         params = {\n@@ -504,7 +520,7 @@ class Https2ClientProtocolTestCase(TestCase):\n             data = json.loads(str(response.body, content_encoding))\n             self.assertEqual(data, params)\n \n-        d = self.client.request(request)\n+        d = self.make_request(request)\n         d.addCallback(assert_query_params)\n         d.addErrback(self.fail)\n \n@@ -517,7 +533,7 @@ class Https2ClientProtocolTestCase(TestCase):\n         d_list = []\n         for status in [200, 404]:\n             request = Request(self.get_url(f'/status?n={status}'))\n-            d = self.client.request(request)\n+            d = self.make_request(request)\n             d.addCallback(assert_response_status, status)\n             d.addErrback(self.fail)\n             d_list.append(d)\n@@ -537,7 +553,7 @@ class Https2ClientProtocolTestCase(TestCase):\n             self.assertIsInstance(response.ip_address, IPv4Address)\n             self.assertEqual(str(response.ip_address), '127.0.0.1')\n \n-        d = self.client.request(request)\n+        d = self.make_request(request)\n         d.addCallback(assert_metadata)\n         d.addErrback(self.fail)\n \n@@ -553,7 +569,7 @@ class Https2ClientProtocolTestCase(TestCase):\n             self.assertIn('127.0.0.1', error_msg)\n             self.assertIn(str(request), error_msg)\n \n-        d = self.client.request(request)\n+        d = self.make_request(request)\n         d.addCallback(self.fail)\n         d.addErrback(assert_invalid_hostname)\n         return d\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3685e99cca47a5006258cd4246f255216797641f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 28 | Churn Cumulative: 980 | Contributors (this commit): 1 | Commits (past 90d): 17 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -18,8 +18,9 @@ from twisted.web.client import URI\n from twisted.web.http import Request as TxRequest\n from twisted.web.server import Site, NOT_DONE_YET\n from twisted.web.static import File\n+from twisted.internet.error import TimeoutError\n \n-from scrapy.core.http2.protocol import H2ClientFactory\n+from scrapy.core.http2.protocol import H2ClientFactory, H2ClientProtocol\n from scrapy.core.http2.stream import InactiveStreamClosed, InvalidHostname\n from scrapy.http import Request, Response, JsonRequest\n from scrapy.settings import Settings\n@@ -134,6 +135,11 @@ class NoContentLengthHeader(LeafResource):\n         request.finish()\n \n \n+class TimeoutResponse(LeafResource):\n+    def render_GET(self, request: TxRequest):\n+        return NOT_DONE_YET\n+\n+\n class QueryParams(LeafResource):\n     def render_GET(self, request: TxRequest):\n         request.setHeader('Content-Type', 'application/json; charset=UTF-8')\n@@ -172,6 +178,7 @@ class Https2ClientProtocolTestCase(TestCase):\n         r.putChild(b'no-content-length-header', NoContentLengthHeader())\n         r.putChild(b'status', Status())\n         r.putChild(b'query-params', QueryParams())\n+        r.putChild(b'timeout', TimeoutResponse())\n         return r\n \n     @inlineCallbacks\n@@ -590,3 +597,22 @@ class Https2ClientProtocolTestCase(TestCase):\n         ]\n \n         return DeferredList(d_list, fireOnOneErrback=True)\n+\n+    def test_connection_timeout(self):\n+        request = Request(self.get_url('/timeout'))\n+        d = self.make_request(request)\n+\n+        # Update the timer to 1s to test connection timeout\n+        self.client.setTimeout(1)\n+\n+        def assert_timeout_error(failure: Failure):\n+            for err in failure.value.reasons:\n+                if isinstance(err, TimeoutError):\n+                    self.assertIn(f\"Connection was IDLE for more than {H2ClientProtocol.IDLE_TIMEOUT}s\", str(err))\n+                    break\n+            else:\n+                self.fail()\n+\n+        d.addCallback(self.fail)\n+        d.addErrback(assert_timeout_error)\n+        return d\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8252a6f8d8930ce23bdf8a6f51038b4ce49d1968", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 40 | Lines Deleted: 23 | Files Changed: 2 | Hunks: 13 | Methods Changed: 12 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 63 | Churn Cumulative: 182 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -50,6 +50,7 @@ class H2DownloadHandler:\n \n         def print_result(result):\n             print(result)\n+            return result\n \n         d.addCallback(print_result)\n         return d\n\n@@ -1,15 +1,19 @@\n-from typing import Dict, Tuple\n+from typing import Dict, Tuple, Optional\n \n from twisted.internet import defer\n+from twisted.internet._sslverify import _setAcceptableProtocols, ClientTLSOptions\n from twisted.internet.base import ReactorBase\n from twisted.internet.defer import Deferred\n-from twisted.internet.endpoints import SSL4ClientEndpoint, optionsForClientTLS\n-from twisted.web.client import URI, BrowserLikePolicyForHTTPS\n+from twisted.internet.endpoints import SSL4ClientEndpoint\n+from twisted.python.failure import Failure\n+from twisted.web.client import URI, BrowserLikePolicyForHTTPS, _StandardEndpointFactory\n+from twisted.web.iweb import IPolicyForHTTPS\n+from zope.interface import implementer\n+from zope.interface.verify import verifyObject\n \n from scrapy.core.http2.protocol import H2ClientProtocol, H2ClientFactory\n from scrapy.http.request import Request\n from scrapy.settings import Settings\n-from scrapy.utils.python import to_bytes, to_unicode\n \n \n class H2ConnectionPool:\n@@ -26,39 +30,51 @@ class H2ConnectionPool:\n         return self._new_connection(key, uri, endpoint)\n \n     def _new_connection(self, key: Tuple, uri: URI, endpoint: SSL4ClientEndpoint) -> Deferred:\n-        factory = H2ClientFactory(uri, self.settings)\n-        d = endpoint.connect(factory)\n+        conn_lost_deferred = Deferred()\n+        conn_lost_deferred.addCallback(self._remove_connection, key)\n \n-        def put_connection(conn: H2ClientProtocol) -> H2ClientProtocol:\n+        factory = H2ClientFactory(uri, self.settings, conn_lost_deferred)\n+        d = endpoint.connect(factory)\n+        d.addCallback(self.put_connection, key)\n+        return d\n+\n+    def put_connection(self, conn: H2ClientProtocol, key: Tuple) -> H2ClientProtocol:\n         self._connections[key] = conn\n         return conn\n \n-        d.addCallback(put_connection)\n-        return d\n+    def _remove_connection(self, reason: Failure, key: Tuple) -> None:\n+        self._connections.pop(key)\n \n-    def _remove_connection(self, key) -> None:\n-        conn = self._connections.pop(key)\n-        conn.loseConnection()\n+\n+@implementer(IPolicyForHTTPS)\n+class H2WrappedContextFactory:\n+    def __init__(self, context_factory) -> None:\n+        verifyObject(IPolicyForHTTPS, context_factory)\n+        self._wrapped_context_factory = context_factory\n+\n+    def creatorForNetloc(self, hostname, port) -> ClientTLSOptions:\n+        options = self._wrapped_context_factory.creatorForNetloc(hostname, port)\n+        _setAcceptableProtocols(options._ctx, [b'h2'])\n+        return options\n \n \n class H2Agent:\n     def __init__(\n         self, reactor: ReactorBase, pool: H2ConnectionPool,\n-        context_factory=BrowserLikePolicyForHTTPS()\n+        context_factory=BrowserLikePolicyForHTTPS(),\n+        connect_timeout: Optional[float] = None, bind_address: Optional[bytes] = None\n     ) -> None:\n         self._reactor = reactor\n         self._pool = pool\n-        self._context_factory = context_factory\n+        self._context_factory = H2WrappedContextFactory(context_factory)\n+        self._endpoint_factory = _StandardEndpointFactory(\n+            self._reactor, self._context_factory,\n+            connect_timeout, bind_address\n+        )\n \n     def request(self, request: Request) -> Deferred:\n-        uri = URI.fromBytes(to_bytes(request.url, encoding='ascii'))\n-        # options = optionsForClientTLS(hostname=to_unicode(uri.host), acceptableProtocols=[b'h2'])\n-        # Hacky fix: Use options instead of self._context_factory to make endpoint work for HTTP/2\n-        endpoint = SSL4ClientEndpoint(self._reactor, to_unicode(uri.host), uri.port, self._context_factory)\n+        uri = URI.fromBytes(bytes(request.url, encoding='utf-8'))\n+        endpoint = self._endpoint_factory.endpointForURI(uri)\n         d = self._pool.get_connection(uri, endpoint)\n-\n-        def cb_connected(conn: H2ClientProtocol):\n-            return conn.request(request)\n-\n-        d.addCallback(cb_connected)\n+        d.addCallback(lambda conn: conn.request(request))\n         return d\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
