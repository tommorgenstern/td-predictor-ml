{"custom_id": "scrapy#c2d716807ab1d011352bc7dd81e3f213c32d0464", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 79 | Lines Deleted: 114 | Files Changed: 9 | Hunks: 64 | Methods Changed: 33 | Complexity Δ (Sum/Max): 26/35 | Churn Δ: 193 | Churn Cumulative: 648 | Contributors (this commit): 9 | Commits (past 90d): 13 | Contributors (cumulative): 28 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,6 +1,5 @@\n import six\n import pytest\n-from twisted.python import log\n \n collect_ignore = [\"scrapy/stats.py\", \"scrapy/project.py\"]\n \n@@ -10,35 +9,6 @@ if six.PY3:\n         if len(file_path) > 0 and file_path[0] != '#':\n             collect_ignore.append(file_path)\n \n-class LogObservers:\n-    \"\"\"Class for keeping track of log observers across test modules\"\"\"\n-\n-    def __init__(self):\n-        self.observers = []\n-\n-    def add(self, logfile='test.log'):\n-        fileobj = open(logfile, 'wb')\n-        observer = log.FileLogObserver(fileobj)\n-        log.startLoggingWithObserver(observer.emit, 0)\n-        self.observers.append((fileobj, observer))\n-\n-    def remove(self):\n-        fileobj, observer = self.observers.pop()\n-        log.removeObserver(observer.emit)\n-        fileobj.close()\n-\n-\n-@pytest.fixture(scope='module')\n-def log_observers():\n-    return LogObservers()\n-\n-\n-@pytest.fixture()\n-def setlog(request, log_observers):\n-    \"\"\"Attach test.log file observer to twisted log, for trial compatibility\"\"\"\n-    log_observers.add()\n-    request.addfinalizer(log_observers.remove)\n-\n \n @pytest.fixture()\n def chdir(tmpdir):\n\n@@ -46,19 +46,6 @@ def get_testenv():\n     env['PYTHONPATH'] = get_pythonpath()\n     return env\n \n-def get_testlog():\n-    \"\"\"Get Scrapy log of current test, ignoring the rest\"\"\"\n-    with open(\"test.log\", \"rb\") as fp:\n-        loglines = fp.readlines()\n-\n-    thistest = []\n-    for line in loglines[::-1]:\n-        thistest.append(line)\n-        if \"[-] -->\" in line:\n-            break\n-    return \"\".join(thistest[::-1])\n-\n-\n def assert_samelines(testcase, text1, text2, msg=None):\n     \"\"\"Asserts text1 and text2 have the same lines, ignoring differences in\n     line endings between platforms\n\n@@ -39,7 +39,6 @@ class TestCloseSpider(TestCase):\n         close_on = 5\n         crawler = get_crawler(ErrorSpider, {'CLOSESPIDER_ERRORCOUNT': close_on})\n         yield crawler.crawl(total=1000000)\n-        self.flushLoggedErrors(crawler.spider.exception_cls)\n         reason = crawler.spider.meta['close_reason']\n         self.assertEqual(reason, 'closespider_errorcount')\n         key = 'spider_exceptions/{name}'\\\n\n@@ -1,8 +1,12 @@\n import json\n import socket\n+import logging\n+\n+from testfixtures import LogCapture\n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n-from scrapy.utils.test import get_crawler, get_testlog\n+\n+from scrapy.utils.test import get_crawler\n from tests import mock\n from tests.spiders import FollowAllSpider, DelaySpider, SimpleSpider, \\\n     BrokenStartRequestsSpider, SingleRequestSpider, DuplicateStartRequestsSpider\n@@ -72,36 +76,47 @@ class CrawlTestCase(TestCase):\n     @defer.inlineCallbacks\n     def test_retry_503(self):\n         crawler = get_crawler(SimpleSpider)\n+        with LogCapture() as l:\n             yield crawler.crawl(\"http://localhost:8998/status?n=503\")\n-        self._assert_retried()\n+        self._assert_retried(l)\n \n     @defer.inlineCallbacks\n     def test_retry_conn_failed(self):\n         crawler = get_crawler(SimpleSpider)\n+        with LogCapture() as l:\n             yield crawler.crawl(\"http://localhost:65432/status?n=503\")\n-        self._assert_retried()\n+        self._assert_retried(l)\n \n     @defer.inlineCallbacks\n     def test_retry_dns_error(self):\n         with mock.patch('socket.gethostbyname',\n                         side_effect=socket.gaierror(-5, 'No address associated with hostname')):\n             crawler = get_crawler(SimpleSpider)\n+            with LogCapture() as l:\n                 yield crawler.crawl(\"http://example.com/\")\n-            self._assert_retried()\n+            self._assert_retried(l)\n \n     @defer.inlineCallbacks\n     def test_start_requests_bug_before_yield(self):\n+        with LogCapture('scrapy', level=logging.ERROR) as l:\n             crawler = get_crawler(BrokenStartRequestsSpider)\n             yield crawler.crawl(fail_before_yield=1)\n-        errors = self.flushLoggedErrors(ZeroDivisionError)\n-        self.assertEqual(len(errors), 1)\n+\n+        self.assertEqual(len(l.records), 1)\n+        record = l.records[0]\n+        self.assertIsNotNone(record.exc_info)\n+        self.assertIs(record.exc_info[0], ZeroDivisionError)\n \n     @defer.inlineCallbacks\n     def test_start_requests_bug_yielding(self):\n+        with LogCapture('scrapy', level=logging.ERROR) as l:\n             crawler = get_crawler(BrokenStartRequestsSpider)\n             yield crawler.crawl(fail_yielding=1)\n-        errors = self.flushLoggedErrors(ZeroDivisionError)\n-        self.assertEqual(len(errors), 1)\n+\n+        self.assertEqual(len(l.records), 1)\n+        record = l.records[0]\n+        self.assertIsNotNone(record.exc_info)\n+        self.assertIs(record.exc_info[0], ZeroDivisionError)\n \n     @defer.inlineCallbacks\n     def test_start_requests_lazyness(self):\n@@ -145,28 +160,29 @@ foo body\n with multiples lines\n '''})\n         crawler = get_crawler(SimpleSpider)\n+        with LogCapture() as l:\n             yield crawler.crawl(\"http://localhost:8998/raw?{0}\".format(query))\n-        log = get_testlog()\n-        self.assertEqual(log.count(\"Got response 200\"), 1)\n+        self.assertEqual(str(l).count(\"Got response 200\"), 1)\n \n     @defer.inlineCallbacks\n     def test_retry_conn_lost(self):\n         # connection lost after receiving data\n         crawler = get_crawler(SimpleSpider)\n+        with LogCapture() as l:\n             yield crawler.crawl(\"http://localhost:8998/drop?abort=0\")\n-        self._assert_retried()\n+        self._assert_retried(l)\n \n     @defer.inlineCallbacks\n     def test_retry_conn_aborted(self):\n         # connection lost before receiving data\n         crawler = get_crawler(SimpleSpider)\n+        with LogCapture() as l:\n             yield crawler.crawl(\"http://localhost:8998/drop?abort=1\")\n-        self._assert_retried()\n+        self._assert_retried(l)\n \n-    def _assert_retried(self):\n-        log = get_testlog()\n-        self.assertEqual(log.count(\"Retrying\"), 2)\n-        self.assertEqual(log.count(\"Gave up retrying\"), 1)\n+    def _assert_retried(self, log):\n+        self.assertEqual(str(log).count(\"Retrying\"), 2)\n+        self.assertEqual(str(log).count(\"Gave up retrying\"), 1)\n \n     @defer.inlineCallbacks\n     def test_referer_header(self):\n\n@@ -1,9 +1,9 @@\n from __future__ import print_function\n+from testfixtures import LogCapture\n from twisted.trial import unittest\n from twisted.python.failure import Failure\n from twisted.internet import reactor\n from twisted.internet.defer import Deferred, inlineCallbacks\n-from twisted.python import log as txlog\n \n from scrapy.http import Request, Response\n from scrapy.spider import Spider\n@@ -11,7 +11,6 @@ from scrapy.utils.request import request_fingerprint\n from scrapy.contrib.pipeline.media import MediaPipeline\n from scrapy.utils.signal import disconnect_all\n from scrapy import signals\n-from scrapy import log\n \n \n def _mocked_download_func(request, info):\n@@ -60,26 +59,21 @@ class BaseMediaPipelineTestCase(unittest.TestCase):\n         fail = Failure(Exception())\n         results = [(True, 1), (False, fail)]\n \n-        events = []\n-        txlog.addObserver(events.append)\n+        with LogCapture() as l:\n             new_item = self.pipe.item_completed(results, item, self.info)\n-        txlog.removeObserver(events.append)\n-        self.flushLoggedErrors()\n \n         assert new_item is item\n-        assert len(events) == 1\n-        assert events[0]['logLevel'] == log.ERROR\n-        assert events[0]['failure'] is fail\n+        assert len(l.records) == 1\n+        record = l.records[0]\n+        assert record.levelname == 'ERROR'\n+        assert record.failure is fail\n \n         # disable failure logging and check again\n         self.pipe.LOG_FAILED_RESULTS = False\n-        events = []\n-        txlog.addObserver(events.append)\n+        with LogCapture() as l:\n             new_item = self.pipe.item_completed(results, item, self.info)\n-        txlog.removeObserver(events.append)\n-        self.flushLoggedErrors()\n         assert new_item is item\n-        assert len(events) == 0\n+        assert len(l.records) == 0\n \n     @inlineCallbacks\n     def test_default_process_item(self):\n\n@@ -5,10 +5,11 @@ import time\n from threading import Thread\n from libmproxy import controller, proxy\n from netlib import http_auth\n+from testfixtures import LogCapture\n \n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n-from scrapy.utils.test import get_testlog, get_crawler\n+from scrapy.utils.test import get_crawler\n from scrapy.http import Request\n from tests.spiders import SimpleSpider, SingleRequestSpider\n from tests.mockserver import MockServer\n@@ -50,39 +51,44 @@ class ProxyConnectTestCase(TestCase):\n     @defer.inlineCallbacks\n     def test_https_connect_tunnel(self):\n         crawler = get_crawler(SimpleSpider)\n+        with LogCapture() as l:\n             yield crawler.crawl(\"https://localhost:8999/status?n=200\")\n-        self._assert_got_response_code(200)\n+        self._assert_got_response_code(200, l)\n \n     @defer.inlineCallbacks\n     def test_https_noconnect(self):\n         os.environ['https_proxy'] = 'http://scrapy:scrapy@localhost:8888?noconnect'\n         crawler = get_crawler(SimpleSpider)\n+        with LogCapture() as l:\n             yield crawler.crawl(\"https://localhost:8999/status?n=200\")\n-        self._assert_got_response_code(200)\n+        self._assert_got_response_code(200, l)\n         os.environ['https_proxy'] = 'http://scrapy:scrapy@localhost:8888'\n \n     @defer.inlineCallbacks\n     def test_https_connect_tunnel_error(self):\n         crawler = get_crawler(SimpleSpider)\n+        with LogCapture() as l:\n             yield crawler.crawl(\"https://localhost:99999/status?n=200\")\n-        self._assert_got_tunnel_error()\n+        self._assert_got_tunnel_error(l)\n \n     @defer.inlineCallbacks\n     def test_https_tunnel_auth_error(self):\n         os.environ['https_proxy'] = 'http://wrong:wronger@localhost:8888'\n         crawler = get_crawler(SimpleSpider)\n+        with LogCapture() as l:\n             yield crawler.crawl(\"https://localhost:8999/status?n=200\")\n         # The proxy returns a 407 error code but it does not reach the client;\n         # he just sees a TunnelError.\n-        self._assert_got_tunnel_error()\n+        self._assert_got_tunnel_error(l)\n         os.environ['https_proxy'] = 'http://scrapy:scrapy@localhost:8888'\n \n     @defer.inlineCallbacks\n     def test_https_tunnel_without_leak_proxy_authorization_header(self):\n         request = Request(\"https://localhost:8999/echo\")\n         crawler = get_crawler(SingleRequestSpider)\n+        with LogCapture() as l:\n             yield crawler.crawl(seed=request)\n-        self._assert_got_response_code(200)\n+        self._assert_got_response_code(200, l)\n         echo = json.loads(crawler.spider.meta['responses'][0].body)\n         self.assertTrue('Proxy-Authorization' not in echo['headers'])\n \n@@ -90,13 +96,12 @@ class ProxyConnectTestCase(TestCase):\n     def test_https_noconnect_auth_error(self):\n         os.environ['https_proxy'] = 'http://wrong:wronger@localhost:8888?noconnect'\n         crawler = get_crawler(SimpleSpider)\n+        with LogCapture() as l:\n             yield crawler.crawl(\"https://localhost:8999/status?n=200\")\n-        self._assert_got_response_code(407)\n+        self._assert_got_response_code(407, l)\n \n-    def _assert_got_response_code(self, code):\n-        log = get_testlog()\n-        self.assertEqual(log.count('Crawled (%d)' % code), 1)\n+    def _assert_got_response_code(self, code, log):\n+        self.assertEqual(str(log).count('Crawled (%d)' % code), 1)\n \n-    def _assert_got_tunnel_error(self):\n-        log = get_testlog()\n-        self.assertEqual(log.count('TunnelError'), 1)\n+    def _assert_got_tunnel_error(self, log):\n+        self.assertEqual(str(log).count('TunnelError'), 1)\n\n@@ -1,9 +1,10 @@\n from unittest import TestCase\n \n+from testfixtures import LogCapture\n from twisted.trial.unittest import TestCase as TrialTestCase\n from twisted.internet import defer\n \n-from scrapy.utils.test import get_crawler, get_testlog\n+from scrapy.utils.test import get_crawler\n from tests.mockserver import MockServer\n from scrapy.http import Response, Request\n from scrapy.spider import Spider\n@@ -174,14 +175,13 @@ class TestHttpErrorMiddlewareIntegrational(TrialTestCase):\n     @defer.inlineCallbacks\n     def test_logging(self):\n         crawler = get_crawler(_HttpErrorSpider)\n+        with LogCapture() as log:\n             yield crawler.crawl(bypass_status_codes={402})\n-        # print(get_testlog())\n         self.assertEqual(crawler.spider.parsed, {'200', '402'})\n         self.assertEqual(crawler.spider.skipped, {'402'})\n         self.assertEqual(crawler.spider.failed, {'404', '500'})\n \n-        log = get_testlog()\n-        self.assertIn('Ignoring response <404', log)\n-        self.assertIn('Ignoring response <500', log)\n-        self.assertNotIn('Ignoring response <200', log)\n-        self.assertNotIn('Ignoring response <402', log)\n+        self.assertIn('Ignoring response <404', str(log))\n+        self.assertIn('Ignoring response <500', str(log))\n+        self.assertNotIn('Ignoring response <200', str(log))\n+        self.assertNotIn('Ignoring response <402', str(log))\n\n@@ -74,7 +74,6 @@ class DeferUtilsTest(unittest.TestCase):\n     def test_process_parallel_failure(self):\n         d = process_parallel([cb1, cb_fail, cb3], 'res', 'v1', 'v2')\n         self.failUnlessFailure(d, TypeError)\n-        self.flushLoggedErrors()\n         return d\n \n \n\n@@ -1,11 +1,11 @@\n+from testfixtures import LogCapture\n from twisted.trial import unittest\n-from twisted.python import log as txlog\n from twisted.python.failure import Failure\n from twisted.internet import defer, reactor\n \n from scrapy.xlib.pydispatch import dispatcher\n from scrapy.utils.signal import send_catch_log, send_catch_log_deferred\n-from scrapy import log\n+\n \n class SendCatchLogTest(unittest.TestCase):\n \n@@ -14,26 +14,24 @@ class SendCatchLogTest(unittest.TestCase):\n         test_signal = object()\n         handlers_called = set()\n \n-        def log_received(event):\n-            handlers_called.add(log_received)\n-            assert \"error_handler\" in event['message'][0]\n-            assert event['logLevel'] == log.ERROR\n-\n-        txlog.addObserver(log_received)\n         dispatcher.connect(self.error_handler, signal=test_signal)\n         dispatcher.connect(self.ok_handler, signal=test_signal)\n-        result = yield defer.maybeDeferred(self._get_result, test_signal, arg='test', \\\n-            handlers_called=handlers_called)\n+        with LogCapture() as l:\n+            result = yield defer.maybeDeferred(\n+                self._get_result, test_signal, arg='test',\n+                handlers_called=handlers_called\n+            )\n \n         assert self.error_handler in handlers_called\n         assert self.ok_handler in handlers_called\n-        assert log_received in handlers_called\n+        self.assertEqual(len(l.records), 1)\n+        record = l.records[0]\n+        self.assertIn('error_handler', record.getMessage())\n+        self.assertEqual(record.levelname, 'ERROR')\n         self.assertEqual(result[0][0], self.error_handler)\n         self.assert_(isinstance(result[0][1], Failure))\n         self.assertEqual(result[1], (self.ok_handler, \"OK\"))\n \n-        txlog.removeObserver(log_received)\n-        self.flushLoggedErrors()\n         dispatcher.disconnect(self.error_handler, signal=test_signal)\n         dispatcher.disconnect(self.ok_handler, signal=test_signal)\n \n@@ -73,12 +71,9 @@ class SendCatchLogTest2(unittest.TestCase):\n     def test_error_logged_if_deferred_not_supported(self):\n         test_signal = object()\n         test_handler = lambda: defer.Deferred()\n-        log_events = []\n-        txlog.addObserver(log_events.append)\n         dispatcher.connect(test_handler, test_signal)\n+        with LogCapture() as l:\n             send_catch_log(test_signal)\n-        self.assertTrue(log_events)\n-        self.assertIn(\"Cannot return deferreds from signal handler\", str(log_events))\n-        txlog.removeObserver(log_events.append)\n-        self.flushLoggedErrors()\n+        self.assertEqual(len(l.records), 1)\n+        self.assertIn(\"Cannot return deferreds from signal handler\", str(l))\n         dispatcher.disconnect(test_handler, test_signal)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6f9b423215df8591b0d9e0062a40cc28678236ac", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 22 | Files Changed: 4 | Hunks: 20 | Methods Changed: 9 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 64 | Churn Cumulative: 3221 | Contributors (this commit): 20 | Commits (past 90d): 7 | Contributors (cumulative): 33 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,7 +10,7 @@ from time import time\n from twisted.internet import defer\n from twisted.python.failure import Failure\n \n-from scrapy import log, signals\n+from scrapy import signals\n from scrapy.core.scraper import Scraper\n from scrapy.exceptions import DontCloseSpider\n from scrapy.http import Response, Request\n@@ -204,7 +204,7 @@ class ExecutionEngine(object):\n             if isinstance(response, Response):\n                 response.request = request # tie request to response received\n                 logkws = self.logformatter.crawled(request, response, spider)\n-                log.msg(spider=spider, **logkws)\n+                logger._log(extra={'spider': spider}, **logkws)\n                 self.signals.send_catch_log(signal=signals.response_received, \\\n                     response=response, request=request, spider=spider)\n             return response\n\n@@ -15,7 +15,6 @@ from scrapy import signals\n from scrapy.http import Request, Response\n from scrapy.item import BaseItem\n from scrapy.core.spidermw import SpiderMiddlewareManager\n-from scrapy import log\n \n logger = logging.getLogger('scrapy')\n \n@@ -221,7 +220,7 @@ class Scraper(object):\n             ex = output.value\n             if isinstance(ex, DropItem):\n                 logkws = self.logformatter.dropped(item, ex, response, spider)\n-                log.msg(spider=spider, **logkws)\n+                logger._log(extra={'spider': spider}, **logkws)\n                 return self.signals.send_catch_log_deferred(\n                     signal=signals.item_dropped, item=item, response=response,\n                     spider=spider, exception=output.value)\n@@ -230,7 +229,7 @@ class Scraper(object):\n                              extra={'spider': spider, 'failure': output})\n         else:\n             logkws = self.logformatter.scraped(output, response, spider)\n-            log.msg(spider=spider, **logkws)\n+            logger._log(extra={'spider': spider}, **logkws)\n             return self.signals.send_catch_log_deferred(\n                 signal=signals.item_scraped, item=output, response=response,\n                 spider=spider)\n\n@@ -1,47 +1,68 @@\n import os\n+import logging\n \n from twisted.python.failure import Failure\n \n-from scrapy import log\n \n+SCRAPEDMSG = u\"Scraped from %(src)s\" + os.linesep + \"%(item)s\"\n+DROPPEDMSG = u\"Dropped: %(exception)s\" + os.linesep + \"%(item)s\"\n+CRAWLEDMSG = u\"Crawled (%(status)s) %(request)s (referer: %(referer)s)%(flags)s\"\n \n-SCRAPEDFMT = u\"Scraped from %(src)s\" + os.linesep + \"%(item)s\"\n-DROPPEDFMT = u\"Dropped: %(exception)s\" + os.linesep + \"%(item)s\"\n-CRAWLEDFMT = u\"Crawled (%(status)s) %(request)s (referer: %(referer)s)%(flags)s\"\n \n class LogFormatter(object):\n-    \"\"\"Class for generating log messages for different actions. All methods\n-    must return a plain string which doesn't include the log level or the\n-    timestamp\n+    \"\"\"Class for generating log messages for different actions.\n+\n+    All methods must return a dictionary listing the parameters `level`, `msg`\n+    and `args` which are going to be used for constructing the log message when\n+    calling logging.log.\n+\n+    Dictionary keys for the method outputs:\n+        * `level` should be the log level for that action, you can use those\n+        from the python logging library: logging.DEBUG, logging.INFO,\n+        logging.WARNING, logging.ERROR and logging.CRITICAL.\n+\n+        * `msg` should be a string that can contain different formatting\n+        placeholders. This string, formatted with the provided `args`, is going\n+        to be the log message for that action.\n+\n+        * `args` should be a tuple or dict with the formatting placeholders for\n+        `msg`.  The final log message is computed as output['msg'] %\n+        output['args'].\n     \"\"\"\n \n     def crawled(self, request, response, spider):\n         flags = ' %s' % str(response.flags) if response.flags else ''\n         return {\n-            'level': log.DEBUG,\n-            'format': CRAWLEDFMT,\n+            'level': logging.DEBUG,\n+            'msg': CRAWLEDMSG,\n+            'args': {\n                 'status': response.status,\n                 'request': request,\n                 'referer': request.headers.get('Referer'),\n                 'flags': flags,\n             }\n+        }\n \n     def scraped(self, item, response, spider):\n         src = response.getErrorMessage() if isinstance(response, Failure) else response\n         return {\n-            'level': log.DEBUG,\n-            'format': SCRAPEDFMT,\n+            'level': logging.DEBUG,\n+            'msg': SCRAPEDMSG,\n+            'args': {\n                 'src': src,\n                 'item': item,\n             }\n+        }\n \n     def dropped(self, item, exception, response, spider):\n         return {\n-            'level': log.WARNING,\n-            'format': DROPPEDFMT,\n+            'level': logging.WARNING,\n+            'msg': DROPPEDMSG,\n+            'args': {\n                 'exception': exception,\n                 'item': item,\n             }\n+        }\n \n     @classmethod\n     def from_crawler(cls, crawler):\n\n@@ -24,14 +24,14 @@ class LoggingContribTest(unittest.TestCase):\n         req = Request(\"http://www.example.com\")\n         res = Response(\"http://www.example.com\")\n         logkws = self.formatter.crawled(req, res, self.spider)\n-        logline = logkws['format'] % logkws\n+        logline = logkws['msg'] % logkws['args']\n         self.assertEqual(logline,\n             \"Crawled (200) <GET http://www.example.com> (referer: None)\")\n \n         req = Request(\"http://www.example.com\", headers={'referer': 'http://example.com'})\n         res = Response(\"http://www.example.com\", flags=['cached'])\n         logkws = self.formatter.crawled(req, res, self.spider)\n-        logline = logkws['format'] % logkws\n+        logline = logkws['msg'] % logkws['args']\n         self.assertEqual(logline,\n             \"Crawled (200) <GET http://www.example.com> (referer: http://example.com) ['cached']\")\n \n@@ -40,7 +40,7 @@ class LoggingContribTest(unittest.TestCase):\n         exception = Exception(u\"\\u2018\")\n         response = Response(\"http://www.example.com\")\n         logkws = self.formatter.dropped(item, exception, response, self.spider)\n-        logline = logkws['format'] % logkws\n+        logline = logkws['msg'] % logkws['args']\n         lines = logline.splitlines()\n         assert all(isinstance(x, unicode) for x in lines)\n         self.assertEqual(lines, [u\"Dropped: \\u2018\", '{}'])\n@@ -50,7 +50,7 @@ class LoggingContribTest(unittest.TestCase):\n         item['name'] = u'\\xa3'\n         response = Response(\"http://www.example.com\")\n         logkws = self.formatter.scraped(item, response, self.spider)\n-        logline = logkws['format'] % logkws\n+        logline = logkws['msg'] % logkws['args']\n         lines = logline.splitlines()\n         assert all(isinstance(x, unicode) for x in lines)\n         self.assertEqual(lines, [u\"Scraped from <200 http://www.example.com>\", u'name: \\xa3'])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8baad552676de1ea0f3789a03279a025ed26f17e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 51 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/3 | Churn Δ: 58 | Churn Cumulative: 1232 | Contributors (this commit): 7 | Commits (past 90d): 13 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -15,6 +15,7 @@ from scrapy.signalmanager import SignalManager\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n from scrapy.utils.misc import load_object\n+from scrapy.utils.log import configure_logging, log_scrapy_info\n from scrapy import signals\n \n logger = logging.getLogger('scrapy')\n@@ -142,8 +143,8 @@ class CrawlerProcess(CrawlerRunner):\n         super(CrawlerProcess, self).__init__(settings)\n         install_shutdown_handlers(self._signal_shutdown)\n         self.stopping = False\n-        self.log_observer = log.start_from_settings(self.settings)\n-        log.scrapy_info(settings)\n+        configure_logging(settings)\n+        log_scrapy_info(settings)\n \n     def _signal_shutdown(self, signum, _):\n         install_shutdown_handlers(self._signal_kill)\n@@ -157,7 +158,6 @@ class CrawlerProcess(CrawlerRunner):\n         signame = signal_names[signum]\n         logger.info('Received %(signame)s twice, forcing unclean shutdown',\n                     {'signame': signame})\n-        self._stop_logging()\n         reactor.callFromThread(self._stop_reactor)\n \n     def start(self, stop_after_crawl=True):\n@@ -176,10 +176,6 @@ class CrawlerProcess(CrawlerRunner):\n         reactor.addSystemEventTrigger('before', 'shutdown', self.stop)\n         reactor.run(installSignalHandlers=False)  # blocking call\n \n-    def _stop_logging(self):\n-        if self.log_observer:\n-            self.log_observer.stop()\n-\n     def _stop_reactor(self, _=None):\n         try:\n             reactor.stop()\n\n@@ -0,0 +1,48 @@\n+# -*- coding: utf-8 -*-\n+\n+import sys\n+import logging\n+from logging.config import dictConfig\n+\n+from twisted.python import log as twisted_log\n+\n+import scrapy\n+from scrapy.settings import overridden_settings\n+\n+logger = logging.getLogger('scrapy')\n+\n+\n+DEFAULT_LOGGING = {\n+    'version': 1,\n+    'disable_existing_loggers': False,\n+    'loggers': {\n+        'scrapy': {\n+            'level': 'DEBUG',\n+        },\n+        'twisted': {\n+            'level': 'ERROR',\n+        },\n+    }\n+}\n+\n+\n+def configure_logging(settings=None):\n+    if not sys.warnoptions:\n+        # Route warnings through python logging\n+        logging.captureWarnings(True)\n+\n+    observer = twisted_log.PythonLoggingObserver('twisted')\n+    observer.start()\n+\n+    dictConfig(DEFAULT_LOGGING)\n+\n+\n+def log_scrapy_info(settings):\n+    logger.info(\"Scrapy %(version)s started (bot: %(bot)s)\",\n+                {'version': scrapy.__version__, 'bot': settings['BOT_NAME']})\n+\n+    logger.info(\"Optional features available: %(features)s\",\n+                {'features': \", \".join(scrapy.optional_features)})\n+\n+    d = dict(overridden_settings(settings))\n+    logger.info(\"Overridden settings: %(settings)r\", {'settings': d})\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b75556ef796efe806279778441a11cdc3c024e07", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 72 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 8/5 | Churn Δ: 72 | Churn Cumulative: 120 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,9 +1,11 @@\n # -*- coding: utf-8 -*-\n \n+import os\n import sys\n import logging\n from logging.config import dictConfig\n \n+from twisted.python.failure import Failure\n from twisted.python import log as twisted_log\n \n import scrapy\n@@ -12,12 +14,36 @@ from scrapy.settings import overridden_settings\n logger = logging.getLogger('scrapy')\n \n \n+class FailureFormatter(logging.Filter):\n+    \"\"\"Extract exc_info from Failure instances provided as contextual data\n+\n+    This filter mimics Twisted log.err formatting for its first `_stuff`\n+    argument, which means that reprs of non Failure objects are appended to the\n+    log messages.\n+    \"\"\"\n+\n+    def filter(self, record):\n+        failure = record.__dict__.get('failure')\n+        if failure:\n+            if isinstance(failure, Failure):\n+                record.exc_info = (failure.type, failure.value, failure.tb)\n+            else:\n+                record.msg += os.linesep + repr(failure)\n+        return True\n+\n+\n DEFAULT_LOGGING = {\n     'version': 1,\n     'disable_existing_loggers': False,\n+    'filters': {\n+        'failure_formatter': {\n+            '()': 'scrapy.utils.log.FailureFormatter',\n+        },\n+    },\n     'loggers': {\n         'scrapy': {\n             'level': 'DEBUG',\n+            'filters': ['failure_formatter'],\n         },\n         'twisted': {\n             'level': 'ERROR',\n\n@@ -0,0 +1,46 @@\n+# -*- coding: utf-8 -*-\n+import os\n+import logging\n+import unittest\n+\n+from testfixtures import LogCapture\n+from twisted.python.failure import Failure\n+\n+from scrapy.utils.log import FailureFormatter\n+\n+\n+class FailureFormatterTest(unittest.TestCase):\n+\n+    def setUp(self):\n+        self.logger = logging.getLogger('test')\n+        self.filter = FailureFormatter()\n+        self.logger.addFilter(self.filter)\n+\n+    def tearDown(self):\n+        self.logger.removeFilter(self.filter)\n+\n+    def test_failure_format(self):\n+        with LogCapture() as l:\n+            try:\n+                0/0\n+            except ZeroDivisionError:\n+                self.logger.exception('test log msg')\n+                failure = Failure()\n+\n+            self.logger.error('test log msg', extra={'failure': failure})\n+\n+        self.assertEqual(len(l.records), 2)\n+        exc_record, failure_record = l.records\n+        self.assertTupleEqual(failure_record.exc_info, exc_record.exc_info)\n+\n+        formatter = logging.Formatter()\n+        self.assertMultiLineEqual(formatter.format(failure_record),\n+                                  formatter.format(exc_record))\n+\n+    def test_non_failure_format(self):\n+        with LogCapture() as l:\n+            self.logger.error('test log msg', extra={'failure': 3})\n+\n+        self.assertEqual(len(l.records), 1)\n+        self.assertMultiLineEqual(l.records[0].getMessage(),\n+                                  'test log msg' + os.linesep + '3')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ac40ef611a892daa51b90f214875fa06f11806dc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 51 | Lines Deleted: 9 | Files Changed: 4 | Hunks: 8 | Methods Changed: 11 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 60 | Churn Cumulative: 1507 | Contributors (this commit): 8 | Commits (past 90d): 21 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -53,7 +53,6 @@ class Command(ScrapyCommand):\n         # The crawler is created this way since the Shell manually handles the\n         # crawling engine, so the set up in the crawl method won't work\n         crawler = self.crawler_process._create_crawler(spidercls)\n-        self.crawler_process._setup_crawler_logging(crawler)\n         # The Shell class needs a persistent engine in the crawler\n         crawler.engine = crawler._create_engine()\n         crawler.engine.start()\n\n@@ -15,7 +15,7 @@ from scrapy.signalmanager import SignalManager\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n from scrapy.utils.misc import load_object\n-from scrapy.utils.log import configure_logging, log_scrapy_info\n+from scrapy.utils.log import LogCounterHandler, configure_logging, log_scrapy_info\n from scrapy import signals\n \n logger = logging.getLogger('scrapy')\n@@ -32,6 +32,12 @@ class Crawler(object):\n \n         self.signals = SignalManager(self)\n         self.stats = load_object(self.settings['STATS_CLASS'])(self)\n+\n+        handler = LogCounterHandler(self, level=settings.get('LOG_LEVEL'))\n+        logging.root.addHandler(handler)\n+        self.signals.connect(lambda: logging.root.removeHandler(handler),\n+                             signals.engine_stopped)\n+\n         lf_cls = load_object(self.settings['LOG_FORMATTER'])\n         self.logformatter = lf_cls.from_crawler(self)\n         self.extensions = ExtensionManager.from_crawler(self)\n@@ -103,7 +109,6 @@ class CrawlerRunner(object):\n         crawler = crawler_or_spidercls\n         if not isinstance(crawler_or_spidercls, Crawler):\n             crawler = self._create_crawler(crawler_or_spidercls)\n-            self._setup_crawler_logging(crawler)\n \n         self.crawlers.add(crawler)\n         d = crawler.crawl(*args, **kwargs)\n@@ -121,11 +126,6 @@ class CrawlerRunner(object):\n             spidercls = self.spider_loader.load(spidercls)\n         return Crawler(spidercls, self.settings)\n \n-    def _setup_crawler_logging(self, crawler):\n-        log_observer = log.start_from_crawler(crawler)\n-        if log_observer:\n-            crawler.signals.connect(log_observer.stop, signals.engine_stopped)\n-\n     def stop(self):\n         return defer.DeferredList([c.stop() for c in list(self.crawlers)])\n \n\n@@ -72,3 +72,15 @@ def log_scrapy_info(settings):\n \n     d = dict(overridden_settings(settings))\n     logger.info(\"Overridden settings: %(settings)r\", {'settings': d})\n+\n+\n+class LogCounterHandler(logging.Handler):\n+    \"\"\"Record log levels count into a crawler stats\"\"\"\n+\n+    def __init__(self, crawler, *args, **kwargs):\n+        super(LogCounterHandler, self).__init__(*args, **kwargs)\n+        self.crawler = crawler\n+\n+    def emit(self, record):\n+        sname = 'log_count/{}'.format(record.levelname)\n+        self.crawler.stats.inc_value(sname)\n\n@@ -6,7 +6,8 @@ import unittest\n from testfixtures import LogCapture\n from twisted.python.failure import Failure\n \n-from scrapy.utils.log import FailureFormatter\n+from scrapy.utils.log import FailureFormatter, LogCounterHandler\n+from scrapy.utils.test import get_crawler\n \n \n class FailureFormatterTest(unittest.TestCase):\n@@ -44,3 +45,33 @@ class FailureFormatterTest(unittest.TestCase):\n         self.assertEqual(len(l.records), 1)\n         self.assertMultiLineEqual(l.records[0].getMessage(),\n                                   'test log msg' + os.linesep + '3')\n+\n+\n+class LogCounterHandlerTest(unittest.TestCase):\n+\n+    def setUp(self):\n+        self.logger = logging.getLogger('test')\n+        self.logger.setLevel(logging.NOTSET)\n+        self.logger.propagate = False\n+        self.crawler = get_crawler(settings_dict={'LOG_LEVEL': 'WARNING'})\n+        self.handler = LogCounterHandler(self.crawler)\n+        self.logger.addHandler(self.handler)\n+\n+    def tearDown(self):\n+        self.logger.propagate = True\n+        self.logger.removeHandler(self.handler)\n+\n+    def test_init(self):\n+        self.assertIsNone(self.crawler.stats.get_value('log_count/DEBUG'))\n+        self.assertIsNone(self.crawler.stats.get_value('log_count/INFO'))\n+        self.assertIsNone(self.crawler.stats.get_value('log_count/WARNING'))\n+        self.assertIsNone(self.crawler.stats.get_value('log_count/ERROR'))\n+        self.assertIsNone(self.crawler.stats.get_value('log_count/CRITICAL'))\n+\n+    def test_accepted_level(self):\n+        self.logger.error('test log msg')\n+        self.assertEqual(self.crawler.stats.get_value('log_count/ERROR'), 1)\n+\n+    def test_filtered_out_level(self):\n+        self.logger.debug('test log msg')\n+        self.assertIsNone(self.crawler.stats.get_value('log_count/INFO'))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1c8708eb829b9b57729860cecfd7cc8d1eaffaea", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 8 | Files Changed: 4 | Hunks: 9 | Methods Changed: 8 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 43 | Churn Cumulative: 367 | Contributors (this commit): 8 | Commits (past 90d): 11 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,9 +3,9 @@ Base class for Scrapy spiders\n \n See documentation in docs/topics/spiders.rst\n \"\"\"\n+import logging\n import warnings\n \n-from scrapy import log\n from scrapy import signals\n from scrapy.http import Request\n from scrapy.utils.trackref import object_ref\n@@ -31,11 +31,19 @@ class Spider(object_ref):\n         if not hasattr(self, 'start_urls'):\n             self.start_urls = []\n \n-    def log(self, message, level=log.DEBUG, **kw):\n-        \"\"\"Log the given messages at the given log level. Always use this\n-        method to send log messages from your spider\n+    @property\n+    def logger(self):\n+        logger = logging.getLogger(self.name)\n+        return logging.LoggerAdapter(logger, {'spider': self})\n+\n+    def log(self, message, level=logging.DEBUG, **kw):\n+        \"\"\"Log the given message at the given log level\n+\n+        This helper wraps a log call to the logger within the spider, but you\n+        can use it directly (e.g. Spider.logger.info('msg')) or use any other\n+        Python logger too.\n         \"\"\"\n-        log.msg(message, spider=self, level=level, **kw)\n+        self.logger.log(level, message, **kw)\n \n     @classmethod\n     def from_crawler(cls, crawler, *args, **kwargs):\n\n@@ -74,7 +74,7 @@ class SimpleSpider(MetaSpider):\n         self.start_urls = [url]\n \n     def parse(self, response):\n-        self.log(\"Got response %d\" % response.status)\n+        self.logger.info(\"Got response %d\" % response.status)\n \n \n class ItemSpider(FollowAllSpider):\n\n@@ -142,7 +142,7 @@ class MySpider(scrapy.Spider):\n     name = 'myspider'\n \n     def start_requests(self):\n-        self.log(\"It Works!\")\n+        self.logger.debug(\"It Works!\")\n         return []\n \"\"\")\n         p = self.proc('runspider', fname)\n@@ -197,7 +197,7 @@ class MySpider(scrapy.Spider):\n \n     def parse(self, response):\n         if getattr(self, 'test_arg', None):\n-            self.log('It Works!')\n+            self.logger.debug('It Works!')\n         return [scrapy.Item(), dict(foo='bar')]\n \"\"\".format(self.spider_name))\n \n\n@@ -2,6 +2,8 @@ import gzip\n import inspect\n import warnings\n from io import BytesIO\n+\n+from testfixtures import LogCapture\n from twisted.trial import unittest\n \n from scrapy import signals\n@@ -102,6 +104,23 @@ class SpiderTest(unittest.TestCase):\n         self.assertEqual(settings.get('TEST2'), 'spider')\n         self.assertEqual(settings.get('TEST3'), 'project')\n \n+    def test_logger(self):\n+        spider = self.spider_class('example.com')\n+        with LogCapture() as l:\n+            spider.logger.info('test log msg')\n+        l.check(('example.com', 'INFO', 'test log msg'))\n+\n+        record = l.records[0]\n+        self.assertIn('spider', record.__dict__)\n+        self.assertIs(record.spider, spider)\n+\n+    def test_log(self):\n+        spider = self.spider_class('example.com')\n+        with mock.patch('scrapy.spider.Spider.logger') as mock_logger:\n+            spider.log('test log msg', 'INFO')\n+        mock_logger.log.assert_called_once_with('INFO', 'test log msg')\n+\n+\n class InitSpiderTest(SpiderTest):\n \n     spider_class = InitSpider\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ffd97f2f1b96296dccf65598ff3ce49e477e5fea", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 28 | Churn Cumulative: 114 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -53,6 +53,14 @@ DEFAULT_LOGGING = {\n \n \n def configure_logging(settings=None):\n+    \"\"\"Initialize and configure default loggers\n+\n+    This function does:\n+      - Route warnings and twisted logging through Python standard logging\n+      - Set FailureFormatter filter on Scrapy logger\n+      - Assign DEBUG and ERROR level to Scrapy and Twisted loggers respectively\n+      - Create a handler for the root logger according to given settings\n+    \"\"\"\n     if not sys.warnoptions:\n         # Route warnings through python logging\n         logging.captureWarnings(True)\n@@ -61,6 +69,26 @@ def configure_logging(settings=None):\n     observer.start()\n \n     dictConfig(DEFAULT_LOGGING)\n+    if settings:\n+        logging.root.setLevel(logging.NOTSET)\n+\n+        # Set up the default log handler\n+        filename = settings.get('LOG_FILE')\n+        if filename:\n+            encoding = settings.get('LOG_ENCODING')\n+            handler = logging.FileHandler(filename, encoding=encoding)\n+        elif settings.getbool('LOG_ENABLED'):\n+            handler = logging.StreamHandler()\n+        else:\n+            handler = logging.NullHandler()\n+\n+        formatter = logging.Formatter(\n+            fmt='%(asctime)s [%(name)s] %(levelname)s: %(message)s',\n+            datefmt='%Y-%m-%d %H:%M:%S%z'\n+        )\n+        handler.setFormatter(formatter)\n+        handler.setLevel(settings.get('LOG_LEVEL'))\n+        logging.root.addHandler(handler)\n \n \n def log_scrapy_info(settings):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6acb3848fbc8405cb0985355561ae207b2e444a1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 7/4 | Churn Δ: 40 | Churn Cumulative: 233 | Contributors (this commit): 1 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -72,6 +72,9 @@ def configure_logging(settings=None):\n     if settings:\n         logging.root.setLevel(logging.NOTSET)\n \n+        if settings.getbool('LOG_STDOUT'):\n+            sys.stdout = StreamLogger(logging.getLogger('stdout'))\n+\n         # Set up the default log handler\n         filename = settings.get('LOG_FILE')\n         if filename:\n@@ -102,6 +105,22 @@ def log_scrapy_info(settings):\n     logger.info(\"Overridden settings: %(settings)r\", {'settings': d})\n \n \n+class StreamLogger(object):\n+    \"\"\"Fake file-like stream object that redirects writes to a logger instance\n+\n+    Taken from:\n+        http://www.electricmonk.nl/log/2011/08/14/redirect-stdout-and-stderr-to-a-logger-in-python/\n+    \"\"\"\n+    def __init__(self, logger, log_level=logging.INFO):\n+        self.logger = logger\n+        self.log_level = log_level\n+        self.linebuf = ''\n+\n+    def write(self, buf):\n+        for line in buf.rstrip().splitlines():\n+            self.logger.log(self.log_level, line.rstrip())\n+\n+\n class LogCounterHandler(logging.Handler):\n     \"\"\"Record log levels count into a crawler stats\"\"\"\n \n\n@@ -1,12 +1,14 @@\n # -*- coding: utf-8 -*-\n+from __future__ import print_function\n import os\n+import sys\n import logging\n import unittest\n \n from testfixtures import LogCapture\n from twisted.python.failure import Failure\n \n-from scrapy.utils.log import FailureFormatter, LogCounterHandler\n+from scrapy.utils.log import FailureFormatter, LogCounterHandler, StreamLogger\n from scrapy.utils.test import get_crawler\n \n \n@@ -75,3 +77,20 @@ class LogCounterHandlerTest(unittest.TestCase):\n     def test_filtered_out_level(self):\n         self.logger.debug('test log msg')\n         self.assertIsNone(self.crawler.stats.get_value('log_count/INFO'))\n+\n+\n+class StreamLoggerTest(unittest.TestCase):\n+\n+    def setUp(self):\n+        self.stdout = sys.stdout\n+        logger = logging.getLogger('test')\n+        logger.setLevel(logging.WARNING)\n+        sys.stdout = StreamLogger(logger, logging.ERROR)\n+\n+    def tearDown(self):\n+        sys.stdout = self.stdout\n+\n+    def test_redirect(self):\n+        with LogCapture() as l:\n+            print('test log msg')\n+        l.check(('test', 'ERROR', 'test log msg'))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c174d78f1221d65affcd94b72cfccee8d63cdf1b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 327 | Files Changed: 2 | Hunks: 4 | Methods Changed: 33 | Complexity Δ (Sum/Max): -65/0 | Churn Δ: 334 | Churn Cumulative: 920 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 11 | DMM Complexity: 0.185\n\nDIFF:\n@@ -1,161 +1,10 @@\n \"\"\"\n-Scrapy logging facility\n+This module is kept to provide a helpful warning about its removal.\n+\"\"\"\n \n-See documentation in docs/topics/logging.rst\n-\"\"\"\n-import sys\n-import logging\n import warnings\n-\n-from twisted.python import log\n-\n-import scrapy\n-from scrapy.utils.python import unicode_to_str\n-from scrapy.settings import overridden_settings\n-\n-# Logging levels\n-DEBUG = logging.DEBUG\n-INFO = logging.INFO\n-WARNING = logging.WARNING\n-ERROR = logging.ERROR\n-CRITICAL = logging.CRITICAL\n-SILENT = CRITICAL + 1\n-\n-level_names = {\n-    logging.DEBUG: \"DEBUG\",\n-    logging.INFO: \"INFO\",\n-    logging.WARNING: \"WARNING\",\n-    logging.ERROR: \"ERROR\",\n-    logging.CRITICAL: \"CRITICAL\",\n-    SILENT: \"SILENT\",\n-}\n-\n-class ScrapyFileLogObserver(log.FileLogObserver):\n-\n-    def __init__(self, f, level=INFO, encoding='utf-8', crawler=None):\n-        self.level = level\n-        self.encoding = encoding\n-        self.crawler = crawler\n-        if crawler:\n-            self.emit = self._emit_with_crawler\n-        else:\n-            self.emit = self._emit\n-        log.FileLogObserver.__init__(self, f)\n-\n-    def _emit(self, eventDict):\n-        ev = _adapt_eventdict(eventDict, self.level, self.encoding,\n-                              self.crawler)\n-        if ev is not None:\n-            log.FileLogObserver.emit(self, ev)\n-        return ev\n-\n-    def _emit_with_crawler(self, eventDict):\n-        ev = self._emit(eventDict)\n-        if ev:\n-            level = ev['logLevel']\n-            sname = 'log_count/%s' % level_names.get(level, level)\n-            self.crawler.stats.inc_value(sname)\n-\n-def _adapt_eventdict(eventDict, log_level=INFO, encoding='utf-8',\n-                     crawler=None, prepend_level=True):\n-    \"\"\"Adapt Twisted log eventDict making it suitable for logging with a Scrapy\n-    log observer. It may return None to indicate that the event should be\n-    ignored by a Scrapy log observer.\n-\n-    `log_level` is the minimum level being logged, and `encoding` is the log\n-    encoding.\n-    \"\"\"\n-    ev = eventDict.copy()\n-    if ev['isError']:\n-        ev.setdefault('logLevel', ERROR)\n-\n-    # ignore non-error messages from outside scrapy\n-    if ev.get('system') != 'scrapy' and not ev['isError']:\n-        return\n-\n-    level = ev.get('logLevel')\n-    if level < log_level:\n-        return\n-\n-    spider = ev.get('spider')\n-    if spider:\n-        ev['system'] = unicode_to_str(spider.name, encoding)\n-    if crawler and (not spider or spider.crawler is not crawler):\n-        # ignore events not triggered by own spiders in crawlers' observers\n-        return\n-    if not crawler and spider:\n-        # ignore spiders' events in observers without crawler\n-        return\n-\n-    lvlname = level_names.get(level, 'NOLEVEL')\n-    message = ev.get('message')\n-    if message:\n-        message = [unicode_to_str(x, encoding) for x in message]\n-        if prepend_level:\n-            message[0] = \"%s: %s\" % (lvlname, message[0])\n-        ev['message'] = message\n-\n-    why = ev.get('why')\n-    if why:\n-        why = unicode_to_str(why, encoding)\n-        if prepend_level:\n-            why = \"%s: %s\" % (lvlname, why)\n-        ev['why'] = why\n-\n-    fmt = ev.get('format')\n-    if fmt:\n-        fmt = unicode_to_str(fmt, encoding)\n-        if prepend_level:\n-            fmt = \"%s: %s\" % (lvlname, fmt)\n-        ev['format'] = fmt\n-\n-    return ev\n-\n-def _get_log_level(level_name_or_id):\n-    if isinstance(level_name_or_id, int):\n-        return level_name_or_id\n-    elif isinstance(level_name_or_id, basestring):\n-        return globals()[level_name_or_id]\n-    else:\n-        raise ValueError(\"Unknown log level: %r\" % level_name_or_id)\n-\n-def start(logfile=None, loglevel='INFO', logstdout=True, logencoding='utf-8', crawler=None):\n-    loglevel = _get_log_level(loglevel)\n-    file = open(logfile, 'a') if logfile else sys.stderr\n-    log_observer = ScrapyFileLogObserver(file, loglevel, logencoding, crawler)\n-    _oldshowwarning = warnings.showwarning\n-    log.startLoggingWithObserver(log_observer.emit, setStdout=logstdout)\n-    # restore warnings, wrongly silenced by Twisted\n-    warnings.showwarning = _oldshowwarning\n-    return log_observer\n-\n-def msg(message=None, _level=INFO, **kw):\n-    kw['logLevel'] = kw.pop('level', _level)\n-    kw.setdefault('system', 'scrapy')\n-    if message is None:\n-        log.msg(**kw)\n-    else:\n-        log.msg(message, **kw)\n-\n-def err(_stuff=None, _why=None, **kw):\n-    kw['logLevel'] = kw.pop('level', ERROR)\n-    kw.setdefault('system', 'scrapy')\n-    log.err(_stuff, _why, **kw)\n-\n-def start_from_settings(settings, crawler=None):\n-    if settings.getbool('LOG_ENABLED'):\n-        return start(settings['LOG_FILE'], settings['LOG_LEVEL'], settings['LOG_STDOUT'],\n-            settings['LOG_ENCODING'], crawler)\n-\n-def scrapy_info(settings):\n-    msg(\"Scrapy %s started (bot: %s)\" % (scrapy.__version__,\n-        settings['BOT_NAME']))\n-\n-    msg(\"Optional features available: %s\" % \", \".join(scrapy.optional_features),\n-        level=INFO)\n-\n-    d = dict(overridden_settings(settings))\n-    msg(format=\"Overridden settings: %(settings)r\", settings=d, level=INFO)\n-\n-def start_from_crawler(crawler):\n-    return start_from_settings(crawler.settings, crawler)\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+warnings.warn(\"Module `scrapy.log` has been deprecated, Scrapy now relies on \"\n+              \"the builtin Python library for logging. Read the updated \"\n+              \"logging entry in the documentation to learn more.\",\n+              ScrapyDeprecationWarning, stacklevel=2)\n\n@@ -1,169 +0,0 @@\n-from io import BytesIO\n-\n-from twisted.python import log as txlog, failure\n-from twisted.trial import unittest\n-\n-from scrapy import log\n-from scrapy.spider import Spider\n-from scrapy.settings import default_settings\n-from scrapy.utils.test import get_crawler\n-\n-class LogTest(unittest.TestCase):\n-\n-    def test_get_log_level(self):\n-        default_log_level = getattr(log, default_settings.LOG_LEVEL)\n-        self.assertEqual(log._get_log_level('WARNING'), log.WARNING)\n-        self.assertEqual(log._get_log_level(log.WARNING), log.WARNING)\n-        self.assertRaises(ValueError, log._get_log_level, object())\n-\n-class ScrapyFileLogObserverTest(unittest.TestCase):\n-\n-    level = log.INFO\n-    encoding = 'utf-8'\n-\n-    def setUp(self):\n-        self.f = BytesIO()\n-        self.log_observer = log.ScrapyFileLogObserver(self.f, self.level, self.encoding)\n-        self.log_observer.start()\n-\n-    def tearDown(self):\n-        self.flushLoggedErrors()\n-        self.log_observer.stop()\n-\n-    def logged(self):\n-        return self.f.getvalue().strip()[25:]\n-\n-    def first_log_line(self):\n-        logged = self.logged()\n-        return logged.splitlines()[0] if logged else ''\n-\n-    def test_msg_basic(self):\n-        log.msg(\"Hello\")\n-        self.assertEqual(self.logged(), \"[scrapy] INFO: Hello\")\n-\n-    def test_msg_ignore_spider(self):\n-        spider = Spider(\"myspider\")\n-        log.msg(\"Hello\", spider=spider)\n-        self.failIf(self.logged())\n-\n-    def test_msg_level1(self):\n-        log.msg(\"Hello\", level=log.WARNING)\n-        self.assertEqual(self.logged(), \"[scrapy] WARNING: Hello\")\n-\n-    def test_msg_level2(self):\n-        log.msg(\"Hello\", log.WARNING)\n-        self.assertEqual(self.logged(), \"[scrapy] WARNING: Hello\")\n-\n-    def test_msg_wrong_level(self):\n-        log.msg(\"Hello\", level=9999)\n-        self.assertEqual(self.logged(), \"[scrapy] NOLEVEL: Hello\")\n-\n-    def test_msg_encoding(self):\n-        log.msg(u\"Price: \\xa3100\")\n-        self.assertEqual(self.logged(), \"[scrapy] INFO: Price: \\xc2\\xa3100\")\n-\n-    def test_msg_ignore_level(self):\n-        log.msg(\"Hello\", level=log.DEBUG)\n-        log.msg(\"World\", level=log.INFO)\n-        self.assertEqual(self.logged(), \"[scrapy] INFO: World\")\n-\n-    def test_msg_ignore_system(self):\n-        txlog.msg(\"Hello\")\n-        self.failIf(self.logged())\n-\n-    def test_msg_ignore_system_err(self):\n-        txlog.msg(\"Hello\")\n-        self.failIf(self.logged())\n-\n-    def test_err_noargs(self):\n-        try:\n-            a = 1/0\n-        except:\n-            log.err()\n-        self.assertIn('Traceback', self.logged())\n-        self.assertIn('ZeroDivisionError', self.logged())\n-\n-    def test_err_why(self):\n-        log.err(TypeError(\"bad type\"), \"Wrong type\")\n-        self.assertEqual(self.first_log_line(), \"[scrapy] ERROR: Wrong type\")\n-        self.assertIn('TypeError', self.logged())\n-        self.assertIn('bad type', self.logged())\n-\n-    def test_error_outside_scrapy(self):\n-        \"\"\"Scrapy logger should still print outside errors\"\"\"\n-        txlog.err(TypeError(\"bad type\"), \"Wrong type\")\n-        self.assertEqual(self.first_log_line(), \"[-] ERROR: Wrong type\")\n-        self.assertIn('TypeError', self.logged())\n-        self.assertIn('bad type', self.logged())\n-\n-# this test fails in twisted trial observer, not in scrapy observer\n-#    def test_err_why_encoding(self):\n-#        log.err(TypeError(\"bad type\"), u\"\\xa3\")\n-#        self.assertEqual(self.first_log_line(), \"[scrapy] ERROR: \\xc2\\xa3\")\n-\n-    def test_err_exc(self):\n-        log.err(TypeError(\"bad type\"))\n-        self.assertIn('Unhandled Error', self.logged())\n-        self.assertIn('TypeError', self.logged())\n-        self.assertIn('bad type', self.logged())\n-\n-    def test_err_failure(self):\n-        log.err(failure.Failure(TypeError(\"bad type\")))\n-        self.assertIn('Unhandled Error', self.logged())\n-        self.assertIn('TypeError', self.logged())\n-        self.assertIn('bad type', self.logged())\n-\n-\n-class Latin1ScrapyFileLogObserverTest(ScrapyFileLogObserverTest):\n-\n-    encoding = 'latin-1'\n-\n-    def test_msg_encoding(self):\n-        log.msg(u\"Price: \\xa3100\")\n-        logged = self.f.getvalue().strip()[25:]\n-        self.assertEqual(self.logged(), \"[scrapy] INFO: Price: \\xa3100\")\n-\n-# this test fails in twisted trial observer, not in scrapy observer\n-#    def test_err_why_encoding(self):\n-#        log.err(TypeError(\"bad type\"), u\"\\xa3\")\n-#        self.assertEqual(self.first_log_line(), \"[scrapy] ERROR: \\xa3\")\n-\n-\n-class CrawlerScrapyFileLogObserverTest(unittest.TestCase):\n-\n-    def setUp(self):\n-        self.f = BytesIO()\n-        self.crawler = get_crawler(Spider)\n-        self.spider = self.crawler.spider = self.crawler._create_spider('test')\n-        self.log_observer = log.ScrapyFileLogObserver(self.f, log.INFO,\n-                                                      'utf-8', self.crawler)\n-        self.log_observer.start()\n-\n-    def tearDown(self):\n-        self.flushLoggedErrors()\n-        self.log_observer.stop()\n-\n-    def logged(self):\n-        return self.f.getvalue().strip()[25:]\n-\n-    def test_msg_basic(self):\n-        log.msg(\"Hello\", spider=self.spider)\n-        self.assertEqual(self.logged(), \"[test] INFO: Hello\")\n-\n-    def test_msg_ignore_scrapy_channel(self):\n-        log.msg(\"Hello\")\n-        self.failIf(self.logged())\n-\n-    def test_msg_ignore_another_crawler(self):\n-        crawler = get_crawler(Spider)\n-        log.msg(\"Hello\", spider=crawler._create_spider('test'))\n-        self.failIf(self.logged())\n-\n-    def test_msg_stats_log(self):\n-        assert self.crawler.stats.get_value('log_count/INFO', 0) == 0\n-        log.msg(\"Hello\", spider=self.spider)\n-        self.assertEqual(self.crawler.stats.get_value('log_count/INFO'), 1)\n-\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
