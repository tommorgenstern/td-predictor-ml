{"custom_id": "scrapy#566758eeb7c4033b340b3d2f409725e4cfa2a1b5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 5 | Churn Cumulative: 42 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -21,8 +21,6 @@ class Command(ScrapyCommand):\n         ScrapyCommand.add_options(self, parser)\n         parser.add_option(\"--nopipeline\", dest=\"nopipeline\", action=\"store_true\", \\\n             help=\"disable scraped item pipeline\")\n-        parser.add_option(\"--restrict\", dest=\"restrict\", action=\"store_true\", \\\n-            help=\"restrict crawling only to the given urls\")\n         parser.add_option(\"-n\", \"--nofollow\", dest=\"nofollow\", action=\"store_true\", \\\n             help=\"don't follow links (for use with URLs only)\")\n         parser.add_option(\"-c\", \"--callback\", dest=\"callback\", action=\"store\", \\\n@@ -33,9 +31,6 @@ class Command(ScrapyCommand):\n         if opts.nopipeline:\n             settings.overrides['ITEM_PIPELINES'] = []\n \n-        if opts.restrict:\n-            settings.overrides['RESTRICT_TO_URLS'] = args\n-\n         if opts.nofollow:\n             settings.overrides['CRAWLSPIDER_FOLLOW_LINKS'] = False\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#924219dd5a2c272fd5e010d917e46916c81d763b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 47 | Lines Deleted: 80 | Files Changed: 3 | Hunks: 11 | Methods Changed: 9 | Complexity Δ (Sum/Max): -2/4 | Churn Δ: 127 | Churn Cumulative: 484 | Contributors (this commit): 2 | Commits (past 90d): 20 | Contributors (cumulative): 6 | DMM Complexity: 0.05263157894736842\n\nDIFF:\n@@ -10,10 +10,18 @@ from scrapy import log\n from scrapy.spider import spiders\n from scrapy.xlib import lsprofcalltree\n from scrapy.conf import settings\n+from scrapy.command.models import ScrapyCommand\n \n # This dict holds information about the executed command for later use\n command_executed = {}\n \n+def save_command_executed(cmdname, cmd, args, opts):\n+    \"\"\"Save command executed info for later reference\"\"\"\n+    command_executed['name'] = cmdname\n+    command_executed['class'] = cmd\n+    command_executed['args'] = args[:]\n+    command_executed['opts'] = opts.__dict__.copy()\n+\n def find_commands(dir):\n     try:\n         return [f[:-3] for f in os.listdir(dir) if not f.startswith('_') and \\\n@@ -45,20 +53,26 @@ def get_command_name(argv):\n         if not arg.startswith('-'):\n             return arg\n \n-def usage():\n-    s = \"Usage\\n\"\n-    s += \"=====\\n\"\n-    s += \"scrapy-ctl.py <command> [options] [args]\\n\"\n-    s += \"  Run a command\\n\\n\"\n-    s += \"scrapy-ctl.py <command> -h\\n\"\n-    s += \"  Print command help and options\\n\\n\"\n-    s += \"Available commands\\n\"\n-    s += \"===================\\n\"\n+def print_usage(inside_project):\n+    if inside_project:\n+        print \"Scrapy %s - project: %s\\n\" % (scrapy.__version__, \\\n+            settings['BOT_NAME'])\n+    else:\n+        print \"Scrapy %s - no active project\\n\" % scrapy.__version__\n+    print \"Usage\"\n+    print \"=====\\n\"\n+    print \"To run a command:\"\n+    print \"  scrapy-ctl.py <command> [options] [args]\\n\"\n+    print \"To get help:\"\n+    print \"  scrapy-ctl.py <command> -h\\n\"\n+    print \"Available commands\"\n+    print \"==================\\n\"\n     cmds = get_commands_dict()\n     for cmdname, cmdclass in sorted(cmds.iteritems()):\n-        s += \"%s %s\\n\" % (cmdname, cmdclass.syntax())\n-        s += \"  %s\\n\" % cmdclass.short_desc()\n-    return s\n+        if inside_project or not cmdclass.requires_project:\n+            print \"%s %s\" % (cmdname, cmdclass.syntax())\n+            print \"  %s\" % cmdclass.short_desc()\n+    print\n \n def update_default_settings(module, cmdname):\n     if not module:\n@@ -82,44 +96,38 @@ def execute(argv=None):\n     update_default_settings('scrapy.conf.commands', cmdname)\n     update_default_settings(settings['COMMANDS_SETTINGS_MODULE'], cmdname)\n \n-    if not cmdname:\n-        print \"Scrapy %s\\n\" % scrapy.__version__\n-        print usage()\n-        sys.exit(2)\n-\n     parser = optparse.OptionParser(formatter=optparse.TitledHelpFormatter(), \\\n         conflict_handler='resolve', add_help_option=False)\n \n     if cmdname in cmds:\n         cmd = cmds[cmdname]\n         cmd.add_options(parser)\n+        opts, args = parser.parse_args(args=argv[1:])\n+        cmd.process_options(args, opts)\n         parser.usage = \"%%prog %s %s\" % (cmdname, cmd.syntax())\n         parser.description = cmd.long_desc()\n-    else:\n-        print \"Scrapy %s\\n\" % scrapy.__version__\n-        print \"Unknown command: %s\\n\" % cmdname\n-        print 'Type \"%s -h\" for help' % argv[0]\n-        sys.exit(2)\n-\n-    (opts, args) = parser.parse_args(args=argv[1:])\n-    del args[0]  # args[0] is cmdname\n-\n-    if opts.help:\n-        parser.print_help()\n-        sys.exit()\n-\n-    # storing command executed info for later reference\n-    command_executed['name'] = cmdname\n-    command_executed['class'] = cmd\n-    command_executed['args'] = args[:]\n-    command_executed['opts'] = opts.__dict__.copy()\n-\n-    cmd.process_options(args, opts)\n         if cmd.requires_project and not settings.settings_module:\n             print \"Error running: scrapy-ctl.py %s\\n\" % cmdname\n             print \"Cannot find project settings module in python path: %s\" % \\\n                 settings.settings_module_path\n             sys.exit(1)\n+        if opts.help:\n+            parser.print_help()\n+            sys.exit()\n+    elif not cmdname:\n+        cmd = ScrapyCommand()\n+        cmd.add_options(parser)\n+        opts, args = parser.parse_args(args=argv)\n+        cmd.process_options(args, opts)\n+        print_usage(settings.settings_module)\n+        sys.exit(2)\n+    else:\n+        print \"Unknown command: %s\\n\" % cmdname\n+        print 'Use \"scrapy-ctl.py -h\" for help' \n+        sys.exit(2)\n+\n+    del args[0]  # remove command name from args\n+    save_command_executed(cmdname, cmd, args, opts)\n     spiders.load()\n     log.start()\n     ret = run_command(cmd, args, opts)\n\n@@ -1,40 +0,0 @@\n-import textwrap\n-\n-from scrapy.command import ScrapyCommand, cmdline\n-\n-class Command(ScrapyCommand):\n-\n-    requires_project = False\n-\n-    def syntax(self):\n-        return \"<command>\"\n-    \n-    def short_desc(self):\n-        return \"Provides extended help for the given command\"\n-\n-    def run(self, args, opts):\n-        if not args:\n-            return False\n-\n-        commands = cmdline.get_commands_dict()\n-\n-        cmdname = args[0]\n-        if cmdname in commands:\n-            cmd = commands[cmdname]\n-            help = getattr(cmd, 'help', None) or getattr(cmd, 'long_desc', None)\n-            title = \"%s command\" % cmdname\n-            print title\n-            print \"-\" * len(title)\n-            print\n-            print cmd.short_desc()\n-            print\n-            print \"usage: %s %s\" % (cmdname, cmd.syntax())\n-            print\n-            print \"\\n\".join(textwrap.wrap(help()))\n-            print\n-            print \"For a list of supported arguments use:\"\n-            print\n-            print \"   scrapy-ctl.py %s -h\" % cmdname\n-            print\n-        else:\n-            print \"Unknown command: %s\" % cmdname\n\n@@ -12,14 +12,13 @@ class Command(ScrapyCommand):\n     requires_project = False\n \n     def syntax(self):\n-        return \"[url]\"\n+        return \"[url|file]\"\n \n     def short_desc(self):\n         return \"Interactive scraping console\"\n \n     def long_desc(self):\n-        return \"Interactive console for scraping the given url. For scraping \" \\\n-            \"local files you can use a URL like file://path/to/file.html\"\n+        return \"Interactive console for scraping the given url\"\n \n     def update_vars(self, vars):\n         \"\"\"You can use this function to update the Scrapy objects that will be\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#60cbf24c89acb961cf3514592468a3c074d4d0c2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 37 | Files Changed: 5 | Hunks: 13 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 63 | Churn Cumulative: 316 | Contributors (this commit): 3 | Commits (past 90d): 17 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,14 +2,11 @@\n \n import googledir\n \n-PROJECT_NAME = 'googledir'\n-\n-BOT_NAME = PROJECT_NAME\n+BOT_NAME = 'googledir'\n BOT_VERSION = '1.0'\n \n SPIDER_MODULES = ['googledir.spiders']\n NEWSPIDER_MODULE = 'googledir.spiders'\n-TEMPLATES_DIR = '%s/templates' % googledir.__path__[0]\n DEFAULT_ITEM_CLASS = 'scrapy.item.ScrapedItem'\n USER_AGENT = '%s/%s' % (BOT_NAME, BOT_VERSION)\n \n\n@@ -72,8 +72,8 @@ class Command(ScrapyCommand):\n     def _genspider(self, module, domain, template_name, template_file):\n         \"\"\"Generate the spider module, based on the given template\"\"\"\n         tvars = {\n-            'project_name': settings.get('PROJECT_NAME'),\n-            'ProjectName': string_camelcase(settings.get('PROJECT_NAME')),\n+            'project_name': settings.get('BOT_NAME'),\n+            'ProjectName': string_camelcase(settings.get('BOT_NAME')),\n             'module': module,\n             'site': domain,\n             'classname': '%sSpider' % ''.join([s.capitalize() \\\n\n@@ -1,20 +1,17 @@\n-#!/usr/bin/env python\n-\n import sys\n-import os\n import string\n import re\n+import shutil\n+from os.path import join, exists\n \n import scrapy\n from scrapy.command import ScrapyCommand\n from scrapy.utils.template import render_templatefile, string_camelcase\n from scrapy.utils.python import ignore_patterns, copytree\n \n-PROJECT_TEMPLATES_PATH = os.path.join(scrapy.__path__[0], 'templates/project')\n+TEMPLATES_PATH = join(scrapy.__path__[0], 'templates', 'project')\n \n-# This is the list of templatefile's path that are rendered *after copying* to\n-# the new project directory.\n-TEMPLATES = (\n+TEMPLATES_TO_RENDER = (\n     ('scrapy-ctl.py',),\n     ('${project_name}', 'settings.py.tmpl'),\n     ('${project_name}', 'items.py.tmpl'),\n@@ -36,33 +33,21 @@ class Command(ScrapyCommand):\n     def run(self, args, opts):\n         if len(args) != 1:\n             return False\n-\n         project_name = args[0]\n-        if not re.search(r'^[_a-zA-Z]\\w*$', project_name): # If it's not a valid directory name.\n-            # Provide a smart error message, depending on the error.\n-            if not re.search(r'^[a-zA-Z]', project_name):\n-                message = 'Project names must begin with a letter'\n-            else:\n-                message = 'Project names must contain only letters, numbers and underscores'\n-            print \"Invalid project name: %s\\n\\n%s\" % (project_name, message)\n+        if not re.search(r'^[_a-zA-Z]\\w*$', project_name):\n+            print 'Error: Project names must begin with a letter and contain only\\n' \\\n+                'letters, numbers and underscores'\n             sys.exit(1)\n-        else:\n-            if os.path.exists(project_name):\n-                print \"%s dir already exists\" % project_name\n+        elif exists(project_name):\n+            print \"Error: directory %r already exists\" % project_name\n             sys.exit(1)\n \n-            project_root_path = project_name\n-\n-            roottpl = os.path.join(PROJECT_TEMPLATES_PATH, 'root')\n-            copytree(roottpl, project_name, ignore=IGNORE)\n-\n-            moduletpl = os.path.join(PROJECT_TEMPLATES_PATH, 'module')\n-            copytree(moduletpl, '%s/%s' % (project_name, project_name),\n-                ignore=IGNORE)\n-\n-            for paths in TEMPLATES:\n-                path = os.path.join(*paths)\n-                tplfile = os.path.join(project_root_path,\n+        moduletpl = join(TEMPLATES_PATH, 'module')\n+        copytree(moduletpl, join(project_name, project_name), ignore=IGNORE)\n+        shutil.copy(join(TEMPLATES_PATH, 'scrapy-ctl.py'), project_name)\n+        for paths in TEMPLATES_TO_RENDER:\n+            path = join(*paths)\n+            tplfile = join(project_name,\n                 string.Template(path).substitute(project_name=project_name))\n             render_templatefile(tplfile, project_name=project_name,\n                 ProjectName=string_camelcase(project_name))\n\n@@ -1 +1,8 @@\n-# Place here all your scrapy spiders\n+# This package will contain the spiders of your Scrapy project\n+#\n+# To create the first spider for your project use this command:\n+#\n+#   scrapy-ctl.py genspider myspider myspider-domain.com\n+#\n+# For more info see:\n+# http://doc.scrapy.org/topics/spiders.html\n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6e028fd91b8e039c2f4b60cb98f23eab78f983fe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 529 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -122,7 +122,7 @@ class XmlItemExporterTest(BaseItemExporterTest):\n \n class JsonLinesItemExporterTest(BaseItemExporterTest):\n \n-    def _get_exporter(self, **kwargs):\n+    def setUp(self):\n         try:\n             import json\n         except ImportError:\n@@ -130,12 +130,15 @@ class JsonLinesItemExporterTest(BaseItemExporterTest):\n                 import simplejson\n             except ImportError:\n                 raise unittest.SkipTest(\"simplejson module not available\")\n+        super(JsonLinesItemExporterTest, self).setUp()\n+\n+    def _get_exporter(self, **kwargs):\n         from scrapy.contrib.exporter.jsonlines import JsonLinesItemExporter\n         return JsonLinesItemExporter(self.output, **kwargs)\n \n     def _check_output(self):\n-        import simplejson\n-        exported = simplejson.loads(self.output.getvalue().strip())\n+        from scrapy.contrib.exporter.jsonlines import json\n+        exported = json.loads(self.output.getvalue().strip())\n         self.assertEqual(exported, dict(self.i))\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5ba466a6e1e7f61ab2555e3aa64e9dfeccda2408", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 79 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -6,10 +6,14 @@ from scrapy.contrib_exp.djangoitem import DjangoItem, Field\n os.environ['DJANGO_SETTINGS_MODULE'] = 'scrapy.tests.test_djangoitem.settings'\n \n try:\n-    from models import Person\n-    django = True\n+    import django\n except ImportError:\n-    django = False\n+    django = None\n+\n+if django:\n+    from models import Person\n+else:\n+    Person = None\n \n \n class BasePersonItem(DjangoItem):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#91ee71222f1e8f549884dc29424c8f66a5cbf12a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 57 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -15,17 +15,17 @@ class ProjectTest(unittest.TestCase):\n         self.cwd = self.temp_path\n         self.proj_path = join(self.temp_path, self.project_name)\n         self.proj_mod_path = join(self.proj_path, self.project_name)\n+        self.env = os.environ.copy()\n \n     def tearDown(self):\n         rmtree(self.temp_path)\n \n     def call(self, new_args, **kwargs):\n         out = os.tmpfile()\n-        env = getattr(self, 'env', os.environ)\n         args = [sys.executable, '-m', 'scrapy.command.cmdline']\n         args.extend(new_args)\n         return subprocess.call(args, stdout=out, stderr=out, cwd=self.cwd, \\\n-            env=env, **kwargs)\n+            env=self.env, **kwargs)\n \n \n class StartprojectTest(ProjectTest):\n@@ -53,12 +53,8 @@ class CommandTest(ProjectTest):\n \n     def setUp(self):\n         super(CommandTest, self).setUp()\n-\n         self.call(['startproject', self.project_name])\n-\n         self.cwd = join(self.temp_path, self.project_name)\n-\n-        self.env = os.environ.copy()\n         self.env.pop('SCRAPY_SETTINGS_DISABLED', None)\n         self.env['SCRAPY_SETTINGS_MODULE'] = '%s.settings' % self.project_name\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9dc7a3a37318322668dbe457a2363f323a77f319", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 27 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -50,7 +50,8 @@ def start(logfile=None, loglevel=None, logstdout=None):\n     # set log observer\n     if log.defaultObserver: # check twisted log not already started\n         logfile = logfile or settings['LOG_FILE'] or settings['LOGFILE']\n-        logstdout = logstdout or settings.getbool('LOG_STDOUT')\n+        if logstdout is None:\n+            logstdout = settings.getbool('LOG_STDOUT')\n \n         file = open(logfile, 'a') if logfile else sys.stderr\n         log.startLogging(file, setStdout=logstdout)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#884f0c878f04be741022d0914e2f28ecd8be3481", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 17 | Files Changed: 3 | Hunks: 13 | Methods Changed: 9 | Complexity Δ (Sum/Max): 3/1 | Churn Δ: 43 | Churn Cumulative: 287 | Contributors (this commit): 2 | Commits (past 90d): 19 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -17,6 +17,7 @@ class StatsCollector(object):\n     def __init__(self):\n         self._dump = settings.getbool('STATS_DUMP')\n         self._stats = {None: {}} # None is for global stats\n+        dispatcher.connect(self._engine_stopped, signal=signals.engine_stopped)\n \n     def get_value(self, key, default=None, domain=None):\n         return self._stats[domain].get(key, default)\n@@ -60,11 +61,16 @@ class StatsCollector(object):\n         if self._dump:\n             log.msg(\"Dumping domain stats:\\n\" + pprint.pformat(stats), \\\n                 domain=domain)\n+        self._persist_stats(stats, domain)\n \n-    def engine_stopped(self):\n+    def _engine_stopped(self):\n+        stats = self.get_stats()\n         if self._dump:\n-            log.msg(\"Dumping global stats:\\n\" + pprint.pformat(self.get_stats()))\n+            log.msg(\"Dumping global stats:\\n\" + pprint.pformat(stats))\n+        self._persist_stats(stats, domain=None)\n \n+    def _persist_stats(self, stats, domain=None):\n+        pass\n \n class MemoryStatsCollector(StatsCollector):\n \n@@ -72,9 +78,8 @@ class MemoryStatsCollector(StatsCollector):\n         super(MemoryStatsCollector, self).__init__()\n         self.domain_stats = {}\n         \n-    def close_domain(self, domain, reason):\n-        self.domain_stats[domain] = self._stats[domain]\n-        super(MemoryStatsCollector, self).close_domain(domain, reason)\n+    def _persist_stats(self, stats, domain=None):\n+        self.domain_stats[domain] = stats\n \n \n class DummyStatsCollector(StatsCollector):\n\n@@ -16,14 +16,16 @@ class MysqlStatsCollector(StatsCollector):\n         mysqluri = settings['STATS_MYSQL_URI']\n         self._mysql_conn = mysql_connect(mysqluri, use_unicode=False) if mysqluri else None\n         \n-    def close_domain(self, domain, reason):\n-        if self._mysql_conn:\n+    def _persist_stats(self, stats, domain=None):\n+        if domain is None: # only store domain-specific stats\n+            return\n+        if self._mysql_conn is None:\n+            return\n         stored = datetime.utcnow()\n-            datas = pickle.dumps(self._stats[domain])\n+        datas = pickle.dumps(stats)\n         table = 'domain_data_history'\n \n         c = self._mysql_conn.cursor()\n         c.execute(\"INSERT INTO %s (domain,stored,data) VALUES (%%s,%%s,%%s)\" % table, \\\n             (domain, stored, datas))\n         self._mysql_conn.commit()\n-        super(MysqlStatsCollector, self).close_domain(domain, reason)\n\n@@ -7,6 +7,7 @@ Requires the boto library: http://code.google.com/p/boto/\n from datetime import datetime\n \n import boto\n+from twisted.internet import threads\n \n from scrapy.stats.collector import StatsCollector\n from scrapy import log\n@@ -21,16 +22,17 @@ class SimpledbStatsCollector(StatsCollector):\n         sdb = boto.connect_sdb()\n         sdb.create_domain(self._sdbdomain)\n         \n-    def close_domain(self, domain, reason):\n-        if self._sdbdomain:\n+    def _persist_stats(self, stats, domain=None):\n+        if domain is None: # only store domain-specific stats\n+            return\n+        if not self._sdbdomain:\n+            return\n         if self._async:\n-                from twisted.internet import threads\n-                dfd = threads.deferToThread(self._persist_to_sdb, domain, \\\n-                    self._stats[domain].copy())\n-                dfd.addErrback(log.err, 'Error uploading stats to SimpleDB', domain=domain)\n+            dfd = threads.deferToThread(self._persist_to_sdb, domain, stats.copy())\n+            dfd.addErrback(log.err, 'Error uploading stats to SimpleDB', \\\n+                domain=domain)\n         else:\n-                self._persist_to_sdb(domain, self._stats[domain])\n-        super(SimpledbStatsCollector, self).close_domain(domain, reason)\n+            self._persist_to_sdb(domain, stats)\n \n     def _persist_to_sdb(self, domain, stats):\n         ts = datetime.utcnow().isoformat()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7cb546ba28a53b50969db384e79dc6bb394de4ad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 81 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -11,7 +11,7 @@ except ImportError:\n     django = None\n \n if django:\n-    from models import Person\n+    from .models import Person\n else:\n     Person = None\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#719fc2e7df4ca70ebdd7076854e7cf92c49ee9a8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 9 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 28 | Churn Cumulative: 107 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -6,21 +6,21 @@ Requires the boto library: http://code.google.com/p/boto/\n \n from datetime import datetime\n \n-import boto\n+from boto import connect_sdb\n from twisted.internet import threads\n \n from scrapy.stats.collector import StatsCollector\n from scrapy import log\n from scrapy.conf import settings\n \n+\n class SimpledbStatsCollector(StatsCollector):\n \n     def __init__(self):\n         super(SimpledbStatsCollector, self).__init__()\n         self._sdbdomain = settings['STATS_SDB_DOMAIN']\n         self._async = settings.getbool('STATS_SDB_ASYNC')\n-        sdb = boto.connect_sdb()\n-        sdb.create_domain(self._sdbdomain)\n+        connect_sdb().create_domain(self._sdbdomain)\n \n     def _persist_stats(self, stats, domain=None):\n         if domain is None: # only store domain-specific stats\n@@ -35,21 +35,27 @@ class SimpledbStatsCollector(StatsCollector):\n             self._persist_to_sdb(domain, stats)\n \n     def _persist_to_sdb(self, domain, stats):\n-        ts = datetime.utcnow().isoformat()\n+        ts = self._get_timestamp(domain).isoformat()\n         sdb_item_id = \"%s_%s\" % (domain, ts)\n-        sdb_item = dict([(k, self._to_sdb_value(v)) for k, v in stats.iteritems()])\n+        sdb_item = dict((k, self._to_sdb_value(v, k)) for k, v in stats.iteritems())\n         sdb_item['domain'] = domain\n         sdb_item['timestamp'] = self._to_sdb_value(ts)\n-        sdb = boto.connect_sdb()\n-        sdb.batch_put_attributes(self._sdbdomain, {sdb_item_id: sdb_item})\n+        connect_sdb().put_attributes(self._sdbdomain, sdb_item_id, sdb_item)\n \n-    def _to_sdb_value(self, obj):\n-        if isinstance(obj, int):\n+    def _get_timestamp(self, domain):\n+        return datetime.utcnow()\n+\n+    def _to_sdb_value(self, obj, ref=None):\n+        if isinstance(obj, bool):\n+            return u'%d' % obj\n+        elif isinstance(obj, (int, long)):\n             return \"%016d\" % obj\n         elif isinstance(obj, datetime):\n             return obj.isoformat()\n         elif isinstance(obj, basestring):\n             return obj\n+        elif obj is None:\n+            return u''\n         else:\n-            raise TypeError(\"SimpledbStatsCollector unsupported type: %s\" % \\\n-                type(obj).__name__)\n+            raise TypeError(\"%s unsupported type '%s' referenced as '%s'\" % \\\n+                (type(self).__name__, type(obj).__name__, ref))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8ef2cf0add448b1893e1e42b0167b1956ba3cd51", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 27 | Files Changed: 1 | Hunks: 9 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 40 | Churn Cumulative: 97 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -20,7 +20,7 @@ class ProjectTest(unittest.TestCase):\n     def tearDown(self):\n         rmtree(self.temp_path)\n \n-    def call(self, new_args, **kwargs):\n+    def call(self, *new_args, **kwargs):\n         out = os.tmpfile()\n         args = [sys.executable, '-m', 'scrapy.command.cmdline']\n         args.extend(new_args)\n@@ -31,8 +31,7 @@ class ProjectTest(unittest.TestCase):\n class StartprojectTest(ProjectTest):\n \n     def test_startproject(self):\n-        ret = self.call(['startproject', self.project_name])\n-        self.assertEqual(ret, 0)\n+        self.assertEqual(0, self.call('startproject', self.project_name))\n \n         assert exists(join(self.proj_path, 'scrapy-ctl.py'))\n         assert exists(join(self.proj_path, 'testproject'))\n@@ -42,18 +41,15 @@ class StartprojectTest(ProjectTest):\n         assert exists(join(self.proj_mod_path, 'settings.py'))\n         assert exists(join(self.proj_mod_path, 'spiders', '__init__.py'))\n \n-        ret = self.call(['startproject', self.project_name])\n-        self.assertEqual(ret, 1)\n-\n-        ret = self.call(['startproject', 'wrong---project---name'])\n-        self.assertEqual(ret, 1)\n+        self.assertEqual(1, self.call('startproject', self.project_name))\n+        self.assertEqual(1, self.call('startproject', 'wrong---project---name'))\n \n \n class CommandTest(ProjectTest):\n \n     def setUp(self):\n         super(CommandTest, self).setUp()\n-        self.call(['startproject', self.project_name])\n+        self.call('startproject', self.project_name)\n         self.cwd = join(self.temp_path, self.project_name)\n         self.env.pop('SCRAPY_SETTINGS_DISABLED', None)\n         self.env['SCRAPY_SETTINGS_MODULE'] = '%s.settings' % self.project_name\n@@ -62,35 +58,25 @@ class CommandTest(ProjectTest):\n class MiscCommandsTest(CommandTest):\n \n     def test_crawl(self):\n-        ret = self.call(['crawl'])\n-        self.assertEqual(ret, 0)\n+        self.assertEqual(0, self.call('crawl'))\n \n     def test_genspider_subcommands(self):\n-        ret = self.call(['genspider', '--list'])\n-        self.assertEqual(ret, 0)\n-\n-        ret = self.call(['genspider', '--dump'])\n-        self.assertEqual(ret, 0)\n-\n-        ret = self.call(['genspider', '--dump', '--template=basic'])\n-        self.assertEqual(ret, 0)\n+        self.assertEqual(0, self.call('genspider', '--list'))\n+        self.assertEqual(0, self.call('genspider', '--dump'))\n+        self.assertEqual(0, self.call('genspider', '--dump', '--template=basic'))\n \n     def test_list(self):\n-        ret = self.call(['list'])\n-        self.assertEqual(ret, 0)\n+        self.assertEqual(0, self.call('list'))\n \n \n class BaseGenspiderTest(CommandTest):\n     template = 'basic'\n \n     def test_genspider(self):\n-        ret = self.call(['genspider', 'testspider', 'test.com',\n-                          '--template=%s' % self.template])\n-        self.assertEqual(ret, 0)\n+        self.assertEqual(0, self.call('genspider', 'testspider', 'test.com', \\\n+                '--template=%s' % self.template))\n         assert exists(join(self.proj_mod_path, 'spiders', 'testspider.py'))\n-\n-        ret = self.call(['genspider', 'otherspider', 'test.com'])\n-        self.assertEqual(ret, 1)\n+        self.assertEqual(1, self.call('genspider', 'otherspider', 'test.com'))\n \n \n class CrawlGenspiderTest(BaseGenspiderTest):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1f50c24b6cde3353d6db24a049e932a96078cf2d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 100 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -22,8 +22,7 @@ class ProjectTest(unittest.TestCase):\n \n     def call(self, *new_args, **kwargs):\n         out = os.tmpfile()\n-        args = [sys.executable, '-m', 'scrapy.command.cmdline']\n-        args.extend(new_args)\n+        args = (sys.executable, '-m', 'scrapy.command.cmdline') + new_args\n         return subprocess.call(args, stdout=out, stderr=out, cwd=self.cwd, \\\n             env=self.env, **kwargs)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#062761166c7fe2e51e0696021f93401f065e5374", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 47 | Lines Deleted: 17 | Files Changed: 3 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/6 | Churn Δ: 64 | Churn Cumulative: 171 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,16 +4,14 @@ A Stats collector for persisting stats to Amazon SimpleDB.\n Requires the boto library: http://code.google.com/p/boto/\n \"\"\"\n \n-from datetime import datetime\n-\n from boto import connect_sdb\n from twisted.internet import threads\n \n+from scrapy.utils.simpledb import to_sdb_value\n from scrapy.stats.collector import StatsCollector\n from scrapy import log\n from scrapy.conf import settings\n \n-\n class SimpledbStatsCollector(StatsCollector):\n \n     def __init__(self):\n@@ -45,17 +43,9 @@ class SimpledbStatsCollector(StatsCollector):\n     def _get_timestamp(self, domain):\n         return datetime.utcnow()\n \n-    def _to_sdb_value(self, obj, ref=None):\n-        if isinstance(obj, bool):\n-            return u'%d' % obj\n-        elif isinstance(obj, (int, long)):\n-            return \"%016d\" % obj\n-        elif isinstance(obj, datetime):\n-            return obj.isoformat()\n-        elif isinstance(obj, basestring):\n-            return obj\n-        elif obj is None:\n-            return u''\n-        else:\n-            raise TypeError(\"%s unsupported type '%s' referenced as '%s'\" % \\\n-                (type(self).__name__, type(obj).__name__, ref))\n+    def _to_sdb_value(self, obj, key=None):\n+        try:\n+            return to_sdb_value(obj)\n+        except TypeError:\n+            raise TypeError(\"%s unsupported type %r used in key %r\" % \\\n+                (type(self).__name__, type(obj).__name__, key))\n\n@@ -0,0 +1,21 @@\n+import unittest\n+from datetime import datetime\n+\n+from scrapy.utils.simpledb import to_sdb_value\n+\n+class SimpleddbUtilsTest(unittest.TestCase):\n+\n+    def test_to_sdb_value(self):\n+        self.assertEqual(to_sdb_value(123), u'0000000000000123')\n+        self.assertEqual(to_sdb_value(123L), u'0000000000000123')\n+        self.assertEqual(to_sdb_value(True), u'1')\n+        self.assertEqual(to_sdb_value(False), u'0')\n+        self.assertEqual(to_sdb_value(None), u'')\n+        self.assertEqual(to_sdb_value(datetime(2009, 01, 01, 10, 10, 10)), \\\n+            u'2009-01-01T10:10:10')\n+        self.assertEqual(to_sdb_value('test'), 'test')\n+        self.assertEqual(to_sdb_value(u'test'), u'test')\n+        self.assertRaises(TypeError, to_sdb_value, object())\n+\n+if __name__ == \"__main__\":\n+    unittest.main()\n\n@@ -0,0 +1,19 @@\n+\"\"\"Helper functions for Amazon SimpleDB\"\"\"\n+\n+from datetime import datetime\n+\n+def to_sdb_value(obj):\n+    \"\"\"Convert the given object to proper value to store in Amazon SimpleDB\"\"\"\n+    if isinstance(obj, bool):\n+        return u'%d' % obj\n+    elif isinstance(obj, (int, long)):\n+        return \"%016d\" % obj\n+    elif isinstance(obj, datetime):\n+        return obj.isoformat()\n+    elif isinstance(obj, basestring):\n+        return obj\n+    elif obj is None:\n+        return u''\n+    else:\n+        raise TypeError(\"Unsupported Type: %s\" % type(obj).__name__)\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bcf81cd3c40c9d756a48e9398fb6388f8d0ea623", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 89 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -7,15 +7,15 @@ See documentation in docs/topics/settings.rst\n import os\n import cPickle as pickle\n \n+from scrapy.conf import default_settings\n+\n import_ = lambda x: __import__(x, {}, {}, [''])\n \n class Settings(object):\n \n-    default_settings_module = 'scrapy.conf.default_settings'\n-\n     def __init__(self):\n         self.defaults = {}\n-        self.global_defaults = import_(self.default_settings_module)\n+        self.global_defaults = default_settings\n         self.disabled = os.environ.get('SCRAPY_SETTINGS_DISABLED', False)\n         settings_module_path = os.environ.get('SCRAPY_SETTINGS_MODULE', \\\n             os.environ.get('SCRAPYSETTINGS_MODULE', 'scrapy_settings'))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#91df6fcc7c7de1d3b30239129f0fb1f150fa5533", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 27 | Files Changed: 1 | Hunks: 3 | Methods Changed: 9 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 54 | Churn Cumulative: 154 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -54,41 +54,41 @@ class CommandTest(ProjectTest):\n         self.env['SCRAPY_SETTINGS_MODULE'] = '%s.settings' % self.project_name\n \n \n+class GenspiderCommandTest(CommandTest):\n+\n+    def test_template_default(self, *args):\n+        self.assertEqual(0, self.call('genspider', 'testspider', 'test.com', *args))\n+        assert exists(join(self.proj_mod_path, 'spiders', 'testspider.py'))\n+        self.assertEqual(1, self.call('genspider', 'otherspider', 'test.com'))\n+\n+    def test_template_basic(self):\n+        self.test_template_default('--template=basic')\n+\n+    def test_template_csvfeed(self):\n+        self.test_template_default('--template=csvfeed')\n+\n+    def test_template_xmlfeed(self):\n+        self.test_template_default('--template=xmlfeed')\n+\n+    def test_template_crawl(self):\n+        self.test_template_default('--template=crawl')\n+\n+    def test_list(self):\n+        self.assertEqual(0, self.call('genspider', '--list'))\n+\n+    def test_dump(self):\n+        self.assertEqual(0, self.call('genspider', '--dump'))\n+        self.assertEqual(0, self.call('genspider', '--dump', '--template=basic'))\n+\n+\n class MiscCommandsTest(CommandTest):\n \n     def test_crawl(self):\n         self.assertEqual(0, self.call('crawl'))\n \n-    def test_genspider_subcommands(self):\n-        self.assertEqual(0, self.call('genspider', '--list'))\n-        self.assertEqual(0, self.call('genspider', '--dump'))\n-        self.assertEqual(0, self.call('genspider', '--dump', '--template=basic'))\n-\n     def test_list(self):\n         self.assertEqual(0, self.call('list'))\n \n \n-class BaseGenspiderTest(CommandTest):\n-    template = 'basic'\n-\n-    def test_genspider(self):\n-        self.assertEqual(0, self.call('genspider', 'testspider', 'test.com', \\\n-                '--template=%s' % self.template))\n-        assert exists(join(self.proj_mod_path, 'spiders', 'testspider.py'))\n-        self.assertEqual(1, self.call('genspider', 'otherspider', 'test.com'))\n-\n-\n-class CrawlGenspiderTest(BaseGenspiderTest):\n-    template = 'crawl'\n-\n-\n-class CsvFeedGenspiderTest(BaseGenspiderTest):\n-    template = 'csvfeed'\n-\n-\n-class XMLFeedGenspiderTest(BaseGenspiderTest):\n-    template = 'xmlfeed'\n-\n-\n if __name__ == '__main__':\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4f00c12a0ab9a9185674fafb501f4ed15eae5840", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 37 | Files Changed: 3 | Hunks: 8 | Methods Changed: 8 | Complexity Δ (Sum/Max): -20/0 | Churn Δ: 41 | Churn Cumulative: 848 | Contributors (this commit): 4 | Commits (past 90d): 50 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -31,7 +31,6 @@ class ExecutionEngine(object):\n         self.keep_alive = False\n         self.closing = {} # dict (domain -> reason) of spiders being closed\n         self.tasks = []\n-        self.ports = []\n         self.running = False\n         self.paused = False\n         self.control_reactor = True\n@@ -63,32 +62,6 @@ class ExecutionEngine(object):\n             tsk.start(interval, now)\n         return tsk\n \n-    def removetask(self, tsk):\n-        \"\"\"Remove a looping task previously added with addtask() method\"\"\"\n-        self.tasks = [(t, i, n) for (t, i, n) in self.tasks if t is not tsk]\n-        if tsk.running:\n-            tsk.stop()\n-\n-    def listenTCP(self, *args, **kwargs):\n-        if self.running:\n-            self.ports.append(reactor.listenTCP(*args, **kwargs))\n-        else:\n-            self.ports.append((args, kwargs))\n-\n-    def clean_reactor(self):\n-        \"\"\"Leaves the reactor in a clean state by removing all pending tasks\n-        and listening ports. It can only be called when the engine is not\n-        running.\n-        \"\"\"\n-        if not self.running:\n-            for tsk, _, _ in self.tasks:\n-                if tsk.running:\n-                    tsk.stop()\n-            self.tasks = []\n-            for p in [p for p in self.ports if not isinstance(p, tuple)]:\n-                p.stopListening()\n-            self.ports = []\n-\n     def start(self, control_reactor=True):\n         \"\"\"Start the execution engine\"\"\"\n         if not self.running:\n@@ -99,11 +72,6 @@ class ExecutionEngine(object):\n             self.addtask(self._mainloop, 5.0)\n             for tsk, interval, now in self.tasks:\n                 tsk.start(interval, now)\n-            for args, kwargs in [t for t in self.ports if isinstance(t, tuple)]:\n-                try:\n-                    reactor.listenTCP(*args, **kwargs)\n-                except CannotListenError:\n-                    log.msg(\"Cannot listen on TCP port %d\" % args[0], level=log.ERROR)\n             self.running = True\n             if control_reactor:\n                 reactor.run() # blocking call\n@@ -121,8 +89,6 @@ class ExecutionEngine(object):\n                 if tsk.running:\n                     tsk.stop()\n             self.tasks = []\n-            for p in [p for p in self.ports if not isinstance(p, tuple)]:\n-                p.stopListening()\n             if self.control_reactor and reactor.running:\n                 reactor.stop()\n             send_catch_log(signal=signals.engine_stopped, sender=self.__class__)\n\n@@ -8,7 +8,7 @@ import pprint\n \n from twisted.conch import manhole, telnet\n from twisted.conch.insults import insults\n-from twisted.internet import protocol\n+from twisted.internet import reactor, protocol\n \n from scrapy.extension import extensions\n from scrapy.core.manager import scrapymanager\n@@ -50,4 +50,4 @@ class TelnetConsole(protocol.ServerFactory):\n         self.protocol = makeProtocol\n         self.noisy = False\n         port = settings.getint('TELNETCONSOLE_PORT')\n-        scrapyengine.listenTCP(port, self)\n+        reactor.callWhenRunning(reactor.listenTCP, port, self)\n\n@@ -8,6 +8,7 @@ import re\n import socket\n from datetime import datetime\n \n+from twisted.internet import reactor\n from twisted.web import server, resource\n from scrapy.xlib.pydispatch import dispatcher\n \n@@ -89,4 +90,4 @@ class WebConsole(server.Site):\n         server.Site.__init__(self, WebConsoleResource(), logPath=logfile)\n         self.noisy = False\n         port = settings.getint('WEBCONSOLE_PORT')\n-        scrapyengine.listenTCP(port, self)\n+        reactor.callWhenRunning(reactor.listenTCP, port, self)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8f013bf2275114990d5a1c56831a34e544aa9d4f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 94 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -6,9 +6,14 @@ See documentation in docs/topics/settings.rst\n \n import os\n import cPickle as pickle\n+import warnings\n \n from scrapy.conf import default_settings\n \n+if 'SCRAPYSETTINGS_MODULE' in os.environ:\n+    warnings.warn(\"SCRAPYSETTINGS_MODULE environment variable is deprecated, \" \\\n+        \"use SCRAPY_SETTINGS_MODULE instead\", DeprecationWarning, stacklevel=2)\n+\n import_ = lambda x: __import__(x, {}, {}, [''])\n \n class Settings(object):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1dd996f0b648a0f1687e8fba48a1893e5aad9995", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 133 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -4,6 +4,8 @@ A Stats collector for persisting stats to Amazon SimpleDB.\n Requires the boto library: http://code.google.com/p/boto/\n \"\"\"\n \n+from datetime import datetime\n+\n from boto import connect_sdb\n from twisted.internet import threads\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e8e760c9740b8a3f7da565467c2290197583ebb2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 50 | Files Changed: 4 | Hunks: 20 | Methods Changed: 9 | Complexity Δ (Sum/Max): 4/7 | Churn Δ: 79 | Churn Cumulative: 1164 | Contributors (this commit): 4 | Commits (past 90d): 82 | Contributors (cumulative): 10 | DMM Complexity: 0.125\n\nDIFF:\n@@ -119,7 +119,6 @@ MEMUSAGE_REPORT = False\n MEMUSAGE_WARNING_MB = 0\n \n MYSQL_CONNECTION_SETTINGS = {}\n-MYSQL_CONNECTION_PING_PERIOD = 0\n \n NEWSPIDER_MODULE = ''\n \n\n@@ -7,8 +7,9 @@ See documentation in docs/topics/extensions.rst\n import os\n import socket\n \n-from scrapy.xlib.pydispatch import dispatcher\n+from twisted.internet import task\n \n+from scrapy.xlib.pydispatch import dispatcher\n from scrapy.core import signals\n from scrapy import log\n from scrapy.core.manager import scrapymanager\n@@ -26,24 +27,15 @@ class MemoryUsage(object):\n             raise NotConfigured\n         if not os.path.exists('/proc'):\n             raise NotConfigured\n-\n         self.warned = False\n \n-        scrapyengine.addtask(self.update, 60.0, now=True)\n-\n         self.notify_mails = settings.getlist('MEMUSAGE_NOTIFY')\n         self.limit = settings.getint('MEMUSAGE_LIMIT_MB')*1024*1024\n         self.warning = settings.getint('MEMUSAGE_WARNING_MB')*1024*1024\n         self.report = settings.getbool('MEMUSAGE_REPORT')\n-\n-        if self.limit:\n-            scrapyengine.addtask(self._check_limit, 60.0, now=True)\n-        if self.warning:\n-            scrapyengine.addtask(self._check_warning, 60.0, now=True)\n-\n         self.mail = MailSender()\n-\n         dispatcher.connect(self.engine_started, signal=signals.engine_started)\n+        dispatcher.connect(self.engine_stopped, signal=signals.engine_stopped)\n \n \n     @property\n@@ -52,6 +44,23 @@ class MemoryUsage(object):\n \n     def engine_started(self):\n         stats.set_value('memusage/startup', self.virtual)\n+        self.tasks = []\n+        tsk = task.LoopingCall(self.update)\n+        self.tasks.append(tsk)\n+        tsk.start(60.0, now=True)\n+        if self.limit:\n+            tsk = task.LoopingCall(self._check_limit)\n+            self.tasks.append(tsk)\n+            tsk.start(60.0, now=True)\n+        if self.warning:\n+            tsk = task.LoopingCall(self._check_warning)\n+            self.tasks.append(tsk)\n+            tsk.start(60.0, now=True)\n+\n+    def engine_stopped(self):\n+        for tsk in self.tasks:\n+            if tsk.running:\n+                tsk.stop()\n \n     def update(self):\n         stats.max_value('memusage/max', self.virtual)\n\n@@ -7,7 +7,6 @@ For more information see docs/topics/architecture.rst\n from datetime import datetime\n \n from twisted.internet import reactor, task\n-from twisted.internet.error import CannotListenError\n from twisted.python.failure import Failure\n from scrapy.xlib.pydispatch import dispatcher\n \n@@ -30,11 +29,11 @@ class ExecutionEngine(object):\n         self.configured = False\n         self.keep_alive = False\n         self.closing = {} # dict (domain -> reason) of spiders being closed\n-        self.tasks = []\n         self.running = False\n         self.paused = False\n         self.control_reactor = True\n         self._next_request_pending = set()\n+        self._mainloop_task = task.LoopingCall(self._mainloop)\n \n     def configure(self):\n         \"\"\"\n@@ -46,49 +45,31 @@ class ExecutionEngine(object):\n         self.scraper = Scraper(self)\n         self.configured = True\n \n-    def addtask(self, function, interval, args=None, kwargs=None, now=False):\n-        \"\"\"\n-        Adds a looping task. Use this instead of twisted task.LooopingCall to\n-        make sure the reactor is left in a clean state after the engine is\n-        stopped.\n-        \"\"\"\n-        if not args:\n-            args = []\n-        if not kwargs:\n-            kwargs = {}\n-        tsk = task.LoopingCall(function, *args, **kwargs)\n-        self.tasks.append((tsk, interval, now))\n-        if self.running:\n-            tsk.start(interval, now)\n-        return tsk\n-\n     def start(self, control_reactor=True):\n         \"\"\"Start the execution engine\"\"\"\n-        if not self.running:\n+        if self.running:\n+            return\n         self.control_reactor = control_reactor\n-            reactor.callLater(0, self._mainloop)\n         self.start_time = datetime.utcnow()\n         send_catch_log(signal=signals.engine_started, sender=self.__class__)\n-            self.addtask(self._mainloop, 5.0)\n-            for tsk, interval, now in self.tasks:\n-                tsk.start(interval, now)\n+        self._mainloop_task.start(5.0, now=True)\n+        reactor.callWhenRunning(self._mainloop)\n         self.running = True\n         if control_reactor:\n             reactor.run() # blocking call\n \n     def stop(self):\n         \"\"\"Stop the execution engine\"\"\"\n-        if self.running:\n+        if not self.running:\n+            return\n         self.running = False\n         for domain in self.open_domains:\n             spider = spiders.fromdomain(domain)\n             send_catch_log(signal=signals.domain_closed, sender=self.__class__, \\\n                 domain=domain, spider=spider, reason='shutdown')\n             stats.close_domain(domain, reason='shutdown')\n-            for tsk, _, _ in self.tasks: # stop looping calls\n-                if tsk.running:\n-                    tsk.stop()\n-            self.tasks = []\n+        if self._mainloop_task.running:\n+            self._mainloop_task.stop()\n         if self.control_reactor and reactor.running:\n             reactor.stop()\n         send_catch_log(signal=signals.engine_stopped, sender=self.__class__)\n\n@@ -4,7 +4,6 @@ import re\n \n import MySQLdb\n \n-from scrapy.core.engine import scrapyengine\n from scrapy.conf import settings\n from scrapy import log\n \n@@ -37,13 +36,4 @@ def mysql_connect(db_uri_or_dict, **kwargs):\n         d['host'], d['user']), level=log.DEBUG)\n     conn = MySQLdb.connect(**d)\n \n-    # connection keep-alive\n-    def conn_ping():\n-        log.msg(\"Pinging connection to %s/%s\" % (d['host'], d['db']), \\\n-            level=log.DEBUG)\n-        conn.ping()\n-    ping_period = settings.getint(\"MYSQL_CONNECTION_PING_PERIOD\")\n-    if ping_period:\n-        scrapyengine.addtask(conn_ping, ping_period)\n-\n     return conn\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
