{"custom_id": "scrapy#76c84bda38915363c049fac718892d54ccf24d5e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 29/29 | Churn Δ: 14 | Churn Cumulative: 31 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,6 +12,12 @@ class ItemField(object):\n         self.required = required\n         self.default = default\n \n+    def assign(self, value):\n+        if hasattr(value, '__iter__'):\n+            return self.to_python(self.deiter(value))\n+        else:\n+            return self.to_python(value)\n+\n     def to_python(self, value):\n         \"\"\"\n         Converts the input value into the expected Python data type.\n@@ -19,9 +25,9 @@ class ItemField(object):\n         \"\"\"\n         return value\n \n-    def from_list(self, list):\n-        \"Converts the input list into the expected Python data type.\"\n-        return self.to_python(list.join(''))\n+    def deiter(self, value):\n+        \"Converts the input iterable into a single value.\"\n+        return ' '.join(value)\n \n     def default_value(self):\n         \"Returns the default value for this field\"\n\n@@ -17,7 +17,7 @@ class Item(object):\n     def __setattr__(self, name, value):\n         if not name.startswith('_'):\n             if name in self._fields.keys():\n-                self._values[name] = self._fields[name].to_python(value)\n+                self._values[name] = self._fields[name].assign(value)\n             else:\n                 raise AttributeError(name)\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5c45ecb7ff29347bf68cd506cf0b8f755b6407a5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 40 | Contributors (this commit): 1 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -10,7 +10,7 @@ class ItemFieldValueError(Exception):\n class ItemField(object):\n     def __init__(self, required=False, default=None):\n         self.required = required\n-        self.default = default\n+        self._default = default\n \n     def assign(self, value):\n         if hasattr(value, '__iter__'):\n@@ -29,9 +29,10 @@ class ItemField(object):\n         \"Converts the input iterable into a single value.\"\n         return ' '.join(value)\n \n-    def default_value(self):\n+    @property\n+    def default(self):\n         \"Returns the default value for this field\"\n-        return self.default or self.to_python(None)\n+        return self._default or self.to_python(None)\n \n \n class BooleanItemField(ItemField):\n\n@@ -28,7 +28,7 @@ class Item(object):\n             try:\n                 return self._values[name]\n             except KeyError:\n-                return self._fields[name].default_value()\n+                return self._fields[name].default\n         else:\n             return object.__getattribute__(self, name)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#db4ccaf78f98a761308ccbf76d8e533380b422c4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 23 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -3,6 +3,10 @@ import decimal\n import re\n \n \n+__all__ = ['BooleanItemField', 'DateItemField', 'DecimalItemField',\n+           'FloatItemField', 'IntegerItemField', 'StringItemField']\n+\n+\n class ItemFieldValueError(Exception):\n     pass\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#81ce9bd458267612a32e1b7d5dcfb844bc98a4d0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 7 | Churn Cumulative: 30 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -14,7 +14,7 @@ class ItemFieldValueError(Exception):\n class ItemField(object):\n     def __init__(self, required=False, default=None):\n         self.required = required\n-        self._default = default\n+        self.default = default or self.to_python(None)\n \n     def assign(self, value):\n         if hasattr(value, '__iter__'):\n@@ -33,11 +33,6 @@ class ItemField(object):\n         \"Converts the input iterable into a single value.\"\n         return ' '.join(value)\n \n-    @property\n-    def default(self):\n-        \"Returns the default value for this field\"\n-        return self._default or self.to_python(None)\n-\n \n class BooleanItemField(ItemField):\n     def to_python(self, value):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7fc8d590f3dc091ae3e6cdfec18145af4e110b3c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 416 | Files Changed: 9 | Hunks: 3 | Methods Changed: 32 | Complexity Δ (Sum/Max): -107/0 | Churn Δ: 416 | Churn Cumulative: 980 | Contributors (this commit): 4 | Commits (past 90d): 12 | Contributors (cumulative): 16 | DMM Complexity: 0.3474025974025974\n\nDIFF:\n@@ -1,25 +0,0 @@\n-from scrapy.command import ScrapyCommand\n-from scrapy.fetcher import fetch\n-from scrapy.spider import spiders\n-from scrapy.item import ScrapedItem\n-\n-def get_item_attr(pagedata, attr=\"guid\"):\n-    spider = spiders.fromurl(pagedata.url)\n-    items = spider.parse(pagedata)\n-    attrs = [getattr(i, attr) for i in items if isinstance(i, ScrapedItem)]\n-    return attrs\n-\n-def get_attr(url, attr=\"guid\"):\n-    pagedatas = fetch([url])\n-    if pagedatas:\n-        return get_item_attr(pagedatas[0], attr)\n-\n-class Command(ScrapyCommand):\n-    def syntax(self):\n-        return \"<url> <attribute>\"\n-\n-    def short_desc(self):\n-        return \"Print an attribute from the item scraped in the given URL\"\n-\n-    def run(self, args, opts):\n-        print get_attr(args[0], args[1])\n\n@@ -1,184 +0,0 @@\n-import os\n-\n-from scrapy.command import ScrapyCommand\n-from scrapy.replay import Replay\n-from scrapy.utils import display\n-from scrapy.conf import settings\n-from scrapy.command import cmdline\n-\n-class Command(ScrapyCommand):\n-    def syntax(self):\n-        return \"[options] <replay_file> [action]\"\n-\n-    def short_desc(self):\n-        return \"Replay a session previously recorded with crawl --record\"\n-\n-    def help(self):\n-        s = \"Replay a session previously recorded with crawl --record\\n\"\n-        s += \"\\n\"\n-        s += \"Available actions:\\n\"\n-        s += \"  crawl: just replay the crawl (default if action omitted)\\n\"\n-        s += \"  diff: replay the crawl and show differences in items scraped/passed\\n\"\n-        s += \"  update: replay the crawl and update both scraped and passed items\\n\"\n-        s += \"  showitems: show items stored\\n\"\n-        s += \"  showpages: show all responses downloaded (not only HTML pages)\\n\"\n-        return s\n-\n-    def add_options(self, parser):\n-        ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"-v\", \"--verbose\", dest=\"verbose\", action=\"store_true\", help=\"show verbose output (full items/responses)\")\n-        parser.add_option(\"-t\", \"--item-type\", dest=\"itype\", help=\"item type (scraped, passed). default: scraped\", metavar=\"TYPE\")\n-        parser.add_option(\"--output\", dest=\"outfile\", help=\"write output to FILE. if omitted uses stdout\", metavar=\"FILE\")\n-        parser.add_option(\"--nocolour\", dest=\"nocolour\", action=\"store_true\", help=\"disable colorized output (for console only)\")\n-        parser.add_option(\"-i\", \"--ignore\", dest=\"ignores\", action=\"append\", help=\"item attribute to ignore. can be passed multiple times\", metavar=\"ATTR\")\n-        parser.add_option(\"--target\", dest=\"targets\", action=\"append\", help=\"crawl TARGET instead of recorded urls/domains. can be passed multiple times\")\n-        # adding option to update \n-        parser.add_option(\"--pages\", dest=\"pages\", action=\"store_true\", help=\"update all the pages in the replay file, recording it again.\")\n-        parser.add_option(\"-q\", \"--quiet\", dest=\"quiet\", action=\"store_true\", help=\"no verbose mode in option diff.\")\n-\n-    def process_options(self, args, opts):\n-        if args:\n-            ScrapyCommand.process_options(self, args, opts)\n-            self.opts = opts\n-            self.action = args[1] if len(args) > 1 else 'crawl'\n-            mode = 'update' if self.action == 'update' else 'play'\n-            usedir = args and os.path.isdir(args[0])\n-            self.replay = Replay(args[0], mode=mode, usedir=usedir)\n-            if self.action not in ['crawl', 'diff', 'update']:\n-                settings.overrides['LOG_ENABLED'] = False\n-\n-    def run(self, args, opts):\n-        if not args:\n-            print \"A <replay_dir> is required\"\n-            return\n-\n-        display.nocolour = opts.nocolour\n-\n-        if opts.itype == 'passed':\n-            self.before_db = self.replay.passed_old\n-            self.now_db = self.replay.passed_new\n-        else: # default is 'scraped'\n-            opts.itype = 'scraped'\n-            self.before_db = self.replay.scraped_old\n-            self.now_db = self.replay.scraped_new\n-\n-        actionfunc = getattr(self, 'action_%s' % self.action, None)\n-        if actionfunc:\n-            rep = actionfunc(opts)\n-            if rep:\n-                if opts.outfile:\n-                    f = open(opts.outfile, \"w\")\n-                    f.write(rep)\n-                    f.close()\n-                else:\n-                    print rep,\n-            self.replay.cleanup()\n-        else:\n-            print \"Unknown replay action: %s\" % self.action\n-\n-    def action_crawl(self, opts):\n-        self.replay.play(args=opts.targets)\n-\n-    def action_update(self, opts):\n-        self.replay.update(args=opts.targets, opts=opts.__dict__)\n-        if (opts.pages):\n-            args = ['scrapy-crawl', 'crawl']\n-            args.extend(self.replay.options['args'])\n-            for k in self.replay.options['opts']:                \n-                if self.replay.options['opts'][k]:                   \n-                    args.append(\"--%s\" % k)\n-                    if self.replay.options['opts'][k] != True:\n-                        args.append(self.replay.options['opts'][k])\n-            cmdline.execute_with_args(args)\n-\n-    def action_showitems(self, opts):\n-        s = \"\"\n-        s += self._format_items(self.before_db.values())\n-        s += \">>> Total: %d items %s\\n\" % (len(self.before_db), opts.itype)\n-        return s\n-\n-    def action_showpages(self, opts):\n-        s = \"\"\n-        if self.opts.verbose:\n-            for r in self.replay.responses_old.values():\n-                s += \">>> %s\\n\" % str(r)\n-                s += display.pformat(r)\n-        else:\n-            s += \"\\n\".join([str(r) for r in self.replay.responses_old.values()]) + \"\\n\"\n-        s += \">>> Total: %d responses received\\n\" % len(self.replay.responses_old)\n-        return s\n-\n-    def action_diff(self, opts):\n-        self.action_crawl(opts)\n-\n-        guids_before = set(self.before_db.keys())\n-        guids_now = set(map(str, self.now_db.keys()))\n-\n-        guids_new = guids_now - guids_before\n-        guids_missing = guids_before - guids_now\n-        guids_both = guids_now & guids_before\n-\n-        changed_items, chreport = self._report_differences(self.before_db, self.now_db, guids_both)\n-\n-        ok_items = len(guids_both) - changed_items\n-        new_items = len(guids_new)\n-        missing_items = len(guids_missing)\n-\n-        if (new_items - missing_items - changed_items) == 0 and opts.quiet:\n-            s = \"\"\n-        else:\n-            s = \"CRAWLING DIFFERENCES REPORT\\n\\n\"\n-\n-            s += \"Total items     : %d\\n\" % (len(guids_both) + new_items + missing_items)\n-            s += \"  Items OK      : %d\\n\" % ok_items\n-            s += \"  New items     : %d\\n\" % new_items\n-            s += \"  Missing items : %d\\n\" % missing_items\n-            s += \"  Changed items : %d\\n\" % changed_items\n-\n-            s += \"\\n\"\n-            s += \"- NEW ITEMS (%d) -----------------------------------------\\n\" % new_items\n-            s += self._format_items([self.now_db[g] for g in guids_new])\n-\n-            s += \"\\n\"\n-            s += \"- MISSING ITEMS (%d) -------------------------------------\\n\" % missing_items\n-            s += self._format_items([self.before_db[g] for g in guids_missing])\n-\n-            s += \"\\n\"\n-            s += \"- CHANGED ITEMS (%d) -------------------------------------\\n\" % changed_items\n-\n-            s += chreport\n-\n-        return s\n-\n-    def _report_differences(self, old_items, new_items, guids):\n-        items_old = [old_items[g] for g in guids]\n-        items_new = [new_items[g] for g in guids]\n-\n-        c = 0\n-        s = \"\"\n-        for old, new in zip(items_old, items_new):\n-            d = self._item_diff(old, new)\n-            if d:\n-                c += 1\n-                s += d\n-        return c, s\n-\n-    def _item_diff(self, old, new):\n-        delta = new - old\n-\n-        s = \"\"\n-        if delta.diff:\n-            s += \">>> Item guid=%s name=%s\\n\" % (old.guid, old.name)\n-            s += display.pformat(delta.diff) + \"\\n\"\n-        return s\n-\n-    def _format_items(self, items):\n-        if self.opts.verbose:\n-            s = display.pformat(items)\n-        else:\n-            s = \"\"\n-            for i in items:\n-                s += \"%s\\n\" % str(i)\n-                s += \"  <%s>\\n\" % i.url\n-        return s\n-\n\n\n\n\n\n\n\n@@ -1,207 +0,0 @@\n-from __future__ import with_statement\n-\n-import os\n-import cPickle as pickle\n-import shutil\n-import tempfile\n-import tarfile\n-import copy\n-import hashlib\n-\n-from pydispatch import dispatcher\n-\n-from scrapy import log\n-from scrapy.core import signals\n-from scrapy.core.manager import scrapymanager\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.conf import settings\n-from scrapy.item import ScrapedItem\n-\n-class Replay(object):\n-    \"\"\"\n-    This is the Replay extension, used to replay a crawling and see if the\n-    items scraped are the same.\n-    \"\"\"\n-\n-    def __init__(self, repfile, mode='play', usedir=False):\n-        \"\"\"\n-        Available modes: record, play, update\n-\n-        repfile can be either a dir (is usedir=True) or a tar.gz file (if\n-        usedir=False)\n-        \"\"\"\n-\n-        # XXX: this is ugly, and should be removed. but how?\n-        cachemw = 'scrapy.contrib.downloadermiddleware.cache.CacheMiddleware'\n-        if not cachemw in settings['DOWNLOADER_MIDDLEWARES']:\n-            raise NotConfigured(\"Cache middleware must be enabled to use Replay\")\n-\n-        self.recording = mode == 'record'\n-        self.updating = mode == 'update'\n-        self.playing = mode == 'play'\n-\n-        self.options = {}\n-        self.scraped_old = {}\n-        self.scraped_new = {}\n-        self.passed_old = {}\n-        self.passed_new = {}\n-        self.responses_old = {}\n-        self.responses_new = {}\n-\n-        #self._opendb(repfile, usedir)\n-        self._load(repfile, usedir)\n-\n-        settings.overrides['CACHE2_DIR'] = self.cache2dir\n-        settings.overrides['CACHE2_EXPIRATION_SECS'] = -1\n-        settings.overrides['CACHE2_IGNORE_MISSING'] = self.playing or self.updating\n-        settings.overrides['CACHE2_SECTORIZE'] = False\n-\n-        dispatcher.connect(self.engine_started, signal=signals.engine_started)\n-        dispatcher.connect(self.engine_stopped, signal=signals.engine_stopped)\n-        dispatcher.connect(self.item_scraped, signal=signals.item_scraped)\n-        dispatcher.connect(self.item_passed, signal=signals.item_passed)\n-        dispatcher.connect(self.response_received, signal=signals.response_received)\n-\n-\n-    def play(self, args=None, opts=None):\n-        if self.recording:\n-            raise ValueError(\"Replay.play() not available in record mode\")\n-\n-        args = args or self.options['args']\n-        opts = opts or self.options['opts']\n-        scrapymanager.runonce(*args, **opts)\n-\n-    def record(self, args=None, opts=None):\n-        self.recording = True\n-        self.options.clear()\n-        self.options['args'] = args or []\n-        self.options['opts'] = opts or {}\n-\n-        if os.path.exists(self.cache2dir):\n-            shutil.rmtree(self.cache2dir)\n-        else:\n-            os.mkdir(self.cache2dir)\n-\n-    def update(self, args=None, opts=None):\n-        self.updating = True\n-        if (opts['pages']):\n-            args = self.options['args']\n-            opts = self.options['opts']\n-            settings.overrides['CACHE2_EXPIRATION_SECS'] = 0            \n-            settings.overrides['CACHE2_IGNORE_MISSING'] = False\n-        else:            \n-            self.play(args, opts)\n-\n-    def engine_started(self):\n-        log.msg(\"Replay: recording session in %s\" % self.repfile)\n-\n-    def engine_stopped(self):\n-        if self.recording or self.updating:\n-            log.msg(\"Replay: recorded in %s: %d/%d scraped/passed items, %d received responses\" % \\\n-                (self.repfile, len(self.scraped_old), len(self.passed_old), len(self.responses_old)))\n-        self._save()\n-        self.cleanup()\n-\n-    def clean_private(self, src):\n-        if isinstance(src, ScrapedItem):\n-            target = src.__dict__\n-        elif isinstance(src, (list, tuple)):\n-            return type(src)(self.clean_private(elem) for elem in src)\n-        elif isinstance(src, dict):\n-            target = src\n-        else:\n-            return src\n-        for key, val in target.items():\n-            if key[0] == '_' and key[1] != '_':\n-                del target[key]\n-            else:\n-                target[key] = self.clean_private(val)\n-        return src\n-\n-    def item_scraped(self, item, spider):\n-        item = self.clean_private(copy.deepcopy(item))\n-        if self.recording or self.updating:\n-            self.scraped_old[str(item.guid)] = item\n-        else:\n-            self.scraped_new[str(item.guid)] = item\n-\n-    def item_passed(self, item, spider):\n-        item = self.clean_private(copy.deepcopy(item))\n-        if self.recording or self.updating:\n-            self.passed_old[str(item.guid)] = item\n-        else:\n-            self.passed_new[str(item.guid)] = item\n-\n-    def response_received(self, response, spider):\n-        key = hashlib.sha1(response.body).hexdigest()\n-        if (self.recording or self.updating) and key:\n-            self.responses_old[key] = response.copy()\n-        elif key:\n-            self.responses_new[key] = response.copy()\n-\n-    def _load(self, repfile, usedir):\n-        if usedir:\n-            if os.path.isdir(repfile):\n-                replay_dir = repfile\n-            elif self.recording:\n-                os.makedirs(repfile)\n-                replay_dir = repfile\n-            else:\n-                raise IOError(\"No such dir: \" % repfile)\n-        elif os.path.exists(repfile) and (self.playing or self.updating):\n-            if tarfile.is_tarfile(repfile):\n-                replay_dir = tempfile.mkdtemp(prefix=\"replay-\")\n-                tar = tarfile.open(repfile)\n-                tar.extractall(replay_dir)\n-                tar.close()\n-            else:\n-                raise IOError(\"Wrong tarfile: %s\" % repfile)\n-        elif self.recording:\n-            replay_dir = tempfile.mkdtemp(prefix=\"replay-\")\n-        else:\n-            raise IOError(\"No such file: %s\" % repfile)\n-\n-        self.cache2dir = os.path.join(replay_dir, \"httpcache\")\n-        self.replay_dir = replay_dir\n-        self.repfile = repfile\n-        self.usedir = usedir\n-\n-        self.options_path = os.path.join(replay_dir, \"options.db\")\n-        self.scraped_path = os.path.join(replay_dir, \"items_scraped.db\")\n-        self.passed_path = os.path.join(replay_dir, \"items_passed.db\")\n-        self.responses_path = os.path.join(replay_dir, \"responses.db\")\n-\n-        if self.updating:\n-            with open(self.options_path, 'r') as f:\n-                self.options = pickle.load(f)\n-        if self.playing:\n-            with open(self.options_path, 'r') as f:\n-                self.options = pickle.load(f)\n-            with open(self.responses_path, 'r') as f:\n-                self.responses_old = pickle.load(f)\n-            with open(self.scraped_path, 'r') as f:\n-                self.scraped_old = pickle.load(f)\n-            with open(self.passed_path, 'r') as f:\n-                self.passed_old = pickle.load(f)\n-\n-    def _save(self):\n-        if self.recording or self.updating:\n-            with open(self.options_path, 'w') as f:\n-                pickle.dump(self.options, f)\n-            with open(self.responses_path, 'w') as f:\n-                pickle.dump(self.responses_old, f)\n-            with open(self.scraped_path, 'w') as f:\n-                pickle.dump(self.scraped_old, f)\n-            with open(self.passed_path, 'w') as f:\n-                pickle.dump(self.passed_old, f)\n-\n-            if not self.usedir:\n-                if self.recording or self.updating:\n-                    tar = tarfile.open(self.repfile, \"w:gz\")\n-                    for name in os.listdir(self.replay_dir):\n-                        tar.add(os.path.join(self.replay_dir, name), name)\n-                    tar.close()\n-\n-    def cleanup(self):\n-        if not self.usedir and os.path.exists(self.replay_dir):\n-            shutil.rmtree(self.replay_dir)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#26bcc826e3d584815eaaf3f7974a5175aa79e653", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 19 | Files Changed: 2 | Hunks: 18 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 38 | Churn Cumulative: 89 | Contributors (this commit): 1 | Commits (past 90d): 12 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -3,15 +3,15 @@ import decimal\n import re\n \n \n-__all__ = ['BooleanItemField', 'DateItemField', 'DecimalItemField',\n-           'FloatItemField', 'IntegerItemField', 'StringItemField']\n+__all__ = ['BooleanField', 'DateField', 'DecimalField',\n+           'FloatField', 'IntegerField', 'StringField']\n \n \n-class ItemFieldValueError(Exception):\n+class FieldValueError(Exception):\n     pass\n \n \n-class ItemField(object):\n+class Field(object):\n     def __init__(self, required=False, default=None):\n         self.required = required\n         self.default = default or self.to_python(None)\n@@ -34,18 +34,18 @@ class ItemField(object):\n         return ' '.join(value)\n \n \n-class BooleanItemField(ItemField):\n+class BooleanField(Field):\n     def to_python(self, value):\n         if value in (True, False): return value\n         if value in ('t', 'True', '1'): return True\n         if value in ('f', 'False', '0'): return False\n-        raise ItemFieldValueError(\"This value must be either True or False.\")\n+        raise FieldValueError(\"This value must be either True or False.\")\n \n \n ansi_date_re = re.compile(r'^\\d{4}-\\d{1,2}-\\d{1,2}$')\n \n \n-class DateItemField(ItemField):\n+class DateField(Field):\n     def to_python(self, value):\n         if value is None:\n             return value\n@@ -55,51 +55,51 @@ class DateItemField(ItemField):\n             return value\n \n         if not ansi_date_re.search(value):\n-            raise ItemFieldValueError(\n+            raise FieldValueError(\n                 \"Enter a valid date in YYYY-MM-DD format.\")\n \n         year, month, day = map(int, value.split('-'))\n         try:\n             return datetime.date(year, month, day)\n         except ValueError, e:\n-            raise ItemFieldValueError(\"Invalid date: %s\" % str(e))\n+            raise FieldValueError(\"Invalid date: %s\" % str(e))\n \n \n-class DecimalItemField(ItemField):\n+class DecimalField(Field):\n     def to_python(self, value):\n         if value is None:\n             return value\n         try:\n             return decimal.Decimal(value)\n         except decimal.InvalidOperation:\n-            raise ItemFieldValueError(\"This value must be a decimal number.\")\n+            raise FieldValueError(\"This value must be a decimal number.\")\n \n \n-class FloatItemField(ItemField):\n+class FloatField(Field):\n     def to_python(self, value):\n         if value is None:\n             return value\n         try:\n             return float(value)\n         except (TypeError, ValueError):\n-            raise ItemFieldValueError(\"This value must be a float.\")\n+            raise FieldValueError(\"This value must be a float.\")\n \n \n-class IntegerItemField(ItemField):\n+class IntegerField(Field):\n     def to_python(self, value):\n         if value is None:\n             return value\n         try:\n             return int(value)\n         except (TypeError, ValueError):\n-            raise ItemFieldValueError(\"This value must be an integer.\")\n+            raise FieldValueError(\"This value must be an integer.\")\n \n \n-class StringItemField(ItemField):\n+class StringField(Field):\n     def to_python(self, value):\n         if isinstance(value, basestring):\n             return value\n         if value is None:\n             return value\n-        raise ItemFieldValueError(\"This field must be a string.\")\n+        raise FieldValueError(\"This field must be a string.\")\n \n\n@@ -1,4 +1,4 @@\n-from scrapy.contrib_exp.newitem.fields import ItemField\n+from scrapy.contrib_exp.newitem.fields import Field\n \n \n class Item(object):\n@@ -12,7 +12,7 @@ class Item(object):\n \n     def _get_fields(self):\n         return dict(i for i in self.__class__.__dict__.iteritems() \\\n-                    if isinstance(i[1], ItemField))\n+                    if isinstance(i[1], Field))\n \n     def __setattr__(self, name, value):\n         if not name.startswith('_'):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5338a48db8608ba323577e2b551b2f2f0fc41c3b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 9 | Churn Cumulative: 104 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,5 @@\n from scrapy.command import ScrapyCommand\n from scrapy.core.manager import scrapymanager\n-from scrapy.replay import Replay\n from scrapy.conf import settings\n from scrapy.utils.url import is_url\n from scrapy.spider import spiders\n@@ -20,8 +19,6 @@ class Command(ScrapyCommand):\n         parser.add_option(\"--nocache\", dest=\"nocache\", action=\"store_true\", help=\"disable HTTP cache\")\n         parser.add_option(\"--nopipeline\", dest=\"nopipeline\", action=\"store_true\", help=\"disable scraped item pipeline\")\n         parser.add_option(\"--restrict\", dest=\"restrict\", action=\"store_true\", help=\"restrict crawling only to the given urls\")\n-        parser.add_option(\"--record\", dest=\"record\", help=\"use FILE for recording session (see replay command)\", metavar=\"FILE\")\n-        parser.add_option(\"--record-dir\", dest=\"recorddir\", help=\"use DIR for recording (instead of file)\", metavar=\"DIR\")\n         parser.add_option(\"-n\", \"--nofollow\", dest=\"nofollow\", action=\"store_true\", help=\"don't follow links (for use with URLs only)\")\n         parser.add_option(\"-c\", \"--callback\", dest=\"callback\", action=\"store\", help=\"use the provided callback for starting to crawl the given url\")\n \n@@ -39,12 +36,6 @@ class Command(ScrapyCommand):\n         if opts.nofollow:\n             settings.overrides['CRAWLSPIDER_FOLLOW_LINKS'] = False\n \n-        if opts.record or opts.recorddir:\n-            # self.replay is used for preventing Replay signals handler from\n-            # disconnecting since pydispatcher uses weak references \n-            self.replay = Replay(opts.record or opts.recorddir, mode='record', usedir=bool(opts.recorddir))\n-            self.replay.record(args=args, opts=opts.__dict__)\n-\n     def run(self, args, opts):\n         if opts.callback:\n             requests = []\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fe7935005c5b6cad1983d0e82a4610225cbb5a23", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 16 | Churn Cumulative: 211 | Contributors (this commit): 2 | Commits (past 90d): 12 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,7 +6,7 @@ from pydispatch import dispatcher\n \n from scrapy.core import signals\n from scrapy.http import Request\n-from scrapy.core.exceptions import NotConfigured\n+from scrapy.core.exceptions import NotConfigured, IgnoreRequest\n from scrapy.utils.request import request_fingerprint\n from scrapy.utils.misc import load_object\n from scrapy.conf import settings\n@@ -38,15 +38,17 @@ class DuplicatesFilterMiddleware(object):\n         dispatcher.connect(self.filter.open, signals.domain_open)\n         dispatcher.connect(self.filter.close, signals.domain_closed)\n \n+    def process_spider_input(self, response, spider):\n+        if not self.filter.add(spider.domain_name, response.request):\n+            raise IgnoreRequest(\"Skipped (already processed): %s\" % response.request)\n+\n     def process_spider_output(self, response, result, spider):\n         domain = spider.domain_name\n-        self.filter.add(domain, response.request)\n-\n         for req in result:\n             if isinstance(req, Request):\n                 added = self.filter.add(domain, req)\n                 if not (added or req.dont_filter):\n-                    log.msg('Skipped (already visited): %s' % req, log.TRACE, domain=domain)\n+                    log.msg('Skipped (already processed): %s' % req, log.TRACE, domain=domain)\n                     continue\n             yield req\n \n\n@@ -2,6 +2,7 @@ import unittest\n \n from scrapy.spider import spiders\n from scrapy.http import Request, Response\n+from scrapy.core.exceptions import IgnoreRequest\n from scrapy.contrib.spidermiddleware.duplicatesfilter import DuplicatesFilterMiddleware, SimplePerDomainFilter\n \n class DuplicatesFilterMiddlewareTest(unittest.TestCase):\n@@ -23,6 +24,7 @@ class DuplicatesFilterMiddlewareTest(unittest.TestCase):\n         r2 = Request('http://scrapytest.org/2')\n         r3 = Request('http://scrapytest.org/2')\n \n+        mw.process_spider_input(response, self.spider)\n         filtered = list(mw.process_spider_output(response, [r0, r1, r2, r3], self.spider))\n \n         assert r0 not in filtered\n@@ -30,6 +32,10 @@ class DuplicatesFilterMiddlewareTest(unittest.TestCase):\n         assert r2 in filtered\n         assert r3 not in filtered\n \n+        response = Response('http://scrapytest.org/')\n+        response.request = Request('http://scrapytest.org/')\n+        self.assertRaises(IgnoreRequest, mw.process_spider_input, response, self.spider)\n+\n         mw.filter.close('scrapytest.org')\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#756cda3873861e9538924aaffb45146c4a60cc3f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 7 | Churn Cumulative: 218 | Contributors (this commit): 2 | Commits (past 90d): 14 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -39,8 +39,7 @@ class DuplicatesFilterMiddleware(object):\n         dispatcher.connect(self.filter.close, signals.domain_closed)\n \n     def process_spider_input(self, response, spider):\n-        if not self.filter.add(spider.domain_name, response.request):\n-            raise IgnoreRequest(\"Skipped (already processed): %s\" % response.request)\n+        self.filter.add(spider.domain_name, response.request)\n \n     def process_spider_output(self, response, result, spider):\n         domain = spider.domain_name\n\n@@ -32,10 +32,6 @@ class DuplicatesFilterMiddlewareTest(unittest.TestCase):\n         assert r2 in filtered\n         assert r3 not in filtered\n \n-        response = Response('http://scrapytest.org/')\n-        response.request = Request('http://scrapytest.org/')\n-        self.assertRaises(IgnoreRequest, mw.process_spider_input, response, self.spider)\n-\n         mw.filter.close('scrapytest.org')\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#edf5dfb264ff353653e0d78dc198e0aa7c0ecf02", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 71 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 6 | Complexity Δ (Sum/Max): 24/24 | Churn Δ: 71 | Churn Cumulative: 71 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.5294117647058824\n\nDIFF:\n@@ -0,0 +1,71 @@\n+import inspect\n+\n+\n+class ItemExtractor(object):\n+    def __init__(self, response=None, item=None):\n+        if item:\n+            self.item_instance = item\n+        else:\n+            self.item_instance = self.item_class()\n+        \n+        self._response = response\n+        self._field_extractors = self._get_field_extractors()\n+\n+    def _get_field_extractors(self):\n+        fe = {}\n+        for field in self.item_instance._fields.keys():\n+            if self.__class__.__dict__.has_key(field):\n+                fe[field] = self.__class__.__dict__[field]\n+        return fe\n+\n+    def __setattr__(self, name, value):\n+        if not (name.startswith('_') or name == 'item_instance'):\n+            if name in self._field_extractors.keys():\n+                setattr(self.item_instance, name, \n+                        self._field_extractors[name](value, self._response))\n+            else:\n+                raise AttributeError(name)\n+        else:\n+            object.__setattr__(self, name, value)\n+\n+    def __getattribute__(self, name):\n+        if not (name.startswith('_') or name.startswith('item_')):\n+            return getattr(self.item_instance, name)\n+        else:\n+            return object.__getattribute__(self, name)\n+\n+\n+class ExtractorField(object):\n+    def __init__(self, funcs):\n+        self._funcs = []\n+\n+        for func in funcs:\n+            if inspect.isfunction(func):\n+                func_args, _, _, _ = inspect.getargspec(func)\n+            elif hasattr(func, '__call__'):\n+                try:\n+                    func_args, _, _, _ = inspect.getargspec(func.__call__)\n+                except Exception:\n+                    func_args = []\n+\n+            takes_args = True if 'adaptor_args' in func_args else False\n+            self._funcs.append((func, takes_args))\n+\n+    def __call__(self, value, kwargs=None):\n+        values = [value]\n+\n+        for func, takes_args in self._funcs:\n+            next_round = []\n+\n+            for val in values:\n+                val = func(val, kwargs) if takes_args else func(val)\n+\n+                if isinstance(val, tuple):\n+                    next_round.extend(val)\n+                elif val is not None:\n+                    next_round.append(val)\n+\n+            values = next_round\n+\n+        return list(values)\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#65e8be46bca946d93c9e8eb7060b5fc6b8cbaff2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 75 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -37,6 +37,10 @@ class ItemExtractor(object):\n \n class ExtractorField(object):\n     def __init__(self, funcs):\n+        if not hasattr(funcs, '__iter__'):\n+            raise TypeError(\n+                'You must initialize ExtractorField with a list of callables')\n+\n         self._funcs = []\n \n         for func in funcs:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ac735a185e8b7efcf2a341318f01d26ac85d30bf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 22 | Files Changed: 1 | Hunks: 10 | Methods Changed: 2 | Complexity Δ (Sum/Max): -6/0 | Churn Δ: 27 | Churn Cumulative: 130 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -16,46 +16,29 @@ MAX_REDIRECT_LOOP = 10\n \n class RedirectMiddleware(object):\n     def process_exception(self, request, exception, spider):\n-        if isinstance(exception, HttpException):\n+        if not isinstance(exception, HttpException):\n+            return\n+\n         status = exception.status\n         response = exception.response\n \n-            if status in [302, 303]:\n+        if status in set([302, 303]):\n             redirected_url = urljoin(request.url, response.headers['location'][0])\n-                if not getattr(spider, \"no_redirect\", False):\n             redirected = request.replace(url=redirected_url, method='GET', body=None)\n-                    # This is needed to avoid redirection loops with requests that contain dont_filter = True\n-                    # Example (9 May 2008): http://www.55max.com/product/001_photography.asp?3233,0,0,0,Michael+Banks\n-                    if isinstance(redirected.dont_filter, int):\n-                        if not hasattr(redirected, \"original_dont_filter\"):\n-                            redirected.original_dont_filter = redirected.dont_filter\n-                        if redirected.dont_filter <= -MAX_REDIRECT_LOOP:\n-                            raise RedirectLoop(\"Exited redirect loop with %s consecutive visits to the same url.\" % (redirected.original_dont_filter + MAX_REDIRECT_LOOP) )\n-                        redirected.dont_filter -= 1\n-                    else:\n-                        redirected.dont_filter = False\n             log.msg(\"Redirecting (%d) to %s from %s\" % (status, redirected, request), level=log.DEBUG, domain=spider.domain_name)\n             return redirected\n-                log.msg(\"Ignored redirecting (%d) to %s from %s (disabled by spider)\" % (status, redirected_url, request), level=log.DEBUG, domain=spider.domain_name)\n-                return response\n \n         if status in [301, 307]:\n             redirected_url = urljoin(request.url, response.headers['location'][0])\n-                if not getattr(spider, \"no_redirect\", False):\n             redirected = request.replace(url=redirected_url)\n-                    # This is needed to avoid redirection loops with requests that contain dont_filter = True\n-                    # Example (9 May 2008): http://www.55max.com/product/001_photography.asp?3233,0,0,0,Michael+Banks\n-                    redirected.dont_filter = False\n             log.msg(\"Redirecting (%d) to %s from %s\" % (status, redirected, request), level=log.DEBUG, domain=spider.domain_name)\n             return redirected\n-                log.msg(\"Ignored redirecting (%d) to %s from %s (disabled by spider)\" % (status, redirected_url, request), level=log.DEBUG, domain=spider.domain_name)\n-                return response\n \n     def process_response(self, request, response, spider):\n-        if isinstance(response, Response):\n         interval, url = get_meta_refresh(response)\n         if url and int(interval) < META_REFRESH_MAXSEC:\n             redirected = request.replace(url=urljoin(request.url, url))\n             log.msg(\"Redirecting (meta refresh) to %s from %s\" % (redirected, request), level=log.DEBUG, domain=spider.domain_name)\n             return redirected\n+\n         return response\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0e98526daa08dbdfbd8dc42d8447935f437b84f5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 84 | Files Changed: 5 | Hunks: 24 | Methods Changed: 14 | Complexity Δ (Sum/Max): -3/4 | Churn Δ: 111 | Churn Cumulative: 731 | Contributors (this commit): 3 | Commits (past 90d): 37 | Contributors (cumulative): 11 | DMM Complexity: 0.0\n\nDIFF:\n@@ -76,7 +76,7 @@ DOWNLOADER_MIDDLEWARES = [\n \n DOWNLOADER_STATS = True\n \n-DUPLICATESFILTER_FILTERCLASS = 'scrapy.contrib.spidermiddleware.duplicatesfilter.SimplePerDomainFilter'\n+DUPLICATESFILTER_FILTERCLASS = 'scrapy.core.filters.SimplePerDomainFilter'\n \n ENABLED_SPIDERS_FILE = ''\n \n\n@@ -1,10 +1,15 @@\n import re\n+from pydispatch import dispatcher\n \n+from scrapy.core import signals\n from scrapy import log\n from scrapy.http import Request, Response\n from scrapy.core.exceptions import HttpException\n+from scrapy.core.filters import duplicatesfilter\n+from scrapy.utils.misc import load_object\n from scrapy.utils.url import urljoin_rfc as urljoin\n from scrapy.utils.response import get_meta_refresh\n+from scrapy.conf import settings\n \n class RedirectLoop(Exception):\n     pass\n@@ -19,26 +24,31 @@ class RedirectMiddleware(object):\n         if not isinstance(exception, HttpException):\n             return\n \n+        domain = spider.domain_name\n         status = exception.status\n         response = exception.response\n \n         if status in set([302, 303]):\n             redirected_url = urljoin(request.url, response.headers['location'][0])\n             redirected = request.replace(url=redirected_url, method='GET', body=None)\n-            log.msg(\"Redirecting (%d) to %s from %s\" % (status, redirected, request), level=log.DEBUG, domain=spider.domain_name)\n-            return redirected\n+            return self._redirect(redirected, request, spider, status)\n \n         if status in [301, 307]:\n             redirected_url = urljoin(request.url, response.headers['location'][0])\n             redirected = request.replace(url=redirected_url)\n-            log.msg(\"Redirecting (%d) to %s from %s\" % (status, redirected, request), level=log.DEBUG, domain=spider.domain_name)\n-            return redirected\n+            return self._redirect(redirected, request, spider, status)\n \n     def process_response(self, request, response, spider):\n         interval, url = get_meta_refresh(response)\n         if url and int(interval) < META_REFRESH_MAXSEC:\n             redirected = request.replace(url=urljoin(request.url, url))\n-            log.msg(\"Redirecting (meta refresh) to %s from %s\" % (redirected, request), level=log.DEBUG, domain=spider.domain_name)\n-            return redirected\n+            return self._redirect(redirected, request, spider, 'meta refresh') or response\n \n         return response\n+\n+    def _redirect(self, redirected, request, spider, reason):\n+        domain = spider.domain_name\n+        if duplicatesfilter.add(domain, redirected) or redirected.dont_filter:\n+            log.msg(\"Redirecting (%s) to %s from %s\" % (reason, redirected, request), level=log.DEBUG, domain=domain)\n+            return redirected\n+\n\n@@ -2,73 +2,26 @@\n DuplicatesFilterMiddleware: Filter out already visited urls\n \"\"\"\n \n-from pydispatch import dispatcher\n-\n-from scrapy.core import signals\n from scrapy.http import Request\n-from scrapy.core.exceptions import NotConfigured, IgnoreRequest\n-from scrapy.utils.request import request_fingerprint\n-from scrapy.utils.misc import load_object\n from scrapy.conf import settings\n+from scrapy.core.filters import duplicatesfilter\n from scrapy import log\n \n \n class DuplicatesFilterMiddleware(object):\n-    \"\"\"Filter out duplicate requests to avoid visiting same page more than once.\n-\n-    filter class (defined by DUPLICATESFILTER_FILTERCLASS setting) must\n-    inplement a simple API:\n-\n-      * open(domain) called when a new domain starts\n-\n-      * close(domain) called when a domain is going to be closed.\n-\n-      * add(domain, request) called each time a new request needs to be tested\n-        looking if it is was already seen or is a new one.  This is the most\n-        important method for a filtering class.\n-\n-    \"\"\"\n-\n-    def __init__(self):\n-        clspath = settings.get('DUPLICATESFILTER_FILTERCLASS')\n-        if not clspath:\n-            raise NotConfigured\n-\n-        self.filter = load_object(clspath)()\n-        dispatcher.connect(self.filter.open, signals.domain_open)\n-        dispatcher.connect(self.filter.close, signals.domain_closed)\n+    \"\"\"Filter out already seen requests to avoid visiting pages more than once.\"\"\"\n \n     def process_spider_input(self, response, spider):\n-        self.filter.add(spider.domain_name, response.request)\n+        duplicatesfilter.add(spider.domain_name, response.request)\n \n     def process_spider_output(self, response, result, spider):\n         domain = spider.domain_name\n         for req in result:\n             if isinstance(req, Request):\n-                added = self.filter.add(domain, req)\n+                added = duplicatesfilter.add(domain, req)\n                 if not (added or req.dont_filter):\n                     log.msg('Skipped (already processed): %s' % req, log.TRACE, domain=domain)\n                     continue\n             yield req\n \n \n-class SimplePerDomainFilter(dict):\n-    \"\"\"Filter out a request if already seen for same domain\"\"\"\n-\n-    def open(self, domain):\n-        \"\"\"Initialize the resources needed for filtering for this domain\"\"\"\n-        self[domain] = set()\n-\n-    def close(self, domain):\n-        \"\"\"Remove the resources reserved for filtering for this domain\"\"\"\n-        del self[domain]\n-\n-    def add(self, domain, request):\n-        \"\"\"Add the fingerprint of a request to the domain set if a equivalent fingerprint has not been added.\n-        This method will return true if the fingerprint was added and false otherwise.\n-        \"\"\"\n-        fp = request_fingerprint(request)\n-        if fp not in self[domain]:\n-            self[domain].add(fp)\n-            return True\n-        return False\n\n@@ -2,6 +2,7 @@ import unittest\n \n from scrapy.contrib.downloadermiddleware.redirect import RedirectMiddleware\n from scrapy.core.exceptions import HttpException\n+from scrapy.core.filters import duplicatesfilter\n from scrapy.spider import spiders\n from scrapy.http import Request, Response, Headers\n \n@@ -11,6 +12,7 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         spiders.spider_modules = ['scrapy.tests.test_spiders']\n         spiders.reload()\n         self.spider = spiders.fromdomain('scrapytest.org')\n+        duplicatesfilter.open('scrapytest.org')\n \n     def test_process_exception(self):\n \n@@ -28,7 +30,7 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         self.assertEqual(req2.url, url2)\n \n         url = 'http://www.example.com/302'\n-        url2 = 'http://www.example.com/redirected'\n+        url2 = 'http://www.example.com/redirected2'\n         req = Request(url, method='POST')\n         hdr = Headers({'Location': [url2]})\n         rsp = Response(url, headers=hdr)\n\n@@ -3,7 +3,8 @@ import unittest\n from scrapy.spider import spiders\n from scrapy.http import Request, Response\n from scrapy.core.exceptions import IgnoreRequest\n-from scrapy.contrib.spidermiddleware.duplicatesfilter import DuplicatesFilterMiddleware, SimplePerDomainFilter\n+from scrapy.contrib.spidermiddleware.duplicatesfilter import DuplicatesFilterMiddleware\n+from scrapy.core.filters import duplicatesfilter\n \n class DuplicatesFilterMiddlewareTest(unittest.TestCase):\n \n@@ -11,10 +12,10 @@ class DuplicatesFilterMiddlewareTest(unittest.TestCase):\n         spiders.spider_modules = ['scrapy.tests.test_spiders']\n         spiders.reload()\n         self.spider = spiders.fromdomain('scrapytest.org')\n+        duplicatesfilter.open('scrapytest.org')\n \n     def test_process_spider_output(self):\n         mw = DuplicatesFilterMiddleware()\n-        mw.filter.open('scrapytest.org')\n \n         response = Response('http://scrapytest.org/')\n         response.request = Request('http://scrapytest.org/')\n@@ -31,26 +32,3 @@ class DuplicatesFilterMiddlewareTest(unittest.TestCase):\n         assert r1 in filtered\n         assert r2 in filtered\n         assert r3 not in filtered\n-\n-        mw.filter.close('scrapytest.org')\n-\n-\n-class SimplePerDomainFilterTest(unittest.TestCase):\n-\n-    def test_filter(self):\n-        domain = 'scrapytest.org'\n-        filter = SimplePerDomainFilter()\n-        filter.open(domain)\n-        assert domain in filter\n-\n-        r1 = Request('http://scrapytest.org/1')\n-        r2 = Request('http://scrapytest.org/2')\n-        r3 = Request('http://scrapytest.org/2')\n-\n-        assert filter.add(domain, r1)\n-        assert filter.add(domain, r2)\n-        assert not filter.add(domain, r3)\n-\n-        filter.close(domain)\n-        assert domain not in filter\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
