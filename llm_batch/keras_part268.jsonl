{"custom_id": "keras#962cc5bb4de9d385509fcd096d345caa4f8d2d54", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 290 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -47,6 +47,7 @@ def test_basic_batchnorm():\n \n @keras_test\n def test_batchnorm_correctness_1d():\n+    np.random.seed(1337)\n     model = Sequential()\n     norm = normalization.BatchNormalization(input_shape=(10,), momentum=0.8)\n     model.add(norm)\n@@ -65,6 +66,7 @@ def test_batchnorm_correctness_1d():\n \n @keras_test\n def test_batchnorm_correctness_2d():\n+    np.random.seed(1337)\n     model = Sequential()\n     norm = normalization.BatchNormalization(axis=1, input_shape=(10, 6),\n                                             momentum=0.8)\n@@ -84,13 +86,13 @@ def test_batchnorm_correctness_2d():\n \n @keras_test\n def test_batchnorm_training_argument():\n+    np.random.seed(1337)\n     bn1 = normalization.BatchNormalization(input_shape=(10,))\n     x1 = Input(shape=(10,))\n     y1 = bn1(x1, training=True)\n     assert bn1.updates\n \n     model1 = Model(x1, y1)\n-    np.random.seed(123)\n     x = np.random.normal(loc=5.0, scale=10.0, size=(20, 10))\n     output_a = model1.predict(x)\n \n@@ -123,6 +125,7 @@ def test_batchnorm_mode_twice():\n \n @keras_test\n def test_batchnorm_convnet():\n+    np.random.seed(1337)\n     model = Sequential()\n     norm = normalization.BatchNormalization(axis=1, input_shape=(3, 4, 4),\n                                             momentum=0.8)\n@@ -144,6 +147,7 @@ def test_batchnorm_convnet():\n @pytest.mark.skipif((K.backend() == 'theano'),\n                     reason='Bug with theano backend')\n def test_batchnorm_convnet_no_center_no_scale():\n+    np.random.seed(1337)\n     model = Sequential()\n     norm = normalization.BatchNormalization(axis=-1, center=False, scale=False,\n                                             input_shape=(3, 4, 4), momentum=0.8)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3184efd620840fb6032526df32a09185b7ae18e2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 19 | Churn Cumulative: 213 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,10 +6,9 @@ from keras import backend as K\n from keras.models import Sequential, Model\n from keras.layers import convolutional_recurrent, Input\n from keras.utils.test_utils import layer_test\n+from keras.utils.test_utils import keras_test\n from keras import regularizers\n \n-\n-def test_convolutional_recurrent():\n num_row = 3\n num_col = 3\n filters = 2\n@@ -18,6 +17,11 @@ def test_convolutional_recurrent():\n input_num_row = 5\n input_num_col = 5\n sequence_len = 2\n+\n+\n+@keras_test\n+def test_convolutional_recurrent():\n+\n     for data_format in ['channels_first', 'channels_last']:\n \n         if data_format == 'channels_first':\n@@ -59,10 +63,15 @@ def test_convolutional_recurrent():\n                                         'padding': 'valid'},\n                                 input_shape=inputs.shape)\n \n-            # No need to check following tests for both data formats\n-            if data_format == 'channels_first' or return_sequences:\n-                continue\n \n+@keras_test\n+def test_convolutional_recurrent_statefulness():\n+\n+    data_format = 'channels_last'\n+    return_sequences = False\n+    inputs = np.random.rand(num_samples, sequence_len,\n+                            input_num_row, input_num_col,\n+                            input_channel)\n     # Tests for statefulness\n     model = Sequential()\n     kwargs = {'data_format': data_format,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bb8cc405f3ea3b74a9f3908b606a35f08d8d0be8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 379 | Files Changed: 37 | Hunks: 380 | Methods Changed: 1 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 395 | Churn Cumulative: 27686 | Contributors (this commit): 134 | Commits (past 90d): 128 | Contributors (cumulative): 329 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,9 @@\n+import pytest\r\n+from keras import backend as K\r\n+\r\n+\r\n+@pytest.fixture(autouse=True)\r\n+def clear_session_after_test():\r\n+    yield\r\n+    if K.backend() == 'tensorflow' or K.backend() == 'cntk':\r\n+        K.clear_session()\r\n\n@@ -2,7 +2,6 @@ import pytest\n import random\n import os\n from multiprocessing import Process, Queue\n-from keras.utils.test_utils import keras_test\n from keras import applications\n from keras import backend as K\n \n@@ -57,13 +56,11 @@ def _get_output_shape(model_fn):\n         return model.output_shape\n \n \n-@keras_test\n def _test_application_basic(app, last_dim=1000):\n     output_shape = _get_output_shape(lambda: app(weights=None))\n     assert output_shape == (None, last_dim)\n \n \n-@keras_test\n def _test_application_notop(app, last_dim):\n     output_shape = _get_output_shape(\n         lambda: app(weights=None, include_top=False))\n\n@@ -2,13 +2,12 @@ from __future__ import print_function\n import numpy as np\n import pytest\n \n-from keras.utils.test_utils import get_test_data, keras_test\n+from keras.utils.test_utils import get_test_data\n from keras.models import Sequential\n from keras import layers\n from keras.utils.np_utils import to_categorical\n \n \n-@keras_test\n def test_image_classification():\n     np.random.seed(1337)\n     input_shape = (16, 16, 3)\n\n@@ -3,7 +3,7 @@ import numpy as np\n import pytest\n import string\n \n-from keras.utils.test_utils import get_test_data, keras_test\n+from keras.utils.test_utils import get_test_data\n from keras.utils.np_utils import to_categorical\n from keras.models import Sequential\n from keras import layers, optimizers\n@@ -11,7 +11,6 @@ import keras.backend as K\n import keras\n \n \n-@keras_test\n def test_temporal_classification():\n     '''\n     Classify temporal sequences of float numbers\n@@ -44,7 +43,6 @@ def test_temporal_classification():\n     model = Sequential.from_config(config)\n \n \n-@keras_test\n def test_temporal_classification_functional():\n     '''\n     Classify temporal sequences of float numbers\n@@ -74,7 +72,6 @@ def test_temporal_classification_functional():\n     assert(history.history['acc'][-1] >= 0.8)\n \n \n-@keras_test\n def test_temporal_regression():\n     '''\n     Predict float numbers (regression) based on sequences\n@@ -95,7 +92,6 @@ def test_temporal_regression():\n     assert(history.history['loss'][-1] < 1.)\n \n \n-@keras_test\n def test_3d_to_3d():\n     '''\n     Apply a same Dense layer for each element of time dimension of the input\n@@ -119,7 +115,6 @@ def test_3d_to_3d():\n     assert(history.history['loss'][-1] < 1.)\n \n \n-@keras_test\n def test_stacked_lstm_char_prediction():\n     '''\n     Learn alphabetical char sequence with stacked LSTM.\n@@ -173,7 +168,6 @@ def test_stacked_lstm_char_prediction():\n     assert(generated == alphabet)\n \n \n-@keras_test\n def test_masked_temporal():\n     '''\n     Confirm that even with masking on both inputs and outputs, cross-entropies are\n@@ -212,7 +206,6 @@ def test_masked_temporal():\n \n \n @pytest.mark.skipif(K.backend() != 'tensorflow', reason='Requires TF backend')\n-@keras_test\n def test_embedding_with_clipnorm():\n     model = Sequential()\n     model.add(layers.Embedding(input_dim=1, output_dim=1))\n\n@@ -6,12 +6,10 @@ import pytest\n import keras\n from keras import layers\n from keras.utils.test_utils import get_test_data\n-from keras.utils.test_utils import keras_test\n \n \n @pytest.mark.skipif(keras.backend.backend() != 'tensorflow',\n                     reason='Requires TF backend')\n-@keras_test\n def test_tf_optimizer():\n     import tensorflow as tf\n \n\n@@ -1,7 +1,7 @@\n from __future__ import print_function\n import pytest\n \n-from keras.utils.test_utils import get_test_data, keras_test\n+from keras.utils.test_utils import get_test_data\n from keras.models import Sequential\n from keras import layers\n import keras\n@@ -10,7 +10,6 @@ from keras.utils.np_utils import to_categorical\n num_classes = 2\n \n \n-@keras_test\n def test_vector_classification():\n     '''\n     Classify random float vectors into 2 classes with logistic regression\n@@ -43,7 +42,6 @@ def test_vector_classification():\n     model = Sequential.from_config(config)\n \n \n-@keras_test\n def test_vector_classification_functional():\n     (x_train, y_train), (x_test, y_test) = get_test_data(num_train=500,\n                                                          num_test=200,\n@@ -66,7 +64,6 @@ def test_vector_classification_functional():\n     assert(history.history['val_acc'][-1] > 0.8)\n \n \n-@keras_test\n def test_vector_regression():\n     '''\n     Perform float data prediction (regression) using 2 layer MLP\n\n@@ -3,7 +3,6 @@ from numpy.testing import assert_allclose\n import numpy as np\n import scipy.sparse as sparse\n import warnings\n-from keras.utils.test_utils import keras_test\n \n from keras import backend as K\n from keras.backend import floatx, set_floatx, variable\n@@ -101,7 +100,6 @@ def assert_list_keras_shape(t_list, z_list):\n                     assert t._keras_shape[i] == z.shape[i]\n \n \n-@keras_test\n def check_single_tensor_operation(function_name, x_shape_or_val, backend_list, **kwargs):\n     shape_or_val = kwargs.pop('shape_or_val', True)\n     assert_value_equality = kwargs.pop('assert_value_equality', True)\n@@ -130,7 +128,6 @@ def check_single_tensor_operation(function_name, x_shape_or_val, backend_list, *\n     assert_list_keras_shape(t_list, z_list)\n \n \n-@keras_test\n def check_two_tensor_operation(function_name, x_shape_or_val,\n                                y_shape_or_val, backend_list, **kwargs):\n     concat_args = kwargs.pop('concat_args', False)\n@@ -168,7 +165,6 @@ def check_two_tensor_operation(function_name, x_shape_or_val,\n     assert_list_keras_shape(t_list, z_list)\n \n \n-@keras_test\n def check_composed_tensor_operations(first_function_name, first_function_args,\n                                      second_function_name, second_function_args,\n                                      input_shape, backend_list):\n\n@@ -4,7 +4,6 @@ from numpy.testing import assert_allclose\n \n from keras import backend as K\n from keras import constraints\n-from keras.utils.test_utils import keras_test\n \n \n def get_test_values():\n@@ -30,7 +29,6 @@ def test_serialization():\n         assert fn.__class__ == ref_fn.__class__\n \n \n-@keras_test\n def test_max_norm():\n     array = get_example_array()\n     for m in get_test_values():\n@@ -50,14 +48,12 @@ def test_max_norm():\n     assert_allclose(x_normed_actual, x_normed_target, rtol=1e-05)\n \n \n-@keras_test\n def test_non_neg():\n     non_neg_instance = constraints.non_neg()\n     normed = non_neg_instance(K.variable(get_example_array()))\n     assert(np.all(np.min(K.eval(normed), axis=1) == 0.))\n \n \n-@keras_test\n def test_unit_norm():\n     unit_norm_instance = constraints.unit_norm()\n     normalized = unit_norm_instance(K.variable(get_example_array()))\n@@ -68,7 +64,6 @@ def test_unit_norm():\n     assert(np.abs(largest_difference) < 10e-5)\n \n \n-@keras_test\n def test_min_max_norm():\n     array = get_example_array()\n     for m in get_test_values():\n\n@@ -8,7 +8,6 @@ from keras.engine import Input, Layer, saving, get_source_inputs\n from keras.models import Model, Sequential\n from keras import backend as K\n from keras.models import model_from_json, model_from_yaml\n-from keras.utils.test_utils import keras_test\n from keras.initializers import Constant\n \n \n@@ -18,7 +17,6 @@ skipif_no_tf_gpu = pytest.mark.skipif(\n     reason='Requires TensorFlow backend and a GPU')\n \n \n-@keras_test\n def test_get_updates_for():\n     a = Input(shape=(2,))\n     dense_layer = Dense(1)\n@@ -29,7 +27,6 @@ def test_get_updates_for():\n     assert dense_layer.get_updates_for(None) == [1]\n \n \n-@keras_test\n def test_get_losses_for():\n     a = Input(shape=(2,))\n     dense_layer = Dense(1)\n@@ -40,7 +37,6 @@ def test_get_losses_for():\n     assert dense_layer.get_losses_for(None) == [1]\n \n \n-@keras_test\n def test_trainable_weights():\n     a = Input(shape=(2,))\n     b = Dense(1)(a)\n@@ -153,7 +149,6 @@ def test_learning_phase():\n     assert fn_outputs_no_dp[1].sum() != fn_outputs_dp[1].sum()\n \n \n-@keras_test\n def test_layer_call_arguments():\n     # Test the ability to pass and serialize arguments to `call`.\n     inp = layers.Input(shape=(2,))\n@@ -173,7 +168,6 @@ def test_layer_call_arguments():\n     assert not model.uses_learning_phase\n \n \n-@keras_test\n def test_node_construction():\n     ####################################################\n     # test basics\n@@ -256,7 +250,6 @@ def test_node_construction():\n     assert dense.get_output_mask_at(1) is None\n \n \n-@keras_test\n def test_multi_input_layer():\n     ####################################################\n     # test multi-input layer\n@@ -323,7 +316,6 @@ def test_multi_input_layer():\n     assert [x.shape for x in fn_outputs] == [(10, 64), (10, 5)]\n \n \n-@keras_test\n def test_recursion():\n     ####################################################\n     # test recursion\n@@ -503,7 +495,6 @@ def test_recursion():\n         Dense(2)(x)\n \n \n-@keras_test\n def test_load_layers():\n     from keras.layers import ConvLSTM2D, TimeDistributed\n     from keras.layers import Bidirectional, Conv2D, Input\n@@ -583,7 +574,6 @@ def convert_weights(layer, weights):\n     return weights\n \n \n-@keras_test\n @pytest.mark.parametrize(\"layer\", [\n     layers.GRU(2, input_shape=[3, 5]),\n     layers.LSTM(2, input_shape=[3, 5]),\n@@ -602,7 +592,6 @@ def test_preprocess_weights_for_loading(layer):\n                 for (x, y) in zip(weights1, weights2)])\n \n \n-@keras_test\n @pytest.mark.parametrize(\"layer\", [\n     layers.Conv2D(2, (3, 3), input_shape=[5, 5, 3]),\n     layers.Conv2DTranspose(2, (5, 5),\n@@ -619,7 +608,6 @@ def test_preprocess_weights_for_loading_for_model(layer):\n                 for (x, y) in zip(weights1, weights2)])\n \n \n-@keras_test\n @pytest.mark.parametrize('layer_class,args', [\n     (layers.GRU, {'units': 2, 'input_shape': [3, 5]}),\n     (layers.GRU, {'units': 2, 'input_shape': [3, 5], 'reset_after': True}),\n@@ -638,7 +626,6 @@ def test_preprocess_weights_for_loading_rnn_should_be_idempotent(layer_class, ar\n     assert all([np.allclose(x, y, 1e-5) for (x, y) in zip(weights1, weights2)])\n \n \n-@keras_test\n @pytest.mark.parametrize('layer_class,args', [\n     (layers.CuDNNGRU, {'units': 2, 'input_shape': [3, 5]}),\n     (layers.CuDNNLSTM, {'units': 2, 'input_shape': [3, 5]}),\n@@ -649,7 +636,6 @@ def test_preprocess_weights_for_loading_cudnn_rnn_should_be_idempotent(layer_cla\n     test_preprocess_weights_for_loading_rnn_should_be_idempotent(layer_class, args)\n \n \n-@keras_test\n def test_recursion_with_bn_and_loss():\n     model1 = Sequential([\n         layers.Dense(5, input_dim=5, activity_regularizer='l1'),\n@@ -676,7 +662,6 @@ def test_recursion_with_bn_and_loss():\n     model2.fit(x, y, verbose=0, epochs=1)\n \n \n-@keras_test\n def test_activity_regularization_with_model_composition():\n \n     def reg(x):\n@@ -699,7 +684,6 @@ def test_activity_regularization_with_model_composition():\n     assert loss == 4\n \n \n-@keras_test\n def test_shared_layer_depth_is_correct():\n     # Basic outline here: we have a shared embedding layer, and two inputs that\n     # go through different depths of computation in the graph before\n@@ -728,7 +712,6 @@ def test_shared_layer_depth_is_correct():\n     assert input1_depth == input2_depth\n \n \n-@keras_test\n def test_layer_sharing_at_heterogeneous_depth():\n     x_val = np.random.random((10, 5))\n \n@@ -750,7 +733,6 @@ def test_layer_sharing_at_heterogeneous_depth():\n     np.testing.assert_allclose(output_val, output_val_2, atol=1e-6)\n \n \n-@keras_test\n def test_layer_sharing_at_heterogeneous_depth_with_concat():\n     input_shape = (16, 9, 3)\n     input_layer = Input(shape=input_shape)\n@@ -778,7 +760,6 @@ def test_layer_sharing_at_heterogeneous_depth_with_concat():\n     np.testing.assert_allclose(output_val, output_val_2, atol=1e-6)\n \n \n-@keras_test\n def test_multi_output_mask():\n     \"\"\"Fixes #7589\"\"\"\n     class TestMultiOutputLayer(Layer):\n@@ -808,7 +789,6 @@ def test_multi_output_mask():\n     assert K.int_shape(z)[1:] == (16, 16, 3)\n \n \n-@keras_test\n def test_constant_initializer_with_numpy():\n     model = Sequential()\n     model.add(Dense(2, input_shape=(3,),\n\n@@ -17,7 +17,6 @@ from keras.utils.generic_utils import slice_arrays\n from keras.models import Sequential\n from keras import backend as K\n from keras.utils import Sequence\n-from keras.utils.test_utils import keras_test\n from keras.callbacks import LambdaCallback\n \n \n@@ -71,7 +70,6 @@ def threadsafe_generator(f):\n     return g\n \n \n-@keras_test\n def test_check_array_length_consistency():\n     training_utils.check_array_length_consistency(None, None, None)\n     a_np = np.random.random((4, 3, 3))\n@@ -93,7 +91,6 @@ def test_check_array_length_consistency():\n         training_utils.check_array_length_consistency([a_np], None, [b_np])\n \n \n-@keras_test\n def testslice_arrays():\n     input_a = np.random.random((10, 3))\n     slice_arrays(None)\n@@ -114,7 +111,6 @@ def testslice_arrays():\n     slice_arrays(input_a, stop=2)\n \n \n-@keras_test\n def test_weighted_masked_objective():\n     a = Input(shape=(3,), name='input_a')\n \n@@ -127,7 +123,6 @@ def test_weighted_masked_objective():\n     weighted_function(a, a, None)\n \n \n-@keras_test\n def test_model_methods():\n     a = Input(shape=(3,), name='input_a')\n     b = Input(shape=(3,), name='input_b')\n@@ -565,7 +560,6 @@ def test_model_methods():\n \n @pytest.mark.skipif(sys.version_info < (3,),\n                     reason='Cannot catch warnings in python 2')\n-@keras_test\n def test_warnings():\n     a = Input(shape=(3,), name='input_a')\n     b = Input(shape=(3,), name='input_b')\n@@ -607,7 +601,6 @@ def test_warnings():\n         'A warning was raised for Sequence.')\n \n \n-@keras_test\n def test_sparse_inputs_targets():\n     test_inputs = [sparse.random(6, 3, density=0.25).tocsr() for _ in range(2)]\n     test_outputs = [sparse.random(6, i, density=0.25).tocsr() for i in range(3, 5)]\n@@ -625,7 +618,6 @@ def test_sparse_inputs_targets():\n \n @pytest.mark.skipif(K.backend() != 'tensorflow',\n                     reason='sparse operations supported only by TensorFlow')\n-@keras_test\n def test_sparse_placeholder_fit():\n     test_inputs = [sparse.random(6, 3, density=0.25).tocsr() for _ in range(2)]\n     test_outputs = [sparse.random(6, i, density=0.25).tocsr() for i in range(3, 5)]\n@@ -641,7 +633,6 @@ def test_sparse_placeholder_fit():\n     model.evaluate(test_inputs, test_outputs, batch_size=2)\n \n \n-@keras_test\n def test_trainable_argument():\n     x = np.random.random((5, 3))\n     y = np.random.random((5, 2))\n@@ -665,7 +656,6 @@ def test_trainable_argument():\n     assert_allclose(out, out_2)\n \n \n-@keras_test\n def test_with_list_as_targets():\n     model = Sequential()\n     model.add(Dense(1, input_dim=3, trainable=False))\n@@ -676,7 +666,6 @@ def test_with_list_as_targets():\n     model.train_on_batch(x, y)\n \n \n-@keras_test\n def test_check_not_failing():\n     a = np.random.random((2, 1, 3))\n     training_utils.check_loss_and_target_compatibility(\n@@ -685,7 +674,6 @@ def test_check_not_failing():\n         [a], [losses.categorical_crossentropy], [(2, None, 3)])\n \n \n-@keras_test\n def test_check_last_is_one():\n     a = np.random.random((2, 3, 1))\n     with pytest.raises(ValueError) as exc:\n@@ -695,7 +683,6 @@ def test_check_last_is_one():\n     assert 'You are passing a target array' in str(exc)\n \n \n-@keras_test\n def test_check_bad_shape():\n     a = np.random.random((2, 3, 5))\n     with pytest.raises(ValueError) as exc:\n@@ -707,7 +694,6 @@ def test_check_bad_shape():\n \n @pytest.mark.skipif(K.backend() != 'tensorflow',\n                     reason='Requires TensorFlow backend')\n-@keras_test\n def test_model_with_input_feed_tensor():\n     \"\"\"We test building a model with a TF variable as input.\n     We should be able to call fit, evaluate, predict,\n@@ -849,7 +835,6 @@ def test_model_with_input_feed_tensor():\n     assert out.shape == (10 * 3, 4)\n \n \n-@keras_test\n def test_model_with_partial_loss():\n     a = Input(shape=(3,), name='input_a')\n     a_2 = Dense(4, name='dense_1')(a)\n@@ -891,7 +876,6 @@ def test_model_with_partial_loss():\n     out = model.evaluate(input_a_np, [output_a_np])\n \n \n-@keras_test\n @pytest.mark.skipif((K.backend() == 'cntk'),\n                     reason='cntk does not support external loss yet')\n def test_model_with_external_loss():\n@@ -1045,7 +1029,6 @@ def test_model_with_external_loss():\n         assert out[1].shape == (10 * 3, 4)\n \n \n-@keras_test\n def test_target_tensors():\n     # single-output, as list\n     model = keras.models.Sequential()\n@@ -1123,7 +1106,6 @@ def test_target_tensors():\n                          sample_weight={'dense_a': np.random.random((10,))})\n \n \n-@keras_test\n def test_model_custom_target_tensors():\n     a = Input(shape=(3,), name='input_a')\n     b = Input(shape=(3,), name='input_b')\n@@ -1187,7 +1169,6 @@ def test_model_custom_target_tensors():\n \n @pytest.mark.skipif(sys.version_info < (3,),\n                     reason='Cannot catch warnings in python 2')\n-@keras_test\n def test_trainable_weights_count_consistency():\n     \"\"\"Tests the trainable weights consistency check of Model.\n \n@@ -1230,7 +1211,6 @@ def test_trainable_weights_count_consistency():\n         'Warning raised even when .compile() is called after modifying .trainable')\n \n \n-@keras_test\n def test_pandas_dataframe():\n     input_a = Input(shape=(3,), name='input_a')\n     input_b = Input(shape=(3,), name='input_b')\n@@ -1310,7 +1290,6 @@ def test_pandas_dataframe():\n                           [output_a_df, output_b_df])\n \n \n-@keras_test\n @pytest.mark.skipif(K.backend() != 'tensorflow', reason='Requires TensorFlow')\n @pytest.mark.skipif((K.backend() == 'tensorflow' and\n                      not hasattr(K.get_session(),\n@@ -1339,7 +1318,6 @@ def test_training_and_eval_methods_on_symbolic_tensors_single_io():\n               validation_data=(inputs, targets), validation_steps=2)\n \n \n-@keras_test\n @pytest.mark.skipif(K.backend() != 'tensorflow', reason='Requires TensorFlow')\n @pytest.mark.skipif((K.backend() == 'tensorflow' and\n                      not hasattr(K.get_session(),\n@@ -1437,7 +1415,6 @@ def test_training_and_eval_methods_on_symbolic_tensors_multi_io():\n     model.test_on_batch([input_a_tf, input_b_tf], [output_d_tf, output_e_tf])\n \n \n-@keras_test\n def test_model_with_crossentropy_losses_channels_first():\n     \"\"\"Tests use of all crossentropy losses with `channels_first`.\n \n@@ -1520,7 +1497,6 @@ def test_model_with_crossentropy_losses_channels_first():\n                                           'channels_first and channels_last.'))\n \n \n-@keras_test\n def test_dynamic_set_inputs():\n     model = Sequential()\n     model.add(Dense(16, input_dim=32))\n\n@@ -1,49 +1,41 @@\n import pytest\n from keras.utils.test_utils import layer_test\n-from keras.utils.test_utils import keras_test\n from keras import layers\n \n \n-@keras_test\n def test_leaky_relu():\n     for alpha in [0., .5, -1.]:\n         layer_test(layers.LeakyReLU, kwargs={'alpha': alpha},\n                    input_shape=(2, 3, 4))\n \n \n-@keras_test\n def test_prelu():\n     layer_test(layers.PReLU, kwargs={},\n                input_shape=(2, 3, 4))\n \n \n-@keras_test\n def test_prelu_share():\n     layer_test(layers.PReLU, kwargs={'shared_axes': 1},\n                input_shape=(2, 3, 4))\n \n \n-@keras_test\n def test_elu():\n     for alpha in [0., .5, -1.]:\n         layer_test(layers.ELU, kwargs={'alpha': alpha},\n                    input_shape=(2, 3, 4))\n \n \n-@keras_test\n def test_thresholded_relu():\n     layer_test(layers.ThresholdedReLU, kwargs={'theta': 0.5},\n                input_shape=(2, 3, 4))\n \n \n-@keras_test\n def test_softmax():\n     for axis in [1, -1]:\n         layer_test(layers.Softmax, kwargs={'axis': axis},\n                    input_shape=(2, 3, 4))\n \n \n-@keras_test\n def test_relu():\n     for max_value in [None, 1., 6.]:\n         layer_test(layers.ReLU, kwargs={'max_value': max_value},\n\n@@ -3,7 +3,6 @@ import numpy as np\n from numpy.testing import assert_allclose\n \n from keras.utils.test_utils import layer_test\n-from keras.utils.test_utils import keras_test\n from keras import backend as K\n from keras.layers import convolutional\n from keras.models import Sequential\n@@ -16,7 +15,6 @@ else:\n     _convolution_paddings = ['valid', 'same']\n \n \n-@keras_test\n @pytest.mark.skipif((K.backend() == 'cntk' and K.dev.type() == 0),\n                     reason='cntk only support dilated conv on GPU')\n @pytest.mark.parametrize(\n@@ -43,7 +41,6 @@ def test_causal_dilated_conv(layer_kwargs, input_length, expected_output):\n                kwargs=layer_kwargs, expected_output=expected_output)\n \n \n-@keras_test\n @pytest.mark.parametrize(\n     'padding,strides',\n     [(padding, strides)\n@@ -78,7 +75,6 @@ def test_conv_1d(padding, strides):\n                input_shape=(batch_size, steps, input_dim))\n \n \n-@keras_test\n @pytest.mark.skipif((K.backend() == 'cntk' and K.dev.type() == 0),\n                     reason='cntk only support dilated conv on GPU')\n def test_conv_1d_dilation():\n@@ -97,7 +93,6 @@ def test_conv_1d_dilation():\n                input_shape=(batch_size, steps, input_dim))\n \n \n-@keras_test\n def test_conv_1d_channels_first():\n     batch_size = 2\n     steps = 8\n@@ -112,7 +107,6 @@ def test_conv_1d_channels_first():\n                input_shape=(batch_size, input_dim, steps))\n \n \n-@keras_test\n @pytest.mark.parametrize(\n     'strides,padding',\n     [(strides, padding)\n@@ -137,7 +131,6 @@ def test_convolution_2d(strides, padding):\n                input_shape=(num_samples, stack_size, num_row, num_col))\n \n \n-@keras_test\n def test_convolution_2d_channels_last():\n     num_samples = 2\n     filters = 2\n@@ -162,7 +155,6 @@ def test_convolution_2d_channels_last():\n                input_shape=(num_samples, num_row, num_col, stack_size))\n \n \n-@keras_test\n @pytest.mark.skipif((K.backend() == 'cntk' and K.dev.type() == 0),\n                     reason='cntk only supports dilated conv on GPU')\n def test_convolution_2d_dilation():\n@@ -182,7 +174,6 @@ def test_convolution_2d_dilation():\n                input_shape=(num_samples, num_row, num_col, stack_size))\n \n \n-@keras_test\n def test_convolution_2d_invalid():\n     filters = 2\n     padding = _convolution_paddings[-1]\n@@ -194,7 +185,6 @@ def test_convolution_2d_invalid():\n             batch_input_shape=(None, None, 5, None))])\n \n \n-@keras_test\n @pytest.mark.parametrize(\n     'padding,out_padding,strides',\n     [(padding, out_padding, strides)\n@@ -222,7 +212,6 @@ def test_conv2d_transpose(padding, out_padding, strides):\n                fixed_batch_size=True)\n \n \n-@keras_test\n @pytest.mark.skipif((K.backend() == 'cntk' and K.dev.type() == 0),\n                     reason='cntk only supports dilated conv transpose on GPU')\n def test_conv2d_transpose_dilation():\n@@ -253,7 +242,6 @@ def test_conv2d_transpose_dilation():\n                expected_output=expected_output)\n \n \n-@keras_test\n def test_conv2d_transpose_channels_first():\n     num_samples = 2\n     filters = 2\n@@ -279,7 +267,6 @@ def test_conv2d_transpose_channels_first():\n                fixed_batch_size=True)\n \n \n-@keras_test\n def test_conv2d_transpose_invalid():\n     filters = 2\n     stack_size = 3\n@@ -316,7 +303,6 @@ def test_conv2d_transpose_invalid():\n             batch_input_shape=(None, num_row, num_col, stack_size))])\n \n \n-@keras_test\n @pytest.mark.parametrize(\n     'padding,strides,multiplier,dilation_rate',\n     [(padding, strides, multiplier, dilation_rate)\n@@ -344,7 +330,6 @@ def test_separable_conv_1d(padding, strides, multiplier, dilation_rate):\n                input_shape=(num_samples, num_step, stack_size))\n \n \n-@keras_test\n def test_separable_conv_1d_additional_args():\n     num_samples = 2\n     filters = 6\n@@ -371,7 +356,6 @@ def test_separable_conv_1d_additional_args():\n                input_shape=(num_samples, stack_size, num_step))\n \n \n-@keras_test\n def test_separable_conv_1d_invalid():\n     filters = 6\n     padding = 'valid'\n@@ -381,7 +365,6 @@ def test_separable_conv_1d_invalid():\n             batch_input_shape=(None, 5, None))])\n \n \n-@keras_test\n @pytest.mark.parametrize(\n     'padding,strides,multiplier,dilation_rate',\n     [(padding, strides, multiplier, dilation_rate)\n@@ -412,7 +395,6 @@ def test_separable_conv_2d(padding, strides, multiplier, dilation_rate):\n         input_shape=(num_samples, num_row, num_col, stack_size))\n \n \n-@keras_test\n def test_separable_conv_2d_additional_args():\n     num_samples = 2\n     filters = 6\n@@ -440,7 +422,6 @@ def test_separable_conv_2d_additional_args():\n                input_shape=(num_samples, stack_size, num_row, num_col))\n \n \n-@keras_test\n def test_separable_conv_2d_invalid():\n     filters = 6\n     padding = 'valid'\n@@ -450,7 +431,6 @@ def test_separable_conv_2d_invalid():\n             batch_input_shape=(None, None, 5, None))])\n \n \n-@keras_test\n @pytest.mark.parametrize(\n     'padding,strides,multiplier',\n     [(padding, strides, multiplier)\n@@ -476,7 +456,6 @@ def test_depthwise_conv_2d(padding, strides, multiplier):\n                             stack_size))\n \n \n-@keras_test\n def test_depthwise_conv_2d_additional_args():\n     num_samples = 2\n     stack_size = 3\n@@ -501,7 +480,6 @@ def test_depthwise_conv_2d_additional_args():\n                input_shape=(num_samples, stack_size, num_row, num_col))\n \n \n-@keras_test\n def test_depthwise_conv_2d_invalid():\n     padding = 'valid'\n     with pytest.raises(ValueError):\n@@ -511,7 +489,6 @@ def test_depthwise_conv_2d_invalid():\n             batch_input_shape=(None, None, 5, None))])\n \n \n-@keras_test\n @pytest.mark.parametrize(\n     'padding,strides',\n     [(padding, strides)\n@@ -538,7 +515,6 @@ def test_convolution_3d(padding, strides):\n                             stack_size))\n \n \n-@keras_test\n def test_convolution_3d_additional_args():\n     num_samples = 2\n     filters = 2\n@@ -566,7 +542,6 @@ def test_convolution_3d_additional_args():\n                             stack_size))\n \n \n-@keras_test\n @pytest.mark.parametrize(\n     'padding,out_padding,strides,data_format',\n     [(padding, out_padding, strides, data_format)\n@@ -596,7 +571,6 @@ def test_conv3d_transpose(padding, out_padding, strides, data_format):\n         fixed_batch_size=True)\n \n \n-@keras_test\n def test_conv3d_transpose_additional_args():\n     filters = 2\n     stack_size = 3\n@@ -623,7 +597,6 @@ def test_conv3d_transpose_additional_args():\n                fixed_batch_size=True)\n \n \n-@keras_test\n def test_conv3d_transpose_invalid():\n     filters = 2\n     stack_size = 3\n@@ -662,7 +635,6 @@ def test_conv3d_transpose_invalid():\n             batch_input_shape=(None, num_depth, num_row, num_col, stack_size))])\n \n \n-@keras_test\n def test_zero_padding_1d():\n     num_samples = 2\n     input_dim = 2\n@@ -699,7 +671,6 @@ def test_zero_padding_1d():\n     layer.get_config()\n \n \n-@keras_test\n @pytest.mark.parametrize(\n     'data_format,padding',\n     [(data_format, padding)\n@@ -722,7 +693,6 @@ def test_zero_padding_2d(data_format, padding):\n                input_shape=inputs.shape)\n \n \n-@keras_test\n def test_zero_padding_2d_correctness():\n     num_samples = 2\n     stack_size = 2\n@@ -774,7 +744,6 @@ def test_zero_padding_2d_correctness():\n             assert_allclose(np_output[:, :, 1:-2, 3:-4], 1.)\n \n \n-@keras_test\n @pytest.mark.parametrize(\n     'data_format,padding',\n     [(data_format, padding)\n@@ -796,7 +765,6 @@ def test_zero_padding_3d(data_format, padding):\n                input_shape=inputs.shape)\n \n \n-@keras_test\n def test_zero_padding_3d_correctness():\n     num_samples = 2\n     stack_size = 2\n@@ -849,14 +817,12 @@ def test_zero_padding_3d_correctness():\n             assert_allclose(np_output[:, :, 1:-2, 3:-4, 0:-2], 1.)\n \n \n-@keras_test\n def test_upsampling_1d():\n     layer_test(convolutional.UpSampling1D,\n                kwargs={'size': 2},\n                input_shape=(3, 5, 4))\n \n \n-@keras_test\n def test_upsampling_2d():\n     num_samples = 2\n     stack_size = 2\n@@ -997,7 +963,6 @@ def test_upsampling_3d():\n                     assert_allclose(np_output, expected_out)\n \n \n-@keras_test\n @pytest.mark.skipif((K.backend() == 'cntk'),\n                     reason=\"cntk does not support slice to 0 dimension\")\n def test_cropping_1d():\n@@ -1142,7 +1107,6 @@ def test_cropping_3d():\n         layer = convolutional.Cropping3D(cropping=lambda x: x)\n \n \n-@keras_test\n @pytest.mark.skipif((K.backend() == 'cntk'),\n                     reason='CNTK does not support float64')\n @pytest.mark.parametrize(\n\n@@ -7,20 +7,17 @@ from keras import layers\n from keras.models import Model\n from keras.models import Sequential\n from keras.utils.test_utils import layer_test\n-from keras.utils.test_utils import keras_test\n from keras import regularizers\n from keras import constraints\n from keras.layers import deserialize as deserialize_layer\n \n \n-@keras_test\n def test_masking():\n     layer_test(layers.Masking,\n                kwargs={},\n                input_shape=(3, 2, 3))\n \n \n-@keras_test\n def test_dropout():\n     layer_test(layers.Dropout,\n                kwargs={'rate': 0.5},\n@@ -61,7 +58,6 @@ def test_dropout():\n                            input_shape=input_shape)\n \n \n-@keras_test\n def test_activation():\n     # with string argument\n     layer_test(layers.Activation,\n@@ -74,7 +70,6 @@ def test_activation():\n                input_shape=(3, 2))\n \n \n-@keras_test\n def test_reshape():\n     layer_test(layers.Reshape,\n                kwargs={'target_shape': (8, 1)},\n@@ -93,14 +88,12 @@ def test_reshape():\n                input_shape=(None, None, 4))\n \n \n-@keras_test\n def test_permute():\n     layer_test(layers.Permute,\n                kwargs={'dims': (2, 1)},\n                input_shape=(3, 2, 4))\n \n \n-@keras_test\n def test_flatten():\n \n     def test_4d():\n@@ -161,14 +154,12 @@ def test_flatten():\n     test_5d()\n \n \n-@keras_test\n def test_repeat_vector():\n     layer_test(layers.RepeatVector,\n                kwargs={'n': 3},\n                input_shape=(3, 2))\n \n \n-@keras_test\n def test_lambda():\n     layer_test(layers.Lambda,\n                kwargs={'function': lambda x: x + 1},\n@@ -285,7 +276,6 @@ def test_lambda():\n     ld = deserialize_layer({'class_name': 'Lambda', 'config': config})\n \n \n-@keras_test\n @pytest.mark.skipif((K.backend() == 'theano'),\n                     reason=\"theano cannot compute \"\n                            \"the output shape automatically.\")\n@@ -295,7 +285,6 @@ def test_lambda_output_shape():\n                input_shape=(3, 2, 4))\n \n \n-@keras_test\n def test_dense():\n     layer_test(layers.Dense,\n                kwargs={'units': 3},\n@@ -329,7 +318,6 @@ def test_dense():\n     assert len(layer.losses) == 2\n \n \n-@keras_test\n def test_activity_regularization():\n     layer = layers.ActivityRegularization(l1=0.01, l2=0.01)\n \n@@ -348,7 +336,6 @@ def test_activity_regularization():\n     model.compile('rmsprop', 'mse')\n \n \n-@keras_test\n def test_sequential_as_downstream_of_masking_layer():\n \n     inputs = layers.Input(shape=(3, 4))\n\n@@ -4,7 +4,6 @@ from numpy.testing import assert_allclose\n import keras\n import keras.backend as K\n from keras.utils.test_utils import layer_test\n-from keras.utils.test_utils import keras_test\n import time\n \n \n@@ -14,7 +13,6 @@ skipif_no_tf_gpu = pytest.mark.skipif(\n     reason='Requires TensorFlow backend and a GPU')\n \n \n-@keras_test\n @skipif_no_tf_gpu\n def test_cudnn_rnn_canonical_to_params_lstm():\n     units = 1\n@@ -72,7 +70,6 @@ def test_cudnn_rnn_canonical_to_params_lstm():\n     assert diff < 1e-8\n \n \n-@keras_test\n @skipif_no_tf_gpu\n def test_cudnn_rnn_canonical_to_params_gru():\n     units = 7\n@@ -122,7 +119,6 @@ def test_cudnn_rnn_canonical_to_params_gru():\n     assert diff < 1e-8\n \n \n-@keras_test\n @pytest.mark.parametrize('rnn_type', ['lstm', 'gru'], ids=['LSTM', 'GRU'])\n @skipif_no_tf_gpu\n def test_cudnn_rnn_timing(rnn_type):\n@@ -161,7 +157,6 @@ def test_cudnn_rnn_timing(rnn_type):\n     assert speedup > 3\n \n \n-@keras_test\n @skipif_no_tf_gpu\n def test_cudnn_rnn_basics():\n     input_size = 10\n@@ -189,7 +184,6 @@ def test_cudnn_rnn_basics():\n                     input_shape=(num_samples, timesteps, input_size))\n \n \n-@keras_test\n @skipif_no_tf_gpu\n def test_trainability():\n     input_size = 10\n@@ -210,7 +204,6 @@ def test_trainability():\n         assert len(layer.non_trainable_weights) == 0\n \n \n-@keras_test\n @skipif_no_tf_gpu\n def test_regularizer():\n     input_size = 10\n@@ -237,7 +230,6 @@ def test_regularizer():\n         assert len(layer.get_losses_for(x)) == 1\n \n \n-@keras_test\n @skipif_no_tf_gpu\n def test_return_state():\n     input_size = 10\n@@ -261,7 +253,6 @@ def test_return_state():\n             keras.backend.eval(layer.states[0]), state, atol=1e-4)\n \n \n-@keras_test\n @skipif_no_tf_gpu\n def test_specify_initial_state_keras_tensor():\n     input_size = 10\n@@ -290,7 +281,6 @@ def test_specify_initial_state_keras_tensor():\n         model.fit([inputs] + initial_state, targets)\n \n \n-@keras_test\n @skipif_no_tf_gpu\n def test_statefulness():\n     input_size = 10\n@@ -337,7 +327,6 @@ def test_statefulness():\n         assert(out4.max() != out5.max())\n \n \n-@keras_test\n @skipif_no_tf_gpu\n def test_cudnnrnn_bidirectional():\n     rnn = keras.layers.CuDNNGRU\n\n@@ -1,11 +1,10 @@\n import pytest\n-from keras.utils.test_utils import layer_test, keras_test\n+from keras.utils.test_utils import layer_test\n from keras.layers.embeddings import Embedding\n from keras.models import Sequential\n import keras.backend as K\n \n \n-@keras_test\n def test_embedding():\n     layer_test(Embedding,\n                kwargs={'output_dim': 4, 'input_dim': 10, 'input_length': 2},\n@@ -30,7 +29,6 @@ def test_embedding():\n                expected_output_dtype=K.floatx())\n \n \n-@keras_test\n def test_embedding_invalid():\n \n     # len(input_length) should be equal to len(input_shape) - 1\n\n@@ -1,11 +1,9 @@\n import pytest\n \n from keras.utils.test_utils import layer_test\n-from keras.utils.test_utils import keras_test\n from keras.layers import local\n \n \n-@keras_test\n def test_locallyconnected_1d():\n     num_samples = 2\n     num_steps = 8\n@@ -26,7 +24,6 @@ def test_locallyconnected_1d():\n                input_shape=(num_samples, num_steps, input_dim))\n \n \n-@keras_test\n def test_locallyconnected_2d():\n     num_samples = 5\n     filters = 3\n\n@@ -5,11 +5,9 @@ from keras import layers\n from keras import models\n from keras import backend as K\n from keras.utils.test_utils import layer_test\n-from keras.utils.test_utils import keras_test\n from keras.layers import merge\n \n \n-@keras_test\n def test_merge_add():\n     i1 = layers.Input(shape=(4, 5))\n     i2 = layers.Input(shape=(4, 5))\n@@ -42,7 +40,6 @@ def test_merge_add():\n         add_layer.compute_mask([i1, i2, i3], [None, None])\n \n \n-@keras_test\n def test_merge_subtract():\n     i1 = layers.Input(shape=(4, 5))\n     i2 = layers.Input(shape=(4, 5))\n@@ -77,7 +74,6 @@ def test_merge_subtract():\n         subtract_layer([i1])\n \n \n-@keras_test\n def test_merge_multiply():\n     i1 = layers.Input(shape=(4, 5))\n     i2 = layers.Input(shape=(4, 5))\n@@ -98,7 +94,6 @@ def test_merge_multiply():\n     assert_allclose(out, x1 * x2 * x3, atol=1e-4)\n \n \n-@keras_test\n def test_merge_average():\n     i1 = layers.Input(shape=(4, 5))\n     i2 = layers.Input(shape=(4, 5))\n@@ -117,7 +112,6 @@ def test_merge_average():\n     assert_allclose(out, 0.5 * (x1 + x2), atol=1e-4)\n \n \n-@keras_test\n def test_merge_maximum():\n     i1 = layers.Input(shape=(4, 5))\n     i2 = layers.Input(shape=(4, 5))\n@@ -136,7 +130,6 @@ def test_merge_maximum():\n     assert_allclose(out, np.maximum(x1, x2), atol=1e-4)\n \n \n-@keras_test\n def test_merge_minimum():\n     i1 = layers.Input(shape=(4, 5))\n     i2 = layers.Input(shape=(4, 5))\n@@ -155,7 +148,6 @@ def test_merge_minimum():\n     assert_allclose(out, np.minimum(x1, x2), atol=1e-4)\n \n \n-@keras_test\n def test_merge_concatenate():\n     i1 = layers.Input(shape=(None, 5))\n     i2 = layers.Input(shape=(None, 5))\n@@ -208,7 +200,6 @@ def test_merge_concatenate():\n         concat_layer([i1])\n \n \n-@keras_test\n def test_merge_dot():\n     i1 = layers.Input(shape=(4,))\n     i2 = layers.Input(shape=(4,))\n@@ -238,7 +229,6 @@ def test_merge_dot():\n     assert_allclose(out, expected, atol=1e-4)\n \n \n-@keras_test\n def test_merge_broadcast():\n     # shapes provided\n     i1 = layers.Input(shape=(4, 5))\n@@ -288,7 +278,6 @@ def test_merge_broadcast():\n         K.ndim = k_ndim\n \n \n-@keras_test\n def test_masking_concatenate():\n     input1 = layers.Input(shape=(6,))\n     input2 = layers.Input(shape=(6,))\n\n@@ -1,11 +1,9 @@\n import pytest\n from keras.utils.test_utils import layer_test\n-from keras.utils.test_utils import keras_test\n from keras.layers import noise\n from keras import backend as K\n \n \n-@keras_test\n @pytest.mark.skipif((K.backend() == 'cntk'),\n                     reason=\"cntk does not support it yet\")\n def test_GaussianNoise():\n@@ -14,7 +12,6 @@ def test_GaussianNoise():\n                input_shape=(3, 2, 3))\n \n \n-@keras_test\n @pytest.mark.skipif((K.backend() == 'cntk'),\n                     reason=\"cntk does not support it yet\")\n def test_GaussianDropout():\n@@ -23,7 +20,6 @@ def test_GaussianDropout():\n                input_shape=(3, 2, 3))\n \n \n-@keras_test\n @pytest.mark.skipif((K.backend() == 'cntk'),\n                     reason=\"cntk does not support it yet\")\n def test_AlphaDropout():\n\n@@ -4,7 +4,7 @@ from numpy.testing import assert_allclose\n \n from keras.layers import Input\n from keras import regularizers\n-from keras.utils.test_utils import layer_test, keras_test\n+from keras.utils.test_utils import layer_test\n from keras.layers import normalization\n from keras.models import Sequential, Model\n from keras import backend as K\n@@ -16,7 +16,6 @@ input_4 = np.expand_dims(np.arange(10.), axis=1)\n input_shapes = [np.ones((10, 10)), np.ones((10, 10, 10))]\n \n \n-@keras_test\n def test_basic_batchnorm():\n     layer_test(normalization.BatchNormalization,\n                kwargs={'momentum': 0.9,\n@@ -45,7 +44,6 @@ def test_basic_batchnorm():\n                    input_shape=(3, 4, 2, 4))\n \n \n-@keras_test\n def test_batchnorm_correctness_1d():\n     np.random.seed(1337)\n     model = Sequential()\n@@ -64,7 +62,6 @@ def test_batchnorm_correctness_1d():\n     assert_allclose(out.std(), 1.0, atol=1e-1)\n \n \n-@keras_test\n def test_batchnorm_correctness_2d():\n     np.random.seed(1337)\n     model = Sequential()\n@@ -84,7 +81,6 @@ def test_batchnorm_correctness_2d():\n     assert_allclose(out.std(axis=(0, 2)), 1.0, atol=1.1e-1)\n \n \n-@keras_test\n def test_batchnorm_training_argument():\n     np.random.seed(1337)\n     bn1 = normalization.BatchNormalization(input_shape=(10,))\n@@ -109,7 +105,6 @@ def test_batchnorm_training_argument():\n     assert not bn2.updates\n \n \n-@keras_test\n def test_batchnorm_mode_twice():\n     # This is a regression test for issue #4881 with the old\n     # batch normalization functions in the Theano backend.\n@@ -123,7 +118,6 @@ def test_batchnorm_mode_twice():\n     model.predict(x)\n \n \n-@keras_test\n def test_batchnorm_convnet():\n     np.random.seed(1337)\n     model = Sequential()\n@@ -143,7 +137,6 @@ def test_batchnorm_convnet():\n     assert_allclose(np.std(out, axis=(0, 2, 3)), 1.0, atol=1e-1)\n \n \n-@keras_test\n @pytest.mark.skipif((K.backend() == 'theano'),\n                     reason='Bug with theano backend')\n def test_batchnorm_convnet_no_center_no_scale():\n@@ -163,7 +156,6 @@ def test_batchnorm_convnet_no_center_no_scale():\n     assert_allclose(np.std(out, axis=(0, 2, 3)), 1.0, atol=1e-1)\n \n \n-@keras_test\n def test_shared_batchnorm():\n     '''Test that a BN layer can be shared\n     across different data streams.\n@@ -191,7 +183,6 @@ def test_shared_batchnorm():\n     new_model.train_on_batch(x, x)\n \n \n-@keras_test\n def test_that_trainable_disables_updates():\n     val_a = np.random.random((10, 4))\n     val_out = np.random.random((10, 4))\n@@ -230,7 +221,6 @@ def test_that_trainable_disables_updates():\n     assert_allclose(x1, x2, atol=1e-7)\n \n \n-@keras_test\n def test_batchnorm_trainable():\n     bn_mean = 0.5\n     bn_std = 10.\n\n@@ -1,14 +1,13 @@\n import numpy as np\n import pytest\n \n-from keras.utils.test_utils import keras_test, layer_test\n+from keras.utils.test_utils import layer_test\n from keras.layers import pooling\n from keras.layers import Masking\n from keras.layers import convolutional\n from keras.models import Sequential\n \n \n-@keras_test\n @pytest.mark.parametrize(\n     'padding,stride,data_format',\n     [(padding, stride, data_format)\n@@ -24,7 +23,6 @@ def test_maxpooling_1d(padding, stride, data_format):\n                input_shape=(3, 5, 4))\n \n \n-@keras_test\n @pytest.mark.parametrize(\n     'strides',\n     [(1, 1), (2, 3)]\n@@ -38,7 +36,6 @@ def test_maxpooling_2d(strides):\n                input_shape=(3, 5, 6, 4))\n \n \n-@keras_test\n @pytest.mark.parametrize(\n     'strides,data_format,input_shape',\n     [(2, None, (3, 11, 12, 10, 4)),\n@@ -54,7 +51,6 @@ def test_maxpooling_3d(strides, data_format, input_shape):\n                input_shape=input_shape)\n \n \n-@keras_test\n @pytest.mark.parametrize(\n     'padding,stride,data_format',\n     [(padding, stride, data_format)\n@@ -70,7 +66,6 @@ def test_averagepooling_1d(padding, stride, data_format):\n                input_shape=(3, 5, 4))\n \n \n-@keras_test\n @pytest.mark.parametrize(\n     'strides,padding,data_format,input_shape',\n     [((2, 2), 'same', None, (3, 5, 6, 4)),\n@@ -86,7 +81,6 @@ def test_averagepooling_2d(strides, padding, data_format, input_shape):\n                input_shape=input_shape)\n \n \n-@keras_test\n @pytest.mark.parametrize(\n     'strides,data_format,input_shape',\n     [(2, None, (3, 11, 12, 10, 4)),\n@@ -103,7 +97,6 @@ def test_averagepooling_3d(strides, data_format, input_shape):\n                input_shape=input_shape)\n \n \n-@keras_test\n @pytest.mark.parametrize(\n     'data_format,pooling_class',\n     [(data_format, pooling_class)\n@@ -117,7 +110,6 @@ def test_globalpooling_1d(data_format, pooling_class):\n                input_shape=(3, 4, 5))\n \n \n-@keras_test\n def test_globalpooling_1d_supports_masking():\n     # Test GlobalAveragePooling1D supports masking\n     model = Sequential()\n@@ -131,7 +123,6 @@ def test_globalpooling_1d_supports_masking():\n     assert np.array_equal(output[0], model_input[0, 0, :])\n \n \n-@keras_test\n @pytest.mark.parametrize(\n     'data_format,pooling_class',\n     [(data_format, pooling_class)\n@@ -145,7 +136,6 @@ def test_globalpooling_2d(data_format, pooling_class):\n                input_shape=(3, 4, 5, 6))\n \n \n-@keras_test\n @pytest.mark.parametrize(\n     'data_format,pooling_class',\n     [(data_format, pooling_class)\n\n@@ -18,7 +18,6 @@ num_samples, timesteps, embedding_dim, units = 2, 5, 4, 3\n embedding_num = 12\n \n \n-@keras_test\n def rnn_test(f):\n     \"\"\"\n     All the recurrent layers share the same interface,\n@@ -32,7 +31,6 @@ def rnn_test(f):\n     ])(f)\n \n \n-@keras_test\n def rnn_cell_test(f):\n     f = keras_test(f)\n     return pytest.mark.parametrize('cell_class', [\n@@ -239,7 +237,6 @@ def test_trainability(layer_class):\n     assert len(layer.non_trainable_weights) == 0\n \n \n-@keras_test\n def test_masking_layer():\n     ''' This test based on a previously failing issue here:\n     https://github.com/keras-team/keras/issues/1567\n@@ -430,7 +427,6 @@ def test_state_reuse_with_dropout(layer_class):\n     outputs = model.predict(inputs)\n \n \n-@keras_test\n def test_minimal_rnn_cell_non_layer():\n \n     class MinimalRNNCell(object):\n@@ -466,7 +462,6 @@ def test_minimal_rnn_cell_non_layer():\n     model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 32)))\n \n \n-@keras_test\n def test_minimal_rnn_cell_non_layer_multiple_states():\n \n     class MinimalRNNCell(object):\n@@ -506,7 +501,6 @@ def test_minimal_rnn_cell_non_layer_multiple_states():\n     model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 32)))\n \n \n-@keras_test\n def test_minimal_rnn_cell_layer():\n \n     class MinimalRNNCell(keras.layers.Layer):\n@@ -632,7 +626,6 @@ def test_builtin_rnn_cell_layer(cell_class):\n     assert_allclose(y_np, y_np_2, atol=1e-4)\n \n \n-@keras_test\n @pytest.mark.skipif((K.backend() in ['cntk', 'theano']),\n                     reason='Not supported.')\n def test_stacked_rnn_dropout():\n@@ -649,7 +642,6 @@ def test_stacked_rnn_dropout():\n     model.train_on_batch(x_np, y_np)\n \n \n-@keras_test\n def test_stacked_rnn_attributes():\n     cells = [recurrent.LSTMCell(3),\n              recurrent.LSTMCell(3, kernel_regularizer='l2')]\n@@ -672,7 +664,6 @@ def test_stacked_rnn_attributes():\n     assert layer.get_losses_for(x) == [y]\n \n \n-@keras_test\n def test_stacked_rnn_compute_output_shape():\n     cells = [recurrent.LSTMCell(3),\n              recurrent.LSTMCell(6)]\n@@ -711,7 +702,6 @@ def test_batch_size_equal_one(layer_class):\n     model.train_on_batch(x, y)\n \n \n-@keras_test\n def test_rnn_cell_with_constants_layer():\n \n     class RNNCellWithConstants(keras.layers.Layer):\n@@ -820,7 +810,6 @@ def test_rnn_cell_with_constants_layer():\n     assert_allclose(y_np, y_np_2, atol=1e-4)\n \n \n-@keras_test\n def test_rnn_cell_with_constants_layer_passing_initial_state():\n \n     class RNNCellWithConstants(keras.layers.Layer):\n@@ -920,7 +909,6 @@ def test_rnn_cell_identity_initializer(layer_class):\n                           np.concatenate([np.identity(units)] * num_kernels, axis=1))\n \n \n-@keras_test\n @pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\n def test_inconsistent_output_state_size():\n \n\n@@ -2,7 +2,6 @@ import pytest\n import numpy as np\n import copy\n from numpy.testing import assert_allclose\n-from keras.utils.test_utils import keras_test\n from keras.utils import CustomObjectScope\n from keras.layers import wrappers, Input, Layer\n from keras.layers import RNN\n@@ -12,7 +11,6 @@ from keras import backend as K\n from keras.utils.generic_utils import object_list_uid, to_list\n \n \n-@keras_test\n def test_TimeDistributed():\n     # first, test with Dense layer\n     model = Sequential()\n@@ -127,7 +125,6 @@ def test_TimeDistributed():\n     assert K.int_shape(td._input_map[uid]) == (None, 2)\n \n \n-@keras_test\n @pytest.mark.skipif((K.backend() == 'cntk'),\n                     reason='Flaky with CNTK backend')\n def test_TimeDistributed_learning_phase():\n@@ -140,7 +137,6 @@ def test_TimeDistributed_learning_phase():\n     assert_allclose(np.mean(y), 0., atol=1e-1, rtol=1e-1)\n \n \n-@keras_test\n def test_TimeDistributed_trainable():\n     # test layers that need learning_phase to be set\n     x = Input(shape=(3, 2))\n@@ -156,7 +152,6 @@ def test_TimeDistributed_trainable():\n     assert len(layer.trainable_weights) == 2\n \n \n-@keras_test\n @pytest.mark.skipif((K.backend() == 'cntk'),\n                     reason='Unknown timestamps for RNN not supported in CNTK.')\n def test_TimeDistributed_with_masked_embedding_and_unspecified_shape():\n@@ -188,7 +183,6 @@ def test_TimeDistributed_with_masked_embedding_and_unspecified_shape():\n     assert mask_outputs[-1] is None  # final layer\n \n \n-@keras_test\n def test_TimeDistributed_with_masking_layer():\n     # test with Masking layer\n     model = Sequential()\n@@ -211,7 +205,6 @@ def test_TimeDistributed_with_masking_layer():\n     assert np.array_equal(mask_outputs_val[1], np.any(model_input, axis=-1))\n \n \n-@keras_test\n def test_regularizers():\n     model = Sequential()\n     model.add(wrappers.TimeDistributed(\n@@ -231,7 +224,6 @@ def test_regularizers():\n     assert len(model.losses) == 1\n \n \n-@keras_test\n def test_Bidirectional():\n     rnn = layers.SimpleRNN\n     samples = 2\n@@ -286,7 +278,6 @@ def test_Bidirectional():\n         model.fit(x, y, epochs=1, batch_size=1)\n \n \n-@keras_test\n @pytest.mark.skipif((K.backend() == 'cntk'),\n                     reason='Unknown timestamps not supported in CNTK.')\n def test_Bidirectional_dynamic_timesteps():\n@@ -311,7 +302,6 @@ def test_Bidirectional_dynamic_timesteps():\n         model.fit(x, y, epochs=1, batch_size=1)\n \n \n-@keras_test\n @pytest.mark.parametrize('merge_mode', ['sum', 'mul', 'ave', 'concat', None])\n def test_Bidirectional_merged_value(merge_mode):\n     rnn = layers.LSTM\n@@ -372,7 +362,6 @@ def test_Bidirectional_merged_value(merge_mode):\n         assert_allclose(state_birnn, state_inner, atol=1e-5)\n \n \n-@keras_test\n @pytest.mark.skipif(K.backend() == 'theano', reason='Not supported.')\n @pytest.mark.parametrize('merge_mode', ['sum', 'concat', None])\n def test_Bidirectional_dropout(merge_mode):\n@@ -403,7 +392,6 @@ def test_Bidirectional_dropout(merge_mode):\n         assert_allclose(x1, x2, atol=1e-5)\n \n \n-@keras_test\n def test_Bidirectional_state_reuse():\n     rnn = layers.LSTM\n     samples = 2\n@@ -431,7 +419,6 @@ def test_Bidirectional_state_reuse():\n     outputs = model.predict(inputs)\n \n \n-@keras_test\n def test_Bidirectional_with_constants():\n     class RNNCellWithConstants(Layer):\n         def __init__(self, units, **kwargs):\n@@ -512,7 +499,6 @@ def test_Bidirectional_with_constants():\n     assert_allclose(y_np, y_np_3, atol=1e-4)\n \n \n-@keras_test\n def test_Bidirectional_with_constants_layer_passing_initial_state():\n     class RNNCellWithConstants(Layer):\n         def __init__(self, units, **kwargs):\n@@ -603,7 +589,6 @@ def test_Bidirectional_with_constants_layer_passing_initial_state():\n     assert_allclose(y_np, y_np_3, atol=1e-4)\n \n \n-@keras_test\n def test_Bidirectional_trainable():\n     # test layers that need learning_phase to be set\n     x = Input(shape=(3, 2))\n@@ -616,7 +601,6 @@ def test_Bidirectional_trainable():\n     assert len(layer.trainable_weights) == 6\n \n \n-@keras_test\n def test_Bidirectional_updates():\n     x = Input(shape=(3, 2))\n     layer = wrappers.Bidirectional(layers.SimpleRNN(3))\n@@ -632,7 +616,6 @@ def test_Bidirectional_updates():\n     assert len(layer.get_updates_for(x)) == 2\n \n \n-@keras_test\n def test_Bidirectional_losses():\n     x = Input(shape=(3, 2))\n     layer = wrappers.Bidirectional(\n\n@@ -1,11 +1,9 @@\n import pytest\n import json\n-from keras.utils.test_utils import keras_test\n import keras\n import numpy as np\n \n \n-@keras_test\n def test_dense_legacy_interface():\n     old_layer = keras.layers.Dense(input_dim=3, output_dim=2, name='d')\n     new_layer = keras.layers.Dense(2, input_shape=(3,), name='d')\n@@ -29,7 +27,6 @@ def test_dense_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n-@keras_test\n def test_dropout_legacy_interface():\n     old_layer = keras.layers.Dropout(p=3, name='drop')\n     new_layer1 = keras.layers.Dropout(rate=3, name='drop')\n@@ -38,7 +35,6 @@ def test_dropout_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer2.get_config())\n \n \n-@keras_test\n def test_embedding_legacy_interface():\n     old_layer = keras.layers.Embedding(4, 2, name='d')\n     new_layer = keras.layers.Embedding(output_dim=2, input_dim=4, name='d')\n@@ -59,7 +55,6 @@ def test_embedding_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n-@keras_test\n def test_maxpooling1d_legacy_interface():\n     old_layer = keras.layers.MaxPool1D(pool_length=2,\n                                        border_mode='valid',\n@@ -76,7 +71,6 @@ def test_maxpooling1d_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n-@keras_test\n def test_avgpooling1d_legacy_interface():\n     old_layer = keras.layers.AvgPool1D(pool_length=2,\n                                        border_mode='valid',\n@@ -89,21 +83,18 @@ def test_avgpooling1d_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n-@keras_test\n def test_prelu_legacy_interface():\n     old_layer = keras.layers.PReLU(init='zero', name='p')\n     new_layer = keras.layers.PReLU('zero', name='p')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n-@keras_test\n def test_gaussiannoise_legacy_interface():\n     old_layer = keras.layers.GaussianNoise(sigma=0.5, name='gn')\n     new_layer = keras.layers.GaussianNoise(stddev=0.5, name='gn')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n-@keras_test\n def test_lstm_legacy_interface():\n     old_layer = keras.layers.LSTM(input_shape=[3, 5], output_dim=2, name='d')\n     new_layer = keras.layers.LSTM(2, input_shape=[3, 5], name='d')\n@@ -179,7 +170,6 @@ def test_lstm_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n-@keras_test\n def test_simplernn_legacy_interface():\n     old_layer = keras.layers.SimpleRNN(input_shape=[3, 5], output_dim=2, name='d')\n     new_layer = keras.layers.SimpleRNN(2, input_shape=[3, 5], name='d')\n@@ -204,7 +194,6 @@ def test_simplernn_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n-@keras_test\n def test_gru_legacy_interface():\n     old_layer = keras.layers.GRU(input_shape=[3, 5], output_dim=2, name='d')\n     new_layer = keras.layers.GRU(2, input_shape=[3, 5], name='d')\n@@ -231,7 +220,6 @@ def test_gru_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n-@keras_test\n def test_gaussiandropout_legacy_interface():\n     old_layer = keras.layers.GaussianDropout(p=0.6, name='drop')\n     new_layer1 = keras.layers.GaussianDropout(rate=0.6, name='drop')\n@@ -240,7 +228,6 @@ def test_gaussiandropout_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer2.get_config())\n \n \n-@keras_test\n def test_maxpooling2d_legacy_interface():\n     old_layer = keras.layers.MaxPooling2D(\n         pool_size=(2, 2), border_mode='valid', name='maxpool2d')\n@@ -273,7 +260,6 @@ def test_maxpooling2d_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n-@keras_test\n def test_avgpooling2d_legacy_interface():\n     old_layer = keras.layers.AveragePooling2D(\n         pool_size=(2, 2), border_mode='valid', name='avgpooling2d')\n@@ -308,7 +294,6 @@ def test_avgpooling2d_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n-@keras_test\n def test_maxpooling3d_legacy_interface():\n     old_layer = keras.layers.MaxPooling3D(\n         pool_size=(2, 2, 2), border_mode='valid', name='maxpool3d')\n@@ -343,7 +328,6 @@ def test_maxpooling3d_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n-@keras_test\n def test_avgpooling3d_legacy_interface():\n     old_layer = keras.layers.AveragePooling3D(\n         pool_size=(2, 2, 2), border_mode='valid', name='avgpooling3d')\n@@ -379,7 +363,6 @@ def test_avgpooling3d_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n-@keras_test\n def test_global_maxpooling2d_legacy_interface():\n     old_layer = keras.layers.GlobalMaxPooling2D(dim_ordering='tf',\n                                                 name='global_maxpool2d')\n@@ -399,7 +382,6 @@ def test_global_maxpooling2d_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n-@keras_test\n def test_global_avgpooling2d_legacy_interface():\n     old_layer = keras.layers.GlobalAveragePooling2D(dim_ordering='tf',\n                                                     name='global_avgpool2d')\n@@ -419,7 +401,6 @@ def test_global_avgpooling2d_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n-@keras_test\n def test_global_maxpooling3d_legacy_interface():\n     old_layer = keras.layers.GlobalMaxPooling3D(dim_ordering='tf',\n                                                 name='global_maxpool3d')\n@@ -439,7 +420,6 @@ def test_global_maxpooling3d_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n-@keras_test\n def test_global_avgpooling3d_legacy_interface():\n     old_layer = keras.layers.GlobalAveragePooling3D(dim_ordering='tf',\n                                                     name='global_avgpool3d')\n@@ -459,7 +439,6 @@ def test_global_avgpooling3d_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n-@keras_test\n def test_upsampling1d_legacy_interface():\n     old_layer = keras.layers.UpSampling1D(length=3, name='us1d')\n     new_layer_1 = keras.layers.UpSampling1D(size=3, name='us1d')\n@@ -468,7 +447,6 @@ def test_upsampling1d_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer_2.get_config())\n \n \n-@keras_test\n def test_upsampling2d_legacy_interface():\n     old_layer = keras.layers.UpSampling2D((2, 2), dim_ordering='tf', name='us2d')\n     new_layer = keras.layers.UpSampling2D((2, 2), data_format='channels_last',\n@@ -476,7 +454,6 @@ def test_upsampling2d_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n-@keras_test\n def test_upsampling3d_legacy_interface():\n     old_layer = keras.layers.UpSampling3D((2, 2, 2),\n                                           dim_ordering='tf',\n@@ -487,7 +464,6 @@ def test_upsampling3d_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n-@keras_test\n def test_conv2d_legacy_interface():\n     old_layer = keras.layers.Convolution2D(5, 3, 3, name='conv')\n     new_layer = keras.layers.Conv2D(5, (3, 3), name='conv')\n@@ -524,7 +500,6 @@ def test_conv2d_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n-@keras_test\n def test_deconv2d_legacy_interface():\n     old_layer = keras.layers.Deconvolution2D(5, 3, 3, (6, 7, 5), name='deconv')\n     new_layer = keras.layers.Conv2DTranspose(5, (3, 3), name='deconv')\n@@ -570,7 +545,6 @@ def test_deconv2d_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n-@keras_test\n def test_conv1d_legacy_interface():\n     old_layer = keras.layers.Convolution1D(5,\n                                            filter_length=3,\n@@ -601,7 +575,6 @@ def test_conv1d_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n-@keras_test\n def test_separable_conv2d_legacy_interface():\n     old_layer = keras.layers.SeparableConv2D(5, 3, 3, name='conv')\n     new_layer = keras.layers.SeparableConv2D(5, (3, 3), name='conv')\n@@ -641,7 +614,6 @@ def test_separable_conv2d_legacy_interface():\n     assert old_config == new_config\n \n \n-@keras_test\n def test_conv3d_legacy_interface():\n     old_layer = keras.layers.Convolution3D(5, 3, 3, 4, name='conv')\n     new_layer = keras.layers.Conv3D(5, (3, 3, 4), name='conv')\n@@ -689,7 +661,6 @@ def test_conv3d_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n-@keras_test\n def test_convlstm2d_legacy_interface():\n     old_layer = keras.layers.ConvLSTM2D(5, 3, 3, name='conv')\n     new_layer = keras.layers.ConvLSTM2D(5, (3, 3), name='conv')\n@@ -734,7 +705,6 @@ def test_convlstm2d_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n-@keras_test\n def test_batchnorm_legacy_interface():\n     old_layer = keras.layers.BatchNormalization(mode=0, name='bn')\n     new_layer = keras.layers.BatchNormalization(name='bn')\n@@ -750,7 +720,6 @@ def test_batchnorm_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n-@keras_test\n def test_atrousconv1d_legacy_interface():\n     old_layer = keras.layers.AtrousConvolution1D(5, 3,\n                                                  init='normal',\n@@ -775,7 +744,6 @@ def test_atrousconv1d_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n-@keras_test\n def test_atrousconv2d_legacy_interface():\n     old_layer = keras.layers.AtrousConvolution2D(\n         5, 3, 3,\n@@ -803,7 +771,6 @@ def test_atrousconv2d_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n-@keras_test\n def test_zeropadding2d_legacy_interface():\n     old_layer = keras.layers.ZeroPadding2D(padding={'right_pad': 4,\n                                                     'bottom_pad': 2,\n@@ -817,7 +784,6 @@ def test_zeropadding2d_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n-@keras_test\n def test_zeropadding3d_legacy_interface():\n     old_layer = keras.layers.ZeroPadding3D((2, 2, 2),\n                                            dim_ordering='tf',\n@@ -828,21 +794,18 @@ def test_zeropadding3d_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n-@keras_test\n def test_cropping2d_legacy_interface():\n     old_layer = keras.layers.Cropping2D(dim_ordering='tf', name='c2d')\n     new_layer = keras.layers.Cropping2D(data_format='channels_last', name='c2d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n-@keras_test\n def test_cropping3d_legacy_interface():\n     old_layer = keras.layers.Cropping3D(dim_ordering='tf', name='c3d')\n     new_layer = keras.layers.Cropping3D(data_format='channels_last', name='c3d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n-@keras_test\n def test_generator_methods_interface():\n     def train_generator():\n         x = np.random.randn(2, 2)\n@@ -890,7 +853,6 @@ def test_spatialdropout1d_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer_2.get_config())\n \n \n-@keras_test\n def test_spatialdropout2d_legacy_interface():\n     old_layer = keras.layers.SpatialDropout2D(p=0.5,\n                                               dim_ordering='tf',\n@@ -905,7 +867,6 @@ def test_spatialdropout2d_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer_2.get_config())\n \n \n-@keras_test\n def test_spatialdropout3d_legacy_interface():\n     old_layer = keras.layers.SpatialDropout3D(p=0.5,\n                                               dim_ordering='tf',\n@@ -920,7 +881,6 @@ def test_spatialdropout3d_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer_2.get_config())\n \n \n-@keras_test\n def test_optimizer_get_updates_legacy_interface():\n     for optimizer_cls in [keras.optimizers.RMSprop,\n                           keras.optimizers.SGD,\n\n@@ -1,13 +1,11 @@\n import pytest\n \n-from keras.utils.test_utils import keras_test\n from keras.utils.test_utils import layer_test\n from keras.legacy import layers as legacy_layers\n from keras import regularizers\n from keras import constraints\n \n \n-@keras_test\n def test_highway():\n     layer_test(legacy_layers.Highway,\n                kwargs={},\n@@ -22,7 +20,6 @@ def test_highway():\n                input_shape=(3, 2))\n \n \n-@keras_test\n def test_maxout_dense():\n     layer_test(legacy_layers.MaxoutDense,\n                kwargs={'output_dim': 3},\n\n@@ -5,7 +5,6 @@ from numpy.testing import assert_allclose\n import keras\n from keras import metrics\n from keras import backend as K\n-from keras.utils.test_utils import keras_test\n \n all_metrics = [\n     metrics.binary_accuracy,\n@@ -29,7 +28,6 @@ all_sparse_metrics = [\n ]\n \n \n-@keras_test\n def test_metrics():\n     y_a = K.variable(np.random.random((6, 7)))\n     y_b = K.variable(np.random.random((6, 7)))\n@@ -39,7 +37,6 @@ def test_metrics():\n         assert K.eval(output).shape == (6,)\n \n \n-@keras_test\n def test_sparse_metrics():\n     for metric in all_sparse_metrics:\n         y_a = K.variable(np.random.randint(0, 7, (6,)), dtype=K.floatx())\n@@ -47,7 +44,6 @@ def test_sparse_metrics():\n         assert K.eval(metric(y_a, y_b)).shape == (6,)\n \n \n-@keras_test\n def test_sparse_categorical_accuracy_correctness():\n     y_a = K.variable(np.random.randint(0, 7, (6,)), dtype=K.floatx())\n     y_b = K.variable(np.random.random((6, 7)), dtype=K.floatx())\n@@ -84,7 +80,6 @@ def test_invalid_get():\n \n @pytest.mark.skipif((K.backend() == 'cntk'),\n                     reason='CNTK backend does not support top_k yet')\n-@keras_test\n def test_top_k_categorical_accuracy():\n     y_pred = K.variable(np.array([[0.3, 0.2, 0.1], [0.1, 0.2, 0.7]]))\n     y_true = K.variable(np.array([[0, 1, 0], [1, 0, 0]]))\n@@ -101,7 +96,6 @@ def test_top_k_categorical_accuracy():\n \n @pytest.mark.skipif((K.backend() == 'cntk'),\n                     reason='CNTK backend does not support top_k yet')\n-@keras_test\n def test_sparse_top_k_categorical_accuracy():\n     y_pred = K.variable(np.array([[0.3, 0.2, 0.1], [0.1, 0.2, 0.7]]))\n     y_true = K.variable(np.array([[1], [0]]))\n@@ -119,7 +113,6 @@ def test_sparse_top_k_categorical_accuracy():\n     assert failure_result == 0\n \n \n-@keras_test\n @pytest.mark.parametrize('metrics_mode', ['list', 'dict'])\n def test_stateful_metrics(metrics_mode):\n     np.random.seed(1334)\n\n@@ -7,7 +7,6 @@ from keras.utils import test_utils\n from keras import optimizers, Input\n from keras.models import Sequential, Model\n from keras.layers.core import Dense, Activation, Lambda\n-from keras.utils.test_utils import keras_test\n from keras.utils.np_utils import to_categorical\n from keras import backend as K\n \n@@ -64,7 +63,6 @@ def _test_optimizer(optimizer, target=0.75):\n     assert_allclose(bias, 2.)\n \n \n-@keras_test\n @pytest.mark.skipif((K.backend() != 'tensorflow'),\n                     reason=\"Only Tensorflow raises a \"\n                            \"ValueError if the gradient is null.\")\n@@ -80,66 +78,55 @@ def test_no_grad():\n                 batch_size=10, epochs=10)\n \n \n-@keras_test\n def test_sgd():\n     sgd = optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n     _test_optimizer(sgd)\n \n \n-@keras_test\n def test_rmsprop():\n     _test_optimizer(optimizers.RMSprop())\n     _test_optimizer(optimizers.RMSprop(decay=1e-3))\n \n \n-@keras_test\n def test_adagrad():\n     _test_optimizer(optimizers.Adagrad())\n     _test_optimizer(optimizers.Adagrad(decay=1e-3))\n \n \n-@keras_test\n def test_adadelta():\n     _test_optimizer(optimizers.Adadelta(), target=0.6)\n     _test_optimizer(optimizers.Adadelta(decay=1e-3), target=0.6)\n \n \n-@keras_test\n def test_adam():\n     _test_optimizer(optimizers.Adam())\n     _test_optimizer(optimizers.Adam(decay=1e-3))\n \n \n-@keras_test\n def test_adamax():\n     _test_optimizer(optimizers.Adamax())\n     _test_optimizer(optimizers.Adamax(decay=1e-3))\n \n \n-@keras_test\n def test_nadam():\n     _test_optimizer(optimizers.Nadam())\n \n \n-@keras_test\n def test_adam_amsgrad():\n     _test_optimizer(optimizers.Adam(amsgrad=True))\n     _test_optimizer(optimizers.Adam(amsgrad=True, decay=1e-3))\n \n \n-@keras_test\n def test_clipnorm():\n     sgd = optimizers.SGD(lr=0.01, momentum=0.9, clipnorm=0.5)\n     _test_optimizer(sgd)\n \n \n-@keras_test\n def test_clipvalue():\n     sgd = optimizers.SGD(lr=0.01, momentum=0.9, clipvalue=0.5)\n     _test_optimizer(sgd)\n \n \n-@keras_test\n @pytest.mark.skipif((K.backend() != 'tensorflow'),\n                     reason='Requires TensorFlow backend')\n def test_tfoptimizer():\n\n@@ -17,7 +17,6 @@ from keras.layers.pooling import MaxPooling2D\n from keras.layers.pooling import GlobalAveragePooling1D\n from keras.layers.pooling import GlobalAveragePooling2D\n from keras.utils.test_utils import get_test_data\n-from keras.utils.test_utils import keras_test\n from keras import backend as K\n from keras.utils import np_utils\n try:\n@@ -47,7 +46,6 @@ def get_data_callbacks(num_train=train_samples,\n                          num_classes=num_classes)\n \n \n-@keras_test\n def test_TerminateOnNaN():\n     np.random.seed(1337)\n     (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n@@ -90,7 +88,6 @@ def test_TerminateOnNaN():\n     assert loss[0] == np.inf or np.isnan(loss[0])\n \n \n-@keras_test\n def test_stop_training_csv(tmpdir):\n     np.random.seed(1337)\n     fp = str(tmpdir / 'test.csv')\n@@ -139,7 +136,6 @@ def test_stop_training_csv(tmpdir):\n     os.remove(fp)\n \n \n-@keras_test\n def test_ModelCheckpoint(tmpdir):\n     np.random.seed(1337)\n     filepath = str(tmpdir / 'checkpoint.h5')\n@@ -212,7 +208,6 @@ def test_ModelCheckpoint(tmpdir):\n     assert not tmpdir.listdir()\n \n \n-@keras_test\n def test_EarlyStopping():\n     np.random.seed(1337)\n     (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n@@ -239,7 +234,6 @@ def test_EarlyStopping():\n                         validation_data=(X_test, y_test), callbacks=cbks, epochs=20)\n \n \n-@keras_test\n def test_EarlyStopping_reuse():\n     np.random.seed(1337)\n     patience = 3\n@@ -262,7 +256,6 @@ def test_EarlyStopping_reuse():\n     assert len(hist.epoch) >= patience\n \n \n-@keras_test\n def test_EarlyStopping_patience():\n     class DummyModel(object):\n         def __init__(self):\n@@ -294,7 +287,6 @@ def test_EarlyStopping_patience():\n     assert epochs_trained == 3\n \n \n-@keras_test\n def test_EarlyStopping_baseline():\n     class DummyModel(object):\n         def __init__(self):\n@@ -330,7 +322,6 @@ def test_EarlyStopping_baseline():\n     assert baseline_not_met == 2\n \n \n-@keras_test\n def test_EarlyStopping_final_weights():\n     class DummyModel(object):\n         def __init__(self):\n@@ -367,7 +358,6 @@ def test_EarlyStopping_final_weights():\n     assert early_stop.model.get_weights() == 4\n \n \n-@keras_test\n def test_EarlyStopping_final_weights_when_restoring_model_weights():\n     class DummyModel(object):\n         def __init__(self):\n@@ -408,7 +398,6 @@ def test_EarlyStopping_final_weights_when_restoring_model_weights():\n     assert early_stop.model.get_weights() == 2\n \n \n-@keras_test\n def test_LearningRateScheduler():\n     np.random.seed(1337)\n     (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n@@ -427,7 +416,6 @@ def test_LearningRateScheduler():\n     assert (float(K.get_value(model.optimizer.lr)) - 0.2) < K.epsilon()\n \n \n-@keras_test\n def test_ReduceLROnPlateau():\n     np.random.seed(1337)\n     (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n@@ -462,7 +450,6 @@ def test_ReduceLROnPlateau():\n     assert_allclose(float(K.get_value(model.optimizer.lr)), 0.1, atol=K.epsilon())\n \n \n-@keras_test\n def test_ReduceLROnPlateau_patience():\n     class DummyOptimizer(object):\n         def __init__(self):\n@@ -487,7 +474,6 @@ def test_ReduceLROnPlateau_patience():\n     assert all([lr == 1.0 for lr in lrs[:-1]]) and lrs[-1] < 1.0\n \n \n-@keras_test\n def test_ReduceLROnPlateau_backwards_compatibility():\n     import warnings\n     with warnings.catch_warnings(record=True) as ws:\n@@ -500,7 +486,6 @@ def test_ReduceLROnPlateau_backwards_compatibility():\n     assert reduce_on_plateau.min_delta == 1e-13\n \n \n-@keras_test\n def test_CSVLogger(tmpdir):\n     np.random.seed(1337)\n     filepath = str(tmpdir / 'log.tsv')\n@@ -556,7 +541,6 @@ def test_CSVLogger(tmpdir):\n     assert not tmpdir.listdir()\n \n \n-@keras_test\n def test_TensorBoard(tmpdir):\n     np.random.seed(np.random.randint(1, 1e7))\n     filepath = str(tmpdir / 'logs')\n@@ -639,7 +623,6 @@ def test_TensorBoard(tmpdir):\n     assert not tmpdir.listdir()\n \n \n-@keras_test\n @pytest.mark.skipif((K.backend() != 'tensorflow'),\n                     reason='Requires TensorFlow backend')\n def test_TensorBoard_histogram_freq_must_have_validation_data(tmpdir):\n@@ -709,7 +692,6 @@ def test_TensorBoard_histogram_freq_must_have_validation_data(tmpdir):\n     assert 'validation_data must be provided' in str(raised_exception.value)\n \n \n-@keras_test\n def test_TensorBoard_multi_input_output(tmpdir):\n     np.random.seed(np.random.randint(1, 1e7))\n     filepath = str(tmpdir / 'logs')\n@@ -788,7 +770,6 @@ def test_TensorBoard_multi_input_output(tmpdir):\n     assert not tmpdir.listdir()\n \n \n-@keras_test\n def test_TensorBoard_convnet(tmpdir):\n     np.random.seed(np.random.randint(1, 1e7))\n     filepath = str(tmpdir / 'logs')\n@@ -828,7 +809,6 @@ def test_TensorBoard_convnet(tmpdir):\n     assert not tmpdir.listdir()\n \n \n-@keras_test\n def test_TensorBoard_display_float_from_logs(tmpdir):\n     filepath = str(tmpdir / 'logs')\n \n@@ -860,7 +840,6 @@ def test_TensorBoard_display_float_from_logs(tmpdir):\n     assert not tmpdir.listdir()\n \n \n-@keras_test\n def test_CallbackValData():\n     np.random.seed(1337)\n     (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n@@ -905,7 +884,6 @@ def test_CallbackValData():\n     assert cbk.validation_data[2].shape == cbk2.validation_data[2].shape\n \n \n-@keras_test\n def test_LambdaCallback():\n     np.random.seed(1337)\n     (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n@@ -936,7 +914,6 @@ def test_LambdaCallback():\n     assert not p.is_alive()\n \n \n-@keras_test\n def test_TensorBoard_with_ReduceLROnPlateau(tmpdir):\n     import shutil\n     np.random.seed(np.random.randint(1, 1e7))\n@@ -970,7 +947,6 @@ def test_TensorBoard_with_ReduceLROnPlateau(tmpdir):\n     assert not tmpdir.listdir()\n \n \n-@keras_test\n def tests_RemoteMonitor():\n     (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n     y_test = np_utils.to_categorical(y_test)\n@@ -988,7 +964,6 @@ def tests_RemoteMonitor():\n                   validation_data=(X_test, y_test), callbacks=cbks, epochs=1)\n \n \n-@keras_test\n def tests_RemoteMonitorWithJsonPayload():\n     (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n     y_test = np_utils.to_categorical(y_test)\n\n@@ -10,7 +10,7 @@ import keras\n from keras.models import Sequential\n from keras.layers import Dense, Activation\n from keras.utils import np_utils\n-from keras.utils.test_utils import get_test_data, keras_test\n+from keras.utils.test_utils import get_test_data\n from keras.models import model_from_json, model_from_yaml\n from keras import losses\n from keras.engine.training_utils import make_batches\n@@ -34,7 +34,6 @@ def in_tmpdir(tmpdir):\n     assert not tmpdir.listdir()\n \n \n-@keras_test\n def test_sequential_pop():\n     model = Sequential()\n     model.add(Dense(num_hidden, input_dim=input_dim))\n@@ -67,7 +66,6 @@ def _get_test_data():\n     return (x_train, y_train), (x_test, y_test)\n \n \n-@keras_test\n def test_sequential_fit_generator():\n     (x_train, y_train), (x_test, y_test) = _get_test_data()\n \n@@ -106,7 +104,6 @@ def test_sequential_fit_generator():\n     model.evaluate(x_train, y_train)\n \n \n-@keras_test\n def test_sequential(in_tmpdir):\n     (x_train, y_train), (x_test, y_test) = _get_test_data()\n \n@@ -182,7 +179,6 @@ def test_sequential(in_tmpdir):\n     model_from_yaml(yaml_str)\n \n \n-@keras_test\n def test_nested_sequential(in_tmpdir):\n     (x_train, y_train), (x_test, y_test) = _get_test_data()\n \n@@ -248,7 +244,6 @@ def test_nested_sequential(in_tmpdir):\n     model_from_yaml(yaml_str)\n \n \n-@keras_test\n def test_sequential_count_params():\n     input_dim = 20\n     num_units = 10\n@@ -271,7 +266,6 @@ def test_sequential_count_params():\n     assert(n == model.count_params())\n \n \n-@keras_test\n def test_nested_sequential_trainability():\n     input_dim = 20\n     num_units = 10\n@@ -291,7 +285,6 @@ def test_nested_sequential_trainability():\n     assert len(model.trainable_weights) == 4\n \n \n-@keras_test\n def test_rebuild_model():\n     model = Sequential()\n     model.add(Dense(128, input_shape=(784,)))\n@@ -302,7 +295,6 @@ def test_rebuild_model():\n     assert(model.get_layer(index=-1).output_shape == (None, 32))\n \n \n-@keras_test\n def test_clone_functional_model():\n     val_a = np.random.random((10, 4))\n     val_b = np.random.random((10, 4))\n@@ -347,7 +339,6 @@ def test_clone_functional_model():\n     new_model.train_on_batch(None, val_out)\n \n \n-@keras_test\n def test_clone_sequential_model():\n     val_a = np.random.random((10, 4))\n     val_out = np.random.random((10, 4))\n@@ -382,7 +373,6 @@ def test_clone_sequential_model():\n     new_model.train_on_batch(None, val_out)\n \n \n-@keras_test\n def test_sequential_update_disabling():\n     val_a = np.random.random((10, 4))\n     val_out = np.random.random((10, 4))\n@@ -410,7 +400,6 @@ def test_sequential_update_disabling():\n     assert np.abs(np.sum(x1 - x2)) > 1e-5\n \n \n-@keras_test\n def test_sequential_deferred_build():\n     model = keras.models.Sequential()\n     model.add(keras.layers.Dense(3))\n@@ -437,7 +426,6 @@ def test_sequential_deferred_build():\n     assert len(new_model.weights) == 4\n \n \n-@keras_test\n def test_nested_sequential_deferred_build():\n     inner_model = keras.models.Sequential()\n     inner_model.add(keras.layers.Dense(3))\n\n@@ -7,12 +7,10 @@ from keras.utils.generic_utils import has_arg\n from keras.utils.generic_utils import Progbar\n from keras.utils.generic_utils import func_dump\n from keras.utils.generic_utils import func_load\n-from keras.utils.test_utils import keras_test\n from keras import activations\n from keras import regularizers\n \n \n-@keras_test\n def test_progbar():\n     values_s = [None,\n                 [['key1', 1], ['key2', 1e-4]],\n\n@@ -7,10 +7,8 @@ from keras.layers import Dense\n from keras.layers import Flatten\n from keras.models import Sequential\n from keras.utils import layer_utils\n-from keras.utils.test_utils import keras_test\n \n \n-@keras_test\n def test_convert_weights():\n     def get_model(shape, data_format):\n         model = Sequential()\n\n@@ -11,7 +11,6 @@ import pytest\n import time\n import tempfile\n import tensorflow as tf\n-from keras.utils.test_utils import keras_test\n from keras.preprocessing.image import ImageDataGenerator\n \n \n@@ -25,7 +24,6 @@ if K.backend() == 'tensorflow':\n                                     reason='Requires 8 GPUs.')\n \n \n-@keras_test\n def test_multi_gpu_simple_model():\n     print('####### test simple model')\n     num_samples = 1000\n@@ -52,7 +50,6 @@ def test_multi_gpu_simple_model():\n     parallel_model.fit(x, y, epochs=epochs)\n \n \n-@keras_test\n def test_multi_gpu_multi_io_model():\n     print('####### test multi-io model')\n     num_samples = 1000\n@@ -88,7 +85,6 @@ def test_multi_gpu_multi_io_model():\n     parallel_model.fit([a_x, b_x], [a_y, b_y], epochs=epochs)\n \n \n-@keras_test\n def test_multi_gpu_invalid_devices():\n     input_shape = (1000, 10)\n     model = keras.models.Sequential()\n@@ -120,7 +116,6 @@ def test_multi_gpu_invalid_devices():\n         parallel_model.fit(x, y, epochs=2)\n \n \n-@keras_test\n def test_serialization():\n     model = keras.models.Sequential()\n     model.add(keras.layers.Dense(3,\n@@ -272,7 +267,6 @@ def multi_gpu_application_folder_generator_benchmark():\n         print('%d gpus training:' % i, total_time)\n \n \n-@keras_test\n def test_multi_gpu_with_multi_input_layers():\n     inputs = keras.Input((4, 3))\n     init_state = keras.Input((3,))\n@@ -286,7 +280,6 @@ def test_multi_gpu_with_multi_input_layers():\n     parallel_model.train_on_batch(x, y)\n \n \n-@keras_test\n def test_multi_gpu_with_siamese():\n     input_shape = (3,)\n     nested_model = keras.models.Sequential([\n\n@@ -2,12 +2,10 @@ from __future__ import absolute_import\n from __future__ import print_function\n import pytest\n \n-from keras.utils.test_utils import keras_test\n from keras.models import Model, Sequential\n from keras.layers import Dense, Input\n \n \n-@keras_test\n def test_layer_trainability_switch():\n     # with constructor argument, in Sequential\n     model = Sequential()\n@@ -38,7 +36,6 @@ def test_layer_trainability_switch():\n     assert model.trainable_weights == []\n \n \n-@keras_test\n def test_model_trainability_switch():\n     # a non-trainable model has no trainable weights\n     x = Input(shape=(1,))\n@@ -54,7 +51,6 @@ def test_model_trainability_switch():\n     assert model.trainable_weights == []\n \n \n-@keras_test\n def test_nested_model_trainability():\n     # a Sequential inside a Model\n     inner_model = Sequential()\n\n@@ -4,12 +4,10 @@ import pytest\n from keras.models import Sequential\n from keras.engine.training_utils import weighted_masked_objective\n from keras.layers import TimeDistributed, Masking, Dense\n-from keras.utils.test_utils import keras_test\n from keras import losses\n from keras import backend as K\n \n \n-@keras_test\n def test_masking():\n     np.random.seed(1337)\n     x = np.array([[[1], [1]],\n@@ -24,7 +22,6 @@ def test_masking():\n     assert loss == 0\n \n \n-@keras_test\n def test_loss_masking():\n     weighted_loss = weighted_masked_objective(losses.get('mae'))\n     shape = (3, 4, 2)\n\n@@ -8,7 +8,6 @@ from keras.utils.test_utils import get_test_data\n from keras.models import Sequential, Model\n from keras.layers import Dense, Activation, GRU, TimeDistributed, Input\n from keras.utils import np_utils\n-from keras.utils.test_utils import keras_test\n from numpy.testing import assert_almost_equal, assert_array_almost_equal\n \n num_classes = 10\n@@ -73,7 +72,6 @@ def create_temporal_sequential_model():\n     return model\n \n \n-@keras_test\n def test_sequential_class_weights():\n     model = create_sequential_model()\n     model.compile(loss=loss, optimizer='rmsprop')\n@@ -99,7 +97,6 @@ def test_sequential_class_weights():\n     assert(score < standard_score_sequential)\n \n \n-@keras_test\n def test_sequential_sample_weights():\n     model = create_sequential_model()\n     model.compile(loss=loss, optimizer='rmsprop')\n@@ -123,7 +120,6 @@ def test_sequential_sample_weights():\n     assert(score < standard_score_sequential)\n \n \n-@keras_test\n def test_sequential_temporal_sample_weights():\n     ((x_train, y_train), (x_test, y_test),\n      (sample_weight, class_weight, test_ids)) = _get_test_data()\n@@ -162,7 +158,6 @@ def test_sequential_temporal_sample_weights():\n     assert(score < standard_score_sequential)\n \n \n-@keras_test\n def test_weighted_metrics_with_sample_weight():\n     decimal = decimal_precision[K.backend()]\n \n@@ -207,7 +202,6 @@ def test_weighted_metrics_with_sample_weight():\n     assert_almost_equal(loss_score, weighted_metric_score, decimal=decimal)\n \n \n-@keras_test\n def test_weighted_metrics_with_no_sample_weight():\n     decimal = decimal_precision[K.backend()]\n \n@@ -244,7 +238,6 @@ def test_weighted_metrics_with_no_sample_weight():\n     assert_almost_equal(loss_score, weighted_metric_score, decimal=decimal)\n \n \n-@keras_test\n def test_weighted_metrics_with_weighted_accuracy_metric():\n     model = create_sequential_model()\n     model.compile(loss=loss, optimizer='rmsprop',\n@@ -259,7 +252,6 @@ def test_weighted_metrics_with_weighted_accuracy_metric():\n     assert history.history['acc'] != history.history['weighted_acc']\n \n \n-@keras_test\n def test_weighted_metrics_with_multiple_outputs():\n     decimal = decimal_precision[K.backend()]\n \n@@ -289,7 +281,6 @@ def test_weighted_metrics_with_multiple_outputs():\n     assert_almost_equal(unweighted_metric * weight, weighted_metric, decimal=decimal)\n \n \n-@keras_test\n def test_class_weight_wrong_classes():\n     model = create_sequential_model()\n     model.compile(loss=loss, optimizer='rmsprop')\n\n@@ -13,7 +13,6 @@ from keras.layers import Input\n from keras import optimizers\n from keras import losses\n from keras import metrics\n-from keras.utils.test_utils import keras_test\n \n if sys.version_info[0] == 3:\n     import pickle\n@@ -27,7 +26,6 @@ skipif_no_tf_gpu = pytest.mark.skipif(\n     reason='Requires TensorFlow backend and a GPU')\n \n \n-@keras_test\n def test_sequential_model_pickling():\n     model = Sequential()\n     model.add(Dense(2, input_shape=(3,)))\n@@ -60,7 +58,6 @@ def test_sequential_model_pickling():\n     assert_allclose(out, out2, atol=1e-05)\n \n \n-@keras_test\n def test_sequential_model_pickling_2():\n     # test with custom optimizer, loss\n     custom_opt = optimizers.rmsprop\n@@ -83,7 +80,6 @@ def test_sequential_model_pickling_2():\n     assert_allclose(out, out2, atol=1e-05)\n \n \n-@keras_test\n def test_functional_model_pickling():\n     inputs = Input(shape=(3,))\n     x = Dense(2)(inputs)\n@@ -106,7 +102,6 @@ def test_functional_model_pickling():\n     assert_allclose(out, out2, atol=1e-05)\n \n \n-@keras_test\n def test_pickling_multiple_metrics_outputs():\n     inputs = Input(shape=(5,))\n     x = Dense(5)(inputs)\n@@ -132,7 +127,6 @@ def test_pickling_multiple_metrics_outputs():\n     assert_allclose(out, out2, atol=1e-05)\n \n \n-@keras_test\n def test_pickling_without_compilation():\n     \"\"\"Test pickling model without compiling.\n     \"\"\"\n@@ -143,7 +137,6 @@ def test_pickling_without_compilation():\n     model = pickle.loads(pickle.dumps(model))\n \n \n-@keras_test\n def test_pickling_right_after_compilation():\n     model = Sequential()\n     model.add(Dense(2, input_shape=(3,)))\n@@ -154,7 +147,6 @@ def test_pickling_right_after_compilation():\n     model = pickle.loads(pickle.dumps(model))\n \n \n-@keras_test\n def test_pickling_unused_layers_is_ok():\n     a = Input(shape=(256, 512, 6))\n     b = Input(shape=(256, 512, 1))\n\n@@ -17,7 +17,6 @@ from keras.initializers import Constant\n from keras import optimizers\n from keras import losses\n from keras import metrics\n-from keras.utils.test_utils import keras_test\n from keras.models import save_model, load_model\n \n \n@@ -27,7 +26,6 @@ skipif_no_tf_gpu = pytest.mark.skipif(\n     reason='Requires TensorFlow backend and a GPU')\n \n \n-@keras_test\n def test_sequential_model_saving():\n     model = Sequential()\n     model.add(Dense(2, input_shape=(3,)))\n@@ -61,7 +59,6 @@ def test_sequential_model_saving():\n     assert_allclose(out, out2, atol=1e-05)\n \n \n-@keras_test\n def test_sequential_model_saving_2():\n     # test with custom optimizer, loss\n     custom_opt = optimizers.rmsprop\n@@ -88,7 +85,6 @@ def test_sequential_model_saving_2():\n     assert_allclose(out, out2, atol=1e-05)\n \n \n-@keras_test\n def test_functional_model_saving():\n     inputs = Input(shape=(3,))\n     x = Dense(2)(inputs)\n@@ -113,7 +109,6 @@ def test_functional_model_saving():\n     assert_allclose(out, out2, atol=1e-05)\n \n \n-@keras_test\n def test_model_saving_to_pre_created_h5py_file():\n     inputs = Input(shape=(3,))\n     x = Dense(2)(inputs)\n@@ -151,7 +146,6 @@ def test_model_saving_to_pre_created_h5py_file():\n     assert_allclose(out, out2, atol=1e-05)\n \n \n-@keras_test\n def test_model_saving_to_binary_stream():\n     inputs = Input(shape=(3,))\n     x = Dense(2)(inputs)\n@@ -193,7 +187,6 @@ def test_model_saving_to_binary_stream():\n     assert_allclose(out, out2, atol=1e-05)\n \n \n-@keras_test\n def test_saving_multiple_metrics_outputs():\n     inputs = Input(shape=(5,))\n     x = Dense(5)(inputs)\n@@ -222,7 +215,6 @@ def test_saving_multiple_metrics_outputs():\n     assert_allclose(out, out2, atol=1e-05)\n \n \n-@keras_test\n def test_saving_without_compilation():\n     \"\"\"Test saving model without compiling.\n     \"\"\"\n@@ -236,7 +228,6 @@ def test_saving_without_compilation():\n     os.remove(fname)\n \n \n-@keras_test\n def test_saving_right_after_compilation():\n     model = Sequential()\n     model.add(Dense(2, input_shape=(3,)))\n@@ -250,7 +241,6 @@ def test_saving_right_after_compilation():\n     os.remove(fname)\n \n \n-@keras_test\n def test_saving_unused_layers_is_ok():\n     a = Input(shape=(256, 512, 6))\n     b = Input(shape=(256, 512, 1))\n@@ -264,7 +254,6 @@ def test_saving_unused_layers_is_ok():\n     os.remove(fname)\n \n \n-@keras_test\n def test_loading_weights_by_name_and_reshape():\n     \"\"\"\n     test loading model weights by name on:\n@@ -346,7 +335,6 @@ def test_loading_weights_by_name_and_reshape():\n     os.remove(fname)\n \n \n-@keras_test\n def test_loading_weights_by_name_2():\n     \"\"\"\n     test loading model weights by name on:\n@@ -405,7 +393,6 @@ def test_loading_weights_by_name_2():\n     assert_allclose(np.zeros_like(jessica[1]), jessica[1])  # biases init to 0\n \n \n-@keras_test\n def test_loading_weights_by_name_skip_mismatch():\n     \"\"\"\n     test skipping layers while loading model weights by name on:\n@@ -458,7 +445,6 @@ def square_fn(x):\n     return x * x\n \n \n-@keras_test\n def test_saving_lambda_custom_objects():\n     inputs = Input(shape=(3,))\n     x = Lambda(lambda x: square_fn(x), output_shape=(3,))(inputs)\n@@ -483,7 +469,6 @@ def test_saving_lambda_custom_objects():\n     assert_allclose(out, out2, atol=1e-05)\n \n \n-@keras_test\n def test_saving_lambda_numpy_array_arguments():\n     mean = np.random.random((4, 2, 3))\n     std = np.abs(np.random.random((4, 2, 3))) + 1e-5\n@@ -503,7 +488,6 @@ def test_saving_lambda_numpy_array_arguments():\n     assert_allclose(std, model.layers[1].arguments['std'])\n \n \n-@keras_test\n def test_saving_custom_activation_function():\n     x = Input(shape=(3,))\n     output = Dense(3, activation=K.cos)(x)\n@@ -527,7 +511,6 @@ def test_saving_custom_activation_function():\n     assert_allclose(out, out2, atol=1e-05)\n \n \n-@keras_test\n def test_saving_model_with_long_layer_names():\n     # This layer name will make the `layers_name` HDF5 attribute blow\n     # out of proportion. Note that it fits into the internal HDF5\n@@ -570,7 +553,6 @@ def test_saving_model_with_long_layer_names():\n     assert_allclose(out, out2, atol=1e-05)\n \n \n-@keras_test\n def test_saving_model_with_long_weights_names():\n     x = Input(shape=(2,), name='nested_model_input')\n     f = x\n@@ -616,7 +598,6 @@ def test_saving_model_with_long_weights_names():\n     assert_allclose(out, out2, atol=1e-05)\n \n \n-@keras_test\n def test_saving_recurrent_layer_with_init_state():\n     vector_size = 8\n     input_length = 20\n@@ -636,7 +617,6 @@ def test_saving_recurrent_layer_with_init_state():\n     os.remove(fname)\n \n \n-@keras_test\n def test_saving_recurrent_layer_without_bias():\n     vector_size = 8\n     input_length = 20\n@@ -652,7 +632,6 @@ def test_saving_recurrent_layer_without_bias():\n     os.remove(fname)\n \n \n-@keras_test\n def test_saving_constant_initializer_with_numpy():\n     \"\"\"Test saving and loading model of constant initializer with numpy inputs.\n     \"\"\"\n@@ -668,7 +647,6 @@ def test_saving_constant_initializer_with_numpy():\n     os.remove(fname)\n \n \n-@keras_test\n @pytest.mark.parametrize('implementation', [1, 2], ids=['impl1', 'impl2'])\n @pytest.mark.parametrize('bidirectional',\n                          [False, True],\n@@ -756,7 +734,6 @@ def _convert_model_weights(source_model, target_model):\n     os.remove(fname)\n \n \n-@keras_test\n @pytest.mark.parametrize('to_cudnn', [False, True], ids=['from_cudnn', 'to_cudnn'])\n @pytest.mark.parametrize('rnn_type', ['LSTM', 'GRU'], ids=['LSTM', 'GRU'])\n @skipif_no_tf_gpu\n\n@@ -5,7 +5,6 @@ import pytest\n import numpy as np\n from keras.models import Sequential\n from keras.layers.core import Dense\n-from keras.utils.test_utils import keras_test\n from keras.utils import Sequence\n from keras import backend as K\n \n@@ -67,7 +66,6 @@ def in_tmpdir(tmpdir):\n     assert not tmpdir.listdir()\n \n \n-@keras_test\n def test_multiprocessing_training():\n     arr_data = np.random.randint(0, 256, (50, 2))\n     arr_labels = np.random.randint(0, 2, 50)\n@@ -276,7 +274,6 @@ def test_multiprocessing_training():\n                             use_multiprocessing=False)\n \n \n-@keras_test\n def test_multiprocessing_training_from_file(in_tmpdir):\n     arr_data = np.random.randint(0, 256, (50, 2))\n     arr_labels = np.random.randint(0, 2, 50)\n@@ -397,7 +394,6 @@ def test_multiprocessing_training_from_file(in_tmpdir):\n     os.remove('data.npz')\n \n \n-@keras_test\n def test_multiprocessing_predicting():\n     arr_data = np.random.randint(0, 256, (50, 2))\n \n@@ -486,7 +482,6 @@ def test_multiprocessing_predicting():\n                             use_multiprocessing=False)\n \n \n-@keras_test\n def test_multiprocessing_evaluating():\n     arr_data = np.random.randint(0, 256, (50, 2))\n     arr_labels = np.random.randint(0, 2, 50)\n@@ -578,7 +573,6 @@ def test_multiprocessing_evaluating():\n                              use_multiprocessing=False)\n \n \n-@keras_test\n def test_multiprocessing_fit_error():\n     arr_data = np.random.randint(0, 256, (50, 2))\n     arr_labels = np.random.randint(0, 2, 50)\n@@ -691,7 +685,6 @@ def test_multiprocessing_fit_error():\n                             use_multiprocessing=False)\n \n \n-@keras_test\n def test_multiprocessing_evaluate_error():\n     arr_data = np.random.randint(0, 256, (50, 2))\n     arr_labels = np.random.randint(0, 2, 50)\n@@ -794,7 +787,6 @@ def test_multiprocessing_evaluate_error():\n                                  use_multiprocessing=False)\n \n \n-@keras_test\n def test_multiprocessing_predict_error():\n     arr_data = np.random.randint(0, 256, (50, 2))\n     good_batches = 3\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
