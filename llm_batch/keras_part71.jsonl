{"custom_id": "keras#02d5f72be4ae1bc37a30336d3d83be24d0d2db35", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 14 | Churn Cumulative: 1007 | Contributors (this commit): 10 | Commits (past 90d): 41 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -41,18 +41,8 @@ def placeholder(shape=None, ndim=None, dtype=_FLOATX, name=None):\n         raise Exception('Specify either a shape or ndim value.')\n     if shape is not None:\n         ndim = len(shape)\n-    if ndim == 0:\n-        return T.scalar(name=name, dtype=dtype)\n-    elif ndim == 1:\n-        return T.vector(name=name, dtype=dtype)\n-    elif ndim == 2:\n-        return T.matrix(name=name, dtype=dtype)\n-    elif ndim == 3:\n-        return T.tensor3(name=name, dtype=dtype)\n-    elif ndim == 4:\n-        return T.tensor4(name=name, dtype=dtype)\n-    else:\n-        raise Exception('ndim too large: ' + str(ndim))\n+    broadcast = (False,) * ndim\n+    return T.TensorType(dtype, broadcast)(name)\n \n \n def shape(x):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#65b5899d068daadb9101025dd8b8a7e4ceecec35", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 5 | Churn Cumulative: 320 | Contributors (this commit): 8 | Commits (past 90d): 8 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -7,10 +7,6 @@ def mean_squared_error(y_true, y_pred):\n     return K.mean(K.square(y_pred - y_true), axis=-1)\n \n \n-def root_mean_squared_error(y_true, y_pred):\n-    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n-\n-\n def mean_absolute_error(y_true, y_pred):\n     return K.mean(K.abs(y_pred - y_true), axis=-1)\n \n@@ -56,7 +52,6 @@ def cosine_proximity(y_true, y_pred):\n \n # aliases\n mse = MSE = mean_squared_error\n-rmse = RMSE = root_mean_squared_error\n mae = MAE = mean_absolute_error\n mape = MAPE = mean_absolute_percentage_error\n msle = MSLE = mean_squared_logarithmic_error\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7e678b8315666baf896381e28bf2a805fc1584b5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 13/13 | Churn Δ: 46 | Churn Cumulative: 4297 | Contributors (this commit): 34 | Commits (past 90d): 31 | Contributors (cumulative): 36 | DMM Complexity: 1.0\n\nDIFF:\n@@ -97,6 +97,7 @@ def weighted_objective(fn):\n         # apply sample weighting\n         if weights is not None:\n             score_array *= weights\n+            score_array /= K.mean(K.cast(K.not_equal(weights, 0), K.floatx()))\n         return K.mean(score_array)\n     return weighted\n \n\n@@ -5,8 +5,9 @@ import string\n \n from keras.utils.test_utils import get_test_data\n from keras.models import Sequential\n-from keras.layers.core import TimeDistributedDense, Dropout, Dense\n+from keras.layers.core import TimeDistributedDense, Dropout, Dense, Activation\n from keras.layers.recurrent import GRU, LSTM\n+from keras.layers.embeddings import Embedding\n from keras.utils.np_utils import to_categorical\n \n \n@@ -126,6 +127,48 @@ def test_stacked_lstm_char_prediction():\n     # check that it did generate the alphabet correctly\n     assert(generated == alphabet)\n \n+def test_masked_temporal():\n+    '''\n+    Confirm that even with masking on both inputs and outputs, cross-entropies are\n+    of the expected scale.\n+\n+    In this task, there are variable length inputs of integers from 1-9, and a random\n+    subset of unmasked outputs. Each of these outputs has a 50% probability of being\n+    the input number unchanged, and a 50% probability of being 2*input%10.\n+\n+    The ground-truth best cross-entropy loss should, then be -log(0.5) = 0.69\n+\n+    '''\n+    np.random.seed(55318)\n+    model = Sequential()\n+    model.add(Embedding(10, 20, mask_zero=True))\n+    model.add(TimeDistributedDense(10))\n+    model.add(Activation('softmax'))\n+    model.compile(loss='categorical_crossentropy',\n+                  optimizer='adam', sample_weight_mode=\"temporal\")\n+\n+    X = np.random.random_integers(1, 9, (50000, 20))\n+    for rowi in range(X.shape[0]):\n+        padding = np.random.random_integers(X.shape[1]/2)\n+        X[rowi, :padding] = 0\n+\n+    # 50% of the time the correct output is the input. The other 50% of the time\n+    # it's 2*input%10\n+    y = (X * np.random.random_integers(1, 2, X.shape))%10\n+    Y = np.zeros((y.size, 10), dtype='int32')\n+    for i, target in enumerate(y.flat):\n+        Y[i, target] = 1\n+    Y = Y.reshape(y.shape + (10,))\n+\n+    # Mask 50% of the outputs via sample weights\n+    sample_weight = np.random.random_integers(0, 1, y.shape)\n+    print(\"X shape: \", X.shape)\n+    print(\"Y shape: \", Y.shape)\n+    print(\"sample_weight shape: \", Y.shape)\n+\n+    history = model.fit(X, Y, validation_split=0.05, sample_weight=sample_weight,verbose=1, nb_epoch=2)\n+    ground_truth = -np.log(0.5)\n+    assert(np.abs(history.history['val_loss'][-1] - ground_truth) < 0.05)\n \n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#217cdd8b8517f54323e705cced749017274241d2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 39 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -4,12 +4,17 @@ from keras import objectives\n from keras import backend as K\n \n \n-allobj = [objectives.mean_squared_error, objectives.root_mean_squared_error,\n-          objectives.mean_absolute_error, objectives.mean_absolute_percentage_error,\n-          objectives.mean_squared_logarithmic_error, objectives.squared_hinge,\n-          objectives.hinge, objectives.categorical_crossentropy, objectives.binary_crossentropy, objectives.poisson,\n+allobj = [objectives.mean_squared_error,\n+          objectives.mean_absolute_error,\n+          objectives.mean_absolute_percentage_error,\n+          objectives.mean_squared_logarithmic_error,\n+          objectives.squared_hinge,\n+          objectives.hinge, objectives.categorical_crossentropy,\n+          objectives.binary_crossentropy,\n+          objectives.poisson,\n           objectives.cosine_proximity]\n \n+\n def test_objective_shapes_3d():\n     y_a = K.variable(np.random.random((5, 6, 7)))\n     y_b = K.variable(np.random.random((5, 6, 7)))\n@@ -17,6 +22,7 @@ def test_objective_shapes_3d():\n         objective_output = obj(y_a, y_b)\n         assert K.eval(objective_output).shape == (5, 6)\n \n+\n def test_objective_shapes_2d():\n     y_a = K.variable(np.random.random((6, 7)))\n     y_b = K.variable(np.random.random((6, 7)))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f854dcb83f2fc67ab58c577a0f6ab2ee10cf5970", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 4254 | Contributors (this commit): 35 | Commits (past 90d): 30 | Contributors (cumulative): 35 | DMM Complexity: 0.0\n\nDIFF:\n@@ -995,6 +995,7 @@ class Sequential(Model, containers.Sequential):\n \n         generator_threads = [threading.Thread(target=generator_task) for _ in range(nb_worker)]\n         for thread in generator_threads:\n+            thread.daemon = True\n             thread.start()\n \n         self.stop_training = False\n@@ -1452,6 +1453,7 @@ class Graph(Model, containers.Graph):\n \n         generator_threads = [threading.Thread(target=generator_task) for _ in range(nb_worker)]\n         for thread in generator_threads:\n+            thread.daemon = True\n             thread.start()\n \n         self.stop_training = False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e14deedd789fd4c4102808ae61c8da6bd56946e8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 48 | Lines Deleted: 48 | Files Changed: 1 | Hunks: 9 | Methods Changed: 3 | Complexity Δ (Sum/Max): -7/0 | Churn Δ: 96 | Churn Cumulative: 177 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,59 +1,59 @@\n-import pytest\n-from keras.preprocessing.image import *\n-from PIL import Image\n-import numpy as np\n-import os\n-import shutil\n+# import pytest\n+# from keras.preprocessing.image import *\n+# from PIL import Image\n+# import numpy as np\n+# import os\n+# import shutil\n \n \n-def setup_function(func):\n-    os.mkdir('test_images')\n-    os.mkdir('test_images/rgb')\n-    os.mkdir('test_images/gsc')\n+# def setup_function(func):\n+#     os.mkdir('test_images')\n+#     os.mkdir('test_images/rgb')\n+#     os.mkdir('test_images/gsc')\n \n-    img_w = img_h = 20\n-    for n in range(8):\n-        bias = np.random.rand(img_w, img_h, 1) * 64\n-        variance = np.random.rand(img_w, img_h, 1) * (255-64)\n-        imarray = np.random.rand(img_w, img_h, 3) * variance + bias\n-        im = Image.fromarray(imarray.astype('uint8')).convert('RGBA')\n-        im.save('test_images/rgb/rgb_test_image_'+str(n)+'.png')\n+#     img_w = img_h = 20\n+#     for n in range(8):\n+#         bias = np.random.rand(img_w, img_h, 1) * 64\n+#         variance = np.random.rand(img_w, img_h, 1) * (255-64)\n+#         imarray = np.random.rand(img_w, img_h, 3) * variance + bias\n+#         im = Image.fromarray(imarray.astype('uint8')).convert('RGBA')\n+#         im.save('test_images/rgb/rgb_test_image_'+str(n)+'.png')\n \n-        imarray = np.random.rand(img_w, img_h, 1) * variance + bias\n-        im = Image.fromarray(imarray.astype('uint8').squeeze()).convert('L')\n-        im.save('test_images/gsc/gsc_test_image_'+str(n)+'.png')\n+#         imarray = np.random.rand(img_w, img_h, 1) * variance + bias\n+#         im = Image.fromarray(imarray.astype('uint8').squeeze()).convert('L')\n+#         im.save('test_images/gsc/gsc_test_image_'+str(n)+'.png')\n \n \n-def teardown_function(func):\n-    shutil.rmtree('test_images')\n+# def teardown_function(func):\n+#     shutil.rmtree('test_images')\n \n \n-def test_image_data_generator():\n-    for color_mode in ['gsc', 'rgb']:\n-        file_list = list_pictures('test_images/' + color_mode)\n-        img_list = []\n-        for f in file_list:\n-            img_list.append(img_to_array(load_img(f))[None, ...])\n+# def test_image_data_generator():\n+#     for color_mode in ['gsc', 'rgb']:\n+#         file_list = list_pictures('test_images/' + color_mode)\n+#         img_list = []\n+#         for f in file_list:\n+#             img_list.append(img_to_array(load_img(f))[None, ...])\n \n-        images = np.vstack(img_list)\n-        generator = ImageDataGenerator(\n-            featurewise_center=True,\n-            samplewise_center=True,\n-            featurewise_std_normalization=True,\n-            samplewise_std_normalization=True,\n-            zca_whitening=True,\n-            rotation_range=90.,\n-            width_shift_range=10.,\n-            height_shift_range=10.,\n-            shear_range=0.5,\n-            horizontal_flip=True,\n-            vertical_flip=True)\n-        generator.fit(images, augment=True)\n+#         images = np.vstack(img_list)\n+#         generator = ImageDataGenerator(\n+#             featurewise_center=True,\n+#             samplewise_center=True,\n+#             featurewise_std_normalization=True,\n+#             samplewise_std_normalization=True,\n+#             zca_whitening=True,\n+#             rotation_range=90.,\n+#             width_shift_range=10.,\n+#             height_shift_range=10.,\n+#             shear_range=0.5,\n+#             horizontal_flip=True,\n+#             vertical_flip=True)\n+#         generator.fit(images, augment=True)\n \n-        for x, y in generator.flow(images, np.arange(images.shape[0]),\n-                                   shuffle=True, save_to_dir='test_images'):\n-            assert x.shape[1:] == images.shape[1:]\n-            break\n+#         for x, y in generator.flow(images, np.arange(images.shape[0]),\n+#                                    shuffle=True, save_to_dir='test_images'):\n+#             assert x.shape[1:] == images.shape[1:]\n+#             break\n \n-if __name__ == '__main__':\n-    pytest.main([__file__])\n+# if __name__ == '__main__':\n+#     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9b10ab298099777556529f2d794c78a6dbe43288", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 61 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -38,8 +38,8 @@ def test_temporal_classification():\n \n def test_temporal_regression():\n     '''\n-    Predict float numbers (regression) based on sequences of float numbers of length 3 using\n-    single layer of GRU units\n+    Predict float numbers (regression) based on sequences\n+    of float numbers of length 3 using a single layer of GRU units\n     '''\n     np.random.seed(1337)\n     (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=500,\n@@ -127,6 +127,7 @@ def test_stacked_lstm_char_prediction():\n     # check that it did generate the alphabet correctly\n     assert(generated == alphabet)\n \n+\n def test_masked_temporal():\n     '''\n     Confirm that even with masking on both inputs and outputs, cross-entropies are\n@@ -145,15 +146,16 @@ def test_masked_temporal():\n     model.add(TimeDistributedDense(10))\n     model.add(Activation('softmax'))\n     model.compile(loss='categorical_crossentropy',\n-                  optimizer='adam', sample_weight_mode=\"temporal\")\n+                  optimizer='adam',\n+                  sample_weight_mode=\"temporal\")\n \n     X = np.random.random_integers(1, 9, (50000, 20))\n     for rowi in range(X.shape[0]):\n         padding = np.random.random_integers(X.shape[1] / 2)\n         X[rowi, :padding] = 0\n \n-    # 50% of the time the correct output is the input. The other 50% of the time\n-    # it's 2*input%10\n+    # 50% of the time the correct output is the input.\n+    # The other 50% of the time it's 2 * input % 10\n     y = (X * np.random.random_integers(1, 2, X.shape)) % 10\n     Y = np.zeros((y.size, 10), dtype='int32')\n     for i, target in enumerate(y.flat):\n@@ -166,7 +168,9 @@ def test_masked_temporal():\n     print(\"Y shape: \", Y.shape)\n     print(\"sample_weight shape: \", Y.shape)\n \n-    history = model.fit(X, Y, validation_split=0.05, sample_weight=sample_weight,verbose=1, nb_epoch=2)\n+    history = model.fit(X, Y, validation_split=0.05,\n+                        sample_weight=sample_weight,\n+                        verbose=1, nb_epoch=2)\n     ground_truth = -np.log(0.5)\n     assert(np.abs(history.history['val_loss'][-1] - ground_truth) < 0.05)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6f5f3d3fb6a474fcdc9a1bf9f97ab612379b7dc3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 6 | Files Changed: 3 | Hunks: 8 | Methods Changed: 6 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 32 | Churn Cumulative: 9688 | Contributors (this commit): 44 | Commits (past 90d): 130 | Contributors (cumulative): 61 | DMM Complexity: 0.8823529411764706\n\nDIFF:\n@@ -305,7 +305,7 @@ class Graph(Layer):\n             raise Exception('Duplicate node identifier: ' + name)\n         self.namespace.add(name)\n         self.input_order.append(name)\n-        layer = Layer()  # empty layer\n+        layer = Layer(name=name)  # empty layer\n         if input_shape:\n             layer.set_input_shape((None,) + tuple(input_shape))\n         elif batch_input_shape:\n@@ -351,6 +351,7 @@ class Graph(Layer):\n         '''\n         if name in self.namespace:\n             raise Exception('Duplicate node identifier: ' + name)\n+        layer.name = name\n         if input:\n             if input not in self.namespace:\n                 raise Exception('Unknown node/input identifier: ' + input)\n\n@@ -549,10 +549,11 @@ class Merge(Layer):\n             inputs = OrderedDict()\n             for i in range(len(self.layers)):\n                 X = self.layers[i].get_output(train)\n-                if X.name is None:\n+                name = getattr(self.layers[i], 'name')\n+                if name is None:\n                     raise ValueError('merge_mode=\"join\" only works with named inputs.')\n                 else:\n-                    inputs[X.name] = X\n+                    inputs[name] = X\n             return inputs\n         elif self.mode == 'mul':\n             s = self.layers[0].get_output(train)\n@@ -1669,10 +1670,11 @@ class Siamese(Layer):\n         o = OrderedDict()\n         for i in range(len(self.inputs)):\n             X = self.get_output_at(i, train)\n-            if X.name is None:\n+            name = getattr(self.inputs[i], 'name')\n+            if name is None:\n                 raise ValueError('merge_mode=\"join\" '\n                                  'only works with named inputs.')\n-            o[X.name] = X\n+            o[name] = X\n         return o\n \n     def get_output_sum(self, train=False):\n\n@@ -421,6 +421,22 @@ def test_lambda():\n     nloss = model.evaluate([X_test, X_test], y_test, verbose=0)\n     assert(loss == nloss)\n \n+    # test \"join\" mode in Lambda\n+    def difference(input_dict):\n+        assert(len(input_dict) == 2)\n+        keys = input_dict.keys()\n+        return input_dict[keys[0]] - input_dict[keys[1]]\n+\n+    g = Graph()\n+    g.add_input(name='input_a', input_shape=(2,))\n+    g.add_input(name='input_b', input_shape=(2,))\n+    g.add_node(Lambda(difference),\n+               inputs=['input_a', 'input_b'],\n+               merge_mode='join',\n+               name='d')\n+    g.add_output(name='output', input='d')\n+    g.compile(loss={'output': 'categorical_crossentropy'}, optimizer='rmsprop')\n+\n \n def test_sequential_count_params():\n     input_dim = 20\n@@ -945,4 +961,5 @@ def test_count_params():\n \n \n if __name__ == '__main__':\n-    pytest.main([__file__])\n+    # pytest.main([__file__])\n+    test_lambda()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f3fd56db501f75b72db253a4e1d06792b81c2a60", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 4257 | Contributors (this commit): 35 | Commits (past 90d): 31 | Contributors (cumulative): 35 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1089,6 +1089,9 @@ class Graph(Model, containers.Graph):\n                 you will need to set the sample weight mode for this output\n                 to \"temporal\".\n         '''\n+        assert type(loss) is dict, 'The \"loss\" argument should be a dictionary.'\n+        assert type(sample_weight_modes) is dict, 'The \"sample_weight_modes\" argument should be a dictionary.'\n+\n         self.sample_weight_modes = sample_weight_modes\n         ys = []\n         ys_train = []\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#359f91ff6c189f625a875e37592cd84f3f554d28", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1084 | Contributors (this commit): 5 | Commits (past 90d): 22 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -424,7 +424,7 @@ def test_lambda():\n     # test \"join\" mode in Lambda\n     def difference(input_dict):\n         assert(len(input_dict) == 2)\n-        keys = input_dict.keys()\n+        keys = list(input_dict.keys())\n         return input_dict[keys[0]] - input_dict[keys[1]]\n \n     g = Graph()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#652f2eb56dfd8b5f8549d06d66bae86bf98454f3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 123 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 5 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 123 | Churn Cumulative: 123 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,123 @@\n+'''Train a Siamese MLP on pairs of digits from the MNIST dataset.\n+\n+It follows Hadsell-et-al.'06 [1] by computing the Euclidean distance on the\n+output of the shared network and by optimizing the contrastive loss (see paper\n+for mode details).\n+\n+[1] \"Dimensionality Reduction by Learning an Invariant Mapping\"\n+    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n+\n+Run on GPU: THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python mnist_siamese_graph.py\n+\n+Get to 99.5% test accuracy after 20 epochs.\n+3 seconds per epoch on a Titan X GPU\n+'''\n+from __future__ import absolute_import\n+from __future__ import print_function\n+import numpy as np\n+np.random.seed(1337)  # for reproducibility\n+\n+import random\n+from keras.datasets import mnist\n+from keras.models import Sequential, Graph\n+from keras.layers.core import *\n+from keras.optimizers import SGD, RMSprop\n+from keras import backend as K\n+\n+\n+def euclidean_distance(inputs):\n+    assert len(inputs) == 2, \\\n+        'Euclidean distance needs 2 inputs, %d given' % len(inputs)\n+    u, v = inputs.values()\n+    return K.sqrt((K.square(u - v)).sum(axis=1, keepdims=True))\n+\n+\n+def contrastive_loss(y, d):\n+    \"\"\" Contrastive loss from Hadsell-et-al.'06\n+        http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n+    \"\"\"\n+    margin = 1\n+    return K.mean(y * K.square(d) + (1 - y) * K.square(K.maximum(margin - d, 0)))\n+\n+\n+def create_pairs(x, digit_indices):\n+    \"\"\" Positive and negative pair creation.\n+        Alternates between positive and negative pairs.\n+    \"\"\"\n+    pairs = []\n+    labels = []\n+    n = min([len(digit_indices[d]) for d in range(10)]) - 1\n+    for d in range(10):\n+        for i in range(n):\n+            z1, z2 = digit_indices[d][i], digit_indices[d][i+1]\n+            pairs += [[x[z1], x[z2]]]\n+            inc = random.randrange(1, 10)\n+            dn = (d + inc) % 10\n+            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\n+            pairs += [[x[z1], x[z2]]]\n+            labels += [1, 0]\n+    return np.array(pairs), np.array(labels)\n+\n+\n+def create_base_network(in_dim):\n+    \"\"\" Base network to be shared (eq. to feature extraction).\n+    \"\"\"\n+    seq = Sequential()\n+    seq.add(Dense(128, input_shape=(in_dim,), activation='relu'))\n+    seq.add(Dropout(0.1))\n+    seq.add(Dense(128, activation='relu'))\n+    seq.add(Dropout(0.1))\n+    seq.add(Dense(128, activation='relu'))\n+    return seq\n+\n+\n+def compute_accuracy(predictions, labels):\n+    \"\"\" Compute classification accuracy with a fixed threshold on distances.\n+    \"\"\"\n+    return labels[predictions.ravel() < 0.5].mean()\n+\n+\n+# the data, shuffled and split between tran and test sets\n+(X_train, y_train), (X_test, y_test) = mnist.load_data()\n+X_train = X_train.reshape(60000, 784)\n+X_test = X_test.reshape(10000, 784)\n+X_train = X_train.astype('float32')\n+X_test = X_test.astype('float32')\n+X_train /= 255\n+X_test /= 255\n+in_dim = 784\n+nb_epoch = 20\n+\n+# create training+test positive and negative pairs\n+digit_indices = [np.where(y_train == i)[0] for i in range(10)]\n+tr_pairs, tr_y = create_pairs(X_train, digit_indices)\n+\n+digit_indices = [np.where(y_test == i)[0] for i in range(10)]\n+te_pairs, te_y = create_pairs(X_test, digit_indices)\n+\n+# network definition\n+base_network = create_base_network(in_dim)\n+\n+g = Graph()\n+g.add_input(name='input_a', input_shape=(in_dim,))\n+g.add_input(name='input_b', input_shape=(in_dim,))\n+g.add_shared_node(base_network, name='shared', inputs=['input_a', 'input_b'],\n+                  merge_mode='join')\n+g.add_node(Lambda(euclidean_distance), name='d', input='shared')\n+g.add_output(name='output', input='d')\n+\n+# train\n+rms = RMSprop()\n+g.compile(loss={'output': contrastive_loss}, optimizer=rms)\n+g.fit({'input_a': tr_pairs[:, 0], 'input_b': tr_pairs[:, 1], 'output': tr_y},\n+      validation_data={'input_a': te_pairs[:, 0], 'input_b': te_pairs[:, 1], 'output': te_y},\n+      batch_size=128, nb_epoch=nb_epoch)\n+\n+# compute final accuracy on training and test sets\n+pred = g.predict({'input_a': tr_pairs[:, 0], 'input_b': tr_pairs[:, 1]})['output']\n+tr_acc = compute_accuracy(pred, tr_y)\n+pred = g.predict({'input_a': te_pairs[:, 0], 'input_b': te_pairs[:, 1]})['output']\n+te_acc = compute_accuracy(pred, te_y)\n+\n+print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\n+print('* Accuracy on test set: %0.2f%%' % (100 * te_acc))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#99991779e5234c57feee1723addd5b523acfc852", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 20 | Files Changed: 1 | Hunks: 15 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 41 | Churn Cumulative: 164 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -9,7 +9,7 @@ for mode details).\n \n Run on GPU: THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python mnist_siamese_graph.py\n \n-Get to 99.5% test accuracy after 20 epochs.\n+Gets to 99.5% test accuracy after 20 epochs.\n 3 seconds per epoch on a Titan X GPU\n '''\n from __future__ import absolute_import\n@@ -20,30 +20,30 @@ np.random.seed(1337)  # for reproducibility\n import random\n from keras.datasets import mnist\n from keras.models import Sequential, Graph\n-from keras.layers.core import *\n+from keras.layers.core import Dense, Dropout, Lambda\n from keras.optimizers import SGD, RMSprop\n from keras import backend as K\n \n \n def euclidean_distance(inputs):\n-    assert len(inputs) == 2, \\\n-        'Euclidean distance needs 2 inputs, %d given' % len(inputs)\n+    assert len(inputs) == 2, ('Euclidean distance needs '\n+                              '2 inputs, %d given' % len(inputs))\n     u, v = inputs.values()\n-    return K.sqrt((K.square(u - v)).sum(axis=1, keepdims=True))\n+    return K.sqrt(K.sum(K.square(u - v), axis=1, keepdims=True))\n \n \n def contrastive_loss(y, d):\n-    \"\"\" Contrastive loss from Hadsell-et-al.'06\n+    '''Contrastive loss from Hadsell-et-al.'06\n     http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n-    \"\"\"\n+    '''\n     margin = 1\n     return K.mean(y * K.square(d) + (1 - y) * K.square(K.maximum(margin - d, 0)))\n \n \n def create_pairs(x, digit_indices):\n-    \"\"\" Positive and negative pair creation.\n+    '''Positive and negative pair creation.\n     Alternates between positive and negative pairs.\n-    \"\"\"\n+    '''\n     pairs = []\n     labels = []\n     n = min([len(digit_indices[d]) for d in range(10)]) - 1\n@@ -59,11 +59,11 @@ def create_pairs(x, digit_indices):\n     return np.array(pairs), np.array(labels)\n \n \n-def create_base_network(in_dim):\n-    \"\"\" Base network to be shared (eq. to feature extraction).\n-    \"\"\"\n+def create_base_network(input_dim):\n+    '''Base network to be shared (eq. to feature extraction).\n+    '''\n     seq = Sequential()\n-    seq.add(Dense(128, input_shape=(in_dim,), activation='relu'))\n+    seq.add(Dense(128, input_shape=(input_dim,), activation='relu'))\n     seq.add(Dropout(0.1))\n     seq.add(Dense(128, activation='relu'))\n     seq.add(Dropout(0.1))\n@@ -72,8 +72,8 @@ def create_base_network(in_dim):\n \n \n def compute_accuracy(predictions, labels):\n-    \"\"\" Compute classification accuracy with a fixed threshold on distances.\n-    \"\"\"\n+    '''Compute classification accuracy with a fixed threshold on distances.\n+    '''\n     return labels[predictions.ravel() < 0.5].mean()\n \n \n@@ -85,7 +85,7 @@ X_train = X_train.astype('float32')\n X_test = X_test.astype('float32')\n X_train /= 255\n X_test /= 255\n-in_dim = 784\n+input_dim = 784\n nb_epoch = 20\n \n # create training+test positive and negative pairs\n@@ -96,11 +96,11 @@ digit_indices = [np.where(y_test == i)[0] for i in range(10)]\n te_pairs, te_y = create_pairs(X_test, digit_indices)\n \n # network definition\n-base_network = create_base_network(in_dim)\n+base_network = create_base_network(input_dim)\n \n g = Graph()\n-g.add_input(name='input_a', input_shape=(in_dim,))\n-g.add_input(name='input_b', input_shape=(in_dim,))\n+g.add_input(name='input_a', input_shape=(input_dim,))\n+g.add_input(name='input_b', input_shape=(input_dim,))\n g.add_shared_node(base_network, name='shared', inputs=['input_a', 'input_b'],\n                   merge_mode='join')\n g.add_node(Lambda(euclidean_distance), name='d', input='shared')\n@@ -111,7 +111,8 @@ rms = RMSprop()\n g.compile(loss={'output': contrastive_loss}, optimizer=rms)\n g.fit({'input_a': tr_pairs[:, 0], 'input_b': tr_pairs[:, 1], 'output': tr_y},\n       validation_data={'input_a': te_pairs[:, 0], 'input_b': te_pairs[:, 1], 'output': te_y},\n-      batch_size=128, nb_epoch=nb_epoch)\n+      batch_size=128,\n+      nb_epoch=nb_epoch)\n \n # compute final accuracy on training and test sets\n pred = g.predict({'input_a': tr_pairs[:, 0], 'input_b': tr_pairs[:, 1]})['output']\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5824f2eb99b6aa955fb568c564a741af711d64f7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 7454 | Contributors (this commit): 38 | Commits (past 90d): 73 | Contributors (cumulative): 38 | DMM Complexity: None\n\nDIFF:\n@@ -549,7 +549,7 @@ class Merge(Layer):\n             inputs = OrderedDict()\n             for i in range(len(self.layers)):\n                 X = self.layers[i].get_output(train)\n-                name = getattr(self.layers[i], 'name')\n+                name = getattr(self.layers[i], 'name', None)\n                 if name is None:\n                     raise ValueError('merge_mode=\"join\" only works with named inputs.')\n                 else:\n@@ -1670,7 +1670,7 @@ class Siamese(Layer):\n         o = OrderedDict()\n         for i in range(len(self.inputs)):\n             X = self.get_output_at(i, train)\n-            name = getattr(self.inputs[i], 'name')\n+            name = getattr(self.inputs[i], 'name', None)\n             if name is None:\n                 raise ValueError('merge_mode=\"join\" '\n                                  'only works with named inputs.')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1b22915f8575a26e14c0f5aa77103e24e927e4dd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 90 | Lines Deleted: 90 | Files Changed: 4 | Hunks: 9 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 180 | Churn Cumulative: 11032 | Contributors (this commit): 49 | Commits (past 90d): 109 | Contributors (cumulative): 67 | DMM Complexity: None\n\nDIFF:\n@@ -117,8 +117,8 @@ def code_snippet(snippet):\n \n \n def process_class_docstring(docstring):\n-    docstring = re.sub(r'    # (.*)\\n',\n-                       r'    __\\1__\\n\\n',\n+    docstring = re.sub(r'\\n    # (.*)\\n',\n+                       r'\\n    __\\1__\\n\\n',\n                        docstring)\n \n     docstring = re.sub(r'    ([^\\s\\\\]+):(.*)\\n',\n\n@@ -6,8 +6,8 @@ import numpy as np\n \n class LeakyReLU(MaskedLayer):\n     '''Special version of a Rectified Linear Unit\n-    that allows a small gradient when the unit is not active\n-    (`f(x) = alpha*x for x < 0`).\n+    that allows a small gradient when the unit is not active:\n+    `f(x) = alpha*x for x < 0`.\n \n     # Input shape\n         Arbitrary. Use the keyword argument `input_shape`\n\n@@ -345,53 +345,6 @@ class Masking(MaskedLayer):\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n-class TimeDistributedMerge(Layer):\n-    '''Sum/multiply/average over the outputs of a TimeDistributed layer.\n-\n-    # Input shape\n-        3D tensor with shape: `(samples, steps, features)`.\n-\n-    # Output shape\n-        2D tensor with shape: `(samples, features)`.\n-\n-    # Arguments\n-        mode: one of {'sum', 'mul', 'ave'}\n-    '''\n-    input_ndim = 3\n-\n-    def __init__(self, mode='sum', **kwargs):\n-        super(TimeDistributedMerge, self).__init__(**kwargs)\n-        self.mode = mode\n-        self.trainable_weights = []\n-        self.regularizers = []\n-        self.constraints = []\n-        self.updates = []\n-\n-    @property\n-    def output_shape(self):\n-        return (None, self.input_shape[2])\n-\n-    def get_output(self, train=False):\n-        X = self.get_input(train)\n-        if self.mode == 'ave':\n-            s = K.mean(X, axis=1)\n-            return s\n-        if self.mode == 'sum':\n-            s = K.sum(X, axis=1)\n-            return s\n-        elif self.mode == 'mul':\n-            s = K.prod(X, axis=1)\n-            return s\n-        else:\n-            raise Exception('Unknown merge mode')\n-\n-    def get_config(self):\n-        config = {'name': self.__class__.__name__,\n-                  'mode': self.mode}\n-        base_config = super(TimeDistributedMerge, self).get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n-\n-\n class Merge(Layer):\n     '''Merge the output of a list of layers or containers into a single tensor.\n \n@@ -626,6 +579,53 @@ class Merge(Layer):\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n+class TimeDistributedMerge(Layer):\n+    '''Sum/multiply/average over the outputs of a TimeDistributed layer.\n+\n+    # Input shape\n+        3D tensor with shape: `(samples, steps, features)`.\n+\n+    # Output shape\n+        2D tensor with shape: `(samples, features)`.\n+\n+    # Arguments\n+        mode: one of {'sum', 'mul', 'ave'}\n+    '''\n+    input_ndim = 3\n+\n+    def __init__(self, mode='sum', **kwargs):\n+        super(TimeDistributedMerge, self).__init__(**kwargs)\n+        self.mode = mode\n+        self.trainable_weights = []\n+        self.regularizers = []\n+        self.constraints = []\n+        self.updates = []\n+\n+    @property\n+    def output_shape(self):\n+        return (None, self.input_shape[2])\n+\n+    def get_output(self, train=False):\n+        X = self.get_input(train)\n+        if self.mode == 'ave':\n+            s = K.mean(X, axis=1)\n+            return s\n+        if self.mode == 'sum':\n+            s = K.sum(X, axis=1)\n+            return s\n+        elif self.mode == 'mul':\n+            s = K.prod(X, axis=1)\n+            return s\n+        else:\n+            raise Exception('Unknown merge mode')\n+\n+    def get_config(self):\n+        config = {'name': self.__class__.__name__,\n+                  'mode': self.mode}\n+        base_config = super(TimeDistributedMerge, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n class Dropout(MaskedLayer):\n     '''Apply Dropout to the input. Dropout consists in randomly setting\n     a fraction `p` of input units to 0 at each update during training time,\n@@ -986,42 +986,6 @@ class Dense(Layer):\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n-class ActivityRegularization(Layer):\n-    '''Layer that passes through its input unchanged, but applies an update\n-    to the cost function based on the activity.\n-\n-    # Input shape\n-        Arbitrary. Use the keyword argument `input_shape`\n-        (tuple of integers, does not include the samples axis)\n-        when using this layer as the first layer in a model.\n-\n-    # Output shape\n-        Same shape as input.\n-\n-    # Arguments\n-        l1: L1 regularization factor.\n-        l2: L2 regularization factor.\n-    '''\n-    def __init__(self, l1=0., l2=0., **kwargs):\n-        super(ActivityRegularization, self).__init__(**kwargs)\n-        self.l1 = l1\n-        self.l2 = l2\n-\n-        activity_regularizer = ActivityRegularizer(l1=l1, l2=l2)\n-        activity_regularizer.set_layer(self)\n-        self.regularizers = [activity_regularizer]\n-\n-    def get_output(self, train=False):\n-        return self.get_input(train)\n-\n-    def get_config(self):\n-        config = {'name': self.__class__.__name__,\n-                  'l1': self.l1,\n-                  'l2': self.l2}\n-        base_config = super(ActivityRegularization, self).get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n-\n-\n class TimeDistributedDense(MaskedLayer):\n     '''Apply a same Dense layer for each dimension[1] (time_dimension) input.\n     Especially useful after a recurrent network with 'return_sequence=True'.\n@@ -1147,6 +1111,42 @@ class TimeDistributedDense(MaskedLayer):\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n+class ActivityRegularization(Layer):\n+    '''Layer that passes through its input unchanged, but applies an update\n+    to the cost function based on the activity.\n+\n+    # Input shape\n+        Arbitrary. Use the keyword argument `input_shape`\n+        (tuple of integers, does not include the samples axis)\n+        when using this layer as the first layer in a model.\n+\n+    # Output shape\n+        Same shape as input.\n+\n+    # Arguments\n+        l1: L1 regularization factor.\n+        l2: L2 regularization factor.\n+    '''\n+    def __init__(self, l1=0., l2=0., **kwargs):\n+        super(ActivityRegularization, self).__init__(**kwargs)\n+        self.l1 = l1\n+        self.l2 = l2\n+\n+        activity_regularizer = ActivityRegularizer(l1=l1, l2=l2)\n+        activity_regularizer.set_layer(self)\n+        self.regularizers = [activity_regularizer]\n+\n+    def get_output(self, train=False):\n+        return self.get_input(train)\n+\n+    def get_config(self):\n+        config = {'name': self.__class__.__name__,\n+                  'l1': self.l1,\n+                  'l2': self.l2}\n+        base_config = super(ActivityRegularization, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n class AutoEncoder(Layer):\n     '''A customizable autoencoder model.\n \n@@ -1168,14 +1168,15 @@ class AutoEncoder(Layer):\n \n     # Examples\n     ```python\n-    from keras.layers import containers\n+        from keras.layers import containers, AutoEncoder, Dense\n+        from keras import models\n \n         # input shape: (nb_samples, 32)\n         encoder = containers.Sequential([Dense(16, input_dim=32), Dense(8)])\n         decoder = containers.Sequential([Dense(16, input_dim=8), Dense(32)])\n \n         autoencoder = AutoEncoder(encoder=encoder, decoder=decoder, output_reconstruction=True)\n-    model = Sequential()\n+        model = models.Sequential()\n         model.add(autoencoder)\n \n         # training the autoencoder:\n\n@@ -52,7 +52,6 @@ class Recurrent(MaskedLayer):\n         of timesteps. To introduce masks to your data,\n         use an [Embedding](embeddings.md) layer with the `mask_zero` parameter\n         set to `True`.\n-        **Note:** for the time being, masking is only supported with Theano.\n \n     # TensorFlow warning\n         For the time being, when using the TensorFlow backend,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#523e24e8acf8ecc6d6b31e78cdc6e4c3967bbbdb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 27 | Churn Cumulative: 1034 | Contributors (this commit): 10 | Commits (past 90d): 42 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -441,13 +441,17 @@ def rnn(step_function, inputs, initial_states,\n             the step function, of shape (samples, ...).\n     '''\n     ndim = inputs.ndim\n-    assert ndim >= 3, \"Input should be at least 3D.\"\n+    assert ndim >= 3, 'Input should be at least 3D.'\n+\n     axes = [1, 0] + list(range(2, ndim))\n     inputs = inputs.dimshuffle(axes)\n-    if mask is None:\n-        mask = expand_dims(ones_like(T.sum(inputs, axis=-1)))\n-    else:\n+\n+    if mask is not None:\n         mask = mask.dimshuffle(axes)\n+        # build an all-zero tensor of shape (samples, output_dim)\n+        initial_output = step_function(inputs[0], initial_states)[0] * 0\n+        # Theano gets confused by broadcasting patterns in the scan op\n+        initial_output = T.unbroadcast(initial_output, 0, 1)\n \n         def _step(input, mask, output_tm1, *states):\n             output, new_states = step_function(input, states)\n@@ -458,16 +462,21 @@ def rnn(step_function, inputs, initial_states,\n                 return_states.append(T.switch(mask, new_state, state))\n             return [output] + return_states\n \n-    # build an all-zero tensor of shape (samples, output_dim)\n-    initial_output = step_function(inputs[0], initial_states)[0] * 0\n-    # Theano gets confused by broadcasting patterns in the scan op\n-    initial_output = T.unbroadcast(initial_output, 0, 1)\n-\n         results, _ = theano.scan(\n             _step,\n             sequences=[inputs, mask],\n             outputs_info=[initial_output] + initial_states,\n             go_backwards=go_backwards)\n+    else:\n+        def _step(input, *states):\n+            output, new_states = step_function(input, states)\n+            return [output] + new_states\n+\n+        results, _ = theano.scan(\n+            _step,\n+            sequences=inputs,\n+            outputs_info=[None] + initial_states,\n+            go_backwards=go_backwards)\n \n     # deal with Theano API inconsistency\n     if type(results) is list:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d68e3316dafb0a49e60420bf1067e3145547f997", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 97 | Lines Deleted: 25 | Files Changed: 3 | Hunks: 16 | Methods Changed: 8 | Complexity Δ (Sum/Max): 14/7 | Churn Δ: 122 | Churn Cumulative: 5854 | Contributors (this commit): 41 | Commits (past 90d): 49 | Contributors (cumulative): 50 | DMM Complexity: 0.6875\n\nDIFF:\n@@ -456,6 +456,7 @@ class TensorBoard(Callback):\n                             'with the TensorFlow backend.')\n         self.log_dir = log_dir\n         self.histogram_freq = histogram_freq\n+        self.merged = None\n \n     def _set_model(self, model):\n         import tensorflow as tf\n@@ -463,7 +464,7 @@ class TensorBoard(Callback):\n \n         self.model = model\n         self.sess = KTF._get_session()\n-        if self.histogram_freq:\n+        if self.histogram_freq and not self.merged:\n             mod_type = self.model.get_config()['name']\n             if mod_type == 'Sequential':\n                 layers = {l.get_config()['name']: l for l in self.model.layers}\n\n@@ -970,8 +970,17 @@ class Sequential(Model, containers.Sequential):\n                 _stop.set()\n                 raise Exception('The generator output tuple must have '\n                                 '2 or 3 elements.')\n+\n+            sample_weight = standardize_weights(y, sample_weight=sample_weight,\n+                                                sample_weight_mode=self.sample_weight_mode)\n             return X, y, sample_weight\n \n+        if do_validation:\n+            X_val, y_val, sample_weight_val = input_validation(validation_data)\n+            self.validation_data = X_val + [y_val, sample_weight_val]\n+        else:\n+            self.validation_data = None\n+\n         # start generator thread storing batches into a queue\n         generator_queue = queue.Queue()\n         _stop = threading.Event()\n@@ -1043,10 +1052,9 @@ class Sequential(Model, containers.Sequential):\n                             raise NotImplementedError()\n                         else:\n                             # input validation\n-                            X, y, sample_weight = input_validation(validation_data)\n-                            val_outs = self.evaluate(X, y,\n+                            val_outs = self.evaluate(X_val, y_val,\n                                                      show_accuracy=show_accuracy,\n-                                                     sample_weight=sample_weight,\n+                                                     sample_weight=sample_weight_val,\n                                                      verbose=0)\n                         if type(val_outs) != list:\n                             val_outs = [val_outs]\n@@ -1431,8 +1439,19 @@ class Graph(Model, containers.Graph):\n                        [len(sample_weight[name]) for name in sample_weight.keys()])) != 1:\n                 raise Exception('All input arrays and target arrays must have '\n                                 'the same number of samples.')\n+            sample_weight = {name: standardize_weights(data[name],\n+                             sample_weight=sample_weight.get(name),\n+                             sample_weight_mode=self.sample_weight_modes.get(name)) for name in self.output_order}\n             return data, sample_weight\n \n+        if do_validation:\n+            data_val, sample_weight_val = input_validation(validation_data)\n+            sample_weight_val_l = [sample_weight_val[name] for name in self.output_order]\n+            y_val = [standardize_y(data_val[name]) for name in self.output_order]\n+            self.validation_data = [data_val[name] for name in self.input_order] + y_val + sample_weight_val_l\n+        else:\n+            self.validation_data = None\n+\n         # start generator thread storing batches into a queue\n         generator_queue = queue.Queue()\n         _stop = threading.Event()\n@@ -1498,10 +1517,8 @@ class Graph(Model, containers.Graph):\n                             _stop.set()\n                             raise NotImplementedError()\n                         else:\n-                            # input validation\n-                            data, sample_weight = input_validation(validation_data)\n-                            val_outs = self.evaluate(data,\n-                                                     sample_weight=sample_weight,\n+                            val_outs = self.evaluate(data_val,\n+                                                     sample_weight=sample_weight_val,\n                                                      verbose=0)\n                         if type(val_outs) != list:\n                             val_outs = [val_outs]\n\n@@ -136,7 +136,30 @@ def test_TensorBoard():\n                                                          nb_class=nb_class)\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n-    # case 1 Sequential wo accuracy\n+\n+    def data_generator(train):\n+        if train:\n+            max_batch_index = len(X_train) // batch_size\n+        else:\n+            max_batch_index = len(X_test) // batch_size\n+        i = 0\n+        while 1:\n+            if train:\n+                yield (X_train[i * batch_size: (i + 1) * batch_size], y_train[i * batch_size: (i + 1) * batch_size])\n+            else:\n+                yield (X_test[i * batch_size: (i + 1) * batch_size], y_test[i * batch_size: (i + 1) * batch_size])\n+            i += 1\n+            i = i % max_batch_index\n+\n+    def data_generator_graph(train):\n+        while 1:\n+            if train:\n+                yield {'X_vars': X_train, 'output': y_train}\n+            else:\n+                yield {'X_vars': X_test, 'output': y_test}\n+\n+    # case 1 Sequential\n+\n     with tf.Graph().as_default():\n         session = tf.Session('')\n         KTF._set_session(session)\n@@ -147,28 +170,42 @@ def test_TensorBoard():\n \n         tsb = callbacks.TensorBoard(log_dir=filepath, histogram_freq=1)\n         cbks = [tsb]\n+\n+        # fit with validation data\n+        model.fit(X_train, y_train, batch_size=batch_size, show_accuracy=False,\n+                  validation_data=(X_test, y_test), callbacks=cbks, nb_epoch=2)\n+\n+        # fit with validation data and accuracy\n         model.fit(X_train, y_train, batch_size=batch_size, show_accuracy=True,\n                   validation_data=(X_test, y_test), callbacks=cbks, nb_epoch=2)\n+\n+        # fit generator with validation data\n+        model.fit_generator(data_generator(True), len(X_train), nb_epoch=2,\n+                            show_accuracy=False,\n+                            validation_data=(X_test, y_test),\n+                            callbacks=cbks)\n+\n+        # fit generator without validation data\n+        model.fit_generator(data_generator(True), len(X_train), nb_epoch=2,\n+                            show_accuracy=False,\n+                            callbacks=cbks)\n+\n+        # fit generator with validation data and accuracy\n+        model.fit_generator(data_generator(True), len(X_train), nb_epoch=2,\n+                            show_accuracy=True,\n+                            validation_data=(X_test, y_test),\n+                            callbacks=cbks)\n+\n+        # fit generator without validation data and accuracy\n+        model.fit_generator(data_generator(True), len(X_train), nb_epoch=2,\n+                            show_accuracy=True,\n+                            callbacks=cbks)\n+\n         assert os.path.exists(filepath)\n         shutil.rmtree(filepath)\n \n-    # case 2 Sequential w accuracy\n-    with tf.Graph().as_default():\n-        session = tf.Session('')\n-        KTF._set_session(session)\n-        model = Sequential()\n-        model.add(Dense(nb_hidden, input_dim=input_dim, activation='relu'))\n-        model.add(Dense(nb_class, activation='softmax'))\n-        model.compile(loss='categorical_crossentropy', optimizer='sgd')\n+    # case 2 Graph\n \n-        tsb = callbacks.TensorBoard(log_dir=filepath, histogram_freq=1)\n-        cbks = [tsb]\n-        model.fit(X_train, y_train, batch_size=batch_size, show_accuracy=True,\n-                  validation_data=(X_test, y_test), callbacks=cbks, nb_epoch=2)\n-        assert os.path.exists(filepath)\n-        shutil.rmtree(filepath)\n-\n-    # case 3 Graph\n     with tf.Graph().as_default():\n         session = tf.Session('')\n         KTF._set_session(session)\n@@ -185,10 +222,27 @@ def test_TensorBoard():\n \n         tsb = callbacks.TensorBoard(log_dir=filepath, histogram_freq=1)\n         cbks = [tsb]\n+\n+        # fit with validation\n         model.fit({'X_vars': X_train, 'output': y_train},\n                   batch_size=batch_size,\n                   validation_data={'X_vars': X_test, 'output': y_test},\n                   callbacks=cbks, nb_epoch=2)\n+\n+        # fit wo validation\n+        model.fit({'X_vars': X_train, 'output': y_train},\n+                  batch_size=batch_size,\n+                  callbacks=cbks, nb_epoch=2)\n+\n+        # fit generator with validation\n+        model.fit_generator(data_generator_graph(True), 1000, nb_epoch=2,\n+                            validation_data={'X_vars': X_test, 'output': y_test},\n+                            callbacks=cbks)\n+\n+        # fit generator wo validation\n+        model.fit_generator(data_generator_graph(True), 1000, nb_epoch=2,\n+                            callbacks=cbks)\n+\n         assert os.path.exists(filepath)\n         shutil.rmtree(filepath)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
