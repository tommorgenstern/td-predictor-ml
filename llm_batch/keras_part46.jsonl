{"custom_id": "keras#88cc30010352d47f25a0bced84e73ae805dc055d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 259 | Contributors (this commit): 7 | Commits (past 90d): 5 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -33,7 +33,7 @@ img_rows, img_cols = 32, 32\n # the CIFAR10 images are RGB\n img_channels = 3\n \n-# the data, shuffled and split between tran and test sets\n+# the data, shuffled and split between train and test sets\n (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n print('X_train shape:', X_train.shape)\n print(X_train.shape[0], 'train samples')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#71ee360c2090217ad092f6c0af3b14f80633875d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 16 | Churn Cumulative: 292 | Contributors (this commit): 8 | Commits (past 90d): 9 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -54,6 +54,22 @@ class PReLU(MaskedLayer):\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n+class ELU(MaskedLayer):\n+    def __init__(self, alpha=1.0, **kwargs):\n+        super(ELU, self).__init__(**kwargs)\n+        self.alpha = alpha\n+\n+    def get_output(self, train):\n+        X = self.get_input(train)\n+        return ((X + abs(X)) / 2.0) + self.alpha * (T.exp((X - abs(X)) / 2.0) - 1)\n+\n+    def get_config(self):\n+        config = {\"name\": self.__class__.__name__,\n+                  \"alpha\": self.alpha}\n+        base_config = super(ELU, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n class ParametricSoftplus(MaskedLayer):\n     '''\n         Parametric Softplus of the form: alpha * log(1 + exp(beta * X))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#138e77d0d625359b7735ab12748ea4f0866fe4ac", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 221 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 23 | Complexity Δ (Sum/Max): 108/108 | Churn Δ: 221 | Churn Cumulative: 3196 | Contributors (this commit): 31 | Commits (past 90d): 81 | Contributors (cumulative): 31 | DMM Complexity: 0.6123595505617978\n\nDIFF:\n@@ -1144,3 +1144,224 @@ class LambdaMerge(Lambda):\n                   }\n         base_config = super(LambdaMerge, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+class Siamese(Layer):\n+\n+    def __init__(self, layer, inputs, merge_mode='concat', concat_axis=1, dot_axes=-1):\n+\n+        if merge_mode not in ['sum', 'mul', 'concat', 'ave', 'join', 'cos', 'dot', None]:\n+            raise Exception(\"Invalid merge mode: \" + str(mode))\n+\n+        if merge_mode in {'cos', 'dot'}:\n+            if len(inputs) > 2:\n+                raise Exception(mode + \" merge takes exactly 2 layers\")\n+\n+        self.layer = layer\n+        self.inputs = inputs\n+        self.params = []\n+        self.merge_mode = merge_mode\n+        self.concat_axis = concat_axis\n+        self.dot_axes = dot_axes\n+        layer.set_previous(inputs[0])\n+        self.regularizers = []\n+        self.constraints = []\n+        self.updates = []\n+        layers = [layer]\n+        if merge_mode:\n+            layers += inputs\n+        for l in layers:\n+            params, regs, consts, updates = l.get_params()\n+            self.regularizers += regs\n+            self.updates += updates\n+            # params and constraints have the same size\n+            for p, c in zip(params, consts):\n+                if p not in self.params:\n+                    self.params.append(p)\n+                    self.constraints.append(c)\n+\n+    @property\n+    def output_shape(self):\n+        if merge_mode is None:\n+            return self.layer.output_shape\n+        input_shapes = [self.get_output_shape(i) for i in range(len(inputs))]\n+        if self.merge_mode in ['sum', 'mul', 'ave']:\n+            return input_shapes[0]\n+        elif self.merge_mode == 'concat':\n+            output_shape = list(input_shapes[0])\n+            for shape in input_shapes[1:]:\n+                output_shape[self.concat_axis] += shape[self.concat_axis]\n+            return tuple(output_shape)\n+        elif self.merge_mode == 'join':\n+            return None\n+        elif self.merge_mode == 'dot':\n+            shape1 = list(input_shapes[0])\n+            shape2 = list(input_shapes[1])\n+            for i in self.dot_axes[0]:\n+                shape1.pop(i)\n+            for i in self.dot_axes[1]:\n+                shape2.pop(i)\n+            shape = shape1 + shape2[1:]\n+            if len(shape) == 1:\n+                shape.append(1)\n+            return tuple(shape)\n+        elif self.merge_mode == 'cos':\n+            return tuple(input_shapes[0][0], 1)\n+\n+    def get_params(self):\n+        return self.params, self.regularizers, self.constraints, self.updates\n+\n+    def get_output_at(self, head, train=False):\n+        if hasattr(self.layer, 'previous'):\n+            self.layer.previous = self.inputs[head]\n+        else:\n+            self.layer.layers[0].previous = self.inputs[head]\n+        return self.layer.get_output(train)\n+\n+    def get_output_shape(self, head, train=False):\n+        self.layer.set_previous(self.inputs[head])\n+        return self.layer.output_shape\n+\n+    def get_output_join(self, train=False):\n+        o = OrderedDict()\n+        for i in range(len(inputs)):\n+            X = self.get_output_at(i, train)\n+            if X.name is None:\n+                raise ValueError(\"merge_mode='join' only works with named inputs\")\n+            o[X.name] = X\n+        return o\n+\n+    def get_output_sum(self, train=False):\n+        s = self.get_output_at(0, train)\n+        for i in range(1, len(self.inputs)):\n+            s += self.get_output_at(i, train)\n+        return s\n+\n+    def get_output_ave(self, train=False):\n+        n = len(self.inputs)\n+        s = self.get_output_at(0, train)\n+        for i in range(1, n):\n+            s += self.get_output_at(i, train)\n+        s /= n\n+        return s\n+\n+    def get_output_concat(self, train=False):\n+        inputs = [self.get_output_at(i, train) for i in range(len(self.inputs))]\n+        return T.concatenate(inputs, axis=self.concat_axis)\n+\n+    def get_output_mul(self, train=False):\n+        s = self.get_output_at(0, train)\n+        for i in range(1, len(self.inputs)):\n+            s *= self.get_output_at(i, train)\n+        return s\n+\n+    def get_output_dot(self, train=False):\n+        l1 = self.get_output_at(0, train)\n+        l2 = self.get_output_at(1, train)\n+        output = T.batched_tensordot(l1, l2, self.dot_axes)\n+        output = output.dimshuffle((0, 'x'))\n+        return output\n+\n+    def get_output_cos(self, train=False):\n+        l1 = self.get_output_at(0, train)\n+        l2 = self.get_output_at(1, train)\n+        output, _ = theano.scan(lambda v1, v2: T.dot(v1, v2)/T.sqrt(T.dot(v1, v1) * T.dot(v2, v2)), sequences=[l1, l2], outputs_info=None)\n+        output = output.dimshuffle((0, 'x'))\n+        return output\n+\n+    def get_output(self, train=False):\n+        mode = self.merge_mode\n+        if mode == 'join':\n+            return self.get_output_join(train)\n+        elif mode == 'concat':\n+            return self.get_output_concat(train)\n+        elif mode == 'sum':\n+            return self.get_output_sum(train)\n+        elif mode == 'ave':\n+            return self.get_output_ave(train)\n+        elif mode == 'mul':\n+            return self.get_output_mul(train)\n+        elif mode == 'dot':\n+            return self.get_output_dot(train)\n+        elif mode == 'cos':\n+            return self.get_output_dot(train)\n+\n+    def get_input(self, train=False):\n+        res = []\n+        for i in range(len(self.inputs)):\n+            o = self.inputs[i].get_input(train)\n+            if type(o) != list:\n+                o = [o]\n+            for output in o:\n+                if output not in res:\n+                    res.append(output)\n+        return res\n+\n+    @property\n+    def input(self):\n+        return self.get_input()\n+\n+    def supports_masked_input(self):\n+        return False\n+\n+    def get_output_mask(self, train=None):\n+        return None\n+\n+    def get_weights(self):\n+        weights = layer.get_weights()\n+        if merge_mode:\n+            for m in self.inputs:\n+                weights += m.get_weights()\n+        return weights\n+\n+    def set_weights(self, weights):\n+        nb_param = len(self.layer.params)\n+        self.layer.set_weights(weights[:nb_param])\n+        weights = weights[nb_param:]\n+        if merge_mode:\n+            for i in range(len(self.inputs)):\n+                nb_param = len(self.inputs[i].params)\n+                self.inputs[i].set_weights(weights[:nb_param])\n+                weights = weights[nb_param:]\n+\n+    def get_config(self):\n+\n+        config = {\"name\": self.__class__.__name__,\n+                  \"layer\": self.layer.get_config,\n+                  \"inputs\": [m.get_config() for m in self.inputs],\n+                  \"merge_mode\": self.merge_mode,\n+                  \"concat_axis\": self.concat_axis,\n+                  \"dot_axes\": self.dot_axes\n+                  }\n+        base_config = super(Siamese, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+class SiameseHead(Layer):\n+    def __init__(self, head):\n+        self.head = head\n+        self.params = []\n+\n+    def get_output(self, train=False):\n+        return self.get_input(train)\n+\n+    @property\n+    def input_shape(self):\n+        return self.previous.get_output_shape(self.head)\n+\n+    def get_input(self, train=False):\n+        return self.previous.get_output_at(self.head, train)\n+\n+    def get_config(self):\n+\n+        config = {\"name\": self.__class__.__name__,\n+                  \"head\": self.head\n+                  }\n+\n+        base_config = super(SiameseHead, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+    def set_previous(self, layer):\n+        self.previous = layer\n+\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#268513483221c4568d9d50f6d468dafb283529a8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 7 | Churn Cumulative: 3203 | Contributors (this commit): 31 | Commits (past 90d): 82 | Contributors (cumulative): 31 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1365,3 +1365,10 @@ class SiameseHead(Layer):\n         self.previous = layer\n \n \n+def add_shared_layer(layer, inputs):\n+    input_layers = [l.layers[-1] for l in inputs]\n+    s = Siamese(layer, input_layers, merge_mode=None)\n+    for i in range(len(inputs)):\n+        sh = SiameseHead(i)\n+        inputs[i].add(s)\n+        inputs[i].add(sh)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7867abdd6998e3ec0172ffb0bce6adca8890a40d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 1147 | Files Changed: 1 | Hunks: 4 | Methods Changed: 40 | Complexity Δ (Sum/Max): -266/0 | Churn Δ: 1174 | Churn Cumulative: 4377 | Contributors (this commit): 31 | Commits (past 90d): 83 | Contributors (cumulative): 31 | DMM Complexity: 0.2857142857142857\n\nDIFF:\n@@ -1,1153 +1,21 @@\n-# -*- coding: utf-8 -*-\n-from __future__ import absolute_import, division\n-\n-import theano\n-import theano.tensor as T\n-import numpy as np\n-\n-from collections import OrderedDict\n-import copy\n-\n-from .. import activations, initializations, regularizers, constraints\n-from ..utils.theano_utils import shared_zeros, floatX, ndim_tensor\n-from ..utils.generic_utils import make_tuple\n-from ..regularizers import ActivityRegularizer, Regularizer\n-\n-from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n-from six.moves import zip\n-\n-import marshal\n-import types\n-import sys\n-\n-\n-class Layer(object):\n-    def __init__(self, **kwargs):\n-        for kwarg in kwargs:\n-            assert kwarg in {'input_shape', 'trainable'}, \"Keyword argument not understood: \" + kwarg\n-        if 'input_shape' in kwargs:\n-            self.set_input_shape(kwargs['input_shape'])\n-        if 'trainable' in kwargs:\n-            self._trainable = kwargs['trainable']\n-        if not hasattr(self, 'params'):\n-            self.params = []\n-\n-    def set_previous(self, layer, connection_map={}):\n-        assert self.nb_input == layer.nb_output == 1, \"Cannot connect layers: input count and output count should be 1.\"\n-        if hasattr(self, 'input_ndim'):\n-            assert self.input_ndim == len(layer.output_shape), \"Incompatible shapes: layer expected input with ndim=\" +\\\n-                str(self.input_ndim) + \" but previous layer has output_shape \" + str(layer.output_shape)\n-        if layer.get_output_mask() is not None:\n-            assert self.supports_masked_input(), \"Cannot connect non-masking layer to layer with masked output\"\n-        self.previous = layer\n-        self.build()\n-\n-    def build(self):\n-        '''Instantiation of layer weights.\n-\n-        Called after `set_previous`, or after `set_input_shape`,\n-        once the layer has a defined input shape.\n-        Must be implemented on all layers that have weights.\n-        '''\n-        pass\n-\n-    @property\n-    def trainable(self):\n-        if hasattr(self, '_trainable'):\n-            return self._trainable\n-        else:\n-            return True\n-\n-    @trainable.setter\n-    def trainable(self, value):\n-        self._trainable = value\n-\n-    @property\n-    def nb_input(self):\n-        return 1\n-\n-    @property\n-    def nb_output(self):\n-        return 1\n-\n-    @property\n-    def input_shape(self):\n-        # if layer is not connected (e.g. input layer),\n-        # input shape can be set manually via _input_shape attribute.\n-        if hasattr(self, 'previous'):\n-            return self.previous.output_shape\n-        elif hasattr(self, '_input_shape'):\n-            return self._input_shape\n-        else:\n-            raise Exception('Layer is not connected. Did you forget to set \"input_shape\"?')\n-\n-    def set_input_shape(self, input_shape):\n-        if type(input_shape) not in [tuple, list]:\n-            raise Exception('Invalid input shape - input_shape should be a tuple of int.')\n-        input_shape = (None,) + tuple(input_shape)\n-        if hasattr(self, 'input_ndim') and self.input_ndim:\n-            if self.input_ndim != len(input_shape):\n-                raise Exception('Invalid input shape - Layer expects input ndim=' +\n-                                str(self.input_ndim) + ', was provided with input shape ' + str(input_shape))\n-        self._input_shape = input_shape\n-        self.input = ndim_tensor(len(self._input_shape))\n-        self.build()\n-\n-    @property\n-    def output_shape(self):\n-        # default assumption: tensor shape unchanged.\n-        return self.input_shape\n-\n-    def get_output(self, train=False):\n-        return self.get_input(train)\n-\n-    def get_input(self, train=False):\n-        if hasattr(self, 'previous'):\n-            return self.previous.get_output(train=train)\n-        elif hasattr(self, 'input'):\n-            return self.input\n-        else:\n-            raise Exception('Layer is not connected\\\n-                and is not an input layer.')\n-\n-    def supports_masked_input(self):\n-        ''' Whether or not this layer respects the output mask of its previous layer in its calculations. If you try\n-        to attach a layer that does *not* support masked_input to a layer that gives a non-None output_mask() that is\n-        an error'''\n-        return False\n-\n-    def get_output_mask(self, train=None):\n-        '''\n-        For some models (such as RNNs) you want a way of being able to mark some output data-points as\n-        \"masked\", so they are not used in future calculations. In such a model, get_output_mask() should return a mask\n-        of one less dimension than get_output() (so if get_output is (nb_samples, nb_timesteps, nb_dimensions), then the mask\n-        is (nb_samples, nb_timesteps), with a one for every unmasked datapoint, and a zero for every masked one.\n-\n-        If there is *no* masking then it shall return None. For instance if you attach an Activation layer (they support masking)\n-        to a layer with an output_mask, then that Activation shall also have an output_mask. If you attach it to a layer with no\n-        such mask, then the Activation's get_output_mask shall return None.\n-\n-        Some layers have an output_mask even if their input is unmasked, notably Embedding which can turn the entry \"0\" into\n-        a mask.\n-        '''\n-        return None\n-\n-    def set_weights(self, weights):\n-        assert len(self.params) == len(weights), 'Provided weight array does not match layer weights (' + \\\n-            str(len(self.params)) + ' layer params vs. ' + str(len(weights)) + ' provided weights)'\n-        for p, w in zip(self.params, weights):\n-            if p.eval().shape != w.shape:\n-                raise Exception(\"Layer shape %s not compatible with weight shape %s.\" % (p.eval().shape, w.shape))\n-            p.set_value(floatX(w))\n-\n-    def get_weights(self):\n-        weights = []\n-        for p in self.params:\n-            weights.append(p.get_value())\n-        return weights\n-\n-    def get_config(self):\n-        config = {\"name\": self.__class__.__name__}\n-        if hasattr(self, '_input_shape'):\n-            config['input_shape'] = self._input_shape[1:]\n-        if hasattr(self, '_trainable'):\n-            config['trainable'] = self._trainable\n-        return config\n-\n-    def get_params(self):\n-        consts = []\n-        updates = []\n-\n-        if hasattr(self, 'regularizers'):\n-            regularizers = self.regularizers\n-        else:\n-            regularizers = []\n-\n-        if hasattr(self, 'constraints') and len(self.constraints) == len(self.params):\n-            for c in self.constraints:\n-                if c:\n-                    consts.append(c)\n-                else:\n-                    consts.append(constraints.identity())\n-        elif hasattr(self, 'constraint') and self.constraint:\n-            consts += [self.constraint for _ in range(len(self.params))]\n-        else:\n-            consts += [constraints.identity() for _ in range(len(self.params))]\n-\n-        if hasattr(self, 'updates') and self.updates:\n-            updates += self.updates\n-\n-        return self.params, regularizers, consts, updates\n-\n-    def set_name(self, name):\n-        for i in range(len(self.params)):\n-            self.params[i].name = '%s_p%d' % (name, i)\n-\n-    def count_params(self):\n-        return sum([np.prod(p.shape.eval()) for p in self.params])\n-\n-\n-class MaskedLayer(Layer):\n-    '''\n-    If your layer trivially supports masking (by simply copying the input mask to the output), then subclass MaskedLayer\n-    instead of Layer, and make sure that you incorporate the input mask into your calculation of get_output()\n-    '''\n-    def supports_masked_input(self):\n-        return True\n-\n-    def get_input_mask(self, train=False):\n-        if hasattr(self, 'previous'):\n-            return self.previous.get_output_mask(train)\n-        else:\n-            return None\n-\n-    def get_output_mask(self, train=False):\n-        ''' The default output mask is just the input mask unchanged. Override this in your own\n-        implementations if, for instance, you are reshaping the input'''\n-        return self.get_input_mask(train)\n-\n-\n-class Masking(MaskedLayer):\n-    \"\"\"Mask an input sequence by using a mask value to identify padding.\n-\n-    This layer copies the input to the output layer with identified padding\n-    replaced with 0s and creates an output mask in the process.\n-\n-    At each timestep, if the values all equal `mask_value`,\n-    then the corresponding mask value for the timestep is 0 (skipped),\n-    otherwise it is 1.\n-\n-    \"\"\"\n-    def __init__(self, mask_value=0., **kwargs):\n-        super(Masking, self).__init__(**kwargs)\n-        self.mask_value = mask_value\n-        self.input = T.tensor3()\n-\n-    def get_output_mask(self, train=False):\n-        X = self.get_input(train)\n-        return T.any(T.ones_like(X) * (1. - T.eq(X, self.mask_value)), axis=-1)\n-\n-    def get_output(self, train=False):\n-        X = self.get_input(train)\n-        return X * T.shape_padright(T.any((1. - T.eq(X, self.mask_value)), axis=-1))\n-\n-    def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"mask_value\": self.mask_value}\n-        base_config = super(Masking, self).get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-class TimeDistributedMerge(Layer):\n-    '''Sum/multiply/average over the outputs of a TimeDistributed layer.\n-\n-    mode: {'sum', 'mul', 'ave'}\n-    Tensor input dimensions:   (nb_sample, time, features)\n-    Tensor output dimensions:  (nb_sample, features)\n-    '''\n-    input_ndim = 3\n-\n-    def __init__(self, mode='sum', **kwargs):\n-        super(TimeDistributedMerge, self).__init__(**kwargs)\n-        self.mode = mode\n-        self.params = []\n-        self.regularizers = []\n-        self.constraints = []\n-        self.updates = []\n-\n-    @property\n-    def output_shape(self):\n-        return (None, self.input_shape[2])\n-\n-    def get_output(self, train=False):\n-        X = self.get_input(train)\n-        if self.mode == 'ave':\n-            s = theano.tensor.mean(X, axis=1)\n-            return s\n-        if self.mode == 'sum':\n-            s = theano.tensor.sum(X, axis=1)\n-            return s\n-        elif self.mode == 'mul':\n-            s = theano.tensor.mul(X, axis=1)\n-            return s\n-        else:\n-            raise Exception('Unknown merge mode')\n-\n-    def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"mode\": self.mode}\n-        base_config = super(TimeDistributedMerge, self).get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-class Merge(Layer):\n-    def __init__(self, layers, mode='sum', concat_axis=-1, dot_axes=-1):\n-        ''' Merge the output of a list of layers or containers into a single tensor.\n-            mode: {'sum', 'mul', 'concat', 'ave', 'join'}\n-        '''\n-        if len(layers) < 2:\n-            raise Exception(\"Please specify two or more input layers (or containers) to merge\")\n-\n-        if mode not in {'sum', 'mul', 'concat', 'ave', 'join', 'cos', 'dot'}:\n-            raise Exception(\"Invalid merge mode: \" + str(mode))\n-\n-        if mode in {'sum', 'mul', 'ave', 'cos'}:\n-            input_shapes = set([l.output_shape for l in layers])\n-            if len(input_shapes) > 1:\n-                raise Exception(\"Only layers of same output shape can be merged using \" + mode + \" mode. \" +\n-                                \"Layer shapes: %s\" % ([l.output_shape for l in layers]))\n-        if mode in {'cos', 'dot'}:\n-            if len(layers) > 2:\n-                raise Exception(mode + \" merge takes exactly 2 layers\")\n-            shape1 = layers[0].output_shape\n-            shape2 = layers[1].output_shape\n-            n1 = len(shape1)\n-            n2 = len(shape2)\n-            if mode == 'dot':\n-                if type(dot_axes) == int:\n-                    if dot_axes < 0:\n-                        dot_axes = [range(dot_axes % n1, n1), range(dot_axes % n2, n2)]\n-                    else:\n-                        dot_axes = [range(n1 - dot_axes, n2), range(1, dot_axes + 1)]\n-                if type(dot_axes) not in [list, tuple]:\n-                    raise Exception(\"Invalid type for dot_axes - should be a list.\")\n-                if len(dot_axes) != 2:\n-                    raise Exception(\"Invalid format for dot_axes - should contain two elements.\")\n-                if type(dot_axes[0]) not in [list, tuple, range] or type(dot_axes[1]) not in [list, tuple, range]:\n-                    raise Exception(\"Invalid format for dot_axes - list elements should have type 'list' or 'tuple'.\")\n-                for i in range(len(dot_axes[0])):\n-                    if shape1[dot_axes[0][i]] != shape2[dot_axes[1][i]]:\n-                        raise Exception(\"Dimension incompatibility using dot mode: \" +\n-                                        \"%s != %s. \" % (shape1[dot_axes[0][i]], shape2[dot_axes[1][i]]) +\n-                                        \"Layer shapes: %s, %s\" % (shape1, shape2))\n-        elif mode == 'concat':\n-            input_shapes = set()\n-            for l in layers:\n-                oshape = list(l.output_shape)\n-                oshape.pop(concat_axis)\n-                oshape = tuple(oshape)\n-                input_shapes.add(oshape)\n-            if len(input_shapes) > 1:\n-                raise Exception(\"'concat' mode can only merge layers with matching \" +\n-                                \"output shapes except for the concat axis. \" +\n-                                \"Layer shapes: %s\" % ([l.output_shape for l in layers]))\n-\n-        self.mode = mode\n-        self.concat_axis = concat_axis\n-        self.dot_axes = dot_axes\n-        self.layers = layers\n-        self.params = []\n-        self.regularizers = []\n-        self.constraints = []\n-        self.updates = []\n-        for l in self.layers:\n-            params, regs, consts, updates = l.get_params()\n-            self.regularizers += regs\n-            self.updates += updates\n-            # params and constraints have the same size\n-            for p, c in zip(params, consts):\n-                if p not in self.params:\n-                    self.params.append(p)\n-                    self.constraints.append(c)\n-\n-    @property\n-    def output_shape(self):\n-        input_shapes = [layer.output_shape for layer in self.layers]\n-        if self.mode in ['sum', 'mul', 'ave']:\n-            return input_shapes[0]\n-        elif self.mode == 'concat':\n-            output_shape = list(input_shapes[0])\n-            for shape in input_shapes[1:]:\n-                output_shape[self.concat_axis] += shape[self.concat_axis]\n-            return tuple(output_shape)\n-        elif self.mode == 'join':\n-            return None\n-        elif self.mode == 'dot':\n-            shape1 = list(input_shapes[0])\n-            shape2 = list(input_shapes[1])\n-            dot_axes = []\n-            for axes in self.dot_axes:\n-                dot_axes.append([index-1 for index in axes])\n-            tensordot_output = np.tensordot(np.zeros(tuple(shape1[1:])),\n-                                            np.zeros(tuple(shape2[1:])),\n-                                            axes=dot_axes)\n-            if len(tensordot_output.shape) == 0:\n-                shape = (1,)\n-            else:\n-                shape = tensordot_output.shape\n-            return (shape1[0],) + shape\n-        elif self.mode == 'cos':\n-            return tuple(input_shapes[0][0], 1)\n-\n-    def get_params(self):\n-        return self.params, self.regularizers, self.constraints, self.updates\n-\n-    def get_output(self, train=False):\n-        if self.mode == 'sum' or self.mode == 'ave':\n-            s = self.layers[0].get_output(train)\n-            for i in range(1, len(self.layers)):\n-                s += self.layers[i].get_output(train)\n-            if self.mode == 'ave':\n-                s /= len(self.layers)\n-            return s\n-        elif self.mode == 'concat':\n-            inputs = [self.layers[i].get_output(train) for i in range(len(self.layers))]\n-            return T.concatenate(inputs, axis=self.concat_axis)\n-        elif self.mode == 'join':\n-            inputs = OrderedDict()\n-            for i in range(len(self.layers)):\n-                X = self.layers[i].get_output(train)\n-                if X.name is None:\n-                    raise ValueError(\"merge_mode='join' only works with named inputs\")\n-                else:\n-                    inputs[X.name] = X\n-            return inputs\n-        elif self.mode == 'mul':\n-            s = self.layers[0].get_output(train)\n-            for i in range(1, len(self.layers)):\n-                s *= self.layers[i].get_output(train)\n-            return s\n-        elif self.mode == 'dot':\n-            l1 = self.layers[0].get_output(train)\n-            l2 = self.layers[1].get_output(train)\n-            output = T.batched_tensordot(l1, l2, self.dot_axes)\n-            output_shape = list(self.output_shape)\n-            output_shape[0] = l1.shape[0]\n-            output = output.reshape(tuple(output_shape))\n-            return output\n-        elif self.mode == 'cos':\n-            l1 = self.layers[0].get_output(train)\n-            l2 = self.layers[1].get_output(train)\n-            output, _ = theano.scan(lambda v1, v2: T.dot(v1, v2) / T.sqrt(T.dot(v1, v1) * T.dot(v2, v2)),\n-                                    sequences=[l1, l2],\n-                                    outputs_info=None)\n-            return output\n-        else:\n-            raise Exception('Unknown merge mode')\n-\n-    def get_input(self, train=False):\n-        res = []\n-        for i in range(len(self.layers)):\n-            o = self.layers[i].get_input(train)\n-            if not type(o) == list:\n-                o = [o]\n-            for output in o:\n-                if output not in res:\n-                    res.append(output)\n-        return res\n-\n-    @property\n-    def input(self):\n-        return self.get_input()\n-\n-    def supports_masked_input(self):\n-        return False\n-\n-    def get_output_mask(self, train=None):\n-        return None\n-\n-    def get_weights(self):\n-        weights = []\n-        for l in self.layers:\n-            weights += l.get_weights()\n-        return weights\n-\n-    def set_weights(self, weights):\n-        for i in range(len(self.layers)):\n-            nb_param = len(self.layers[i].params)\n-            self.layers[i].set_weights(weights[:nb_param])\n-            weights = weights[nb_param:]\n-\n-    def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"layers\": [l.get_config() for l in self.layers],\n-                  \"mode\": self.mode,\n-                  \"concat_axis\": self.concat_axis,\n-                  \"dot_axes\": self.dot_axes}\n-        base_config = super(Merge, self).get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-class Dropout(MaskedLayer):\n-    '''\n-        Hinton's dropout.\n-    '''\n-    def __init__(self, p, **kwargs):\n-        super(Dropout, self).__init__(**kwargs)\n-        self.p = p\n-        self.srng = RandomStreams(seed=np.random.randint(10e6))\n-\n-    def get_output(self, train=False):\n-        X = self.get_input(train)\n-        if self.p > 0.:\n-            retain_prob = 1. - self.p\n-            if train:\n-                X *= self.srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX) / retain_prob\n-        return X\n-\n-    def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"p\": self.p}\n-        base_config = super(Dropout, self).get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-class Activation(MaskedLayer):\n-    '''\n-        Apply an activation function to an output.\n-    '''\n-    def __init__(self, activation, target=0, beta=0.1, **kwargs):\n-        super(Activation, self).__init__(**kwargs)\n-        self.activation = activations.get(activation)\n-        self.target = target\n-        self.beta = beta\n-\n-    def get_output(self, train=False):\n-        X = self.get_input(train)\n-        return self.activation(X)\n-\n-    def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"activation\": self.activation.__name__,\n-                  \"target\": self.target,\n-                  \"beta\": self.beta}\n-        base_config = super(Activation, self).get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-class Reshape(Layer):\n-    '''\n-        Reshape an output to a certain shape.\n-        Can't be used as first layer in a model (no fixed input!)\n-        First dimension is assumed to be nb_samples.\n-    '''\n-    def __init__(self, dims, **kwargs):\n-        super(Reshape, self).__init__(**kwargs)\n-        self.dims = tuple(dims)\n-\n-    @property\n-    def output_shape(self):\n-        return (self.input_shape[0],) + self.dims\n-\n-    def get_output(self, train=False):\n-        X = self.get_input(train)\n-        new_shape = (X.shape[0],) + self.dims\n-        return theano.tensor.reshape(X, new_shape)\n-\n-    def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"dims\": self.dims}\n-        base_config = super(Reshape, self).get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-class Permute(Layer):\n-    '''\n-        Permute the dimensions of the input according to the given tuple.\n-    '''\n-    def __init__(self, dims, **kwargs):\n-        super(Permute, self).__init__(**kwargs)\n-        self.dims = tuple(dims)\n-\n-    @property\n-    def output_shape(self):\n-        input_shape = list(self.input_shape)\n-        output_shape = copy.copy(input_shape)\n-        for i, dim in enumerate(self.dims):\n-            target_dim = input_shape[dim]\n-            output_shape[i+1] = target_dim\n-        return tuple(output_shape)\n-\n-    def get_output(self, train=False):\n-        X = self.get_input(train)\n-        return X.dimshuffle((0,) + self.dims)\n-\n-    def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"dims\": self.dims}\n-        base_config = super(Permute, self).get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-class Flatten(Layer):\n-    '''\n-        Reshape input to flat shape.\n-        First dimension is assumed to be nb_samples.\n-    '''\n-    def __init__(self, **kwargs):\n-        super(Flatten, self).__init__(**kwargs)\n-\n-    @property\n-    def output_shape(self):\n-        input_shape = self.input_shape\n-        return (input_shape[0], np.prod(input_shape[1:]))\n-\n-    def get_output(self, train=False):\n-        X = self.get_input(train)\n-        size = theano.tensor.prod(X.shape) // X.shape[0]\n-        nshape = (X.shape[0], size)\n-        return theano.tensor.reshape(X, nshape)\n-\n-\n-class RepeatVector(Layer):\n-    '''\n-        Repeat input n times.\n-\n-        Dimensions of input are assumed to be (nb_samples, dim).\n-        Return tensor of shape (nb_samples, n, dim).\n-    '''\n-    def __init__(self, n, **kwargs):\n-        super(RepeatVector, self).__init__(**kwargs)\n-        self.n = n\n-\n-    @property\n-    def output_shape(self):\n-        input_shape = self.input_shape\n-        return (input_shape[0], self.n, input_shape[1])\n-\n-    def get_output(self, train=False):\n-        X = self.get_input(train)\n-        tensors = [X]*self.n\n-        stacked = theano.tensor.stack(*tensors)\n-        return stacked.dimshuffle((1, 0, 2))\n-\n-    def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"n\": self.n}\n-        base_config = super(RepeatVector, self).get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-class Dense(Layer):\n-    '''\n-        Just your regular fully connected NN layer.\n-    '''\n-    input_ndim = 2\n-\n-    def __init__(self, output_dim, init='glorot_uniform', activation='linear', weights=None,\n-                 W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n-                 W_constraint=None, b_constraint=None, input_dim=None, **kwargs):\n-        self.init = initializations.get(init)\n-        self.activation = activations.get(activation)\n-        self.output_dim = output_dim\n-\n-        self.W_regularizer = regularizers.get(W_regularizer)\n-        self.b_regularizer = regularizers.get(b_regularizer)\n-        self.activity_regularizer = regularizers.get(activity_regularizer)\n-\n-        self.W_constraint = constraints.get(W_constraint)\n-        self.b_constraint = constraints.get(b_constraint)\n-        self.constraints = [self.W_constraint, self.b_constraint]\n-\n-        self.initial_weights = weights\n-\n-        self.input_dim = input_dim\n-        if self.input_dim:\n-            kwargs['input_shape'] = (self.input_dim,)\n-        super(Dense, self).__init__(**kwargs)\n-\n-    def build(self):\n-        input_dim = self.input_shape[1]\n-\n-        self.input = T.matrix()\n-        self.W = self.init((input_dim, self.output_dim))\n-        self.b = shared_zeros((self.output_dim,))\n-\n-        self.params = [self.W, self.b]\n-\n-        self.regularizers = []\n-        if self.W_regularizer:\n-            self.W_regularizer.set_param(self.W)\n-            self.regularizers.append(self.W_regularizer)\n-\n-        if self.b_regularizer:\n-            self.b_regularizer.set_param(self.b)\n-            self.regularizers.append(self.b_regularizer)\n-\n-        if self.activity_regularizer:\n-            self.activity_regularizer.set_layer(self)\n-            self.regularizers.append(self.activity_regularizer)\n-\n-        if self.initial_weights is not None:\n-            self.set_weights(self.initial_weights)\n-            del self.initial_weights\n-\n-    @property\n-    def output_shape(self):\n-        return (self.input_shape[0], self.output_dim)\n-\n-    def get_output(self, train=False):\n-        X = self.get_input(train)\n-        output = self.activation(T.dot(X, self.W) + self.b)\n-        return output\n-\n-    def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"output_dim\": self.output_dim,\n-                  \"init\": self.init.__name__,\n-                  \"activation\": self.activation.__name__,\n-                  \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n-                  \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n-                  \"activity_regularizer\": self.activity_regularizer.get_config() if self.activity_regularizer else None,\n-                  \"W_constraint\": self.W_constraint.get_config() if self.W_constraint else None,\n-                  \"b_constraint\": self.b_constraint.get_config() if self.b_constraint else None,\n-                  \"input_dim\": self.input_dim}\n-        base_config = super(Dense, self).get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-class ActivityRegularization(Layer):\n-    '''\n-        Layer that passes through its input unchanged, but applies an update\n-        to the cost function based on the activity.\n-    '''\n-    def __init__(self, l1=0., l2=0., **kwargs):\n-        super(ActivityRegularization, self).__init__(**kwargs)\n-        self.l1 = l1\n-        self.l2 = l2\n-\n-        activity_regularizer = ActivityRegularizer(l1=l1, l2=l2)\n-        activity_regularizer.set_layer(self)\n-        self.regularizers = [activity_regularizer]\n-\n-    def get_output(self, train=False):\n-        return self.get_input(train)\n-\n-    def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"l1\": self.l1,\n-                  \"l2\": self.l2}\n-        base_config = super(ActivityRegularization, self).get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-class TimeDistributedDense(MaskedLayer):\n-    '''\n-       Apply a same Dense layer for each dimension[1] (time_dimension) input.\n-       Especially useful after a recurrent network with 'return_sequence=True'.\n-       Tensor input dimensions:   (nb_sample, time_dimension, input_dim)\n-       Tensor output dimensions:  (nb_sample, time_dimension, output_dim)\n-\n-    '''\n-    input_ndim = 3\n-\n-    def __init__(self, output_dim, init='glorot_uniform', activation='linear', weights=None,\n-                 W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n-                 W_constraint=None, b_constraint=None, input_dim=None, input_length=None, **kwargs):\n-        self.output_dim = output_dim\n-        self.init = initializations.get(init)\n-        self.activation = activations.get(activation)\n-\n-        self.W_regularizer = regularizers.get(W_regularizer)\n-        self.b_regularizer = regularizers.get(b_regularizer)\n-        self.activity_regularizer = regularizers.get(activity_regularizer)\n-\n-        self.W_constraint = constraints.get(W_constraint)\n-        self.b_constraint = constraints.get(b_constraint)\n-        self.constraints = [self.W_constraint, self.b_constraint]\n-\n-        self.initial_weights = weights\n-\n-        self.input_dim = input_dim\n-        self.input_length = input_length\n-        if self.input_dim:\n-            kwargs['input_shape'] = (self.input_length, self.input_dim)\n-        super(TimeDistributedDense, self).__init__(**kwargs)\n-\n-    def build(self):\n-        input_dim = self.input_shape[2]\n-\n-        self.input = T.tensor3()\n-        self.W = self.init((input_dim, self.output_dim))\n-        self.b = shared_zeros((self.output_dim))\n-\n-        self.params = [self.W, self.b]\n-        self.regularizers = []\n-\n-        if self.W_regularizer:\n-            self.W_regularizer.set_param(self.W)\n-            self.regularizers.append(self.W_regularizer)\n-\n-        if self.b_regularizer:\n-            self.b_regularizer.set_param(self.b)\n-            self.regularizers.append(self.b_regularizer)\n-\n-        if self.activity_regularizer:\n-            self.activity_regularizer.set_layer(self)\n-            self.regularizers.append(self.activity_regularizer)\n-\n-        if self.initial_weights is not None:\n-            self.set_weights(self.initial_weights)\n-            del self.initial_weights\n-\n-    @property\n-    def output_shape(self):\n-        input_shape = self.input_shape\n-        return (input_shape[0], input_shape[1], self.output_dim)\n-\n-    def get_output(self, train=False):\n-        X = self.get_input(train)\n-        output = self.activation(T.dot(X.dimshuffle(1, 0, 2), self.W) + self.b)\n-        return output.dimshuffle(1, 0, 2)\n-\n-    def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"output_dim\": self.output_dim,\n-                  \"init\": self.init.__name__,\n-                  \"activation\": self.activation.__name__,\n-                  \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n-                  \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n-                  \"activity_regularizer\": self.activity_regularizer.get_config() if self.activity_regularizer else None,\n-                  \"W_constraint\": self.W_constraint.get_config() if self.W_constraint else None,\n-                  \"b_constraint\": self.b_constraint.get_config() if self.b_constraint else None,\n-                  \"input_dim\": self.input_dim,\n-                  \"input_length\": self.input_length}\n-        base_config = super(TimeDistributedDense, self).get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-class AutoEncoder(Layer):\n-    '''A customizable autoencoder model.\n-\n-    Tensor input dimensions: same as encoder input\n-    Tensor output dimensions:\n-        if output_reconstruction:\n-            same as encoder output\n-        else:\n-            same as decoder output\n-    '''\n-    def __init__(self, encoder, decoder, output_reconstruction=True, weights=None, **kwargs):\n-        super(AutoEncoder, self).__init__(**kwargs)\n-\n-        self.output_reconstruction = output_reconstruction\n-        self.encoder = encoder\n-        self.decoder = decoder\n-\n-        self.decoder.set_previous(self.encoder)\n-\n-        self.params = []\n-        self.regularizers = []\n-        self.constraints = []\n-        self.updates = []\n-        for layer in [self.encoder, self.decoder]:\n-            params, regularizers, constraints, updates = layer.get_params()\n-            self.regularizers += regularizers\n-            self.updates += updates\n-            for p, c in zip(params, constraints):\n-                if p not in self.params:\n-                    self.params.append(p)\n-                    self.constraints.append(c)\n-\n-        if weights is not None:\n-            self.set_weights(weights)\n-\n-    def set_previous(self, node):\n-        self.encoder.set_previous(node)\n-\n-    def get_weights(self):\n-        weights = []\n-        for layer in [self.encoder, self.decoder]:\n-            weights += layer.get_weights()\n-        return weights\n-\n-    def set_weights(self, weights):\n-        nb_param = len(self.encoder.params)\n-        self.encoder.set_weights(weights[:nb_param])\n-        self.decoder.set_weights(weights[nb_param:])\n-\n-    def get_input(self, train=False):\n-        return self.encoder.get_input(train)\n-\n-    @property\n-    def input(self):\n-        return self.encoder.input\n-\n-    def _get_hidden(self, train=False):\n-        return self.encoder.get_output(train)\n-\n-    @property\n-    def input_shape(self):\n-        return self.encoder.input_shape\n-\n-    @property\n-    def output_shape(self):\n-        if self.output_reconstruction:\n-            return self.encoder.previous.output_shape\n-        else:\n-            return self.decoder.previous.output_shape\n-\n-    def get_output(self, train=False):\n-        if not train and not self.output_reconstruction:\n-            return self.encoder.get_output(train)\n-\n-        return self.decoder.get_output(train)\n-\n-    def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n-                \"encoder_config\": self.encoder.get_config(),\n-                \"decoder_config\": self.decoder.get_config(),\n-                \"output_reconstruction\": self.output_reconstruction}\n-\n-\n-class MaxoutDense(Layer):\n-    '''\n-        Max-out layer, nb_feature is the number of pieces in the piecewise linear approx.\n-        Refer to http://arxiv.org/pdf/1302.4389.pdf\n-    '''\n-    input_ndim = 2\n-\n-    def __init__(self, output_dim, nb_feature=4, init='glorot_uniform', weights=None,\n-                 W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n-                 W_constraint=None, b_constraint=None, input_dim=None, **kwargs):\n-        self.output_dim = output_dim\n-        self.nb_feature = nb_feature\n-        self.init = initializations.get(init)\n-\n-        self.W_regularizer = regularizers.get(W_regularizer)\n-        self.b_regularizer = regularizers.get(b_regularizer)\n-        self.activity_regularizer = regularizers.get(activity_regularizer)\n-\n-        self.W_constraint = constraints.get(W_constraint)\n-        self.b_constraint = constraints.get(b_constraint)\n-        self.constraints = [self.W_constraint, self.b_constraint]\n-\n-        self.initial_weights = weights\n-        self.input_dim = input_dim\n-        if self.input_dim:\n-            kwargs['input_shape'] = (self.input_dim,)\n-        super(MaxoutDense, self).__init__(**kwargs)\n-\n-    def build(self):\n-        input_dim = self.input_shape[1]\n-\n-        self.input = T.matrix()\n-        self.W = self.init((self.nb_feature, input_dim, self.output_dim))\n-        self.b = shared_zeros((self.nb_feature, self.output_dim))\n-\n-        self.params = [self.W, self.b]\n-        self.regularizers = []\n-\n-        if self.W_regularizer:\n-            self.W_regularizer.set_param(self.W)\n-            self.regularizers.append(self.W_regularizer)\n-\n-        if self.b_regularizer:\n-            self.b_regularizer.set_param(self.b)\n-            self.regularizers.append(self.b_regularizer)\n-\n-        if self.activity_regularizer:\n-            self.activity_regularizer.set_layer(self)\n-            self.regularizers.append(self.activity_regularizer)\n-\n-        if self.initial_weights is not None:\n-            self.set_weights(self.initial_weights)\n-            del self.initial_weights\n-\n-    @property\n-    def output_shape(self):\n-        return (self.input_shape[0], self.output_dim)\n-\n-    def get_output(self, train=False):\n-        X = self.get_input(train)\n-        # -- don't need activation since it's just linear.\n-        output = T.max(T.dot(X, self.W) + self.b, axis=1)\n-        return output\n-\n-    def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"output_dim\": self.output_dim,\n-                  \"init\": self.init.__name__,\n-                  \"nb_feature\": self.nb_feature,\n-                  \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n-                  \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n-                  \"activity_regularizer\": self.activity_regularizer.get_config() if self.activity_regularizer else None,\n-                  \"W_constraint\": self.W_constraint.get_config() if self.W_constraint else None,\n-                  \"b_constraint\": self.b_constraint.get_config() if self.b_constraint else None,\n-                  \"input_dim\": self.input_dim}\n-        base_config = super(MaxoutDense, self).get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-class Lambda(Layer):\n-\n-    \"\"\"Lambda layer for evaluating arbitrary function\n-\n-    Input shape\n-    -----------\n-    output_shape of previous layer\n-\n-    Output shape\n-    ------------\n-    Specified by output_shape argument\n-\n-    Arguments\n-    ---------\n-    function - The function to be evaluated. Takes one argument : output of previous layer\n-    output_shape - Expected output shape from function. Could be a tuple or a function of the shape of the input\n-    \"\"\"\n-\n-    def __init__(self, function, output_shape=None, **kwargs):\n-        super(Lambda, self).__init__(**kwargs)\n-        py3 = sys.version_info[0] == 3\n-        if py3:\n-            self.function = marshal.dumps(function.__code__)\n-        else:\n-            self.function = marshal.dumps(function.func_code)\n-        if output_shape is None:\n-            self._output_shape = None\n-        elif type(output_shape) in {tuple, list} :\n-            self._output_shape = tuple(output_shape)\n-        else:\n-            if py3:\n-                self._output_shape = marshal.dumps(output_shape.__code__)\n-            else:\n-                self._output_shape = marshal.dumps(output_shape.func_code)\n-\n-    @property\n-    def output_shape(self):\n-        if self._output_shape is None:\n-            return self.input_shape\n-        elif type(self._output_shape) == tuple:\n-            return (self.input_shape[0], ) + self._output_shape\n-        else:\n-            output_shape_func = marshal.loads(self._output_shape)\n-            output_shape_func = types.FunctionType(output_shape_func, globals())\n-            shape = output_shape_func(self.previous.output_shape)\n-            if type(shape) not in {list, tuple}:\n-                raise Exception(\"output_shape function must return a tuple\")\n-            return tuple(shape)\n-\n-    def get_output(self, train=False):\n-        func = marshal.loads(self.function)\n-        func = types.FunctionType(func, globals())\n-        if hasattr(self, 'previous'):\n-            return func(self.previous.get_output(train))\n-        else:\n-            return func(self.input)\n-\n-\n-class MaskedLambda(MaskedLayer, Lambda):\n-    pass\n-\n-\n-class LambdaMerge(Lambda):\n-    \"\"\"LambdaMerge layer for evaluating arbitrary function over multiple inputs\n-\n-    Input shape\n-    -----------\n-    None\n-\n-    Output shape\n-    ------------\n-    Specified by output_shape argument\n-\n-    Arguments\n-    ---------\n-    layers - Input layers. Similar to layers argument of Merge\n-    function - The function to be evaluated. Takes one argument : list of outputs from input layers\n-    output_shape - Expected output shape from function. Could be a tuple or a function of list of input shapes\n-    \"\"\"\n-    def __init__(self, layers, function, output_shape=None):\n-        if len(layers) < 2:\n-            raise Exception(\"Please specify two or more input layers (or containers) to merge\")\n-        self.layers = layers\n-        self.params = []\n-        self.regularizers = []\n-        self.constraints = []\n-        self.updates = []\n-        for l in self.layers:\n-            params, regs, consts, updates = l.get_params()\n-            self.regularizers += regs\n-            self.updates += updates\n-            # params and constraints have the same size\n-            for p, c in zip(params, consts):\n-                if p not in self.params:\n-                    self.params.append(p)\n-                    self.constraints.append(c)\n-        py3 = sys.version_info[0] == 3\n-        if py3:\n-            self.function = marshal.dumps(function.__code__)\n-        else:\n-            self.function = marshal.dumps(function.func_code)\n-        if output_shape is None:\n-            self._output_shape = None\n-        elif type(output_shape) in {tuple, list}:\n-            self._output_shape = tuple(output_shape)\n-        else:\n-            if py3:\n-                self._output_shape = marshal.dumps(output_shape.__code__)\n-            else:\n-                self._output_shape = marshal.dumps(output_shape.func_code)\n-\n-    @property\n-    def output_shape(self):\n-        input_shapes = [layer.output_shape for layer in self.layers]\n-        if self._output_shape is None:\n-            return input_shapes[0]\n-        elif type(self._output_shape) == tuple:\n-            return (input_shapes[0][0], ) + self._output_shape\n-        else:\n-            output_shape_func = marshal.loads(self._output_shape)\n-            output_shape_func = types.FunctionType(output_shape_func, globals())\n-            shape = output_shape_func(input_shapes)\n-            if type(shape) not in {list, tuple}:\n-                raise Exception(\"output_shape function must return a tuple\")\n-            return tuple(shape)\n-\n-    def get_params(self):\n-        return self.params, self.regularizers, self.constraints, self.updates\n-\n-    def get_output(self, train=False):\n-        func = marshal.loads(self.function)\n-        func = types.FunctionType(func, globals())\n-        inputs = [layer.get_output(train) for layer in self.layers]\n-        return func(inputs)\n-\n-    def get_input(self, train=False):\n-        res = []\n-        for i in range(len(self.layers)):\n-            o = self.layers[i].get_input(train)\n-            if not type(o) == list:\n-                o = [o]\n-            for output in o:\n-                if output not in res:\n-                    res.append(output)\n-        return res\n-\n-    @property\n-    def input(self):\n-        return self.get_input()\n-\n-    def supports_masked_input(self):\n-        return False\n-\n-    def get_output_mask(self, train=None):\n-        return None\n-\n-    def get_weights(self):\n-        weights = []\n-        for l in self.layers:\n-            weights += l.get_weights()\n-        return weights\n-\n-    def set_weights(self, weights):\n-        for i in range(len(self.layers)):\n-            nb_param = len(self.layers[i].params)\n-            self.layers[i].set_weights(weights[:nb_param])\n-            weights = weights[nb_param:]\n-\n-    def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"layers\": [l.get_config() for l in self.layers],\n-                  \"function\": self.function,\n-                  \"output_shape\": self._output_shape\n-                  }\n-        base_config = super(LambdaMerge, self).get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n-\n \n class Siamese(Layer):\n \n+    '''Shared layer with multiple inputs\n+\n+    Output shape\n+    ------------\n+    Depends on merge_mode argument\n+\n+    Arguments\n+    ---------\n+    layer - The layer to be shared across multiple inputs\n+    inputs - inputs to the shared layer\n+    merge_mode - Similar to mode argument of Merge layer\n+    concat_axis - Similar to concat_axis argument of Merge layer\n+    dot_axes - Similar to dot_axes argument of Merge layer\n+    '''\n+\n     def __init__(self, layer, inputs, merge_mode='concat', concat_axis=1, dot_axes=-1):\n \n         if merge_mode not in ['sum', 'mul', 'concat', 'ave', 'join', 'cos', 'dot', None]:\n@@ -1338,6 +206,15 @@ class Siamese(Layer):\n \n \n class SiameseHead(Layer):\n+\n+    '''This layer should be added only on top of a Siamese layer with merge_mode = None\n+\n+    Outputs the output of the Siamese layer at a given index, specified by the head argument\n+\n+    Arguments\n+    ---------\n+    head - The index at which the output of the Siamese layer should be obtained\n+    '''\n     def __init__(self, head):\n         self.head = head\n         self.params = []\n@@ -1366,6 +243,9 @@ class SiameseHead(Layer):\n \n \n def add_shared_layer(layer, inputs):\n+    '''\n+    Use this function to add a shared layer across multiple Sequential models without merging the outputs\n+    '''\n     input_layers = [l.layers[-1] for l in inputs]\n     s = Siamese(layer, input_layers, merge_mode=None)\n     for i in range(len(inputs)):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
