{"custom_id": "keras#c3810a76dd61217342b357cf0ed7ce6d898e877b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 113 | Lines Deleted: 6 | Files Changed: 5 | Hunks: 5 | Methods Changed: 8 | Complexity Δ (Sum/Max): 11/6 | Churn Δ: 119 | Churn Cumulative: 645 | Contributors (this commit): 11 | Commits (past 90d): 95 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -111,6 +111,7 @@ from keras_core.layers.reshaping.reshape import Reshape\n from keras_core.layers.reshaping.up_sampling1d import UpSampling1D\n from keras_core.layers.reshaping.up_sampling2d import UpSampling2D\n from keras_core.layers.reshaping.up_sampling3d import UpSampling3D\n+from keras_core.layers.reshaping.zero_padding1d import ZeroPadding1D\n from keras_core.layers.reshaping.zero_padding2d import ZeroPadding2D\n from keras_core.layers.reshaping.zero_padding3d import ZeroPadding3D\n from keras_core.layers.rnn.bidirectional import Bidirectional\n\n@@ -26,10 +26,12 @@ class Cropping1D(Layer):\n      [[8 9]]]\n \n     Args:\n-        cropping: Integer or tuple of integers of length 2.\n-            How many units should be trimmed off at the beginning and end of\n-            the cropping dimension (axis 1).\n-            If a single int is provided, the same value will be used for both.\n+        cropping: Int, or tuple of int (length 2), or dictionary.\n+            - If int: how many units should be trimmed off at the beginning and\n+              end of the cropping dimension (axis 1).\n+            - If tuple of 2 ints: how many units should be trimmed off at the\n+              beginning and end of the cropping dimension\n+              (`(left_crop, right_crop)`).\n \n     Input shape:\n         3D tensor with shape `(batch_size, axis_to_crop, features)`\n\n@@ -0,0 +1,69 @@\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.input_spec import InputSpec\n+from keras_core.layers.layer import Layer\n+\n+\n+@keras_core_export(\"keras_core.layers.ZeroPadding1D\")\n+class ZeroPadding1D(Layer):\n+    \"\"\"Zero-padding layer for 1D input (e.g. temporal sequence).\n+\n+    Examples:\n+\n+    >>> input_shape = (2, 2, 3)\n+    >>> x = np.arange(np.prod(input_shape)).reshape(input_shape)\n+    >>> x\n+    [[[ 0  1  2]\n+      [ 3  4  5]]\n+     [[ 6  7  8]\n+      [ 9 10 11]]]\n+    >>> y = keras_core.layers.ZeroPadding1D(padding=2)(x)\n+    >>> y\n+    [[[ 0  0  0]\n+      [ 0  0  0]\n+      [ 0  1  2]\n+      [ 3  4  5]\n+      [ 0  0  0]\n+      [ 0  0  0]]\n+     [[ 0  0  0]\n+      [ 0  0  0]\n+      [ 6  7  8]\n+      [ 9 10 11]\n+      [ 0  0  0]\n+      [ 0  0  0]]]\n+\n+    Args:\n+        padding: Int, or tuple of int (length 2), or dictionary.\n+            - If int: how many zeros to add at the beginning and end of\n+              the padding dimension (axis 1).\n+            - If tuple of 2 ints: how many zeros to add at the beginning and the\n+              end of the padding dimension (`(left_pad, right_pad)`).\n+\n+    Input shape:\n+        3D tensor with shape `(batch_size, axis_to_pad, features)`\n+\n+    Output shape:\n+        3D tensor with shape `(batch_size, padded_axis, features)`\n+    \"\"\"\n+\n+    def __init__(self, padding=1, name=None, dtype=None):\n+        super().__init__(name=name, dtype=dtype)\n+        if isinstance(padding, int):\n+            padding = (padding, padding)\n+        self.padding = padding\n+        self.input_spec = InputSpec(ndim=3)\n+\n+    def compute_output_shape(self, input_shape):\n+        output_shape = list(input_shape)\n+        if input_shape[1] is not None:\n+            input_shape[1] += self.padding[0] + self.padding[1]\n+        return tuple(output_shape)\n+\n+    def call(self, inputs):\n+        all_dims_padding = ((0, 0), self.padding, (0, 0))\n+        return ops.pad(inputs, all_dims_padding)\n+\n+    def get_config(self):\n+        config = {\"padding\": self.padding}\n+        base_config = super().get_config()\n+        return {**base_config, **config}\n\n@@ -0,0 +1,35 @@\n+import numpy as np\n+import pytest\n+from absl.testing import parameterized\n+\n+from keras_core import backend\n+from keras_core import layers\n+from keras_core import testing\n+\n+\n+class ZeroPadding1DTest(testing.TestCase, parameterized.TestCase):\n+    def test_zero_padding_1d(self):\n+        inputs = np.random.rand(1, 2, 3)\n+        outputs = layers.ZeroPadding1D(padding=(1, 2))(inputs)\n+\n+        for index in [0, -1, -2]:\n+            self.assertAllClose(outputs[:, index, :], 0.0)\n+        self.assertAllClose(outputs[:, 1:-2, :], inputs)\n+\n+    @parameterized.named_parameters((\"one_tuple\", (2, 2)), (\"one_int\", 2))\n+    def test_zero_padding_1d_with_same_padding(self, padding):\n+        inputs = np.random.rand(1, 2, 3)\n+        outputs = layers.ZeroPadding1D(padding=padding)(inputs)\n+\n+        for index in [0, 1, -1, -2]:\n+            self.assertAllClose(outputs[:, index, :], 0.0)\n+        self.assertAllClose(outputs[:, 2:-2, :], inputs)\n+\n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes\",\n+    )\n+    def test_zero_padding_1d_with_dynamic_spatial_dim(self):\n+        input_layer = layers.Input(batch_shape=(1, None, 3))\n+        padded = layers.ZeroPadding1D((1, 2))(input_layer)\n+        self.assertEqual(padded.shape, (1, None, 3))\n\n@@ -67,8 +67,8 @@ class ZeroPadding2D(Layer):\n           `(batch_size, channels, padded_height, padded_width)`\n     \"\"\"\n \n-    def __init__(self, padding=(1, 1), data_format=None, **kwargs):\n-        super().__init__(**kwargs)\n+    def __init__(self, padding=(1, 1), data_format=None, name=None, dtype=None):\n+        super().__init__(name=name, dtype=dtype)\n         self.data_format = backend.standardize_data_format(data_format)\n         if isinstance(padding, int):\n             self.padding = ((padding, padding), (padding, padding))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5b7e5f88b27d3786cacdf1cea5c58fe4a9be3940", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2289 | Lines Deleted: 162 | Files Changed: 16 | Hunks: 53 | Methods Changed: 50 | Complexity Δ (Sum/Max): 96/60 | Churn Δ: 2451 | Churn Cumulative: 5722 | Contributors (this commit): 13 | Commits (past 90d): 153 | Contributors (cumulative): 42 | DMM Complexity: 0.411214953271028\n\nDIFF:\n@@ -42,6 +42,8 @@ ALL_OBJECTS = {\n }\n \n ALL_OBJECTS_DICT = {fn.__name__: fn for fn in ALL_OBJECTS}\n+# Additional aliases\n+ALL_OBJECTS_DICT[\"swish\"] = silu\n \n \n @keras_core_export(\"keras_core.activations.serialize\")\n@@ -89,8 +91,8 @@ def get(identifier):\n     if identifier is None:\n         return linear\n     if isinstance(identifier, (str, dict)):\n-        return deserialize(identifier)\n-    elif callable(identifier):\n+        identifier = deserialize(identifier)\n+    if callable(identifier):\n         return identifier\n     raise TypeError(\n         f\"Could not interpret activation function identifier: {identifier}\"\n\n\n@@ -4,6 +4,8 @@ from absl.testing import parameterized\n \n from keras_core import backend\n from keras_core import testing\n+from keras_core.applications import efficientnet\n+from keras_core.applications import efficientnet_v2\n from keras_core.applications import mobilenet\n from keras_core.applications import mobilenet_v2\n from keras_core.applications import mobilenet_v3\n@@ -19,14 +21,32 @@ except ImportError:\n     PIL = None\n \n MODEL_LIST = [\n-    # cls, last_dim\n+    # vgg\n     (vgg16.VGG16, 512, vgg16),\n     (vgg19.VGG19, 512, vgg19),\n+    # xception\n     (xception.Xception, 2048, xception),\n+    # mobilnet\n     (mobilenet.MobileNet, 1024, mobilenet),\n     (mobilenet_v2.MobileNetV2, 1280, mobilenet_v2),\n     (mobilenet_v3.MobileNetV3Small, 576, mobilenet_v3),\n     (mobilenet_v3.MobileNetV3Large, 960, mobilenet_v3),\n+    # efficientnet\n+    (efficientnet.EfficientNetB0, 1280, efficientnet),\n+    (efficientnet.EfficientNetB1, 1280, efficientnet),\n+    (efficientnet.EfficientNetB2, 1408, efficientnet),\n+    (efficientnet.EfficientNetB3, 1536, efficientnet),\n+    (efficientnet.EfficientNetB4, 1792, efficientnet),\n+    (efficientnet.EfficientNetB5, 2048, efficientnet),\n+    (efficientnet.EfficientNetB6, 2304, efficientnet),\n+    (efficientnet.EfficientNetB7, 2560, efficientnet),\n+    (efficientnet_v2.EfficientNetV2B0, 1280, efficientnet_v2),\n+    (efficientnet_v2.EfficientNetV2B1, 1280, efficientnet_v2),\n+    (efficientnet_v2.EfficientNetV2B2, 1408, efficientnet_v2),\n+    (efficientnet_v2.EfficientNetV2B3, 1536, efficientnet_v2),\n+    (efficientnet_v2.EfficientNetV2S, 1280, efficientnet_v2),\n+    (efficientnet_v2.EfficientNetV2M, 1280, efficientnet_v2),\n+    (efficientnet_v2.EfficientNetV2L, 1280, efficientnet_v2),\n ]\n # Add names for `named_parameters`.\n MODEL_LIST = [(e[0].__name__, *e) for e in MODEL_LIST]\n\n@@ -0,0 +1,855 @@\n+import copy\n+import math\n+\n+from tensorflow.io import gfile\n+\n+from keras_core import backend\n+from keras_core import layers\n+from keras_core.api_export import keras_core_export\n+from keras_core.applications import imagenet_utils\n+from keras_core.models import Functional\n+from keras_core.operations import operation_utils\n+from keras_core.utils import file_utils\n+\n+BASE_WEIGHTS_PATH = \"https://storage.googleapis.com/keras-applications/\"\n+\n+WEIGHTS_HASHES = {\n+    \"b0\": (\n+        \"902e53a9f72be733fc0bcb005b3ebbac\",\n+        \"50bc09e76180e00e4465e1a485ddc09d\",\n+    ),\n+    \"b1\": (\n+        \"1d254153d4ab51201f1646940f018540\",\n+        \"74c4e6b3e1f6a1eea24c589628592432\",\n+    ),\n+    \"b2\": (\n+        \"b15cce36ff4dcbd00b6dd88e7857a6ad\",\n+        \"111f8e2ac8aa800a7a99e3239f7bfb39\",\n+    ),\n+    \"b3\": (\n+        \"ffd1fdc53d0ce67064dc6a9c7960ede0\",\n+        \"af6d107764bb5b1abb91932881670226\",\n+    ),\n+    \"b4\": (\n+        \"18c95ad55216b8f92d7e70b3a046e2fc\",\n+        \"ebc24e6d6c33eaebbd558eafbeedf1ba\",\n+    ),\n+    \"b5\": (\n+        \"ace28f2a6363774853a83a0b21b9421a\",\n+        \"38879255a25d3c92d5e44e04ae6cec6f\",\n+    ),\n+    \"b6\": (\n+        \"165f6e37dce68623721b423839de8be5\",\n+        \"9ecce42647a20130c1f39a5d4cb75743\",\n+    ),\n+    \"b7\": (\n+        \"8c03f828fec3ef71311cd463b6759d99\",\n+        \"cbcfe4450ddf6f3ad90b1b398090fe4a\",\n+    ),\n+}\n+\n+DEFAULT_BLOCKS_ARGS = [\n+    {\n+        \"kernel_size\": 3,\n+        \"repeats\": 1,\n+        \"filters_in\": 32,\n+        \"filters_out\": 16,\n+        \"expand_ratio\": 1,\n+        \"id_skip\": True,\n+        \"strides\": 1,\n+        \"se_ratio\": 0.25,\n+    },\n+    {\n+        \"kernel_size\": 3,\n+        \"repeats\": 2,\n+        \"filters_in\": 16,\n+        \"filters_out\": 24,\n+        \"expand_ratio\": 6,\n+        \"id_skip\": True,\n+        \"strides\": 2,\n+        \"se_ratio\": 0.25,\n+    },\n+    {\n+        \"kernel_size\": 5,\n+        \"repeats\": 2,\n+        \"filters_in\": 24,\n+        \"filters_out\": 40,\n+        \"expand_ratio\": 6,\n+        \"id_skip\": True,\n+        \"strides\": 2,\n+        \"se_ratio\": 0.25,\n+    },\n+    {\n+        \"kernel_size\": 3,\n+        \"repeats\": 3,\n+        \"filters_in\": 40,\n+        \"filters_out\": 80,\n+        \"expand_ratio\": 6,\n+        \"id_skip\": True,\n+        \"strides\": 2,\n+        \"se_ratio\": 0.25,\n+    },\n+    {\n+        \"kernel_size\": 5,\n+        \"repeats\": 3,\n+        \"filters_in\": 80,\n+        \"filters_out\": 112,\n+        \"expand_ratio\": 6,\n+        \"id_skip\": True,\n+        \"strides\": 1,\n+        \"se_ratio\": 0.25,\n+    },\n+    {\n+        \"kernel_size\": 5,\n+        \"repeats\": 4,\n+        \"filters_in\": 112,\n+        \"filters_out\": 192,\n+        \"expand_ratio\": 6,\n+        \"id_skip\": True,\n+        \"strides\": 2,\n+        \"se_ratio\": 0.25,\n+    },\n+    {\n+        \"kernel_size\": 3,\n+        \"repeats\": 1,\n+        \"filters_in\": 192,\n+        \"filters_out\": 320,\n+        \"expand_ratio\": 6,\n+        \"id_skip\": True,\n+        \"strides\": 1,\n+        \"se_ratio\": 0.25,\n+    },\n+]\n+\n+CONV_KERNEL_INITIALIZER = {\n+    \"class_name\": \"VarianceScaling\",\n+    \"config\": {\n+        \"scale\": 2.0,\n+        \"mode\": \"fan_out\",\n+        \"distribution\": \"truncated_normal\",\n+    },\n+}\n+\n+DENSE_KERNEL_INITIALIZER = {\n+    \"class_name\": \"VarianceScaling\",\n+    \"config\": {\n+        \"scale\": 1.0 / 3.0,\n+        \"mode\": \"fan_out\",\n+        \"distribution\": \"uniform\",\n+    },\n+}\n+\n+BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n+\n+Reference:\n+- [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](\n+    https://arxiv.org/abs/1905.11946) (ICML 2019)\n+\n+This function returns a Keras image classification model,\n+optionally loaded with weights pre-trained on ImageNet.\n+\n+For image classification use cases, see\n+[this page for detailed examples](\n+https://keras.io/api/applications/#usage-examples-for-image-classification-models).\n+\n+For transfer learning use cases, make sure to read the\n+[guide to transfer learning & fine-tuning](\n+https://keras.io/guides/transfer_learning/).\n+\n+Note: each Keras Application expects a specific kind of input preprocessing.\n+For EfficientNet, input preprocessing is included as part of the model\n+(as a `Rescaling` layer), and thus\n+`keras_core.applications.efficientnet.preprocess_input` is actually a\n+pass-through function. EfficientNet models expect their inputs to be float\n+tensors of pixels with values in the `[0-255]` range.\n+\n+Args:\n+    include_top: Whether to include the fully-connected\n+        layer at the top of the network. Defaults to `True`.\n+    weights: One of `None` (random initialization),\n+        `\"imagenet\"` (pre-training on ImageNet),\n+        or the path to the weights file to be loaded.\n+        Defaults to `\"imagenet\"`.\n+    input_tensor: Optional Keras tensor\n+        (i.e. output of `layers.Input()`)\n+        to use as image input for the model.\n+    input_shape: Optional shape tuple, only to be specified\n+        if `include_top` is False.\n+        It should have exactly 3 inputs channels.\n+    pooling: Optional pooling mode for feature extraction\n+        when `include_top` is `False`. Defaults to `None`.\n+        - `None` means that the output of the model will be\n+            the 4D tensor output of the\n+            last convolutional layer.\n+        - `avg` means that global average pooling\n+            will be applied to the output of the\n+            last convolutional layer, and thus\n+            the output of the model will be a 2D tensor.\n+        - `max` means that global max pooling will\n+            be applied.\n+    classes: Optional number of classes to classify images\n+        into, only to be specified if `include_top` is True, and\n+        if no `weights` argument is specified. 1000 is how many\n+        ImageNet classes there are. Defaults to `1000`.\n+    classifier_activation: A `str` or callable. The activation function to use\n+        on the \"top\" layer. Ignored unless `include_top=True`. Set\n+        `classifier_activation=None` to return the logits of the \"top\" layer.\n+        Defaults to 'softmax'.\n+        When loading pretrained weights, `classifier_activation` can only\n+        be `None` or `\"softmax\"`.\n+\n+Returns:\n+    A model instance.\n+\"\"\"\n+\n+\n+IMAGENET_STDDEV_RGB = [0.229, 0.224, 0.225]\n+\n+\n+def EfficientNet(\n+    width_coefficient,\n+    depth_coefficient,\n+    default_size,\n+    dropout_rate=0.2,\n+    drop_connect_rate=0.2,\n+    depth_divisor=8,\n+    activation=\"swish\",\n+    blocks_args=\"default\",\n+    model_name=\"efficientnet\",\n+    include_top=True,\n+    weights=\"imagenet\",\n+    input_tensor=None,\n+    input_shape=None,\n+    pooling=None,\n+    classes=1000,\n+    classifier_activation=\"softmax\",\n+):\n+    \"\"\"Instantiates the EfficientNet architecture.\n+\n+    Args:\n+      width_coefficient: float, scaling coefficient for network width.\n+      depth_coefficient: float, scaling coefficient for network depth.\n+      default_size: integer, default input image size.\n+      dropout_rate: float, dropout rate before final classifier layer.\n+      drop_connect_rate: float, dropout rate at skip connections.\n+      depth_divisor: integer, a unit of network width.\n+      activation: activation function.\n+      blocks_args: list of dicts, parameters to construct block modules.\n+      model_name: string, model name.\n+      include_top: whether to include the fully-connected\n+          layer at the top of the network.\n+      weights: one of `None` (random initialization),\n+            'imagenet' (pre-training on ImageNet),\n+            or the path to the weights file to be loaded.\n+      input_tensor: optional Keras tensor\n+          (i.e. output of `layers.Input()`)\n+          to use as image input for the model.\n+      input_shape: optional shape tuple, only to be specified\n+          if `include_top` is False.\n+          It should have exactly 3 inputs channels.\n+      pooling: optional pooling mode for feature extraction\n+          when `include_top` is `False`.\n+          - `None` means that the output of the model will be\n+              the 4D tensor output of the\n+              last convolutional layer.\n+          - `avg` means that global average pooling\n+              will be applied to the output of the\n+              last convolutional layer, and thus\n+              the output of the model will be a 2D tensor.\n+          - `max` means that global max pooling will\n+              be applied.\n+      classes: optional number of classes to classify images\n+          into, only to be specified if `include_top` is True, and\n+          if no `weights` argument is specified.\n+      classifier_activation: A `str` or callable. The activation function to use\n+          on the \"top\" layer. Ignored unless `include_top=True`. Set\n+          `classifier_activation=None` to return the logits of the \"top\" layer.\n+\n+    Returns:\n+        A model instance.\n+    \"\"\"\n+    if blocks_args == \"default\":\n+        blocks_args = DEFAULT_BLOCKS_ARGS\n+\n+    if not (weights in {\"imagenet\", None} or gfile.exists(weights)):\n+        raise ValueError(\n+            \"The `weights` argument should be either \"\n+            \"`None` (random initialization), `imagenet` \"\n+            \"(pre-training on ImageNet), \"\n+            \"or the path to the weights file to be loaded.\"\n+        )\n+\n+    if weights == \"imagenet\" and include_top and classes != 1000:\n+        raise ValueError(\n+            'If using `weights=\"imagenet\"` with `include_top`'\n+            \" as true, `classes` should be 1000\"\n+        )\n+\n+    # Determine proper input shape\n+    input_shape = imagenet_utils.obtain_input_shape(\n+        input_shape,\n+        default_size=default_size,\n+        min_size=32,\n+        data_format=backend.image_data_format(),\n+        require_flatten=include_top,\n+        weights=weights,\n+    )\n+\n+    if input_tensor is None:\n+        img_input = layers.Input(shape=input_shape)\n+    else:\n+        if not backend.is_keras_tensor(input_tensor):\n+            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n+        else:\n+            img_input = input_tensor\n+\n+    bn_axis = 3 if backend.image_data_format() == \"channels_last\" else 1\n+\n+    def round_filters(filters, divisor=depth_divisor):\n+        \"\"\"Round number of filters based on depth multiplier.\"\"\"\n+        filters *= width_coefficient\n+        new_filters = max(\n+            divisor, int(filters + divisor / 2) // divisor * divisor\n+        )\n+        # Make sure that round down does not go down by more than 10%.\n+        if new_filters < 0.9 * filters:\n+            new_filters += divisor\n+        return int(new_filters)\n+\n+    def round_repeats(repeats):\n+        \"\"\"Round number of repeats based on depth multiplier.\"\"\"\n+        return int(math.ceil(depth_coefficient * repeats))\n+\n+    # Build stem\n+    x = img_input\n+    x = layers.Rescaling(1.0 / 255.0)(x)\n+    x = layers.Normalization(axis=bn_axis)(x)\n+    if weights == \"imagenet\":\n+        # Note that the normaliztion layer uses square value of STDDEV as the\n+        # variance for the layer: result = (input - mean) / sqrt(var)\n+        # However, the original implemenetation uses (input - mean) / var to\n+        # normalize the input, we need to divide another sqrt(var) to match the\n+        # original implementation.\n+        # See https://github.com/tensorflow/tensorflow/issues/49930 for more\n+        # details\n+        x = layers.Rescaling(\n+            [1.0 / math.sqrt(stddev) for stddev in IMAGENET_STDDEV_RGB]\n+        )(x)\n+\n+    x = layers.ZeroPadding2D(\n+        padding=imagenet_utils.correct_pad(x, 3), name=\"stem_conv_pad\"\n+    )(x)\n+    x = layers.Conv2D(\n+        round_filters(32),\n+        3,\n+        strides=2,\n+        padding=\"valid\",\n+        use_bias=False,\n+        kernel_initializer=CONV_KERNEL_INITIALIZER,\n+        name=\"stem_conv\",\n+    )(x)\n+    x = layers.BatchNormalization(axis=bn_axis, name=\"stem_bn\")(x)\n+    x = layers.Activation(activation, name=\"stem_activation\")(x)\n+\n+    # Build blocks\n+    blocks_args = copy.deepcopy(blocks_args)\n+\n+    b = 0\n+    blocks = float(sum(round_repeats(args[\"repeats\"]) for args in blocks_args))\n+    for i, args in enumerate(blocks_args):\n+        assert args[\"repeats\"] > 0\n+        # Update block input and output filters based on depth multiplier.\n+        args[\"filters_in\"] = round_filters(args[\"filters_in\"])\n+        args[\"filters_out\"] = round_filters(args[\"filters_out\"])\n+\n+        for j in range(round_repeats(args.pop(\"repeats\"))):\n+            # The first block needs to take care of stride and filter size\n+            # increase.\n+            if j > 0:\n+                args[\"strides\"] = 1\n+                args[\"filters_in\"] = args[\"filters_out\"]\n+            x = block(\n+                x,\n+                activation,\n+                drop_connect_rate * b / blocks,\n+                name=f\"block{i + 1}{chr(j + 97)}_\",\n+                **args,\n+            )\n+            b += 1\n+\n+    # Build top\n+    x = layers.Conv2D(\n+        round_filters(1280),\n+        1,\n+        padding=\"same\",\n+        use_bias=False,\n+        kernel_initializer=CONV_KERNEL_INITIALIZER,\n+        name=\"top_conv\",\n+    )(x)\n+    x = layers.BatchNormalization(axis=bn_axis, name=\"top_bn\")(x)\n+    x = layers.Activation(activation, name=\"top_activation\")(x)\n+    if include_top:\n+        x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(x)\n+        if dropout_rate > 0:\n+            x = layers.Dropout(dropout_rate, name=\"top_dropout\")(x)\n+        imagenet_utils.validate_activation(classifier_activation, weights)\n+        x = layers.Dense(\n+            classes,\n+            activation=classifier_activation,\n+            kernel_initializer=DENSE_KERNEL_INITIALIZER,\n+            name=\"predictions\",\n+        )(x)\n+    else:\n+        if pooling == \"avg\":\n+            x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(x)\n+        elif pooling == \"max\":\n+            x = layers.GlobalMaxPooling2D(name=\"max_pool\")(x)\n+\n+    # Ensure that the model takes into account\n+    # any potential predecessors of `input_tensor`.\n+    if input_tensor is not None:\n+        inputs = operation_utils.get_source_inputs(input_tensor)\n+    else:\n+        inputs = img_input\n+\n+    # Create model.\n+    model = Functional(inputs, x, name=model_name)\n+\n+    # Load weights.\n+    if weights == \"imagenet\":\n+        if include_top:\n+            file_suffix = \".h5\"\n+            file_hash = WEIGHTS_HASHES[model_name[-2:]][0]\n+        else:\n+            file_suffix = \"_notop.h5\"\n+            file_hash = WEIGHTS_HASHES[model_name[-2:]][1]\n+        file_name = model_name + file_suffix\n+        weights_path = file_utils.get_file(\n+            file_name,\n+            BASE_WEIGHTS_PATH + file_name,\n+            cache_subdir=\"models\",\n+            file_hash=file_hash,\n+        )\n+        model.load_weights(weights_path)\n+    elif weights is not None:\n+        model.load_weights(weights)\n+    return model\n+\n+\n+def block(\n+    inputs,\n+    activation=\"swish\",\n+    drop_rate=0.0,\n+    name=\"\",\n+    filters_in=32,\n+    filters_out=16,\n+    kernel_size=3,\n+    strides=1,\n+    expand_ratio=1,\n+    se_ratio=0.0,\n+    id_skip=True,\n+):\n+    \"\"\"An inverted residual block.\n+\n+    Args:\n+        inputs: input tensor.\n+        activation: activation function.\n+        drop_rate: float between 0 and 1, fraction of the input units to drop.\n+        name: string, block label.\n+        filters_in: integer, the number of input filters.\n+        filters_out: integer, the number of output filters.\n+        kernel_size: integer, the dimension of the convolution window.\n+        strides: integer, the stride of the convolution.\n+        expand_ratio: integer, scaling coefficient for the input filters.\n+        se_ratio: float between 0 and 1, fraction to squeeze the input filters.\n+        id_skip: boolean.\n+\n+    Returns:\n+        output tensor for the block.\n+    \"\"\"\n+    bn_axis = 3 if backend.image_data_format() == \"channels_last\" else 1\n+\n+    # Expansion phase\n+    filters = filters_in * expand_ratio\n+    if expand_ratio != 1:\n+        x = layers.Conv2D(\n+            filters,\n+            1,\n+            padding=\"same\",\n+            use_bias=False,\n+            kernel_initializer=CONV_KERNEL_INITIALIZER,\n+            name=name + \"expand_conv\",\n+        )(inputs)\n+        x = layers.BatchNormalization(axis=bn_axis, name=name + \"expand_bn\")(x)\n+        x = layers.Activation(activation, name=name + \"expand_activation\")(x)\n+    else:\n+        x = inputs\n+\n+    # Depthwise Convolution\n+    if strides == 2:\n+        x = layers.ZeroPadding2D(\n+            padding=imagenet_utils.correct_pad(x, kernel_size),\n+            name=name + \"dwconv_pad\",\n+        )(x)\n+        conv_pad = \"valid\"\n+    else:\n+        conv_pad = \"same\"\n+    x = layers.DepthwiseConv2D(\n+        kernel_size,\n+        strides=strides,\n+        padding=conv_pad,\n+        use_bias=False,\n+        depthwise_initializer=CONV_KERNEL_INITIALIZER,\n+        name=name + \"dwconv\",\n+    )(x)\n+    x = layers.BatchNormalization(axis=bn_axis, name=name + \"bn\")(x)\n+    x = layers.Activation(activation, name=name + \"activation\")(x)\n+\n+    # Squeeze and Excitation phase\n+    if 0 < se_ratio <= 1:\n+        filters_se = max(1, int(filters_in * se_ratio))\n+        se = layers.GlobalAveragePooling2D(name=name + \"se_squeeze\")(x)\n+        if bn_axis == 1:\n+            se_shape = (filters, 1, 1)\n+        else:\n+            se_shape = (1, 1, filters)\n+        se = layers.Reshape(se_shape, name=name + \"se_reshape\")(se)\n+        se = layers.Conv2D(\n+            filters_se,\n+            1,\n+            padding=\"same\",\n+            activation=activation,\n+            kernel_initializer=CONV_KERNEL_INITIALIZER,\n+            name=name + \"se_reduce\",\n+        )(se)\n+        se = layers.Conv2D(\n+            filters,\n+            1,\n+            padding=\"same\",\n+            activation=\"sigmoid\",\n+            kernel_initializer=CONV_KERNEL_INITIALIZER,\n+            name=name + \"se_expand\",\n+        )(se)\n+        x = layers.multiply([x, se], name=name + \"se_excite\")\n+\n+    # Output phase\n+    x = layers.Conv2D(\n+        filters_out,\n+        1,\n+        padding=\"same\",\n+        use_bias=False,\n+        kernel_initializer=CONV_KERNEL_INITIALIZER,\n+        name=name + \"project_conv\",\n+    )(x)\n+    x = layers.BatchNormalization(axis=bn_axis, name=name + \"project_bn\")(x)\n+    if id_skip and strides == 1 and filters_in == filters_out:\n+        if drop_rate > 0:\n+            x = layers.Dropout(\n+                drop_rate, noise_shape=(None, 1, 1, 1), name=name + \"drop\"\n+            )(x)\n+        x = layers.add([x, inputs], name=name + \"add\")\n+    return x\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.applications.efficientnet.EfficientNetB0\",\n+        \"keras_core.applications.EfficientNetB0\",\n+    ]\n+)\n+def EfficientNetB0(\n+    include_top=True,\n+    weights=\"imagenet\",\n+    input_tensor=None,\n+    input_shape=None,\n+    pooling=None,\n+    classes=1000,\n+    classifier_activation=\"softmax\",\n+    **kwargs,\n+):\n+    return EfficientNet(\n+        1.0,\n+        1.0,\n+        224,\n+        0.2,\n+        model_name=\"efficientnetb0\",\n+        include_top=include_top,\n+        weights=weights,\n+        input_tensor=input_tensor,\n+        input_shape=input_shape,\n+        pooling=pooling,\n+        classes=classes,\n+        classifier_activation=classifier_activation,\n+        **kwargs,\n+    )\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.applications.efficientnet.EfficientNetB1\",\n+        \"keras_core.applications.EfficientNetB1\",\n+    ]\n+)\n+def EfficientNetB1(\n+    include_top=True,\n+    weights=\"imagenet\",\n+    input_tensor=None,\n+    input_shape=None,\n+    pooling=None,\n+    classes=1000,\n+    classifier_activation=\"softmax\",\n+    **kwargs,\n+):\n+    return EfficientNet(\n+        1.0,\n+        1.1,\n+        240,\n+        0.2,\n+        model_name=\"efficientnetb1\",\n+        include_top=include_top,\n+        weights=weights,\n+        input_tensor=input_tensor,\n+        input_shape=input_shape,\n+        pooling=pooling,\n+        classes=classes,\n+        classifier_activation=classifier_activation,\n+        **kwargs,\n+    )\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.applications.efficientnet.EfficientNetB2\",\n+        \"keras_core.applications.EfficientNetB2\",\n+    ]\n+)\n+def EfficientNetB2(\n+    include_top=True,\n+    weights=\"imagenet\",\n+    input_tensor=None,\n+    input_shape=None,\n+    pooling=None,\n+    classes=1000,\n+    classifier_activation=\"softmax\",\n+    **kwargs,\n+):\n+    return EfficientNet(\n+        1.1,\n+        1.2,\n+        260,\n+        0.3,\n+        model_name=\"efficientnetb2\",\n+        include_top=include_top,\n+        weights=weights,\n+        input_tensor=input_tensor,\n+        input_shape=input_shape,\n+        pooling=pooling,\n+        classes=classes,\n+        classifier_activation=classifier_activation,\n+        **kwargs,\n+    )\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.applications.efficientnet.EfficientNetB3\",\n+        \"keras_core.applications.EfficientNetB3\",\n+    ]\n+)\n+def EfficientNetB3(\n+    include_top=True,\n+    weights=\"imagenet\",\n+    input_tensor=None,\n+    input_shape=None,\n+    pooling=None,\n+    classes=1000,\n+    classifier_activation=\"softmax\",\n+    **kwargs,\n+):\n+    return EfficientNet(\n+        1.2,\n+        1.4,\n+        300,\n+        0.3,\n+        model_name=\"efficientnetb3\",\n+        include_top=include_top,\n+        weights=weights,\n+        input_tensor=input_tensor,\n+        input_shape=input_shape,\n+        pooling=pooling,\n+        classes=classes,\n+        classifier_activation=classifier_activation,\n+        **kwargs,\n+    )\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.applications.efficientnet.EfficientNetB4\",\n+        \"keras_core.applications.EfficientNetB4\",\n+    ]\n+)\n+def EfficientNetB4(\n+    include_top=True,\n+    weights=\"imagenet\",\n+    input_tensor=None,\n+    input_shape=None,\n+    pooling=None,\n+    classes=1000,\n+    classifier_activation=\"softmax\",\n+    **kwargs,\n+):\n+    return EfficientNet(\n+        1.4,\n+        1.8,\n+        380,\n+        0.4,\n+        model_name=\"efficientnetb4\",\n+        include_top=include_top,\n+        weights=weights,\n+        input_tensor=input_tensor,\n+        input_shape=input_shape,\n+        pooling=pooling,\n+        classes=classes,\n+        classifier_activation=classifier_activation,\n+        **kwargs,\n+    )\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.applications.efficientnet.EfficientNetB5\",\n+        \"keras_core.applications.EfficientNetB5\",\n+    ]\n+)\n+def EfficientNetB5(\n+    include_top=True,\n+    weights=\"imagenet\",\n+    input_tensor=None,\n+    input_shape=None,\n+    pooling=None,\n+    classes=1000,\n+    classifier_activation=\"softmax\",\n+    **kwargs,\n+):\n+    return EfficientNet(\n+        1.6,\n+        2.2,\n+        456,\n+        0.4,\n+        model_name=\"efficientnetb5\",\n+        include_top=include_top,\n+        weights=weights,\n+        input_tensor=input_tensor,\n+        input_shape=input_shape,\n+        pooling=pooling,\n+        classes=classes,\n+        classifier_activation=classifier_activation,\n+        **kwargs,\n+    )\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.applications.efficientnet.EfficientNetB6\",\n+        \"keras_core.applications.EfficientNetB6\",\n+    ]\n+)\n+def EfficientNetB6(\n+    include_top=True,\n+    weights=\"imagenet\",\n+    input_tensor=None,\n+    input_shape=None,\n+    pooling=None,\n+    classes=1000,\n+    classifier_activation=\"softmax\",\n+    **kwargs,\n+):\n+    return EfficientNet(\n+        1.8,\n+        2.6,\n+        528,\n+        0.5,\n+        model_name=\"efficientnetb6\",\n+        include_top=include_top,\n+        weights=weights,\n+        input_tensor=input_tensor,\n+        input_shape=input_shape,\n+        pooling=pooling,\n+        classes=classes,\n+        classifier_activation=classifier_activation,\n+        **kwargs,\n+    )\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.applications.efficientnet.EfficientNetB7\",\n+        \"keras_core.applications.EfficientNetB7\",\n+    ]\n+)\n+def EfficientNetB7(\n+    include_top=True,\n+    weights=\"imagenet\",\n+    input_tensor=None,\n+    input_shape=None,\n+    pooling=None,\n+    classes=1000,\n+    classifier_activation=\"softmax\",\n+    **kwargs,\n+):\n+    return EfficientNet(\n+        2.0,\n+        3.1,\n+        600,\n+        0.5,\n+        model_name=\"efficientnetb7\",\n+        include_top=include_top,\n+        weights=weights,\n+        input_tensor=input_tensor,\n+        input_shape=input_shape,\n+        pooling=pooling,\n+        classes=classes,\n+        classifier_activation=classifier_activation,\n+        **kwargs,\n+    )\n+\n+\n+EfficientNetB0.__doc__ = BASE_DOCSTRING.format(name=\"EfficientNetB0\")\n+EfficientNetB1.__doc__ = BASE_DOCSTRING.format(name=\"EfficientNetB1\")\n+EfficientNetB2.__doc__ = BASE_DOCSTRING.format(name=\"EfficientNetB2\")\n+EfficientNetB3.__doc__ = BASE_DOCSTRING.format(name=\"EfficientNetB3\")\n+EfficientNetB4.__doc__ = BASE_DOCSTRING.format(name=\"EfficientNetB4\")\n+EfficientNetB5.__doc__ = BASE_DOCSTRING.format(name=\"EfficientNetB5\")\n+EfficientNetB6.__doc__ = BASE_DOCSTRING.format(name=\"EfficientNetB6\")\n+EfficientNetB7.__doc__ = BASE_DOCSTRING.format(name=\"EfficientNetB7\")\n+\n+\n+@keras_core_export(\"keras_core.applications.efficientnet.preprocess_input\")\n+def preprocess_input(x, data_format=None):\n+    \"\"\"A placeholder method for backward compatibility.\n+\n+    The preprocessing logic has been included in the efficientnet model\n+    implementation. Users are no longer required to call this method to\n+    normalize the input data. This method does nothing and only kept as a\n+    placeholder to align the API surface between old and new version of model.\n+\n+    Args:\n+        x: A floating point `numpy.array` or a tensor.\n+        data_format: Optional data format of the image tensor/array. `None`\n+            means the global setting `keras_core.backend.image_data_format()`\n+            is used (unless you changed it, it uses `\"channels_last\"`).\n+            Defaults to `None`.\n+\n+    Returns:\n+        Unchanged `numpy.array` or tensor.\n+    \"\"\"\n+    return x\n+\n+\n+@keras_core_export(\"keras_core.applications.efficientnet.decode_predictions\")\n+def decode_predictions(preds, top=5):\n+    return imagenet_utils.decode_predictions(preds, top=top)\n+\n+\n+decode_predictions.__doc__ = imagenet_utils.decode_predictions.__doc__\n\n@@ -0,0 +1,1346 @@\n+import copy\n+import math\n+\n+from tensorflow.io import gfile\n+\n+from keras_core import backend\n+from keras_core import initializers\n+from keras_core import layers\n+from keras_core.api_export import keras_core_export\n+from keras_core.applications import imagenet_utils\n+from keras_core.models import Functional\n+from keras_core.operations import operation_utils\n+from keras_core.utils import file_utils\n+\n+BASE_WEIGHTS_PATH = \"https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/\"  # noqa: E501\n+\n+WEIGHTS_HASHES = {\n+    \"b0\": (\n+        \"21ecbf6da12460d5c40bb2f29ceb2188\",\n+        \"893217f2bb855e2983157299931e43ff\",\n+    ),\n+    \"b1\": (\n+        \"069f0534ff22adf035c89e2d9547a9dc\",\n+        \"0e80663031ca32d657f9caa404b6ec37\",\n+    ),\n+    \"b2\": (\n+        \"424e49f28180edbde1e94797771950a7\",\n+        \"1dfe2e7a5d45b6632553a8961ea609eb\",\n+    ),\n+    \"b3\": (\n+        \"1f1fc43bd98a6e4fd8fdfd551e02c7a0\",\n+        \"f6abf7b5849ac99a89b50dd3fd532856\",\n+    ),\n+    \"-s\": (\n+        \"e1d88a8495beba45748fedd0cecbe016\",\n+        \"af0682fb74e8c54910f2d4393339c070\",\n+    ),\n+    \"-m\": (\n+        \"a3bf6aa3276309f4fc6a34aa114c95cd\",\n+        \"1b8dc055df72dde80d614482840fe342\",\n+    ),\n+    \"-l\": (\n+        \"27e6d408b53c7ebc868fefa357689935\",\n+        \"b0b66b5c863aef5b46e8608fe1711615\",\n+    ),\n+}\n+\n+DEFAULT_BLOCKS_ARGS = {\n+    \"efficientnetv2-s\": [\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 2,\n+            \"input_filters\": 24,\n+            \"output_filters\": 24,\n+            \"expand_ratio\": 1,\n+            \"se_ratio\": 0.0,\n+            \"strides\": 1,\n+            \"conv_type\": 1,\n+        },\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 4,\n+            \"input_filters\": 24,\n+            \"output_filters\": 48,\n+            \"expand_ratio\": 4,\n+            \"se_ratio\": 0.0,\n+            \"strides\": 2,\n+            \"conv_type\": 1,\n+        },\n+        {\n+            \"conv_type\": 1,\n+            \"expand_ratio\": 4,\n+            \"input_filters\": 48,\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 4,\n+            \"output_filters\": 64,\n+            \"se_ratio\": 0,\n+            \"strides\": 2,\n+        },\n+        {\n+            \"conv_type\": 0,\n+            \"expand_ratio\": 4,\n+            \"input_filters\": 64,\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 6,\n+            \"output_filters\": 128,\n+            \"se_ratio\": 0.25,\n+            \"strides\": 2,\n+        },\n+        {\n+            \"conv_type\": 0,\n+            \"expand_ratio\": 6,\n+            \"input_filters\": 128,\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 9,\n+            \"output_filters\": 160,\n+            \"se_ratio\": 0.25,\n+            \"strides\": 1,\n+        },\n+        {\n+            \"conv_type\": 0,\n+            \"expand_ratio\": 6,\n+            \"input_filters\": 160,\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 15,\n+            \"output_filters\": 256,\n+            \"se_ratio\": 0.25,\n+            \"strides\": 2,\n+        },\n+    ],\n+    \"efficientnetv2-m\": [\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 3,\n+            \"input_filters\": 24,\n+            \"output_filters\": 24,\n+            \"expand_ratio\": 1,\n+            \"se_ratio\": 0,\n+            \"strides\": 1,\n+            \"conv_type\": 1,\n+        },\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 5,\n+            \"input_filters\": 24,\n+            \"output_filters\": 48,\n+            \"expand_ratio\": 4,\n+            \"se_ratio\": 0,\n+            \"strides\": 2,\n+            \"conv_type\": 1,\n+        },\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 5,\n+            \"input_filters\": 48,\n+            \"output_filters\": 80,\n+            \"expand_ratio\": 4,\n+            \"se_ratio\": 0,\n+            \"strides\": 2,\n+            \"conv_type\": 1,\n+        },\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 7,\n+            \"input_filters\": 80,\n+            \"output_filters\": 160,\n+            \"expand_ratio\": 4,\n+            \"se_ratio\": 0.25,\n+            \"strides\": 2,\n+            \"conv_type\": 0,\n+        },\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 14,\n+            \"input_filters\": 160,\n+            \"output_filters\": 176,\n+            \"expand_ratio\": 6,\n+            \"se_ratio\": 0.25,\n+            \"strides\": 1,\n+            \"conv_type\": 0,\n+        },\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 18,\n+            \"input_filters\": 176,\n+            \"output_filters\": 304,\n+            \"expand_ratio\": 6,\n+            \"se_ratio\": 0.25,\n+            \"strides\": 2,\n+            \"conv_type\": 0,\n+        },\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 5,\n+            \"input_filters\": 304,\n+            \"output_filters\": 512,\n+            \"expand_ratio\": 6,\n+            \"se_ratio\": 0.25,\n+            \"strides\": 1,\n+            \"conv_type\": 0,\n+        },\n+    ],\n+    \"efficientnetv2-l\": [\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 4,\n+            \"input_filters\": 32,\n+            \"output_filters\": 32,\n+            \"expand_ratio\": 1,\n+            \"se_ratio\": 0,\n+            \"strides\": 1,\n+            \"conv_type\": 1,\n+        },\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 7,\n+            \"input_filters\": 32,\n+            \"output_filters\": 64,\n+            \"expand_ratio\": 4,\n+            \"se_ratio\": 0,\n+            \"strides\": 2,\n+            \"conv_type\": 1,\n+        },\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 7,\n+            \"input_filters\": 64,\n+            \"output_filters\": 96,\n+            \"expand_ratio\": 4,\n+            \"se_ratio\": 0,\n+            \"strides\": 2,\n+            \"conv_type\": 1,\n+        },\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 10,\n+            \"input_filters\": 96,\n+            \"output_filters\": 192,\n+            \"expand_ratio\": 4,\n+            \"se_ratio\": 0.25,\n+            \"strides\": 2,\n+            \"conv_type\": 0,\n+        },\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 19,\n+            \"input_filters\": 192,\n+            \"output_filters\": 224,\n+            \"expand_ratio\": 6,\n+            \"se_ratio\": 0.25,\n+            \"strides\": 1,\n+            \"conv_type\": 0,\n+        },\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 25,\n+            \"input_filters\": 224,\n+            \"output_filters\": 384,\n+            \"expand_ratio\": 6,\n+            \"se_ratio\": 0.25,\n+            \"strides\": 2,\n+            \"conv_type\": 0,\n+        },\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 7,\n+            \"input_filters\": 384,\n+            \"output_filters\": 640,\n+            \"expand_ratio\": 6,\n+            \"se_ratio\": 0.25,\n+            \"strides\": 1,\n+            \"conv_type\": 0,\n+        },\n+    ],\n+    \"efficientnetv2-b0\": [\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 1,\n+            \"input_filters\": 32,\n+            \"output_filters\": 16,\n+            \"expand_ratio\": 1,\n+            \"se_ratio\": 0,\n+            \"strides\": 1,\n+            \"conv_type\": 1,\n+        },\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 2,\n+            \"input_filters\": 16,\n+            \"output_filters\": 32,\n+            \"expand_ratio\": 4,\n+            \"se_ratio\": 0,\n+            \"strides\": 2,\n+            \"conv_type\": 1,\n+        },\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 2,\n+            \"input_filters\": 32,\n+            \"output_filters\": 48,\n+            \"expand_ratio\": 4,\n+            \"se_ratio\": 0,\n+            \"strides\": 2,\n+            \"conv_type\": 1,\n+        },\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 3,\n+            \"input_filters\": 48,\n+            \"output_filters\": 96,\n+            \"expand_ratio\": 4,\n+            \"se_ratio\": 0.25,\n+            \"strides\": 2,\n+            \"conv_type\": 0,\n+        },\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 5,\n+            \"input_filters\": 96,\n+            \"output_filters\": 112,\n+            \"expand_ratio\": 6,\n+            \"se_ratio\": 0.25,\n+            \"strides\": 1,\n+            \"conv_type\": 0,\n+        },\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 8,\n+            \"input_filters\": 112,\n+            \"output_filters\": 192,\n+            \"expand_ratio\": 6,\n+            \"se_ratio\": 0.25,\n+            \"strides\": 2,\n+            \"conv_type\": 0,\n+        },\n+    ],\n+    \"efficientnetv2-b1\": [\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 1,\n+            \"input_filters\": 32,\n+            \"output_filters\": 16,\n+            \"expand_ratio\": 1,\n+            \"se_ratio\": 0,\n+            \"strides\": 1,\n+            \"conv_type\": 1,\n+        },\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 2,\n+            \"input_filters\": 16,\n+            \"output_filters\": 32,\n+            \"expand_ratio\": 4,\n+            \"se_ratio\": 0,\n+            \"strides\": 2,\n+            \"conv_type\": 1,\n+        },\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 2,\n+            \"input_filters\": 32,\n+            \"output_filters\": 48,\n+            \"expand_ratio\": 4,\n+            \"se_ratio\": 0,\n+            \"strides\": 2,\n+            \"conv_type\": 1,\n+        },\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 3,\n+            \"input_filters\": 48,\n+            \"output_filters\": 96,\n+            \"expand_ratio\": 4,\n+            \"se_ratio\": 0.25,\n+            \"strides\": 2,\n+            \"conv_type\": 0,\n+        },\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 5,\n+            \"input_filters\": 96,\n+            \"output_filters\": 112,\n+            \"expand_ratio\": 6,\n+            \"se_ratio\": 0.25,\n+            \"strides\": 1,\n+            \"conv_type\": 0,\n+        },\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 8,\n+            \"input_filters\": 112,\n+            \"output_filters\": 192,\n+            \"expand_ratio\": 6,\n+            \"se_ratio\": 0.25,\n+            \"strides\": 2,\n+            \"conv_type\": 0,\n+        },\n+    ],\n+    \"efficientnetv2-b2\": [\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 1,\n+            \"input_filters\": 32,\n+            \"output_filters\": 16,\n+            \"expand_ratio\": 1,\n+            \"se_ratio\": 0,\n+            \"strides\": 1,\n+            \"conv_type\": 1,\n+        },\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 2,\n+            \"input_filters\": 16,\n+            \"output_filters\": 32,\n+            \"expand_ratio\": 4,\n+            \"se_ratio\": 0,\n+            \"strides\": 2,\n+            \"conv_type\": 1,\n+        },\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 2,\n+            \"input_filters\": 32,\n+            \"output_filters\": 48,\n+            \"expand_ratio\": 4,\n+            \"se_ratio\": 0,\n+            \"strides\": 2,\n+            \"conv_type\": 1,\n+        },\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 3,\n+            \"input_filters\": 48,\n+            \"output_filters\": 96,\n+            \"expand_ratio\": 4,\n+            \"se_ratio\": 0.25,\n+            \"strides\": 2,\n+            \"conv_type\": 0,\n+        },\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 5,\n+            \"input_filters\": 96,\n+            \"output_filters\": 112,\n+            \"expand_ratio\": 6,\n+            \"se_ratio\": 0.25,\n+            \"strides\": 1,\n+            \"conv_type\": 0,\n+        },\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 8,\n+            \"input_filters\": 112,\n+            \"output_filters\": 192,\n+            \"expand_ratio\": 6,\n+            \"se_ratio\": 0.25,\n+            \"strides\": 2,\n+            \"conv_type\": 0,\n+        },\n+    ],\n+    \"efficientnetv2-b3\": [\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 1,\n+            \"input_filters\": 32,\n+            \"output_filters\": 16,\n+            \"expand_ratio\": 1,\n+            \"se_ratio\": 0,\n+            \"strides\": 1,\n+            \"conv_type\": 1,\n+        },\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 2,\n+            \"input_filters\": 16,\n+            \"output_filters\": 32,\n+            \"expand_ratio\": 4,\n+            \"se_ratio\": 0,\n+            \"strides\": 2,\n+            \"conv_type\": 1,\n+        },\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 2,\n+            \"input_filters\": 32,\n+            \"output_filters\": 48,\n+            \"expand_ratio\": 4,\n+            \"se_ratio\": 0,\n+            \"strides\": 2,\n+            \"conv_type\": 1,\n+        },\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 3,\n+            \"input_filters\": 48,\n+            \"output_filters\": 96,\n+            \"expand_ratio\": 4,\n+            \"se_ratio\": 0.25,\n+            \"strides\": 2,\n+            \"conv_type\": 0,\n+        },\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 5,\n+            \"input_filters\": 96,\n+            \"output_filters\": 112,\n+            \"expand_ratio\": 6,\n+            \"se_ratio\": 0.25,\n+            \"strides\": 1,\n+            \"conv_type\": 0,\n+        },\n+        {\n+            \"kernel_size\": 3,\n+            \"num_repeat\": 8,\n+            \"input_filters\": 112,\n+            \"output_filters\": 192,\n+            \"expand_ratio\": 6,\n+            \"se_ratio\": 0.25,\n+            \"strides\": 2,\n+            \"conv_type\": 0,\n+        },\n+    ],\n+}\n+\n+CONV_KERNEL_INITIALIZER = {\n+    \"class_name\": \"VarianceScaling\",\n+    \"config\": {\n+        \"scale\": 2.0,\n+        \"mode\": \"fan_out\",\n+        \"distribution\": \"truncated_normal\",\n+    },\n+}\n+\n+DENSE_KERNEL_INITIALIZER = {\n+    \"class_name\": \"VarianceScaling\",\n+    \"config\": {\n+        \"scale\": 1.0 / 3.0,\n+        \"mode\": \"fan_out\",\n+        \"distribution\": \"uniform\",\n+    },\n+}\n+\n+BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n+\n+Reference:\n+- [EfficientNetV2: Smaller Models and Faster Training](\n+    https://arxiv.org/abs/2104.00298) (ICML 2021)\n+\n+This function returns a Keras image classification model,\n+optionally loaded with weights pre-trained on ImageNet.\n+\n+For image classification use cases, see\n+[this page for detailed examples](\n+https://keras.io/api/applications/#usage-examples-for-image-classification-models).\n+\n+For transfer learning use cases, make sure to read the\n+[guide to transfer learning & fine-tuning](\n+https://keras.io/guides/transfer_learning/).\n+\n+Note: each Keras Application expects a specific kind of input preprocessing.\n+For EfficientNetV2, by default input preprocessing is included as a part of\n+the model (as a `Rescaling` layer), and thus\n+`keras_core.applications.efficientnet_v2.preprocess_input` is actually a\n+pass-through function. In this use case, EfficientNetV2 models expect their\n+inputs to be float tensors of pixels with values in the `[0, 255]` range.\n+At the same time, preprocessing as a part of the model (i.e. `Rescaling`\n+layer) can be disabled by setting `include_preprocessing` argument to `False`.\n+With preprocessing disabled EfficientNetV2 models expect their inputs to be\n+float tensors of pixels with values in the `[-1, 1]` range.\n+\n+Args:\n+    include_top: Boolean, whether to include the fully-connected\n+        layer at the top of the network. Defaults to `True`.\n+    weights: One of `None` (random initialization),\n+        `\"imagenet\"` (pre-training on ImageNet),\n+        or the path to the weights file to be loaded. Defaults to `\"imagenet\"`.\n+    input_tensor: Optional Keras tensor\n+        (i.e. output of `layers.Input()`)\n+        to use as image input for the model.\n+    input_shape: Optional shape tuple, only to be specified\n+        if `include_top` is `False`.\n+        It should have exactly 3 inputs channels.\n+    pooling: Optional pooling mode for feature extraction\n+        when `include_top` is `False`. Defaults to None.\n+        - `None` means that the output of the model will be\n+            the 4D tensor output of the\n+            last convolutional layer.\n+        - `\"avg\"` means that global average pooling\n+            will be applied to the output of the\n+            last convolutional layer, and thus\n+            the output of the model will be a 2D tensor.\n+        - `\"max\"` means that global max pooling will\n+            be applied.\n+    classes: Optional number of classes to classify images\n+        into, only to be specified if `include_top` is `True`, and\n+        if no `weights` argument is specified. Defaults to 1000 (number of\n+        ImageNet classes).\n+    classifier_activation: A string or callable. The activation function to use\n+        on the \"top\" layer. Ignored unless `include_top=True`. Set\n+        `classifier_activation=None` to return the logits of the \"top\" layer.\n+        Defaults to `\"softmax\"`.\n+        When loading pretrained weights, `classifier_activation` can only\n+        be `None` or `\"softmax\"`.\n+\n+Returns:\n+    A model instance.\n+\"\"\"\n+\n+\n+def round_filters(filters, width_coefficient, min_depth, depth_divisor):\n+    \"\"\"Round number of filters based on depth multiplier.\"\"\"\n+    filters *= width_coefficient\n+    minimum_depth = min_depth or depth_divisor\n+    new_filters = max(\n+        minimum_depth,\n+        int(filters + depth_divisor / 2) // depth_divisor * depth_divisor,\n+    )\n+    return int(new_filters)\n+\n+\n+def round_repeats(repeats, depth_coefficient):\n+    \"\"\"Round number of repeats based on depth multiplier.\"\"\"\n+    return int(math.ceil(depth_coefficient * repeats))\n+\n+\n+def MBConvBlock(\n+    input_filters,\n+    output_filters,\n+    expand_ratio=1,\n+    kernel_size=3,\n+    strides=1,\n+    se_ratio=0.0,\n+    bn_momentum=0.9,\n+    activation=\"swish\",\n+    survival_probability=0.8,\n+    name=None,\n+):\n+    \"\"\"MBConv block: Mobile Inverted Residual Bottleneck.\"\"\"\n+    bn_axis = 3 if backend.image_data_format() == \"channels_last\" else 1\n+\n+    if name is None:\n+        name = backend.get_uid(\"block0\")\n+\n+    def apply(inputs):\n+        # Expansion phase\n+        filters = input_filters * expand_ratio\n+        if expand_ratio != 1:\n+            x = layers.Conv2D(\n+                filters=filters,\n+                kernel_size=1,\n+                strides=1,\n+                kernel_initializer=CONV_KERNEL_INITIALIZER,\n+                padding=\"same\",\n+                data_format=\"channels_last\",\n+                use_bias=False,\n+                name=name + \"expand_conv\",\n+            )(inputs)\n+            x = layers.BatchNormalization(\n+                axis=bn_axis,\n+                momentum=bn_momentum,\n+                name=name + \"expand_bn\",\n+            )(x)\n+            x = layers.Activation(activation, name=name + \"expand_activation\")(\n+                x\n+            )\n+        else:\n+            x = inputs\n+\n+        # Depthwise conv\n+        x = layers.DepthwiseConv2D(\n+            kernel_size=kernel_size,\n+            strides=strides,\n+            depthwise_initializer=CONV_KERNEL_INITIALIZER,\n+            padding=\"same\",\n+            data_format=\"channels_last\",\n+            use_bias=False,\n+            name=name + \"dwconv2\",\n+        )(x)\n+        x = layers.BatchNormalization(\n+            axis=bn_axis, momentum=bn_momentum, name=name + \"bn\"\n+        )(x)\n+        x = layers.Activation(activation, name=name + \"activation\")(x)\n+\n+        # Squeeze and excite\n+        if 0 < se_ratio <= 1:\n+            filters_se = max(1, int(input_filters * se_ratio))\n+            se = layers.GlobalAveragePooling2D(name=name + \"se_squeeze\")(x)\n+            if bn_axis == 1:\n+                se_shape = (filters, 1, 1)\n+            else:\n+                se_shape = (1, 1, filters)\n+            se = layers.Reshape(se_shape, name=name + \"se_reshape\")(se)\n+\n+            se = layers.Conv2D(\n+                filters_se,\n+                1,\n+                padding=\"same\",\n+                activation=activation,\n+                kernel_initializer=CONV_KERNEL_INITIALIZER,\n+                name=name + \"se_reduce\",\n+            )(se)\n+            se = layers.Conv2D(\n+                filters,\n+                1,\n+                padding=\"same\",\n+                activation=\"sigmoid\",\n+                kernel_initializer=CONV_KERNEL_INITIALIZER,\n+                name=name + \"se_expand\",\n+            )(se)\n+\n+            x = layers.multiply([x, se], name=name + \"se_excite\")\n+\n+        # Output phase\n+        x = layers.Conv2D(\n+            filters=output_filters,\n+            kernel_size=1,\n+            strides=1,\n+            kernel_initializer=CONV_KERNEL_INITIALIZER,\n+            padding=\"same\",\n+            data_format=\"channels_last\",\n+            use_bias=False,\n+            name=name + \"project_conv\",\n+        )(x)\n+        x = layers.BatchNormalization(\n+            axis=bn_axis, momentum=bn_momentum, name=name + \"project_bn\"\n+        )(x)\n+\n+        if strides == 1 and input_filters == output_filters:\n+            if survival_probability:\n+                x = layers.Dropout(\n+                    survival_probability,\n+                    noise_shape=(None, 1, 1, 1),\n+                    name=name + \"drop\",\n+                )(x)\n+            x = layers.add([x, inputs], name=name + \"add\")\n+\n+        return x\n+\n+    return apply\n+\n+\n+def FusedMBConvBlock(\n+    input_filters,\n+    output_filters,\n+    expand_ratio=1,\n+    kernel_size=3,\n+    strides=1,\n+    se_ratio=0.0,\n+    bn_momentum=0.9,\n+    activation=\"swish\",\n+    survival_probability=0.8,\n+    name=None,\n+):\n+    \"\"\"Fuses the proj conv1x1 and depthwise_conv into a conv2d.\"\"\"\n+    bn_axis = 3 if backend.image_data_format() == \"channels_last\" else 1\n+\n+    if name is None:\n+        name = backend.get_uid(\"block0\")\n+\n+    def apply(inputs):\n+        filters = input_filters * expand_ratio\n+        if expand_ratio != 1:\n+            x = layers.Conv2D(\n+                filters,\n+                kernel_size=kernel_size,\n+                strides=strides,\n+                kernel_initializer=CONV_KERNEL_INITIALIZER,\n+                data_format=\"channels_last\",\n+                padding=\"same\",\n+                use_bias=False,\n+                name=name + \"expand_conv\",\n+            )(inputs)\n+            x = layers.BatchNormalization(\n+                axis=bn_axis, momentum=bn_momentum, name=name + \"expand_bn\"\n+            )(x)\n+            x = layers.Activation(\n+                activation=activation, name=name + \"expand_activation\"\n+            )(x)\n+        else:\n+            x = inputs\n+\n+        # Squeeze and excite\n+        if 0 < se_ratio <= 1:\n+            filters_se = max(1, int(input_filters * se_ratio))\n+            se = layers.GlobalAveragePooling2D(name=name + \"se_squeeze\")(x)\n+            if bn_axis == 1:\n+                se_shape = (filters, 1, 1)\n+            else:\n+                se_shape = (1, 1, filters)\n+\n+            se = layers.Reshape(se_shape, name=name + \"se_reshape\")(se)\n+\n+            se = layers.Conv2D(\n+                filters_se,\n+                1,\n+                padding=\"same\",\n+                activation=activation,\n+                kernel_initializer=CONV_KERNEL_INITIALIZER,\n+                name=name + \"se_reduce\",\n+            )(se)\n+            se = layers.Conv2D(\n+                filters,\n+                1,\n+                padding=\"same\",\n+                activation=\"sigmoid\",\n+                kernel_initializer=CONV_KERNEL_INITIALIZER,\n+                name=name + \"se_expand\",\n+            )(se)\n+\n+            x = layers.multiply([x, se], name=name + \"se_excite\")\n+\n+        # Output phase:\n+        x = layers.Conv2D(\n+            output_filters,\n+            kernel_size=1 if expand_ratio != 1 else kernel_size,\n+            strides=1 if expand_ratio != 1 else strides,\n+            kernel_initializer=CONV_KERNEL_INITIALIZER,\n+            padding=\"same\",\n+            use_bias=False,\n+            name=name + \"project_conv\",\n+        )(x)\n+        x = layers.BatchNormalization(\n+            axis=bn_axis, momentum=bn_momentum, name=name + \"project_bn\"\n+        )(x)\n+        if expand_ratio == 1:\n+            x = layers.Activation(\n+                activation=activation, name=name + \"project_activation\"\n+            )(x)\n+\n+        # Residual:\n+        if strides == 1 and input_filters == output_filters:\n+            if survival_probability:\n+                x = layers.Dropout(\n+                    survival_probability,\n+                    noise_shape=(None, 1, 1, 1),\n+                    name=name + \"drop\",\n+                )(x)\n+            x = layers.add([x, inputs], name=name + \"add\")\n+        return x\n+\n+    return apply\n+\n+\n+def EfficientNetV2(\n+    width_coefficient,\n+    depth_coefficient,\n+    default_size,\n+    dropout_rate=0.2,\n+    drop_connect_rate=0.2,\n+    depth_divisor=8,\n+    min_depth=8,\n+    bn_momentum=0.9,\n+    activation=\"swish\",\n+    blocks_args=\"default\",\n+    model_name=\"efficientnetv2\",\n+    include_top=True,\n+    weights=\"imagenet\",\n+    input_tensor=None,\n+    input_shape=None,\n+    pooling=None,\n+    classes=1000,\n+    classifier_activation=\"softmax\",\n+    include_preprocessing=True,\n+):\n+    \"\"\"Instantiates the EfficientNetV2 architecture using given scaling\n+    coefficients.\n+\n+    Args:\n+        width_coefficient: float, scaling coefficient for network width.\n+        depth_coefficient: float, scaling coefficient for network depth.\n+        default_size: integer, default input image size.\n+        dropout_rate: float, dropout rate before final classifier layer.\n+        drop_connect_rate: float, dropout rate at skip connections.\n+        depth_divisor: integer, a unit of network width.\n+        min_depth: integer, minimum number of filters.\n+        bn_momentum: float. Momentum parameter for Batch Normalization layers.\n+        activation: activation function.\n+        blocks_args: list of dicts, parameters to construct block modules.\n+        model_name: string, model name.\n+        include_top: whether to include the fully-connected layer at the top of\n+            the network.\n+        weights: one of `None` (random initialization), `\"imagenet\"`\n+            (pre-training on ImageNet),\n+            or the path to the weights file to be loaded.\n+        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`) or\n+            numpy array to use as image input for the model.\n+        input_shape: optional shape tuple, only to be specified if `include_top`\n+            is `False`. It should have exactly 3 inputs channels.\n+        pooling: optional pooling mode for feature extraction when `include_top`\n+            is `False`.\n+            - `None` means that the output of the model will be the\n+                4D tensor output of the last convolutional layer.\n+            - \"avg\" means that global average pooling will be applied to\n+                the output of the last convolutional layer,\n+                and thus the output of the model will be a 2D tensor.\n+            - `\"max\"` means that global max pooling will be applied.\n+        classes: optional number of classes to classify images into,\n+            only to be specified if `include_top` is `True`, and if no `weights`\n+            argument is specified.\n+        classifier_activation: A string or callable. The activation function to\n+            use on the \"top\" layer. Ignored unless `include_top=True`. Set\n+            `classifier_activation=None` to return the logits of the \"top\"\n+            layer.\n+        include_preprocessing: Boolean, whether to include the preprocessing\n+            layer (`Rescaling`) at the bottom of the network.\n+            Defaults to `True`.\n+\n+    Returns:\n+        A model instance.\n+    \"\"\"\n+\n+    if blocks_args == \"default\":\n+        blocks_args = DEFAULT_BLOCKS_ARGS[model_name]\n+\n+    if not (weights in {\"imagenet\", None} or gfile.exists(weights)):\n+        raise ValueError(\n+            \"The `weights` argument should be either \"\n+            \"`None` (random initialization), `imagenet` \"\n+            \"(pre-training on ImageNet), \"\n+            \"or the path to the weights file to be loaded.\"\n+            f\"Received: weights={weights}\"\n+        )\n+\n+    if weights == \"imagenet\" and include_top and classes != 1000:\n+        raise ValueError(\n+            'If using `weights=\"imagenet\"` with `include_top`'\n+            \" as true, `classes` should be 1000\"\n+        )\n+\n+    # Determine proper input shape\n+    input_shape = imagenet_utils.obtain_input_shape(\n+        input_shape,\n+        default_size=default_size,\n+        min_size=32,\n+        data_format=backend.image_data_format(),\n+        require_flatten=include_top,\n+        weights=weights,\n+    )\n+\n+    if input_tensor is None:\n+        img_input = layers.Input(shape=input_shape)\n+    else:\n+        if not backend.is_keras_tensor(input_tensor):\n+            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n+        else:\n+            img_input = input_tensor\n+\n+    bn_axis = 3 if backend.image_data_format() == \"channels_last\" else 1\n+\n+    x = img_input\n+\n+    if include_preprocessing:\n+        # Apply original V1 preprocessing for Bx variants\n+        # if number of channels allows it\n+        num_channels = input_shape[bn_axis - 1]\n+        if model_name.split(\"-\")[-1].startswith(\"b\") and num_channels == 3:\n+            x = layers.Rescaling(scale=1.0 / 255)(x)\n+            x = layers.Normalization(\n+                mean=[0.485, 0.456, 0.406],\n+                variance=[0.229**2, 0.224**2, 0.225**2],\n+                axis=bn_axis,\n+            )(x)\n+        else:\n+            x = layers.Rescaling(scale=1.0 / 128.0, offset=-1)(x)\n+\n+    # Build stem\n+    stem_filters = round_filters(\n+        filters=blocks_args[0][\"input_filters\"],\n+        width_coefficient=width_coefficient,\n+        min_depth=min_depth,\n+        depth_divisor=depth_divisor,\n+    )\n+    x = layers.Conv2D(\n+        filters=stem_filters,\n+        kernel_size=3,\n+        strides=2,\n+        kernel_initializer=CONV_KERNEL_INITIALIZER,\n+        padding=\"same\",\n+        use_bias=False,\n+        name=\"stem_conv\",\n+    )(x)\n+    x = layers.BatchNormalization(\n+        axis=bn_axis,\n+        momentum=bn_momentum,\n+        name=\"stem_bn\",\n+    )(x)\n+    x = layers.Activation(activation, name=\"stem_activation\")(x)\n+\n+    # Build blocks\n+    blocks_args = copy.deepcopy(blocks_args)\n+    b = 0\n+    blocks = float(sum(args[\"num_repeat\"] for args in blocks_args))\n+\n+    for i, args in enumerate(blocks_args):\n+        assert args[\"num_repeat\"] > 0\n+\n+        # Update block input and output filters based on depth multiplier.\n+        args[\"input_filters\"] = round_filters(\n+            filters=args[\"input_filters\"],\n+            width_coefficient=width_coefficient,\n+            min_depth=min_depth,\n+            depth_divisor=depth_divisor,\n+        )\n+        args[\"output_filters\"] = round_filters(\n+            filters=args[\"output_filters\"],\n+            width_coefficient=width_coefficient,\n+            min_depth=min_depth,\n+            depth_divisor=depth_divisor,\n+        )\n+\n+        # Determine which conv type to use:\n+        block = {0: MBConvBlock, 1: FusedMBConvBlock}[args.pop(\"conv_type\")]\n+        repeats = round_repeats(\n+            repeats=args.pop(\"num_repeat\"), depth_coefficient=depth_coefficient\n+        )\n+        for j in range(repeats):\n+            # The first block needs to take care of stride and filter size\n+            # increase.\n+            if j > 0:\n+                args[\"strides\"] = 1\n+                args[\"input_filters\"] = args[\"output_filters\"]\n+\n+            x = block(\n+                activation=activation,\n+                bn_momentum=bn_momentum,\n+                survival_probability=drop_connect_rate * b / blocks,\n+                name=f\"block{i + 1}{chr(j + 97)}_\",\n+                **args,\n+            )(x)\n+            b += 1\n+\n+    # Build top\n+    top_filters = round_filters(\n+        filters=1280,\n+        width_coefficient=width_coefficient,\n+        min_depth=min_depth,\n+        depth_divisor=depth_divisor,\n+    )\n+    x = layers.Conv2D(\n+        filters=top_filters,\n+        kernel_size=1,\n+        strides=1,\n+        kernel_initializer=CONV_KERNEL_INITIALIZER,\n+        padding=\"same\",\n+        data_format=\"channels_last\",\n+        use_bias=False,\n+        name=\"top_conv\",\n+    )(x)\n+    x = layers.BatchNormalization(\n+        axis=bn_axis,\n+        momentum=bn_momentum,\n+        name=\"top_bn\",\n+    )(x)\n+    x = layers.Activation(activation=activation, name=\"top_activation\")(x)\n+\n+    if include_top:\n+        x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(x)\n+        if dropout_rate > 0:\n+            x = layers.Dropout(dropout_rate, name=\"top_dropout\")(x)\n+        imagenet_utils.validate_activation(classifier_activation, weights)\n+        x = layers.Dense(\n+            classes,\n+            activation=classifier_activation,\n+            kernel_initializer=DENSE_KERNEL_INITIALIZER,\n+            bias_initializer=initializers.Constant(0.0),\n+            name=\"predictions\",\n+        )(x)\n+    else:\n+        if pooling == \"avg\":\n+            x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(x)\n+        elif pooling == \"max\":\n+            x = layers.GlobalMaxPooling2D(name=\"max_pool\")(x)\n+\n+    # Ensure that the model takes into account\n+    # any potential predecessors of `input_tensor`.\n+    if input_tensor is not None:\n+        inputs = operation_utils.get_source_inputs(input_tensor)\n+    else:\n+        inputs = img_input\n+\n+    # Create model.\n+    model = Functional(inputs, x, name=model_name)\n+\n+    # Load weights.\n+    if weights == \"imagenet\":\n+        if include_top:\n+            file_suffix = \".h5\"\n+            file_hash = WEIGHTS_HASHES[model_name[-2:]][0]\n+        else:\n+            file_suffix = \"_notop.h5\"\n+            file_hash = WEIGHTS_HASHES[model_name[-2:]][1]\n+        file_name = model_name + file_suffix\n+        weights_path = file_utils.get_file(\n+            file_name,\n+            BASE_WEIGHTS_PATH + file_name,\n+            cache_subdir=\"models\",\n+            file_hash=file_hash,\n+        )\n+        model.load_weights(weights_path)\n+    elif weights is not None:\n+        model.load_weights(weights)\n+\n+    return model\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.applications.efficientnet_v2.EfficientNetV2B0\",\n+        \"keras_core.applications.EfficientNetV2B0\",\n+    ]\n+)\n+def EfficientNetV2B0(\n+    include_top=True,\n+    weights=\"imagenet\",\n+    input_tensor=None,\n+    input_shape=None,\n+    pooling=None,\n+    classes=1000,\n+    classifier_activation=\"softmax\",\n+    include_preprocessing=True,\n+):\n+    return EfficientNetV2(\n+        width_coefficient=1.0,\n+        depth_coefficient=1.0,\n+        default_size=224,\n+        model_name=\"efficientnetv2-b0\",\n+        include_top=include_top,\n+        weights=weights,\n+        input_tensor=input_tensor,\n+        input_shape=input_shape,\n+        pooling=pooling,\n+        classes=classes,\n+        classifier_activation=classifier_activation,\n+        include_preprocessing=include_preprocessing,\n+    )\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.applications.efficientnet_v2.EfficientNetV2B1\",\n+        \"keras_core.applications.EfficientNetV2B1\",\n+    ]\n+)\n+def EfficientNetV2B1(\n+    include_top=True,\n+    weights=\"imagenet\",\n+    input_tensor=None,\n+    input_shape=None,\n+    pooling=None,\n+    classes=1000,\n+    classifier_activation=\"softmax\",\n+    include_preprocessing=True,\n+):\n+    return EfficientNetV2(\n+        width_coefficient=1.0,\n+        depth_coefficient=1.1,\n+        default_size=240,\n+        model_name=\"efficientnetv2-b1\",\n+        include_top=include_top,\n+        weights=weights,\n+        input_tensor=input_tensor,\n+        input_shape=input_shape,\n+        pooling=pooling,\n+        classes=classes,\n+        classifier_activation=classifier_activation,\n+        include_preprocessing=include_preprocessing,\n+    )\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.applications.efficientnet_v2.EfficientNetV2B2\",\n+        \"keras_core.applications.EfficientNetV2B2\",\n+    ]\n+)\n+def EfficientNetV2B2(\n+    include_top=True,\n+    weights=\"imagenet\",\n+    input_tensor=None,\n+    input_shape=None,\n+    pooling=None,\n+    classes=1000,\n+    classifier_activation=\"softmax\",\n+    include_preprocessing=True,\n+):\n+    return EfficientNetV2(\n+        width_coefficient=1.1,\n+        depth_coefficient=1.2,\n+        default_size=260,\n+        model_name=\"efficientnetv2-b2\",\n+        include_top=include_top,\n+        weights=weights,\n+        input_tensor=input_tensor,\n+        input_shape=input_shape,\n+        pooling=pooling,\n+        classes=classes,\n+        classifier_activation=classifier_activation,\n+        include_preprocessing=include_preprocessing,\n+    )\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.applications.efficientnet_v2.EfficientNetV2B3\",\n+        \"keras_core.applications.EfficientNetV2B3\",\n+    ]\n+)\n+def EfficientNetV2B3(\n+    include_top=True,\n+    weights=\"imagenet\",\n+    input_tensor=None,\n+    input_shape=None,\n+    pooling=None,\n+    classes=1000,\n+    classifier_activation=\"softmax\",\n+    include_preprocessing=True,\n+):\n+    return EfficientNetV2(\n+        width_coefficient=1.2,\n+        depth_coefficient=1.4,\n+        default_size=300,\n+        model_name=\"efficientnetv2-b3\",\n+        include_top=include_top,\n+        weights=weights,\n+        input_tensor=input_tensor,\n+        input_shape=input_shape,\n+        pooling=pooling,\n+        classes=classes,\n+        classifier_activation=classifier_activation,\n+        include_preprocessing=include_preprocessing,\n+    )\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.applications.efficientnet_v2.EfficientNetV2S\",\n+        \"keras_core.applications.EfficientNetV2S\",\n+    ]\n+)\n+def EfficientNetV2S(\n+    include_top=True,\n+    weights=\"imagenet\",\n+    input_tensor=None,\n+    input_shape=None,\n+    pooling=None,\n+    classes=1000,\n+    classifier_activation=\"softmax\",\n+    include_preprocessing=True,\n+):\n+    return EfficientNetV2(\n+        width_coefficient=1.0,\n+        depth_coefficient=1.0,\n+        default_size=384,\n+        model_name=\"efficientnetv2-s\",\n+        include_top=include_top,\n+        weights=weights,\n+        input_tensor=input_tensor,\n+        input_shape=input_shape,\n+        pooling=pooling,\n+        classes=classes,\n+        classifier_activation=classifier_activation,\n+        include_preprocessing=include_preprocessing,\n+    )\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.applications.efficientnet_v2.EfficientNetV2M\",\n+        \"keras_core.applications.EfficientNetV2M\",\n+    ]\n+)\n+def EfficientNetV2M(\n+    include_top=True,\n+    weights=\"imagenet\",\n+    input_tensor=None,\n+    input_shape=None,\n+    pooling=None,\n+    classes=1000,\n+    classifier_activation=\"softmax\",\n+    include_preprocessing=True,\n+):\n+    return EfficientNetV2(\n+        width_coefficient=1.0,\n+        depth_coefficient=1.0,\n+        default_size=480,\n+        model_name=\"efficientnetv2-m\",\n+        include_top=include_top,\n+        weights=weights,\n+        input_tensor=input_tensor,\n+        input_shape=input_shape,\n+        pooling=pooling,\n+        classes=classes,\n+        classifier_activation=classifier_activation,\n+        include_preprocessing=include_preprocessing,\n+    )\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.applications.efficientnet_v2.EfficientNetV2L\",\n+        \"keras_core.applications.EfficientNetV2L\",\n+    ]\n+)\n+def EfficientNetV2L(\n+    include_top=True,\n+    weights=\"imagenet\",\n+    input_tensor=None,\n+    input_shape=None,\n+    pooling=None,\n+    classes=1000,\n+    classifier_activation=\"softmax\",\n+    include_preprocessing=True,\n+):\n+    return EfficientNetV2(\n+        width_coefficient=1.0,\n+        depth_coefficient=1.0,\n+        default_size=480,\n+        model_name=\"efficientnetv2-l\",\n+        include_top=include_top,\n+        weights=weights,\n+        input_tensor=input_tensor,\n+        input_shape=input_shape,\n+        pooling=pooling,\n+        classes=classes,\n+        classifier_activation=classifier_activation,\n+        include_preprocessing=include_preprocessing,\n+    )\n+\n+\n+EfficientNetV2B0.__doc__ = BASE_DOCSTRING.format(name=\"EfficientNetV2B0\")\n+EfficientNetV2B1.__doc__ = BASE_DOCSTRING.format(name=\"EfficientNetV2B1\")\n+EfficientNetV2B2.__doc__ = BASE_DOCSTRING.format(name=\"EfficientNetV2B2\")\n+EfficientNetV2B3.__doc__ = BASE_DOCSTRING.format(name=\"EfficientNetV2B3\")\n+EfficientNetV2S.__doc__ = BASE_DOCSTRING.format(name=\"EfficientNetV2S\")\n+EfficientNetV2M.__doc__ = BASE_DOCSTRING.format(name=\"EfficientNetV2M\")\n+EfficientNetV2L.__doc__ = BASE_DOCSTRING.format(name=\"EfficientNetV2L\")\n+\n+\n+@keras_core_export(\"keras_core.applications.efficientnet_v2.preprocess_input\")\n+def preprocess_input(x, data_format=None):\n+    \"\"\"A placeholder method for backward compatibility.\n+\n+    The preprocessing logic has been included in the EfficientNetV2 model\n+    implementation. Users are no longer required to call this method to\n+    normalize the input data. This method does nothing and only kept as a\n+    placeholder to align the API surface between old and new version of model.\n+\n+    Args:\n+        x: A floating point `numpy.array` or a tensor.\n+        data_format: Optional data format of the image tensor/array. Defaults to\n+            None, in which case the global setting\n+            `keras_core.backend.image_data_format()` is used\n+            (unless you changed it, it defaults to \"channels_last\").{mode}\n+\n+    Returns:\n+        Unchanged `numpy.array` or tensor.\n+    \"\"\"\n+    return x\n+\n+\n+@keras_core_export(\"keras_core.applications.efficientnet_v2.decode_predictions\")\n+def decode_predictions(preds, top=5):\n+    return imagenet_utils.decode_predictions(preds, top=top)\n+\n+\n+decode_predictions.__doc__ = imagenet_utils.decode_predictions.__doc__\n\n@@ -120,7 +120,7 @@ def MobileNetV2(\n \n     if weights == \"imagenet\" and include_top and classes != 1000:\n         raise ValueError(\n-            'If using `weights` as `\"imagenet\"` with `include_top` '\n+            'If using `weights=\"imagenet\"` with `include_top` '\n             f\"as true, `classes` should be 1000. Received `classes={classes}`\"\n         )\n \n\n@@ -109,7 +109,7 @@ Args:\n     include_top: Boolean, whether to include the fully-connected\n         layer at the top of the network. Defaults to `True`.\n     weights: String, one of `None` (random initialization),\n-        'imagenet' (pre-training on ImageNet),\n+        `\"imagenet\"` (pre-training on ImageNet),\n         or the path to the weights file to be loaded.\n     input_tensor: Optional Keras tensor (i.e. output of\n         `layers.Input()`)\n@@ -127,7 +127,7 @@ Args:\n         - `max` means that global max pooling will\n             be applied.\n     classes: Integer, optional number of classes to classify images\n-        into, only to be specified if `include_top` is True, and\n+        into, only to be specified if `include_top` is `True`, and\n         if no `weights` argument is specified.\n     dropout_rate: fraction of the input units to drop on the last layer.\n     classifier_activation: A `str` or callable. The activation function to use\n@@ -176,7 +176,7 @@ def MobileNetV3(\n \n     if weights == \"imagenet\" and include_top and classes != 1000:\n         raise ValueError(\n-            'If using `weights` as `\"imagenet\"` with `include_top` '\n+            'If using `weights=\"imagenet\"` with `include_top` '\n             \"as true, `classes` should be 1000.  \"\n             f\"Received classes={classes}\"\n         )\n\n@@ -111,7 +111,6 @@ from keras_core.layers.reshaping.reshape import Reshape\n from keras_core.layers.reshaping.up_sampling1d import UpSampling1D\n from keras_core.layers.reshaping.up_sampling2d import UpSampling2D\n from keras_core.layers.reshaping.up_sampling3d import UpSampling3D\n-from keras_core.layers.reshaping.zero_padding1d import ZeroPadding1D\n from keras_core.layers.reshaping.zero_padding2d import ZeroPadding2D\n from keras_core.layers.reshaping.zero_padding3d import ZeroPadding3D\n from keras_core.layers.rnn.bidirectional import Bidirectional\n\n@@ -63,14 +63,15 @@ class BaseDepthwiseConv(Layer):\n             specified, the same dilation rate will be used for all dimensions.\n         activation: Activation function. If `None`, no activation is applied.\n         use_bias: bool, if `True`, bias will be added to the output.\n-        kernel_initializer: Initializer for the convolution kernel. If `None`,\n-            the default initializer (`\"glorot_uniform\"`) will be used.\n+        depthwise_initializer: Initializer for the depthwsie convolution\n+            kernel. If `None`, the default initializer (`\"glorot_uniform\"`)\n+            will be used.\n         bias_initializer: Initializer for the bias vector. If `None`, the\n             default initializer (`\"zeros\"`) will be used.\n-        kernel_regularizer: Optional regularizer for the convolution kernel.\n+        depthwise_regularizer: Optional regularizer for the convolution kernel.\n         bias_regularizer: Optional regularizer for the bias vector.\n         activity_regularizer: Optional regularizer function for the output.\n-        kernel_constraint: Optional projection function to be applied to the\n+        depthwise_constraint: Optional projection function to be applied to the\n             kernel after being updated by an `Optimizer` (e.g. used to implement\n             norm constraints or value constraints for layer weights). The\n             function must take as input the unprojected variable and must return\n@@ -91,12 +92,12 @@ class BaseDepthwiseConv(Layer):\n         dilation_rate=1,\n         activation=None,\n         use_bias=True,\n-        kernel_initializer=\"glorot_uniform\",\n+        depthwise_initializer=\"glorot_uniform\",\n         bias_initializer=\"zeros\",\n-        kernel_regularizer=None,\n+        depthwise_regularizer=None,\n         bias_regularizer=None,\n         activity_regularizer=None,\n-        kernel_constraint=None,\n+        depthwise_constraint=None,\n         bias_constraint=None,\n         trainable=True,\n         name=None,\n@@ -119,11 +120,11 @@ class BaseDepthwiseConv(Layer):\n         self.data_format = standardize_data_format(data_format)\n         self.activation = activations.get(activation)\n         self.use_bias = use_bias\n-        self.kernel_initializer = initializers.get(kernel_initializer)\n+        self.depthwise_initializer = initializers.get(depthwise_initializer)\n         self.bias_initializer = initializers.get(bias_initializer)\n-        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n+        self.depthwise_regularizer = regularizers.get(depthwise_regularizer)\n         self.bias_regularizer = regularizers.get(bias_regularizer)\n-        self.kernel_constraint = constraints.get(kernel_constraint)\n+        self.depthwise_constraint = constraints.get(depthwise_constraint)\n         self.bias_constraint = constraints.get(bias_constraint)\n         self.input_spec = InputSpec(min_ndim=self.rank + 2)\n         self.data_format = self.data_format\n@@ -164,16 +165,16 @@ class BaseDepthwiseConv(Layer):\n         self.input_spec = InputSpec(\n             min_ndim=self.rank + 2, axes={channel_axis: input_channel}\n         )\n-        kernel_shape = self.kernel_size + (\n+        depthwise_shape = self.kernel_size + (\n             input_channel,\n             self.depth_multiplier,\n         )\n         self.kernel = self.add_weight(\n             name=\"kernel\",\n-            shape=kernel_shape,\n-            initializer=self.kernel_initializer,\n-            regularizer=self.kernel_regularizer,\n-            constraint=self.kernel_constraint,\n+            shape=depthwise_shape,\n+            initializer=self.depthwise_initializer,\n+            regularizer=self.depthwise_regularizer,\n+            constraint=self.depthwise_constraint,\n             trainable=True,\n             dtype=self.dtype,\n         )\n@@ -249,14 +250,14 @@ class BaseDepthwiseConv(Layer):\n                 \"dilation_rate\": self.dilation_rate,\n                 \"activation\": activations.serialize(self.activation),\n                 \"use_bias\": self.use_bias,\n-                \"kernel_initializer\": initializers.serialize(\n-                    self.kernel_initializer\n+                \"depthwise_initializer\": initializers.serialize(\n+                    self.depthwise_initializer\n                 ),\n                 \"bias_initializer\": initializers.serialize(\n                     self.bias_initializer\n                 ),\n-                \"kernel_regularizer\": regularizers.serialize(\n-                    self.kernel_regularizer\n+                \"depthwise_regularizer\": regularizers.serialize(\n+                    self.depthwise_regularizer\n                 ),\n                 \"bias_regularizer\": regularizers.serialize(\n                     self.bias_regularizer\n@@ -264,8 +265,8 @@ class BaseDepthwiseConv(Layer):\n                 \"activity_regularizer\": regularizers.serialize(\n                     self.activity_regularizer\n                 ),\n-                \"kernel_constraint\": constraints.serialize(\n-                    self.kernel_constraint\n+                \"depthwise_constraint\": constraints.serialize(\n+                    self.depthwise_constraint\n                 ),\n                 \"bias_constraint\": constraints.serialize(self.bias_constraint),\n             }\n\n@@ -51,14 +51,15 @@ class DepthwiseConv1D(BaseDepthwiseConv):\n             rate to use for dilated convolution.\n         activation: Activation function. If `None`, no activation is applied.\n         use_bias: bool, if `True`, bias will be added to the output.\n-        kernel_initializer: Initializer for the convolution kernel. If `None`,\n-            the default initializer (`\"glorot_uniform\"`) will be used.\n+        depthwise_initializer: Initializer for the convolution kernel.\n+            If `None`, the default initializer (`\"glorot_uniform\"`)\n+            will be used.\n         bias_initializer: Initializer for the bias vector. If `None`, the\n             default initializer (`\"zeros\"`) will be used.\n-        kernel_regularizer: Optional regularizer for the convolution kernel.\n+        depthwise_regularizer: Optional regularizer for the convolution kernel.\n         bias_regularizer: Optional regularizer for the bias vector.\n         activity_regularizer: Optional regularizer function for the output.\n-        kernel_constraint: Optional projection function to be applied to the\n+        depthwise_constraint: Optional projection function to be applied to the\n             kernel after being updated by an `Optimizer` (e.g. used to implement\n             norm constraints or value constraints for layer weights). The\n             function must take as input the unprojected variable and must return\n@@ -106,12 +107,12 @@ class DepthwiseConv1D(BaseDepthwiseConv):\n         dilation_rate=1,\n         activation=None,\n         use_bias=True,\n-        kernel_initializer=\"glorot_uniform\",\n+        depthwise_initializer=\"glorot_uniform\",\n         bias_initializer=\"zeros\",\n-        kernel_regularizer=None,\n+        depthwise_regularizer=None,\n         bias_regularizer=None,\n         activity_regularizer=None,\n-        kernel_constraint=None,\n+        depthwise_constraint=None,\n         bias_constraint=None,\n         **kwargs\n     ):\n@@ -125,12 +126,12 @@ class DepthwiseConv1D(BaseDepthwiseConv):\n             dilation_rate=dilation_rate,\n             activation=activation,\n             use_bias=use_bias,\n-            kernel_initializer=kernel_initializer,\n+            depthwise_initializer=depthwise_initializer,\n             bias_initializer=bias_initializer,\n-            kernel_regularizer=kernel_regularizer,\n+            depthwise_regularizer=depthwise_regularizer,\n             bias_regularizer=bias_regularizer,\n             activity_regularizer=activity_regularizer,\n-            kernel_constraint=kernel_constraint,\n+            depthwise_constraint=depthwise_constraint,\n             bias_constraint=bias_constraint,\n             **kwargs\n         )\n\n@@ -51,14 +51,15 @@ class DepthwiseConv2D(BaseDepthwiseConv):\n             rate to use for dilated convolution.\n         activation: Activation function. If `None`, no activation is applied.\n         use_bias: bool, if `True`, bias will be added to the output.\n-        kernel_initializer: Initializer for the convolution kernel. If `None`,\n-            the default initializer (`\"glorot_uniform\"`) will be used.\n+        depthwise_initializer: Initializer for the convolution kernel.\n+            If `None`, the default initializer (`\"glorot_uniform\"`)\n+            will be used.\n         bias_initializer: Initializer for the bias vector. If `None`, the\n             default initializer (`\"zeros\"`) will be used.\n-        kernel_regularizer: Optional regularizer for the convolution kernel.\n+        depthwise_regularizer: Optional regularizer for the convolution kernel.\n         bias_regularizer: Optional regularizer for the bias vector.\n         activity_regularizer: Optional regularizer function for the output.\n-        kernel_constraint: Optional projection function to be applied to the\n+        depthwise_constraint: Optional projection function to be applied to the\n             kernel after being updated by an `Optimizer` (e.g. used to implement\n             norm constraints or value constraints for layer weights). The\n             function must take as input the unprojected variable and must return\n@@ -106,12 +107,12 @@ class DepthwiseConv2D(BaseDepthwiseConv):\n         dilation_rate=(1, 1),\n         activation=None,\n         use_bias=True,\n-        kernel_initializer=\"glorot_uniform\",\n+        depthwise_initializer=\"glorot_uniform\",\n         bias_initializer=\"zeros\",\n-        kernel_regularizer=None,\n+        depthwise_regularizer=None,\n         bias_regularizer=None,\n         activity_regularizer=None,\n-        kernel_constraint=None,\n+        depthwise_constraint=None,\n         bias_constraint=None,\n         **kwargs\n     ):\n@@ -125,12 +126,12 @@ class DepthwiseConv2D(BaseDepthwiseConv):\n             dilation_rate=dilation_rate,\n             activation=activation,\n             use_bias=use_bias,\n-            kernel_initializer=kernel_initializer,\n+            depthwise_initializer=depthwise_initializer,\n             bias_initializer=bias_initializer,\n-            kernel_regularizer=kernel_regularizer,\n+            depthwise_regularizer=depthwise_regularizer,\n             bias_regularizer=bias_regularizer,\n             activity_regularizer=activity_regularizer,\n-            kernel_constraint=kernel_constraint,\n+            depthwise_constraint=depthwise_constraint,\n             bias_constraint=bias_constraint,\n             **kwargs\n         )\n\n@@ -170,6 +170,14 @@ class Normalization(Layer):\n                 initializer=\"ones\",\n                 trainable=False,\n             )\n+            # For backwards compatibility with older saved models.\n+            self.count = self.add_weight(\n+                name=\"count\",\n+                shape=(),\n+                dtype=\"int\",\n+                initializer=\"zeros\",\n+                trainable=False,\n+            )\n             self.built = True\n             self.finalize_state()\n         else:\n@@ -307,6 +315,6 @@ class Normalization(Layer):\n         return config\n \n     def load_own_variables(self, store):\n-        # Ensure that we call finalize_state after variable loading.\n         super().load_own_variables(store)\n+        # Ensure that we call finalize_state after variable loading.\n         self.finalize_state()\n\n@@ -26,12 +26,10 @@ class Cropping1D(Layer):\n      [[8 9]]]\n \n     Args:\n-        cropping: Int, or tuple of int (length 2), or dictionary.\n-            - If int: how many units should be trimmed off at the beginning and\n-              end of the cropping dimension (axis 1).\n-            - If tuple of 2 ints: how many units should be trimmed off at the\n-              beginning and end of the cropping dimension\n-              (`(left_crop, right_crop)`).\n+        cropping: Integer or tuple of integers of length 2.\n+            How many units should be trimmed off at the beginning and end of\n+            the cropping dimension (axis 1).\n+            If a single int is provided, the same value will be used for both.\n \n     Input shape:\n         3D tensor with shape `(batch_size, axis_to_crop, features)`\n\n@@ -1,69 +0,0 @@\n-from keras_core import operations as ops\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.input_spec import InputSpec\n-from keras_core.layers.layer import Layer\n-\n-\n-@keras_core_export(\"keras_core.layers.ZeroPadding1D\")\n-class ZeroPadding1D(Layer):\n-    \"\"\"Zero-padding layer for 1D input (e.g. temporal sequence).\n-\n-    Examples:\n-\n-    >>> input_shape = (2, 2, 3)\n-    >>> x = np.arange(np.prod(input_shape)).reshape(input_shape)\n-    >>> x\n-    [[[ 0  1  2]\n-      [ 3  4  5]]\n-     [[ 6  7  8]\n-      [ 9 10 11]]]\n-    >>> y = keras_core.layers.ZeroPadding1D(padding=2)(x)\n-    >>> y\n-    [[[ 0  0  0]\n-      [ 0  0  0]\n-      [ 0  1  2]\n-      [ 3  4  5]\n-      [ 0  0  0]\n-      [ 0  0  0]]\n-     [[ 0  0  0]\n-      [ 0  0  0]\n-      [ 6  7  8]\n-      [ 9 10 11]\n-      [ 0  0  0]\n-      [ 0  0  0]]]\n-\n-    Args:\n-        padding: Int, or tuple of int (length 2), or dictionary.\n-            - If int: how many zeros to add at the beginning and end of\n-              the padding dimension (axis 1).\n-            - If tuple of 2 ints: how many zeros to add at the beginning and the\n-              end of the padding dimension (`(left_pad, right_pad)`).\n-\n-    Input shape:\n-        3D tensor with shape `(batch_size, axis_to_pad, features)`\n-\n-    Output shape:\n-        3D tensor with shape `(batch_size, padded_axis, features)`\n-    \"\"\"\n-\n-    def __init__(self, padding=1, name=None, dtype=None):\n-        super().__init__(name=name, dtype=dtype)\n-        if isinstance(padding, int):\n-            padding = (padding, padding)\n-        self.padding = padding\n-        self.input_spec = InputSpec(ndim=3)\n-\n-    def compute_output_shape(self, input_shape):\n-        output_shape = list(input_shape)\n-        if input_shape[1] is not None:\n-            input_shape[1] += self.padding[0] + self.padding[1]\n-        return tuple(output_shape)\n-\n-    def call(self, inputs):\n-        all_dims_padding = ((0, 0), self.padding, (0, 0))\n-        return ops.pad(inputs, all_dims_padding)\n-\n-    def get_config(self):\n-        config = {\"padding\": self.padding}\n-        base_config = super().get_config()\n-        return {**base_config, **config}\n\n@@ -1,35 +0,0 @@\n-import numpy as np\n-import pytest\n-from absl.testing import parameterized\n-\n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import testing\n-\n-\n-class ZeroPadding1DTest(testing.TestCase, parameterized.TestCase):\n-    def test_zero_padding_1d(self):\n-        inputs = np.random.rand(1, 2, 3)\n-        outputs = layers.ZeroPadding1D(padding=(1, 2))(inputs)\n-\n-        for index in [0, -1, -2]:\n-            self.assertAllClose(outputs[:, index, :], 0.0)\n-        self.assertAllClose(outputs[:, 1:-2, :], inputs)\n-\n-    @parameterized.named_parameters((\"one_tuple\", (2, 2)), (\"one_int\", 2))\n-    def test_zero_padding_1d_with_same_padding(self, padding):\n-        inputs = np.random.rand(1, 2, 3)\n-        outputs = layers.ZeroPadding1D(padding=padding)(inputs)\n-\n-        for index in [0, 1, -1, -2]:\n-            self.assertAllClose(outputs[:, index, :], 0.0)\n-        self.assertAllClose(outputs[:, 2:-2, :], inputs)\n-\n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes\",\n-    )\n-    def test_zero_padding_1d_with_dynamic_spatial_dim(self):\n-        input_layer = layers.Input(batch_shape=(1, None, 3))\n-        padded = layers.ZeroPadding1D((1, 2))(input_layer)\n-        self.assertEqual(padded.shape, (1, None, 3))\n\n@@ -67,8 +67,8 @@ class ZeroPadding2D(Layer):\n           `(batch_size, channels, padded_height, padded_width)`\n     \"\"\"\n \n-    def __init__(self, padding=(1, 1), data_format=None, name=None, dtype=None):\n-        super().__init__(name=name, dtype=dtype)\n+    def __init__(self, padding=(1, 1), data_format=None, **kwargs):\n+        super().__init__(**kwargs)\n         self.data_format = backend.standardize_data_format(data_format)\n         if isinstance(padding, int):\n             self.padding = ((padding, padding), (padding, padding))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
