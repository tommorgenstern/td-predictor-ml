{"custom_id": "scrapy#495acba223ec780f1260d3ec72e6b3a4b54f3c3a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 624 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,12 +8,12 @@ from zope.interface import implements\n from twisted.internet import defer, reactor, protocol\n from twisted.web.client import Agent, ProxyAgent, ResponseDone, \\\n         ResponseFailed, HTTPConnectionPool\n-from twisted.web.http_headers import Headers\n+from twisted.web.http_headers import Headers as TxHeaders\n from twisted.web.http import PotentialDataLoss\n from twisted.web.iweb import IBodyProducer\n from twisted.internet.endpoints import TCP4ClientEndpoint\n \n-from scrapy.http import Headers as ScrapyHeaders\n+from scrapy.http import Headers\n from scrapy.responsetypes import responsetypes\n from scrapy.core.downloader.webclient import _parse\n from scrapy.utils.misc import load_object\n@@ -25,7 +25,8 @@ class Http11DownloadHandler(object):\n \n     def __init__(self, settings):\n         self._pool = HTTPConnectionPool(reactor, persistent=True)\n-        self._contextFactory = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])\n+        self._contextFactoryClass = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])\n+        self._contextFactory = self._contextFactoryClass()\n \n     def download_request(self, request, spider):\n         \"\"\"Return a deferred for the HTTP download\"\"\"\n@@ -50,7 +51,7 @@ class ScrapyAgent(object):\n     def download_request(self, request):\n         url = urldefrag(request.url)[0]\n         method = request.method\n-        headers = Headers(request.headers)\n+        headers = TxHeaders(request.headers)\n         bodyproducer = _RequestBodyProducer(request.body) if request.body else None\n         agent = self._get_agent(request)\n         start_time = time()\n@@ -90,7 +91,7 @@ class ScrapyAgent(object):\n             request.meta[flag] = True\n         url = urldefrag(request.url)[0]\n         status = int(txresponse.code)\n-        headers = ScrapyHeaders(txresponse.headers.getAllRawHeaders())\n+        headers = Headers(txresponse.headers.getAllRawHeaders())\n         respcls = responsetypes.from_args(headers=headers, url=url)\n         return respcls(url=url, status=status, headers=headers, body=body)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cf4d4bc0946d3eda4433d721b8dbf55a459785c1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 6 | Methods Changed: 9 | Complexity Δ (Sum/Max): 8/6 | Churn Δ: 54 | Churn Cumulative: 233 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,8 +1,9 @@\n import json, random, urllib\n from time import time\n-from twisted.web.server import Site\n+from twisted.web.server import Site, NOT_DONE_YET\n from twisted.web.resource import Resource\n from twisted.internet import reactor\n+from twisted.internet.task import deferLater\n \n \n _id = lambda x: x\n@@ -52,6 +53,20 @@ class Follow(Resource):\n         s += \"\"\"</body>\"\"\"\n         return s\n \n+class Delay(Resource):\n+\n+    isLeaf = True\n+\n+    def render_GET(self, request):\n+        n = getarg(request, \"n\", 1, type=float)\n+        d = deferLater(reactor, n, lambda: (request, n))\n+        d.addCallback(self._delayedRender)\n+        return NOT_DONE_YET\n+\n+    def _delayedRender(self, (request, n)):\n+        request.write(\"Response delayed for %0.3f seconds\\n\" % n)\n+        request.finish()\n+\n class Log(Resource):\n \n     isLeaf = True\n@@ -68,6 +83,7 @@ class Root(Resource):\n         Resource.__init__(self)\n         self.log = []\n         self.putChild(\"follow\", Follow())\n+        self.putChild(\"delay\", Delay())\n         self.putChild(\"log\", Log(self.log))\n \n     def getChild(self, request, name):\n\n@@ -24,6 +24,25 @@ class FollowAllSpider(BaseSpider):\n         for link in self.link_extractor.extract_links(response):\n             yield Request(link.url, callback=self.parse)\n \n+class DelaySpider(BaseSpider):\n+\n+    name = 'delay'\n+\n+    def __init__(self, n=1):\n+        self.n = n\n+        self.t1 = self.t2 = self.t2_err = 0\n+\n+    def start_requests(self):\n+        self.t1 = time.time()\n+        yield Request(\"http://localhost:8998/delay?n=%s\" % self.n, \\\n+            callback=self.parse, errback=self.errback)\n+\n+    def parse(self, response):\n+        self.t2 = time.time()\n+\n+    def errback(self, failure):\n+        self.t2_err = time.time()\n+\n def docrawl(spider, settings=None):\n     crawler = get_crawler(settings)\n     crawler.configure()\n@@ -55,3 +74,20 @@ class CrawlTestCase(TestCase):\n         for t2 in spider.times[1:]:\n             self.assertTrue(t2-t > 0.15, \"download delay too small: %s\" % (t2-t))\n             t = t2\n+\n+    @defer.inlineCallbacks\n+    def test_timeout_success(self):\n+        spider = DelaySpider(n=0.5)\n+        yield docrawl(spider)\n+        self.assertTrue(spider.t1 > 0)\n+        self.assertTrue(spider.t2 > 0)\n+        self.assertTrue(spider.t2 > spider.t1)\n+\n+    @defer.inlineCallbacks\n+    def test_timeout_failure(self):\n+        spider = DelaySpider(n=0.5)\n+        yield docrawl(spider, {\"DOWNLOAD_TIMEOUT\": 0.35})\n+        self.assertTrue(spider.t1 > 0)\n+        self.assertTrue(spider.t2 == 0)\n+        self.assertTrue(spider.t2_err > 0)\n+        self.assertTrue(spider.t2_err > spider.t1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#14e7121674f06aed521e671eacd1325613d6464d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 198 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,7 +5,7 @@ from subprocess import Popen, PIPE\n from scrapy.spider import BaseSpider\n from scrapy.http import Request\n from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\n-from scrapy.utils.test import get_crawler\n+from scrapy.utils.test import get_crawler, get_testenv\n \n class FollowAllSpider(BaseSpider):\n \n@@ -52,7 +52,8 @@ def docrawl(spider, settings=None):\n class CrawlTestCase(TestCase):\n \n     def setUp(self):\n-        self.proc = Popen([sys.executable, '-u', '-m', 'scrapy.tests.mockserver'], stdout=PIPE)\n+        self.proc = Popen([sys.executable, '-u', '-m', 'scrapy.tests.mockserver'],\n+            stdout=PIPE, env=get_testenv())\n         self.proc.stdout.readline()\n \n     def tearDown(self):\n\n@@ -67,6 +67,14 @@ def get_pythonpath():\n     scrapy_path = __import__('scrapy').__path__[0]\n     return os.path.dirname(scrapy_path) + sep + os.environ.get('PYTHONPATH', '')\n \n+def get_testenv():\n+    \"\"\"Return a OS environment dict suitable to fork processes that need to import\n+    this installation of Scrapy, instead of a system installed one.\n+    \"\"\"\n+    env = os.environ.copy()\n+    env['PYTHONPATH'] = get_pythonpath()\n+    return env\n+\n def assert_samelines(testcase, text1, text2, msg=None):\n     \"\"\"Asserts text1 and text2 have the same lines, ignoring differences in\n     line endings between platforms\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c2561ca160ea3cfa18e5ff50b1e6cb789bc8681b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 505 | Contributors (this commit): 5 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,15 +1,13 @@\n import sys\n-import os\n from subprocess import Popen, PIPE\n import unittest\n \n-from scrapy.utils.test import get_pythonpath\n+from scrapy.utils.test import get_testenv\n \n class CmdlineTest(unittest.TestCase):\n \n     def setUp(self):\n-        self.env = os.environ.copy()\n-        self.env['PYTHONPATH'] = get_pythonpath()\n+        self.env = get_testenv()\n         self.env['SCRAPY_SETTINGS_MODULE'] = 'scrapy.tests.test_cmdline.settings'\n \n     def _execute(self, *new_args, **kwargs):\n\n@@ -9,7 +9,7 @@ from tempfile import mkdtemp\n from twisted.trial import unittest\n \n from scrapy.utils.python import retry_on_eintr\n-from scrapy.utils.test import get_pythonpath\n+from scrapy.utils.test import get_testenv\n \n class ProjectTest(unittest.TestCase):\n     project_name = 'testproject'\n@@ -19,8 +19,7 @@ class ProjectTest(unittest.TestCase):\n         self.cwd = self.temp_path\n         self.proj_path = join(self.temp_path, self.project_name)\n         self.proj_mod_path = join(self.proj_path, self.project_name)\n-        self.env = os.environ.copy()\n-        self.env['PYTHONPATH'] = get_pythonpath()\n+        self.env = get_testenv()\n \n     def tearDown(self):\n         rmtree(self.temp_path)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#db195ee4df9c0821d1bb6fb227bf2bc9d1f0a862", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 33 | Churn Cumulative: 143 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -67,6 +67,36 @@ class Delay(Resource):\n         request.write(\"Response delayed for %0.3f seconds\\n\" % n)\n         request.finish()\n \n+class Status(Resource):\n+\n+    isLeaf = True\n+\n+    def render_GET(self, request):\n+        n = getarg(request, \"n\", 200, type=int)\n+        request.setResponseCode(n)\n+        return \"\"\n+\n+class Partial(Resource):\n+\n+    isLeaf = True\n+\n+    def render_GET(self, request):\n+        request.setHeader(\"Content-Length\", \"1024\")\n+        d = deferLater(reactor, 0, lambda: request)\n+        d.addCallback(self._delayedRender)\n+        return NOT_DONE_YET\n+\n+    def _delayedRender(self, request):\n+        request.write(\"partial content\\n\")\n+        request.finish()\n+\n+class Drop(Partial):\n+\n+    def _delayedRender(self, request):\n+        request.write(\"this connection will be dropped\\n\")\n+        request.channel.transport.loseConnection()\n+        request.finish()\n+\n class Log(Resource):\n \n     isLeaf = True\n@@ -82,8 +112,11 @@ class Root(Resource):\n     def __init__(self):\n         Resource.__init__(self)\n         self.log = []\n+        self.putChild(\"status\", Status())\n         self.putChild(\"follow\", Follow())\n         self.putChild(\"delay\", Delay())\n+        self.putChild(\"partial\", Partial())\n+        self.putChild(\"drop\", Drop())\n         self.putChild(\"log\", Log(self.log))\n \n     def getChild(self, request, name):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a7b7c892167009bcb4368add567983c89d422b87", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 5 | Churn Cumulative: 832 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -184,7 +184,10 @@ class Scraper(object):\n         \"\"\"\n         if spider_failure is download_failure:\n             errmsg = spider_failure.getErrorMessage()\n-            if errmsg:\n+            if spider_failure.frames:\n+                log.err(spider_failure, 'Error downloading %s' % request,\n+                        level=log.ERROR, spider=spider)\n+            elif errmsg:\n                 log.msg(format='Error downloading %(request)s: %(errmsg)s',\n                         level=log.ERROR, spider=spider, request=request, errmsg=errmsg)\n             return\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#99f8d5733a1630afd5ddd3c6c6af92905d55bc32", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 7 | Complexity Δ (Sum/Max): 9/6 | Churn Δ: 46 | Churn Cumulative: 244 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,7 +5,7 @@ from subprocess import Popen, PIPE\n from scrapy.spider import BaseSpider\n from scrapy.http import Request\n from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\n-from scrapy.utils.test import get_crawler, get_testenv\n+from scrapy.utils.test import get_crawler, get_testenv, get_testlog\n \n class FollowAllSpider(BaseSpider):\n \n@@ -43,6 +43,16 @@ class DelaySpider(BaseSpider):\n     def errback(self, failure):\n         self.t2_err = time.time()\n \n+class SimpleSpider(BaseSpider):\n+\n+    name = 'simple'\n+\n+    def __init__(self, url=\"http://localhost:8998\"):\n+        self.start_urls = [url]\n+\n+    def parse(self, response):\n+        self.log(\"Got response %d\" % response.status)\n+\n def docrawl(spider, settings=None):\n     crawler = get_crawler(settings)\n     crawler.configure()\n@@ -92,3 +102,26 @@ class CrawlTestCase(TestCase):\n         self.assertTrue(spider.t2 == 0)\n         self.assertTrue(spider.t2_err > 0)\n         self.assertTrue(spider.t2_err > spider.t1)\n+\n+    @defer.inlineCallbacks\n+    def test_retry_503(self):\n+        spider = SimpleSpider(\"http://localhost:8998/status?n=503\")\n+        yield docrawl(spider)\n+        self._assert_retried()\n+\n+    @defer.inlineCallbacks\n+    def test_retry_conn_failed(self):\n+        spider = SimpleSpider(\"http://localhost:65432/status?n=503\")\n+        yield docrawl(spider)\n+        self._assert_retried()\n+\n+    @defer.inlineCallbacks\n+    def test_retry_dns_error(self):\n+        spider = SimpleSpider(\"http://localhost666/status?n=503\")\n+        yield docrawl(spider)\n+        self._assert_retried()\n+\n+    def _assert_retried(self):\n+        log = get_testlog()\n+        self.assertEqual(log.count(\"Retrying\"), 2)\n+        self.assertEqual(log.count(\"Gave up retrying\"), 1)\n\n@@ -75,6 +75,17 @@ def get_testenv():\n     env['PYTHONPATH'] = get_pythonpath()\n     return env\n \n+def get_testlog():\n+    \"\"\"Get Scrapy log of current test, ignoring the rest\"\"\"\n+    thistest = []\n+    loglines = open(\"test.log\").readlines()\n+    for l in loglines[::-1]:\n+        thistest.append(l)\n+        if \"[-] -->\" in l:\n+            break\n+    return \"\".join(thistest[::-1])\n+\n+\n def assert_samelines(testcase, text1, text2, msg=None):\n     \"\"\"Asserts text1 and text2 have the same lines, ignoring differences in\n     line endings between platforms\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#edbe525d19a21f54ef6a2b4e5960ef4ad0ac735a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 140 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -66,7 +66,7 @@ class TelnetConsole(protocol.ServerFactory):\n             'engine': self.crawler.engine,\n             'spider': spider,\n             'slot': slot,\n-            'manager': self.crawler,\n+            'crawler': self.crawler,\n             'extensions': self.crawler.extensions,\n             'stats': self.crawler.stats,\n             'spiders': self.crawler.spiders,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5c40741d65710f452774c7e04120244624df9756", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 77 | Lines Deleted: 60 | Files Changed: 3 | Hunks: 9 | Methods Changed: 17 | Complexity Δ (Sum/Max): 2/9 | Churn Δ: 137 | Churn Cumulative: 443 | Contributors (this commit): 2 | Commits (past 90d): 15 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,5 @@\n-import json, random, urllib\n-from time import time\n+import sys, time, json, random, urllib\n+from subprocess import Popen, PIPE\n from twisted.web.server import Site, NOT_DONE_YET\n from twisted.web.resource import Resource\n from twisted.internet import reactor\n@@ -19,7 +19,7 @@ _request_args = {\n \n def encode_request(request):\n     \"\"\"Encode request into a JSON-serializable type\"\"\"\n-    d = {\"time\": time()}\n+    d = {\"time\": time.time()}\n     for k, func in _request_args.iteritems():\n         d[k] = func(getattr(request, k))\n     return d\n@@ -125,6 +125,19 @@ class Root(Resource):\n     def render(self, request):\n         return 'Scrapy mock HTTP server\\n'\n \n+class MockServer():\n+\n+    def __enter__(self):\n+        from scrapy.utils.test import get_testenv\n+        self.proc = Popen([sys.executable, '-u', '-m', 'scrapy.tests.mockserver'],\n+            stdout=PIPE, env=get_testenv())\n+        self.proc.stdout.readline()\n+\n+    def __exit__(self, exc_type, exc_value, traceback):\n+        self.proc.kill()\n+        self.proc.wait()\n+        time.sleep(0.2)\n+\n \n if __name__ == \"__main__\":\n     root = Root()\n\n@@ -0,0 +1,55 @@\n+\"\"\"\n+Some spiders used for testing and benchmarking\n+\"\"\"\n+\n+import time\n+\n+from scrapy.spider import BaseSpider\n+from scrapy.http import Request\n+from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\n+\n+class FollowAllSpider(BaseSpider):\n+\n+    name = 'follow'\n+    link_extractor = SgmlLinkExtractor()\n+\n+    def __init__(self, total=10, show=20, order=\"rand\"):\n+        self.urls_visited = []\n+        self.times = []\n+        url = \"http://localhost:8998/follow?total=%s&show=%s&order=%s\" % (total, show, order)\n+        self.start_urls = [url]\n+\n+    def parse(self, response):\n+        self.urls_visited.append(response.url)\n+        self.times.append(time.time())\n+        for link in self.link_extractor.extract_links(response):\n+            yield Request(link.url, callback=self.parse)\n+\n+class DelaySpider(BaseSpider):\n+\n+    name = 'delay'\n+\n+    def __init__(self, n=1):\n+        self.n = n\n+        self.t1 = self.t2 = self.t2_err = 0\n+\n+    def start_requests(self):\n+        self.t1 = time.time()\n+        yield Request(\"http://localhost:8998/delay?n=%s\" % self.n, \\\n+            callback=self.parse, errback=self.errback)\n+\n+    def parse(self, response):\n+        self.t2 = time.time()\n+\n+    def errback(self, failure):\n+        self.t2_err = time.time()\n+\n+class SimpleSpider(BaseSpider):\n+\n+    name = 'simple'\n+\n+    def __init__(self, url=\"http://localhost:8998\"):\n+        self.start_urls = [url]\n+\n+    def parse(self, response):\n+        self.log(\"Got response %d\" % response.status)\n\n@@ -1,57 +1,9 @@\n-import sys, time\n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n-from subprocess import Popen, PIPE\n-from scrapy.spider import BaseSpider\n-from scrapy.http import Request\n-from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\n-from scrapy.utils.test import get_crawler, get_testenv, get_testlog\n+from scrapy.utils.test import get_crawler, get_testlog\n+from scrapy.tests.spiders import FollowAllSpider, DelaySpider, SimpleSpider\n+from scrapy.tests.mockserver import MockServer\n \n-class FollowAllSpider(BaseSpider):\n-\n-    name = 'follow'\n-    link_extractor = SgmlLinkExtractor()\n-\n-    def __init__(self, total=10, show=20, order=\"rand\"):\n-        self.urls_visited = []\n-        self.times = []\n-        url = \"http://localhost:8998/follow?total=%s&show=%s&order=%s\" % (total, show, order)\n-        self.start_urls = [url]\n-\n-    def parse(self, response):\n-        self.urls_visited.append(response.url)\n-        self.times.append(time.time())\n-        for link in self.link_extractor.extract_links(response):\n-            yield Request(link.url, callback=self.parse)\n-\n-class DelaySpider(BaseSpider):\n-\n-    name = 'delay'\n-\n-    def __init__(self, n=1):\n-        self.n = n\n-        self.t1 = self.t2 = self.t2_err = 0\n-\n-    def start_requests(self):\n-        self.t1 = time.time()\n-        yield Request(\"http://localhost:8998/delay?n=%s\" % self.n, \\\n-            callback=self.parse, errback=self.errback)\n-\n-    def parse(self, response):\n-        self.t2 = time.time()\n-\n-    def errback(self, failure):\n-        self.t2_err = time.time()\n-\n-class SimpleSpider(BaseSpider):\n-\n-    name = 'simple'\n-\n-    def __init__(self, url=\"http://localhost:8998\"):\n-        self.start_urls = [url]\n-\n-    def parse(self, response):\n-        self.log(\"Got response %d\" % response.status)\n \n def docrawl(spider, settings=None):\n     crawler = get_crawler(settings)\n@@ -62,14 +14,11 @@ def docrawl(spider, settings=None):\n class CrawlTestCase(TestCase):\n \n     def setUp(self):\n-        self.proc = Popen([sys.executable, '-u', '-m', 'scrapy.tests.mockserver'],\n-            stdout=PIPE, env=get_testenv())\n-        self.proc.stdout.readline()\n+        self.mockserver = MockServer()\n+        self.mockserver.__enter__()\n \n     def tearDown(self):\n-        self.proc.kill()\n-        self.proc.wait()\n-        time.sleep(0.2)\n+        self.mockserver.__exit__(None, None, None)\n \n     @defer.inlineCallbacks\n     def test_follow_all(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#76087e336aa95439773d6db07ae93d70a1cbba06", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 20 | Churn Cumulative: 20 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,20 @@\n+from scrapy.command import ScrapyCommand\n+from scrapy.tests.spiders import FollowAllSpider\n+from scrapy.tests.mockserver import MockServer\n+\n+class Command(ScrapyCommand):\n+\n+    default_settings = {\n+        'LOG_LEVEL': 'INFO',\n+        'LOGSTATS_INTERVAL': 1,\n+        'CLOSESPIDER_TIMEOUT': 10,\n+    }\n+\n+    def short_desc(self):\n+        return \"Run quick benchmark test\"\n+\n+    def run(self, args, opts):\n+        with MockServer():\n+            spider = FollowAllSpider(total=100000)\n+            self.crawler.crawl(spider)\n+            self.crawler.start()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ca12886acbd1abcbc39ab1029ba06ddd8d852927", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 44 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -42,7 +42,7 @@ master_doc = 'index'\n \n # General information about the project.\n project = u'Scrapy'\n-copyright = u'2008-2012, Scrapinghub'\n+copyright = u'2008-2013, Scrapy developers'\n \n # The version info for the project you're documenting, acts as replacement for\n # |version| and |release|, also used in various other places throughout the\n@@ -173,7 +173,7 @@ htmlhelp_basename = 'Scrapydoc'\n # (source start file, target name, title, author, document class [howto/manual]).\n latex_documents = [\n   ('index', 'Scrapy.tex', ur'Scrapy Documentation',\n-   ur'Scrapinghub', 'manual'),\n+   ur'Scrapy developers', 'manual'),\n ]\n \n # The name of an image file (relative to this directory) to place at the top of\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#485a954571cb659e6fbb3059ac1f4b7954fef145", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 6 | Churn Cumulative: 245 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -32,6 +32,9 @@ class Command(ScrapyCommand):\n                 self.settings.overrides['FEED_URI'] = 'stdout:'\n             else:\n                 self.settings.overrides['FEED_URI'] = opts.output\n+            valid_output_formats = self.settings['FEED_EXPORTERS'].keys() + self.settings['FEED_EXPORTERS_BASE'].keys()\n+            if opts.output_format not in valid_output_formats:\n+                raise UsageError('Invalid/unrecognized output format: %s, Expected %s' % (opts.output_format,valid_output_formats))\n             self.settings.overrides['FEED_FORMAT'] = opts.output_format\n \n     def run(self, args, opts):\n\n@@ -54,6 +54,9 @@ class Command(ScrapyCommand):\n                 self.settings.overrides['FEED_URI'] = 'stdout:'\n             else:\n                 self.settings.overrides['FEED_URI'] = opts.output\n+            valid_output_formats = self.settings['FEED_EXPORTERS'].keys() + self.settings['FEED_EXPORTERS_BASE'].keys()\n+            if opts.output_format not in valid_output_formats:\n+                raise UsageError('Invalid/unrecognized output format: %s, Expected %s' % (opts.output_format,valid_output_formats))\n             self.settings.overrides['FEED_FORMAT'] = opts.output_format\n \n     def run(self, args, opts):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1ed5643eb2237f588044a6595d5e56d8e54039c2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 11 | Churn Cumulative: 548 | Contributors (this commit): 6 | Commits (past 90d): 6 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -15,7 +15,7 @@ class CloseSpider(object):\n \n     def __init__(self, crawler):\n         self.crawler = crawler\n-        self.timeout = crawler.settings.getint('CLOSESPIDER_TIMEOUT')\n+        self.timeout = crawler.settings.getfloat('CLOSESPIDER_TIMEOUT')\n         self.itemcount = crawler.settings.getint('CLOSESPIDER_ITEMCOUNT')\n         self.pagecount = crawler.settings.getint('CLOSESPIDER_PAGECOUNT')\n         self.errorcount = crawler.settings.getint('CLOSESPIDER_ERRORCOUNT')\n\n@@ -226,3 +226,12 @@ ITEM_PIPELINES = ['{0}.pipelines.MyPipeline']\n                 '-c', 'parse', 'http://scrapinghub.com')\n         log = p.stderr.read()\n         self.assert_(\"[scrapy] INFO: It Works!\" in log, log)\n+\n+\n+class BenchCommandTest(CommandTest):\n+\n+    def test_run(self):\n+        p = self.proc('bench', '-s', 'LOGSTATS_INTERVAL=0.001',\n+                '-s', 'CLOSESPIDER_TIMEOUT=0.01')\n+        log = p.stderr.read()\n+        self.assert_('INFO: Crawled' in log, log)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#96b8fb69be8fc2a8d0cbb4850bf01c87ebfa070a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 36 | Files Changed: 1 | Hunks: 11 | Methods Changed: 13 | Complexity Δ (Sum/Max): -6/0 | Churn Δ: 53 | Churn Cumulative: 162 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -3,20 +3,13 @@ from twisted.internet import task\n from scrapy.exceptions import NotConfigured\n from scrapy import log, signals\n \n-class Slot(object):\n-\n-    def __init__(self):\n-        self.items = 0\n-        self.itemsprev = 0\n-        self.pages = 0\n-        self.pagesprev = 0\n \n class LogStats(object):\n     \"\"\"Log basic scraping stats periodically\"\"\"\n \n-    def __init__(self, interval=60.0):\n+    def __init__(self, stats, interval=60.0):\n+        self.stats = stats\n         self.interval = interval\n-        self.slots = {}\n         self.multiplier = 60.0 / self.interval\n \n     @classmethod\n@@ -24,40 +17,28 @@ class LogStats(object):\n         interval = crawler.settings.getfloat('LOGSTATS_INTERVAL')\n         if not interval:\n             raise NotConfigured\n-        o = cls(interval)\n-        crawler.signals.connect(o.item_scraped, signal=signals.item_scraped)\n-        crawler.signals.connect(o.response_received, signal=signals.response_received)\n+        o = cls(crawler.stats, interval)\n         crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n         crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n-        crawler.signals.connect(o.engine_started, signal=signals.engine_started)\n-        crawler.signals.connect(o.engine_stopped, signal=signals.engine_stopped)\n         return o\n \n-    def item_scraped(self, spider):\n-        self.slots[spider].items += 1\n-\n-    def response_received(self, spider):\n-        self.slots[spider].pages += 1\n-\n     def spider_opened(self, spider):\n-        self.slots[spider] = Slot()\n+        self.pagesprev = 0\n+        self.itemsprev = 0\n \n-    def spider_closed(self, spider):\n-        del self.slots[spider]\n+        self.task = task.LoopingCall(self.log, spider)\n+        self.task.start(self.interval)\n \n-    def engine_started(self):\n-        self.tsk = task.LoopingCall(self.log)\n-        self.tsk.start(self.interval)\n-\n-    def log(self):\n-        for spider, slot in self.slots.items():\n-            irate = (slot.items - slot.itemsprev) * self.multiplier\n-            prate = (slot.pages - slot.pagesprev) * self.multiplier\n-            slot.pagesprev, slot.itemsprev = slot.pages, slot.items\n+    def log(self, spider):\n+        items = self.stats.get_value('item_scraped_count', 0)\n+        pages = self.stats.get_value('response_received_count', 0)\n+        irate = (items - self.itemsprev) * self.multiplier\n+        prate = (pages - self.pagesprev) * self.multiplier\n+        self.pagesprev, self.itemsprev = pages, items\n         msg = \"Crawled %d pages (at %d pages/min), scraped %d items (at %d items/min)\" \\\n-                % (slot.pages, prate, slot.items, irate)\n+            % (pages, prate, items, irate)\n         log.msg(msg, spider=spider)\n \n-    def engine_stopped(self):\n-        if self.tsk.running:\n-            self.tsk.stop()\n+    def spider_closed(self, spider, reason):\n+        if self.task.running:\n+            self.task.stop()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f35266e6a5760e17fd1f17127ce92e8adc6a7110", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 5 | Churn Cumulative: 86 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2,7 +2,7 @@\n This module contains some assorted functions used in tests\n \"\"\"\n \n-import os, sys\n+import os\n \n from twisted.trial.unittest import SkipTest\n \n@@ -63,9 +63,8 @@ def get_crawler(settings_dict=None):\n def get_pythonpath():\n     \"\"\"Return a PYTHONPATH suitable to use in processes so that they find this\n     installation of Scrapy\"\"\"\n-    sep = ';' if sys.platform == 'win32' else ':'\n     scrapy_path = __import__('scrapy').__path__[0]\n-    return os.path.dirname(scrapy_path) + sep + os.environ.get('PYTHONPATH', '')\n+    return os.path.dirname(scrapy_path) + os.pathsep + os.environ.get('PYTHONPATH', '')\n \n def get_testenv():\n     \"\"\"Return a OS environment dict suitable to fork processes that need to import\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2bb2ba66f68dc33b830a5ed02f4d79f8010e29e6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 32 | Files Changed: 1 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 34 | Churn Cumulative: 196 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,4 +1,4 @@\n-import sys, time, json, random, urllib\n+import sys, time, random, urllib\n from subprocess import Popen, PIPE\n from twisted.web.server import Site, NOT_DONE_YET\n from twisted.web.resource import Resource\n@@ -6,24 +6,6 @@ from twisted.internet import reactor\n from twisted.internet.task import deferLater\n \n \n-_id = lambda x: x\n-_request_args = {\n-    \"args\": _id,\n-    \"clientproto\": _id,\n-    \"requestHeaders\": lambda x: x._rawHeaders,\n-    \"responseHeaders\": lambda x: x._rawHeaders,\n-    \"method\": _id,\n-    \"path\": _id,\n-    \"uri\": _id,\n-}\n-\n-def encode_request(request):\n-    \"\"\"Encode request into a JSON-serializable type\"\"\"\n-    d = {\"time\": time.time()}\n-    for k, func in _request_args.iteritems():\n-        d[k] = func(getattr(request, k))\n-    return d\n-\n def getarg(request, name, default=None, type=str):\n     if name in request.args:\n         return type(request.args[name][0])\n@@ -97,29 +79,17 @@ class Drop(Partial):\n         request.channel.transport.loseConnection()\n         request.finish()\n \n-class Log(Resource):\n-\n-    isLeaf = True\n-\n-    def __init__(self, log):\n-        self.log = log\n-\n-    def render(self, request):\n-        return json.dumps(self.log)\n-\n class Root(Resource):\n \n     def __init__(self):\n         Resource.__init__(self)\n-        self.log = []\n         self.putChild(\"status\", Status())\n         self.putChild(\"follow\", Follow())\n         self.putChild(\"delay\", Delay())\n         self.putChild(\"partial\", Partial())\n         self.putChild(\"drop\", Drop())\n-        self.putChild(\"log\", Log(self.log))\n \n-    def getChild(self, request, name):\n+    def getChild(self, name, request):\n         return self\n \n     def render(self, request):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#971ef5c78e91787a74e536635b7c96f267321e3a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 144 | Lines Deleted: 32 | Files Changed: 3 | Hunks: 21 | Methods Changed: 23 | Complexity Δ (Sum/Max): 14/7 | Churn Δ: 176 | Churn Cumulative: 354 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,27 +11,28 @@ from twisted.python import log as txlog\n \n from scrapy import signals, log\n \n+\n class CloseSpider(object):\n \n     def __init__(self, crawler):\n         self.crawler = crawler\n-        self.timeout = crawler.settings.getfloat('CLOSESPIDER_TIMEOUT')\n-        self.itemcount = crawler.settings.getint('CLOSESPIDER_ITEMCOUNT')\n-        self.pagecount = crawler.settings.getint('CLOSESPIDER_PAGECOUNT')\n-        self.errorcount = crawler.settings.getint('CLOSESPIDER_ERRORCOUNT')\n \n-        self.errorcounts = defaultdict(int)\n-        self.pagecounts = defaultdict(int)\n-        self.counts = defaultdict(int)\n-        self.tasks = {}\n+        self.close_on = {\n+            'timeout': crawler.settings.getfloat('CLOSESPIDER_TIMEOUT'),\n+            'itemcount': crawler.settings.getint('CLOSESPIDER_ITEMCOUNT'),\n+            'pagecount': crawler.settings.getint('CLOSESPIDER_PAGECOUNT'),\n+            'errorcount': crawler.settings.getint('CLOSESPIDER_ERRORCOUNT'),\n+            }\n \n-        if self.errorcount:\n+        self.counter = defaultdict(int)\n+\n+        if self.close_on.get('errorcount'):\n             txlog.addObserver(self.catch_log)\n-        if self.pagecount:\n+        if self.close_on.get('pagecount'):\n             crawler.signals.connect(self.page_count, signal=signals.response_received)\n-        if self.timeout:\n+        if self.close_on.get('timeout'):\n             crawler.signals.connect(self.spider_opened, signal=signals.spider_opened)\n-        if self.itemcount:\n+        if self.close_on.get('itemcount'):\n             crawler.signals.connect(self.item_scraped, signal=signals.item_scraped)\n         crawler.signals.connect(self.spider_closed, signal=signals.spider_closed)\n \n@@ -43,29 +44,26 @@ class CloseSpider(object):\n         if event.get('logLevel') == log.ERROR:\n             spider = event.get('spider')\n             if spider:\n-                self.errorcounts[spider] += 1\n-                if self.errorcounts[spider] == self.errorcount:\n+                self.counter['errorcount'] += 1\n+                if self.counter['errorcount'] == self.close_on['errorcount']:\n                     self.crawler.engine.close_spider(spider, 'closespider_errorcount')\n \n     def page_count(self, response, request, spider):\n-        self.pagecounts[spider] += 1\n-        if self.pagecounts[spider] == self.pagecount:\n+        self.counter['pagecount'] += 1\n+        if self.counter['pagecount'] == self.close_on['pagecount']:\n             self.crawler.engine.close_spider(spider, 'closespider_pagecount')\n \n     def spider_opened(self, spider):\n-        self.tasks[spider] = reactor.callLater(self.timeout, \\\n-            self.crawler.engine.close_spider, spider=spider, \\\n+        self.task = reactor.callLater(self.close_on['timeout'], \\\n+            self.crawler.engine.close_spider, spider, \\\n             reason='closespider_timeout')\n \n     def item_scraped(self, item, spider):\n-        self.counts[spider] += 1\n-        if self.counts[spider] == self.itemcount:\n+        self.counter['itemcount'] += 1\n+        if self.counter['itemcount'] == self.close_on['itemcount']:\n             self.crawler.engine.close_spider(spider, 'closespider_itemcount')\n \n     def spider_closed(self, spider):\n-        self.counts.pop(spider, None)\n-        self.pagecounts.pop(spider, None)\n-        self.errorcounts.pop(spider, None)\n-        tsk = self.tasks.pop(spider, None)\n-        if tsk and tsk.active():\n-            tsk.cancel()\n+        task = getattr(self, 'task', False)\n+        if task and task.active():\n+            task.cancel()\n\n@@ -6,14 +6,29 @@ import time\n \n from scrapy.spider import BaseSpider\n from scrapy.http import Request\n+from scrapy.item import Item\n from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\n \n-class FollowAllSpider(BaseSpider):\n+\n+class MetaSpider(BaseSpider):\n+\n+    name = 'meta'\n+\n+    def __init__(self, *args, **kwargs):\n+        super(MetaSpider, self).__init__(*args, **kwargs)\n+        self.meta = {}\n+\n+    def closed(self, reason):\n+        self.meta['close_reason'] = reason\n+\n+\n+class FollowAllSpider(MetaSpider):\n \n     name = 'follow'\n     link_extractor = SgmlLinkExtractor()\n \n-    def __init__(self, total=10, show=20, order=\"rand\"):\n+    def __init__(self, total=10, show=20, order=\"rand\", *args, **kwargs):\n+        super(FollowAllSpider, self).__init__(*args, **kwargs)\n         self.urls_visited = []\n         self.times = []\n         url = \"http://localhost:8998/follow?total=%s&show=%s&order=%s\" % (total, show, order)\n@@ -25,11 +40,13 @@ class FollowAllSpider(BaseSpider):\n         for link in self.link_extractor.extract_links(response):\n             yield Request(link.url, callback=self.parse)\n \n-class DelaySpider(BaseSpider):\n+\n+class DelaySpider(MetaSpider):\n \n     name = 'delay'\n \n-    def __init__(self, n=1):\n+    def __init__(self, n=1, *args, **kwargs):\n+        super(DelaySpider, self).__init__(*args, **kwargs)\n         self.n = n\n         self.t1 = self.t2 = self.t2_err = 0\n \n@@ -44,12 +61,42 @@ class DelaySpider(BaseSpider):\n     def errback(self, failure):\n         self.t2_err = time.time()\n \n-class SimpleSpider(BaseSpider):\n+\n+class SimpleSpider(MetaSpider):\n \n     name = 'simple'\n \n-    def __init__(self, url=\"http://localhost:8998\"):\n+    def __init__(self, url=\"http://localhost:8998\", *args, **kwargs):\n+        super(SimpleSpider, self).__init__(*args, **kwargs)\n         self.start_urls = [url]\n \n     def parse(self, response):\n         self.log(\"Got response %d\" % response.status)\n+\n+\n+class ItemSpider(FollowAllSpider):\n+\n+    name = 'item'\n+\n+    def parse(self, response):\n+        for request in super(ItemSpider, self).parse(response):\n+            yield request\n+            yield Item()\n+\n+\n+class DefaultError(Exception):\n+    pass\n+\n+\n+class ErrorSpider(FollowAllSpider):\n+\n+    name = 'error'\n+    exception_cls = DefaultError\n+\n+    def raise_exception(self):\n+        raise self.exception_cls('Expected exception')\n+\n+    def parse(self, response):\n+        for request in super(ErrorSpider, self).parse(response):\n+            yield request\n+            self.raise_exception()\n\n@@ -0,0 +1,67 @@\n+from twisted.internet import defer\n+from twisted.trial.unittest import TestCase\n+from scrapy.utils.test import get_crawler\n+from scrapy.tests.spiders import FollowAllSpider, ItemSpider, ErrorSpider\n+from scrapy.tests.mockserver import MockServer\n+\n+\n+def docrawl(spider, settings=None):\n+    crawler = get_crawler(settings)\n+    crawler.configure()\n+    crawler.crawl(spider)\n+    return crawler.start()\n+\n+class TestCloseSpider(TestCase):\n+\n+    def setUp(self):\n+        self.mockserver = MockServer()\n+        self.mockserver.__enter__()\n+\n+    def tearDown(self):\n+        self.mockserver.__exit__(None, None, None)\n+\n+    @defer.inlineCallbacks\n+    def test_closespider_itemcount(self):\n+        spider = ItemSpider()\n+        close_on = 5\n+        yield docrawl(spider, {'CLOSESPIDER_ITEMCOUNT': close_on})\n+        reason = spider.meta['close_reason']\n+        self.assertEqual(reason, 'closespider_itemcount')\n+        itemcount = spider.crawler.stats.get_value('item_scraped_count')\n+        self.assertTrue(itemcount >= close_on)\n+\n+    @defer.inlineCallbacks\n+    def test_closespider_pagecount(self):\n+        spider = FollowAllSpider()\n+        close_on = 5\n+        yield docrawl(spider, {'CLOSESPIDER_PAGECOUNT': close_on})\n+        reason = spider.meta['close_reason']\n+        self.assertEqual(reason, 'closespider_pagecount')\n+        pagecount = spider.crawler.stats.get_value('response_received_count')\n+        self.assertTrue(pagecount >= close_on)\n+\n+    @defer.inlineCallbacks\n+    def test_closespider_errorcount(self):\n+        spider = ErrorSpider(total=1000000)\n+        close_on = 5\n+        yield docrawl(spider, {'CLOSESPIDER_ERRORCOUNT': close_on})\n+        self.flushLoggedErrors(spider.exception_cls)\n+        reason = spider.meta['close_reason']\n+        self.assertEqual(reason, 'closespider_errorcount')\n+        key = 'spider_exceptions/{name}'\\\n+                .format(name=spider.exception_cls.__name__)\n+        errorcount = spider.crawler.stats.get_value(key)\n+        self.assertTrue(errorcount >= close_on)\n+\n+    @defer.inlineCallbacks\n+    def test_closespider_timeout(self):\n+        spider = FollowAllSpider(total=1000000)\n+        close_on = 0.1\n+        yield docrawl(spider, {'CLOSESPIDER_TIMEOUT': close_on})\n+        reason = spider.meta['close_reason']\n+        self.assertEqual(reason, 'closespider_timeout')\n+        stats = spider.crawler.stats\n+        start = stats.get_value('start_time')\n+        stop = stats.get_value('finish_time')\n+        diff = stop - start\n+        self.assertTrue(diff.total_seconds() >= close_on)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#14e382ae480446abfb1c3dbb9daf574ffe723ad7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 70 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -64,4 +64,5 @@ class TestCloseSpider(TestCase):\n         start = stats.get_value('start_time')\n         stop = stats.get_value('finish_time')\n         diff = stop - start\n-        self.assertTrue(diff.total_seconds() >= close_on)\n+        total_seconds = diff.seconds + diff.microseconds\n+        self.assertTrue(total_seconds >= close_on)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#58dc16f48dcf53ba7e7be20ed335b972e9f9e8e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 25 | Churn Cumulative: 383 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 0.9444444444444444\n\nDIFF:\n@@ -67,10 +67,14 @@ class Command(ScrapyCommand):\n             import setuptools\n         except ImportError:\n             raise UsageError(\"setuptools not installed\")\n+\n+        urllib2.install_opener(urllib2.build_opener(HTTPRedirectHandler))\n+\n         if opts.list_targets:\n             for name, target in _get_targets().items():\n                 print \"%-20s %s\" % (name, target['url'])\n             return\n+\n         if opts.list_projects:\n             target = _get_target(opts.list_projects)\n             req = urllib2.Request(_url(target, 'listprojects.json'))\n@@ -230,3 +234,24 @@ def _build_egg():\n def _create_default_setup_py(**kwargs):\n     with open('setup.py', 'w') as f:\n         f.write(_SETUP_PY_TEMPLATE % kwargs)\n+\n+\n+class HTTPRedirectHandler(urllib2.HTTPRedirectHandler):\n+\n+    def redirect_request(self, req, fp, code, msg, headers, newurl):\n+        newurl = newurl.replace(' ', '%20')\n+        if code in (301, 307):\n+            return urllib2.Request(newurl,\n+                                   data=req.get_data(),\n+                                   headers=req.headers,\n+                                   origin_req_host=req.get_origin_req_host(),\n+                                   unverifiable=True)\n+        elif code in (302, 303):\n+            newheaders = dict((k, v) for k, v in req.headers.items()\n+                              if k.lower() not in (\"content-length\", \"content-type\"))\n+            return urllib2.Request(newurl,\n+                                   headers=newheaders,\n+                                   origin_req_host=req.get_origin_req_host(),\n+                                   unverifiable=True)\n+        else:\n+            raise urllib2.HTTPError(req.get_full_url(), code, msg, headers, fp)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b61a4e719caa83dedfc7acb354ba36c7e476df86", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 10 | Churn Cumulative: 183 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -7,9 +7,8 @@ See documentation in docs/topics/extensions.rst\n from collections import defaultdict\n \n from twisted.internet import reactor\n-from twisted.python import log as txlog\n \n-from scrapy import signals, log\n+from scrapy import signals\n \n \n class CloseSpider(object):\n@@ -27,7 +26,7 @@ class CloseSpider(object):\n         self.counter = defaultdict(int)\n \n         if self.close_on.get('errorcount'):\n-            txlog.addObserver(self.catch_log)\n+            crawler.signals.connect(self.error_count, signal=signals.spider_error)\n         if self.close_on.get('pagecount'):\n             crawler.signals.connect(self.page_count, signal=signals.response_received)\n         if self.close_on.get('timeout'):\n@@ -40,10 +39,7 @@ class CloseSpider(object):\n     def from_crawler(cls, crawler):\n         return cls(crawler)\n \n-    def catch_log(self, event):\n-        if event.get('logLevel') == log.ERROR:\n-            spider = event.get('spider')\n-            if spider:\n+    def error_count(self, failure, response, spider):\n         self.counter['errorcount'] += 1\n         if self.counter['errorcount'] == self.close_on['errorcount']:\n             self.crawler.engine.close_spider(spider, 'closespider_errorcount')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
