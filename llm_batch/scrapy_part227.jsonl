{"custom_id": "scrapy#5b31dfe3c970f7e16dc911bb8b028a0fff54a7f9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 19 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -48,6 +48,11 @@ class CrawlSpider(Spider):\n     def process_results(self, response, results):\n         return results\n \n+    def _build_request(self, rule, link):\n+        r = Request(url=link.url, callback=self._response_downloaded)\n+        r.meta.update(rule=rule, link_text=link.text)\n+        return r\n+\n     def _requests_to_follow(self, response):\n         if not isinstance(response, HtmlResponse):\n             return\n@@ -59,8 +64,7 @@ class CrawlSpider(Spider):\n                 links = rule.process_links(links)\n             for link in links:\n                 seen.add(link)\n-                r = Request(url=link.url, callback=self._response_downloaded)\n-                r.meta.update(rule=n, link_text=link.text)\n+                r = self._build_request(n, link)\n                 yield rule.process_request(r)\n \n     def _response_downloaded(self, response):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ae0ea31abd4e65f7decb4482d1669d7af17bca0a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 12 | Churn Cumulative: 627 | Contributors (this commit): 34 | Commits (past 90d): 7 | Contributors (cumulative): 43 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,6 +8,7 @@ except ImportError:\n from six.moves.urllib.parse import urlunparse\n \n from scrapy.utils.httpobj import urlparse_cached\n+from scrapy.exceptions import NotConfigured\n from scrapy.utils.python import to_bytes\n \n \n@@ -21,6 +22,8 @@ class HttpProxyMiddleware(object):\n \n     @classmethod\n     def from_crawler(cls, crawler):\n+        if not crawler.settings.getbool('HTTPPROXY_ENABLED'):\n+            raise NotConfigured\n         auth_encoding = crawler.settings.get('HTTPPROXY_AUTH_ENCODING')\n         return cls(auth_encoding)\n \n\n@@ -174,6 +174,7 @@ HTTPCACHE_DBM_MODULE = 'anydbm' if six.PY2 else 'dbm'\n HTTPCACHE_POLICY = 'scrapy.extensions.httpcache.DummyPolicy'\n HTTPCACHE_GZIP = False\n \n+HTTPPROXY_ENABLED = True\n HTTPPROXY_AUTH_ENCODING = 'latin-1'\n \n IMAGES_STORE_S3_ACL = 'private'\n\n@@ -1,11 +1,14 @@\n import os\n import sys\n+from functools import partial\n from twisted.trial.unittest import TestCase, SkipTest\n \n from scrapy.downloadermiddlewares.httpproxy import HttpProxyMiddleware\n from scrapy.exceptions import NotConfigured\n from scrapy.http import Response, Request\n from scrapy.spiders import Spider\n+from scrapy.crawler import Crawler\n+from scrapy.settings import Settings\n \n spider = Spider('foo')\n \n@@ -20,6 +23,11 @@ class TestDefaultHeadersMiddleware(TestCase):\n     def tearDown(self):\n         os.environ = self._oldenv\n \n+    def test_not_enabled(self):\n+        settings = Settings({'HTTPPROXY_ENABLED': False})\n+        crawler = Crawler(spider, settings)\n+        self.assertRaises(NotConfigured, partial(HttpProxyMiddleware.from_crawler, crawler))\n+\n     def test_no_enviroment_proxies(self):\n         os.environ = {'dummy_proxy': 'reset_env_and_do_not_raise'}\n         mw = HttpProxyMiddleware()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e285b1d6c2aaa1fdfe788f1894b0196bc64d1be1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 9 | Files Changed: 7 | Hunks: 17 | Methods Changed: 12 | Complexity Δ (Sum/Max): 8/6 | Churn Δ: 53 | Churn Cumulative: 1442 | Contributors (this commit): 27 | Commits (past 90d): 10 | Contributors (cumulative): 52 | DMM Complexity: 1.0\n\nDIFF:\n@@ -22,6 +22,7 @@ from twisted.web.client import ResponseFailed\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.response import response_status_message\n from scrapy.core.downloader.handlers.http11 import TunnelError\n+from scrapy.utils.python import global_object_name\n \n logger = logging.getLogger(__name__)\n \n@@ -35,16 +36,18 @@ class RetryMiddleware(object):\n                            ConnectionLost, TCPTimedOutError, ResponseFailed,\n                            IOError, TunnelError)\n \n-    def __init__(self, settings):\n+    def __init__(self, crawler):\n+        settings = crawler.settings\n         if not settings.getbool('RETRY_ENABLED'):\n             raise NotConfigured\n         self.max_retry_times = settings.getint('RETRY_TIMES')\n         self.retry_http_codes = set(int(x) for x in settings.getlist('RETRY_HTTP_CODES'))\n         self.priority_adjust = settings.getint('RETRY_PRIORITY_ADJUST')\n+        self.stats = crawler.stats\n \n     @classmethod\n     def from_crawler(cls, crawler):\n-        return cls(crawler.settings)\n+        return cls(crawler)\n \n     def process_response(self, request, response, spider):\n         if request.meta.get('dont_retry', False):\n@@ -70,8 +73,15 @@ class RetryMiddleware(object):\n             retryreq.meta['retry_times'] = retries\n             retryreq.dont_filter = True\n             retryreq.priority = request.priority + self.priority_adjust\n+\n+            if isinstance(reason, Exception):\n+                reason = global_object_name(reason.__class__)\n+\n+            self.stats.inc_value('retry/count')\n+            self.stats.inc_value('retry/reason_count/%s' % reason)\n             return retryreq\n         else:\n+            self.stats.inc_value('retry/max_reached')\n             logger.debug(\"Gave up retrying %(request)s (failed %(retries)d times): %(reason)s\",\n                          {'request': request, 'retries': retries, 'reason': reason},\n                          extra={'spider': spider})\n\n@@ -1,6 +1,8 @@\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.request import request_httprepr\n from scrapy.utils.response import response_httprepr\n+from scrapy.utils.python import global_object_name\n+\n \n class DownloaderStats(object):\n \n@@ -27,6 +29,6 @@ class DownloaderStats(object):\n         return response\n \n     def process_exception(self, request, exception, spider):\n-        ex_class = \"%s.%s\" % (exception.__class__.__module__, exception.__class__.__name__)\n+        ex_class = global_object_name(exception.__class__)\n         self.stats.inc_value('downloader/exception_count', spider=spider)\n         self.stats.inc_value('downloader/exception_type_count/%s' % ex_class, spider=spider)\n\n@@ -113,7 +113,7 @@ def md5sum(file):\n         m.update(d)\n     return m.hexdigest()\n \n+\n def rel_has_nofollow(rel):\n     \"\"\"Return True if link rel attribute has nofollow type\"\"\"\n     return True if rel is not None and 'nofollow' in rel.split() else False\n-    \n\n@@ -344,3 +344,14 @@ def without_none_values(iterable):\n         return {k: v for k, v in six.iteritems(iterable) if v is not None}\n     except AttributeError:\n         return type(iterable)((v for v in iterable if v is not None))\n+\n+\n+def global_object_name(obj):\n+    \"\"\"\n+    Return full name of a global object.\n+\n+    >>> from scrapy import Request\n+    >>> global_object_name(Request)\n+    'scrapy.http.request.Request'\n+    \"\"\"\n+    return \"%s.%s\" % (obj.__module__, obj.__name__)\n\n@@ -43,7 +43,8 @@ def get_meta_refresh(response):\n def response_status_message(status):\n     \"\"\"Return status code plus status text descriptive message\n     \"\"\"\n-    return '%s %s' % (status, to_native_str(http.RESPONSES.get(int(status), \"Unknown Status\")))\n+    message = http.RESPONSES.get(int(status), \"Unknown Status\")\n+    return '%s %s' % (status, to_native_str(message))\n \n \n def response_httprepr(response):\n\n@@ -13,9 +13,9 @@ from scrapy.utils.test import get_crawler\n \n class RetryTest(unittest.TestCase):\n     def setUp(self):\n-        crawler = get_crawler(Spider)\n-        self.spider = crawler._create_spider('foo')\n-        self.mw = RetryMiddleware.from_crawler(crawler)\n+        self.crawler = get_crawler(Spider)\n+        self.spider = self.crawler._create_spider('foo')\n+        self.mw = RetryMiddleware.from_crawler(self.crawler)\n         self.mw.max_retry_times = 2\n \n     def test_priority_adjust(self):\n@@ -70,6 +70,10 @@ class RetryTest(unittest.TestCase):\n         # discard it\n         assert self.mw.process_response(req, rsp, self.spider) is rsp\n \n+        assert self.crawler.stats.get_value('retry/max_reached') == 1\n+        assert self.crawler.stats.get_value('retry/reason_count/503 Service Unavailable') == 2\n+        assert self.crawler.stats.get_value('retry/count') == 2\n+\n     def test_twistederrors(self):\n         exceptions = [defer.TimeoutError, TCPTimedOutError, TimeoutError,\n                 DNSLookupError, ConnectionRefusedError, ConnectionDone,\n@@ -79,6 +83,11 @@ class RetryTest(unittest.TestCase):\n             req = Request('http://www.scrapytest.org/%s' % exc.__name__)\n             self._test_retry_exception(req, exc('foo'))\n \n+        stats = self.crawler.stats\n+        assert stats.get_value('retry/max_reached') == len(exceptions)\n+        assert stats.get_value('retry/count') == len(exceptions) * 2\n+        assert stats.get_value('retry/reason_count/twisted.internet.defer.TimeoutError') == 2\n+\n     def _test_retry_exception(self, req, exception):\n         # first retry\n         req = self.mw.process_exception(req, exception, self.spider)\n\n@@ -101,7 +101,9 @@ class ProxyConnectTestCase(TestCase):\n         self._assert_got_response_code(407, l)\n \n     def _assert_got_response_code(self, code, log):\n+        print(log)\n         self.assertEqual(str(log).count('Crawled (%d)' % code), 1)\n \n     def _assert_got_tunnel_error(self, log):\n-        self.assertEqual(str(log).count('TunnelError'), 1)\n+        print(log)\n+        self.assertIn('TunnelError', str(log))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#39df675f091cf904dc904acd9538a0bebe2e55cd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 38 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -36,18 +36,16 @@ class RetryMiddleware(object):\n                            ConnectionLost, TCPTimedOutError, ResponseFailed,\n                            IOError, TunnelError)\n \n-    def __init__(self, crawler):\n-        settings = crawler.settings\n+    def __init__(self, settings):\n         if not settings.getbool('RETRY_ENABLED'):\n             raise NotConfigured\n         self.max_retry_times = settings.getint('RETRY_TIMES')\n         self.retry_http_codes = set(int(x) for x in settings.getlist('RETRY_HTTP_CODES'))\n         self.priority_adjust = settings.getint('RETRY_PRIORITY_ADJUST')\n-        self.stats = crawler.stats\n \n     @classmethod\n     def from_crawler(cls, crawler):\n-        return cls(crawler)\n+        return cls(crawler.settings)\n \n     def process_response(self, request, response, spider):\n         if request.meta.get('dont_retry', False):\n@@ -65,6 +63,7 @@ class RetryMiddleware(object):\n     def _retry(self, request, reason, spider):\n         retries = request.meta.get('retry_times', 0) + 1\n \n+        stats = spider.crawler.stats\n         if retries <= self.max_retry_times:\n             logger.debug(\"Retrying %(request)s (failed %(retries)d times): %(reason)s\",\n                          {'request': request, 'retries': retries, 'reason': reason},\n@@ -77,11 +76,11 @@ class RetryMiddleware(object):\n             if isinstance(reason, Exception):\n                 reason = global_object_name(reason.__class__)\n \n-            self.stats.inc_value('retry/count')\n-            self.stats.inc_value('retry/reason_count/%s' % reason)\n+            stats.inc_value('retry/count')\n+            stats.inc_value('retry/reason_count/%s' % reason)\n             return retryreq\n         else:\n-            self.stats.inc_value('retry/max_reached')\n+            stats.inc_value('retry/max_reached')\n             logger.debug(\"Gave up retrying %(request)s (failed %(retries)d times): %(reason)s\",\n                          {'request': request, 'retries': retries, 'reason': reason},\n                          extra={'spider': spider})\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e1ceaf3b5fa29326f032c4ed3f50943384b9e63d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 427 | Contributors (this commit): 12 | Commits (past 90d): 5 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -43,7 +43,7 @@ setup(\n     ],\n     install_requires=[\n         'Twisted>=13.1.0',\n-        'w3lib>=1.15.0',\n+        'w3lib>=1.17.0',\n         'queuelib',\n         'lxml',\n         'pyOpenSSL',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#877057fac0d47e5ece95a55594706c91c8855883", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 59 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 10/10 | Churn Δ: 59 | Churn Cumulative: 429 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,8 +8,12 @@ See documentation in docs/topics/request-response.rst\n import six\n from six.moves.urllib.parse import urljoin\n \n+import parsel\n from w3lib.encoding import html_to_unicode, resolve_encoding, \\\n     html_body_declared_encoding, http_content_type_encoding\n+\n+from scrapy.link import Link\n+from scrapy.http.request import Request\n from scrapy.http.response import Response\n from scrapy.utils.response import get_base_url\n from scrapy.utils.python import memoizemethod_noargs, to_native_str\n@@ -116,3 +120,58 @@ class TextResponse(Response):\n \n     def css(self, query):\n         return self.selector.css(query)\n+\n+    def follow(self, url, callback=None, method='GET', headers=None, body=None,\n+               cookies=None, meta=None, encoding=None, priority=0,\n+               dont_filter=False, errback=None):\n+        # type: (...) -> Request\n+        \"\"\"\n+        Return a scrapy.Request instance to follow a link ``url``.\n+\n+        ``url`` can be:\n+\n+        * absolute URL;\n+        * relative URL;\n+        * scrapy.link.Link object (e.g. a link extractor result);\n+        * attribute Selector (not SelectorList) - e.g.\n+          ``response.css('a::attr(href)')[0]`` or\n+          ``response.xpath('//img/@src')[0]``.\n+        * a Selector for ``<a>`` element, e.g.\n+          ``response.css('a.my_link')[0]``.\n+        \"\"\"\n+        if isinstance(url, Link):\n+            url = url.url\n+        elif isinstance(url, parsel.Selector):\n+            url = _url_from_selector(url)\n+        elif isinstance(url, parsel.SelectorList):\n+            raise ValueError(\"Please pass either string\")\n+\n+\n+        encoding = self.encoding if encoding is None else encoding\n+        url = self.urljoin(url)\n+        return Request(url, callback,\n+                       method=method,\n+                       headers=headers,\n+                       body=body,\n+                       cookies=cookies,\n+                       meta=meta,\n+                       encoding=encoding,\n+                       priority=priority,\n+                       dont_filter=dont_filter,\n+                       errback=errback)\n+\n+\n+def _url_from_selector(sel):\n+    # type: (parsel.Selector) -> str\n+    if isinstance(sel.root, six.string_types):\n+        # e.g. ::attr(href) result\n+        return sel.root\n+    if not hasattr(sel.root, 'tag'):\n+        raise ValueError(\"Unsupported selector: %s\" % sel)\n+    if sel.root.tag != 'a':\n+        raise ValueError(\"Only <a> elements are supported; got <%s>\" %\n+                         sel.root.tag)\n+    href = sel.root.get('href')\n+    if href is None:\n+        raise ValueError(\"<a> element has no href attribute: %s\" % sel)\n+    return href\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#71dd5d0bf9d0c41d70e72b0fd0a89528ef246065", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 434 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -142,10 +142,9 @@ class TextResponse(Response):\n         if isinstance(url, Link):\n             url = url.url\n         elif isinstance(url, parsel.Selector):\n-            url = _url_from_selector(url)\n+            url = _url_from_selector(url).strip()\n         elif isinstance(url, parsel.SelectorList):\n-            raise ValueError(\"Please pass either string\")\n-\n+            raise ValueError(\"SelectorList is not supported\")\n \n         encoding = self.encoding if encoding is None else encoding\n         url = self.urljoin(url)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#608c3f0c452bd508aa24bc9d4bf759e0b11e1683", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 114 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 10 | Methods Changed: 15 | Complexity Δ (Sum/Max): 18/18 | Churn Δ: 119 | Churn Cumulative: 801 | Contributors (this commit): 8 | Commits (past 90d): 7 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,6 +11,7 @@ from six.moves.urllib.parse import urljoin\n import parsel\n from w3lib.encoding import html_to_unicode, resolve_encoding, \\\n     html_body_declared_encoding, http_content_type_encoding\n+from w3lib.html import strip_html5_whitespace\n \n from scrapy.link import Link\n from scrapy.http.request import Request\n@@ -142,7 +143,7 @@ class TextResponse(Response):\n         if isinstance(url, Link):\n             url = url.url\n         elif isinstance(url, parsel.Selector):\n-            url = _url_from_selector(url).strip()\n+            url = _url_from_selector(url)\n         elif isinstance(url, parsel.SelectorList):\n             raise ValueError(\"SelectorList is not supported\")\n \n@@ -164,7 +165,7 @@ def _url_from_selector(sel):\n     # type: (parsel.Selector) -> str\n     if isinstance(sel.root, six.string_types):\n         # e.g. ::attr(href) result\n-        return sel.root\n+        return strip_html5_whitespace(sel.root)\n     if not hasattr(sel.root, 'tag'):\n         raise ValueError(\"Unsupported selector: %s\" % sel)\n     if sel.root.tag != 'a':\n@@ -173,4 +174,4 @@ def _url_from_selector(sel):\n     href = sel.root.get('href')\n     if href is None:\n         raise ValueError(\"<a> element has no href attribute: %s\" % sel)\n-    return href\n+    return strip_html5_whitespace(href)\n\n@@ -26,9 +26,12 @@ try:\n except ImportError:\n     import mock\n \n-tests_datadir = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'sample_data')\n+tests_datadir = os.path.join(os.path.abspath(os.path.dirname(__file__)),\n+                             'sample_data')\n+\n \n def get_testdata(*paths):\n     \"\"\"Return test data\"\"\"\n     path = os.path.join(tests_datadir, *paths)\n-    return open(path, 'rb').read()\n+    with open(path, 'rb') as f:\n+        return f.read()\n\n@@ -1,3 +1,4 @@\n+# -*- coding: utf-8 -*-\n import unittest\n \n import six\n@@ -8,6 +9,8 @@ from scrapy.http import (Request, Response, TextResponse, HtmlResponse,\n from scrapy.selector import Selector\n from scrapy.utils.python import to_native_str\n from scrapy.exceptions import NotSupported\n+from scrapy.link import Link\n+from tests import get_testdata\n \n \n class BaseResponseTest(unittest.TestCase):\n@@ -356,6 +359,11 @@ class HtmlResponseTest(TextResponseTest):\n \n     response_class = HtmlResponse\n \n+    def _links_response(self):\n+        body = get_testdata('link_extractor', 'sgml_linkextractor.html')\n+        resp = self.response_class('http://example.com/index', body=body)\n+        return resp\n+\n     def test_html_encoding(self):\n \n         body = b\"\"\"<html><head><title>Some page</title><meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-1\">\n@@ -388,6 +396,103 @@ class HtmlResponseTest(TextResponseTest):\n         r1 = self.response_class(\"http://www.example.com\", body=body)\n         self._assert_response_values(r1, 'gb2312', body)\n \n+    def assert_followed_url(self, follow_obj, target_url, response=None):\n+        if response is None:\n+            response = self._links_response()\n+        req = response.follow(follow_obj)\n+        self.assertEqual(req.url, target_url)\n+        return req\n+\n+    def test_follow_url_absolute(self):\n+        self.assert_followed_url('http://foo.example.com',\n+                                 'http://foo.example.com')\n+\n+    def test_follow_url_relative(self):\n+        self.assert_followed_url('foo',\n+                                 'http://example.com/foo')\n+\n+    def test_follow_link(self):\n+        self.assert_followed_url(Link('http://example.com/foo'),\n+                                 'http://example.com/foo')\n+\n+    def test_follow_selector(self):\n+        resp = self._links_response()\n+        urls = [\n+            'http://example.com/sample2.html',\n+            'http://example.com/sample3.html',\n+            'http://example.com/sample3.html',\n+            'http://www.google.com/something',\n+            'http://example.com/innertag.html'\n+        ]\n+\n+        # select <a> elements\n+        for sellist in [resp.css('a'), resp.xpath('//a')]:\n+            for sel, url in zip(sellist, urls):\n+                self.assert_followed_url(sel, url, response=resp)\n+\n+        # href attributes should work\n+        for sellist in [resp.css('a::attr(href)'), resp.xpath('//a/@href')]:\n+            for sel, url in zip(sellist, urls):\n+                self.assert_followed_url(sel, url, response=resp)\n+\n+        # non-a elements are not supported\n+        self.assertRaises(ValueError, resp.follow, resp.css('div')[0])\n+\n+    def test_follow_selector_list(self):\n+        resp = self._links_response()\n+        self.assertRaisesRegex(ValueError, 'SelectorList',\n+                               resp.follow, resp.css('a'))\n+\n+    def test_follow_selector_attribute(self):\n+        resp = self._links_response()\n+        for src in resp.css('img::attr(src)'):\n+            self.assert_followed_url(src, 'http://example.com/sample2.jpg')\n+\n+    def test_follow_whitespace_url(self):\n+        self.assert_followed_url('foo ',\n+                                 'http://example.com/foo%20')\n+\n+    def test_follow_whitespace_link(self):\n+        self.assert_followed_url(Link('http://example.com/foo '),\n+                                 'http://example.com/foo%20')\n+\n+    def test_follow_whitespace_selector(self):\n+        resp = self.response_class(\n+            'http://example.com',\n+            body=b'''<html><body><a href=\" foo\\n\">click me</a></body></html>'''\n+        )\n+        self.assert_followed_url(resp.css('a')[0],\n+                                 'http://example.com/foo',\n+                                 response=resp)\n+        self.assert_followed_url(resp.css('a::attr(href)')[0],\n+                                 'http://example.com/foo',\n+                                 response=resp)\n+\n+    def test_follow_encoding(self):\n+        resp1 = self.response_class(\n+            'http://example.com',\n+            encoding='utf8',\n+            body='<html><body><a href=\"foo?привет\">click me</a></body></html>'.encode('utf8')\n+        )\n+        req = self.assert_followed_url(\n+            resp1.css('a')[0],\n+            'http://example.com/foo?%D0%BF%D1%80%D0%B8%D0%B2%D0%B5%D1%82',\n+            response=resp1,\n+        )\n+        self.assertEqual(req.encoding, 'utf8')\n+\n+        resp2 = self.response_class(\n+            'http://example.com',\n+            encoding='cp1251',\n+            body='<html><body><a href=\"foo?привет\">click me</a></body></html>'.encode('cp1251')\n+        )\n+        req = self.assert_followed_url(\n+            resp2.css('a')[0],\n+            'http://example.com/foo?%EF%F0%E8%E2%E5%F2',\n+            response=resp2,\n+        )\n+        self.assertEqual(req.encoding, 'cp1251')\n+\n \n class XmlResponseTest(TextResponseTest):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2674f317df9e4970f5953db3a6df04331246d8c9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 135 | Lines Deleted: 109 | Files Changed: 3 | Hunks: 11 | Methods Changed: 15 | Complexity Δ (Sum/Max): 1/2 | Churn Δ: 244 | Churn Cumulative: 1254 | Contributors (this commit): 8 | Commits (past 90d): 9 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,7 +6,9 @@ See documentation in docs/topics/request-response.rst\n \"\"\"\n from six.moves.urllib.parse import urljoin\n \n+from scrapy.http.request import Request\n from scrapy.http.headers import Headers\n+from scrapy.link import Link\n from scrapy.utils.trackref import object_ref\n from scrapy.http.common import obsolete_setter\n from scrapy.exceptions import NotSupported\n@@ -101,3 +103,30 @@ class Response(object_ref):\n         is text (subclasses of TextResponse).\n         \"\"\"\n         raise NotSupported(\"Response content isn't text\")\n+\n+    def follow(self, url, callback=None, method='GET', headers=None, body=None,\n+               cookies=None, meta=None, encoding='utf-8', priority=0,\n+               dont_filter=False, errback=None):\n+        # type: (...) -> Request\n+        \"\"\"\n+        Return a scrapy.Request instance to follow a link ``url``.\n+\n+        ``url`` can be:\n+\n+        * absolute URL;\n+        * relative URL;\n+        * scrapy.link.Link object.\n+        \"\"\"\n+        if isinstance(url, Link):\n+            url = url.url\n+        url = self.urljoin(url)\n+        return Request(url, callback,\n+                       method=method,\n+                       headers=headers,\n+                       body=body,\n+                       cookies=cookies,\n+                       meta=meta,\n+                       encoding=encoding,\n+                       priority=priority,\n+                       dont_filter=dont_filter,\n+                       errback=errback)\n\n@@ -140,16 +140,12 @@ class TextResponse(Response):\n         * a Selector for ``<a>`` element, e.g.\n           ``response.css('a.my_link')[0]``.\n         \"\"\"\n-        if isinstance(url, Link):\n-            url = url.url\n-        elif isinstance(url, parsel.Selector):\n+        if isinstance(url, parsel.Selector):\n             url = _url_from_selector(url)\n         elif isinstance(url, parsel.SelectorList):\n             raise ValueError(\"SelectorList is not supported\")\n-\n         encoding = self.encoding if encoding is None else encoding\n-        url = self.urljoin(url)\n-        return Request(url, callback,\n+        return super(TextResponse, self).follow(url, callback,\n             method=method,\n             headers=headers,\n             body=body,\n@@ -158,7 +154,8 @@ class TextResponse(Response):\n             encoding=encoding,\n             priority=priority,\n             dont_filter=dont_filter,\n-                       errback=errback)\n+            errback=errback\n+        )\n \n \n def _url_from_selector(sel):\n\n@@ -143,6 +143,38 @@ class BaseResponseTest(unittest.TestCase):\n             r.css('body')\n             r.xpath('//body')\n \n+    def test_follow_url_absolute(self):\n+        self._assert_followed_url('http://foo.example.com',\n+                                  'http://foo.example.com')\n+\n+    def test_follow_url_relative(self):\n+        self._assert_followed_url('foo',\n+                                  'http://example.com/foo')\n+\n+    def test_follow_link(self):\n+        self._assert_followed_url(Link('http://example.com/foo'),\n+                                  'http://example.com/foo')\n+\n+    def test_follow_whitespace_url(self):\n+        self._assert_followed_url('foo ',\n+                                  'http://example.com/foo%20')\n+\n+    def test_follow_whitespace_link(self):\n+        self._assert_followed_url(Link('http://example.com/foo '),\n+                                  'http://example.com/foo%20')\n+\n+    def _assert_followed_url(self, follow_obj, target_url, response=None):\n+        if response is None:\n+            response = self._links_response()\n+        req = response.follow(follow_obj)\n+        self.assertEqual(req.url, target_url)\n+        return req\n+\n+    def _links_response(self):\n+        body = get_testdata('link_extractor', 'sgml_linkextractor.html')\n+        resp = self.response_class('http://example.com/index', body=body)\n+        return resp\n+\n \n class TextResponseTest(BaseResponseTest):\n \n@@ -354,16 +386,81 @@ class TextResponseTest(BaseResponseTest):\n         absolute = 'http://www.example.com/elsewhere/test'\n         self.assertEqual(joined, absolute)\n \n+    def test_follow_selector(self):\n+        resp = self._links_response()\n+        urls = [\n+            'http://example.com/sample2.html',\n+            'http://example.com/sample3.html',\n+            'http://example.com/sample3.html',\n+            'http://www.google.com/something',\n+            'http://example.com/innertag.html'\n+        ]\n+\n+        # select <a> elements\n+        for sellist in [resp.css('a'), resp.xpath('//a')]:\n+            for sel, url in zip(sellist, urls):\n+                self._assert_followed_url(sel, url, response=resp)\n+\n+        # href attributes should work\n+        for sellist in [resp.css('a::attr(href)'), resp.xpath('//a/@href')]:\n+            for sel, url in zip(sellist, urls):\n+                self._assert_followed_url(sel, url, response=resp)\n+\n+        # non-a elements are not supported\n+        self.assertRaises(ValueError, resp.follow, resp.css('div')[0])\n+\n+    def test_follow_selector_list(self):\n+        resp = self._links_response()\n+        self.assertRaisesRegex(ValueError, 'SelectorList',\n+                               resp.follow, resp.css('a'))\n+\n+    def test_follow_selector_attribute(self):\n+        resp = self._links_response()\n+        for src in resp.css('img::attr(src)'):\n+            self._assert_followed_url(src, 'http://example.com/sample2.jpg')\n+\n+    def test_follow_whitespace_selector(self):\n+        resp = self.response_class(\n+            'http://example.com',\n+            body=b'''<html><body><a href=\" foo\\n\">click me</a></body></html>'''\n+        )\n+        self._assert_followed_url(resp.css('a')[0],\n+                                 'http://example.com/foo',\n+                                  response=resp)\n+        self._assert_followed_url(resp.css('a::attr(href)')[0],\n+                                 'http://example.com/foo',\n+                                  response=resp)\n+\n+    def test_follow_encoding(self):\n+        resp1 = self.response_class(\n+            'http://example.com',\n+            encoding='utf8',\n+            body='<html><body><a href=\"foo?привет\">click me</a></body></html>'.encode('utf8')\n+        )\n+        req = self._assert_followed_url(\n+            resp1.css('a')[0],\n+            'http://example.com/foo?%D0%BF%D1%80%D0%B8%D0%B2%D0%B5%D1%82',\n+            response=resp1,\n+        )\n+        self.assertEqual(req.encoding, 'utf8')\n+\n+        resp2 = self.response_class(\n+            'http://example.com',\n+            encoding='cp1251',\n+            body='<html><body><a href=\"foo?привет\">click me</a></body></html>'.encode('cp1251')\n+        )\n+        req = self._assert_followed_url(\n+            resp2.css('a')[0],\n+            'http://example.com/foo?%EF%F0%E8%E2%E5%F2',\n+            response=resp2,\n+        )\n+        self.assertEqual(req.encoding, 'cp1251')\n+\n \n class HtmlResponseTest(TextResponseTest):\n \n     response_class = HtmlResponse\n \n-    def _links_response(self):\n-        body = get_testdata('link_extractor', 'sgml_linkextractor.html')\n-        resp = self.response_class('http://example.com/index', body=body)\n-        return resp\n-\n     def test_html_encoding(self):\n \n         body = b\"\"\"<html><head><title>Some page</title><meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-1\">\n@@ -396,103 +493,6 @@ class HtmlResponseTest(TextResponseTest):\n         r1 = self.response_class(\"http://www.example.com\", body=body)\n         self._assert_response_values(r1, 'gb2312', body)\n \n-    def assert_followed_url(self, follow_obj, target_url, response=None):\n-        if response is None:\n-            response = self._links_response()\n-        req = response.follow(follow_obj)\n-        self.assertEqual(req.url, target_url)\n-        return req\n-\n-    def test_follow_url_absolute(self):\n-        self.assert_followed_url('http://foo.example.com',\n-                                 'http://foo.example.com')\n-\n-    def test_follow_url_relative(self):\n-        self.assert_followed_url('foo',\n-                                 'http://example.com/foo')\n-\n-    def test_follow_link(self):\n-        self.assert_followed_url(Link('http://example.com/foo'),\n-                                 'http://example.com/foo')\n-\n-    def test_follow_selector(self):\n-        resp = self._links_response()\n-        urls = [\n-            'http://example.com/sample2.html',\n-            'http://example.com/sample3.html',\n-            'http://example.com/sample3.html',\n-            'http://www.google.com/something',\n-            'http://example.com/innertag.html'\n-        ]\n-\n-        # select <a> elements\n-        for sellist in [resp.css('a'), resp.xpath('//a')]:\n-            for sel, url in zip(sellist, urls):\n-                self.assert_followed_url(sel, url, response=resp)\n-\n-        # href attributes should work\n-        for sellist in [resp.css('a::attr(href)'), resp.xpath('//a/@href')]:\n-            for sel, url in zip(sellist, urls):\n-                self.assert_followed_url(sel, url, response=resp)\n-\n-        # non-a elements are not supported\n-        self.assertRaises(ValueError, resp.follow, resp.css('div')[0])\n-\n-    def test_follow_selector_list(self):\n-        resp = self._links_response()\n-        self.assertRaisesRegex(ValueError, 'SelectorList',\n-                               resp.follow, resp.css('a'))\n-\n-    def test_follow_selector_attribute(self):\n-        resp = self._links_response()\n-        for src in resp.css('img::attr(src)'):\n-            self.assert_followed_url(src, 'http://example.com/sample2.jpg')\n-\n-    def test_follow_whitespace_url(self):\n-        self.assert_followed_url('foo ',\n-                                 'http://example.com/foo%20')\n-\n-    def test_follow_whitespace_link(self):\n-        self.assert_followed_url(Link('http://example.com/foo '),\n-                                 'http://example.com/foo%20')\n-\n-    def test_follow_whitespace_selector(self):\n-        resp = self.response_class(\n-            'http://example.com',\n-            body=b'''<html><body><a href=\" foo\\n\">click me</a></body></html>'''\n-        )\n-        self.assert_followed_url(resp.css('a')[0],\n-                                 'http://example.com/foo',\n-                                 response=resp)\n-        self.assert_followed_url(resp.css('a::attr(href)')[0],\n-                                 'http://example.com/foo',\n-                                 response=resp)\n-\n-    def test_follow_encoding(self):\n-        resp1 = self.response_class(\n-            'http://example.com',\n-            encoding='utf8',\n-            body='<html><body><a href=\"foo?привет\">click me</a></body></html>'.encode('utf8')\n-        )\n-        req = self.assert_followed_url(\n-            resp1.css('a')[0],\n-            'http://example.com/foo?%D0%BF%D1%80%D0%B8%D0%B2%D0%B5%D1%82',\n-            response=resp1,\n-        )\n-        self.assertEqual(req.encoding, 'utf8')\n-\n-        resp2 = self.response_class(\n-            'http://example.com',\n-            encoding='cp1251',\n-            body='<html><body><a href=\"foo?привет\">click me</a></body></html>'.encode('cp1251')\n-        )\n-        req = self.assert_followed_url(\n-            resp2.css('a')[0],\n-            'http://example.com/foo?%EF%F0%E8%E2%E5%F2',\n-            response=resp2,\n-        )\n-        self.assertEqual(req.encoding, 'cp1251')\n-\n \n class XmlResponseTest(TextResponseTest):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#160da6abab8954906181ce69593ff6e84d950ac1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 535 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -411,7 +411,7 @@ class TextResponseTest(BaseResponseTest):\n \n     def test_follow_selector_list(self):\n         resp = self._links_response()\n-        self.assertRaisesRegex(ValueError, 'SelectorList',\n+        self.assertRaisesRegexp(ValueError, 'SelectorList',\n                                 resp.follow, resp.css('a'))\n \n     def test_follow_selector_attribute(self):\n@@ -435,7 +435,7 @@ class TextResponseTest(BaseResponseTest):\n         resp1 = self.response_class(\n             'http://example.com',\n             encoding='utf8',\n-            body='<html><body><a href=\"foo?привет\">click me</a></body></html>'.encode('utf8')\n+            body=u'<html><body><a href=\"foo?привет\">click me</a></body></html>'.encode('utf8')\n         )\n         req = self._assert_followed_url(\n             resp1.css('a')[0],\n@@ -447,7 +447,7 @@ class TextResponseTest(BaseResponseTest):\n         resp2 = self.response_class(\n             'http://example.com',\n             encoding='cp1251',\n-            body='<html><body><a href=\"foo?привет\">click me</a></body></html>'.encode('cp1251')\n+            body=u'<html><body><a href=\"foo?привет\">click me</a></body></html>'.encode('cp1251')\n         )\n         req = self._assert_followed_url(\n             resp2.css('a')[0],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5b79c6a679b66868c89302a1693e5dedc62b6f61", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 14 | Files Changed: 2 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 29 | Churn Cumulative: 754 | Contributors (this commit): 8 | Commits (past 90d): 8 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -109,13 +109,14 @@ class Response(object_ref):\n                dont_filter=False, errback=None):\n         # type: (...) -> Request\n         \"\"\"\n-        Return a scrapy.Request instance to follow a link ``url``.\n+        Return a :class:`~.Request` instance to follow a link ``url``.\n+        It accepts the same arguments as ``Request.__init__`` method,\n+        but ``url`` can be a relative URL or a ``scrapy.link.Link`` object,\n+        not only an absolute URL.\n         \n-        ``url`` can be:\n-\n-        * absolute URL;\n-        * relative URL;\n-        * scrapy.link.Link object.\n+        :class:`~.TextResponse` provides a :meth:`~.TextResponse.follow` \n+        method which supports selectors in addition to absolute/relative URLs\n+        and Link objects.\n         \"\"\"\n         if isinstance(url, Link):\n             url = url.url\n\n@@ -13,7 +13,6 @@ from w3lib.encoding import html_to_unicode, resolve_encoding, \\\n     html_body_declared_encoding, http_content_type_encoding\n from w3lib.html import strip_html5_whitespace\n \n-from scrapy.link import Link\n from scrapy.http.request import Request\n from scrapy.http.response import Response\n from scrapy.utils.response import get_base_url\n@@ -127,18 +126,19 @@ class TextResponse(Response):\n                dont_filter=False, errback=None):\n         # type: (...) -> Request\n         \"\"\"\n-        Return a scrapy.Request instance to follow a link ``url``.\n+        Return a :class:`~.Request` instance to follow a link ``url``.\n+        It accepts the same arguments as ``Request.__init__`` method,\n+        but ``url`` can be not only an absolute URL, but also\n         \n-        ``url`` can be:\n-\n-        * absolute URL;\n-        * relative URL;\n-        * scrapy.link.Link object (e.g. a link extractor result);\n-        * attribute Selector (not SelectorList) - e.g.\n+        * a relative URL;\n+        * a scrapy.link.Link object (e.g. a link extractor result);\n+        * an attribute Selector (not SelectorList) - e.g.\n           ``response.css('a::attr(href)')[0]`` or\n           ``response.xpath('//img/@src')[0]``.\n         * a Selector for ``<a>`` element, e.g.\n           ``response.css('a.my_link')[0]``.\n+          \n+        See :ref:`response-follow-example` for usage examples.\n         \"\"\"\n         if isinstance(url, parsel.Selector):\n             url = _url_from_selector(url)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fade5763af3d03f076f3317589038201bbdeccaf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 13 | Churn Cumulative: 548 | Contributors (this commit): 6 | Commits (past 90d): 5 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -414,11 +414,24 @@ class TextResponseTest(BaseResponseTest):\n         self.assertRaisesRegexp(ValueError, 'SelectorList',\n                                 resp.follow, resp.css('a'))\n \n+    def test_follow_selector_invalid(self):\n+        resp = self._links_response()\n+        self.assertRaisesRegexp(ValueError, 'Unsupported',\n+                                resp.follow, resp.xpath('count(//div)')[0])\n+\n     def test_follow_selector_attribute(self):\n         resp = self._links_response()\n         for src in resp.css('img::attr(src)'):\n             self._assert_followed_url(src, 'http://example.com/sample2.jpg')\n \n+    def test_follow_selector_no_href(self):\n+        resp = self.response_class(\n+            url='http://example.com',\n+            body=b'<html><body><a name=123>click me</a></body></html>',\n+        )\n+        self.assertRaisesRegexp(ValueError, 'no href',\n+                                resp.follow, resp.css('a')[0])\n+\n     def test_follow_whitespace_selector(self):\n         resp = self.response_class(\n             'http://example.com',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#074caf434e255bc96f106e57e3e288028f372485", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 8 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 18 | Churn Cumulative: 1381 | Contributors (this commit): 20 | Commits (past 90d): 2 | Contributors (cumulative): 27 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,10 +5,13 @@ This module implements the FormRequest class which is a more convenient class\n See documentation in docs/topics/request-response.rst\n \"\"\"\n \n+import six\n from six.moves.urllib.parse import urljoin, urlencode\n+\n import lxml.html\n from parsel.selector import create_root_node\n-import six\n+from w3lib.html import strip_html5_whitespace\n+\n from scrapy.http.request import Request\n from scrapy.utils.python import to_bytes, is_listlike\n from scrapy.utils.response import get_base_url\n@@ -51,7 +54,10 @@ class FormRequest(Request):\n \n def _get_form_url(form, url):\n     if url is None:\n-        return urljoin(form.base_url, form.action)\n+        action = form.get('action')\n+        if action is None:\n+            return form.base_url\n+        return urljoin(form.base_url, strip_html5_whitespace(action))\n     return urljoin(form.base_url, url)\n \n \n\n@@ -556,7 +556,6 @@ class FormRequestTest(RequestTest):\n         fs = _qs(req, to_unicode=True, encoding='latin1')\n         self.assertTrue(fs[u'price in \\u00a5'])\n \n-\n     def test_from_response_multiple_forms_clickdata(self):\n         response = _buildresponse(\n             \"\"\"<form name=\"form1\">\n@@ -1002,6 +1001,11 @@ class FormRequestTest(RequestTest):\n         req = self.request_class.from_response(response)\n         self.assertEqual(req.url, 'http://b.com/test_form')\n \n+    def test_spaces_in_action(self):\n+        resp = _buildresponse('<body><form action=\" path\\n\"></form></body>')\n+        req = self.request_class.from_response(resp)\n+        self.assertEqual(req.url, 'http://example.com/path')\n+\n     def test_from_response_css(self):\n         response = _buildresponse(\n             \"\"\"<form action=\"post.php\" method=\"POST\">\n@@ -1023,12 +1027,14 @@ class FormRequestTest(RequestTest):\n         self.assertRaises(ValueError, self.request_class.from_response,\n                           response, formcss=\"input[name='abc']\")\n \n+\n def _buildresponse(body, **kwargs):\n     kwargs.setdefault('body', body)\n     kwargs.setdefault('url', 'http://example.com')\n     kwargs.setdefault('encoding', 'utf-8')\n     return HtmlResponse(**kwargs)\n \n+\n def _qs(req, encoding='utf-8', to_unicode=False):\n     if req.method == 'POST':\n         qs = req.body\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ad36a4a6ae8376a779f9feb08adfb2ca4a59dbb4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 22 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -10,9 +10,10 @@ linkre = re.compile(\n         \"<a\\s.*?href=(\\\"[.#]+?\\\"|\\'[.#]+?\\'|[^\\s]+?)(>|\\s.*?>)(.*?)<[/ ]?a>\",\n         re.DOTALL | re.IGNORECASE)\n \n+\n def clean_link(link_text):\n     \"\"\"Remove leading and trailing whitespace and punctuation\"\"\"\n-    return link_text.strip(\"\\t\\r\\n '\\\"\")\n+    return link_text.strip(\"\\t\\r\\n '\\\"\\x0c\")\n \n \n class RegexLinkExtractor(SgmlLinkExtractor):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d079e15fe2fcf269d040ac435e2eb414d2b4c334", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 52 | Lines Deleted: 13 | Files Changed: 6 | Hunks: 26 | Methods Changed: 15 | Complexity Δ (Sum/Max): 4/1 | Churn Δ: 65 | Churn Cumulative: 1717 | Contributors (this commit): 22 | Commits (past 90d): 6 | Contributors (cumulative): 40 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,7 +1,6 @@\n \"\"\"\n HTMLParser-based link extractor\n \"\"\"\n-\n import warnings\n import six\n from six.moves.html_parser import HTMLParser\n@@ -11,12 +10,14 @@ from w3lib.url import safe_url_string\n \n from scrapy.link import Link\n from scrapy.utils.python import unique as unique_list\n+from scrapy.utils.url import trim_href_attribute\n from scrapy.exceptions import ScrapyDeprecationWarning\n \n \n class HtmlParserLinkExtractor(HTMLParser):\n \n-    def __init__(self, tag=\"a\", attr=\"href\", process=None, unique=False):\n+    def __init__(self, tag=\"a\", attr=\"href\", process=None, unique=False,\n+                 strip=True):\n         HTMLParser.__init__(self)\n \n         warnings.warn(\n@@ -29,6 +30,7 @@ class HtmlParserLinkExtractor(HTMLParser):\n         self.scan_attr = attr if callable(attr) else lambda a: a == attr\n         self.process_attr = process if callable(process) else lambda v: v\n         self.unique = unique\n+        self.strip = strip\n \n     def _extract_links(self, response_text, response_url, response_encoding):\n         self.reset()\n@@ -70,6 +72,8 @@ class HtmlParserLinkExtractor(HTMLParser):\n             for attr, value in attrs:\n                 if self.scan_attr(attr):\n                     url = self.process_attr(value)\n+                    if self.strip:\n+                        url = trim_href_attribute(url)\n                     link = Link(url=url)\n                     self.links.append(link)\n                     self.current_link = link\n\n@@ -9,8 +9,9 @@ import lxml.etree as etree\n from scrapy.link import Link\n from scrapy.utils.misc import arg_to_iter, rel_has_nofollow\n from scrapy.utils.python import unique as unique_list, to_native_str\n-from scrapy.linkextractors import FilteringLinkExtractor\n from scrapy.utils.response import get_base_url\n+from scrapy.utils.url import trim_href_attribute\n+from scrapy.linkextractors import FilteringLinkExtractor\n \n \n # from lxml/src/lxml/html/__init__.py\n@@ -27,11 +28,13 @@ def _nons(tag):\n \n \n class LxmlParserLinkExtractor(object):\n-    def __init__(self, tag=\"a\", attr=\"href\", process=None, unique=False):\n+    def __init__(self, tag=\"a\", attr=\"href\", process=None, unique=False,\n+                 strip=True):\n         self.scan_tag = tag if callable(tag) else lambda t: t == tag\n         self.scan_attr = attr if callable(attr) else lambda a: a == attr\n         self.process_attr = process if callable(process) else lambda v: v\n         self.unique = unique\n+        self.strip = strip\n \n     def _iter_links(self, document):\n         for el in document.iter(etree.Element):\n@@ -49,6 +52,8 @@ class LxmlParserLinkExtractor(object):\n         for el, attr, attr_val in self._iter_links(selector.root):\n             # pseudo lxml.html.HtmlElement.make_links_absolute(base_url)\n             try:\n+                if self.strip:\n+                    attr_val = trim_href_attribute(attr_val)\n                 attr_val = urljoin(base_url, attr_val)\n             except ValueError:\n                 continue  # skipping bogus links\n@@ -85,12 +90,13 @@ class LxmlLinkExtractor(FilteringLinkExtractor):\n \n     def __init__(self, allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(),\n                  tags=('a', 'area'), attrs=('href',), canonicalize=True,\n-                 unique=True, process_value=None, deny_extensions=None, restrict_css=()):\n+                 unique=True, process_value=None, deny_extensions=None, restrict_css=(),\n+                 strip=True):\n         tags, attrs = set(arg_to_iter(tags)), set(arg_to_iter(attrs))\n         tag_func = lambda x: x in tags\n         attr_func = lambda x: x in attrs\n         lx = LxmlParserLinkExtractor(tag=tag_func, attr=attr_func,\n-            unique=unique, process=process_value)\n+            unique=unique, process=process_value, strip=strip)\n \n         super(LxmlLinkExtractor, self).__init__(lx, allow=allow, deny=deny,\n             allow_domains=allow_domains, deny_domains=deny_domains,\n\n@@ -7,18 +7,19 @@ import warnings\n from sgmllib import SGMLParser\n \n from w3lib.url import safe_url_string\n-from scrapy.selector import Selector\n from scrapy.link import Link\n from scrapy.linkextractors import FilteringLinkExtractor\n from scrapy.utils.misc import arg_to_iter, rel_has_nofollow\n from scrapy.utils.python import unique as unique_list, to_unicode\n from scrapy.utils.response import get_base_url\n+from scrapy.utils.url import trim_href_attribute\n from scrapy.exceptions import ScrapyDeprecationWarning\n \n \n class BaseSgmlLinkExtractor(SGMLParser):\n \n-    def __init__(self, tag=\"a\", attr=\"href\", unique=False, process_value=None):\n+    def __init__(self, tag=\"a\", attr=\"href\", unique=False, process_value=None,\n+                 strip=True):\n         warnings.warn(\n             \"BaseSgmlLinkExtractor is deprecated and will be removed in future releases. \"\n             \"Please use scrapy.linkextractors.LinkExtractor\",\n@@ -30,6 +31,7 @@ class BaseSgmlLinkExtractor(SGMLParser):\n         self.process_value = (lambda v: v) if process_value is None else process_value\n         self.current_link = None\n         self.unique = unique\n+        self.strip = strip\n \n     def _extract_links(self, response_text, response_url, response_encoding, base_url=None):\n         \"\"\" Do the real extraction work \"\"\"\n@@ -81,6 +83,8 @@ class BaseSgmlLinkExtractor(SGMLParser):\n                 if self.scan_attr(attr):\n                     url = self.process_value(value)\n                     if url is not None:\n+                        if self.strip:\n+                            url = trim_href_attribute(url)\n                         link = Link(url=url, nofollow=rel_has_nofollow(dict(attrs).get('rel')))\n                         self.links.append(link)\n                         self.current_link = link\n@@ -103,7 +107,8 @@ class SgmlLinkExtractor(FilteringLinkExtractor):\n \n     def __init__(self, allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(),\n                  tags=('a', 'area'), attrs=('href',), canonicalize=True, unique=True,\n-                 process_value=None, deny_extensions=None, restrict_css=()):\n+                 process_value=None, deny_extensions=None, restrict_css=(),\n+                 strip=True):\n \n         warnings.warn(\n             \"SgmlLinkExtractor is deprecated and will be removed in future releases. \"\n@@ -118,7 +123,7 @@ class SgmlLinkExtractor(FilteringLinkExtractor):\n         with warnings.catch_warnings():\n             warnings.simplefilter('ignore', ScrapyDeprecationWarning)\n             lx = BaseSgmlLinkExtractor(tag=tag_func, attr=attr_func,\n-                unique=unique, process_value=process_value)\n+                unique=unique, process_value=process_value, strip=strip)\n \n         super(SgmlLinkExtractor, self).__init__(lx, allow=allow, deny=deny,\n             allow_domains=allow_domains, deny_domains=deny_domains,\n\n@@ -103,3 +103,16 @@ def guess_scheme(url):\n         return any_to_uri(url)\n     else:\n         return add_http_if_no_scheme(url)\n+\n+\n+def trim_href_attribute(href):\n+    \"\"\"\n+    Process href attribute of ``a`` or ``area`` elements according to HTML5\n+    standards (strip all leading and trailing whitespaces). References:\n+\n+    * https://www.w3.org/TR/html5/links.html#links-created-by-a-and-area-elements\n+    * https://www.w3.org/TR/html5/infrastructure.html#valid-url-potentially-surrounded-by-spaces\n+    * https://www.w3.org/TR/html5/infrastructure.html#strip-leading-and-trailing-whitespace\n+    * https://www.w3.org/TR/html5/infrastructure.html#space-character\n+    \"\"\"\n+    return href.strip(' \\t\\n\\r\\x0c')\n\n@@ -32,6 +32,7 @@ class Base:\n                 Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n                 Link(url='http://www.google.com/something', text=u''),\n                 Link(url='http://example.com/innertag.html', text=u'inner tag'),\n+                Link(url='http://example.com/page%204.html', text=u'href with whitespaces'),\n             ])\n \n         def test_extract_filter_allow(self):\n@@ -281,6 +282,7 @@ class Base:\n                 Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n                 Link(url='http://www.google.com/something', text=u''),\n                 Link(url='http://example.com/innertag.html', text=u'inner tag'),\n+                Link(url='http://example.com/page%204.html', text=u'href with whitespaces'),\n             ])\n \n             lx = self.extractor_cls(attrs=(\"href\",\"src\"), tags=(\"a\",\"area\",\"img\"), deny_extensions=())\n@@ -291,6 +293,7 @@ class Base:\n                 Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n                 Link(url='http://www.google.com/something', text=u''),\n                 Link(url='http://example.com/innertag.html', text=u'inner tag'),\n+                Link(url='http://example.com/page%204.html', text=u'href with whitespaces'),\n             ])\n \n             lx = self.extractor_cls(attrs=None)\n\n@@ -117,12 +117,14 @@ class HtmlParserLinkExtractorTestCase(unittest.TestCase):\n     def test_extraction(self):\n         # Default arguments\n         lx = HtmlParserLinkExtractor()\n-        self.assertEqual(lx.extract_links(self.response),\n-                         [Link(url='http://example.com/sample2.html', text=u'sample 2'),\n+        self.assertEqual(lx.extract_links(self.response), [\n+            Link(url='http://example.com/sample2.html', text=u'sample 2'),\n             Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n             Link(url='http://example.com/sample3.html', text=u'sample 3 repetition'),\n             Link(url='http://www.google.com/something', text=u''),\n-                          Link(url='http://example.com/innertag.html', text=u'inner tag'),])\n+            Link(url='http://example.com/innertag.html', text=u'inner tag'),\n+            Link(url='http://example.com/page%204.html', text=u'href with whitespaces'),\n+        ])\n \n     def test_link_wrong_href(self):\n         html = \"\"\"\n@@ -220,3 +222,9 @@ class RegexLinkExtractorTestCase(unittest.TestCase):\n         self.assertEqual([link for link in lx.extract_links(response)], [\n             Link(url='http://b.com/test.html', text=u'', nofollow=False),\n         ])\n+\n+    @unittest.expectedFailure\n+    def test_extraction(self):\n+        # RegexLinkExtractor doesn't parse URLs with leading/trailing\n+        # whitespaces correctly.\n+        super(RegexLinkExtractorTestCase, self).test_extraction()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
