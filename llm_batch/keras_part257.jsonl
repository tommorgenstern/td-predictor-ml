{"custom_id": "keras#bb9b800ebba8914b69db362cfac4e7c8a9b17a9e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 31 | Churn Cumulative: 10630 | Contributors (this commit): 94 | Commits (past 90d): 10 | Contributors (cumulative): 116 | DMM Complexity: 0.6111111111111112\n\nDIFF:\n@@ -207,9 +207,17 @@ class Model(Network):\n                 for name in self.output_names:\n                     tmp_target_tensors.append(target_tensors.get(name, None))\n                 target_tensors = tmp_target_tensors\n+            elif K.is_tensor(target_tensors):\n+                if len(self.outputs) != 1:\n+                    raise ValueError('The model has ' + str(len(self.outputs)) +\n+                                     ' outputs, but you passed a single tensor as '\n+                                     '`target_tensors`. Expected a list or a dict '\n+                                     'of tensors.')\n+                target_tensors = [target_tensors]\n             else:\n-                raise TypeError('Expected `target_tensors` to be '\n-                                'a list or dict, but got:', target_tensors)\n+                raise TypeError('Expected `target_tensors` to be a tensor, '\n+                                'a list of tensors, or dict of tensors, but got:', target_tensors)\n+\n         for i in range(len(self.outputs)):\n             if i in skip_target_indices:\n                 self.targets.append(None)\n\n@@ -310,6 +310,7 @@ def test_model_methods():\n         def gen_data():\n             while True:\n                 yield (np.asarray([]), np.asarray([]))\n+\n         out = model.evaluate_generator(gen_data(), steps=1)\n \n     # x is not a list of numpy arrays.\n@@ -436,6 +437,7 @@ def test_model_methods():\n         def gen_data():\n             while True:\n                 yield (np.asarray([]), np.asarray([]))\n+\n         out = model.fit_generator(generator=gen_data(), epochs=5,\n                                   initial_epoch=0, validation_data=gen_data(),\n                                   callbacks=[tracker_cb])\n@@ -448,6 +450,7 @@ def test_model_methods():\n             gen_counters[i] += 1\n             yield ([np.random.random((1, 3)), np.random.random((1, 3))],\n                    [np.random.random((1, 4)), np.random.random((1, 3))])\n+\n     out = model.fit_generator(generator=gen_data(0), epochs=3,\n                               steps_per_epoch=2,\n                               validation_data=gen_data(1),\n@@ -1008,6 +1011,11 @@ def test_target_tensors():\n                   target_tensors={'dense': target})\n     model.train_on_batch(input_val, None)\n \n+    # single-output, as tensor\n+    model.compile(optimizer='rmsprop', loss='mse',\n+                  target_tensors=target)\n+    model.train_on_batch(input_val, None)\n+\n     # test invalid arguments\n     with pytest.raises(TypeError):\n         model.compile(optimizer='rmsprop', loss='mse',\n@@ -1044,6 +1052,16 @@ def test_target_tensors():\n                                   'dense_b': target_b})\n     model.train_on_batch(input_val, None)\n \n+    # multi-output, not enough target tensors when `target_tensors` is not a dict\n+    with pytest.raises(ValueError, match='When passing a list as `target_tensors`, it should have one entry per model '\n+                                         'output. The model has \\d outputs, but you passed target_tensors='):\n+        model.compile(optimizer='rmsprop', loss='mse',\n+                      target_tensors=[target_a])\n+    with pytest.raises(ValueError, match='The model has \\d outputs, but you passed a single tensor as '\n+                                         '`target_tensors`. Expected a list or a dict of tensors.'):\n+        model.compile(optimizer='rmsprop', loss='mse',\n+                      target_tensors=target_a)\n+\n     # test with sample weights\n     model.compile(optimizer='rmsprop', loss='mse',\n                   target_tensors=[target_a, target_b])\n@@ -1371,6 +1389,7 @@ def test_model_with_crossentropy_losses_channels_first():\n     `channels_first` or `channels_last` image_data_format.\n     Tests PR #9715.\n     \"\"\"\n+\n     def prepare_simple_model(input_tensor, loss_name, target):\n         axis = 1 if K.image_data_format() == 'channels_first' else -1\n         if loss_name == 'sparse_categorical_crossentropy':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cf7d3d259f93b1ddbc601208280a460ee3379591", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 4 | Churn Cumulative: 3070 | Contributors (this commit): 75 | Commits (past 90d): 4 | Contributors (cumulative): 75 | DMM Complexity: 1.0\n\nDIFF:\n@@ -637,6 +637,10 @@ class LearningRateScheduler(Callback):\n             print('\\nEpoch %05d: LearningRateScheduler setting learning '\n                   'rate to %s.' % (epoch + 1, lr))\n \n+    def on_epoch_end(self, epoch, logs=None):\n+        logs = logs or {}\n+        logs['lr'] = K.get_value(self.model.optimizer.lr)\n+\n \n class TensorBoard(Callback):\n     \"\"\"TensorBoard basic visualizations.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e6817ea2cacff41f2b11b5c148a8f220288fcf82", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 199 | Lines Deleted: 106 | Files Changed: 8 | Hunks: 66 | Methods Changed: 30 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 305 | Churn Cumulative: 4871 | Contributors (this commit): 36 | Commits (past 90d): 14 | Contributors (cumulative): 61 | DMM Complexity: 1.0\n\nDIFF:\n@@ -140,16 +140,16 @@ def test_convolutional_recurrent():\n                        input_shape=inputs.shape)\n \n             # check state initialization\n-            layer = convolutional_recurrent.ConvLSTM2D(filters=filters,\n-                                                       kernel_size=(num_row, num_col),\n-                                                       data_format=data_format,\n-                                                       return_sequences=return_sequences)\n+            layer = convolutional_recurrent.ConvLSTM2D(\n+                filters=filters, kernel_size=(num_row, num_col),\n+                data_format=data_format, return_sequences=return_sequences)\n             layer.build(inputs.shape)\n             x = Input(batch_shape=inputs.shape)\n             initial_state = layer.get_initial_state(x)\n             y = layer(x, initial_state=initial_state)\n             model = Model(x, y)\n-            assert model.predict(inputs).shape == layer.compute_output_shape(inputs.shape)\n+            assert (model.predict(inputs).shape ==\n+                    layer.compute_output_shape(inputs.shape))\n \n \n if __name__ == '__main__':\n\n@@ -60,7 +60,8 @@ def test_causal_dilated_conv():\n                    'kernel_initializer': 'ones',\n                    'use_bias': False,\n                },\n-               expected_output=np.float32([[[0], [1], [2], [4], [6], [9], [12], [15], [18], [21]]])\n+               expected_output=np.float32(\n+                   [[[0], [1], [2], [4], [6], [9], [12], [15], [18], [21]]])\n                )\n \n \n@@ -179,9 +180,8 @@ def test_convolution_2d():\n \n     # Test invalid use case\n     with pytest.raises(ValueError):\n-        model = Sequential([convolutional.Conv2D(filters=filters,\n-                                                 kernel_size=kernel_size,\n-                                                 padding=padding,\n+        model = Sequential([convolutional.Conv2D(\n+            filters=filters, kernel_size=kernel_size, padding=padding,\n             batch_input_shape=(None, None, 5, None))])\n \n \n@@ -301,9 +301,8 @@ def test_separable_conv_1d():\n \n     # Test invalid use case\n     with pytest.raises(ValueError):\n-        model = Sequential([convolutional.SeparableConv1D(filters=filters,\n-                                                          kernel_size=3,\n-                                                          padding=padding,\n+        model = Sequential([convolutional.SeparableConv1D(\n+            filters=filters, kernel_size=3, padding=padding,\n             batch_input_shape=(None, 5, None))])\n \n \n@@ -328,7 +327,8 @@ def test_separable_conv_2d():\n                     if dilation_rate != (1, 1) and K.backend() == 'cntk':\n                         continue\n \n-                    layer_test(convolutional.SeparableConv2D,\n+                    layer_test(\n+                        convolutional.SeparableConv2D,\n                         kwargs={'filters': filters,\n                                 'kernel_size': (3, 3),\n                                 'padding': padding,\n@@ -355,9 +355,8 @@ def test_separable_conv_2d():\n \n     # Test invalid use case\n     with pytest.raises(ValueError):\n-        model = Sequential([convolutional.SeparableConv2D(filters=filters,\n-                                                          kernel_size=3,\n-                                                          padding=padding,\n+        model = Sequential([convolutional.SeparableConv2D(\n+            filters=filters, kernel_size=3, padding=padding,\n             batch_input_shape=(None, None, 5, None))])\n \n \n@@ -534,7 +533,8 @@ def test_conv3d_transpose():\n                         continue\n                     if strides == (1, 1, 1) and out_padding == (1, 1, 1):\n                         continue\n-                    layer_test(convolutional.Conv3DTranspose,\n+                    layer_test(\n+                        convolutional.Conv3DTranspose,\n                         kwargs={'filters': filters,\n                                 'kernel_size': 3,\n                                 'padding': padding,\n@@ -739,7 +739,8 @@ def test_zero_padding_3d():\n                    kwargs={'padding': (2, 2, 2), 'data_format': data_format},\n                    input_shape=inputs.shape)\n         layer_test(convolutional.ZeroPadding3D,\n-                   kwargs={'padding': ((1, 2), (3, 4), (0, 2)), 'data_format': data_format},\n+                   kwargs={'padding': ((1, 2), (3, 4), (0, 2)),\n+                           'data_format': data_format},\n                    input_shape=inputs.shape)\n \n         # correctness test\n@@ -1036,5 +1037,6 @@ def test_cropping_3d():\n     with pytest.raises(ValueError):\n         layer = convolutional.Cropping3D(cropping=lambda x: x)\n \n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n@@ -44,14 +44,18 @@ def test_dropout():\n                 input_shape = (2,) + shape + (3,)\n             else:\n                 input_shape = (2, 3) + shape\n-            layer_test(layers.SpatialDropout2D if len(shape) == 2 else layers.SpatialDropout3D,\n+            if len(shape) == 2:\n+                layer = layers.SpatialDropout2D\n+            else:\n+                layer = layers.SpatialDropout3D\n+            layer_test(layer,\n                        kwargs={'rate': 0.5,\n                                'data_format': data_format},\n                        input_shape=input_shape)\n \n             # Test invalid use cases\n             with pytest.raises(ValueError):\n-                layer_test(layers.SpatialDropout2D if len(shape) == 2 else layers.SpatialDropout3D,\n+                layer_test(layer,\n                            kwargs={'rate': 0.5,\n                                    'data_format': 'channels_middle'},\n                            input_shape=input_shape)\n@@ -362,7 +366,8 @@ def test_sequential_as_downstream_of_masking_layer():\n               np.random.random((10, 3, 5)), epochs=1, batch_size=6)\n \n     mask_outputs = [model.layers[1].compute_mask(model.layers[1].input)]\n-    mask_outputs += [model.layers[2].compute_mask(model.layers[2].input, mask_outputs[-1])]\n+    mask_outputs += [model.layers[2].compute_mask(model.layers[2].input,\n+                                                  mask_outputs[-1])]\n     func = K.function([model.input], mask_outputs)\n     mask_outputs_val = func([model_input])\n     assert np.array_equal(mask_outputs_val[0], np.any(model_input, axis=-1))\n\n@@ -9,7 +9,8 @@ import time\n \n \n skipif_no_tf_gpu = pytest.mark.skipif(\n-    (K.backend() != 'tensorflow') or (not K.tensorflow_backend._get_available_gpus()),\n+    ((K.backend() != 'tensorflow') or\n+     (not K.tensorflow_backend._get_available_gpus())),\n     reason='Requires TensorFlow backend and a GPU')\n \n \n\n@@ -22,7 +22,8 @@ def test_embedding():\n                input_dtype='int32',\n                expected_output_dtype=K.floatx())\n     layer_test(Embedding,\n-               kwargs={'output_dim': 4, 'input_dim': 10, 'mask_zero': True, 'input_length': (None, 5)},\n+               kwargs={'output_dim': 4, 'input_dim': 10, 'mask_zero': True,\n+                       'input_length': (None, 5)},\n                input_shape=(3, 2, 5),\n                input_dtype='int32',\n                expected_output_dtype=K.floatx())\n\n@@ -66,7 +66,8 @@ def test_batchnorm_correctness_1d():\n @keras_test\n def test_batchnorm_correctness_2d():\n     model = Sequential()\n-    norm = normalization.BatchNormalization(axis=1, input_shape=(10, 6), momentum=0.8)\n+    norm = normalization.BatchNormalization(axis=1, input_shape=(10, 6),\n+                                            momentum=0.8)\n     model.add(norm)\n     model.compile(loss='mse', optimizer='rmsprop')\n \n@@ -123,7 +124,8 @@ def test_batchnorm_mode_twice():\n @keras_test\n def test_batchnorm_convnet():\n     model = Sequential()\n-    norm = normalization.BatchNormalization(axis=1, input_shape=(3, 4, 4), momentum=0.8)\n+    norm = normalization.BatchNormalization(axis=1, input_shape=(3, 4, 4),\n+                                            momentum=0.8)\n     model.add(norm)\n     model.compile(loss='mse', optimizer='sgd')\n \n@@ -243,5 +245,6 @@ def test_batchnorm_trainable():\n     out = model.predict(input_4)\n     assert_allclose((input_4 - np.mean(input_4)) / np.std(input_4), out, atol=1e-3)\n \n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n@@ -163,7 +163,8 @@ def test_TimeDistributed_with_masked_embedding_and_unspecified_shape():\n     # test with unspecified shape and Embeddings with mask_zero\n     model = Sequential()\n     model.add(wrappers.TimeDistributed(layers.Embedding(5, 6, mask_zero=True),\n-                                       input_shape=(None, None)))  # N by t_1 by t_2 by 6\n+                                       input_shape=(None, None)))\n+    # the shape so far: (N, t_1, t_2, 6)\n     model.add(wrappers.TimeDistributed(layers.SimpleRNN(7, return_sequences=True)))\n     model.add(wrappers.TimeDistributed(layers.SimpleRNN(8, return_sequences=False)))\n     model.add(layers.SimpleRNN(1, return_sequences=False))\n@@ -202,7 +203,8 @@ def test_TimeDistributed_with_masking_layer():\n     model.fit(model_input,\n               np.random.random((10, 3, 5)), epochs=1, batch_size=6)\n     mask_outputs = [model.layers[0].compute_mask(model.input)]\n-    mask_outputs += [model.layers[1].compute_mask(model.layers[1].input, mask_outputs[-1])]\n+    mask_outputs += [model.layers[1].compute_mask(model.layers[1].input,\n+                                                  mask_outputs[-1])]\n     func = K.function([model.input], mask_outputs)\n     mask_outputs_val = func([model_input])\n     assert np.array_equal(mask_outputs_val[0], np.any(model_input, axis=-1))\n@@ -332,10 +334,12 @@ def test_Bidirectional_merged_value(merge_mode):\n \n     # basic case\n     inputs = Input((timesteps, dim))\n-    layer = wrappers.Bidirectional(rnn(units, return_sequences=True), merge_mode=merge_mode)\n+    layer = wrappers.Bidirectional(rnn(units, return_sequences=True),\n+                                   merge_mode=merge_mode)\n     f_merged = K.function([inputs], to_list(layer(inputs)))\n     f_forward = K.function([inputs], [layer.forward_layer.call(inputs)])\n-    f_backward = K.function([inputs], [K.reverse(layer.backward_layer.call(inputs), 1)])\n+    f_backward = K.function([inputs],\n+                            [K.reverse(layer.backward_layer.call(inputs), 1)])\n \n     y_merged = f_merged(X)\n     y_expected = to_list(merge_func(f_forward(X)[0], f_backward(X)[0]))\n@@ -345,7 +349,8 @@ def test_Bidirectional_merged_value(merge_mode):\n \n     # test return_state\n     inputs = Input((timesteps, dim))\n-    layer = wrappers.Bidirectional(rnn(units, return_state=True), merge_mode=merge_mode)\n+    layer = wrappers.Bidirectional(rnn(units, return_state=True),\n+                                   merge_mode=merge_mode)\n     f_merged = K.function([inputs], layer(inputs))\n     f_forward = K.function([inputs], layer.forward_layer.call(inputs))\n     f_backward = K.function([inputs], layer.backward_layer.call(inputs))\n@@ -407,7 +412,8 @@ def test_Bidirectional_state_reuse():\n     units = 3\n \n     input1 = Input((timesteps, dim))\n-    layer = wrappers.Bidirectional(rnn(units, return_state=True, return_sequences=True))\n+    layer = wrappers.Bidirectional(rnn(units, return_state=True,\n+                                       return_sequences=True))\n     state = layer(input1)[1:]\n \n     # test passing invalid initial_state: passing a tensor\n@@ -561,7 +567,8 @@ def test_Bidirectional_with_constants_layer_passing_initial_state():\n     model = Model([x, s_for, s_bac, c], y)\n     model.compile(optimizer='rmsprop', loss='mse')\n     model.train_on_batch(\n-        [np.zeros((6, 5, 5)), np.zeros((6, 32)), np.zeros((6, 32)), np.zeros((6, 3))],\n+        [np.zeros((6, 5, 5)), np.zeros((6, 32)),\n+         np.zeros((6, 32)), np.zeros((6, 3))],\n         np.zeros((6, 64))\n     )\n \n\n@@ -32,10 +32,10 @@ def test_dense_legacy_interface():\n @keras_test\n def test_dropout_legacy_interface():\n     old_layer = keras.layers.Dropout(p=3, name='drop')\n-    new_layer_1 = keras.layers.Dropout(rate=3, name='drop')\n-    new_layer_2 = keras.layers.Dropout(3, name='drop')\n-    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer_1.get_config())\n-    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer_2.get_config())\n+    new_layer1 = keras.layers.Dropout(rate=3, name='drop')\n+    new_layer2 = keras.layers.Dropout(3, name='drop')\n+    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer1.get_config())\n+    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer2.get_config())\n \n \n @keras_test\n@@ -109,7 +109,8 @@ def test_lstm_legacy_interface():\n     new_layer = keras.layers.LSTM(2, input_shape=[3, 5], name='d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n-    old_layer = keras.layers.LSTM(input_shape=[3, 5], output_dim=2, name='d', consume_less='mem')\n+    old_layer = keras.layers.LSTM(input_shape=[3, 5], output_dim=2, name='d',\n+                                  consume_less='mem')\n     new_layer = keras.layers.LSTM(2, input_shape=[3, 5], name='d', implementation=1)\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n@@ -120,10 +121,12 @@ def test_lstm_legacy_interface():\n \n     old_layer = keras.layers.LSTM(input_dim=5,\n                                   output_dim=2, name='d', consume_less='mem')\n-    new_layer = keras.layers.LSTM(2, input_shape=[None, 5], name='d', implementation=1)\n+    new_layer = keras.layers.LSTM(2, input_shape=[None, 5], name='d',\n+                                  implementation=1)\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n-    old_layer = keras.layers.LSTM(input_shape=[3, 5], output_dim=2, name='d', consume_less='gpu')\n+    old_layer = keras.layers.LSTM(input_shape=[3, 5], output_dim=2, name='d',\n+                                  consume_less='gpu')\n     new_layer = keras.layers.LSTM(2, input_shape=[3, 5], name='d', implementation=2)\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n@@ -231,160 +234,227 @@ def test_gru_legacy_interface():\n @keras_test\n def test_gaussiandropout_legacy_interface():\n     old_layer = keras.layers.GaussianDropout(p=0.6, name='drop')\n-    new_layer_1 = keras.layers.GaussianDropout(rate=0.6, name='drop')\n-    new_layer_2 = keras.layers.GaussianDropout(0.6, name='drop')\n-    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer_1.get_config())\n-    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer_2.get_config())\n+    new_layer1 = keras.layers.GaussianDropout(rate=0.6, name='drop')\n+    new_layer2 = keras.layers.GaussianDropout(0.6, name='drop')\n+    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer1.get_config())\n+    assert json.dumps(old_layer.get_config()) == json.dumps(new_layer2.get_config())\n \n \n @keras_test\n def test_maxpooling2d_legacy_interface():\n-    old_layer = keras.layers.MaxPooling2D(pool_size=(2, 2), border_mode='valid', name='maxpool2d')\n-    new_layer = keras.layers.MaxPool2D(pool_size=2, padding='valid', name='maxpool2d')\n+    old_layer = keras.layers.MaxPooling2D(\n+        pool_size=(2, 2), border_mode='valid', name='maxpool2d')\n+    new_layer = keras.layers.MaxPool2D(\n+        pool_size=2, padding='valid', name='maxpool2d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n     old_layer = keras.layers.MaxPooling2D((2, 2), 2, 'valid', name='maxpool2d')\n-    new_layer = keras.layers.MaxPool2D(pool_size=2, strides=2, padding='valid', name='maxpool2d')\n+    new_layer = keras.layers.MaxPool2D(\n+        pool_size=2, strides=2, padding='valid', name='maxpool2d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n-    old_layer = keras.layers.MaxPooling2D((2, 2), padding='valid', dim_ordering='tf', name='maxpool2d')\n-    new_layer = keras.layers.MaxPool2D(pool_size=2, padding='valid', data_format='channels_last', name='maxpool2d')\n+    old_layer = keras.layers.MaxPooling2D(\n+        (2, 2), padding='valid', dim_ordering='tf', name='maxpool2d')\n+    new_layer = keras.layers.MaxPool2D(\n+        pool_size=2, padding='valid', data_format='channels_last', name='maxpool2d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n-    old_layer = keras.layers.MaxPooling2D((2, 2), padding='valid', dim_ordering='th', name='maxpool2d')\n-    new_layer = keras.layers.MaxPool2D(pool_size=2, padding='valid', data_format='channels_first', name='maxpool2d')\n+    old_layer = keras.layers.MaxPooling2D(\n+        (2, 2), padding='valid', dim_ordering='th', name='maxpool2d')\n+    new_layer = keras.layers.MaxPool2D(\n+        pool_size=2, padding='valid', data_format='channels_first',\n+        name='maxpool2d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n-    old_layer = keras.layers.MaxPooling2D((2, 2), padding='valid', dim_ordering='default', name='maxpool2d')\n-    new_layer = keras.layers.MaxPool2D(pool_size=2, padding='valid', name='maxpool2d')\n+    old_layer = keras.layers.MaxPooling2D(\n+        (2, 2), padding='valid', dim_ordering='default', name='maxpool2d')\n+    new_layer = keras.layers.MaxPool2D(\n+        pool_size=2, padding='valid', name='maxpool2d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n @keras_test\n def test_avgpooling2d_legacy_interface():\n-    old_layer = keras.layers.AveragePooling2D(pool_size=(2, 2), border_mode='valid', name='avgpooling2d')\n-    new_layer = keras.layers.AvgPool2D(pool_size=(2, 2), padding='valid', name='avgpooling2d')\n+    old_layer = keras.layers.AveragePooling2D(\n+        pool_size=(2, 2), border_mode='valid', name='avgpooling2d')\n+    new_layer = keras.layers.AvgPool2D(\n+        pool_size=(2, 2), padding='valid', name='avgpooling2d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n-    old_layer = keras.layers.AveragePooling2D((2, 2), (2, 2), 'valid', name='avgpooling2d')\n-    new_layer = keras.layers.AvgPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid', name='avgpooling2d')\n+    old_layer = keras.layers.AveragePooling2D(\n+        (2, 2), (2, 2), 'valid', name='avgpooling2d')\n+    new_layer = keras.layers.AvgPool2D(\n+        pool_size=(2, 2), strides=(2, 2), padding='valid', name='avgpooling2d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n-    old_layer = keras.layers.AveragePooling2D((2, 2), padding='valid', dim_ordering='tf', name='avgpooling2d')\n-    new_layer = keras.layers.AvgPool2D(pool_size=2, padding='valid', data_format='channels_last', name='avgpooling2d')\n+    old_layer = keras.layers.AveragePooling2D(\n+        (2, 2), padding='valid', dim_ordering='tf', name='avgpooling2d')\n+    new_layer = keras.layers.AvgPool2D(\n+        pool_size=2, padding='valid', data_format='channels_last',\n+        name='avgpooling2d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n-    old_layer = keras.layers.AveragePooling2D((2, 2), padding='valid', dim_ordering='th', name='avgpooling2d')\n-    new_layer = keras.layers.AvgPool2D(pool_size=2, padding='valid', data_format='channels_first', name='avgpooling2d')\n+    old_layer = keras.layers.AveragePooling2D(\n+        (2, 2), padding='valid', dim_ordering='th', name='avgpooling2d')\n+    new_layer = keras.layers.AvgPool2D(\n+        pool_size=2, padding='valid', data_format='channels_first',\n+        name='avgpooling2d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n-    old_layer = keras.layers.AveragePooling2D((2, 2), padding='valid', dim_ordering='default', name='avgpooling2d')\n-    new_layer = keras.layers.AvgPool2D(pool_size=2, padding='valid', name='avgpooling2d')\n+    old_layer = keras.layers.AveragePooling2D(\n+        (2, 2), padding='valid', dim_ordering='default', name='avgpooling2d')\n+    new_layer = keras.layers.AvgPool2D(\n+        pool_size=2, padding='valid', name='avgpooling2d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n @keras_test\n def test_maxpooling3d_legacy_interface():\n-    old_layer = keras.layers.MaxPooling3D(pool_size=(2, 2, 2), border_mode='valid', name='maxpool3d')\n-    new_layer = keras.layers.MaxPool3D(pool_size=(2, 2, 2), padding='valid', name='maxpool3d')\n+    old_layer = keras.layers.MaxPooling3D(\n+        pool_size=(2, 2, 2), border_mode='valid', name='maxpool3d')\n+    new_layer = keras.layers.MaxPool3D(\n+        pool_size=(2, 2, 2), padding='valid', name='maxpool3d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n-    old_layer = keras.layers.MaxPooling3D((2, 2, 2), (2, 2, 2), 'valid', name='maxpool3d')\n-    new_layer = keras.layers.MaxPool3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='valid', name='maxpool3d')\n+    old_layer = keras.layers.MaxPooling3D(\n+        (2, 2, 2), (2, 2, 2), 'valid', name='maxpool3d')\n+    new_layer = keras.layers.MaxPool3D(\n+        pool_size=(2, 2, 2), strides=(2, 2, 2), padding='valid', name='maxpool3d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n-    old_layer = keras.layers.MaxPooling3D((2, 2, 2), padding='valid', dim_ordering='tf', name='maxpool3d')\n-    new_layer = keras.layers.MaxPool3D(pool_size=(2, 2, 2), padding='valid', data_format='channels_last', name='maxpool3d')\n+    old_layer = keras.layers.MaxPooling3D(\n+        (2, 2, 2), padding='valid', dim_ordering='tf', name='maxpool3d')\n+    new_layer = keras.layers.MaxPool3D(\n+        pool_size=(2, 2, 2), padding='valid', data_format='channels_last',\n+        name='maxpool3d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n-    old_layer = keras.layers.MaxPooling3D((2, 2, 2), padding='valid', dim_ordering='th', name='maxpool3d')\n-    new_layer = keras.layers.MaxPool3D(pool_size=(2, 2, 2), padding='valid', data_format='channels_first', name='maxpool3d')\n+    old_layer = keras.layers.MaxPooling3D(\n+        (2, 2, 2), padding='valid', dim_ordering='th', name='maxpool3d')\n+    new_layer = keras.layers.MaxPool3D(\n+        pool_size=(2, 2, 2), padding='valid', data_format='channels_first',\n+        name='maxpool3d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n-    old_layer = keras.layers.MaxPooling3D((2, 2, 2), padding='valid', dim_ordering='default', name='maxpool3d')\n-    new_layer = keras.layers.MaxPool3D(pool_size=(2, 2, 2), padding='valid', name='maxpool3d')\n+    old_layer = keras.layers.MaxPooling3D(\n+        (2, 2, 2), padding='valid', dim_ordering='default', name='maxpool3d')\n+    new_layer = keras.layers.MaxPool3D(\n+        pool_size=(2, 2, 2), padding='valid', name='maxpool3d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n @keras_test\n def test_avgpooling3d_legacy_interface():\n-    old_layer = keras.layers.AveragePooling3D(pool_size=(2, 2, 2), border_mode='valid', name='avgpooling3d')\n-    new_layer = keras.layers.AvgPool3D(pool_size=(2, 2, 2), padding='valid', name='avgpooling3d')\n+    old_layer = keras.layers.AveragePooling3D(\n+        pool_size=(2, 2, 2), border_mode='valid', name='avgpooling3d')\n+    new_layer = keras.layers.AvgPool3D(\n+        pool_size=(2, 2, 2), padding='valid', name='avgpooling3d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n-    old_layer = keras.layers.AveragePooling3D((2, 2, 2), (2, 2, 2), 'valid', name='avgpooling3d')\n-    new_layer = keras.layers.AvgPool3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='valid', name='avgpooling3d')\n+    old_layer = keras.layers.AveragePooling3D(\n+        (2, 2, 2), (2, 2, 2), 'valid', name='avgpooling3d')\n+    new_layer = keras.layers.AvgPool3D(\n+        pool_size=(2, 2, 2), strides=(2, 2, 2), padding='valid',\n+        name='avgpooling3d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n-    old_layer = keras.layers.AveragePooling3D((2, 2, 2), padding='valid', dim_ordering='tf', name='avgpooling3d')\n-    new_layer = keras.layers.AvgPool3D(pool_size=(2, 2, 2), padding='valid', data_format='channels_last', name='avgpooling3d')\n+    old_layer = keras.layers.AveragePooling3D(\n+        (2, 2, 2), padding='valid', dim_ordering='tf', name='avgpooling3d')\n+    new_layer = keras.layers.AvgPool3D(\n+        pool_size=(2, 2, 2), padding='valid', data_format='channels_last',\n+        name='avgpooling3d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n-    old_layer = keras.layers.AveragePooling3D((2, 2, 2), padding='valid', dim_ordering='th', name='avgpooling3d')\n-    new_layer = keras.layers.AvgPool3D(pool_size=(2, 2, 2), padding='valid', data_format='channels_first', name='avgpooling3d')\n+    old_layer = keras.layers.AveragePooling3D(\n+        (2, 2, 2), padding='valid', dim_ordering='th', name='avgpooling3d')\n+    new_layer = keras.layers.AvgPool3D(\n+        pool_size=(2, 2, 2), padding='valid', data_format='channels_first',\n+        name='avgpooling3d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n-    old_layer = keras.layers.AveragePooling3D((2, 2, 2), padding='valid', dim_ordering='default', name='avgpooling3d')\n-    new_layer = keras.layers.AvgPool3D(pool_size=(2, 2, 2), padding='valid', name='avgpooling3d')\n+    old_layer = keras.layers.AveragePooling3D(\n+        (2, 2, 2), padding='valid', dim_ordering='default', name='avgpooling3d')\n+    new_layer = keras.layers.AvgPool3D(\n+        pool_size=(2, 2, 2), padding='valid', name='avgpooling3d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n @keras_test\n def test_global_maxpooling2d_legacy_interface():\n-    old_layer = keras.layers.GlobalMaxPooling2D(dim_ordering='tf', name='global_maxpool2d')\n-    new_layer = keras.layers.GlobalMaxPool2D(data_format='channels_last', name='global_maxpool2d')\n+    old_layer = keras.layers.GlobalMaxPooling2D(dim_ordering='tf',\n+                                                name='global_maxpool2d')\n+    new_layer = keras.layers.GlobalMaxPool2D(data_format='channels_last',\n+                                             name='global_maxpool2d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n-    old_layer = keras.layers.GlobalMaxPooling2D(dim_ordering='th', name='global_maxpool2d')\n-    new_layer = keras.layers.GlobalMaxPool2D(data_format='channels_first', name='global_maxpool2d')\n+    old_layer = keras.layers.GlobalMaxPooling2D(dim_ordering='th',\n+                                                name='global_maxpool2d')\n+    new_layer = keras.layers.GlobalMaxPool2D(data_format='channels_first',\n+                                             name='global_maxpool2d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n-    old_layer = keras.layers.GlobalMaxPooling2D(dim_ordering='default', name='global_maxpool2d')\n+    old_layer = keras.layers.GlobalMaxPooling2D(dim_ordering='default',\n+                                                name='global_maxpool2d')\n     new_layer = keras.layers.GlobalMaxPool2D(name='global_maxpool2d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n @keras_test\n def test_global_avgpooling2d_legacy_interface():\n-    old_layer = keras.layers.GlobalAveragePooling2D(dim_ordering='tf', name='global_avgpool2d')\n-    new_layer = keras.layers.GlobalAvgPool2D(data_format='channels_last', name='global_avgpool2d')\n+    old_layer = keras.layers.GlobalAveragePooling2D(dim_ordering='tf',\n+                                                    name='global_avgpool2d')\n+    new_layer = keras.layers.GlobalAvgPool2D(data_format='channels_last',\n+                                             name='global_avgpool2d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n-    old_layer = keras.layers.GlobalAveragePooling2D(dim_ordering='th', name='global_avgpool2d')\n-    new_layer = keras.layers.GlobalAvgPool2D(data_format='channels_first', name='global_avgpool2d')\n+    old_layer = keras.layers.GlobalAveragePooling2D(dim_ordering='th',\n+                                                    name='global_avgpool2d')\n+    new_layer = keras.layers.GlobalAvgPool2D(data_format='channels_first',\n+                                             name='global_avgpool2d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n-    old_layer = keras.layers.GlobalAveragePooling2D(dim_ordering='default', name='global_avgpool2d')\n+    old_layer = keras.layers.GlobalAveragePooling2D(dim_ordering='default',\n+                                                    name='global_avgpool2d')\n     new_layer = keras.layers.GlobalAvgPool2D(name='global_avgpool2d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n @keras_test\n def test_global_maxpooling3d_legacy_interface():\n-    old_layer = keras.layers.GlobalMaxPooling3D(dim_ordering='tf', name='global_maxpool3d')\n-    new_layer = keras.layers.GlobalMaxPool3D(data_format='channels_last', name='global_maxpool3d')\n+    old_layer = keras.layers.GlobalMaxPooling3D(dim_ordering='tf',\n+                                                name='global_maxpool3d')\n+    new_layer = keras.layers.GlobalMaxPool3D(data_format='channels_last',\n+                                             name='global_maxpool3d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n-    old_layer = keras.layers.GlobalMaxPooling3D(dim_ordering='th', name='global_maxpool3d')\n-    new_layer = keras.layers.GlobalMaxPool3D(data_format='channels_first', name='global_maxpool3d')\n+    old_layer = keras.layers.GlobalMaxPooling3D(dim_ordering='th',\n+                                                name='global_maxpool3d')\n+    new_layer = keras.layers.GlobalMaxPool3D(data_format='channels_first',\n+                                             name='global_maxpool3d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n-    old_layer = keras.layers.GlobalMaxPooling3D(dim_ordering='default', name='global_maxpool3d')\n+    old_layer = keras.layers.GlobalMaxPooling3D(dim_ordering='default',\n+                                                name='global_maxpool3d')\n     new_layer = keras.layers.GlobalMaxPool3D(name='global_maxpool3d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n @keras_test\n def test_global_avgpooling3d_legacy_interface():\n-    old_layer = keras.layers.GlobalAveragePooling3D(dim_ordering='tf', name='global_avgpool3d')\n-    new_layer = keras.layers.GlobalAvgPool3D(data_format='channels_last', name='global_avgpool3d')\n+    old_layer = keras.layers.GlobalAveragePooling3D(dim_ordering='tf',\n+                                                    name='global_avgpool3d')\n+    new_layer = keras.layers.GlobalAvgPool3D(data_format='channels_last',\n+                                             name='global_avgpool3d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n-    old_layer = keras.layers.GlobalAveragePooling3D(dim_ordering='th', name='global_avgpool3d')\n-    new_layer = keras.layers.GlobalAvgPool3D(data_format='channels_first', name='global_avgpool3d')\n+    old_layer = keras.layers.GlobalAveragePooling3D(dim_ordering='th',\n+                                                    name='global_avgpool3d')\n+    new_layer = keras.layers.GlobalAvgPool3D(data_format='channels_first',\n+                                             name='global_avgpool3d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n-    old_layer = keras.layers.GlobalAveragePooling3D(dim_ordering='default', name='global_avgpool3d')\n+    old_layer = keras.layers.GlobalAveragePooling3D(dim_ordering='default',\n+                                                    name='global_avgpool3d')\n     new_layer = keras.layers.GlobalAvgPool3D(name='global_avgpool3d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n@@ -401,7 +471,8 @@ def test_upsampling1d_legacy_interface():\n @keras_test\n def test_upsampling2d_legacy_interface():\n     old_layer = keras.layers.UpSampling2D((2, 2), dim_ordering='tf', name='us2d')\n-    new_layer = keras.layers.UpSampling2D((2, 2), data_format='channels_last', name='us2d')\n+    new_layer = keras.layers.UpSampling2D((2, 2), data_format='channels_last',\n+                                          name='us2d')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n@@ -459,15 +530,18 @@ def test_deconv2d_legacy_interface():\n     new_layer = keras.layers.Conv2DTranspose(5, (3, 3), name='deconv')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n-    old_layer = keras.layers.Deconvolution2D(5, 3, 3, output_shape=(6, 7, 5), name='deconv')\n+    old_layer = keras.layers.Deconvolution2D(5, 3, 3, output_shape=(6, 7, 5),\n+                                             name='deconv')\n     new_layer = keras.layers.Conv2DTranspose(5, (3, 3), name='deconv')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n-    old_layer = keras.layers.Deconvolution2D(5, 3, nb_col=3, output_shape=(6, 7, 5), name='deconv')\n+    old_layer = keras.layers.Deconvolution2D(5, 3, nb_col=3, output_shape=(6, 7, 5),\n+                                             name='deconv')\n     new_layer = keras.layers.Conv2DTranspose(5, (3, 3), name='deconv')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n-    old_layer = keras.layers.Deconvolution2D(5, nb_row=3, nb_col=3, output_shape=(6, 7, 5), name='deconv')\n+    old_layer = keras.layers.Deconvolution2D(5, nb_row=3, nb_col=3,\n+                                             output_shape=(6, 7, 5), name='deconv')\n     new_layer = keras.layers.Conv2DTranspose(5, (3, 3), name='deconv')\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#67fc091c6daf70bd91717e59799c3e014509ca0c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 1123 | Contributors (this commit): 10 | Commits (past 90d): 1 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -7,10 +7,11 @@ from keras.models import Sequential\n from keras.layers.core import Dense\n from keras.utils.test_utils import keras_test\n from keras.utils import Sequence\n+from keras import backend as K\n \n STEPS_PER_EPOCH = 100\n STEPS = 100\n-WORKERS = 4\n+WORKERS = 4 if K.backend() != 'tensorflow' else 2\n \n \n class DummySequence(Sequence):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b2f75b7a3d1da174a4859b1c26e2faab99041d29", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 2196 | Contributors (this commit): 22 | Commits (past 90d): 11 | Contributors (cumulative): 26 | DMM Complexity: 1.0\n\nDIFF:\n@@ -45,7 +45,7 @@ def save_model(model, filepath, overwrite=True, include_optimizer=True):\n         model: Keras model instance to be saved.\n         filepath: one of the following:\n             - string, path where to save the model, or\n-            - h5py.File object where to save the model\n+            - h5py.File or h5py.Group object where to save the model\n         overwrite: Whether we should overwrite any existing\n             model at the target location, or instead\n             ask the user with a manual prompt.\n@@ -95,7 +95,7 @@ def save_model(model, filepath, overwrite=True, include_optimizer=True):\n \n     from .. import __version__ as keras_version\n \n-    if not isinstance(filepath, h5py.File):\n+    if not isinstance(filepath, h5py.Group):\n         # If file exists and should not be overwritten.\n         if not overwrite and os.path.isfile(filepath):\n             proceed = ask_to_proceed_with_overwrite(filepath)\n@@ -182,7 +182,7 @@ def save_model(model, filepath, overwrite=True, include_optimizer=True):\n                             param_dset[()] = val\n                         else:\n                             param_dset[:] = val\n-        f.flush()\n+        f.file.flush()\n     finally:\n         if opened_new_file:\n             f.close()\n@@ -194,7 +194,7 @@ def load_model(filepath, custom_objects=None, compile=True):\n     # Arguments\n         filepath: one of the following:\n             - string, path to the saved model, or\n-            - h5py.File object from which to load the model\n+            - h5py.File or h5py.Group object from which to load the model\n         custom_objects: Optional dictionary mapping names\n             (strings) to custom classes or functions to be\n             considered during deserialization.\n@@ -244,7 +244,7 @@ def load_model(filepath, custom_objects=None, compile=True):\n             return custom_objects[obj]\n         return obj\n \n-    opened_new_file = not isinstance(filepath, h5py.File)\n+    opened_new_file = not isinstance(filepath, h5py.Group)\n     if opened_new_file:\n         f = h5py.File(filepath, mode='r')\n     else:\n\n@@ -141,6 +141,13 @@ def test_model_saving_to_pre_created_h5py_file():\n         out2 = loaded_model.predict(x)\n     assert_allclose(out, out2, atol=1e-05)\n \n+    with h5py.File(fname, mode='r+') as h5file:\n+        g = h5file.create_group('model')\n+        save_model(model, g)\n+        loaded_model = load_model(g)\n+        out2 = loaded_model.predict(x)\n+    assert_allclose(out, out2, atol=1e-05)\n+\n \n @keras_test\n def test_model_saving_to_binary_stream():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0151028441d30525a0a553eb7fab4b08269e8266", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 5 | Churn Cumulative: 635 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -6,6 +6,7 @@ from __future__ import print_function\n \n import copy\n import numpy as np\n+import warnings\n \n from .. import backend as K\n from .. import losses\n@@ -476,6 +477,10 @@ def standardize_weights(y,\n                              'sample-wise weights, make sure your '\n                              'sample_weight array is 1D.')\n \n+    if sample_weight is not None and class_weight is not None:\n+        warnings.warn('Found both `sample_weight` and `class_weight`: '\n+                      '`class_weight` argument will be ignored.')\n+\n     if sample_weight is not None:\n         if len(sample_weight.shape) > len(y.shape):\n             raise ValueError('Found a sample_weight with shape' +\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#695a928224bb8d5efc6483fce1c40dae65dc7bbd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 5 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 32 | Churn Cumulative: 2849 | Contributors (this commit): 25 | Commits (past 90d): 8 | Contributors (cumulative): 28 | DMM Complexity: 1.0\n\nDIFF:\n@@ -462,9 +462,24 @@ class GlobalAveragePooling1D(_GlobalPooling1D):\n         `(batch_size, features)`\n     \"\"\"\n \n-    def call(self, inputs):\n+    def __init__(self, **kwargs):\n+        super(GlobalAveragePooling1D, self).__init__(**kwargs)\n+        self.supports_masking = True\n+\n+    def call(self, inputs, mask=None):\n+        if mask is not None:\n+            mask = K.cast(mask, K.floatx())\n+            input_shape = K.int_shape(inputs)\n+            broadcast_shape = [-1, input_shape[1], 1]\n+            mask = K.reshape(mask, broadcast_shape)\n+            inputs *= mask\n+            return K.sum(inputs, axis=1) / K.sum(mask, axis=1)\n+        else:\n             return K.mean(inputs, axis=1)\n \n+    def compute_mask(self, inputs, mask=None):\n+        return None\n+\n \n class GlobalMaxPooling1D(_GlobalPooling1D):\n     \"\"\"Global max pooling operation for temporal data.\n\n@@ -7,6 +7,7 @@ from keras.utils.test_utils import keras_test\n from keras import backend as K\n from keras.layers import convolutional\n from keras.layers import pooling\n+from keras.layers import Masking\n from keras.models import Sequential\n \n \n@@ -413,6 +414,20 @@ def test_globalpooling_1d():\n                input_shape=(3, 4, 5))\n \n \n+@keras_test\n+def test_globalpooling_1d_supports_masking():\n+    # Test GlobalAveragePooling1D supports masking\n+    model = Sequential()\n+    model.add(Masking(mask_value=0., input_shape=(3, 4)))\n+    model.add(pooling.GlobalAveragePooling1D())\n+    model.compile(loss='mae', optimizer='adam')\n+\n+    model_input = np.random.randint(low=1, high=5, size=(2, 3, 4))\n+    model_input[0, 1:, :] = 0\n+    output = model.predict(model_input)\n+    assert np.array_equal(output[0], model_input[0, 0, :])\n+\n+\n @keras_test\n def test_globalpooling_2d():\n     layer_test(pooling.GlobalMaxPooling2D,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c7b7328cc99fd5d7c298e57c6020043451d89a61", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 46 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 16/15 | Churn Δ: 50 | Churn Cumulative: 6448 | Contributors (this commit): 91 | Commits (past 90d): 10 | Contributors (cumulative): 96 | DMM Complexity: 0.16216216216216217\n\nDIFF:\n@@ -613,7 +613,26 @@ def any(x, axis=None, keepdims=False):\n def all(x, axis=None, keepdims=False):\n     \"\"\"Bitwise reduction (logical AND).\n     \"\"\"\n-    return T.all(x, axis=axis, keepdims=keepdims)\n+    y = T.all(x, axis=axis, keepdims=keepdims)\n+    if hasattr(x, '_keras_shape'):\n+        if axis is None:\n+            y._keras_shape = (1,) * len(x._keras_shape) if keepdims else (1,)\n+        else:\n+            if isinstance(axis, int):\n+                axis_list = [axis]\n+            else:\n+                axis_list = list(set(int(a) for a in axis))\n+            keras_shape_list = list(x._keras_shape)\n+            if keepdims:\n+                for a in axis_list:\n+                    keras_shape_list[a] = 1\n+            else:\n+                for a in axis_list[::-1]:\n+                    keras_shape_list.pop(a)\n+                if not keras_shape_list:\n+                    keras_shape_list = (1,)\n+            y._keras_shape = tuple(keras_shape_list)\n+    return y\n \n \n def argmax(x, axis=-1):\n@@ -885,13 +904,25 @@ def concatenate(tensors, axis=-1):\n     if py_all([is_sparse(x) for x in tensors]):\n         axis = axis % ndim(tensors[0])\n         if axis == 0:\n-            return th_sparse_module.basic.vstack(tensors, format='csr')\n+            output = th_sparse_module.basic.vstack(tensors, format='csr')\n         elif axis == 1:\n-            return th_sparse_module.basic.hstack(tensors, format='csr')\n+            output = th_sparse_module.basic.hstack(tensors, format='csr')\n         else:\n             raise ValueError('Invalid concat axis for sparse matrix:', axis)\n     else:\n-        return T.concatenate([to_dense(x) for x in tensors], axis=axis)\n+        output = T.concatenate([to_dense(x) for x in tensors], axis=axis)\n+\n+    if py_all([hasattr(tensor, '_keras_shape') for tensor in tensors]):\n+        input_shapes = [tensor._keras_shape for tensor in tensors]\n+        output_shape = list(input_shapes[0])\n+        for shape in input_shapes[1:]:\n+            if output_shape[axis] is None or shape[axis] is None:\n+                output_shape[axis] = None\n+                break\n+            output_shape[axis] += shape[axis]\n+        output._keras_shape = tuple(output_shape)\n+\n+    return output\n \n \n def reshape(x, shape):\n\n@@ -288,5 +288,16 @@ def test_merge_broadcast():\n         K.ndim = k_ndim\n \n \n+@keras_test\n+def test_masking_concatenate():\n+    input1 = layers.Input(shape=(6,))\n+    input2 = layers.Input(shape=(6,))\n+    x1 = layers.Embedding(10, 5, input_length=6, mask_zero=True)(input1)\n+    x2 = layers.Embedding(10, 5, input_length=6, mask_zero=True)(input2)\n+    x = layers.concatenate([x1, x2])\n+    x = layers.wrappers.TimeDistributed(layers.Dense(3, activation='softmax'))(x)\n+    models.Model(inputs=[input1, input2], outputs=[x])\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
