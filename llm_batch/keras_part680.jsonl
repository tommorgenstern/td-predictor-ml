{"custom_id": "keras#8f2def124f4270e0e3b6020be00a10a92173f2cf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 48 | Churn Cumulative: 318 | Contributors (this commit): 6 | Commits (past 90d): 15 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,37 @@\n+import torch\n+\n+from keras_core import ops\n+from keras_core import optimizers\n+from keras_core.backend.torch.optimizers import torch_parallel_optimizer\n+\n+\n+class Adagrad(\n+    torch_parallel_optimizer.TorchParallelOptimizer, optimizers.Adagrad\n+):\n+    def _parallel_update_step(\n+        self,\n+        grads,\n+        variables,\n+        learning_rate,\n+    ):\n+        keras_variables = variables\n+        variables = [v.value for v in variables]\n+\n+        dtype = variables[0].dtype\n+        lr = ops.cast(learning_rate, dtype)\n+\n+        accumulators = [\n+            self._accumulators[self._get_variable_index(variable)].value\n+            for variable in keras_variables\n+        ]\n+        torch._foreach_add_(accumulators, torch._foreach_mul(grads, grads))\n+        torch._foreach_add_(\n+            variables,\n+            torch._foreach_div(\n+                torch._foreach_mul(grads, lr),\n+                torch._foreach_sqrt(\n+                    torch._foreach_add(accumulators, self.epsilon)\n+                ),\n+            ),\n+            alpha=-1,\n+        )\n\n@@ -8,6 +8,7 @@ class TorchOptimizer(BaseOptimizer):\n     def __new__(cls, *args, **kwargs):\n         # Import locally to avoid circular imports.\n         from keras_core.backend.torch.optimizers import torch_adadelta\n+        from keras_core.backend.torch.optimizers import torch_adagrad\n         from keras_core.backend.torch.optimizers import torch_adam\n         from keras_core.backend.torch.optimizers import torch_adamw\n         from keras_core.backend.torch.optimizers import torch_rmsprop\n@@ -15,6 +16,7 @@ class TorchOptimizer(BaseOptimizer):\n \n         OPTIMIZERS = {\n             optimizers.Adadelta: torch_adadelta.Adadelta,\n+            optimizers.Adagrad: torch_adagrad.Adagrad,\n             optimizers.Adam: torch_adam.Adam,\n             optimizers.AdamW: torch_adamw.AdamW,\n             optimizers.RMSprop: torch_rmsprop.RMSprop,\n\n@@ -4,6 +4,7 @@\n import numpy as np\n \n from keras_core import backend\n+from keras_core import ops\n from keras_core import testing\n from keras_core.optimizers.adagrad import Adagrad\n \n@@ -19,7 +20,7 @@ class AdagradTest(testing.TestCase):\n \n     def test_single_step(self):\n         optimizer = Adagrad(learning_rate=0.5)\n-        grads = np.array([1.0, 6.0, 7.0, 2.0])\n+        grads = ops.array([1.0, 6.0, 7.0, 2.0])\n         vars = backend.Variable([1.0, 2.0, 3.0, 4.0])\n         optimizer.apply_gradients(zip([grads], [vars]))\n         self.assertAllClose(\n@@ -28,7 +29,7 @@ class AdagradTest(testing.TestCase):\n \n     def test_weight_decay(self):\n         grads, var1, var2, var3 = (\n-            np.zeros(()),\n+            ops.zeros(()),\n             backend.Variable(2.0),\n             backend.Variable(2.0, name=\"exclude\"),\n             backend.Variable(2.0),\n@@ -54,8 +55,8 @@ class AdagradTest(testing.TestCase):\n         )\n \n         x = backend.Variable(np.ones([10]))\n-        grads = np.arange(0.1, 1.1, 0.1)\n-        first_grads = np.full((10,), 0.01)\n+        grads = ops.arange(0.1, 1.1, 0.1)\n+        first_grads = ops.full((10,), 0.01)\n \n         # fmt: off\n         golden = np.array(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#60fa86c35d0d5440aaac262667c688ab9ce67aa6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 59 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 63 | Churn Cumulative: 502 | Contributors (this commit): 7 | Commits (past 90d): 18 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,52 @@\n+import torch\n+\n+from keras_core import ops\n+from keras_core import optimizers\n+from keras_core.backend.torch.optimizers import torch_parallel_optimizer\n+\n+\n+class Adamax(\n+    torch_parallel_optimizer.TorchParallelOptimizer, optimizers.Adamax\n+):\n+    def _parallel_update_step(\n+        self,\n+        grads,\n+        variables,\n+        learning_rate,\n+    ):\n+        keras_variables = variables\n+        variables = [v.value for v in variables]\n+\n+        dtype = variables[0].dtype\n+        lr = ops.cast(learning_rate, dtype)\n+\n+        local_step = ops.cast(self.iterations + 1, dtype)\n+\n+        beta_1_power = ops.power(ops.cast(self.beta_1, dtype), local_step)\n+\n+        m_list = [\n+            self._m[self._get_variable_index(variable)].value\n+            for variable in keras_variables\n+        ]\n+        u_list = [\n+            self._u[self._get_variable_index(variable)].value\n+            for variable in keras_variables\n+        ]\n+\n+        torch._foreach_mul_(m_list, self.beta_1)\n+        torch._foreach_add_(m_list, grads, alpha=1 - self.beta_1)\n+\n+        torch._foreach_mul_(u_list, self.beta_2)\n+        torch._foreach_maximum_(u_list, torch._foreach_abs(grads))\n+\n+        torch._foreach_add_(\n+            variables,\n+            torch._foreach_div(\n+                torch._foreach_mul(m_list, lr),\n+                torch._foreach_mul(\n+                    torch._foreach_add(u_list, self.epsilon),\n+                    1 - beta_1_power,\n+                ),\n+            ),\n+            alpha=-1,\n+        )\n\n@@ -10,6 +10,7 @@ class TorchOptimizer(BaseOptimizer):\n         from keras_core.backend.torch.optimizers import torch_adadelta\n         from keras_core.backend.torch.optimizers import torch_adagrad\n         from keras_core.backend.torch.optimizers import torch_adam\n+        from keras_core.backend.torch.optimizers import torch_adamax\n         from keras_core.backend.torch.optimizers import torch_adamw\n         from keras_core.backend.torch.optimizers import torch_rmsprop\n         from keras_core.backend.torch.optimizers import torch_sgd\n@@ -18,6 +19,7 @@ class TorchOptimizer(BaseOptimizer):\n             optimizers.Adadelta: torch_adadelta.Adadelta,\n             optimizers.Adagrad: torch_adagrad.Adagrad,\n             optimizers.Adam: torch_adam.Adam,\n+            optimizers.Adamax: torch_adamax.Adamax,\n             optimizers.AdamW: torch_adamw.AdamW,\n             optimizers.RMSprop: torch_rmsprop.RMSprop,\n             optimizers.SGD: torch_sgd.SGD,\n\n@@ -4,6 +4,7 @@\n import numpy as np\n \n from keras_core import backend\n+from keras_core import ops\n from keras_core import testing\n from keras_core.optimizers.adamax import Adamax\n \n@@ -20,14 +21,14 @@ class AdamaxTest(testing.TestCase):\n \n     def test_single_step(self):\n         optimizer = Adamax(learning_rate=0.5)\n-        grads = np.array([1.0, 6.0, 7.0, 2.0])\n+        grads = ops.array([1.0, 6.0, 7.0, 2.0])\n         vars = backend.Variable([1.0, 2.0, 3.0, 4.0])\n         optimizer.apply_gradients(zip([grads], [vars]))\n         self.assertAllClose(vars, [0.5, 1.5, 2.5, 3.5], rtol=1e-4, atol=1e-4)\n \n     def test_weight_decay(self):\n         grads, var1, var2, var3 = (\n-            np.zeros(()),\n+            ops.zeros(()),\n             backend.Variable(2.0),\n             backend.Variable(2.0, name=\"exclude\"),\n             backend.Variable(2.0),\n@@ -53,8 +54,8 @@ class AdamaxTest(testing.TestCase):\n         )\n \n         x = backend.Variable(np.ones([10]))\n-        grads = np.arange(0.1, 1.1, 0.1)\n-        first_grads = np.full((10,), 0.01)\n+        grads = ops.arange(0.1, 1.1, 0.1)\n+        first_grads = ops.full((10,), 0.01)\n \n         # fmt: off\n         golden = np.array(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#99d4a75f1bed2477c343da74e8005baafa255180", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1185 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 16 | Complexity Δ (Sum/Max): 20/10 | Churn Δ: 1185 | Churn Cumulative: 1185 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,486 @@\n+\"\"\"\n+Title: Speaker Recognition\n+Author: [Fadi Badine](https://twitter.com/fadibadine)\n+Converted to Keras Core by: [Fadi Badine](https://twitter.com/fadibadine)\n+Date created: 14/06/2020\n+Last modified: 19/07/2023\n+Description: Classify speakers using Fast Fourier Transform (FFT) and a 1D Convnet.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+This example demonstrates how to create a model to classify speakers from the\n+frequency domain representation of speech recordings, obtained via Fast Fourier\n+Transform (FFT).\n+\n+It shows the following:\n+\n+- How to use `tf.data` to load, preprocess and feed audio streams into a model\n+- How to create a 1D convolutional network with residual\n+connections for audio classification.\n+\n+Our process:\n+\n+- We prepare a dataset of speech samples from different speakers, with the speaker as label.\n+- We add background noise to these samples to augment our data.\n+- We take the FFT of these samples.\n+- We train a 1D convnet to predict the correct speaker given a noisy FFT speech sample.\n+\n+Note:\n+\n+- This example should be run with TensorFlow 2.3 or higher, or `tf-nightly`.\n+- The noise samples in the dataset need to be resampled to a sampling rate of 16000 Hz\n+before using the code in this example. In order to do this, you will need to have\n+installed `ffmpg`.\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import os\n+os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n+\n+import shutil\n+import numpy as np\n+\n+import tensorflow as tf\n+import keras_core as keras\n+\n+from pathlib import Path\n+from IPython.display import display, Audio\n+\n+# Get the data from https://www.kaggle.com/kongaevans/speaker-recognition-dataset/download\n+# and save it to the 'Downloads' folder in your HOME directory\n+DATASET_ROOT = os.path.join(os.path.expanduser(\"~\"), \"Downloads/16000_pcm_speeches\")\n+\n+# The folders in which we will put the audio samples and the noise samples\n+AUDIO_SUBFOLDER = \"audio\"\n+NOISE_SUBFOLDER = \"noise\"\n+\n+DATASET_AUDIO_PATH = os.path.join(DATASET_ROOT, AUDIO_SUBFOLDER)\n+DATASET_NOISE_PATH = os.path.join(DATASET_ROOT, NOISE_SUBFOLDER)\n+\n+# Percentage of samples to use for validation\n+VALID_SPLIT = 0.1\n+\n+# Seed to use when shuffling the dataset and the noise\n+SHUFFLE_SEED = 43\n+\n+# The sampling rate to use.\n+# This is the one used in all the audio samples.\n+# We will resample all the noise to this sampling rate.\n+# This will also be the output size of the audio wave samples\n+# (since all samples are of 1 second long)\n+SAMPLING_RATE = 16000\n+\n+# The factor to multiply the noise with according to:\n+#   noisy_sample = sample + noise * prop * scale\n+#      where prop = sample_amplitude / noise_amplitude\n+SCALE = 0.5\n+\n+BATCH_SIZE = 128\n+EPOCHS = 1\n+\n+\n+\"\"\"\n+## Data preparation\n+\n+The dataset is composed of 7 folders, divided into 2 groups:\n+\n+- Speech samples, with 5 folders for 5 different speakers. Each folder contains\n+1500 audio files, each 1 second long and sampled at 16000 Hz.\n+- Background noise samples, with 2 folders and a total of 6 files. These files\n+are longer than 1 second (and originally not sampled at 16000 Hz, but we will resample them to 16000 Hz).\n+We will use those 6 files to create 354 1-second-long noise samples to be used for training.\n+\n+Let's sort these 2 categories into 2 folders:\n+\n+- An `audio` folder which will contain all the per-speaker speech sample folders\n+- A `noise` folder which will contain all the noise samples\n+\"\"\"\n+\n+\"\"\"\n+Before sorting the audio and noise categories into 2 folders,\n+we have the following directory structure:\n+\n+```\n+main_directory/\n+...speaker_a/\n+...speaker_b/\n+...speaker_c/\n+...speaker_d/\n+...speaker_e/\n+...other/\n+..._background_noise_/\n+```\n+\n+After sorting, we end up with the following structure:\n+\n+```\n+main_directory/\n+...audio/\n+......speaker_a/\n+......speaker_b/\n+......speaker_c/\n+......speaker_d/\n+......speaker_e/\n+...noise/\n+......other/\n+......_background_noise_/\n+```\n+\"\"\"\n+\n+# If folder `audio`, does not exist, create it, otherwise do nothing\n+if os.path.exists(DATASET_AUDIO_PATH) is False:\n+    os.makedirs(DATASET_AUDIO_PATH)\n+\n+# If folder `noise`, does not exist, create it, otherwise do nothing\n+if os.path.exists(DATASET_NOISE_PATH) is False:\n+    os.makedirs(DATASET_NOISE_PATH)\n+\n+for folder in os.listdir(DATASET_ROOT):\n+    if os.path.isdir(os.path.join(DATASET_ROOT, folder)):\n+        if folder in [AUDIO_SUBFOLDER, NOISE_SUBFOLDER]:\n+            # If folder is `audio` or `noise`, do nothing\n+            continue\n+        elif folder in [\"other\", \"_background_noise_\"]:\n+            # If folder is one of the folders that contains noise samples,\n+            # move it to the `noise` folder\n+            shutil.move(\n+                os.path.join(DATASET_ROOT, folder),\n+                os.path.join(DATASET_NOISE_PATH, folder),\n+            )\n+        else:\n+            # Otherwise, it should be a speaker folder, then move it to\n+            # `audio` folder\n+            shutil.move(\n+                os.path.join(DATASET_ROOT, folder),\n+                os.path.join(DATASET_AUDIO_PATH, folder),\n+            )\n+\n+\"\"\"\n+## Noise preparation\n+\n+In this section:\n+\n+- We load all noise samples (which should have been resampled to 16000)\n+- We split those noise samples to chunks of 16000 samples which\n+correspond to 1 second duration each\n+\"\"\"\n+\n+# Get the list of all noise files\n+noise_paths = []\n+for subdir in os.listdir(DATASET_NOISE_PATH):\n+    subdir_path = Path(DATASET_NOISE_PATH) / subdir\n+    if os.path.isdir(subdir_path):\n+        noise_paths += [\n+            os.path.join(subdir_path, filepath)\n+            for filepath in os.listdir(subdir_path)\n+            if filepath.endswith(\".wav\")\n+        ]\n+\n+print(\n+    \"Found {} files belonging to {} directories\".format(\n+        len(noise_paths), len(os.listdir(DATASET_NOISE_PATH))\n+    )\n+)\n+\n+\"\"\"\n+Resample all noise samples to 16000 Hz\n+\"\"\"\n+\n+command = (\n+    \"for dir in `ls -1 \" + DATASET_NOISE_PATH + \"`; do \"\n+    \"for file in `ls -1 \" + DATASET_NOISE_PATH + \"/$dir/*.wav`; do \"\n+    \"sample_rate=`ffprobe -hide_banner -loglevel panic -show_streams \"\n+    \"$file | grep sample_rate | cut -f2 -d=`; \"\n+    \"if [ $sample_rate -ne 16000 ]; then \"\n+    \"ffmpeg -hide_banner -loglevel panic -y \"\n+    \"-i $file -ar 16000 temp.wav; \"\n+    \"mv temp.wav $file; \"\n+    \"fi; done; done\"\n+)\n+os.system(command)\n+\n+\n+# Split noise into chunks of 16,000 steps each\n+def load_noise_sample(path):\n+    sample, sampling_rate = tf.audio.decode_wav(\n+        tf.io.read_file(path), desired_channels=1\n+    )\n+    if sampling_rate == SAMPLING_RATE:\n+        # Number of slices of 16000 each that can be generated from the noise sample\n+        slices = int(sample.shape[0] / SAMPLING_RATE)\n+        sample = tf.split(sample[: slices * SAMPLING_RATE], slices)\n+        return sample\n+    else:\n+        print(\"Sampling rate for {} is incorrect. Ignoring it\".format(path))\n+        return None\n+\n+\n+noises = []\n+for path in noise_paths:\n+    sample = load_noise_sample(path)\n+    if sample:\n+        noises.extend(sample)\n+noises = tf.stack(noises)\n+\n+print(\n+    \"{} noise files were split into {} noise samples where each is {} sec. long\".format(\n+        len(noise_paths), noises.shape[0], noises.shape[1] // SAMPLING_RATE\n+    )\n+)\n+\n+\"\"\"\n+## Dataset generation\n+\"\"\"\n+\n+\n+def paths_and_labels_to_dataset(audio_paths, labels):\n+    \"\"\"Constructs a dataset of audios and labels.\"\"\"\n+    path_ds = tf.data.Dataset.from_tensor_slices(audio_paths)\n+    audio_ds = path_ds.map(\n+        lambda x: path_to_audio(x), num_parallel_calls=tf.data.AUTOTUNE\n+    )\n+    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n+    return tf.data.Dataset.zip((audio_ds, label_ds))\n+\n+\n+def path_to_audio(path):\n+    \"\"\"Reads and decodes an audio file.\"\"\"\n+    audio = tf.io.read_file(path)\n+    audio, _ = tf.audio.decode_wav(audio, 1, SAMPLING_RATE)\n+    return audio\n+\n+\n+def add_noise(audio, noises=None, scale=0.5):\n+    if noises is not None:\n+        # Create a random tensor of the same size as audio ranging from\n+        # 0 to the number of noise stream samples that we have.\n+        tf_rnd = tf.random.uniform(\n+            (tf.shape(audio)[0],), 0, noises.shape[0], dtype=tf.int32\n+        )\n+        noise = tf.gather(noises, tf_rnd, axis=0)\n+\n+        # Get the amplitude proportion between the audio and the noise\n+        prop = tf.math.reduce_max(audio, axis=1) / tf.math.reduce_max(noise, axis=1)\n+        prop = tf.repeat(tf.expand_dims(prop, axis=1), tf.shape(audio)[1], axis=1)\n+\n+        # Adding the rescaled noise to audio\n+        audio = audio + noise * prop * scale\n+\n+    return audio\n+\n+\n+def audio_to_fft(audio):\n+    # Since tf.signal.fft applies FFT on the innermost dimension,\n+    # we need to squeeze the dimensions and then expand them again\n+    # after FFT\n+    audio = tf.squeeze(audio, axis=-1)\n+    fft = tf.signal.fft(\n+        tf.cast(tf.complex(real=audio, imag=tf.zeros_like(audio)), tf.complex64)\n+    )\n+    fft = tf.expand_dims(fft, axis=-1)\n+\n+    # Return the absolute value of the first half of the FFT\n+    # which represents the positive frequencies\n+    return tf.math.abs(fft[:, : (audio.shape[1] // 2), :])\n+\n+\n+# Get the list of audio file paths along with their corresponding labels\n+\n+class_names = os.listdir(DATASET_AUDIO_PATH)\n+print(\n+    \"Our class names: {}\".format(\n+        class_names,\n+    )\n+)\n+\n+audio_paths = []\n+labels = []\n+for label, name in enumerate(class_names):\n+    print(\n+        \"Processing speaker {}\".format(\n+            name,\n+        )\n+    )\n+    dir_path = Path(DATASET_AUDIO_PATH) / name\n+    speaker_sample_paths = [\n+        os.path.join(dir_path, filepath)\n+        for filepath in os.listdir(dir_path)\n+        if filepath.endswith(\".wav\")\n+    ]\n+    audio_paths += speaker_sample_paths\n+    labels += [label] * len(speaker_sample_paths)\n+\n+print(\n+    \"Found {} files belonging to {} classes.\".format(len(audio_paths), len(class_names))\n+)\n+\n+# Shuffle\n+rng = np.random.RandomState(SHUFFLE_SEED)\n+rng.shuffle(audio_paths)\n+rng = np.random.RandomState(SHUFFLE_SEED)\n+rng.shuffle(labels)\n+\n+# Split into training and validation\n+num_val_samples = int(VALID_SPLIT * len(audio_paths))\n+print(\"Using {} files for training.\".format(len(audio_paths) - num_val_samples))\n+train_audio_paths = audio_paths[:-num_val_samples]\n+train_labels = labels[:-num_val_samples]\n+\n+print(\"Using {} files for validation.\".format(num_val_samples))\n+valid_audio_paths = audio_paths[-num_val_samples:]\n+valid_labels = labels[-num_val_samples:]\n+\n+# Create 2 datasets, one for training and the other for validation\n+train_ds = paths_and_labels_to_dataset(train_audio_paths, train_labels)\n+train_ds = train_ds.shuffle(buffer_size=BATCH_SIZE * 8, seed=SHUFFLE_SEED).batch(\n+    BATCH_SIZE\n+)\n+\n+valid_ds = paths_and_labels_to_dataset(valid_audio_paths, valid_labels)\n+valid_ds = valid_ds.shuffle(buffer_size=32 * 8, seed=SHUFFLE_SEED).batch(32)\n+\n+\n+# Add noise to the training set\n+train_ds = train_ds.map(\n+    lambda x, y: (add_noise(x, noises, scale=SCALE), y),\n+    num_parallel_calls=tf.data.AUTOTUNE,\n+)\n+\n+# Transform audio wave to the frequency domain using `audio_to_fft`\n+train_ds = train_ds.map(\n+    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.AUTOTUNE\n+)\n+train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n+\n+valid_ds = valid_ds.map(\n+    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.AUTOTUNE\n+)\n+valid_ds = valid_ds.prefetch(tf.data.AUTOTUNE)\n+\n+\"\"\"\n+## Model Definition\n+\"\"\"\n+\n+\n+def residual_block(x, filters, conv_num=3, activation=\"relu\"):\n+    # Shortcut\n+    s = keras.layers.Conv1D(filters, 1, padding=\"same\")(x)\n+    for i in range(conv_num - 1):\n+        x = keras.layers.Conv1D(filters, 3, padding=\"same\")(x)\n+        x = keras.layers.Activation(activation)(x)\n+    x = keras.layers.Conv1D(filters, 3, padding=\"same\")(x)\n+    x = keras.layers.Add()([x, s])\n+    x = keras.layers.Activation(activation)(x)\n+    return keras.layers.MaxPool1D(pool_size=2, strides=2)(x)\n+\n+\n+def build_model(input_shape, num_classes):\n+    inputs = keras.layers.Input(shape=input_shape, name=\"input\")\n+\n+    x = residual_block(inputs, 16, 2)\n+    x = residual_block(x, 32, 2)\n+    x = residual_block(x, 64, 3)\n+    x = residual_block(x, 128, 3)\n+    x = residual_block(x, 128, 3)\n+\n+    x = keras.layers.AveragePooling1D(pool_size=3, strides=3)(x)\n+    x = keras.layers.Flatten()(x)\n+    x = keras.layers.Dense(256, activation=\"relu\")(x)\n+    x = keras.layers.Dense(128, activation=\"relu\")(x)\n+\n+    outputs = keras.layers.Dense(num_classes, activation=\"softmax\", name=\"output\")(x)\n+\n+    return keras.models.Model(inputs=inputs, outputs=outputs)\n+\n+\n+model = build_model((SAMPLING_RATE // 2, 1), len(class_names))\n+\n+model.summary()\n+\n+# Compile the model using Adam's default learning rate\n+model.compile(\n+    optimizer=\"Adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n+)\n+\n+# Add callbacks:\n+# 'EarlyStopping' to stop training when the model is not enhancing anymore\n+# 'ModelCheckPoint' to always keep the model that has the best val_accuracy\n+model_save_filename = \"model.keras\"\n+\n+earlystopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n+mdlcheckpoint_cb = keras.callbacks.ModelCheckpoint(\n+    model_save_filename, monitor=\"val_accuracy\", save_best_only=True\n+)\n+\n+\"\"\"\n+## Training\n+\"\"\"\n+\n+history = model.fit(\n+    train_ds,\n+    epochs=EPOCHS,\n+    validation_data=valid_ds,\n+    callbacks=[earlystopping_cb, mdlcheckpoint_cb],\n+)\n+\n+\"\"\"\n+## Evaluation\n+\"\"\"\n+\n+print(model.evaluate(valid_ds))\n+\n+\"\"\"\n+We get ~ 98% validation accuracy.\n+\"\"\"\n+\n+\"\"\"\n+## Demonstration\n+\n+Let's take some samples and:\n+\n+- Predict the speaker\n+- Compare the prediction with the real speaker\n+- Listen to the audio to see that despite the samples being noisy,\n+the model is still pretty accurate\n+\"\"\"\n+\n+SAMPLES_TO_DISPLAY = 10\n+\n+test_ds = paths_and_labels_to_dataset(valid_audio_paths, valid_labels)\n+test_ds = test_ds.shuffle(buffer_size=BATCH_SIZE * 8, seed=SHUFFLE_SEED).batch(\n+    BATCH_SIZE\n+)\n+\n+test_ds = test_ds.map(\n+    lambda x, y: (add_noise(x, noises, scale=SCALE), y),\n+    num_parallel_calls=tf.data.AUTOTUNE,\n+)\n+\n+for audios, labels in test_ds.take(1):\n+    # Get the signal FFT\n+    ffts = audio_to_fft(audios)\n+    # Predict\n+    y_pred = model.predict(ffts)\n+    # Take random samples\n+    rnd = np.random.randint(0, BATCH_SIZE, SAMPLES_TO_DISPLAY)\n+    audios = audios.numpy()[rnd, :, :]\n+    labels = labels.numpy()[rnd]\n+    y_pred = np.argmax(y_pred, axis=-1)[rnd]\n+\n+    for index in range(SAMPLES_TO_DISPLAY):\n+        # For every sample, print the true and predicted label\n+        # as well as run the voice with the noise\n+        print(\n+            \"Speaker:\\33{} {}\\33[0m\\tPredicted:\\33{} {}\\33[0m\".format(\n+                \"[92m\" if labels[index] == y_pred[index] else \"[91m\",\n+                class_names[labels[index]],\n+                \"[92m\" if labels[index] == y_pred[index] else \"[91m\",\n+                class_names[y_pred[index]],\n+            )\n+        )\n+        display(Audio(audios[index, :, :].squeeze(), rate=SAMPLING_RATE))\n\n@@ -0,0 +1,699 @@\n+\"\"\"\n+Title: English speaker accent recognition using Transfer Learning\n+Author: [Fadi Badine](https://twitter.com/fadibadine)\n+Converted to Keras Core by: [Fadi Badine](https://twitter.com/fadibadine)\n+Date created: 2022/04/16\n+Last modified: 2023/07/19\n+Description: Training a model to classify UK & Ireland accents using feature extraction from Yamnet.\n+Accelerator: GPU\n+\"\"\"\n+\n+\"\"\"\n+## Introduction\n+\n+The following example shows how to use feature extraction in order to\n+train a model to classify the English accent spoken in an audio wave.\n+\n+Instead of training a model from scratch, transfer learning enables us to\n+take advantage of existing state-of-the-art deep learning models and use them as feature extractors.\n+\n+Our process:\n+\n+* Use a TF Hub pre-trained model (Yamnet) and apply it as part of the tf.data pipeline which transforms\n+the audio files into feature vectors.\n+* Train a dense model on the feature vectors.\n+* Use the trained model for inference on a new audio file.\n+\n+Note:\n+\n+* We need to install TensorFlow IO in order to resample audio files to 16 kHz as required by Yamnet model.\n+* In the test section, ffmpeg is used to convert the mp3 file to wav.\n+\n+You can install TensorFlow IO with the following command:\n+\"\"\"\n+\n+\"\"\"shell\n+pip install -U -q tensorflow_io\n+\"\"\"\n+\n+\"\"\"\n+## Configuration\n+\"\"\"\n+\n+SEED = 1337\n+EPOCHS = 1\n+BATCH_SIZE = 64\n+VALIDATION_RATIO = 0.1\n+MODEL_NAME = \"uk_irish_accent_recognition\"\n+\n+# Location where the dataset will be downloaded.\n+# By default (None), keras.utils.get_file will use ~/.keras/ as the CACHE_DIR\n+CACHE_DIR = None\n+\n+# The location of the dataset\n+URL_PATH = \"https://www.openslr.org/resources/83/\"\n+\n+# List of datasets compressed files that contain the audio files\n+zip_files = {\n+    0: \"irish_english_male.zip\",\n+    1: \"midlands_english_female.zip\",\n+    2: \"midlands_english_male.zip\",\n+    3: \"northern_english_female.zip\",\n+    4: \"northern_english_male.zip\",\n+    5: \"scottish_english_female.zip\",\n+    6: \"scottish_english_male.zip\",\n+    7: \"southern_english_female.zip\",\n+    8: \"southern_english_male.zip\",\n+    9: \"welsh_english_female.zip\",\n+    10: \"welsh_english_male.zip\",\n+}\n+\n+# We see that there are 2 compressed files for each accent (except Irish):\n+# - One for male speakers\n+# - One for female speakers\n+# However, we will be using a gender agnostic dataset.\n+\n+# List of gender agnostic categories\n+gender_agnostic_categories = [\n+    \"ir\",  # Irish\n+    \"mi\",  # Midlands\n+    \"no\",  # Northern\n+    \"sc\",  # Scottish\n+    \"so\",  # Southern\n+    \"we\",  # Welsh\n+]\n+\n+class_names = [\n+    \"Irish\",\n+    \"Midlands\",\n+    \"Northern\",\n+    \"Scottish\",\n+    \"Southern\",\n+    \"Welsh\",\n+    \"Not a speech\",\n+]\n+\n+\"\"\"\n+## Imports\n+\"\"\"\n+\n+import os\n+os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n+\n+import io\n+import csv\n+import numpy as np\n+import pandas as pd\n+import tensorflow as tf\n+import tensorflow_hub as hub\n+import tensorflow_io as tfio\n+import keras_core as keras\n+import matplotlib.pyplot as plt\n+import seaborn as sns\n+from scipy import stats\n+from IPython.display import Audio\n+\n+\n+# Set all random seeds in order to get reproducible results\n+keras.utils.set_random_seed(SEED)\n+\n+# Where to download the dataset\n+DATASET_DESTINATION = os.path.join(CACHE_DIR if CACHE_DIR else \"~/.keras/\", \"datasets\")\n+\n+\"\"\"\n+## Yamnet Model\n+\n+Yamnet is an audio event classifier trained on the AudioSet dataset to predict audio\n+events from the AudioSet ontology. It is available on TensorFlow Hub.\n+\n+Yamnet accepts a 1-D tensor of audio samples with a sample rate of 16 kHz.\n+As output, the model returns a 3-tuple:\n+\n+* Scores of shape `(N, 521)` representing the scores of the 521 classes.\n+* Embeddings of shape `(N, 1024)`.\n+* The log-mel spectrogram of the entire audio frame.\n+\n+We will use the embeddings, which are the features extracted from the audio samples, as the input to our dense model.\n+\n+For more detailed information about Yamnet, please refer to its [TensorFlow Hub](https://tfhub.dev/google/yamnet/1) page.\n+\"\"\"\n+\n+yamnet_model = hub.load(\"https://tfhub.dev/google/yamnet/1\")\n+\n+\"\"\"\n+## Dataset\n+\n+The dataset used is the\n+[Crowdsourced high-quality UK and Ireland English Dialect speech data set](https://openslr.org/83/)\n+which consists of a total of 17,877 high-quality audio wav files.\n+\n+This dataset includes over 31 hours of recording from 120 volunteers who self-identify as\n+native speakers of Southern England, Midlands, Northern England, Wales, Scotland and Ireland.\n+\n+For more info, please refer to the above link or to the following paper:\n+[Open-source Multi-speaker Corpora of the English Accents in the British Isles](https://aclanthology.org/2020.lrec-1.804.pdf)\n+\"\"\"\n+\n+\"\"\"\n+## Download the data\n+\"\"\"\n+\n+# CSV file that contains information about the dataset. For each entry, we have:\n+# - ID\n+# - wav file name\n+# - transcript\n+line_index_file = keras.utils.get_file(\n+    fname=\"line_index_file\", origin=URL_PATH + \"line_index_all.csv\"\n+)\n+\n+# Download the list of compressed files that contain the audio wav files\n+for i in zip_files:\n+    fname = zip_files[i].split(\".\")[0]\n+    url = URL_PATH + zip_files[i]\n+\n+    zip_file = keras.utils.get_file(fname=fname, origin=url, extract=True)\n+    os.remove(zip_file)\n+\n+\"\"\"\n+## Load the data in a Dataframe\n+\n+Of the 3 columns (ID, filename and transcript), we are only interested in the filename column in order to read the audio file.\n+We will ignore the other two.\n+\"\"\"\n+\n+dataframe = pd.read_csv(\n+    line_index_file, names=[\"id\", \"filename\", \"transcript\"], usecols=[\"filename\"]\n+)\n+dataframe.head()\n+\n+\"\"\"\n+Let's now preprocess the dataset by:\n+\n+* Adjusting the filename (removing a leading space & adding \".wav\" extension to the\n+filename).\n+* Creating a label using the first 2 characters of the filename which indicate the\n+accent.\n+* Shuffling the samples.\n+\"\"\"\n+\n+\n+# The purpose of this function is to preprocess the dataframe by applying the following:\n+# - Cleaning the filename from a leading space\n+# - Generating a label column that is gender agnostic i.e.\n+#   welsh english male and welsh english female for example are both labeled as\n+#   welsh english\n+# - Add extension .wav to the filename\n+# - Shuffle samples\n+def preprocess_dataframe(dataframe):\n+    # Remove leading space in filename column\n+    dataframe[\"filename\"] = dataframe.apply(lambda row: row[\"filename\"].strip(), axis=1)\n+\n+    # Create gender agnostic labels based on the filename first 2 letters\n+    dataframe[\"label\"] = dataframe.apply(\n+        lambda row: gender_agnostic_categories.index(row[\"filename\"][:2]), axis=1\n+    )\n+\n+    # Add the file path to the name\n+    dataframe[\"filename\"] = dataframe.apply(\n+        lambda row: os.path.join(DATASET_DESTINATION, row[\"filename\"] + \".wav\"), axis=1\n+    )\n+\n+    # Shuffle the samples\n+    dataframe = dataframe.sample(frac=1, random_state=SEED).reset_index(drop=True)\n+\n+    return dataframe\n+\n+\n+dataframe = preprocess_dataframe(dataframe)\n+dataframe.head()\n+\n+\"\"\"\n+## Prepare training & validation sets\n+\n+Let's split the samples creating training and validation sets.\n+\"\"\"\n+\n+split = int(len(dataframe) * (1 - VALIDATION_RATIO))\n+train_df = dataframe[:split]\n+valid_df = dataframe[split:]\n+\n+print(\n+    f\"We have {train_df.shape[0]} training samples & {valid_df.shape[0]} validation ones\"\n+)\n+\n+\"\"\"\n+## Prepare a TensorFlow Dataset\n+\n+Next, we need to create a `tf.data.Dataset`.\n+This is done by creating a `dataframe_to_dataset` function that does the following:\n+\n+* Create a dataset using filenames and labels.\n+* Get the Yamnet embeddings by calling another function `filepath_to_embeddings`.\n+* Apply caching, reshuffling and setting batch size.\n+\n+The `filepath_to_embeddings` does the following:\n+\n+* Load audio file.\n+* Resample audio to 16 kHz.\n+* Generate scores and embeddings from Yamnet model.\n+* Since Yamnet generates multiple samples for each audio file,\n+this function also duplicates the label for all the generated samples\n+that have `score=0` (speech) whereas sets the label for the others as\n+'other' indicating that this audio segment is not a speech and we won't label it as one of the accents.\n+\n+The below `load_16k_audio_file` is copied from the following tutorial\n+[Transfer learning with YAMNet for environmental sound classification](https://www.tensorflow.org/tutorials/audio/transfer_learning_audio)\n+\"\"\"\n+\n+\n+@tf.function\n+def load_16k_audio_wav(filename):\n+    # Read file content\n+    file_content = tf.io.read_file(filename)\n+\n+    # Decode audio wave\n+    audio_wav, sample_rate = tf.audio.decode_wav(file_content, desired_channels=1)\n+    audio_wav = tf.squeeze(audio_wav, axis=-1)\n+    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n+\n+    # Resample to 16k\n+    audio_wav = tfio.audio.resample(audio_wav, rate_in=sample_rate, rate_out=16000)\n+\n+    return audio_wav\n+\n+\n+def filepath_to_embeddings(filename, label):\n+    # Load 16k audio wave\n+    audio_wav = load_16k_audio_wav(filename)\n+\n+    # Get audio embeddings & scores.\n+    # The embeddings are the audio features extracted using transfer learning\n+    # while scores will be used to identify time slots that are not speech\n+    # which will then be gathered into a specific new category 'other'\n+    scores, embeddings, _ = yamnet_model(audio_wav)\n+\n+    # Number of embeddings in order to know how many times to repeat the label\n+    embeddings_num = tf.shape(embeddings)[0]\n+    labels = tf.repeat(label, embeddings_num)\n+\n+    # Change labels for time-slots that are not speech into a new category 'other'\n+    labels = tf.where(tf.argmax(scores, axis=1) == 0, label, len(class_names) - 1)\n+\n+    # Using one-hot in order to use AUC\n+    return (embeddings, tf.one_hot(labels, len(class_names)))\n+\n+\n+def dataframe_to_dataset(dataframe, batch_size=64):\n+    dataset = tf.data.Dataset.from_tensor_slices(\n+        (dataframe[\"filename\"], dataframe[\"label\"])\n+    )\n+\n+    dataset = dataset.map(\n+        lambda x, y: filepath_to_embeddings(x, y),\n+        num_parallel_calls=tf.data.experimental.AUTOTUNE,\n+    ).unbatch()\n+\n+    return dataset.cache().batch(batch_size).prefetch(tf.data.AUTOTUNE)\n+\n+\n+train_ds = dataframe_to_dataset(train_df)\n+valid_ds = dataframe_to_dataset(valid_df)\n+\n+\"\"\"\n+## Build the model\n+\n+The model that we use consists of:\n+\n+* An input layer which is the embedding output of the Yamnet classifier.\n+* 4 dense hidden layers and 4 dropout layers.\n+* An output dense layer.\n+\n+The model's hyperparameters were selected using\n+[KerasTuner](https://keras.io/keras_tuner/).\n+\"\"\"\n+\n+keras.backend.clear_session()\n+\n+\n+def build_and_compile_model():\n+    inputs = keras.layers.Input(shape=(1024,), name=\"embedding\")\n+\n+    x = keras.layers.Dense(256, activation=\"relu\", name=\"dense_1\")(inputs)\n+    x = keras.layers.Dropout(0.15, name=\"dropout_1\")(x)\n+\n+    x = keras.layers.Dense(384, activation=\"relu\", name=\"dense_2\")(x)\n+    x = keras.layers.Dropout(0.2, name=\"dropout_2\")(x)\n+\n+    x = keras.layers.Dense(192, activation=\"relu\", name=\"dense_3\")(x)\n+    x = keras.layers.Dropout(0.25, name=\"dropout_3\")(x)\n+\n+    x = keras.layers.Dense(384, activation=\"relu\", name=\"dense_4\")(x)\n+    x = keras.layers.Dropout(0.2, name=\"dropout_4\")(x)\n+\n+    outputs = keras.layers.Dense(len(class_names), activation=\"softmax\", name=\"ouput\")(\n+        x\n+    )\n+\n+    model = keras.Model(inputs=inputs, outputs=outputs, name=\"accent_recognition\")\n+\n+    model.compile(\n+        optimizer=keras.optimizers.Adam(learning_rate=1.9644e-5),\n+        loss=keras.losses.CategoricalCrossentropy(),\n+        metrics=[\"accuracy\", keras.metrics.AUC(name=\"auc\")],\n+    )\n+\n+    return model\n+\n+\n+model = build_and_compile_model()\n+model.summary()\n+\n+\"\"\"\n+## Class weights calculation\n+\n+Since the dataset is quite unbalanced, we wil use `class_weight` argument during training.\n+\n+Getting the class weights is a little tricky because even though we know the number of\n+audio files for each class, it does not represent the number of samples for that class\n+since Yamnet transforms each audio file into multiple audio samples of 0.96 seconds each.\n+So every audio file will be split into a number of samples that is proportional to its length.\n+\n+Therefore, to get those weights, we have to calculate the number of samples for each class\n+after preprocessing through Yamnet.\n+\"\"\"\n+\n+class_counts = tf.zeros(shape=(len(class_names),), dtype=tf.int32)\n+\n+for x, y in iter(train_ds):\n+    class_counts = class_counts + tf.math.bincount(\n+        tf.cast(tf.math.argmax(y, axis=1), tf.int32), minlength=len(class_names)\n+    )\n+\n+class_weight = {\n+    i: tf.math.reduce_sum(class_counts).numpy() / class_counts[i].numpy()\n+    for i in range(len(class_counts))\n+}\n+\n+print(class_weight)\n+\n+\"\"\"\n+## Callbacks\n+\n+We use Keras callbacks in order to:\n+\n+* Stop whenever the validation AUC stops improving.\n+* Save the best model.\n+* Call TensorBoard in order to later view the training and validation logs.\n+\"\"\"\n+\n+early_stopping_cb = keras.callbacks.EarlyStopping(\n+    monitor=\"val_auc\", patience=10, restore_best_weights=True\n+)\n+\n+model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\n+    MODEL_NAME + \".keras\", monitor=\"val_auc\", save_best_only=True\n+)\n+\n+tensorboard_cb = keras.callbacks.TensorBoard(\n+    os.path.join(os.curdir, \"logs\", model.name)\n+)\n+\n+callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n+\n+\"\"\"\n+## Training\n+\"\"\"\n+\n+history = model.fit(\n+    train_ds,\n+    epochs=EPOCHS,\n+    validation_data=valid_ds,\n+    class_weight=class_weight,\n+    callbacks=callbacks,\n+    verbose=2,\n+)\n+\n+\"\"\"\n+## Results\n+\n+Let's plot the training and validation AUC and accuracy.\n+\"\"\"\n+\n+fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))\n+\n+axs[0].plot(range(EPOCHS), history.history[\"accuracy\"], label=\"Training\")\n+axs[0].plot(range(EPOCHS), history.history[\"val_accuracy\"], label=\"Validation\")\n+axs[0].set_xlabel(\"Epochs\")\n+axs[0].set_title(\"Training & Validation Accuracy\")\n+axs[0].legend()\n+axs[0].grid(True)\n+\n+axs[1].plot(range(EPOCHS), history.history[\"auc\"], label=\"Training\")\n+axs[1].plot(range(EPOCHS), history.history[\"val_auc\"], label=\"Validation\")\n+axs[1].set_xlabel(\"Epochs\")\n+axs[1].set_title(\"Training & Validation AUC\")\n+axs[1].legend()\n+axs[1].grid(True)\n+\n+plt.show()\n+\n+\"\"\"\n+## Evaluation\n+\"\"\"\n+\n+train_loss, train_acc, train_auc = model.evaluate(train_ds)\n+valid_loss, valid_acc, valid_auc = model.evaluate(valid_ds)\n+\n+\"\"\"\n+Let's try to compare our model's performance to Yamnet's using one of Yamnet metrics (d-prime)\n+Yamnet achieved a d-prime value of 2.318.\n+Let's check our model's performance.\n+\"\"\"\n+\n+\n+# The following function calculates the d-prime score from the AUC\n+def d_prime(auc):\n+    standard_normal = stats.norm()\n+    d_prime = standard_normal.ppf(auc) * np.sqrt(2.0)\n+    return d_prime\n+\n+\n+print(\n+    \"train d-prime: {0:.3f}, validation d-prime: {1:.3f}\".format(\n+        d_prime(train_auc), d_prime(valid_auc)\n+    )\n+)\n+\n+\"\"\"\n+We can see that the model achieves the following results:\n+\n+Results    | Training  | Validation\n+-----------|-----------|------------\n+Accuracy   | 54%       | 51%\n+AUC        | 0.91      | 0.89\n+d-prime    | 1.882     | 1.740\n+\n+\"\"\"\n+\n+\"\"\"\n+## Confusion Matrix\n+\n+Let's now plot the confusion matrix for the validation dataset.\n+\n+The confusion matrix lets us see, for every class, not only how many samples were correctly classified,\n+but also which other classes were the samples confused with.\n+\n+It allows us to calculate the precision and recall for every class.\n+\"\"\"\n+\n+# Create x and y tensors\n+x_valid = None\n+y_valid = None\n+\n+for x, y in iter(valid_ds):\n+    if x_valid is None:\n+        x_valid = x.numpy()\n+        y_valid = y.numpy()\n+    else:\n+        x_valid = np.concatenate((x_valid, x.numpy()), axis=0)\n+        y_valid = np.concatenate((y_valid, y.numpy()), axis=0)\n+\n+# Generate predictions\n+y_pred = model.predict(x_valid)\n+\n+# Calculate confusion matrix\n+confusion_mtx = tf.math.confusion_matrix(\n+    np.argmax(y_valid, axis=1), np.argmax(y_pred, axis=1)\n+)\n+\n+# Plot the confusion matrix\n+plt.figure(figsize=(10, 8))\n+sns.heatmap(\n+    confusion_mtx, xticklabels=class_names, yticklabels=class_names, annot=True, fmt=\"g\"\n+)\n+plt.xlabel(\"Prediction\")\n+plt.ylabel(\"Label\")\n+plt.title(\"Validation Confusion Matrix\")\n+plt.show()\n+\n+\"\"\"\n+## Precision & recall\n+\n+For every class:\n+\n+* Recall is the ratio of correctly classified samples i.e. it shows how many samples\n+of this specific class, the model is able to detect.\n+It is the ratio of diagonal elements to the sum of all elements in the row.\n+* Precision shows the accuracy of the classifier. It is the ratio of correctly predicted\n+samples among the ones classified as belonging to this class.\n+It is the ratio of diagonal elements to the sum of all elements in the column.\n+\"\"\"\n+\n+for i, label in enumerate(class_names):\n+    precision = confusion_mtx[i, i] / np.sum(confusion_mtx[:, i])\n+    recall = confusion_mtx[i, i] / np.sum(confusion_mtx[i, :])\n+    print(\n+        \"{0:15} Precision:{1:.2f}%; Recall:{2:.2f}%\".format(\n+            label, precision * 100, recall * 100\n+        )\n+    )\n+\n+\"\"\"\n+## Run inference on test data\n+\n+Let's now run a test on a single audio file.\n+Let's check this example from [The Scottish Voice](https://www.thescottishvoice.org.uk/home/)\n+\n+We will:\n+\n+* Download the mp3 file.\n+* Convert it to a 16k wav file.\n+* Run the model on the wav file.\n+* Plot the results.\n+\"\"\"\n+\n+filename = \"audio-sample-Stuart\"\n+url = \"https://www.thescottishvoice.org.uk/files/cm/files/\"\n+\n+if os.path.exists(filename + \".wav\") == False:\n+    print(f\"Downloading {filename}.mp3 from {url}\")\n+    command = f\"wget {url}{filename}.mp3\"\n+    os.system(command)\n+\n+    print(f\"Converting mp3 to wav and resampling to 16 kHZ\")\n+    command = (\n+        f\"ffmpeg -hide_banner -loglevel panic -y -i {filename}.mp3 -acodec \"\n+        f\"pcm_s16le -ac 1 -ar 16000 {filename}.wav\"\n+    )\n+    os.system(command)\n+\n+filename = filename + \".wav\"\n+\n+\n+\"\"\"\n+The below function `yamnet_class_names_from_csv` was copied and very slightly changed\n+from this [Yamnet Notebook](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/yamnet.ipynb).\n+\"\"\"\n+\n+\n+def yamnet_class_names_from_csv(yamnet_class_map_csv_text):\n+    \"\"\"Returns list of class names corresponding to score vector.\"\"\"\n+    yamnet_class_map_csv = io.StringIO(yamnet_class_map_csv_text)\n+    yamnet_class_names = [\n+        name for (class_index, mid, name) in csv.reader(yamnet_class_map_csv)\n+    ]\n+    yamnet_class_names = yamnet_class_names[1:]  # Skip CSV header\n+    return yamnet_class_names\n+\n+\n+yamnet_class_map_path = yamnet_model.class_map_path().numpy()\n+yamnet_class_names = yamnet_class_names_from_csv(\n+    tf.io.read_file(yamnet_class_map_path).numpy().decode(\"utf-8\")\n+)\n+\n+\n+def calculate_number_of_non_speech(scores):\n+    number_of_non_speech = tf.math.reduce_sum(\n+        tf.where(tf.math.argmax(scores, axis=1, output_type=tf.int32) != 0, 1, 0)\n+    )\n+\n+    return number_of_non_speech\n+\n+\n+def filename_to_predictions(filename):\n+    # Load 16k audio wave\n+    audio_wav = load_16k_audio_wav(filename)\n+\n+    # Get audio embeddings & scores.\n+    scores, embeddings, mel_spectrogram = yamnet_model(audio_wav)\n+\n+    print(\n+        \"Out of {} samples, {} are not speech\".format(\n+            scores.shape[0], calculate_number_of_non_speech(scores)\n+        )\n+    )\n+\n+    # Predict the output of the accent recognition model with embeddings as input\n+    predictions = model.predict(embeddings)\n+\n+    return audio_wav, predictions, mel_spectrogram\n+\n+\n+\"\"\"\n+Let's run the model on the audio file:\n+\"\"\"\n+\n+audio_wav, predictions, mel_spectrogram = filename_to_predictions(filename)\n+\n+infered_class = class_names[predictions.mean(axis=0).argmax()]\n+print(f\"The main accent is: {infered_class} English\")\n+\n+\"\"\"\n+Listen to the audio\n+\"\"\"\n+\n+Audio(audio_wav, rate=16000)\n+\n+\"\"\"\n+The below function was copied from this [Yamnet notebook](tinyurl.com/4a8xn7at) and adjusted to our need.\n+\n+This function plots the following:\n+\n+* Audio waveform\n+* Mel spectrogram\n+* Predictions for every time step\n+\"\"\"\n+\n+plt.figure(figsize=(10, 6))\n+\n+# Plot the waveform.\n+plt.subplot(3, 1, 1)\n+plt.plot(audio_wav)\n+plt.xlim([0, len(audio_wav)])\n+\n+# Plot the log-mel spectrogram (returned by the model).\n+plt.subplot(3, 1, 2)\n+plt.imshow(\n+    mel_spectrogram.numpy().T, aspect=\"auto\", interpolation=\"nearest\", origin=\"lower\"\n+)\n+\n+# Plot and label the model output scores for the top-scoring classes.\n+mean_predictions = np.mean(predictions, axis=0)\n+\n+top_class_indices = np.argsort(mean_predictions)[::-1]\n+plt.subplot(3, 1, 3)\n+plt.imshow(\n+    predictions[:, top_class_indices].T,\n+    aspect=\"auto\",\n+    interpolation=\"nearest\",\n+    cmap=\"gray_r\",\n+)\n+\n+# patch_padding = (PATCH_WINDOW_SECONDS / 2) / PATCH_HOP_SECONDS\n+# values from the model documentation\n+patch_padding = (0.025 / 2) / 0.01\n+plt.xlim([-patch_padding - 0.5, predictions.shape[0] + patch_padding - 0.5])\n+# Label the top_N classes.\n+yticks = range(0, len(class_names), 1)\n+plt.yticks(yticks, [class_names[top_class_indices[x]] for x in yticks])\n+_ = plt.ylim(-0.5 + np.array([len(class_names), 0]))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2c2962158efaef0050bca9a41e76f29e52bd8610", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 189 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 189 | Churn Cumulative: 189 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -0,0 +1,189 @@\n+\"\"\"\n+Title: Actor Critic Method\n+Author: [Apoorv Nandan](https://twitter.com/NandanApoorv)\n+Converted to Keras Core by: [Muhammad Anas Raza](https://anasrz.com)\n+Date created: 2020/05/13\n+Last modified: 2023/07/19\n+Description: Implement Actor Critic Method in CartPole environment.\n+Accelerator: NONE\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+This script shows an implementation of Actor Critic method on CartPole-V0 environment.\n+\n+### Actor Critic Method\n+\n+As an agent takes actions and moves through an environment, it learns to map\n+the observed state of the environment to two possible outputs:\n+\n+1. Recommended action: A probability value for each action in the action space.\n+   The part of the agent responsible for this output is called the **actor**.\n+2. Estimated rewards in the future: Sum of all rewards it expects to receive in the\n+   future. The part of the agent responsible for this output is the **critic**.\n+\n+Agent and Critic learn to perform their tasks, such that the recommended actions\n+from the actor maximize the rewards.\n+\n+### CartPole-V1\n+\n+A pole is attached to a cart placed on a frictionless track. The agent has to apply\n+force to move the cart. It is rewarded for every time step the pole\n+remains upright. The agent, therefore, must learn to keep the pole from falling over.\n+\n+### References\n+\n+- [CartPole](http://www.derongliu.org/adp/adp-cdrom/Barto1983.pdf)\n+- [Actor Critic Method](https://hal.inria.fr/hal-00840470/document)\n+\"\"\"\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import os\n+os.environ['KERAS_BACKEND'] = 'tensorflow'\n+\n+import keras_core as keras\n+from keras_core import layers\n+\n+import gym\n+import numpy as np\n+import tensorflow as tf\n+\n+\n+# Configuration parameters for the whole setup\n+seed = 42\n+gamma = 0.99  # Discount factor for past rewards\n+max_steps_per_episode = 10000\n+env = gym.make(\"CartPole-v1\", new_step_api=True)  # Create the environment\n+env.reset(seed=seed)\n+eps = np.finfo(np.float32).eps.item()  # Smallest number such that 1.0 + eps != 1.0\n+\n+\"\"\"\n+## Implement Actor Critic network\n+\n+This network learns two functions:\n+\n+1. Actor: This takes as input the state of our environment and returns a\n+probability value for each action in its action space.\n+2. Critic: This takes as input the state of our environment and returns\n+an estimate of total rewards in the future.\n+\n+In our implementation, they share the initial layer.\n+\"\"\"\n+\n+num_inputs = 4\n+num_actions = 2\n+num_hidden = 128\n+\n+inputs = layers.Input(shape=(num_inputs,))\n+common = layers.Dense(num_hidden, activation=\"relu\")(inputs)\n+action = layers.Dense(num_actions, activation=\"softmax\")(common)\n+critic = layers.Dense(1)(common)\n+\n+model = keras.Model(inputs=inputs, outputs=[action, critic])\n+\n+\"\"\"\n+## Train\n+\"\"\"\n+\n+optimizer = keras.optimizers.Adam(learning_rate=0.01)\n+huber_loss = keras.losses.Huber()\n+action_probs_history = []\n+critic_value_history = []\n+rewards_history = []\n+running_reward = 0\n+episode_count = 0\n+\n+while True:  # Run until solved\n+    state = env.reset()\n+    episode_reward = 0\n+    with tf.GradientTape() as tape:\n+        for timestep in range(1, max_steps_per_episode):\n+            # env.render(); Adding this line would show the attempts\n+            # of the agent in a pop up window.\n+\n+            state = tf.convert_to_tensor(state)\n+            state = tf.expand_dims(state, 0)\n+\n+            # Predict action probabilities and estimated future rewards\n+            # from environment state\n+            action_probs, critic_value = model(state)\n+            critic_value_history.append(critic_value[0, 0])\n+\n+            # Sample action from action probability distribution\n+            action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n+            action_probs_history.append(tf.math.log(action_probs[0, action]))\n+\n+            # Apply the sampled action in our environment\n+            state, reward, done, _, _ = env.step(action)\n+            rewards_history.append(reward)\n+            episode_reward += reward\n+\n+            if done:\n+                break\n+\n+        # Update running reward to check condition for solving\n+        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n+\n+        # Calculate expected value from rewards\n+        # - At each timestep what was the total reward received after that timestep\n+        # - Rewards in the past are discounted by multiplying them with gamma\n+        # - These are the labels for our critic\n+        returns = []\n+        discounted_sum = 0\n+        for r in rewards_history[::-1]:\n+            discounted_sum = r + gamma * discounted_sum\n+            returns.insert(0, discounted_sum)\n+\n+        # Normalize\n+        returns = np.array(returns)\n+        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n+        returns = returns.tolist()\n+\n+        # Calculating loss values to update our network\n+        history = zip(action_probs_history, critic_value_history, returns)\n+        actor_losses = []\n+        critic_losses = []\n+        for log_prob, value, ret in history:\n+            # At this point in history, the critic estimated that we would get a\n+            # total reward = `value` in the future. We took an action with log probability\n+            # of `log_prob` and ended up recieving a total reward = `ret`.\n+            # The actor must be updated so that it predicts an action that leads to\n+            # high rewards (compared to critic's estimate) with high probability.\n+            diff = ret - value\n+            actor_losses.append(-log_prob * diff)  # actor loss\n+\n+            # The critic must be updated so that it predicts a better estimate of\n+            # the future rewards.\n+            critic_losses.append(\n+                huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n+            )\n+\n+        # Backpropagation\n+        loss_value = sum(actor_losses) + sum(critic_losses)\n+        grads = tape.gradient(loss_value, model.trainable_variables)\n+        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n+\n+        # Clear the loss and reward history\n+        action_probs_history.clear()\n+        critic_value_history.clear()\n+        rewards_history.clear()\n+\n+    # Log details\n+    episode_count += 1\n+    if episode_count % 10 == 0:\n+        template = \"running reward: {:.2f} at episode {}\"\n+        print(template.format(running_reward, episode_count))\n+\n+    if running_reward > 195:  # Condition to consider the task solved\n+        print(\"Solved at episode {}!\".format(episode_count))\n+        break\n+\"\"\"\n+## Visualizations\n+In early stages of training:\n+![Imgur](https://i.imgur.com/5gCs5kH.gif)\n+\n+In later stages of training:\n+![Imgur](https://i.imgur.com/5ziiZUD.gif)\n+\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
