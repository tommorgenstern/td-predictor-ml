{"custom_id": "scrapy#14613638099dffd9a1e67edf1ea63e1729d7b097", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 30 | Files Changed: 6 | Hunks: 23 | Methods Changed: 13 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 58 | Churn Cumulative: 1594 | Contributors (this commit): 6 | Commits (past 90d): 18 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -71,11 +71,11 @@ class XMLFeedSpider(BaseSpider):\n         if self.iterator == 'iternodes':\n             nodes = self._iternodes(response)\n         elif self.iterator == 'xml':\n-            selector = Selector(response, contenttype='xml')\n+            selector = Selector(response, type='xml')\n             self._register_namespaces(selector)\n             nodes = selector.xpath('//%s' % self.itertag)\n         elif self.iterator == 'html':\n-            selector = Selector(response, contenttype='html')\n+            selector = Selector(response, type='html')\n             self._register_namespaces(selector)\n             nodes = selector.xpath('//%s' % self.itertag)\n         else:\n\n@@ -11,7 +11,7 @@ def xmliter_lxml(obj, nodename, namespace=None):\n     for _, node in iterable:\n         nodetext = etree.tostring(node)\n         node.clear()\n-        xs = Selector(text=nodetext, contenttype='xml')\n+        xs = Selector(text=nodetext, type='xml')\n         if namespace:\n             xs.register_namespace('x', namespace)\n         yield xs.xpath(selxpath)[0]\n\n@@ -10,7 +10,7 @@ __all__ = ['HtmlXPathSelector', 'XmlXPathSelector', 'XPathSelector',\n \n class XPathSelector(Selector):\n     __slots__ = ()\n-    default_contenttype = 'html'\n+    _default_type = 'html'\n \n     def __init__(self, *a, **kw):\n         import warnings\n@@ -28,12 +28,12 @@ class XPathSelector(Selector):\n \n class XmlXPathSelector(XPathSelector):\n     __slots__ = ()\n-    default_contenttype = 'xml'\n+    _default_type = 'xml'\n \n \n class HtmlXPathSelector(XPathSelector):\n     __slots__ = ()\n-    default_contenttype = 'html'\n+    _default_type = 'html'\n \n \n class XPathSelectorList(SelectorList):\n\n@@ -25,38 +25,37 @@ _ctgroup = {\n }\n \n \n-def _ct(response, ct):\n-    if ct is None:\n+def _st(response, st):\n+    if st is None:\n         return 'xml' if isinstance(response, XmlResponse) else 'html'\n-    elif ct in ('xml', 'html'):\n-        return ct\n+    elif st in ('xml', 'html'):\n+        return st\n     else:\n-        raise ValueError('Invalid contenttype: %s' % ct)\n+        raise ValueError('Invalid type: %s' % st)\n \n \n-def _response_from_text(text, ct):\n-    rt = XmlResponse if ct == 'xml' else HtmlResponse\n+def _response_from_text(text, st):\n+    rt = XmlResponse if st == 'xml' else HtmlResponse\n     return rt(url='about:blank', encoding='utf-8',\n               body=unicode_to_str(text, 'utf-8'))\n \n \n class Selector(object_ref):\n \n-    __slots__ = ['response', 'text', 'namespaces', 'contenttype', '_expr', '_root',\n+    __slots__ = ['response', 'text', 'namespaces', 'type', '_expr', '_root',\n                  '__weakref__', '_parser', '_csstranslator', '_tostring_method']\n \n-    default_contenttype = None\n+    _default_type = None\n \n-    def __init__(self, response=None, text=None, namespaces=None, contenttype=None,\n+    def __init__(self, response=None, text=None, type=None, namespaces=None,\n                  _root=None, _expr=None):\n-\n-        self.contenttype = ct = _ct(response, contenttype or self.default_contenttype)\n-        self._parser = _ctgroup[ct]['_parser']\n-        self._csstranslator = _ctgroup[ct]['_csstranslator']\n-        self._tostring_method = _ctgroup[ct]['_tostring_method']\n+        self.type = st = _st(response, type or self._default_type)\n+        self._parser = _ctgroup[st]['_parser']\n+        self._csstranslator = _ctgroup[st]['_csstranslator']\n+        self._tostring_method = _ctgroup[st]['_tostring_method']\n \n         if text is not None:\n-            response = _response_from_text(text, ct)\n+            response = _response_from_text(text, st)\n \n         if response is not None:\n             _root = LxmlDocument(response, self._parser)\n@@ -82,7 +81,7 @@ class Selector(object_ref):\n \n         result = [self.__class__(_root=x, _expr=query,\n                                  namespaces=self.namespaces,\n-                                 contenttype=self.contenttype)\n+                                 type=self.type)\n                   for x in result]\n         return SelectorList(result)\n \n@@ -169,4 +168,3 @@ class SelectorList(list):\n     @deprecated(use_instead='.xpath()')\n     def select(self, xpath):\n         return self.xpath(xpath)\n-\n\n@@ -59,23 +59,23 @@ class SelectorTestCase(unittest.TestCase):\n         \"\"\"Test that XML and HTML Selector's behave differently\"\"\"\n         # some text which is parsed differently by XML and HTML flavors\n         text = '<div><img src=\"a.jpg\"><p>Hello</div>'\n-        hs = self.sscls(text=text, contenttype='html')\n+        hs = self.sscls(text=text, type='html')\n         self.assertEqual(hs.xpath(\"//div\").extract(),\n                          [u'<div><img src=\"a.jpg\"><p>Hello</p></div>'])\n \n-        xs = self.sscls(text=text, contenttype='xml')\n+        xs = self.sscls(text=text, type='xml')\n         self.assertEqual(xs.xpath(\"//div\").extract(),\n                          [u'<div><img src=\"a.jpg\"><p>Hello</p></img></div>'])\n \n     def test_flavor_detection(self):\n         text = '<div><img src=\"a.jpg\"><p>Hello</div>'\n         sel = self.sscls(XmlResponse('http://example.com', body=text))\n-        self.assertEqual(sel.contenttype, 'xml')\n+        self.assertEqual(sel.type, 'xml')\n         self.assertEqual(sel.xpath(\"//div\").extract(),\n                          [u'<div><img src=\"a.jpg\"><p>Hello</p></img></div>'])\n \n         sel = self.sscls(HtmlResponse('http://example.com', body=text))\n-        self.assertEqual(sel.contenttype, 'html')\n+        self.assertEqual(sel.type, 'html')\n         self.assertEqual(sel.xpath(\"//div\").extract(),\n                          [u'<div><img src=\"a.jpg\"><p>Hello</p></div>'])\n \n@@ -183,7 +183,7 @@ class SelectorTestCase(unittest.TestCase):\n     def test_selector_over_text(self):\n         hs = self.sscls(text='<root>lala</root>')\n         self.assertEqual(hs.extract(), u'<html><body><root>lala</root></body></html>')\n-        xs = self.sscls(text='<root>lala</root>', contenttype='xml')\n+        xs = self.sscls(text='<root>lala</root>', type='xml')\n         self.assertEqual(xs.extract(), u'<root>lala</root>')\n         self.assertEqual(xs.xpath('.').extract(), [u'<root>lala</root>'])\n \n\n@@ -29,7 +29,7 @@ def xmliter(obj, nodename):\n     r = re.compile(r\"<%s[\\s>].*?</%s>\" % (nodename, nodename), re.DOTALL)\n     for match in r.finditer(text):\n         nodetext = header_start + match.group() + header_end\n-        yield Selector(text=nodetext, contenttype='xml').xpath('//' + nodename)[0]\n+        yield Selector(text=nodetext, type='xml').xpath('//' + nodename)[0]\n \n \n def csviter(obj, delimiter=None, headers=None, encoding=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#757e262267ef3c5c77018dbb309f91079cf9b82d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 225 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -122,6 +122,13 @@ try:\n except ImportError:\n     from distutils.core import setup\n else:\n-    setup_args['install_requires'] = ['Twisted>=10.0.0', 'w3lib>=1.2', 'queuelib', 'lxml', 'pyOpenSSL', 'cssselect>0.8']\n+    setup_args['install_requires'] = [\n+        'Twisted>=10.0.0',\n+        'w3lib>=1.2',\n+        'queuelib',\n+        'lxml',\n+        'pyOpenSSL',\n+        'cssselect>=0.9',\n+    ]\n \n setup(**setup_args)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#192c323a0c7d39d83714d42e11c656f084a29a9c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 227 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -55,7 +55,7 @@ if root_dir != '':\n     os.chdir(root_dir)\n \n def is_not_module(filename):\n-    return os.path.splitext(f)[1] not in ['.py', '.pyc', '.pyo']\n+    return os.path.splitext(filename)[1] not in ['.py', '.pyc', '.pyo']\n \n for scrapy_dir in ['scrapy']:\n     for dirpath, dirnames, filenames in os.walk(scrapy_dir):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ddcd5710000c4ce2973d4fc81822b398521939d4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 7 | Files Changed: 4 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 1456 | Contributors (this commit): 11 | Commits (past 90d): 11 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,4 @@\n import signal\n-from itertools import chain\n \n from twisted.internet import reactor, defer\n \n\n@@ -24,6 +24,7 @@ class CloseSpider(Exception):\n     \"\"\"Raise this from callbacks to request the spider to be closed\"\"\"\n \n     def __init__(self, reason='cancelled'):\n+        super(CloseSpider, self).__init__()\n         self.reason = reason\n \n # Items\n\n@@ -6,8 +6,6 @@ This module must not depend on any module outside the Standard Library.\n \"\"\"\n \n import copy\n-from collections import deque, defaultdict\n-from itertools import chain\n \n from scrapy.utils.py27 import OrderedDict\n \n@@ -46,7 +44,7 @@ class MultiValueDict(dict):\n         try:\n             list_ = dict.__getitem__(self, key)\n         except KeyError:\n-            raise MultiValueDictKeyError, \"Key %r not found in %r\" % (key, self)\n+            raise MultiValueDictKeyError(\"Key %r not found in %r\" % (key, self))\n         try:\n             return list_[-1]\n         except IndexError:\n@@ -124,7 +122,7 @@ class MultiValueDict(dict):\n     def update(self, *args, **kwargs):\n         \"update() extends rather than replaces existing key lists. Also accepts keyword args.\"\n         if len(args) > 1:\n-            raise TypeError, \"update expected at most 1 arguments, got %d\" % len(args)\n+            raise TypeError(\"update expected at most 1 arguments, got %d\" % len(args))\n         if args:\n             other_dict = args[0]\n             if isinstance(other_dict, MultiValueDict):\n@@ -135,7 +133,7 @@ class MultiValueDict(dict):\n                     for key, value in other_dict.items():\n                         self.setlistdefault(key, []).append(value)\n                 except TypeError:\n-                    raise ValueError, \"MultiValueDict.update() takes either a MultiValueDict or dictionary\"\n+                    raise ValueError(\"MultiValueDict.update() takes either a MultiValueDict or dictionary\")\n         for key, value in kwargs.iteritems():\n             self.setlistdefault(key, []).append(value)\n \n\n@@ -5,11 +5,12 @@ library.\n Some of the functions that used to be imported from this module have been moved\n to the w3lib.url module. Always import those from there instead.\n \"\"\"\n+import posixpath\n import urlparse\n import urllib\n import cgi\n \n-from w3lib.url import *\n+from w3lib.url import add_or_replace_parameter, safe_url_string\n from scrapy.utils.python import unicode_to_str\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#105e46205b3bd3f749dbc1132ce5a302ecb10616", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 395 | Contributors (this commit): 9 | Commits (past 90d): 2 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -10,7 +10,8 @@ import urlparse\n import urllib\n import cgi\n \n-from w3lib.url import add_or_replace_parameter, safe_url_string\n+# scrapy.utils.url was moved to w3lib.url and import * ensures this move doesn't break old code\n+from w3lib.url import *\n from scrapy.utils.python import unicode_to_str\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dc666f6e5d27d2fe5d1d3129a90f741ea5634f5c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 12 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 165 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,7 +1,6 @@\n \"\"\"\n Base class for Scrapy commands\n \"\"\"\n-\n import os\n import warnings\n from optparse import OptionGroup\n@@ -10,6 +9,7 @@ from twisted.python import failure\n from scrapy.utils.conf import arglist_to_dict\n from scrapy.exceptions import UsageError, ScrapyDeprecationWarning\n \n+\n class ScrapyCommand(object):\n \n     requires_project = False\n@@ -38,12 +38,14 @@ class ScrapyCommand(object):\n \n             old_start = crawler.start\n             self.crawler_process.started = False\n+\n             def wrapped_start():\n                 if self.crawler_process.started:\n                     old_start()\n                 else:\n                     self.crawler_process.started = True\n                     self.crawler_process.start()\n+\n             crawler.start = wrapped_start\n \n             self.set_crawler(crawler)\n@@ -81,22 +83,22 @@ class ScrapyCommand(object):\n         Populate option parse with options available for this command\n         \"\"\"\n         group = OptionGroup(parser, \"Global Options\")\n-        group.add_option(\"--logfile\", metavar=\"FILE\", \\\n+        group.add_option(\"--logfile\", metavar=\"FILE\",\n             help=\"log file. if omitted stderr will be used\")\n-        group.add_option(\"-L\", \"--loglevel\", metavar=\"LEVEL\", \\\n-            default=None, \\\n+        group.add_option(\"-L\", \"--loglevel\", metavar=\"LEVEL\", default=None,\n             help=\"log level (default: %s)\" % self.settings['LOG_LEVEL'])\n-        group.add_option(\"--nolog\", action=\"store_true\", \\\n+        group.add_option(\"--nolog\", action=\"store_true\",\n             help=\"disable logging completely\")\n-        group.add_option(\"--profile\", metavar=\"FILE\", default=None, \\\n+        group.add_option(\"--profile\", metavar=\"FILE\", default=None,\n             help=\"write python cProfile stats to FILE\")\n-        group.add_option(\"--lsprof\", metavar=\"FILE\", default=None, \\\n+        group.add_option(\"--lsprof\", metavar=\"FILE\", default=None,\n             help=\"write lsprof profiling stats to FILE\")\n-        group.add_option(\"--pidfile\", metavar=\"FILE\", \\\n+        group.add_option(\"--pidfile\", metavar=\"FILE\",\n             help=\"write process ID to FILE\")\n-        group.add_option(\"-s\", \"--set\", action=\"append\", default=[], metavar=\"NAME=VALUE\", \\\n+        group.add_option(\"-s\", \"--set\", action=\"append\", default=[], metavar=\"NAME=VALUE\",\n             help=\"set/override setting (may be repeated)\")\n         group.add_option(\"--pdb\", action=\"store_true\", help=\"enable pdb on failure\")\n+\n         parser.add_option_group(group)\n \n     def process_options(self, args, opts):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d381a357325b92398f2cffe18f1001a8ece83c5f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 123 | Lines Deleted: 129 | Files Changed: 46 | Hunks: 97 | Methods Changed: 64 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 252 | Churn Cumulative: 12707 | Contributors (this commit): 26 | Commits (past 90d): 79 | Contributors (cumulative): 177 | DMM Complexity: None\n\nDIFF:\n@@ -29,9 +29,9 @@ def get_commands():\n \n def cmd_help(args, opts):\n     \"\"\"help - list available commands\"\"\"\n-    print \"Available commands:\"\n+    print(\"Available commands:\")\n     for _, func in sorted(get_commands().items()):\n-        print \"  \", func.__doc__\n+        print(\"  \", func.__doc__)\n \n def cmd_stop(args, opts):\n     \"\"\"stop <spider> - stop a running spider\"\"\"\n@@ -40,29 +40,29 @@ def cmd_stop(args, opts):\n def cmd_list_running(args, opts):\n     \"\"\"list-running - list running spiders\"\"\"\n     for x in json_get(opts, 'crawler/engine/open_spiders'):\n-        print x\n+        print(x)\n \n def cmd_list_available(args, opts):\n     \"\"\"list-available - list name of available spiders\"\"\"\n     for x in jsonrpc_call(opts, 'crawler/spiders', 'list'):\n-        print x\n+        print(x)\n \n def cmd_list_resources(args, opts):\n     \"\"\"list-resources - list available web service resources\"\"\"\n     for x in json_get(opts, '')['resources']:\n-        print x\n+        print(x)\n \n def cmd_get_spider_stats(args, opts):\n     \"\"\"get-spider-stats <spider> - get stats of a running spider\"\"\"\n     stats = jsonrpc_call(opts, 'stats', 'get_stats', args[0])\n     for name, value in stats.items():\n-        print \"%-40s %s\" % (name, value)\n+        print(\"%-40s %s\" % (name, value))\n \n def cmd_get_global_stats(args, opts):\n     \"\"\"get-global-stats - get global stats\"\"\"\n     stats = jsonrpc_call(opts, 'stats', 'get_stats')\n     for name, value in stats.items():\n-        print \"%-40s %s\" % (name, value)\n+        print(\"%-40s %s\" % (name, value))\n \n def get_wsurl(opts, path):\n     return urljoin(\"http://%s:%s/\"% (opts.host, opts.port), path)\n@@ -101,12 +101,12 @@ def main():\n     try:\n         cmd(args, opts)\n     except IndexError:\n-        print cmd.__doc__\n-    except JsonRpcError, e:\n-        print str(e)\n+        print(cmd.__doc__)\n+    except JsonRpcError as e:\n+        print(str(e))\n         if e.data:\n-            print \"Server Traceback below:\"\n-            print e.data\n+            print(\"Server Traceback below:\")\n+            print(e.data)\n \n \n if __name__ == '__main__':\n\n@@ -8,7 +8,7 @@ version_info = tuple(__version__.split('.')[:3])\n import sys, os, warnings\n \n if sys.version_info < (2, 6):\n-    print \"Scrapy %s requires Python 2.6 or above\" % __version__\n+    print(\"Scrapy %s requires Python 2.6 or above\" % __version__)\n     sys.exit(1)\n \n # ignore noisy twisted deprecation warnings\n\n@@ -59,34 +59,34 @@ def _pop_command_name(argv):\n \n def _print_header(settings, inproject):\n     if inproject:\n-        print \"Scrapy %s - project: %s\\n\" % (scrapy.__version__, \\\n-            settings['BOT_NAME'])\n+        print(\"Scrapy %s - project: %s\\n\" % (scrapy.__version__, \\\n+            settings['BOT_NAME']))\n     else:\n-        print \"Scrapy %s - no active project\\n\" % scrapy.__version__\n+        print(\"Scrapy %s - no active project\\n\" % scrapy.__version__)\n \n def _print_commands(settings, inproject):\n     _print_header(settings, inproject)\n-    print \"Usage:\"\n-    print \"  scrapy <command> [options] [args]\\n\"\n-    print \"Available commands:\"\n+    print(\"Usage:\")\n+    print(\"  scrapy <command> [options] [args]\\n\")\n+    print(\"Available commands:\")\n     cmds = _get_commands_dict(settings, inproject)\n     for cmdname, cmdclass in sorted(cmds.iteritems()):\n-        print \"  %-13s %s\" % (cmdname, cmdclass.short_desc())\n+        print(\"  %-13s %s\" % (cmdname, cmdclass.short_desc()))\n     if not inproject:\n-        print\n-        print \"  [ more ]      More commands available when run from project directory\"\n-    print\n-    print 'Use \"scrapy <command> -h\" to see more info about a command'\n+        print()\n+        print(\"  [ more ]      More commands available when run from project directory\")\n+    print()\n+    print('Use \"scrapy <command> -h\" to see more info about a command')\n \n def _print_unknown_command(settings, cmdname, inproject):\n     _print_header(settings, inproject)\n-    print \"Unknown command: %s\\n\" % cmdname\n-    print 'Use \"scrapy\" to see available commands'\n+    print(\"Unknown command: %s\\n\" % cmdname)\n+    print('Use \"scrapy\" to see available commands')\n \n def _run_print_help(parser, func, *a, **kw):\n     try:\n         func(*a, **kw)\n-    except UsageError, e:\n+    except UsageError as e:\n         if str(e):\n             parser.error(str(e))\n         if e.print_help:\n\n@@ -64,9 +64,9 @@ class Command(ScrapyCommand):\n         # start checks\n         if opts.list:\n             for spider, methods in sorted(contract_reqs.iteritems()):\n-                print spider\n+                print(spider)\n                 for method in sorted(methods):\n-                    print '  * %s' % method\n+                    print('  * %s' % method)\n         else:\n             self.crawler_process.start()\n             self.results.printErrors()\n\n@@ -72,7 +72,7 @@ class Command(ScrapyCommand):\n \n         if opts.list_targets:\n             for name, target in _get_targets().items():\n-                print \"%-20s %s\" % (name, target['url'])\n+                print(\"%-20s %s\" % (name, target['url']))\n             return\n \n         if opts.list_projects:\n@@ -81,7 +81,7 @@ class Command(ScrapyCommand):\n             _add_auth_header(req, target)\n             f = urllib2.urlopen(req)\n             projects = json.loads(f.read())['projects']\n-            print os.linesep.join(projects)\n+            print(os.linesep.join(projects))\n             return\n \n         tmpdir = None\n@@ -208,12 +208,12 @@ def _http_post(request):\n     try:\n         f = urllib2.urlopen(request)\n         _log(\"Server response (%s):\" % f.code)\n-        print f.read()\n+        print(f.read())\n         return True\n-    except urllib2.HTTPError, e:\n+    except urllib2.HTTPError as e:\n         _log(\"Deploy failed (%s):\" % e.code)\n-        print e.read()\n-    except urllib2.URLError, e:\n+        print(e.read())\n+    except urllib2.URLError as e:\n         _log(\"Deploy failed: %s\" % e)\n \n def _build_egg():\n\n@@ -30,15 +30,15 @@ class Command(ScrapyCommand):\n     def _print_headers(self, headers, prefix):\n         for key, values in headers.items():\n             for value in values:\n-                print '%s %s: %s' % (prefix, key, value)\n+                print('%s %s: %s' % (prefix, key, value))\n \n     def _print_response(self, response, opts):\n         if opts.headers:\n             self._print_headers(response.request.headers, '>')\n-            print '>'\n+            print('>')\n             self._print_headers(response.headers, '<')\n         else:\n-            print response.body\n+            print(response.body)\n \n     def run(self, args, opts):\n         if len(args) != 1 or not is_url(args[0]):\n\n@@ -49,7 +49,7 @@ class Command(ScrapyCommand):\n         if opts.dump:\n             template_file = self._find_template(opts.dump)\n             if template_file:\n-                print open(template_file, 'r').read()\n+                print(open(template_file, 'r').read())\n             return\n         if len(args) != 2:\n             raise UsageError()\n@@ -58,7 +58,7 @@ class Command(ScrapyCommand):\n         module = sanitize_module_name(name)\n \n         if self.settings.get('BOT_NAME') == module:\n-            print \"Cannot create a spider with the same name as your project\"\n+            print(\"Cannot create a spider with the same name as your project\")\n             return\n \n         try:\n@@ -69,8 +69,8 @@ class Command(ScrapyCommand):\n         else:\n             # if spider already exists and not --force then halt\n             if not opts.force:\n-                print \"Spider %r already exists in module:\" % name\n-                print \"  %s\" % spider.__module__\n+                print(\"Spider %r already exists in module:\" % name)\n+                print(\"  %s\" % spider.__module__)\n                 return\n         template_file = self._find_template(opts.template)\n         if template_file:\n@@ -94,22 +94,22 @@ class Command(ScrapyCommand):\n         spider_file = \"%s.py\" % join(spiders_dir, module)\n         shutil.copyfile(template_file, spider_file)\n         render_templatefile(spider_file, **tvars)\n-        print \"Created spider %r using template %r in module:\" % (name, \\\n-            template_name)\n-        print \"  %s.%s\" % (spiders_module.__name__, module)\n+        print(\"Created spider %r using template %r in module:\" % (name, \\\n+            template_name))\n+        print(\"  %s.%s\" % (spiders_module.__name__, module))\n \n     def _find_template(self, template):\n         template_file = join(self.templates_dir, '%s.tmpl' % template)\n         if exists(template_file):\n             return template_file\n-        print \"Unable to find template: %s\\n\" % template\n-        print 'Use \"scrapy genspider --list\" to see all available templates.'\n+        print(\"Unable to find template: %s\\n\" % template)\n+        print('Use \"scrapy genspider --list\" to see all available templates.')\n \n     def _list_templates(self):\n-        print \"Available templates:\"\n+        print(\"Available templates:\")\n         for filename in sorted(os.listdir(self.templates_dir)):\n             if filename.endswith('.tmpl'):\n-                print \"  %s\" % splitext(filename)[0]\n+                print(\"  %s\" % splitext(filename)[0])\n \n     @property\n     def templates_dir(self):\n\n@@ -11,4 +11,4 @@ class Command(ScrapyCommand):\n     def run(self, args, opts):\n         crawler = self.crawler_process.create_crawler()\n         for s in crawler.spiders.list():\n-            print s\n+            print(s)\n\n@@ -68,7 +68,7 @@ class Command(ScrapyCommand):\n         else:\n             items = self.items.get(lvl, [])\n \n-        print \"# Scraped Items \", \"-\"*60\n+        print(\"# Scraped Items \", \"-\"*60)\n         display.pprint([dict(x) for x in items], colorize=colour)\n \n     def print_requests(self, lvl=None, colour=True):\n@@ -81,7 +81,7 @@ class Command(ScrapyCommand):\n         else:\n             requests = self.requests.get(lvl, [])\n \n-        print \"# Requests \", \"-\"*65\n+        print(\"# Requests \", \"-\"*65)\n         display.pprint(requests, colorize=colour)\n \n     def print_results(self, opts):\n@@ -89,13 +89,13 @@ class Command(ScrapyCommand):\n \n         if opts.verbose:\n             for level in xrange(1, self.max_level+1):\n-                print '\\n>>> DEPTH LEVEL: %s <<<' % level\n+                print('\\n>>> DEPTH LEVEL: %s <<<' % level)\n                 if not opts.noitems:\n                     self.print_items(level, colour)\n                 if not opts.nolinks:\n                     self.print_requests(level, colour)\n         else:\n-            print '\\n>>> STATUS DEPTH LEVEL %s <<<' % self.max_level\n+            print('\\n>>> STATUS DEPTH LEVEL %s <<<' % self.max_level)\n             if not opts.noitems:\n                 self.print_items(colour=colour)\n             if not opts.nolinks:\n\n@@ -67,7 +67,7 @@ class Command(ScrapyCommand):\n             raise UsageError(\"File not found: %s\\n\" % filename)\n         try:\n             module = _import_file(filename)\n-        except (ImportError, ValueError), e:\n+        except (ImportError, ValueError) as e:\n             raise UsageError(\"Unable to load %r: %s\\n\" % (filename, e))\n         spclasses = list(iter_spider_classes(module))\n         if not spclasses:\n\n@@ -27,12 +27,12 @@ class Command(ScrapyCommand):\n     def run(self, args, opts):\n         settings = self.crawler_process.settings\n         if opts.get:\n-            print settings.get(opts.get)\n+            print(settings.get(opts.get))\n         elif opts.getbool:\n-            print settings.getbool(opts.getbool)\n+            print(settings.getbool(opts.getbool))\n         elif opts.getint:\n-            print settings.getint(opts.getint)\n+            print(settings.getint(opts.getint))\n         elif opts.getfloat:\n-            print settings.getfloat(opts.getfloat)\n+            print(settings.getfloat(opts.getfloat))\n         elif opts.getlist:\n-            print settings.getlist(opts.getlist)\n+            print(settings.getlist(opts.getlist))\n\n@@ -36,11 +36,11 @@ class Command(ScrapyCommand):\n             raise UsageError()\n         project_name = args[0]\n         if not re.search(r'^[_a-zA-Z]\\w*$', project_name):\n-            print 'Error: Project names must begin with a letter and contain only\\n' \\\n-                'letters, numbers and underscores'\n+            print('Error: Project names must begin with a letter and contain only\\n' \\\n+                'letters, numbers and underscores')\n             sys.exit(1)\n         elif exists(project_name):\n-            print \"Error: directory %r already exists\" % project_name\n+            print(\"Error: directory %r already exists\" % project_name)\n             sys.exit(1)\n \n         moduletpl = join(TEMPLATES_PATH, 'module')\n\n@@ -25,11 +25,11 @@ class Command(ScrapyCommand):\n             import lxml.etree\n             lxml_version = \".\".join(map(str, lxml.etree.LXML_VERSION))\n             libxml2_version = \".\".join(map(str, lxml.etree.LIBXML_VERSION))\n-            print \"Scrapy  : %s\" % scrapy.__version__\n-            print \"lxml    : %s\" % lxml_version\n-            print \"libxml2 : %s\" % libxml2_version\n-            print \"Twisted : %s\" % twisted.version.short()\n-            print \"Python  : %s\" % sys.version.replace(\"\\n\", \"- \")\n-            print \"Platform: %s\" % platform.platform()\n+            print(\"Scrapy  : %s\" % scrapy.__version__)\n+            print(\"lxml    : %s\" % lxml_version)\n+            print(\"libxml2 : %s\" % libxml2_version)\n+            print(\"Twisted : %s\" % twisted.version.short())\n+            print(\"Python  : %s\" % sys.version.replace(\"\\n\", \"- \"))\n+            print(\"Platform: %s\" % platform.platform())\n         else:\n-            print \"Scrapy %s\" % scrapy.__version__\n+            print(\"Scrapy %s\" % scrapy.__version__)\n\n@@ -51,12 +51,12 @@ class DjangoItem(Item):\n \n         try:\n             self.instance.clean_fields(exclude=exclude)\n-        except ValidationError, e:\n+        except ValidationError as e:\n             self._errors = e.update_error_dict(self._errors)\n \n         try:\n             self.instance.clean()\n-        except ValidationError, e:\n+        except ValidationError as e:\n             self._errors = e.update_error_dict(self._errors)\n \n         # uniqueness is not checked, because it is faster to check it when\n\n@@ -81,7 +81,7 @@ class ItemLoader(object):\n         proc = wrap_loader_context(proc, self.context)\n         try:\n             return proc(self._values[field_name])\n-        except Exception, e:\n+        except Exception as e:\n             raise ValueError(\"Error with output processor: field=%r value=%r error='%s: %s'\" % \\\n                 (field_name, self._values[field_name], type(e).__name__, str(e)))\n \n\n@@ -18,7 +18,7 @@ class DownloadHandlers(object):\n             cls = load_object(clspath)\n             try:\n                 dh = cls(crawler.settings)\n-            except NotConfigured, ex:\n+            except NotConfigured as ex:\n                 self._notconfigured[scheme] = str(ex)\n             else:\n                 self._handlers[scheme] = dh\n\n@@ -41,7 +41,7 @@ class S3DownloadHandler(object):\n \n         try:\n             self.conn = _S3Connection(aws_access_key_id, aws_secret_access_key)\n-        except Exception, ex:\n+        except Exception as ex:\n             raise NotConfigured(str(ex))\n         self._download_http = httpdownloadhandler(settings).download_request\n \n\n@@ -111,7 +111,7 @@ class ExecutionEngine(object):\n                 request = slot.start_requests.next()\n             except StopIteration:\n                 slot.start_requests = None\n-            except Exception, exc:\n+            except Exception as exc:\n                 log.err(None, 'Obtaining request from start requests', \\\n                         spider=spider)\n             else:\n\n@@ -77,7 +77,7 @@ class Scheduler(object):\n         try:\n             reqd = request_to_dict(request, self.spider)\n             self.dqs.push(reqd, -request.priority)\n-        except ValueError, e: # non serializable request\n+        except ValueError as e: # non serializable request\n             if self.logunser:\n                 log.msg(format=\"Unable to serialize request: %(request)s - reason: %(reason)s\",\n                         level=log.ERROR, spider=self.spider,\n\n@@ -34,7 +34,7 @@ class MiddlewareManager(object):\n                 else:\n                     mw = mwcls()\n                 middlewares.append(mw)\n-            except NotConfigured, e:\n+            except NotConfigured as e:\n                 if e.args:\n                     clsname = clspath.split('.')[-1]\n                     log.msg(format=\"Disabled %(clsname)s: %(eargs)s\",\n\n@@ -48,7 +48,7 @@ class Shell(object):\n         else:\n             self.populate_vars()\n         if self.code:\n-            print eval(self.code, globals(), self.vars)\n+            print(eval(self.code, globals(), self.vars))\n         else:\n             start_python_console(self.vars)\n \n@@ -116,7 +116,7 @@ class Shell(object):\n         self.p(\"  view(response)    View response in a browser\")\n \n     def p(self, line=''):\n-        print \"[s] %s\" % line\n+        print(\"[s] %s\" % line)\n \n     def _is_relevant(self, value):\n         return isinstance(value, self.relevant_classes)\n\n@@ -24,7 +24,7 @@ def _serializable_queue(queue_class, serialize, deserialize):\n def _pickle_serialize(obj):\n     try:\n         return pickle.dumps(obj, protocol=2)\n-    except pickle.PicklingError, e:\n+    except pickle.PicklingError as e:\n         raise ValueError(str(e))\n \n PickleFifoDiskQueue = _serializable_queue(queue.FifoDiskQueue, \\\n\n@@ -13,7 +13,7 @@ error = KeyError\n \n _DATABASES = collections.defaultdict(DummyDB)\n \n-def open(file, flag='r', mode=0666):\n+def open(file, flag='r', mode=0o666):\n     \"\"\"Open or create a dummy database compatible.\n \n     Arguments `flag` and `mode` are ignored.\n\n@@ -190,7 +190,7 @@ if __name__ == \"__main__\":\n     def print_listening():\n         httpHost = httpPort.getHost()\n         httpsHost = httpsPort.getHost()\n-        print \"Mock server running at http://%s:%d and https://%s:%d\" % (\n-            httpHost.host, httpHost.port, httpsHost.host, httpsHost.port)\n+        print(\"Mock server running at http://%s:%d and https://%s:%d\" % (\n+            httpHost.host, httpHost.port, httpsHost.host, httpsHost.port))\n     reactor.callWhenRunning(print_listening)\n     reactor.run()\n\n@@ -227,13 +227,13 @@ class ItemLoaderTest(unittest.TestCase):\n         il.add_value('name', [u'$10'])\n         try:\n             float('$10')\n-        except Exception, e:\n+        except Exception as e:\n             expected_exc_str = str(e)\n \n         exc = None\n         try:\n             il.load_item()\n-        except Exception, e:\n+        except Exception as e:\n             exc = e\n         assert isinstance(exc, ValueError)\n         s = str(exc)\n\n@@ -4,7 +4,7 @@ class ScrapyUtilsTest(unittest.TestCase):\n     def test_required_openssl_version(self):\n         try:\n             module = __import__('OpenSSL', {}, {}, [''])\n-        except ImportError, ex:\n+        except ImportError as ex:\n             raise unittest.SkipTest(\"OpenSSL is not available\")\n \n         if hasattr(module, '__version__'):\n\n@@ -242,9 +242,9 @@ class RFC2616PolicyTest(DefaultStorageTest):\n                 assert isinstance(result, Response)\n                 return result\n         except Exception:\n-            print 'Request', request\n-            print 'Response', response\n-            print 'Result', result\n+            print('Request', request)\n+            print('Response', response)\n+            print('Result', result)\n             raise\n \n     def test_request_cacheability(self):\n\n@@ -65,8 +65,8 @@ def start_test_site(debug=False):\n \n     port = reactor.listenTCP(0, server.Site(r), interface=\"127.0.0.1\")\n     if debug:\n-        print \"Test server running at http://localhost:%d/ - hit Ctrl-C to finish.\" \\\n-            % port.getHost().port\n+        print(\"Test server running at http://localhost:%d/ - hit Ctrl-C to finish.\" \\\n+            % port.getHost().port)\n     return port\n \n \n\n@@ -11,7 +11,7 @@ from scrapy.settings import Settings\n skip = False\n try:\n     from PIL import Image\n-except ImportError, e:\n+except ImportError as e:\n     skip = 'Missing Python Imaging Library, install https://pypi.python.org/pypi/Pillow'\n else:\n     encoders = set(('jpeg_encoder', 'jpeg_decoder'))\n\n@@ -193,7 +193,7 @@ class SelectorTestCase(unittest.TestCase):\n         xpath = \"//test[@foo='bar]\"\n         try:\n             x.xpath(xpath)\n-        except ValueError, e:\n+        except ValueError as e:\n             assert xpath in str(e), \"Exception message does not contain invalid xpath\"\n         except Exception:\n             raise AssertionError(\"A invalid XPath does not raise ValueError\")\n\n@@ -53,7 +53,7 @@ class DeferUtilsTest(unittest.TestCase):\n         gotexc = False\n         try:\n             yield process_chain([cb1, cb_fail, cb3], 'res', 'v1', 'v2')\n-        except TypeError, e:\n+        except TypeError as e:\n             gotexc = True\n         self.failUnless(gotexc)\n \n\n@@ -59,7 +59,7 @@ class JsonRpcUtilsTestCase(unittest.TestCase):\n         raised = False\n         try:\n             jsonrpc_client_call('url', 'test', _urllib=ul)\n-        except JsonRpcError, e:\n+        except JsonRpcError as e:\n             raised = True\n             self.assertEqual(e.code, 123)\n             self.assertEqual(e.message, 'hello')\n\n@@ -46,7 +46,7 @@ class MultiValueDict(dict):\n         try:\n             list_ = dict.__getitem__(self, key)\n         except KeyError:\n-            raise MultiValueDictKeyError, \"Key %r not found in %r\" % (key, self)\n+            raise MultiValueDictKeyError(\"Key %r not found in %r\" % (key, self))\n         try:\n             return list_[-1]\n         except IndexError:\n@@ -124,7 +124,7 @@ class MultiValueDict(dict):\n     def update(self, *args, **kwargs):\n         \"update() extends rather than replaces existing key lists. Also accepts keyword args.\"\n         if len(args) > 1:\n-            raise TypeError, \"update expected at most 1 arguments, got %d\" % len(args)\n+            raise TypeError(\"update expected at most 1 arguments, got %d\" % len(args))\n         if args:\n             other_dict = args[0]\n             if isinstance(other_dict, MultiValueDict):\n@@ -135,7 +135,7 @@ class MultiValueDict(dict):\n                     for key, value in other_dict.items():\n                         self.setlistdefault(key, []).append(value)\n                 except TypeError:\n-                    raise ValueError, \"MultiValueDict.update() takes either a MultiValueDict or dictionary\"\n+                    raise ValueError(\"MultiValueDict.update() takes either a MultiValueDict or dictionary\")\n         for key, value in kwargs.iteritems():\n             self.setlistdefault(key, []).append(value)\n \n\n@@ -40,7 +40,7 @@ def mustbe_deferred(f, *args, **kw):\n     # FIXME: Hack to avoid introspecting tracebacks. This to speed up\n     # processing of IgnoreRequest errors which are, by far, the most common\n     # exception in Scrapy - see #125\n-    except IgnoreRequest, e:\n+    except IgnoreRequest as e:\n         return defer_fail(failure.Failure(e))\n     except:\n         return defer_fail(failure.Failure())\n\n@@ -20,4 +20,4 @@ def pformat(obj, *args, **kwargs):\n     return _colorize(pformat_(obj), kwargs.pop('colorize', True))\n \n def pprint(obj, *args, **kwargs):\n-    print pformat(obj, *args, **kwargs)\n+    print(pformat(obj, *args, **kwargs))\n\n@@ -27,14 +27,14 @@ def get_engine_status(engine):\n     for test in global_tests:\n         try:\n             status['global'] += [(test, eval(test))]\n-        except Exception, e:\n+        except Exception as e:\n             status['global'] += [(test, \"%s (exception)\" % type(e).__name__)]\n     for spider in engine.slots.keys():\n         x = []\n         for test in spider_tests:\n             try:\n                 x += [(test, eval(test))]\n-            except Exception, e:\n+            except Exception as e:\n                 x += [(test, \"%s (exception)\" % type(e).__name__)]\n             status['spiders'][spider] = x\n     return status\n@@ -52,5 +52,5 @@ def format_engine_status(engine=None):\n     return s\n \n def print_engine_status(engine):\n-    print format_engine_status(engine)\n+    print(format_engine_status(engine))\n \n\n@@ -53,7 +53,7 @@ def jsonrpc_server_call(target, jsonrpc_request, json_decoder=None):\n \n     try:\n         req = json_decoder.decode(jsonrpc_request)\n-    except Exception, e:\n+    except Exception as e:\n         return jsonrpc_error(None, jsonrpc_errors.PARSE_ERROR, 'Parse error', \\\n             traceback.format_exc())\n \n@@ -72,7 +72,7 @@ def jsonrpc_server_call(target, jsonrpc_request, json_decoder=None):\n     kw = dict([(str(k), v) for k, v in kw.items()]) # convert kw keys to str\n     try:\n         return jsonrpc_result(id, method(*a, **kw))\n-    except Exception, e:\n+    except Exception as e:\n         return jsonrpc_error(id, jsonrpc_errors.INTERNAL_ERROR, str(e), \\\n             traceback.format_exc())\n \n\n@@ -31,18 +31,18 @@ def load_object(path):\n     try:\n         dot = path.rindex('.')\n     except ValueError:\n-        raise ValueError, \"Error loading object '%s': not a full path\" % path\n+        raise ValueError(\"Error loading object '%s': not a full path\" % path)\n \n     module, name = path[:dot], path[dot+1:]\n     try:\n         mod = __import__(module, {}, {}, [''])\n-    except ImportError, e:\n-        raise ImportError, \"Error loading object '%s': %s\" % (path, e)\n+    except ImportError as e:\n+        raise ImportError(\"Error loading object '%s': %s\" % (path, e))\n \n     try:\n         obj = getattr(mod, name)\n     except AttributeError:\n-        raise NameError, \"Module '%s' doesn't define any object named '%s'\" % (module, name)\n+        raise NameError(\"Module '%s' doesn't define any object named '%s'\" % (module, name))\n \n     return obj\n \n\n@@ -274,6 +274,6 @@ def retry_on_eintr(function, *args, **kw):\n     while True:\n         try:\n             return function(*args, **kw)\n-        except IOError, e:\n+        except IOError as e:\n             if e.errno != errno.EINTR:\n                 raise\n\n@@ -13,7 +13,7 @@ def assert_aws_environ():\n     \"\"\"\n     try:\n         import boto\n-    except ImportError, e:\n+    except ImportError as e:\n         raise SkipTest(str(e))\n \n     if 'AWS_ACCESS_KEY_ID' not in os.environ:\n\n@@ -27,5 +27,5 @@ def test_site():\n \n if __name__ == '__main__':\n     port = reactor.listenTCP(0, test_site(), interface=\"127.0.0.1\")\n-    print \"http://localhost:%d/\" % port.getHost().port\n+    print(\"http://localhost:%d/\" % port.getHost().port)\n     reactor.run()\n\n@@ -42,7 +42,7 @@ def format_live_refs(ignore=NoneType):\n     return s\n \n def print_live_refs(*a, **kw):\n-    print format_live_refs(*a, **kw)\n+    print(format_live_refs(*a, **kw))\n \n def get_oldest(class_name):\n     for cls, wdict in live_refs.iteritems():\n\n@@ -33,12 +33,6 @@ __author__ = \"Patrick K. O'Brien <pobrien@orbtech.com>\"\n __cvsid__ = \"$Id: dispatcher.py,v 1.1.1.1 2006/07/07 15:59:38 mcfletch Exp $\"\n __version__ = \"$Revision: 1.1.1.1 $\"[11:-2]\n \n-try:\n-\tTrue\n-except NameError:\n-\tTrue = 1==1\n-\tFalse = 1==0\n-\n class _Parameter:\n \t\"\"\"Used to represent default parameter values.\"\"\"\n \tdef __repr__(self):\n@@ -379,13 +373,13 @@ def _removeReceiver(receiver):\n \tbackKey = id(receiver)\n \ttry:\n \t\tbackSet = sendersBack.pop(backKey)\n-\texcept KeyError, err:\n+\texcept KeyError as err:\n \t\treturn False \n \telse:\n \t\tfor senderkey in backSet:\n \t\t\ttry:\n \t\t\t\tsignals = connections[senderkey].keys()\n-\t\t\texcept KeyError,err:\n+\t\t\texcept KeyError as err:\n \t\t\t\tpass\n \t\t\telse:\n \t\t\t\tfor signal in signals:\n@@ -396,7 +390,7 @@ def _removeReceiver(receiver):\n \t\t\t\t\telse:\n \t\t\t\t\t\ttry:\n \t\t\t\t\t\t\treceivers.remove( receiver )\n-\t\t\t\t\t\texcept Exception, err:\n+\t\t\t\t\t\texcept Exception as err:\n \t\t\t\t\t\t\tpass\n \t\t\t\t\t_cleanupConnections(senderkey, signal)\n \n\n@@ -50,7 +50,7 @@ def sendRobust(\n \t\t\t\t*arguments,\n \t\t\t\t**named\n \t\t\t)\n-\t\texcept Exception, err:\n+\t\texcept Exception as err:\n \t\t\tresponses.append((receiver, err))\n \t\telse:\n \t\t\tresponses.append((receiver, response))\n\n@@ -109,13 +109,13 @@ class BoundMethodWeakref(object):\n \t\t\t\ttry:\n \t\t\t\t\tif callable( function ):\n \t\t\t\t\t\tfunction( self )\n-\t\t\t\texcept Exception, e:\n+\t\t\t\texcept Exception as e:\n \t\t\t\t\ttry:\n \t\t\t\t\t\ttraceback.print_exc()\n-\t\t\t\t\texcept AttributeError, err:\n-\t\t\t\t\t\tprint '''Exception during saferef %s cleanup function %s: %s'''%(\n+\t\t\t\t\texcept AttributeError as err:\n+\t\t\t\t\t\tprint('''Exception during saferef %s cleanup function %s: %s'''%(\n \t\t\t\t\t\t\tself, function, e\n-\t\t\t\t\t\t)\n+\t\t\t\t\t\t))\n \t\tself.deletionMethods = [onDelete]\n \t\tself.key = self.calculateKey( target )\n \t\tself.weakSelf = weakref.ref(target.im_self, remove)\n\n@@ -392,7 +392,7 @@ class HTTPClientParser(HTTPParser):\n             proto, strnumber = strversion.split('/')\n             major, minor = strnumber.split('.')\n             major, minor = int(major), int(minor)\n-        except ValueError, e:\n+        except ValueError as e:\n             raise BadResponseVersion(str(e), strversion)\n         if major < 0 or minor < 0:\n             raise BadResponseVersion(\"version may not be negative\", strversion)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c2ced9a2341d63e8b66ff39e8f3c170e32dadff6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 152 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -10,7 +10,7 @@ class Command(ScrapyCommand):\n         return \"[options] <spider>\"\n \n     def short_desc(self):\n-        return \"Start crawling from a spider or URL\"\n+        return \"Start crawling from a spider\"\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ae4a61a2d530bc55103ea786f950abde3ce6739d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 0 | Files Changed: 27 | Hunks: 27 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 27 | Churn Cumulative: 6869 | Contributors (this commit): 19 | Commits (past 90d): 70 | Contributors (cumulative): 101 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,5 @@\n #!/usr/bin/env python\n+from __future__ import print_function\n from time import time\n from collections import deque\n from twisted.web.server import Site, NOT_DONE_YET\n\n@@ -11,6 +11,7 @@ command).\n \n \"\"\"\n \n+from __future__ import print_function\n import sys, optparse, urllib, json\n from urlparse import urljoin\n \n\n@@ -1,6 +1,7 @@\n \"\"\"\n Scrapy - a screen scraping framework written in Python\n \"\"\"\n+from __future__ import print_function\n import pkgutil\n __version__ = pkgutil.get_data(__package__, 'VERSION').strip()\n version_info = tuple(__version__.split('.')[:3])\n\n@@ -1,3 +1,4 @@\n+from __future__ import print_function\n import sys\n import optparse\n import cProfile\n\n@@ -1,3 +1,4 @@\n+from __future__ import print_function\n from collections import defaultdict\n from functools import wraps\n from unittest import TextTestRunner\n\n@@ -1,3 +1,4 @@\n+from __future__ import print_function\n import sys\n import os\n import glob\n\n@@ -1,3 +1,4 @@\n+from __future__ import print_function\n from w3lib.url import is_url\n \n from scrapy.command import ScrapyCommand\n\n@@ -1,3 +1,4 @@\n+from __future__ import print_function\n import os\n import shutil\n import string\n\n@@ -1,3 +1,4 @@\n+from __future__ import print_function\n from scrapy.command import ScrapyCommand\n \n class Command(ScrapyCommand):\n\n@@ -1,3 +1,4 @@\n+from __future__ import print_function\n from w3lib.url import is_url\n from scrapy.command import ScrapyCommand\n from scrapy.http import Request\n\n@@ -1,3 +1,4 @@\n+from __future__ import print_function\n from scrapy.command import ScrapyCommand\n \n class Command(ScrapyCommand):\n\n@@ -1,3 +1,4 @@\n+from __future__ import print_function\n import sys\n import string\n import re\n\n@@ -1,3 +1,4 @@\n+from __future__ import print_function\n import sys\n import platform\n \n\n@@ -1,3 +1,4 @@\n+from __future__ import print_function\n import os\n import cPickle as pickle\n from time import time\n\n@@ -1,3 +1,4 @@\n+from __future__ import print_function\n from collections import defaultdict\n from twisted.internet.defer import Deferred, DeferredList\n from twisted.python.failure import Failure\n\n@@ -1,3 +1,4 @@\n+from __future__ import print_function\n import os\n from scrapy.utils.request import request_fingerprint\n from scrapy.utils.job import job_dir\n\n@@ -3,6 +3,7 @@ Scrapy Shell\n \n See documentation in docs/topics/shell.rst\n \"\"\"\n+from __future__ import print_function\n import signal\n \n from twisted.internet import reactor, threads, defer\n\n@@ -1,3 +1,4 @@\n+from __future__ import print_function\n import sys, time, random, urllib, os\n from subprocess import Popen, PIPE\n from twisted.web.server import Site, NOT_DONE_YET\n\n@@ -1,3 +1,4 @@\n+from __future__ import print_function\n import time\n import tempfile\n import shutil\n\n@@ -10,6 +10,7 @@ module with the ``runserver`` argument::\n     python test_engine.py runserver\n \"\"\"\n \n+from __future__ import print_function\n import sys, os, re, urlparse\n \n from twisted.internet import reactor, defer\n\n@@ -1,3 +1,4 @@\n+from __future__ import print_function\n from twisted.trial import unittest\n from twisted.python.failure import Failure\n from twisted.internet import reactor\n\n@@ -1,3 +1,4 @@\n+from __future__ import print_function\n import unittest\n from scrapy.http import Request\n from scrapy.utils.request import request_fingerprint, _fingerprint_cache, \\\n\n@@ -2,6 +2,7 @@\n pprint and pformat wrappers with colorization support\n \"\"\"\n \n+from __future__ import print_function\n import sys\n from pprint import pformat as pformat_\n \n\n@@ -1,5 +1,6 @@\n \"\"\"Some debugging functions for working with the Scrapy engine\"\"\"\n \n+from __future__ import print_function\n from time import time # used in global tests code\n \n def get_engine_status(engine):\n\n@@ -3,6 +3,7 @@ This module provides some useful functions for working with\n scrapy.http.Request objects\n \"\"\"\n \n+from __future__ import print_function\n import hashlib\n import weakref\n from urlparse import urlunparse\n\n@@ -1,3 +1,4 @@\n+from __future__ import print_function\n import urlparse\n \n from twisted.internet import reactor\n\n@@ -9,6 +9,7 @@ and no performance penalty at all when disabled (as object_ref becomes just an\n alias to object in that case).\n \"\"\"\n \n+from __future__ import print_function\n import weakref, os\n from collections import defaultdict\n from time import time\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
