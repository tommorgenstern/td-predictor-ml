{"custom_id": "keras#ee17ccc3746dbbc8bc20e2bc48ea5dc11ef36f30", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 86 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,9 +1,6 @@\n import pytest\n-import numpy as np\n-from numpy.testing import assert_allclose\n \n from keras.utils.test_utils import layer_test\n-from keras import backend as K\n from keras.layers import local\n \n \n@@ -74,6 +71,4 @@ def test_locallyconnected_2d():\n \n \n if __name__ == '__main__':\n-    # pytest.main([__file__])\n-    test_locallyconnected_1d()\n-    test_locallyconnected_2d()\n+    pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3927505d1a2bd6f14544bc623da8c725630a0d80", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 288 | Lines Deleted: 23 | Files Changed: 3 | Hunks: 34 | Methods Changed: 23 | Complexity Δ (Sum/Max): 25/12 | Churn Δ: 311 | Churn Cumulative: 9872 | Contributors (this commit): 56 | Commits (past 90d): 58 | Contributors (cumulative): 63 | DMM Complexity: 0.6342857142857142\n\nDIFF:\n@@ -5,6 +5,7 @@ import warnings\n import copy\n import time\n import numpy as np\n+import multiprocessing\n import threading\n try:\n     import queue\n@@ -401,13 +402,23 @@ def standardize_weights(y, sample_weight=None, class_weight=None,\n \n \n def generator_queue(generator, max_q_size=10,\n-                    wait_time=0.05, nb_worker=1):\n-    '''Builds a threading queue out of a data generator.\n+                    wait_time=0.05, nb_worker=1, pickle_safe=False):\n+    '''Builds a queue out of a data generator.\n+    If pickle_safe, use a multiprocessing approach. Else, use threading.\n     Used in `fit_generator`, `evaluate_generator`, `predict_generator`.\n+\n     '''\n+\n+    generator_threads = []\n+    if pickle_safe:\n+        q = multiprocessing.Queue(maxsize=max_q_size)\n+        _stop = multiprocessing.Event()\n+    else:\n         q = queue.Queue()\n         _stop = threading.Event()\n \n+    try:\n+\n         def data_generator_task():\n             while not _stop.is_set():\n                 try:\n@@ -423,13 +434,27 @@ def generator_queue(generator, max_q_size=10,\n                     _stop.set()\n                     raise\n \n-    generator_threads = [threading.Thread(target=data_generator_task)\n-                         for _ in range(nb_worker)]\n-\n-    for thread in generator_threads:\n+        for i in range(nb_worker):\n+            if pickle_safe:\n+                # Reset random seed else all children processes share the same seed\n+                np.random.seed()\n+                thread = multiprocessing.Process(target=data_generator_task)\n+            else:\n+                thread = threading.Thread(target=data_generator_task)\n+            generator_threads.append(thread)\n             thread.daemon = True\n             thread.start()\n \n+    except:\n+        _stop.set()\n+        if pickle_safe:\n+            # Terminate all daemon processes\n+            for p in generator_threads:\n+                if p.is_alive():\n+                    p.terminate()\n+            q.close()\n+        raise\n+\n     return q, _stop\n \n \n@@ -1035,7 +1060,8 @@ class Model(Container):\n             split_at = int(len(x[0]) * (1. - validation_split))\n             x, val_x = (slice_X(x, 0, split_at), slice_X(x, split_at))\n             y, val_y = (slice_X(y, 0, split_at), slice_X(y, split_at))\n-            sample_weights, val_sample_weights = (slice_X(sample_weights, 0, split_at), slice_X(sample_weights, split_at))\n+            sample_weights, val_sample_weights = (\n+                slice_X(sample_weights, 0, split_at), slice_X(sample_weights, split_at))\n             self._make_test_function()\n             val_f = self.test_function\n             if self.uses_learning_phase:\n@@ -1255,7 +1281,7 @@ class Model(Container):\n     def fit_generator(self, generator, samples_per_epoch, nb_epoch,\n                       verbose=1, callbacks=[],\n                       validation_data=None, nb_val_samples=None,\n-                      class_weight={}, max_q_size=10):\n+                      class_weight={}, max_q_size=10, nb_worker=1, pickle_safe=False):\n         '''Fits the model on data generated batch-by-batch by\n         a Python generator.\n         The generator is run in parallel to the model, for efficiency.\n@@ -1286,6 +1312,11 @@ class Model(Container):\n             class_weight: dictionary mapping class indices to a weight\n                 for the class.\n             max_q_size: maximum size for the generator queue\n+            nb_worker: maximum number of processes to spin up when using process based threading\n+            pickle_safe: if True, use process based threading. Note that because\n+                this implementation relies on multiprocessing, you should not pass non\n+                non picklable arguments to the generator as they can't be passed\n+                easily to children processes.\n \n         # Returns\n             A `History` object.\n@@ -1364,7 +1395,8 @@ class Model(Container):\n             self.validation_data = None\n \n         # start generator thread storing batches into a queue\n-        data_gen_queue, _stop = generator_queue(generator, max_q_size=max_q_size)\n+        data_gen_queue, _stop = generator_queue(generator, max_q_size=max_q_size, nb_worker=nb_worker,\n+                                                pickle_safe=pickle_safe)\n \n         callback_model.stop_training = False\n         while epoch < nb_epoch:\n@@ -1457,10 +1489,12 @@ class Model(Container):\n                 break\n \n         _stop.set()\n+        if pickle_safe:\n+            data_gen_queue.close()\n         callbacks.on_train_end()\n         return self.history\n \n-    def evaluate_generator(self, generator, val_samples, max_q_size=10):\n+    def evaluate_generator(self, generator, val_samples, max_q_size=10, nb_worker=1, pickle_safe=False):\n         '''Evaluates the model on a data generator. The generator should\n         return the same kind of data as accepted by `test_on_batch`.\n \n@@ -1472,6 +1506,11 @@ class Model(Container):\n                 total number of samples to generate from `generator`\n                 before returning.\n             max_q_size: maximum size for the generator queue\n+            nb_worker: maximum number of processes to spin up when using process based threading\n+            pickle_safe: if True, use process based threading. Note that because\n+                this implementation relies on multiprocessing, you should not pass non\n+                non picklable arguments to the generator as they can't be passed\n+                easily to children processes.\n \n         # Returns\n             Scalar test loss (if the model has a single output and no metrics)\n@@ -1485,7 +1524,8 @@ class Model(Container):\n         wait_time = 0.01\n         all_outs = []\n         weights = []\n-        data_gen_queue, _stop = generator_queue(generator, max_q_size=max_q_size)\n+        data_gen_queue, _stop = generator_queue(generator, max_q_size=max_q_size, nb_worker=nb_worker,\n+                                                pickle_safe=pickle_safe)\n \n         while processed_samples < val_samples:\n             generator_output = None\n@@ -1529,6 +1569,8 @@ class Model(Container):\n             weights.append(nb_samples)\n \n         _stop.set()\n+        if pickle_safe:\n+            data_gen_queue.close()\n         if type(outs) is not list:\n             return np.average(np.asarray(all_outs),\n                               weights=weights)\n@@ -1539,7 +1581,7 @@ class Model(Container):\n                                            weights=weights))\n             return averages\n \n-    def predict_generator(self, generator, val_samples, max_q_size=10):\n+    def predict_generator(self, generator, val_samples, max_q_size=10, nb_worker=1, pickle_safe=False):\n         '''Generates predictions for the input samples from a data generator.\n         The generator should return the same kind of data as accepted by\n         `predict_on_batch`.\n@@ -1549,6 +1591,11 @@ class Model(Container):\n             val_samples: total number of samples to generate from `generator`\n                 before returning.\n             max_q_size: maximum size for the generator queue\n+            nb_worker: maximum number of processes to spin up when using process based threading\n+            pickle_safe: if True, use process based threading. Note that because\n+                this implementation relies on multiprocessing, you should not pass non\n+                non picklable arguments to the generator as they can't be passed\n+                easily to children processes.\n \n         # Returns\n             Numpy array(s) of predictions.\n@@ -1558,7 +1605,8 @@ class Model(Container):\n         processed_samples = 0\n         wait_time = 0.01\n         all_outs = []\n-        data_gen_queue, _stop = generator_queue(generator, max_q_size=max_q_size)\n+        data_gen_queue, _stop = generator_queue(generator, max_q_size=max_q_size, nb_worker=nb_worker,\n+                                                pickle_safe=pickle_safe)\n \n         while processed_samples < val_samples:\n             generator_output = None\n@@ -1610,6 +1658,8 @@ class Model(Container):\n             processed_samples += nb_samples\n \n         _stop.set()\n+        if pickle_safe:\n+            data_gen_queue.close()\n         if len(all_outs) == 1:\n             return all_outs[0]\n         return all_outs\n\n@@ -578,7 +578,7 @@ class Sequential(Model):\n     def fit_generator(self, generator, samples_per_epoch, nb_epoch,\n                       verbose=1, callbacks=[],\n                       validation_data=None, nb_val_samples=None,\n-                      class_weight=None, max_q_size=10, **kwargs):\n+                      class_weight=None, max_q_size=10, nb_worker=1, pickle_safe=False, **kwargs):\n         '''Fits the model on data generated batch-by-batch by\n         a Python generator.\n         The generator is run in parallel to the model, for efficiency.\n@@ -609,6 +609,11 @@ class Sequential(Model):\n             class_weight: dictionary mapping class indices to a weight\n                 for the class.\n             max_q_size: maximum size for the generator queue\n+            nb_worker: maximum number of processes to spin up\n+            pickle_safe: if True, use process based threading. Note that because\n+                this implementation relies on multiprocessing, you should not pass non\n+                non picklable arguments to the generator as they can't be passed\n+                easily to children processes.\n \n         # Returns\n             A `History` object.\n@@ -632,6 +637,9 @@ class Sequential(Model):\n         '''\n         if self.model is None:\n             raise Exception('The model needs to be compiled before being used.')\n+        if nb_worker > 1 and not pickle_safe:\n+            warnings.warn('The \"nb_worker\" argument is deprecated when pickle_safe is False')\n+            nb_worker = 1  # For backward compatibility\n         if 'show_accuracy' in kwargs:\n             kwargs.pop('show_accuracy')\n             warnings.warn('The \"show_accuracy\" argument is deprecated, '\n@@ -639,10 +647,6 @@ class Sequential(Model):\n                           'the model at compile time:\\n'\n                           '`model.compile(optimizer, loss, '\n                           'metrics=[\"accuracy\"])`')\n-        if 'nb_worker' in kwargs:\n-            kwargs.pop('nb_worker')\n-            warnings.warn('The \"nb_worker\" argument is deprecated, '\n-                          'please remove it from your code.')\n         if 'nb_val_worker' in kwargs:\n             kwargs.pop('nb_val_worker')\n             warnings.warn('The \"nb_val_worker\" argument is deprecated, '\n@@ -658,9 +662,11 @@ class Sequential(Model):\n                                         validation_data=validation_data,\n                                         nb_val_samples=nb_val_samples,\n                                         class_weight=class_weight,\n-                                        max_q_size=max_q_size)\n+                                        max_q_size=max_q_size,\n+                                        nb_worker=nb_worker,\n+                                        pickle_safe=pickle_safe)\n \n-    def evaluate_generator(self, generator, val_samples, max_q_size=10, **kwargs):\n+    def evaluate_generator(self, generator, val_samples, max_q_size=10, nb_worker=1, pickle_safe=False, **kwargs):\n         '''Evaluates the model on a data generator. The generator should\n         return the same kind of data as accepted by `test_on_batch`.\n \n@@ -672,9 +678,17 @@ class Sequential(Model):\n                 total number of samples to generate from `generator`\n                 before returning.\n             max_q_size: maximum size for the generator queue\n+            nb_worker: maximum number of processes to spin up\n+            pickle_safe: if True, use process based threading. Note that because\n+                this implementation relies on multiprocessing, you should not pass non\n+                non picklable arguments to the generator as they can't be passed\n+                easily to children processes.\n         '''\n         if self.model is None:\n             raise Exception('The model needs to be compiled before being used.')\n+        if nb_worker > 1 and not pickle_safe:\n+            warnings.warn('The \"nb_worker\" argument is deprecated when pickle_safe is False')\n+            nb_worker = 1  # For backward compatibility\n         if 'show_accuracy' in kwargs:\n             kwargs.pop('show_accuracy')\n             warnings.warn('The \"show_accuracy\" argument is deprecated, '\n@@ -690,9 +704,11 @@ class Sequential(Model):\n                             str(kwargs))\n         return self.model.evaluate_generator(generator,\n                                              val_samples,\n-                                             max_q_size=max_q_size)\n+                                             max_q_size=max_q_size,\n+                                             nb_worker=nb_worker,\n+                                             pickle_safe=pickle_safe)\n \n-    def predict_generator(self, generator, val_samples, max_q_size=10):\n+    def predict_generator(self, generator, val_samples, max_q_size=10, nb_worker=1, pickle_safe=False):\n         '''Generates predictions for the input samples from a data generator.\n         The generator should return the same kind of data as accepted by\n         `predict_on_batch`.\n@@ -702,14 +718,24 @@ class Sequential(Model):\n             val_samples: total number of samples to generate from `generator`\n                 before returning.\n             max_q_size: maximum size for the generator queue\n+            nb_worker: maximum number of processes to spin up\n+            pickle_safe: if True, use process based threading. Note that because\n+                this implementation relies on multiprocessing, you should not pass non\n+                non picklable arguments to the generator as they can't be passed\n+                easily to children processes.\n \n         # Returns\n             A Numpy array of predictions.\n         '''\n         if self.model is None:\n             self.build()\n+        if nb_worker > 1 and not pickle_safe:\n+            warnings.warn('The \"nb_worker\" argument is deprecated when pickle_safe is False')\n+            nb_worker = 1  # For backward compatibility\n         return self.model.predict_generator(generator, val_samples,\n-                                            max_q_size=max_q_size)\n+                                            max_q_size=max_q_size,\n+                                            nb_worker=nb_worker,\n+                                            pickle_safe=pickle_safe)\n \n     def get_config(self):\n         '''Returns the model configuration\n\n@@ -0,0 +1,189 @@\n+from __future__ import print_function\n+import pytest\n+import numpy as np\n+from keras.models import Sequential\n+from keras.layers.core import Dense, Activation\n+\n+\n+def test_multiprocessing_training():\n+\n+    reached_end = False\n+\n+    arr_data = np.random.randint(0,256, (500, 200))\n+    arr_labels = np.random.randint(0, 2, 500)\n+\n+    def myGenerator():\n+\n+        batch_size = 32\n+        n_samples = 500\n+\n+        while True:\n+            batch_index = np.random.randint(0, n_samples - batch_size)\n+            start = batch_index\n+            end = start + batch_size\n+            X = arr_data[start: end]\n+            y = arr_labels[start: end]\n+            yield X, y\n+\n+    # Build a NN\n+    model = Sequential()\n+    model.add(Dense(10, input_shape=(200, )))\n+    model.add(Activation('relu'))\n+    model.add(Dense(1))\n+    model.add(Activation('linear'))\n+    model.compile(loss='mse', optimizer='adadelta')\n+\n+    model.fit_generator(myGenerator(),\n+                        samples_per_epoch=320,\n+                        nb_epoch=1,\n+                        verbose=1,\n+                        max_q_size=10,\n+                        nb_worker=4,\n+                        pickle_safe=True)\n+\n+    model.fit_generator(myGenerator(),\n+                        samples_per_epoch=320,\n+                        nb_epoch=1,\n+                        verbose=1,\n+                        max_q_size=10,\n+                        pickle_safe=False)\n+\n+    reached_end = True\n+\n+    assert reached_end\n+\n+\n+def test_multiprocessing_training_fromfile():\n+\n+    reached_end = False\n+\n+    arr_data = np.random.randint(0,256, (500, 200))\n+    arr_labels = np.random.randint(0, 2, 500)\n+    np.savez(\"data.npz\", **{\"data\": arr_data, \"labels\": arr_labels})\n+\n+    def myGenerator():\n+\n+        batch_size = 32\n+        n_samples = 500\n+\n+        arr = np.load(\"data.npz\")\n+\n+        while True:\n+            batch_index = np.random.randint(0, n_samples - batch_size)\n+            start = batch_index\n+            end = start + batch_size\n+            X = arr[\"data\"][start: end]\n+            y = arr[\"labels\"][start: end]\n+            yield X, y\n+\n+    # Build a NN\n+    model = Sequential()\n+    model.add(Dense(10, input_shape=(200, )))\n+    model.add(Activation('relu'))\n+    model.add(Dense(1))\n+    model.add(Activation('linear'))\n+    model.compile(loss='mse', optimizer='adadelta')\n+\n+    model.fit_generator(myGenerator(),\n+                        samples_per_epoch=320,\n+                        nb_epoch=1,\n+                        verbose=1,\n+                        max_q_size=10,\n+                        nb_worker=2,\n+                        pickle_safe=True)\n+\n+    model.fit_generator(myGenerator(),\n+                        samples_per_epoch=320,\n+                        nb_epoch=1,\n+                        verbose=1,\n+                        max_q_size=10,\n+                        pickle_safe=False)\n+    reached_end = True\n+\n+    assert reached_end\n+\n+\n+def test_multiprocessing_predicting():\n+\n+    reached_end = False\n+\n+    arr_data = np.random.randint(0,256, (500, 200))\n+\n+    def myGenerator():\n+\n+        batch_size = 32\n+        n_samples = 500\n+\n+        while True:\n+            batch_index = np.random.randint(0, n_samples - batch_size)\n+            start = batch_index\n+            end = start + batch_size\n+            X = arr_data[start: end]\n+            yield X\n+\n+    # Build a NN\n+    model = Sequential()\n+    model.add(Dense(10, input_shape=(200, )))\n+    model.add(Activation('relu'))\n+    model.add(Dense(1))\n+    model.add(Activation('linear'))\n+    model.compile(loss='mse', optimizer='adadelta')\n+    model.predict_generator(myGenerator(),\n+                            val_samples=320,\n+                            max_q_size=10,\n+                            nb_worker=2,\n+                            pickle_safe=True)\n+    model.predict_generator(myGenerator(),\n+                            val_samples=320,\n+                            max_q_size=10,\n+                            pickle_safe=False)\n+    reached_end = True\n+\n+    assert reached_end\n+\n+\n+def test_multiprocessing_evaluating():\n+\n+    reached_end = False\n+\n+    arr_data = np.random.randint(0,256, (500, 200))\n+    arr_labels = np.random.randint(0, 2, 500)\n+\n+    def myGenerator():\n+\n+        batch_size = 32\n+        n_samples = 500\n+\n+        while True:\n+            batch_index = np.random.randint(0, n_samples - batch_size)\n+            start = batch_index\n+            end = start + batch_size\n+            X = arr_data[start: end]\n+            y = arr_labels[start: end]\n+            yield X, y\n+\n+    # Build a NN\n+    model = Sequential()\n+    model.add(Dense(10, input_shape=(200, )))\n+    model.add(Activation('relu'))\n+    model.add(Dense(1))\n+    model.add(Activation('linear'))\n+    model.compile(loss='mse', optimizer='adadelta')\n+\n+    model.evaluate_generator(myGenerator(),\n+                             val_samples=320,\n+                             max_q_size=10,\n+                             nb_worker=2,\n+                             pickle_safe=True)\n+    model.evaluate_generator(myGenerator(),\n+                             val_samples=320,\n+                             max_q_size=10,\n+                             pickle_safe=False)\n+    reached_end = True\n+\n+    assert reached_end\n+\n+\n+if __name__ == '__main__':\n+\n+    pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b4adce34dcf026679ca7b19ccede9104719e544a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 10444 | Contributors (this commit): 49 | Commits (past 90d): 15 | Contributors (cumulative): 49 | DMM Complexity: None\n\nDIFF:\n@@ -387,7 +387,14 @@ class Lambda(Layer):\n         function: The function to be evaluated.\n             Takes one argument: the output of previous layer\n         output_shape: Expected output shape from function.\n-            Could be a tuple or a function of the shape of the input\n+            Can be a tuple or function.\n+            If a tuple, it only specifies the first dimension onward; \n+                 sample dimension is assumed either the same as the input:\n+                 `output_shape = (input_shape[0], ) + output_shape`\n+                 or, the input is `None` and the sample dimension is also `None`:\n+                 `output_shape = (None, ) + output_shape`\n+            If a function, it specifies the entire shape as a function of \n+                 the input shape: `output_shape = f(input_shape)`\n         arguments: optional dictionary of keyword arguments to be passed\n             to the function.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d106908a576c535835a60716895fd3471ac3d6f4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 3711 | Contributors (this commit): 34 | Commits (past 90d): 39 | Contributors (cumulative): 51 | DMM Complexity: None\n\nDIFF:\n@@ -224,8 +224,7 @@ def batch_dot(x, y, axes=None):\n     make sure that ndim is at least 2.\n \n     # Example\n-        Assume x = [[1, 2]   and y = [[5, 6]\n-                    [3, 4]]           [7, 8]]\n+        Assume x = [[1, 2], [3, 4]]   and y = [[5, 6], [7, 8]]\n         batch_dot(x, y, axes=1) = [[17, 53]] which is the main diagonal\n         of x.dot(y.T), although we never have to calculate the off-diagonal\n         elements.\n\n@@ -128,8 +128,7 @@ def batch_dot(x, y, axes=None):\n     make sure that ndim is at least 2.\n \n     # Example\n-        Assume x = [[1, 2]   and y = [[5, 6]\n-                    [3, 4]]           [7, 8]]\n+        Assume x = [[1, 2], [3, 4]]   and y = [[5, 6], [7, 8]]\n         batch_dot(x, y, axes=1) = [[17, 53]] which is the main diagonal\n         of x.dot(y.T), although we never have to calculate the off-diagonal\n         elements.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#27c83c693d76c84cd704ca467d29f39dbdf4c855", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 16 | Churn Cumulative: 4038 | Contributors (this commit): 21 | Commits (past 90d): 39 | Contributors (cumulative): 26 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1055,7 +1055,7 @@ class Merge(Layer):\n             a list of layer instances. Must be more\n             than one layer/tensor.\n         mode: string or lambda/function. If string, must be one\n-            of: 'sum', 'mul', 'concat', 'ave', 'cos', 'dot'.\n+            of: 'sum', 'mul', 'concat', 'ave', 'cos', 'dot', 'max'.\n             If lambda/function, it should take as input a list of tensors\n             and return a single tensor.\n         concat_axis: integer, axis to use in mode `concat`.\n@@ -1130,7 +1130,7 @@ class Merge(Layer):\n         as appropriate.\n         '''\n         if not hasattr(mode, '__call__'):\n-            if mode not in {'sum', 'mul', 'concat', 'ave', 'cos', 'dot'}:\n+            if mode not in {'sum', 'mul', 'concat', 'ave', 'cos', 'dot', 'max'}:\n                 raise Exception('Invalid merge mode: ' + str(mode))\n         if type(layers) not in {list, tuple} or len(layers) < 2:\n             raise Exception('A Merge should only be applied to a list of '\n@@ -1148,7 +1148,7 @@ class Merge(Layer):\n                 layer_output_shape = layer_output_shape[tensor_indices[i]]\n             input_shapes.append(layer_output_shape)\n \n-        if mode in {'sum', 'mul', 'ave', 'cos'}:\n+        if mode in {'sum', 'mul', 'ave', 'cos', 'max'}:\n             input_shapes_set = set(input_shapes)\n             if len(input_shapes_set) > 1:\n                 raise Exception('Only layers of same output shape can '\n@@ -1214,7 +1214,11 @@ class Merge(Layer):\n             for i in range(1, len(inputs)):\n                 s *= inputs[i]\n             return s\n-\n+        elif self.mode == 'max':\n+            s = inputs[0]\n+            for i in range(1, len(inputs)):\n+                s = K.maximum(s, inputs[i])\n+            return s\n         elif self.mode == 'dot':\n             l1 = inputs[0]\n             l2 = inputs[1]\n@@ -1293,7 +1297,7 @@ class Merge(Layer):\n                                 '`output_shape` to Merge.')\n         # pre-defined merge modes\n         input_shapes = input_shape\n-        if self.mode in ['sum', 'mul', 'ave']:\n+        if self.mode in ['sum', 'mul', 'ave', 'max']:\n             # all tuples in input_shapes should be the same\n             return input_shapes[0]\n         elif self.mode == 'concat':\n\n@@ -21,7 +21,7 @@ def test_merge():\n     inputs = [np.random.random(shape) for shape in input_shapes]\n \n     # test functional API\n-    for mode in ['sum', 'mul', 'concat', 'ave']:\n+    for mode in ['sum', 'mul', 'concat', 'ave', 'max']:\n         print(mode)\n         input_a = Input(shape=input_shapes[0][1:])\n         input_b = Input(shape=input_shapes[1][1:])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f9a4f6f30658207ed4c0252d0e671bd2f8fdb53c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 8 | Churn Cumulative: 101 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,9 +1,11 @@\n import numpy as np\n \n+from collections import defaultdict\n+\n # the type of float to use throughout the session.\n _FLOATX = 'float32'\n _EPSILON = 10e-8\n-_UID_PREFIXES = {}\n+_UID_PREFIXES = defaultdict(int)\n _IMAGE_DIM_ORDERING = 'th'\n \n \n@@ -60,9 +62,5 @@ def set_image_dim_ordering(dim_ordering):\n \n \n def get_uid(prefix=''):\n-    if prefix not in _UID_PREFIXES:\n-        _UID_PREFIXES[prefix] = 1\n-        return 1\n-    else:\n     _UID_PREFIXES[prefix] += 1\n     return _UID_PREFIXES[prefix]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#28819d36a45ffa8222d27d0ecd0c4f2c8b632a76", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 18 | Churn Cumulative: 20 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,24 +1,42 @@\n from __future__ import print_function\n import pytest\n+import time\n+import random\n from keras.datasets import cifar10, cifar100, reuters, imdb, mnist\n \n \n def test_cifar():\n+    # only run data download tests 20% of the time\n+    # to speed up frequent testing\n+    random.seed(time.time())\n+    if random.random() > 0.8:\n         (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n         (X_train, y_train), (X_test, y_test) = cifar100.load_data('fine')\n         (X_train, y_train), (X_test, y_test) = cifar100.load_data('coarse')\n \n \n def test_reuters():\n+    # only run data download tests 20% of the time\n+    # to speed up frequent testing\n+    random.seed(time.time())\n+    if random.random() > 0.8:\n         (X_train, y_train), (X_test, y_test) = reuters.load_data()\n         (X_train, y_train), (X_test, y_test) = reuters.load_data(maxlen=10)\n \n \n def test_mnist():\n+    # only run data download tests 20% of the time\n+    # to speed up frequent testing\n+    random.seed(time.time())\n+    if random.random() > 0.8:\n         (X_train, y_train), (X_test, y_test) = mnist.load_data()\n \n \n def test_imdb():\n+    # only run data download tests 20% of the time\n+    # to speed up frequent testing\n+    random.seed(time.time())\n+    if random.random() > 0.8:\n         (X_train, y_train), (X_test, y_test) = imdb.load_data()\n         (X_train, y_train), (X_test, y_test) = imdb.load_data(maxlen=40)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#eaff5bdfd709dbc9f3fd73e89dcd424caec505b3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 30 | Churn Cumulative: 1937 | Contributors (this commit): 22 | Commits (past 90d): 21 | Contributors (cumulative): 22 | DMM Complexity: None\n\nDIFF:\n@@ -1085,7 +1085,7 @@ def conv2d(x, kernel, strides=(1, 1), border_mode='valid',\n         border_mode: string, \"same\" or \"valid\".\n         dim_ordering: \"tf\" or \"th\".\n             Whether to use Theano or TensorFlow dimension ordering\n-        in inputs/kernels/ouputs.\n+            for inputs/kernels/ouputs.\n     '''\n     if dim_ordering not in {'th', 'tf'}:\n         raise Exception('Unknown dim_ordering ' + str(dim_ordering))\n@@ -1103,6 +1103,18 @@ def deconv2d(x, kernel, output_shape, strides=(1, 1),\n              border_mode='valid',\n              dim_ordering=_IMAGE_DIM_ORDERING,\n              image_shape=None, filter_shape=None):\n+    '''2D deconvolution (i.e. transposed convolution).\n+\n+    # Arguments\n+        x: input tensor.\n+        kernel: kernel tensor.\n+        output_shape: 1D int tensor for the output shape.\n+        strides: strides tuple.\n+        border_mode: string, \"same\" or \"valid\".\n+        dim_ordering: \"tf\" or \"th\".\n+            Whether to use Theano or TensorFlow dimension ordering\n+            for inputs/kernels/ouputs.\n+    '''\n     if dim_ordering not in {'th', 'tf'}:\n         raise Exception('Unknown dim_ordering ' + str(dim_ordering))\n \n@@ -1141,8 +1153,10 @@ def separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1),\n         raise Exception('Unknown dim_ordering ' + str(dim_ordering))\n \n     x = _preprocess_conv2d_input(x, dim_ordering)\n-    depthwise_kernel = _preprocess_conv2d_kernel(depthwise_kernel, dim_ordering)\n-    pointwise_kernel = _preprocess_conv2d_kernel(pointwise_kernel, dim_ordering)\n+    depthwise_kernel = _preprocess_conv2d_kernel(depthwise_kernel,\n+                                                 dim_ordering)\n+    pointwise_kernel = _preprocess_conv2d_kernel(pointwise_kernel,\n+                                                 dim_ordering)\n     padding = _preprocess_border_mode(border_mode)\n     strides = (1,) + strides + (1,)\n \n@@ -1154,6 +1168,16 @@ def separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1),\n def conv3d(x, kernel, strides=(1, 1, 1),\n            border_mode='valid', dim_ordering=_IMAGE_DIM_ORDERING,\n            volume_shape=None, filter_shape=None):\n+    '''3D convolution.\n+\n+    # Arguments\n+        kernel: kernel tensor.\n+        strides: strides tuple.\n+        border_mode: string, \"same\" or \"valid\".\n+        dim_ordering: \"tf\" or \"th\".\n+            Whether to use Theano or TensorFlow dimension ordering\n+            for inputs/kernels/ouputs.\n+    '''\n     if dim_ordering not in {'th', 'tf'}:\n         raise Exception('Unknown dim_ordering ' + str(dim_ordering))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3ffff6d5793c5b6c1c2fab5416ca8ed9c4556044", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2924 | Contributors (this commit): 18 | Commits (past 90d): 35 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -1286,7 +1286,7 @@ class Merge(Layer):\n                 output_shape = self._output_shape(input_shape)\n                 return output_shape\n             elif self._output_shape is not None:\n-                return (input_shape[0],) + tuple(self._output_shape)\n+                return (input_shape[0][0],) + tuple(self._output_shape)\n             else:\n                 # TODO: consider shape auto-inference with TF\n                 raise Exception('The Merge layer ' + self.name +\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#729e802e8544a39ab9d4f01c4dd6512405a4918a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 1602 | Contributors (this commit): 22 | Commits (past 90d): 12 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -365,10 +365,15 @@ class RemoteMonitor(Callback):\n             HTTP POST, with a `data` argument which is a \n             JSON-encoded dictionary of event data.\n     '''\n-    def __init__(self, root='http://localhost:9000', path='/publish/epoch/end/'):\n+\n+    def __init__(self,\n+                 root='http://localhost:9000',\n+                 path='/publish/epoch/end/',\n+                 field='data'):\n         super(RemoteMonitor, self).__init__()\n         self.root = root\n         self.path = path\n+        self.field = field\n \n     def on_epoch_end(self, epoch, logs={}):\n         import requests\n@@ -376,10 +381,9 @@ class RemoteMonitor(Callback):\n         send['epoch'] = epoch\n         for k, v in logs.items():\n             send[k] = v\n-\n         try:\n             requests.post(self.root + self.path,\n-                          {'data': json.dumps(send)})\n+                          {self.field: json.dumps(send)})\n         except:\n             print('Warning: could not reach RemoteMonitor '\n                   'root server at ' + str(self.root))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a78ad01bb44137f206dc8167626ea8cf73c7958b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 4303 | Contributors (this commit): 27 | Commits (past 90d): 11 | Contributors (cumulative): 27 | DMM Complexity: None\n\nDIFF:\n@@ -190,9 +190,9 @@ class Recurrent(Layer):\n     def get_initial_states(self, x):\n         # build an all-zero tensor of shape (samples, output_dim)\n         initial_state = K.zeros_like(x)  # (samples, timesteps, input_dim)\n-        initial_state = K.sum(initial_state, axis=1)  # (samples, input_dim)\n-        reducer = K.zeros((self.input_dim, self.output_dim))\n-        initial_state = K.dot(initial_state, reducer)  # (samples, output_dim)\n+        initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)\n+        initial_state = K.expand_dims(initial_state)  # (samples, 1)\n+        initial_state = K.tile(initial_state, [1, self.output_dim])  # (samples, output_dim)\n         initial_states = [initial_state for _ in range(len(self.states))]\n         return initial_states\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#22502a8fe8830b68255da7ef2b74138f53cbc8a0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 562 | Contributors (this commit): 8 | Commits (past 90d): 4 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,5 +1,4 @@\n from __future__ import absolute_import\n-import numpy as np\n from . import backend as K\n \n \n@@ -18,8 +17,8 @@ class Regularizer(object):\n \n \n class EigenvalueRegularizer(Regularizer):\n-    '''This takes a constant that controls the \n-    regularization by Eigenvalue Decay on the \n+    '''This takes a constant that controls\n+    the regularization by Eigenvalue Decay on the\n     current layer and outputs the regularized\n     loss (evaluated on the training data) and\n     the original loss (evaluated on the\n@@ -41,7 +40,6 @@ class EigenvalueRegularizer(Regularizer):\n                             'and embedding layers.')\n         WW = K.dot(K.transpose(W), W)\n         dim1, dim2 = K.eval(K.shape(WW))  # number of neurons in the layer\n-        k = self.k\n \n         # power method for approximating the dominant eigenvector:\n         o = K.ones([dim1, 1])  # initial values for the dominant eigenvector\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#64e1320ca006a62f1188377a617bd3347dcfce7a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 2025 | Contributors (this commit): 16 | Commits (past 90d): 31 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -396,9 +396,9 @@ def standardize_weights(y, sample_weight=None, class_weight=None,\n         return weights\n     else:\n         if sample_weight_mode is None:\n-            return np.ones((y.shape[0],))\n+            return np.ones((y.shape[0],), dtype=K.floatx())\n         else:\n-            return np.ones((y.shape[0], y.shape[1]))\n+            return np.ones((y.shape[0], y.shape[1]), dtype=K.floatx())\n \n \n def generator_queue(generator, max_q_size=10,\n@@ -883,7 +883,7 @@ class Model(Container):\n             if batch_index == 0:\n                 for batch_out in batch_outs:\n                     shape = (nb_sample,) + batch_out.shape[1:]\n-                    outs.append(np.zeros(shape))\n+                    outs.append(np.zeros(shape, dtype=K.floatx()))\n \n             for i, batch_out in enumerate(batch_outs):\n                 outs[i][batch_start:batch_end] = batch_out\n@@ -1650,7 +1650,7 @@ class Model(Container):\n             if len(all_outs) == 0:\n                 for out in outs:\n                     shape = (val_samples,) + out.shape[1:]\n-                    all_outs.append(np.zeros(shape))\n+                    all_outs.append(np.zeros(shape, dtype=K.floatx()))\n \n             for i, out in enumerate(outs):\n                 all_outs[i][processed_samples:(processed_samples + nb_samples)] = out\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1ea3f44f06694aad184644f9b73135abc3eb5e29", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 1940 | Contributors (this commit): 22 | Commits (past 90d): 19 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1270,4 +1270,5 @@ def random_binomial(shape, p=0.0, dtype=_FLOATX, seed=None):\n     if seed is None:\n         seed = np.random.randint(10e6)\n     return tf.select(tf.random_uniform(shape, dtype=dtype, seed=seed) <= p,\n-                     tf.ones(shape), tf.zeros(shape))\n+                     tf.ones(shape, dtype=dtype),\n+                     tf.zeros(shape, dtype=dtype))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b6a776b24278bebaec721c3cfead33bfc01e32cf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 18 | Churn Cumulative: 8510 | Contributors (this commit): 47 | Commits (past 90d): 12 | Contributors (cumulative): 50 | DMM Complexity: 1.0\n\nDIFF:\n@@ -158,6 +158,22 @@ class Sequential(Model):\n         self.built = False\n         self._flattened_layers = None\n \n+    def pop(self):\n+        '''Removes the last layer in the model.\n+        '''\n+        if not self.layers:\n+            raise Exception('There are no layers in the model.')\n+\n+        self.layers.pop()\n+        if not self.layers:\n+            self.outputs = []\n+            self.inbound_nodes = []\n+            self.outbound_nodes = []\n+        else:\n+            self.layers[-1].outbound_nodes = []\n+            self.outputs = [self.layers[-1].output]\n+        self.built = False\n+\n     def call(self, x, mask=None):\n         if not self.built:\n             self.build()\n\n@@ -59,6 +59,8 @@ def test_sequential_fit_generator():\n     model.add(Dense(nb_hidden, input_shape=(input_dim,)))\n     model.add(Activation('relu'))\n     model.add(Dense(nb_class))\n+    model.pop()\n+    model.add(Dense(nb_class))\n     model.add(Activation('softmax'))\n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#98974efa5f51d6f55afbf2bc125d6fd090bcf782", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2027 | Contributors (this commit): 17 | Commits (past 90d): 27 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -510,7 +510,7 @@ class Model(Container):\n                                 'it should have one entry per model outputs. '\n                                 'The model has ' + str(len(self.outputs)) +\n                                 ' outputs, but you passed loss_weights=' +\n-                                str(loss))\n+                                str(loss_weights))\n             loss_weights_list = loss_weights\n         else:\n             raise Exception('Could not interpret loss_weights argument: ' +\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#55e5680535ab7bed75757d9109b58323f8aa72ee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 864 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -146,13 +146,8 @@ PAGES = [\n         'classes': [\n             convolutional.Convolution1D,\n             convolutional.Convolution2D,\n+            convolutional.AtrousConv2D,\n             convolutional.Convolution3D,\n-            convolutional.MaxPooling1D,\n-            convolutional.MaxPooling2D,\n-            convolutional.MaxPooling3D,\n-            convolutional.AveragePooling1D,\n-            convolutional.AveragePooling2D,\n-            convolutional.AveragePooling3D,\n             convolutional.UpSampling1D,\n             convolutional.UpSampling2D,\n             convolutional.UpSampling3D,\n@@ -161,6 +156,17 @@ PAGES = [\n             convolutional.ZeroPadding3D,\n         ],\n     },\n+    {\n+        'page': 'layers/pooling.md',\n+        'classes': [\n+            convolutional.MaxPooling1D,\n+            convolutional.MaxPooling2D,\n+            convolutional.MaxPooling3D,\n+            convolutional.AveragePooling1D,\n+            convolutional.AveragePooling2D,\n+            convolutional.AveragePooling3D,\n+        ],\n+    },\n     {\n         'page': 'layers/recurrent.md',\n         'classes': [\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ca467cc50ee49776140830e297dd52b3f26c8f76", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 15 | Methods Changed: 4 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 50 | Churn Cumulative: 3710 | Contributors (this commit): 18 | Commits (past 90d): 32 | Contributors (cumulative): 19 | DMM Complexity: 0.0\n\nDIFF:\n@@ -914,7 +914,7 @@ class InputLayer(Layer):\n     '''TODO: dosctring\n     '''\n     def __init__(self, input_shape=None, batch_input_shape=None,\n-                 input_dtype=None, name=None):\n+                 input_dtype=None, input_tensor=None, name=None):\n         self.input_spec = None\n         self.supports_masking = False\n         self.uses_learning_phase = False\n@@ -934,25 +934,48 @@ class InputLayer(Layer):\n             name = prefix + '_' + str(K.get_uid(prefix))\n         self.name = name\n \n+        if input_shape and batch_input_shape:\n+            raise ValueError('Only provide the input_shape OR '\n+                             'batch_input_shape argument to '\n+                             'InputLayer, not both at the same time.')\n+        if input_tensor is not None:\n+            if not input_shape and not batch_input_shape:\n+                # attempt automatic input shape inference\n+                try:\n+                    batch_input_shape = K.int_shape(input_tensor)\n+                except:\n+                    raise ValueError('InputLayer was provided an input_tensor argument, '\n+                                     'but its input shape cannot be automatically inferred. '\n+                                     'You should pass an input_shape or batch_input_shape '\n+                                     'argument.')\n         if not batch_input_shape:\n-            assert input_shape, 'An Input layer should be passed either a `batch_input_shape` or an `input_shape`.'\n+            if not input_shape:\n+                raise ValueError('An Input layer should be passed either '\n+                                 'a `batch_input_shape` or an `input_shape`.')\n+            else:\n                 batch_input_shape = (None,) + tuple(input_shape)\n         else:\n             batch_input_shape = tuple(batch_input_shape)\n+\n         if not input_dtype:\n+            if input_tensor is None:\n                 input_dtype = K.floatx()\n+            else:\n+                input_dtype = K.dtype(input_tensor)\n \n         self.batch_input_shape = batch_input_shape\n         self.input_dtype = input_dtype\n \n+        if input_tensor is None:\n             input_tensor = K.placeholder(shape=batch_input_shape,\n                                          dtype=input_dtype,\n                                          name=self.name)\n+        else:\n+            input_tensor._keras_shape = batch_input_shape\n         # create an input node to add to self.outbound_node\n         # and set output_tensors' _keras_history\n         input_tensor._uses_learning_phase = False\n         input_tensor._keras_history = (self, 0, 0)\n-        shape = input_tensor._keras_shape\n         Node(self,\n              inbound_layers=[],\n              node_indices=[],\n@@ -961,8 +984,8 @@ class InputLayer(Layer):\n              output_tensors=[input_tensor],\n              input_masks=[None],\n              output_masks=[None],\n-             input_shapes=[shape],\n-             output_shapes=[shape])\n+             input_shapes=[batch_input_shape],\n+             output_shapes=[batch_input_shape])\n \n     def get_config(self):\n         config = {'batch_input_shape': self.batch_input_shape,\n@@ -972,7 +995,8 @@ class InputLayer(Layer):\n \n \n def Input(shape=None, batch_shape=None,\n-          name=None, dtype=K.floatx()):\n+          name=None, dtype=K.floatx(),\n+          tensor=None):\n     '''`Input()` is used to instantiate a Keras tensor.\n     A Keras tensor is a tensor object from the underlying backend\n     (Theano or TensorFlow), which we augment with certain\n@@ -1014,14 +1038,15 @@ def Input(shape=None, batch_shape=None,\n         model = Model(input=a, output=b)\n         ```\n     '''\n-    if not batch_shape:\n+    if not batch_shape and tensor is None:\n         assert shape, ('Please provide to Input either a `shape`' +\n                        ' or a `batch_shape` argument. Note that ' +\n                        '`shape` does not include the batch '\n                        'dimension.')\n         batch_shape = (None,) + tuple(shape)\n     input_layer = InputLayer(batch_input_shape=batch_shape,\n-                             name=name, input_dtype=dtype)\n+                             name=name, input_dtype=dtype,\n+                             input_tensor=tensor)\n     # return tensor including _keras_shape and _keras_history\n     # note that in this case train_output and test_output are the same pointer.\n     outputs = input_layer.inbound_nodes[0].output_tensors\n\n@@ -2,7 +2,7 @@ import pytest\n import json\n import numpy as np\n \n-from keras.layers import Dense, Dropout\n+from keras.layers import Dense, Dropout, InputLayer\n from keras.engine import merge, Input, get_source_inputs\n from keras.models import Model\n from keras import backend as K\n@@ -392,6 +392,13 @@ def test_recursion():\n         # test merge\n         o_tf = merge([j_tf, k_tf], mode='concat', concat_axis=1)\n \n+        # test tensor input\n+        x = tf.placeholder(shape=(None, 2), dtype=K.floatx())\n+        input_layer = InputLayer(input_tensor=x)\n+\n+        x = Input(tensor=x)\n+        y = Dense(2)(x)\n+\n \n def test_functional_guide():\n     # MNIST\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
