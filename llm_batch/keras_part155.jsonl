{"custom_id": "keras#2562172adcd15e60763dfd601d8f0a5697a2bbdd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 4451 | Contributors (this commit): 54 | Commits (past 90d): 45 | Contributors (cumulative): 54 | DMM Complexity: None\n\nDIFF:\n@@ -1640,7 +1640,7 @@ def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n     if data_format not in {'channels_first', 'channels_last'}:\n         raise ValueError('Unknown data_format ' + data_format)\n \n-    if data_format == 'channels_first':\n+    if data_format == 'channels_last':\n         output_shape = (output_shape[0],\n                         output_shape[3],\n                         output_shape[1],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#18cce177a8bd29b5076c4e232830377e46ed0416", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 5 | Churn Cumulative: 4456 | Contributors (this commit): 54 | Commits (past 90d): 46 | Contributors (cumulative): 54 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1233,6 +1233,11 @@ def in_train_phase(x, alt, training=None):\n         else:\n             return alt\n \n+    if callable(x):\n+        x = x()\n+    if callable(alt):\n+        alt = alt()\n+\n     # else: assume learning phase is a placeholder tensor.\n     x = theano.ifelse.ifelse(training, x, alt)\n     if uses_learning_phase:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#36ac91f0576b2295df1f3f8b23c305d69698a0ff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 510 | Lines Deleted: 1035 | Files Changed: 26 | Hunks: 156 | Methods Changed: 152 | Complexity Δ (Sum/Max): -25/22 | Churn Δ: 1545 | Churn Cumulative: 35130 | Contributors (this commit): 148 | Commits (past 90d): 138 | Contributors (cumulative): 240 | DMM Complexity: 0.3067484662576687\n\nDIFF:\n@@ -11,7 +11,7 @@ class Constraint(object):\n         return w\n \n     def get_config(self):\n-        return {'name': self.__class__.__name__}\n+        return {}\n \n \n class MaxNorm(Constraint):\n@@ -38,19 +38,18 @@ class MaxNorm(Constraint):\n         - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting Srivastava, Hinton, et al. 2014](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n     \"\"\"\n \n-    def __init__(self, m=2, axis=0):\n-        self.m = m\n+    def __init__(self, max_value=2, axis=0):\n+        self.max_value = max_value\n         self.axis = axis\n \n     def __call__(self, w):\n         norms = K.sqrt(K.sum(K.square(w), axis=self.axis, keepdims=True))\n-        desired = K.clip(norms, 0, self.m)\n-        p *= (desired / (K.epsilon() + norms))\n-        return p\n+        desired = K.clip(norms, 0, self.max_value)\n+        w *= (desired / (K.epsilon() + norms))\n+        return w\n \n     def get_config(self):\n-        return {'name': self.__class__.__name__,\n-                'm': self.m,\n+        return {'max_value': self.max_value,\n                 'axis': self.axis}\n \n \n@@ -58,9 +57,9 @@ class NonNeg(Constraint):\n     \"\"\"Constrains the weights to be non-negative.\n     \"\"\"\n \n-    def __call__(self, p):\n-        p *= K.cast(p >= 0., K.floatx())\n-        return p\n+    def __call__(self, w):\n+        w *= K.cast(w >= 0., K.floatx())\n+        return w\n \n \n class UnitNorm(Constraint):\n@@ -89,8 +88,7 @@ class UnitNorm(Constraint):\n                                                keepdims=True)))\n \n     def get_config(self):\n-        return {'name': self.__class__.__name__,\n-                'axis': self.axis}\n+        return {'axis': self.axis}\n \n \n class MinMaxNorm(Constraint):\n@@ -100,10 +98,11 @@ class MinMaxNorm(Constraint):\n     to have the norm between a lower bound and an upper bound.\n \n     # Arguments\n-        low: the minimum norm for the incoming weights.\n-        high: the maximum norm for the incoming weights.\n+        min_value: the minimum norm for the incoming weights.\n+        max_value: the maximum norm for the incoming weights.\n         rate: rate for enforcing the constraint: weights will be\n-            rescaled to yield (1 - rate) * norm + rate * norm.clip(low, high).\n+            rescaled to yield\n+            `(1 - rate) * norm + rate * norm.clip(min_value, max_value)`.\n             Effectively, this means that rate=1.0 stands for strict\n             enforcement of the constraint, while rate<1.0 means that\n             weights will be rescaled at each step to slowly move\n@@ -120,23 +119,22 @@ class MinMaxNorm(Constraint):\n             to constrain the weights of each filter tensor of size\n             `(rows, cols, input_depth)`.\n     \"\"\"\n-    def __init__(self, low=0.0, high=1.0, rate=1.0, axis=0):\n-        self.low = low\n-        self.high = high\n+    def __init__(self, min_value=0.0, max_value=1.0, rate=1.0, axis=0):\n+        self.min_value = min_value\n+        self.max_value = max_value\n         self.rate = rate\n         self.axis = axis\n \n     def __call__(self, w):\n         norms = K.sqrt(K.sum(K.square(w), axis=self.axis, keepdims=True))\n-        desired = (self.rate * K.clip(norms, self.low, self.high) +\n+        desired = (self.rate * K.clip(norms, self.min_value, self.max_value) +\n                    (1 - self.rate) * norms)\n         w *= (desired / (K.epsilon() + norms))\n         return w\n \n     def get_config(self):\n-        return {'name': self.__class__.__name__,\n-                'low': self.low,\n-                'high': self.high,\n+        return {'min_value': self.min_value,\n+                'max_value': self.max_value,\n                 'rate': self.rate,\n                 'axis': self.axis}\n \n@@ -146,6 +144,7 @@ class MinMaxNorm(Constraint):\n max_norm = MaxNorm\n non_neg = NonNeg\n unit_norm = UnitNorm\n+min_max_norm = MinMaxNorm\n \n \n def serialize(constraint):\n\n@@ -2114,15 +2114,9 @@ class Container(Layer):\n         f.close()\n \n     def save_weights_to_hdf5_group(self, f):\n-        if hasattr(self, 'flattened_layers'):\n-            # Support for legacy Sequential/Merge behavior.\n-            flattened_layers = self.flattened_layers\n-        else:\n-            flattened_layers = self.layers\n+        f.attrs['layer_names'] = [layer.name.encode('utf8') for layer in self.layers]\n \n-        f.attrs['layer_names'] = [layer.name.encode('utf8') for layer in flattened_layers]\n-\n-        for layer in flattened_layers:\n+        for layer in self.layers:\n             g = f.create_group(layer.name)\n             symbolic_weights = layer.weights\n             weight_values = K.batch_get_value(symbolic_weights)\n@@ -2171,39 +2165,11 @@ class Container(Layer):\n             f.close()\n \n     def load_weights_from_hdf5_group(self, f):\n-        \"\"\"Weight loading is based on layer order in a list\n-        (matching model.flattened_layers for Sequential models,\n-        and model.layers for Model class instances), not\n-        on layer names.\n-        Layers that have no weights are skipped.\n-        \"\"\"\n-        if hasattr(self, 'flattened_layers'):\n-            # Support for legacy Sequential/Merge behavior.\n-            flattened_layers = self.flattened_layers\n-        else:\n-            flattened_layers = self.layers\n-\n-        if 'nb_layers' in f.attrs:\n-            # Legacy format.\n-            nb_layers = f.attrs['nb_layers']\n-            if nb_layers != len(flattened_layers):\n-                raise ValueError('You are trying to load a weight file '\n-                                 'containing ' + str(nb_layers) +\n-                                 ' layers into a model with ' +\n-                                 str(len(flattened_layers)) + ' layers.')\n-\n-            for k in range(nb_layers):\n-                g = f['layer_{}'.format(k)]\n-                weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n-                flattened_layers[k].set_weights(weights)\n-        else:\n-            # New file format.\n         filtered_layers = []\n-            for layer in flattened_layers:\n+        for layer in self.layers:\n             weights = layer.weights\n             if weights:\n                 filtered_layers.append(layer)\n-            flattened_layers = filtered_layers\n \n         layer_names = [n.decode('utf8') for n in f.attrs['layer_names']]\n         filtered_layer_names = []\n@@ -2213,11 +2179,11 @@ class Container(Layer):\n             if len(weight_names):\n                 filtered_layer_names.append(name)\n         layer_names = filtered_layer_names\n-            if len(layer_names) != len(flattened_layers):\n+        if len(layer_names) != len(filtered_layers):\n             raise ValueError('You are trying to load a weight file '\n                              'containing ' + str(len(layer_names)) +\n                              ' layers into a model with ' +\n-                                 str(len(flattened_layers)) + ' layers.')\n+                             str(len(filtered_layers)) + ' layers.')\n \n         # We batch weight value assignments in a single backend call\n         # which provides a speedup in TensorFlow.\n@@ -2226,7 +2192,7 @@ class Container(Layer):\n             g = f[name]\n             weight_names = [n.decode('utf8') for n in g.attrs['weight_names']]\n             weight_values = [g[weight_name] for weight_name in weight_names]\n-                layer = flattened_layers[k]\n+            layer = filtered_layers[k]\n             symbolic_weights = layer.weights\n             if len(weight_values) != len(symbolic_weights):\n                 raise ValueError('Layer #' + str(k) +\n@@ -2247,23 +2213,12 @@ class Container(Layer):\n         (instead of topological weight loading).\n         Layers that have no matching name are skipped.\n         \"\"\"\n-        if hasattr(self, 'flattened_layers'):\n-            # Support for legacy Sequential/Merge behavior.\n-            flattened_layers = self.flattened_layers\n-        else:\n-            flattened_layers = self.layers\n-\n-        if 'nb_layers' in f.attrs:\n-            raise ValueError('The weight file you are trying to load is'\n-                             ' in a legacy format that does not support'\n-                             ' name-based weight loading.')\n-        else:\n         # New file format.\n         layer_names = [n.decode('utf8') for n in f.attrs['layer_names']]\n \n         # Reverse index of layer name to list of layers with name.\n         index = {}\n-            for layer in flattened_layers:\n+        for layer in self.layers:\n             if layer.name:\n                 index.setdefault(layer.name, []).append(layer)\n \n@@ -2340,13 +2295,7 @@ class Container(Layer):\n \n     def summary(self, line_length=100, positions=[.33, .55, .67, 1.]):\n         from keras.utils.layer_utils import print_summary\n-\n-        if hasattr(self, 'flattened_layers'):\n-            # Support for legacy Sequential/Merge behavior.\n-            flattened_layers = self.flattened_layers\n-        else:\n-            flattened_layers = self.layers\n-        print_summary(flattened_layers,\n+        print_summary(self.layers,\n                       getattr(self, 'container_nodes', None),\n                       line_length=line_length,\n                       positions=positions)\n\n@@ -861,7 +861,6 @@ class Model(Container):\n         })\n         callbacks.on_train_begin()\n         callback_model.stop_training = False\n-        self.validation_data = val_ins\n \n         for epoch in range(initial_epoch, epochs):\n             callbacks.on_epoch_begin(epoch)\n@@ -1501,9 +1500,6 @@ class Model(Container):\n                                  str(validation_data))\n             val_x, val_y, val_sample_weights = self._standardize_user_data(\n                 val_x, val_y, val_sample_weight)\n-            self.validation_data = val_x + [val_y, val_sample_weights]\n-        else:\n-            self.validation_data = None\n \n         enqueuer = None\n \n\n@@ -780,6 +780,8 @@ class Dense(Layer):\n                  kernel_constraint=None,\n                  bias_constraint=None,\n                  **kwargs):\n+        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n+            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n         super(Dense, self).__init__(**kwargs)\n         self.units = units\n         self.activation = activations.get(activation)\n\n@@ -264,7 +264,6 @@ class Sequential(Model):\n         self.inbound_nodes = []\n         self.outbound_nodes = []\n         self.built = False\n-        self._flattened_layers = None\n \n         if not name:\n             prefix = 'sequential_'\n@@ -346,7 +345,6 @@ class Sequential(Model):\n \n         self.layers.append(layer)\n         self.built = False\n-        self._flattened_layers = None\n \n     def pop(self):\n         \"\"\"Removes the last layer in the model.\n@@ -366,7 +364,6 @@ class Sequential(Model):\n             self.inbound_nodes[0].output_tensors = self.outputs\n             self.inbound_nodes[0].output_shapes = [self.outputs[0]._keras_shape]\n         self.built = False\n-        self._flattened_layers = None\n \n     def get_layer(self, name=None, index=None):\n         \"\"\"Returns a layer based on either its name (unique)\n@@ -380,12 +377,12 @@ class Sequential(Model):\n         # Returns\n             A layer instance.\n         \"\"\"\n-        if not self.built:\n+        if self.model is None:\n             self.build()\n         return self.model.get_layer(name, index)\n \n     def call(self, x, mask=None):\n-        if not self.built:\n+        if self.model is None:\n             self.build()\n         return self.model.call(x, mask)\n \n@@ -421,28 +418,10 @@ class Sequential(Model):\n \n     @property\n     def uses_learning_phase(self):\n-        if not self.built:\n+        if self.model is None:\n             self.build()\n         return self.model.uses_learning_phase\n \n-    @property\n-    def flattened_layers(self):\n-        return self.model.flattened_layers\n-\n-    def _gather_list_attr(self, attr):\n-        all_attrs = []\n-        for layer in self.flattened_layers:\n-            all_attrs += getattr(layer, attr, [])\n-        return all_attrs\n-\n-    def _gather_dict_attr(self, attr):\n-        all_attrs = {}\n-        for layer in self.flattened_layers:\n-            layer_dict = getattr(layer, attr, {})\n-            all_attrs = dict(list(all_attrs.items()) +\n-                             list(layer_dict.items()))\n-        return all_attrs\n-\n     @property\n     def trainable(self):\n         return self._trainable\n@@ -455,58 +434,63 @@ class Sequential(Model):\n \n     @property\n     def trainable_weights(self):\n-        if not self.trainable:\n-            return []\n-        # support for legacy behavior\n-        return self._gather_list_attr('trainable_weights')\n+        if self.model is None:\n+            self.build()\n+        return self.model.trainable_weights\n \n     @property\n     def non_trainable_weights(self):\n-        # support for legacy behavior\n-        weights = self._gather_list_attr('non_trainable_weights')\n-        if not self.trainable:\n-            trainable_weights = self._gather_list_attr('trainable_weights')\n-            return trainable_weights + weights\n-        return weights\n+        if self.model is None:\n+            self.build()\n+        return self.model.non_trainable_weights\n \n     @property\n     def updates(self):\n+        if self.model is None:\n+            self.build()\n         return self.model.updates\n \n     @property\n     def state_updates(self):\n-        # support for legacy behavior\n+        if self.model is None:\n+            self.build()\n         return self.model.state_updates\n \n     def get_updates_for(self, inputs):\n+        if self.model is None:\n+            self.build()\n         return self.model.get_updates_for(inputs)\n \n     @property\n     def losses(self):\n+        if self.model is None:\n+            self.build()\n         return self.model.losses\n \n     def get_losses_for(self, inputs):\n+        if self.model is None:\n+            self.build()\n         return self.model.get_losses_for(inputs)\n \n     @property\n     def regularizers(self):\n-        # support for legacy behavior\n-        return self._gather_list_attr('regularizers')\n+        if self.model is None:\n+            self.build()\n+        return self.model.regularizers\n \n     @property\n     def constraints(self):\n-        # support for legacy behavior\n-        return self._gather_dict_attr('constraints')\n+        if self.model is None:\n+            self.build()\n+        return self.model.constraints\n \n     def get_weights(self):\n         \"\"\"Returns the weights of the model,\n         as a flat list of Numpy arrays.\n         \"\"\"\n-        # support for legacy behavior\n-        weights = []\n-        for layer in self.flattened_layers:\n-            weights += layer.get_weights()\n-        return weights\n+        if self.model is None:\n+            self.build()\n+        return self.model.get_weights()\n \n     def set_weights(self, weights):\n         \"\"\"Sets the weights of the model.\n@@ -514,19 +498,9 @@ class Sequential(Model):\n         of Numpy arrays with shapes and types matching\n         the output of `model.get_weights()`.\n         \"\"\"\n-        # support for legacy behavior\n-        for layer in self.flattened_layers:\n-            num_param = len(layer.weights)\n-            layer.set_weights(weights[:num_param])\n-            weights = weights[num_param:]\n-\n-    @property\n-    def validation_data(self):\n-        return self.model.validation_data\n-\n-    @property\n-    def training_data(self):\n-        return self.model.training_data\n+        if self.model is None:\n+            self.build()\n+        self.model.set_weights(weights)\n \n     def compile(self, optimizer, loss,\n                 metrics=None,\n@@ -561,11 +535,6 @@ class Sequential(Model):\n         \"\"\"\n         # create the underlying model\n         self.build()\n-        # legacy kwarg support\n-        if 'class_mode' in kwargs:\n-            warnings.warn('\"class_mode\" argument is deprecated, '\n-                          'please remove it.')\n-            kwargs.pop('class_mode')\n         # call compile method of Model class\n         self.model.compile(optimizer, loss,\n                            metrics=metrics,\n@@ -581,7 +550,7 @@ class Sequential(Model):\n \n     def fit(self, x, y, batch_size=32, epochs=10, verbose=1, callbacks=None,\n             validation_split=0., validation_data=None, shuffle=True,\n-            class_weight=None, sample_weight=None, initial_epoch=0, **kwargs):\n+            class_weight=None, sample_weight=None, initial_epoch=0):\n         \"\"\"Trains the model for a fixed number of epochs.\n \n         # Arguments\n@@ -628,16 +597,6 @@ class Sequential(Model):\n         if self.model is None:\n             raise RuntimeError('The model needs to be compiled '\n                                'before being used.')\n-        if 'show_accuracy' in kwargs:\n-            kwargs.pop('show_accuracy')\n-            warnings.warn('The \"show_accuracy\" argument is deprecated, '\n-                          'instead you should pass the \"accuracy\" metric to '\n-                          'the model at compile time:\\n'\n-                          '`model.compile(optimizer, loss, '\n-                          'metrics=[\"accuracy\"])`')\n-        if kwargs:\n-            raise TypeError('Received unknown keyword arguments: ' +\n-                            str(kwargs))\n         return self.model.fit(x, y,\n                               batch_size=batch_size,\n                               epochs=epochs,\n@@ -651,7 +610,7 @@ class Sequential(Model):\n                               initial_epoch=initial_epoch)\n \n     def evaluate(self, x, y, batch_size=32, verbose=1,\n-                 sample_weight=None, **kwargs):\n+                 sample_weight=None):\n         \"\"\"Computes the loss on some input data, batch by batch.\n \n         # Arguments\n@@ -671,16 +630,6 @@ class Sequential(Model):\n         if self.model is None:\n             raise RuntimeError('The model needs to be compiled '\n                                'before being used.')\n-        if 'show_accuracy' in kwargs:\n-            kwargs.pop('show_accuracy')\n-            warnings.warn('The \"show_accuracy\" argument is deprecated, '\n-                          'instead you should pass the \"accuracy\" metric to '\n-                          'the model at compile time:\\n'\n-                          '`model.compile(optimizer, loss, '\n-                          'metrics=[\"accuracy\"])`')\n-        if kwargs:\n-            raise TypeError('Received unknown keyword arguments: ' +\n-                            str(kwargs))\n         return self.model.evaluate(x, y,\n                                    batch_size=batch_size,\n                                    verbose=verbose,\n@@ -710,7 +659,7 @@ class Sequential(Model):\n         return self.model.predict_on_batch(x)\n \n     def train_on_batch(self, x, y, class_weight=None,\n-                       sample_weight=None, **kwargs):\n+                       sample_weight=None):\n         \"\"\"Single gradient update over one batch of samples.\n \n         # Arguments\n@@ -730,22 +679,12 @@ class Sequential(Model):\n         if self.model is None:\n             raise RuntimeError('The model needs to be compiled '\n                                'before being used.')\n-        if 'accuracy' in kwargs:\n-            kwargs.pop('accuracy')\n-            warnings.warn('The \"accuracy\" argument is deprecated, '\n-                          'instead you should pass the \"accuracy\" metric to '\n-                          'the model at compile time:\\n'\n-                          '`model.compile(optimizer, loss, '\n-                          'metrics=[\"accuracy\"])`')\n-        if kwargs:\n-            raise TypeError('Received unknown keyword arguments: ' +\n-                            str(kwargs))\n         return self.model.train_on_batch(x, y,\n                                          sample_weight=sample_weight,\n                                          class_weight=class_weight)\n \n     def test_on_batch(self, x, y,\n-                      sample_weight=None, **kwargs):\n+                      sample_weight=None):\n         \"\"\"Evaluates the model over a single batch of samples.\n \n         # Arguments\n@@ -763,16 +702,6 @@ class Sequential(Model):\n         if self.model is None:\n             raise RuntimeError('The model needs to be compiled '\n                                'before being used.')\n-        if 'accuracy' in kwargs:\n-            kwargs.pop('accuracy')\n-            warnings.warn('The \"accuracy\" argument is deprecated, '\n-                          'instead you should pass the \"accuracy\" metric to '\n-                          'the model at compile time:\\n'\n-                          '`model.compile(optimizer, loss, '\n-                          'metrics=[\"accuracy\"])`')\n-        if kwargs:\n-            raise TypeError('Received unknown keyword arguments: ' +\n-                            str(kwargs))\n         return self.model.test_on_batch(x, y,\n                                         sample_weight=sample_weight)\n \n@@ -820,7 +749,7 @@ class Sequential(Model):\n                       verbose=1, callbacks=None,\n                       validation_data=None, num_val_samples=None,\n                       class_weight=None, max_q_size=10, workers=1,\n-                      pickle_safe=False, initial_epoch=0, **kwargs):\n+                      pickle_safe=False, initial_epoch=0):\n         \"\"\"Fits the model on data generated batch-by-batch by\n         a Python generator.\n         The generator is run in parallel to the model, for efficiency.\n@@ -882,24 +811,6 @@ class Sequential(Model):\n         if self.model is None:\n             raise RuntimeError('The model needs to be compiled '\n                                'before being used.')\n-        if workers > 1 and not pickle_safe:\n-            warnings.warn('The \"workers\" argument is deprecated '\n-                          'when pickle_safe is False')\n-            workers = 1  # For backward compatibility\n-        if 'show_accuracy' in kwargs:\n-            kwargs.pop('show_accuracy')\n-            warnings.warn('The \"show_accuracy\" argument is deprecated, '\n-                          'instead you should pass the \"accuracy\" metric to '\n-                          'the model at compile time:\\n'\n-                          '`model.compile(optimizer, loss, '\n-                          'metrics=[\"accuracy\"])`')\n-        if 'num_val_worker' in kwargs:\n-            kwargs.pop('num_val_worker')\n-            warnings.warn('The \"num_val_worker\" argument is deprecated, '\n-                          'please remove it from your code.')\n-        if kwargs:\n-            raise TypeError('Received unknown keyword arguments: ' +\n-                            str(kwargs))\n         return self.model.fit_generator(generator,\n                                         samples_per_epoch,\n                                         epochs,\n@@ -915,7 +826,7 @@ class Sequential(Model):\n \n     def evaluate_generator(self, generator, val_samples,\n                            max_q_size=10, workers=1,\n-                           pickle_safe=False, **kwargs):\n+                           pickle_safe=False):\n         \"\"\"Evaluates the model on a data generator. The generator should\n         return the same kind of data as accepted by `test_on_batch`.\n \n@@ -936,23 +847,6 @@ class Sequential(Model):\n         if self.model is None:\n             raise RuntimeError('The model needs to be compiled '\n                                'before being used.')\n-        if workers > 1 and not pickle_safe:\n-            warnings.warn('The \"workers\" argument is deprecated '\n-                          'when pickle_safe is False')\n-            workers = 1  # For backward compatibility\n-        if 'show_accuracy' in kwargs:\n-            kwargs.pop('show_accuracy')\n-            warnings.warn('The \"show_accuracy\" argument is deprecated, '\n-                          'instead you should pass the \"accuracy\" metric to '\n-                          'the model at compile time:\\n'\n-                          '`model.compile(optimizer, loss, '\n-                          'metrics=[\"accuracy\"])`')\n-        if 'verbose' in kwargs:\n-            kwargs.pop('verbose')\n-            warnings.warn('The \"verbose\" argument is deprecated.')\n-        if kwargs:\n-            raise TypeError('Received unknown keyword arguments: ' +\n-                            str(kwargs))\n         return self.model.evaluate_generator(generator,\n                                              val_samples,\n                                              max_q_size=max_q_size,\n@@ -981,10 +875,6 @@ class Sequential(Model):\n         \"\"\"\n         if self.model is None:\n             self.build()\n-        if workers > 1 and not pickle_safe:\n-            warnings.warn('The \"workers\" argument is deprecated '\n-                          'when pickle_safe is False')\n-            workers = 1  # For backward compatibility\n         return self.model.predict_generator(generator, val_samples,\n                                             max_q_size=max_q_size,\n                                             workers=workers,\n\n@@ -106,7 +106,8 @@ def serialize_keras_object(instance):\n \n def deserialize_keras_object(identifier, module_objects=None,\n                              custom_objects=None,\n-                             printable_module_name='object'):\n+                             printable_module_name='object',\n+                             instantiate=True):\n     if isinstance(identifier, dict):\n         # In this case we are dealing with a Keras config dictionary.\n         config = identifier\n\n@@ -7,17 +7,27 @@ from keras import activations\n \n \n def get_standard_values():\n-    '''\n-    These are just a set of floats used for testing the activation\n-    functions, and are useful in multiple tests.\n-    '''\n+    \"\"\"A set of floats used for testing the activations.\n+    \"\"\"\n     return np.array([[0, 0.1, 0.5, 0.9, 1.0]], dtype=K.floatx())\n \n \n+def test_serialization():\n+    all_activations = ['softmax', 'relu', 'elu', 'tanh',\n+                       'sigmoid', 'hard_sigmoid', 'linear',\n+                       'softplus', 'softsign']\n+    for name in all_activations:\n+        fn = activations.get(name)\n+        ref_fn = getattr(activations, name)\n+        assert fn == ref_fn\n+        config = activations.serialize(fn)\n+        fn = activations.deserialize(config)\n+        assert fn == ref_fn\n+\n+\n def test_softmax():\n-    '''\n-    Test using a reference implementation of softmax\n-    '''\n+    \"\"\"Test using a reference implementation of softmax.\n+    \"\"\"\n     def softmax(values):\n         m = np.max(values)\n         e = np.exp(values - m)\n@@ -41,9 +51,8 @@ def test_time_distributed_softmax():\n \n \n def test_softplus():\n-    '''\n-    Test using a reference softplus implementation\n-    '''\n+    \"\"\"Test using a reference softplus implementation.\n+    \"\"\"\n     def softplus(x):\n         return np.log(np.ones_like(x) + np.exp(x))\n \n@@ -57,9 +66,8 @@ def test_softplus():\n \n \n def test_softsign():\n-    '''\n-    Test using a reference softsign implementation\n-    '''\n+    \"\"\"Test using a reference softsign implementation.\n+    \"\"\"\n     def softsign(x):\n         return np.divide(x, np.ones_like(x) + np.absolute(x))\n \n@@ -73,9 +81,8 @@ def test_softsign():\n \n \n def test_sigmoid():\n-    '''\n-    Test using a numerically stable reference sigmoid implementation\n-    '''\n+    \"\"\"Test using a numerically stable reference sigmoid implementation.\n+    \"\"\"\n     def ref_sigmoid(x):\n         if x >= 0:\n             return 1 / (1 + np.exp(-x))\n@@ -94,14 +101,9 @@ def test_sigmoid():\n \n \n def test_hard_sigmoid():\n-    '''\n-    Test using a reference hard sigmoid implementation\n-    '''\n+    \"\"\"Test using a reference hard sigmoid implementation.\n+    \"\"\"\n     def ref_hard_sigmoid(x):\n-        '''\n-        Reference hard sigmoid with slope and shift values from theano, see\n-        https://github.com/Theano/Theano/blob/master/theano/tensor/nnet/sigm.py\n-        '''\n         x = (x * 0.2) + 0.5\n         z = 0.0 if x <= 0 else (1.0 if x >= 1 else x)\n         return z\n@@ -117,17 +119,11 @@ def test_hard_sigmoid():\n \n \n def test_relu():\n-    '''\n-    Relu implementation doesn't depend on the value being\n-    a theano variable. Testing ints, floats and theano tensors.\n-    '''\n     x = K.placeholder(ndim=2)\n     f = K.function([x], [activations.relu(x)])\n \n     test_values = get_standard_values()\n     result = f([test_values])[0]\n-\n-    # because no negatives in test values\n     assert_allclose(result, test_values, rtol=1e-05)\n \n \n@@ -137,8 +133,6 @@ def test_elu():\n \n     test_values = get_standard_values()\n     result = f([test_values])[0]\n-\n-    # because no negatives in test values\n     assert_allclose(result, test_values, rtol=1e-05)\n \n     negative_values = np.array([[-1, -2]], dtype=K.floatx())\n@@ -161,11 +155,7 @@ def test_tanh():\n \n \n def test_linear():\n-    '''\n-    This function does no input validation, it just returns the thing\n-    that was passed in.\n-    '''\n-    xs = [1, 5, True, None, 'foo']\n+    xs = [1, 5, True, None]\n     for x in xs:\n         assert(x == activations.linear(x))\n \n\n\n@@ -0,0 +1,78 @@\n+import pytest\n+import numpy as np\n+from numpy.testing import assert_allclose\n+\n+from keras import backend as K\n+from keras import constraints\n+\n+\n+def get_test_values():\n+    return [0.1, 0.5, 3, 8, 1e-7]\n+\n+\n+def get_example_array():\n+    np.random.seed(3537)\n+    example_array = np.random.random((100, 100)) * 100. - 50.\n+    example_array[0, 0] = 0.  # 0 could possibly cause trouble\n+    return example_array\n+\n+\n+def test_serialization():\n+    all_activations = ['max_norm', 'non_neg',\n+                       'unit_norm', 'min_max_norm']\n+    for name in all_activations:\n+        fn = constraints.get(name)\n+        ref_fn = getattr(constraints, name)()\n+        assert fn.__class__ == ref_fn.__class__\n+        config = constraints.serialize(fn)\n+        fn = constraints.deserialize(config)\n+        assert fn.__class__ == ref_fn.__class__\n+\n+\n+def test_max_norm():\n+    array = get_example_array()\n+    for m in get_test_values():\n+        norm_instance = constraints.max_norm(m)\n+        normed = norm_instance(K.variable(array))\n+        assert(np.all(K.eval(normed) < m))\n+\n+    # a more explicit example\n+    norm_instance = constraints.max_norm(2.0)\n+    x = np.array([[0, 0, 0], [1.0, 0, 0], [3, 0, 0], [3, 3, 3]]).T\n+    x_normed_target = np.array([[0, 0, 0], [1.0, 0, 0],\n+                                [2.0, 0, 0],\n+                                [2. / np.sqrt(3),\n+                                 2. / np.sqrt(3),\n+                                 2. / np.sqrt(3)]]).T\n+    x_normed_actual = K.eval(norm_instance(K.variable(x)))\n+    assert_allclose(x_normed_actual, x_normed_target, rtol=1e-05)\n+\n+\n+def test_non_neg():\n+    non_neg_instance = constraints.non_neg()\n+    normed = non_neg_instance(K.variable(get_example_array()))\n+    assert(np.all(np.min(K.eval(normed), axis=1) == 0.))\n+\n+\n+def test_unit_norm():\n+    unit_norm_instance = constraints.unit_norm()\n+    normalized = unit_norm_instance(K.variable(get_example_array()))\n+    norm_of_normalized = np.sqrt(np.sum(K.eval(normalized) ** 2, axis=0))\n+    # In the unit norm constraint, it should be equal to 1.\n+    difference = norm_of_normalized - 1.\n+    largest_difference = np.max(np.abs(difference))\n+    assert(np.abs(largest_difference) < 10e-5)\n+\n+\n+def test_min_max_norm():\n+    array = get_example_array()\n+    for m in get_test_values():\n+        norm_instance = constraints.min_max_norm(min_value=m, max_value=m * 2)\n+        normed = norm_instance(K.variable(array))\n+        value = K.eval(normed)\n+        l2 = np.sqrt(np.sum(np.square(value), axis=0))\n+        assert not l2[l2 < m]\n+        assert not l2[l2 > m * 2 + 1e-5]\n+\n+if __name__ == '__main__':\n+    pytest.main([__file__])\n\n\n@@ -1,20 +1,20 @@\n import pytest\n import numpy as np\n \n-from keras import objectives\n+from keras import losses\n from keras import backend as K\n \n \n-allobj = [objectives.mean_squared_error,\n-          objectives.mean_absolute_error,\n-          objectives.mean_absolute_percentage_error,\n-          objectives.mean_squared_logarithmic_error,\n-          objectives.squared_hinge,\n-          objectives.hinge, objectives.categorical_crossentropy,\n-          objectives.binary_crossentropy,\n-          objectives.kullback_leibler_divergence,\n-          objectives.poisson,\n-          objectives.cosine_proximity]\n+allobj = [losses.mean_squared_error,\n+          losses.mean_absolute_error,\n+          losses.mean_absolute_percentage_error,\n+          losses.mean_squared_logarithmic_error,\n+          losses.squared_hinge,\n+          losses.hinge, losses.categorical_crossentropy,\n+          losses.binary_crossentropy,\n+          losses.kullback_leibler_divergence,\n+          losses.poisson,\n+          losses.cosine_proximity]\n \n \n def test_objective_shapes_3d():\n@@ -36,13 +36,13 @@ def test_objective_shapes_2d():\n def test_cce_one_hot():\n     y_a = K.variable(np.random.randint(0, 7, (5, 6)))\n     y_b = K.variable(np.random.random((5, 6, 7)))\n-    objective_output = objectives.sparse_categorical_crossentropy(y_a, y_b)\n+    objective_output = losses.sparse_categorical_crossentropy(y_a, y_b)\n     assert K.eval(objective_output).shape == (5, 6)\n \n     y_a = K.variable(np.random.randint(0, 7, (6,)))\n     y_b = K.variable(np.random.random((6, 7)))\n-    assert K.eval(objectives.sparse_categorical_crossentropy(y_a, y_b)).shape == (6,)\n+    assert K.eval(losses.sparse_categorical_crossentropy(y_a, y_b)).shape == (6,)\n \n \n-if __name__ == \"__main__\":\n+if __name__ == '__main__':\n     pytest.main([__file__])\n\n@@ -0,0 +1,58 @@\n+import pytest\n+import numpy as np\n+\n+from keras import metrics\n+from keras import backend as K\n+\n+all_metrics = [\n+    metrics.binary_accuracy,\n+    metrics.categorical_accuracy,\n+    metrics.mean_squared_error,\n+    metrics.mean_absolute_error,\n+    metrics.mean_absolute_percentage_error,\n+    metrics.mean_squared_logarithmic_error,\n+    metrics.squared_hinge,\n+    metrics.hinge,\n+    metrics.categorical_crossentropy,\n+    metrics.binary_crossentropy,\n+    metrics.poisson,\n+    metrics.cosine_proximity,\n+]\n+\n+all_sparse_metrics = [\n+    metrics.sparse_categorical_accuracy,\n+    metrics.sparse_categorical_crossentropy,\n+]\n+\n+\n+def test_metrics():\n+    y_a = K.variable(np.random.random((6, 7)))\n+    y_b = K.variable(np.random.random((6, 7)))\n+    for metric in all_metrics:\n+        output = metric(y_a, y_b)\n+        assert K.eval(output).shape == ()\n+\n+\n+def test_sparse_metrics():\n+    for metric in all_sparse_metrics:\n+        y_a = K.variable(np.random.randint(0, 7, (6,)), dtype=K.floatx())\n+        y_b = K.variable(np.random.random((6, 7)), dtype=K.floatx())\n+        assert K.eval(metric(y_a, y_b)).shape == ()\n+\n+\n+def test_top_k_categorical_accuracy():\n+    y_pred = K.variable(np.array([[0.3, 0.2, 0.1], [0.1, 0.2, 0.7]]))\n+    y_true = K.variable(np.array([[0, 1, 0], [1, 0, 0]]))\n+    success_result = K.eval(metrics.top_k_categorical_accuracy(y_true, y_pred,\n+                                                               k=3))\n+    assert success_result == 1\n+    partial_result = K.eval(metrics.top_k_categorical_accuracy(y_true, y_pred,\n+                                                               k=2))\n+    assert partial_result == 0.5\n+    failure_result = K.eval(metrics.top_k_categorical_accuracy(y_true, y_pred,\n+                                                               k=1))\n+    assert failure_result == 0\n+\n+\n+if __name__ == '__main__':\n+    pytest.main([__file__])\n\n@@ -0,0 +1,92 @@\n+from __future__ import print_function\n+import pytest\n+import numpy as np\n+\n+from keras.utils import test_utils\n+from keras import optimizers\n+from keras.models import Sequential\n+from keras.layers.core import Dense, Activation\n+from keras.utils.np_utils import to_categorical\n+\n+\n+def get_test_data():\n+    np.random.seed(1337)\n+    (x_train, y_train), _ = test_utils.get_test_data(num_train=1000,\n+                                                     num_test=200,\n+                                                     input_shape=(10,),\n+                                                     classification=True,\n+                                                     num_classes=2)\n+    y_train = to_categorical(y_train)\n+    return x_train, y_train\n+\n+\n+def get_model(input_dim, num_hidden, output_dim):\n+    model = Sequential()\n+    model.add(Dense(num_hidden, input_shape=(input_dim,)))\n+    model.add(Activation('relu'))\n+    model.add(Dense(output_dim))\n+    model.add(Activation('softmax'))\n+    return model\n+\n+\n+def _test_optimizer(optimizer, target=0.75):\n+    x_train, y_train = get_test_data()\n+    model = get_model(x_train.shape[1], 10, y_train.shape[1])\n+    model.compile(loss='categorical_crossentropy',\n+                  optimizer=optimizer,\n+                  metrics=['accuracy'])\n+    history = model.fit(x_train, y_train, epochs=1, batch_size=16, verbose=0)\n+    assert history.history['acc'][-1] >= target\n+    config = optimizers.serialize(optimizer)\n+    optim = optimizers.deserialize(config)\n+    new_config = optimizers.serialize(optim)\n+    new_config['class_name'] = new_config['class_name'].lower()\n+    assert config == new_config\n+\n+\n+def test_sgd():\n+    sgd = optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n+    _test_optimizer(sgd)\n+\n+\n+def test_rmsprop():\n+    _test_optimizer(optimizers.RMSprop())\n+    _test_optimizer(optimizers.RMSprop(decay=1e-3))\n+\n+\n+def test_adagrad():\n+    _test_optimizer(optimizers.Adagrad())\n+    _test_optimizer(optimizers.Adagrad(decay=1e-3))\n+\n+\n+def test_adadelta():\n+    _test_optimizer(optimizers.Adadelta())\n+    _test_optimizer(optimizers.Adadelta(decay=1e-3))\n+\n+\n+def test_adam():\n+    _test_optimizer(optimizers.Adam())\n+    _test_optimizer(optimizers.Adam(decay=1e-3))\n+\n+\n+def test_adamax():\n+    _test_optimizer(optimizers.Adamax())\n+    _test_optimizer(optimizers.Adamax(decay=1e-3))\n+\n+\n+def test_nadam():\n+    _test_optimizer(optimizers.Nadam())\n+\n+\n+def test_clipnorm():\n+    sgd = optimizers.SGD(lr=0.01, momentum=0.9, clipnorm=0.5)\n+    _test_optimizer(sgd)\n+\n+\n+def test_clipvalue():\n+    sgd = optimizers.SGD(lr=0.01, momentum=0.9, clipvalue=0.5)\n+    _test_optimizer(sgd)\n+\n+\n+if __name__ == '__main__':\n+    pytest.main([__file__])\n\n\n\n\n@@ -0,0 +1,68 @@\n+import pytest\n+import numpy as np\n+\n+from keras.models import Sequential\n+from keras.layers import Dense\n+from keras.layers import Activation\n+from keras.layers import Flatten\n+from keras.layers import ActivityRegularization\n+from keras.layers import Embedding\n+from keras.utils import np_utils\n+from keras.utils import test_utils\n+from keras import regularizers\n+\n+data_dim = 5\n+num_classes = 2\n+epochs = 1\n+batch_size = 10\n+\n+\n+def get_data():\n+    np.random.seed(1337)\n+\n+    # the data, shuffled and split between tran and test sets\n+    (x_train, y_train), (x_test, y_test) = test_utils.get_test_data(num_train=10,\n+                                                                    num_test=10,\n+                                                                    input_shape=(data_dim,),\n+                                                                    classification=True,\n+                                                                    num_classes=num_classes)\n+    y_train = np_utils.to_categorical(y_train, num_classes)\n+    y_test = np_utils.to_categorical(y_test, num_classes)\n+\n+    return (x_train, y_train), (x_test, y_test)\n+\n+\n+def create_model(kernel_regularizer=None, activity_regularizer=None):\n+    model = Sequential()\n+    model.add(Dense(50, input_shape=(data_dim,)))\n+    model.add(Activation('relu'))\n+    model.add(Dense(num_classes, kernel_regularizer=kernel_regularizer,\n+                    activity_regularizer=activity_regularizer))\n+    model.add(Activation('softmax'))\n+    return model\n+\n+\n+def test_kernel_regularization():\n+    (x_train, y_train), (x_test, y_test) = get_data()\n+    for reg in [regularizers.l1(),\n+                regularizers.l2(),\n+                regularizers.l1_l2()]:\n+        model = create_model(kernel_regularizer=reg)\n+        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+        assert len(model.losses) == 1\n+        model.fit(x_train, y_train, batch_size=batch_size,\n+                  epochs=epochs, verbose=0)\n+\n+\n+def test_activity_regularization():\n+    (x_train, y_train), (x_test, y_test) = get_data()\n+    for reg in [regularizers.l1(), regularizers.l2()]:\n+        model = create_model(activity_regularizer=reg)\n+        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+        assert len(model.losses) == 1\n+        model.fit(x_train, y_train, batch_size=batch_size,\n+                  epochs=epochs, verbose=0)\n+\n+\n+if __name__ == '__main__':\n+    pytest.main([__file__])\n\n@@ -1,18 +1,16 @@\n import os\n-import sys\n import multiprocessing\n \n import numpy as np\n import pytest\n from csv import Sniffer\n+import shutil\n from keras import optimizers\n-\n-np.random.seed(1337)\n-\n from keras import callbacks\n from keras.models import Sequential\n from keras.layers.core import Dense\n from keras.utils.test_utils import get_test_data\n+from keras.utils.test_utils import keras_test\n from keras import backend as K\n from keras.utils import np_utils\n \n@@ -24,7 +22,9 @@ train_samples = 20\n test_samples = 20\n \n \n+@keras_test\n def test_ModelCheckpoint():\n+    np.random.seed(1337)\n     filepath = 'checkpoint.h5'\n     (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n                                                          num_test=test_samples,\n@@ -98,7 +98,9 @@ def test_ModelCheckpoint():\n     os.remove(filepath.format(epoch=3))\n \n \n+@keras_test\n def test_EarlyStopping():\n+    np.random.seed(1337)\n     (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n                                                          num_test=test_samples,\n                                                          input_shape=(input_dim,),\n@@ -127,7 +129,9 @@ def test_EarlyStopping():\n                         validation_data=(X_test, y_test), callbacks=cbks, epochs=20)\n \n \n+@keras_test\n def test_EarlyStopping_reuse():\n+    np.random.seed(1337)\n     patience = 3\n     data = np.random.random((100, 1))\n     labels = np.where(data > 0.5, 1, 0)\n@@ -148,7 +152,9 @@ def test_EarlyStopping_reuse():\n     assert len(hist.epoch) >= patience\n \n \n+@keras_test\n def test_LearningRateScheduler():\n+    np.random.seed(1337)\n     (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n                                                          num_test=test_samples,\n                                                          input_shape=(input_dim,),\n@@ -169,7 +175,9 @@ def test_LearningRateScheduler():\n     assert (float(K.get_value(model.optimizer.lr)) - 0.2) < K.epsilon()\n \n \n+@keras_test\n def test_ReduceLROnPlateau():\n+    np.random.seed(1337)\n     (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n                                                          num_test=test_samples,\n                                                          input_shape=(input_dim,),\n@@ -204,7 +212,9 @@ def test_ReduceLROnPlateau():\n     assert np.allclose(float(K.get_value(model.optimizer.lr)), 0.1, atol=K.epsilon())\n \n \n+@keras_test\n def test_CSVLogger():\n+    np.random.seed(1337)\n     filepath = 'log.tsv'\n     sep = '\\t'\n     (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n@@ -257,10 +267,11 @@ def test_CSVLogger():\n     os.remove(filepath)\n \n \n+@keras_test\n @pytest.mark.skipif((K.backend() != 'tensorflow'),\n                     reason=\"Requires tensorflow backend\")\n def test_TensorBoard():\n-    import shutil\n+    np.random.seed(1337)\n \n     filepath = './logs'\n     (X_train, y_train), (X_test, y_test) = get_test_data(\n@@ -336,7 +347,9 @@ def test_TensorBoard():\n     shutil.rmtree(filepath)\n \n \n+@keras_test\n def test_LambdaCallback():\n+    np.random.seed(1337)\n     (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n                                                          num_test=test_samples,\n                                                          input_shape=(input_dim,),\n@@ -367,6 +380,7 @@ def test_LambdaCallback():\n     assert not p.is_alive()\n \n \n+@keras_test\n @pytest.mark.skipif((K.backend() != 'tensorflow'),\n                     reason=\"Requires tensorflow backend\")\n def test_TensorBoard_with_ReduceLROnPlateau():\n\n@@ -1,48 +0,0 @@\n-import pytest\n-import numpy as np\n-from numpy.testing import assert_allclose\n-\n-from keras import backend as K\n-from keras import constraints\n-\n-\n-test_values = [0.1, 0.5, 3, 8, 1e-7]\n-np.random.seed(3537)\n-example_array = np.random.random((100, 100)) * 100. - 50.\n-example_array[0, 0] = 0.  # 0 could possibly cause trouble\n-\n-\n-def test_maxnorm():\n-    for m in test_values:\n-        norm_instance = constraints.maxnorm(m)\n-        normed = norm_instance(K.variable(example_array))\n-        assert(np.all(K.eval(normed) < m))\n-\n-    # a more explicit example\n-    norm_instance = constraints.maxnorm(2.0)\n-    x = np.array([[0, 0, 0], [1.0, 0, 0], [3, 0, 0], [3, 3, 3]]).T\n-    x_normed_target = np.array([[0, 0, 0], [1.0, 0, 0],\n-                                [2.0, 0, 0],\n-                                [2. / np.sqrt(3), 2. / np.sqrt(3), 2. / np.sqrt(3)]]).T\n-    x_normed_actual = K.eval(norm_instance(K.variable(x)))\n-    assert_allclose(x_normed_actual, x_normed_target, rtol=1e-05)\n-\n-\n-def test_nonneg():\n-    nonneg_instance = constraints.nonneg()\n-    normed = nonneg_instance(K.variable(example_array))\n-    assert(np.all(np.min(K.eval(normed), axis=1) == 0.))\n-\n-\n-def test_unitnorm():\n-    unitnorm_instance = constraints.unitnorm()\n-    normalized = unitnorm_instance(K.variable(example_array))\n-    norm_of_normalized = np.sqrt(np.sum(K.eval(normalized)**2, axis=0))\n-    # in the unit norm constraint, it should be equal to 1.\n-    difference = norm_of_normalized - 1.\n-    largest_difference = np.max(np.abs(difference))\n-    assert(np.abs(largest_difference) < 10e-5)\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,119 +0,0 @@\n-import pytest\n-import numpy as np\n-\n-from keras import metrics\n-from keras import backend as K\n-\n-all_metrics = [\n-    metrics.binary_accuracy,\n-    metrics.categorical_accuracy,\n-    metrics.mean_squared_error,\n-    metrics.mean_absolute_error,\n-    metrics.mean_absolute_percentage_error,\n-    metrics.mean_squared_logarithmic_error,\n-    metrics.squared_hinge,\n-    metrics.hinge,\n-    metrics.categorical_crossentropy,\n-    metrics.binary_crossentropy,\n-    metrics.poisson,\n-    metrics.cosine_proximity,\n-    metrics.matthews_correlation,\n-]\n-\n-all_sparse_metrics = [\n-    metrics.sparse_categorical_accuracy,\n-    metrics.sparse_categorical_crossentropy,\n-]\n-\n-\n-def test_metrics():\n-    y_a = K.variable(np.random.random((6, 7)))\n-    y_b = K.variable(np.random.random((6, 7)))\n-    for metric in all_metrics:\n-        output = metric(y_a, y_b)\n-        assert K.eval(output).shape == ()\n-\n-\n-def test_matthews_correlation():\n-    y_true = K.variable(np.array([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0]))\n-    y_pred = K.variable(np.array([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0]))\n-\n-    # Calculated using sklearn.metrics.matthews_corrcoef\n-    expected = -0.14907119849998601\n-\n-    actual = K.eval(metrics.matthews_correlation(y_true, y_pred))\n-    epsilon = 1e-05\n-    assert expected - epsilon <= actual <= expected + epsilon\n-\n-\n-def test_precision():\n-    y_true = K.variable(np.array([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0]))\n-    y_pred = K.variable(np.array([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0]))\n-\n-    # Calculated using sklearn.metrics.precision_score\n-    expected = 0.40000000000000002\n-\n-    actual = K.eval(metrics.precision(y_true, y_pred))\n-    epsilon = 1e-05\n-    assert expected - epsilon <= actual <= expected + epsilon\n-\n-\n-def test_recall():\n-    y_true = K.variable(np.array([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0]))\n-    y_pred = K.variable(np.array([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0]))\n-\n-    # Calculated using sklearn.metrics.recall_score\n-    expected = 0.2857142857142857\n-\n-    actual = K.eval(metrics.recall(y_true, y_pred))\n-    epsilon = 1e-05\n-    assert expected - epsilon <= actual <= expected + epsilon\n-\n-\n-def test_fbeta_score():\n-    y_true = K.variable(np.array([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0]))\n-    y_pred = K.variable(np.array([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0]))\n-\n-    # Calculated using sklearn.metrics.fbeta_score\n-    expected = 0.30303030303030304\n-\n-    actual = K.eval(metrics.fbeta_score(y_true, y_pred, beta=2))\n-    epsilon = 1e-05\n-    assert expected - epsilon <= actual <= expected + epsilon\n-\n-\n-def test_fmeasure():\n-    y_true = K.variable(np.array([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0]))\n-    y_pred = K.variable(np.array([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0]))\n-\n-    # Calculated using sklearn.metrics.f1_score\n-    expected = 0.33333333333333331\n-\n-    actual = K.eval(metrics.fmeasure(y_true, y_pred))\n-    epsilon = 1e-05\n-    assert expected - epsilon <= actual <= expected + epsilon\n-\n-\n-def test_sparse_metrics():\n-    for metric in all_sparse_metrics:\n-        y_a = K.variable(np.random.randint(0, 7, (6,)), dtype=K.floatx())\n-        y_b = K.variable(np.random.random((6, 7)), dtype=K.floatx())\n-        assert K.eval(metric(y_a, y_b)).shape == ()\n-\n-\n-def test_top_k_categorical_accuracy():\n-    y_pred = K.variable(np.array([[0.3, 0.2, 0.1], [0.1, 0.2, 0.7]]))\n-    y_true = K.variable(np.array([[0, 1, 0], [1, 0, 0]]))\n-    success_result = K.eval(metrics.top_k_categorical_accuracy(y_true, y_pred,\n-                                                               k=3))\n-    assert success_result == 1\n-    partial_result = K.eval(metrics.top_k_categorical_accuracy(y_true, y_pred,\n-                                                               k=2))\n-    assert partial_result == 0.5\n-    failure_result = K.eval(metrics.top_k_categorical_accuracy(y_true, y_pred,\n-                                                               k=1))\n-    assert failure_result == 0\n-\n-\n-if __name__ == \"__main__\":\n-    pytest.main([__file__])\n\n@@ -1,278 +0,0 @@\n-from __future__ import print_function\n-import pytest\n-import numpy as np\n-from keras.models import Sequential\n-from keras.layers.core import Dense\n-from keras.utils.test_utils import keras_test\n-\n-\n-@keras_test\n-def test_multiprocessing_training():\n-\n-    reached_end = False\n-\n-    arr_data = np.random.randint(0, 256, (500, 2))\n-    arr_labels = np.random.randint(0, 2, 500)\n-\n-    def myGenerator():\n-\n-        batch_size = 32\n-        n_samples = 500\n-\n-        while True:\n-            batch_index = np.random.randint(0, n_samples - batch_size)\n-            start = batch_index\n-            end = start + batch_size\n-            X = arr_data[start: end]\n-            y = arr_labels[start: end]\n-            yield X, y\n-\n-    # Build a NN\n-    model = Sequential()\n-    model.add(Dense(1, input_shape=(2, )))\n-    model.compile(loss='mse', optimizer='adadelta')\n-\n-    model.fit_generator(myGenerator(),\n-                        samples_per_epoch=320,\n-                        epochs=1,\n-                        verbose=1,\n-                        max_q_size=10,\n-                        workers=4,\n-                        pickle_safe=True)\n-\n-    model.fit_generator(myGenerator(),\n-                        samples_per_epoch=320,\n-                        epochs=1,\n-                        verbose=1,\n-                        max_q_size=10,\n-                        pickle_safe=False)\n-\n-    reached_end = True\n-\n-    assert reached_end\n-\n-\n-@keras_test\n-def test_multiprocessing_training_fromfile():\n-\n-    reached_end = False\n-\n-    arr_data = np.random.randint(0, 256, (500, 2))\n-    arr_labels = np.random.randint(0, 2, 500)\n-    np.savez(\"data.npz\", **{\"data\": arr_data, \"labels\": arr_labels})\n-\n-    def myGenerator():\n-\n-        batch_size = 32\n-        n_samples = 500\n-\n-        arr = np.load(\"data.npz\")\n-\n-        while True:\n-            batch_index = np.random.randint(0, n_samples - batch_size)\n-            start = batch_index\n-            end = start + batch_size\n-            X = arr[\"data\"][start: end]\n-            y = arr[\"labels\"][start: end]\n-            yield X, y\n-\n-    # Build a NN\n-    model = Sequential()\n-    model.add(Dense(1, input_shape=(2, )))\n-    model.compile(loss='mse', optimizer='adadelta')\n-\n-    model.fit_generator(myGenerator(),\n-                        samples_per_epoch=320,\n-                        epochs=1,\n-                        verbose=1,\n-                        max_q_size=10,\n-                        workers=2,\n-                        pickle_safe=True)\n-\n-    model.fit_generator(myGenerator(),\n-                        samples_per_epoch=320,\n-                        epochs=1,\n-                        verbose=1,\n-                        max_q_size=10,\n-                        pickle_safe=False)\n-    reached_end = True\n-\n-    assert reached_end\n-\n-\n-@keras_test\n-def test_multiprocessing_predicting():\n-\n-    reached_end = False\n-\n-    arr_data = np.random.randint(0, 256, (500, 2))\n-\n-    def myGenerator():\n-\n-        batch_size = 32\n-        n_samples = 500\n-\n-        while True:\n-            batch_index = np.random.randint(0, n_samples - batch_size)\n-            start = batch_index\n-            end = start + batch_size\n-            X = arr_data[start: end]\n-            yield X\n-\n-    # Build a NN\n-    model = Sequential()\n-    model.add(Dense(1, input_shape=(2, )))\n-    model.compile(loss='mse', optimizer='adadelta')\n-    model.predict_generator(myGenerator(),\n-                            val_samples=320,\n-                            max_q_size=10,\n-                            workers=2,\n-                            pickle_safe=True)\n-    model.predict_generator(myGenerator(),\n-                            val_samples=320,\n-                            max_q_size=10,\n-                            pickle_safe=False)\n-    reached_end = True\n-\n-    assert reached_end\n-\n-\n-@keras_test\n-def test_multiprocessing_evaluating():\n-\n-    reached_end = False\n-\n-    arr_data = np.random.randint(0, 256, (500, 2))\n-    arr_labels = np.random.randint(0, 2, 500)\n-\n-    def myGenerator():\n-\n-        batch_size = 32\n-        n_samples = 500\n-\n-        while True:\n-            batch_index = np.random.randint(0, n_samples - batch_size)\n-            start = batch_index\n-            end = start + batch_size\n-            X = arr_data[start: end]\n-            y = arr_labels[start: end]\n-            yield X, y\n-\n-    # Build a NN\n-    model = Sequential()\n-    model.add(Dense(1, input_shape=(2, )))\n-    model.compile(loss='mse', optimizer='adadelta')\n-\n-    model.evaluate_generator(myGenerator(),\n-                             val_samples=320,\n-                             max_q_size=10,\n-                             workers=2,\n-                             pickle_safe=True)\n-    model.evaluate_generator(myGenerator(),\n-                             val_samples=320,\n-                             max_q_size=10,\n-                             pickle_safe=False)\n-    reached_end = True\n-\n-    assert reached_end\n-\n-\n-@keras_test\n-def test_multiprocessing_fit_error():\n-\n-    batch_size = 32\n-    good_batches = 5\n-\n-    def myGenerator():\n-        \"\"\"Raises an exception after a few good batches\"\"\"\n-        for i in range(good_batches):\n-            yield (np.random.randint(batch_size, 256, (500, 2)),\n-                   np.random.randint(batch_size, 2, 500))\n-        raise RuntimeError\n-\n-    model = Sequential()\n-    model.add(Dense(1, input_shape=(2, )))\n-    model.compile(loss='mse', optimizer='adadelta')\n-\n-    samples = batch_size * (good_batches + 1)\n-\n-    with pytest.raises(Exception):\n-        model.fit_generator(\n-            myGenerator(), samples, 1,\n-            workers=4, pickle_safe=True,\n-        )\n-\n-    with pytest.raises(Exception):\n-        model.fit_generator(\n-            myGenerator(), samples, 1,\n-            pickle_safe=False,\n-        )\n-\n-\n-@keras_test\n-def test_multiprocessing_evaluate_error():\n-\n-    batch_size = 32\n-    good_batches = 5\n-\n-    def myGenerator():\n-        \"\"\"Raises an exception after a few good batches\"\"\"\n-        for i in range(good_batches):\n-            yield (np.random.randint(batch_size, 256, (500, 2)),\n-                   np.random.randint(batch_size, 2, 500))\n-        raise RuntimeError\n-\n-    model = Sequential()\n-    model.add(Dense(1, input_shape=(2, )))\n-    model.compile(loss='mse', optimizer='adadelta')\n-\n-    samples = batch_size * (good_batches + 1)\n-\n-    with pytest.raises(Exception):\n-        model.evaluate_generator(\n-            myGenerator(), samples, 1,\n-            workers=4, pickle_safe=True,\n-        )\n-\n-    with pytest.raises(Exception):\n-        model.evaluate_generator(\n-            myGenerator(), samples, 1,\n-            pickle_safe=False,\n-        )\n-\n-\n-@keras_test\n-def test_multiprocessing_predict_error():\n-\n-    batch_size = 32\n-    good_batches = 5\n-\n-    def myGenerator():\n-        \"\"\"Raises an exception after a few good batches\"\"\"\n-        for i in range(good_batches):\n-            yield (np.random.randint(batch_size, 256, (500, 2)),\n-                   np.random.randint(batch_size, 2, 500))\n-        raise RuntimeError\n-\n-    model = Sequential()\n-    model.add(Dense(1, input_shape=(2, )))\n-    model.compile(loss='mse', optimizer='adadelta')\n-\n-    samples = batch_size * (good_batches + 1)\n-\n-    with pytest.raises(Exception):\n-        model.predict_generator(\n-            myGenerator(), samples, 1,\n-            workers=4, pickle_safe=True,\n-        )\n-\n-    with pytest.raises(Exception):\n-        model.predict_generator(\n-            myGenerator(), samples, 1,\n-            pickle_safe=False,\n-        )\n-\n-\n-if __name__ == '__main__':\n-\n-    pytest.main([__file__])\n\n@@ -1,88 +0,0 @@\n-from __future__ import print_function\n-import pytest\n-import numpy as np\n-np.random.seed(1337)\n-\n-from keras.utils.test_utils import get_test_data\n-from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\n-from keras.models import Sequential\n-from keras.layers.core import Dense, Activation\n-from keras.utils.np_utils import to_categorical\n-\n-\n-(X_train, y_train), (X_test, y_test) = get_test_data(num_train=1000,\n-                                                     num_test=200,\n-                                                     input_shape=(10,),\n-                                                     classification=True,\n-                                                     num_classes=2)\n-y_train = to_categorical(y_train)\n-y_test = to_categorical(y_test)\n-\n-\n-def get_model(input_dim, num_hidden, output_dim):\n-    model = Sequential()\n-    model.add(Dense(num_hidden, input_shape=(input_dim,)))\n-    model.add(Activation('relu'))\n-    model.add(Dense(output_dim))\n-    model.add(Activation('softmax'))\n-    return model\n-\n-\n-def _test_optimizer(optimizer, target=0.89):\n-    model = get_model(X_train.shape[1], 10, y_train.shape[1])\n-    model.compile(loss='categorical_crossentropy',\n-                  optimizer=optimizer,\n-                  metrics=['accuracy'])\n-    history = model.fit(X_train, y_train, epochs=12, batch_size=16,\n-                        validation_data=(X_test, y_test), verbose=2)\n-    config = optimizer.get_config()\n-    assert type(config) == dict\n-    assert history.history['val_acc'][-1] >= target\n-\n-\n-def test_sgd():\n-    sgd = SGD(lr=0.01, momentum=0.9, nesterov=True)\n-    _test_optimizer(sgd)\n-\n-\n-def test_rmsprop():\n-    _test_optimizer(RMSprop())\n-    _test_optimizer(RMSprop(decay=1e-3))\n-\n-\n-def test_adagrad():\n-    _test_optimizer(Adagrad())\n-    _test_optimizer(Adagrad(decay=1e-3))\n-\n-\n-def test_adadelta():\n-    _test_optimizer(Adadelta(), target=0.83)\n-    _test_optimizer(Adadelta(decay=1e-3), target=0.83)\n-\n-\n-def test_adam():\n-    _test_optimizer(Adam())\n-    _test_optimizer(Adam(decay=1e-3))\n-\n-\n-def test_adamax():\n-    _test_optimizer(Adamax())\n-    _test_optimizer(Adamax(decay=1e-3))\n-\n-\n-def test_nadam():\n-    _test_optimizer(Nadam())\n-\n-\n-def test_clipnorm():\n-    sgd = SGD(lr=0.01, momentum=0.9, clipnorm=0.5)\n-    _test_optimizer(sgd)\n-\n-\n-def test_clipvalue():\n-    sgd = SGD(lr=0.01, momentum=0.9, clipvalue=0.5)\n-    _test_optimizer(sgd)\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,88 +0,0 @@\n-import pytest\n-import numpy as np\n-np.random.seed(1337)\n-\n-from keras.models import Sequential\n-from keras.layers import Merge\n-from keras.layers import Dense\n-from keras.layers import Activation\n-from keras.layers import Flatten\n-from keras.layers import ActivityRegularization\n-from keras.layers import Embedding\n-from keras.datasets import mnist\n-from keras.utils import np_utils\n-from keras import regularizers\n-\n-num_classes = 10\n-batch_size = 128\n-epochs = 5\n-weighted_class = 9\n-standard_weight = 1\n-high_weight = 5\n-max_train_samples = 5000\n-max_test_samples = 1000\n-\n-\n-def get_data():\n-    # the data, shuffled and split between tran and test sets\n-    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n-    X_train = X_train.reshape(60000, 784)[:max_train_samples]\n-    X_test = X_test.reshape(10000, 784)[:max_test_samples]\n-    X_train = X_train.astype(\"float32\") / 255\n-    X_test = X_test.astype(\"float32\") / 255\n-\n-    # convert class vectors to binary class matrices\n-    y_train = y_train[:max_train_samples]\n-    y_test = y_test[:max_test_samples]\n-    Y_train = np_utils.to_categorical(y_train, num_classes)\n-    Y_test = np_utils.to_categorical(y_test, num_classes)\n-    test_ids = np.where(y_test == np.array(weighted_class))[0]\n-\n-    return (X_train, Y_train), (X_test, Y_test), test_ids\n-\n-\n-def create_model(weight_reg=None, activity_reg=None):\n-    model = Sequential()\n-    model.add(Dense(50, input_shape=(784,)))\n-    model.add(Activation('relu'))\n-    model.add(Dense(10, W_regularizer=weight_reg,\n-                    activity_regularizer=activity_reg))\n-    model.add(Activation('softmax'))\n-    return model\n-\n-\n-def test_Eigenvalue_reg():\n-    (X_train, Y_train), (X_test, Y_test), test_ids = get_data()\n-    reg = regularizers.EigenvalueRegularizer(0.01)\n-    model = create_model(weight_reg=reg)\n-    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-    model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n-    model.evaluate(X_test[test_ids, :], Y_test[test_ids, :], verbose=0)\n-\n-\n-def test_W_reg():\n-    (X_train, Y_train), (X_test, Y_test), test_ids = get_data()\n-    for reg in [regularizers.l1(),\n-                regularizers.l2(),\n-                regularizers.l1l2()]:\n-        model = create_model(weight_reg=reg)\n-        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-        assert len(model.losses) == 1\n-        model.fit(X_train, Y_train, batch_size=batch_size,\n-                  epochs=epochs, verbose=0)\n-        model.evaluate(X_test[test_ids, :], Y_test[test_ids, :], verbose=0)\n-\n-\n-def test_A_reg():\n-    (X_train, Y_train), (X_test, Y_test), test_ids = get_data()\n-    for reg in [regularizers.activity_l1(), regularizers.activity_l2()]:\n-        model = create_model(activity_reg=reg)\n-        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-        assert len(model.losses) == 1\n-        model.fit(X_train, Y_train, batch_size=batch_size,\n-                  epochs=epochs, verbose=0)\n-        model.evaluate(X_test[test_ids, :], Y_test[test_ids, :], verbose=0)\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -3,16 +3,16 @@ from __future__ import print_function\n import pytest\n import os\n import numpy as np\n-np.random.seed(1337)\n \n from keras import backend as K\n from keras.models import Sequential\n-from keras.layers.core import Dense, Activation, Merge, Lambda\n+from keras.layers import Dense, Activation, Lambda\n from keras.utils import np_utils\n from keras.utils.test_utils import get_test_data, keras_test\n from keras.models import model_from_json, model_from_yaml\n-from keras import objectives\n+from keras import losses\n from keras.engine.training import make_batches\n+from keras.legacy.layers import Merge\n \n \n input_dim = 16\n@@ -45,31 +45,31 @@ def _get_test_data():\n     train_samples = 100\n     test_samples = 50\n \n-    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n+    (x_train, y_train), (x_test, y_test) = get_test_data(num_train=train_samples,\n                                                          num_test=test_samples,\n                                                          input_shape=(input_dim,),\n                                                          classification=True,\n                                                          num_classes=4)\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n-    return (X_train, y_train), (X_test, y_test)\n+    return (x_train, y_train), (x_test, y_test)\n \n \n @keras_test\n def test_sequential_fit_generator():\n-    (X_train, y_train), (X_test, y_test) = _get_test_data()\n+    (x_train, y_train), (x_test, y_test) = _get_test_data()\n \n     def data_generator(train):\n         if train:\n-            max_batch_index = len(X_train) // batch_size\n+            max_batch_index = len(x_train) // batch_size\n         else:\n-            max_batch_index = len(X_test) // batch_size\n+            max_batch_index = len(x_test) // batch_size\n         i = 0\n         while 1:\n             if train:\n-                yield (X_train[i * batch_size: (i + 1) * batch_size], y_train[i * batch_size: (i + 1) * batch_size])\n+                yield (x_train[i * batch_size: (i + 1) * batch_size], y_train[i * batch_size: (i + 1) * batch_size])\n             else:\n-                yield (X_test[i * batch_size: (i + 1) * batch_size], y_test[i * batch_size: (i + 1) * batch_size])\n+                yield (x_test[i * batch_size: (i + 1) * batch_size], y_test[i * batch_size: (i + 1) * batch_size])\n             i += 1\n             i = i % max_batch_index\n \n@@ -82,23 +82,23 @@ def test_sequential_fit_generator():\n     model.add(Activation('softmax'))\n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n-    model.fit_generator(data_generator(True), len(X_train), epochs)\n-    model.fit_generator(data_generator(True), len(X_train), epochs, validation_data=(X_test, y_test))\n-    model.fit_generator(data_generator(True), len(X_train), epochs,\n+    model.fit_generator(data_generator(True), len(x_train), epochs)\n+    model.fit_generator(data_generator(True), len(x_train), epochs, validation_data=(x_test, y_test))\n+    model.fit_generator(data_generator(True), len(x_train), epochs,\n                         validation_data=data_generator(False), num_val_samples=batch_size * 3)\n-    model.fit_generator(data_generator(True), len(X_train), epochs, max_q_size=2)\n-    model.evaluate(X_train, y_train)\n+    model.fit_generator(data_generator(True), len(x_train), epochs, max_q_size=2)\n+    model.evaluate(x_train, y_train)\n \n \n @keras_test\n def test_sequential():\n-    (X_train, y_train), (X_test, y_test) = _get_test_data()\n+    (x_train, y_train), (x_test, y_test) = _get_test_data()\n \n     # TODO: factor out\n     def data_generator(x, y, batch_size=50):\n         index_array = np.arange(len(x))\n         while 1:\n-            batches = make_batches(len(X_test), batch_size)\n+            batches = make_batches(len(x_test), batch_size)\n             for batch_index, (batch_start, batch_end) in enumerate(batches):\n                 batch_ids = index_array[batch_start:batch_end]\n                 x_batch = x[batch_ids]\n@@ -112,25 +112,25 @@ def test_sequential():\n     model.add(Activation('softmax'))\n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n-    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_test, y_test))\n-    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=2, validation_split=0.1)\n-    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n-    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, shuffle=False)\n+    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n+    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=2, validation_split=0.1)\n+    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n+    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, shuffle=False)\n \n-    model.train_on_batch(X_train[:32], y_train[:32])\n+    model.train_on_batch(x_train[:32], y_train[:32])\n \n-    loss = model.evaluate(X_test, y_test)\n+    loss = model.evaluate(x_test, y_test)\n \n-    prediction = model.predict_generator(data_generator(X_test, y_test), X_test.shape[0], max_q_size=2)\n-    gen_loss = model.evaluate_generator(data_generator(X_test, y_test, 50), X_test.shape[0], max_q_size=2)\n-    pred_loss = K.eval(K.mean(objectives.get(model.loss)(K.variable(y_test), K.variable(prediction))))\n+    prediction = model.predict_generator(data_generator(x_test, y_test), x_test.shape[0], max_q_size=2)\n+    gen_loss = model.evaluate_generator(data_generator(x_test, y_test, 50), x_test.shape[0], max_q_size=2)\n+    pred_loss = K.eval(K.mean(losses.get(model.loss)(K.variable(y_test), K.variable(prediction))))\n \n     assert(np.isclose(pred_loss, loss))\n     assert(np.isclose(gen_loss, loss))\n \n-    model.predict(X_test, verbose=0)\n-    model.predict_classes(X_test, verbose=0)\n-    model.predict_proba(X_test, verbose=0)\n+    model.predict(x_test, verbose=0)\n+    model.predict_classes(x_test, verbose=0)\n+    model.predict_proba(x_test, verbose=0)\n \n     fname = 'test_sequential_temp.h5'\n     model.save_weights(fname, overwrite=True)\n@@ -143,7 +143,7 @@ def test_sequential():\n     model.load_weights(fname)\n     os.remove(fname)\n \n-    nloss = model.evaluate(X_test, y_test, verbose=0)\n+    nloss = model.evaluate(x_test, y_test, verbose=0)\n     assert(loss == nloss)\n \n     # test serialization\n@@ -160,7 +160,7 @@ def test_sequential():\n \n @keras_test\n def test_nested_sequential():\n-    (X_train, y_train), (X_test, y_test) = _get_test_data()\n+    (x_train, y_train), (x_test, y_test) = _get_test_data()\n \n     inner = Sequential()\n     inner.add(Dense(num_hidden, input_shape=(input_dim,)))\n@@ -175,18 +175,18 @@ def test_nested_sequential():\n     model.add(Activation('softmax'))\n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n-    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_test, y_test))\n-    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=2, validation_split=0.1)\n-    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n-    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, shuffle=False)\n+    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n+    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=2, validation_split=0.1)\n+    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n+    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, shuffle=False)\n \n-    model.train_on_batch(X_train[:32], y_train[:32])\n+    model.train_on_batch(x_train[:32], y_train[:32])\n \n-    loss = model.evaluate(X_test, y_test, verbose=0)\n+    loss = model.evaluate(x_test, y_test, verbose=0)\n \n-    model.predict(X_test, verbose=0)\n-    model.predict_classes(X_test, verbose=0)\n-    model.predict_proba(X_test, verbose=0)\n+    model.predict(x_test, verbose=0)\n+    model.predict_classes(x_test, verbose=0)\n+    model.predict_proba(x_test, verbose=0)\n \n     fname = 'test_nested_sequential_temp.h5'\n     model.save_weights(fname, overwrite=True)\n@@ -206,7 +206,7 @@ def test_nested_sequential():\n     model.load_weights(fname)\n     os.remove(fname)\n \n-    nloss = model.evaluate(X_test, y_test, verbose=0)\n+    nloss = model.evaluate(x_test, y_test, verbose=0)\n     assert(loss == nloss)\n \n     # test serialization\n@@ -223,7 +223,7 @@ def test_nested_sequential():\n \n @keras_test\n def test_merge_sum():\n-    (X_train, y_train), (X_test, y_test) = _get_test_data()\n+    (x_train, y_train), (x_test, y_test) = _get_test_data()\n     left = Sequential()\n     left.add(Dense(num_hidden, input_shape=(input_dim,)))\n     left.add(Activation('relu'))\n@@ -238,16 +238,16 @@ def test_merge_sum():\n     model.add(Activation('softmax'))\n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n-    model.fit([X_train, X_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=([X_test, X_test], y_test))\n-    model.fit([X_train, X_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_split=0.1)\n-    model.fit([X_train, X_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n-    model.fit([X_train, X_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, shuffle=False)\n+    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=([x_test, x_test], y_test))\n+    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_split=0.1)\n+    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n+    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, shuffle=False)\n \n-    loss = model.evaluate([X_test, X_test], y_test, verbose=0)\n+    loss = model.evaluate([x_test, x_test], y_test, verbose=0)\n \n-    model.predict([X_test, X_test], verbose=0)\n-    model.predict_classes([X_test, X_test], verbose=0)\n-    model.predict_proba([X_test, X_test], verbose=0)\n+    model.predict([x_test, x_test], verbose=0)\n+    model.predict_classes([x_test, x_test], verbose=0)\n+    model.predict_proba([x_test, x_test], verbose=0)\n \n     # test weight saving\n     fname = 'test_merge_sum_temp.h5'\n@@ -266,7 +266,7 @@ def test_merge_sum():\n     os.remove(fname)\n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n-    nloss = model.evaluate([X_test, X_test], y_test, verbose=0)\n+    nloss = model.evaluate([x_test, x_test], y_test, verbose=0)\n     assert(loss == nloss)\n \n     # test serialization\n@@ -283,7 +283,7 @@ def test_merge_sum():\n \n @keras_test\n def test_merge_dot():\n-    (X_train, y_train), (X_test, y_test) = _get_test_data()\n+    (x_train, y_train), (x_test, y_test) = _get_test_data()\n \n     left = Sequential()\n     left.add(Dense(input_dim=input_dim, output_dim=num_hidden))\n@@ -318,7 +318,7 @@ def test_merge_dot():\n \n @keras_test\n def test_merge_concat():\n-    (X_train, y_train), (X_test, y_test) = _get_test_data()\n+    (x_train, y_train), (x_test, y_test) = _get_test_data()\n \n     left = Sequential(name='branch_1')\n     left.add(Dense(num_hidden, input_shape=(input_dim,), name='dense_1'))\n@@ -334,31 +334,31 @@ def test_merge_concat():\n     model.add(Activation('softmax', name='softmax'))\n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n-    model.fit([X_train, X_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=([X_test, X_test], y_test))\n-    model.fit([X_train, X_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_split=0.1)\n-    model.fit([X_train, X_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n-    model.fit([X_train, X_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, shuffle=False)\n+    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=([x_test, x_test], y_test))\n+    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_split=0.1)\n+    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n+    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, shuffle=False)\n \n-    loss = model.evaluate([X_test, X_test], y_test, verbose=0)\n+    loss = model.evaluate([x_test, x_test], y_test, verbose=0)\n \n-    model.predict([X_test, X_test], verbose=0)\n-    model.predict_classes([X_test, X_test], verbose=0)\n-    model.predict_proba([X_test, X_test], verbose=0)\n+    model.predict([x_test, x_test], verbose=0)\n+    model.predict_classes([x_test, x_test], verbose=0)\n+    model.predict_proba([x_test, x_test], verbose=0)\n     model.get_config()\n \n     fname = 'test_merge_concat_temp.h5'\n     model.save_weights(fname, overwrite=True)\n-    model.fit([X_train, X_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n+    model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n     model.load_weights(fname)\n     os.remove(fname)\n \n-    nloss = model.evaluate([X_test, X_test], y_test, verbose=0)\n+    nloss = model.evaluate([x_test, x_test], y_test, verbose=0)\n     assert(loss == nloss)\n \n \n @keras_test\n def test_merge_recursivity():\n-    (X_train, y_train), (X_test, y_test) = _get_test_data()\n+    (x_train, y_train), (x_test, y_test) = _get_test_data()\n     left = Sequential()\n     left.add(Dense(num_hidden, input_shape=(input_dim,)))\n     left.add(Activation('relu'))\n@@ -382,23 +382,23 @@ def test_merge_recursivity():\n     model.add(Activation('softmax'))\n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n-    model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=([X_test, X_test, X_test], y_test))\n-    model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_split=0.1)\n-    model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n-    model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, shuffle=False)\n+    model.fit([x_train, x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=([x_test, x_test, x_test], y_test))\n+    model.fit([x_train, x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_split=0.1)\n+    model.fit([x_train, x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n+    model.fit([x_train, x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, shuffle=False)\n \n-    loss = model.evaluate([X_test, X_test, X_test], y_test, verbose=0)\n+    loss = model.evaluate([x_test, x_test, x_test], y_test, verbose=0)\n \n-    model.predict([X_test, X_test, X_test], verbose=0)\n-    model.predict_classes([X_test, X_test, X_test], verbose=0)\n-    model.predict_proba([X_test, X_test, X_test], verbose=0)\n+    model.predict([x_test, x_test, x_test], verbose=0)\n+    model.predict_classes([x_test, x_test, x_test], verbose=0)\n+    model.predict_proba([x_test, x_test, x_test], verbose=0)\n \n     fname = 'test_merge_recursivity_temp.h5'\n     model.save_weights(fname, overwrite=True)\n     model.load_weights(fname)\n     os.remove(fname)\n \n-    nloss = model.evaluate([X_test, X_test, X_test], y_test, verbose=0)\n+    nloss = model.evaluate([x_test, x_test, x_test], y_test, verbose=0)\n     assert(loss == nloss)\n \n     # test serialization\n@@ -415,7 +415,7 @@ def test_merge_recursivity():\n \n @keras_test\n def test_merge_overlap():\n-    (X_train, y_train), (X_test, y_test) = _get_test_data()\n+    (x_train, y_train), (x_test, y_test) = _get_test_data()\n     left = Sequential()\n     left.add(Dense(num_hidden, input_shape=(input_dim,)))\n     left.add(Activation('relu'))\n@@ -426,17 +426,17 @@ def test_merge_overlap():\n     model.add(Activation('softmax'))\n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n-    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_test, y_test))\n-    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=2, validation_split=0.1)\n-    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n-    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, shuffle=False)\n+    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n+    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=2, validation_split=0.1)\n+    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n+    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, shuffle=False)\n \n-    model.train_on_batch(X_train[:32], y_train[:32])\n+    model.train_on_batch(x_train[:32], y_train[:32])\n \n-    loss = model.evaluate(X_test, y_test, verbose=0)\n-    model.predict(X_test, verbose=0)\n-    model.predict_classes(X_test, verbose=0)\n-    model.predict_proba(X_test, verbose=0)\n+    loss = model.evaluate(x_test, y_test, verbose=0)\n+    model.predict(x_test, verbose=0)\n+    model.predict_classes(x_test, verbose=0)\n+    model.predict_proba(x_test, verbose=0)\n \n     fname = 'test_merge_overlap_temp.h5'\n     print(model.layers)\n@@ -446,7 +446,7 @@ def test_merge_overlap():\n     model.load_weights(fname)\n     os.remove(fname)\n \n-    nloss = model.evaluate(X_test, y_test, verbose=0)\n+    nloss = model.evaluate(x_test, y_test, verbose=0)\n     assert(loss == nloss)\n \n     # test serialization\n\n@@ -1,41 +0,0 @@\n-from __future__ import absolute_import\r\n-from __future__ import print_function\r\n-import pytest\r\n-\r\n-from keras.models import Model\r\n-from keras.layers import Dense, Input\r\n-from keras.utils.test_utils import keras_test\r\n-from keras import backend as K\r\n-from keras.backend import theano_backend as KTH\r\n-from keras.backend import tensorflow_backend as KTF\r\n-\r\n-import scipy.sparse as sparse\r\n-import numpy as np\r\n-np.random.seed(1337)\r\n-\r\n-\r\n-input_dim = 16\r\n-num_hidden = 8\r\n-num_class = 4\r\n-batch_size = 32\r\n-epochs = 1\r\n-\r\n-\r\n-def do_sparse():\r\n-    return K == KTF or KTH.th_sparse_module\r\n-\r\n-\r\n-@keras_test\r\n-def test_sparse_mlp():\r\n-    if not do_sparse():\r\n-        return\r\n-\r\n-    input = Input(batch_shape=(None, input_dim), sparse=True)\r\n-    hidden = Dense(num_hidden, activation='relu')(input)\r\n-    hidden = Dense(num_hidden, activation='relu')(hidden)\r\n-    predictions = Dense(num_class, activation='sigmoid')(hidden)\r\n-    model = Model(input=[input], output=predictions)\r\n-    model.compile(loss='mse', optimizer='sgd')\r\n-    x = sparse.rand(batch_size, input_dim, density=0.1, format='csr')\r\n-    y = np.random.random((batch_size, num_class))\r\n-    model.fit(x, y, epochs=1)\r\n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
