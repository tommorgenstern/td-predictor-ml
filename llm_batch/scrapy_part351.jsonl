{"custom_id": "scrapy#ea299dfd7ce8766c2c9430fe8b297fddad45adee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 16 | Files Changed: 2 | Hunks: 16 | Methods Changed: 21 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 50 | Churn Cumulative: 1918 | Contributors (this commit): 51 | Commits (past 90d): 7 | Contributors (cumulative): 65 | DMM Complexity: 0.8461538461538461\n\nDIFF:\n@@ -10,7 +10,21 @@ from contextlib import contextmanager\n from functools import partial\n from importlib import import_module\n from pkgutil import iter_modules\n-from typing import TYPE_CHECKING, Any, Callable, Union\n+from types import ModuleType\n+from typing import (\n+    IO,\n+    TYPE_CHECKING,\n+    Any,\n+    Callable,\n+    Deque,\n+    Generator,\n+    Iterable,\n+    List,\n+    Optional,\n+    Pattern,\n+    Union,\n+    cast,\n+)\n \n from w3lib.html import replace_entities\n \n@@ -26,7 +40,7 @@ if TYPE_CHECKING:\n _ITERABLE_SINGLE_VALUES = dict, Item, str, bytes\n \n \n-def arg_to_iter(arg):\n+def arg_to_iter(arg: Any) -> Iterable[Any]:\n     \"\"\"Convert an argument to an iterable. The argument can be a None, single\n     value, or an iterable.\n \n@@ -35,7 +49,7 @@ def arg_to_iter(arg):\n     if arg is None:\n         return []\n     if not isinstance(arg, _ITERABLE_SINGLE_VALUES) and hasattr(arg, \"__iter__\"):\n-        return arg\n+        return cast(Iterable[Any], arg)\n     return [arg]\n \n \n@@ -72,7 +86,7 @@ def load_object(path: Union[str, Callable]) -> Any:\n     return obj\n \n \n-def walk_modules(path):\n+def walk_modules(path: str) -> List[ModuleType]:\n     \"\"\"Loads a module and all its submodules from the given module path and\n     returns them. If *any* module throws an exception while importing, that\n     exception is thrown back.\n@@ -80,7 +94,7 @@ def walk_modules(path):\n     For example: walk_modules('scrapy.utils')\n     \"\"\"\n \n-    mods = []\n+    mods: List[ModuleType] = []\n     mod = import_module(path)\n     mods.append(mod)\n     if hasattr(mod, \"__path__\"):\n@@ -94,7 +108,9 @@ def walk_modules(path):\n     return mods\n \n \n-def extract_regex(regex, text, encoding=\"utf-8\"):\n+def extract_regex(\n+    regex: Union[str, Pattern], text: str, encoding: str = \"utf-8\"\n+) -> List[str]:\n     \"\"\"Extract a list of unicode strings from the given text/encoding using the following policies:\n \n     * if the regex contains a named group called \"extract\" that will be returned\n@@ -111,9 +127,11 @@ def extract_regex(regex, text, encoding=\"utf-8\"):\n         regex = re.compile(regex, re.UNICODE)\n \n     try:\n-        strings = [regex.search(text).group(\"extract\")]  # named group\n+        # named group\n+        strings = [regex.search(text).group(\"extract\")]  # type: ignore[union-attr]\n     except Exception:\n-        strings = regex.findall(text)  # full regex or numbered groups\n+        # full regex or numbered groups\n+        strings = regex.findall(text)\n     strings = flatten(strings)\n \n     if isinstance(text, str):\n@@ -123,7 +141,7 @@ def extract_regex(regex, text, encoding=\"utf-8\"):\n     ]\n \n \n-def md5sum(file):\n+def md5sum(file: IO) -> str:\n     \"\"\"Calculate the md5 checksum of a file-like object without reading its\n     whole content in memory.\n \n@@ -140,7 +158,7 @@ def md5sum(file):\n     return m.hexdigest()\n \n \n-def rel_has_nofollow(rel):\n+def rel_has_nofollow(rel: Optional[str]) -> bool:\n     \"\"\"Return True if link rel attribute has nofollow type\"\"\"\n     return rel is not None and \"nofollow\" in rel.replace(\",\", \" \").split()\n \n@@ -181,7 +199,7 @@ def create_instance(objcls, settings, crawler, *args, **kwargs):\n \n \n @contextmanager\n-def set_environ(**kwargs):\n+def set_environ(**kwargs: str) -> Generator[None, Any, None]:\n     \"\"\"Temporarily set environment variables inside the context manager and\n     fully restore previous environment afterwards\n     \"\"\"\n@@ -198,11 +216,11 @@ def set_environ(**kwargs):\n                 os.environ[k] = v\n \n \n-def walk_callable(node):\n+def walk_callable(node: ast.AST) -> Generator[ast.AST, Any, None]:\n     \"\"\"Similar to ``ast.walk``, but walks only function body and skips nested\n     functions defined within the node.\n     \"\"\"\n-    todo = deque([node])\n+    todo: Deque[ast.AST] = deque([node])\n     walked_func_def = False\n     while todo:\n         node = todo.popleft()\n@@ -227,7 +245,7 @@ def is_generator_with_return_value(callable: Callable) -> bool:\n     if callable in _generator_callbacks_cache:\n         return bool(_generator_callbacks_cache[callable])\n \n-    def returns_none(return_node):\n+    def returns_none(return_node: ast.Return) -> bool:\n         value = return_node.value\n         return (\n             value is None or isinstance(value, ast.NameConstant) and value.value is None\n\n@@ -22,7 +22,7 @@ from typing import (\n from scrapy.utils.asyncgen import as_async_generator\n \n \n-def flatten(x):\n+def flatten(x: Iterable) -> list:\n     \"\"\"flatten(sequence) -> list\n \n     Returns a single, flat list which contains all elements retrieved\n@@ -42,7 +42,7 @@ def flatten(x):\n     return list(iflatten(x))\n \n \n-def iflatten(x):\n+def iflatten(x: Iterable) -> Iterable:\n     \"\"\"iflatten(sequence) -> iterator\n \n     Similar to ``.flatten()``, but returns iterator instead\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#43ee483a0dc6c7883ba9f219ac8b4fd95647b83d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 12 | Methods Changed: 13 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 23 | Churn Cumulative: 348 | Contributors (this commit): 15 | Commits (past 90d): 3 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,7 +1,8 @@\n import asyncio\n import sys\n+from asyncio import AbstractEventLoop, AbstractEventLoopPolicy\n from contextlib import suppress\n-from typing import Any, Callable, Dict, Optional, Sequence\n+from typing import Any, Callable, Dict, Optional, Sequence, Type\n from warnings import catch_warnings, filterwarnings, warn\n \n from twisted.internet import asyncioreactor, error\n@@ -57,7 +58,7 @@ class CallLaterOnce:\n         return self._func(*self._a, **self._kw)\n \n \n-def set_asyncio_event_loop_policy():\n+def set_asyncio_event_loop_policy() -> None:\n     \"\"\"The policy functions from asyncio often behave unexpectedly,\n     so we restrict their use to the absolutely essential case.\n     This should only be used to install the reactor.\n@@ -65,7 +66,7 @@ def set_asyncio_event_loop_policy():\n     _get_asyncio_event_loop_policy()\n \n \n-def get_asyncio_event_loop_policy():\n+def get_asyncio_event_loop_policy() -> AbstractEventLoopPolicy:\n     warn(\n         \"Call to deprecated function \"\n         \"scrapy.utils.reactor.get_asyncio_event_loop_policy().\\n\"\n@@ -81,7 +82,7 @@ def get_asyncio_event_loop_policy():\n     return _get_asyncio_event_loop_policy()\n \n \n-def _get_asyncio_event_loop_policy():\n+def _get_asyncio_event_loop_policy() -> AbstractEventLoopPolicy:\n     policy = asyncio.get_event_loop_policy()\n     if (\n         sys.version_info >= (3, 8)\n@@ -93,7 +94,7 @@ def _get_asyncio_event_loop_policy():\n     return policy\n \n \n-def install_reactor(reactor_path, event_loop_path=None):\n+def install_reactor(reactor_path: str, event_loop_path: Optional[str] = None) -> None:\n     \"\"\"Installs the :mod:`~twisted.internet.reactor` with the specified\n     import path. Also installs the asyncio event loop with the specified import\n     path if the asyncio reactor is enabled\"\"\"\n@@ -111,14 +112,14 @@ def install_reactor(reactor_path, event_loop_path=None):\n             installer()\n \n \n-def _get_asyncio_event_loop():\n+def _get_asyncio_event_loop() -> AbstractEventLoop:\n     return set_asyncio_event_loop(None)\n \n \n-def set_asyncio_event_loop(event_loop_path):\n+def set_asyncio_event_loop(event_loop_path: Optional[str]) -> AbstractEventLoop:\n     \"\"\"Sets and returns the event loop with specified import path.\"\"\"\n     if event_loop_path is not None:\n-        event_loop_class = load_object(event_loop_path)\n+        event_loop_class: Type[AbstractEventLoop] = load_object(event_loop_path)\n         event_loop = event_loop_class()\n         asyncio.set_event_loop(event_loop)\n     else:\n@@ -146,7 +147,7 @@ def set_asyncio_event_loop(event_loop_path):\n     return event_loop\n \n \n-def verify_installed_reactor(reactor_path):\n+def verify_installed_reactor(reactor_path: str) -> None:\n     \"\"\"Raises :exc:`Exception` if the installed\n     :mod:`~twisted.internet.reactor` does not match the specified import\n     path.\"\"\"\n@@ -162,7 +163,7 @@ def verify_installed_reactor(reactor_path):\n         raise Exception(msg)\n \n \n-def verify_installed_asyncio_event_loop(loop_path):\n+def verify_installed_asyncio_event_loop(loop_path: str) -> None:\n     from twisted.internet import reactor\n \n     loop_class = load_object(loop_path)\n@@ -181,7 +182,7 @@ def verify_installed_asyncio_event_loop(loop_path):\n     )\n \n \n-def is_asyncio_reactor_installed():\n+def is_asyncio_reactor_installed() -> bool:\n     from twisted.internet import reactor\n \n     return isinstance(reactor, asyncioreactor.AsyncioSelectorReactor)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4da86915109f77066c306130ecd122a17430191d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 17 | Churn Cumulative: 265 | Contributors (this commit): 15 | Commits (past 90d): 1 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,15 +1,18 @@\n import struct\n from gzip import GzipFile\n from io import BytesIO\n+from typing import List\n+\n+from scrapy.http import Response\n \n \n-def gunzip(data):\n+def gunzip(data: bytes) -> bytes:\n     \"\"\"Gunzip the given data and return as much data as possible.\n \n     This is resilient to CRC checksum errors.\n     \"\"\"\n     f = GzipFile(fileobj=BytesIO(data))\n-    output_list = []\n+    output_list: List[bytes] = []\n     chunk = b\".\"\n     while chunk:\n         try:\n@@ -18,17 +21,13 @@ def gunzip(data):\n         except (IOError, EOFError, struct.error):\n             # complete only if there is some data, otherwise re-raise\n             # see issue 87 about catching struct.error\n-            # some pages are quite small so output_list is empty and f.extrabuf\n-            # contains the whole page content\n-            if output_list or getattr(f, \"extrabuf\", None):\n-                try:\n-                    output_list.append(f.extrabuf[-f.extrasize :])\n-                finally:\n+            # some pages are quite small so output_list is empty\n+            if output_list:\n                 break\n             else:\n                 raise\n     return b\"\".join(output_list)\n \n \n-def gzip_magic_number(response):\n+def gzip_magic_number(response: Response) -> bool:\n     return response.body[:3] == b\"\\x1f\\x8b\\x08\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d400f1ac0664768d04985f0a00bc6d47a54957fe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 22 | Files Changed: 1 | Hunks: 25 | Methods Changed: 24 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 58 | Churn Cumulative: 1188 | Contributors (this commit): 29 | Commits (past 90d): 5 | Contributors (cumulative): 29 | DMM Complexity: 0.8333333333333334\n\nDIFF:\n@@ -1,6 +1,7 @@\n \"\"\"\n This module contains essential stuff that should've come with Python itself ;)\n \"\"\"\n+import collections.abc\n import gc\n import inspect\n import re\n@@ -12,9 +13,17 @@ from typing import (\n     Any,\n     AsyncGenerator,\n     AsyncIterable,\n+    AsyncIterator,\n+    Callable,\n+    Dict,\n+    Generator,\n     Iterable,\n+    Iterator,\n+    List,\n     Mapping,\n     Optional,\n+    Pattern,\n+    Tuple,\n     Union,\n     overload,\n )\n@@ -78,7 +87,7 @@ def is_listlike(x: Any) -> bool:\n     return hasattr(x, \"__iter__\") and not isinstance(x, (str, bytes))\n \n \n-def unique(list_, key=lambda x: x):\n+def unique(list_: Iterable, key: Callable[[Any], Any] = lambda x: x) -> list:\n     \"\"\"efficient function to uniquify a list preserving item order\"\"\"\n     seen = set()\n     result = []\n@@ -124,7 +133,9 @@ def to_bytes(\n     return text.encode(encoding, errors)\n \n \n-def re_rsearch(pattern, text, chunk_size=1024):\n+def re_rsearch(\n+    pattern: Union[str, Pattern], text: str, chunk_size: int = 1024\n+) -> Optional[Tuple[int, int]]:\n     \"\"\"\n     This function does a reverse search in a text using a regular expression\n     given in the attribute 'pattern'.\n@@ -138,7 +149,7 @@ def re_rsearch(pattern, text, chunk_size=1024):\n     the start position of the match, and the ending (regarding the entire text).\n     \"\"\"\n \n-    def _chunk_iter():\n+    def _chunk_iter() -> Generator[Tuple[str, int], Any, None]:\n         offset = len(text)\n         while True:\n             offset -= chunk_size * 1024\n@@ -158,14 +169,14 @@ def re_rsearch(pattern, text, chunk_size=1024):\n     return None\n \n \n-def memoizemethod_noargs(method):\n+def memoizemethod_noargs(method: Callable) -> Callable:\n     \"\"\"Decorator to cache the result of a method (without arguments) using a\n     weak reference to its object\n     \"\"\"\n-    cache = weakref.WeakKeyDictionary()\n+    cache: weakref.WeakKeyDictionary[Any, Any] = weakref.WeakKeyDictionary()\n \n     @wraps(method)\n-    def new_method(self, *args, **kwargs):\n+    def new_method(self: Any, *args: Any, **kwargs: Any) -> Any:\n         if self not in cache:\n             cache[self] = method(self, *args, **kwargs)\n         return cache[self]\n@@ -187,12 +198,12 @@ def binary_is_text(data: bytes) -> bool:\n     return all(c not in _BINARYCHARS for c in data)\n \n \n-def get_func_args(func, stripself=False):\n+def get_func_args(func: Callable, stripself: bool = False) -> List[str]:\n     \"\"\"Return the argument name list of a callable object\"\"\"\n     if not callable(func):\n         raise TypeError(f\"func must be callable, got '{type(func).__name__}'\")\n \n-    args = []\n+    args: List[str] = []\n     try:\n         sig = inspect.signature(func)\n     except ValueError:\n@@ -217,7 +228,7 @@ def get_func_args(func, stripself=False):\n     return args\n \n \n-def get_spec(func):\n+def get_spec(func: Callable) -> Tuple[List[str], Dict[str, Any]]:\n     \"\"\"Returns (args, kwargs) tuple for a function\n     >>> import re\n     >>> get_spec(re.match)\n@@ -246,7 +257,7 @@ def get_spec(func):\n     else:\n         raise TypeError(f\"{type(func)} is not callable\")\n \n-    defaults = spec.defaults or []\n+    defaults: Tuple[Any, ...] = spec.defaults or ()\n \n     firstdefault = len(spec.args) - len(defaults)\n     args = spec.args[:firstdefault]\n@@ -254,7 +265,9 @@ def get_spec(func):\n     return args, kwargs\n \n \n-def equal_attributes(obj1, obj2, attributes):\n+def equal_attributes(\n+    obj1: Any, obj2: Any, attributes: Optional[List[Union[str, Callable]]]\n+) -> bool:\n     \"\"\"Compare two objects attributes\"\"\"\n     # not attributes given return False by default\n     if not attributes:\n@@ -282,19 +295,20 @@ def without_none_values(iterable: Iterable) -> Iterable:\n     ...\n \n \n-def without_none_values(iterable):\n+def without_none_values(iterable: Union[Mapping, Iterable]) -> Union[dict, Iterable]:\n     \"\"\"Return a copy of ``iterable`` with all ``None`` entries removed.\n \n     If ``iterable`` is a mapping, return a dictionary where all pairs that have\n     value ``None`` have been removed.\n     \"\"\"\n-    try:\n+    if isinstance(iterable, collections.abc.Mapping):\n         return {k: v for k, v in iterable.items() if v is not None}\n-    except AttributeError:\n-        return type(iterable)((v for v in iterable if v is not None))\n+    else:\n+        # the iterable __init__ must take another iterable\n+        return type(iterable)(v for v in iterable if v is not None)  # type: ignore[call-arg]\n \n \n-def global_object_name(obj):\n+def global_object_name(obj: Any) -> str:\n     \"\"\"\n     Return full name of a global object.\n \n@@ -307,14 +321,14 @@ def global_object_name(obj):\n \n if hasattr(sys, \"pypy_version_info\"):\n \n-    def garbage_collect():\n+    def garbage_collect() -> None:\n         # Collecting weakreferences can take two collections on PyPy.\n         gc.collect()\n         gc.collect()\n \n else:\n \n-    def garbage_collect():\n+    def garbage_collect() -> None:\n         gc.collect()\n \n \n@@ -329,10 +343,10 @@ class MutableChain(Iterable):\n     def extend(self, *iterables: Iterable) -> None:\n         self.data = chain(self.data, chain.from_iterable(iterables))\n \n-    def __iter__(self):\n+    def __iter__(self) -> Iterator:\n         return self\n \n-    def __next__(self):\n+    def __next__(self) -> Any:\n         return next(self.data)\n \n \n@@ -353,8 +367,8 @@ class MutableAsyncChain(AsyncIterable):\n     def extend(self, *iterables: Union[Iterable, AsyncIterable]) -> None:\n         self.data = _async_chain(self.data, _async_chain(*iterables))\n \n-    def __aiter__(self):\n+    def __aiter__(self) -> AsyncIterator:\n         return self\n \n-    async def __anext__(self):\n+    async def __anext__(self) -> Any:\n         return await self.data.__anext__()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9661a5c4913e4be314572e12a49c215e7631db88", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 23 | Files Changed: 1 | Hunks: 12 | Methods Changed: 21 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 59 | Churn Cumulative: 527 | Contributors (this commit): 17 | Commits (past 90d): 1 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,12 +2,12 @@\n \n import inspect\n import warnings\n-from typing import List, Tuple\n+from typing import Any, List, Optional, Tuple, Type, overload\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n \n \n-def attribute(obj, oldattr, newattr, version=\"0.12\"):\n+def attribute(obj: Any, oldattr: str, newattr: str, version: str = \"0.12\") -> None:\n     cname = obj.__class__.__name__\n     warnings.warn(\n         f\"{cname}.{oldattr} attribute is deprecated and will be no longer supported \"\n@@ -18,16 +18,16 @@ def attribute(obj, oldattr, newattr, version=\"0.12\"):\n \n \n def create_deprecated_class(\n-    name,\n-    new_class,\n-    clsdict=None,\n-    warn_category=ScrapyDeprecationWarning,\n-    warn_once=True,\n-    old_class_path=None,\n-    new_class_path=None,\n-    subclass_warn_message=\"{cls} inherits from deprecated class {old}, please inherit from {new}.\",\n-    instance_warn_message=\"{cls} is deprecated, instantiate {new} instead.\",\n-):\n+    name: str,\n+    new_class: type,\n+    clsdict: Optional[dict[str, Any]] = None,\n+    warn_category: Type[Warning] = ScrapyDeprecationWarning,\n+    warn_once: bool = True,\n+    old_class_path: Optional[str] = None,\n+    new_class_path: Optional[str] = None,\n+    subclass_warn_message: str = \"{cls} inherits from deprecated class {old}, please inherit from {new}.\",\n+    instance_warn_message: str = \"{cls} is deprecated, instantiate {new} instead.\",\n+) -> type:\n     \"\"\"\n     Return a \"deprecated\" class that causes its subclasses to issue a warning.\n     Subclasses of ``new_class`` are considered subclasses of this class.\n@@ -53,17 +53,20 @@ def create_deprecated_class(\n     OldName.\n     \"\"\"\n \n-    class DeprecatedClass(new_class.__class__):\n-        deprecated_class = None\n-        warned_on_subclass = False\n+    # https://github.com/python/mypy/issues/4177\n+    class DeprecatedClass(new_class.__class__):  # type: ignore[misc, name-defined]\n+        deprecated_class: Optional[type] = None\n+        warned_on_subclass: bool = False\n \n-        def __new__(metacls, name, bases, clsdict_):\n+        def __new__(\n+            metacls, name: str, bases: Tuple[type, ...], clsdict_: dict[str, Any]\n+        ) -> type:\n             cls = super().__new__(metacls, name, bases, clsdict_)\n             if metacls.deprecated_class is None:\n                 metacls.deprecated_class = cls\n             return cls\n \n-        def __init__(cls, name, bases, clsdict_):\n+        def __init__(cls, name: str, bases: Tuple[type, ...], clsdict_: dict[str, Any]):\n             meta = cls.__class__\n             old = meta.deprecated_class\n             if old in bases and not (warn_once and meta.warned_on_subclass):\n@@ -81,10 +84,10 @@ def create_deprecated_class(\n         # see https://www.python.org/dev/peps/pep-3119/#overloading-isinstance-and-issubclass\n         # and https://docs.python.org/reference/datamodel.html#customizing-instance-and-subclass-checks\n         # for implementation details\n-        def __instancecheck__(cls, inst):\n+        def __instancecheck__(cls, inst: Any) -> bool:\n             return any(cls.__subclasscheck__(c) for c in (type(inst), inst.__class__))\n \n-        def __subclasscheck__(cls, sub):\n+        def __subclasscheck__(cls, sub: type) -> bool:\n             if cls is not DeprecatedClass.deprecated_class:\n                 # we should do the magic only if second `issubclass` argument\n                 # is the deprecated class itself - subclasses of the\n@@ -98,7 +101,7 @@ def create_deprecated_class(\n             mro = getattr(sub, \"__mro__\", ())\n             return any(c in {cls, new_class} for c in mro)\n \n-        def __call__(cls, *args, **kwargs):\n+        def __call__(cls, *args: Any, **kwargs: Any) -> Any:\n             old = DeprecatedClass.deprecated_class\n             if cls is old:\n                 msg = instance_warn_message.format(\n@@ -125,7 +128,7 @@ def create_deprecated_class(\n     return deprecated_cls\n \n \n-def _clspath(cls, forced=None):\n+def _clspath(cls: type, forced: Optional[str] = None) -> str:\n     if forced is not None:\n         return forced\n     return f\"{cls.__module__}.{cls.__name__}\"\n@@ -134,7 +137,17 @@ def _clspath(cls, forced=None):\n DEPRECATION_RULES: List[Tuple[str, str]] = []\n \n \n-def update_classpath(path):\n+@overload\n+def update_classpath(path: str) -> str:\n+    ...\n+\n+\n+@overload\n+def update_classpath(path: Any) -> Any:\n+    ...\n+\n+\n+def update_classpath(path: Any) -> Any:\n     \"\"\"Update a deprecated path from an object with its new location\"\"\"\n     for prefix, replacement in DEPRECATION_RULES:\n         if isinstance(path, str) and path.startswith(prefix):\n@@ -147,7 +160,7 @@ def update_classpath(path):\n     return path\n \n \n-def method_is_overridden(subclass, base_class, method_name):\n+def method_is_overridden(subclass: type, base_class: type, method_name: str) -> bool:\n     \"\"\"\n     Return True if a method named ``method_name`` of a ``base_class``\n     is overridden in a ``subclass``.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b8277f4cab7941989402a8339634a91dcd3500c4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 37 | Lines Deleted: 20 | Files Changed: 1 | Hunks: 18 | Methods Changed: 29 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 57 | Churn Cumulative: 901 | Contributors (this commit): 22 | Commits (past 90d): 2 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,13 +9,16 @@ from typing import (\n     Any,\n     AsyncGenerator,\n     AsyncIterable,\n+    AsyncIterator,\n     Callable,\n     Coroutine,\n+    Dict,\n     Generator,\n     Iterable,\n     Iterator,\n     List,\n     Optional,\n+    Tuple,\n     Union,\n     cast,\n )\n@@ -44,7 +47,7 @@ def defer_fail(_failure: Failure) -> Deferred:\n     return d\n \n \n-def defer_succeed(result) -> Deferred:\n+def defer_succeed(result: Any) -> Deferred:\n     \"\"\"Same as twisted.internet.defer.succeed but delay calling callback until\n     next reactor loop\n \n@@ -58,7 +61,7 @@ def defer_succeed(result) -> Deferred:\n     return d\n \n \n-def defer_result(result) -> Deferred:\n+def defer_result(result: Any) -> Deferred:\n     if isinstance(result, Deferred):\n         return result\n     if isinstance(result, failure.Failure):\n@@ -66,7 +69,7 @@ def defer_result(result) -> Deferred:\n     return defer_succeed(result)\n \n \n-def mustbe_deferred(f: Callable, *args, **kw) -> Deferred:\n+def mustbe_deferred(f: Callable, *args: Any, **kw: Any) -> Deferred:\n     \"\"\"Same as twisted.internet.defer.maybeDeferred, but delay calling\n     callback/errback to next reactor loop\n     \"\"\"\n@@ -84,7 +87,7 @@ def mustbe_deferred(f: Callable, *args, **kw) -> Deferred:\n \n \n def parallel(\n-    iterable: Iterable, count: int, callable: Callable, *args, **named\n+    iterable: Iterable, count: int, callable: Callable, *args: Any, **named: Any\n ) -> Deferred:\n     \"\"\"Execute a callable over the objects in the given iterable, in parallel,\n     using no more than ``count`` concurrent calls.\n@@ -146,14 +149,14 @@ class _AsyncCooperatorAdapter(Iterator):\n         self,\n         aiterable: AsyncIterable,\n         callable: Callable,\n-        *callable_args,\n-        **callable_kwargs\n+        *callable_args: Any,\n+        **callable_kwargs: Any,\n     ):\n-        self.aiterator = aiterable.__aiter__()\n-        self.callable = callable\n-        self.callable_args = callable_args\n-        self.callable_kwargs = callable_kwargs\n-        self.finished = False\n+        self.aiterator: AsyncIterator = aiterable.__aiter__()\n+        self.callable: Callable = callable\n+        self.callable_args: Tuple[Any, ...] = callable_args\n+        self.callable_kwargs: Dict[str, Any] = callable_kwargs\n+        self.finished: bool = False\n         self.waiting_deferreds: List[Deferred] = []\n         self.anext_deferred: Optional[Deferred] = None\n \n@@ -201,7 +204,11 @@ class _AsyncCooperatorAdapter(Iterator):\n \n \n def parallel_async(\n-    async_iterable: AsyncIterable, count: int, callable: Callable, *args, **named\n+    async_iterable: AsyncIterable,\n+    count: int,\n+    callable: Callable,\n+    *args: Any,\n+    **named: Any,\n ) -> Deferred:\n     \"\"\"Like parallel but for async iterators\"\"\"\n     coop = Cooperator()\n@@ -210,7 +217,9 @@ def parallel_async(\n     return dl\n \n \n-def process_chain(callbacks: Iterable[Callable], input, *a, **kw) -> Deferred:\n+def process_chain(\n+    callbacks: Iterable[Callable], input: Any, *a: Any, **kw: Any\n+) -> Deferred:\n     \"\"\"Return a Deferred built by chaining the given callbacks\"\"\"\n     d: Deferred = Deferred()\n     for x in callbacks:\n@@ -220,7 +229,11 @@ def process_chain(callbacks: Iterable[Callable], input, *a, **kw) -> Deferred:\n \n \n def process_chain_both(\n-    callbacks: Iterable[Callable], errbacks: Iterable[Callable], input, *a, **kw\n+    callbacks: Iterable[Callable],\n+    errbacks: Iterable[Callable],\n+    input: Any,\n+    *a: Any,\n+    **kw: Any,\n ) -> Deferred:\n     \"\"\"Return a Deferred built by chaining the given callbacks and errbacks\"\"\"\n     d: Deferred = Deferred()\n@@ -240,7 +253,9 @@ def process_chain_both(\n     return d\n \n \n-def process_parallel(callbacks: Iterable[Callable], input, *a, **kw) -> Deferred:\n+def process_parallel(\n+    callbacks: Iterable[Callable], input: Any, *a: Any, **kw: Any\n+) -> Deferred:\n     \"\"\"Return a Deferred with the output of all successful calls to the given\n     callbacks\n     \"\"\"\n@@ -250,7 +265,9 @@ def process_parallel(callbacks: Iterable[Callable], input, *a, **kw) -> Deferred\n     return d\n \n \n-def iter_errback(iterable: Iterable, errback: Callable, *a, **kw) -> Generator:\n+def iter_errback(\n+    iterable: Iterable, errback: Callable, *a: Any, **kw: Any\n+) -> Generator:\n     \"\"\"Wraps an iterable calling an errback if an error is caught while\n     iterating it.\n     \"\"\"\n@@ -265,7 +282,7 @@ def iter_errback(iterable: Iterable, errback: Callable, *a, **kw) -> Generator:\n \n \n async def aiter_errback(\n-    aiterable: AsyncIterable, errback: Callable, *a, **kw\n+    aiterable: AsyncIterable, errback: Callable, *a: Any, **kw: Any\n ) -> AsyncGenerator:\n     \"\"\"Wraps an async iterable calling an errback if an error is caught while\n     iterating it. Similar to scrapy.utils.defer.iter_errback()\n@@ -280,7 +297,7 @@ async def aiter_errback(\n             errback(failure.Failure(), *a, **kw)\n \n \n-def deferred_from_coro(o) -> Any:\n+def deferred_from_coro(o: Any) -> Any:\n     \"\"\"Converts a coroutine into a Deferred, or returns the object as is if it isn't a coroutine\"\"\"\n     if isinstance(o, Deferred):\n         return o\n@@ -303,13 +320,13 @@ def deferred_f_from_coro_f(coro_f: Callable[..., Coroutine]) -> Callable:\n     \"\"\"\n \n     @wraps(coro_f)\n-    def f(*coro_args, **coro_kwargs):\n+    def f(*coro_args: Any, **coro_kwargs: Any) -> Any:\n         return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))\n \n     return f\n \n \n-def maybeDeferred_coro(f: Callable, *args, **kw) -> Deferred:\n+def maybeDeferred_coro(f: Callable, *args: Any, **kw: Any) -> Deferred:\n     \"\"\"Copy of defer.maybeDeferred that also converts coroutines to Deferreds.\"\"\"\n     try:\n         result = f(*args, **kw)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c04b9ba19de85154c7bfec536f584f31456e44b3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 103 | Contributors (this commit): 10 | Commits (past 90d): 1 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,10 +4,10 @@ import re\n import string\n from os import PathLike\n from pathlib import Path\n-from typing import Union\n+from typing import Any, Union\n \n \n-def render_templatefile(path: Union[str, PathLike], **kwargs):\n+def render_templatefile(path: Union[str, PathLike], **kwargs: Any) -> None:\n     path_obj = Path(path)\n     raw = path_obj.read_text(\"utf8\")\n \n@@ -24,7 +24,7 @@ def render_templatefile(path: Union[str, PathLike], **kwargs):\n CAMELCASE_INVALID_CHARS = re.compile(r\"[^a-zA-Z\\d]\")\n \n \n-def string_camelcase(string):\n+def string_camelcase(string: str) -> str:\n     \"\"\"Convert a word  to its CamelCase version and remove invalid chars\n \n     >>> string_camelcase('lost-pound')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#36507ddb7b55d6fc4bfdd19286d82057ef3fb5cf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 5 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 310 | Contributors (this commit): 9 | Commits (past 90d): 1 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,9 +2,13 @@\n \n # used in global tests code\n from time import time  # noqa: F401\n+from typing import TYPE_CHECKING, Any, List, Tuple\n+\n+if TYPE_CHECKING:\n+    from scrapy.core.engine import ExecutionEngine\n \n \n-def get_engine_status(engine):\n+def get_engine_status(engine: \"ExecutionEngine\") -> List[Tuple[str, Any]]:\n     \"\"\"Return a report of the current engine status\"\"\"\n     tests = [\n         \"time()-engine.start_time\",\n@@ -23,7 +27,7 @@ def get_engine_status(engine):\n         \"engine.scraper.slot.needs_backout()\",\n     ]\n \n-    checks = []\n+    checks: List[Tuple[str, Any]] = []\n     for test in tests:\n         try:\n             checks += [(test, eval(test))]\n@@ -33,7 +37,7 @@ def get_engine_status(engine):\n     return checks\n \n \n-def format_engine_status(engine=None):\n+def format_engine_status(engine: \"ExecutionEngine\") -> str:\n     checks = get_engine_status(engine)\n     s = \"Execution engine status\\n\\n\"\n     for test, result in checks:\n@@ -43,5 +47,5 @@ def format_engine_status(engine=None):\n     return s\n \n \n-def print_engine_status(engine):\n+def print_engine_status(engine: \"ExecutionEngine\") -> None:\n     print(format_engine_status(engine))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f38cea9c8c5410e0553c666f090fb150a68ae591", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 7 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 112 | Contributors (this commit): 12 | Commits (past 90d): 1 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -6,17 +6,18 @@ import ctypes\n import platform\n import sys\n from pprint import pformat as pformat_\n+from typing import Any\n \n from packaging.version import Version as parse_version\n \n \n-def _enable_windows_terminal_processing():\n+def _enable_windows_terminal_processing() -> bool:\n     # https://stackoverflow.com/a/36760881\n-    kernel32 = ctypes.windll.kernel32\n+    kernel32 = ctypes.windll.kernel32  # type: ignore[attr-defined]\n     return bool(kernel32.SetConsoleMode(kernel32.GetStdHandle(-11), 7))\n \n \n-def _tty_supports_color():\n+def _tty_supports_color() -> bool:\n     if sys.platform != \"win32\":\n         return True\n \n@@ -28,7 +29,7 @@ def _tty_supports_color():\n     return _enable_windows_terminal_processing()\n \n \n-def _colorize(text, colorize=True):\n+def _colorize(text: str, colorize: bool = True) -> str:\n     if not colorize or not sys.stdout.isatty() or not _tty_supports_color():\n         return text\n     try:\n@@ -42,9 +43,9 @@ def _colorize(text, colorize=True):\n         return highlight(text, PythonLexer(), TerminalFormatter())\n \n \n-def pformat(obj, *args, **kwargs):\n+def pformat(obj: Any, *args: Any, **kwargs: Any) -> str:\n     return _colorize(pformat_(obj), kwargs.pop(\"colorize\", True))\n \n \n-def pprint(obj, *args, **kwargs):\n+def pprint(obj: Any, *args: Any, **kwargs: Any) -> None:\n     print(pformat(obj, *args, **kwargs))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#54fa04aa0a47314f121ff5a7627e7e47d4d2c013", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 55 | Lines Deleted: 32 | Files Changed: 4 | Hunks: 33 | Methods Changed: 30 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 87 | Churn Cumulative: 3927 | Contributors (this commit): 47 | Commits (past 90d): 7 | Contributors (cumulative): 85 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,11 +4,13 @@ import logging\n import pprint\n import signal\n import warnings\n-from typing import TYPE_CHECKING, Optional, Type, Union\n+from typing import TYPE_CHECKING, Any, Dict, Optional, Set, Type, Union\n \n from twisted.internet import defer\n from zope.interface.exceptions import DoesNotImplement\n \n+from scrapy.spiderloader import SpiderLoader\n+\n try:\n     # zope >= 5.0 only supports MultipleInvalid\n     from zope.interface.exceptions import MultipleInvalid\n@@ -171,7 +173,7 @@ class CrawlerRunner:\n     )\n \n     @staticmethod\n-    def _get_spider_loader(settings):\n+    def _get_spider_loader(settings) -> SpiderLoader:\n         \"\"\"Get SpiderLoader instance from settings\"\"\"\n         cls_path = settings.get(\"SPIDER_LOADER_CLASS\")\n         loader_cls = load_object(cls_path)\n@@ -190,13 +192,13 @@ class CrawlerRunner:\n             )\n         return loader_cls.from_settings(settings.frozencopy())\n \n-    def __init__(self, settings=None):\n+    def __init__(self, settings: Union[Dict[str, Any], Settings, None] = None):\n         if isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n         self.settings = settings\n         self.spider_loader = self._get_spider_loader(settings)\n-        self._crawlers = set()\n-        self._active = set()\n+        self._crawlers: Set[Crawler] = set()\n+        self._active: Set[defer.Deferred] = set()\n         self.bootstrap_failed = False\n \n     @property\n@@ -252,7 +254,9 @@ class CrawlerRunner:\n \n         return d.addBoth(_done)\n \n-    def create_crawler(self, crawler_or_spidercls):\n+    def create_crawler(\n+        self, crawler_or_spidercls: Union[Type[Spider], str, Crawler]\n+    ) -> Crawler:\n         \"\"\"\n         Return a :class:`~scrapy.crawler.Crawler` object.\n \n@@ -272,7 +276,7 @@ class CrawlerRunner:\n             return crawler_or_spidercls\n         return self._create_crawler(crawler_or_spidercls)\n \n-    def _create_crawler(self, spidercls):\n+    def _create_crawler(self, spidercls: Union[str, Type[Spider]]) -> Crawler:\n         if isinstance(spidercls, str):\n             spidercls = self.spider_loader.load(spidercls)\n         return Crawler(spidercls, self.settings)\n\n@@ -1,10 +1,13 @@\n import traceback\n import warnings\n from collections import defaultdict\n+from typing import DefaultDict, Dict, List, Tuple, Type\n \n from zope.interface import implementer\n \n+from scrapy import Spider\n from scrapy.interfaces import ISpiderLoader\n+from scrapy.settings import BaseSettings\n from scrapy.utils.misc import walk_modules\n from scrapy.utils.spider import iter_spider_classes\n \n@@ -16,11 +19,11 @@ class SpiderLoader:\n     in a Scrapy project.\n     \"\"\"\n \n-    def __init__(self, settings):\n+    def __init__(self, settings: BaseSettings):\n         self.spider_modules = settings.getlist(\"SPIDER_MODULES\")\n         self.warn_only = settings.getbool(\"SPIDER_LOADER_WARN_ONLY\")\n-        self._spiders = {}\n-        self._found = defaultdict(list)\n+        self._spiders: Dict[str, Type[Spider]] = {}\n+        self._found: DefaultDict[str, List[Tuple[str, str]]] = defaultdict(list)\n         self._load_all_spiders()\n \n     def _check_name_duplicates(self):\n@@ -68,7 +71,7 @@ class SpiderLoader:\n     def from_settings(cls, settings):\n         return cls(settings)\n \n-    def load(self, spider_name):\n+    def load(self, spider_name: str) -> Type[Spider]:\n         \"\"\"\n         Return the Spider class for the given spider name. If the spider\n         name is not found, raise a KeyError.\n\n@@ -7,24 +7,30 @@ import os\n from importlib import import_module\n from pathlib import Path\n from posixpath import split\n-from unittest import mock\n+from typing import Any, Coroutine, Dict, List, Optional, Tuple, Type\n+from unittest import TestCase, mock\n \n+from twisted.internet.defer import Deferred\n from twisted.trial.unittest import SkipTest\n \n+from scrapy import Spider\n+from scrapy.crawler import Crawler\n from scrapy.utils.boto import is_botocore_available\n \n \n-def assert_gcs_environ():\n+def assert_gcs_environ() -> None:\n     if \"GCS_PROJECT_ID\" not in os.environ:\n         raise SkipTest(\"GCS_PROJECT_ID not found\")\n \n \n-def skip_if_no_boto():\n+def skip_if_no_boto() -> None:\n     if not is_botocore_available():\n         raise SkipTest(\"missing botocore library\")\n \n \n-def get_gcs_content_and_delete(bucket, path):\n+def get_gcs_content_and_delete(\n+    bucket: Any, path: str\n+) -> Tuple[bytes, List[Dict[str, str]], Any]:\n     from google.cloud import storage\n \n     client = storage.Client(project=os.environ.get(\"GCS_PROJECT_ID\"))\n@@ -37,8 +43,13 @@ def get_gcs_content_and_delete(bucket, path):\n \n \n def get_ftp_content_and_delete(\n-    path, host, port, username, password, use_active_mode=False\n-):\n+    path: str,\n+    host: str,\n+    port: int,\n+    username: str,\n+    password: str,\n+    use_active_mode: bool = False,\n+) -> bytes:\n     from ftplib import FTP\n \n     ftp = FTP()\n@@ -46,19 +57,23 @@ def get_ftp_content_and_delete(\n     ftp.login(username, password)\n     if use_active_mode:\n         ftp.set_pasv(False)\n-    ftp_data = []\n+    ftp_data: List[bytes] = []\n \n-    def buffer_data(data):\n+    def buffer_data(data: bytes) -> None:\n         ftp_data.append(data)\n \n     ftp.retrbinary(f\"RETR {path}\", buffer_data)\n     dirname, filename = split(path)\n     ftp.cwd(dirname)\n     ftp.delete(filename)\n-    return \"\".join(ftp_data)\n+    return b\"\".join(ftp_data)\n \n \n-def get_crawler(spidercls=None, settings_dict=None, prevent_warnings=True):\n+def get_crawler(\n+    spidercls: Optional[Type[Spider]] = None,\n+    settings_dict: Optional[Dict[str, Any]] = None,\n+    prevent_warnings: bool = True,\n+) -> Crawler:\n     \"\"\"Return an unconfigured Crawler object. If settings_dict is given, it\n     will be used to populate the crawler settings with a project level\n     priority.\n@@ -82,7 +97,7 @@ def get_pythonpath() -> str:\n     return str(Path(scrapy_path).parent) + os.pathsep + os.environ.get(\"PYTHONPATH\", \"\")\n \n \n-def get_testenv():\n+def get_testenv() -> Dict[str, str]:\n     \"\"\"Return a OS environment dict suitable to fork processes that need to import\n     this installation of Scrapy, instead of a system installed one.\n     \"\"\"\n@@ -91,21 +106,23 @@ def get_testenv():\n     return env\n \n \n-def assert_samelines(testcase, text1, text2, msg=None):\n+def assert_samelines(\n+    testcase: TestCase, text1: str, text2: str, msg: Optional[str] = None\n+) -> None:\n     \"\"\"Asserts text1 and text2 have the same lines, ignoring differences in\n     line endings between platforms\n     \"\"\"\n     testcase.assertEqual(text1.splitlines(), text2.splitlines(), msg)\n \n \n-def get_from_asyncio_queue(value):\n-    q = asyncio.Queue()\n+def get_from_asyncio_queue(value: Any) -> Coroutine:\n+    q: asyncio.Queue = asyncio.Queue()\n     getter = q.get()\n     q.put_nowait(value)\n     return getter\n \n \n-def mock_google_cloud_storage():\n+def mock_google_cloud_storage() -> Tuple[Any, Any, Any]:\n     \"\"\"Creates autospec mocks for google-cloud-storage Client, Bucket and Blob\n     classes and set their proper return values.\n     \"\"\"\n@@ -122,7 +139,7 @@ def mock_google_cloud_storage():\n     return (client_mock, bucket_mock, blob_mock)\n \n \n-def get_web_client_agent_req(url):\n+def get_web_client_agent_req(url: str) -> Deferred:\n     from twisted.internet import reactor\n     from twisted.web.client import Agent  # imports twisted.internet.reactor\n \n\n@@ -32,6 +32,7 @@ from scrapy.utils.test import (\n     get_gcs_content_and_delete,\n     skip_if_no_boto,\n )\n+from tests.mockserver import MockFTPServer\n \n from .test_pipeline_media import _mocked_download_func\n \n@@ -639,14 +640,12 @@ class TestGCSFilesStore(unittest.TestCase):\n class TestFTPFileStore(unittest.TestCase):\n     @defer.inlineCallbacks\n     def test_persist(self):\n-        uri = os.environ.get(\"FTP_TEST_FILE_URI\")\n-        if not uri:\n-            raise unittest.SkipTest(\"No FTP URI available for testing\")\n         data = b\"TestFTPFilesStore: \\xe2\\x98\\x83\"\n         buf = BytesIO(data)\n         meta = {\"foo\": \"bar\"}\n         path = \"full/filename\"\n-        store = FTPFilesStore(uri)\n+        with MockFTPServer() as ftp_server:\n+            store = FTPFilesStore(ftp_server.url(\"/\"))\n             empty_dict = yield store.stat_file(path, info=None)\n             self.assertEqual(empty_dict, {})\n             yield store.persist_file(path, buf, info=None, meta=meta, headers=None)\n@@ -663,7 +662,7 @@ class TestFTPFileStore(unittest.TestCase):\n                 store.password,\n                 store.USE_ACTIVE_MODE,\n             )\n-        self.assertEqual(data.decode(), content)\n+        self.assertEqual(data, content)\n \n \n class ItemWithFiles(Item):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0ec79e316619c1c98b0a1dd4fb0edea6e6de803d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 535 | Contributors (this commit): 17 | Commits (past 90d): 2 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -2,7 +2,7 @@\n \n import inspect\n import warnings\n-from typing import Any, List, Optional, Tuple, Type, overload\n+from typing import Any, Dict, List, Optional, Tuple, Type, overload\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n \n@@ -20,7 +20,7 @@ def attribute(obj: Any, oldattr: str, newattr: str, version: str = \"0.12\") -> No\n def create_deprecated_class(\n     name: str,\n     new_class: type,\n-    clsdict: Optional[dict[str, Any]] = None,\n+    clsdict: Optional[Dict[str, Any]] = None,\n     warn_category: Type[Warning] = ScrapyDeprecationWarning,\n     warn_once: bool = True,\n     old_class_path: Optional[str] = None,\n@@ -59,14 +59,14 @@ def create_deprecated_class(\n         warned_on_subclass: bool = False\n \n         def __new__(\n-            metacls, name: str, bases: Tuple[type, ...], clsdict_: dict[str, Any]\n+            metacls, name: str, bases: Tuple[type, ...], clsdict_: Dict[str, Any]\n         ) -> type:\n             cls = super().__new__(metacls, name, bases, clsdict_)\n             if metacls.deprecated_class is None:\n                 metacls.deprecated_class = cls\n             return cls\n \n-        def __init__(cls, name: str, bases: Tuple[type, ...], clsdict_: dict[str, Any]):\n+        def __init__(cls, name: str, bases: Tuple[type, ...], clsdict_: Dict[str, Any]):\n             meta = cls.__class__\n             old = meta.deprecated_class\n             if old in bases and not (warn_once and meta.warned_on_subclass):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e03c6bb70a8915f997771472ef05efc0c3ad6bd6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 2280 | Contributors (this commit): 37 | Commits (past 90d): 5 | Contributors (cumulative): 46 | DMM Complexity: 0.0\n\nDIFF:\n@@ -9,8 +9,6 @@ from typing import TYPE_CHECKING, Any, Dict, Optional, Set, Type, Union\n from twisted.internet import defer\n from zope.interface.exceptions import DoesNotImplement\n \n-from scrapy.spiderloader import SpiderLoader\n-\n try:\n     # zope >= 5.0 only supports MultipleInvalid\n     from zope.interface.exceptions import MultipleInvalid\n@@ -27,6 +25,7 @@ from scrapy.interfaces import ISpiderLoader\n from scrapy.logformatter import LogFormatter\n from scrapy.settings import Settings, overridden_settings\n from scrapy.signalmanager import SignalManager\n+from scrapy.spiderloader import SpiderLoader\n from scrapy.statscollectors import StatsCollector\n from scrapy.utils.log import (\n     LogCounterHandler,\n\n@@ -24,7 +24,6 @@ def gunzip(data: bytes) -> bytes:\n             # some pages are quite small so output_list is empty\n             if output_list:\n                 break\n-            else:\n             raise\n     return b\"\".join(output_list)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#33153855ea04d95545970970c3be08226b679f10", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 112 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 10 | Complexity Δ (Sum/Max): 26/26 | Churn Δ: 112 | Churn Cumulative: 112 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.8690476190476191\n\nDIFF:\n@@ -0,0 +1,112 @@\n+import logging\n+from datetime import datetime\n+\n+from twisted.internet import task\n+\n+from scrapy import signals\n+from scrapy.exceptions import NotConfigured\n+from scrapy.utils.serialize import ScrapyJSONEncoder\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class LogStatsExtended:\n+    \"\"\"Log basic scraping stats periodically\"\"\"\n+\n+    def __init__(\n+            self, stats, interval=60.0, extended=False, ext_include=None, ext_exclude=None\n+    ):\n+        self.stats = stats\n+        self.interval = interval\n+        self.multiplier = 60.0 / self.interval\n+        self.task = None\n+        self.extended = extended\n+        self.encoder = ScrapyJSONEncoder(sort_keys=True, indent=4)\n+        self.ext_include = ext_include\n+        self.ext_exclude = ext_exclude\n+\n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        interval = crawler.settings.getfloat(\"LOGSTATS_INTERVAL\")\n+        extended = crawler.settings.getbool(\"LOGSTATS_EXTENDED_ENABLED\")\n+        ext_include = crawler.settings.getlist(\"LOGSTATS_EXTENDED_INCLUDE\", [])\n+        ext_exclude = crawler.settings.getlist(\"LOGSTATS_EXTENDED_EXCLUDE\", [])\n+        if not interval:\n+            raise NotConfigured\n+        o = cls(crawler.stats, interval, extended, ext_include, ext_exclude)\n+        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n+        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n+        return o\n+\n+    def spider_opened(self, spider):\n+        self.time_prev = datetime.utcnow()\n+        self.delta_prev = {}\n+        self.stats_prev = {}\n+\n+        self.task = task.LoopingCall(self.log, spider)\n+        self.task.start(self.interval)\n+\n+    def log(self, spider):\n+        data = {}\n+        data.update(self.log_timing())\n+        data.update(self.log_delta())\n+        data.update(self.log_crawler_stats())\n+        logger.info(self.encoder.encode(data))\n+\n+\n+    def log_delta(self):\n+        num_stats = {\n+            k: v\n+            for k, v in self.stats._stats.items()\n+            if isinstance(v, (int, float)) and self.delta_param_allowed(k)\n+        }\n+        delta = {k: v - self.delta_prev.get(k, 0) for k, v in num_stats.items()}\n+        self.delta_prev = num_stats\n+        return {\"delta\": delta}\n+\n+    def log_timing(self):\n+        now = datetime.utcnow()\n+        time = {\n+            \"log_interval\": self.interval,\n+            \"start_time\": self.stats._stats[\"start_time\"],\n+            \"utcnow\": now,\n+            \"log_interval_real\": (now - self.time_prev).total_seconds(),\n+            \"elapsed\": (now - self.stats._stats[\"start_time\"]).total_seconds(),\n+        }\n+        self.time_prev = now\n+        return {\"time\": time}\n+\n+    def log_time(self):\n+        num_stats = {\n+            k: v\n+            for k, v in self.stats._stats.items()\n+            if isinstance(v, (int, float)) and self.delta_param_allowed(k)\n+        }\n+        delta = {k: v - self.stats_prev.get(k, 0) for k, v in num_stats.items()}\n+        self.stats_prev = num_stats\n+        return {\"delta\": delta}\n+\n+    def log_crawler_stats(self):\n+        return {\"stats\": self.stats.get_stats()}\n+\n+    def delta_param_allowed(self, stat_name):\n+        for p in self.ext_exclude:\n+            if p in stat_name:\n+                return False\n+        for p in self.ext_include:\n+            if p in stat_name:\n+                return True\n+        if self.ext_include:\n+            return False\n+        else:\n+            return True\n+\n+    def spider_closed(self, spider, reason):\n+        data = {}\n+        data.update(self.log_timing())\n+        data.update(self.log_delta())\n+        data.update(self.log_crawler_stats())\n+        logger.info(self.encoder.encode(data))\n+\n+        if self.task and self.task.running:\n+            self.task.stop()\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
