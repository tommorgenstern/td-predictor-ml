{"custom_id": "scrapy#fcc6becc586e6f895cdeed66a579d5735d215de1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 47 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 8 | Methods Changed: 8 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 51 | Churn Cumulative: 4700 | Contributors (this commit): 34 | Commits (past 90d): 5 | Contributors (cumulative): 57 | DMM Complexity: 0.95\n\nDIFF:\n@@ -150,7 +150,7 @@ class FileFeedStorage:\n \n class S3FeedStorage(BlockingFeedStorage):\n \n-    def __init__(self, uri, access_key=None, secret_key=None, acl=None, *,\n+    def __init__(self, uri, access_key=None, secret_key=None, acl=None, endpoint_url=None, *,\n                  feed_options=None):\n         if not is_botocore_available():\n             raise NotConfigured('missing botocore library')\n@@ -160,11 +160,13 @@ class S3FeedStorage(BlockingFeedStorage):\n         self.secret_key = u.password or secret_key\n         self.keyname = u.path[1:]  # remove first \"/\"\n         self.acl = acl\n+        self.endpoint_url = endpoint_url\n         import botocore.session\n         session = botocore.session.get_session()\n         self.s3_client = session.create_client(\n             's3', aws_access_key_id=self.access_key,\n-            aws_secret_access_key=self.secret_key)\n+            aws_secret_access_key=self.secret_key,\n+            endpoint_url=self.endpoint_url)\n         if feed_options and feed_options.get('overwrite', True) is False:\n             logger.warning('S3 does not support appending to files. To '\n                            'suppress this warning, remove the overwrite '\n@@ -178,6 +180,7 @@ class S3FeedStorage(BlockingFeedStorage):\n             access_key=crawler.settings['AWS_ACCESS_KEY_ID'],\n             secret_key=crawler.settings['AWS_SECRET_ACCESS_KEY'],\n             acl=crawler.settings['FEED_STORAGE_S3_ACL'] or None,\n+            endpoint_url=crawler.settings['AWS_ENDPOINT_URL'] or None,\n             feed_options=feed_options,\n         )\n \n\n@@ -326,6 +326,17 @@ class S3FeedStorageTest(unittest.TestCase):\n         self.assertEqual(storage.secret_key, 'secret_key')\n         self.assertEqual(storage.acl, 'custom-acl')\n \n+    def test_init_with_endpoint_url(self):\n+        storage = S3FeedStorage(\n+            's3://mybucket/export.csv',\n+            'access_key',\n+            'secret_key',\n+            endpoint_url='https://example.com'\n+        )\n+        self.assertEqual(storage.access_key, 'access_key')\n+        self.assertEqual(storage.secret_key, 'secret_key')\n+        self.assertEqual(storage.endpoint_url, 'https://example.com')\n+\n     def test_from_crawler_without_acl(self):\n         settings = {\n             'AWS_ACCESS_KEY_ID': 'access_key',\n@@ -340,6 +351,20 @@ class S3FeedStorageTest(unittest.TestCase):\n         self.assertEqual(storage.secret_key, 'secret_key')\n         self.assertEqual(storage.acl, None)\n \n+    def test_without_endpoint_url(self):\n+        settings = {\n+            'AWS_ACCESS_KEY_ID': 'access_key',\n+            'AWS_SECRET_ACCESS_KEY': 'secret_key',\n+        }\n+        crawler = get_crawler(settings_dict=settings)\n+        storage = S3FeedStorage.from_crawler(\n+            crawler,\n+            's3://mybucket/export.csv',\n+        )\n+        self.assertEqual(storage.access_key, 'access_key')\n+        self.assertEqual(storage.secret_key, 'secret_key')\n+        self.assertEqual(storage.endpoint_url, None)\n+\n     def test_from_crawler_with_acl(self):\n         settings = {\n             'AWS_ACCESS_KEY_ID': 'access_key',\n@@ -355,6 +380,21 @@ class S3FeedStorageTest(unittest.TestCase):\n         self.assertEqual(storage.secret_key, 'secret_key')\n         self.assertEqual(storage.acl, 'custom-acl')\n \n+    def test_from_crawler_with_endpoint_url(self):\n+        settings = {\n+            'AWS_ACCESS_KEY_ID': 'access_key',\n+            'AWS_SECRET_ACCESS_KEY': 'secret_key',\n+            'AWS_ENDPOINT_URL': 'https://example.com',\n+        }\n+        crawler = get_crawler(settings_dict=settings)\n+        storage = S3FeedStorage.from_crawler(\n+            crawler,\n+            's3://mybucket/export.csv'\n+        )\n+        self.assertEqual(storage.access_key, 'access_key')\n+        self.assertEqual(storage.secret_key, 'secret_key')\n+        self.assertEqual(storage.endpoint_url, 'https://example.com')\n+\n     @defer.inlineCallbacks\n     def test_store_botocore_without_acl(self):\n         skip_if_no_boto()\n@@ -1917,8 +1957,8 @@ class FileFeedStoragePreFeedOptionsTest(unittest.TestCase):\n \n class S3FeedStorageWithoutFeedOptions(S3FeedStorage):\n \n-    def __init__(self, uri, access_key, secret_key, acl):\n-        super().__init__(uri, access_key, secret_key, acl)\n+    def __init__(self, uri, access_key, secret_key, acl, endpoint_url):\n+        super().__init__(uri, access_key, secret_key, acl, endpoint_url)\n \n \n class S3FeedStorageWithoutFeedOptionsWithFromCrawler(S3FeedStorage):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bcce0660573cf9309e3decf853488da2ef8bc576", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 23 | Churn Cumulative: 1156 | Contributors (this commit): 29 | Commits (past 90d): 3 | Contributors (cumulative): 29 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,6 +11,7 @@ import sys\n import warnings\n from datetime import datetime\n from tempfile import NamedTemporaryFile\n+from typing import Any, Optional, Tuple\n from urllib.parse import unquote, urlparse\n \n from twisted.internet import defer, threads\n@@ -54,16 +55,19 @@ class ItemFilter:\n     :param feed_options: feed specific options passed from FeedExporter\n     :type feed_options: dict\n     \"\"\"\n+    feed_options: Optional[dict]\n+    item_classes: Tuple\n \n-    def __init__(self, feed_options):\n+    def __init__(self, feed_options: Optional[dict]) -> None:\n         self.feed_options = feed_options\n-        self.item_classes = set()\n+        if feed_options is not None:\n+            self.item_classes = tuple(\n+                load_object(item_class) for item_class in feed_options.get(\"item_classes\") or ()\n+            )\n+        else:\n+            self.item_classes = tuple()\n \n-        if 'item_classes' in self.feed_options:\n-            for item_class in self.feed_options['item_classes']:\n-                self.item_classes.add(load_object(item_class))\n-\n-    def accepts(self, item):\n+    def accepts(self, item: Any) -> bool:\n         \"\"\"\n         Return ``True`` if `item` should be exported or ``False`` otherwise.\n \n@@ -73,9 +77,8 @@ class ItemFilter:\n         :rtype: bool\n         \"\"\"\n         if self.item_classes:\n-            return isinstance(item, tuple(self.item_classes))\n-\n-        return True    # accept all items if none declared in item_classes\n+            return isinstance(item, self.item_classes)\n+        return True  # accept all items by default\n \n \n class IFeedStorage(Interface):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7306a81188f81964ad85f4936ce29e3aa0084447", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1417 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -19,7 +19,7 @@ class SpiderMiddlewareTestCase(TestCase):\n     def setUp(self):\n         self.request = Request('http://example.com/index.html')\n         self.response = Response(self.request.url, request=self.request)\n-        self.crawler = get_crawler(Spider)\n+        self.crawler = get_crawler(Spider, {'SPIDER_MIDDLEWARES_BASE': {}})\n         self.spider = self.crawler._create_spider('foo')\n         self.mwman = SpiderMiddlewareManager.from_crawler(self.crawler)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#70dddfe2b293171db4c58511175edf55bbe1831c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 56 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,6 @@\n import asyncio\n import sys\n+from typing import Optional\n \n from scrapy import Spider\n from scrapy.crawler import CrawlerProcess\n@@ -31,6 +32,7 @@ class UrlSpider(Spider):\n \n \n if __name__ == \"__main__\":\n+    ASYNCIO_EVENT_LOOP: Optional[str]\n     try:\n         ASYNCIO_EVENT_LOOP = sys.argv[1]\n     except IndexError:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#abe0b37d307d40897ca6d7e61aa5c137c8e6a4c1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 14 | Churn Cumulative: 344 | Contributors (this commit): 14 | Commits (past 90d): 1 | Contributors (cumulative): 14 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,5 +1,3 @@\n-from urllib.parse import unquote\n-\n from scrapy.core.downloader.handlers.http import HTTPDownloadHandler\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.boto import is_botocore_available\n@@ -59,7 +57,7 @@ class S3DownloadHandler:\n         url = f'{scheme}://{bucket}.s3.amazonaws.com{path}'\n         if self.anon:\n             request = request.replace(url=url)\n-        elif self._signer is not None:\n+        else:\n             import botocore.awsrequest\n             awsrequest = botocore.awsrequest.AWSRequest(\n                 method=request.method,\n@@ -69,14 +67,4 @@ class S3DownloadHandler:\n             self._signer.add_auth(awsrequest)\n             request = request.replace(\n                 url=url, headers=awsrequest.headers.items())\n-        else:\n-            signed_headers = self.conn.make_request(\n-                method=request.method,\n-                bucket=bucket,\n-                key=unquote(p.path),\n-                query_args=unquote(p.query),\n-                headers=request.headers,\n-                data=request.body,\n-            )\n-            request = request.replace(url=url, headers=signed_headers)\n         return self._download_http(request, spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7e4321f201a795166d779f2aa0b36d38cb50106e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 6 | Files Changed: 5 | Hunks: 16 | Methods Changed: 11 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 26 | Churn Cumulative: 6318 | Contributors (this commit): 58 | Commits (past 90d): 13 | Contributors (cumulative): 118 | DMM Complexity: 0.6153846153846154\n\nDIFF:\n@@ -10,6 +10,7 @@ class S3DownloadHandler:\n     def __init__(self, settings, *,\n                  crawler=None,\n                  aws_access_key_id=None, aws_secret_access_key=None,\n+                 aws_session_token=None,\n                  httpdownloadhandler=HTTPDownloadHandler, **kw):\n         if not is_botocore_available():\n             raise NotConfigured('missing botocore library')\n@@ -18,6 +19,8 @@ class S3DownloadHandler:\n             aws_access_key_id = settings['AWS_ACCESS_KEY_ID']\n         if not aws_secret_access_key:\n             aws_secret_access_key = settings['AWS_SECRET_ACCESS_KEY']\n+        if not aws_session_token:\n+            aws_session_token = settings['AWS_SESSION_TOKEN']\n \n         # If no credentials could be found anywhere,\n         # consider this an anonymous connection request by default;\n@@ -36,7 +39,7 @@ class S3DownloadHandler:\n         if not self.anon:\n             SignerCls = botocore.auth.AUTH_TYPE_MAPS['s3']\n             self._signer = SignerCls(botocore.credentials.Credentials(\n-                aws_access_key_id, aws_secret_access_key))\n+                aws_access_key_id, aws_secret_access_key, aws_session_token))\n \n         _http_handler = create_instance(\n             objcls=httpdownloadhandler,\n\n@@ -154,13 +154,14 @@ class FileFeedStorage:\n class S3FeedStorage(BlockingFeedStorage):\n \n     def __init__(self, uri, access_key=None, secret_key=None, acl=None, endpoint_url=None, *,\n-                 feed_options=None):\n+                 feed_options=None, session_token=None):\n         if not is_botocore_available():\n             raise NotConfigured('missing botocore library')\n         u = urlparse(uri)\n         self.bucketname = u.hostname\n         self.access_key = u.username or access_key\n         self.secret_key = u.password or secret_key\n+        self.session_token = session_token\n         self.keyname = u.path[1:]  # remove first \"/\"\n         self.acl = acl\n         self.endpoint_url = endpoint_url\n@@ -169,6 +170,7 @@ class S3FeedStorage(BlockingFeedStorage):\n         self.s3_client = session.create_client(\n             's3', aws_access_key_id=self.access_key,\n             aws_secret_access_key=self.secret_key,\n+            aws_session_token=self.session_token,\n             endpoint_url=self.endpoint_url)\n         if feed_options and feed_options.get('overwrite', True) is False:\n             logger.warning('S3 does not support appending to files. To '\n@@ -182,6 +184,7 @@ class S3FeedStorage(BlockingFeedStorage):\n             uri,\n             access_key=crawler.settings['AWS_ACCESS_KEY_ID'],\n             secret_key=crawler.settings['AWS_SECRET_ACCESS_KEY'],\n+            session_token=crawler.settings['AWS_SESSION_TOKEN'],\n             acl=crawler.settings['FEED_STORAGE_S3_ACL'] or None,\n             endpoint_url=crawler.settings['AWS_ENDPOINT_URL'] or None,\n             feed_options=feed_options,\n\n@@ -79,6 +79,7 @@ class FSFilesStore:\n class S3FilesStore:\n     AWS_ACCESS_KEY_ID = None\n     AWS_SECRET_ACCESS_KEY = None\n+    AWS_SESSION_TOKEN = None\n     AWS_ENDPOINT_URL = None\n     AWS_REGION_NAME = None\n     AWS_USE_SSL = None\n@@ -98,6 +99,7 @@ class S3FilesStore:\n             's3',\n             aws_access_key_id=self.AWS_ACCESS_KEY_ID,\n             aws_secret_access_key=self.AWS_SECRET_ACCESS_KEY,\n+            aws_session_token=self.AWS_SESSION_TOKEN,\n             endpoint_url=self.AWS_ENDPOINT_URL,\n             region_name=self.AWS_REGION_NAME,\n             use_ssl=self.AWS_USE_SSL,\n@@ -349,6 +351,7 @@ class FilesPipeline(MediaPipeline):\n         s3store = cls.STORE_SCHEMES['s3']\n         s3store.AWS_ACCESS_KEY_ID = settings['AWS_ACCESS_KEY_ID']\n         s3store.AWS_SECRET_ACCESS_KEY = settings['AWS_SECRET_ACCESS_KEY']\n+        s3store.AWS_SESSION_TOKEN = settings['AWS_SESSION_TOKEN']\n         s3store.AWS_ENDPOINT_URL = settings['AWS_ENDPOINT_URL']\n         s3store.AWS_REGION_NAME = settings['AWS_REGION_NAME']\n         s3store.AWS_USE_SSL = settings['AWS_USE_SSL']\n\n@@ -92,6 +92,7 @@ class ImagesPipeline(FilesPipeline):\n         s3store = cls.STORE_SCHEMES['s3']\n         s3store.AWS_ACCESS_KEY_ID = settings['AWS_ACCESS_KEY_ID']\n         s3store.AWS_SECRET_ACCESS_KEY = settings['AWS_SECRET_ACCESS_KEY']\n+        s3store.AWS_SESSION_TOKEN = settings['AWS_SESSION_TOKEN']\n         s3store.AWS_ENDPOINT_URL = settings['AWS_ENDPOINT_URL']\n         s3store.AWS_REGION_NAME = settings['AWS_REGION_NAME']\n         s3store.AWS_USE_SSL = settings['AWS_USE_SSL']\n\n@@ -244,7 +244,8 @@ class S3FeedStorageTest(unittest.TestCase):\n     def test_parse_credentials(self):\n         skip_if_no_boto()\n         aws_credentials = {'AWS_ACCESS_KEY_ID': 'settings_key',\n-                           'AWS_SECRET_ACCESS_KEY': 'settings_secret'}\n+                           'AWS_SECRET_ACCESS_KEY': 'settings_secret',\n+                           'AWS_SESSION_TOKEN': 'settings_token'}\n         crawler = get_crawler(settings_dict=aws_credentials)\n         # Instantiate with crawler\n         storage = S3FeedStorage.from_crawler(\n@@ -253,12 +254,15 @@ class S3FeedStorageTest(unittest.TestCase):\n         )\n         self.assertEqual(storage.access_key, 'settings_key')\n         self.assertEqual(storage.secret_key, 'settings_secret')\n+        self.assertEqual(storage.session_token, 'settings_token')\n         # Instantiate directly\n         storage = S3FeedStorage('s3://mybucket/export.csv',\n                                 aws_credentials['AWS_ACCESS_KEY_ID'],\n-                                aws_credentials['AWS_SECRET_ACCESS_KEY'])\n+                                aws_credentials['AWS_SECRET_ACCESS_KEY'],\n+                                session_token=aws_credentials['AWS_SESSION_TOKEN'])\n         self.assertEqual(storage.access_key, 'settings_key')\n         self.assertEqual(storage.secret_key, 'settings_secret')\n+        self.assertEqual(storage.session_token, 'settings_token')\n         # URI priority > settings priority\n         storage = S3FeedStorage('s3://uri_key:uri_secret@mybucket/export.csv',\n                                 aws_credentials['AWS_ACCESS_KEY_ID'],\n@@ -1957,8 +1961,8 @@ class FileFeedStoragePreFeedOptionsTest(unittest.TestCase):\n \n class S3FeedStorageWithoutFeedOptions(S3FeedStorage):\n \n-    def __init__(self, uri, access_key, secret_key, acl, endpoint_url):\n-        super().__init__(uri, access_key, secret_key, acl, endpoint_url)\n+    def __init__(self, uri, access_key, secret_key, acl, endpoint_url, **kwargs):\n+        super().__init__(uri, access_key, secret_key, acl, endpoint_url, **kwargs)\n \n \n class S3FeedStorageWithoutFeedOptionsWithFromCrawler(S3FeedStorage):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cc89f6be381d72a2528c8a672158671305019324", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 78 | Lines Deleted: 12 | Files Changed: 3 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 2/3 | Churn Δ: 90 | Churn Cumulative: 2265 | Contributors (this commit): 23 | Commits (past 90d): 5 | Contributors (cumulative): 51 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,7 +4,7 @@ responses in Scrapy.\n \n See documentation in docs/topics/request-response.rst\n \"\"\"\n-from typing import Generator\n+from typing import Generator, Tuple\n from urllib.parse import urljoin\n \n from scrapy.exceptions import NotSupported\n@@ -16,6 +16,19 @@ from scrapy.utils.trackref import object_ref\n \n \n class Response(object_ref):\n+    \"\"\"An object that represents an HTTP response, which is usually\n+    downloaded (by the Downloader) and fed to the Spiders for processing.\n+    \"\"\"\n+\n+    attributes: Tuple[str, ...] = (\n+        \"url\", \"status\", \"headers\", \"body\", \"flags\", \"request\", \"certificate\", \"ip_address\", \"protocol\",\n+    )\n+    \"\"\"A tuple of :class:`str` objects containing the name of all public\n+    attributes of the class that are also keyword parameters of the\n+    ``__init__`` method.\n+\n+    Currently used by :meth:`Response.replace`.\n+    \"\"\"\n \n     def __init__(\n         self,\n@@ -97,12 +110,8 @@ class Response(object_ref):\n         return self.replace()\n \n     def replace(self, *args, **kwargs):\n-        \"\"\"Create a new Response with the same attributes except for those\n-        given new values.\n-        \"\"\"\n-        for x in [\n-            \"url\", \"status\", \"headers\", \"body\", \"request\", \"flags\", \"certificate\", \"ip_address\", \"protocol\",\n-        ]:\n+        \"\"\"Create a new Response with the same attributes except for those given new values\"\"\"\n+        for x in self.attributes:\n             kwargs.setdefault(x, getattr(self, x))\n         cls = kwargs.pop('cls', self.__class__)\n         return cls(*args, **kwargs)\n\n@@ -8,7 +8,7 @@ See documentation in docs/topics/request-response.rst\n import json\n import warnings\n from contextlib import suppress\n-from typing import Generator\n+from typing import Generator, Tuple\n from urllib.parse import urljoin\n \n import parsel\n@@ -30,6 +30,8 @@ class TextResponse(Response):\n     _DEFAULT_ENCODING = 'ascii'\n     _cached_decoded_json = _NONE\n \n+    attributes: Tuple[str, ...] = Response.attributes + (\"encoding\",)\n+\n     def __init__(self, *args, **kwargs):\n         self._encoding = kwargs.pop('encoding', None)\n         self._cached_benc = None\n@@ -53,10 +55,6 @@ class TextResponse(Response):\n         else:\n             super()._set_body(body)\n \n-    def replace(self, *args, **kwargs):\n-        kwargs.setdefault('encoding', self.encoding)\n-        return Response.replace(self, *args, **kwargs)\n-\n     @property\n     def encoding(self):\n         return self._declared_encoding() or self._body_inferred_encoding()\n\n@@ -820,3 +820,62 @@ class XmlResponseTest(TextResponseTest):\n             response.xpath(\"//s1:elem/text()\", namespaces={'s1': 'http://scrapy.org'}).getall(),\n             response.selector.xpath(\"//s2:elem/text()\").getall(),\n         )\n+\n+\n+class CustomResponse(TextResponse):\n+    attributes = TextResponse.attributes + (\"foo\", \"bar\")\n+\n+    def __init__(self, *args, **kwargs) -> None:\n+        self.foo = kwargs.pop(\"foo\", None)\n+        self.bar = kwargs.pop(\"bar\", None)\n+        self.lost = kwargs.pop(\"lost\", None)\n+        super().__init__(*args, **kwargs)\n+\n+\n+class CustomResponseTest(TextResponseTest):\n+    response_class = CustomResponse\n+\n+    def test_copy(self):\n+        super().test_copy()\n+        r1 = self.response_class(url=\"https://example.org\", status=200, foo=\"foo\", bar=\"bar\", lost=\"lost\")\n+        r2 = r1.copy()\n+        self.assertIsInstance(r2, self.response_class)\n+        self.assertEqual(r1.foo, r2.foo)\n+        self.assertEqual(r1.bar, r2.bar)\n+        self.assertEqual(r1.lost, \"lost\")\n+        self.assertIsNone(r2.lost)\n+\n+    def test_replace(self):\n+        super().test_replace()\n+        r1 = self.response_class(url=\"https://example.org\", status=200, foo=\"foo\", bar=\"bar\", lost=\"lost\")\n+\n+        r2 = r1.replace(foo=\"new-foo\", bar=\"new-bar\", lost=\"new-lost\")\n+        self.assertIsInstance(r2, self.response_class)\n+        self.assertEqual(r1.foo, \"foo\")\n+        self.assertEqual(r1.bar, \"bar\")\n+        self.assertEqual(r1.lost, \"lost\")\n+        self.assertEqual(r2.foo, \"new-foo\")\n+        self.assertEqual(r2.bar, \"new-bar\")\n+        self.assertEqual(r2.lost, \"new-lost\")\n+\n+        r3 = r1.replace(foo=\"new-foo\", bar=\"new-bar\")\n+        self.assertIsInstance(r3, self.response_class)\n+        self.assertEqual(r1.foo, \"foo\")\n+        self.assertEqual(r1.bar, \"bar\")\n+        self.assertEqual(r1.lost, \"lost\")\n+        self.assertEqual(r3.foo, \"new-foo\")\n+        self.assertEqual(r3.bar, \"new-bar\")\n+        self.assertIsNone(r3.lost)\n+\n+        r4 = r1.replace(foo=\"new-foo\")\n+        self.assertIsInstance(r4, self.response_class)\n+        self.assertEqual(r1.foo, \"foo\")\n+        self.assertEqual(r1.bar, \"bar\")\n+        self.assertEqual(r1.lost, \"lost\")\n+        self.assertEqual(r4.foo, \"new-foo\")\n+        self.assertEqual(r4.bar, \"bar\")\n+        self.assertIsNone(r4.lost)\n+\n+        with self.assertRaises(TypeError) as ctx:\n+            r1.replace(unknown=\"unknown\")\n+        self.assertEqual(str(ctx.exception), \"__init__() got an unexpected keyword argument 'unknown'\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ef6fb933b568c497ab3745284bc2a02725bced1c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 190 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,5 @@\n \"\"\"Helper functions for working with signals\"\"\"\n-import collections\n+import collections.abc\n import logging\n \n from twisted.internet.defer import DeferredList, Deferred\n@@ -21,7 +21,7 @@ def send_catch_log(signal=Any, sender=Anonymous, *arguments, **named):\n     Failures instead of exceptions.\n     \"\"\"\n     dont_log = named.pop('dont_log', ())\n-    dont_log = tuple(dont_log) if isinstance(dont_log, collections.Sequence) else (dont_log,)\n+    dont_log = tuple(dont_log) if isinstance(dont_log, collections.abc.Sequence) else (dont_log,)\n     dont_log += (StopDownload, )\n     spider = named.get('spider', None)\n     responses = []\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#93bf1ae7e3966d56539411c59d172c2971ee61e0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 1279 | Contributors (this commit): 20 | Commits (past 90d): 4 | Contributors (cumulative): 23 | DMM Complexity: 1.0\n\nDIFF:\n@@ -878,4 +878,4 @@ class CustomResponseTest(TextResponseTest):\n \n         with self.assertRaises(TypeError) as ctx:\n             r1.replace(unknown=\"unknown\")\n-        self.assertEqual(str(ctx.exception), \"__init__() got an unexpected keyword argument 'unknown'\")\n+        self.assertTrue(str(ctx.exception).endswith(\"__init__() got an unexpected keyword argument 'unknown'\"))\n\n@@ -158,12 +158,14 @@ class CallbackKeywordArgumentsTestCase(TestCase):\n                 if key in line.getMessage():\n                     exceptions[key] = line\n         self.assertEqual(exceptions['takes_less'].exc_info[0], TypeError)\n-        self.assertEqual(\n-            str(exceptions['takes_less'].exc_info[1]),\n+        self.assertTrue(\n+            str(exceptions['takes_less'].exc_info[1]).endswith(\n                 \"parse_takes_less() got an unexpected keyword argument 'number'\"\n             )\n+        )\n         self.assertEqual(exceptions['takes_more'].exc_info[0], TypeError)\n-        self.assertEqual(\n-            str(exceptions['takes_more'].exc_info[1]),\n+        self.assertTrue(\n+            str(exceptions['takes_more'].exc_info[1]).endswith(\n                 \"parse_takes_more() missing 1 required positional argument: 'other'\"\n             )\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#94baa4b27273e5a779bb977cea4c8eb5301fc3bf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 3582 | Contributors (this commit): 31 | Commits (past 90d): 5 | Contributors (cumulative): 31 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1932,8 +1932,9 @@ class FileFeedStoragePreFeedOptionsTest(unittest.TestCase):\n     maxDiff = None\n \n     def test_init(self):\n+        with tempfile.NamedTemporaryFile() as temp:\n             settings_dict = {\n-            'FEED_URI': 'file:///tmp/foobar',\n+                'FEED_URI': f'file:///{temp.name}',\n                 'FEED_STORAGES': {\n                     'file': FileFeedStorageWithoutFeedOptions\n                 },\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8e7d2ef13312bfb4ec5e1800f00108806ddc12e8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 623 | Contributors (this commit): 55 | Commits (past 90d): 1 | Contributors (cumulative): 55 | DMM Complexity: None\n\nDIFF:\n@@ -199,6 +199,8 @@ ITEM_PROCESSOR = 'scrapy.pipelines.ItemPipelineManager'\n ITEM_PIPELINES = {}\n ITEM_PIPELINES_BASE = {}\n \n+JOBDIR = ''\n+\n LOG_ENABLED = True\n LOG_ENCODING = 'utf-8'\n LOG_FORMATTER = 'scrapy.logformatter.LogFormatter'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#48eff4ee8f21535e98baf2bdff91749fee002c10", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 625 | Contributors (this commit): 55 | Commits (past 90d): 2 | Contributors (cumulative): 55 | DMM Complexity: None\n\nDIFF:\n@@ -199,8 +199,6 @@ ITEM_PROCESSOR = 'scrapy.pipelines.ItemPipelineManager'\n ITEM_PIPELINES = {}\n ITEM_PIPELINES_BASE = {}\n \n-JOBDIR = ''\n-\n LOG_ENABLED = True\n LOG_ENCODING = 'utf-8'\n LOG_FORMATTER = 'scrapy.logformatter.LogFormatter'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1ba0f68483cfb8aa62759e21b3479ec9bea94beb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 625 | Contributors (this commit): 29 | Commits (past 90d): 1 | Contributors (cumulative): 29 | DMM Complexity: None\n\nDIFF:\n@@ -138,7 +138,7 @@ def md5sum(file):\n \n def rel_has_nofollow(rel):\n     \"\"\"Return True if link rel attribute has nofollow type\"\"\"\n-    return rel is not None and 'nofollow' in rel.split()\n+    return rel is not None and 'nofollow' in rel.replace(',', ' ').split()\n \n \n def create_instance(objcls, settings, crawler, *args, **kwargs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#18b6f30a7359d1a798c30888ddd10a1612d8e711", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 226 | Contributors (this commit): 11 | Commits (past 90d): 1 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,7 +4,7 @@ import unittest\n from unittest import mock\n \n from scrapy.item import Item, Field\n-from scrapy.utils.misc import arg_to_iter, create_instance, load_object, set_environ, walk_modules\n+from scrapy.utils.misc import arg_to_iter, create_instance, load_object, rel_has_nofollow, set_environ, walk_modules\n \n \n __doctests__ = ['scrapy.utils.misc']\n@@ -162,6 +162,12 @@ class UtilsMiscTestCase(unittest.TestCase):\n             assert os.environ.get('some_test_environ') == 'test_value'\n         assert os.environ.get('some_test_environ') == 'test'\n \n+    def test_rel_has_nofollow(self):\n+        assert os.environ.get('some_test_environ') is None\n+        asert rel_has_nofollow('ugc nofollow') == True\n+        asert rel_has_nofollow('ugc,nofollow') == True\n+        asert rel_has_nofollow('ugc') == False\n+\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#07d20a8ce45ab0cbf61d08214db4963302661257", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 233 | Contributors (this commit): 11 | Commits (past 90d): 2 | Contributors (cumulative): 11 | DMM Complexity: 0.0\n\nDIFF:\n@@ -163,10 +163,9 @@ class UtilsMiscTestCase(unittest.TestCase):\n         assert os.environ.get('some_test_environ') == 'test'\n \n     def test_rel_has_nofollow(self):\n-        assert os.environ.get('some_test_environ') is None\n-        asert rel_has_nofollow('ugc nofollow') == True\n-        asert rel_has_nofollow('ugc,nofollow') == True\n-        asert rel_has_nofollow('ugc') == False\n+        assert rel_has_nofollow('ugc nofollow') == True\n+        assert rel_has_nofollow('ugc,nofollow') == True\n+        assert rel_has_nofollow('ugc') == False\n \n \n if __name__ == \"__main__\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#295f0e2bf5c352c6ddf27a188af10bf122d1c6b0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 239 | Contributors (this commit): 11 | Commits (past 90d): 3 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -163,9 +163,9 @@ class UtilsMiscTestCase(unittest.TestCase):\n         assert os.environ.get('some_test_environ') == 'test'\n \n     def test_rel_has_nofollow(self):\n-        assert rel_has_nofollow('ugc nofollow') == True\n-        assert rel_has_nofollow('ugc,nofollow') == True\n-        assert rel_has_nofollow('ugc') == False\n+        assert rel_has_nofollow('ugc nofollow') is True\n+        assert rel_has_nofollow('ugc,nofollow') is True\n+        assert rel_has_nofollow('ugc') is False\n \n \n if __name__ == \"__main__\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ce9d6c658b21a5d9d9605a2683b7a143f2077dfa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 243 | Contributors (this commit): 11 | Commits (past 90d): 4 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -166,6 +166,10 @@ class UtilsMiscTestCase(unittest.TestCase):\n         assert rel_has_nofollow('ugc nofollow') is True\n         assert rel_has_nofollow('ugc,nofollow') is True\n         assert rel_has_nofollow('ugc') is False\n+        assert rel_has_nofollow('nofollow') is True\n+        assert rel_has_nofollow('nofollowfoo') is False\n+        assert rel_has_nofollow('foonofollow') is False\n+        assert rel_has_nofollow('ugc,  ,  nofollow') is True\n \n \n if __name__ == \"__main__\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#983b89ad4f72730c37b32c9240a697a4c1f24183", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 5 | Churn Cumulative: 273 | Contributors (this commit): 9 | Commits (past 90d): 1 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -118,6 +118,11 @@ class SpiderLoaderTest(unittest.TestCase):\n             settings = Settings({'SPIDER_MODULES': [module],\n                                  'SPIDER_LOADER_WARN_ONLY': True})\n             spider_loader = SpiderLoader.from_settings(settings)\n+            if str(w[0].message).startswith(\"_SixMetaPathImporter\"):\n+                # needed on 3.10 because of https://github.com/benjaminp/six/issues/349,\n+                # at least until all six versions we can import (including botocore.vendored.six)\n+                # are updated to 1.16.0+\n+                w.pop(0)\n             self.assertIn(\"Could not load spiders from module\", str(w[0].message))\n \n             spiders = spider_loader.list()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2814e0e1972fa38151b6800c881d49f50edf9c6b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -36/0 | Churn Δ: 2 | Churn Cumulative: 1419 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -15,7 +15,7 @@ class SpiderMiddlewareTestCase(TestCase):\n     def setUp(self):\n         self.request = Request('http://example.com/index.html')\n         self.response = Response(self.request.url, request=self.request)\n-        self.crawler = get_crawler(Spider)\n+        self.crawler = get_crawler(Spider, {'SPIDER_MIDDLEWARES_BASE': {}})\n         self.spider = self.crawler._create_spider('foo')\n         self.mwman = SpiderMiddlewareManager.from_crawler(self.crawler)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bbeed6ae8fd9aed3651b104e4cc3e56495e1b1b9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 15 | Files Changed: 2 | Hunks: 9 | Methods Changed: 9 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 49 | Churn Cumulative: 1017 | Contributors (this commit): 15 | Commits (past 90d): 2 | Contributors (cumulative): 23 | DMM Complexity: 1.0\n\nDIFF:\n@@ -79,35 +79,45 @@ class CaselessDict(dict):\n         return dict.pop(self, self.normkey(key), *args)\n \n \n-class CaseInsensitiveDict(collections.UserDict):\n+class CaseInsensitiveDict(collections.UserDict,):\n     \"\"\"A dict-like structure that accepts strings or bytes as keys and allows case-insensitive lookups.\n \n     It also allows overriding key and value normalization by defining custom `normkey` and `normvalue` methods.\n     \"\"\"\n \n+    def __init__(self, *args, **kwargs) -> None:\n+        self._keys: dict = {}\n+        super().__init__(*args, **kwargs)\n+\n     def __getitem__(self, key: AnyStr) -> Any:\n-        return super().__getitem__(self.normkey(key))\n+        normalized_key = self.normkey(key)\n+        return super().__getitem__(self._keys[normalized_key.lower()])\n \n     def __setitem__(self, key: AnyStr, value: Any) -> None:\n-        super().__setitem__(self.normkey(key), self.normvalue(value))\n+        normalized_key = self.normkey(key)\n+        if normalized_key.lower() in self._keys:\n+            del self[self._keys[normalized_key.lower()]]\n+        super().__setitem__(normalized_key, self.normvalue(value))\n+        self._keys[normalized_key.lower()] = normalized_key\n \n     def __delitem__(self, key: AnyStr) -> None:\n-        super().__delitem__(self.normkey(key))\n+        normalized_key = self.normkey(key)\n+        stored_key = self._keys.pop(normalized_key.lower())\n+        super().__delitem__(stored_key)\n \n     def __contains__(self, key: AnyStr) -> bool:  # type: ignore[override]\n-        return super().__contains__(self.normkey(key))\n-\n-    def normkey(self, key: AnyStr) -> AnyStr:\n-        \"\"\"Method to normalize dictionary key access\"\"\"\n-        return key.lower()\n-\n-    def normvalue(self, value: Any) -> Any:\n-        \"\"\"Method to normalize values prior to be set\"\"\"\n-        return value\n+        normalized_key = self.normkey(key)\n+        return normalized_key.lower() in self._keys\n \n     def __repr__(self) -> str:\n         return f\"<{self.__class__.__name__}: {super().__repr__()}>\"\n \n+    def normkey(self, key: AnyStr) -> AnyStr:\n+        return key\n+\n+    def normvalue(self, value: Any) -> Any:\n+        return value\n+\n \n class LocalCache(collections.OrderedDict):\n     \"\"\"Dictionary with a finite number of keys.\n\n@@ -1,4 +1,5 @@\n import copy\n+from typing import Iterator\n import unittest\n import warnings\n from collections.abc import Mapping, MutableMapping\n@@ -191,8 +192,16 @@ class CaseInsensitiveDictTest(CaseInsensitiveDictMixin, unittest.TestCase):\n     dict_class = CaseInsensitiveDict\n \n     def test_repr(self):\n-        d = self.dict_class({\"foo\": \"bar\"})\n-        self.assertEqual(repr(d), \"<CaseInsensitiveDict: {'foo': 'bar'}>\")\n+        d1 = self.dict_class({\"foo\": \"bar\"})\n+        self.assertEqual(repr(d1), \"<CaseInsensitiveDict: {'foo': 'bar'}>\")\n+        d2 = self.dict_class({\"AsDf\": \"QwErTy\", \"FoO\": \"bAr\"})\n+        self.assertEqual(repr(d2), \"<CaseInsensitiveDict: {'AsDf': 'QwErTy', 'FoO': 'bAr'}>\")\n+\n+    def test_iter(self):\n+        d = self.dict_class({\"AsDf\": \"QwErTy\", \"FoO\": \"bAr\"})\n+        iterkeys = iter(d)\n+        self.assertIsInstance(iterkeys, Iterator)\n+        self.assertEqual(list(iterkeys), [\"AsDf\", \"FoO\"])\n \n \n class CaselessDictTest(CaseInsensitiveDictMixin, unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#10ebf6384ed58253c237224d523e602b1f3c2224", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 668 | Contributors (this commit): 14 | Commits (past 90d): 2 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -79,7 +79,7 @@ class CaselessDict(dict):\n         return dict.pop(self, self.normkey(key), *args)\n \n \n-class CaseInsensitiveDict(collections.UserDict,):\n+class CaseInsensitiveDict(collections.UserDict):\n     \"\"\"A dict-like structure that accepts strings or bytes as keys and allows case-insensitive lookups.\n \n     It also allows overriding key and value normalization by defining custom `normkey` and `normvalue` methods.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3f635eb683821667b7a46a99531a95a8c05b6e1b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 6 | Methods Changed: 7 | Complexity Δ (Sum/Max): 11/8 | Churn Δ: 42 | Churn Cumulative: 1713 | Contributors (this commit): 28 | Commits (past 90d): 2 | Contributors (cumulative): 35 | DMM Complexity: 0.9655172413793104\n\nDIFF:\n@@ -4,6 +4,7 @@ import string\n \n from importlib import import_module\n from os.path import join, dirname, abspath, exists, splitext\n+from urllib.parse import urlparse\n \n import scrapy\n from scrapy.commands import ScrapyCommand\n@@ -22,6 +23,14 @@ def sanitize_module_name(module_name):\n     return module_name\n \n \n+def extract_domain(url):\n+    \"\"\"Extract domain name from URL string\"\"\"\n+    o = urlparse(url)\n+    if o.scheme == '' and o.netloc == '':\n+        o = urlparse(\"//\" + url.lstrip(\"/\"))\n+    return o.netloc\n+\n+\n class Command(ScrapyCommand):\n \n     requires_project = False\n@@ -59,7 +68,8 @@ class Command(ScrapyCommand):\n         if len(args) != 2:\n             raise UsageError()\n \n-        name, domain = args[0:2]\n+        name, url = args[0:2]\n+        domain = extract_domain(url)\n         module = sanitize_module_name(name)\n \n         if self.settings.get('BOT_NAME') == module:\n\n@@ -3,6 +3,7 @@ import json\n import optparse\n import os\n import platform\n+import re\n import subprocess\n import sys\n import tempfile\n@@ -94,6 +95,15 @@ class ProjectTest(unittest.TestCase):\n \n         return p, to_unicode(stdout), to_unicode(stderr)\n \n+    def find_in_file(self, filename, regex):\n+        \"\"\"Find first pattern occurrence in file\"\"\"\n+        pattern = re.compile(regex)\n+        with open(filename, \"r\") as f:\n+            for line in f:\n+                match = pattern.search(line)\n+                if match is not None:\n+                    return match\n+\n \n class StartprojectTest(ProjectTest):\n \n@@ -482,6 +492,26 @@ class GenspiderCommandTest(CommandTest):\n     def test_same_filename_as_existing_spider_force(self):\n         self.test_same_filename_as_existing_spider(force=True)\n \n+    def test_url(self, url='test.com', domain=\"test.com\"):\n+        self.assertEqual(0, self.call('genspider', '--force', 'test_name', url))\n+        self.assertEqual(domain,\n+                         self.find_in_file(join(self.proj_mod_path,\n+                                                'spiders', 'test_name.py'),\n+                                           r'allowed_domains\\s*=\\s*\\[\\'(.+)\\'\\]').group(1))\n+        self.assertEqual('http://%s/' % domain,\n+                         self.find_in_file(join(self.proj_mod_path,\n+                                                'spiders', 'test_name.py'),\n+                                           r'start_urls\\s*=\\s*\\[\\'(.+)\\'\\]').group(1))\n+\n+    def test_url_schema(self):\n+        self.test_url('http://test.com', 'test.com')\n+\n+    def test_url_path(self):\n+        self.test_url('test.com/some/other/page', 'test.com')\n+\n+    def test_url_schema_path(self):\n+        self.test_url('https://test.com/some/other/page', 'test.com')\n+\n \n class GenspiderStandaloneCommandTest(ProjectTest):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#43ea21e8306bc66e8f07abc84cce680726abc7dc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 654 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 7 | Methods Changed: 30 | Complexity Δ (Sum/Max): 62/43 | Churn Δ: 654 | Churn Cumulative: 5397 | Contributors (this commit): 37 | Commits (past 90d): 12 | Contributors (cumulative): 62 | DMM Complexity: 0.9497816593886463\n\nDIFF:\n@@ -20,6 +20,7 @@ from zope.interface import implementer, Interface\n \n from scrapy import signals\n from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n+from scrapy.extensions.postprocessing import PostProcessingManager\n from scrapy.utils.boto import is_botocore_available\n from scrapy.utils.conf import feed_complete_default_values_from_settings\n from scrapy.utils.ftp import ftp_store_file\n@@ -396,6 +397,9 @@ class FeedExporter:\n         \"\"\"\n         storage = self._get_storage(uri, feed_options)\n         file = storage.open(spider)\n+        if \"postprocessing\" in feed_options:\n+            file = PostProcessingManager(feed_options[\"postprocessing\"], file, feed_options)\n+\n         exporter = self._get_exporter(\n             file=file,\n             format=feed_options['format'],\n\n@@ -0,0 +1,154 @@\n+\"\"\"\n+Extension for processing data before they are exported to feeds.\n+\"\"\"\n+from bz2 import BZ2File\n+from gzip import GzipFile\n+from io import IOBase\n+from lzma import LZMAFile\n+from typing import Any, BinaryIO, Dict, List\n+\n+from scrapy.utils.misc import load_object\n+\n+\n+class GzipPlugin:\n+    \"\"\"\n+    Compresses received data using `gzip <https://en.wikipedia.org/wiki/Gzip>`_.\n+\n+    Accepted ``feed_options`` parameters:\n+\n+    - `gzip_compresslevel`\n+    - `gzip_mtime`\n+    - `gzip_filename`\n+\n+    See :py:class:`gzip.GzipFile` for more info about parameters.\n+    \"\"\"\n+\n+    def __init__(self, file: BinaryIO, feed_options: Dict[str, Any]) -> None:\n+        self.file = file\n+        self.feed_options = feed_options\n+        compress_level = self.feed_options.get(\"gzip_compresslevel\", 9)\n+        mtime = self.feed_options.get(\"gzip_mtime\")\n+        filename = self.feed_options.get(\"gzip_filename\")\n+        self.gzipfile = GzipFile(fileobj=self.file, mode=\"wb\", compresslevel=compress_level,\n+                                 mtime=mtime, filename=filename)\n+\n+    def write(self, data: bytes) -> int:\n+        return self.gzipfile.write(data)\n+\n+    def close(self) -> None:\n+        self.gzipfile.close()\n+        self.file.close()\n+\n+\n+class Bz2Plugin:\n+    \"\"\"\n+    Compresses received data using `bz2 <https://en.wikipedia.org/wiki/Bzip2>`_.\n+\n+    Accepted ``feed_options`` parameters:\n+\n+    - `bz2_compresslevel`\n+\n+    See :py:class:`bz2.BZ2File` for more info about parameters.\n+    \"\"\"\n+\n+    def __init__(self, file: BinaryIO, feed_options: Dict[str, Any]) -> None:\n+        self.file = file\n+        self.feed_options = feed_options\n+        compress_level = self.feed_options.get(\"bz2_compresslevel\", 9)\n+        self.bz2file = BZ2File(filename=self.file, mode=\"wb\", compresslevel=compress_level)\n+\n+    def write(self, data: bytes) -> int:\n+        return self.bz2file.write(data)\n+\n+    def close(self) -> None:\n+        self.bz2file.close()\n+        self.file.close()\n+\n+\n+class LZMAPlugin:\n+    \"\"\"\n+    Compresses received data using `lzma <https://en.wikipedia.org/wiki/Lempel–Ziv–Markov_chain_algorithm>`_.\n+\n+    Accepted ``feed_options`` parameters:\n+\n+    - `lzma_format`\n+    - `lzma_check`\n+    - `lzma_preset`\n+    - `lzma_filters`\n+\n+    .. note::\n+        ``lzma_filters`` cannot be used in pypy version 7.3.1 and older.\n+\n+    See :py:class:`lzma.LZMAFile` for more info about parameters.\n+    \"\"\"\n+\n+    def __init__(self, file: BinaryIO, feed_options: Dict[str, Any]) -> None:\n+        self.file = file\n+        self.feed_options = feed_options\n+\n+        format = self.feed_options.get(\"lzma_format\")\n+        check = self.feed_options.get(\"lzma_check\", -1)\n+        preset = self.feed_options.get(\"lzma_preset\")\n+        filters = self.feed_options.get(\"lzma_filters\")\n+        self.lzmafile = LZMAFile(filename=self.file, mode=\"wb\", format=format,\n+                                 check=check, preset=preset, filters=filters)\n+\n+    def write(self, data: bytes) -> int:\n+        return self.lzmafile.write(data)\n+\n+    def close(self) -> None:\n+        self.lzmafile.close()\n+        self.file.close()\n+\n+\n+# io.IOBase is subclassed here, so that exporters can use the PostProcessingManager\n+# instance as a file like writable object. This could be needed by some exporters\n+# such as CsvItemExporter which wraps the feed storage with io.TextIOWrapper.\n+class PostProcessingManager(IOBase):\n+    \"\"\"\n+    This will manage and use declared plugins to process data in a\n+    pipeline-ish way.\n+    :param plugins: all the declared plugins for the feed\n+    :type plugins: list\n+    :param file: final target file where the processed data will be written\n+    :type file: file like object\n+    \"\"\"\n+\n+    def __init__(self, plugins: List[Any], file: BinaryIO, feed_options: Dict[str, Any]) -> None:\n+        self.plugins = self._load_plugins(plugins)\n+        self.file = file\n+        self.feed_options = feed_options\n+        self.head_plugin = self._get_head_plugin()\n+\n+    def write(self, data: bytes) -> int:\n+        \"\"\"\n+        Uses all the declared plugins to process data first, then writes\n+        the processed data to target file.\n+        :param data: data passed to be written to target file\n+        :type data: bytes\n+        :return: returns number of bytes written\n+        :rtype: int\n+        \"\"\"\n+        return self.head_plugin.write(data)\n+\n+    def tell(self) -> int:\n+        return self.file.tell()\n+\n+    def close(self) -> None:\n+        \"\"\"\n+        Close the target file along with all the plugins.\n+        \"\"\"\n+        self.head_plugin.close()\n+\n+    def writable(self) -> bool:\n+        return True\n+\n+    def _load_plugins(self, plugins: List[Any]) -> List[Any]:\n+        plugins = [load_object(plugin) for plugin in plugins]\n+        return plugins\n+\n+    def _get_head_plugin(self) -> Any:\n+        prev = self.file\n+        for plugin in self.plugins[::-1]:\n+            prev = plugin(prev, self.feed_options)\n+        return prev\n\n@@ -1,5 +1,8 @@\n+import bz2\n import csv\n+import gzip\n import json\n+import lzma\n import os\n import random\n import shutil\n@@ -1473,6 +1476,499 @@ class FeedExportTest(FeedExportTestBase):\n             self.assertEqual(row['expected'], data[feed_options['format']])\n \n \n+class FeedPostProcessedExportsTest(FeedExportTestBase):\n+    __test__ = True\n+\n+    items = [{'foo': 'bar'}]\n+    expected = b'foo\\r\\nbar\\r\\n'\n+\n+    class MyPlugin1:\n+        def __init__(self, file, feed_options):\n+            self.file = file\n+            self.feed_options = feed_options\n+            self.char = self.feed_options.get('plugin1_char', b'')\n+\n+        def write(self, data):\n+            written_count = self.file.write(data)\n+            written_count += self.file.write(self.char)\n+            return written_count\n+\n+        def close(self):\n+            self.file.close()\n+\n+    def _named_tempfile(self, name):\n+        return os.path.join(self.temp_dir, name)\n+\n+    @defer.inlineCallbacks\n+    def run_and_export(self, spider_cls, settings):\n+        \"\"\" Run spider with specified settings; return exported data with filename. \"\"\"\n+\n+        FEEDS = settings.get('FEEDS') or {}\n+        settings['FEEDS'] = {\n+            printf_escape(path_to_url(file_path)): feed_options\n+            for file_path, feed_options in FEEDS.items()\n+        }\n+\n+        content = {}\n+        try:\n+            with MockServer() as s:\n+                runner = CrawlerRunner(Settings(settings))\n+                spider_cls.start_urls = [s.url('/')]\n+                yield runner.crawl(spider_cls)\n+\n+            for file_path, feed_options in FEEDS.items():\n+                if not os.path.exists(str(file_path)):\n+                    continue\n+\n+                with open(str(file_path), 'rb') as f:\n+                    content[str(file_path)] = f.read()\n+\n+        finally:\n+            for file_path in FEEDS.keys():\n+                if not os.path.exists(str(file_path)):\n+                    continue\n+\n+                os.remove(str(file_path))\n+\n+        return content\n+\n+    def get_gzip_compressed(self, data, compresslevel=9, mtime=0, filename=''):\n+        data_stream = BytesIO()\n+        gzipf = gzip.GzipFile(fileobj=data_stream, filename=filename, mtime=mtime,\n+                              compresslevel=compresslevel, mode=\"wb\")\n+        gzipf.write(data)\n+        gzipf.close()\n+        data_stream.seek(0)\n+        return data_stream.read()\n+\n+    @defer.inlineCallbacks\n+    def test_gzip_plugin(self):\n+\n+        filename = self._named_tempfile('gzip_file')\n+\n+        settings = {\n+            'FEEDS': {\n+                filename: {\n+                    'format': 'csv',\n+                    'postprocessing': ['scrapy.extensions.postprocessing.GzipPlugin'],\n+                },\n+            },\n+        }\n+\n+        data = yield self.exported_data(self.items, settings)\n+        try:\n+            gzip.decompress(data[filename])\n+        except OSError:\n+            self.fail(\"Received invalid gzip data.\")\n+\n+    @defer.inlineCallbacks\n+    def test_gzip_plugin_compresslevel(self):\n+\n+        filename_to_compressed = {\n+            self._named_tempfile('compresslevel_0'): self.get_gzip_compressed(self.expected, compresslevel=0),\n+            self._named_tempfile('compresslevel_9'): self.get_gzip_compressed(self.expected, compresslevel=9),\n+        }\n+\n+        settings = {\n+            'FEEDS': {\n+                self._named_tempfile('compresslevel_0'): {\n+                    'format': 'csv',\n+                    'postprocessing': ['scrapy.extensions.postprocessing.GzipPlugin'],\n+                    'gzip_compresslevel': 0,\n+                    'gzip_mtime': 0,\n+                    'gzip_filename': \"\",\n+                },\n+                self._named_tempfile('compresslevel_9'): {\n+                    'format': 'csv',\n+                    'postprocessing': ['scrapy.extensions.postprocessing.GzipPlugin'],\n+                    'gzip_compresslevel': 9,\n+                    'gzip_mtime': 0,\n+                    'gzip_filename': \"\",\n+                },\n+            },\n+        }\n+\n+        data = yield self.exported_data(self.items, settings)\n+\n+        for filename, compressed in filename_to_compressed.items():\n+            result = gzip.decompress(data[filename])\n+            self.assertEqual(compressed, data[filename])\n+            self.assertEqual(self.expected, result)\n+\n+    @defer.inlineCallbacks\n+    def test_gzip_plugin_mtime(self):\n+        filename_to_compressed = {\n+            self._named_tempfile('mtime_123'): self.get_gzip_compressed(self.expected, mtime=123),\n+            self._named_tempfile('mtime_123456789'): self.get_gzip_compressed(self.expected, mtime=123456789),\n+        }\n+\n+        settings = {\n+            'FEEDS': {\n+                self._named_tempfile('mtime_123'): {\n+                    'format': 'csv',\n+                    'postprocessing': ['scrapy.extensions.postprocessing.GzipPlugin'],\n+                    'gzip_mtime': 123,\n+                    'gzip_filename': \"\",\n+                },\n+                self._named_tempfile('mtime_123456789'): {\n+                    'format': 'csv',\n+                    'postprocessing': ['scrapy.extensions.postprocessing.GzipPlugin'],\n+                    'gzip_mtime': 123456789,\n+                    'gzip_filename': \"\",\n+                },\n+            },\n+        }\n+\n+        data = yield self.exported_data(self.items, settings)\n+\n+        for filename, compressed in filename_to_compressed.items():\n+            result = gzip.decompress(data[filename])\n+            self.assertEqual(compressed, data[filename])\n+            self.assertEqual(self.expected, result)\n+\n+    @defer.inlineCallbacks\n+    def test_gzip_plugin_filename(self):\n+        filename_to_compressed = {\n+            self._named_tempfile('filename_FILE1'): self.get_gzip_compressed(self.expected, filename=\"FILE1\"),\n+            self._named_tempfile('filename_FILE2'): self.get_gzip_compressed(self.expected, filename=\"FILE2\"),\n+        }\n+\n+        settings = {\n+            'FEEDS': {\n+                self._named_tempfile('filename_FILE1'): {\n+                    'format': 'csv',\n+                    'postprocessing': ['scrapy.extensions.postprocessing.GzipPlugin'],\n+                    'gzip_mtime': 0,\n+                    'gzip_filename': \"FILE1\",\n+                },\n+                self._named_tempfile('filename_FILE2'): {\n+                    'format': 'csv',\n+                    'postprocessing': ['scrapy.extensions.postprocessing.GzipPlugin'],\n+                    'gzip_mtime': 0,\n+                    'gzip_filename': \"FILE2\",\n+                },\n+            },\n+        }\n+\n+        data = yield self.exported_data(self.items, settings)\n+\n+        for filename, compressed in filename_to_compressed.items():\n+            result = gzip.decompress(data[filename])\n+            self.assertEqual(compressed, data[filename])\n+            self.assertEqual(self.expected, result)\n+\n+    @defer.inlineCallbacks\n+    def test_lzma_plugin(self):\n+\n+        filename = self._named_tempfile('lzma_file')\n+\n+        settings = {\n+            'FEEDS': {\n+                filename: {\n+                    'format': 'csv',\n+                    'postprocessing': ['scrapy.extensions.postprocessing.LZMAPlugin'],\n+                },\n+            },\n+        }\n+\n+        data = yield self.exported_data(self.items, settings)\n+        try:\n+            lzma.decompress(data[filename])\n+        except lzma.LZMAError:\n+            self.fail(\"Received invalid lzma data.\")\n+\n+    @defer.inlineCallbacks\n+    def test_lzma_plugin_format(self):\n+\n+        filename_to_compressed = {\n+            self._named_tempfile('format_FORMAT_XZ'): lzma.compress(self.expected, format=lzma.FORMAT_XZ),\n+            self._named_tempfile('format_FORMAT_ALONE'): lzma.compress(self.expected, format=lzma.FORMAT_ALONE),\n+        }\n+\n+        settings = {\n+            'FEEDS': {\n+                self._named_tempfile('format_FORMAT_XZ'): {\n+                    'format': 'csv',\n+                    'postprocessing': ['scrapy.extensions.postprocessing.LZMAPlugin'],\n+                    'lzma_format': lzma.FORMAT_XZ,\n+                },\n+                self._named_tempfile('format_FORMAT_ALONE'): {\n+                    'format': 'csv',\n+                    'postprocessing': ['scrapy.extensions.postprocessing.LZMAPlugin'],\n+                    'lzma_format': lzma.FORMAT_ALONE,\n+                },\n+            },\n+        }\n+\n+        data = yield self.exported_data(self.items, settings)\n+\n+        for filename, compressed in filename_to_compressed.items():\n+            result = lzma.decompress(data[filename])\n+            self.assertEqual(compressed, data[filename])\n+            self.assertEqual(self.expected, result)\n+\n+    @defer.inlineCallbacks\n+    def test_lzma_plugin_check(self):\n+\n+        filename_to_compressed = {\n+            self._named_tempfile('check_CHECK_NONE'): lzma.compress(self.expected, check=lzma.CHECK_NONE),\n+            self._named_tempfile('check_CHECK_CRC256'): lzma.compress(self.expected, check=lzma.CHECK_SHA256),\n+        }\n+\n+        settings = {\n+            'FEEDS': {\n+                self._named_tempfile('check_CHECK_NONE'): {\n+                    'format': 'csv',\n+                    'postprocessing': ['scrapy.extensions.postprocessing.LZMAPlugin'],\n+                    'lzma_check': lzma.CHECK_NONE,\n+                },\n+                self._named_tempfile('check_CHECK_CRC256'): {\n+                    'format': 'csv',\n+                    'postprocessing': ['scrapy.extensions.postprocessing.LZMAPlugin'],\n+                    'lzma_check': lzma.CHECK_SHA256,\n+                },\n+            },\n+        }\n+\n+        data = yield self.exported_data(self.items, settings)\n+\n+        for filename, compressed in filename_to_compressed.items():\n+            result = lzma.decompress(data[filename])\n+            self.assertEqual(compressed, data[filename])\n+            self.assertEqual(self.expected, result)\n+\n+    @defer.inlineCallbacks\n+    def test_lzma_plugin_preset(self):\n+\n+        filename_to_compressed = {\n+            self._named_tempfile('preset_PRESET_0'): lzma.compress(self.expected, preset=0),\n+            self._named_tempfile('preset_PRESET_9'): lzma.compress(self.expected, preset=9),\n+        }\n+\n+        settings = {\n+            'FEEDS': {\n+                self._named_tempfile('preset_PRESET_0'): {\n+                    'format': 'csv',\n+                    'postprocessing': ['scrapy.extensions.postprocessing.LZMAPlugin'],\n+                    'lzma_preset': 0,\n+                },\n+                self._named_tempfile('preset_PRESET_9'): {\n+                    'format': 'csv',\n+                    'postprocessing': ['scrapy.extensions.postprocessing.LZMAPlugin'],\n+                    'lzma_preset': 9,\n+                },\n+            },\n+        }\n+\n+        data = yield self.exported_data(self.items, settings)\n+\n+        for filename, compressed in filename_to_compressed.items():\n+            result = lzma.decompress(data[filename])\n+            self.assertEqual(compressed, data[filename])\n+            self.assertEqual(self.expected, result)\n+\n+    @defer.inlineCallbacks\n+    def test_lzma_plugin_filters(self):\n+        import sys\n+        if \"PyPy\" in sys.version:\n+            # https://foss.heptapod.net/pypy/pypy/-/issues/3527\n+            raise unittest.SkipTest(\"lzma filters doesn't work in PyPy\")\n+\n+        filters = [{'id': lzma.FILTER_LZMA2}]\n+        compressed = lzma.compress(self.expected, filters=filters)\n+        filename = self._named_tempfile('filters')\n+\n+        settings = {\n+            'FEEDS': {\n+                filename: {\n+                    'format': 'csv',\n+                    'postprocessing': ['scrapy.extensions.postprocessing.LZMAPlugin'],\n+                    'lzma_filters': filters,\n+                },\n+            },\n+        }\n+\n+        data = yield self.exported_data(self.items, settings)\n+        self.assertEqual(compressed, data[filename])\n+        result = lzma.decompress(data[filename])\n+        self.assertEqual(self.expected, result)\n+\n+    @defer.inlineCallbacks\n+    def test_bz2_plugin(self):\n+\n+        filename = self._named_tempfile('bz2_file')\n+\n+        settings = {\n+            'FEEDS': {\n+                filename: {\n+                    'format': 'csv',\n+                    'postprocessing': ['scrapy.extensions.postprocessing.Bz2Plugin'],\n+                },\n+            },\n+        }\n+\n+        data = yield self.exported_data(self.items, settings)\n+        try:\n+            bz2.decompress(data[filename])\n+        except OSError:\n+            self.fail(\"Received invalid bz2 data.\")\n+\n+    @defer.inlineCallbacks\n+    def test_bz2_plugin_compresslevel(self):\n+\n+        filename_to_compressed = {\n+            self._named_tempfile('compresslevel_1'): bz2.compress(self.expected, compresslevel=1),\n+            self._named_tempfile('compresslevel_9'): bz2.compress(self.expected, compresslevel=9),\n+        }\n+\n+        settings = {\n+            'FEEDS': {\n+                self._named_tempfile('compresslevel_1'): {\n+                    'format': 'csv',\n+                    'postprocessing': ['scrapy.extensions.postprocessing.Bz2Plugin'],\n+                    'bz2_compresslevel': 1,\n+                },\n+                self._named_tempfile('compresslevel_9'): {\n+                    'format': 'csv',\n+                    'postprocessing': ['scrapy.extensions.postprocessing.Bz2Plugin'],\n+                    'bz2_compresslevel': 9,\n+                },\n+            },\n+        }\n+\n+        data = yield self.exported_data(self.items, settings)\n+\n+        for filename, compressed in filename_to_compressed.items():\n+            result = bz2.decompress(data[filename])\n+            self.assertEqual(compressed, data[filename])\n+            self.assertEqual(self.expected, result)\n+\n+    @defer.inlineCallbacks\n+    def test_custom_plugin(self):\n+        filename = self._named_tempfile('csv_file')\n+\n+        settings = {\n+            'FEEDS': {\n+                filename: {\n+                    'format': 'csv',\n+                    'postprocessing': [self.MyPlugin1],\n+                },\n+            },\n+        }\n+\n+        data = yield self.exported_data(self.items, settings)\n+        self.assertEqual(self.expected, data[filename])\n+\n+    @defer.inlineCallbacks\n+    def test_custom_plugin_with_parameter(self):\n+\n+        expected = b'foo\\r\\n\\nbar\\r\\n\\n'\n+        filename = self._named_tempfile('newline')\n+\n+        settings = {\n+            'FEEDS': {\n+                filename: {\n+                    'format': 'csv',\n+                    'postprocessing': [self.MyPlugin1],\n+                    'plugin1_char': b'\\n'\n+                },\n+            },\n+        }\n+\n+        data = yield self.exported_data(self.items, settings)\n+        self.assertEqual(expected, data[filename])\n+\n+    @defer.inlineCallbacks\n+    def test_custom_plugin_with_compression(self):\n+\n+        expected = b'foo\\r\\n\\nbar\\r\\n\\n'\n+\n+        filename_to_decompressor = {\n+            self._named_tempfile('bz2'): bz2.decompress,\n+            self._named_tempfile('lzma'): lzma.decompress,\n+            self._named_tempfile('gzip'): gzip.decompress,\n+        }\n+\n+        settings = {\n+            'FEEDS': {\n+                self._named_tempfile('bz2'): {\n+                    'format': 'csv',\n+                    'postprocessing': [self.MyPlugin1, 'scrapy.extensions.postprocessing.Bz2Plugin'],\n+                    'plugin1_char': b'\\n',\n+                },\n+                self._named_tempfile('lzma'): {\n+                    'format': 'csv',\n+                    'postprocessing': [self.MyPlugin1, 'scrapy.extensions.postprocessing.LZMAPlugin'],\n+                    'plugin1_char': b'\\n',\n+                },\n+                self._named_tempfile('gzip'): {\n+                    'format': 'csv',\n+                    'postprocessing': [self.MyPlugin1, 'scrapy.extensions.postprocessing.GzipPlugin'],\n+                    'plugin1_char': b'\\n',\n+                },\n+            },\n+        }\n+\n+        data = yield self.exported_data(self.items, settings)\n+\n+        for filename, decompressor in filename_to_decompressor.items():\n+            result = decompressor(data[filename])\n+            self.assertEqual(expected, result)\n+\n+    @defer.inlineCallbacks\n+    def test_exports_compatibility_with_postproc(self):\n+        import marshal\n+        import pickle\n+        filename_to_expected = {\n+            self._named_tempfile('csv'): b'foo\\r\\nbar\\r\\n',\n+            self._named_tempfile('json'): b'[\\n{\"foo\": \"bar\"}\\n]',\n+            self._named_tempfile('jsonlines'): b'{\"foo\": \"bar\"}\\n',\n+            self._named_tempfile('xml'): b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n'\n+                                         b'<items>\\n<item><foo>bar</foo></item>\\n</items>',\n+        }\n+\n+        settings = {\n+            'FEEDS': {\n+                self._named_tempfile('csv'): {\n+                    'format': 'csv',\n+                    'postprocessing': [self.MyPlugin1],\n+                    # empty plugin to activate postprocessing.PostProcessingManager\n+                },\n+                self._named_tempfile('json'): {\n+                    'format': 'json',\n+                    'postprocessing': [self.MyPlugin1],\n+                },\n+                self._named_tempfile('jsonlines'): {\n+                    'format': 'jsonlines',\n+                    'postprocessing': [self.MyPlugin1],\n+                },\n+                self._named_tempfile('xml'): {\n+                    'format': 'xml',\n+                    'postprocessing': [self.MyPlugin1],\n+                },\n+                self._named_tempfile('marshal'): {\n+                    'format': 'marshal',\n+                    'postprocessing': [self.MyPlugin1],\n+                },\n+                self._named_tempfile('pickle'): {\n+                    'format': 'pickle',\n+                    'postprocessing': [self.MyPlugin1],\n+                },\n+            },\n+        }\n+\n+        data = yield self.exported_data(self.items, settings)\n+\n+        for filename, result in data.items():\n+            if 'pickle' in filename:\n+                expected, result = self.items[0], pickle.loads(result)\n+            elif 'marshal' in filename:\n+                expected, result = self.items[0], marshal.loads(result)\n+            else:\n+                expected = filename_to_expected[filename]\n+            self.assertEqual(expected, result)\n+\n+\n class BatchDeliveriesTest(FeedExportTestBase):\n     __test__ = True\n     _file_mark = '_%(batch_time)s_#%(batch_id)02d_'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
