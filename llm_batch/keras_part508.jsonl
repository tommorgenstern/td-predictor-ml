{"custom_id": "keras#b4d6de62716a96f04843c78850c507a140f6fa42", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 14 | Churn Cumulative: 1031 | Contributors (this commit): 8 | Commits (past 90d): 1 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1020,6 +1020,20 @@ class BidirectionalTest(tf.test.TestCase, parameterized.TestCase):\n         self.assertAllClose(output1, output3)\n         self.assertNotAllClose(output1, output2)\n \n+    def test_trainable_parameter_argument(self):\n+        inp = keras.layers.Input([None, 3])\n+        rnn = keras.layers.SimpleRNN(units=3)\n+        bid = keras.layers.Bidirectional(rnn)\n+        model = keras.Model(inp, bid(inp))\n+\n+        clone_trainable = keras.models.clone_model(model)\n+        assert clone_trainable.get_config() == model.get_config()\n+\n+        bid.trainable = False\n+\n+        clone_untrainable = keras.models.clone_model(model)\n+        assert clone_untrainable.get_config() == model.get_config()\n+\n \n def _to_list(ls):\n     if isinstance(ls, list):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#914c142e2e84b7607c5c46c84b204fadd4cbdf0a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 6/2 | Churn Δ: 26 | Churn Cumulative: 2767 | Contributors (this commit): 10 | Commits (past 90d): 5 | Contributors (cumulative): 23 | DMM Complexity: 1.0\n\nDIFF:\n@@ -982,6 +982,14 @@ class GRULayerTest(test_combinations.TestCase):\n         )\n         self.assertEqual(state.shape, initial_state.shape)\n \n+    def test_cloned_weight_names(self):\n+        inp = keras.Input([None, 3])\n+        rnn = keras.layers.GRU(units=3)\n+        model = keras.Model(inp, rnn(inp))\n+        clone = keras.models.clone_model(model)\n+        assert len(model.weights) == len(clone.weights)\n+        for a, b in zip(model.weights, clone.weights):\n+            assert a.name == b.name\n \n @test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n class GRULayerGenericTest(tf.test.TestCase):\n\n@@ -1413,6 +1413,15 @@ class LSTMLayerTest(test_combinations.TestCase):\n \n         self.assertAllClose(out7, out6, atol=1e-5)\n \n+    def test_cloned_weight_names(self):\n+        inp = keras.Input([None, 3])\n+        rnn = keras.layers.LSTM(units=3)\n+        model = keras.Model(inp, rnn(inp))\n+        clone = keras.models.clone_model(model)\n+        assert len(model.weights) == len(clone.weights)\n+        for a, b in zip(model.weights, clone.weights):\n+            assert a.name == b.name\n+\n \n if __name__ == \"__main__\":\n     tf.test.main()\n\n@@ -238,6 +238,15 @@ class SimpleRNNLayerTest(tf.test.TestCase, parameterized.TestCase):\n         )\n         self.assertEqual(state.shape, initial_state.shape)\n \n+    def test_cloned_weight_names(self):\n+        inp = keras.Input([None, 3])\n+        rnn = keras.layers.SimpleRNN(units=3)\n+        model = keras.Model(inp, rnn(inp))\n+        clone = keras.models.clone_model(model)\n+        assert len(model.weights) == len(clone.weights)\n+        for a, b in zip(model.weights, clone.weights):\n+            print(a.name, b.name)\n+            assert a.name == b.name\n \n if __name__ == \"__main__\":\n     tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b7c47868923a8b3a5ed8e7d2aa0110b23ab79203", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 82 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -245,7 +245,6 @@ class SimpleRNNLayerTest(tf.test.TestCase, parameterized.TestCase):\n         clone = keras.models.clone_model(model)\n         assert len(model.weights) == len(clone.weights)\n         for a, b in zip(model.weights, clone.weights):\n-            print(a.name, b.name)\n             assert a.name == b.name\n \n if __name__ == \"__main__\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2b9d901113f47f5e1c1805e10ecbc622a14dcb22", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 46 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 47 | Churn Cumulative: 1771 | Contributors (this commit): 9 | Commits (past 90d): 4 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -445,9 +445,23 @@ class Bidirectional(Wrapper):\n             return [output] + states\n         return output\n \n-    def reset_states(self):\n+    def reset_states(self, states=None):\n+        if not self.stateful:\n+            raise AttributeError(\"Layer must be stateful.\")\n+\n+        if states is None:\n             self.forward_layer.reset_states()\n             self.backward_layer.reset_states()\n+        else:\n+            if not isinstance(states, (list, tuple)):\n+                raise ValueError(\"Unrecognized value for `states`. \"\n+                f\"Received: {states}\"\n+                \"Expected `states` to be list or tuple\"\n+            )\n+\n+            half = len(states) // 2\n+            self.forward_layer.reset_states(states[:half])\n+            self.backward_layer.reset_states(states[half:])\n \n     def build(self, input_shape):\n         with backend.name_scope(self.forward_layer.name):\n\n@@ -1020,6 +1020,37 @@ class BidirectionalTest(tf.test.TestCase, parameterized.TestCase):\n         self.assertAllClose(output1, output3)\n         self.assertNotAllClose(output1, output2)\n \n+    def test_reset_states(self):\n+        ref_state = np.random.rand(1, 3).astype(np.float32)\n+\n+        # build model\n+        inp = keras.Input(batch_shape=[1, 2, 3])\n+\n+        stateful = keras.layers.SimpleRNN(units=3, stateful=True)\n+        stateless = keras.layers.SimpleRNN(units=3, stateful=False)\n+\n+        bid_stateless = keras.layers.Bidirectional(stateless)\n+        bid_stateful = keras.layers.Bidirectional(stateful)\n+\n+        model = keras.Model(inp, [\n+            bid_stateless(inp),\n+            bid_stateful(inp),\n+        ])\n+\n+        try:\n+            bid_stateless.reset_states()\n+            assert False, \"Expected AttributeError\"\n+        except AttributeError:\n+            pass\n+\n+        try:\n+            bid_stateless.reset_states([])\n+            assert False, \"Expected AttributeError\"\n+        except AttributeError:\n+            pass\n+        \n+        bid_stateful.reset_states()\n+        bid_stateful.reset_states([ref_state, ref_state])\n \n def _to_list(ls):\n     if isinstance(ls, list):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#be7531c70654815db60755ec3d9620f46827d286", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 101 | Files Changed: 3 | Hunks: 14 | Methods Changed: 7 | Complexity Δ (Sum/Max): -9/0 | Churn Δ: 125 | Churn Cumulative: 1320 | Contributors (this commit): 33 | Commits (past 90d): 11 | Contributors (cumulative): 46 | DMM Complexity: 0.0\n\nDIFF:\n@@ -58,7 +58,6 @@ from keras.utils.layer_utils import get_source_inputs\n # Deprecated\n from keras.utils.np_utils import normalize\n from keras.utils.np_utils import to_categorical\n-from keras.utils.np_utils import to_ordinal\n from keras.utils.data_utils import pad_sequences\n \n # Evaluation related\n\n@@ -34,17 +34,19 @@ def to_categorical(y, num_classes=None, dtype=\"float32\"):\n         dtype: The data type expected by the input. Default: `'float32'`.\n \n     Returns:\n-        A binary matrix representation of the input as a NumPy array. The class\n-        axis is placed last.\n+        A binary matrix representation of the input. The class axis is placed\n+        last.\n \n     Example:\n \n     >>> a = tf.keras.utils.to_categorical([0, 1, 2, 3], num_classes=4)\n+    >>> a = tf.constant(a, shape=[4, 4])\n     >>> print(a)\n+    tf.Tensor(\n       [[1. 0. 0. 0.]\n        [0. 1. 0. 0.]\n        [0. 0. 1. 0.]\n-     [0. 0. 0. 1.]]\n+       [0. 0. 0. 1.]], shape=(4, 4), dtype=float32)\n \n     >>> b = tf.constant([.9, .04, .03, .03,\n     ...                  .3, .45, .15, .13,\n@@ -61,12 +63,9 @@ def to_categorical(y, num_classes=None, dtype=\"float32\"):\n     \"\"\"\n     y = np.array(y, dtype=\"int\")\n     input_shape = y.shape\n-\n-    # Shrink the last dimension if the shape is (..., 1).\n     if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n         input_shape = tuple(input_shape[:-1])\n-\n-    y = y.reshape(-1)\n+    y = y.ravel()\n     if not num_classes:\n         num_classes = np.max(y) + 1\n     n = y.shape[0]\n@@ -77,51 +76,6 @@ def to_categorical(y, num_classes=None, dtype=\"float32\"):\n     return categorical\n \n \n-@keras_export(\"keras.utils.to_ordinal\")\n-def to_ordinal(y, num_classes=None, dtype=\"float32\"):\n-    \"\"\"Converts a class vector (integers) to an ordinal class matrix for ordinal\n-        regression/classification.\n-\n-    Args:\n-        y: Array-like with class values to be converted into a matrix\n-            (integers from 0 to `num_classes - 1`).\n-        num_classes: Total number of classes. If `None`, this would be inferred\n-            as `max(y) + 1`.\n-        dtype: The data type expected by the input. Default: `'float32'`.\n-\n-    Returns:\n-        A ordinal regression matrix representation of the input as a NumPy\n-        array. The class axis is placed last.\n-\n-    Example:\n-\n-    >>> a = tf.keras.utils.to_ordinal([0, 1, 2, 3], num_classes=4)\n-    >>> print(a)\n-    [[0. 0. 0.]\n-     [1. 0. 0.]\n-     [1. 1. 0.]\n-     [1. 1. 1.]]\n-    \"\"\"\n-    y = np.array(y, dtype=\"int\")\n-    input_shape = y.shape\n-\n-    # Shrink the last dimension if the shape is (..., 1).\n-    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n-        input_shape = tuple(input_shape[:-1])\n-\n-    y = y.reshape(-1)\n-    if not num_classes:\n-        num_classes = np.max(y) + 1\n-    n = y.shape[0]\n-    range_values = np.arange(num_classes - 1)\n-    range_values = np.tile(np.expand_dims(range_values, 0), [n, 1])\n-    ordinal = np.zeros((n, num_classes - 1), dtype=dtype)\n-    ordinal[range_values < np.expand_dims(y, -1)] = 1\n-    output_shape = input_shape + (num_classes - 1,)\n-    ordinal = np.reshape(ordinal, output_shape)\n-    return ordinal\n-\n-\n @keras_export(\"keras.utils.normalize\")\n def normalize(x, axis=-1, order=2):\n     \"\"\"Normalizes a Numpy array.\n\n@@ -16,28 +16,29 @@\n \n import numpy as np\n import tensorflow.compat.v2 as tf\n-from absl.testing import parameterized\n \n-from keras.testing_infra import test_combinations\n from keras.utils import np_utils\n \n-NUM_CLASSES = 5\n \n-\n-class TestNPUtils(test_combinations.TestCase):\n-    @parameterized.parameters(\n-        [\n-            ((1,), (1, NUM_CLASSES)),\n-            ((3,), (3, NUM_CLASSES)),\n-            ((4, 3), (4, 3, NUM_CLASSES)),\n-            ((5, 4, 3), (5, 4, 3, NUM_CLASSES)),\n-            ((3, 1), (3, NUM_CLASSES)),\n-            ((3, 2, 1), (3, 2, NUM_CLASSES)),\n+class TestNPUtils(tf.test.TestCase):\n+    def test_to_categorical(self):\n+        num_classes = 5\n+        shapes = [(1,), (3,), (4, 3), (5, 4, 3), (3, 1), (3, 2, 1)]\n+        expected_shapes = [\n+            (1, num_classes),\n+            (3, num_classes),\n+            (4, 3, num_classes),\n+            (5, 4, 3, num_classes),\n+            (3, num_classes),\n+            (3, 2, num_classes),\n         ]\n-    )\n-    def test_to_categorical(self, shape, expected_shape):\n-        label = np.random.randint(0, NUM_CLASSES, shape)\n-        one_hot = np_utils.to_categorical(label, NUM_CLASSES)\n+        labels = [np.random.randint(0, num_classes, shape) for shape in shapes]\n+        one_hots = [\n+            np_utils.to_categorical(label, num_classes) for label in labels\n+        ]\n+        for label, one_hot, expected_shape in zip(\n+            labels, one_hots, expected_shapes\n+        ):\n             # Check shape\n             self.assertEqual(one_hot.shape, expected_shape)\n             # Make sure there is only one 1 in a row\n@@ -47,37 +48,6 @@ class TestNPUtils(test_combinations.TestCase):\n                 np.all(np.argmax(one_hot, -1).reshape(label.shape) == label)\n             )\n \n-    def test_to_categorial_without_num_classes(self):\n-        label = [0, 2, 5]\n-        one_hot = np_utils.to_categorical(label)\n-        self.assertEqual(one_hot.shape, (3, 5 + 1))\n-\n-    @parameterized.parameters(\n-        [\n-            ((1,), (1, NUM_CLASSES - 1)),\n-            ((3,), (3, NUM_CLASSES - 1)),\n-            ((4, 3), (4, 3, NUM_CLASSES - 1)),\n-            ((5, 4, 3), (5, 4, 3, NUM_CLASSES - 1)),\n-            ((3, 1), (3, NUM_CLASSES - 1)),\n-            ((3, 2, 1), (3, 2, NUM_CLASSES - 1)),\n-        ]\n-    )\n-    def test_to_ordinal(self, shape, expected_shape):\n-        label = np.random.randint(0, NUM_CLASSES, shape)\n-        ordinal = np_utils.to_ordinal(label, NUM_CLASSES)\n-        # Check shape\n-        self.assertEqual(ordinal.shape, expected_shape)\n-        # Make sure all the values are either 0 or 1\n-        self.assertTrue(np.all(np.logical_or(ordinal == 0, ordinal == 1)))\n-        # Get original labels back from ordinal matrix\n-        self.assertTrue(\n-            np.all(ordinal.cumprod(-1).sum(-1).reshape(label.shape) == label)\n-        )\n-\n-    def test_to_ordinal_without_num_classes(self):\n-        label = [0, 2, 5]\n-        one_hot = np_utils.to_ordinal(label)\n-        self.assertEqual(one_hot.shape, (3, 5))\n \n if __name__ == \"__main__\":\n     tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0a1d269304fa53814558c41151f293c20bffc1b9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 4434 | Contributors (this commit): 45 | Commits (past 90d): 5 | Contributors (cumulative): 45 | DMM Complexity: None\n\nDIFF:\n@@ -212,6 +212,21 @@ def get_file(\n \n     Returns:\n         Path to the downloaded file\n+\n+    **/!\\ Warning on malicious downloads /!\\ **\n+    Downloading something from the Internet carries a risk.\n+    NEVER download a file/archive if you do not trust the source.\n+    We recommend that you specify the `file_hash` argument\n+    (if the hash of the source file is known) to make sure that the file you\n+    are getting is the one you expect.\n+\n+    **/!\\ Warning on file extraction /!\\**\n+    Extracting a compressed archive carries a risk.\n+    NEVER extract archives from untrusted sources without prior inspection.\n+    If you set `extract=True`, and the archive is in `tar` format,\n+    it is possible that files will be created outside of the target `cache_dir`,\n+    e.g. archive members may have absolute filenames\n+    starting with `\"/\"` or filenames with two dots, `\"..\"`.\n     \"\"\"\n     if origin is None:\n         raise ValueError(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#af76a1a391bf32ef9fb03615654fd8d5875788c1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 497 | Lines Deleted: 1116 | Files Changed: 3 | Hunks: 156 | Methods Changed: 73 | Complexity Δ (Sum/Max): -56/0 | Churn Δ: 1613 | Churn Cumulative: 13540 | Contributors (this commit): 14 | Commits (past 90d): 4 | Contributors (cumulative): 30 | DMM Complexity: 0.0026455026455026454\n\nDIFF:\n@@ -19,7 +19,6 @@ import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n \n import keras\n-from keras.layers.preprocessing import image_preprocessing\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n \n@@ -643,12 +642,9 @@ class TestSequentialEagerIntegration(test_combinations.TestCase):\n         self.assertTrue(model.built)\n \n \n-class ImageAugmentLayer(image_preprocessing.BaseImageAugmentationLayer):\n-    def augment_image(self, image, transformation=None):\n-        return image\n-\n-    def augment_label(self, label, transformation=None):\n-        return label\n+class ImageAugmentLayer(keras.layers.Layer):\n+    def call(self, inputs):\n+        return inputs\n \n \n if __name__ == \"__main__\":\n\n@@ -14,9 +14,9 @@\n # ==============================================================================\n \"\"\"Keras image preprocessing layers.\"\"\"\n \n-\n import numpy as np\n import tensorflow.compat.v2 as tf\n+from tensorflow.python.util.tf_export import keras_export\n \n from keras import backend\n from keras.engine import base_layer\n@@ -25,19 +25,9 @@ from keras.layers.preprocessing import preprocessing_utils as utils\n from keras.utils import image_utils\n from keras.utils import tf_utils\n \n-# isort: off\n-from tensorflow.python.ops import stateless_random_ops\n-from tensorflow.python.util.tf_export import keras_export\n-from tensorflow.tools.docs import doc_controls\n-\n H_AXIS = -3\n W_AXIS = -2\n \n-IMAGES = \"images\"\n-LABELS = \"labels\"\n-TARGETS = \"targets\"\n-BOUNDING_BOXES = \"bounding_boxes\"\n-\n \n def check_fill_mode_and_interpolation(fill_mode, interpolation):\n     if fill_mode not in {\"reflect\", \"wrap\", \"constant\", \"nearest\"}:\n@@ -60,9 +50,9 @@ class Resizing(base_layer.Layer):\n \n     This layer resizes an image input to a target height and width. The input\n     should be a 4D (batched) or 3D (unbatched) tensor in `\"channels_last\"`\n-    format.  Input pixel values can be of any range (e.g. `[0., 1.)` or `[0,\n-    255]`) and of interger or floating point dtype. By default, the layer will\n-    output floats.\n+    format. Input pixel values can be of any range\n+    (e.g. `[0., 1.)` or `[0, 255]`) and of integer or floating point dtype.\n+    By default, the layer will output floats.\n \n     This layer can be called on tf.RaggedTensor batches of input images of\n     distinct sizes, and will resize the outputs to dense tensors of uniform\n@@ -74,14 +64,16 @@ class Resizing(base_layer.Layer):\n     Args:\n         height: Integer, the height of the output shape.\n         width: Integer, the width of the output shape.\n-      interpolation: String, the interpolation method. Defaults to `\"bilinear\"`.\n-        Supports `\"bilinear\"`, `\"nearest\"`, `\"bicubic\"`, `\"area\"`, `\"lanczos3\"`,\n-        `\"lanczos5\"`, `\"gaussian\"`, `\"mitchellcubic\"`.\n+        interpolation: String, the interpolation method.\n+            Defaults to `\"bilinear\"`.\n+            Supports `\"bilinear\"`, `\"nearest\"`, `\"bicubic\"`, `\"area\"`,\n+            `\"lanczos3\"`, `\"lanczos5\"`, `\"gaussian\"`, `\"mitchellcubic\"`.\n         crop_to_aspect_ratio: If True, resize the images without aspect\n-        ratio distortion. When the original aspect ratio differs from the target\n-        aspect ratio, the output image will be cropped so as to return the\n-        largest possible window in the image (of size `(height, width)`) that\n-        matches the target aspect ratio. By default\n+            ratio distortion. When the original aspect ratio differs\n+            from the target aspect ratio, the output image will be\n+            cropped so as to return the\n+            largest possible window in the image (of size `(height, width)`)\n+            that matches the target aspect ratio. By default\n             (`crop_to_aspect_ratio=False`), aspect ratio may not be preserved.\n     \"\"\"\n \n@@ -104,14 +96,15 @@ class Resizing(base_layer.Layer):\n         base_preprocessing_layer.keras_kpl_gauge.get_cell(\"Resizing\").set(True)\n \n     def call(self, inputs):\n-        # tf.image.resize will always output float32 and operate more\n-        # efficiently on float32 unless interpolation is nearest, in which case\n-        # ouput type matches input type.\n+        # tf.image.resize will always output float32\n+        # and operate more efficiently on float32\n+        # unless interpolation is nearest, in which case ouput type matches\n+        # input type.\n         if self.interpolation == \"nearest\":\n             input_dtype = self.compute_dtype\n         else:\n             input_dtype = tf.float32\n-        inputs = utils.ensure_tensor(inputs, dtype=input_dtype)\n+        inputs = convert_inputs(inputs, dtype=input_dtype)\n         size = [self.height, self.width]\n         if self.crop_to_aspect_ratio:\n \n@@ -162,13 +155,13 @@ class CenterCrop(base_layer.Layer):\n     \"\"\"A preprocessing layer which crops images.\n \n     This layers crops the central portion of the images to a target size. If an\n-    image is smaller than the target size, it will be resized and cropped so as\n-    to return the largest possible window in the image that matches the target\n-    aspect ratio.\n+    image is smaller than the target size, it will be resized and cropped\n+    so as to return the largest possible window in the image that matches\n+    the target aspect ratio.\n \n     Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and\n-    of interger or floating point dtype. By default, the layer will output\n-    floats.\n+    of integer or floating point dtype.\n+    By default, the layer will output floats.\n \n     For an overview and full list of preprocessing layers, see the preprocessing\n     [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n@@ -198,7 +191,7 @@ class CenterCrop(base_layer.Layer):\n         )\n \n     def call(self, inputs):\n-        inputs = utils.ensure_tensor(inputs, self.compute_dtype)\n+        inputs = convert_inputs(inputs, self.compute_dtype)\n         input_shape = tf.shape(inputs)\n         h_diff = input_shape[H_AXIS] - self.height\n         w_diff = input_shape[W_AXIS] - self.width\n@@ -236,290 +229,12 @@ class CenterCrop(base_layer.Layer):\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n-@keras_export(\"keras.__internal__.layers.BaseImageAugmentationLayer\")\n-class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n-    \"\"\"Abstract base layer for image augmentation.\n-\n-    This layer contains base functionalities for preprocessing layers which\n-    augment image related data, eg. image and in future, label and bounding\n-    boxes.  The subclasses could avoid making certain mistakes and reduce code\n-    duplications.\n-\n-    This layer requires you to implement one method: `augment_image()`, which\n-    augments one single image during the training. There are a few additional\n-    methods that you can implement for added functionality on the layer:\n-\n-    `augment_label()`, which handles label augmentation if the layer supports\n-    that.\n-\n-    `augment_bounding_boxes()` is not implemented by this layer. Please use\n-    preprocessing layers in [KerasCV](https://keras.io/keras_cv/)\n-    for bounding box augmentation support.\n-\n-    `get_random_transformation()`, which should produce a random transformation\n-    setting. The tranformation object, which could be any type, will be passed\n-    to `augment_image`, `augment_label` and `augment_bounding_boxes`, to\n-    coodinate the randomness behavior, eg, in the RandomFlip layer, the image\n-    and bounding_boxes should be changed in the same way.\n-\n-    The `call()` method support two formats of inputs:\n-    1. Single image tensor with 3D (HWC) or 4D (NHWC) format.\n-    2. A dict of tensors with stable keys. The supported keys are:\n-      `\"images\"`, `\"labels\"` and `\"bounding_boxes\"` at the moment. We might add\n-      more keys in future when we support more types of augmentation.\n-\n-    The output of the `call()` will be in two formats, which will be the same\n-    structure as the inputs.\n-\n-    The `call()` will handle the logic detecting the training/inference mode,\n-    unpack the inputs, forward to the correct function, and pack the output back\n-    to the same structure as the inputs.\n-\n-    By default the `call()` method leverages the `tf.vectorized_map()` function.\n-    Auto-vectorization can be disabled by setting `self.auto_vectorize = False`\n-    in your `__init__()` method.  When disabled, `call()` instead relies\n-    on `tf.map_fn()`. For example:\n-\n-    ```python\n-    class SubclassLayer(BaseImageAugmentationLayer):\n-      def __init__(self):\n-        super().__init__()\n-        self.auto_vectorize = False\n-    ```\n-\n-    Example:\n-\n-    ```python\n-    class RandomContrast(BaseImageAugmentationLayer):\n-\n-      def __init__(self, factor=(0.5, 1.5), **kwargs):\n-        super().__init__(**kwargs)\n-        self._factor = factor\n-\n-      def augment_image(self, image, transformation):\n-        random_factor = tf.random.uniform([], self._factor[0], self._factor[1])\n-        mean = tf.math.reduced_mean(inputs, axis=-1, keep_dim=True)\n-        return (inputs - mean) * random_factor + mean\n-    ```\n-\n-    Note that since the randomness is also a common functionnality, this layer\n-    also includes a tf.keras.backend.RandomGenerator, which can be used to\n-    produce the random numbers.  The random number generator is stored in the\n-    `self._random_generator` attribute.\n-    \"\"\"\n-\n-    def __init__(self, rate=1.0, seed=None, **kwargs):\n-        super().__init__(seed=seed, **kwargs)\n-        self.rate = rate\n-\n-    @property\n-    def auto_vectorize(self):\n-        \"\"\"Control whether automatic vectorization occurs.\n-\n-        By default the `call()` method leverages the `tf.vectorized_map()`\n-        function.  Auto-vectorization can be disabled by setting\n-        `self.auto_vectorize = False` in your `__init__()` method.  When\n-        disabled, `call()` instead relies on `tf.map_fn()`. For example:\n-\n-        ```python\n-        class SubclassLayer(BaseImageAugmentationLayer):\n-          def __init__(self):\n-            super().__init__()\n-            self.auto_vectorize = False\n-        ```\n-        \"\"\"\n-        return getattr(self, \"_auto_vectorize\", True)\n-\n-    @auto_vectorize.setter\n-    def auto_vectorize(self, auto_vectorize):\n-        self._auto_vectorize = auto_vectorize\n-\n-    @property\n-    def _map_fn(self):\n-        if self.auto_vectorize:\n-            return lambda fn, x: tf.vectorized_map(fn, x, warn=False)\n-        else:\n-            return tf.map_fn\n-\n-    @doc_controls.for_subclass_implementers\n-    def augment_image(self, image, transformation):\n-        \"\"\"Augment a single image during training.\n-\n-        Args:\n-          image: 3D image input tensor to the layer. Forwarded from\n-            `layer.call()`.\n-          transformation: The transformation object produced by\n-            `get_random_transformation`. Used to coordinate the randomness\n-            between image, label and bounding box.\n-\n-        Returns:\n-          output 3D tensor, which will be forward to `layer.call()`.\n-        \"\"\"\n-        raise NotImplementedError()\n-\n-    @doc_controls.for_subclass_implementers\n-    def augment_label(self, label, transformation):\n-        \"\"\"Augment a single label during training.\n-\n-        Args:\n-          label: 1D label to the layer. Forwarded from `layer.call()`.\n-          transformation: The transformation object produced by\n-            `get_random_transformation`. Used to coordinate the randomness\n-            between image, label and bounding box.\n-\n-        Returns:\n-          output 1D tensor, which will be forward to `layer.call()`.\n-        \"\"\"\n-        raise NotImplementedError()\n-\n-    @doc_controls.for_subclass_implementers\n-    def augment_target(self, target, transformation):\n-        \"\"\"Augment a single target during training.\n-\n-        Args:\n-          target: 1D label to the layer. Forwarded from `layer.call()`.\n-          transformation: The transformation object produced by\n-            `get_random_transformation`. Used to coordinate the randomness\n-            between image, label and bounding box.\n-\n-        Returns:\n-          output 1D tensor, which will be forward to `layer.call()`.\n-        \"\"\"\n-        return self.augment_label(target, transformation)\n-\n-    @doc_controls.for_subclass_implementers\n-    def augment_bounding_boxes(\n-        self, image, bounding_boxes, transformation=None\n-    ):\n-        \"\"\"Augment bounding boxes for one image during training.\n-\n-        Args:\n-          image: 3D image input tensor to the layer. Forwarded from\n-            `layer.call()`.\n-          bounding_boxes: 2D bounding boxes to the layer. Forwarded from\n-            `call()`.\n-          transformation: The transformation object produced by\n-            `get_random_transformation`. Used to coordinate the randomness\n-            between image, label and bounding box.\n-\n-        Returns:\n-          output 2D tensor, which will be forward to `layer.call()`.\n-        \"\"\"\n-        layer = self.__class__.__name__\n-        raise NotImplementedError(\n-            \"In order to use bounding_boxes, \"\n-            \"please use \"\n-            f\"keras_cv.layers.{layer} \"\n-            f\"instead of keras.layers.{layer}.\"\n-        )\n-\n-    @doc_controls.for_subclass_implementers\n-    def get_random_transformation(\n-        self, image=None, label=None, bounding_box=None\n-    ):\n-        \"\"\"Produce random transformation config for one single input.\n-\n-        This is used to produce same randomness between\n-        image/label/bounding_box.\n-\n-        Args:\n-          image: 3D image tensor from inputs.\n-          label: optional 1D label tensor from inputs.\n-          bounding_box: optional 2D bounding boxes tensor from inputs.\n-\n-        Returns:\n-          Any type of object, which will be forwarded to `augment_image`,\n-          `augment_label` and `augment_bounding_box` as the `transformation`\n-          parameter.\n-        \"\"\"\n-        return None\n-\n-    def call(self, inputs, training=True):\n-        inputs = self._ensure_inputs_are_compute_dtype(inputs)\n-        if training:\n-            inputs, is_dict, use_targets = self._format_inputs(inputs)\n-            images = inputs[IMAGES]\n-            if images.shape.rank == 3:\n-                return self._format_output(\n-                    self._augment(inputs), is_dict, use_targets\n-                )\n-            elif images.shape.rank == 4:\n-                return self._format_output(\n-                    self._batch_augment(inputs), is_dict, use_targets\n-                )\n-            else:\n-                raise ValueError(\n-                    \"Image augmentation layers are expecting inputs to be \"\n-                    \"rank 3 (HWC) or 4D (NHWC) tensors. Got shape: \"\n-                    f\"{images.shape}\"\n-                )\n-        else:\n-            return inputs\n-\n-    def _augment(self, inputs):\n-        image = inputs.get(IMAGES, None)\n-        label = inputs.get(LABELS, None)\n-        bounding_box = inputs.get(BOUNDING_BOXES, None)\n-        transformation = self.get_random_transformation(\n-            image=image, label=label, bounding_box=bounding_box\n-        )\n-        image = self.augment_image(image, transformation=transformation)\n-        result = {IMAGES: image}\n-        if label is not None:\n-            label = self.augment_target(label, transformation=transformation)\n-            result[LABELS] = label\n-        if bounding_box is not None:\n-            bounding_box = self.augment_bounding_boxes(\n-                image, bounding_box, transformation=transformation\n-            )\n-            result[BOUNDING_BOXES] = bounding_box\n-        return result\n-\n-    def _batch_augment(self, inputs):\n-        return self._map_fn(self._augment, inputs)\n-\n-    def _format_inputs(self, inputs):\n-        if tf.is_tensor(inputs):\n-            # single image input tensor\n-            return {IMAGES: inputs}, False, False\n-        elif isinstance(inputs, dict) and TARGETS in inputs:\n-            # TODO(scottzhu): Check if it only contains the valid keys\n-            inputs[LABELS] = inputs[TARGETS]\n-            del inputs[TARGETS]\n-            return inputs, True, True\n-        elif isinstance(inputs, dict):\n-            return inputs, True, False\n-        else:\n-            raise ValueError(\n-                f\"Expect the inputs to be image tensor or dict. Got {inputs}\"\n-            )\n-\n-    def _format_output(self, output, is_dict, use_targets):\n-        if not is_dict:\n-            return output[IMAGES]\n-        elif use_targets:\n-            output[TARGETS] = output[LABELS]\n-            del output[LABELS]\n-            return output\n-        else:\n-            return output\n-\n-    def _ensure_inputs_are_compute_dtype(self, inputs):\n-        if isinstance(inputs, dict):\n-            inputs[IMAGES] = utils.ensure_tensor(\n-                inputs[IMAGES], self.compute_dtype\n-            )\n-        else:\n-            inputs = utils.ensure_tensor(inputs, self.compute_dtype)\n-        return inputs\n-\n-\n @keras_export(\n     \"keras.layers.RandomCrop\",\n     \"keras.layers.experimental.preprocessing.RandomCrop\",\n     v1=[],\n )\n-class RandomCrop(BaseImageAugmentationLayer):\n+class RandomCrop(base_layer.BaseRandomLayer):\n     \"\"\"A preprocessing layer which randomly crops images during training.\n \n     During training, this layer will randomly choose a location to crop images\n@@ -533,7 +248,7 @@ class RandomCrop(BaseImageAugmentationLayer):\n     True when calling the layer.\n \n     Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and\n-    of interger or floating point dtype. By default, the layer will output\n+    of integer or floating point dtype. By default, the layer will output\n     floats.\n \n     For an overview and full list of preprocessing layers, see the preprocessing\n@@ -565,52 +280,35 @@ class RandomCrop(BaseImageAugmentationLayer):\n         self.seed = seed\n \n     def call(self, inputs, training=True):\n-\n-        if training:\n-            return super().call(inputs, training)\n-        else:\n-            inputs = self._ensure_inputs_are_compute_dtype(inputs)\n-            inputs, is_dict, targets = self._format_inputs(inputs)\n-            output = inputs\n-            # self._resize() returns valid results for both batched and\n-            # unbatched\n-            output[\"images\"] = self._resize(inputs[\"images\"])\n-            return self._format_output(output, is_dict, targets)\n-\n-    def get_random_transformation(\n-        self, image=None, label=None, bounding_box=None\n-    ):\n-        input_shape = tf.shape(image)\n+        inputs = convert_inputs(inputs, dtype=self.compute_dtype)\n+        input_shape = tf.shape(inputs)\n         h_diff = input_shape[H_AXIS] - self.height\n         w_diff = input_shape[W_AXIS] - self.width\n+\n+        def random_crop():\n             dtype = input_shape.dtype\n-        rands = self._random_generator.random_uniform([2], 0, dtype.max, dtype)\n+            rands = self._random_generator.random_uniform(\n+                [2], 0, dtype.max, dtype\n+            )\n             h_start = rands[0] % (h_diff + 1)\n             w_start = rands[1] % (w_diff + 1)\n-        return {\"top\": h_start, \"left\": w_start}\n-\n-    def augment_image(self, image, transformation):\n-        input_shape = tf.shape(image)\n-        h_diff = input_shape[H_AXIS] - self.height\n-        w_diff = input_shape[W_AXIS] - self.width\n-        return tf.cond(\n-            tf.reduce_all((h_diff >= 0, w_diff >= 0)),\n-            lambda: self._crop(image, transformation),\n-            lambda: self._resize(image),\n-        )\n-\n-    def _crop(self, image, transformation):\n-        top = transformation[\"top\"]\n-        left = transformation[\"left\"]\n             return tf.image.crop_to_bounding_box(\n-            image, top, left, self.height, self.width\n+                inputs, h_start, w_start, self.height, self.width\n             )\n \n-    def _resize(self, image):\n-        outputs = image_utils.smart_resize(image, [self.height, self.width])\n+        def resize():\n+            outputs = image_utils.smart_resize(\n+                inputs, [self.height, self.width]\n+            )\n             # smart_resize will always output float32, so we need to re-cast.\n             return tf.cast(outputs, self.compute_dtype)\n \n+        return tf.cond(\n+            tf.reduce_all((training, h_diff >= 0, w_diff >= 0)),\n+            random_crop,\n+            resize,\n+        )\n+\n     def compute_output_shape(self, input_shape):\n         input_shape = tf.TensorShape(input_shape).as_list()\n         input_shape[H_AXIS] = self.height\n@@ -639,11 +337,11 @@ class Rescaling(base_layer.Layer):\n \n     For instance:\n \n-    1. To rescale an input in the ``[0, 255]`` range\n+    1. To rescale an input in the `[0, 255]` range\n     to be in the `[0, 1]` range, you would pass `scale=1./255`.\n \n-    2. To rescale an input in the ``[0, 255]`` range to be in the `[-1, 1]`\n-    range, you would pass `scale=1./127.5, offset=-1`.\n+    2. To rescale an input in the `[0, 255]` range to be in the `[-1, 1]` range,\n+    you would pass `scale=1./127.5, offset=-1`.\n \n     The rescaling is applied both during training and inference. Inputs can be\n     of integer or floating point dtype, and by default the layer will output\n@@ -671,6 +369,7 @@ class Rescaling(base_layer.Layer):\n \n     def call(self, inputs):\n         dtype = self.compute_dtype\n+        inputs = convert_inputs(inputs, dtype=dtype)\n         scale = tf.cast(self.scale, dtype)\n         offset = tf.cast(self.offset, dtype)\n         return tf.cast(inputs, dtype) * scale + offset\n@@ -697,7 +396,7 @@ HORIZONTAL_AND_VERTICAL = \"horizontal_and_vertical\"\n     \"keras.layers.experimental.preprocessing.RandomFlip\",\n     v1=[],\n )\n-class RandomFlip(BaseImageAugmentationLayer):\n+class RandomFlip(base_layer.BaseRandomLayer):\n     \"\"\"A preprocessing layer which randomly flips images during training.\n \n     This layer will flip the images horizontally and or vertically based on the\n@@ -705,8 +404,8 @@ class RandomFlip(BaseImageAugmentationLayer):\n     input. Call the layer with `training=True` to flip the input.\n \n     Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and\n-    of interger or floating point dtype. By default, the layer will output\n-    floats.\n+    of integer or floating point dtype.\n+    By default, the layer will output floats.\n \n     For an overview and full list of preprocessing layers, see the preprocessing\n     [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n@@ -719,7 +418,7 @@ class RandomFlip(BaseImageAugmentationLayer):\n         3D (unbatched) or 4D (batched) tensor with shape:\n         `(..., height, width, channels)`, in `\"channels_last\"` format.\n \n-    Arguments:\n+    Args:\n         mode: String indicating which flip mode to use. Can be `\"horizontal\"`,\n             `\"vertical\"`, or `\"horizontal_and_vertical\"`. Defaults to\n             `\"horizontal_and_vertical\"`. `\"horizontal\"` is a left-right flip and\n@@ -747,33 +446,42 @@ class RandomFlip(BaseImageAugmentationLayer):\n                 f\"RandomFlip layer {self.name} received an unknown mode \"\n                 f\"argument {mode}\"\n             )\n-        self.auto_vectorize = False\n+        self.seed = seed\n \n-    def augment_label(self, label, transformation):\n-        return label\n+    def call(self, inputs, training=True):\n+        inputs = convert_inputs(inputs, self.compute_dtype)\n \n-    def augment_image(self, image, transformation):\n-        flipped_outputs = image\n-        if self.horizontal and transformation[\"flip_horizontal\"]:\n-            flipped_outputs = tf.image.flip_left_right(flipped_outputs)\n-        if self.vertical and transformation[\"flip_vertical\"]:\n-            flipped_outputs = tf.image.flip_up_down(flipped_outputs)\n-        flipped_outputs.set_shape(image.shape)\n+        def random_flipped_inputs(inputs):\n+            flipped_outputs = inputs\n+            if self.horizontal:\n+                seed = self._random_generator.make_seed_for_stateless_op()\n+                if seed is not None:\n+                    flipped_outputs = tf.image.stateless_random_flip_left_right(\n+                        flipped_outputs, seed=seed\n+                    )\n+                else:\n+                    flipped_outputs = tf.image.random_flip_left_right(\n+                        flipped_outputs,\n+                        self._random_generator.make_legacy_seed(),\n+                    )\n+            if self.vertical:\n+                seed = self._random_generator.make_seed_for_stateless_op()\n+                if seed is not None:\n+                    flipped_outputs = tf.image.stateless_random_flip_up_down(\n+                        flipped_outputs, seed=seed\n+                    )\n+                else:\n+                    flipped_outputs = tf.image.random_flip_up_down(\n+                        flipped_outputs,\n+                        self._random_generator.make_legacy_seed(),\n+                    )\n+            flipped_outputs.set_shape(inputs.shape)\n             return flipped_outputs\n \n-    def get_random_transformation(\n-        self, image=None, label=None, bounding_box=None\n-    ):\n-        flip_horizontal = False\n-        flip_vertical = False\n-        if self.horizontal:\n-            flip_horizontal = np.random.choice([True, False])\n-        if self.vertical:\n-            flip_vertical = np.random.choice([True, False])\n-        return {\n-            \"flip_horizontal\": flip_horizontal,\n-            \"flip_vertical\": flip_vertical,\n-        }\n+        if training:\n+            return random_flipped_inputs(inputs)\n+        else:\n+            return inputs\n \n     def compute_output_shape(self, input_shape):\n         return input_shape\n@@ -781,6 +489,7 @@ class RandomFlip(BaseImageAugmentationLayer):\n     def get_config(self):\n         config = {\n             \"mode\": self.mode,\n+            \"seed\": self.seed,\n         }\n         base_config = super().get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n@@ -792,14 +501,14 @@ class RandomFlip(BaseImageAugmentationLayer):\n     \"keras.layers.experimental.preprocessing.RandomTranslation\",\n     v1=[],\n )\n-class RandomTranslation(BaseImageAugmentationLayer):\n+class RandomTranslation(base_layer.BaseRandomLayer):\n     \"\"\"A preprocessing layer which randomly translates images during training.\n \n     This layer will apply random translations to each image during training,\n     filling empty space according to `fill_mode`.\n \n     Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and\n-    of interger or floating point dtype. By default, the layer will output\n+    of integer or floating point dtype. By default, the layer will output\n     floats.\n \n     For an overview and full list of preprocessing layers, see the preprocessing\n@@ -817,17 +526,19 @@ class RandomTranslation(BaseImageAugmentationLayer):\n       width_factor: a float represented as fraction of value, or a tuple of size\n           2 representing lower and upper bound for shifting horizontally. A\n           negative value means shifting image left, while a positive value means\n-        shifting image right. When represented as a single positive float, this\n-        value is used for both the upper and lower bound. For instance,\n-        `width_factor=(-0.2, 0.3)` results in an output shifted left by 20%, and\n-        shifted right by 30%. `width_factor=0.2` results in an output height\n-        shifted left or right by 20%.\n+          shifting image right. When represented as a single positive float,\n+          this value is used for both the upper and lower bound. For instance,\n+          `width_factor=(-0.2, 0.3)` results in an output shifted left by 20%,\n+          and shifted right by 30%. `width_factor=0.2` results\n+          in an output height shifted left or right by 20%.\n       fill_mode: Points outside the boundaries of the input are filled according\n-        to the given mode (one of `{\"constant\", \"reflect\", \"wrap\", \"nearest\"}`).\n+          to the given mode\n+          (one of `{\"constant\", \"reflect\", \"wrap\", \"nearest\"}`).\n           - *reflect*: `(d c b a | a b c d | d c b a)` The input is extended by\n               reflecting about the edge of the last pixel.\n           - *constant*: `(k k k k | a b c d | k k k k)` The input is extended by\n-          filling all values beyond the edge with the same constant value k = 0.\n+              filling all values beyond the edge with the same constant value\n+              k = 0.\n           - *wrap*: `(a b c d | a b c d | a b c d)` The input is extended by\n               wrapping around to the opposite edge.\n           - *nearest*: `(a a a a | a b c d | d d d d)` The input is extended by\n@@ -875,8 +586,8 @@ class RandomTranslation(BaseImageAugmentationLayer):\n             )\n         if abs(self.height_lower) > 1.0 or abs(self.height_upper) > 1.0:\n             raise ValueError(\n-                \"`height_factor` must have values between [-1, 1], \"\n-                f\"got {height_factor}\"\n+                \"`height_factor` argument must have values between [-1, 1]. \"\n+                f\"Received: height_factor={height_factor}\"\n             )\n \n         self.width_factor = width_factor\n@@ -904,23 +615,39 @@ class RandomTranslation(BaseImageAugmentationLayer):\n         self.interpolation = interpolation\n         self.seed = seed\n \n-    @tf.function\n-    def augment_image(self, image, transformation):\n+    def call(self, inputs, training=True):\n+        inputs = convert_inputs(inputs, self.compute_dtype)\n+\n+        def random_translated_inputs(inputs):\n             \"\"\"Translated inputs with random ops.\"\"\"\n-        # The transform op only accepts rank 4 inputs, so if we have an\n-        # unbatched image, we need to temporarily expand dims to a batch.\n-        original_shape = image.shape\n-        inputs = tf.expand_dims(image, 0)\n+            # The transform op only accepts rank 4 inputs,\n+            # so if we have an unbatched image,\n+            # we need to temporarily expand dims to a batch.\n+            original_shape = inputs.shape\n+            unbatched = inputs.shape.rank == 3\n+            if unbatched:\n+                inputs = tf.expand_dims(inputs, 0)\n \n             inputs_shape = tf.shape(inputs)\n+            batch_size = inputs_shape[0]\n             img_hd = tf.cast(inputs_shape[H_AXIS], tf.float32)\n             img_wd = tf.cast(inputs_shape[W_AXIS], tf.float32)\n-        height_translation = transformation[\"height_translation\"]\n-        width_translation = transformation[\"width_translation\"]\n-        height_translation = height_translation * img_hd\n-        width_translation = width_translation * img_wd\n+            height_translate = self._random_generator.random_uniform(\n+                shape=[batch_size, 1],\n+                minval=self.height_lower,\n+                maxval=self.height_upper,\n+                dtype=tf.float32,\n+            )\n+            height_translate = height_translate * img_hd\n+            width_translate = self._random_generator.random_uniform(\n+                shape=[batch_size, 1],\n+                minval=self.width_lower,\n+                maxval=self.width_upper,\n+                dtype=tf.float32,\n+            )\n+            width_translate = width_translate * img_wd\n             translations = tf.cast(\n-            tf.concat([width_translation, height_translation], axis=1),\n+                tf.concat([width_translate, height_translate], axis=1),\n                 dtype=tf.float32,\n             )\n             output = transform(\n@@ -930,40 +657,15 @@ class RandomTranslation(BaseImageAugmentationLayer):\n                 fill_mode=self.fill_mode,\n                 fill_value=self.fill_value,\n             )\n-\n+            if unbatched:\n                 output = tf.squeeze(output, 0)\n             output.set_shape(original_shape)\n             return output\n \n-    def get_random_transformation(\n-        self, image=None, label=None, bounding_box=None\n-    ):\n-        del image, label, bounding_box\n-        batch_size = 1\n-        height_translation = self._random_generator.random_uniform(\n-            shape=[batch_size, 1],\n-            minval=self.height_lower,\n-            maxval=self.height_upper,\n-            dtype=tf.float32,\n-        )\n-        width_translation = self._random_generator.random_uniform(\n-            shape=[batch_size, 1],\n-            minval=self.width_lower,\n-            maxval=self.width_upper,\n-            dtype=tf.float32,\n-        )\n-        return {\n-            \"height_translation\": height_translation,\n-            \"width_translation\": width_translation,\n-        }\n-\n-    def _batch_augment(self, inputs):\n-        # Change to vectorized_map for better performance, as well as work\n-        # around issue for different tensorspec between inputs and outputs.\n-        return tf.vectorized_map(self._augment, inputs)\n-\n-    def augment_label(self, label, transformation):\n-        return label\n+        if training:\n+            return random_translated_inputs(inputs)\n+        else:\n+            return inputs\n \n     def compute_output_shape(self, input_shape):\n         return input_shape\n@@ -990,8 +692,8 @@ def get_translation_matrix(translations, name=None):\n         name: The name of the op.\n \n     Returns:\n-      A tensor of shape `(num_images, 8)` projective transforms which can be\n-        given to `transform`.\n+        A tensor of shape `(num_images, 8)` projective transforms\n+            which can be given to `transform`.\n     \"\"\"\n     with backend.name_scope(name or \"translation_matrix\"):\n         num_translations = tf.shape(translations)[0]\n@@ -1028,20 +730,24 @@ def transform(\n \n     Args:\n         images: A tensor of shape\n-        `(num_images, num_rows, num_columns, num_channels)` (NHWC). The rank\n-        must be statically known (the shape is not `TensorShape(None)`).\n-      transforms: Projective transform matrix/matrices. A vector of length 8 or\n-        tensor of size N x 8. If one row of transforms is [a0, a1, a2, b0, b1,\n-        b2, c0, c1], then it maps the *output* point `(x, y)` to a transformed\n-        *input* point\n-        `(x', y') = ((a0 x + a1 y + a2) / k, (b0 x + b1 y + b2) / k)`,\n-        where `k = c0 x + c1 y + 1`. The transforms are *inverted* compared\n-        to the transform mapping input points to output points. Note that\n-        gradients are not backpropagated into transformation parameters.\n-      fill_mode: Points outside the boundaries of the input are filled according\n-        to the given mode (one of `{\"constant\", \"reflect\", \"wrap\", \"nearest\"}`).\n-      fill_value: a float represents the value to be filled outside the\n-        boundaries when `fill_mode=\"constant\"`.\n+            `(num_images, num_rows, num_columns, num_channels)` (NHWC).\n+            The rank must be statically known\n+            (the shape is not `TensorShape(None)`).\n+        transforms: Projective transform matrix/matrices.\n+            A vector of length 8 or tensor of size N x 8.\n+            If one row of transforms is [a0, a1, a2, b0, b1, b2,\n+            c0, c1], then it maps the *output* point `(x, y)`\n+            to a transformed *input* point\n+            `(x', y') = ((a0 x + a1 y + a2) / k, (b0 x + b1 y + b2) / k)`, where\n+            `k = c0 x + c1 y + 1`. The transforms are *inverted* compared to the\n+            transform mapping input points to output points.\n+            Note that gradients are not backpropagated\n+            into transformation parameters.\n+        fill_mode: Points outside the boundaries of the input are filled\n+            according to the given mode\n+            (one of `{\"constant\", \"reflect\", \"wrap\", \"nearest\"}`).\n+        fill_value: a float represents the value to be filled outside\n+            the boundaries when `fill_mode=\"constant\"`.\n         interpolation: Interpolation mode. Supported values: `\"nearest\"`,\n             `\"bilinear\"`.\n         output_shape: Output dimension after the transform, `[height, width]`.\n@@ -1050,17 +756,17 @@ def transform(\n \n     Fill mode behavior for each valid value is as follows:\n \n-    - reflect (d c b a | a b c d | d c b a)\n+    - `\"reflect\"`: `(d c b a | a b c d | d c b a)`\n     The input is extended by reflecting about the edge of the last pixel.\n \n-    - constant (k k k k | a b c d | k k k k)\n+    - `\"constant\"`: `(k k k k | a b c d | k k k k)`\n     The input is extended by filling all\n     values beyond the edge with the same constant value k = 0.\n \n-    - wrap (a b c d | a b c d | a b c d)\n+    - `\"wrap\"`: `(a b c d | a b c d | a b c d)`\n     The input is extended by wrapping around to the opposite edge.\n \n-    - nearest (a a a a | a b c d | d d d d)\n+    - `\"nearest\"`: `(a a a a | a b c d | d d d d)`\n     The input is extended by the nearest pixel.\n \n     Input shape:\n@@ -1075,10 +781,6 @@ def transform(\n         Image(s) with the same type and shape as `images`, with the given\n         transform(s) applied. Transformed coordinates outside of the input image\n         will be filled with zeros.\n-\n-    Raises:\n-      TypeError: If `image` is an invalid type.\n-      ValueError: If output shape is not 1-D int32 Tensor.\n     \"\"\"\n     with backend.name_scope(name or \"transform\"):\n         if output_shape is None:\n@@ -1096,7 +798,7 @@ def transform(\n             raise ValueError(\n                 \"output_shape must be a 1-D Tensor of 2 elements: \"\n                 \"new_height, new_width, instead got \"\n-                f\"{output_shape}\"\n+                f\"output_shape={output_shape}\"\n             )\n \n         fill_value = tf.convert_to_tensor(\n@@ -1117,18 +819,21 @@ def get_rotation_matrix(angles, image_height, image_width, name=None):\n     \"\"\"Returns projective transform(s) for the given angle(s).\n \n     Args:\n-      angles: A scalar angle to rotate all images by, or (for batches of images)\n-        a vector with an angle to rotate each image in the batch. The rank must\n-        be statically known (the shape is not `TensorShape(None)`).\n+        angles: A scalar angle to rotate all images by,\n+            or (for batches of images) a vector with an angle to\n+            rotate each image in the batch. The rank must be\n+            statically known (the shape is not `TensorShape(None)`).\n         image_height: Height of the image(s) to be transformed.\n         image_width: Width of the image(s) to be transformed.\n         name: The name of the op.\n \n     Returns:\n-      A tensor of shape (num_images, 8). Projective transforms which can be\n-        given to operation `image_projective_transform_v2`. If one row of\n-        transforms is [a0, a1, a2, b0, b1, b2, c0, c1], then it maps the\n-        *output* point `(x, y)` to a transformed *input* point\n+        A tensor of shape (num_images, 8).\n+            Projective transforms which can be given\n+            to operation `image_projective_transform_v2`.\n+            If one row of transforms is\n+            [a0, a1, a2, b0, b1, b2, c0, c1], then it maps the *output* point\n+            `(x, y)` to a transformed *input* point\n             `(x', y') = ((a0 x + a1 y + a2) / k, (b0 x + b1 y + b2) / k)`,\n             where `k = c0 x + c1 y + 1`.\n     \"\"\"\n@@ -1167,7 +872,7 @@ def get_rotation_matrix(angles, image_height, image_width, name=None):\n     \"keras.layers.experimental.preprocessing.RandomRotation\",\n     v1=[],\n )\n-class RandomRotation(BaseImageAugmentationLayer):\n+class RandomRotation(base_layer.BaseRandomLayer):\n     \"\"\"A preprocessing layer which randomly rotates images during training.\n \n     This layer will apply random rotations to each image, filling empty space\n@@ -1178,8 +883,8 @@ class RandomRotation(BaseImageAugmentationLayer):\n     rotations at inference time, set `training` to True when calling the layer.\n \n     Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and\n-    of interger or floating point dtype. By default, the layer will output\n-    floats.\n+    of integer or floating point dtype.\n+    By default, the layer will output floats.\n \n     For an overview and full list of preprocessing layers, see the preprocessing\n     [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n@@ -1192,31 +897,39 @@ class RandomRotation(BaseImageAugmentationLayer):\n         3D (unbatched) or 4D (batched) tensor with shape:\n         `(..., height, width, channels)`, in `\"channels_last\"` format\n \n-    Arguments:\n+    Args:\n         factor: a float represented as fraction of 2 Pi, or a tuple of size 2\n             representing lower and upper bound for rotating clockwise and\n-        counter-clockwise. A positive values means rotating counter clock-wise,\n-        while a negative value means clock-wise. When represented as a single\n-        float, this value is used for both the upper and lower bound. For\n-        instance, `factor=(-0.2, 0.3)` results in an output rotation by a random\n-        amount in the range `[-20% * 2pi, 30% * 2pi]`. `factor=0.2` results in\n-        an output rotating by a random amount in the range\n-        `[-20% * 2pi, 20% * 2pi]`.\n-      fill_mode: Points outside the boundaries of the input are filled according\n-        to the given mode (one of `{\"constant\", \"reflect\", \"wrap\", \"nearest\"}`).\n-        - *reflect*: `(d c b a | a b c d | d c b a)` The input is extended by\n-          reflecting about the edge of the last pixel.\n-        - *constant*: `(k k k k | a b c d | k k k k)` The input is extended by\n-          filling all values beyond the edge with the same constant value k = 0.\n+            counter-clockwise. A positive values means rotating\n+            counter clock-wise,\n+            while a negative value means clock-wise.\n+            When represented as a single\n+            float, this value is used for both the upper and lower bound.\n+            For instance, `factor=(-0.2, 0.3)`\n+            results in an output rotation by a random\n+            amount in the range `[-20% * 2pi, 30% * 2pi]`.\n+            `factor=0.2` results in an\n+            output rotating by a random amount\n+            in the range `[-20% * 2pi, 20% * 2pi]`.\n+        fill_mode: Points outside the boundaries of the input are filled\n+            according to the given mode\n+            (one of `{\"constant\", \"reflect\", \"wrap\", \"nearest\"}`).\n+            - *reflect*: `(d c b a | a b c d | d c b a)`\n+                The input is extended by reflecting about\n+                the edge of the last pixel.\n+            - *constant*: `(k k k k | a b c d | k k k k)`\n+                The input is extended by\n+                filling all values beyond the edge with\n+                the same constant value k = 0.\n             - *wrap*: `(a b c d | a b c d | a b c d)` The input is extended by\n                 wrapping around to the opposite edge.\n-        - *nearest*: `(a a a a | a b c d | d d d d)` The input is extended by\n-          the nearest pixel.\n+            - *nearest*: `(a a a a | a b c d | d d d d)`\n+                The input is extended by the nearest pixel.\n         interpolation: Interpolation mode. Supported values: `\"nearest\"`,\n             `\"bilinear\"`.\n         seed: Integer. Used to create a random seed.\n-      fill_value: a float represents the value to be filled outside the\n-        boundaries when `fill_mode=\"constant\"`.\n+        fill_value: a float represents the value to be filled outside\n+            the boundaries when `fill_mode=\"constant\"`.\n     \"\"\"\n \n     def __init__(\n@@ -1241,7 +954,8 @@ class RandomRotation(BaseImageAugmentationLayer):\n             self.upper = factor\n         if self.upper < self.lower:\n             raise ValueError(\n-                f\"Factor cannot have negative values, got {factor}\"\n+                \"`factor` argument cannot have a negative value. \"\n+                f\"Received: factor={factor}\"\n             )\n         check_fill_mode_and_interpolation(fill_mode, interpolation)\n         self.fill_mode = fill_mode\n@@ -1249,37 +963,43 @@ class RandomRotation(BaseImageAugmentationLayer):\n         self.interpolation = interpolation\n         self.seed = seed\n \n-    def get_random_transformation(\n-        self, image=None, label=None, bounding_box=None\n-    ):\n+    def call(self, inputs, training=True):\n+        inputs = convert_inputs(inputs, self.compute_dtype)\n+\n+        def random_rotated_inputs(inputs):\n+            \"\"\"Rotated inputs with random ops.\"\"\"\n+            original_shape = inputs.shape\n+            unbatched = inputs.shape.rank == 3\n+            # The transform op only accepts rank 4 inputs,\n+            # so if we have an unbatched image,\n+            # we need to temporarily expand dims to a batch.\n+            if unbatched:\n+                inputs = tf.expand_dims(inputs, 0)\n+            inputs_shape = tf.shape(inputs)\n+            batch_size = inputs_shape[0]\n+            img_hd = tf.cast(inputs_shape[H_AXIS], tf.float32)\n+            img_wd = tf.cast(inputs_shape[W_AXIS], tf.float32)\n             min_angle = self.lower * 2.0 * np.pi\n             max_angle = self.upper * 2.0 * np.pi\n-        angle = self._random_generator.random_uniform(\n-            shape=[1], minval=min_angle, maxval=max_angle\n+            angles = self._random_generator.random_uniform(\n+                shape=[batch_size], minval=min_angle, maxval=max_angle\n             )\n-        return {\"angle\": angle}\n-\n-    def augment_image(self, image, transformation):\n-        image = utils.ensure_tensor(image, self.compute_dtype)\n-        original_shape = image.shape\n-        image = tf.expand_dims(image, 0)\n-        image_shape = tf.shape(image)\n-        img_hd = tf.cast(image_shape[H_AXIS], tf.float32)\n-        img_wd = tf.cast(image_shape[W_AXIS], tf.float32)\n-        angle = transformation[\"angle\"]\n             output = transform(\n-            image,\n-            get_rotation_matrix(angle, img_hd, img_wd),\n+                inputs,\n+                get_rotation_matrix(angles, img_hd, img_wd),\n                 fill_mode=self.fill_mode,\n                 fill_value=self.fill_value,\n                 interpolation=self.interpolation,\n             )\n+            if unbatched:\n                 output = tf.squeeze(output, 0)\n             output.set_shape(original_shape)\n             return output\n \n-    def augment_label(self, label, transformation):\n-        return label\n+        if training:\n+            return random_rotated_inputs(inputs)\n+        else:\n+            return inputs\n \n     def compute_output_shape(self, input_shape):\n         return input_shape\n@@ -1301,50 +1021,61 @@ class RandomRotation(BaseImageAugmentationLayer):\n     \"keras.layers.experimental.preprocessing.RandomZoom\",\n     v1=[],\n )\n-class RandomZoom(BaseImageAugmentationLayer):\n+class RandomZoom(base_layer.BaseRandomLayer):\n     \"\"\"A preprocessing layer which randomly zooms images during training.\n \n     This layer will randomly zoom in or out on each axis of an image\n     independently, filling empty space according to `fill_mode`.\n \n     Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and\n-    of interger or floating point dtype. By default, the layer will output\n-    floats.\n+    of integer or floating point dtype.\n+    By default, the layer will output floats.\n \n     For an overview and full list of preprocessing layers, see the preprocessing\n     [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n \n     Args:\n-      height_factor: a float represented as fraction of value, or a tuple of\n-        size 2 representing lower and upper bound for zooming vertically. When\n-        represented as a single float, this value is used for both the upper and\n-        lower bound. A positive value means zooming out, while a negative value\n-        means zooming in. For instance, `height_factor=(0.2, 0.3)` result in an\n-        output zoomed out by a random amount in the range `[+20%, +30%]`.\n-        `height_factor=(-0.3, -0.2)` result in an output zoomed in by a random\n-        amount in the range `[+20%, +30%]`.\n-      width_factor: a float represented as fraction of value, or a tuple of size\n-        2 representing lower and upper bound for zooming horizontally. When\n-        represented as a single float, this value is used for both the upper and\n-        lower bound. For instance, `width_factor=(0.2, 0.3)` result in an output\n-        zooming out between 20% to 30%. `width_factor=(-0.3, -0.2)` result in an\n-        output zooming in between 20% to 30%. Defaults to `None`, i.e., zooming\n-        vertical and horizontal directions by preserving the aspect ratio.\n-      fill_mode: Points outside the boundaries of the input are filled according\n-        to the given mode (one of `{\"constant\", \"reflect\", \"wrap\", \"nearest\"}`).\n-        - *reflect*: `(d c b a | a b c d | d c b a)` The input is extended by\n-          reflecting about the edge of the last pixel.\n-        - *constant*: `(k k k k | a b c d | k k k k)` The input is extended by\n-          filling all values beyond the edge with the same constant value k = 0.\n+        height_factor: a float represented as fraction of value,\n+            or a tuple of size 2 representing lower and upper bound\n+            for zooming vertically. When represented as a single float,\n+            this value is used for both the upper and\n+            lower bound. A positive value means zooming out,\n+            while a negative value\n+            means zooming in. For instance, `height_factor=(0.2, 0.3)`\n+            result in an output zoomed out by a random amount\n+            in the range `[+20%, +30%]`.\n+            `height_factor=(-0.3, -0.2)` result in an output zoomed\n+            in by a random amount in the range `[+20%, +30%]`.\n+        width_factor: a float represented as fraction of value,\n+            or a tuple of size 2 representing lower and upper bound\n+            for zooming horizontally. When\n+            represented as a single float, this value is used\n+            for both the upper and\n+            lower bound. For instance, `width_factor=(0.2, 0.3)`\n+            result in an output\n+            zooming out between 20% to 30%.\n+            `width_factor=(-0.3, -0.2)` result in an\n+            output zooming in between 20% to 30%. Defaults to `None`,\n+            i.e., zooming vertical and horizontal directions\n+            by preserving the aspect ratio.\n+        fill_mode: Points outside the boundaries of the input are\n+            filled according to the given mode\n+            (one of `{\"constant\", \"reflect\", \"wrap\", \"nearest\"}`).\n+            - *reflect*: `(d c b a | a b c d | d c b a)`\n+                The input is extended by reflecting about\n+                the edge of the last pixel.\n+            - *constant*: `(k k k k | a b c d | k k k k)`\n+                The input is extended by filling all values beyond\n+                the edge with the same constant value k = 0.\n             - *wrap*: `(a b c d | a b c d | a b c d)` The input is extended by\n                 wrapping around to the opposite edge.\n-        - *nearest*: `(a a a a | a b c d | d d d d)` The input is extended by\n-          the nearest pixel.\n+            - *nearest*: `(a a a a | a b c d | d d d d)`\n+                The input is extended by the nearest pixel.\n         interpolation: Interpolation mode. Supported values: `\"nearest\"`,\n             `\"bilinear\"`.\n         seed: Integer. Used to create a random seed.\n-      fill_value: a float represents the value to be filled outside the\n-        boundaries when `fill_mode=\"constant\"`.\n+        fill_value: a float represents the value to be filled outside\n+            the boundaries when `fill_mode=\"constant\"`.\n \n     Example:\n \n@@ -1387,8 +1118,8 @@ class RandomZoom(BaseImageAugmentationLayer):\n \n         if abs(self.height_lower) > 1.0 or abs(self.height_upper) > 1.0:\n             raise ValueError(\n-                \"`height_factor` must have values between [-1, 1], \"\n-                f\"got {height_factor}\"\n+                \"`height_factor` argument must have values between [-1, 1]. \"\n+                f\"Received: height_factor={height_factor}\"\n             )\n \n         self.width_factor = width_factor\n@@ -1402,8 +1133,8 @@ class RandomZoom(BaseImageAugmentationLayer):\n \n             if self.width_lower < -1.0 or self.width_upper < -1.0:\n                 raise ValueError(\n-                    \"`width_factor` must have values larger than -1, \"\n-                    f\"got {width_factor}\"\n+                    \"`width_factor` argument must have values larger than -1. \"\n+                    f\"Received: width_factor={width_factor}\"\n                 )\n \n         check_fill_mode_and_interpolation(fill_mode, interpolation)\n@@ -1413,50 +1144,54 @@ class RandomZoom(BaseImageAugmentationLayer):\n         self.interpolation = interpolation\n         self.seed = seed\n \n-    def get_random_transformation(\n-        self, image=None, label=None, bounding_box=None\n-    ):\n+    def call(self, inputs, training=True):\n+        inputs = convert_inputs(inputs, self.compute_dtype)\n+\n+        def random_zoomed_inputs(inputs):\n+            \"\"\"Zoomed inputs with random ops.\"\"\"\n+            original_shape = inputs.shape\n+            unbatched = inputs.shape.rank == 3\n+            # The transform op only accepts rank 4 inputs,\n+            # so if we have an unbatched image,\n+            # we need to temporarily expand dims to a batch.\n+            if unbatched:\n+                inputs = tf.expand_dims(inputs, 0)\n+            inputs_shape = tf.shape(inputs)\n+            batch_size = inputs_shape[0]\n+            img_hd = tf.cast(inputs_shape[H_AXIS], tf.float32)\n+            img_wd = tf.cast(inputs_shape[W_AXIS], tf.float32)\n             height_zoom = self._random_generator.random_uniform(\n-            shape=[1, 1],\n+                shape=[batch_size, 1],\n                 minval=1.0 + self.height_lower,\n                 maxval=1.0 + self.height_upper,\n             )\n             if self.width_factor is not None:\n                 width_zoom = self._random_generator.random_uniform(\n-                shape=[1, 1],\n+                    shape=[batch_size, 1],\n                     minval=1.0 + self.width_lower,\n                     maxval=1.0 + self.width_upper,\n                 )\n             else:\n                 width_zoom = height_zoom\n-\n-        return {\"height_zoom\": height_zoom, \"width_zoom\": width_zoom}\n-\n-    def augment_image(self, image, transformation):\n-        image = utils.ensure_tensor(image, self.compute_dtype)\n-        original_shape = image.shape\n-        image = tf.expand_dims(image, 0)\n-        image_shape = tf.shape(image)\n-        img_hd = tf.cast(image_shape[H_AXIS], tf.float32)\n-        img_wd = tf.cast(image_shape[W_AXIS], tf.float32)\n-        width_zoom = transformation[\"width_zoom\"]\n-        height_zoom = transformation[\"height_zoom\"]\n             zooms = tf.cast(\n                 tf.concat([width_zoom, height_zoom], axis=1), dtype=tf.float32\n             )\n             output = transform(\n-            image,\n+                inputs,\n                 get_zoom_matrix(zooms, img_hd, img_wd),\n                 fill_mode=self.fill_mode,\n                 fill_value=self.fill_value,\n                 interpolation=self.interpolation,\n             )\n+            if unbatched:\n                 output = tf.squeeze(output, 0)\n             output.set_shape(original_shape)\n             return output\n \n-    def augment_label(self, label, transformation):\n-        return label\n+        if training:\n+            return random_zoomed_inputs(inputs)\n+        else:\n+            return inputs\n \n     def compute_output_shape(self, input_shape):\n         return input_shape\n@@ -1478,8 +1213,8 @@ def get_zoom_matrix(zooms, image_height, image_width, name=None):\n     \"\"\"Returns projective transform(s) for the given zoom(s).\n \n     Args:\n-      zooms: A matrix of 2-element lists representing `[zx, zy]` to zoom for\n-        each image (for a batch of images).\n+        zooms: A matrix of 2-element lists representing `[zx, zy]`\n+            to zoom for each image (for a batch of images).\n         image_height: Height of the image(s) to be transformed.\n         image_width: Width of the image(s) to be transformed.\n         name: The name of the op.\n@@ -1522,20 +1257,21 @@ def get_zoom_matrix(zooms, image_height, image_width, name=None):\n     \"keras.layers.experimental.preprocessing.RandomContrast\",\n     v1=[],\n )\n-class RandomContrast(BaseImageAugmentationLayer):\n+class RandomContrast(base_layer.BaseRandomLayer):\n     \"\"\"A preprocessing layer which randomly adjusts contrast during training.\n \n-    This layer will randomly adjust the contrast of an image or images by a\n-    random factor. Contrast is adjusted independently for each channel of each\n-    image during training.\n+    This layer will randomly adjust the contrast of an image or images\n+    by a random factor. Contrast is adjusted independently\n+    for each channel of each image during training.\n \n     For each channel, this layer computes the mean of the image pixels in the\n     channel and then adjusts each component `x` of each pixel to\n     `(x - mean) * contrast_factor + mean`.\n \n     Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and\n-    in integer or floating point dtype. By default, the layer will output\n-    floats. The output value will be clipped to the range `[0, 255]`, the valid\n+    in integer or floating point dtype.\n+    By default, the layer will output floats.\n+    The output value will be clipped to the range `[0, 255]`, the valid\n     range of RGB colors.\n \n     For an overview and full list of preprocessing layers, see the preprocessing\n@@ -1549,13 +1285,14 @@ class RandomContrast(BaseImageAugmentationLayer):\n         3D (unbatched) or 4D (batched) tensor with shape:\n         `(..., height, width, channels)`, in `\"channels_last\"` format.\n \n-    Arguments:\n+    Args:\n         factor: a positive float represented as fraction of value, or a tuple of\n-        size 2 representing lower and upper bound. When represented as a single\n-        float, lower = upper. The contrast factor will be randomly picked\n-        between `[1.0 - lower, 1.0 + upper]`. For any pixel x in the channel,\n-        the output will be `(x - mean) * factor + mean` where `mean` is the mean\n-        value of the channel.\n+            size 2 representing lower and upper bound.\n+            When represented as a single float, lower = upper.\n+            The contrast factor will be randomly picked between\n+            `[1.0 - lower, 1.0 + upper]`. For any pixel x in the channel,\n+            the output will be `(x - mean) * factor + mean`\n+            where `mean` is the mean value of the channel.\n         seed: Integer. Used to create a random seed.\n     \"\"\"\n \n@@ -1572,33 +1309,36 @@ class RandomContrast(BaseImageAugmentationLayer):\n             self.lower = self.upper = factor\n         if self.lower < 0.0 or self.upper < 0.0 or self.lower > 1.0:\n             raise ValueError(\n-                \"Factor cannot have negative values or greater than 1.0,\"\n-                f\" got {factor}\"\n+                \"`factor` argument cannot have negative values or values \"\n+                \"greater than 1.\"\n+                f\"Received: factor={factor}\"\n             )\n         self.seed = seed\n \n-    def get_random_transformation(\n-        self, image=None, label=None, bounding_box=None\n-    ):\n-        lower = 1.0 - self.lower\n-        upper = 1.0 + self.upper\n-        random_seed = self._random_generator.make_seed_for_stateless_op()\n-        contrast_factor = stateless_random_ops.stateless_random_uniform(\n-            shape=[], minval=lower, maxval=upper, seed=random_seed\n-        )\n-        return {\"contrast_factor\": contrast_factor}\n+    def call(self, inputs, training=True):\n+        inputs = convert_inputs(inputs, self.compute_dtype)\n \n-    def augment_image(self, image, transformation):\n-        contrast_factor = transformation[\"contrast_factor\"]\n-        output = tf.image.adjust_contrast(\n-            image, contrast_factor=contrast_factor\n+        def random_contrasted_inputs(inputs):\n+            seed = self._random_generator.make_seed_for_stateless_op()\n+            if seed is not None:\n+                output = tf.image.stateless_random_contrast(\n+                    inputs, 1.0 - self.lower, 1.0 + self.upper, seed=seed\n+                )\n+            else:\n+                output = tf.image.random_contrast(\n+                    inputs,\n+                    1.0 - self.lower,\n+                    1.0 + self.upper,\n+                    seed=self._random_generator.make_legacy_seed(),\n                 )\n             output = tf.clip_by_value(output, 0, 255)\n-        output.set_shape(image.shape)\n+            output.set_shape(inputs.shape)\n             return output\n \n-    def augment_label(self, label, transformation):\n-        return label\n+        if training:\n+            return random_contrasted_inputs(inputs)\n+        else:\n+            return inputs\n \n     def compute_output_shape(self, input_shape):\n         return input_shape\n@@ -1613,7 +1353,7 @@ class RandomContrast(BaseImageAugmentationLayer):\n \n \n @keras_export(\"keras.layers.RandomBrightness\", v1=[])\n-class RandomBrightness(BaseImageAugmentationLayer):\n+class RandomBrightness(base_layer.BaseRandomLayer):\n     \"\"\"A preprocessing layer which randomly adjusts brightness during training.\n \n     This layer will randomly increase/reduce the brightness for the input RGB\n@@ -1631,22 +1371,26 @@ class RandomBrightness(BaseImageAugmentationLayer):\n             factor is used to determine the lower bound and upper bound of the\n             brightness adjustment. A float value will be chosen randomly between\n             the limits. When -1.0 is chosen, the output image will be black, and\n-        when 1.0 is chosen, the image will be fully white. When only one float\n-        is provided, eg, 0.2, then -0.2 will be used for lower bound and 0.2\n+            when 1.0 is chosen, the image will be fully white.\n+            When only one float is provided, eg, 0.2,\n+            then -0.2 will be used for lower bound and 0.2\n             will be used for upper bound.\n-      value_range: Optional list/tuple of 2 floats for the lower and upper limit\n-        of the values of the input data. Defaults to [0.0, 255.0]. Can be\n-        changed to e.g. [0.0, 1.0] if the image input has been scaled before\n-        this layer.  The brightness adjustment will be scaled to this range, and\n-        the output values will be clipped to this range.\n+        value_range: Optional list/tuple of 2 floats\n+            for the lower and upper limit\n+            of the values of the input data. Defaults to [0.0, 255.0].\n+            Can be changed to e.g. [0.0, 1.0] if the image input\n+            has been scaled before this layer.\n+            The brightness adjustment will be scaled to this range, and the\n+            output values will be clipped to this range.\n         seed: optional integer, for fixed RNG behavior.\n \n     Inputs: 3D (HWC) or 4D (NHWC) tensor, with float or int dtype. Input pixel\n         values can be of any range (e.g. `[0., 1.)` or `[0, 255]`)\n \n     Output: 3D (HWC) or 4D (NHWC) tensor with brightness adjusted based on the\n-      `factor`. By default, the layer will output floats. The output value will\n-      be clipped to the range `[0, 255]`, the valid range of RGB colors, and\n+        `factor`. By default, the layer will output floats.\n+        The output value will be clipped to the range `[0, 255]`,\n+        the valid range of RGB colors, and\n         rescaled based on the `value_range` if needed.\n \n     Sample usage:\n@@ -1687,26 +1431,6 @@ class RandomBrightness(BaseImageAugmentationLayer):\n         self._set_value_range(value_range)\n         self._seed = seed\n \n-    def augment_image(self, image, transformation):\n-        return self._brightness_adjust(image, transformation[\"rgb_delta\"])\n-\n-    def augment_label(self, label, transformation):\n-        return label\n-\n-    def get_random_transformation(\n-        self, image=None, label=None, bounding_box=None\n-    ):\n-        rgb_delta_shape = (1, 1, 1)\n-        random_rgb_delta = self._random_generator.random_uniform(\n-            shape=rgb_delta_shape,\n-            minval=self._factor[0],\n-            maxval=self._factor[1],\n-        )\n-        random_rgb_delta = random_rgb_delta * (\n-            self._value_range[1] - self._value_range[0]\n-        )\n-        return {\"rgb_delta\": random_rgb_delta}\n-\n     def _set_value_range(self, value_range):\n         if not isinstance(value_range, (tuple, list)):\n             raise ValueError(\n@@ -1740,18 +1464,36 @@ class RandomBrightness(BaseImageAugmentationLayer):\n                 self._FACTOR_VALIDATION_ERROR + f\"Got {input_number}\"\n             )\n \n-    def _brightness_adjust(self, image, rgb_delta):\n-        image = utils.ensure_tensor(image, self.compute_dtype)\n-        rank = image.shape.rank\n-        if rank != 3:\n+    def call(self, inputs, training=True):\n+        inputs = convert_inputs(inputs, dtype=self.compute_dtype)\n+        if training:\n+            return self._brightness_adjust(inputs)\n+        else:\n+            return inputs\n+\n+    def _brightness_adjust(self, images):\n+        rank = images.shape.rank\n+        if rank == 3:\n+            rgb_delta_shape = (1, 1, 1)\n+        elif rank == 4:\n+            # Keep only the batch dim. This will ensure to have same adjustment\n+            # with in one image, but different across the images.\n+            rgb_delta_shape = [tf.shape(images)[0], 1, 1, 1]\n+        else:\n             raise ValueError(\n-                \"Expected the input image to be rank 3. Got \"\n-                f\"inputs.shape = {image.shape}\"\n+                \"Expected the input image to be rank 3 or 4. Got \"\n+                f\"inputs.shape = {images.shape}\"\n             )\n-        rgb_delta = tf.cast(rgb_delta, image.dtype)\n-        image += rgb_delta\n+        rgb_delta = self._random_generator.random_uniform(\n+            shape=rgb_delta_shape,\n+            minval=self._factor[0],\n+            maxval=self._factor[1],\n+        )\n+        rgb_delta = rgb_delta * (self._value_range[1] - self._value_range[0])\n+        rgb_delta = tf.cast(rgb_delta, images.dtype)\n+        images += rgb_delta\n         return tf.clip_by_value(\n-            image, self._value_range[0], self._value_range[1]\n+            images, self._value_range[0], self._value_range[1]\n         )\n \n     def get_config(self):\n@@ -1769,13 +1511,13 @@ class RandomBrightness(BaseImageAugmentationLayer):\n     \"keras.layers.experimental.preprocessing.RandomHeight\",\n     v1=[],\n )\n-class RandomHeight(BaseImageAugmentationLayer):\n+class RandomHeight(base_layer.BaseRandomLayer):\n     \"\"\"A preprocessing layer which randomly varies image height during training.\n \n     This layer adjusts the height of a batch of images by a random factor.\n     The input should be a 3D (unbatched) or 4D (batched) tensor in the\n     `\"channels_last\"` image data format. Input pixel values can be of any range\n-    (e.g. `[0., 1.)` or `[0, 255]`) and of interger or floating point dtype. By\n+    (e.g. `[0., 1.)` or `[0, 255]`) and of integer or floating point dtype. By\n     default, the layer will output floats.\n \n \n@@ -1785,16 +1527,19 @@ class RandomHeight(BaseImageAugmentationLayer):\n     [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n \n     Args:\n-      factor: A positive float (fraction of original height), or a tuple of size\n-        2 representing lower and upper bound for resizing vertically. When\n-        represented as a single float, this value is used for both the upper and\n-        lower bound. For instance, `factor=(0.2, 0.3)` results in an output with\n+        factor: A positive float (fraction of original height),\n+            or a tuple of size 2 representing lower and upper bound\n+            for resizing vertically. When represented as a single float,\n+            this value is used for both the upper and\n+            lower bound. For instance, `factor=(0.2, 0.3)` results\n+            in an output with\n             height changed by a random amount in the range `[20%, 30%]`.\n-        `factor=(-0.2, 0.3)` results in an output with height changed by a\n-        random amount in the range `[-20%, +30%]`. `factor=0.2` results in an\n-        output with height changed by a random amount in the range\n-        `[-20%, +20%]`.\n-      interpolation: String, the interpolation method. Defaults to `\"bilinear\"`.\n+            `factor=(-0.2, 0.3)` results in an output with height\n+            changed by a random amount in the range `[-20%, +30%]`.\n+            `factor=0.2` results in an output with\n+            height changed by a random amount in the range `[-20%, +20%]`.\n+        interpolation: String, the interpolation method.\n+            Defaults to `\"bilinear\"`.\n             Supports `\"bilinear\"`, `\"nearest\"`, `\"bicubic\"`, `\"area\"`,\n             `\"lanczos3\"`, `\"lanczos5\"`, `\"gaussian\"`, `\"mitchellcubic\"`.\n         seed: Integer. Used to create a random seed.\n@@ -1823,12 +1568,13 @@ class RandomHeight(BaseImageAugmentationLayer):\n \n         if self.height_upper < self.height_lower:\n             raise ValueError(\n-                \"`factor` cannot have upper bound less than \"\n-                f\"lower bound, got {factor}\"\n+                \"`factor` argument cannot have an upper bound lesser than the \"\n+                f\"lower bound. Received: factor={factor}\"\n             )\n         if self.height_lower < -1.0 or self.height_upper < -1.0:\n             raise ValueError(\n-                f\"`factor` must have values larger than -1, got {factor}\"\n+                \"`factor` argument must have values larger than -1. \"\n+                f\"Received: factor={factor}\"\n             )\n         self.interpolation = interpolation\n         self._interpolation_method = image_utils.get_interpolation(\n@@ -1836,45 +1582,38 @@ class RandomHeight(BaseImageAugmentationLayer):\n         )\n         self.seed = seed\n \n-    def get_random_transformation(\n-        self, image=None, label=None, bounding_box=None\n-    ):\n+    def call(self, inputs, training=True):\n+        inputs = convert_inputs(inputs)\n+\n+        def random_height_inputs(inputs):\n+            \"\"\"Inputs height-adjusted with random ops.\"\"\"\n+            inputs_shape = tf.shape(inputs)\n+            img_hd = tf.cast(inputs_shape[H_AXIS], tf.float32)\n+            img_wd = inputs_shape[W_AXIS]\n             height_factor = self._random_generator.random_uniform(\n                 shape=[],\n                 minval=(1.0 + self.height_lower),\n                 maxval=(1.0 + self.height_upper),\n             )\n-        inputs_shape = tf.shape(image)\n-        img_hd = tf.cast(inputs_shape[H_AXIS], tf.float32)\n             adjusted_height = tf.cast(height_factor * img_hd, tf.int32)\n-        return {\"height\": adjusted_height}\n-\n-    def _batch_augment(self, inputs):\n-        images = self.augment_image(\n-            inputs[IMAGES],\n-            transformation=self.get_random_transformation(image=inputs[IMAGES]),\n-        )\n-        result = {IMAGES: images}\n-        # to-do augment bbox to clip bbox to resized height value\n-        return result\n-\n-    def augment_image(self, image, transformation):\n-        # The batch dimension of the input=image is not modified. The output\n-        # would be accurate for both unbatched and batched input\n-        inputs_shape = tf.shape(image)\n-        img_wd = inputs_shape[W_AXIS]\n-        adjusted_height = transformation[\"height\"]\n             adjusted_size = tf.stack([adjusted_height, img_wd])\n             output = tf.image.resize(\n-            images=image, size=adjusted_size, method=self._interpolation_method\n+                images=inputs,\n+                size=adjusted_size,\n+                method=self._interpolation_method,\n             )\n-        # tf.resize will output float32 in many cases regardless of input type.\n+            # tf.resize will output float32 regardless of input type.\n             output = tf.cast(output, self.compute_dtype)\n-        output_shape = list(image.shape)\n+            output_shape = inputs.shape.as_list()\n             output_shape[H_AXIS] = None\n             output.set_shape(output_shape)\n             return output\n \n+        if training:\n+            return random_height_inputs(inputs)\n+        else:\n+            return inputs\n+\n     def compute_output_shape(self, input_shape):\n         input_shape = tf.TensorShape(input_shape).as_list()\n         input_shape[H_AXIS] = None\n@@ -1895,14 +1634,14 @@ class RandomHeight(BaseImageAugmentationLayer):\n     \"keras.layers.experimental.preprocessing.RandomWidth\",\n     v1=[],\n )\n-class RandomWidth(BaseImageAugmentationLayer):\n+class RandomWidth(base_layer.BaseRandomLayer):\n     \"\"\"A preprocessing layer which randomly varies image width during training.\n \n     This layer will randomly adjusts the width of a batch of images of a\n     batch of images by a random factor. The input should be a 3D (unbatched) or\n     4D (batched) tensor in the `\"channels_last\"` image data format. Input pixel\n-    values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and of interger\n-    or floating point dtype. By default, the layer will output floats.\n+    values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and of integer or\n+    floating point dtype. By default, the layer will output floats.\n \n     By default, this layer is inactive during inference.\n \n@@ -1910,17 +1649,21 @@ class RandomWidth(BaseImageAugmentationLayer):\n     [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n \n     Args:\n-      factor: A positive float (fraction of original width), or a tuple of size\n-        2 representing lower and upper bound for resizing vertically. When\n-        represented as a single float, this value is used for both the upper and\n-        lower bound. For instance, `factor=(0.2, 0.3)` results in an output with\n+        factor: A positive float (fraction of original width),\n+            or a tuple of size 2 representing lower and upper bound\n+            for resizing vertically. When represented as a single float,\n+            this value is used for both the upper and\n+            lower bound. For instance, `factor=(0.2, 0.3)`\n+            results in an output with\n             width changed by a random amount in the range `[20%, 30%]`.\n-        `factor=(-0.2, 0.3)` results in an output with width changed by a random\n-        amount in the range `[-20%, +30%]`. `factor=0.2` results in an output\n-        with width changed by a random amount in the range `[-20%, +20%]`.\n-      interpolation: String, the interpolation method. Defaults to `bilinear`.\n-        Supports `\"bilinear\"`, `\"nearest\"`, `\"bicubic\"`, `\"area\"`, `\"lanczos3\"`,\n-        `\"lanczos5\"`, `\"gaussian\"`, `\"mitchellcubic\"`.\n+            `factor=(-0.2, 0.3)` results in an output with width changed\n+            by a random amount in the range `[-20%, +30%]`.\n+            `factor=0.2` results in an output with width changed\n+            by a random amount in the range `[-20%, +20%]`.\n+        interpolation: String, the interpolation method.\n+            Defaults to `bilinear`.\n+            Supports `\"bilinear\"`, `\"nearest\"`, `\"bicubic\"`, `\"area\"`,\n+            `\"lanczos3\"`, `\"lanczos5\"`, `\"gaussian\"`, `\"mitchellcubic\"`.\n         seed: Integer. Used to create a random seed.\n \n     Input shape:\n@@ -1946,51 +1689,27 @@ class RandomWidth(BaseImageAugmentationLayer):\n             self.width_upper = factor\n         if self.width_upper < self.width_lower:\n             raise ValueError(\n-                \"`factor` cannot have upper bound less than \"\n-                f\"lower bound, got {factor}\"\n+                \"`factor` argument cannot have an upper bound less than the \"\n+                f\"lower bound. Received: factor={factor}\"\n             )\n         if self.width_lower < -1.0 or self.width_upper < -1.0:\n             raise ValueError(\n-                f\"`factor` must have values larger than -1, got {factor}\"\n+                \"`factor` argument must have values larger than -1. \"\n+                f\"Received: factor={factor}\"\n             )\n         self.interpolation = interpolation\n         self._interpolation_method = image_utils.get_interpolation(\n             interpolation\n         )\n         self.seed = seed\n-        self.auto_vectorize = False\n \n-    def _batch_augment(self, inputs):\n-        images = self.augment_image(\n-            inputs[IMAGES],\n-            transformation=self.get_random_transformation(image=inputs[IMAGES]),\n-        )\n-        result = {IMAGES: images}\n-        # to-do augment bbox to clip bbox to resized width value\n-        return result\n+    def call(self, inputs, training=True):\n+        inputs = convert_inputs(inputs)\n \n-    def augment_image(self, image, transformation):\n-        # The batch dimension of the input=image is not modified. The output\n-        # would be accurate for both unbatched and batched input\n-        inputs = utils.ensure_tensor(image)\n+        def random_width_inputs(inputs):\n+            \"\"\"Inputs width-adjusted with random ops.\"\"\"\n             inputs_shape = tf.shape(inputs)\n             img_hd = inputs_shape[H_AXIS]\n-        adjusted_width = transformation[\"width\"]\n-        adjusted_size = tf.stack([img_hd, adjusted_width])\n-        output = tf.image.resize(\n-            images=inputs, size=adjusted_size, method=self._interpolation_method\n-        )\n-        # tf.resize will output float32 in many cases regardless of input type.\n-        output = tf.cast(output, self.compute_dtype)\n-        output_shape = inputs.shape.as_list()\n-        output_shape[W_AXIS] = None\n-        output.set_shape(output_shape)\n-        return output\n-\n-    def get_random_transformation(\n-        self, image=None, label=None, bounding_box=None\n-    ):\n-        inputs_shape = tf.shape(image)\n             img_wd = tf.cast(inputs_shape[W_AXIS], tf.float32)\n             width_factor = self._random_generator.random_uniform(\n                 shape=[],\n@@ -1998,7 +1717,23 @@ class RandomWidth(BaseImageAugmentationLayer):\n                 maxval=(1.0 + self.width_upper),\n             )\n             adjusted_width = tf.cast(width_factor * img_wd, tf.int32)\n-        return {\"width\": adjusted_width}\n+            adjusted_size = tf.stack([img_hd, adjusted_width])\n+            output = tf.image.resize(\n+                images=inputs,\n+                size=adjusted_size,\n+                method=self._interpolation_method,\n+            )\n+            # tf.resize will output float32 regardless of input type.\n+            output = tf.cast(output, self.compute_dtype)\n+            output_shape = inputs.shape.as_list()\n+            output_shape[W_AXIS] = None\n+            output.set_shape(output_shape)\n+            return output\n+\n+        if training:\n+            return random_width_inputs(inputs)\n+        else:\n+            return inputs\n \n     def compute_output_shape(self, input_shape):\n         input_shape = tf.TensorShape(input_shape).as_list()\n@@ -2013,3 +1748,18 @@ class RandomWidth(BaseImageAugmentationLayer):\n         }\n         base_config = super().get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+def convert_inputs(inputs, dtype=None):\n+    if isinstance(inputs, dict):\n+        raise ValueError(\n+            \"This layer can only process a tensor representing an image or \"\n+            f\"a batch of images. Received: type(inputs)={type(inputs)}.\"\n+            \"If you need to pass a dict containing \"\n+            \"images, labels, and bounding boxes, you should \"\n+            \"instead use the preprocessing and augmentation layers \"\n+            \"from `keras_cv.layers`. See docs at \"\n+            \"https://keras.io/api/keras_cv/layers/\"\n+        )\n+    inputs = utils.ensure_tensor(inputs, dtype=dtype)\n+    return inputs\n\n@@ -325,8 +325,8 @@ class CenterCropTest(test_combinations.TestCase):\n         with test_utils.use_gpu():\n             layer = image_preprocessing.CenterCrop(height, width)\n             actual_output = layer(inp)\n-            # In this case, output should equal resizing with crop_to_aspect\n-            # ratio.\n+            # In this case, output should equal resizing\n+            # with crop_to_aspect ratio.\n             resize_layer = image_preprocessing.Resizing(\n                 height, width, crop_to_aspect_ratio=True\n             )\n@@ -393,8 +393,8 @@ class RandomCropTest(test_combinations.TestCase):\n         with test_utils.use_gpu():\n             layer = image_preprocessing.RandomCrop(height, width)\n             actual_output = layer(inp)\n-            # In this case, output should equal resizing with crop_to_aspect\n-            # ratio.\n+            # In this case, output should equal resizing\n+            # with crop_to_aspect ratio.\n             resize_layer = image_preprocessing.Resizing(\n                 height, width, crop_to_aspect_ratio=True\n             )\n@@ -487,55 +487,6 @@ class RandomCropTest(test_combinations.TestCase):\n                 actual_output = layer(inp, training=True)\n                 self.assertAllClose(inp[2:10, 2:10, :], actual_output)\n \n-    def test_batched_input(self):\n-        np.random.seed(1337)\n-        inp = np.random.random((20, 16, 16, 3))\n-        mock_offset = [2, 2]\n-        with test_utils.use_gpu():\n-            layer = image_preprocessing.RandomCrop(8, 8)\n-            with tf.compat.v1.test.mock.patch.object(\n-                layer._random_generator,\n-                \"random_uniform\",\n-                return_value=mock_offset,\n-            ):\n-                actual_output = layer(inp, training=True)\n-                self.assertAllClose(inp[:, 2:10, 2:10, :], actual_output)\n-\n-    def test_augment_image(self):\n-        np.random.seed(1337)\n-        inp = np.random.random((16, 16, 3))\n-        mock_offset = [2, 2]\n-        with test_utils.use_gpu():\n-            layer = image_preprocessing.RandomCrop(8, 8)\n-            with tf.compat.v1.test.mock.patch.object(\n-                layer._random_generator,\n-                \"random_uniform\",\n-                return_value=mock_offset,\n-            ):\n-                actual_output = layer.augment_image(\n-                    inp,\n-                    transformation=layer.get_random_transformation(image=inp),\n-                )\n-                self.assertAllClose(inp[2:10, 2:10, :], actual_output)\n-\n-    def test_training_false(self):\n-        np.random.seed(1337)\n-        height, width = 4, 6\n-        inp = np.random.random((12, 8, 16, 3))\n-        inp_dict = {\"images\": inp}\n-        with test_utils.use_gpu():\n-            layer = image_preprocessing.RandomCrop(height, width)\n-            # test wih tensor input\n-            actual_output = layer(inp, training=False)\n-            resized_inp = tf.image.resize(inp, size=[4, 8])\n-            expected_output = resized_inp[:, :, 1:7, :]\n-            self.assertAllClose(expected_output, actual_output)\n-            # test with dictionary input\n-            actual_output = layer(inp_dict, training=False)\n-            resized_inp = tf.image.resize(inp, size=[4, 8])\n-            expected_output = resized_inp[:, :, 1:7, :]\n-            self.assertAllClose(expected_output, actual_output[\"images\"])\n-\n     @test_utils.run_v2_only\n     def test_uint8_input(self):\n         inputs = keras.Input((128, 128, 3), batch_size=2, dtype=tf.uint8)\n@@ -608,9 +559,8 @@ class RandomFlipTest(test_combinations.TestCase):\n         orig_width = 8\n         channels = 3\n         if mock_random is None:\n-            mock_random = [True for _ in range(num_samples)]\n-            if mode == \"horizontal_and_vertical\":\n-                mock_random *= 2\n+            mock_random = [1 for _ in range(num_samples)]\n+            mock_random = np.reshape(mock_random, [2, 1, 1, 1])\n         inp = np.random.random((num_samples, orig_height, orig_width, channels))\n         if expected_output is None:\n             expected_output = inp\n@@ -619,9 +569,9 @@ class RandomFlipTest(test_combinations.TestCase):\n             if mode == \"vertical\" or mode == \"horizontal_and_vertical\":\n                 expected_output = np.flip(expected_output, axis=1)\n         with tf.compat.v1.test.mock.patch.object(\n-            np.random,\n-            \"choice\",\n-            side_effect=mock_random,\n+            stateless_random_ops,\n+            \"stateless_random_uniform\",\n+            return_value=mock_random,\n         ):\n             with test_utils.use_gpu():\n                 layer = image_preprocessing.RandomFlip(mode)\n@@ -638,7 +588,8 @@ class RandomFlipTest(test_combinations.TestCase):\n \n     def test_random_flip_horizontal_half(self):\n         np.random.seed(1337)\n-        mock_random = [True, False]\n+        mock_random = [1, 0]\n+        mock_random = np.reshape(mock_random, [2, 1, 1, 1])\n         input_images = np.random.random((2, 5, 8, 3)).astype(np.float32)\n         expected_output = input_images.copy()\n         expected_output[0, :, :, :] = np.flip(input_images[0, :, :, :], axis=1)\n@@ -646,7 +597,8 @@ class RandomFlipTest(test_combinations.TestCase):\n \n     def test_random_flip_vertical_half(self):\n         np.random.seed(1337)\n-        mock_random = [True, False]\n+        mock_random = [1, 0]\n+        mock_random = np.reshape(mock_random, [2, 1, 1, 1])\n         input_images = np.random.random((2, 5, 8, 3)).astype(np.float32)\n         expected_output = input_images.copy()\n         expected_output[0, :, :, :] = np.flip(input_images[0, :, :, :], axis=0)\n@@ -663,11 +615,12 @@ class RandomFlipTest(test_combinations.TestCase):\n     def test_random_flip_default(self):\n         input_images = np.random.random((2, 5, 8, 3)).astype(np.float32)\n         expected_output = np.flip(np.flip(input_images, axis=1), axis=2)\n-        mock_random = [True, True, True, True]\n+        mock_random = [1, 1]\n+        mock_random = np.reshape(mock_random, [2, 1, 1, 1])\n         with tf.compat.v1.test.mock.patch.object(\n-            np.random,\n-            \"choice\",\n-            side_effect=mock_random,\n+            stateless_random_ops,\n+            \"stateless_random_uniform\",\n+            return_value=mock_random,\n         ):\n             with self.cached_session():\n                 layer = image_preprocessing.RandomFlip()\n@@ -684,11 +637,11 @@ class RandomFlipTest(test_combinations.TestCase):\n     def test_random_flip_unbatched_image(self):\n         input_image = np.random.random((4, 4, 1)).astype(np.float32)\n         expected_output = np.flip(input_image, axis=0)\n-        mock_random = [True, True, True, True]\n+        # mock_random = np.reshape([0.], [1, 1, 1])\n         with tf.compat.v1.test.mock.patch.object(\n-            np.random,\n-            \"choice\",\n-            side_effect=mock_random,\n+            stateless_random_ops,\n+            \"stateless_random_uniform\",\n+            return_value=0.0,\n         ):\n             with self.cached_session():\n                 layer = image_preprocessing.RandomFlip(\"vertical\")\n@@ -703,28 +656,6 @@ class RandomFlipTest(test_combinations.TestCase):\n         layer = image_preprocessing.RandomFlip(dtype=\"uint8\")\n         self.assertAllEqual(layer(inputs).dtype, \"uint8\")\n \n-    @test_utils.run_v2_only\n-    def test_bounding_box_error(self):\n-        image = tf.zeros([20, 20, 3])\n-        bboxes = np.array(\n-            [\n-                [[0, 0, 10, 10], [4, 4, 12, 12]],\n-                [[0, 0, 10, 10], [4, 4, 12, 12]],\n-            ],\n-            dtype=\"int32\",\n-        )\n-        input = {\"images\": [image, image], \"bounding_boxes\": bboxes}\n-        layer = \"RandomFlip\"\n-        with self.assertRaisesRegex(\n-            NotImplementedError,\n-            \"In order to use bounding_boxes, \"\n-            \"please use \"\n-            f\"keras_cv.layers.{layer} \"\n-            f\"instead of keras.layers.{layer}.\",\n-        ):\n-            layer = image_preprocessing.RandomFlip()\n-            layer(input)\n-\n \n @test_combinations.run_all_keras_modes(always_skip_v1=True)\n class RandomContrastTest(test_combinations.TestCase):\n@@ -807,8 +738,8 @@ class RandomContrastTest(test_combinations.TestCase):\n \n     def test_output_value_clip(self):\n         input_images = np.random.random((5, 8, 3)).astype(np.float32) * 255.0\n-        # Give a factor range [1.0, 11.0] so that it will produce large\n-        # contrast.\n+        # Give a factor range [1.0, 11.0] so that\n+        # it will produce large contrast.\n         layer = image_preprocessing.RandomContrast((0.0, 10.0))\n         output = layer(input_images)\n         self.assertLessEqual(tf.reduce_max(output), 255.0)\n@@ -831,25 +762,6 @@ class RandomContrastTest(test_combinations.TestCase):\n                 actual_output = layer(inp, training=True)\n                 self.assertAllClose(expected_output, actual_output)\n \n-    def test_augment_image(self):\n-        np.random.seed(1337)\n-        mock_random = 0.2\n-        inp = np.random.random((4, 4, 1))\n-        inp_mean = np.mean(inp, axis=0, keepdims=True)\n-        inp_mean = np.mean(inp_mean, axis=1, keepdims=True)\n-        expected_output = (inp - inp_mean) * mock_random + inp_mean\n-        with tf.compat.v1.test.mock.patch.object(\n-            stateless_random_ops,\n-            \"stateless_random_uniform\",\n-            return_value=mock_random,\n-        ):\n-            with test_utils.use_gpu():\n-                layer = image_preprocessing.RandomContrast((0.2, 0.5))\n-                actual_output = layer.augment_image(\n-                    inp, transformation=layer.get_random_transformation()\n-                )\n-                self.assertAllClose(expected_output, actual_output)\n-\n     @test_utils.run_v2_only\n     def test_output_dtypes(self):\n         inputs = np.array([[[1], [2]], [[3], [4]]], dtype=\"float64\")\n@@ -918,27 +830,6 @@ class RandomBrightnessTest(test_combinations.TestCase):\n         self.assertLessEqual(tf.math.reduce_max(diff), 0)\n         self.assertLess(tf.math.reduce_mean(diff), 0)\n \n-    def test_augment_image(self):\n-        # Always scale up, but randomly between 0 ~ 255\n-        layer = image_preprocessing.RandomBrightness([0, 1.0])\n-        image = np.random.randint(0, 255, size=(224, 224, 3))\n-        output = layer.augment_image(\n-            image, transformation=layer.get_random_transformation()\n-        )\n-        diff = output - image\n-        self.assertGreaterEqual(tf.math.reduce_min(diff), 0)\n-        self.assertGreater(tf.math.reduce_mean(diff), 0)\n-\n-        # Always scale down, but randomly between 0 ~ 255\n-        layer = image_preprocessing.RandomBrightness([-1.0, 0.0])\n-        image = np.random.randint(0, 255, size=(224, 224, 3))\n-        output = layer.augment_image(\n-            image, transformation=layer.get_random_transformation()\n-        )\n-        diff = output - image\n-        self.assertLessEqual(tf.math.reduce_max(diff), 0)\n-        self.assertLess(tf.math.reduce_mean(diff), 0)\n-\n     @test_utils.run_v2_only\n     def test_scale_output(self):\n         layer = image_preprocessing.RandomBrightness([0, 1.0], seed=1337)\n@@ -1013,7 +904,6 @@ class RandomBrightnessTest(test_combinations.TestCase):\n         self.assertEqual(reconstructed_layer._seed, layer._seed)\n \n \n-@test_utils.run_v2_only\n @test_combinations.run_all_keras_modes(always_skip_v1=True)\n class RandomTranslationTest(test_combinations.TestCase):\n     def _run_test(self, height_factor, width_factor):\n@@ -1899,8 +1789,7 @@ class RandomRotationTest(test_combinations.TestCase):\n             self.assertAllClose(expected_output, actual_output)\n \n     def test_distribution_strategy(self):\n-        \"\"\"Tests that RandomRotation can be created within distribution\n-        strategies.\"\"\"\n+        \"\"\"Tests that RandomRotation can be created within DistStrats.\"\"\"\n         input_images = np.random.random((2, 5, 8, 3)).astype(np.float32)\n         with test_utils.use_gpu():\n             strat = tf.distribute.MirroredStrategy(devices=[\"cpu\", \"gpu\"])\n@@ -1937,28 +1826,6 @@ class RandomRotationTest(test_combinations.TestCase):\n             expected_output = np.reshape(expected_output, (5, 5, 1))\n             self.assertAllClose(expected_output, output_image)\n \n-    def test_augment_image(self):\n-        with test_utils.use_gpu():\n-            input_image = np.reshape(np.arange(0, 25), (5, 5, 1)).astype(\n-                np.float32\n-            )\n-            # 180 rotation.\n-            layer = image_preprocessing.RandomRotation(factor=(0.5, 0.5))\n-            output_image = layer.augment_image(\n-                input_image, transformation=layer.get_random_transformation()\n-            )\n-            expected_output = np.asarray(\n-                [\n-                    [24, 23, 22, 21, 20],\n-                    [19, 18, 17, 16, 15],\n-                    [14, 13, 12, 11, 10],\n-                    [9, 8, 7, 6, 5],\n-                    [4, 3, 2, 1, 0],\n-                ]\n-            ).astype(np.float32)\n-            expected_output = np.reshape(expected_output, (5, 5, 1))\n-            self.assertAllClose(expected_output, output_image)\n-\n     @test_utils.run_v2_only\n     def test_output_dtypes(self):\n         inputs = np.array([[[1], [2]], [[3], [4]]], dtype=\"float64\")\n@@ -2106,29 +1973,6 @@ class RandomZoomTest(test_combinations.TestCase):\n             expected_output = np.reshape(expected_output, (5, 5, 1))\n             self.assertAllEqual(expected_output, output_image)\n \n-    def test_augment_image(self):\n-        with test_utils.use_gpu():\n-            input_image = np.reshape(np.arange(0, 25), (5, 5, 1)).astype(\n-                np.int64\n-            )\n-            layer = image_preprocessing.RandomZoom(\n-                (-0.5, -0.5), (-0.5, -0.5), interpolation=\"nearest\"\n-            )\n-            output_image = layer.augment_image(\n-                input_image, transformation=layer.get_random_transformation()\n-            )\n-            expected_output = np.asarray(\n-                [\n-                    [6, 7, 7, 8, 8],\n-                    [11, 12, 12, 13, 13],\n-                    [11, 12, 12, 13, 13],\n-                    [16, 17, 17, 18, 18],\n-                    [16, 17, 17, 18, 18],\n-                ]\n-            ).astype(np.int64)\n-            expected_output = np.reshape(expected_output, (5, 5, 1))\n-            self.assertAllEqual(expected_output, output_image)\n-\n     @test_utils.run_v2_only\n     def test_output_dtypes(self):\n         inputs = np.array([[[1], [2]], [[3], [4]]], dtype=\"float64\")\n@@ -2185,9 +2029,10 @@ class RandomHeightTest(test_combinations.TestCase):\n                     dtype\n                 )\n                 layer = image_preprocessing.RandomHeight(factor=(1.0, 1.0))\n-                # Return type of RandomHeight() is float32 if `interpolation` is\n-                # not set to `ResizeMethod.NEAREST_NEIGHBOR`; cast `layer` to\n-                # desired dtype.\n+                # Return type of RandomHeight() is float32\n+                # if `interpolation` is not\n+                # set to `ResizeMethod.NEAREST_NEIGHBOR`;\n+                # cast `layer` to desired dtype.\n                 output_image = tf.cast(\n                     layer(np.expand_dims(input_image, axis=0)), dtype=dtype\n                 )\n@@ -2253,39 +2098,6 @@ class RandomHeightTest(test_combinations.TestCase):\n                 img_out = layer(img, training=True)\n                 self.assertEqual(img_out.shape[0], 3)\n \n-    @test_utils.run_v2_only\n-    def test_batched_input(self):\n-        # need (maxval - minval) * rnd + minval = 0.6\n-        mock_factor = 0.6\n-        with test_utils.use_gpu():\n-            images = np.random.random((5, 5, 8, 3))\n-            layer = image_preprocessing.RandomHeight(0.4)\n-            with tf.compat.v1.test.mock.patch.object(\n-                layer._random_generator,\n-                \"random_uniform\",\n-                return_value=mock_factor,\n-            ):\n-                img_out = layer(images, training=True)\n-                self.assertEqual(img_out.shape[1], 3)\n-\n-    @test_utils.run_v2_only\n-    def test_augment_image(self):\n-        # need (maxval - minval) * rnd + minval = 0.6\n-        mock_factor = 0.6\n-        with test_utils.use_gpu():\n-            img = np.random.random((5, 8, 3))\n-            layer = image_preprocessing.RandomHeight(0.4)\n-            with tf.compat.v1.test.mock.patch.object(\n-                layer._random_generator,\n-                \"random_uniform\",\n-                return_value=mock_factor,\n-            ):\n-                img_out = layer.augment_image(\n-                    img,\n-                    transformation=layer.get_random_transformation(image=img),\n-                )\n-                self.assertEqual(img_out.shape[0], 3)\n-\n     @test_utils.run_v2_only\n     def test_output_dtypes(self):\n         inputs = np.array([[[1], [2]], [[3], [4]]], dtype=\"float64\")\n@@ -2342,9 +2154,10 @@ class RandomWidthTest(test_combinations.TestCase):\n                     dtype\n                 )\n                 layer = image_preprocessing.RandomWidth(factor=(1.0, 1.0))\n-                # Return type of RandomWidth() is float32 if `interpolation` is\n-                # not set to `ResizeMethod.NEAREST_NEIGHBOR`; cast `layer` to\n-                # desired dtype.\n+                # Return type of RandomWidth() is float32\n+                # if `interpolation` is not\n+                # set to `ResizeMethod.NEAREST_NEIGHBOR`;\n+                # cast `layer` to desired dtype.\n                 output_image = tf.cast(\n                     layer(np.expand_dims(input_image, axis=0)), dtype=dtype\n                 )\n@@ -2405,39 +2218,6 @@ class RandomWidthTest(test_combinations.TestCase):\n                 img_out = layer(img, training=True)\n                 self.assertEqual(img_out.shape[1], 3)\n \n-    @test_utils.run_v2_only\n-    def test_batched_input(self):\n-        # need (maxval - minval) * rnd + minval = 0.6\n-        mock_factor = 0.6\n-        with test_utils.use_gpu():\n-            img = np.random.random((12, 8, 5, 3))\n-            layer = image_preprocessing.RandomWidth(0.4)\n-            with tf.compat.v1.test.mock.patch.object(\n-                layer._random_generator,\n-                \"random_uniform\",\n-                return_value=mock_factor,\n-            ):\n-                img_out = layer(img, training=True)\n-                self.assertEqual(img_out.shape[2], 3)\n-\n-    @test_utils.run_v2_only\n-    def test_augment_image(self):\n-        # need (maxval - minval) * rnd + minval = 0.6\n-        mock_factor = 0.6\n-        with test_utils.use_gpu():\n-            img = np.random.random((8, 5, 3))\n-            layer = image_preprocessing.RandomWidth(0.4)\n-            with tf.compat.v1.test.mock.patch.object(\n-                layer._random_generator,\n-                \"random_uniform\",\n-                return_value=mock_factor,\n-            ):\n-                img_out = layer.augment_image(\n-                    img,\n-                    transformation=layer.get_random_transformation(image=img),\n-                )\n-                self.assertEqual(img_out.shape[1], 3)\n-\n     @test_utils.run_v2_only\n     def test_output_dtypes(self):\n         inputs = np.array([[[1], [2]], [[3], [4]]], dtype=\"float64\")\n@@ -2447,33 +2227,6 @@ class RandomWidthTest(test_combinations.TestCase):\n         self.assertAllEqual(layer(inputs).dtype, \"uint8\")\n \n \n-@test_combinations.run_all_keras_modes(always_skip_v1=True)\n-class WithLabelsTest(test_combinations.TestCase):\n-    @parameterized.named_parameters(\n-        (\"RandomZoom\", image_preprocessing.RandomZoom, {\"height_factor\": 0.1}),\n-        (\n-            \"RandomBrightness\",\n-            image_preprocessing.RandomBrightness,\n-            {\"factor\": 0.5},\n-        ),\n-        (\"RandomContrast\", image_preprocessing.RandomContrast, {\"factor\": 0.5}),\n-        (\"RandomRotation\", image_preprocessing.RandomRotation, {\"factor\": 0.2}),\n-    )\n-    def test_layer_with_labels(self, layer_cls, init_args):\n-        layer = layer_cls(**init_args)\n-\n-        img = tf.random.uniform(\n-            shape=(3, 512, 512, 3), minval=0, maxval=1, dtype=tf.float32\n-        )\n-        labels = tf.constant(\n-            ([[1, 0, 0], [0, 0, 1], [0, 1, 0]]), dtype=tf.float32\n-        )\n-\n-        inputs = {\"images\": img, \"labels\": labels}\n-        outputs = layer(inputs)\n-        self.assertAllClose(labels, outputs[\"labels\"])\n-\n-\n @test_combinations.run_all_keras_modes(always_skip_v1=True)\n class LearningPhaseTest(test_combinations.TestCase):\n     def test_plain_call(self):\n@@ -2509,6 +2262,7 @@ class LearningPhaseTest(test_combinations.TestCase):\n @test_combinations.run_all_keras_modes(always_skip_v1=True)\n class DeterminismTest(test_combinations.TestCase):\n     @parameterized.named_parameters(\n+        (\"random_flip\", image_preprocessing.RandomFlip),\n         (\n             \"random_contrast\",\n             functools.partial(image_preprocessing.RandomContrast, factor=1.0),\n@@ -2550,124 +2304,5 @@ class DeterminismTest(test_combinations.TestCase):\n         )\n \n \n-class RandomAddLayer(image_preprocessing.BaseImageAugmentationLayer):\n-    def __init__(self, value_range=(0.0, 1.0), fixed_value=None, **kwargs):\n-        super().__init__(**kwargs)\n-        self.value_range = value_range\n-        self.fixed_value = fixed_value\n-\n-    def get_random_transformation(\n-        self, image=None, label=None, bounding_box=None\n-    ):\n-        if self.fixed_value:\n-            return self.fixed_value\n-        return self._random_generator.random_uniform(\n-            [], minval=self.value_range[0], maxval=self.value_range[1]\n-        )\n-\n-    def augment_image(self, image, transformation):\n-        return image + transformation\n-\n-    def augment_label(self, label, transformation):\n-        return label + transformation\n-\n-\n-class VectorizeDisabledLayer(image_preprocessing.BaseImageAugmentationLayer):\n-    def __init__(self, **kwargs):\n-        self.auto_vectorize = False\n-        super().__init__(**kwargs)\n-\n-\n-class FilterLayer(image_preprocessing.BaseImageAugmentationLayer):\n-    # Testing layer for check whether the training flag is set properly for KPL\n-\n-    def augment_image(self, image, transformation):\n-        # Returns zeros based on the original image\n-        return tf.zeros_like(image)\n-\n-\n-@test_combinations.run_all_keras_modes(always_skip_v1=True)\n-class BaseImageAugmentationLayerTest(test_combinations.TestCase):\n-    def test_augment_single_image(self):\n-        add_layer = RandomAddLayer(fixed_value=2.0)\n-        image = np.random.random(size=(8, 8, 3)).astype(\"float32\")\n-        output = add_layer(image)\n-\n-        self.assertAllClose(image + 2.0, output)\n-\n-    def test_augment_dict_return_type(self):\n-        add_layer = RandomAddLayer(fixed_value=2.0)\n-        image = np.random.random(size=(8, 8, 3)).astype(\"float32\")\n-        output = add_layer({\"images\": image})\n-\n-        self.assertIsInstance(output, dict)\n-\n-    def test_auto_vectorize_disabled(self):\n-        vectorize_disabled_layer = VectorizeDisabledLayer()\n-        self.assertFalse(vectorize_disabled_layer.auto_vectorize)\n-        self.assertEqual(vectorize_disabled_layer._map_fn, tf.map_fn)\n-\n-    @test_utils.run_v2_only\n-    def test_augment_casts_dtypes(self):\n-        add_layer = RandomAddLayer(fixed_value=2.0)\n-        images = tf.ones((2, 8, 8, 3), dtype=\"uint8\")\n-        output = add_layer(images)\n-\n-        self.assertAllClose(\n-            tf.ones((2, 8, 8, 3), dtype=\"float32\") * 3.0, output\n-        )\n-\n-    def test_augment_batch_images(self):\n-        add_layer = RandomAddLayer()\n-        images = np.random.random(size=(2, 8, 8, 3)).astype(\"float32\")\n-        output = add_layer(images)\n-\n-        diff = output - images\n-        # Make sure the first image and second image get different augmentation\n-        self.assertNotAllClose(diff[0], diff[1])\n-\n-    def test_augment_image_and_label(self):\n-        add_layer = RandomAddLayer(fixed_value=2.0)\n-        image = np.random.random(size=(8, 8, 3)).astype(\"float32\")\n-        label = np.random.random(size=(1,)).astype(\"float32\")\n-\n-        output = add_layer({\"images\": image, \"labels\": label})\n-        expected_output = {\"images\": image + 2.0, \"labels\": label + 2.0}\n-        self.assertAllClose(output, expected_output)\n-\n-    def test_augment_image_and_target(self):\n-        add_layer = RandomAddLayer(fixed_value=2.0)\n-        image = np.random.random(size=(8, 8, 3)).astype(\"float32\")\n-        label = np.random.random(size=(1,)).astype(\"float32\")\n-\n-        output = add_layer({\"images\": image, \"targets\": label})\n-        expected_output = {\"images\": image + 2.0, \"targets\": label + 2.0}\n-        self.assertAllClose(output, expected_output)\n-\n-    def test_augment_batch_images_and_labels(self):\n-        add_layer = RandomAddLayer()\n-        images = np.random.random(size=(2, 8, 8, 3)).astype(\"float32\")\n-        labels = np.random.random(size=(2, 1)).astype(\"float32\")\n-        output = add_layer({\"images\": images, \"labels\": labels})\n-\n-        image_diff = output[\"images\"] - images\n-        label_diff = output[\"labels\"] - labels\n-        # Make sure the first image and second image get different augmentation\n-        self.assertNotAllClose(image_diff[0], image_diff[1])\n-        self.assertNotAllClose(label_diff[0], label_diff[1])\n-\n-    def test_training_flag(self):\n-        # See b/251520266 for more details.\n-        inputs = tf.ones((10, 8, 8, 3), dtype=\"float32\")\n-        dropout = keras.layers.Dropout(rate=0.00001)\n-        filter = FilterLayer()\n-        output = dropout(inputs)\n-        output = filter(output)\n-\n-        # Make sure the outputs are all zeros, which the behavior for\n-        # FilterLayer when `training` is True\n-        self.assertAllClose(output, tf.zeros((10, 8, 8, 3), dtype=\"float32\"))\n-\n-\n if __name__ == \"__main__\":\n     tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
