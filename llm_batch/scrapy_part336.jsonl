{"custom_id": "scrapy#42056090516bb0cc5d349e232298c711ec452bc5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 287 | Contributors (this commit): 21 | Commits (past 90d): 4 | Contributors (cumulative): 21 | DMM Complexity: None\n\nDIFF:\n@@ -291,9 +291,10 @@ intersphinx_mapping = {\n     'pytest': ('https://docs.pytest.org/en/latest', None),\n     'python': ('https://docs.python.org/3', None),\n     'sphinx': ('https://www.sphinx-doc.org/en/master', None),\n-    'tox': ('https://tox.readthedocs.io/en/latest', None),\n-    'twisted': ('https://twistedmatrix.com/documents/current', None),\n-    'twistedapi': ('https://twistedmatrix.com/documents/current/api', None),\n+    'tox': ('https://tox.wiki/en/latest/', None),\n+    'twisted': ('https://docs.twisted.org/en/stable/', None),\n+    'twistedapi': ('https://docs.twisted.org/en/stable/api/', None),\n+    'w3lib': ('https://w3lib.readthedocs.io/en/latest', None),\n }\n intersphinx_disabled_reftypes = []\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b21c16099ed0acb0589677afd98c0ae3b78cd17d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 12/12 | Churn Δ: 8 | Churn Cumulative: 2313 | Contributors (this commit): 16 | Commits (past 90d): 3 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -305,8 +305,8 @@ class ProcessSpiderOutputInvalidResult(BaseAsyncSpiderMiddlewareTestCase):\n         with self.assertRaisesRegex(\n             _InvalidOutput,\n             (\n-                \"\\.process_spider_output must return an iterable, got <class \"\n-                \"'NoneType'>\"\n+                r\"\\.process_spider_output must return an iterable, got <class \"\n+                r\"'NoneType'>\"\n             ),\n         ):\n             yield self._get_middleware_result(\n@@ -317,7 +317,7 @@ class ProcessSpiderOutputInvalidResult(BaseAsyncSpiderMiddlewareTestCase):\n     def test_coroutine(self):\n         with self.assertRaisesRegex(\n             _InvalidOutput,\n-            \"\\.process_spider_output must be an asynchronous generator\",\n+            r\"\\.process_spider_output must be an asynchronous generator\",\n         ):\n             yield self._get_middleware_result(\n                 ProcessSpiderOutputCoroutineMiddleware,\n\n@@ -2,8 +2,6 @@ import functools\n import gc\n import operator\n import platform\n-import unittest\n-from datetime import datetime\n from itertools import count\n from warnings import catch_warnings, filterwarnings\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#af7dd16d8ded3e6cb2946603688f4f4a5212e80f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 331 | Lines Deleted: 28 | Files Changed: 2 | Hunks: 17 | Methods Changed: 23 | Complexity Δ (Sum/Max): 21/17 | Churn Δ: 359 | Churn Cumulative: 626 | Contributors (this commit): 16 | Commits (past 90d): 3 | Contributors (cumulative): 23 | DMM Complexity: 0.9427480916030534\n\nDIFF:\n@@ -45,31 +45,37 @@ class HttpProxyMiddleware:\n         return creds, proxy_url\n \n     def process_request(self, request, spider):\n-        # ignore if proxy is already set\n+        creds, proxy_url = None, None\n         if 'proxy' in request.meta:\n-            if request.meta['proxy'] is None:\n-                return\n-            # extract credentials if present\n+            if request.meta['proxy'] is not None:\n                 creds, proxy_url = self._get_proxy(request.meta['proxy'], '')\n-            request.meta['proxy'] = proxy_url\n-            if creds and not request.headers.get('Proxy-Authorization'):\n-                request.headers['Proxy-Authorization'] = b'Basic ' + creds\n-            return\n-        elif not self.proxies:\n-            return\n-\n+        elif self.proxies:\n             parsed = urlparse_cached(request)\n             scheme = parsed.scheme\n-\n+            if (\n+                (\n                     # 'no_proxy' is only supported by http schemes\n-        if scheme in ('http', 'https') and proxy_bypass(parsed.hostname):\n-            return\n+                    scheme not in ('http', 'https')\n+                    or not proxy_bypass(parsed.hostname)\n+                )\n+                and scheme in self.proxies\n+            ):\n+                creds, proxy_url = self.proxies[scheme]\n \n-        if scheme in self.proxies:\n-            self._set_proxy(request, scheme)\n+        self._set_proxy_and_creds(request, proxy_url, creds)\n \n-    def _set_proxy(self, request, scheme):\n-        creds, proxy = self.proxies[scheme]\n-        request.meta['proxy'] = proxy\n+    def _set_proxy_and_creds(self, request, proxy_url, creds):\n+        if proxy_url:\n+            request.meta['proxy'] = proxy_url\n+        elif request.meta.get('proxy') is not None:\n+            request.meta['proxy'] = None\n         if creds:\n-            request.headers['Proxy-Authorization'] = b'Basic ' + creds\n+            request.headers[b'Proxy-Authorization'] = b'Basic ' + creds\n+            request.meta['_auth_proxy'] = proxy_url\n+        elif '_auth_proxy' in request.meta:\n+            if proxy_url != request.meta['_auth_proxy']:\n+                if b'Proxy-Authorization' in request.headers:\n+                    del request.headers[b'Proxy-Authorization']\n+                del request.meta['_auth_proxy']\n+        elif b'Proxy-Authorization' in request.headers:\n+            del request.headers[b'Proxy-Authorization']\n\n@@ -65,12 +65,12 @@ class TestHttpProxyMiddleware(TestCase):\n         mw = HttpProxyMiddleware()\n         req = Request('http://scrapytest.org')\n         assert mw.process_request(req, spider) is None\n-        self.assertEqual(req.meta, {'proxy': 'https://proxy:3128'})\n+        self.assertEqual(req.meta['proxy'], 'https://proxy:3128')\n         self.assertEqual(req.headers.get('Proxy-Authorization'), b'Basic dXNlcjpwYXNz')\n         # proxy from request.meta\n         req = Request('http://scrapytest.org', meta={'proxy': 'https://username:password@proxy:3128'})\n         assert mw.process_request(req, spider) is None\n-        self.assertEqual(req.meta, {'proxy': 'https://proxy:3128'})\n+        self.assertEqual(req.meta['proxy'], 'https://proxy:3128')\n         self.assertEqual(req.headers.get('Proxy-Authorization'), b'Basic dXNlcm5hbWU6cGFzc3dvcmQ=')\n \n     def test_proxy_auth_empty_passwd(self):\n@@ -78,12 +78,12 @@ class TestHttpProxyMiddleware(TestCase):\n         mw = HttpProxyMiddleware()\n         req = Request('http://scrapytest.org')\n         assert mw.process_request(req, spider) is None\n-        self.assertEqual(req.meta, {'proxy': 'https://proxy:3128'})\n+        self.assertEqual(req.meta['proxy'], 'https://proxy:3128')\n         self.assertEqual(req.headers.get('Proxy-Authorization'), b'Basic dXNlcjo=')\n         # proxy from request.meta\n         req = Request('http://scrapytest.org', meta={'proxy': 'https://username:@proxy:3128'})\n         assert mw.process_request(req, spider) is None\n-        self.assertEqual(req.meta, {'proxy': 'https://proxy:3128'})\n+        self.assertEqual(req.meta['proxy'], 'https://proxy:3128')\n         self.assertEqual(req.headers.get('Proxy-Authorization'), b'Basic dXNlcm5hbWU6')\n \n     def test_proxy_auth_encoding(self):\n@@ -92,26 +92,26 @@ class TestHttpProxyMiddleware(TestCase):\n         mw = HttpProxyMiddleware(auth_encoding='utf-8')\n         req = Request('http://scrapytest.org')\n         assert mw.process_request(req, spider) is None\n-        self.assertEqual(req.meta, {'proxy': 'https://proxy:3128'})\n+        self.assertEqual(req.meta['proxy'], 'https://proxy:3128')\n         self.assertEqual(req.headers.get('Proxy-Authorization'), b'Basic bcOhbjpwYXNz')\n \n         # proxy from request.meta\n         req = Request('http://scrapytest.org', meta={'proxy': 'https://\\u00FCser:pass@proxy:3128'})\n         assert mw.process_request(req, spider) is None\n-        self.assertEqual(req.meta, {'proxy': 'https://proxy:3128'})\n+        self.assertEqual(req.meta['proxy'], 'https://proxy:3128')\n         self.assertEqual(req.headers.get('Proxy-Authorization'), b'Basic w7xzZXI6cGFzcw==')\n \n         # default latin-1 encoding\n         mw = HttpProxyMiddleware(auth_encoding='latin-1')\n         req = Request('http://scrapytest.org')\n         assert mw.process_request(req, spider) is None\n-        self.assertEqual(req.meta, {'proxy': 'https://proxy:3128'})\n+        self.assertEqual(req.meta['proxy'], 'https://proxy:3128')\n         self.assertEqual(req.headers.get('Proxy-Authorization'), b'Basic beFuOnBhc3M=')\n \n         # proxy from request.meta, latin-1 encoding\n         req = Request('http://scrapytest.org', meta={'proxy': 'https://\\u00FCser:pass@proxy:3128'})\n         assert mw.process_request(req, spider) is None\n-        self.assertEqual(req.meta, {'proxy': 'https://proxy:3128'})\n+        self.assertEqual(req.meta['proxy'], 'https://proxy:3128')\n         self.assertEqual(req.headers.get('Proxy-Authorization'), b'Basic /HNlcjpwYXNz')\n \n     def test_proxy_already_seted(self):\n@@ -152,3 +152,300 @@ class TestHttpProxyMiddleware(TestCase):\n         # '/var/run/docker.sock' may be used by the user for\n         # no_proxy value but is not parseable and should be skipped\n         assert 'no' not in mw.proxies\n+\n+    def test_add_proxy_without_credentials(self):\n+        middleware = HttpProxyMiddleware()\n+        request = Request('https://example.com')\n+        assert middleware.process_request(request, spider) is None\n+        request.meta['proxy'] = 'https://example.com'\n+        assert middleware.process_request(request, spider) is None\n+        self.assertEqual(request.meta['proxy'], 'https://example.com')\n+        self.assertNotIn(b'Proxy-Authorization', request.headers)\n+\n+    def test_add_proxy_with_credentials(self):\n+        middleware = HttpProxyMiddleware()\n+        request = Request('https://example.com')\n+        assert middleware.process_request(request, spider) is None\n+        request.meta['proxy'] = 'https://user1:password1@example.com'\n+        assert middleware.process_request(request, spider) is None\n+        self.assertEqual(request.meta['proxy'], 'https://example.com')\n+        encoded_credentials = middleware._basic_auth_header(\n+            'user1',\n+            'password1',\n+        )\n+        self.assertEqual(\n+            request.headers['Proxy-Authorization'],\n+            b'Basic ' + encoded_credentials,\n+        )\n+\n+    def test_remove_proxy_without_credentials(self):\n+        middleware = HttpProxyMiddleware()\n+        request = Request(\n+            'https://example.com',\n+            meta={'proxy': 'https://example.com'},\n+        )\n+        assert middleware.process_request(request, spider) is None\n+        request.meta['proxy'] = None\n+        assert middleware.process_request(request, spider) is None\n+        self.assertIsNone(request.meta['proxy'])\n+        self.assertNotIn(b'Proxy-Authorization', request.headers)\n+\n+    def test_remove_proxy_with_credentials(self):\n+        middleware = HttpProxyMiddleware()\n+        request = Request(\n+            'https://example.com',\n+            meta={'proxy': 'https://user1:password1@example.com'},\n+        )\n+        assert middleware.process_request(request, spider) is None\n+        request.meta['proxy'] = None\n+        assert middleware.process_request(request, spider) is None\n+        self.assertIsNone(request.meta['proxy'])\n+        self.assertNotIn(b'Proxy-Authorization', request.headers)\n+\n+    def test_add_credentials(self):\n+        \"\"\"If the proxy request meta switches to a proxy URL with the same\n+        proxy and adds credentials (there were no credentials before), the new\n+        credentials must be used.\"\"\"\n+        middleware = HttpProxyMiddleware()\n+        request = Request(\n+            'https://example.com',\n+            meta={'proxy': 'https://example.com'},\n+        )\n+        assert middleware.process_request(request, spider) is None\n+\n+        request.meta['proxy'] = 'https://user1:password1@example.com'\n+        assert middleware.process_request(request, spider) is None\n+        self.assertEqual(request.meta['proxy'], 'https://example.com')\n+        encoded_credentials = middleware._basic_auth_header(\n+            'user1',\n+            'password1',\n+        )\n+        self.assertEqual(\n+            request.headers['Proxy-Authorization'],\n+            b'Basic ' + encoded_credentials,\n+        )\n+\n+    def test_change_credentials(self):\n+        \"\"\"If the proxy request meta switches to a proxy URL with different\n+        credentials, those new credentials must be used.\"\"\"\n+        middleware = HttpProxyMiddleware()\n+        request = Request(\n+            'https://example.com',\n+            meta={'proxy': 'https://user1:password1@example.com'},\n+        )\n+        assert middleware.process_request(request, spider) is None\n+        request.meta['proxy'] = 'https://user2:password2@example.com'\n+        assert middleware.process_request(request, spider) is None\n+        self.assertEqual(request.meta['proxy'], 'https://example.com')\n+        encoded_credentials = middleware._basic_auth_header(\n+            'user2',\n+            'password2',\n+        )\n+        self.assertEqual(\n+            request.headers['Proxy-Authorization'],\n+            b'Basic ' + encoded_credentials,\n+        )\n+\n+    def test_remove_credentials(self):\n+        \"\"\"If the proxy request meta switches to a proxy URL with the same\n+        proxy but no credentials, the original credentials must be still\n+        used.\n+\n+        To remove credentials while keeping the same proxy URL, users must\n+        delete the Proxy-Authorization header.\n+        \"\"\"\n+        middleware = HttpProxyMiddleware()\n+        request = Request(\n+            'https://example.com',\n+            meta={'proxy': 'https://user1:password1@example.com'},\n+        )\n+        assert middleware.process_request(request, spider) is None\n+\n+        request.meta['proxy'] = 'https://example.com'\n+        assert middleware.process_request(request, spider) is None\n+        self.assertEqual(request.meta['proxy'], 'https://example.com')\n+        encoded_credentials = middleware._basic_auth_header(\n+            'user1',\n+            'password1',\n+        )\n+        self.assertEqual(\n+            request.headers['Proxy-Authorization'],\n+            b'Basic ' + encoded_credentials,\n+        )\n+\n+        request.meta['proxy'] = 'https://example.com'\n+        del request.headers[b'Proxy-Authorization']\n+        assert middleware.process_request(request, spider) is None\n+        self.assertEqual(request.meta['proxy'], 'https://example.com')\n+        self.assertNotIn(b'Proxy-Authorization', request.headers)\n+\n+    def test_change_proxy_add_credentials(self):\n+        middleware = HttpProxyMiddleware()\n+        request = Request(\n+            'https://example.com',\n+            meta={'proxy': 'https://example.com'},\n+        )\n+        assert middleware.process_request(request, spider) is None\n+\n+        request.meta['proxy'] = 'https://user1:password1@example.org'\n+        assert middleware.process_request(request, spider) is None\n+        self.assertEqual(request.meta['proxy'], 'https://example.org')\n+        encoded_credentials = middleware._basic_auth_header(\n+            'user1',\n+            'password1',\n+        )\n+        self.assertEqual(\n+            request.headers['Proxy-Authorization'],\n+            b'Basic ' + encoded_credentials,\n+        )\n+\n+    def test_change_proxy_keep_credentials(self):\n+        middleware = HttpProxyMiddleware()\n+        request = Request(\n+            'https://example.com',\n+            meta={'proxy': 'https://user1:password1@example.com'},\n+        )\n+        assert middleware.process_request(request, spider) is None\n+\n+        request.meta['proxy'] = 'https://user1:password1@example.org'\n+        assert middleware.process_request(request, spider) is None\n+        self.assertEqual(request.meta['proxy'], 'https://example.org')\n+        encoded_credentials = middleware._basic_auth_header(\n+            'user1',\n+            'password1',\n+        )\n+        self.assertEqual(\n+            request.headers['Proxy-Authorization'],\n+            b'Basic ' + encoded_credentials,\n+        )\n+\n+        # Make sure, indirectly, that _auth_proxy is updated.\n+        request.meta['proxy'] = 'https://example.com'\n+        assert middleware.process_request(request, spider) is None\n+        self.assertEqual(request.meta['proxy'], 'https://example.com')\n+        self.assertNotIn(b'Proxy-Authorization', request.headers)\n+\n+    def test_change_proxy_change_credentials(self):\n+        middleware = HttpProxyMiddleware()\n+        request = Request(\n+            'https://example.com',\n+            meta={'proxy': 'https://user1:password1@example.com'},\n+        )\n+        assert middleware.process_request(request, spider) is None\n+\n+        request.meta['proxy'] = 'https://user2:password2@example.org'\n+        assert middleware.process_request(request, spider) is None\n+        self.assertEqual(request.meta['proxy'], 'https://example.org')\n+        encoded_credentials = middleware._basic_auth_header(\n+            'user2',\n+            'password2',\n+        )\n+        self.assertEqual(\n+            request.headers['Proxy-Authorization'],\n+            b'Basic ' + encoded_credentials,\n+        )\n+\n+    def test_change_proxy_remove_credentials(self):\n+        \"\"\"If the proxy request meta switches to a proxy URL with a different\n+        proxy and no credentials, no credentials must be used.\"\"\"\n+        middleware = HttpProxyMiddleware()\n+        request = Request(\n+            'https://example.com',\n+            meta={'proxy': 'https://user1:password1@example.com'},\n+        )\n+        assert middleware.process_request(request, spider) is None\n+        request.meta['proxy'] = 'https://example.org'\n+        assert middleware.process_request(request, spider) is None\n+        self.assertEqual(request.meta, {'proxy': 'https://example.org'})\n+        self.assertNotIn(b'Proxy-Authorization', request.headers)\n+\n+    def test_change_proxy_remove_credentials_preremoved_header(self):\n+        \"\"\"Corner case of proxy switch with credentials removal where the\n+        credentials have been removed beforehand.\n+\n+        It ensures that our implementation does not assume that the credentials\n+        header exists when trying to remove it.\n+        \"\"\"\n+        middleware = HttpProxyMiddleware()\n+        request = Request(\n+            'https://example.com',\n+            meta={'proxy': 'https://user1:password1@example.com'},\n+        )\n+        assert middleware.process_request(request, spider) is None\n+        request.meta['proxy'] = 'https://example.org'\n+        del request.headers[b'Proxy-Authorization']\n+        assert middleware.process_request(request, spider) is None\n+        self.assertEqual(request.meta, {'proxy': 'https://example.org'})\n+        self.assertNotIn(b'Proxy-Authorization', request.headers)\n+\n+    def test_proxy_authentication_header_undefined_proxy(self):\n+        middleware = HttpProxyMiddleware()\n+        request = Request(\n+            'https://example.com',\n+            headers={'Proxy-Authorization': 'Basic foo'},\n+        )\n+        assert middleware.process_request(request, spider) is None\n+        self.assertNotIn('proxy', request.meta)\n+        self.assertNotIn(b'Proxy-Authorization', request.headers)\n+\n+    def test_proxy_authentication_header_disabled_proxy(self):\n+        middleware = HttpProxyMiddleware()\n+        request = Request(\n+            'https://example.com',\n+            headers={'Proxy-Authorization': 'Basic foo'},\n+            meta={'proxy': None},\n+        )\n+        assert middleware.process_request(request, spider) is None\n+        self.assertIsNone(request.meta['proxy'])\n+        self.assertNotIn(b'Proxy-Authorization', request.headers)\n+\n+    def test_proxy_authentication_header_proxy_without_credentials(self):\n+        middleware = HttpProxyMiddleware()\n+        request = Request(\n+            'https://example.com',\n+            headers={'Proxy-Authorization': 'Basic foo'},\n+            meta={'proxy': 'https://example.com'},\n+        )\n+        assert middleware.process_request(request, spider) is None\n+        self.assertEqual(request.meta['proxy'], 'https://example.com')\n+        self.assertNotIn(b'Proxy-Authorization', request.headers)\n+\n+    def test_proxy_authentication_header_proxy_with_same_credentials(self):\n+        middleware = HttpProxyMiddleware()\n+        encoded_credentials = middleware._basic_auth_header(\n+            'user1',\n+            'password1',\n+        )\n+        request = Request(\n+            'https://example.com',\n+            headers={'Proxy-Authorization': b'Basic ' + encoded_credentials},\n+            meta={'proxy': 'https://user1:password1@example.com'},\n+        )\n+        assert middleware.process_request(request, spider) is None\n+        self.assertEqual(request.meta['proxy'], 'https://example.com')\n+        self.assertEqual(\n+            request.headers['Proxy-Authorization'],\n+            b'Basic ' + encoded_credentials,\n+        )\n+\n+    def test_proxy_authentication_header_proxy_with_different_credentials(self):\n+        middleware = HttpProxyMiddleware()\n+        encoded_credentials1 = middleware._basic_auth_header(\n+            'user1',\n+            'password1',\n+        )\n+        request = Request(\n+            'https://example.com',\n+            headers={'Proxy-Authorization': b'Basic ' + encoded_credentials1},\n+            meta={'proxy': 'https://user2:password2@example.com'},\n+        )\n+        assert middleware.process_request(request, spider) is None\n+        self.assertEqual(request.meta['proxy'], 'https://example.com')\n+        encoded_credentials2 = middleware._basic_auth_header(\n+            'user2',\n+            'password2',\n+        )\n+        self.assertEqual(\n+            request.headers['Proxy-Authorization'],\n+            b'Basic ' + encoded_credentials2,\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#56e2eeac1066cd2d3c710b76a3d6210b3ac257f5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 770 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -52,7 +52,7 @@ class UtilsRequestTest(unittest.TestCase):\n class FingerprintTest(unittest.TestCase):\n     maxDiff = None\n \n-    function = staticmethod(fingerprint)\n+    function: staticmethod = staticmethod(fingerprint)\n     cache: Union[\n         \"WeakKeyDictionary[Request, Dict[Tuple[Optional[Tuple[bytes, ...]], bool], bytes]]\",\n         \"WeakKeyDictionary[Request, Dict[Tuple[Optional[Tuple[bytes, ...]], bool], str]]\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c7b90c6e1e3de20256d8bf6fc5d18da44d562b0d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 17 | Churn Cumulative: 94 | Contributors (this commit): 6 | Commits (past 90d): 1 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -28,25 +28,22 @@ class DepthMiddleware:\n         return cls(maxdepth, crawler.stats, verbose, prio)\n \n     def process_spider_output(self, response, result, spider):\n-        # base case (depth=0)\n-        if 'depth' not in response.meta:\n-            response.meta['depth'] = 0\n-            if self.verbose_stats:\n-                self.stats.inc_value('request_depth_count/0', spider=spider)\n-\n+        self._init_depth(response, spider)\n         return (r for r in result or () if self._filter(r, response, spider))\n \n     async def process_spider_output_async(self, response, result, spider):\n+        self._init_depth(response, spider)\n+        async for r in result or ():\n+            if self._filter(r, response, spider):\n+                yield r\n+\n+    def _init_depth(self, response, spider):\n         # base case (depth=0)\n         if 'depth' not in response.meta:\n             response.meta['depth'] = 0\n             if self.verbose_stats:\n                 self.stats.inc_value('request_depth_count/0', spider=spider)\n \n-        async for r in result or ():\n-            if self._filter(r, response, spider):\n-                yield r\n-\n     def _filter(self, request, response, spider):\n         if not isinstance(request, Request):\n             return True\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5735e93541d26ad3aef5e4fc45fa3d1c4dc0fa2a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 10 | Churn Cumulative: 1234 | Contributors (this commit): 32 | Commits (past 90d): 1 | Contributors (cumulative): 32 | DMM Complexity: 1.0\n\nDIFF:\n@@ -350,13 +350,19 @@ class FeedExporter:\n         return defer.DeferredList(deferred_list) if deferred_list else None\n \n     def _close_slot(self, slot, spider):\n+\n+        def get_file(slot_):\n+            if isinstance(slot_.file, PostProcessingManager):\n+                return slot_.file.file\n+            return slot_.file\n+\n         if not slot.itemcount and not slot.store_empty:\n             # We need to call slot.storage.store nonetheless to get the file\n             # properly closed.\n-            return defer.maybeDeferred(slot.storage.store, slot.file)\n+            return defer.maybeDeferred(slot.storage.store, get_file(slot))\n         slot.finish_exporting()\n         logmsg = f\"{slot.format} feed ({slot.itemcount} items) in: {slot.uri}\"\n-        d = defer.maybeDeferred(slot.storage.store, slot.file)\n+        d = defer.maybeDeferred(slot.storage.store, get_file(slot))\n \n         d.addCallback(\n             self._handle_store_success, logmsg, spider, type(slot.storage).__name__\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4be9c969fd4bdc3206bdabfb5c22e524d9e355ac", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 850 | Contributors (this commit): 13 | Commits (past 90d): 2 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,3 @@\n-import logging\n import random\n from time import time\n from datetime import datetime\n@@ -13,8 +12,6 @@ from scrapy import signals\n from scrapy.core.downloader.middleware import DownloaderMiddlewareManager\n from scrapy.core.downloader.handlers import DownloadHandlers\n \n-logger = logging.getLogger(__name__)\n-\n \n class Slot:\n     \"\"\"Downloader slot\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d599fff2b97ff96c3face707b980926b6eb48a2b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 55 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 8 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 55 | Churn Cumulative: 4623 | Contributors (this commit): 32 | Commits (past 90d): 9 | Contributors (cumulative): 32 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1517,6 +1517,61 @@ class FeedExportTest(FeedExportTestBase):\n             data = yield self.exported_data(items, settings)\n             self.assertEqual(row['expected'], data[feed_options['format']])\n \n+    @defer.inlineCallbacks\n+    def test_storage_file_no_postprocessing(self):\n+\n+        @implementer(IFeedStorage)\n+        class Storage:\n+\n+            def __init__(self, uri, *, feed_options=None):\n+                pass\n+\n+            def open(self, spider):\n+                Storage.open_file = tempfile.NamedTemporaryFile(prefix='feed-')\n+                return Storage.open_file\n+\n+            def store(self, file):\n+                Storage.store_file = file\n+                file.close()\n+\n+        settings = {\n+            'FEEDS': {self._random_temp_filename(): {'format': 'jsonlines'}},\n+            'FEED_STORAGES': {'file': Storage},\n+        }\n+        yield self.exported_no_data(settings)\n+        self.assertIs(Storage.open_file, Storage.store_file)\n+\n+    @defer.inlineCallbacks\n+    def test_storage_file_postprocessing(self):\n+\n+        @implementer(IFeedStorage)\n+        class Storage:\n+\n+            def __init__(self, uri, *, feed_options=None):\n+                pass\n+\n+            def open(self, spider):\n+                Storage.open_file = tempfile.NamedTemporaryFile(prefix='feed-')\n+                return Storage.open_file\n+\n+            def store(self, file):\n+                Storage.store_file = file\n+                file.close()\n+\n+        settings = {\n+            'FEEDS': {\n+                self._random_temp_filename(): {\n+                    'format': 'jsonlines',\n+                    'postprocessing': [\n+                        'scrapy.extensions.postprocessing.GzipPlugin',\n+                    ],\n+                },\n+            },\n+            'FEED_STORAGES': {'file': Storage},\n+        }\n+        yield self.exported_no_data(settings)\n+        self.assertIs(Storage.open_file, Storage.store_file)\n+\n \n class FeedPostProcessedExportsTest(FeedExportTestBase):\n     __test__ = True\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#aabdd0b657e1b4398a5a1aad79044401e9dbf909", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 853 | Contributors (this commit): 13 | Commits (past 90d): 2 | Contributors (cumulative): 13 | DMM Complexity: 0.0\n\nDIFF:\n@@ -204,5 +204,4 @@ class Downloader:\n         mintime = time() - age\n         for key, slot in list(self.slots.items()):\n             if not slot.active and slot.lastseen + slot.delay < mintime:\n-                inactive_slot = self.slots.pop(key)\n-                inactive_slot.close()\n+                self.slots.pop(key).close()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#13c5ad7e688271ae47f2a489bf665cb15387b3a3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 638 | Contributors (this commit): 31 | Commits (past 90d): 1 | Contributors (cumulative): 31 | DMM Complexity: 0.0\n\nDIFF:\n@@ -9,6 +9,7 @@ from collections import deque\n from contextlib import contextmanager\n from importlib import import_module\n from pkgutil import iter_modules\n+from functools import partial\n \n from w3lib.html import replace_entities\n \n@@ -226,7 +227,11 @@ def is_generator_with_return_value(callable):\n         return value is None or isinstance(value, ast.NameConstant) and value.value is None\n \n     if inspect.isgeneratorfunction(callable):\n-        code = re.sub(r\"^[\\t ]+\", \"\", inspect.getsource(callable))\n+        func = callable\n+        while isinstance(func, partial):\n+            func = func.func\n+\n+        code = re.sub(r\"^[\\t ]+\", \"\", inspect.getsource(func))\n         tree = ast.parse(code)\n         for node in walk_callable(tree):\n             if isinstance(node, ast.Return) and not returns_none(node):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e411ea94eb853eedeee111469c6442d275864a09", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 14 | Churn Cumulative: 793 | Contributors (this commit): 17 | Commits (past 90d): 1 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,8 +11,13 @@ from typing import Generator, Tuple\n from urllib.parse import urljoin\n \n import parsel\n-from w3lib.encoding import (html_body_declared_encoding, html_to_unicode,\n-                            http_content_type_encoding, resolve_encoding)\n+from w3lib.encoding import (\n+    html_body_declared_encoding,\n+    html_to_unicode,\n+    http_content_type_encoding,\n+    resolve_encoding,\n+    read_bom,\n+)\n from w3lib.html import strip_html5_whitespace\n \n from scrapy.http import Request\n@@ -60,6 +65,7 @@ class TextResponse(Response):\n     def _declared_encoding(self):\n         return (\n             self._encoding\n+            or self._bom_encoding()\n             or self._headers_encoding()\n             or self._body_declared_encoding()\n         )\n@@ -117,6 +123,10 @@ class TextResponse(Response):\n     def _body_declared_encoding(self):\n         return html_body_declared_encoding(self.body)\n \n+    @memoizemethod_noargs\n+    def _bom_encoding(self):\n+        return read_bom(self.body)[0]\n+\n     @property\n     def selector(self):\n         from scrapy.selector import Selector\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a988c4b78b9ca104b74cff60c46c1842e3f25652", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1084 | Contributors (this commit): 21 | Commits (past 90d): 1 | Contributors (cumulative): 21 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,4 @@\n+import codecs\n import unittest\n from unittest import mock\n \n@@ -358,6 +359,8 @@ class TextResponseTest(BaseResponseTest):\n                                  headers={\"Content-type\": [\"text/html; charset=gb2312\"]})\n         r7 = self.response_class(\"http://www.example.com\", body=b\"\\xa8D\",\n                                  headers={\"Content-type\": [\"text/html; charset=gbk\"]})\n+        r8 = self.response_class(\"http://www.example.com\", body=codecs.BOM_UTF8 + b\"\\xc2\\xa3\",\n+                                 headers={\"Content-type\": [\"text/html; charset=cp1251\"]})\n \n         self.assertEqual(r1._headers_encoding(), \"utf-8\")\n         self.assertEqual(r2._headers_encoding(), None)\n@@ -367,7 +370,10 @@ class TextResponseTest(BaseResponseTest):\n         self.assertEqual(r3._declared_encoding(), \"cp1252\")\n         self.assertEqual(r4._headers_encoding(), None)\n         self.assertEqual(r5._headers_encoding(), None)\n+        self.assertEqual(r8._headers_encoding(), \"cp1251\")\n+        self.assertEqual(r8._declared_encoding(), \"utf-8\")\n         self._assert_response_encoding(r5, \"utf-8\")\n+        self._assert_response_encoding(r8, \"utf-8\")\n         assert r4._body_inferred_encoding() is not None and r4._body_inferred_encoding() != 'ascii'\n         self._assert_response_values(r1, 'utf-8', \"\\xa3\")\n         self._assert_response_values(r2, 'utf-8', \"\\xa3\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f4bcc3e67de0896999be48d1b0fcede0be0d69e4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 9/8 | Churn Δ: 15 | Churn Cumulative: 1770 | Contributors (this commit): 25 | Commits (past 90d): 4 | Contributors (cumulative): 35 | DMM Complexity: 1.0\n\nDIFF:\n@@ -23,6 +23,8 @@ from scrapy.utils.test import get_crawler\n from scrapy.extensions.throttle import AutoThrottle\n from scrapy.extensions import telnet\n from scrapy.utils.test import get_testenv\n+from pkg_resources import parse_version\n+from w3lib import __version__ as w3lib_version\n \n from tests.mockserver import MockServer\n \n@@ -381,10 +383,13 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n     def test_ipv6_default_name_resolver(self):\n         log = self.run_script('default_name_resolver.py')\n         self.assertIn('Spider closed (finished)', log)\n+        if parse_version(w3lib_version) < parse_version(\"2.0.0\"):\n             self.assertIn(\"'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 1,\", log)\n             self.assertIn(\n                 \"twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: ::1.\",\n                 log)\n+        else:\n+            self.assertIn(\"ValueError: invalid hostname:\", log)\n \n     def test_caching_hostname_resolver_ipv6(self):\n         log = self.run_script(\"caching_hostname_resolver_ipv6.py\")\n\n@@ -22,6 +22,8 @@ from scrapy.spiders import (\n from scrapy.linkextractors import LinkExtractor\n from scrapy.utils.test import get_crawler\n from tests import get_testdata\n+from pkg_resources import parse_version\n+from w3lib import __version__ as w3lib_version\n \n \n class SpiderTest(unittest.TestCase):\n@@ -360,10 +362,12 @@ class CrawlSpiderTest(SpiderTest):\n         output = list(spider._requests_to_follow(response))\n         self.assertEqual(len(output), 3)\n         self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))\n-        self.assertEqual([r.url for r in output],\n-                         ['http://EXAMPLE.ORG/SOMEPAGE/ITEM/12.HTML',\n+        urls = ['http://EXAMPLE.ORG/SOMEPAGE/ITEM/12.HTML',\n                 'http://EXAMPLE.ORG/ABOUT.HTML',\n-                          'http://EXAMPLE.ORG/NOFOLLOW.HTML'])\n+                'http://EXAMPLE.ORG/NOFOLLOW.HTML']\n+        if parse_version(w3lib_version) >= parse_version('2.0.0'):\n+            urls = list(map(lambda u: u.replace(\"EXAMPLE.ORG\", \"example.org\"), urls))\n+        self.assertEqual([r.url for r in output], urls)\n \n     def test_process_request_instance_method_with_response(self):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1289422284991c09cbee8cea17bef837b3efc31b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 5 | Churn Cumulative: 1055 | Contributors (this commit): 16 | Commits (past 90d): 4 | Contributors (cumulative): 16 | DMM Complexity: 0.0\n\nDIFF:\n@@ -380,16 +380,15 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         self.assertIn('Spider closed (finished)', log)\n         self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n \n+    @mark.skipif(parse_version(w3lib_version) >= parse_version(\"2.0.0\"),\n+                 reason='w3lib 2.0.0 and later do not allow invalid domains.')\n     def test_ipv6_default_name_resolver(self):\n         log = self.run_script('default_name_resolver.py')\n         self.assertIn('Spider closed (finished)', log)\n-        if parse_version(w3lib_version) < parse_version(\"2.0.0\"):\n         self.assertIn(\"'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 1,\", log)\n         self.assertIn(\n             \"twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: ::1.\",\n             log)\n-        else:\n-            self.assertIn(\"ValueError: invalid hostname:\", log)\n \n     def test_caching_hostname_resolver_ipv6(self):\n         log = self.run_script(\"caching_hostname_resolver_ipv6.py\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#582a6bf6dbcb01da40cbec6b51269add2db39cf1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 13 | Churn Cumulative: 733 | Contributors (this commit): 19 | Commits (past 90d): 2 | Contributors (cumulative): 19 | DMM Complexity: 0.0\n\nDIFF:\n@@ -22,8 +22,7 @@ from scrapy.spiders import (\n from scrapy.linkextractors import LinkExtractor\n from scrapy.utils.test import get_crawler\n from tests import get_testdata\n-from pkg_resources import parse_version\n-from w3lib import __version__ as w3lib_version\n+from w3lib.url import safe_url_string\n \n \n class SpiderTest(unittest.TestCase):\n@@ -362,12 +361,10 @@ class CrawlSpiderTest(SpiderTest):\n         output = list(spider._requests_to_follow(response))\n         self.assertEqual(len(output), 3)\n         self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))\n-        urls = ['http://EXAMPLE.ORG/SOMEPAGE/ITEM/12.HTML',\n-                'http://EXAMPLE.ORG/ABOUT.HTML',\n-                'http://EXAMPLE.ORG/NOFOLLOW.HTML']\n-        if parse_version(w3lib_version) >= parse_version('2.0.0'):\n-            urls = list(map(lambda u: u.replace(\"EXAMPLE.ORG\", \"example.org\"), urls))\n-        self.assertEqual([r.url for r in output], urls)\n+        self.assertEqual([r.url for r in output],\n+                         [safe_url_string('http://EXAMPLE.ORG/SOMEPAGE/ITEM/12.HTML'),\n+                          safe_url_string('http://EXAMPLE.ORG/ABOUT.HTML'),\n+                          safe_url_string('http://EXAMPLE.ORG/NOFOLLOW.HTML')])\n \n     def test_process_request_instance_method_with_response(self):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#77c055ee28a767b2c8222274cc7556ab9cc56edc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 17 | Churn Cumulative: 643 | Contributors (this commit): 16 | Commits (past 90d): 5 | Contributors (cumulative): 23 | DMM Complexity: 0.7\n\nDIFF:\n@@ -78,4 +78,7 @@ class HttpProxyMiddleware:\n                     del request.headers[b'Proxy-Authorization']\n                 del request.meta['_auth_proxy']\n         elif b'Proxy-Authorization' in request.headers:\n+            if proxy_url:\n+                request.meta['_auth_proxy'] = proxy_url\n+            else:\n                 del request.headers[b'Proxy-Authorization']\n\n@@ -400,6 +400,9 @@ class TestHttpProxyMiddleware(TestCase):\n         self.assertNotIn(b'Proxy-Authorization', request.headers)\n \n     def test_proxy_authentication_header_proxy_without_credentials(self):\n+        \"\"\"As long as the proxy URL in request metadata remains the same, the\n+        Proxy-Authorization header is used and kept, and may even be\n+        changed.\"\"\"\n         middleware = HttpProxyMiddleware()\n         request = Request(\n             'https://example.com',\n@@ -408,7 +411,16 @@ class TestHttpProxyMiddleware(TestCase):\n         )\n         assert middleware.process_request(request, spider) is None\n         self.assertEqual(request.meta['proxy'], 'https://example.com')\n-        self.assertNotIn(b'Proxy-Authorization', request.headers)\n+        self.assertEqual(request.headers['Proxy-Authorization'], b'Basic foo')\n+\n+        assert middleware.process_request(request, spider) is None\n+        self.assertEqual(request.meta['proxy'], 'https://example.com')\n+        self.assertEqual(request.headers['Proxy-Authorization'], b'Basic foo')\n+\n+        request.headers['Proxy-Authorization'] = b'Basic bar'\n+        assert middleware.process_request(request, spider) is None\n+        self.assertEqual(request.meta['proxy'], 'https://example.com')\n+        self.assertEqual(request.headers['Proxy-Authorization'], b'Basic bar')\n \n     def test_proxy_authentication_header_proxy_with_same_credentials(self):\n         middleware = HttpProxyMiddleware()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#385acd5598fb06a126ac485087b58a4581096876", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 1692 | Contributors (this commit): 38 | Commits (past 90d): 5 | Contributors (cumulative): 48 | DMM Complexity: None\n\nDIFF:\n@@ -20,13 +20,13 @@ def has_environment_marker_platform_impl_support():\n \n install_requires = [\n     'Twisted>=18.9.0',\n-    'cryptography>=2.8',\n+    'cryptography>=3.3',\n     'cssselect>=0.9.1',\n     'itemloaders>=1.0.1',\n     'parsel>=1.5.0',\n-    'pyOpenSSL>=19.1.0',\n+    'pyOpenSSL>=21.0.0',\n     'queuelib>=1.4.2',\n-    'service_identity>=16.0.0',\n+    'service_identity>=18.1.0',\n     'w3lib>=1.17.0',\n     'zope.interface>=5.1.0',\n     'protego>=0.1.15',\n\n@@ -327,7 +327,7 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n \n     def test_reactor_default_twisted_reactor_select(self):\n         log = self.run_script('reactor_default_twisted_reactor_select.py')\n-        if platform.system() == 'Windows':\n+        if platform.system() in ['Windows', 'Darwin']:\n             # The goal of this test function is to test that, when a reactor is\n             # installed (the default one here) and a different reactor is\n             # configured (select here), an error raises.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#79a4bc3da02bfe2b07c596f6b677760b2c04f96f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 665 | Contributors (this commit): 13 | Commits (past 90d): 2 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -21,7 +21,7 @@ class ScrapyClientContextFactory(BrowserLikePolicyForHTTPS):\n     which allows TLS protocol negotiation\n \n     'A TLS/SSL connection established with [this method] may\n-     understand the SSLv3, TLSv1, TLSv1.1 and TLSv1.2 protocols.'\n+     understand the TLSv1, TLSv1.1 and TLSv1.2 protocols.'\n     \"\"\"\n \n     def __init__(self, method=SSL.SSLv23_METHOD, tls_verbose_logging=False, tls_ciphers=None, *args, **kwargs):\n\n@@ -11,7 +11,6 @@ from scrapy.utils.ssl import x509name_to_string, get_temp_key_info\n logger = logging.getLogger(__name__)\n \n \n-METHOD_SSLv3 = 'SSLv3'\n METHOD_TLS = 'TLS'\n METHOD_TLSv10 = 'TLSv1.0'\n METHOD_TLSv11 = 'TLSv1.1'\n@@ -20,7 +19,6 @@ METHOD_TLSv12 = 'TLSv1.2'\n \n openssl_methods = {\n     METHOD_TLS: SSL.SSLv23_METHOD,                      # protocol negotiation (recommended)\n-    METHOD_SSLv3: SSL.SSLv3_METHOD,                     # SSL 3 (NOT recommended)\n     METHOD_TLSv10: SSL.TLSv1_METHOD,                    # TLS 1.0 only\n     METHOD_TLSv11: getattr(SSL, 'TLSv1_1_METHOD', 5),   # TLS 1.1 only\n     METHOD_TLSv12: getattr(SSL, 'TLSv1_2_METHOD', 6),   # TLS 1.2 only\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
