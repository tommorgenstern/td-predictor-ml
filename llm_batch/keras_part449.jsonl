{"custom_id": "keras#c2f9cf4b65a18420b09fe11ea4ee342284ee5bdf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 112 | Lines Deleted: 31 | Files Changed: 2 | Hunks: 14 | Methods Changed: 14 | Complexity Δ (Sum/Max): 11/7 | Churn Δ: 143 | Churn Cumulative: 7357 | Contributors (this commit): 9 | Commits (past 90d): 53 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -672,7 +672,6 @@ class RandomFlip(BaseImageAugmentationLayer):\n     else:\n       raise ValueError('RandomFlip layer {name} received an unknown mode '\n                        'argument {arg}'.format(name=self.name, arg=mode))\n-    self.seed = seed\n     self.auto_vectorize = False\n \n   def augment_label(self, label, transformation):\n@@ -680,32 +679,52 @@ class RandomFlip(BaseImageAugmentationLayer):\n \n   def augment_image(self, image, transformation):\n     flipped_outputs = image\n-    if self.horizontal:\n-      seed = self._random_generator.make_seed_for_stateless_op()\n-      if seed is not None:\n-        flipped_outputs = tf.image.stateless_random_flip_left_right(\n-            flipped_outputs, seed=seed)\n-      else:\n-        flipped_outputs = tf.image.random_flip_left_right(\n-            flipped_outputs, self._random_generator.make_legacy_seed())\n-    if self.vertical:\n-      seed = self._random_generator.make_seed_for_stateless_op()\n-      if seed is not None:\n-        flipped_outputs = tf.image.stateless_random_flip_up_down(\n-            flipped_outputs, seed=seed)\n-      else:\n-        flipped_outputs = tf.image.random_flip_up_down(\n-            flipped_outputs, self._random_generator.make_legacy_seed())\n+    if self.horizontal and transformation['flip_horizontal']:\n+      flipped_outputs = tf.image.flip_left_right(flipped_outputs)\n+    if self.vertical and transformation['flip_vertical']:\n+      flipped_outputs = tf.image.flip_up_down(flipped_outputs)\n     flipped_outputs.set_shape(image.shape)\n     return flipped_outputs\n \n+  def get_random_transformation(self,\n+                                image=None,\n+                                label=None,\n+                                bounding_box=None):\n+    flip_horizontal = False\n+    flip_vertical = False\n+    if self.horizontal:\n+      flip_horizontal = np.random.choice([True, False])\n+    if self.vertical:\n+      flip_vertical = np.random.choice([True, False])\n+    return {'flip_horizontal': flip_horizontal, 'flip_vertical': flip_vertical}\n+\n+  def augment_bounding_boxes(self, image, bounding_boxes, transformation=None):\n+    transformation = transformation or self.get_random_transformation()\n+    image = tf.expand_dims(image, 0)\n+    image_shape = tf.shape(image)\n+    h = image_shape[H_AXIS]\n+    w = image_shape[W_AXIS]\n+    bboxes_out = tf.identity(bounding_boxes)\n+    if transformation['flip_horizontal']:\n+      bboxes_out = tf.stack([\n+          w - bboxes_out[:, 2], bboxes_out[:, 1], w - bboxes_out[:, 0],\n+          bboxes_out[:, 3]\n+      ],\n+                            axis=-1)\n+    if transformation['flip_vertical']:\n+      bboxes_out = tf.stack([\n+          bboxes_out[:, 0], h - bboxes_out[:, 3], bboxes_out[:, 2],\n+          h - bboxes_out[:, 1]\n+      ],\n+                            axis=-1)\n+    return bboxes_out\n+\n   def compute_output_shape(self, input_shape):\n     return input_shape\n \n   def get_config(self):\n     config = {\n         'mode': self.mode,\n-        'seed': self.seed,\n     }\n     base_config = super(RandomFlip, self).get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n\n@@ -537,7 +537,7 @@ class RandomFlipTest(test_combinations.TestCase):\n     orig_width = 8\n     channels = 3\n     if mock_random is None:\n-      mock_random = [0.0 for _ in range(num_samples)]\n+      mock_random = [True for _ in range(num_samples)]\n       if mode == 'horizontal_and_vertical':\n         mock_random *= 2\n     inp = np.random.random((num_samples, orig_height, orig_width, channels))\n@@ -548,8 +548,8 @@ class RandomFlipTest(test_combinations.TestCase):\n       if mode == 'vertical' or mode == 'horizontal_and_vertical':\n         expected_output = np.flip(expected_output, axis=1)\n     with tf.compat.v1.test.mock.patch.object(\n-        stateless_random_ops,\n-        'stateless_random_uniform',\n+        np.random,\n+        'choice',\n         side_effect=mock_random,\n     ):\n       with test_utils.use_gpu():\n@@ -566,7 +566,7 @@ class RandomFlipTest(test_combinations.TestCase):\n \n   def test_random_flip_horizontal_half(self):\n     np.random.seed(1337)\n-    mock_random = [0.0, 1.0]\n+    mock_random = [True, False]\n     input_images = np.random.random((2, 5, 8, 3)).astype(np.float32)\n     expected_output = input_images.copy()\n     expected_output[0, :, :, :] = np.flip(input_images[0, :, :, :], axis=1)\n@@ -574,7 +574,7 @@ class RandomFlipTest(test_combinations.TestCase):\n \n   def test_random_flip_vertical_half(self):\n     np.random.seed(1337)\n-    mock_random = [0.0, 1.0]\n+    mock_random = [True, False]\n     input_images = np.random.random((2, 5, 8, 3)).astype(np.float32)\n     expected_output = input_images.copy()\n     expected_output[0, :, :, :] = np.flip(input_images[0, :, :, :], axis=0)\n@@ -591,10 +591,10 @@ class RandomFlipTest(test_combinations.TestCase):\n   def test_random_flip_default(self):\n     input_images = np.random.random((2, 5, 8, 3)).astype(np.float32)\n     expected_output = np.flip(np.flip(input_images, axis=1), axis=2)\n-    mock_random = [0.0, 0.0, 0.0, 0.0]\n+    mock_random = [True, True, True, True]\n     with tf.compat.v1.test.mock.patch.object(\n-        stateless_random_ops,\n-        'stateless_random_uniform',\n+        np.random,\n+        'choice',\n         side_effect=mock_random,\n     ):\n       with self.cached_session():\n@@ -612,11 +612,11 @@ class RandomFlipTest(test_combinations.TestCase):\n   def test_random_flip_unbatched_image(self):\n     input_image = np.random.random((4, 4, 1)).astype(np.float32)\n     expected_output = np.flip(input_image, axis=0)\n-    # mock_random = np.reshape([0.], [1, 1, 1])\n+    mock_random = [True, True, True, True]\n     with tf.compat.v1.test.mock.patch.object(\n-        stateless_random_ops,\n-        'stateless_random_uniform',\n-        return_value=0.,\n+        np.random,\n+        'choice',\n+        side_effect=mock_random,\n     ):\n       with self.cached_session():\n         layer = image_preprocessing.RandomFlip('vertical')\n@@ -631,6 +631,69 @@ class RandomFlipTest(test_combinations.TestCase):\n     layer = image_preprocessing.RandomFlip(dtype='uint8')\n     self.assertAllEqual(layer(inputs).dtype, 'uint8')\n \n+  @test_utils.run_v2_only\n+  def test_augment_bbox_horizontal(self):\n+    image = tf.zeros([1, 20, 20, 3])\n+    bboxes = np.array([[0, 0, 10, 10], [4, 4, 12, 12]], dtype='int32')\n+    layer = image_preprocessing.RandomFlip()\n+    output = layer.augment_bounding_boxes(\n+        image,\n+        bboxes,\n+        transformation={\n+            'flip_horizontal': True,\n+            'flip_vertical': False\n+        })\n+    expected_output = [[10, 0, 20, 10], [8, 4, 16, 12]]\n+    self.assertAllClose(expected_output, output)\n+\n+  @test_utils.run_v2_only\n+  def test_augment_bbox_vertical(self):\n+    image = tf.zeros([1, 20, 20, 3])\n+    bboxes = np.array([[0, 0, 10, 10], [4, 4, 12, 12]], dtype='int32')\n+    layer = image_preprocessing.RandomFlip()\n+    output = layer.augment_bounding_boxes(\n+        image,\n+        bboxes,\n+        transformation={\n+            'flip_horizontal': False,\n+            'flip_vertical': True\n+        })\n+    expected_output = [[0, 10, 10, 20], [4, 8, 12, 16]]\n+    self.assertAllClose(expected_output, output)\n+\n+  @test_utils.run_v2_only\n+  def test_augment_bbox_both(self):\n+    image = tf.zeros([1, 20, 20, 3])\n+    bboxes = np.array([[0, 0, 10, 10], [4, 4, 12, 12]], dtype='int32')\n+    layer = image_preprocessing.RandomFlip()\n+    output = layer.augment_bounding_boxes(\n+        image,\n+        bboxes,\n+        transformation={\n+            'flip_horizontal': True,\n+            'flip_vertical': True\n+        })\n+    expected_output = [[10, 10, 20, 20], [8, 8, 16, 16]]\n+    self.assertAllClose(expected_output, output)\n+\n+  @test_utils.run_v2_only\n+  def test_augment_bbox_batched_input(self):\n+    image = tf.zeros([20, 20, 3])\n+    bboxes = np.array(\n+        [[[0, 0, 10, 10], [4, 4, 12, 12]], [[0, 0, 10, 10], [4, 4, 12, 12]]],\n+        dtype='int32')\n+    input = {'images': [image, image], 'bounding_boxes': bboxes}\n+    mock_random = [True, True, True, True]\n+    with tf.compat.v1.test.mock.patch.object(\n+        np.random,\n+        'choice',\n+        side_effect=mock_random,\n+    ):\n+      layer = image_preprocessing.RandomFlip()\n+      output = layer(input, training=True)\n+    expected_output = [[[10, 10, 20, 20], [8, 8, 16, 16]],\n+                       [[10, 10, 20, 20], [8, 8, 16, 16]]]\n+    self.assertAllClose(expected_output, output['bounding_boxes'])\n \n @test_combinations.run_all_keras_modes(always_skip_v1=True)\n class RandomContrastTest(test_combinations.TestCase):\n@@ -2030,7 +2093,6 @@ class LearningPhaseTest(test_combinations.TestCase):\n class DeterminismTest(test_combinations.TestCase):\n \n   @parameterized.named_parameters(\n-      ('random_flip', image_preprocessing.RandomFlip),\n       ('random_contrast',\n        functools.partial(image_preprocessing.RandomContrast, factor=1.)),\n       ('random_crop',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8a150fe81f1c65ee00af4fe71e4f40cfef9fae87", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 2425 | Contributors (this commit): 1 | Commits (past 90d): 9 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -223,15 +223,15 @@ class ConvNeXtBlock(Model):\n     else:\n       self.gamma = None\n     \n-    self.dw_conv_1 = layers.Conv2D(\n+    self.depthwise_conv_1 = layers.Conv2D(\n       filters=projection_dim, kernel_size=7, padding=\"same\",\n       groups=projection_dim, name=name + \"_depthwise_conv\")\n     self.layer_norm = layers.LayerNormalization(epsilon=1e-6, \n       name=name + \"_layernorm\")\n-    self.pw_conv_1 = layers.Dense(4 * projection_dim,\n+    self.pointwise_conv_1 = layers.Dense(4 * projection_dim,\n       name=name + \"_pointwise_conv_1\")\n     self.act_fn = layers.Activation(\"gelu\", name=name + \"_gelu\")\n-    self.pw_conv_2 = layers.Dense(projection_dim, \n+    self.pointwise_conv_2 = layers.Dense(projection_dim, \n       name=name + \"_pointwise_conv_2\")\n     self.drop_path = (\n       StochasticDepth(drop_path_rate, name=name + \"_stochastic_depth\")\n@@ -242,11 +242,11 @@ class ConvNeXtBlock(Model):\n   def call(self, inputs):\n     x = inputs\n \n-    x = self.dw_conv_1(x)\n+    x = self.depthwise_conv_1(x)\n     x = self.layer_norm(x)\n-    x = self.pw_conv_1(x)\n+    x = self.pointwise_conv_1(x)\n     x = self.act_fn(x)\n-    x = self.pw_conv_2(x)\n+    x = self.pointwise_conv_2(x)\n \n     if self.gamma is not None:\n       x = self.gamma * x\n@@ -433,7 +433,7 @@ def ConvNeXt(depths,\n   # Stochastic depth schedule.\n   # This is referred from the original ConvNeXt codebase:\n   # https://github.com/facebookresearch/ConvNeXt/blob/main/models/convnext.py#L86\n-  dp_rates = [x for x in tf.linspace(0.0, drop_path_rate, sum(depths))]\n+  depth_drop_rates = [x for x in tf.linspace(0.0, drop_path_rate, sum(depths))]\n \n   # First apply downsampling blocks and then apply ConvNeXt stages.\n   cur = 0\n@@ -442,7 +442,7 @@ def ConvNeXt(depths,\n     for j in range(depths[i]):\n       x = ConvNeXtBlock(\n         projection_dim=projection_dims[i],\n-        drop_path_rate=dp_rates[cur + j],\n+        drop_path_rate=depth_drop_rates[cur + j],\n         layer_scale_init_value=layer_scale_init_value,\n         name=model_name + f\"_stage_{i}_block_{j}\",\n       )(x)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e5171b4226815bc3f863de3f06cbdbe5f5f85931", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 22 | Files Changed: 2 | Hunks: 18 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 51 | Churn Cumulative: 2651 | Contributors (this commit): 9 | Commits (past 90d): 12 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -15,6 +15,12 @@\n \"\"\"Keras Applications are premade architectures with pre-trained weights.\"\"\"\n # pylint: disable=g-bad-import-order\n \n+from keras.applications.convnext import ConvNeXtTiny\n+from keras.applications.convnext import ConvNeXtSmall\n+from keras.applications.convnext import ConvNeXtBase\n+from keras.applications.convnext import ConvNeXtLarge\n+from keras.applications.convnext import ConvNeXtXLarge\n+\n from keras.applications.densenet import DenseNet121\n from keras.applications.densenet import DenseNet169\n from keras.applications.densenet import DenseNet201\n\n@@ -39,23 +39,22 @@ BASE_WEIGHTS_PATH = \"https://storage.googleapis.com/convnext-tf/keras-applicatio\n \n WEIGHTS_HASHES = {\n   \"tiny\":\n-    (\"dec324e40ebe943afc7b75b72484646eeb092c04bb079df35911d7080364f9a8\",\n-      \"4d4f0e079db2cc0e627b55f7d0d76c367145d14f2c90674415373457cd822346\"),\n+    (\"594e0f8c77df6cdf30d07e92f19d530921a53ff8301edd130c1bb9ad3dd6f25b\",\n+      \"278a5149e8c6e26f001051db5a8398f40f233994af2921a072bdca54389a9048\"),\n   \"small\":\n-    (\"f964ea5cd5618a1e64902a74ca5ccff3797a4fa5dba11a14f2c4d1a562b72f08\",\n-      \"fd8f0ac74faa4e364d7cb5b2d32af9ae35b54ce5e80525b5beb7b7571320065a\"),\n+    (\"8f26cee79fea02bbbbdc721e7dd5d416562106b7768ec37459850c7b23f26cb2\",\n+      \"b0700a330b2e8bfa6862b61d333d3d8860e566ce2f9395a2165e506935bba547\"),\n   \"base\":\n-    (\"d30e0c509f4e1abe2784d33765d4391ce8fbff259b0bd79f4a63684b20db87d2\",\n-      \"736f7a96cd933ee568611e29f334737fb9aebaaea021ea7adfe4d2f5cbb4a9aa\"),\n+    (\"2c08893b86245f4fc1d80f584faeb7431b23e895b03f71fc3d943a3489b60089\",\n+      \"a3d12bf8938796ca00721db89cb9e5a1af77f847401348adef16fab3611233d3\"),\n   \"large\":\n-    (\"8a304c66deb782b0d59837bc13127068901adaaa280cfac604d3341aaf44b2cf\",\n-      \"b02b623b3c28586423e6be4aa214e2f5619280b97b4ef6b35ffb686e83235f01\"),\n+    (\"f17f979cc7cd23906a88f490ff1249d2b0dedc44a601f5700a0873afc1b0ad02\",\n+      \"74ff1e5f6eee45c62194aaee4a3347d5e2cf574caccad414b2b117b3ad110b32\"),\n   \"xlarge\":\n-    (\"da65d1294d386c71aebd81bc2520b8d42f7f60eee4414806c60730cd63eb15cb\",\n-      \"2bfbf5f0c2b3f004f1c32e9a76661e11a9ac49014ed2a68a49ecd0cd6c88d377\"),\n+    (\"784b923c2db18fec883093b265c2a44c4fc401425f206a3ca036015290f401e8\",\n+      \"ba28c90cf6a4c64acfe0ca3af3ad920652c50f24668a3aaaf245a1c47e7bfe6f\"),\n }\n \n-\n MODEL_CONFIGS = {\n   \"tiny\": {\n     \"depths\": [3, 3, 9, 3],\n@@ -335,7 +334,9 @@ def ConvNeXt(depths,\n     default_size: Default input image size.\n     model_name: An optional name for the model.\n     include_preprocessing: boolean denoting whther to include preprocessing in\n-      the model.\n+      the model. When `weights=\"imagenet\"` this should be always set to True.\n+      But for other models (e.g., randomly initialized) users should set it\n+      to False and apply preprocessing to data accordingly.\n     include_top: Boolean denoting whether to include classification head to the\n       model.\n     weights: one of `None` (random initialization), `\"imagenet\"` (pre-training \n@@ -497,10 +498,10 @@ def ConvNeXtTiny(model_name=\"convnext_tiny\",\n   classifier_activation=\"softmax\"):\n   return ConvNeXt(\n     depths=MODEL_CONFIGS[\"tiny\"][\"depths\"],\n-    projection_dims=[\"tiny\"][\"projection_dims\"],\n+    projection_dims=MODEL_CONFIGS[\"tiny\"][\"projection_dims\"],\n     drop_path_rate=0.0,\n     layer_scale_init_value=1e-6,\n-    default_size=[\"tiny\"][\"default_size\"],\n+    default_size=MODEL_CONFIGS[\"tiny\"][\"default_size\"],\n     model_name=model_name,\n     include_top=include_top,\n     include_preprocessing=include_preprocessing,\n@@ -525,10 +526,10 @@ def ConvNeXtSmall(model_name=\"convnext_small\",\n   classifier_activation=\"softmax\"):\n   return ConvNeXt(\n     depths=MODEL_CONFIGS[\"small\"][\"depths\"],\n-    projection_dims=[\"small\"][\"projection_dims\"],\n+    projection_dims=MODEL_CONFIGS[\"small\"][\"projection_dims\"],\n     drop_path_rate=0.0,\n     layer_scale_init_value=1e-6,\n-    default_size=[\"small\"][\"default_size\"],\n+    default_size=MODEL_CONFIGS[\"small\"][\"default_size\"],\n     model_name=model_name,\n     include_top=include_top,\n     include_preprocessing=include_preprocessing,\n@@ -553,10 +554,10 @@ def ConvNeXtBase(model_name=\"convnext_base\",\n   classifier_activation=\"softmax\"):\n   return ConvNeXt(\n     depths=MODEL_CONFIGS[\"base\"][\"depths\"],\n-    projection_dims=[\"base\"][\"projection_dims\"],\n+    projection_dims=MODEL_CONFIGS[\"base\"][\"projection_dims\"],\n     drop_path_rate=0.0,\n     layer_scale_init_value=1e-6,\n-    default_size=[\"base\"][\"default_size\"],\n+    default_size=MODEL_CONFIGS[\"base\"][\"default_size\"],\n     model_name=model_name,\n     include_top=include_top,\n     include_preprocessing=include_preprocessing,\n@@ -581,10 +582,10 @@ def ConvNeXtLarge(model_name=\"convnext_large\",\n   classifier_activation=\"softmax\"):\n   return ConvNeXt(\n     depths=MODEL_CONFIGS[\"large\"][\"depths\"],\n-    projection_dims=[\"large\"][\"projection_dims\"],\n+    projection_dims=MODEL_CONFIGS[\"large\"][\"projection_dims\"],\n     drop_path_rate=0.0,\n     layer_scale_init_value=1e-6,\n-    default_size=[\"large\"][\"default_size\"],\n+    default_size=MODEL_CONFIGS[\"large\"][\"default_size\"],\n     model_name=model_name,\n     include_top=include_top,\n     include_preprocessing=include_preprocessing,\n@@ -609,10 +610,10 @@ def ConvNeXtXLarge(model_name=\"convnext_xlarge\",\n   classifier_activation=\"softmax\"):\n   return ConvNeXt(\n     depths=MODEL_CONFIGS[\"xlarge\"][\"depths\"],\n-    projection_dims=[\"xlarge\"][\"projection_dims\"],\n+    projection_dims=MODEL_CONFIGS[\"xlarge\"][\"projection_dims\"],\n     drop_path_rate=0.0,\n     layer_scale_init_value=1e-6,\n-    default_size=[\"xlarge\"][\"default_size\"],\n+    default_size=MODEL_CONFIGS[\"xlarge\"][\"default_size\"],\n     model_name=model_name,\n     include_top=include_top,\n     include_preprocessing=include_preprocessing,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6b9f091981eb44feaf299cd69af7e8f231bcb570", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 478 | Contributors (this commit): 9 | Commits (past 90d): 7 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -20,6 +20,7 @@ from absl import flags\n from absl.testing import parameterized\n import numpy as np\n \n+from keras.applications import convnext\n from keras.applications import densenet\n from keras.applications import efficientnet\n from keras.applications import efficientnet_v2\n@@ -55,6 +56,10 @@ ARG_TO_MODEL = {\n     'mobilenet_v2': (mobilenet_v2, [mobilenet_v2.MobileNetV2]),\n     'mobilenet_v3_small': (mobilenet_v3, [mobilenet_v3.MobileNetV3Small]),\n     'mobilenet_v3_large': (mobilenet_v3, [mobilenet_v3.MobileNetV3Large]),\n+    'convnext': \n+        (convnext, \n+        [convnext.ConvNeXtTiny, convnext.ConvNeXtSmall, convnext.ConvNeXtBase,\n+        convnext.ConvNeXtLarge, convnext.ConvNeXtXLarge]),\n     'densenet':\n         (densenet,\n          [densenet.DenseNet121, densenet.DenseNet169, densenet.DenseNet201]),\n\n@@ -16,6 +16,7 @@\n \n from absl.testing import parameterized\n from keras import backend\n+from keras.applications import convnext\n from keras.applications import densenet\n from keras.applications import efficientnet\n from keras.applications import efficientnet_v2\n@@ -45,6 +46,11 @@ MODEL_LIST_NO_NASNET = [(resnet.ResNet50, 2048), (resnet.ResNet101, 2048),\n                         (mobilenet_v2.MobileNetV2, 1280),\n                         (mobilenet_v3.MobileNetV3Small, 576),\n                         (mobilenet_v3.MobileNetV3Large, 960),\n+                        (convnext.ConvNeXtTiny, 768),\n+                        (convnext.ConvNeXtSmall, 768),\n+                        (convnext.ConvNeXtBase, 1024),\n+                        (convnext.ConvNeXtLarge, 1536),\n+                        (convnext.ConvNeXtXLarge, 2048),\n                         (densenet.DenseNet121, 1024),\n                         (densenet.DenseNet169, 1664),\n                         (densenet.DenseNet201, 1920),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a80bbb24533a4c26bf6d33bf316dcc1693be4432", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 2056 | Contributors (this commit): 15 | Commits (past 90d): 9 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -30,6 +30,7 @@ from keras.engine import training_utils\n from keras.utils import data_utils\n from keras.utils import dataset_creator\n from keras.utils import tf_utils\n+from tensorflow.python.distribute.input_lib import DistributedDataset\n from tensorflow.python.framework import type_spec\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n@@ -1197,6 +1198,7 @@ class DataHandler:\n           break\n         if self._adapter.should_recreate_iterator():\n           data_iterator = iter(self._dataset)\n+          if not isinstance(self._dataset, DistributedDataset):\n             self._inferred_steps = self._infer_steps(self._steps_per_epoch, self._dataset)\n         yield epoch, data_iterator\n         self._adapter.on_epoch_end()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#28466bbce89eb7efb0b0b83026a28e3d0aab4427", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 1286 | Contributors (this commit): 3 | Commits (past 90d): 21 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -258,8 +258,7 @@ def _get_next_sample(dataset_iterator,\n           warnings.warn('The dataset is taking longer than 10 seconds to '\n                         'iterate. This may be due to the size of the dataset. '\n                         'Please consider using a smaller dataset'\n-                        '(e.g. < 10,000 samples). \\nTo hide this '\n-                        'warning message, set `data_size_warning_flag=False`.',\n+                        '(e.g. < 10,000 samples).',\n                         category=ResourceWarning,\n                         source='split_dataset')\n           data_size_warning_flag = False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#745a905f56da9134e2b37c527237668d28da4c99", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 2060 | Contributors (this commit): 15 | Commits (past 90d): 10 | Contributors (cumulative): 15 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1199,7 +1199,9 @@ class DataHandler:\n         if self._adapter.should_recreate_iterator():\n           data_iterator = iter(self._dataset)\n           if not isinstance(self._dataset, DistributedDataset):\n-            self._inferred_steps = self._infer_steps(self._steps_per_epoch, self._dataset)\n+            steps = self._infer_steps(self._steps_per_epoch, self._dataset)\n+            if steps is not None:\n+              self._inferred_steps = steps\n         yield epoch, data_iterator\n         self._adapter.on_epoch_end()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#eaba0d1e22f071ab9c222fc8779b3e483bb12eb0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 2474 | Contributors (this commit): 1 | Commits (past 90d): 11 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -229,7 +229,7 @@ class ConvNeXtBlock(Model):\n       name=name + \"_layernorm\")\n     self.pointwise_conv_1 = layers.Dense(4 * projection_dim,\n       name=name + \"_pointwise_conv_1\")\n-    self.act_fn = layers.Activation(\"gelu\", name=name + \"_gelu\")\n+    self.activation = layers.Activation(\"gelu\", name=name + \"_gelu\")\n     self.pointwise_conv_2 = layers.Dense(projection_dim, \n       name=name + \"_pointwise_conv_2\")\n     self.drop_path = (\n@@ -244,7 +244,7 @@ class ConvNeXtBlock(Model):\n     x = self.depthwise_conv_1(x)\n     x = self.layer_norm(x)\n     x = self.pointwise_conv_1(x)\n-    x = self.act_fn(x)\n+    x = self.activation(x)\n     x = self.pointwise_conv_2(x)\n \n     if self.gamma is not None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6b3916813d761c75de61f89bde19538add1a8e2d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 23 | Churn Cumulative: 4113 | Contributors (this commit): 43 | Commits (past 90d): 1 | Contributors (cumulative): 43 | DMM Complexity: None\n\nDIFF:\n@@ -357,10 +357,27 @@ def register_keras_serializable(package='Custom', name=None):\n   The object will be registered under the key 'package>name' where `name`,\n   defaults to the object name if not passed.\n \n+  Example:\n+\n+  ```python\n+  # Note that `'my_package'` is used as the `package` argument here, and since\n+  # the `name` argument is not provided, `'MyDense'` is used as the `name`.\n+  @keras.utils.register_keras_serializable('my_package')\n+  class MyDense(keras.layers.Dense):\n+    pass\n+    \n+  assert keras.utils.get_registered_object('my_package>MyDense') == MyDense\n+  assert keras.utils.get_registered_name(MyDense) == 'my_package>MyDense'\n+  ```\n+\n   Args:\n-    package: The package that this class belongs to.\n-    name: The name to serialize this class under in this package. If None, the\n-      class' name will be used.\n+    package: The package that this class belongs to. This is used for the `key`\n+      (which is 'package>name') to idenfify the class. Note that this is the\n+      first argument passed into the decorator.\n+    name: The name to serialize this class under in this package. If not\n+      provided or `None`, the class' name will be used (note that this is the\n+      case when the decorator is used with only one argument, which becomes the\n+      `package`).\n \n   Returns:\n     A decorator that registers the decorated class with the passed names.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e5922a2cd37d14c1e427fb7ab5ab087439d5f3fa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 2548 | Contributors (this commit): 11 | Commits (past 90d): 7 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1246,6 +1246,10 @@ class LossScaleOptimizerV3(tf.__internal__.tracking.DelegatingTrackableMixin,\n   def iterations(self):\n     return self._optimizer.iterations\n \n+  @iterations.setter\n+  def iterations(self, variable):\n+    self._optimizer.iterations = variable\n+\n   @property\n   def learning_rate(self):\n     return self._optimizer.learning_rate\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e6b80d49573758b16f0c05187022006062896e03", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 36 | Churn Cumulative: 2096 | Contributors (this commit): 15 | Commits (past 90d): 11 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -1394,8 +1394,42 @@ class _ClusterCoordinatorDataHandler(DataHandler):\n   def sync(self):\n     self._model._cluster_coordinator.join()  # pylint: disable=protected-access\n \n-\n+@keras_export(\"keras.__internal__.utils.get_data_handler\", v1=[])\n def get_data_handler(*args, **kwargs):\n+  \"\"\"Creates a `DataHandler`, providing standardized access to a `Dataset`.\n+\n+  See `DataHandler` for the list and definition of the arguments. See the\n+  implementation of `Model.fit()`, `evaluate()`, or `predict()` methods\n+  for complete usage examples. As a rule of tumb, `get_data_handler()` accepts\n+  the same inputs as the `x` argument of `Model.fit()`.\n+\n+  Example:\n+\n+  ```python\n+    def step(iterator):\n+      data = next(iterator)\n+      # result <= Do something with data\n+      return result\n+    tf_step = tf.function(step, reduce_retracing=True)\n+\n+    # Assume x is a tf.data Dataset.\n+    data_handler = data_adapter.get_data_handler(x=x)\n+    for epo_idx, iterator in data_handler.enumerate_epochs():  # Epoch iteration\n+        with data_handler.catch_stop_iteration(): # Stop on dataset exhaustion.\n+          for step in data_handler.steps(): # Step iteration\n+              step_result = step(iterator)\n+  ```\n+\n+  Args:\n+    *args: Arguments passed to the `DataHandler` constructor.\n+    **kwargs: Arguments passed to the `DataHandler` constructor.\n+\n+  Returns:\n+    A `DataHandler` object. If the model's cluster coordinate is set (e.g. the\n+    model was defined under a parameter-server strategy), returns a\n+    `_ClusterCoordinatorDataHandler`.\n+\n+  \"\"\"\n   if getattr(kwargs[\"model\"], \"_cluster_coordinator\", None):\n     return _ClusterCoordinatorDataHandler(*args, **kwargs)\n   return DataHandler(*args, **kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d9ba7649a22ea661219a8918f475b705044559e0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 1538 | Contributors (this commit): 6 | Commits (past 90d): 17 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1992,7 +1992,8 @@ class AUC(base_metric.Metric):\n         'curve': self.curve.value,\n         'summation_method': self.summation_method.value,\n         'multi_label': self.multi_label,\n-        'label_weights': label_weights\n+        'label_weights': label_weights,\n+        'from_logits': self._from_logits\n     }\n     # optimization to avoid serializing a large number of generated thresholds\n     if self._init_from_thresholds:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a8d5bb98d75c28d4a9e658aea3041f966a331dfb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 76 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 15 | Methods Changed: 6 | Complexity Δ (Sum/Max): 10/9 | Churn Δ: 86 | Churn Cumulative: 324 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 3 | DMM Complexity: 0.11538461538461539\n\nDIFF:\n@@ -33,22 +33,28 @@ class SharpnessAwareMinimization(Model):\n   \"\"\"Sharpness aware minimization (SAM) training flow.\n \n   Sharpness-aware minimization (SAM) is a technique that improves the model\n-  generalization and provides robustness to label noise.\n+  generalization and provides robustness to label noise. Mini-batch splitting is\n+  proven to improve the SAM's performance, so users can control how mini batches\n+  are split via setting the `num_batch_splits` argument.\n \n   Args:\n     model: `tf.keras.Model` instance. The inner model that does the\n       forward-backward pass.\n     rho: float, defaults to 0.05. The gradients scaling factor.\n+    num_batch_splits: int, defaults to None. The number of mini batches to\n+      split into from each data batch. If None, batches are not split into\n+      sub-batches.\n     name: string, defaults to None. The name of the SAM model.\n \n   Reference:\n     [Pierre Foret et al., 2020](https://arxiv.org/abs/2010.01412)\n   \"\"\"\n \n-  def __init__(self, model, rho=0.05, name=None):\n+  def __init__(self, model, rho=0.05, num_batch_splits=None, name=None):\n     super().__init__(name=name)\n     self.model = model\n     self.rho = rho\n+    self.num_batch_splits = num_batch_splits\n \n   def train_step(self, data):\n     \"\"\"The logic of one SAM training step.\n@@ -61,32 +67,56 @@ class SharpnessAwareMinimization(Model):\n       A dict mapping metric names to running average values.\n     \"\"\"\n     x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)\n+\n+    if self.num_batch_splits is not None:\n+      x_split = tf.split(x, self.num_batch_splits)\n+      y_split = tf.split(y, self.num_batch_splits)\n+    else:\n+      x_split = [x]\n+      y_split = [y]\n+\n+    gradients_all_batches = []\n+    pred_all_batches = []\n+    for (x_batch, y_batch) in zip(x_split, y_split):\n+      epsilon_w_cache = []\n       with tf.GradientTape() as tape:\n-      pred = self.model(x)\n-      loss = self.compiled_loss(y, pred)\n+        pred = self.model(x_batch)\n+        loss = self.compiled_loss(y_batch, pred)\n+      pred_all_batches.append(pred)\n       trainable_variables = self.model.trainable_variables\n       gradients = tape.gradient(loss, trainable_variables)\n \n-    epsilon_w_cache = []\n       gradients_order2_norm = self._gradients_order2_norm(gradients)\n       scale = self.rho / (gradients_order2_norm + 1e-12)\n \n       for (gradient, variable) in zip(gradients, trainable_variables):\n         epsilon_w = gradient * scale\n-      variable.assign_add(epsilon_w)\n+        self._distributed_apply_epsilon_w(variable, epsilon_w,\n+                                          tf.distribute.get_strategy())\n         epsilon_w_cache.append(epsilon_w)\n \n       with tf.GradientTape() as tape:\n-      pred = self(x)\n-      loss = self.compiled_loss(y, pred)\n-\n+        pred = self(x_batch)\n+        loss = self.compiled_loss(y_batch, pred)\n       gradients = tape.gradient(loss, trainable_variables)\n+      if len(gradients_all_batches) == 0:\n+        for gradient in gradients:\n+          gradients_all_batches.append([gradient])\n+      else:\n+        for (gradient, gradient_all_batches) in zip(gradients,\n+                                                    gradients_all_batches):\n+          gradient_all_batches.append(gradient)\n       for (variable, epsilon_w) in zip(trainable_variables, epsilon_w_cache):\n         # Restore the variable to its original value before `apply_gradients()`.\n-      variable.assign_sub(epsilon_w)\n+        self._distributed_apply_epsilon_w(variable, -epsilon_w,\n+                                          tf.distribute.get_strategy())\n \n+    gradients = []\n+    for gradient_all_batches in gradients_all_batches:\n+      gradients.append(tf.reduce_sum(gradient_all_batches, axis=0))\n     self.optimizer.apply_gradients(zip(gradients, trainable_variables))\n \n+    pred = tf.concat(pred_all_batches, axis=0)\n     self.compiled_metrics.update_state(y, pred, sample_weight)\n     return {m.name: m.result() for m in self.metrics}\n \n@@ -119,6 +149,21 @@ class SharpnessAwareMinimization(Model):\n         config.pop(\"model\"), custom_objects=custom_objects)\n     return cls(model, **config)\n \n+  def _distributed_apply_epsilon_w(self, var, epsilon_w, strategy):\n+    # Helper function to apply epsilon_w on model variables.\n+    if isinstance(tf.distribute.get_strategy(),\n+                  (tf.distribute.experimental.ParameterServerStrategy,\n+                   tf.distribute.experimental.CentralStorageStrategy)):\n+      # Under PSS and CSS, the AggregatingVariable has to be kept in sync.\n+      def distribute_apply(strategy, var, epsilon_w):\n+        strategy.extended.update(\n+          var, lambda x, y: x.assign_add(y), args=(epsilon_w,), group=False)\n+\n+      tf.__internal__.distribute.interim.maybe_merge_call(\n+        distribute_apply, tf.distribute.get_strategy(), var, epsilon_w)\n+    else:\n+      var.assign_add(epsilon_w)\n+\n   def _gradients_order2_norm(self, gradients):\n     norm = tf.norm(\n         tf.stack([tf.norm(grad) for grad in gradients if grad is not None]))\n\n@@ -54,6 +54,27 @@ class SharpnessAwareMinimizationTest(tf.test.TestCase, parameterized.TestCase):\n \n       sam_model.fit(data, label, steps_per_epoch=1)\n \n+  @ds_combinations.generate(\n+      tf.__internal__.test.combinations.combine(strategy=STRATEGIES))\n+  def test_sam_model_fit_with_sub_batch(self, strategy):\n+    with strategy.scope():\n+      model = keras.Sequential([\n+          keras.Input([2, 2]),\n+          keras.layers.Dense(4),\n+          keras.layers.Dense(1),\n+      ])\n+      sam_model = sharpness_aware_minimization.SharpnessAwareMinimization(\n+          model, num_batch_splits=4)\n+      data = tf.random.uniform([48, 2])\n+      label = data[:, 0] > 0.5\n+\n+      sam_model.compile(\n+          optimizer=adam.Adam(),\n+          loss=keras.losses.BinaryCrossentropy(from_logits=True),\n+      )\n+\n+      sam_model.fit(data, label, steps_per_epoch=1)\n+\n   def test_save_sam(self):\n     model = keras.Sequential([\n         keras.Input([2, 2]),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e8ee816b2fcf6cdcb48db81c946a8ffe534f125a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 4 | Churn Cumulative: 2478 | Contributors (this commit): 1 | Commits (past 90d): 12 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -402,7 +402,11 @@ def ConvNeXt(depths,\n     inputs = img_input\n \n   x = inputs\n+  \n   if include_preprocessing:\n+    channel_axis = 3 if backend.image_data_format() == \"channels_last\" else 1\n+    num_channels = input_shape[channel_axis - 1]\n+    if num_channels == 3:\n       x = PreStem(name=model_name)(x)\n   \n   # Stem block.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d3ee960d7e69592f351a93fed8e6415338b3a81a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 293 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -33,6 +33,7 @@ from keras.applications import resnet_v2\n from keras.applications import vgg16\n from keras.applications import vgg19\n from keras.applications import xception\n+from keras import utils\n import tensorflow.compat.v2 as tf\n \n MODEL_LIST_NO_NASNET = [(resnet.ResNet50, 2048), (resnet.ResNet101, 2048),\n@@ -130,6 +131,11 @@ class ApplicationsTest(tf.test.TestCase, parameterized.TestCase):\n     model = app(weights=None)\n     # Can be serialized and deserialized\n     config = model.get_config()\n+    if \"ConvNeXt\" in app.__name__:\n+      custom_objects = {\"ConvNeXtBlock\": convnext.ConvNeXtBlock}\n+      with utils.custom_object_scope(custom_objects):\n+        reconstructed_model = model.__class__.from_config(config)\n+    else:\n       reconstructed_model = model.__class__.from_config(config)\n     self.assertEqual(len(model.weights), len(reconstructed_model.weights))\n     backend.clear_session()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e691392184c3e9a88765c24d4b3332d118389419", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 66 | Lines Deleted: 87 | Files Changed: 2 | Hunks: 41 | Methods Changed: 12 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 153 | Churn Cumulative: 2924 | Contributors (this commit): 7 | Commits (past 90d): 16 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -132,7 +132,7 @@ class ApplicationsTest(tf.test.TestCase, parameterized.TestCase):\n     # Can be serialized and deserialized\n     config = model.get_config()\n     if \"ConvNeXt\" in app.__name__:\n-      custom_objects = {\"ConvNeXtBlock\": convnext.ConvNeXtBlock}\n+      custom_objects = {\"LayerScale\": convnext.LayerScale}\n       with utils.custom_object_scope(custom_objects):\n         reconstructed_model = model.__class__.from_config(config)\n     else:\n\n@@ -27,7 +27,6 @@ References:\n from keras import backend\n from keras import layers\n from keras import utils\n-from keras import Model\n from keras import Sequential\n from keras.applications import imagenet_utils\n from keras.engine import training\n@@ -35,26 +34,27 @@ from keras.engine import training\n import tensorflow.compat.v2 as tf\n from tensorflow.python.util.tf_export import keras_export\n \n-BASE_WEIGHTS_PATH = \"https://storage.googleapis.com/convnext-tf/keras-applications/convnext/\"\n+BASE_WEIGHTS_PATH = \"https://storage.googleapis.com/convnext-tf/keras-applications-temp/convnext/\"\n \n WEIGHTS_HASHES = {\n   \"tiny\":\n-    (\"594e0f8c77df6cdf30d07e92f19d530921a53ff8301edd130c1bb9ad3dd6f25b\",\n-      \"278a5149e8c6e26f001051db5a8398f40f233994af2921a072bdca54389a9048\"),\n+    (\"8ae6e78ce2933352b1ef4008e6dd2f17bc40771563877d156bc6426c7cf503ff\",\n+      \"d547c096cabd03329d7be5562c5e14798aa39ed24b474157cef5e85ab9e49ef1\"),\n   \"small\":\n-    (\"8f26cee79fea02bbbbdc721e7dd5d416562106b7768ec37459850c7b23f26cb2\",\n-      \"b0700a330b2e8bfa6862b61d333d3d8860e566ce2f9395a2165e506935bba547\"),\n+    (\"ce1277d8f1ee5a0ef0e171469089c18f5233860ceaf9b168049cb9263fd7483c\",\n+      \"6fc8009faa2f00c1c1dfce59feea9b0745eb260a7dd11bee65c8e20843da6eab\"),\n   \"base\":\n-    (\"2c08893b86245f4fc1d80f584faeb7431b23e895b03f71fc3d943a3489b60089\",\n-      \"a3d12bf8938796ca00721db89cb9e5a1af77f847401348adef16fab3611233d3\"),\n+    (\"52cbb006d3dadd03f6e095a8ca1aca47aecdd75acb4bc74bce1f5c695d0086e6\",\n+      \"40a20c5548a5e9202f69735ecc06c990e6b7c9d2de39f0361e27baeb24cb7c45\"),\n   \"large\":\n-    (\"f17f979cc7cd23906a88f490ff1249d2b0dedc44a601f5700a0873afc1b0ad02\",\n-      \"74ff1e5f6eee45c62194aaee4a3347d5e2cf574caccad414b2b117b3ad110b32\"),\n+    (\"070c5ed9ed289581e477741d3b34beffa920db8cf590899d6d2c67fba2a198a6\",\n+      \"40a20c5548a5e9202f69735ecc06c990e6b7c9d2de39f0361e27baeb24cb7c45\"),\n   \"xlarge\":\n-    (\"784b923c2db18fec883093b265c2a44c4fc401425f206a3ca036015290f401e8\",\n-      \"ba28c90cf6a4c64acfe0ca3af3ad920652c50f24668a3aaaf245a1c47e7bfe6f\"),\n+    (\"c1f5ccab661354fc3a79a10fa99af82f0fbf10ec65cb894a3ae0815f17a889ee\",\n+      \"de3f8a54174130e0cecdc71583354753d557fcf1f4487331558e2a16ba0cfe05\"),\n }\n \n+\n MODEL_CONFIGS = {\n   \"tiny\": {\n     \"depths\": [3, 3, 9, 3],\n@@ -84,35 +84,28 @@ MODEL_CONFIGS = {\n }\n \n BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n-\n   Reference:\n     - [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545)\n     (CVPR 2022)\n-\n   For image classification use cases, see\n   [this page for detailed examples](\n   https://keras.io/api/applications/#usage-examples-for-image-classification-models).\n-\n   For transfer learning use cases, make sure to read the\n   [guide to transfer learning & fine-tuning](\n     https://keras.io/guides/transfer_learning/).\n-\n   The `base`, `large`, and `xlarge` models were first pre-trained on the\n   ImageNet-21k dataset and then fine-tuned on the ImageNet-1k dataset. The\n   pre-trained parameters of the models were assembled from the\n   [official repository](https://github.com/facebookresearch/ConvNeXt). To get a\n   sense of how these parameters were converted to Keras compatible parameters,\n   please refer to [this repository](https://github.com/sayakpaul/keras-convnext-conversion).\n-\n   Note: Each Keras Application expects a specific kind of input preprocessing.\n   For ConvNeXt, preprocessing is included in the model using a `Normalization` layer.\n   ConvNeXt models expect their inputs to be float or uint8 tensors of pixels with\n   values in the [0-255] range.\n-\n   When calling the `summary()` method after instantiating a ConvNeXt model, prefer\n   setting the `expand_nested` argument `summary()` to `True` to better investigate\n   the instantiated model.\n-\n   Args:\n     include_top: Whether to include the fully-connected\n       layer at the top of the network. Defaults to True.\n@@ -145,7 +138,6 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n       Defaults to `\"softmax\"`.\n       When loading pretrained weights, `classifier_activation` can only\n       be `None` or `\"softmax\"`.\n-\n   Returns:\n     A `keras.Model` instance.\n \"\"\"\n@@ -154,17 +146,14 @@ class StochasticDepth(layers.Layer):\n   \"\"\"Stochastic Depth module. It performs batch-wise dropping rather than\n     sample-wise. In libraries like `timm`, it's similar to `DropPath` layers\n     that drops residual paths sample-wise.\n-\n   Reference:\n     - https://github.com.rwightman/pytorch-image-models\n-\n   Args:\n     drop_path_rate (float): Probability of dropping paths. Should be within\n       [0, 1].\n   \n   Returns:\n     Tensor either with the residual path dropped or kept.\n-\n   \"\"\"\n   def __init__(self, drop_path_rate, **kwargs):\n     super().__init__(**kwargs)\n@@ -185,88 +174,87 @@ class StochasticDepth(layers.Layer):\n     return config\n \n \n-class ConvNeXtBlock(Model):\n+class LayerScale(layers.Layer):\n+  \"\"\"Layer scale module.\n+  Reference:\n+    - hhttps://arxiv.org/abs/2103.17239\n+  Args:\n+    init_values (float): Initial value for layer scale. Should be within\n+      [0, 1].\n+    projection_dim (int): Projection dimensionality.\n+  \n+  Returns:\n+    Tensor multiplied to the scale.\n+  \"\"\"\n+  def __init__(self, init_values, projection_dim, **kwargs):\n+    super().__init__(**kwargs)\n+    self.init_values = init_values\n+    self.projection_dim = projection_dim\n+    self.gamma = tf.Variable(self.init_values * tf.ones((self.projection_dim,)))\n+\n+  def call(self, x):\n+    return x * self.gamma\n+\n+  def get_config(self):\n+    config = super().get_config()\n+    config.update(\n+      {\"init_values\": self.init_values, \"projection_dim\": self.projection_dim}\n+    )\n+    return config \n+\n+def ConvNeXtBlock(projection_dim, drop_path_rate=0.0, \n+    layer_scale_init_value=1e-6, name=None):\n   \"\"\"ConvNeXt block.\n   \n   References:\n     - https://arxiv.org/abs/2201.03545\n     - https://github.com/facebookresearch/ConvNeXt/blob/main/models/convnext.py\n-\n   Notes:\n     In the original ConvNeXt implementation (linked above), the authors use\n     `Dense` layers for pointwise convolutions for increased efficiency. \n     Following that, this implementation also uses the same.\n-\n   Args:\n     projection_dim (int): Number of filters for convolution layers. In the\n     ConvNeXt paper, this is referred to as projection dimension.\n     drop_path (float): Probability of dropping paths. Should be within [0, 1].\n     layer_scale_init_value (float): Layer scale value. Should be a small float\n       number.\n-\n   Returns:\n-    A keras.Model instance.\n+    Output tensor of the block.\n   \"\"\"\n-  def __init__(self, projection_dim, drop_path_rate=0.0, \n-    layer_scale_init_value=1e-6, **kwargs):\n-    super().__init__(**kwargs)\n-    self.projection_dim = projection_dim\n-    self.drop_path_rate = drop_path_rate\n-    self.layer_scale_init_value = layer_scale_init_value\n-    name = kwargs[\"name\"]\n+  if name is None:\n+    name = \"prestem\" + str(backend.get_uid(\"prestem\"))\n   \n-    if layer_scale_init_value > 0.0:\n-      self.gamma = tf.Variable(\n-        layer_scale_init_value * tf.ones((projection_dim,)),\n-        name=name + \"_layer_scale_gamma\")\n-    else:\n-      self.gamma = None\n-    \n-    self.depthwise_conv_1 = layers.Conv2D(\n-      filters=projection_dim, kernel_size=7, padding=\"same\",\n-      groups=projection_dim, name=name + \"_depthwise_conv\")\n-    self.layer_norm = layers.LayerNormalization(epsilon=1e-6, \n-      name=name + \"_layernorm\")\n-    self.pointwise_conv_1 = layers.Dense(4 * projection_dim,\n-      name=name + \"_pointwise_conv_1\")\n-    self.activation = layers.Activation(\"gelu\", name=name + \"_gelu\")\n-    self.pointwise_conv_2 = layers.Dense(projection_dim, \n-      name=name + \"_pointwise_conv_2\")\n-    self.drop_path = (\n-      StochasticDepth(drop_path_rate, name=name + \"_stochastic_depth\")\n-      if drop_path_rate > 0.0\n-      else layers.Activation(\"linear\", name=name + \"_identity\")\n-    )\n-\n-  def call(self, inputs):\n+  def apply(inputs):\n     x = inputs\n     \n-    x = self.depthwise_conv_1(x)\n-    x = self.layer_norm(x)\n-    x = self.pointwise_conv_1(x)\n-    x = self.activation(x)\n-    x = self.pointwise_conv_2(x)\n+    x = layers.Conv2D(\n+      filters=projection_dim, kernel_size=7, padding=\"same\",\n+      groups=projection_dim, name=name + \"_depthwise_conv\")(x)\n+    x = layers.LayerNormalization(epsilon=1e-6, \n+      name=name + \"_layernorm\")(x)\n+    x = layers.Dense(4 * projection_dim,\n+      name=name + \"_pointwise_conv_1\")(x)\n+    x = layers.Activation(\"gelu\", name=name + \"_gelu\")(x)\n+    x = layers.Dense(projection_dim, \n+      name=name + \"_pointwise_conv_2\")(x)\n     \n-    if self.gamma is not None:\n-      x = self.gamma * x\n+    if layer_scale_init_value is not None:\n+      x = LayerScale(layer_scale_init_value, projection_dim,\n+        name=name + \"_layer_scale\")(x)\n+    if drop_path_rate:\n+      layer = StochasticDepth(drop_path_rate, name=name + \"_stochastic_depth\")\n+    else:\n+      layer = layers.Activation(\"linear\", name=name + \"_identity\")\n     \n-    return inputs + self.drop_path(x)\n-\n-  def get_config(self):\n-    config = {\n-      \"projection_dim\": self.projection_dim,\n-      \"drop_path_rate\": self.drop_path_rate,\n-      \"layer_scale_init_value\": self.layer_scale_init_value,\n-    }\n-    return config\n+    return inputs + layer(x)\n+  return apply\n \n \n def PreStem(name=None):\n   \"\"\"Normalizes inputs with ImageNet-1k mean and std.\n-\n   Args:\n     name (str): Name prefix.\n-\n   Returns:\n     Normalized tensor.\n   \"\"\"\n@@ -286,11 +274,9 @@ def PreStem(name=None):\n \n def Head(num_classes=1000, name=None):\n   \"\"\"Implementation of classification head of RegNet.\n-\n   Args:\n     num_classes: number of classes for Dense layer\n     name: name prefix\n-\n   Returns:\n     Output logits tensor.\n   \"\"\"\n@@ -322,7 +308,6 @@ def ConvNeXt(depths,\n   classes=1000,\n   classifier_activation=\"softmax\"):\n   \"\"\"Instantiates ConvNeXt architecture given specific configuration.\n-\n   Args:\n     depths: An iterable containing depths for each individual stages.\n     projection_dims: An iterable containing output number of channels of\n@@ -357,10 +342,8 @@ def ConvNeXt(depths,\n     classifier_activation: A `str` or callable. The activation function to use\n       on the \"top\" layer. Ignored unless `include_top=True`. Set\n       `classifier_activation=None` to return the logits of the \"top\" layer.\n-\n   Returns:\n     A `keras.Model` instance.\n-\n   Raises:\n       ValueError: in case of invalid argument for `weights`,\n         or invalid input shape.\n@@ -402,7 +385,6 @@ def ConvNeXt(depths,\n     inputs = img_input\n \n   x = inputs\n-  \n   if include_preprocessing:\n     channel_axis = 3 if backend.image_data_format() == \"channels_last\" else 1\n     num_channels = input_shape[channel_axis - 1]\n@@ -639,19 +621,16 @@ ConvNeXtXLarge.__doc__ = BASE_DOCSTRING.format(name=\"ConvNeXtXLarge\")\n @keras_export(\"keras.applications.convnext.preprocess_input\")\n def preprocess_input(x, data_format=None):  # pylint: disable=unused-argument\n   \"\"\"A placeholder method for backward compatibility.\n-\n   The preprocessing logic has been included in the efficientnet model\n   implementation. Users are no longer required to call this method to normalize\n   the input data. This method does nothing and only kept as a placeholder to\n   align the API surface between old and new version of model.\n-\n   Args:\n     x: A floating point `numpy.array` or a `tf.Tensor`.\n     data_format: Optional data format of the image tensor/array. Defaults to\n       None, in which case the global setting\n       `tf.keras.backend.image_data_format()` is used (unless you changed it, it\n       defaults to \"channels_last\").{mode}\n-\n   Returns:\n     Unchanged `numpy.array` or `tf.Tensor`.\n   \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
