{"custom_id": "keras#49df01090c05b30d4f5a4012ca60cb135b4909e9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 20 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,7 @@\n from keras.src.api_export import keras_export\n \n # Unique source of truth for the version number.\n-__version__ = \"3.6.0\"\n+__version__ = \"3.7.0\"\n \n \n @keras_export(\"keras.version\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3204664092584e21abe299204a6c125cf73a3259", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1234 | Lines Deleted: 425 | Files Changed: 23 | Hunks: 53 | Methods Changed: 97 | Complexity Δ (Sum/Max): 51/43 | Churn Δ: 1659 | Churn Cumulative: 4661 | Contributors (this commit): 5 | Commits (past 90d): 80 | Contributors (cumulative): 50 | DMM Complexity: 1.0\n\nDIFF:\n@@ -145,6 +145,9 @@ from keras.src.layers.preprocessing.image_preprocessing.auto_contrast import (\n from keras.src.layers.preprocessing.image_preprocessing.center_crop import (\n     CenterCrop,\n )\n+from keras.src.layers.preprocessing.image_preprocessing.max_num_bounding_box import (\n+    MaxNumBoundingBoxes,\n+)\n from keras.src.layers.preprocessing.image_preprocessing.random_brightness import (\n     RandomBrightness,\n )\n\n@@ -4,6 +4,7 @@ This file was autogenerated. Do not edit it by hand,\n since your modifications would be overwritten.\n \"\"\"\n \n+from keras.api.utils import bounding_boxes\n from keras.api.utils import legacy\n from keras.src.backend.common.global_state import clear_session\n from keras.src.backend.common.keras_tensor import is_keras_tensor\n\n@@ -0,0 +1,21 @@\n+\"\"\"DO NOT EDIT.\n+\n+This file was autogenerated. Do not edit it by hand,\n+since your modifications would be overwritten.\n+\"\"\"\n+\n+from keras.src.layers.preprocessing.image_preprocessing.bounding_boxes.converters import (\n+    affine_transform,\n+)\n+from keras.src.layers.preprocessing.image_preprocessing.bounding_boxes.converters import (\n+    clip_to_image_size,\n+)\n+from keras.src.layers.preprocessing.image_preprocessing.bounding_boxes.converters import (\n+    convert_format,\n+)\n+from keras.src.layers.preprocessing.image_preprocessing.bounding_boxes.converters import (\n+    crop,\n+)\n+from keras.src.layers.preprocessing.image_preprocessing.bounding_boxes.converters import (\n+    pad,\n+)\n\n@@ -145,6 +145,9 @@ from keras.src.layers.preprocessing.image_preprocessing.auto_contrast import (\n from keras.src.layers.preprocessing.image_preprocessing.center_crop import (\n     CenterCrop,\n )\n+from keras.src.layers.preprocessing.image_preprocessing.max_num_bounding_box import (\n+    MaxNumBoundingBoxes,\n+)\n from keras.src.layers.preprocessing.image_preprocessing.random_brightness import (\n     RandomBrightness,\n )\n\n@@ -4,6 +4,7 @@ This file was autogenerated. Do not edit it by hand,\n since your modifications would be overwritten.\n \"\"\"\n \n+from keras.api.utils import bounding_boxes\n from keras.api.utils import legacy\n from keras.src.backend.common.global_state import clear_session\n from keras.src.backend.common.keras_tensor import is_keras_tensor\n\n@@ -0,0 +1,21 @@\n+\"\"\"DO NOT EDIT.\n+\n+This file was autogenerated. Do not edit it by hand,\n+since your modifications would be overwritten.\n+\"\"\"\n+\n+from keras.src.layers.preprocessing.image_preprocessing.bounding_boxes.converters import (\n+    affine_transform,\n+)\n+from keras.src.layers.preprocessing.image_preprocessing.bounding_boxes.converters import (\n+    clip_to_image_size,\n+)\n+from keras.src.layers.preprocessing.image_preprocessing.bounding_boxes.converters import (\n+    convert_format,\n+)\n+from keras.src.layers.preprocessing.image_preprocessing.bounding_boxes.converters import (\n+    crop,\n+)\n+from keras.src.layers.preprocessing.image_preprocessing.bounding_boxes.converters import (\n+    pad,\n+)\n\n@@ -88,6 +88,9 @@ from keras.src.layers.preprocessing.image_preprocessing.auto_contrast import (\n from keras.src.layers.preprocessing.image_preprocessing.center_crop import (\n     CenterCrop,\n )\n+from keras.src.layers.preprocessing.image_preprocessing.max_num_bounding_box import (\n+    MaxNumBoundingBoxes,\n+)\n from keras.src.layers.preprocessing.image_preprocessing.random_brightness import (\n     RandomBrightness,\n )\n\n@@ -88,7 +88,10 @@ class AutoContrast(BaseImagePreprocessingLayer):\n         return labels\n \n     def transform_bounding_boxes(\n-        self, bounding_boxes, transformation, training=True\n+        self,\n+        bounding_boxes,\n+        transformation,\n+        training=True,\n     ):\n         return bounding_boxes\n \n\n@@ -64,7 +64,10 @@ class BaseImagePreprocessingLayer(TFDataLayer):\n         raise NotImplementedError()\n \n     def transform_bounding_boxes(\n-        self, bounding_boxes, transformation, training=True\n+        self,\n+        bounding_boxes,\n+        transformation,\n+        training=True,\n     ):\n         raise NotImplementedError()\n \n@@ -88,13 +91,19 @@ class BaseImagePreprocessingLayer(TFDataLayer):\n         return self.backend.numpy.squeeze(outputs, axis=0)\n \n     def transform_single_bounding_box(\n-        self, bounding_box, transformation, training=True\n+        self,\n+        bounding_box,\n+        transformation,\n+        training=True,\n     ):\n-        bounding_boxes = self.backend.numpy.expand_dims(bounding_box, axis=0)\n+        bounding_boxes = self._format_single_input_bounding_box(bounding_box)\n         outputs = self.transform_bounding_boxes(\n-            bounding_boxes, transformation=transformation, training=training\n+            bounding_boxes,\n+            transformation=transformation,\n+            training=training,\n         )\n-        return self.backend.numpy.squeeze(outputs, axis=0)\n+        bounding_box = self._format_single_output_bounding_box(outputs)\n+        return bounding_box\n \n     def transform_single_segmentation_mask(\n         self, segmentation_mask, transformation, training=True\n@@ -144,8 +153,11 @@ class BaseImagePreprocessingLayer(TFDataLayer):\n                         \"`bounding_box_format='xyxy'`.\"\n                     )\n                 bounding_boxes = densify_bounding_boxes(\n-                    data[\"bounding_boxes\"], backend=self.backend\n+                    data[\"bounding_boxes\"],\n+                    is_batched=is_batched,\n+                    backend=self.backend,\n                 )\n+\n                 if is_batched:\n                     data[\"bounding_boxes\"] = self.transform_bounding_boxes(\n                         bounding_boxes,\n@@ -203,6 +215,32 @@ class BaseImagePreprocessingLayer(TFDataLayer):\n             training=training,\n         )\n \n+    def _format_single_input_bounding_box(self, bounding_box):\n+        for key in bounding_box:\n+            if key == \"labels\":\n+                bounding_box[key] = self.backend.numpy.expand_dims(\n+                    bounding_box[key], axis=0\n+                )\n+            if key == \"boxes\":\n+                bounding_box[key] = self.backend.numpy.expand_dims(\n+                    bounding_box[key], axis=0\n+                )\n+\n+        return bounding_box\n+\n+    def _format_single_output_bounding_box(self, bounding_boxes):\n+        for key in bounding_boxes:\n+            if key == \"labels\":\n+                bounding_boxes[key] = self.backend.numpy.squeeze(\n+                    bounding_boxes[key], axis=0\n+                )\n+            if key == \"boxes\":\n+                bounding_boxes[key] = self.backend.numpy.squeeze(\n+                    bounding_boxes[key], axis=0\n+                )\n+\n+        return bounding_boxes\n+\n     def get_config(self):\n         config = super().get_config()\n         if self.bounding_box_format is not None:\n\n@@ -0,0 +1,519 @@\n+import math\n+\n+from keras.src.utils import backend_utils\n+\n+\n+class BoundingBox:\n+    def __init__(self):\n+        self.backend = backend_utils.DynamicBackend()\n+\n+    def convert_format(\n+        self,\n+        boxes,\n+        source: str,\n+        target: str,\n+        height=None,\n+        width=None,\n+        dtype=\"float32\",\n+    ):\n+        \"\"\"Converts `boxes` from one format to another.\n+\n+        Supported formats are:\n+\n+        - `\"xyxy\"`, also known as `corners` format. In this format the first\n+            four axes represent `[left, top, right, bottom]` in that order.\n+        - `\"rel_xyxy\"`. In this format, the axes are the same as `\"xyxy\"` but\n+            the x coordinates are normalized using the image width, and the y\n+            axes the image height. All values in `rel_xyxy` are in the range\n+            `(0, 1)`.\n+        - `\"xywh\"`. In this format the first four axes represent\n+            `[left, top, width, height]`.\n+        - `\"rel_xywh\". In this format the first four axes represent\n+            [left, top, width, height], just like `\"xywh\"`. Unlike `\"xywh\"`,\n+            the values are in the range (0, 1) instead of absolute pixel values.\n+        - `\"center_xyWH\"`. In this format the first two coordinates represent\n+            the x and y coordinates of the center of the bounding box, while the\n+            last two represent the width and height of the bounding box.\n+        - `\"center_yxHW\"`. In this format the first two coordinates represent\n+            the y and x coordinates of the center of the bounding box, while the\n+            last two represent the height and width of the bounding box.\n+        - `\"yxyx\"`. In this format the first four axes represent\n+            [top, left, bottom, right] in that order.\n+        - `\"rel_yxyx\"`. In this format, the axes are the same as `\"yxyx\"` but\n+            the x coordinates are normalized using the image width, and the y\n+            axes the image height. All values in `rel_yxyx` are in the range\n+            (0, 1).\n+        Formats are case insensitive. It is recommended that you capitalize\n+        width and height to maximize the visual difference between `\"xyWH\"`\n+        and `\"xyxy\"`.\n+\n+        Relative formats, abbreviated `rel`, make use of the shapes of the\n+        `images` passed. In these formats, the coordinates, widths, and heights\n+        are all specified as percentages of the host image.\n+\n+        Example:\n+\n+        ```python\n+        boxes = {\n+            \"boxes\": [TODO],\n+            \"labels\": [TODO],\n+        }\n+        boxes_in_xywh = keras.utils.bounding_boxes.convert_format(\n+            boxes,\n+            source='xyxy',\n+            target='xyWH'\n+        )\n+        ```\n+\n+        Args:\n+            boxes: tensor representing bounding boxes in the format specified in\n+                the `source` parameter. `boxes` can optionally have extra\n+                dimensions stacked on the final axis to store metadata. boxes\n+                should be a 3D tensor, with the shape\n+                `[batch_size, num_boxes, 4]`. Alternatively, boxes can be a\n+                dictionary with key 'boxes' containing a tensor matching the\n+                aforementioned spec.\n+            source:One of `\"xyxy\"`, `\"yxyx\"`, `\"xywh\"`, `\"center_xywh\"`,\n+                `\"center_yxhw\"`, `\"rel_xyxy\"`, \"rel_yxyx\", \"rel_xywh\",\n+                \"rel_center_xywh\". Used to specify the original format of the\n+                `boxes` parameter.\n+            target:One of `\"xyxy\"`, `\"yxyx\"`, `\"xywh\"`, `\"center_xywh\"`,\n+                `\"center_yxhw\"`, `\"rel_xyxy\"`, \"rel_yxyx\", \"rel_xywh\",\n+                \"rel_center_xywh\". Used to specify the destination format of\n+                the `boxes` parameter.\n+            images: (Optional) a batch of images aligned with `boxes` on the\n+                first axis. Should be at least 3 dimensions, with the first 3\n+                dimensions representing: `[batch_size, height, width]`. Used in\n+                some converters to compute relative pixel values of the bounding\n+                box dimensions. Required when transforming from a rel format to\n+                a non-rel format.\n+            dtype: the data type to use when transforming the boxes, defaults to\n+                `\"float32\"`.\n+        \"\"\"\n+        if isinstance(boxes, dict):\n+            boxes[\"boxes\"] = self.convert_format(\n+                boxes[\"boxes\"],\n+                source=source,\n+                target=target,\n+                height=height,\n+                width=width,\n+                dtype=dtype,\n+            )\n+            return boxes\n+\n+        to_xyxy_converters = {\n+            \"xyxy\": self._xyxy_to_xyxy,\n+            \"yxyx\": self._yxyx_to_xyxy,\n+            \"xywh\": self._xywh_to_xyxy,\n+            \"center_xywh\": self._center_xywh_to_xyxy,\n+            \"center_yxhw\": self._center_yxhw_to_xyxy,\n+            \"rel_xyxy\": self._rel_xyxy_to_xyxy,\n+            \"rel_yxyx\": self._rel_yxyx_to_xyxy,\n+            \"rel_xywh\": self._rel_xywh_to_xyxy,\n+            \"rel_center_xywh\": self._rel_center_xywh_to_xyxy,\n+        }\n+        from_xyxy_converters = {\n+            \"xyxy\": self._xyxy_to_xyxy,\n+            \"yxyx\": self._xyxy_to_yxyx,\n+            \"xywh\": self._xyxy_to_xywh,\n+            \"center_xywh\": self._xyxy_to_center_xywh,\n+            \"center_yxhw\": self._xyxy_to_center_yxhw,\n+            \"rel_xyxy\": self._xyxy_to_rel_xyxy,\n+            \"rel_yxyx\": self._xyxy_to_rel_yxyx,\n+            \"rel_xywh\": self._xyxy_to_rel_xywh,\n+            \"rel_center_xywh\": self._xyxy_to_rel_center_xywh,\n+        }\n+\n+        ops = self.backend\n+        boxes_shape = ops.shape(boxes)\n+        if boxes_shape[-1] != 4:\n+            raise ValueError(\n+                \"`boxes` must be a tensor with the last dimension of 4. \"\n+                f\"Received: boxes.shape={boxes_shape}\"\n+            )\n+        source = source.lower()\n+        target = target.lower()\n+        if source not in to_xyxy_converters.keys():\n+            raise ValueError(\n+                f\"Available source: {list(to_xyxy_converters.keys())}. \"\n+                f\"Received: source={source}\"\n+            )\n+        if target not in from_xyxy_converters.keys():\n+            raise ValueError(\n+                f\"Available target: {list(from_xyxy_converters.keys())}. \"\n+                f\"Received: target={target}\"\n+            )\n+        boxes = ops.cast(boxes, dtype)\n+        if source == target:\n+            return boxes\n+        if height is not None:\n+            height = ops.cast(height, dtype)\n+        if width is not None:\n+            width = ops.cast(width, dtype)\n+\n+        if source.startswith(\"rel_\") and target.startswith(\"rel_\"):\n+            source = source.replace(\"rel_\", \"\", 1)\n+            target = target.replace(\"rel_\", \"\", 1)\n+        to_xyxy_converter = to_xyxy_converters[source]\n+        from_xyxy_converter = from_xyxy_converters[target]\n+        in_xyxy_boxes = to_xyxy_converter(boxes, height, width)\n+        return from_xyxy_converter(in_xyxy_boxes, height, width)\n+\n+    def clip_to_image_size(\n+        self, bounding_boxes, height=None, width=None, format=\"xyxy\"\n+    ):\n+        if format not in (\"xyxy\", \"rel_xyxy\"):\n+            raise NotImplementedError\n+        if format == \"xyxy\" and (height is None or width is None):\n+            raise ValueError(\n+                \"`height` and `width` must be set if `format='xyxy'`.\"\n+            )\n+\n+        ops = self.backend\n+        boxes, labels = bounding_boxes[\"boxes\"], bounding_boxes[\"labels\"]\n+\n+        if format == \"xyxy\":\n+            x1, y1, x2, y2 = ops.numpy.split(boxes, 4, axis=-1)\n+            x1 = ops.numpy.clip(x1, 0, width)\n+            y1 = ops.numpy.clip(y1, 0, height)\n+            x2 = ops.numpy.clip(x2, 0, width)\n+            y2 = ops.numpy.clip(y2, 0, height)\n+            boxes = ops.numpy.concatenate([x1, y1, x2, y2], axis=-1)\n+\n+            areas = self._compute_area(boxes)\n+            areas = ops.numpy.squeeze(areas, axis=-1)\n+            labels = ops.numpy.where(areas > 0, labels, -1)\n+        elif format == \"rel_xyxy\":\n+            x1, y1, x2, y2 = ops.numpy.split(boxes, 4, axis=-1)\n+            x1 = ops.numpy.clip(x1, 0.0, 1.0)\n+            y1 = ops.numpy.clip(y1, 0.0, 1.0)\n+            x2 = ops.numpy.clip(x2, 0.0, 1.0)\n+            y2 = ops.numpy.clip(y2, 0.0, 1.0)\n+            boxes = ops.numpy.concatenate([x1, y1, x2, y2], axis=-1)\n+\n+            areas = self._compute_area(boxes)\n+            areas = ops.numpy.squeeze(areas, axis=-1)\n+            labels = ops.numpy.where(areas > 0, labels, -1)\n+\n+        result = bounding_boxes.copy()\n+        result[\"boxes\"] = boxes\n+        result[\"labels\"] = labels\n+        return result\n+\n+    def affine(\n+        self,\n+        boxes,\n+        angle,\n+        translate_x,\n+        translate_y,\n+        scale,\n+        shear_x,\n+        shear_y,\n+        height,\n+        width,\n+        center_x=None,\n+        center_y=None,\n+    ):\n+        ops = self.backend\n+\n+        boxes_shape = ops.shape(boxes)\n+        batch_size = boxes_shape[0]\n+        n_boxes = boxes_shape[1]\n+        if center_x is None:\n+            center_x = 0.5\n+        if center_y is None:\n+            center_y = 0.5\n+\n+        matrix = self._compute_inverse_affine_matrix(\n+            center_x,\n+            center_y,\n+            angle,\n+            translate_x,\n+            translate_y,\n+            scale,\n+            shear_x,\n+            shear_y,\n+            height,\n+            width,\n+        )\n+        transposed_matrix = ops.numpy.transpose(matrix[:, :2, :], [0, 2, 1])\n+        points = boxes  # [B, N, 4]\n+        points = ops.numpy.stack(\n+            [\n+                points[..., 0],\n+                points[..., 1],\n+                points[..., 2],\n+                points[..., 1],\n+                points[..., 2],\n+                points[..., 3],\n+                points[..., 0],\n+                points[..., 3],\n+            ],\n+            axis=-1,\n+        )\n+        points = ops.numpy.reshape(points, [batch_size, n_boxes, 4, 2])\n+        points = ops.numpy.concatenate(\n+            [\n+                points,\n+                ops.numpy.ones([batch_size, n_boxes, 4, 1], points.dtype),\n+            ],\n+            axis=-1,\n+        )\n+        transformed_points = ops.numpy.einsum(\n+            \"bnxy,byz->bnxz\", points, transposed_matrix\n+        )\n+        boxes_min = ops.numpy.amin(transformed_points, axis=2)\n+        boxes_max = ops.numpy.amax(transformed_points, axis=2)\n+        outputs = ops.numpy.concatenate([boxes_min, boxes_max], axis=-1)\n+        return outputs\n+\n+    def crop(self, boxes, top, left, height, width):\n+        ops = self.backend\n+\n+        x1, y1, x2, y2 = ops.numpy.split(boxes, 4, axis=-1)\n+        x1 = x1 - left\n+        y1 = y1 - top\n+        x2 = x2 - left\n+        y2 = y2 - top\n+        x1 = ops.numpy.clip(x1, 0, width)\n+        y1 = ops.numpy.clip(y1, 0, height)\n+        x2 = ops.numpy.clip(x2, 0, width)\n+        y2 = ops.numpy.clip(y2, 0, height)\n+        outputs = ops.numpy.concatenate([x1, y1, x2, y2], axis=-1)\n+        return outputs\n+\n+    def pad(self, boxes, top, left):\n+        ops = self.backend\n+\n+        x1, y1, x2, y2 = ops.numpy.split(boxes, 4, axis=-1)\n+        x1 = x1 + left\n+        y1 = y1 + top\n+        x2 = x2 + left\n+        y2 = y2 + top\n+        outputs = ops.numpy.concatenate([x1, y1, x2, y2], axis=-1)\n+        return outputs\n+\n+    # Converters\n+\n+    def _xyxy_to_xyxy(self, boxes, height=None, width=None):\n+        return boxes\n+\n+    def _yxyx_to_xyxy(self, boxes, height=None, width=None):\n+        y1, x1, y2, x2 = self.backend.numpy.split(boxes, 4, axis=-1)\n+        return self.backend.numpy.concatenate([x1, y1, x2, y2], axis=-1)\n+\n+    def _xywh_to_xyxy(self, boxes, height=None, width=None):\n+        x1, y1, w, h = self.backend.numpy.split(boxes, 4, axis=-1)\n+        x2 = x1 + w\n+        y2 = y1 + h\n+        return self.backend.numpy.concatenate([x1, y1, x2, y2], axis=-1)\n+\n+    def _center_xywh_to_xyxy(self, boxes, height=None, width=None):\n+        ops = self.backend\n+        cx, cy, w, h = ops.numpy.split(boxes, 4, axis=-1)\n+        half_w = w / 2.0\n+        half_h = h / 2.0\n+        x1 = cx - half_w\n+        y1 = cy - half_h\n+        x2 = cx + half_w\n+        y2 = cy + half_h\n+        return self.backend.numpy.concatenate([x1, y1, x2, y2], axis=-1)\n+\n+    def _center_yxhw_to_xyxy(self, boxes, height=None, width=None):\n+        ops = self.backend\n+        cy, cx, h, w = ops.numpy.split(boxes, 4, axis=-1)\n+        half_w = w / 2.0\n+        half_h = h / 2.0\n+        x1 = cx - half_w\n+        y1 = cy - half_h\n+        x2 = cx + half_w\n+        y2 = cy + half_h\n+        return self.backend.numpy.concatenate([x1, y1, x2, y2], axis=-1)\n+\n+    def _rel_xyxy_to_xyxy(self, boxes, height=None, width=None):\n+        ops = self.backend\n+        rel_x1, rel_y1, rel_x2, rel_y2 = ops.numpy.split(boxes, 4, axis=-1)\n+        x1 = rel_x1 * width\n+        y1 = rel_y1 * height\n+        x2 = rel_x2 * width\n+        y2 = rel_y2 * height\n+        return self.backend.numpy.concatenate([x1, y1, x2, y2], axis=-1)\n+\n+    def _rel_yxyx_to_xyxy(self, boxes, height=None, width=None):\n+        ops = self.backend\n+        rel_y1, rel_x1, rel_y2, rel_x2 = ops.numpy.split(boxes, 4, axis=-1)\n+        x1 = rel_x1 * width\n+        y1 = rel_y1 * height\n+        x2 = rel_x2 * width\n+        y2 = rel_y2 * height\n+        return self.backend.numpy.concatenate([x1, y1, x2, y2], axis=-1)\n+\n+    def _rel_xywh_to_xyxy(self, boxes, height=None, width=None):\n+        ops = self.backend\n+        rel_x1, rel_y1, rel_w, rel_h = ops.numpy.split(boxes, 4, axis=-1)\n+        x1 = rel_x1 * width\n+        y1 = rel_y1 * height\n+        x2 = (rel_x1 + rel_w) * width\n+        y2 = (rel_y1 + rel_h) * height\n+        return self.backend.numpy.concatenate([x1, y1, x2, y2], axis=-1)\n+\n+    def _rel_center_xywh_to_xyxy(self, boxes, height=None, width=None):\n+        ops = self.backend\n+        rel_cx, rel_cy, rel_w, rel_h = ops.numpy.split(boxes, 4, axis=-1)\n+        half_rel_w = rel_w / 2.0\n+        half_rel_h = rel_h / 2.0\n+        x1 = (rel_cx - half_rel_w) * height\n+        y1 = (rel_cy - half_rel_h) * width\n+        x2 = (rel_cx + half_rel_w) * height\n+        y2 = (rel_cy + half_rel_h) * width\n+        return self.backend.numpy.concatenate([x1, y1, x2, y2], axis=-1)\n+\n+    def _xyxy_to_yxyx(self, boxes, height=None, width=None):\n+        x1, y1, x2, y2 = self.backend.numpy.split(boxes, 4, axis=-1)\n+        return self.backend.numpy.concatenate([y1, x1, y2, x2], axis=-1)\n+\n+    def _xyxy_to_xywh(self, boxes, height=None, width=None):\n+        x1, y1, x2, y2 = self.backend.numpy.split(boxes, 4, axis=-1)\n+        w = x2 - x1\n+        h = y2 - y1\n+        return self.backend.numpy.concatenate([x1, y1, w, h], axis=-1)\n+\n+    def _xyxy_to_center_xywh(self, boxes, height=None, width=None):\n+        x1, y1, x2, y2 = self.backend.numpy.split(boxes, 4, axis=-1)\n+        cx = x1 + ((x2 - x1) / 2.0)\n+        cy = y1 + ((y2 - y1) / 2.0)\n+        w = x2 - x1\n+        h = y2 - y1\n+        return self.backend.numpy.concatenate([cx, cy, w, h], axis=-1)\n+\n+    def _xyxy_to_center_yxhw(self, boxes, height=None, width=None):\n+        x1, y1, x2, y2 = self.backend.numpy.split(boxes, 4, axis=-1)\n+        cx = x1 + ((x2 - x1) / 2.0)\n+        cy = y1 + ((y2 - y1) / 2.0)\n+        w = x2 - x1\n+        h = y2 - y1\n+        return self.backend.numpy.concatenate([cy, cx, h, w], axis=-1)\n+\n+    def _xyxy_to_rel_xyxy(self, boxes, height=None, width=None):\n+        x1, y1, x2, y2 = self.backend.numpy.split(boxes, 4, axis=-1)\n+        rel_x1 = self.backend.numpy.divide(x1, width)\n+        rel_y1 = self.backend.numpy.divide(y1, height)\n+        rel_x2 = self.backend.numpy.divide(x2, width)\n+        rel_y2 = self.backend.numpy.divide(y2, height)\n+        return self.backend.numpy.concatenate(\n+            [rel_x1, rel_y1, rel_x2, rel_y2], axis=-1\n+        )\n+\n+    def _xyxy_to_rel_yxyx(self, boxes, height=None, width=None):\n+        x1, y1, x2, y2 = self.backend.numpy.split(boxes, 4, axis=-1)\n+        rel_x1 = self.backend.numpy.divide(x1, width)\n+        rel_y1 = self.backend.numpy.divide(y1, height)\n+        rel_x2 = self.backend.numpy.divide(x2, width)\n+        rel_y2 = self.backend.numpy.divide(y2, height)\n+        return self.backend.numpy.concatenate(\n+            [rel_y1, rel_x1, rel_y2, rel_x2], axis=-1\n+        )\n+\n+    def _xyxy_to_rel_xywh(self, boxes, height=None, width=None):\n+        x1, y1, x2, y2 = self.backend.numpy.split(boxes, 4, axis=-1)\n+        rel_x1 = x1 / width\n+        rel_y1 = y1 / height\n+        rel_w = (x2 - x1) / width\n+        rel_h = (y2 - y1) / height\n+        return self.backend.numpy.concatenate(\n+            [rel_x1, rel_y1, rel_w, rel_h], axis=-1\n+        )\n+\n+    def _xyxy_to_rel_center_xywh(self, boxes, height=None, width=None):\n+        x1, y1, x2, y2 = self.backend.numpy.split(boxes, 4, axis=-1)\n+        rel_cx = (x1 + ((x2 - x1) / 2.0)) / width\n+        rel_cy = (y1 + ((y2 - y1) / 2.0)) / height\n+        rel_w = (x2 - x1) / width\n+        rel_h = (y2 - y1) / height\n+        return self.backend.numpy.concatenate(\n+            [rel_cx, rel_cy, rel_w, rel_h], axis=-1\n+        )\n+\n+    # Clip\n+    def _compute_area(self, boxes, format=\"xyxy\"):\n+        if format not in (\"xyxy\", \"rel_xyxy\"):\n+            raise NotImplementedError\n+\n+        ops = self.backend\n+        x1, y1, x2, y2 = ops.numpy.split(boxes, 4, axis=-1)\n+        widths = x2 - x1\n+        heights = y2 - y1\n+        return widths * heights\n+\n+    # Affine\n+    def _compute_inverse_affine_matrix(\n+        self,\n+        center_x,\n+        center_y,\n+        angle,\n+        translate_x,\n+        translate_y,\n+        scale,\n+        shear_x,\n+        shear_y,\n+        height,\n+        width,\n+    ):\n+        # Ref: TF._geometry._get_inverse_affine_matrix\n+        ops = self.backend\n+        batch_size = ops.shape(angle)[0]\n+        dtype = angle.dtype\n+        width = ops.cast(width, dtype)\n+        height = ops.cast(height, dtype)\n+\n+        angle = -angle\n+        shear_x = -shear_x\n+        shear_y = -shear_y\n+\n+        cx = center_x * width\n+        cy = center_y * height\n+        rot = ops.numpy.multiply(angle, 1.0 / 180.0 * math.pi)\n+        tx = -translate_x * width\n+        ty = -translate_y * height\n+        sx = ops.numpy.multiply(shear_x, 1.0 / 180.0 * math.pi)\n+        sy = ops.numpy.multiply(shear_y, 1.0 / 180.0 * math.pi)\n+\n+        # Cached results\n+        cos_sy = ops.numpy.cos(sy)\n+        tan_sx = ops.numpy.tan(sx)\n+        rot_minus_sy = rot - sy\n+        cx_plus_tx = cx + tx\n+        cy_plus_ty = cy + ty\n+\n+        # Rotate Scale Shear (RSS) without scaling\n+        a = ops.numpy.cos(rot_minus_sy) / cos_sy\n+        b = -(a * tan_sx + ops.numpy.sin(rot))\n+        c = ops.numpy.sin(rot_minus_sy) / cos_sy\n+        d = ops.numpy.cos(rot) - c * tan_sx\n+\n+        # Inverted rotation matrix with scale and shear\n+        # det([[a, b], [c, d]]) == 1, since det(rotation) = 1 and det(shear) = 1\n+        a0 = d * scale\n+        a1 = -b * scale\n+        b0 = -c * scale\n+        b1 = a * scale\n+        a2 = cx - a0 * cx_plus_tx - a1 * cy_plus_ty\n+        b2 = cy - b0 * cx_plus_tx - b1 * cy_plus_ty\n+\n+        # Shape of matrix: [[batch_size], ...] -> [batch_size, 6]\n+        matrix = ops.numpy.stack(\n+            [\n+                a0,\n+                a1,\n+                a2,\n+                b0,\n+                b1,\n+                b2,\n+                ops.numpy.zeros([batch_size], dtype),\n+                ops.numpy.zeros([batch_size], dtype),\n+                ops.numpy.ones([batch_size], dtype),\n+            ],\n+            axis=-1,\n+        )\n+        matrix = ops.numpy.reshape(matrix, [batch_size, 3, 3])\n+        return matrix\n\n@@ -1,417 +1,104 @@\n-\"\"\"Converter functions for working with bounding box formats.\"\"\"\n-\n-from keras.src import ops\n-from keras.src.utils import tf_utils\n-\n-\n-# Internal exception to propagate the fact images was not passed to a converter\n-# that needs it.\n-class RequiresImagesException(Exception):\n-    pass\n-\n-\n-ALL_AXES = 4\n-\n-\n-def _center_yxhw_to_xyxy(boxes, images=None, image_shape=None):\n-    y, x, height, width = ops.split(boxes, ALL_AXES, axis=-1)\n-    return ops.concatenate(\n-        [x - width / 2.0, y - height / 2.0, x + width / 2.0, y + height / 2.0],\n-        axis=-1,\n+from keras.src.api_export import keras_export\n+from keras.src.layers.preprocessing.image_preprocessing.bounding_boxes.bounding_box import (  # noqa: E501\n+    BoundingBox,\n )\n+from keras.src.utils import backend_utils\n \n \n-def _center_xywh_to_xyxy(boxes, images=None, image_shape=None):\n-    x, y, width, height = ops.split(boxes, ALL_AXES, axis=-1)\n-    return ops.concatenate(\n-        [x - width / 2.0, y - height / 2.0, x + width / 2.0, y + height / 2.0],\n-        axis=-1,\n-    )\n-\n-\n-def _xywh_to_xyxy(boxes, images=None, image_shape=None):\n-    x, y, width, height = ops.split(boxes, ALL_AXES, axis=-1)\n-    return ops.concatenate([x, y, x + width, y + height], axis=-1)\n-\n-\n-def _xyxy_to_center_yxhw(boxes, images=None, image_shape=None):\n-    left, top, right, bottom = ops.split(boxes, ALL_AXES, axis=-1)\n-    return ops.concatenate(\n-        [\n-            (top + bottom) / 2.0,\n-            (left + right) / 2.0,\n-            bottom - top,\n-            right - left,\n-        ],\n-        axis=-1,\n-    )\n-\n-\n-def _rel_xywh_to_xyxy(boxes, images=None, image_shape=None):\n-    image_height, image_width = _image_shape(images, image_shape, boxes)\n-    x, y, width, height = ops.split(boxes, ALL_AXES, axis=-1)\n-    return ops.concatenate(\n-        [\n-            image_width * x,\n-            image_height * y,\n-            image_width * (x + width),\n-            image_height * (y + height),\n-        ],\n-        axis=-1,\n-    )\n-\n-\n-def _xyxy_no_op(boxes, images=None, image_shape=None):\n-    return boxes\n-\n-\n-def _xyxy_to_xywh(boxes, images=None, image_shape=None):\n-    left, top, right, bottom = ops.split(boxes, ALL_AXES, axis=-1)\n-    return ops.concatenate(\n-        [left, top, right - left, bottom - top],\n-        axis=-1,\n-    )\n-\n-\n-def _xyxy_to_rel_xywh(boxes, images=None, image_shape=None):\n-    image_height, image_width = _image_shape(images, image_shape, boxes)\n-    left, top, right, bottom = ops.split(boxes, ALL_AXES, axis=-1)\n-    left, right = (\n-        left / image_width,\n-        right / image_width,\n-    )\n-    top, bottom = top / image_height, bottom / image_height\n-    return ops.concatenate(\n-        [left, top, right - left, bottom - top],\n-        axis=-1,\n-    )\n-\n-\n-def _xyxy_to_center_xywh(boxes, images=None, image_shape=None):\n-    left, top, right, bottom = ops.split(boxes, ALL_AXES, axis=-1)\n-    return ops.concatenate(\n-        [\n-            (left + right) / 2.0,\n-            (top + bottom) / 2.0,\n-            right - left,\n-            bottom - top,\n-        ],\n-        axis=-1,\n-    )\n-\n-\n-def _rel_xyxy_to_xyxy(boxes, images=None, image_shape=None):\n-    image_height, image_width = _image_shape(images, image_shape, boxes)\n-    left, top, right, bottom = ops.split(\n-        boxes,\n-        ALL_AXES,\n-        axis=-1,\n-    )\n-    left, right = left * image_width, right * image_width\n-    top, bottom = top * image_height, bottom * image_height\n-    return ops.concatenate(\n-        [left, top, right, bottom],\n-        axis=-1,\n-    )\n-\n-\n-def _xyxy_to_rel_xyxy(boxes, images=None, image_shape=None):\n-    image_height, image_width = _image_shape(images, image_shape, boxes)\n-    left, top, right, bottom = ops.split(\n-        boxes,\n-        ALL_AXES,\n-        axis=-1,\n-    )\n-    left, right = left / image_width, right / image_width\n-    top, bottom = top / image_height, bottom / image_height\n-    return ops.concatenate(\n-        [left, top, right, bottom],\n-        axis=-1,\n-    )\n-\n-\n-def _yxyx_to_xyxy(boxes, images=None, image_shape=None):\n-    y1, x1, y2, x2 = ops.split(boxes, ALL_AXES, axis=-1)\n-    return ops.concatenate([x1, y1, x2, y2], axis=-1)\n-\n-\n-def _rel_yxyx_to_xyxy(boxes, images=None, image_shape=None):\n-    image_height, image_width = _image_shape(images, image_shape, boxes)\n-    top, left, bottom, right = ops.split(\n-        boxes,\n-        ALL_AXES,\n-        axis=-1,\n-    )\n-    left, right = left * image_width, right * image_width\n-    top, bottom = top * image_height, bottom * image_height\n-    return ops.concatenate(\n-        [left, top, right, bottom],\n-        axis=-1,\n-    )\n-\n-\n-def _xyxy_to_yxyx(boxes, images=None, image_shape=None):\n-    x1, y1, x2, y2 = ops.split(boxes, ALL_AXES, axis=-1)\n-    return ops.concatenate([y1, x1, y2, x2], axis=-1)\n-\n-\n-def _xyxy_to_rel_yxyx(boxes, images=None, image_shape=None):\n-    image_height, image_width = _image_shape(images, image_shape, boxes)\n-    left, top, right, bottom = ops.split(boxes, ALL_AXES, axis=-1)\n-    left, right = left / image_width, right / image_width\n-    top, bottom = top / image_height, bottom / image_height\n-    return ops.concatenate(\n-        [top, left, bottom, right],\n-        axis=-1,\n-    )\n-\n-\n-TO_XYXY_CONVERTERS = {\n-    \"xywh\": _xywh_to_xyxy,\n-    \"center_xywh\": _center_xywh_to_xyxy,\n-    \"center_yxhw\": _center_yxhw_to_xyxy,\n-    \"rel_xywh\": _rel_xywh_to_xyxy,\n-    \"xyxy\": _xyxy_no_op,\n-    \"rel_xyxy\": _rel_xyxy_to_xyxy,\n-    \"yxyx\": _yxyx_to_xyxy,\n-    \"rel_yxyx\": _rel_yxyx_to_xyxy,\n-}\n-\n-FROM_XYXY_CONVERTERS = {\n-    \"xywh\": _xyxy_to_xywh,\n-    \"center_xywh\": _xyxy_to_center_xywh,\n-    \"center_yxhw\": _xyxy_to_center_yxhw,\n-    \"rel_xywh\": _xyxy_to_rel_xywh,\n-    \"xyxy\": _xyxy_no_op,\n-    \"rel_xyxy\": _xyxy_to_rel_xyxy,\n-    \"yxyx\": _xyxy_to_yxyx,\n-    \"rel_yxyx\": _xyxy_to_rel_yxyx,\n-}\n-\n-\n+@keras_export(\"keras.utils.bounding_boxes.convert_format\")\n def convert_format(\n-    boxes, source, target, images=None, image_shape=None, dtype=\"float32\"\n+    boxes, source, target, height=None, width=None, dtype=\"float32\"\n ):\n-    f\"\"\"Converts bounding_boxes from one format to another.\n-\n-    Supported formats are:\n-\n-    - `\"xyxy\"`, also known as `corners` format. In this format the first four\n-        axes represent `[left, top, right, bottom]` in that order.\n-    - `\"rel_xyxy\"`. In this format, the axes are the same as `\"xyxy\"` but the x\n-        coordinates are normalized using the image width, and the y axes the\n-        image height. All values in `rel_xyxy` are in the range `(0, 1)`.\n-    - `\"xywh\"`. In this format the first four axes represent\n-        `[left, top, width, height]`.\n-    - `\"rel_xywh\". In this format the first four axes represent\n-        [left, top, width, height], just like `\"xywh\"`. Unlike `\"xywh\"`, the\n-        values are in the range (0, 1) instead of absolute pixel values.\n-    - `\"center_xyWH\"`. In this format the first two coordinates represent the x\n-        and y coordinates of the center of the bounding box, while the last two\n-        represent the width and height of the bounding box.\n-    - `\"center_yxHW\"`. In this format the first two coordinates represent the y\n-        and x coordinates of the center of the bounding box, while the last two\n-        represent the height and width of the bounding box.\n-    - `\"yxyx\"`. In this format the first four axes represent\n-        [top, left, bottom, right] in that order.\n-    - `\"rel_yxyx\"`. In this format, the axes are the same as `\"yxyx\"` but the x\n-        coordinates are normalized using the image width, and the y axes the\n-        image height. All values in `rel_yxyx` are in the range (0, 1).\n-    Formats are case insensitive. It is recommended that you capitalize width\n-    and height to maximize the visual difference between `\"xyWH\"` and `\"xyxy\"`.\n-\n-    Relative formats, abbreviated `rel`, make use of the shapes of the `images`\n-    passed. In these formats, the coordinates, widths, and heights are all\n-    specified as percentages of the host image.\n-\n-    Example:\n-\n-    ```python\n-    boxes = {\n-        \"boxes\": [TODO],\n-        \"labels\": [TODO],\n-    }\n-    boxes_in_xywh = keras.utils.bounding_boxes.convert_format(\n-        boxes,\n-        source='xyxy',\n-        target='xyWH'\n-    )\n-    ```\n-\n-    Args:\n-        boxes: tensor representing bounding boxes in the format specified in\n-            the `source` parameter. `boxes` can optionally have extra\n-            dimensions stacked on the final axis to store metadata. boxes\n-            should be a 3D tensor, with the shape `[batch_size, num_boxes, 4]`.\n-            Alternatively, boxes can be a dictionary with key 'boxes' containing\n-            a tensor matching the aforementioned spec.\n-        source:One of {\" \".join([f'\"{f}\"' for f in TO_XYXY_CONVERTERS.keys()])}.\n-            Used to specify the original format of the `boxes` parameter.\n-        target:One of {\" \".join([f'\"{f}\"' for f in TO_XYXY_CONVERTERS.keys()])}.\n-            Used to specify the destination format of the `boxes` parameter.\n-        images: (Optional) a batch of images aligned with `boxes` on the first\n-            axis. Should be at least 3 dimensions, with the first 3 dimensions\n-            representing: `[batch_size, height, width]`. Used in some\n-            converters to compute relative pixel values of the bounding box\n-            dimensions. Required when transforming from a rel format to a\n-            non-rel format.\n-        dtype: the data type to use when transforming the boxes, defaults to\n-            `\"float32\"`.\n-    \"\"\"\n-    if isinstance(boxes, dict):\n-        converted_boxes = boxes.copy()\n-        converted_boxes[\"boxes\"] = convert_format(\n-            boxes[\"boxes\"],\n+    # Switch to tensorflow backend if we are in tf.data pipe\n+    box_utils = BoundingBox()\n+    if backend_utils.in_tf_graph():\n+        box_utils.backend.set_backend(\"tensorflow\")\n+    boxes = box_utils.convert_format(\n+        boxes=boxes,\n         source=source,\n         target=target,\n-            images=images,\n-            image_shape=image_shape,\n+        height=height,\n+        width=width,\n         dtype=dtype,\n     )\n-        return converted_boxes\n-\n-    if boxes.shape[-1] is not None and boxes.shape[-1] != 4:\n-        raise ValueError(\n-            \"Expected `boxes` to be a Tensor with a final dimension of \"\n-            f\"`4`. Instead, got `boxes.shape={boxes.shape}`.\"\n-        )\n-    if images is not None and image_shape is not None:\n-        raise ValueError(\n-            \"convert_format() expects either `images` or `image_shape`, but \"\n-            f\"not both. Received images={images} image_shape={image_shape}\"\n-        )\n-\n-    _validate_image_shape(image_shape)\n-\n-    source = source.lower()\n-    target = target.lower()\n-    if source not in TO_XYXY_CONVERTERS:\n-        raise ValueError(\n-            \"`convert_format()` received an unsupported format for the \"\n-            \"argument `source`. `source` should be one of \"\n-            f\"{TO_XYXY_CONVERTERS.keys()}. Got source={source}\"\n-        )\n-    if target not in FROM_XYXY_CONVERTERS:\n-        raise ValueError(\n-            \"`convert_format()` received an unsupported format for the \"\n-            \"argument `target`. `target` should be one of \"\n-            f\"{FROM_XYXY_CONVERTERS.keys()}. Got target={target}\"\n-        )\n-\n-    boxes = ops.cast(boxes, dtype)\n-    if source == target:\n-        return boxes\n-\n-    # rel->rel conversions should not require images\n-    if source.startswith(\"rel\") and target.startswith(\"rel\"):\n-        source = source.replace(\"rel_\", \"\", 1)\n-        target = target.replace(\"rel_\", \"\", 1)\n-\n-    boxes, images, squeeze = _format_inputs(boxes, images)\n-    to_xyxy_fn = TO_XYXY_CONVERTERS[source]\n-    from_xyxy_fn = FROM_XYXY_CONVERTERS[target]\n-\n-    try:\n-        in_xyxy = to_xyxy_fn(boxes, images=images, image_shape=image_shape)\n-        result = from_xyxy_fn(in_xyxy, images=images, image_shape=image_shape)\n-    except RequiresImagesException:\n-        raise ValueError(\n-            \"convert_format() must receive `images` or `image_shape` when \"\n-            \"transforming between relative and absolute formats.\"\n-            f\"convert_format() received source=`{format}`, target=`{format}, \"\n-            f\"but images={images} and image_shape={image_shape}.\"\n-        )\n-\n-    return _format_outputs(result, squeeze)\n-\n-\n-def _format_inputs(boxes, images):\n-    boxes_rank = len(boxes.shape)\n-    if boxes_rank > 3:\n-        raise ValueError(\n-            \"Expected len(boxes.shape)=2, or len(boxes.shape)=3, got \"\n-            f\"len(boxes.shape)={boxes_rank}\"\n-        )\n-    boxes_includes_batch = boxes_rank == 3\n-    # Determine if images needs an expand_dims() call\n-    if images is not None:\n-        images_rank = len(images.shape)\n-        if images_rank > 4:\n-            raise ValueError(\n-                \"Expected len(images.shape)=2, or len(images.shape)=3, got \"\n-                f\"len(images.shape)={images_rank}\"\n-            )\n-        images_include_batch = images_rank == 4\n-        if boxes_includes_batch != images_include_batch:\n-            raise ValueError(\n-                \"convert_format() expects both boxes and images to be batched, \"\n-                \"or both boxes and images to be unbatched. Received \"\n-                f\"len(boxes.shape)={boxes_rank}, \"\n-                f\"len(images.shape)={images_rank}. Expected either \"\n-                \"len(boxes.shape)=2 AND len(images.shape)=3, or \"\n-                \"len(boxes.shape)=3 AND len(images.shape)=4.\"\n-            )\n-        if not images_include_batch:\n-            images = ops.expand_dims(images, axis=0)\n-\n-    if not boxes_includes_batch:\n-        return ops.expand_dims(boxes, axis=0), images, True\n-    return boxes, images, False\n-\n-\n-def _validate_image_shape(image_shape):\n-    # Escape early if image_shape is None and skip validation.\n-    if image_shape is None:\n-        return\n-    # tuple/list\n-    if isinstance(image_shape, (tuple, list)):\n-        if len(image_shape) != 3:\n-            raise ValueError(\n-                \"image_shape should be of length 3, but got \"\n-                f\"image_shape={image_shape}\"\n-            )\n-        return\n-\n-    # tensor\n-    if ops.is_tensor(image_shape):\n-        if len(image_shape.shape) > 1:\n-            raise ValueError(\n-                \"image_shape.shape should be (3), but got \"\n-                f\"image_shape.shape={image_shape.shape}\"\n-            )\n-        if image_shape.shape[0] != 3:\n-            raise ValueError(\n-                \"image_shape.shape should be (3), but got \"\n-                f\"image_shape.shape={image_shape.shape}\"\n-            )\n-        return\n-\n-    # Warn about failure cases\n-    raise ValueError(\n-        \"Expected image_shape to be either a tuple, list, Tensor. \"\n-        f\"Received image_shape={image_shape}\"\n-    )\n-\n-\n-def _format_outputs(boxes, squeeze):\n-    if squeeze:\n-        return ops.squeeze(boxes, axis=0)\n+    # Switch back to original backend\n+    box_utils.backend.reset()\n     return boxes\n \n \n-def _image_shape(images, image_shape, boxes):\n-    if images is None and image_shape is None:\n-        raise RequiresImagesException()\n+@keras_export(\"keras.utils.bounding_boxes.clip_to_image_size\")\n+def clip_to_image_size(bounding_boxes, height=None, width=None, format=\"xyxy\"):\n+    # Switch to tensorflow backend if we are in tf.data pipe\n \n-    if image_shape is None:\n-        if not tf_utils.is_ragged_tensor(images):\n-            image_shape = ops.shape(images)\n-            height, width = image_shape[1], image_shape[2]\n-        else:\n-            height = ops.reshape(images.row_lengths(), (-1, 1))\n-            width = ops.reshape(ops.max(images.row_lengths(axis=2), 1), (-1, 1))\n-            height = ops.expand_dims(height, axis=-1)\n-            width = ops.expand_dims(width, axis=-1)\n-    else:\n-        height, width = image_shape[0], image_shape[1]\n-    return ops.cast(height, boxes.dtype), ops.cast(width, boxes.dtype)\n+    box_utils = BoundingBox()\n+    if backend_utils.in_tf_graph():\n+        box_utils.backend.set_backend(\"tensorflow\")\n+    bounding_boxes = box_utils.clip_to_image_size(\n+        bounding_boxes, height=height, width=width, format=format\n+    )\n+    # Switch back to original backend\n+    box_utils.backend.reset()\n+    return bounding_boxes\n+\n+\n+@keras_export(\"keras.utils.bounding_boxes.affine_transform\")\n+def affine_transform(\n+    boxes,\n+    angle,\n+    translate_x,\n+    translate_y,\n+    scale,\n+    shear_x,\n+    shear_y,\n+    height,\n+    width,\n+    center_x=None,\n+    center_y=None,\n+    format=\"xyxy\",\n+):\n+    if format != \"xyxy\":\n+        raise NotImplementedError\n+    # Switch to tensorflow backend if we are in tf.data pipe\n+    box_utils = BoundingBox()\n+    if backend_utils.in_tf_graph():\n+        box_utils.backend.set_backend(\"tensorflow\")\n+    outputs = box_utils.affine(\n+        boxes,\n+        angle,\n+        translate_x,\n+        translate_y,\n+        scale,\n+        shear_x,\n+        shear_y,\n+        height,\n+        width,\n+        center_x=center_x,\n+        center_y=center_y,\n+    )\n+    box_utils.backend.reset()\n+    return outputs\n+\n+\n+@keras_export(\"keras.utils.bounding_boxes.crop\")\n+def crop(boxes, top, left, height, width, format=\"xyxy\"):\n+    if format != \"xyxy\":\n+        raise NotImplementedError\n+    box_utils = BoundingBox()\n+    if backend_utils.in_tf_graph():\n+        box_utils.backend.set_backend(\"tensorflow\")\n+    outputs = box_utils.crop(boxes, top, left, height, width)\n+    box_utils.backend.reset()\n+    return outputs\n+\n+\n+@keras_export(\"keras.utils.bounding_boxes.pad\")\n+def pad(boxes, top, left, format=\"xyxy\"):\n+    if format != \"xyxy\":\n+        raise NotImplementedError\n+    box_utils = BoundingBox()\n+    if backend_utils.in_tf_graph():\n+        box_utils.backend.set_backend(\"tensorflow\")\n+\n+    outputs = box_utils.pad(boxes, top, left)\n+    box_utils.backend.reset()\n+    return outputs\n\n@@ -2,8 +2,28 @@ from keras.src import backend as current_backend\n from keras.src.utils import tf_utils\n \n \n+def _classes_shape(batched, classes_shape, max_boxes):\n+    if max_boxes is None:\n+        return None\n+    if batched:\n+        return [None, max_boxes] + classes_shape[2:]\n+    return [max_boxes] + classes_shape[2:]\n+\n+\n+def _box_shape(batched, boxes_shape, max_boxes):\n+    # ensure we dont drop the final axis in RaggedTensor mode\n+    if max_boxes is None:\n+        shape = list(boxes_shape)\n+        shape[-1] = 4\n+        return shape\n+    if batched:\n+        return [None, max_boxes, 4]\n+    return [max_boxes, 4]\n+\n+\n def densify_bounding_boxes(\n     bounding_boxes,\n+    is_batched=False,\n     max_boxes=None,\n     boxes_default_value=0,\n     labels_default_value=-1,\n@@ -78,15 +98,19 @@ def densify_bounding_boxes(\n     if tf_utils.is_ragged_tensor(boxes):\n         bounding_boxes[\"boxes\"] = bounding_boxes[\"boxes\"].to_tensor(\n             default_value=boxes_default_value,\n-            shape=\"TODO\",\n+            shape=_classes_shape(\n+                is_batched, bounding_boxes[\"boxes\"].shape, max_boxes\n+            ),\n         )\n         bounding_boxes[\"labels\"] = bounding_boxes[\"labels\"].to_tensor(\n             default_value=labels_default_value,\n-            shape=\"TODO\",\n+            shape=_box_shape(\n+                is_batched, bounding_boxes[\"labels\"].shape, max_boxes\n+            ),\n         )\n         return bounding_boxes\n \n-    bounding_boxes[\"boxes\"] = backend.convert_to_tensor(boxes, dtype=\"int32\")\n+    bounding_boxes[\"boxes\"] = backend.convert_to_tensor(boxes, dtype=\"float32\")\n     bounding_boxes[\"labels\"] = backend.convert_to_tensor(labels)\n     return bounding_boxes\n \n\n@@ -0,0 +1,89 @@\n+from keras.src.api_export import keras_export\n+from keras.src.layers.preprocessing.image_preprocessing.base_image_preprocessing_layer import (  # noqa: E501\n+    BaseImagePreprocessingLayer,\n+)\n+\n+\n+@keras_export(\"keras.layers.MaxNumBoundingBoxes\")\n+class MaxNumBoundingBoxes(BaseImagePreprocessingLayer):\n+    \"\"\"Ensure the maximum number of bounding boxes.\n+\n+    Args:\n+        max_number: Desired output number of bounding boxes.\n+        padding_value: The padding value of the `boxes` and `labels` in\n+            `bounding_boxes`. Defaults to `-1`.\n+    \"\"\"\n+\n+    def __init__(self, max_number, fill_value=-1, **kwargs):\n+        super().__init__(**kwargs)\n+        self.max_number = int(max_number)\n+        self.fill_value = int(fill_value)\n+\n+    def transform_images(self, images, transformation=None, training=True):\n+        return images\n+\n+    def transform_labels(self, labels, transformation=None, training=True):\n+        return labels\n+\n+    def transform_bounding_boxes(\n+        self, bounding_boxes, transformation, training=True\n+    ):\n+        ops = self.backend\n+        boxes = bounding_boxes[\"boxes\"]\n+        labels = bounding_boxes[\"labels\"]\n+        boxes_shape = ops.shape(boxes)\n+        batch_size = boxes_shape[0]\n+        num_boxes = boxes_shape[1]\n+\n+        # Get pad size\n+        pad_size = ops.numpy.maximum(\n+            ops.numpy.subtract(self.max_number, num_boxes), 0\n+        )\n+        boxes = boxes[:, : self.max_number, ...]\n+        boxes = ops.numpy.pad(\n+            boxes,\n+            [[0, 0], [0, pad_size], [0, 0]],\n+            constant_values=self.fill_value,\n+        )\n+        labels = labels[:, : self.max_number]\n+        labels = ops.numpy.pad(\n+            labels, [[0, 0], [0, pad_size]], constant_values=self.fill_value\n+        )\n+\n+        # Ensure shape\n+        boxes = ops.numpy.reshape(boxes, [batch_size, self.max_number, 4])\n+        labels = ops.numpy.reshape(labels, [batch_size, self.max_number])\n+\n+        bounding_boxes = bounding_boxes.copy()\n+        bounding_boxes[\"boxes\"] = boxes\n+        bounding_boxes[\"labels\"] = labels\n+        return bounding_boxes\n+\n+    def transform_segmentation_masks(\n+        self, segmentation_masks, transformation=None, training=True\n+    ):\n+        return self.transform_images(segmentation_masks)\n+\n+    def compute_output_shape(self, input_shape):\n+        if isinstance(input_shape, dict) and \"bounding_boxes\" in input_shape:\n+            input_keys = set(input_shape[\"bounding_boxes\"].keys())\n+            extra_keys = input_keys - set((\"boxes\", \"labels\"))\n+            if extra_keys:\n+                raise KeyError(\n+                    \"There are unsupported keys in `bounding_boxes`: \"\n+                    f\"{list(extra_keys)}. \"\n+                    \"Only `boxes` and `labels` are supported.\"\n+                )\n+\n+            boxes_shape = list(input_shape[\"bounding_boxes\"][\"boxes\"])\n+            boxes_shape[1] = self.max_number\n+            labels_shape = list(input_shape[\"bounding_boxes\"][\"labels\"])\n+            labels_shape[1] = self.max_number\n+            input_shape[\"bounding_boxes\"][\"boxes\"] = boxes_shape\n+            input_shape[\"bounding_boxes\"][\"labels\"] = labels_shape\n+        return input_shape\n+\n+    def get_config(self):\n+        config = super().get_config()\n+        config.update({\"max_number\": self.max_number})\n+        return config\n\n@@ -0,0 +1,77 @@\n+import numpy as np\n+from tensorflow import data as tf_data\n+\n+from keras.src import backend\n+from keras.src import layers\n+from keras.src import testing\n+\n+\n+class MaxNumBoundingBoxesTest(testing.TestCase):\n+    def test_max_num_bounding_boxes_basics(self):\n+        self.run_layer_test(\n+            layers.MaxNumBoundingBoxes,\n+            init_kwargs={\n+                \"max_number\": 40,\n+                \"fill_value\": -1,\n+            },\n+            input_shape=(12, 12, 3),\n+            expected_output_shape=(12, 12, 3),\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=False,\n+            run_training_check=False,\n+        )\n+\n+    def test_output_shapes(self):\n+        if backend.config.image_data_format() == \"channels_last\":\n+            image_shape = (10, 8, 3)\n+        else:\n+            image_shape = (3, 10, 8)\n+        input_image = np.random.random(image_shape)\n+        bounding_boxes = {\n+            \"boxes\": np.array(\n+                [\n+                    [2, 1, 4, 3],\n+                    [6, 4, 8, 6],\n+                ]\n+            ),  # Example boxes (normalized)\n+            \"labels\": np.array([1, 2]),  # Dummy labels\n+        }\n+        layer = layers.MaxNumBoundingBoxes(\n+            max_number=40, bounding_box_format=\"xyxy\"\n+        )\n+\n+        input_data = {\"images\": input_image, \"bounding_boxes\": bounding_boxes}\n+        output = layer(input_data)\n+        self.assertAllEqual(output[\"bounding_boxes\"][\"boxes\"].shape, (40, 4))\n+        self.assertAllEqual(output[\"bounding_boxes\"][\"labels\"].shape, (40,))\n+\n+    def test_output_shapes_with_tf_data(self):\n+        if backend.config.image_data_format() == \"channels_last\":\n+            image_shape = (1, 10, 8, 3)\n+        else:\n+            image_shape = (1, 3, 10, 8)\n+        input_image = np.random.random(image_shape)\n+        bounding_boxes = {\n+            \"boxes\": np.array(\n+                [\n+                    [\n+                        [2, 1, 4, 3],\n+                        [6, 4, 8, 6],\n+                    ]\n+                ]\n+            ),  # Example boxes (normalized)\n+            \"labels\": np.array([[1, 2]]),  # Dummy labels\n+        }\n+        layer = layers.MaxNumBoundingBoxes(\n+            max_number=40, bounding_box_format=\"xyxy\"\n+        )\n+        input_data = {\"images\": input_image, \"bounding_boxes\": bounding_boxes}\n+        ds = tf_data.Dataset.from_tensor_slices(input_data)\n+        ds = ds.map(layer)\n+        ds = ds.batch(1)\n+        output = next(iter(ds))\n+        self.assertAllEqual(output[\"bounding_boxes\"][\"boxes\"].shape, (1, 40, 4))\n+        self.assertAllEqual(output[\"bounding_boxes\"][\"labels\"].shape, (1, 40))\n\n@@ -133,7 +133,10 @@ class RandomBrightness(BaseImagePreprocessingLayer):\n         return labels\n \n     def transform_bounding_boxes(\n-        self, bounding_boxes, transformation, training=True\n+        self,\n+        bounding_boxes,\n+        transformation,\n+        training=True,\n     ):\n         return bounding_boxes\n \n\n@@ -98,7 +98,10 @@ class RandomContrast(BaseImagePreprocessingLayer):\n         return labels\n \n     def transform_bounding_boxes(\n-        self, bounding_boxes, transformation, training=True\n+        self,\n+        bounding_boxes,\n+        transformation,\n+        training=True,\n     ):\n         return bounding_boxes\n \n\n@@ -3,6 +3,9 @@ from keras.src.api_export import keras_export\n from keras.src.layers.preprocessing.image_preprocessing.base_image_preprocessing_layer import (  # noqa: E501\n     BaseImagePreprocessingLayer,\n )\n+from keras.src.layers.preprocessing.image_preprocessing.bounding_boxes.converters import (  # noqa: E501\n+    convert_format,\n+)\n from keras.src.layers.preprocessing.image_preprocessing.bounding_boxes.validation import (  # noqa: E501\n     densify_bounding_boxes,\n )\n@@ -178,11 +181,14 @@ class RandomCrop(BaseImagePreprocessingLayer):\n         return labels\n \n     def transform_bounding_boxes(\n-        self, bounding_boxes, transformation, training=True\n+        self,\n+        bounding_boxes,\n+        transformation,\n+        training=True,\n     ):\n         \"\"\"\n         bounding_boxes = {\n-            \"boxes\": (batch, num_boxes, 4),  # left-top-right-bottom\n+            \"boxes\": (batch, num_boxes, 4),  # left-top-right-bottom (xyxy)\n             \"labels\": (batch, num_boxes, num_classes),\n         }\n         or\n@@ -197,7 +203,16 @@ class RandomCrop(BaseImagePreprocessingLayer):\n                 bounding_boxes, backend=self.backend\n             )\n         boxes = bounding_boxes[\"boxes\"]\n-\n+        # Convert to a standard xyxy as operations are done xyxy by default.\n+        boxes = convert_format(\n+            boxes=boxes,\n+            source=self.bounding_box_format,\n+            target=\"xyxy\",\n+            height=self.height,\n+            width=self.width,\n+        )\n+        h_start = self.backend.cast(h_start, boxes.dtype)\n+        w_start = self.backend.cast(w_start, boxes.dtype)\n         if len(self.backend.shape(boxes)) == 3:\n             boxes = self.backend.numpy.stack(\n                 [\n@@ -218,6 +233,16 @@ class RandomCrop(BaseImagePreprocessingLayer):\n                 ],\n                 axis=-1,\n             )\n+\n+        # Convert to user defined bounding box format\n+        boxes = convert_format(\n+            boxes=boxes,\n+            source=\"xyxy\",\n+            target=self.bounding_box_format,\n+            height=self.height,\n+            width=self.width,\n+        )\n+\n         return {\n             \"boxes\": boxes,\n             \"labels\": bounding_boxes[\"labels\"],\n\n@@ -89,7 +89,10 @@ class RandomFlip(BaseImagePreprocessingLayer):\n         return labels\n \n     def transform_bounding_boxes(\n-        self, bounding_boxes, transformation, training=True\n+        self,\n+        bounding_boxes,\n+        transformation,\n+        training=True,\n     ):\n         raise NotImplementedError\n \n\n@@ -4,6 +4,9 @@ from keras.src.api_export import keras_export\n from keras.src.layers.preprocessing.image_preprocessing.base_image_preprocessing_layer import (  # noqa: E501\n     BaseImagePreprocessingLayer,\n )\n+from keras.src.layers.preprocessing.image_preprocessing.bounding_boxes.converters import (  # noqa: E501\n+    convert_format,\n+)\n from keras.src.random.seed_generator import SeedGenerator\n \n \n@@ -125,9 +128,19 @@ class RandomRotation(BaseImagePreprocessingLayer):\n         return labels\n \n     def transform_bounding_boxes(\n-        self, bounding_boxes, transformation, training=True\n+        self,\n+        bounding_boxes,\n+        transformation,\n+        training=True,\n     ):\n         boxes = bounding_boxes[\"boxes\"]\n+        boxes = convert_format(\n+            boxes=boxes,\n+            source=self.bounding_box_format,\n+            target=\"xyxy\",\n+            height=self.height,\n+            width=self.width,\n+        )\n         shape = self.backend.shape(boxes)\n         ones = self.backend.ones((shape[0], shape[1], 1, 1))\n         homogeneous_boxes = self.backend.concatenate([boxes, ones], axis=2)\n@@ -141,6 +154,13 @@ class RandomRotation(BaseImagePreprocessingLayer):\n         transformed_boxes = self.backend.reshape(\n             transformed_boxes, (shape[0], shape[1], 4)\n         )\n+        boxes = convert_format(\n+            boxes=boxes,\n+            source=\"xyxy\",\n+            target=self.bounding_box_format,\n+            height=self.height,\n+            width=self.width,\n+        )\n         return {\"boxes\": transformed_boxes, \"labels\": bounding_boxes[\"labels\"]}\n \n     def transform_segmentation_masks(\n\n@@ -167,7 +167,10 @@ class RandomTranslation(BaseImagePreprocessingLayer):\n         return labels\n \n     def transform_bounding_boxes(\n-        self, bounding_boxes, transformation, training=True\n+        self,\n+        bounding_boxes,\n+        transformation,\n+        training=True,\n     ):\n         raise NotImplementedError\n \n\n@@ -176,7 +176,10 @@ class RandomZoom(BaseImagePreprocessingLayer):\n         return labels\n \n     def transform_bounding_boxes(\n-        self, bounding_boxes, transformation, training=True\n+        self,\n+        bounding_boxes,\n+        transformation,\n+        training=True,\n     ):\n         raise NotImplementedError\n \n\n@@ -3,6 +3,12 @@ from keras.src.api_export import keras_export\n from keras.src.layers.preprocessing.image_preprocessing.base_image_preprocessing_layer import (  # noqa: E501\n     BaseImagePreprocessingLayer,\n )\n+from keras.src.layers.preprocessing.image_preprocessing.bounding_boxes.converters import (  # noqa: E501\n+    clip_to_image_size,\n+)\n+from keras.src.layers.preprocessing.image_preprocessing.bounding_boxes.converters import (  # noqa: E501\n+    convert_format,\n+)\n from keras.src.ops.core import _saturate_cast\n \n \n@@ -85,6 +91,12 @@ class Resizing(BaseImagePreprocessingLayer):\n         self.pad_to_aspect_ratio = pad_to_aspect_ratio\n         self.fill_mode = fill_mode\n         self.fill_value = fill_value\n+        if self.data_format == \"channels_first\":\n+            self.height_axis = -2\n+            self.width_axis = -1\n+        elif self.data_format == \"channels_last\":\n+            self.height_axis = -3\n+            self.width_axis = -2\n \n     def transform_images(self, images, transformation=None, training=True):\n         size = (self.height, self.width)\n@@ -112,10 +124,154 @@ class Resizing(BaseImagePreprocessingLayer):\n     def transform_labels(self, labels, transformation=None, training=True):\n         return labels\n \n+    def get_random_transformation(self, data, training=True, seed=None):\n+\n+        if isinstance(data, dict):\n+            input_shape = self.backend.shape(data[\"images\"])\n+        else:\n+            input_shape = self.backend.shape(data)\n+\n+        input_height, input_width = (\n+            input_shape[self.height_axis],\n+            input_shape[self.width_axis],\n+        )\n+\n+        return input_height, input_width\n+\n     def transform_bounding_boxes(\n-        self, bounding_boxes, transformation, training=True\n+        self,\n+        bounding_boxes,\n+        transformation,\n+        training=True,\n     ):\n-        raise NotImplementedError\n+        ops = self.backend\n+        input_height, input_width = transformation\n+        mask_negative_1s = ops.numpy.all(bounding_boxes[\"boxes\"] == -1, axis=-1)\n+        mask_zeros = ops.numpy.all(bounding_boxes[\"boxes\"] == 0, axis=-1)\n+        boxes_mask = ops.numpy.logical_or(mask_negative_1s, mask_zeros)\n+\n+        bounding_boxes = convert_format(\n+            bounding_boxes,\n+            source=self.bounding_box_format,\n+            target=\"xyxy\",\n+            height=input_height,\n+            width=input_width,\n+        )\n+\n+        bounding_boxes[\"boxes\"] = self._transform_xyxy(\n+            bounding_boxes[\"boxes\"],\n+            input_height=input_height,\n+            input_width=input_width,\n+        )\n+\n+        bounding_boxes = clip_to_image_size(\n+            bounding_boxes=bounding_boxes,\n+            height=self.height,\n+            width=self.width,\n+            format=\"xyxy\",\n+        )\n+\n+        bounding_boxes[\"boxes\"] = ops.numpy.where(\n+            ops.numpy.expand_dims(boxes_mask, axis=-1),\n+            ops.convert_to_tensor(\n+                [0.0, 0.0, 0.0, 0.0], dtype=bounding_boxes[\"boxes\"].dtype\n+            ),\n+            bounding_boxes[\"boxes\"],\n+        )\n+\n+        bounding_boxes = convert_format(\n+            bounding_boxes,\n+            source=\"xyxy\",\n+            target=self.bounding_box_format,\n+            height=self.height,\n+            width=self.width,\n+        )\n+\n+        return bounding_boxes\n+\n+    def _transform_xyxy(self, boxes, input_height, input_width):\n+        ops = self.backend\n+        input_height = ops.cast(input_height, dtype=boxes.dtype)\n+        input_width = ops.cast(input_width, dtype=boxes.dtype)\n+\n+        if self.pad_to_aspect_ratio:\n+            return self._transform_boxes_pad_to_aspect_ratio(\n+                boxes, input_height, input_width\n+            )\n+        elif self.crop_to_aspect_ratio:\n+            return self._transform_boxes_crop_to_aspect_ratio(\n+                boxes, input_height, input_width\n+            )\n+        else:\n+            return self._transform_boxes_stretch(\n+                boxes, input_height, input_width\n+            )\n+\n+    def _transform_boxes_pad_to_aspect_ratio(\n+        self, boxes, input_height, input_width\n+    ):\n+        \"\"\"Transforms bounding boxes for padding to aspect ratio.\"\"\"\n+        ops = self.backend\n+        height_ratio = ops.cast(self.height / input_height, dtype=boxes.dtype)\n+        width_ratio = ops.cast(self.width / input_width, dtype=boxes.dtype)\n+        min_aspect_ratio = ops.numpy.minimum(height_ratio, width_ratio)\n+        y_offset = (self.height - input_height * min_aspect_ratio) // 2\n+        x_offset = (self.width - input_width * min_aspect_ratio) // 2\n+        return ops.numpy.stack(\n+            [\n+                boxes[..., 0] * min_aspect_ratio + x_offset,\n+                boxes[..., 1] * min_aspect_ratio + y_offset,\n+                boxes[..., 2] * min_aspect_ratio + x_offset,\n+                boxes[..., 3] * min_aspect_ratio + y_offset,\n+            ],\n+            axis=-1,\n+        )\n+\n+    def _transform_boxes_crop_to_aspect_ratio(\n+        self, boxes, input_height, input_width\n+    ):\n+        \"\"\"Transforms bounding boxes for cropping to aspect ratio.\"\"\"\n+        ops = self.backend\n+        source_aspect_ratio = input_width / input_height\n+        target_aspect_ratio = self.width / self.height\n+        new_width = ops.numpy.where(\n+            source_aspect_ratio > target_aspect_ratio,\n+            self.height * source_aspect_ratio,\n+            self.width,\n+        )\n+        new_height = ops.numpy.where(\n+            source_aspect_ratio > target_aspect_ratio,\n+            self.height,\n+            self.width / source_aspect_ratio,\n+        )\n+        scale_x = new_width / input_width\n+        scale_y = new_height / input_height\n+        crop_left = (new_width - self.width) // 2\n+        crop_top = (new_height - self.height) // 2\n+        return ops.numpy.stack(\n+            [\n+                boxes[..., 0] * scale_x - crop_left,\n+                boxes[..., 1] * scale_y - crop_top,\n+                boxes[..., 2] * scale_x - crop_left,\n+                boxes[..., 3] * scale_y - crop_top,\n+            ],\n+            axis=-1,\n+        )\n+\n+    def _transform_boxes_stretch(self, boxes, input_height, input_width):\n+        \"\"\"Transforms bounding boxes by simple stretching.\"\"\"\n+        ops = self.backend\n+        height_ratio = ops.cast(self.height / input_height, dtype=boxes.dtype)\n+        width_ratio = ops.cast(self.width / input_width, dtype=boxes.dtype)\n+        return ops.numpy.stack(\n+            [\n+                boxes[..., 0] * width_ratio,\n+                boxes[..., 1] * height_ratio,\n+                boxes[..., 2] * width_ratio,\n+                boxes[..., 3] * height_ratio,\n+            ],\n+            axis=-1,\n+        )\n \n     def compute_output_shape(self, input_shape):\n         input_shape = list(input_shape)\n\n@@ -221,3 +221,106 @@ class ResizingTest(testing.TestCase):\n             size[0], size[1], data_format=data_format, crop_to_aspect_ratio=True\n         )(img)\n         self.assertEqual(output.shape, (1, *size, 4))\n+\n+    @parameterized.named_parameters(\n+        (\n+            \"with_pad_to_aspect_ratio\",\n+            True,\n+            False,\n+            [[6.0, 2.0, 10.0, 6.0], [14.0, 8.0, 18.0, 12.0]],\n+        ),\n+        (\n+            \"with_crop_to_aspect_ratio\",\n+            False,\n+            True,\n+            [[5.0, 0.5, 10.0, 5.5], [15.0, 8.0, 20.0, 13.0]],\n+        ),\n+        (\n+            \"boxes_stretch\",\n+            False,\n+            False,\n+            [[5.0, 2.0, 10.0, 6.0], [15.0, 8.0, 20.0, 12.0]],\n+        ),\n+    )\n+    def test_resize_bounding_boxes(\n+        self, pad_to_aspect_ratio, crop_to_aspect_ratio, expected_boxes\n+    ):\n+        if backend.config.image_data_format() == \"channels_last\":\n+            image_shape = (10, 8, 3)\n+        else:\n+            image_shape = (3, 10, 8)\n+        input_image = np.random.random(image_shape)\n+        bounding_boxes = {\n+            \"boxes\": np.array(\n+                [\n+                    [2, 1, 4, 3],\n+                    [6, 4, 8, 6],\n+                ]\n+            ),  # Example boxes (normalized)\n+            \"labels\": np.array([[1, 2]]),  # Dummy labels\n+        }\n+        input_data = {\"images\": input_image, \"bounding_boxes\": bounding_boxes}\n+        resizing_layer = layers.Resizing(\n+            height=20,\n+            width=20,\n+            pad_to_aspect_ratio=pad_to_aspect_ratio,\n+            crop_to_aspect_ratio=crop_to_aspect_ratio,\n+            bounding_box_format=\"xyxy\",\n+        )\n+        output = resizing_layer(input_data)\n+        self.assertAllClose(output[\"bounding_boxes\"][\"boxes\"], expected_boxes)\n+\n+    @parameterized.named_parameters(\n+        (\n+            \"with_pad_to_aspect_ratio\",\n+            True,\n+            False,\n+            [[6.0, 2.0, 10.0, 6.0], [14.0, 8.0, 18.0, 12.0]],\n+        ),\n+        (\n+            \"with_crop_to_aspect_ratio\",\n+            False,\n+            True,\n+            [[5.0, 0.5, 10.0, 5.5], [15.0, 8.0, 20.0, 13.0]],\n+        ),\n+        (\n+            \"boxes_stretch\",\n+            False,\n+            False,\n+            [[5.0, 2.0, 10.0, 6.0], [15.0, 8.0, 20.0, 12.0]],\n+        ),\n+    )\n+    def test_resize_tf_data_bounding_boxes(\n+        self, pad_to_aspect_ratio, crop_to_aspect_ratio, expected_boxes\n+    ):\n+        if backend.config.image_data_format() == \"channels_last\":\n+            image_shape = (1, 10, 8, 3)\n+        else:\n+            image_shape = (1, 3, 10, 8)\n+        input_image = np.random.random(image_shape)\n+        bounding_boxes = {\n+            \"boxes\": np.array(\n+                [\n+                    [\n+                        [2, 1, 4, 3],\n+                        [6, 4, 8, 6],\n+                    ]\n+                ]\n+            ),  # Example boxes (normalized)\n+            \"labels\": np.array([[1, 2]]),  # Dummy labels\n+        }\n+\n+        input_data = {\"images\": input_image, \"bounding_boxes\": bounding_boxes}\n+\n+        ds = tf_data.Dataset.from_tensor_slices(input_data)\n+        resizing_layer = layers.Resizing(\n+            height=20,\n+            width=20,\n+            pad_to_aspect_ratio=pad_to_aspect_ratio,\n+            crop_to_aspect_ratio=crop_to_aspect_ratio,\n+            bounding_box_format=\"xyxy\",\n+        )\n+        ds = ds.map(resizing_layer)\n+        output = next(iter(ds))\n+        expected_boxes = np.array(expected_boxes)\n+        self.assertAllClose(output[\"bounding_boxes\"][\"boxes\"], expected_boxes)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
