{"custom_id": "keras#3dcd9c767ce6875fc8b69c74971ac8a552e23131", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 11095 | Contributors (this commit): 61 | Commits (past 90d): 8 | Contributors (cumulative): 61 | DMM Complexity: 0.0\n\nDIFF:\n@@ -349,7 +349,6 @@ class Conv1D(_Conv):\n             kernel_constraint=kernel_constraint,\n             bias_constraint=bias_constraint,\n             **kwargs)\n-        self.input_spec = InputSpec(ndim=3)\n \n     def get_config(self):\n         config = super(Conv1D, self).get_config()\n@@ -481,7 +480,6 @@ class Conv2D(_Conv):\n             kernel_constraint=kernel_constraint,\n             bias_constraint=bias_constraint,\n             **kwargs)\n-        self.input_spec = InputSpec(ndim=4)\n \n     def get_config(self):\n         config = super(Conv2D, self).get_config()\n@@ -610,7 +608,6 @@ class Conv3D(_Conv):\n             kernel_constraint=kernel_constraint,\n             bias_constraint=bias_constraint,\n             **kwargs)\n-        self.input_spec = InputSpec(ndim=5)\n \n     def get_config(self):\n         config = super(Conv3D, self).get_config()\n@@ -756,7 +753,6 @@ class Conv2DTranspose(Conv2D):\n             kernel_constraint=kernel_constraint,\n             bias_constraint=bias_constraint,\n             **kwargs)\n-        self.input_spec = InputSpec(ndim=4)\n \n         self.output_padding = output_padding\n         if self.output_padding is not None:\n@@ -1020,7 +1016,7 @@ class Conv3DTranspose(Conv3D):\n             kernel_constraint=kernel_constraint,\n             bias_constraint=bias_constraint,\n             **kwargs)\n-        self.input_spec = InputSpec(ndim=5)\n+\n         self.output_padding = output_padding\n         if self.output_padding is not None:\n             self.output_padding = conv_utils.normalize_tuple(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#69c30a150f0b2caee7961ca1c0080960ef5ad6f6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 40 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 42 | Churn Cumulative: 4039 | Contributors (this commit): 13 | Commits (past 90d): 9 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -149,8 +149,6 @@ class Sequential(Model):\n                     first_layer = layer.layers[0]\n                     while isinstance(first_layer, (Model, Sequential)):\n                         first_layer = first_layer.layers[0]\n-                    batch_shape = first_layer.batch_input_shape\n-                    dtype = first_layer.dtype\n \n                 if hasattr(first_layer, 'batch_input_shape'):\n                     batch_shape = first_layer.batch_input_shape\n\n@@ -422,5 +422,45 @@ def test_sequential_deferred_build():\n     assert len(new_model.weights) == 4\n \n \n+@keras_test\n+def test_nested_sequential_deferred_build():\n+    inner_model = keras.models.Sequential()\n+    inner_model.add(keras.layers.Dense(3))\n+    inner_model.add(keras.layers.Dense(3))\n+\n+    model = keras.models.Sequential()\n+    model.add(inner_model)\n+    model.add(keras.layers.Dense(5))\n+    model.compile('sgd', 'mse')\n+\n+    assert inner_model.built is False\n+    assert len(inner_model.layers) == 2\n+    assert len(inner_model.weights) == 0\n+    assert model.built is False\n+    assert len(model.layers) == 2\n+    assert len(model.weights) == 0\n+\n+    model.train_on_batch(\n+        np.random.random((2, 4)), np.random.random((2, 5)))\n+\n+    assert inner_model.built is True\n+    assert len(inner_model.layers) == 2\n+    assert len(inner_model.weights) == 4\n+    assert model.built is True\n+    assert len(model.layers) == 2\n+    assert len(model.weights) == 6\n+\n+    config = model.get_config()\n+    new_model = keras.models.Sequential.from_config(config)\n+    assert new_model.built is True\n+    assert len(new_model.layers) == 2\n+    assert len(new_model.weights) == 6\n+\n+    new_inner_model = new_model.layers[0]\n+    assert new_inner_model.built is True\n+    assert len(new_inner_model.layers) == 2\n+    assert len(new_inner_model.weights) == 4\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8fe86302ff1cce23c019fdaf24fc6448c3b30090", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 5 | Churn Cumulative: 1741 | Contributors (this commit): 27 | Commits (past 90d): 7 | Contributors (cumulative): 27 | DMM Complexity: 1.0\n\nDIFF:\n@@ -589,7 +589,10 @@ def collect_class_methods(cls, methods):\n def render_function(function, method=True):\n     subblocks = []\n     signature = get_function_signature(function, method=method)\n-    signature = signature.replace(function.__module__ + '.', '')\n+    if method:\n+        signature = signature.replace(clean_module_name(function.__module__) + '.', '')\n+    else:\n+        signature = signature.replace(clean_module_name(function.__module__) + '.', '', 1)\n     subblocks.append('### ' + function.__name__ + '\\n')\n     subblocks.append(code_snippet(signature))\n     docstring = function.__doc__\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5103fd8134ccf2ef6ed114d525d8b07d148716d3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 44 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 74 | Churn Cumulative: 467 | Contributors (this commit): 10 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -81,6 +81,32 @@ def layer_test(layer_cls, kwargs={}, input_shape=None, input_dtype=None,\n         kwargs['weights'] = weights\n         layer = layer_cls(**kwargs)\n \n+    expected_output_shape = layer.compute_output_shape(input_shape)\n+\n+    def _layer_in_model_test(model):\n+        actual_output = model.predict(input_data)\n+        actual_output_shape = actual_output.shape\n+        for expected_dim, actual_dim in zip(expected_output_shape,\n+                                            actual_output_shape):\n+            if expected_dim is not None:\n+                assert expected_dim == actual_dim\n+        if expected_output is not None:\n+            assert_allclose(actual_output, expected_output, rtol=1e-3)\n+\n+        # test serialization, weight setting at model level\n+        model_config = model.get_config()\n+        recovered_model = model.__class__.from_config(model_config)\n+        if model.weights:\n+            weights = model.get_weights()\n+            recovered_model.set_weights(weights)\n+            _output = recovered_model.predict(input_data)\n+            assert_allclose(_output, actual_output, rtol=1e-3)\n+\n+        # test training mode (e.g. useful for dropout tests)\n+        model.compile('rmsprop', 'mse')\n+        model.train_on_batch(input_data, actual_output)\n+        return actual_output\n+\n     # test in functional API\n     if fixed_batch_size:\n         x = Input(batch_shape=input_shape, dtype=input_dtype)\n@@ -89,59 +115,19 @@ def layer_test(layer_cls, kwargs={}, input_shape=None, input_dtype=None,\n     y = layer(x)\n     assert K.dtype(y) == expected_output_dtype\n \n-    # check shape inference\n+    # check with the functional API\n     model = Model(x, y)\n-    expected_output_shape = layer.compute_output_shape(input_shape)\n-    actual_output = model.predict(input_data)\n-    actual_output_shape = actual_output.shape\n-    for expected_dim, actual_dim in zip(expected_output_shape,\n-                                        actual_output_shape):\n-        if expected_dim is not None:\n-            assert expected_dim == actual_dim\n-    if expected_output is not None:\n-        assert_allclose(actual_output, expected_output, rtol=1e-3)\n-\n-    # test serialization, weight setting at model level\n-    model_config = model.get_config()\n-    recovered_model = Model.from_config(model_config)\n-    if model.weights:\n-        weights = model.get_weights()\n-        recovered_model.set_weights(weights)\n-        _output = recovered_model.predict(input_data)\n-        assert_allclose(_output, actual_output, rtol=1e-3)\n-\n-    # test training mode (e.g. useful for dropout tests)\n-    model.compile('rmsprop', 'mse')\n-    model.train_on_batch(input_data, actual_output)\n+    _layer_in_model_test(model)\n \n     # test as first layer in Sequential API\n     layer_config = layer.get_config()\n     layer_config['batch_input_shape'] = input_shape\n     layer = layer.__class__.from_config(layer_config)\n \n+    # check with the sequential API\n     model = Sequential()\n     model.add(layer)\n-    actual_output = model.predict(input_data)\n-    actual_output_shape = actual_output.shape\n-    for expected_dim, actual_dim in zip(expected_output_shape,\n-                                        actual_output_shape):\n-        if expected_dim is not None:\n-            assert expected_dim == actual_dim\n-    if expected_output is not None:\n-        assert_allclose(actual_output, expected_output, rtol=1e-3)\n-\n-    # test serialization, weight setting at model level\n-    model_config = model.get_config()\n-    recovered_model = Sequential.from_config(model_config)\n-    if model.weights:\n-        weights = model.get_weights()\n-        recovered_model.set_weights(weights)\n-        _output = recovered_model.predict(input_data)\n-        assert_allclose(_output, actual_output, rtol=1e-3)\n-\n-    # test training mode (e.g. useful for dropout tests)\n-    model.compile('rmsprop', 'mse')\n-    model.train_on_batch(input_data, actual_output)\n+    actual_output = _layer_in_model_test(model)\n \n     # for further checks in the caller function\n     return actual_output\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f40f71be3d31990e5aac870fdfb7782595a3db29", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 11096 | Contributors (this commit): 62 | Commits (past 90d): 9 | Contributors (cumulative): 62 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1285,6 +1285,7 @@ class _SeparableConv(_Conv):\n             dilation_rate=dilation_rate,\n             activation=activation,\n             use_bias=use_bias,\n+            bias_initializer=bias_initializer,\n             bias_regularizer=bias_regularizer,\n             activity_regularizer=activity_regularizer,\n             bias_constraint=bias_constraint,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1fcc791ee1c94c2a3efa5072aa2f10f1aab51a2a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 12373 | Contributors (this commit): 79 | Commits (past 90d): 8 | Contributors (cumulative): 79 | DMM Complexity: None\n\nDIFF:\n@@ -485,9 +485,8 @@ class Flatten(Layer):\n \n     ```python\n         model = Sequential()\n-        model.add(Conv2D(64, 3, 3,\n-                         padding='same',\n-                         input_shape=(3, 32, 32)))\n+        model.add(Conv2D(64, (3, 3),\n+                         input_shape=(3, 32, 32), padding='same',))\n         # now: model.output_shape == (None, 64, 32, 32)\n \n         model.add(Flatten())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f86bc57d621fb0325e928271b246d197611b0e65", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 1758 | Contributors (this commit): 28 | Commits (past 90d): 8 | Contributors (cumulative): 28 | DMM Complexity: 0.8333333333333334\n\nDIFF:\n@@ -383,7 +383,10 @@ def get_class_signature(cls):\n     except (TypeError, AttributeError):\n         # in case the class inherits from object and does not\n         # define __init__\n-        class_signature = clean_module_name(cls.__module__) + '.' + cls.__name__ + '()'\n+        class_signature = \"{clean_module_name}.{cls_name}()\".format(\n+            clean_module_name=clean_module_name(cls.__module__),\n+            cls_name=cls.__name__\n+        )\n     return post_process_signature(class_signature)\n \n \n@@ -442,7 +445,8 @@ def count_leading_spaces(s):\n \n def process_list_block(docstring, starting_point, leading_spaces, marker):\n     ending_point = docstring.find('\\n\\n', starting_point)\n-    block = docstring[starting_point:None if ending_point == -1 else ending_point - 1]\n+    block = docstring[starting_point:(None if ending_point == -1 else\n+                                      ending_point - 1)]\n     # Place marker for later reinjection.\n     docstring = docstring.replace(block, marker)\n     lines = block.split('\\n')\n@@ -590,9 +594,11 @@ def render_function(function, method=True):\n     subblocks = []\n     signature = get_function_signature(function, method=method)\n     if method:\n-        signature = signature.replace(clean_module_name(function.__module__) + '.', '')\n+        signature = signature.replace(\n+            clean_module_name(function.__module__) + '.', '')\n     else:\n-        signature = signature.replace(clean_module_name(function.__module__) + '.', '', 1)\n+        signature = signature.replace(\n+            clean_module_name(function.__module__) + '.', '', 1)\n     subblocks.append('### ' + function.__name__ + '\\n')\n     subblocks.append(code_snippet(signature))\n     docstring = function.__doc__\n@@ -675,7 +681,8 @@ if __name__ == '__main__':\n         if os.path.exists(path):\n             template = read_file(path)\n             assert '{{autogenerated}}' in template, ('Template found for ' + path +\n-                                                     ' but missing {{autogenerated}} tag.')\n+                                                     ' but missing {{autogenerated}}'\n+                                                     ' tag.')\n             mkdown = template.replace('{{autogenerated}}', mkdown)\n             print('...inserting autogenerated content into template:', path)\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#066aa6ab7e5aaea495f2f0e1783737ba4567bd66", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 10 | Churn Cumulative: 9777 | Contributors (this commit): 117 | Commits (past 90d): 18 | Contributors (cumulative): 117 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1879,10 +1879,20 @@ def batch_normalization(x, mean, var, beta, gamma, axis=-1, epsilon=1e-3):\n             tf_data_format = None\n \n         if tf_data_format == 'NHWC' or tf_data_format == 'NCHW' and _has_nchw_support():\n+            # The mean / var / beta / gamma may be processed by broadcast\n+            # so it may have extra axes with 1, it is not needed and should be removed\n+            if ndim(mean) > 1:\n+                mean = tf.squeeze(mean)\n+            if ndim(var) > 1:\n+                var = tf.squeeze(var)\n             if beta is None:\n                 beta = zeros_like(mean)\n+            elif ndim(beta) > 1:\n+                beta = tf.squeeze(beta)\n             if gamma is None:\n                 gamma = ones_like(mean)\n+            elif ndim(gamma) > 1:\n+                gamma = tf.squeeze(gamma)\n             y, _, _ = tf.nn.fused_batch_norm(\n                 x,\n                 gamma,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d18c564548104c862892a1f73423e333f11f7ce2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 92 | Files Changed: 7 | Hunks: 35 | Methods Changed: 21 | Complexity Δ (Sum/Max): -24/2 | Churn Δ: 145 | Churn Cumulative: 14423 | Contributors (this commit): 115 | Commits (past 90d): 34 | Contributors (cumulative): 138 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,6 +14,7 @@ from ..utils.layer_utils import count_params\n from ..utils.generic_utils import has_arg\n from ..utils.generic_utils import object_list_uid\n from ..utils.generic_utils import to_list\n+from ..utils.generic_utils import unpack_singleton\n from ..utils.generic_utils import is_all_none\n from ..legacy import interfaces\n \n@@ -427,10 +428,7 @@ class Layer(object):\n                                          'and thus cannot be built. '\n                                          'You can build it manually via: '\n                                          '`layer.build(batch_input_shape)`')\n-                if len(input_shapes) == 1:\n-                    self.build(input_shapes[0])\n-                else:\n-                    self.build(input_shapes)\n+                self.build(unpack_singleton(input_shapes))\n                 self.built = True\n \n                 # Load weights that were specified at layer instantiation.\n@@ -468,10 +466,7 @@ class Layer(object):\n                 if x in inputs_ls:\n                     x = K.identity(x)\n                 output_ls_copy.append(x)\n-            if len(output_ls_copy) == 1:\n-                output = output_ls_copy[0]\n-            else:\n-                output = output_ls_copy\n+            output = unpack_singleton(output_ls_copy)\n \n             # Inferring the output shape is only relevant for Theano.\n             if all([s is not None\n@@ -668,10 +663,7 @@ class Layer(object):\n                              ', but the layer has only ' +\n                              str(len(self._inbound_nodes)) + ' inbound nodes.')\n         values = getattr(self._inbound_nodes[node_index], attr)\n-        if len(values) == 1:\n-            return values[0]\n-        else:\n-            return values\n+        return unpack_singleton(values)\n \n     def get_input_shape_at(self, node_index):\n         \"\"\"Retrieves the input shape(s) of a layer at a given node.\n@@ -897,10 +889,7 @@ class Layer(object):\n             [str(node.input_shapes) for node in self._inbound_nodes])\n         if len(all_input_shapes) == 1:\n             input_shapes = self._inbound_nodes[0].input_shapes\n-            if len(input_shapes) == 1:\n-                return input_shapes[0]\n-            else:\n-                return input_shapes\n+            return unpack_singleton(input_shapes)\n         else:\n             raise AttributeError('The layer \"' + str(self.name) +\n                                  ' has multiple inbound nodes, '\n@@ -932,10 +921,7 @@ class Layer(object):\n             [str(node.output_shapes) for node in self._inbound_nodes])\n         if len(all_output_shapes) == 1:\n             output_shapes = self._inbound_nodes[0].output_shapes\n-            if len(output_shapes) == 1:\n-                return output_shapes[0]\n-            else:\n-                return output_shapes\n+            return unpack_singleton(output_shapes)\n         else:\n             raise AttributeError('The layer \"' + str(self.name) +\n                                  ' has multiple inbound nodes, '\n@@ -1326,9 +1312,7 @@ def _collect_previous_mask(input_tensors):\n             masks.append(mask)\n         else:\n             masks.append(None)\n-    if len(masks) == 1:\n-        return masks[0]\n-    return masks\n+    return unpack_singleton(masks)\n \n \n def _to_snake_case(name):\n@@ -1357,6 +1341,4 @@ def _collect_input_shape(input_tensors):\n             shapes.append(K.int_shape(x))\n         except TypeError:\n             shapes.append(None)\n-    if len(shapes) == 1:\n-        return shapes[0]\n-    return shapes\n+    return unpack_singleton(shapes)\n\n@@ -8,6 +8,7 @@ from .base_layer import Layer\n from .base_layer import Node\n from .. import backend as K\n from ..legacy import interfaces\n+from ..utils.generic_utils import unpack_singleton\n \n \n class InputLayer(Layer):\n@@ -177,7 +178,4 @@ def Input(shape=None, batch_shape=None,\n     # Return tensor including _keras_shape and _keras_history.\n     # Note that in this case train_output and test_output are the same pointer.\n     outputs = input_layer._inbound_nodes[0].output_tensors\n-    if len(outputs) == 1:\n-        return outputs[0]\n-    else:\n-        return outputs\n+    return unpack_singleton(outputs)\n\n@@ -23,6 +23,7 @@ from ..utils.layer_utils import get_source_inputs\n from ..utils.generic_utils import has_arg\n from ..utils.generic_utils import to_list\n from ..utils.generic_utils import object_list_uid\n+from ..utils.generic_utils import unpack_singleton\n from ..legacy import interfaces\n \n try:\n@@ -267,10 +268,7 @@ class Network(Layer):\n             node = layer._inbound_nodes[node_index]\n             mask = node.output_masks[tensor_index]\n             masks.append(mask)\n-        if len(masks) == 1:\n-            mask = masks[0]\n-        else:\n-            mask = masks\n+        mask = unpack_singleton(masks)\n         self._output_mask_cache[mask_cache_key] = mask\n \n         # Build self.input_names and self.output_names.\n@@ -539,9 +537,7 @@ class Network(Layer):\n                                     'Found input_spec = ' +\n                                     str(layer.input_spec))\n                 specs += layer.input_spec\n-        if len(specs) == 1:\n-            return specs[0]\n-        return specs\n+        return unpack_singleton(specs)\n \n     def call(self, inputs, mask=None):\n         \"\"\"Calls the model on new inputs.\n@@ -605,8 +601,8 @@ class Network(Layer):\n         cache_key = ', '.join([str(x) for x in input_shapes])\n         if cache_key in self._output_shape_cache:\n             output_shapes = self._output_shape_cache[cache_key]\n-            if isinstance(output_shapes, list) and len(output_shapes) == 1:\n-                return output_shapes[0]\n+            if isinstance(output_shapes, list):\n+                return unpack_singleton(output_shapes)\n             return output_shapes\n         else:\n             # Bad luck, we have to run the graph manually.\n@@ -643,10 +639,7 @@ class Network(Layer):\n                             input_shape = layers_to_output_shapes[shape_key]\n                             input_shapes.append(input_shape)\n \n-                        if len(input_shapes) == 1:\n-                            output_shape = layer.compute_output_shape(input_shapes[0])\n-                        else:\n-                            output_shape = layer.compute_output_shape(input_shapes)\n+                        output_shape = layer.compute_output_shape(unpack_singleton(input_shapes))\n \n                         output_shapes = to_list(output_shape)\n                         node_index = layer._inbound_nodes.index(node)\n@@ -669,8 +662,8 @@ class Network(Layer):\n                 output_shapes.append(layers_to_output_shapes[key])\n             # Store in cache.\n             self._output_shape_cache[cache_key] = output_shapes\n-            if isinstance(output_shapes, list) and len(output_shapes) == 1:\n-                return output_shapes[0]\n+            if isinstance(output_shapes, list):\n+                return unpack_singleton(output_shapes)\n             return output_shapes\n \n     def run_internal_graph(self, inputs, masks=None):\n@@ -781,12 +774,10 @@ class Network(Layer):\n \n                     # Update _keras_shape.\n                     if all([hasattr(x, '_keras_shape') for x in computed_tensors]):\n-                        if len(computed_tensors) == 1:\n-                            shapes = to_list(layer.compute_output_shape(computed_tensors[0]._keras_shape))\n-                            uses_learning_phase = computed_tensors[0]._uses_learning_phase\n-                        else:\n-                            shapes = to_list(layer.compute_output_shape([x._keras_shape for x in computed_tensors]))\n+                        input_shapes = unpack_singleton([x._keras_shape for x in computed_tensors])\n+                        shapes = to_list(layer.compute_output_shape(input_shapes))\n                         uses_learning_phase = any([x._uses_learning_phase for x in computed_tensors])\n+\n                         for x, s in zip(output_tensors, shapes):\n                             x._keras_shape = s\n                             x._uses_learning_phase = getattr(x, '_uses_learning_phase', False) or uses_learning_phase\n@@ -814,25 +805,17 @@ class Network(Layer):\n         cache_key = object_list_uid(inputs)\n         cache_key += '_' + object_list_uid(masks)\n \n-        if len(output_tensors) == 1:\n-            output_tensors = output_tensors[0]\n-            self._output_tensor_cache[cache_key] = output_tensors\n-        else:\n+        output_tensors = unpack_singleton(output_tensors)\n         self._output_tensor_cache[cache_key] = output_tensors\n \n-        if len(output_masks) == 1:\n-            output_masks = output_masks[0]\n-            self._output_mask_cache[cache_key] = output_masks\n-        else:\n+        output_masks = unpack_singleton(output_masks)\n         self._output_mask_cache[cache_key] = output_masks\n \n         if output_shapes is not None:\n             input_shapes = [x._keras_shape for x in inputs]\n             cache_key = ', '.join([str(x) for x in input_shapes])\n-            if len(output_shapes) == 1:\n-                output_shapes = output_shapes[0]\n-                self._output_shape_cache[cache_key] = output_shapes\n-            else:\n+\n+            output_shapes = unpack_singleton(output_shapes)\n             self._output_shape_cache[cache_key] = output_shapes\n         return output_tensors, output_masks, output_shapes\n \n@@ -1000,10 +983,7 @@ class Network(Layer):\n             # Call layer on its inputs, thus creating the node\n             # and building the layer if needed.\n             if input_tensors:\n-                if len(input_tensors) == 1:\n-                    layer(input_tensors[0], **kwargs)\n-                else:\n-                    layer(input_tensors, **kwargs)\n+                layer(unpack_singleton(input_tensors), **kwargs)\n \n         def process_layer(layer_data):\n             \"\"\"Deserializes a layer, then call it on appropriate inputs.\n\n@@ -25,6 +25,7 @@ from .. import optimizers\n from .. import losses\n from .. import metrics as metrics_module\n from ..utils.generic_utils import slice_arrays\n+from ..utils.generic_utils import unpack_singleton\n from ..legacy import interfaces\n \n \n@@ -620,16 +621,10 @@ class Model(Network):\n \n         if outputs is None:\n             # Obtain symbolic outputs by calling the model.\n-            if len(self.inputs) == 1:\n             if self._expects_training_arg:\n-                    outputs = self.call(self.inputs[0], training=training)\n+                outputs = self.call(unpack_singleton(self.inputs), training=training)\n             else:\n-                    outputs = self.call(self.inputs[0])\n-            else:\n-                if self._expects_training_arg:\n-                    outputs = self.call(self.inputs, training=training)\n-                else:\n-                    outputs = self.call(self.inputs)\n+                outputs = self.call(unpack_singleton(self.inputs))\n         if isinstance(outputs, (list, tuple)):\n             outputs = list(outputs)\n         else:\n@@ -1218,9 +1213,7 @@ class Model(Network):\n             ins = x + y + sample_weights\n         self._make_train_function()\n         outputs = self.train_function(ins)\n-        if len(outputs) == 1:\n-            return outputs[0]\n-        return outputs\n+        return unpack_singleton(outputs)\n \n     def test_on_batch(self, x, y, sample_weight=None):\n         \"\"\"Test the model on a single batch of samples.\n@@ -1259,9 +1252,7 @@ class Model(Network):\n             ins = x + y + sample_weights\n         self._make_test_function()\n         outputs = self.test_function(ins)\n-        if len(outputs) == 1:\n-            return outputs[0]\n-        return outputs\n+        return unpack_singleton(outputs)\n \n     def predict_on_batch(self, x):\n         \"\"\"Returns predictions for a single batch of samples.\n@@ -1279,9 +1270,7 @@ class Model(Network):\n             ins = x\n         self._make_predict_function()\n         outputs = self.predict_function(ins)\n-        if len(outputs) == 1:\n-            return outputs[0]\n-        return outputs\n+        return unpack_singleton(outputs)\n \n     @interfaces.legacy_generator_methods_support\n     def fit_generator(self, generator,\n\n@@ -14,6 +14,7 @@ from .. import backend as K\n from .. import callbacks as cbks\n from ..utils.generic_utils import Progbar\n from ..utils.generic_utils import slice_arrays\n+from ..utils.generic_utils import unpack_singleton\n \n \n def fit_loop(model, f, ins,\n@@ -306,9 +307,7 @@ def predict_loop(model, f, ins, batch_size=32, verbose=0, steps=None):\n                 outs[i][batch_start:batch_end] = batch_out\n             if verbose == 1:\n                 progbar.update(batch_end)\n-        if len(outs) == 1:\n-            return outs[0]\n-        return outs\n+        return unpack_singleton(outs)\n \n \n def test_loop(model, f, ins, batch_size=None, verbose=0, steps=None):\n@@ -415,6 +414,4 @@ def test_loop(model, f, ins, batch_size=None, verbose=0, steps=None):\n         for i in range(len(outs)):\n             if i not in stateful_metric_indices:\n                 outs[i] /= num_samples\n-    if len(outs) == 1:\n-        return outs[0]\n-    return outs\n+    return unpack_singleton(outs)\n\n@@ -12,6 +12,7 @@ from ..utils.data_utils import Sequence\n from ..utils.data_utils import GeneratorEnqueuer\n from ..utils.data_utils import OrderedEnqueuer\n from ..utils.generic_utils import Progbar\n+from ..utils.generic_utils import unpack_singleton\n from .. import callbacks as cbks\n \n \n@@ -375,9 +376,7 @@ def evaluate_generator(model, generator,\n                                        weights=batch_sizes))\n         else:\n             averages.append(float(outs_per_batch[-1][i]))\n-    if len(averages) == 1:\n-        return averages[0]\n-    return averages\n+    return unpack_singleton(averages)\n \n \n def predict_generator(model, generator,\n\n@@ -461,6 +461,22 @@ def to_list(x):\n     return [x]\n \n \n+def unpack_singleton(x):\n+    \"\"\"Gets the first element if the iterable has only one value.\n+\n+    Otherwise return the iterable.\n+\n+    # Argument:\n+        x: A list or tuple.\n+\n+    # Returns:\n+        The same iterable or the first element.\n+    \"\"\"\n+    if len(x) == 1:\n+        return x[0]\n+    return x\n+\n+\n def object_list_uid(object_list):\n     object_list = to_list(object_list)\n     return ', '.join([str(abs(id(x))) for x in object_list])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d2803c0fb7d0ba9361dcba8eb9bcebbf2f774958", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 47 | Lines Deleted: 23 | Files Changed: 5 | Hunks: 16 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 70 | Churn Cumulative: 2507 | Contributors (this commit): 32 | Commits (past 90d): 8 | Contributors (cumulative): 50 | DMM Complexity: 1.0\n\nDIFF:\n@@ -24,7 +24,7 @@ Four digits reversed:\n \n Five digits reversed:\n + One layer LSTM (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs\n-'''\n+'''  # noqa\n \n from __future__ import print_function\n from keras.models import Sequential\n\n@@ -17,7 +17,8 @@ from __future__ import print_function\n \n from keras.models import Sequential, Model\n from keras.layers.embeddings import Embedding\n-from keras.layers import Input, Activation, Dense, Permute, Dropout, add, dot, concatenate\n+from keras.layers import Input, Activation, Dense, Permute, Dropout\n+from keras.layers import add, dot, concatenate\n from keras.layers import LSTM\n from keras.utils.data_utils import get_file\n from keras.preprocessing.sequence import pad_sequences\n@@ -79,7 +80,8 @@ def get_stories(f, only_supporting=False, max_length=None):\n     '''\n     data = parse_stories(f.readlines(), only_supporting=only_supporting)\n     flatten = lambda data: reduce(lambda x, y: x + y, data)\n-    data = [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) < max_length]\n+    data = [(flatten(story), q, answer) for story, q, answer in data\n+            if not max_length or len(flatten(story)) < max_length]\n     return data\n \n \n@@ -94,19 +96,24 @@ def vectorize_stories(data):\n             np.array(answers))\n \n try:\n-    path = get_file('babi-tasks-v1-2.tar.gz', origin='https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz')\n+    path = get_file('babi-tasks-v1-2.tar.gz',\n+                    origin='https://s3.amazonaws.com/text-datasets/'\n+                           'babi_tasks_1-20_v1-2.tar.gz')\n except:\n     print('Error downloading dataset, please download it manually:\\n'\n-          '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz\\n'\n+          '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2'\n+          '.tar.gz\\n'\n           '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz')\n     raise\n \n \n challenges = {\n     # QA1 with 10,000 samples\n-    'single_supporting_fact_10k': 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt',\n+    'single_supporting_fact_10k': 'tasks_1-20_v1-2/en-10k/qa1_'\n+                                  'single-supporting-fact_{}.txt',\n     # QA2 with 10,000 samples\n-    'two_supporting_facts_10k': 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt',\n+    'two_supporting_facts_10k': 'tasks_1-20_v1-2/en-10k/qa2_'\n+                                'two-supporting-facts_{}.txt',\n }\n challenge_type = 'single_supporting_fact_10k'\n challenge = challenges[challenge_type]\n\n@@ -123,7 +123,8 @@ def get_stories(f, only_supporting=False, max_length=None):\n     '''\n     data = parse_stories(f.readlines(), only_supporting=only_supporting)\n     flatten = lambda data: reduce(lambda x, y: x + y, data)\n-    data = [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) < max_length]\n+    data = [(flatten(story), q, answer) for story, q, answer in data\n+            if not max_length or len(flatten(story)) < max_length]\n     return data\n \n \n@@ -140,7 +141,8 @@ def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n         xs.append(x)\n         xqs.append(xq)\n         ys.append(y)\n-    return pad_sequences(xs, maxlen=story_maxlen), pad_sequences(xqs, maxlen=query_maxlen), np.array(ys)\n+    return (pad_sequences(xs, maxlen=story_maxlen),\n+            pad_sequences(xqs, maxlen=query_maxlen), np.array(ys))\n \n RNN = recurrent.LSTM\n EMBED_HIDDEN_SIZE = 50\n@@ -154,10 +156,13 @@ print('RNN / Embed / Sent / Query = {}, {}, {}, {}'.format(RNN,\n                                                            QUERY_HIDDEN_SIZE))\n \n try:\n-    path = get_file('babi-tasks-v1-2.tar.gz', origin='https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz')\n+    path = get_file('babi-tasks-v1-2.tar.gz',\n+                    origin='https://s3.amazonaws.com/text-datasets/'\n+                           'babi_tasks_1-20_v1-2.tar.gz')\n except:\n     print('Error downloading dataset, please download it manually:\\n'\n-          '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz\\n'\n+          '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2'\n+          '.tar.gz\\n'\n           '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz')\n     raise\n \n\n@@ -85,19 +85,26 @@ else:\n         zca_whitening=False,  # apply ZCA whitening\n         zca_epsilon=1e-06,  # epsilon for ZCA whitening\n         rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n-        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n-        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n+        # randomly shift images horizontally (fraction of total width)\n+        width_shift_range=0.1,\n+        # randomly shift images vertically (fraction of total height)\n+        height_shift_range=0.1,\n         shear_range=0.,  # set range for random shear\n         zoom_range=0.,  # set range for random zoom\n         channel_shift_range=0.,  # set range for random channel shifts\n-        fill_mode='nearest',  # set mode for filling points outside the input boundaries\n+        # set mode for filling points outside the input boundaries\n+        fill_mode='nearest',\n         cval=0.,  # value used for fill_mode = \"constant\"\n         horizontal_flip=True,  # randomly flip images\n         vertical_flip=False,  # randomly flip images\n-        rescale=None,  # set rescaling factor (applied before any other transformation)\n-        preprocessing_function=None,  # set function that will be applied on each input\n-        data_format=None,  # image data format, either \"channels_first\" or \"channels_last\"\n-        validation_split=0.0)  # fraction of images reserved for validation (strictly between 0 and 1)\n+        # set rescaling factor (applied before any other transformation)\n+        rescale=None,\n+        # set function that will be applied on each input\n+        preprocessing_function=None,\n+        # image data format, either \"channels_first\" or \"channels_last\"\n+        data_format=None,\n+        # fraction of images reserved for validation (strictly between 0 and 1)\n+        validation_split=0.0)\n \n     # Compute quantities required for feature-wise normalization\n     # (std, mean, and principal components if ZCA whitening is applied).\n\n@@ -211,14 +211,19 @@ else:\n         shear_range=0.,  # set range for random shear\n         zoom_range=0.,  # set range for random zoom\n         channel_shift_range=0.,  # set range for random channel shifts\n-        fill_mode='nearest',  # set mode for filling points outside the input boundaries\n+        # set mode for filling points outside the input boundaries\n+        fill_mode='nearest',\n         cval=0.,  # value used for fill_mode = \"constant\"\n         horizontal_flip=True,  # randomly flip images\n         vertical_flip=False,  # randomly flip images\n-        rescale=None,  # set rescaling factor (applied before any other transformation)\n-        preprocessing_function=None,  # set function that will be applied on each input\n-        data_format=None,  # image data format, either \"channels_first\" or \"channels_last\"\n-        validation_split=0.0)  # fraction of images reserved for validation (strictly between 0 and 1)\n+        # set rescaling factor (applied before any other transformation)\n+        rescale=None,\n+        # set function that will be applied on each input\n+        preprocessing_function=None,\n+        # image data format, either \"channels_first\" or \"channels_last\"\n+        data_format=None,\n+        # fraction of images reserved for validation (strictly between 0 and 1)\n+        validation_split=0.0)\n \n     # Compute quantities required for feature-wise normalization\n     # (std, mean, and principal components if ZCA whitening is applied).\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d40656e5799dfd22e96b8bab217638b2934a6894", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 108 | Lines Deleted: 224 | Files Changed: 14 | Hunks: 72 | Methods Changed: 32 | Complexity Δ (Sum/Max): -46/3 | Churn Δ: 332 | Churn Cumulative: 52678 | Contributors (this commit): 256 | Commits (past 90d): 95 | Contributors (cumulative): 487 | DMM Complexity: 1.0\n\nDIFF:\n@@ -99,6 +99,7 @@ EXCLUDE = {\n     'deserialize',\n     'get',\n     'set_image_dim_ordering',\n+    'normalize_data_format',\n     'image_dim_ordering',\n     'get_variable_shape',\n }\n\n@@ -11,6 +11,7 @@ from .common import set_floatx\n from .common import cast_to_floatx\n from .common import image_data_format\n from .common import set_image_data_format\n+from .common import normalize_data_format\n \n # Set Keras base dir path given KERAS_HOME env variable, if applicable.\n # Otherwise either ~/.keras or /tmp.\n\n@@ -4,7 +4,10 @@ from __future__ import print_function\n \n import cntk as C\n import numpy as np\n-from .common import floatx, epsilon, image_dim_ordering, image_data_format\n+from .common import floatx\n+from .common import epsilon\n+from .common import image_data_format\n+from .common import normalize_data_format\n from collections import defaultdict\n from contextlib import contextmanager\n import warnings\n@@ -184,10 +187,7 @@ def variable(value, dtype=None, name=None, constraint=None):\n \n \n def bias_add(x, bias, data_format=None):\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + str(data_format))\n+    data_format = normalize_data_format(data_format)\n \n     dims = len(x.shape)\n     if dims > 0 and x.shape[0] == C.InferredDimension:\n@@ -1480,10 +1480,7 @@ def hard_sigmoid(x):\n \n def conv1d(x, kernel, strides=1, padding='valid',\n            data_format=None, dilation_rate=1):\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + str(data_format))\n+    data_format = normalize_data_format(data_format)\n \n     if padding == 'causal':\n         # causal (dilated) convolution:\n@@ -1512,10 +1509,7 @@ def conv1d(x, kernel, strides=1, padding='valid',\n \n def conv2d(x, kernel, strides=(1, 1), padding='valid',\n            data_format=None, dilation_rate=(1, 1)):\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + str(data_format))\n+    data_format = normalize_data_format(data_format)\n \n     x = _preprocess_conv2d_input(x, data_format)\n     kernel = _preprocess_conv2d_kernel(kernel, data_format)\n@@ -1546,10 +1540,7 @@ def conv2d(x, kernel, strides=(1, 1), padding='valid',\n \n def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,\n                      padding='valid', data_format=None, dilation_rate=1):\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + str(data_format))\n+    data_format = normalize_data_format(data_format)\n     if isinstance(strides, int):\n         strides = (strides,)\n     if isinstance(dilation_rate, int):\n@@ -1599,10 +1590,7 @@ def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,\n \n def separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1),\n                      padding='valid', data_format=None, dilation_rate=(1, 1)):\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + str(data_format))\n+    data_format = normalize_data_format(data_format)\n \n     x = _preprocess_conv2d_input(x, data_format)\n     depthwise_kernel = _preprocess_conv2d_kernel(depthwise_kernel, data_format)\n@@ -1637,10 +1625,7 @@ def separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1),\n \n def depthwise_conv2d(x, depthwise_kernel, strides=(1, 1), padding='valid',\n                      data_format=None, dilation_rate=(1, 1)):\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + str(data_format))\n+    data_format = normalize_data_format(data_format)\n \n     x = _preprocess_conv2d_input(x, data_format)\n     depthwise_kernel = _preprocess_conv2d_kernel(depthwise_kernel, data_format)\n@@ -1668,10 +1653,7 @@ def depthwise_conv2d(x, depthwise_kernel, strides=(1, 1), padding='valid',\n \n def conv3d(x, kernel, strides=(1, 1, 1), padding='valid',\n            data_format=None, dilation_rate=(1, 1, 1)):\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + str(data_format))\n+    data_format = normalize_data_format(data_format)\n \n     x = _preprocess_conv3d_input(x, data_format)\n     kernel = _preprocess_conv3d_kernel(kernel, data_format)\n@@ -1692,10 +1674,7 @@ def conv3d(x, kernel, strides=(1, 1, 1), padding='valid',\n \n def conv3d_transpose(x, kernel, output_shape, strides=(1, 1, 1),\n                      padding='valid', data_format=None):\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + str(data_format))\n+    data_format = normalize_data_format(data_format)\n \n     x = _preprocess_conv3d_input(x, data_format)\n     kernel = _preprocess_conv3d_kernel(kernel, data_format)\n@@ -1728,10 +1707,7 @@ def conv3d_transpose(x, kernel, output_shape, strides=(1, 1, 1),\n def pool2d(x, pool_size, strides=(1, 1),\n            padding='valid', data_format=None,\n            pool_mode='max'):\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + str(data_format))\n+    data_format = normalize_data_format(data_format)\n \n     padding = _preprocess_border_mode(padding)\n     strides = strides\n@@ -1758,10 +1734,7 @@ def pool2d(x, pool_size, strides=(1, 1),\n \n def pool3d(x, pool_size, strides=(1, 1, 1), padding='valid',\n            data_format=None, pool_mode='max'):\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + str(data_format))\n+    data_format = normalize_data_format(data_format)\n \n     padding = _preprocess_border_mode(padding)\n \n@@ -2101,10 +2074,7 @@ def spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None):\n     assert len(padding) == 2\n     assert len(padding[0]) == 2\n     assert len(padding[1]) == 2\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + str(data_format))\n+    data_format = normalize_data_format(data_format)\n \n     num_dynamic_axis = _get_dynamic_axis_num(x)\n     assert len(x.shape) == 4 - (1 if num_dynamic_axis > 0 else 0)\n@@ -2116,10 +2086,7 @@ def spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None):\n     assert len(padding[0]) == 2\n     assert len(padding[1]) == 2\n     assert len(padding[2]) == 2\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + str(data_format))\n+    data_format = normalize_data_format(data_format)\n \n     num_dynamic_axis = _get_dynamic_axis_num(x)\n     assert len(x.shape) == 5 - (1 if num_dynamic_axis > 0 else 0)\n@@ -2224,10 +2191,7 @@ def in_top_k(predictions, targets, k):\n \n def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                      padding='valid', data_format=None):\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + str(data_format))\n+    data_format = normalize_data_format(data_format)\n \n     x = _preprocess_conv2d_input(x, data_format)\n     kernel = _preprocess_conv2d_kernel(kernel, data_format)\n@@ -2357,10 +2321,7 @@ def _reshape_sequence(x, time_step):\n \n \n def local_conv1d(inputs, kernel, kernel_size, strides, data_format=None):\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + str(data_format))\n+    data_format = normalize_data_format(data_format)\n \n     stride = strides[0]\n     kernel_shape = int_shape(kernel)\n@@ -2389,10 +2350,7 @@ def local_conv2d(inputs,\n                  strides,\n                  output_shape,\n                  data_format=None):\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + str(data_format))\n+    data_format = normalize_data_format(data_format)\n \n     stride_row, stride_col = strides\n     output_row, output_col = output_shape\n\n@@ -147,6 +147,37 @@ def set_image_data_format(data_format):\n     _IMAGE_DATA_FORMAT = str(data_format)\n \n \n+def normalize_data_format(value):\n+    \"\"\"Checks that the value correspond to a valid data format.\n+\n+    # Arguments\n+        value: String or None. `'channels_first'` or `'channels_last'`.\n+\n+    # Returns\n+        A string, either `'channels_first'` or `'channels_last'`\n+\n+    # Example\n+    ```python\n+        >>> from keras import backend as K\n+        >>> K.normalize_data_format(None)\n+        'channels_first'\n+        >>> K.normalize_data_format('channels_last')\n+        'channels_last'\n+    ```\n+\n+    # Raises\n+        ValueError: if `value` or the global `data_format` invalid.\n+    \"\"\"\n+    if value is None:\n+        value = image_data_format()\n+    data_format = value.lower()\n+    if data_format not in {'channels_first', 'channels_last'}:\n+        raise ValueError('The `data_format` argument must be one of '\n+                         '\"channels_first\", \"channels_last\". Received: ' +\n+                         str(value))\n+    return data_format\n+\n+\n # Legacy methods\n \n def set_image_dim_ordering(dim_ordering):\n\n@@ -17,8 +17,9 @@ from collections import defaultdict\n import numpy as np\n import os\n \n-from .common import floatx, epsilon\n-from .common import image_data_format\n+from .common import floatx\n+from .common import epsilon\n+from .common import normalize_data_format\n from ..utils.generic_utils import has_arg\n \n # Legacy functions\n@@ -2235,10 +2236,7 @@ def spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None):\n     assert len(padding) == 2\n     assert len(padding[0]) == 2\n     assert len(padding[1]) == 2\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format: ' + str(data_format))\n+    data_format = normalize_data_format(data_format)\n \n     if data_format == 'channels_first':\n         pattern = [[0, 0],\n@@ -2279,10 +2277,7 @@ def spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None):\n     assert len(padding[0]) == 2\n     assert len(padding[1]) == 2\n     assert len(padding[2]) == 2\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format: ' + str(data_format))\n+    data_format = normalize_data_format(data_format)\n \n     if data_format == 'channels_first':\n         pattern = [\n@@ -3508,10 +3503,7 @@ def conv1d(x, kernel, strides=1, padding='valid',\n         ValueError: If `data_format` is neither\n             `\"channels_last\"` nor `\"channels_first\"`.\n     \"\"\"\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format: ' + str(data_format))\n+    data_format = normalize_data_format(data_format)\n \n     kernel_shape = kernel.get_shape().as_list()\n     if padding == 'causal':\n@@ -3559,10 +3551,7 @@ def conv2d(x, kernel, strides=(1, 1), padding='valid',\n         ValueError: If `data_format` is neither\n             `\"channels_last\"` nor `\"channels_first\"`.\n     \"\"\"\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format: ' + str(data_format))\n+    data_format = normalize_data_format(data_format)\n \n     x, tf_data_format = _preprocess_conv2d_input(x, data_format)\n \n@@ -3601,10 +3590,7 @@ def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n         ValueError: If `data_format` is neither\n             `\"channels_last\"` nor `\"channels_first\"`.\n     \"\"\"\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format: ' + str(data_format))\n+    data_format = normalize_data_format(data_format)\n     if isinstance(output_shape, (tuple, list)):\n         output_shape = tf.stack(output_shape)\n \n@@ -3653,10 +3639,7 @@ def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,\n         ValueError: If `data_format` is neither\n             `\"channels_last\"` nor `\"channels_first\"`.\n     \"\"\"\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format: ' + str(data_format))\n+    data_format = normalize_data_format(data_format)\n     if isinstance(strides, int):\n         strides = (strides,)\n     if isinstance(dilation_rate, int):\n@@ -3714,10 +3697,7 @@ def separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1),\n         ValueError: If `data_format` is neither\n             `\"channels_last\"` nor `\"channels_first\"`.\n     \"\"\"\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format: ' + str(data_format))\n+    data_format = normalize_data_format(data_format)\n \n     x, tf_data_format = _preprocess_conv2d_input(x, data_format)\n     padding = _preprocess_padding(padding)\n@@ -3756,10 +3736,7 @@ def depthwise_conv2d(x, depthwise_kernel, strides=(1, 1), padding='valid',\n         ValueError: If `data_format` is neither\n             `\"channels_last\"` nor `\"channels_first\"`.\n     \"\"\"\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format: ' + str(data_format))\n+    data_format = normalize_data_format(data_format)\n \n     x, tf_data_format = _preprocess_conv2d_input(x, data_format)\n     padding = _preprocess_padding(padding)\n@@ -3799,10 +3776,7 @@ def conv3d(x, kernel, strides=(1, 1, 1), padding='valid',\n         ValueError: If `data_format` is neither\n             `\"channels_last\"` nor `\"channels_first\"`.\n     \"\"\"\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format: ' + str(data_format))\n+    data_format = normalize_data_format(data_format)\n \n     x, tf_data_format = _preprocess_conv3d_input(x, data_format)\n     padding = _preprocess_padding(padding)\n@@ -3839,10 +3813,7 @@ def conv3d_transpose(x, kernel, output_shape, strides=(1, 1, 1),\n         ValueError: If `data_format` is neither\n             `\"channels_last\"` nor `\"channels_first\"`.\n     \"\"\"\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format: ' + str(data_format))\n+    data_format = normalize_data_format(data_format)\n     if isinstance(output_shape, (tuple, list)):\n         output_shape = tf.stack(output_shape)\n \n@@ -3892,10 +3863,7 @@ def pool2d(x, pool_size, strides=(1, 1),\n         ValueError: if `data_format` is neither `\"channels_last\"` or `\"channels_first\"`.\n         ValueError: if `pool_mode` is neither `\"max\"` or `\"avg\"`.\n     \"\"\"\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format: ' + str(data_format))\n+    data_format = normalize_data_format(data_format)\n \n     x, tf_data_format = _preprocess_conv2d_input(x, data_format)\n     padding = _preprocess_padding(padding)\n@@ -3941,10 +3909,7 @@ def pool3d(x, pool_size, strides=(1, 1, 1), padding='valid',\n         ValueError: if `data_format` is neither `\"channels_last\"` or `\"channels_first\"`.\n         ValueError: if `pool_mode` is neither `\"max\"` or `\"avg\"`.\n     \"\"\"\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format: ' + str(data_format))\n+    data_format = normalize_data_format(data_format)\n \n     x, tf_data_format = _preprocess_conv3d_input(x, data_format)\n     padding = _preprocess_padding(padding)\n@@ -3989,10 +3954,7 @@ def bias_add(x, bias, data_format=None):\n                        the bias should be either a vector or\n                        a tensor with ndim(x) - 1 dimension\n     \"\"\"\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format: ' + str(data_format))\n+    data_format = normalize_data_format(data_format)\n     bias_shape = int_shape(bias)\n     if len(bias_shape) != 1 and len(bias_shape) != ndim(x) - 1:\n         raise ValueError('Unexpected bias dimensions %d, expect to be 1 or %d dimensions'\n@@ -4321,10 +4283,7 @@ def local_conv1d(inputs, kernel, kernel_size, strides, data_format=None):\n         ValueError: If `data_format` is neither\n             `\"channels_last\"` nor `\"channels_first\"`.\n     \"\"\"\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format: ' + str(data_format))\n+    data_format = normalize_data_format(data_format)\n \n     stride = strides[0]\n     kernel_shape = int_shape(kernel)\n@@ -4373,10 +4332,7 @@ def local_conv2d(inputs, kernel, kernel_size, strides, output_shape, data_format\n         ValueError: if `data_format` is neither\n                     `channels_last` or `channels_first`.\n     \"\"\"\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format: ' + str(data_format))\n+    data_format = normalize_data_format(data_format)\n \n     stride_row, stride_col = strides\n     output_row, output_col = output_shape\n\n@@ -20,7 +20,9 @@ except ImportError:\n     from theano.sandbox.softsign import softsign as T_softsign\n \n import numpy as np\n-from .common import floatx, epsilon, image_data_format\n+from .common import floatx\n+from .common import epsilon\n+from .common import normalize_data_format\n from ..utils.generic_utils import has_arg\n # Legacy functions\n from .common import set_image_dim_ordering, image_dim_ordering\n@@ -1115,10 +1117,7 @@ def spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None):\n     assert len(padding[1]) == 2\n     top_pad, bottom_pad = padding[0]\n     left_pad, right_pad = padding[1]\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + str(data_format))\n+    data_format = normalize_data_format(data_format)\n \n     input_shape = x.shape\n     if data_format == 'channels_first':\n@@ -1151,10 +1150,7 @@ def spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None):\n     \"\"\"Pad the 2nd, 3rd and 4th dimensions of a 5D tensor\n     with \"padding[0]\", \"padding[1]\" and \"padding[2]\" (resp.) zeros left and right.\n     \"\"\"\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + str(data_format))\n+    data_format = normalize_data_format(data_format)\n \n     input_shape = x.shape\n     if data_format == 'channels_first':\n@@ -1933,10 +1929,7 @@ def conv1d(x, kernel, strides=1, padding='valid',\n         data_format: string, one of \"channels_last\", \"channels_first\"\n         dilation_rate: integer.\n     \"\"\"\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ', data_format)\n+    data_format = normalize_data_format(data_format)\n \n     kernel_shape = int_shape(kernel)\n     if padding == 'causal':\n@@ -1990,10 +1983,7 @@ def conv2d(x, kernel, strides=(1, 1), padding='valid',\n             Whether to use Theano or TensorFlow data format\n         in inputs/kernels/outputs.\n     \"\"\"\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ', data_format)\n+    data_format = normalize_data_format(data_format)\n \n     image_shape = _preprocess_conv2d_image_shape(int_shape(x), data_format)\n     kernel_shape = int_shape(kernel)\n@@ -2033,10 +2023,7 @@ def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n         ValueError: if using an even kernel size with padding 'same'.\n     \"\"\"\n     flip_filters = False\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + data_format)\n+    data_format = normalize_data_format(data_format)\n \n     if data_format == 'channels_last':\n         output_shape = (output_shape[0],\n@@ -2089,10 +2076,7 @@ def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,\n     # Raises\n         ValueError: if `data_format` is neither `\"channels_last\"` or `\"channels_first\"`.\n     \"\"\"\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ', data_format)\n+    data_format = normalize_data_format(data_format)\n     if isinstance(strides, int):\n         strides = (strides,)\n     if isinstance(dilation_rate, int):\n@@ -2163,10 +2147,7 @@ def separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1),\n     # Raises\n         ValueError: if `data_format` is neither `\"channels_last\"` or `\"channels_first\"`.\n     \"\"\"\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ', data_format)\n+    data_format = normalize_data_format(data_format)\n \n     image_shape = _preprocess_conv2d_image_shape(int_shape(x), data_format)\n     depthwise_kernel_shape = int_shape(depthwise_kernel)\n@@ -2221,10 +2202,7 @@ def depthwise_conv2d(x, depthwise_kernel, strides=(1, 1), padding='valid',\n     # Raises\n         ValueError: if `data_format` is neither `\"channels_last\"` or `\"channels_first\"`.\n     \"\"\"\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ', data_format)\n+    data_format = normalize_data_format(data_format)\n \n     image_shape = _preprocess_conv2d_image_shape(int_shape(x), data_format)\n     depthwise_kernel_shape = int_shape(depthwise_kernel)\n@@ -2261,10 +2239,7 @@ def conv3d(x, kernel, strides=(1, 1, 1),\n             Whether to use Theano or TensorFlow data format\n         in inputs/kernels/outputs.\n     \"\"\"\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format:', data_format)\n+    data_format = normalize_data_format(data_format)\n \n     volume_shape = _preprocess_conv3d_volume_shape(int_shape(x), data_format)\n     kernel_shape = int_shape(kernel)\n@@ -2304,10 +2279,7 @@ def conv3d_transpose(x, kernel, output_shape, strides=(1, 1, 1),\n         ValueError: if using an even kernel size with padding 'same'.\n     \"\"\"\n     flip_filters = False\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + data_format)\n+    data_format = normalize_data_format(data_format)\n \n     if data_format == 'channels_last':\n         output_shape = (output_shape[0],\n@@ -2344,10 +2316,7 @@ def conv3d_transpose(x, kernel, output_shape, strides=(1, 1, 1),\n \n def pool2d(x, pool_size, strides=(1, 1), padding='valid',\n            data_format=None, pool_mode='max'):\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format:', data_format)\n+    data_format = normalize_data_format(data_format)\n \n     assert pool_size[0] >= 1 and pool_size[1] >= 1\n \n@@ -2389,10 +2358,7 @@ def pool2d(x, pool_size, strides=(1, 1), padding='valid',\n \n def pool3d(x, pool_size, strides=(1, 1, 1), padding='valid',\n            data_format=None, pool_mode='max'):\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format:', data_format)\n+    data_format = normalize_data_format(data_format)\n \n     if padding == 'same':\n         w_pad = pool_size[0] - 2 if pool_size[0] % 2 == 1 else pool_size[0] - 1\n@@ -2436,10 +2402,7 @@ def pool3d(x, pool_size, strides=(1, 1, 1), padding='valid',\n \n \n def bias_add(x, bias, data_format=None):\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + str(data_format))\n+    data_format = normalize_data_format(data_format)\n     if ndim(bias) != 1 and ndim(bias) != ndim(x) - 1:\n         raise ValueError('Unexpected bias dimensions %d, '\n                          'expect to be 1 or %d dimensions'\n@@ -2701,10 +2664,7 @@ def foldr(fn, elems, initializer=None, name=None):\n \n \n def local_conv1d(inputs, kernel, kernel_size, strides, data_format=None):\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + str(data_format))\n+    data_format = normalize_data_format(data_format)\n \n     stride = strides[0]\n     kernel_shape = int_shape(kernel)\n@@ -2723,10 +2683,7 @@ def local_conv1d(inputs, kernel, kernel_size, strides, data_format=None):\n \n \n def local_conv2d(inputs, kernel, kernel_size, strides, output_shape, data_format=None):\n-    if data_format is None:\n-        data_format = image_data_format()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + str(data_format))\n+    data_format = normalize_data_format(data_format)\n \n     stride_row, stride_col = strides\n     output_row, output_col = output_shape\n\n@@ -107,7 +107,7 @@ class _Conv(Layer):\n         self.kernel_size = conv_utils.normalize_tuple(kernel_size, rank, 'kernel_size')\n         self.strides = conv_utils.normalize_tuple(strides, rank, 'strides')\n         self.padding = conv_utils.normalize_padding(padding)\n-        self.data_format = conv_utils.normalize_data_format(data_format)\n+        self.data_format = K.normalize_data_format(data_format)\n         self.dilation_rate = conv_utils.normalize_tuple(dilation_rate, rank, 'dilation_rate')\n         self.activation = activations.get(activation)\n         self.use_bias = use_bias\n@@ -1934,7 +1934,7 @@ class UpSampling2D(Layer):\n     @interfaces.legacy_upsampling2d_support\n     def __init__(self, size=(2, 2), data_format=None, **kwargs):\n         super(UpSampling2D, self).__init__(**kwargs)\n-        self.data_format = conv_utils.normalize_data_format(data_format)\n+        self.data_format = K.normalize_data_format(data_format)\n         self.size = conv_utils.normalize_tuple(size, 2, 'size')\n         self.input_spec = InputSpec(ndim=4)\n \n@@ -2002,7 +2002,7 @@ class UpSampling3D(Layer):\n \n     @interfaces.legacy_upsampling3d_support\n     def __init__(self, size=(2, 2, 2), data_format=None, **kwargs):\n-        self.data_format = conv_utils.normalize_data_format(data_format)\n+        self.data_format = K.normalize_data_format(data_format)\n         self.size = conv_utils.normalize_tuple(size, 3, 'size')\n         self.input_spec = InputSpec(ndim=5)\n         super(UpSampling3D, self).__init__(**kwargs)\n@@ -2130,7 +2130,7 @@ class ZeroPadding2D(Layer):\n                  data_format=None,\n                  **kwargs):\n         super(ZeroPadding2D, self).__init__(**kwargs)\n-        self.data_format = conv_utils.normalize_data_format(data_format)\n+        self.data_format = K.normalize_data_format(data_format)\n         if isinstance(padding, int):\n             self.padding = ((padding, padding), (padding, padding))\n         elif hasattr(padding, '__len__'):\n@@ -2234,7 +2234,7 @@ class ZeroPadding3D(Layer):\n     @interfaces.legacy_zeropadding3d_support\n     def __init__(self, padding=(1, 1, 1), data_format=None, **kwargs):\n         super(ZeroPadding3D, self).__init__(**kwargs)\n-        self.data_format = conv_utils.normalize_data_format(data_format)\n+        self.data_format = K.normalize_data_format(data_format)\n         if isinstance(padding, int):\n             self.padding = ((padding, padding), (padding, padding), (padding, padding))\n         elif hasattr(padding, '__len__'):\n@@ -2413,7 +2413,7 @@ class Cropping2D(Layer):\n     def __init__(self, cropping=((0, 0), (0, 0)),\n                  data_format=None, **kwargs):\n         super(Cropping2D, self).__init__(**kwargs)\n-        self.data_format = conv_utils.normalize_data_format(data_format)\n+        self.data_format = K.normalize_data_format(data_format)\n         if isinstance(cropping, int):\n             self.cropping = ((cropping, cropping), (cropping, cropping))\n         elif hasattr(cropping, '__len__'):\n@@ -2541,7 +2541,7 @@ class Cropping3D(Layer):\n     def __init__(self, cropping=((1, 1), (1, 1), (1, 1)),\n                  data_format=None, **kwargs):\n         super(Cropping3D, self).__init__(**kwargs)\n-        self.data_format = conv_utils.normalize_data_format(data_format)\n+        self.data_format = K.normalize_data_format(data_format)\n         if isinstance(cropping, int):\n             self.cropping = ((cropping, cropping),\n                              (cropping, cropping),\n\n@@ -563,7 +563,7 @@ class ConvLSTM2DCell(Layer):\n         self.kernel_size = conv_utils.normalize_tuple(kernel_size, 2, 'kernel_size')\n         self.strides = conv_utils.normalize_tuple(strides, 2, 'strides')\n         self.padding = conv_utils.normalize_padding(padding)\n-        self.data_format = conv_utils.normalize_data_format(data_format)\n+        self.data_format = K.normalize_data_format(data_format)\n         self.dilation_rate = conv_utils.normalize_tuple(dilation_rate, 2, 'dilation_rate')\n         self.activation = activations.get(activation)\n         self.recurrent_activation = activations.get(recurrent_activation)\n\n@@ -209,12 +209,7 @@ class SpatialDropout2D(Dropout):\n     @interfaces.legacy_spatialdropoutNd_support\n     def __init__(self, rate, data_format=None, **kwargs):\n         super(SpatialDropout2D, self).__init__(rate, **kwargs)\n-        if data_format is None:\n-            data_format = K.image_data_format()\n-        if data_format not in {'channels_last', 'channels_first'}:\n-            raise ValueError('`data_format` must be in '\n-                             '{`\"channels_last\"`, `\"channels_first\"`}')\n-        self.data_format = data_format\n+        self.data_format = K.normalize_data_format(data_format)\n         self.input_spec = InputSpec(ndim=4)\n \n     def _get_noise_shape(self, inputs):\n@@ -262,12 +257,7 @@ class SpatialDropout3D(Dropout):\n     @interfaces.legacy_spatialdropoutNd_support\n     def __init__(self, rate, data_format=None, **kwargs):\n         super(SpatialDropout3D, self).__init__(rate, **kwargs)\n-        if data_format is None:\n-            data_format = K.image_data_format()\n-        if data_format not in {'channels_last', 'channels_first'}:\n-            raise ValueError('`data_format` must be in '\n-                             '{`\"channels_last\"`, `\"channels_first\"`}')\n-        self.data_format = data_format\n+        self.data_format = K.normalize_data_format(data_format)\n         self.input_spec = InputSpec(ndim=5)\n \n     def _get_noise_shape(self, inputs):\n@@ -497,7 +487,7 @@ class Flatten(Layer):\n     def __init__(self, data_format=None, **kwargs):\n         super(Flatten, self).__init__(**kwargs)\n         self.input_spec = InputSpec(min_ndim=3)\n-        self.data_format = conv_utils.normalize_data_format(data_format)\n+        self.data_format = K.normalize_data_format(data_format)\n \n     def compute_output_shape(self, input_shape):\n         if not all(input_shape[1:]):\n\n@@ -101,7 +101,7 @@ class LocallyConnected1D(Layer):\n         if self.padding != 'valid':\n             raise ValueError('Invalid border mode for LocallyConnected1D '\n                              '(only \"valid\" is supported): ' + padding)\n-        self.data_format = conv_utils.normalize_data_format(data_format)\n+        self.data_format = K.normalize_data_format(data_format)\n         self.activation = activations.get(activation)\n         self.use_bias = use_bias\n         self.kernel_initializer = initializers.get(kernel_initializer)\n@@ -283,7 +283,7 @@ class LocallyConnected2D(Layer):\n         if self.padding != 'valid':\n             raise ValueError('Invalid border mode for LocallyConnected2D '\n                              '(only \"valid\" is supported): ' + padding)\n-        self.data_format = conv_utils.normalize_data_format(data_format)\n+        self.data_format = K.normalize_data_format(data_format)\n         self.activation = activations.get(activation)\n         self.use_bias = use_bias\n         self.kernel_initializer = initializers.get(kernel_initializer)\n\n@@ -126,7 +126,7 @@ class _Pooling2D(Layer):\n         self.pool_size = conv_utils.normalize_tuple(pool_size, 2, 'pool_size')\n         self.strides = conv_utils.normalize_tuple(strides, 2, 'strides')\n         self.padding = conv_utils.normalize_padding(padding)\n-        self.data_format = conv_utils.normalize_data_format(data_format)\n+        self.data_format = K.normalize_data_format(data_format)\n         self.input_spec = InputSpec(ndim=4)\n \n     def compute_output_shape(self, input_shape):\n@@ -287,7 +287,7 @@ class _Pooling3D(Layer):\n         self.pool_size = conv_utils.normalize_tuple(pool_size, 3, 'pool_size')\n         self.strides = conv_utils.normalize_tuple(strides, 3, 'strides')\n         self.padding = conv_utils.normalize_padding(padding)\n-        self.data_format = conv_utils.normalize_data_format(data_format)\n+        self.data_format = K.normalize_data_format(data_format)\n         self.input_spec = InputSpec(ndim=5)\n \n     def compute_output_shape(self, input_shape):\n@@ -488,7 +488,7 @@ class _GlobalPooling2D(Layer):\n     @interfaces.legacy_global_pooling_support\n     def __init__(self, data_format=None, **kwargs):\n         super(_GlobalPooling2D, self).__init__(**kwargs)\n-        self.data_format = conv_utils.normalize_data_format(data_format)\n+        self.data_format = K.normalize_data_format(data_format)\n         self.input_spec = InputSpec(ndim=4)\n \n     def compute_output_shape(self, input_shape):\n@@ -583,7 +583,7 @@ class _GlobalPooling3D(Layer):\n     @interfaces.legacy_global_pooling_support\n     def __init__(self, data_format=None, **kwargs):\n         super(_GlobalPooling3D, self).__init__(**kwargs)\n-        self.data_format = conv_utils.normalize_data_format(data_format)\n+        self.data_format = K.normalize_data_format(data_format)\n         self.input_spec = InputSpec(ndim=5)\n \n     def compute_output_shape(self, input_shape):\n\n@@ -748,7 +748,7 @@ class ConvRecurrent2D(Recurrent):\n         self.kernel_size = conv_utils.normalize_tuple(kernel_size, 2, 'kernel_size')\n         self.strides = conv_utils.normalize_tuple(strides, 2, 'strides')\n         self.padding = conv_utils.normalize_padding(padding)\n-        self.data_format = conv_utils.normalize_data_format(data_format)\n+        self.data_format = K.normalize_data_format(data_format)\n         self.dilation_rate = conv_utils.normalize_tuple(dilation_rate, 2, 'dilation_rate')\n         self.return_sequences = return_sequences\n         self.go_backwards = go_backwards\n\n@@ -48,17 +48,6 @@ def normalize_tuple(value, n, name):\n     return value_tuple\n \n \n-def normalize_data_format(value):\n-    if value is None:\n-        value = K.image_data_format()\n-    data_format = value.lower()\n-    if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('The `data_format` argument must be one of '\n-                         '\"channels_first\", \"channels_last\". Received: ' +\n-                         str(value))\n-    return data_format\n-\n-\n def normalize_padding(value):\n     padding = value.lower()\n     allowed = {'valid', 'same', 'causal'}\n\n@@ -1,6 +1,7 @@\n import pytest\n import numpy as np\n from keras.utils import conv_utils\n+from keras import backend as K\n \n \n def test_normalize_tuple():\n@@ -17,7 +18,7 @@ def test_normalize_tuple():\n \n def test_invalid_data_format():\n     with pytest.raises(ValueError):\n-        conv_utils.normalize_data_format('channels_middle')\n+        K.normalize_data_format('channels_middle')\n \n \n def test_invalid_padding():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
