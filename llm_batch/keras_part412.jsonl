{"custom_id": "keras#e84c83c38c0bdb7212271092e90f5a60959297e1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 7445 | Contributors (this commit): 102 | Commits (past 90d): 13 | Contributors (cumulative): 102 | DMM Complexity: None\n\nDIFF:\n@@ -2081,7 +2081,7 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n \n   * Metrics summary plots\n   * Training graph visualization\n-  * Activation histograms\n+  * Weight histograms\n   * Sampled profiling\n \n   When used in `Model.evaluate`, in addition to epoch summaries, there will be\n@@ -2103,7 +2103,7 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n       log_dir: the path of the directory where to save the log files to be\n         parsed by TensorBoard. e.g. log_dir = os.path.join(working_dir, 'logs')\n         This directory should not be reused by any other callbacks.\n-      histogram_freq: frequency (in epochs) at which to compute activation and\n+      histogram_freq: frequency (in epochs) at which to compute\n         weight histograms for the layers of the model. If set to 0, histograms\n         won't be computed. Validation data (or split) must be specified for\n         histogram visualizations.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b4dca51d0558e788f62a96d1009a07f773a202f4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1840 | Lines Deleted: 1638 | Files Changed: 8 | Hunks: 53 | Methods Changed: 153 | Complexity Δ (Sum/Max): 497/169 | Churn Δ: 3478 | Churn Cumulative: 3478 | Contributors (this commit): 1 | Commits (past 90d): 8 | Contributors (cumulative): 8 | DMM Complexity: 0.75\n\nDIFF:\n@@ -0,0 +1,187 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"All Keras metrics.\"\"\"\n+# pylint: disable=g-bad-import-order\n+\n+from keras.utils.generic_utils import deserialize_keras_object\n+from keras.utils.generic_utils import serialize_keras_object\n+from tensorflow.python.util.tf_export import keras_export\n+\n+# Base classes\n+from keras.metrics.base_metric import Metric\n+from keras.metrics.base_metric import Reduce\n+from keras.metrics.base_metric import Sum\n+from keras.metrics.base_metric import Mean\n+from keras.metrics.base_metric import MeanMetricWrapper\n+from keras.metrics.base_metric import MeanTensor\n+from keras.metrics.base_metric import SumOverBatchSize\n+from keras.metrics.base_metric import SumOverBatchSizeMetricWrapper\n+\n+# Individual metric classes\n+from keras.metrics.metrics import MeanRelativeError\n+from keras.metrics.metrics import Accuracy\n+from keras.metrics.metrics import BinaryAccuracy\n+from keras.metrics.metrics import CategoricalAccuracy\n+from keras.metrics.metrics import SparseCategoricalAccuracy\n+from keras.metrics.metrics import TopKCategoricalAccuracy\n+from keras.metrics.metrics import SparseTopKCategoricalAccuracy\n+from keras.metrics.metrics import FalsePositives\n+from keras.metrics.metrics import FalseNegatives\n+from keras.metrics.metrics import TrueNegatives\n+from keras.metrics.metrics import TruePositives\n+from keras.metrics.metrics import Precision\n+from keras.metrics.metrics import Recall\n+from keras.metrics.metrics import SensitivityAtSpecificity\n+from keras.metrics.metrics import SpecificityAtSensitivity\n+from keras.metrics.metrics import PrecisionAtRecall\n+from keras.metrics.metrics import RecallAtPrecision\n+from keras.metrics.metrics import AUC\n+from keras.metrics.metrics import CosineSimilarity\n+from keras.metrics.metrics import MeanAbsoluteError\n+from keras.metrics.metrics import MeanAbsolutePercentageError\n+from keras.metrics.metrics import MeanSquaredError\n+from keras.metrics.metrics import MeanSquaredLogarithmicError\n+from keras.metrics.metrics import Hinge\n+from keras.metrics.metrics import SquaredHinge\n+from keras.metrics.metrics import CategoricalHinge\n+from keras.metrics.metrics import RootMeanSquaredError\n+from keras.metrics.metrics import LogCoshError\n+from keras.metrics.metrics import Poisson\n+from keras.metrics.metrics import KLDivergence\n+from keras.metrics.metrics import IoU\n+from keras.metrics.metrics import BinaryIoU\n+from keras.metrics.metrics import MeanIoU\n+from keras.metrics.metrics import OneHotIoU\n+from keras.metrics.metrics import OneHotMeanIoU\n+from keras.metrics.metrics import BinaryCrossentropy\n+from keras.metrics.metrics import CategoricalCrossentropy\n+from keras.metrics.metrics import SparseCategoricalCrossentropy\n+\n+from keras.metrics.metrics import _IoUBase\n+from keras.metrics.metrics import _ConfusionMatrixConditionCount\n+from keras.metrics.metrics import SensitivitySpecificityBase\n+\n+# Metric functions\n+from keras.metrics.metrics import accuracy\n+from keras.metrics.metrics import binary_accuracy\n+from keras.metrics.metrics import categorical_accuracy\n+from keras.metrics.metrics import sparse_categorical_accuracy\n+from keras.metrics.metrics import top_k_categorical_accuracy\n+from keras.metrics.metrics import sparse_top_k_categorical_accuracy\n+from keras.metrics.metrics import cosine_similarity\n+from keras.metrics.metrics import binary_crossentropy\n+from keras.metrics.metrics import categorical_crossentropy\n+from keras.metrics.metrics import categorical_hinge\n+from keras.metrics.metrics import hinge\n+from keras.metrics.metrics import squared_hinge\n+from keras.metrics.metrics import kullback_leibler_divergence\n+from keras.metrics.metrics import logcosh\n+from keras.metrics.metrics import mean_absolute_error\n+from keras.metrics.metrics import mean_absolute_percentage_error\n+from keras.metrics.metrics import mean_squared_error\n+from keras.metrics.metrics import mean_squared_logarithmic_error\n+from keras.metrics.metrics import poisson\n+from keras.metrics.metrics import sparse_categorical_crossentropy\n+\n+# Utilities\n+from keras.metrics.base_metric import clone_metric\n+from keras.metrics.base_metric import clone_metrics\n+\n+# Aliases\n+acc = ACC = accuracy\n+bce = BCE = binary_crossentropy\n+mse = MSE = mean_squared_error\n+mae = MAE = mean_absolute_error\n+mape = MAPE = mean_absolute_percentage_error\n+msle = MSLE = mean_squared_logarithmic_error\n+log_cosh = logcosh\n+cosine_proximity = cosine_similarity\n+\n+\n+@keras_export('keras.metrics.serialize')\n+def serialize(metric):\n+  \"\"\"Serializes metric function or `Metric` instance.\n+\n+  Args:\n+    metric: A Keras `Metric` instance or a metric function.\n+\n+  Returns:\n+    Metric configuration dictionary.\n+  \"\"\"\n+  return serialize_keras_object(metric)\n+\n+\n+@keras_export('keras.metrics.deserialize')\n+def deserialize(config, custom_objects=None):\n+  \"\"\"Deserializes a serialized metric class/function instance.\n+\n+  Args:\n+    config: Metric configuration.\n+    custom_objects: Optional dictionary mapping names (strings) to custom\n+      objects (classes and functions) to be considered during deserialization.\n+\n+  Returns:\n+      A Keras `Metric` instance or a metric function.\n+  \"\"\"\n+  return deserialize_keras_object(\n+      config,\n+      module_objects=globals(),\n+      custom_objects=custom_objects,\n+      printable_module_name='metric function')\n+\n+\n+@keras_export('keras.metrics.get')\n+def get(identifier):\n+  \"\"\"Retrieves a Keras metric as a `function`/`Metric` class instance.\n+\n+  The `identifier` may be the string name of a metric function or class.\n+\n+  >>> metric = tf.keras.metrics.get(\"categorical_crossentropy\")\n+  >>> type(metric)\n+  <class 'function'>\n+  >>> metric = tf.keras.metrics.get(\"CategoricalCrossentropy\")\n+  >>> type(metric)\n+  <class '...metrics.CategoricalCrossentropy'>\n+\n+  You can also specify `config` of the metric to this function by passing dict\n+  containing `class_name` and `config` as an identifier. Also note that the\n+  `class_name` must map to a `Metric` class\n+\n+  >>> identifier = {\"class_name\": \"CategoricalCrossentropy\",\n+  ...               \"config\": {\"from_logits\": True}}\n+  >>> metric = tf.keras.metrics.get(identifier)\n+  >>> type(metric)\n+  <class '...metrics.CategoricalCrossentropy'>\n+\n+  Args:\n+    identifier: A metric identifier. One of None or string name of a metric\n+      function/class or metric configuration dictionary or a metric function or\n+      a metric class instance\n+\n+  Returns:\n+    A Keras metric as a `function`/ `Metric` class instance.\n+\n+  Raises:\n+    ValueError: If `identifier` cannot be interpreted.\n+  \"\"\"\n+  if isinstance(identifier, dict):\n+    return deserialize(identifier)\n+  elif isinstance(identifier, str):\n+    return deserialize(str(identifier))\n+  elif callable(identifier):\n+    return identifier\n+  else:\n+    raise ValueError(\n+        f'Could not interpret metric identifier: {identifier}')\n\n@@ -0,0 +1,859 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+# pylint: disable=g-classes-have-attributes\n+# pylint: disable=g-doc-return-or-yield\n+\"\"\"Base Metric classes.\"\"\"\n+\n+import abc\n+import copy\n+import types\n+import warnings\n+\n+from keras import backend\n+from keras.engine import base_layer\n+from keras.engine import base_layer_utils\n+from keras.engine import keras_tensor\n+from keras.saving.saved_model import metric_serialization\n+from keras.utils import generic_utils\n+from keras.utils import losses_utils\n+from keras.utils import metrics_utils\n+from keras.utils.tf_utils import is_tensor_or_variable\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+from tensorflow.tools.docs import doc_controls\n+\n+\n+@keras_export('keras.metrics.Metric')\n+class Metric(base_layer.Layer, metaclass=abc.ABCMeta):\n+  \"\"\"Encapsulates metric logic and state.\n+\n+  Args:\n+    name: (Optional) string name of the metric instance.\n+    dtype: (Optional) data type of the metric result.\n+    **kwargs: Additional layer keywords arguments.\n+\n+  Standalone usage:\n+\n+  ```python\n+  m = SomeMetric(...)\n+  for input in ...:\n+    m.update_state(input)\n+  print('Final result: ', m.result().numpy())\n+  ```\n+\n+  Usage with `compile()` API:\n+\n+  ```python\n+  model = tf.keras.Sequential()\n+  model.add(tf.keras.layers.Dense(64, activation='relu'))\n+  model.add(tf.keras.layers.Dense(64, activation='relu'))\n+  model.add(tf.keras.layers.Dense(10, activation='softmax'))\n+\n+  model.compile(optimizer=tf.keras.optimizers.RMSprop(0.01),\n+                loss=tf.keras.losses.CategoricalCrossentropy(),\n+                metrics=[tf.keras.metrics.CategoricalAccuracy()])\n+\n+  data = np.random.random((1000, 32))\n+  labels = np.random.random((1000, 10))\n+\n+  dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n+  dataset = dataset.batch(32)\n+\n+  model.fit(dataset, epochs=10)\n+  ```\n+\n+  To be implemented by subclasses:\n+  * `__init__()`: All state variables should be created in this method by\n+    calling `self.add_weight()` like: `self.var = self.add_weight(...)`\n+  * `update_state()`: Has all updates to the state variables like:\n+    self.var.assign_add(...).\n+  * `result()`: Computes and returns a scalar value or a dict of scalar values\n+    for the metric from the state variables.\n+\n+  Example subclass implementation:\n+\n+  ```python\n+  class BinaryTruePositives(tf.keras.metrics.Metric):\n+\n+    def __init__(self, name='binary_true_positives', **kwargs):\n+      super(BinaryTruePositives, self).__init__(name=name, **kwargs)\n+      self.true_positives = self.add_weight(name='tp', initializer='zeros')\n+\n+    def update_state(self, y_true, y_pred, sample_weight=None):\n+      y_true = tf.cast(y_true, tf.bool)\n+      y_pred = tf.cast(y_pred, tf.bool)\n+\n+      values = tf.logical_and(tf.equal(y_true, True), tf.equal(y_pred, True))\n+      values = tf.cast(values, self.dtype)\n+      if sample_weight is not None:\n+        sample_weight = tf.cast(sample_weight, self.dtype)\n+        sample_weight = tf.broadcast_to(sample_weight, values.shape)\n+        values = tf.multiply(values, sample_weight)\n+      self.true_positives.assign_add(tf.reduce_sum(values))\n+\n+    def result(self):\n+      return self.true_positives\n+  ```\n+  \"\"\"\n+\n+  def __init__(self, name=None, dtype=None, **kwargs):\n+    super(Metric, self).__init__(name=name, dtype=dtype, **kwargs)\n+    self.stateful = True  # All metric layers are stateful.\n+    self.built = True\n+    if not base_layer_utils.v2_dtype_behavior_enabled():\n+      # We only do this when the V2 behavior is not enabled, as when it is\n+      # enabled, the dtype already defaults to floatx.\n+      self._dtype = (backend.floatx() if dtype is None\n+                     else tf.as_dtype(dtype).name)\n+\n+  def __new__(cls, *args, **kwargs):\n+    obj = super(Metric, cls).__new__(cls)\n+\n+    # If `update_state` is not in eager/tf.function and it is not from a\n+    # built-in metric, wrap it in `tf.function`. This is so that users writing\n+    # custom metrics in v1 need not worry about control dependencies and\n+    # return ops.\n+    if (base_layer_utils.is_in_eager_or_tf_function() or\n+        is_built_in(cls)):\n+      obj_update_state = obj.update_state\n+\n+      def update_state_fn(*args, **kwargs):\n+        control_status = tf.__internal__.autograph.control_status_ctx()\n+        ag_update_state = tf.__internal__.autograph.tf_convert(\n+            obj_update_state, control_status)\n+        return ag_update_state(*args, **kwargs)\n+    else:\n+      if isinstance(obj.update_state, tf.__internal__.function.Function):\n+        update_state_fn = obj.update_state\n+      else:\n+        update_state_fn = tf.function(obj.update_state)\n+\n+    obj.update_state = types.MethodType(\n+        metrics_utils.update_state_wrapper(update_state_fn), obj)\n+\n+    obj_result = obj.result\n+\n+    def result_fn(*args, **kwargs):\n+      control_status = tf.__internal__.autograph.control_status_ctx()\n+      ag_result = tf.__internal__.autograph.tf_convert(\n+          obj_result, control_status)\n+      return ag_result(*args, **kwargs)\n+\n+    obj.result = types.MethodType(metrics_utils.result_wrapper(result_fn), obj)\n+\n+    return obj\n+\n+  def __call__(self, *args, **kwargs):\n+    \"\"\"Accumulates statistics and then computes metric result value.\n+\n+    Args:\n+      *args:\n+      **kwargs: A mini-batch of inputs to the Metric,\n+        passed on to `update_state()`.\n+\n+    Returns:\n+      The metric value tensor.\n+    \"\"\"\n+\n+    def replica_local_fn(*args, **kwargs):\n+      \"\"\"Updates the state of the metric in a replica-local context.\"\"\"\n+      if any(\n+          isinstance(arg, keras_tensor.KerasTensor)\n+          for arg in tf.nest.flatten((args, kwargs))):\n+        update_op = None\n+      else:\n+        update_op = self.update_state(*args, **kwargs)  # pylint: disable=not-callable\n+      update_ops = []\n+      if update_op is not None:\n+        update_ops.append(update_op)\n+      with tf.control_dependencies(update_ops):\n+        result_t = self.result()  # pylint: disable=not-callable\n+\n+        # We are adding the metric object as metadata on the result tensor.\n+        # This is required when we want to use a metric with `add_metric` API on\n+        # a Model/Layer in graph mode. This metric instance will later be used\n+        # to reset variable state after each epoch of training.\n+        # Example:\n+        #   model = Model()\n+        #   mean = Mean()\n+        #   model.add_metric(mean(values), name='mean')\n+        result_t._metric_obj = self  # pylint: disable=protected-access\n+        return result_t\n+\n+    from keras.distribute import distributed_training_utils  # pylint:disable=g-import-not-at-top\n+    return distributed_training_utils.call_replica_local_fn(\n+        replica_local_fn, *args, **kwargs)\n+\n+  def __str__(self):\n+    args = ','.join(f'{k}={v}' for k, v in self.get_config().items())\n+    return f'{self.__class__.__name__}({args})'\n+\n+  def __deepcopy__(self, memo):\n+    result = type(self)(name=self.name, dtype=self.dtype)\n+    memo[id(self)] = result\n+\n+    for k, v in self.__dict__.items():\n+      if k in ['update_state', 'result']:\n+        # `update_state` keeps a closure of `update_state_fn`, and deep\n+        # copying it would result in copying that old reference. Avoid that.\n+        # Likewise for `result`.\n+        continue\n+      if k in ['_obj_reference_counts_dict']:\n+        # `Layer.__setattr__` attempts to flatten the\n+        # `ObjectIdentityDictionary`, which can't be done since it stores\n+        # heterogeneous instances.\n+        tf.Module.__setattr__(result, k, copy.deepcopy(v, memo))\n+      elif k in ['_thread_local', '_metrics_lock']:\n+        # Can't pickle _thread.lock objects.\n+        setattr(result, k, v)\n+      else:\n+        setattr(result, k, copy.deepcopy(v, memo))\n+\n+    return result\n+\n+  @property\n+  def dtype(self):\n+    return self._dtype\n+\n+  def get_config(self):\n+    \"\"\"Returns the serializable config of the metric.\"\"\"\n+    return {'name': self.name, 'dtype': self.dtype}\n+\n+  def reset_state(self):\n+    \"\"\"Resets all of the metric state variables.\n+\n+    This function is called between epochs/steps,\n+    when a metric is evaluated during training.\n+    \"\"\"\n+    if not generic_utils.is_default(self.reset_states):\n+      warnings.warn(\n+          'Metric %s implements a `reset_states()` method; rename it '\n+          'to `reset_state()` (without the final \"s\"). The name '\n+          '`reset_states()` has been deprecated to improve API '\n+          'consistency.' % (self.__class__.__name__,),\n+          stacklevel=2)\n+      return self.reset_states()\n+    else:\n+      backend.batch_set_value([(v, 0) for v in self.variables])\n+\n+  @abc.abstractmethod\n+  def update_state(self, *args, **kwargs):\n+    \"\"\"Accumulates statistics for the metric.\n+\n+    Note: This function is executed as a graph function in graph mode.\n+    This means:\n+      a) Operations on the same resource are executed in textual order.\n+         This should make it easier to do things like add the updated\n+         value of a variable to another, for example.\n+      b) You don't need to worry about collecting the update ops to execute.\n+         All update ops added to the graph by this function will be executed.\n+      As a result, code should generally work the same way with graph or\n+      eager execution.\n+\n+    Args:\n+      *args:\n+      **kwargs: A mini-batch of inputs to the Metric.\n+    \"\"\"\n+    raise NotImplementedError('Must be implemented in subclasses.')\n+\n+  def merge_state(self, metrics):\n+    \"\"\"Merges the state from one or more metrics.\n+\n+    This method can be used by distributed systems to merge the state computed\n+    by different metric instances. Typically the state will be stored in the\n+    form of the metric's weights. For example, a tf.keras.metrics.Mean metric\n+    contains a list of two weight values: a total and a count. If there were two\n+    instances of a tf.keras.metrics.Accuracy that each independently aggregated\n+    partial state for an overall accuracy calculation, these two metric's states\n+    could be combined as follows:\n+\n+    >>> m1 = tf.keras.metrics.Accuracy()\n+    >>> _ = m1.update_state([[1], [2]], [[0], [2]])\n+\n+    >>> m2 = tf.keras.metrics.Accuracy()\n+    >>> _ = m2.update_state([[3], [4]], [[3], [4]])\n+\n+    >>> m2.merge_state([m1])\n+    >>> m2.result().numpy()\n+    0.75\n+\n+    Args:\n+      metrics: an iterable of metrics. The metrics must have compatible state.\n+\n+    Raises:\n+      ValueError: If the provided iterable does not contain metrics matching the\n+        metric's required specifications.\n+    \"\"\"\n+    assign_add_ops = []\n+    for metric in metrics:\n+      if len(self.weights) != len(metric.weights):\n+        raise ValueError(f'Metric {metric} is not compatible with {self}')\n+      for weight, weight_to_add in zip(self.weights, metric.weights):\n+        assign_add_ops.append(weight.assign_add(weight_to_add))\n+    return assign_add_ops\n+\n+  @abc.abstractmethod\n+  def result(self):\n+    \"\"\"Computes and returns the scalar metric value tensor or a dict of scalars.\n+\n+    Result computation is an idempotent operation that simply calculates the\n+    metric value using the state variables.\n+\n+    Returns:\n+      A scalar tensor, or a dictionary of scalar tensors.\n+    \"\"\"\n+    raise NotImplementedError('Must be implemented in subclasses.')\n+\n+  ### For use by subclasses ###\n+  @doc_controls.for_subclass_implementers\n+  def add_weight(\n+      self,\n+      name,\n+      shape=(),\n+      aggregation=tf.VariableAggregation.SUM,\n+      synchronization=tf.VariableSynchronization.ON_READ,\n+      initializer=None,\n+      dtype=None):\n+    \"\"\"Adds state variable. Only for use by subclasses.\"\"\"\n+    if tf.distribute.has_strategy():\n+      strategy = tf.distribute.get_strategy()\n+    else:\n+      strategy = None\n+\n+    # TODO(b/120571621): Make `ON_READ` work with Keras metrics on TPU.\n+    if backend.is_tpu_strategy(strategy):\n+      synchronization = tf.VariableSynchronization.ON_WRITE\n+\n+    with tf.init_scope():\n+      return super(Metric, self).add_weight(\n+          name=name,\n+          shape=shape,\n+          dtype=self._dtype if dtype is None else dtype,\n+          trainable=False,\n+          initializer=initializer,\n+          collections=[],\n+          synchronization=synchronization,\n+          aggregation=aggregation)\n+\n+  ### End: For use by subclasses ###\n+\n+  @property\n+  def trainable_weights(self):\n+    # Overridden from Layer class to track submetric weights.\n+    if self.trainable:\n+      trainable_weights = self._trainable_weights\n+      for m in self._metrics:\n+        trainable_weights += m.trainable_weights\n+      return self._dedup_weights(trainable_weights)\n+    else:\n+      return []\n+\n+  @property\n+  def non_trainable_weights(self):\n+    # Overridden from Layer class to track submetric weights.\n+    if self.trainable:\n+      non_trainable_weights = self._non_trainable_weights\n+      for m in self._metrics:\n+        non_trainable_weights += m.non_trainable_weights\n+    else:\n+      non_trainable_weights = (\n+          self._non_trainable_weights + self._trainable_weights)\n+      for m in self._metrics:\n+        non_trainable_weights += m.weights\n+    return self._dedup_weights(non_trainable_weights)\n+\n+  @property\n+  def _trackable_saved_model_saver(self):\n+    return metric_serialization.MetricSavedModelSaver(self)\n+\n+  @generic_utils.default\n+  @doc_controls.do_not_generate_docs\n+  def reset_states(self):\n+    # Backwards compatibility alias of `reset_state`. New classes should\n+    # only implement `reset_state`.\n+    return self.reset_state()\n+\n+\n+class Reduce(Metric):\n+  \"\"\"Encapsulates metrics that perform a reduce operation on the values.\n+\n+  Args:\n+    reduction: a `tf.keras.metrics.Reduction` enum value.\n+    name: string name of the metric instance.\n+    dtype: (Optional) data type of the metric result.\n+  \"\"\"\n+\n+  def __init__(self, reduction, name, dtype=None):\n+    super(Reduce, self).__init__(name=name, dtype=dtype)\n+    self.reduction = reduction\n+    self.total = self.add_weight(\n+        'total', initializer='zeros')\n+    if reduction in [metrics_utils.Reduction.SUM_OVER_BATCH_SIZE,\n+                     metrics_utils.Reduction.WEIGHTED_MEAN]:\n+      self.count = self.add_weight(\n+          'count', initializer='zeros')\n+\n+  def update_state(self, values, sample_weight=None):\n+    \"\"\"Accumulates statistics for computing the metric.\n+\n+    Args:\n+      values: Per-example value.\n+      sample_weight: Optional weighting of each example. Defaults to 1.\n+\n+    Returns:\n+      Update op.\n+    \"\"\"\n+    [values], sample_weight = \\\n+        metrics_utils.ragged_assert_compatible_and_get_flat_values(\n+            [values], sample_weight)\n+    try:\n+      values = tf.cast(values, self._dtype)\n+    except (ValueError, TypeError):\n+      msg = ('The output of a metric function can only be a single Tensor. '\n+             f'Received: {values}. ')\n+      if isinstance(values, dict):\n+        msg += ('To return a dict of values, implement a custom Metric '\n+                'subclass.')\n+      raise RuntimeError(msg)\n+    if sample_weight is not None:\n+      sample_weight = tf.cast(sample_weight, self._dtype)\n+      # Update dimensions of weights to match with values if possible.\n+      values, _, sample_weight = losses_utils.squeeze_or_expand_dimensions(\n+          values, sample_weight=sample_weight)\n+      try:\n+        # Broadcast weights if possible.\n+        sample_weight = tf.__internal__.ops.broadcast_weights(\n+            sample_weight, values)\n+      except ValueError:\n+        # Reduce values to same ndim as weight array\n+        ndim = backend.ndim(values)\n+        weight_ndim = backend.ndim(sample_weight)\n+        if self.reduction == metrics_utils.Reduction.SUM:\n+          values = tf.reduce_sum(\n+              values, axis=list(range(weight_ndim, ndim)))\n+        else:\n+          values = tf.reduce_mean(\n+              values, axis=list(range(weight_ndim, ndim)))\n+      values = tf.multiply(values, sample_weight)\n+\n+    value_sum = tf.reduce_sum(values)\n+    with tf.control_dependencies([value_sum]):\n+      update_total_op = self.total.assign_add(value_sum)\n+\n+    # Exit early if the reduction doesn't have a denominator.\n+    if self.reduction == metrics_utils.Reduction.SUM:\n+      return update_total_op\n+\n+    # Update `count` for reductions that require a denominator.\n+    if self.reduction == metrics_utils.Reduction.SUM_OVER_BATCH_SIZE:\n+      num_values = tf.cast(tf.size(values), self._dtype)\n+    elif self.reduction == metrics_utils.Reduction.WEIGHTED_MEAN:\n+      if sample_weight is None:\n+        num_values = tf.cast(tf.size(values), self._dtype)\n+      else:\n+        num_values = tf.reduce_sum(sample_weight)\n+    else:\n+      raise NotImplementedError(\n+          f'Reduction \"{self.reduction}\" not implemented. Expected '\n+          '\"sum\", \"weighted_mean\", or \"sum_over_batch_size\".')\n+\n+    with tf.control_dependencies([update_total_op]):\n+      return self.count.assign_add(num_values)\n+\n+  def result(self):\n+    if self.reduction == metrics_utils.Reduction.SUM:\n+      return tf.identity(self.total)\n+    elif self.reduction in [\n+        metrics_utils.Reduction.WEIGHTED_MEAN,\n+        metrics_utils.Reduction.SUM_OVER_BATCH_SIZE\n+    ]:\n+      return tf.math.divide_no_nan(self.total, self.count)\n+    else:\n+      raise NotImplementedError(\n+          f'Reduction \"{self.reduction}\" not implemented. Expected '\n+          '\"sum\", \"weighted_mean\", or \"sum_over_batch_size\".')\n+\n+\n+@keras_export('keras.metrics.Sum')\n+class Sum(Reduce):\n+  \"\"\"Computes the (weighted) sum of the given values.\n+\n+  For example, if values is [1, 3, 5, 7] then the sum is 16.\n+  If the weights were specified as [1, 1, 0, 0] then the sum would be 4.\n+\n+  This metric creates one variable, `total`, that is used to compute the sum of\n+  `values`. This is ultimately returned as `sum`.\n+\n+  If `sample_weight` is `None`, weights default to 1.  Use `sample_weight` of 0\n+  to mask values.\n+\n+  Args:\n+    name: (Optional) string name of the metric instance.\n+    dtype: (Optional) data type of the metric result.\n+\n+  Standalone usage:\n+\n+  >>> m = tf.keras.metrics.Sum()\n+  >>> m.update_state([1, 3, 5, 7])\n+  >>> m.result().numpy()\n+  16.0\n+\n+  Usage with `compile()` API:\n+\n+  ```python\n+  model.add_metric(tf.keras.metrics.Sum(name='sum_1')(outputs))\n+  model.compile(optimizer='sgd', loss='mse')\n+  ```\n+  \"\"\"\n+\n+  def __init__(self, name='sum', dtype=None):\n+    super(Sum, self).__init__(reduction=metrics_utils.Reduction.SUM,\n+                              name=name, dtype=dtype)\n+\n+\n+@keras_export('keras.metrics.Mean')\n+class Mean(Reduce):\n+  \"\"\"Computes the (weighted) mean of the given values.\n+\n+  For example, if values is [1, 3, 5, 7] then the mean is 4.\n+  If the weights were specified as [1, 1, 0, 0] then the mean would be 2.\n+\n+  This metric creates two variables, `total` and `count` that are used to\n+  compute the average of `values`. This average is ultimately returned as `mean`\n+  which is an idempotent operation that simply divides `total` by `count`.\n+\n+  If `sample_weight` is `None`, weights default to 1.\n+  Use `sample_weight` of 0 to mask values.\n+\n+  Args:\n+    name: (Optional) string name of the metric instance.\n+    dtype: (Optional) data type of the metric result.\n+\n+  Standalone usage:\n+\n+  >>> m = tf.keras.metrics.Mean()\n+  >>> m.update_state([1, 3, 5, 7])\n+  >>> m.result().numpy()\n+  4.0\n+  >>> m.reset_state()\n+  >>> m.update_state([1, 3, 5, 7], sample_weight=[1, 1, 0, 0])\n+  >>> m.result().numpy()\n+  2.0\n+\n+  Usage with `compile()` API:\n+\n+  ```python\n+  model.add_metric(tf.keras.metrics.Mean(name='mean_1')(outputs))\n+  model.compile(optimizer='sgd', loss='mse')\n+  ```\n+  \"\"\"\n+\n+  def __init__(self, name='mean', dtype=None):\n+    super(Mean, self).__init__(\n+        reduction=metrics_utils.Reduction.WEIGHTED_MEAN, name=name, dtype=dtype)\n+\n+\n+@keras_export('keras.metrics.MeanMetricWrapper')\n+class MeanMetricWrapper(Mean):\n+  \"\"\"Wraps a stateless metric function with the Mean metric.\n+\n+  You could use this class to quickly build a mean metric from a function. The\n+  function needs to have the signature `fn(y_true, y_pred)` and return a\n+  per-sample loss array. `MeanMetricWrapper.result()` will return\n+  the average metric value across all samples seen so far.\n+\n+  For example:\n+\n+  ```python\n+  def accuracy(y_true, y_pred):\n+    return tf.cast(tf.math.equal(y_true, y_pred), tf.float32)\n+\n+  accuracy_metric = tf.keras.metrics.MeanMetricWrapper(fn=accuracy)\n+\n+  keras_model.compile(..., metrics=accuracy_metric)\n+  ```\n+\n+  Args:\n+    fn: The metric function to wrap, with signature `fn(y_true, y_pred,\n+      **kwargs)`.\n+    name: (Optional) string name of the metric instance.\n+    dtype: (Optional) data type of the metric result.\n+    **kwargs: Keyword arguments to pass on to `fn`.\n+  \"\"\"\n+\n+  def __init__(self, fn, name=None, dtype=None, **kwargs):\n+    super(MeanMetricWrapper, self).__init__(name=name, dtype=dtype)\n+    self._fn = fn\n+    self._fn_kwargs = kwargs\n+\n+  def update_state(self, y_true, y_pred, sample_weight=None):\n+    \"\"\"Accumulates metric statistics.\n+\n+    `y_true` and `y_pred` should have the same shape.\n+\n+    Args:\n+      y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n+      y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n+      sample_weight: Optional `sample_weight` acts as a\n+        coefficient for the metric. If a scalar is provided, then the metric is\n+        simply scaled by the given value. If `sample_weight` is a tensor of size\n+        `[batch_size]`, then the metric for each sample of the batch is rescaled\n+        by the corresponding element in the `sample_weight` vector. If the shape\n+        of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted\n+        to this shape), then each metric element of `y_pred` is scaled by the\n+        corresponding value of `sample_weight`. (Note on `dN-1`: all metric\n+        functions reduce by 1 dimension, usually the last axis (-1)).\n+\n+    Returns:\n+      Update op.\n+    \"\"\"\n+    y_true = tf.cast(y_true, self._dtype)\n+    y_pred = tf.cast(y_pred, self._dtype)\n+    [y_true, y_pred], sample_weight = (\n+        metrics_utils.ragged_assert_compatible_and_get_flat_values(\n+            [y_true, y_pred], sample_weight))\n+    y_pred, y_true = losses_utils.squeeze_or_expand_dimensions(\n+        y_pred, y_true)\n+\n+    ag_fn = tf.__internal__.autograph.tf_convert(self._fn, tf.__internal__.autograph.control_status_ctx())\n+    matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n+    return super(MeanMetricWrapper, self).update_state(\n+        matches, sample_weight=sample_weight)\n+\n+  def get_config(self):\n+    config = {}\n+\n+    if type(self) is MeanMetricWrapper:  # pylint: disable=unidiomatic-typecheck\n+      # Only include function argument when the object is a MeanMetricWrapper\n+      # and not a subclass.\n+      config['fn'] = self._fn\n+\n+    for k, v in self._fn_kwargs.items():\n+      config[k] = backend.eval(v) if is_tensor_or_variable(v) else v\n+    base_config = super(MeanMetricWrapper, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+  @classmethod\n+  def from_config(cls, config):\n+    from keras.metrics import get  # pylint: disable=g-import-not-at-top\n+    # Note that while MeanMetricWrapper itself isn't public, objects of this\n+    # class may be created and added to the model by calling model.compile.\n+    fn = config.pop('fn', None)\n+    if cls is MeanMetricWrapper:\n+      return cls(get(fn), **config)\n+    return super(MeanMetricWrapper, cls).from_config(config)\n+\n+\n+@keras_export('keras.metrics.MeanTensor')\n+class MeanTensor(Metric):\n+  \"\"\"Computes the element-wise (weighted) mean of the given tensors.\n+\n+  `MeanTensor` returns a tensor with the same shape of the input tensors. The\n+  mean value is updated by keeping local variables `total` and `count`. The\n+  `total` tracks the sum of the weighted values, and `count` stores the sum of\n+  the weighted counts.\n+\n+  Args:\n+    name: (Optional) string name of the metric instance.\n+    dtype: (Optional) data type of the metric result.\n+    shape: (Optional) A list of integers, a tuple of integers, or a 1-D Tensor\n+      of type int32. If not specified, the shape is inferred from the values at\n+      the first call of update_state.\n+\n+  Standalone usage:\n+\n+  >>> m = tf.keras.metrics.MeanTensor()\n+  >>> m.update_state([0, 1, 2, 3])\n+  >>> m.update_state([4, 5, 6, 7])\n+  >>> m.result().numpy()\n+  array([2., 3., 4., 5.], dtype=float32)\n+\n+  >>> m.update_state([12, 10, 8, 6], sample_weight= [0, 0.2, 0.5, 1])\n+  >>> m.result().numpy()\n+  array([2.       , 3.6363635, 4.8      , 5.3333335], dtype=float32)\n+\n+  >>> m = tf.keras.metrics.MeanTensor(dtype=tf.float64, shape=(1, 4))\n+  >>> m.result().numpy()\n+  array([[0., 0., 0., 0.]])\n+  >>> m.update_state([[0, 1, 2, 3]])\n+  >>> m.update_state([[4, 5, 6, 7]])\n+  >>> m.result().numpy()\n+  array([[2., 3., 4., 5.]])\n+  \"\"\"\n+\n+  def __init__(self, name='mean_tensor', dtype=None, shape=None):\n+    super(MeanTensor, self).__init__(name=name, dtype=dtype)\n+    self._shape = None\n+    self._total = None\n+    self._count = None\n+    self._built = False\n+    if shape is not None:\n+      self._build(shape)\n+\n+  def _build(self, shape):\n+    self._shape = tf.TensorShape(shape)\n+    self._build_input_shape = self._shape\n+    # Create new state variables\n+    self._total = self.add_weight(\n+        name='total', shape=shape, initializer='zeros')\n+    self._count = self.add_weight(\n+        name='count', shape=shape, initializer='zeros')\n+    with tf.init_scope():\n+      if not tf.executing_eagerly():\n+        backend._initialize_variables(backend._get_session())  # pylint: disable=protected-access\n+    self._built = True\n+\n+  @property\n+  def total(self):\n+    return self._total if self._built else None\n+\n+  @property\n+  def count(self):\n+    return self._count if self._built else None\n+\n+  def update_state(self, values, sample_weight=None):\n+    \"\"\"Accumulates statistics for computing the element-wise mean.\n+\n+    Args:\n+      values: Per-example value.\n+      sample_weight: Optional weighting of each example. Defaults to 1.\n+\n+    Returns:\n+      Update op.\n+    \"\"\"\n+    values = tf.cast(values, self._dtype)\n+    if not self._built:\n+      self._build(values.shape)\n+    elif values.shape != self._shape:\n+      raise ValueError(\n+          'MeanTensor input values must always have the same '\n+          f'shape. Expected shape (set during the first call): {self._shape}. '\n+          f'Got: {values.shape}.')\n+\n+    num_values = tf.ones_like(values)\n+    if sample_weight is not None:\n+      sample_weight = tf.cast(sample_weight, self._dtype)\n+\n+      # Update dimensions of weights to match with values if possible.\n+      values, _, sample_weight = losses_utils.squeeze_or_expand_dimensions(\n+          values, sample_weight=sample_weight)\n+      try:\n+        # Broadcast weights if possible.\n+        sample_weight = tf.__internal__.ops.broadcast_weights(\n+            sample_weight, values)\n+      except ValueError:\n+        # Reduce values to same ndim as weight array\n+        ndim = backend.ndim(values)\n+        weight_ndim = backend.ndim(sample_weight)\n+        values = tf.reduce_mean(\n+            values, axis=list(range(weight_ndim, ndim)))\n+\n+      num_values = tf.multiply(num_values, sample_weight)\n+      values = tf.multiply(values, sample_weight)\n+\n+    update_total_op = self._total.assign_add(values)\n+    with tf.control_dependencies([update_total_op]):\n+      return self._count.assign_add(num_values)\n+\n+  def result(self):\n+    if not self._built:\n+      raise ValueError(\n+          'MeanTensor does not have any value yet. Please call the MeanTensor '\n+          'instance or use `.update_state(value)` before retrieving the result.'\n+          )\n+    return tf.math.divide_no_nan(self.total, self.count)\n+\n+  def reset_state(self):\n+    if self._built:\n+      backend.batch_set_value([\n+          (v, np.zeros(v.shape.as_list())) for v in self.variables\n+      ])\n+\n+\n+class SumOverBatchSize(Reduce):\n+  \"\"\"Computes the weighted sum over batch size of the given values.\n+\n+  For example, if values is [1, 3, 5, 7] then the metric value is 4.\n+  If the weights were specified as [1, 1, 0, 0] then the value would be 1.\n+\n+  This metric creates two variables, `total` and `count` that are used to\n+  compute the average of `values`. This average is ultimately returned as sum\n+  over batch size which is an idempotent operation that simply divides `total`\n+  by `count`.\n+\n+  If `sample_weight` is `None`, weights default to 1.  Use `sample_weight` of 0\n+  to mask values.\n+  \"\"\"\n+\n+  def __init__(self, name='sum_over_batch_size', dtype=None):\n+    super(SumOverBatchSize, self).__init__(\n+        reduction=metrics_utils.Reduction.SUM_OVER_BATCH_SIZE,\n+        name=name,\n+        dtype=dtype)\n+\n+\n+class SumOverBatchSizeMetricWrapper(SumOverBatchSize):\n+  \"\"\"Wraps a function with the `SumOverBatchSizeMetricWrapper` metric.\"\"\"\n+\n+  def __init__(self, fn, name=None, dtype=None, **kwargs):\n+    \"\"\"Creates a `SumOverBatchSizeMetricWrapper` instance.\n+\n+    Args:\n+      fn: The metric function to wrap, with signature `fn(y_true, y_pred,\n+        **kwargs)`.\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+      **kwargs: The keyword arguments that are passed on to `fn`.\n+    \"\"\"\n+    super(SumOverBatchSizeMetricWrapper, self).__init__(name=name, dtype=dtype)\n+    self._fn = fn\n+    self._fn_kwargs = kwargs\n+\n+  def update_state(self, y_true, y_pred, sample_weight=None):\n+    y_true = tf.cast(y_true, self._dtype)\n+    y_pred = tf.cast(y_pred, self._dtype)\n+    y_pred, y_true = losses_utils.squeeze_or_expand_dimensions(\n+        y_pred, y_true)\n+\n+    ag_fn = tf.__internal__.autograph.tf_convert(self._fn, tf.__internal__.autograph.control_status_ctx())\n+    matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n+    return super(SumOverBatchSizeMetricWrapper, self).update_state(\n+        matches, sample_weight=sample_weight)\n+\n+  def get_config(self):\n+    config = {}\n+    for k, v in self._fn_kwargs.items():\n+      config[k] = backend.eval(v) if is_tensor_or_variable(v) else v\n+    base_config = super(SumOverBatchSizeMetricWrapper, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+def clone_metric(metric):\n+  \"\"\"Returns a clone of the metric if stateful, otherwise returns it as is.\"\"\"\n+  if isinstance(metric, Metric):\n+    with tf.init_scope():\n+      return metric.__class__.from_config(metric.get_config())\n+  return metric\n+\n+\n+def clone_metrics(metrics):\n+  \"\"\"Clones the given metric list/dict.\"\"\"\n+  return tf.nest.map_structure(clone_metric, metrics)\n+\n+\n+def is_built_in(cls):\n+  return cls.__module__.startswith('.'.join(Metric.__module__.split('.')[:-1]))\n\n@@ -0,0 +1,739 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for Keras base Metric classes.\"\"\"\n+\n+import copy\n+import os\n+\n+from absl.testing import parameterized\n+from keras import combinations\n+from keras import keras_parameterized\n+from keras import layers\n+from keras import metrics\n+from keras import Model\n+from keras import testing_utils\n+from keras.engine import base_layer\n+from keras.engine import training as training_module\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+\n+\n+@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+class KerasSumTest(tf.test.TestCase, parameterized.TestCase):\n+\n+  def test_sum(self):\n+    with self.test_session():\n+      m = metrics.Sum(name='my_sum')\n+\n+      # check config\n+      self.assertEqual(m.name, 'my_sum')\n+      self.assertTrue(m.stateful)\n+      self.assertEqual(m.dtype, tf.float32)\n+      self.assertLen(m.variables, 1)\n+      self.evaluate(tf.compat.v1.variables_initializer(m.variables))\n+\n+      # check initial state\n+      self.assertEqual(self.evaluate(m.total), 0)\n+\n+      # check __call__()\n+      self.assertEqual(self.evaluate(m(100)), 100)\n+      self.assertEqual(self.evaluate(m.total), 100)\n+\n+      # check update_state() and result() + state accumulation + tensor input\n+      update_op = m.update_state(tf.convert_to_tensor([1, 5]))\n+      self.evaluate(update_op)\n+      self.assertAlmostEqual(self.evaluate(m.result()), 106)\n+      self.assertEqual(self.evaluate(m.total), 106)  # 100 + 1 + 5\n+\n+      # check reset_state()\n+      m.reset_state()\n+      self.assertEqual(self.evaluate(m.total), 0)\n+\n+  def test_sum_with_sample_weight(self):\n+    m = metrics.Sum(dtype=tf.float64)\n+    self.assertEqual(m.dtype, tf.float64)\n+    self.evaluate(tf.compat.v1.variables_initializer(m.variables))\n+\n+    # check scalar weight\n+    result_t = m(100, sample_weight=0.5)\n+    self.assertEqual(self.evaluate(result_t), 50)\n+    self.assertEqual(self.evaluate(m.total), 50)\n+\n+    # check weights not scalar and weights rank matches values rank\n+    result_t = m([1, 5], sample_weight=[1, 0.2])\n+    result = self.evaluate(result_t)\n+    self.assertAlmostEqual(result, 52., 4)  # 50 + 1 + 5 * 0.2\n+    self.assertAlmostEqual(self.evaluate(m.total), 52., 4)\n+\n+    # check weights broadcast\n+    result_t = m([1, 2], sample_weight=0.5)\n+    self.assertAlmostEqual(self.evaluate(result_t), 53.5, 1)  # 52 + 0.5 + 1\n+    self.assertAlmostEqual(self.evaluate(m.total), 53.5, 1)\n+\n+    # check weights squeeze\n+    result_t = m([1, 5], sample_weight=[[1], [0.2]])\n+    self.assertAlmostEqual(self.evaluate(result_t), 55.5, 1)  # 53.5 + 1 + 1\n+    self.assertAlmostEqual(self.evaluate(m.total), 55.5, 1)\n+\n+    # check weights expand\n+    result_t = m([[1], [5]], sample_weight=[1, 0.2])\n+    self.assertAlmostEqual(self.evaluate(result_t), 57.5, 2)  # 55.5 + 1 + 1\n+    self.assertAlmostEqual(self.evaluate(m.total), 57.5, 1)\n+\n+    # check values reduced to the dimensions of weight\n+    result_t = m([[[1., 2.], [3., 2.], [0.5, 4.]]], sample_weight=[0.5])\n+    result = np.round(self.evaluate(result_t), decimals=2)\n+    # result = (prev: 57.5) + 0.5 + 1 + 1.5 + 1 + 0.25 + 2\n+    self.assertAlmostEqual(result, 63.75, 2)\n+    self.assertAlmostEqual(self.evaluate(m.total), 63.75, 2)\n+\n+  def test_sum_graph_with_placeholder(self):\n+    with tf.compat.v1.get_default_graph().as_default(), self.cached_session() as sess:\n+      m = metrics.Sum()\n+      v = tf.compat.v1.placeholder(tf.float32)\n+      w = tf.compat.v1.placeholder(tf.float32)\n+      self.evaluate(tf.compat.v1.variables_initializer(m.variables))\n+\n+      # check __call__()\n+      result_t = m(v, sample_weight=w)\n+      result = sess.run(result_t, feed_dict=({v: 100, w: 0.5}))\n+      self.assertEqual(result, 50)\n+      self.assertEqual(self.evaluate(m.total), 50)\n+\n+      # check update_state() and result()\n+      result = sess.run(result_t, feed_dict=({v: [1, 5], w: [1, 0.2]}))\n+      self.assertAlmostEqual(result, 52., 2)  # 50 + 1 + 5 * 0.2\n+      self.assertAlmostEqual(self.evaluate(m.total), 52., 2)\n+\n+  def test_save_restore(self):\n+    with self.test_session():\n+      checkpoint_directory = self.get_temp_dir()\n+      checkpoint_prefix = os.path.join(checkpoint_directory, 'ckpt')\n+      m = metrics.Sum()\n+      checkpoint = tf.train.Checkpoint(sum=m)\n+      self.evaluate(tf.compat.v1.variables_initializer(m.variables))\n+\n+      # update state\n+      self.evaluate(m(100.))\n+      self.evaluate(m(200.))\n+\n+      # save checkpoint and then add an update\n+      save_path = checkpoint.save(checkpoint_prefix)\n+      self.evaluate(m(1000.))\n+\n+      # restore to the same checkpoint sum object (= 300)\n+      checkpoint.restore(save_path).assert_consumed().run_restore_ops()\n+      self.evaluate(m(300.))\n+      self.assertEqual(600., self.evaluate(m.result()))\n+\n+      # restore to a different checkpoint sum object\n+      restore_sum = metrics.Sum()\n+      restore_checkpoint = tf.train.Checkpoint(sum=restore_sum)\n+      status = restore_checkpoint.restore(save_path)\n+      restore_update = restore_sum(300.)\n+      status.assert_consumed().run_restore_ops()\n+      self.evaluate(restore_update)\n+      self.assertEqual(600., self.evaluate(restore_sum.result()))\n+\n+\n+class MeanTest(keras_parameterized.TestCase):\n+\n+  # TODO(b/120949004): Re-enable garbage collection check\n+  # @test_util.run_in_graph_and_eager_modes(assert_no_eager_garbage=True)\n+  @keras_parameterized.run_all_keras_modes\n+  def test_mean(self):\n+    m = metrics.Mean(name='my_mean')\n+\n+    # check config\n+    self.assertEqual(m.name, 'my_mean')\n+    self.assertTrue(m.stateful)\n+    self.assertEqual(m.dtype, tf.float32)\n+    self.assertEqual(len(m.variables), 2)\n+    self.evaluate(tf.compat.v1.variables_initializer(m.variables))\n+\n+    # check initial state\n+    self.assertEqual(self.evaluate(m.total), 0)\n+    self.assertEqual(self.evaluate(m.count), 0)\n+\n+    # check __call__()\n+    self.assertEqual(self.evaluate(m(100)), 100)\n+    self.assertEqual(self.evaluate(m.total), 100)\n+    self.assertEqual(self.evaluate(m.count), 1)\n+\n+    # check update_state() and result() + state accumulation + tensor input\n+    update_op = m.update_state([\n+        tf.convert_to_tensor(1),\n+        tf.convert_to_tensor(5)\n+    ])\n+    self.evaluate(update_op)\n+    self.assertAlmostEqual(self.evaluate(m.result()), 106 / 3, 2)\n+    self.assertEqual(self.evaluate(m.total), 106)  # 100 + 1 + 5\n+    self.assertEqual(self.evaluate(m.count), 3)\n+\n+    # check reset_state()\n+    m.reset_state()\n+    self.assertEqual(self.evaluate(m.total), 0)\n+    self.assertEqual(self.evaluate(m.count), 0)\n+\n+    # Check save and restore config\n+    m2 = metrics.Mean.from_config(m.get_config())\n+    self.assertEqual(m2.name, 'my_mean')\n+    self.assertTrue(m2.stateful)\n+    self.assertEqual(m2.dtype, tf.float32)\n+    self.assertEqual(len(m2.variables), 2)\n+\n+  @testing_utils.run_v2_only\n+  def test_function_wrapped_reset_state(self):\n+    m = metrics.Mean(name='my_mean')\n+\n+    # check reset_state in function.\n+    @tf.function\n+    def reset_in_fn():\n+      m.reset_state()\n+      return m.update_state(100)\n+\n+    for _ in range(5):\n+      self.evaluate(reset_in_fn())\n+    self.assertEqual(self.evaluate(m.count), 1)\n+\n+  @keras_parameterized.run_all_keras_modes\n+  def test_mean_with_sample_weight(self):\n+    m = metrics.Mean(dtype=tf.float64)\n+    self.assertEqual(m.dtype, tf.float64)\n+    self.evaluate(tf.compat.v1.variables_initializer(m.variables))\n+\n+    # check scalar weight\n+    result_t = m(100, sample_weight=0.5)\n+    self.assertEqual(self.evaluate(result_t), 50 / 0.5)\n+    self.assertEqual(self.evaluate(m.total), 50)\n+    self.assertEqual(self.evaluate(m.count), 0.5)\n+\n+    # check weights not scalar and weights rank matches values rank\n+    result_t = m([1, 5], sample_weight=[1, 0.2])\n+    result = self.evaluate(result_t)\n+    self.assertAlmostEqual(result, 52 / 1.7, 2)\n+    self.assertAlmostEqual(self.evaluate(m.total), 52, 2)  # 50 + 1 + 5 * 0.2\n+    self.assertAlmostEqual(self.evaluate(m.count), 1.7, 2)  # 0.5 + 1.2\n+\n+    # check weights broadcast\n+    result_t = m([1, 2], sample_weight=0.5)\n+    self.assertAlmostEqual(self.evaluate(result_t), 53.5 / 2.7, 2)\n+    self.assertAlmostEqual(self.evaluate(m.total), 53.5, 2)  # 52 + 0.5 + 1\n+    self.assertAlmostEqual(self.evaluate(m.count), 2.7, 2)  # 1.7 + 0.5 + 0.5\n+\n+    # check weights squeeze\n+    result_t = m([1, 5], sample_weight=[[1], [0.2]])\n+    self.assertAlmostEqual(self.evaluate(result_t), 55.5 / 3.9, 2)\n+    self.assertAlmostEqual(self.evaluate(m.total), 55.5, 2)  # 53.5 + 1 + 1\n+    self.assertAlmostEqual(self.evaluate(m.count), 3.9, 2)  # 2.7 + 1.2\n+\n+    # check weights expand\n+    result_t = m([[1], [5]], sample_weight=[1, 0.2])\n+    self.assertAlmostEqual(self.evaluate(result_t), 57.5 / 5.1, 2)\n+    self.assertAlmostEqual(self.evaluate(m.total), 57.5, 2)  # 55.5 + 1 + 1\n+    self.assertAlmostEqual(self.evaluate(m.count), 5.1, 2)  # 3.9 + 1.2\n+\n+    # check values reduced to the dimensions of weight\n+    result_t = m([[[1., 2.], [3., 2.], [0.5, 4.]]], sample_weight=[0.5])\n+    result = np.round(self.evaluate(result_t), decimals=2)  # 58.5 / 5.6\n+    self.assertEqual(result, 10.45)\n+    self.assertEqual(np.round(self.evaluate(m.total), decimals=2), 58.54)\n+    self.assertEqual(np.round(self.evaluate(m.count), decimals=2), 5.6)\n+\n+  @keras_parameterized.run_all_keras_modes\n+  def test_mean_graph_with_placeholder(self):\n+    with tf.compat.v1.get_default_graph().as_default(), self.cached_session() as sess:\n+      m = metrics.Mean()\n+      v = tf.compat.v1.placeholder(tf.float32)\n+      w = tf.compat.v1.placeholder(tf.float32)\n+      self.evaluate(tf.compat.v1.variables_initializer(m.variables))\n+\n+      # check __call__()\n+      result_t = m(v, sample_weight=w)\n+      result = sess.run(result_t, feed_dict=({v: 100, w: 0.5}))\n+      self.assertEqual(self.evaluate(m.total), 50)\n+      self.assertEqual(self.evaluate(m.count), 0.5)\n+      self.assertEqual(result, 50 / 0.5)\n+\n+      # check update_state() and result()\n+      result = sess.run(result_t, feed_dict=({v: [1, 5], w: [1, 0.2]}))\n+      self.assertAlmostEqual(self.evaluate(m.total), 52, 2)  # 50 + 1 + 5 * 0.2\n+      self.assertAlmostEqual(self.evaluate(m.count), 1.7, 2)  # 0.5 + 1.2\n+      self.assertAlmostEqual(result, 52 / 1.7, 2)\n+\n+  @keras_parameterized.run_all_keras_modes\n+  def test_save_restore(self):\n+    checkpoint_directory = self.get_temp_dir()\n+    checkpoint_prefix = os.path.join(checkpoint_directory, 'ckpt')\n+    m = metrics.Mean()\n+    checkpoint = tf.train.Checkpoint(mean=m)\n+    self.evaluate(tf.compat.v1.variables_initializer(m.variables))\n+\n+    # update state\n+    self.evaluate(m(100.))\n+    self.evaluate(m(200.))\n+\n+    # save checkpoint and then add an update\n+    save_path = checkpoint.save(checkpoint_prefix)\n+    self.evaluate(m(1000.))\n+\n+    # restore to the same checkpoint mean object\n+    checkpoint.restore(save_path).assert_consumed().run_restore_ops()\n+    self.evaluate(m(300.))\n+    self.assertEqual(200., self.evaluate(m.result()))\n+\n+    # restore to a different checkpoint mean object\n+    restore_mean = metrics.Mean()\n+    restore_checkpoint = tf.train.Checkpoint(mean=restore_mean)\n+    status = restore_checkpoint.restore(save_path)\n+    restore_update = restore_mean(300.)\n+    status.assert_consumed().run_restore_ops()\n+    self.evaluate(restore_update)\n+    self.assertEqual(200., self.evaluate(restore_mean.result()))\n+    self.assertEqual(3, self.evaluate(restore_mean.count))\n+\n+  @keras_parameterized.run_all_keras_modes\n+  def test_multiple_instances(self):\n+    m = metrics.Mean()\n+    m2 = metrics.Mean()\n+\n+    self.assertEqual(m.name, 'mean')\n+    self.assertEqual(m2.name, 'mean')\n+\n+    self.assertEqual([v.name for v in m.variables],\n+                     testing_utils.get_expected_metric_variable_names(\n+                         ['total', 'count']))\n+    self.assertEqual([v.name for v in m2.variables],\n+                     testing_utils.get_expected_metric_variable_names(\n+                         ['total', 'count'], name_suffix='_1'))\n+\n+    self.evaluate(tf.compat.v1.variables_initializer(m.variables))\n+    self.evaluate(tf.compat.v1.variables_initializer(m2.variables))\n+\n+    # check initial state\n+    self.assertEqual(self.evaluate(m.total), 0)\n+    self.assertEqual(self.evaluate(m.count), 0)\n+    self.assertEqual(self.evaluate(m2.total), 0)\n+    self.assertEqual(self.evaluate(m2.count), 0)\n+\n+    # check __call__()\n+    self.assertEqual(self.evaluate(m(100)), 100)\n+    self.assertEqual(self.evaluate(m.total), 100)\n+    self.assertEqual(self.evaluate(m.count), 1)\n+    self.assertEqual(self.evaluate(m2.total), 0)\n+    self.assertEqual(self.evaluate(m2.count), 0)\n+\n+    self.assertEqual(self.evaluate(m2([63, 10])), 36.5)\n+    self.assertEqual(self.evaluate(m2.total), 73)\n+    self.assertEqual(self.evaluate(m2.count), 2)\n+    self.assertEqual(self.evaluate(m.result()), 100)\n+    self.assertEqual(self.evaluate(m.total), 100)\n+    self.assertEqual(self.evaluate(m.count), 1)\n+\n+  @testing_utils.run_v2_only\n+  def test_deepcopy_of_metrics(self):\n+    m = metrics.Mean(name='my_mean')\n+\n+    m.reset_state()\n+    m.update_state(100)\n+    m_copied = copy.deepcopy(m)\n+    m_copied.update_state(200)\n+\n+    self.assertEqual(self.evaluate(m.result()), 100)\n+    self.assertEqual(self.evaluate(m_copied.result()), 150)\n+\n+    m.reset_state()\n+\n+    self.assertEqual(self.evaluate(m.result()), 0)\n+    self.assertEqual(self.evaluate(m_copied.result()), 150)\n+\n+\n+class MeanTensorTest(tf.test.TestCase, parameterized.TestCase):\n+\n+  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  def test_config(self):\n+    with self.test_session():\n+      m = metrics.MeanTensor(name='mean_by_element')\n+\n+      # check config\n+      self.assertEqual(m.name, 'mean_by_element')\n+      self.assertTrue(m.stateful)\n+      self.assertEqual(m.dtype, tf.float32)\n+      self.assertEmpty(m.variables)\n+\n+      with self.assertRaisesRegex(ValueError, 'does not have any value yet'):\n+        m.result()\n+\n+      self.evaluate(m([[3], [5], [3]]))\n+      self.assertAllEqual(m._shape, [3, 1])\n+\n+      m2 = metrics.MeanTensor.from_config(m.get_config())\n+      self.assertEqual(m2.name, 'mean_by_element')\n+      self.assertTrue(m2.stateful)\n+      self.assertEqual(m2.dtype, tf.float32)\n+      self.assertEmpty(m2.variables)\n+\n+  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  def test_unweighted(self):\n+    with self.test_session():\n+      m = metrics.MeanTensor(dtype=tf.float64)\n+\n+      # check __call__()\n+      self.assertAllClose(self.evaluate(m([100, 40])), [100, 40])\n+      self.assertAllClose(self.evaluate(m.total), [100, 40])\n+      self.assertAllClose(self.evaluate(m.count), [1, 1])\n+\n+      # check update_state() and result() + state accumulation + tensor input\n+      update_op = m.update_state([\n+          tf.convert_to_tensor(1),\n+          tf.convert_to_tensor(5)\n+      ])\n+      self.evaluate(update_op)\n+      self.assertAllClose(self.evaluate(m.result()), [50.5, 22.5])\n+      self.assertAllClose(self.evaluate(m.total), [101, 45])\n+      self.assertAllClose(self.evaluate(m.count), [2, 2])\n+\n+      # check reset_state()\n+      m.reset_state()\n+      self.assertAllClose(self.evaluate(m.total), [0, 0])\n+      self.assertAllClose(self.evaluate(m.count), [0, 0])\n+\n+  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  def test_weighted(self):\n+    with self.test_session():\n+      m = metrics.MeanTensor(dtype=tf.float64)\n+      self.assertEqual(m.dtype, tf.float64)\n+\n+      # check scalar weight\n+      result_t = m([100, 30], sample_weight=0.5)\n+      self.assertAllClose(self.evaluate(result_t), [100, 30])\n+      self.assertAllClose(self.evaluate(m.total), [50, 15])\n+      self.assertAllClose(self.evaluate(m.count), [0.5, 0.5])\n+\n+      # check weights not scalar and weights rank matches values rank\n+      result_t = m([1, 5], sample_weight=[1, 0.2])\n+      result = self.evaluate(result_t)\n+      self.assertAllClose(result, [51 / 1.5, 16 / 0.7], 2)\n+      self.assertAllClose(self.evaluate(m.total), [51, 16])\n+      self.assertAllClose(self.evaluate(m.count), [1.5, 0.7])\n+\n+      # check weights broadcast\n+      result_t = m([1, 2], sample_weight=0.5)\n+      self.assertAllClose(self.evaluate(result_t), [51.5 / 2, 17 / 1.2])\n+      self.assertAllClose(self.evaluate(m.total), [51.5, 17])\n+      self.assertAllClose(self.evaluate(m.count), [2, 1.2])\n+\n+      # check weights squeeze\n+      result_t = m([1, 5], sample_weight=[[1], [0.2]])\n+      self.assertAllClose(self.evaluate(result_t), [52.5 / 3, 18 / 1.4])\n+      self.assertAllClose(self.evaluate(m.total), [52.5, 18])\n+      self.assertAllClose(self.evaluate(m.count), [3, 1.4])\n+\n+      # check weights expand\n+      m = metrics.MeanTensor(dtype=tf.float64)\n+      self.evaluate(tf.compat.v1.variables_initializer(m.variables))\n+      result_t = m([[1], [5]], sample_weight=[1, 0.2])\n+      self.assertAllClose(self.evaluate(result_t), [[1], [5]])\n+      self.assertAllClose(self.evaluate(m.total), [[1], [1]])\n+      self.assertAllClose(self.evaluate(m.count), [[1], [0.2]])\n+\n+  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  def test_invalid_value_shape(self):\n+    m = metrics.MeanTensor(dtype=tf.float64)\n+    m([1])\n+    with self.assertRaisesRegex(\n+        ValueError, 'MeanTensor input values must always have the same shape'):\n+      m([1, 5])\n+\n+  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  def test_build_in_tf_function(self):\n+    \"\"\"Ensure that variables are created correctly in a tf function.\"\"\"\n+    m = metrics.MeanTensor(dtype=tf.float64)\n+\n+    @tf.function\n+    def call_metric(x):\n+      return m(x)\n+\n+    with self.test_session():\n+      self.assertAllClose(self.evaluate(call_metric([100, 40])), [100, 40])\n+      self.assertAllClose(self.evaluate(m.total), [100, 40])\n+      self.assertAllClose(self.evaluate(m.count), [1, 1])\n+      self.assertAllClose(self.evaluate(call_metric([20, 2])), [60, 21])\n+\n+  @combinations.generate(combinations.combine(mode=['eager']))\n+  def test_in_keras_model(self):\n+    class ModelWithMetric(Model):\n+\n+      def __init__(self):\n+        super(ModelWithMetric, self).__init__()\n+        self.dense1 = layers.Dense(\n+            3, activation='relu', kernel_initializer='ones')\n+        self.dense2 = layers.Dense(\n+            1, activation='sigmoid', kernel_initializer='ones')\n+        self.mean_tensor = metrics.MeanTensor()\n+\n+      def call(self, x):\n+        x = self.dense1(x)\n+        x = self.dense2(x)\n+        self.mean_tensor(self.dense1.kernel)\n+        return x\n+\n+    model = ModelWithMetric()\n+    model.compile(\n+        loss='mae',\n+        optimizer='rmsprop',\n+        run_eagerly=True)\n+\n+    x = np.ones((100, 4))\n+    y = np.zeros((100, 1))\n+    model.evaluate(x, y, batch_size=50)\n+    self.assertAllClose(self.evaluate(model.mean_tensor.result()),\n+                        np.ones((4, 3)))\n+    self.assertAllClose(self.evaluate(model.mean_tensor.total),\n+                        np.full((4, 3), 2))\n+    self.assertAllClose(self.evaluate(model.mean_tensor.count),\n+                        np.full((4, 3), 2))\n+\n+    model.evaluate(x, y, batch_size=25)\n+    self.assertAllClose(self.evaluate(model.mean_tensor.result()),\n+                        np.ones((4, 3)))\n+    self.assertAllClose(self.evaluate(model.mean_tensor.total),\n+                        np.full((4, 3), 4))\n+    self.assertAllClose(self.evaluate(model.mean_tensor.count),\n+                        np.full((4, 3), 4))\n+\n+\n+class BinaryTruePositives(metrics.Metric):\n+\n+  def __init__(self, name='binary_true_positives', **kwargs):\n+    super(BinaryTruePositives, self).__init__(name=name, **kwargs)\n+    self.true_positives = self.add_weight(name='tp', initializer='zeros')\n+\n+  def update_state(self, y_true, y_pred, sample_weight=None):\n+    y_true = tf.cast(y_true, tf.bool)\n+    y_pred = tf.cast(y_pred, tf.bool)\n+\n+    values = tf.logical_and(\n+        tf.equal(y_true, True), tf.equal(y_pred, True))\n+    values = tf.cast(values, self.dtype)\n+    if sample_weight is not None:\n+      sample_weight = tf.cast(sample_weight, dtype=self.dtype)\n+      sample_weight = tf.__internal__.ops.broadcast_weights(\n+          sample_weight, values)\n+      values = tf.multiply(values, sample_weight)\n+    self.true_positives.assign_add(tf.reduce_sum(values))\n+\n+  def result(self):\n+    return self.true_positives\n+\n+\n+class BinaryTruePositivesViaControlFlow(metrics.Metric):\n+\n+  def __init__(self, name='binary_true_positives', **kwargs):\n+    super(BinaryTruePositivesViaControlFlow, self).__init__(name=name, **kwargs)\n+    self.true_positives = self.add_weight(name='tp', initializer='zeros')\n+\n+  def update_state(self, y_true, y_pred, sample_weight=None):\n+    y_true = tf.cast(y_true, tf.bool)\n+    y_pred = tf.cast(y_pred, tf.bool)\n+\n+    for i in range(len(y_true)):\n+      for j in range(len(y_true[i])):\n+        if y_true[i][j] and y_pred[i][j]:\n+          if sample_weight is None:\n+            self.true_positives.assign_add(1)\n+          else:\n+            self.true_positives.assign_add(sample_weight[i][0])\n+\n+  def result(self):\n+    if tf.constant(True):\n+      return self.true_positives\n+    return 0.0\n+\n+\n+@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+class CustomMetricsTest(tf.test.TestCase):\n+\n+  def test_config(self):\n+    btp_obj = BinaryTruePositives(name='btp', dtype=tf.int32)\n+    self.assertEqual(btp_obj.name, 'btp')\n+    self.assertEqual(btp_obj.dtype, tf.int32)\n+\n+    # Check save and restore config\n+    btp_obj2 = BinaryTruePositives.from_config(btp_obj.get_config())\n+    self.assertEqual(btp_obj2.name, 'btp')\n+    self.assertEqual(btp_obj2.dtype, tf.int32)\n+\n+  def test_unweighted(self):\n+    btp_obj = BinaryTruePositives()\n+    self.evaluate(tf.compat.v1.variables_initializer(btp_obj.variables))\n+    y_true = tf.constant([[0, 0.9, 0, 1, 0], [0, 0, 1, 1, 1],\n+                          [1, 1, 1, 1, 0], [0, 0, 0, 0, 1.5]])\n+    y_pred = tf.constant([[0, 0, 1, 5, 0], [1, 1, 1, 1, 1],\n+                          [0, 1, 0, 1, 0], [1, 10, 1, 1, 1]])\n+\n+    update_op = btp_obj.update_state(y_true, y_pred)  # pylint: disable=assignment-from-no-return\n+    self.evaluate(update_op)\n+    result = btp_obj.result()\n+    self.assertEqual(7, self.evaluate(result))\n+\n+  def test_weighted(self):\n+    btp_obj = BinaryTruePositives()\n+    self.evaluate(tf.compat.v1.variables_initializer(btp_obj.variables))\n+    y_true = tf.constant([[0, 0.9, 0, 1, 0], [0, 0, 1, 1, 1],\n+                          [1, 1, 1, 1, 0], [0, 0, 0, 0, 1.5]])\n+    y_pred = tf.constant([[0, 0, 1, 5, 0], [1, 1, 1, 1, 1],\n+                          [0, 1, 0, 1, 0], [1, 10, 1, 1, 1]])\n+    sample_weight = tf.constant([[1.], [1.5], [2.], [2.5]])\n+    result = btp_obj(y_true, y_pred, sample_weight=sample_weight)\n+    self.assertEqual(12, self.evaluate(result))\n+\n+  def test_autograph(self):\n+    metric = BinaryTruePositivesViaControlFlow()\n+    self.evaluate(tf.compat.v1.variables_initializer(metric.variables))\n+    y_true = tf.constant([[0, 0.9, 0, 1, 0], [0, 0, 1, 1, 1],\n+                          [1, 1, 1, 1, 0], [0, 0, 0, 0, 1.5]])\n+    y_pred = tf.constant([[0, 0, 1, 5, 0], [1, 1, 1, 1, 1],\n+                          [0, 1, 0, 1, 0], [1, 10, 1, 1, 1]])\n+    sample_weight = tf.constant([[1.], [1.5], [2.], [2.5]])\n+\n+    @tf.function\n+    def compute_metric(y_true, y_pred, sample_weight):\n+      metric(y_true, y_pred, sample_weight)\n+      return metric.result()\n+\n+    result = compute_metric(y_true, y_pred, sample_weight)\n+    self.assertEqual(12, self.evaluate(result))\n+\n+  def test_metric_wrappers_autograph(self):\n+    def metric_fn(y_true, y_pred):\n+      x = tf.constant(0.0)\n+      for i in range(len(y_true)):\n+        for j in range(len(y_true[i])):\n+          if tf.equal(y_true[i][j], y_pred[i][j]) and y_true[i][j] > 0:\n+            x += 1.0\n+      return x\n+\n+    mean_metric = metrics.MeanMetricWrapper(metric_fn)\n+    sum_metric = metrics.SumOverBatchSizeMetricWrapper(metric_fn)\n+    self.evaluate(tf.compat.v1.variables_initializer(mean_metric.variables))\n+    self.evaluate(tf.compat.v1.variables_initializer(sum_metric.variables))\n+\n+    y_true = tf.constant([[0, 0, 0, 1, 0],\n+                          [0, 0, 1, 1, 1],\n+                          [1, 1, 1, 1, 0],\n+                          [1, 1, 1, 0, 1]])\n+    y_pred = tf.constant([[0, 0, 1, 1, 0],\n+                          [1, 1, 1, 1, 1],\n+                          [0, 1, 0, 1, 0],\n+                          [1, 1, 1, 1, 1]])\n+\n+    @tf.function\n+    def tf_functioned_metric_fn(metric, y_true, y_pred):\n+      return metric(y_true, y_pred)\n+\n+    metric_result = tf_functioned_metric_fn(mean_metric, y_true, y_pred)\n+    self.assertAllClose(self.evaluate(metric_result), 10, 1e-2)\n+    metric_result = tf_functioned_metric_fn(sum_metric, y_true, y_pred)\n+    self.assertAllClose(self.evaluate(metric_result), 10, 1e-2)\n+\n+  def test_metric_not_tracked_as_sublayer_in_layer(self):\n+\n+    class MyLayer(base_layer.Layer):\n+\n+      def __init__(self, **kwargs):\n+        super(MyLayer, self).__init__(**kwargs)\n+        self.mean_obj = metrics.Mean(name='my_mean_obj')\n+\n+      def call(self, x):\n+        self.add_metric(\n+            tf.reduce_sum(x), aggregation='mean', name='my_mean_tensor')\n+        self.add_metric(self.mean_obj(x))\n+        return x\n+\n+    layer = MyLayer()\n+    x = np.ones((1, 1))\n+    layer(x)\n+    self.assertLen(list(layer._flatten_layers(include_self=False)), 0)\n+    self.assertLen(layer.metrics, 2)\n+\n+  def test_metric_not_tracked_as_sublayer_in_model(self):\n+\n+    class MyModel(training_module.Model):\n+\n+      def __init__(self, **kwargs):\n+        super(MyModel, self).__init__(**kwargs)\n+        self.mean_obj = metrics.Mean(name='my_mean_obj')\n+\n+      def call(self, x):\n+        self.add_metric(\n+            tf.reduce_sum(x), aggregation='mean', name='my_mean_tensor')\n+        self.add_metric(self.mean_obj(x))\n+        return x\n+\n+    model = MyModel()\n+    x = np.ones((1, 1))\n+    model(x)\n+    self.assertLen(list(model._flatten_layers(include_self=False)), 0)\n+    self.assertLen(model.layers, 0)\n+    self.assertLen(model.metrics, 2)\n+\n+  def test_invalid_custom_metric_class_error_msg(self):\n+    x = layers.Input(shape=(2,))\n+    y = layers.Dense(3)(x)\n+    model = training_module.Model(x, y)\n+\n+    class BadMetric(metrics.Metric):\n+\n+      def update_state(self, y_true, y_pred, sample_weight=None):\n+        return\n+\n+      def result(self):\n+        return\n+\n+    with self.assertRaisesRegex(RuntimeError,\n+                                'can only be a single'):\n+      model.compile('sgd',\n+                    'mse',\n+                    metrics=[BadMetric()])\n+      model.fit(np.ones((10, 2)), np.ones((10, 3)))\n+\n+  def test_invalid_custom_metric_fn_error_msg(self):\n+    x = layers.Input(shape=(2,))\n+    y = layers.Dense(3)(x)\n+    model = training_module.Model(x, y)\n+\n+    def bad_metric(y_true, y_pred, sample_weight=None):  # pylint: disable=unused-argument\n+      return None\n+\n+    def dict_metric(y_true, y_pred, sample_weight=None):  # pylint: disable=unused-argument\n+      return {'value': 0.}\n+\n+    with self.assertRaisesRegex(RuntimeError,\n+                                'The output of a metric function can only be'):\n+      model.compile('sgd',\n+                    'mse',\n+                    metrics=[bad_metric])\n+      model.fit(np.ones((10, 2)), np.ones((10, 3)))\n+    with self.assertRaisesRegex(RuntimeError,\n+                                'To return a dict of values, implement'):\n+      model.compile('sgd',\n+                    'mse',\n+                    metrics=[dict_metric])\n+      model.fit(np.ones((10, 2)), np.ones((10, 3)))\n+\n+\n+if __name__ == '__main__':\n+  tf.test.main()\n\n\n@@ -17,16 +17,10 @@\n \"\"\"Built-in metrics.\"\"\"\n \n import abc\n-import copy\n-import types\n from typing import List, Tuple, Union\n-import warnings\n \n from keras import activations\n from keras import backend\n-from keras.engine import base_layer\n-from keras.engine import base_layer_utils\n-from keras.engine import keras_tensor\n from keras.losses import binary_crossentropy\n from keras.losses import categorical_crossentropy\n from keras.losses import categorical_hinge\n@@ -40,575 +34,19 @@ from keras.losses import mean_squared_logarithmic_error\n from keras.losses import poisson\n from keras.losses import sparse_categorical_crossentropy\n from keras.losses import squared_hinge\n-from keras.saving.saved_model import metric_serialization\n-from keras.utils import generic_utils\n+from keras.metrics import base_metric\n from keras.utils import losses_utils\n from keras.utils import metrics_utils\n-from keras.utils.generic_utils import deserialize_keras_object\n-from keras.utils.generic_utils import serialize_keras_object\n from keras.utils.generic_utils import to_list\n from keras.utils.tf_utils import is_tensor_or_variable\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n from tensorflow.python.util.tf_export import keras_export\n-from tensorflow.tools.docs import doc_controls\n-\n-\n-_SPARSE_CATEGORICAL_UPDATE_STATE_DOCSTRING = \"\"\"Accumulates metric statistics.\n-\n-For sparse categorical metrics, the shapes of `y_true` and `y_pred` are\n-different.\n-\n-Args:\n-  y_true: Ground truth label values. shape = `[batch_size, d0, .. dN-1]` or\n-    shape = `[batch_size, d0, .. dN-1, 1]`.\n-  y_pred: The predicted probability values. shape = `[batch_size, d0, .. dN]`.\n-  sample_weight: Optional `sample_weight` acts as a\n-    coefficient for the metric. If a scalar is provided, then the metric is\n-    simply scaled by the given value. If `sample_weight` is a tensor of size\n-    `[batch_size]`, then the metric for each sample of the batch is rescaled\n-    by the corresponding element in the `sample_weight` vector. If the shape\n-    of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted\n-    to this shape), then each metric element of `y_pred` is scaled by the\n-    corresponding value of `sample_weight`. (Note on `dN-1`: all metric\n-    functions reduce by 1 dimension, usually the last axis (-1)).\n-\n-Returns:\n-  Update op.\n-\"\"\"\n-\n-\n-@keras_export('keras.metrics.Metric')\n-class Metric(base_layer.Layer, metaclass=abc.ABCMeta):\n-  \"\"\"Encapsulates metric logic and state.\n-\n-  Args:\n-    name: (Optional) string name of the metric instance.\n-    dtype: (Optional) data type of the metric result.\n-    **kwargs: Additional layer keywords arguments.\n-\n-  Standalone usage:\n-\n-  ```python\n-  m = SomeMetric(...)\n-  for input in ...:\n-    m.update_state(input)\n-  print('Final result: ', m.result().numpy())\n-  ```\n-\n-  Usage with `compile()` API:\n-\n-  ```python\n-  model = tf.keras.Sequential()\n-  model.add(tf.keras.layers.Dense(64, activation='relu'))\n-  model.add(tf.keras.layers.Dense(64, activation='relu'))\n-  model.add(tf.keras.layers.Dense(10, activation='softmax'))\n-\n-  model.compile(optimizer=tf.keras.optimizers.RMSprop(0.01),\n-                loss=tf.keras.losses.CategoricalCrossentropy(),\n-                metrics=[tf.keras.metrics.CategoricalAccuracy()])\n-\n-  data = np.random.random((1000, 32))\n-  labels = np.random.random((1000, 10))\n-\n-  dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n-  dataset = dataset.batch(32)\n-\n-  model.fit(dataset, epochs=10)\n-  ```\n-\n-  To be implemented by subclasses:\n-  * `__init__()`: All state variables should be created in this method by\n-    calling `self.add_weight()` like: `self.var = self.add_weight(...)`\n-  * `update_state()`: Has all updates to the state variables like:\n-    self.var.assign_add(...).\n-  * `result()`: Computes and returns a scalar value or a dict of scalar values\n-    for the metric from the state variables.\n-\n-  Example subclass implementation:\n-\n-  ```python\n-  class BinaryTruePositives(tf.keras.metrics.Metric):\n-\n-    def __init__(self, name='binary_true_positives', **kwargs):\n-      super(BinaryTruePositives, self).__init__(name=name, **kwargs)\n-      self.true_positives = self.add_weight(name='tp', initializer='zeros')\n-\n-    def update_state(self, y_true, y_pred, sample_weight=None):\n-      y_true = tf.cast(y_true, tf.bool)\n-      y_pred = tf.cast(y_pred, tf.bool)\n-\n-      values = tf.logical_and(tf.equal(y_true, True), tf.equal(y_pred, True))\n-      values = tf.cast(values, self.dtype)\n-      if sample_weight is not None:\n-        sample_weight = tf.cast(sample_weight, self.dtype)\n-        sample_weight = tf.broadcast_to(sample_weight, values.shape)\n-        values = tf.multiply(values, sample_weight)\n-      self.true_positives.assign_add(tf.reduce_sum(values))\n-\n-    def result(self):\n-      return self.true_positives\n-  ```\n-  \"\"\"\n-\n-  def __init__(self, name=None, dtype=None, **kwargs):\n-    super(Metric, self).__init__(name=name, dtype=dtype, **kwargs)\n-    self.stateful = True  # All metric layers are stateful.\n-    self.built = True\n-    if not base_layer_utils.v2_dtype_behavior_enabled():\n-      # We only do this when the V2 behavior is not enabled, as when it is\n-      # enabled, the dtype already defaults to floatx.\n-      self._dtype = (backend.floatx() if dtype is None\n-                     else tf.as_dtype(dtype).name)\n-\n-  def __new__(cls, *args, **kwargs):\n-    obj = super(Metric, cls).__new__(cls)\n-\n-    # If `update_state` is not in eager/tf.function and it is not from a\n-    # built-in metric, wrap it in `tf.function`. This is so that users writing\n-    # custom metrics in v1 need not worry about control dependencies and\n-    # return ops.\n-    if (base_layer_utils.is_in_eager_or_tf_function() or\n-        is_built_in(cls)):\n-      obj_update_state = obj.update_state\n-\n-      def update_state_fn(*args, **kwargs):\n-        control_status = tf.__internal__.autograph.control_status_ctx()\n-        ag_update_state = tf.__internal__.autograph.tf_convert(obj_update_state, control_status)\n-        return ag_update_state(*args, **kwargs)\n-    else:\n-      if isinstance(obj.update_state, tf.__internal__.function.Function):\n-        update_state_fn = obj.update_state\n-      else:\n-        update_state_fn = tf.function(obj.update_state)\n-\n-    obj.update_state = types.MethodType(\n-        metrics_utils.update_state_wrapper(update_state_fn), obj)\n-\n-    obj_result = obj.result\n-\n-    def result_fn(*args, **kwargs):\n-      control_status = tf.__internal__.autograph.control_status_ctx()\n-      ag_result = tf.__internal__.autograph.tf_convert(obj_result, control_status)\n-      return ag_result(*args, **kwargs)\n-\n-    obj.result = types.MethodType(metrics_utils.result_wrapper(result_fn), obj)\n-\n-    return obj\n-\n-  def __call__(self, *args, **kwargs):\n-    \"\"\"Accumulates statistics and then computes metric result value.\n-\n-    Args:\n-      *args:\n-      **kwargs: A mini-batch of inputs to the Metric,\n-        passed on to `update_state()`.\n-\n-    Returns:\n-      The metric value tensor.\n-    \"\"\"\n-\n-    def replica_local_fn(*args, **kwargs):\n-      \"\"\"Updates the state of the metric in a replica-local context.\"\"\"\n-      if any(\n-          isinstance(arg, keras_tensor.KerasTensor)\n-          for arg in tf.nest.flatten((args, kwargs))):\n-        update_op = None\n-      else:\n-        update_op = self.update_state(*args, **kwargs)  # pylint: disable=not-callable\n-      update_ops = []\n-      if update_op is not None:\n-        update_ops.append(update_op)\n-      with tf.control_dependencies(update_ops):\n-        result_t = self.result()  # pylint: disable=not-callable\n-\n-        # We are adding the metric object as metadata on the result tensor.\n-        # This is required when we want to use a metric with `add_metric` API on\n-        # a Model/Layer in graph mode. This metric instance will later be used\n-        # to reset variable state after each epoch of training.\n-        # Example:\n-        #   model = Model()\n-        #   mean = Mean()\n-        #   model.add_metric(mean(values), name='mean')\n-        result_t._metric_obj = self  # pylint: disable=protected-access\n-        return result_t\n-\n-    from keras.distribute import distributed_training_utils  # pylint:disable=g-import-not-at-top\n-    return distributed_training_utils.call_replica_local_fn(\n-        replica_local_fn, *args, **kwargs)\n-\n-  def __str__(self):\n-    args = ','.join(f'{k}={v}' for k, v in self.get_config().items())\n-    return f'{self.__class__.__name__}({args})'\n-\n-  def __deepcopy__(self, memo):\n-    result = type(self)(name=self.name, dtype=self.dtype)\n-    memo[id(self)] = result\n-\n-    for k, v in self.__dict__.items():\n-      if k in ['update_state', 'result']:\n-        # `update_state` keeps a closure of `update_state_fn`, and deep\n-        # copying it would result in copying that old reference. Avoid that.\n-        # Likewise for `result`.\n-        continue\n-      if k in ['_obj_reference_counts_dict']:\n-        # `Layer.__setattr__` attempts to flatten the\n-        # `ObjectIdentityDictionary`, which can't be done since it stores\n-        # heterogeneous instances.\n-        tf.Module.__setattr__(result, k, copy.deepcopy(v, memo))\n-      elif k in ['_thread_local', '_metrics_lock']:\n-        # Can't pickle _thread.lock objects.\n-        setattr(result, k, v)\n-      else:\n-        setattr(result, k, copy.deepcopy(v, memo))\n-\n-    return result\n-\n-  @property\n-  def dtype(self):\n-    return self._dtype\n-\n-  def get_config(self):\n-    \"\"\"Returns the serializable config of the metric.\"\"\"\n-    return {'name': self.name, 'dtype': self.dtype}\n-\n-  def reset_state(self):\n-    \"\"\"Resets all of the metric state variables.\n-\n-    This function is called between epochs/steps,\n-    when a metric is evaluated during training.\n-    \"\"\"\n-    if not generic_utils.is_default(self.reset_states):\n-      warnings.warn(\n-          'Metric %s implements a `reset_states()` method; rename it '\n-          'to `reset_state()` (without the final \"s\"). The name '\n-          '`reset_states()` has been deprecated to improve API '\n-          'consistency.' % (self.__class__.__name__,),\n-          stacklevel=2)\n-      return self.reset_states()\n-    else:\n-      backend.batch_set_value([(v, 0) for v in self.variables])\n-\n-  @abc.abstractmethod\n-  def update_state(self, *args, **kwargs):\n-    \"\"\"Accumulates statistics for the metric.\n-\n-    Note: This function is executed as a graph function in graph mode.\n-    This means:\n-      a) Operations on the same resource are executed in textual order.\n-         This should make it easier to do things like add the updated\n-         value of a variable to another, for example.\n-      b) You don't need to worry about collecting the update ops to execute.\n-         All update ops added to the graph by this function will be executed.\n-      As a result, code should generally work the same way with graph or\n-      eager execution.\n-\n-    Args:\n-      *args:\n-      **kwargs: A mini-batch of inputs to the Metric.\n-    \"\"\"\n-    raise NotImplementedError('Must be implemented in subclasses.')\n-\n-  def merge_state(self, metrics):\n-    \"\"\"Merges the state from one or more metrics.\n-\n-    This method can be used by distributed systems to merge the state computed\n-    by different metric instances. Typically the state will be stored in the\n-    form of the metric's weights. For example, a tf.keras.metrics.Mean metric\n-    contains a list of two weight values: a total and a count. If there were two\n-    instances of a tf.keras.metrics.Accuracy that each independently aggregated\n-    partial state for an overall accuracy calculation, these two metric's states\n-    could be combined as follows:\n-\n-    >>> m1 = tf.keras.metrics.Accuracy()\n-    >>> _ = m1.update_state([[1], [2]], [[0], [2]])\n-\n-    >>> m2 = tf.keras.metrics.Accuracy()\n-    >>> _ = m2.update_state([[3], [4]], [[3], [4]])\n-\n-    >>> m2.merge_state([m1])\n-    >>> m2.result().numpy()\n-    0.75\n-\n-    Args:\n-      metrics: an iterable of metrics. The metrics must have compatible state.\n-\n-    Raises:\n-      ValueError: If the provided iterable does not contain metrics matching the\n-        metric's required specifications.\n-    \"\"\"\n-    assign_add_ops = []\n-    for metric in metrics:\n-      if len(self.weights) != len(metric.weights):\n-        raise ValueError(f'Metric {metric} is not compatible with {self}')\n-      for weight, weight_to_add in zip(self.weights, metric.weights):\n-        assign_add_ops.append(weight.assign_add(weight_to_add))\n-    return assign_add_ops\n-\n-  @abc.abstractmethod\n-  def result(self):\n-    \"\"\"Computes and returns the scalar metric value tensor or a dict of scalars.\n-\n-    Result computation is an idempotent operation that simply calculates the\n-    metric value using the state variables.\n-\n-    Returns:\n-      A scalar tensor, or a dictionary of scalar tensors.\n-    \"\"\"\n-    raise NotImplementedError('Must be implemented in subclasses.')\n-\n-  ### For use by subclasses ###\n-  @doc_controls.for_subclass_implementers\n-  def add_weight(\n-      self,\n-      name,\n-      shape=(),\n-      aggregation=tf.VariableAggregation.SUM,\n-      synchronization=tf.VariableSynchronization.ON_READ,\n-      initializer=None,\n-      dtype=None):\n-    \"\"\"Adds state variable. Only for use by subclasses.\"\"\"\n-    if tf.distribute.has_strategy():\n-      strategy = tf.distribute.get_strategy()\n-    else:\n-      strategy = None\n-\n-    # TODO(b/120571621): Make `ON_READ` work with Keras metrics on TPU.\n-    if backend.is_tpu_strategy(strategy):\n-      synchronization = tf.VariableSynchronization.ON_WRITE\n-\n-    with tf.init_scope():\n-      return super(Metric, self).add_weight(\n-          name=name,\n-          shape=shape,\n-          dtype=self._dtype if dtype is None else dtype,\n-          trainable=False,\n-          initializer=initializer,\n-          collections=[],\n-          synchronization=synchronization,\n-          aggregation=aggregation)\n-\n-  ### End: For use by subclasses ###\n-\n-  @property\n-  def trainable_weights(self):\n-    # Overridden from Layer class to track submetric weights.\n-    if self.trainable:\n-      trainable_weights = self._trainable_weights\n-      for m in self._metrics:\n-        trainable_weights += m.trainable_weights\n-      return self._dedup_weights(trainable_weights)\n-    else:\n-      return []\n-\n-  @property\n-  def non_trainable_weights(self):\n-    # Overridden from Layer class to track submetric weights.\n-    if self.trainable:\n-      non_trainable_weights = self._non_trainable_weights\n-      for m in self._metrics:\n-        non_trainable_weights += m.non_trainable_weights\n-    else:\n-      non_trainable_weights = (\n-          self._non_trainable_weights + self._trainable_weights)\n-      for m in self._metrics:\n-        non_trainable_weights += m.weights\n-    return self._dedup_weights(non_trainable_weights)\n-\n-  @property\n-  def _trackable_saved_model_saver(self):\n-    return metric_serialization.MetricSavedModelSaver(self)\n-\n-  @generic_utils.default\n-  @doc_controls.do_not_generate_docs\n-  def reset_states(self):\n-    # Backwards compatibility alias of `reset_state`. New classes should\n-    # only implement `reset_state`.\n-    return self.reset_state()\n-\n-\n-class Reduce(Metric):\n-  \"\"\"Encapsulates metrics that perform a reduce operation on the values.\n-\n-  Args:\n-    reduction: a `tf.keras.metrics.Reduction` enum value.\n-    name: string name of the metric instance.\n-    dtype: (Optional) data type of the metric result.\n-  \"\"\"\n-\n-  def __init__(self, reduction, name, dtype=None):\n-    super(Reduce, self).__init__(name=name, dtype=dtype)\n-    self.reduction = reduction\n-    self.total = self.add_weight(\n-        'total', initializer='zeros')\n-    if reduction in [metrics_utils.Reduction.SUM_OVER_BATCH_SIZE,\n-                     metrics_utils.Reduction.WEIGHTED_MEAN]:\n-      self.count = self.add_weight(\n-          'count', initializer='zeros')\n-\n-  def update_state(self, values, sample_weight=None):\n-    \"\"\"Accumulates statistics for computing the metric.\n-\n-    Args:\n-      values: Per-example value.\n-      sample_weight: Optional weighting of each example. Defaults to 1.\n-\n-    Returns:\n-      Update op.\n-    \"\"\"\n-    [values], sample_weight = \\\n-        metrics_utils.ragged_assert_compatible_and_get_flat_values(\n-            [values], sample_weight)\n-    try:\n-      values = tf.cast(values, self._dtype)\n-    except (ValueError, TypeError):\n-      msg = ('The output of a metric function can only be a single Tensor. '\n-             f'Received: {values}. ')\n-      if isinstance(values, dict):\n-        msg += ('To return a dict of values, implement a custom Metric '\n-                'subclass.')\n-      raise RuntimeError(msg)\n-    if sample_weight is not None:\n-      sample_weight = tf.cast(sample_weight, self._dtype)\n-      # Update dimensions of weights to match with values if possible.\n-      values, _, sample_weight = losses_utils.squeeze_or_expand_dimensions(\n-          values, sample_weight=sample_weight)\n-      try:\n-        # Broadcast weights if possible.\n-        sample_weight = tf.__internal__.ops.broadcast_weights(\n-            sample_weight, values)\n-      except ValueError:\n-        # Reduce values to same ndim as weight array\n-        ndim = backend.ndim(values)\n-        weight_ndim = backend.ndim(sample_weight)\n-        if self.reduction == metrics_utils.Reduction.SUM:\n-          values = tf.reduce_sum(\n-              values, axis=list(range(weight_ndim, ndim)))\n-        else:\n-          values = tf.reduce_mean(\n-              values, axis=list(range(weight_ndim, ndim)))\n-      values = tf.multiply(values, sample_weight)\n-\n-    value_sum = tf.reduce_sum(values)\n-    with tf.control_dependencies([value_sum]):\n-      update_total_op = self.total.assign_add(value_sum)\n-\n-    # Exit early if the reduction doesn't have a denominator.\n-    if self.reduction == metrics_utils.Reduction.SUM:\n-      return update_total_op\n-\n-    # Update `count` for reductions that require a denominator.\n-    if self.reduction == metrics_utils.Reduction.SUM_OVER_BATCH_SIZE:\n-      num_values = tf.cast(tf.size(values), self._dtype)\n-    elif self.reduction == metrics_utils.Reduction.WEIGHTED_MEAN:\n-      if sample_weight is None:\n-        num_values = tf.cast(tf.size(values), self._dtype)\n-      else:\n-        num_values = tf.reduce_sum(sample_weight)\n-    else:\n-      raise NotImplementedError(\n-          f'Reduction \"{self.reduction}\" not implemented. Expected '\n-          '\"sum\", \"weighted_mean\", or \"sum_over_batch_size\".')\n-\n-    with tf.control_dependencies([update_total_op]):\n-      return self.count.assign_add(num_values)\n-\n-  def result(self):\n-    if self.reduction == metrics_utils.Reduction.SUM:\n-      return tf.identity(self.total)\n-    elif self.reduction in [\n-        metrics_utils.Reduction.WEIGHTED_MEAN,\n-        metrics_utils.Reduction.SUM_OVER_BATCH_SIZE\n-    ]:\n-      return tf.math.divide_no_nan(self.total, self.count)\n-    else:\n-      raise NotImplementedError(\n-          f'Reduction \"{self.reduction}\" not implemented. Expected '\n-          '\"sum\", \"weighted_mean\", or \"sum_over_batch_size\".')\n-\n-\n-@keras_export('keras.metrics.Sum')\n-class Sum(Reduce):\n-  \"\"\"Computes the (weighted) sum of the given values.\n-\n-  For example, if values is [1, 3, 5, 7] then the sum is 16.\n-  If the weights were specified as [1, 1, 0, 0] then the sum would be 4.\n-\n-  This metric creates one variable, `total`, that is used to compute the sum of\n-  `values`. This is ultimately returned as `sum`.\n-\n-  If `sample_weight` is `None`, weights default to 1.  Use `sample_weight` of 0\n-  to mask values.\n-\n-  Args:\n-    name: (Optional) string name of the metric instance.\n-    dtype: (Optional) data type of the metric result.\n-\n-  Standalone usage:\n-\n-  >>> m = tf.keras.metrics.Sum()\n-  >>> m.update_state([1, 3, 5, 7])\n-  >>> m.result().numpy()\n-  16.0\n-\n-  Usage with `compile()` API:\n-\n-  ```python\n-  model.add_metric(tf.keras.metrics.Sum(name='sum_1')(outputs))\n-  model.compile(optimizer='sgd', loss='mse')\n-  ```\n-  \"\"\"\n-\n-  def __init__(self, name='sum', dtype=None):\n-    super(Sum, self).__init__(reduction=metrics_utils.Reduction.SUM,\n-                              name=name, dtype=dtype)\n-\n-\n-@keras_export('keras.metrics.Mean')\n-class Mean(Reduce):\n-  \"\"\"Computes the (weighted) mean of the given values.\n-\n-  For example, if values is [1, 3, 5, 7] then the mean is 4.\n-  If the weights were specified as [1, 1, 0, 0] then the mean would be 2.\n-\n-  This metric creates two variables, `total` and `count` that are used to\n-  compute the average of `values`. This average is ultimately returned as `mean`\n-  which is an idempotent operation that simply divides `total` by `count`.\n-\n-  If `sample_weight` is `None`, weights default to 1.\n-  Use `sample_weight` of 0 to mask values.\n-\n-  Args:\n-    name: (Optional) string name of the metric instance.\n-    dtype: (Optional) data type of the metric result.\n-\n-  Standalone usage:\n-\n-  >>> m = tf.keras.metrics.Mean()\n-  >>> m.update_state([1, 3, 5, 7])\n-  >>> m.result().numpy()\n-  4.0\n-  >>> m.reset_state()\n-  >>> m.update_state([1, 3, 5, 7], sample_weight=[1, 1, 0, 0])\n-  >>> m.result().numpy()\n-  2.0\n-\n-  Usage with `compile()` API:\n-\n-  ```python\n-  model.add_metric(tf.keras.metrics.Mean(name='mean_1')(outputs))\n-  model.compile(optimizer='sgd', loss='mse')\n-  ```\n-  \"\"\"\n-\n-  def __init__(self, name='mean', dtype=None):\n-    super(Mean, self).__init__(\n-        reduction=metrics_utils.Reduction.WEIGHTED_MEAN, name=name, dtype=dtype)\n \n \n @keras_export('keras.metrics.MeanRelativeError')\n-class MeanRelativeError(Mean):\n+class MeanRelativeError(base_metric.Mean):\n   \"\"\"Computes the mean relative error by normalizing with the given values.\n \n   This metric creates two local variables, `total` and `count` that are used to\n@@ -687,98 +125,8 @@ class MeanRelativeError(Mean):\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n-@keras_export('keras.metrics.MeanMetricWrapper')\n-class MeanMetricWrapper(Mean):\n-  \"\"\"Wraps a stateless metric function with the Mean metric.\n-\n-  You could use this class to quickly build a mean metric from a function. The\n-  function needs to have the signature `fn(y_true, y_pred)` and return a\n-  per-sample loss array. `MeanMetricWrapper.result()` will return\n-  the average metric value across all samples seen so far.\n-\n-  For example:\n-\n-  ```python\n-  def accuracy(y_true, y_pred):\n-    return tf.cast(tf.math.equal(y_true, y_pred), tf.float32)\n-\n-  accuracy_metric = tf.keras.metrics.MeanMetricWrapper(fn=accuracy)\n-\n-  keras_model.compile(..., metrics=accuracy_metric)\n-  ```\n-\n-  Args:\n-    fn: The metric function to wrap, with signature `fn(y_true, y_pred,\n-      **kwargs)`.\n-    name: (Optional) string name of the metric instance.\n-    dtype: (Optional) data type of the metric result.\n-    **kwargs: Keyword arguments to pass on to `fn`.\n-  \"\"\"\n-\n-  def __init__(self, fn, name=None, dtype=None, **kwargs):\n-    super(MeanMetricWrapper, self).__init__(name=name, dtype=dtype)\n-    self._fn = fn\n-    self._fn_kwargs = kwargs\n-\n-  def update_state(self, y_true, y_pred, sample_weight=None):\n-    \"\"\"Accumulates metric statistics.\n-\n-    `y_true` and `y_pred` should have the same shape.\n-\n-    Args:\n-      y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n-      y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n-      sample_weight: Optional `sample_weight` acts as a\n-        coefficient for the metric. If a scalar is provided, then the metric is\n-        simply scaled by the given value. If `sample_weight` is a tensor of size\n-        `[batch_size]`, then the metric for each sample of the batch is rescaled\n-        by the corresponding element in the `sample_weight` vector. If the shape\n-        of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted\n-        to this shape), then each metric element of `y_pred` is scaled by the\n-        corresponding value of `sample_weight`. (Note on `dN-1`: all metric\n-        functions reduce by 1 dimension, usually the last axis (-1)).\n-\n-    Returns:\n-      Update op.\n-    \"\"\"\n-    y_true = tf.cast(y_true, self._dtype)\n-    y_pred = tf.cast(y_pred, self._dtype)\n-    [y_true, y_pred], sample_weight = (\n-        metrics_utils.ragged_assert_compatible_and_get_flat_values(\n-            [y_true, y_pred], sample_weight))\n-    y_pred, y_true = losses_utils.squeeze_or_expand_dimensions(\n-        y_pred, y_true)\n-\n-    ag_fn = tf.__internal__.autograph.tf_convert(self._fn, tf.__internal__.autograph.control_status_ctx())\n-    matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n-    return super(MeanMetricWrapper, self).update_state(\n-        matches, sample_weight=sample_weight)\n-\n-  def get_config(self):\n-    config = {}\n-\n-    if type(self) is MeanMetricWrapper:  # pylint: disable=unidiomatic-typecheck\n-      # Only include function argument when the object is a MeanMetricWrapper\n-      # and not a subclass.\n-      config['fn'] = self._fn\n-\n-    for k, v in self._fn_kwargs.items():\n-      config[k] = backend.eval(v) if is_tensor_or_variable(v) else v\n-    base_config = super(MeanMetricWrapper, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-  @classmethod\n-  def from_config(cls, config):\n-    # Note that while MeanMetricWrapper itself isn't public, objects of this\n-    # class may be created and added to the model by calling model.compile.\n-    fn = config.pop('fn', None)\n-    if cls is MeanMetricWrapper:\n-      return cls(get(fn), **config)\n-    return super(MeanMetricWrapper, cls).from_config(config)\n-\n-\n @keras_export('keras.metrics.Accuracy')\n-class Accuracy(MeanMetricWrapper):\n+class Accuracy(base_metric.MeanMetricWrapper):\n   \"\"\"Calculates how often predictions equal labels.\n \n   This metric creates two local variables, `total` and `count` that are used to\n@@ -820,7 +168,7 @@ class Accuracy(MeanMetricWrapper):\n \n \n @keras_export('keras.metrics.BinaryAccuracy')\n-class BinaryAccuracy(MeanMetricWrapper):\n+class BinaryAccuracy(base_metric.MeanMetricWrapper):\n   \"\"\"Calculates how often predictions match binary labels.\n \n   This metric creates two local variables, `total` and `count` that are used to\n@@ -865,7 +213,7 @@ class BinaryAccuracy(MeanMetricWrapper):\n \n \n @keras_export('keras.metrics.CategoricalAccuracy')\n-class CategoricalAccuracy(MeanMetricWrapper):\n+class CategoricalAccuracy(base_metric.MeanMetricWrapper):\n   \"\"\"Calculates how often predictions match one-hot labels.\n \n   You can provide logits of classes as `y_pred`, since argmax of\n@@ -917,7 +265,7 @@ class CategoricalAccuracy(MeanMetricWrapper):\n \n \n @keras_export('keras.metrics.SparseCategoricalAccuracy')\n-class SparseCategoricalAccuracy(MeanMetricWrapper):\n+class SparseCategoricalAccuracy(base_metric.MeanMetricWrapper):\n   \"\"\"Calculates how often predictions match integer labels.\n \n   ```python\n@@ -967,11 +315,34 @@ class SparseCategoricalAccuracy(MeanMetricWrapper):\n         sparse_categorical_accuracy, name, dtype=dtype)\n \n \n+_SPARSE_CATEGORICAL_UPDATE_STATE_DOCSTRING = \"\"\"Accumulates metric statistics.\n+\n+For sparse categorical metrics, the shapes of `y_true` and `y_pred` are\n+different.\n+\n+Args:\n+  y_true: Ground truth label values. shape = `[batch_size, d0, .. dN-1]` or\n+    shape = `[batch_size, d0, .. dN-1, 1]`.\n+  y_pred: The predicted probability values. shape = `[batch_size, d0, .. dN]`.\n+  sample_weight: Optional `sample_weight` acts as a\n+    coefficient for the metric. If a scalar is provided, then the metric is\n+    simply scaled by the given value. If `sample_weight` is a tensor of size\n+    `[batch_size]`, then the metric for each sample of the batch is rescaled\n+    by the corresponding element in the `sample_weight` vector. If the shape\n+    of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted\n+    to this shape), then each metric element of `y_pred` is scaled by the\n+    corresponding value of `sample_weight`. (Note on `dN-1`: all metric\n+    functions reduce by 1 dimension, usually the last axis (-1)).\n+\n+Returns:\n+  Update op.\n+\"\"\"\n+\n SparseCategoricalAccuracy.update_state.__doc__ = _SPARSE_CATEGORICAL_UPDATE_STATE_DOCSTRING\n \n \n @keras_export('keras.metrics.TopKCategoricalAccuracy')\n-class TopKCategoricalAccuracy(MeanMetricWrapper):\n+class TopKCategoricalAccuracy(base_metric.MeanMetricWrapper):\n   \"\"\"Computes how often targets are in the top `K` predictions.\n \n   Args:\n@@ -1010,7 +381,7 @@ class TopKCategoricalAccuracy(MeanMetricWrapper):\n \n \n @keras_export('keras.metrics.SparseTopKCategoricalAccuracy')\n-class SparseTopKCategoricalAccuracy(MeanMetricWrapper):\n+class SparseTopKCategoricalAccuracy(base_metric.MeanMetricWrapper):\n   \"\"\"Computes how often integer targets are in the top `K` predictions.\n \n   Args:\n@@ -1050,7 +421,7 @@ class SparseTopKCategoricalAccuracy(MeanMetricWrapper):\n SparseTopKCategoricalAccuracy.update_state.__doc__ = _SPARSE_CATEGORICAL_UPDATE_STATE_DOCSTRING\n \n \n-class _ConfusionMatrixConditionCount(Metric):\n+class _ConfusionMatrixConditionCount(base_metric.Metric):\n   \"\"\"Calculates the number of the given confusion matrix condition.\n \n   Args:\n@@ -1317,7 +688,7 @@ class TruePositives(_ConfusionMatrixConditionCount):\n \n \n @keras_export('keras.metrics.Precision')\n-class Precision(Metric):\n+class Precision(base_metric.Metric):\n   \"\"\"Computes the precision of the predictions with respect to the labels.\n \n   The metric creates two local variables, `true_positives` and `false_positives`\n@@ -1460,7 +831,7 @@ class Precision(Metric):\n \n \n @keras_export('keras.metrics.Recall')\n-class Recall(Metric):\n+class Recall(base_metric.Metric):\n   \"\"\"Computes the recall of the predictions with respect to the labels.\n \n   This metric creates two local variables, `true_positives` and\n@@ -1589,7 +960,7 @@ class Recall(Metric):\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n-class SensitivitySpecificityBase(Metric, metaclass=abc.ABCMeta):\n+class SensitivitySpecificityBase(base_metric.Metric, metaclass=abc.ABCMeta):\n   \"\"\"Abstract base class for computing sensitivity and specificity.\n \n   For additional information about specificity and sensitivity, see\n@@ -2067,7 +1438,7 @@ class RecallAtPrecision(SensitivitySpecificityBase):\n \n \n @keras_export('keras.metrics.AUC')\n-class AUC(Metric):\n+class AUC(base_metric.Metric):\n   \"\"\"Approximates the AUC (Area under the curve) of the ROC or PR curves.\n \n   The AUC (Area under the curve) of the ROC (Receiver operating\n@@ -2494,8 +1865,7 @@ class AUC(Metric):\n \n     # Sum up the areas of all the rectangles.\n     if self.multi_label:\n-      riemann_terms = tf.multiply(x[:self.num_thresholds - 1] - x[1:],\n-                                        heights)\n+      riemann_terms = tf.multiply(x[:self.num_thresholds - 1] - x[1:], heights)\n       by_label_auc = tf.reduce_sum(\n           riemann_terms, name=self.name + '_by_label', axis=0)\n \n@@ -2549,7 +1919,7 @@ class AUC(Metric):\n \n \n @keras_export('keras.metrics.CosineSimilarity')\n-class CosineSimilarity(MeanMetricWrapper):\n+class CosineSimilarity(base_metric.MeanMetricWrapper):\n   \"\"\"Computes the cosine similarity between the labels and predictions.\n \n   `cosine similarity = (a . b) / ||a|| ||b||`\n@@ -2599,7 +1969,7 @@ class CosineSimilarity(MeanMetricWrapper):\n \n \n @keras_export('keras.metrics.MeanAbsoluteError')\n-class MeanAbsoluteError(MeanMetricWrapper):\n+class MeanAbsoluteError(base_metric.MeanMetricWrapper):\n   \"\"\"Computes the mean absolute error between the labels and predictions.\n \n   Args:\n@@ -2635,7 +2005,7 @@ class MeanAbsoluteError(MeanMetricWrapper):\n \n \n @keras_export('keras.metrics.MeanAbsolutePercentageError')\n-class MeanAbsolutePercentageError(MeanMetricWrapper):\n+class MeanAbsolutePercentageError(base_metric.MeanMetricWrapper):\n   \"\"\"Computes the mean absolute percentage error between `y_true` and `y_pred`.\n \n   Args:\n@@ -2671,7 +2041,7 @@ class MeanAbsolutePercentageError(MeanMetricWrapper):\n \n \n @keras_export('keras.metrics.MeanSquaredError')\n-class MeanSquaredError(MeanMetricWrapper):\n+class MeanSquaredError(base_metric.MeanMetricWrapper):\n   \"\"\"Computes the mean squared error between `y_true` and `y_pred`.\n \n   Args:\n@@ -2707,7 +2077,7 @@ class MeanSquaredError(MeanMetricWrapper):\n \n \n @keras_export('keras.metrics.MeanSquaredLogarithmicError')\n-class MeanSquaredLogarithmicError(MeanMetricWrapper):\n+class MeanSquaredLogarithmicError(base_metric.MeanMetricWrapper):\n   \"\"\"Computes the mean squared logarithmic error between `y_true` and `y_pred`.\n \n   Args:\n@@ -2743,7 +2113,7 @@ class MeanSquaredLogarithmicError(MeanMetricWrapper):\n \n \n @keras_export('keras.metrics.Hinge')\n-class Hinge(MeanMetricWrapper):\n+class Hinge(base_metric.MeanMetricWrapper):\n   \"\"\"Computes the hinge metric between `y_true` and `y_pred`.\n \n   `y_true` values are expected to be -1 or 1. If binary (0 or 1) labels are\n@@ -2778,7 +2148,7 @@ class Hinge(MeanMetricWrapper):\n \n \n @keras_export('keras.metrics.SquaredHinge')\n-class SquaredHinge(MeanMetricWrapper):\n+class SquaredHinge(base_metric.MeanMetricWrapper):\n   \"\"\"Computes the squared hinge metric between `y_true` and `y_pred`.\n \n   `y_true` values are expected to be -1 or 1. If binary (0 or 1) labels are\n@@ -2816,7 +2186,7 @@ class SquaredHinge(MeanMetricWrapper):\n \n \n @keras_export('keras.metrics.CategoricalHinge')\n-class CategoricalHinge(MeanMetricWrapper):\n+class CategoricalHinge(base_metric.MeanMetricWrapper):\n   \"\"\"Computes the categorical hinge metric between `y_true` and `y_pred`.\n \n   Args:\n@@ -2851,7 +2221,7 @@ class CategoricalHinge(MeanMetricWrapper):\n \n \n @keras_export('keras.metrics.RootMeanSquaredError')\n-class RootMeanSquaredError(Mean):\n+class RootMeanSquaredError(base_metric.Mean):\n   \"\"\"Computes root mean squared error metric between `y_true` and `y_pred`.\n \n   Standalone usage:\n@@ -2906,7 +2276,7 @@ class RootMeanSquaredError(Mean):\n \n \n @keras_export('keras.metrics.LogCoshError')\n-class LogCoshError(MeanMetricWrapper):\n+class LogCoshError(base_metric.MeanMetricWrapper):\n   \"\"\"Computes the logarithm of the hyperbolic cosine of the prediction error.\n \n   `logcosh = log((exp(x) + exp(-x))/2)`, where x is the error (y_pred - y_true)\n@@ -2942,7 +2312,7 @@ class LogCoshError(MeanMetricWrapper):\n \n \n @keras_export('keras.metrics.Poisson')\n-class Poisson(MeanMetricWrapper):\n+class Poisson(base_metric.MeanMetricWrapper):\n   \"\"\"Computes the Poisson metric between `y_true` and `y_pred`.\n \n   `metric = y_pred - y_true * log(y_pred)`\n@@ -2978,7 +2348,7 @@ class Poisson(MeanMetricWrapper):\n \n \n @keras_export('keras.metrics.KLDivergence')\n-class KLDivergence(MeanMetricWrapper):\n+class KLDivergence(base_metric.MeanMetricWrapper):\n   \"\"\"Computes Kullback-Leibler divergence metric between `y_true` and `y_pred`.\n \n   `metric = y_true * log(y_true / y_pred)`\n@@ -3014,7 +2384,7 @@ class KLDivergence(MeanMetricWrapper):\n         kullback_leibler_divergence, name, dtype=dtype)\n \n \n-class _IoUBase(Metric):\n+class _IoUBase(base_metric.Metric):\n   \"\"\"Computes the confusion matrix for Intersection-Over-Union metrics.\n \n   Intersection-Over-Union is a common evaluation metric for semantic image\n@@ -3618,134 +2988,8 @@ class OneHotMeanIoU(MeanIoU):\n     return super().update_state(y_true, y_pred, sample_weight)\n \n \n-@keras_export('keras.metrics.MeanTensor')\n-class MeanTensor(Metric):\n-  \"\"\"Computes the element-wise (weighted) mean of the given tensors.\n-\n-  `MeanTensor` returns a tensor with the same shape of the input tensors. The\n-  mean value is updated by keeping local variables `total` and `count`. The\n-  `total` tracks the sum of the weighted values, and `count` stores the sum of\n-  the weighted counts.\n-\n-  Args:\n-    name: (Optional) string name of the metric instance.\n-    dtype: (Optional) data type of the metric result.\n-    shape: (Optional) A list of integers, a tuple of integers, or a 1-D Tensor\n-      of type int32. If not specified, the shape is inferred from the values at\n-      the first call of update_state.\n-\n-  Standalone usage:\n-\n-  >>> m = tf.keras.metrics.MeanTensor()\n-  >>> m.update_state([0, 1, 2, 3])\n-  >>> m.update_state([4, 5, 6, 7])\n-  >>> m.result().numpy()\n-  array([2., 3., 4., 5.], dtype=float32)\n-\n-  >>> m.update_state([12, 10, 8, 6], sample_weight= [0, 0.2, 0.5, 1])\n-  >>> m.result().numpy()\n-  array([2.       , 3.6363635, 4.8      , 5.3333335], dtype=float32)\n-\n-  >>> m = tf.keras.metrics.MeanTensor(dtype=tf.float64, shape=(1, 4))\n-  >>> m.result().numpy()\n-  array([[0., 0., 0., 0.]])\n-  >>> m.update_state([[0, 1, 2, 3]])\n-  >>> m.update_state([[4, 5, 6, 7]])\n-  >>> m.result().numpy()\n-  array([[2., 3., 4., 5.]])\n-  \"\"\"\n-\n-  def __init__(self, name='mean_tensor', dtype=None, shape=None):\n-    super(MeanTensor, self).__init__(name=name, dtype=dtype)\n-    self._shape = None\n-    self._total = None\n-    self._count = None\n-    self._built = False\n-    if shape is not None:\n-      self._build(shape)\n-\n-  def _build(self, shape):\n-    self._shape = tf.TensorShape(shape)\n-    self._build_input_shape = self._shape\n-    # Create new state variables\n-    self._total = self.add_weight(\n-        name='total', shape=shape, initializer='zeros')\n-    self._count = self.add_weight(\n-        name='count', shape=shape, initializer='zeros')\n-    with tf.init_scope():\n-      if not tf.executing_eagerly():\n-        backend._initialize_variables(backend._get_session())  # pylint: disable=protected-access\n-    self._built = True\n-\n-  @property\n-  def total(self):\n-    return self._total if self._built else None\n-\n-  @property\n-  def count(self):\n-    return self._count if self._built else None\n-\n-  def update_state(self, values, sample_weight=None):\n-    \"\"\"Accumulates statistics for computing the element-wise mean.\n-\n-    Args:\n-      values: Per-example value.\n-      sample_weight: Optional weighting of each example. Defaults to 1.\n-\n-    Returns:\n-      Update op.\n-    \"\"\"\n-    values = tf.cast(values, self._dtype)\n-    if not self._built:\n-      self._build(values.shape)\n-    elif values.shape != self._shape:\n-      raise ValueError(\n-          'MeanTensor input values must always have the same '\n-          f'shape. Expected shape (set during the first call): {self._shape}. '\n-          f'Got: {values.shape}.')\n-\n-    num_values = tf.ones_like(values)\n-    if sample_weight is not None:\n-      sample_weight = tf.cast(sample_weight, self._dtype)\n-\n-      # Update dimensions of weights to match with values if possible.\n-      values, _, sample_weight = losses_utils.squeeze_or_expand_dimensions(\n-          values, sample_weight=sample_weight)\n-      try:\n-        # Broadcast weights if possible.\n-        sample_weight = tf.__internal__.ops.broadcast_weights(\n-            sample_weight, values)\n-      except ValueError:\n-        # Reduce values to same ndim as weight array\n-        ndim = backend.ndim(values)\n-        weight_ndim = backend.ndim(sample_weight)\n-        values = tf.reduce_mean(\n-            values, axis=list(range(weight_ndim, ndim)))\n-\n-      num_values = tf.multiply(num_values, sample_weight)\n-      values = tf.multiply(values, sample_weight)\n-\n-    update_total_op = self._total.assign_add(values)\n-    with tf.control_dependencies([update_total_op]):\n-      return self._count.assign_add(num_values)\n-\n-  def result(self):\n-    if not self._built:\n-      raise ValueError(\n-          'MeanTensor does not have any value yet. Please call the MeanTensor '\n-          'instance or use `.update_state(value)` before retrieving the result.'\n-          )\n-    return tf.math.divide_no_nan(self.total, self.count)\n-\n-  def reset_state(self):\n-    if self._built:\n-      backend.batch_set_value([\n-          (v, np.zeros(v.shape.as_list())) for v in self.variables\n-      ])\n-\n-\n @keras_export('keras.metrics.BinaryCrossentropy')\n-class BinaryCrossentropy(MeanMetricWrapper):\n+class BinaryCrossentropy(base_metric.MeanMetricWrapper):\n   \"\"\"Computes the crossentropy metric between the labels and predictions.\n \n   This is the crossentropy metric class to be used when there are only two\n@@ -3798,7 +3042,7 @@ class BinaryCrossentropy(MeanMetricWrapper):\n \n \n @keras_export('keras.metrics.CategoricalCrossentropy')\n-class CategoricalCrossentropy(MeanMetricWrapper):\n+class CategoricalCrossentropy(base_metric.MeanMetricWrapper):\n   \"\"\"Computes the crossentropy metric between the labels and predictions.\n \n   This is the crossentropy metric class to be used when there are multiple\n@@ -3862,7 +3106,7 @@ class CategoricalCrossentropy(MeanMetricWrapper):\n \n \n @keras_export('keras.metrics.SparseCategoricalCrossentropy')\n-class SparseCategoricalCrossentropy(MeanMetricWrapper):\n+class SparseCategoricalCrossentropy(base_metric.MeanMetricWrapper):\n   \"\"\"Computes the crossentropy metric between the labels and predictions.\n \n   Use this crossentropy metric when there are two or more label classes.\n@@ -3935,64 +3179,6 @@ class SparseCategoricalCrossentropy(MeanMetricWrapper):\n SparseCategoricalCrossentropy.update_state.__doc__ = _SPARSE_CATEGORICAL_UPDATE_STATE_DOCSTRING\n \n \n-class SumOverBatchSize(Reduce):\n-  \"\"\"Computes the weighted sum over batch size of the given values.\n-\n-  For example, if values is [1, 3, 5, 7] then the metric value is 4.\n-  If the weights were specified as [1, 1, 0, 0] then the value would be 1.\n-\n-  This metric creates two variables, `total` and `count` that are used to\n-  compute the average of `values`. This average is ultimately returned as sum\n-  over batch size which is an idempotent operation that simply divides `total`\n-  by `count`.\n-\n-  If `sample_weight` is `None`, weights default to 1.  Use `sample_weight` of 0\n-  to mask values.\n-  \"\"\"\n-\n-  def __init__(self, name='sum_over_batch_size', dtype=None):\n-    super(SumOverBatchSize, self).__init__(\n-        reduction=metrics_utils.Reduction.SUM_OVER_BATCH_SIZE,\n-        name=name,\n-        dtype=dtype)\n-\n-\n-class SumOverBatchSizeMetricWrapper(SumOverBatchSize):\n-  \"\"\"Wraps a function with the `SumOverBatchSizeMetricWrapper` metric.\"\"\"\n-\n-  def __init__(self, fn, name=None, dtype=None, **kwargs):\n-    \"\"\"Creates a `SumOverBatchSizeMetricWrapper` instance.\n-\n-    Args:\n-      fn: The metric function to wrap, with signature `fn(y_true, y_pred,\n-        **kwargs)`.\n-      name: (Optional) string name of the metric instance.\n-      dtype: (Optional) data type of the metric result.\n-      **kwargs: The keyword arguments that are passed on to `fn`.\n-    \"\"\"\n-    super(SumOverBatchSizeMetricWrapper, self).__init__(name=name, dtype=dtype)\n-    self._fn = fn\n-    self._fn_kwargs = kwargs\n-\n-  def update_state(self, y_true, y_pred, sample_weight=None):\n-    y_true = tf.cast(y_true, self._dtype)\n-    y_pred = tf.cast(y_pred, self._dtype)\n-    y_pred, y_true = losses_utils.squeeze_or_expand_dimensions(\n-        y_pred, y_true)\n-\n-    ag_fn = tf.__internal__.autograph.tf_convert(self._fn, tf.__internal__.autograph.control_status_ctx())\n-    matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n-    return super(SumOverBatchSizeMetricWrapper, self).update_state(\n-        matches, sample_weight=sample_weight)\n-\n-  def get_config(self):\n-    config = {}\n-    for k, v in self._fn_kwargs.items():\n-      config[k] = backend.eval(v) if is_tensor_or_variable(v) else v\n-    base_config = super(SumOverBatchSizeMetricWrapper, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-\n def accuracy(y_true, y_pred):\n   [y_pred, y_true], _ = \\\n       metrics_utils.ragged_assert_compatible_and_get_flat_values(\n@@ -4167,7 +3353,7 @@ def sparse_top_k_categorical_accuracy(y_true, y_pred, k=5):\n       dtype=backend.floatx())\n \n \n-def cosine_proximity(y_true, y_pred, axis=-1):\n+def cosine_similarity(y_true, y_pred, axis=-1):\n   \"\"\"Computes the cosine similarity between labels and predictions.\n \n   Args:\n@@ -4182,108 +3368,3 @@ def cosine_proximity(y_true, y_pred, axis=-1):\n   y_true = tf.linalg.l2_normalize(y_true, axis=axis)\n   y_pred = tf.linalg.l2_normalize(y_pred, axis=axis)\n   return tf.reduce_sum(y_true * y_pred, axis=axis)\n-\n-# Aliases\n-\n-acc = ACC = accuracy\n-bce = BCE = binary_crossentropy\n-mse = MSE = mean_squared_error\n-mae = MAE = mean_absolute_error\n-mape = MAPE = mean_absolute_percentage_error\n-msle = MSLE = mean_squared_logarithmic_error\n-cosine_similarity = cosine_proximity\n-log_cosh = logcosh\n-\n-\n-def clone_metric(metric):\n-  \"\"\"Returns a clone of the metric if stateful, otherwise returns it as is.\"\"\"\n-  if isinstance(metric, Metric):\n-    with tf.init_scope():\n-      return metric.__class__.from_config(metric.get_config())\n-  return metric\n-\n-\n-def clone_metrics(metrics):\n-  \"\"\"Clones the given metric list/dict.\"\"\"\n-  return tf.nest.map_structure(clone_metric, metrics)\n-\n-\n-@keras_export('keras.metrics.serialize')\n-def serialize(metric):\n-  \"\"\"Serializes metric function or `Metric` instance.\n-\n-  Args:\n-    metric: A Keras `Metric` instance or a metric function.\n-\n-  Returns:\n-    Metric configuration dictionary.\n-  \"\"\"\n-  return serialize_keras_object(metric)\n-\n-\n-@keras_export('keras.metrics.deserialize')\n-def deserialize(config, custom_objects=None):\n-  \"\"\"Deserializes a serialized metric class/function instance.\n-\n-  Args:\n-    config: Metric configuration.\n-    custom_objects: Optional dictionary mapping names (strings) to custom\n-      objects (classes and functions) to be considered during deserialization.\n-\n-  Returns:\n-      A Keras `Metric` instance or a metric function.\n-  \"\"\"\n-  return deserialize_keras_object(\n-      config,\n-      module_objects=globals(),\n-      custom_objects=custom_objects,\n-      printable_module_name='metric function')\n-\n-\n-@keras_export('keras.metrics.get')\n-def get(identifier):\n-  \"\"\"Retrieves a Keras metric as a `function`/`Metric` class instance.\n-\n-  The `identifier` may be the string name of a metric function or class.\n-\n-  >>> metric = tf.keras.metrics.get(\"categorical_crossentropy\")\n-  >>> type(metric)\n-  <class 'function'>\n-  >>> metric = tf.keras.metrics.get(\"CategoricalCrossentropy\")\n-  >>> type(metric)\n-  <class '...keras.metrics.CategoricalCrossentropy'>\n-\n-  You can also specify `config` of the metric to this function by passing dict\n-  containing `class_name` and `config` as an identifier. Also note that the\n-  `class_name` must map to a `Metric` class\n-\n-  >>> identifier = {\"class_name\": \"CategoricalCrossentropy\",\n-  ...               \"config\": {\"from_logits\": True}}\n-  >>> metric = tf.keras.metrics.get(identifier)\n-  >>> type(metric)\n-  <class '...keras.metrics.CategoricalCrossentropy'>\n-\n-  Args:\n-    identifier: A metric identifier. One of None or string name of a metric\n-      function/class or metric configuration dictionary or a metric function or\n-      a metric class instance\n-\n-  Returns:\n-    A Keras metric as a `function`/ `Metric` class instance.\n-\n-  Raises:\n-    ValueError: If `identifier` cannot be interpreted.\n-  \"\"\"\n-  if isinstance(identifier, dict):\n-    return deserialize(identifier)\n-  elif isinstance(identifier, str):\n-    return deserialize(str(identifier))\n-  elif callable(identifier):\n-    return identifier\n-  else:\n-    raise ValueError(\n-        f'Could not interpret metric identifier: {identifier}')\n-\n-\n-def is_built_in(cls):\n-  return cls.__module__ == Metric.__module__\n\n\n\n@@ -12,358 +12,21 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Tests for Keras metrics functions.\"\"\"\n+\"\"\"Tests for Keras metrics.\"\"\"\n \n-\n-import copy\n import json\n import math\n-import os\n \n-from absl.testing import parameterized\n from keras import backend\n from keras import combinations\n from keras import keras_parameterized\n from keras import layers\n from keras import metrics\n-from keras import Model\n from keras import testing_utils\n-from keras.engine import base_layer\n-from keras.engine import training as training_module\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-class KerasSumTest(tf.test.TestCase, parameterized.TestCase):\n-\n-  def test_sum(self):\n-    with self.test_session():\n-      m = metrics.Sum(name='my_sum')\n-\n-      # check config\n-      self.assertEqual(m.name, 'my_sum')\n-      self.assertTrue(m.stateful)\n-      self.assertEqual(m.dtype, tf.float32)\n-      self.assertLen(m.variables, 1)\n-      self.evaluate(tf.compat.v1.variables_initializer(m.variables))\n-\n-      # check initial state\n-      self.assertEqual(self.evaluate(m.total), 0)\n-\n-      # check __call__()\n-      self.assertEqual(self.evaluate(m(100)), 100)\n-      self.assertEqual(self.evaluate(m.total), 100)\n-\n-      # check update_state() and result() + state accumulation + tensor input\n-      update_op = m.update_state(tf.convert_to_tensor([1, 5]))\n-      self.evaluate(update_op)\n-      self.assertAlmostEqual(self.evaluate(m.result()), 106)\n-      self.assertEqual(self.evaluate(m.total), 106)  # 100 + 1 + 5\n-\n-      # check reset_state()\n-      m.reset_state()\n-      self.assertEqual(self.evaluate(m.total), 0)\n-\n-  def test_sum_with_sample_weight(self):\n-    m = metrics.Sum(dtype=tf.float64)\n-    self.assertEqual(m.dtype, tf.float64)\n-    self.evaluate(tf.compat.v1.variables_initializer(m.variables))\n-\n-    # check scalar weight\n-    result_t = m(100, sample_weight=0.5)\n-    self.assertEqual(self.evaluate(result_t), 50)\n-    self.assertEqual(self.evaluate(m.total), 50)\n-\n-    # check weights not scalar and weights rank matches values rank\n-    result_t = m([1, 5], sample_weight=[1, 0.2])\n-    result = self.evaluate(result_t)\n-    self.assertAlmostEqual(result, 52., 4)  # 50 + 1 + 5 * 0.2\n-    self.assertAlmostEqual(self.evaluate(m.total), 52., 4)\n-\n-    # check weights broadcast\n-    result_t = m([1, 2], sample_weight=0.5)\n-    self.assertAlmostEqual(self.evaluate(result_t), 53.5, 1)  # 52 + 0.5 + 1\n-    self.assertAlmostEqual(self.evaluate(m.total), 53.5, 1)\n-\n-    # check weights squeeze\n-    result_t = m([1, 5], sample_weight=[[1], [0.2]])\n-    self.assertAlmostEqual(self.evaluate(result_t), 55.5, 1)  # 53.5 + 1 + 1\n-    self.assertAlmostEqual(self.evaluate(m.total), 55.5, 1)\n-\n-    # check weights expand\n-    result_t = m([[1], [5]], sample_weight=[1, 0.2])\n-    self.assertAlmostEqual(self.evaluate(result_t), 57.5, 2)  # 55.5 + 1 + 1\n-    self.assertAlmostEqual(self.evaluate(m.total), 57.5, 1)\n-\n-    # check values reduced to the dimensions of weight\n-    result_t = m([[[1., 2.], [3., 2.], [0.5, 4.]]], sample_weight=[0.5])\n-    result = np.round(self.evaluate(result_t), decimals=2)\n-    # result = (prev: 57.5) + 0.5 + 1 + 1.5 + 1 + 0.25 + 2\n-    self.assertAlmostEqual(result, 63.75, 2)\n-    self.assertAlmostEqual(self.evaluate(m.total), 63.75, 2)\n-\n-  def test_sum_graph_with_placeholder(self):\n-    with tf.compat.v1.get_default_graph().as_default(), self.cached_session() as sess:\n-      m = metrics.Sum()\n-      v = tf.compat.v1.placeholder(tf.float32)\n-      w = tf.compat.v1.placeholder(tf.float32)\n-      self.evaluate(tf.compat.v1.variables_initializer(m.variables))\n-\n-      # check __call__()\n-      result_t = m(v, sample_weight=w)\n-      result = sess.run(result_t, feed_dict=({v: 100, w: 0.5}))\n-      self.assertEqual(result, 50)\n-      self.assertEqual(self.evaluate(m.total), 50)\n-\n-      # check update_state() and result()\n-      result = sess.run(result_t, feed_dict=({v: [1, 5], w: [1, 0.2]}))\n-      self.assertAlmostEqual(result, 52., 2)  # 50 + 1 + 5 * 0.2\n-      self.assertAlmostEqual(self.evaluate(m.total), 52., 2)\n-\n-  def test_save_restore(self):\n-    with self.test_session():\n-      checkpoint_directory = self.get_temp_dir()\n-      checkpoint_prefix = os.path.join(checkpoint_directory, 'ckpt')\n-      m = metrics.Sum()\n-      checkpoint = tf.train.Checkpoint(sum=m)\n-      self.evaluate(tf.compat.v1.variables_initializer(m.variables))\n-\n-      # update state\n-      self.evaluate(m(100.))\n-      self.evaluate(m(200.))\n-\n-      # save checkpoint and then add an update\n-      save_path = checkpoint.save(checkpoint_prefix)\n-      self.evaluate(m(1000.))\n-\n-      # restore to the same checkpoint sum object (= 300)\n-      checkpoint.restore(save_path).assert_consumed().run_restore_ops()\n-      self.evaluate(m(300.))\n-      self.assertEqual(600., self.evaluate(m.result()))\n-\n-      # restore to a different checkpoint sum object\n-      restore_sum = metrics.Sum()\n-      restore_checkpoint = tf.train.Checkpoint(sum=restore_sum)\n-      status = restore_checkpoint.restore(save_path)\n-      restore_update = restore_sum(300.)\n-      status.assert_consumed().run_restore_ops()\n-      self.evaluate(restore_update)\n-      self.assertEqual(600., self.evaluate(restore_sum.result()))\n-\n-\n-class MeanTest(keras_parameterized.TestCase):\n-\n-  # TODO(b/120949004): Re-enable garbage collection check\n-  # @test_util.run_in_graph_and_eager_modes(assert_no_eager_garbage=True)\n-  @keras_parameterized.run_all_keras_modes\n-  def test_mean(self):\n-    m = metrics.Mean(name='my_mean')\n-\n-    # check config\n-    self.assertEqual(m.name, 'my_mean')\n-    self.assertTrue(m.stateful)\n-    self.assertEqual(m.dtype, tf.float32)\n-    self.assertEqual(len(m.variables), 2)\n-    self.evaluate(tf.compat.v1.variables_initializer(m.variables))\n-\n-    # check initial state\n-    self.assertEqual(self.evaluate(m.total), 0)\n-    self.assertEqual(self.evaluate(m.count), 0)\n-\n-    # check __call__()\n-    self.assertEqual(self.evaluate(m(100)), 100)\n-    self.assertEqual(self.evaluate(m.total), 100)\n-    self.assertEqual(self.evaluate(m.count), 1)\n-\n-    # check update_state() and result() + state accumulation + tensor input\n-    update_op = m.update_state([\n-        tf.convert_to_tensor(1),\n-        tf.convert_to_tensor(5)\n-    ])\n-    self.evaluate(update_op)\n-    self.assertAlmostEqual(self.evaluate(m.result()), 106 / 3, 2)\n-    self.assertEqual(self.evaluate(m.total), 106)  # 100 + 1 + 5\n-    self.assertEqual(self.evaluate(m.count), 3)\n-\n-    # check reset_state()\n-    m.reset_state()\n-    self.assertEqual(self.evaluate(m.total), 0)\n-    self.assertEqual(self.evaluate(m.count), 0)\n-\n-    # Check save and restore config\n-    m2 = metrics.Mean.from_config(m.get_config())\n-    self.assertEqual(m2.name, 'my_mean')\n-    self.assertTrue(m2.stateful)\n-    self.assertEqual(m2.dtype, tf.float32)\n-    self.assertEqual(len(m2.variables), 2)\n-\n-  @testing_utils.run_v2_only\n-  def test_function_wrapped_reset_state(self):\n-    m = metrics.Mean(name='my_mean')\n-\n-    # check reset_state in function.\n-    @tf.function\n-    def reset_in_fn():\n-      m.reset_state()\n-      return m.update_state(100)\n-\n-    for _ in range(5):\n-      self.evaluate(reset_in_fn())\n-    self.assertEqual(self.evaluate(m.count), 1)\n-\n-  @keras_parameterized.run_all_keras_modes\n-  def test_mean_with_sample_weight(self):\n-    m = metrics.Mean(dtype=tf.float64)\n-    self.assertEqual(m.dtype, tf.float64)\n-    self.evaluate(tf.compat.v1.variables_initializer(m.variables))\n-\n-    # check scalar weight\n-    result_t = m(100, sample_weight=0.5)\n-    self.assertEqual(self.evaluate(result_t), 50 / 0.5)\n-    self.assertEqual(self.evaluate(m.total), 50)\n-    self.assertEqual(self.evaluate(m.count), 0.5)\n-\n-    # check weights not scalar and weights rank matches values rank\n-    result_t = m([1, 5], sample_weight=[1, 0.2])\n-    result = self.evaluate(result_t)\n-    self.assertAlmostEqual(result, 52 / 1.7, 2)\n-    self.assertAlmostEqual(self.evaluate(m.total), 52, 2)  # 50 + 1 + 5 * 0.2\n-    self.assertAlmostEqual(self.evaluate(m.count), 1.7, 2)  # 0.5 + 1.2\n-\n-    # check weights broadcast\n-    result_t = m([1, 2], sample_weight=0.5)\n-    self.assertAlmostEqual(self.evaluate(result_t), 53.5 / 2.7, 2)\n-    self.assertAlmostEqual(self.evaluate(m.total), 53.5, 2)  # 52 + 0.5 + 1\n-    self.assertAlmostEqual(self.evaluate(m.count), 2.7, 2)  # 1.7 + 0.5 + 0.5\n-\n-    # check weights squeeze\n-    result_t = m([1, 5], sample_weight=[[1], [0.2]])\n-    self.assertAlmostEqual(self.evaluate(result_t), 55.5 / 3.9, 2)\n-    self.assertAlmostEqual(self.evaluate(m.total), 55.5, 2)  # 53.5 + 1 + 1\n-    self.assertAlmostEqual(self.evaluate(m.count), 3.9, 2)  # 2.7 + 1.2\n-\n-    # check weights expand\n-    result_t = m([[1], [5]], sample_weight=[1, 0.2])\n-    self.assertAlmostEqual(self.evaluate(result_t), 57.5 / 5.1, 2)\n-    self.assertAlmostEqual(self.evaluate(m.total), 57.5, 2)  # 55.5 + 1 + 1\n-    self.assertAlmostEqual(self.evaluate(m.count), 5.1, 2)  # 3.9 + 1.2\n-\n-    # check values reduced to the dimensions of weight\n-    result_t = m([[[1., 2.], [3., 2.], [0.5, 4.]]], sample_weight=[0.5])\n-    result = np.round(self.evaluate(result_t), decimals=2)  # 58.5 / 5.6\n-    self.assertEqual(result, 10.45)\n-    self.assertEqual(np.round(self.evaluate(m.total), decimals=2), 58.54)\n-    self.assertEqual(np.round(self.evaluate(m.count), decimals=2), 5.6)\n-\n-  @keras_parameterized.run_all_keras_modes\n-  def test_mean_graph_with_placeholder(self):\n-    with tf.compat.v1.get_default_graph().as_default(), self.cached_session() as sess:\n-      m = metrics.Mean()\n-      v = tf.compat.v1.placeholder(tf.float32)\n-      w = tf.compat.v1.placeholder(tf.float32)\n-      self.evaluate(tf.compat.v1.variables_initializer(m.variables))\n-\n-      # check __call__()\n-      result_t = m(v, sample_weight=w)\n-      result = sess.run(result_t, feed_dict=({v: 100, w: 0.5}))\n-      self.assertEqual(self.evaluate(m.total), 50)\n-      self.assertEqual(self.evaluate(m.count), 0.5)\n-      self.assertEqual(result, 50 / 0.5)\n-\n-      # check update_state() and result()\n-      result = sess.run(result_t, feed_dict=({v: [1, 5], w: [1, 0.2]}))\n-      self.assertAlmostEqual(self.evaluate(m.total), 52, 2)  # 50 + 1 + 5 * 0.2\n-      self.assertAlmostEqual(self.evaluate(m.count), 1.7, 2)  # 0.5 + 1.2\n-      self.assertAlmostEqual(result, 52 / 1.7, 2)\n-\n-  @keras_parameterized.run_all_keras_modes\n-  def test_save_restore(self):\n-    checkpoint_directory = self.get_temp_dir()\n-    checkpoint_prefix = os.path.join(checkpoint_directory, 'ckpt')\n-    m = metrics.Mean()\n-    checkpoint = tf.train.Checkpoint(mean=m)\n-    self.evaluate(tf.compat.v1.variables_initializer(m.variables))\n-\n-    # update state\n-    self.evaluate(m(100.))\n-    self.evaluate(m(200.))\n-\n-    # save checkpoint and then add an update\n-    save_path = checkpoint.save(checkpoint_prefix)\n-    self.evaluate(m(1000.))\n-\n-    # restore to the same checkpoint mean object\n-    checkpoint.restore(save_path).assert_consumed().run_restore_ops()\n-    self.evaluate(m(300.))\n-    self.assertEqual(200., self.evaluate(m.result()))\n-\n-    # restore to a different checkpoint mean object\n-    restore_mean = metrics.Mean()\n-    restore_checkpoint = tf.train.Checkpoint(mean=restore_mean)\n-    status = restore_checkpoint.restore(save_path)\n-    restore_update = restore_mean(300.)\n-    status.assert_consumed().run_restore_ops()\n-    self.evaluate(restore_update)\n-    self.assertEqual(200., self.evaluate(restore_mean.result()))\n-    self.assertEqual(3, self.evaluate(restore_mean.count))\n-\n-  @keras_parameterized.run_all_keras_modes\n-  def test_multiple_instances(self):\n-    m = metrics.Mean()\n-    m2 = metrics.Mean()\n-\n-    self.assertEqual(m.name, 'mean')\n-    self.assertEqual(m2.name, 'mean')\n-\n-    self.assertEqual([v.name for v in m.variables],\n-                     testing_utils.get_expected_metric_variable_names(\n-                         ['total', 'count']))\n-    self.assertEqual([v.name for v in m2.variables],\n-                     testing_utils.get_expected_metric_variable_names(\n-                         ['total', 'count'], name_suffix='_1'))\n-\n-    self.evaluate(tf.compat.v1.variables_initializer(m.variables))\n-    self.evaluate(tf.compat.v1.variables_initializer(m2.variables))\n-\n-    # check initial state\n-    self.assertEqual(self.evaluate(m.total), 0)\n-    self.assertEqual(self.evaluate(m.count), 0)\n-    self.assertEqual(self.evaluate(m2.total), 0)\n-    self.assertEqual(self.evaluate(m2.count), 0)\n-\n-    # check __call__()\n-    self.assertEqual(self.evaluate(m(100)), 100)\n-    self.assertEqual(self.evaluate(m.total), 100)\n-    self.assertEqual(self.evaluate(m.count), 1)\n-    self.assertEqual(self.evaluate(m2.total), 0)\n-    self.assertEqual(self.evaluate(m2.count), 0)\n-\n-    self.assertEqual(self.evaluate(m2([63, 10])), 36.5)\n-    self.assertEqual(self.evaluate(m2.total), 73)\n-    self.assertEqual(self.evaluate(m2.count), 2)\n-    self.assertEqual(self.evaluate(m.result()), 100)\n-    self.assertEqual(self.evaluate(m.total), 100)\n-    self.assertEqual(self.evaluate(m.count), 1)\n-\n-  @testing_utils.run_v2_only\n-  def test_deepcopy_of_metrics(self):\n-    m = metrics.Mean(name='my_mean')\n-\n-    m.reset_state()\n-    m.update_state(100)\n-    m_copied = copy.deepcopy(m)\n-    m_copied.update_state(200)\n-\n-    self.assertEqual(self.evaluate(m.result()), 100)\n-    self.assertEqual(self.evaluate(m_copied.result()), 150)\n-\n-    m.reset_state()\n-\n-    self.assertEqual(self.evaluate(m.result()), 0)\n-    self.assertEqual(self.evaluate(m_copied.result()), 150)\n-\n-\n @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n class KerasAccuracyTest(tf.test.TestCase):\n \n@@ -1670,161 +1333,6 @@ class OneHotMeanIoUTest(tf.test.TestCase):\n     self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n \n \n-class MeanTensorTest(tf.test.TestCase, parameterized.TestCase):\n-\n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-  def test_config(self):\n-    with self.test_session():\n-      m = metrics.MeanTensor(name='mean_by_element')\n-\n-      # check config\n-      self.assertEqual(m.name, 'mean_by_element')\n-      self.assertTrue(m.stateful)\n-      self.assertEqual(m.dtype, tf.float32)\n-      self.assertEmpty(m.variables)\n-\n-      with self.assertRaisesRegex(ValueError, 'does not have any value yet'):\n-        m.result()\n-\n-      self.evaluate(m([[3], [5], [3]]))\n-      self.assertAllEqual(m._shape, [3, 1])\n-\n-      m2 = metrics.MeanTensor.from_config(m.get_config())\n-      self.assertEqual(m2.name, 'mean_by_element')\n-      self.assertTrue(m2.stateful)\n-      self.assertEqual(m2.dtype, tf.float32)\n-      self.assertEmpty(m2.variables)\n-\n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-  def test_unweighted(self):\n-    with self.test_session():\n-      m = metrics.MeanTensor(dtype=tf.float64)\n-\n-      # check __call__()\n-      self.assertAllClose(self.evaluate(m([100, 40])), [100, 40])\n-      self.assertAllClose(self.evaluate(m.total), [100, 40])\n-      self.assertAllClose(self.evaluate(m.count), [1, 1])\n-\n-      # check update_state() and result() + state accumulation + tensor input\n-      update_op = m.update_state([\n-          tf.convert_to_tensor(1),\n-          tf.convert_to_tensor(5)\n-      ])\n-      self.evaluate(update_op)\n-      self.assertAllClose(self.evaluate(m.result()), [50.5, 22.5])\n-      self.assertAllClose(self.evaluate(m.total), [101, 45])\n-      self.assertAllClose(self.evaluate(m.count), [2, 2])\n-\n-      # check reset_state()\n-      m.reset_state()\n-      self.assertAllClose(self.evaluate(m.total), [0, 0])\n-      self.assertAllClose(self.evaluate(m.count), [0, 0])\n-\n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-  def test_weighted(self):\n-    with self.test_session():\n-      m = metrics.MeanTensor(dtype=tf.float64)\n-      self.assertEqual(m.dtype, tf.float64)\n-\n-      # check scalar weight\n-      result_t = m([100, 30], sample_weight=0.5)\n-      self.assertAllClose(self.evaluate(result_t), [100, 30])\n-      self.assertAllClose(self.evaluate(m.total), [50, 15])\n-      self.assertAllClose(self.evaluate(m.count), [0.5, 0.5])\n-\n-      # check weights not scalar and weights rank matches values rank\n-      result_t = m([1, 5], sample_weight=[1, 0.2])\n-      result = self.evaluate(result_t)\n-      self.assertAllClose(result, [51 / 1.5, 16 / 0.7], 2)\n-      self.assertAllClose(self.evaluate(m.total), [51, 16])\n-      self.assertAllClose(self.evaluate(m.count), [1.5, 0.7])\n-\n-      # check weights broadcast\n-      result_t = m([1, 2], sample_weight=0.5)\n-      self.assertAllClose(self.evaluate(result_t), [51.5 / 2, 17 / 1.2])\n-      self.assertAllClose(self.evaluate(m.total), [51.5, 17])\n-      self.assertAllClose(self.evaluate(m.count), [2, 1.2])\n-\n-      # check weights squeeze\n-      result_t = m([1, 5], sample_weight=[[1], [0.2]])\n-      self.assertAllClose(self.evaluate(result_t), [52.5 / 3, 18 / 1.4])\n-      self.assertAllClose(self.evaluate(m.total), [52.5, 18])\n-      self.assertAllClose(self.evaluate(m.count), [3, 1.4])\n-\n-      # check weights expand\n-      m = metrics.MeanTensor(dtype=tf.float64)\n-      self.evaluate(tf.compat.v1.variables_initializer(m.variables))\n-      result_t = m([[1], [5]], sample_weight=[1, 0.2])\n-      self.assertAllClose(self.evaluate(result_t), [[1], [5]])\n-      self.assertAllClose(self.evaluate(m.total), [[1], [1]])\n-      self.assertAllClose(self.evaluate(m.count), [[1], [0.2]])\n-\n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-  def test_invalid_value_shape(self):\n-    m = metrics.MeanTensor(dtype=tf.float64)\n-    m([1])\n-    with self.assertRaisesRegex(\n-        ValueError, 'MeanTensor input values must always have the same shape'):\n-      m([1, 5])\n-\n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-  def test_build_in_tf_function(self):\n-    \"\"\"Ensure that variables are created correctly in a tf function.\"\"\"\n-    m = metrics.MeanTensor(dtype=tf.float64)\n-\n-    @tf.function\n-    def call_metric(x):\n-      return m(x)\n-\n-    with self.test_session():\n-      self.assertAllClose(self.evaluate(call_metric([100, 40])), [100, 40])\n-      self.assertAllClose(self.evaluate(m.total), [100, 40])\n-      self.assertAllClose(self.evaluate(m.count), [1, 1])\n-      self.assertAllClose(self.evaluate(call_metric([20, 2])), [60, 21])\n-\n-  @combinations.generate(combinations.combine(mode=['eager']))\n-  def test_in_keras_model(self):\n-    class ModelWithMetric(Model):\n-\n-      def __init__(self):\n-        super(ModelWithMetric, self).__init__()\n-        self.dense1 = layers.Dense(\n-            3, activation='relu', kernel_initializer='ones')\n-        self.dense2 = layers.Dense(\n-            1, activation='sigmoid', kernel_initializer='ones')\n-        self.mean_tensor = metrics.MeanTensor()\n-\n-      def call(self, x):\n-        x = self.dense1(x)\n-        x = self.dense2(x)\n-        self.mean_tensor(self.dense1.kernel)\n-        return x\n-\n-    model = ModelWithMetric()\n-    model.compile(\n-        loss='mae',\n-        optimizer='rmsprop',\n-        run_eagerly=True)\n-\n-    x = np.ones((100, 4))\n-    y = np.zeros((100, 1))\n-    model.evaluate(x, y, batch_size=50)\n-    self.assertAllClose(self.evaluate(model.mean_tensor.result()),\n-                        np.ones((4, 3)))\n-    self.assertAllClose(self.evaluate(model.mean_tensor.total),\n-                        np.full((4, 3), 2))\n-    self.assertAllClose(self.evaluate(model.mean_tensor.count),\n-                        np.full((4, 3), 2))\n-\n-    model.evaluate(x, y, batch_size=25)\n-    self.assertAllClose(self.evaluate(model.mean_tensor.result()),\n-                        np.ones((4, 3)))\n-    self.assertAllClose(self.evaluate(model.mean_tensor.total),\n-                        np.full((4, 3), 4))\n-    self.assertAllClose(self.evaluate(model.mean_tensor.count),\n-                        np.full((4, 3), 4))\n-\n-\n @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n class BinaryCrossentropyTest(tf.test.TestCase):\n \n@@ -2270,178 +1778,6 @@ class BinaryTruePositivesViaControlFlow(metrics.Metric):\n     return 0.0\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-class CustomMetricsTest(tf.test.TestCase):\n-\n-  def test_config(self):\n-    btp_obj = BinaryTruePositives(name='btp', dtype=tf.int32)\n-    self.assertEqual(btp_obj.name, 'btp')\n-    self.assertEqual(btp_obj.dtype, tf.int32)\n-\n-    # Check save and restore config\n-    btp_obj2 = BinaryTruePositives.from_config(btp_obj.get_config())\n-    self.assertEqual(btp_obj2.name, 'btp')\n-    self.assertEqual(btp_obj2.dtype, tf.int32)\n-\n-  def test_unweighted(self):\n-    btp_obj = BinaryTruePositives()\n-    self.evaluate(tf.compat.v1.variables_initializer(btp_obj.variables))\n-    y_true = tf.constant([[0, 0.9, 0, 1, 0], [0, 0, 1, 1, 1],\n-                                   [1, 1, 1, 1, 0], [0, 0, 0, 0, 1.5]])\n-    y_pred = tf.constant([[0, 0, 1, 5, 0], [1, 1, 1, 1, 1],\n-                                   [0, 1, 0, 1, 0], [1, 10, 1, 1, 1]])\n-\n-    update_op = btp_obj.update_state(y_true, y_pred)  # pylint: disable=assignment-from-no-return\n-    self.evaluate(update_op)\n-    result = btp_obj.result()\n-    self.assertEqual(7, self.evaluate(result))\n-\n-  def test_weighted(self):\n-    btp_obj = BinaryTruePositives()\n-    self.evaluate(tf.compat.v1.variables_initializer(btp_obj.variables))\n-    y_true = tf.constant([[0, 0.9, 0, 1, 0], [0, 0, 1, 1, 1],\n-                                   [1, 1, 1, 1, 0], [0, 0, 0, 0, 1.5]])\n-    y_pred = tf.constant([[0, 0, 1, 5, 0], [1, 1, 1, 1, 1],\n-                                   [0, 1, 0, 1, 0], [1, 10, 1, 1, 1]])\n-    sample_weight = tf.constant([[1.], [1.5], [2.], [2.5]])\n-    result = btp_obj(y_true, y_pred, sample_weight=sample_weight)\n-    self.assertEqual(12, self.evaluate(result))\n-\n-  def test_autograph(self):\n-    metric = BinaryTruePositivesViaControlFlow()\n-    self.evaluate(tf.compat.v1.variables_initializer(metric.variables))\n-    y_true = tf.constant([[0, 0.9, 0, 1, 0], [0, 0, 1, 1, 1],\n-                                   [1, 1, 1, 1, 0], [0, 0, 0, 0, 1.5]])\n-    y_pred = tf.constant([[0, 0, 1, 5, 0], [1, 1, 1, 1, 1],\n-                                   [0, 1, 0, 1, 0], [1, 10, 1, 1, 1]])\n-    sample_weight = tf.constant([[1.], [1.5], [2.], [2.5]])\n-\n-    @tf.function\n-    def compute_metric(y_true, y_pred, sample_weight):\n-      metric(y_true, y_pred, sample_weight)\n-      return metric.result()\n-\n-    result = compute_metric(y_true, y_pred, sample_weight)\n-    self.assertEqual(12, self.evaluate(result))\n-\n-  def test_metric_wrappers_autograph(self):\n-    def metric_fn(y_true, y_pred):\n-      x = tf.constant(0.0)\n-      for i in range(len(y_true)):\n-        for j in range(len(y_true[i])):\n-          if tf.equal(y_true[i][j], y_pred[i][j]) and y_true[i][j] > 0:\n-            x += 1.0\n-      return x\n-\n-    mean_metric = metrics.MeanMetricWrapper(metric_fn)\n-    sum_metric = metrics.SumOverBatchSizeMetricWrapper(metric_fn)\n-    self.evaluate(tf.compat.v1.variables_initializer(mean_metric.variables))\n-    self.evaluate(tf.compat.v1.variables_initializer(sum_metric.variables))\n-\n-    y_true = tf.constant([[0, 0, 0, 1, 0],\n-                                   [0, 0, 1, 1, 1],\n-                                   [1, 1, 1, 1, 0],\n-                                   [1, 1, 1, 0, 1]])\n-    y_pred = tf.constant([[0, 0, 1, 1, 0],\n-                                   [1, 1, 1, 1, 1],\n-                                   [0, 1, 0, 1, 0],\n-                                   [1, 1, 1, 1, 1]])\n-\n-    @tf.function\n-    def tf_functioned_metric_fn(metric, y_true, y_pred):\n-      return metric(y_true, y_pred)\n-\n-    metric_result = tf_functioned_metric_fn(mean_metric, y_true, y_pred)\n-    self.assertAllClose(self.evaluate(metric_result), 10, 1e-2)\n-    metric_result = tf_functioned_metric_fn(sum_metric, y_true, y_pred)\n-    self.assertAllClose(self.evaluate(metric_result), 10, 1e-2)\n-\n-  def test_metric_not_tracked_as_sublayer_in_layer(self):\n-\n-    class MyLayer(base_layer.Layer):\n-\n-      def __init__(self, **kwargs):\n-        super(MyLayer, self).__init__(**kwargs)\n-        self.mean_obj = metrics.Mean(name='my_mean_obj')\n-\n-      def call(self, x):\n-        self.add_metric(\n-            tf.reduce_sum(x), aggregation='mean', name='my_mean_tensor')\n-        self.add_metric(self.mean_obj(x))\n-        return x\n-\n-    layer = MyLayer()\n-    x = np.ones((1, 1))\n-    layer(x)\n-    self.assertLen(list(layer._flatten_layers(include_self=False)), 0)\n-    self.assertLen(layer.metrics, 2)\n-\n-  def test_metric_not_tracked_as_sublayer_in_model(self):\n-\n-    class MyModel(training_module.Model):\n-\n-      def __init__(self, **kwargs):\n-        super(MyModel, self).__init__(**kwargs)\n-        self.mean_obj = metrics.Mean(name='my_mean_obj')\n-\n-      def call(self, x):\n-        self.add_metric(\n-            tf.reduce_sum(x), aggregation='mean', name='my_mean_tensor')\n-        self.add_metric(self.mean_obj(x))\n-        return x\n-\n-    model = MyModel()\n-    x = np.ones((1, 1))\n-    model(x)\n-    self.assertLen(list(model._flatten_layers(include_self=False)), 0)\n-    self.assertLen(model.layers, 0)\n-    self.assertLen(model.metrics, 2)\n-\n-  def test_invalid_custom_metric_class_error_msg(self):\n-    x = layers.Input(shape=(2,))\n-    y = layers.Dense(3)(x)\n-    model = training_module.Model(x, y)\n-\n-    class BadMetric(metrics.Metric):\n-\n-      def update_state(self, y_true, y_pred, sample_weight=None):\n-        return\n-\n-      def result(self):\n-        return\n-\n-    with self.assertRaisesRegex(RuntimeError,\n-                                'can only be a single'):\n-      model.compile('sgd',\n-                    'mse',\n-                    metrics=[BadMetric()])\n-      model.fit(np.ones((10, 2)), np.ones((10, 3)))\n-\n-  def test_invalid_custom_metric_fn_error_msg(self):\n-    x = layers.Input(shape=(2,))\n-    y = layers.Dense(3)(x)\n-    model = training_module.Model(x, y)\n-\n-    def bad_metric(y_true, y_pred, sample_weight=None):  # pylint: disable=unused-argument\n-      return None\n-\n-    def dict_metric(y_true, y_pred, sample_weight=None):  # pylint: disable=unused-argument\n-      return {'value': 0.}\n-\n-    with self.assertRaisesRegex(RuntimeError,\n-                                'The output of a metric function can only be'):\n-      model.compile('sgd',\n-                    'mse',\n-                    metrics=[bad_metric])\n-      model.fit(np.ones((10, 2)), np.ones((10, 3)))\n-    with self.assertRaisesRegex(RuntimeError,\n-                                'To return a dict of values, implement'):\n-      model.compile('sgd',\n-                    'mse',\n-                    metrics=[dict_metric])\n-      model.fit(np.ones((10, 2)), np.ones((10, 3)))\n-\n-\n def _get_model(compile_metrics):\n   model_layers = [\n       layers.Dense(3, activation='relu', kernel_initializer='ones'),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
