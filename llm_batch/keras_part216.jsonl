{"custom_id": "keras#bbb5bd0e30fe3e75e1b2b7d670f5cbbb8ccd7c2f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 5486 | Contributors (this commit): 69 | Commits (past 90d): 22 | Contributors (cumulative): 69 | DMM Complexity: None\n\nDIFF:\n@@ -290,7 +290,7 @@ def _check_loss_and_target_compatibility(targets, loss_fns, output_shapes):\n                     'If your targets are integer classes, '\n                     'you can convert them to the expected format via:\\n'\n                     '```\\n'\n-                    'from keras.utils.np_utils import to_categorical\\n'\n+                    'from keras.utils import to_categorical\\n'\n                     'y_binary = to_categorical(y_int)\\n'\n                     '```\\n'\n                     '\\n'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8c1af0ab8d617f0d28548bc77e5540142f338068", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 800 | Contributors (this commit): 12 | Commits (past 90d): 3 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -309,7 +309,7 @@ class Concatenate(_Merge):\n     \"\"\"Layer that concatenates a list of inputs.\n \n     It takes as input a list of tensors,\n-    all of the same shape expect for the concatenation axis,\n+    all of the same shape except for the concatenation axis,\n     and returns a single tensor, the concatenation of all inputs.\n \n     # Arguments\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#23446009c3ae97aebc0cd7d2e597ae6ca5c0db20", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 31 | Churn Cumulative: 502 | Contributors (this commit): 13 | Commits (past 90d): 2 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -17,12 +17,16 @@ def to_categorical(y, num_classes=None):\n     # Returns\n         A binary matrix representation of the input.\n     \"\"\"\n-    y = np.array(y, dtype='int').ravel()\n+    y = np.array(y, dtype='int')\n+    input_shape = y.shape\n+    y = y.ravel()\n     if not num_classes:\n         num_classes = np.max(y) + 1\n     n = y.shape[0]\n     categorical = np.zeros((n, num_classes))\n     categorical[np.arange(n), y] = 1\n+    output_shape = input_shape + (num_classes,)\n+    categorical = np.reshape(categorical, output_shape)\n     return categorical\n \n \n\n@@ -0,0 +1,25 @@\n+\"\"\"Tests for functions in np_utils.py.\n+\"\"\"\n+import numpy as np\n+import pytest\n+from keras.utils import to_categorical\n+\n+\n+def test_to_categorical():\n+    num_classes = 5\n+    shapes = [(3,), (4, 3), (5, 4, 3)]\n+    labels = [np.random.randint(0, num_classes, shape) for shape in shapes]\n+    one_hots = [to_categorical(label, num_classes) for label in labels]\n+    for label, one_hot in zip(labels, one_hots):\n+        # Check shape\n+        assert one_hot.shape == label.shape + (num_classes,)\n+        # Make sure there are only 0s and 1s\n+        assert np.array_equal(one_hot, one_hot.astype(bool))\n+        # Make sure there is only one 1 in a row\n+        assert np.all(one_hot.sum(axis=-1) == 1)\n+        # Get original labels back from one hots\n+        assert np.all(np.argmax(one_hot, -1) == label)\n+\n+\n+if __name__ == '__main__':\n+    pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c25fa38deb4efc5445f64af3ec17eae0eb660d2f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 49 | Lines Deleted: 15 | Files Changed: 2 | Hunks: 7 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 64 | Churn Cumulative: 11320 | Contributors (this commit): 82 | Commits (past 90d): 14 | Contributors (cumulative): 91 | DMM Complexity: 0.7\n\nDIFF:\n@@ -18,6 +18,7 @@ from .engine.training import Model\n from .engine import topology\n from .engine.topology import Layer\n from .engine.topology import Input\n+from .engine.topology import InputLayer\n from .legacy import layers as legacy_layers\n from .legacy import models as legacy_models\n from .legacy import interfaces\n@@ -425,37 +426,50 @@ class Sequential(Model):\n                             'an instance of class Layer. '\n                             'Found: ' + str(layer))\n         if not self.outputs:\n-            # first layer in model: check that it is an input layer\n-            if not layer.inbound_nodes:\n-                # create an input layer\n+            # First layer in model: check that it is an input layer.\n+            if not isinstance(layer, (InputLayer, legacy_layers.Merge)):\n+                # Create an input layer.\n+                # First, we need to infer its expected input shape and dtype.\n+                if isinstance(layer, (Model, Sequential)):\n+                    # We were passed a model as first layer.\n+                    # This requires a specific way to figure out the\n+                    # input shape and dtype.\n+                    if not layer.layers:\n+                        raise ValueError('Cannot add an empty model '\n+                                         'to a `Sequential` model.')\n+                    # In case of nested models: recover the first layer\n+                    # of the deepest model to infer input shape and dtype.\n+                    first_layer = layer.layers[0]\n+                    while isinstance(first_layer, (Model, Sequential)):\n+                        first_layer = first_layer.layers[0]\n+                    batch_shape = first_layer.batch_input_shape\n+                    dtype = first_layer.dtype\n+                else:\n+                    # We were passed a regular layer, and it should\n+                    # know about its input shape. Otherwise, that's an error.\n                     if not hasattr(layer, 'batch_input_shape'):\n                         raise ValueError('The first layer in a '\n                                          'Sequential model must '\n                                          'get an `input_shape` or '\n                                          '`batch_input_shape` argument.')\n+                    batch_shape = layer.batch_input_shape\n+                    dtype = layer.dtype\n                 # Instantiate the input layer.\n-                x = Input(batch_shape=layer.batch_input_shape,\n-                          dtype=layer.dtype, name=layer.name + '_input')\n+                x = Input(batch_shape=batch_shape,\n+                          dtype=dtype,\n+                          name=layer.name + '_input')\n                 # This will build the current layer\n                 # and create the node connecting the current layer\n                 # to the input layer we just created.\n                 layer(x)\n \n-            if len(layer.inbound_nodes) != 1:\n-                raise ValueError('A layer added to a Sequential model must '\n-                                 'not already be connected somewhere else. '\n-                                 'Model received layer ' + layer.name +\n-                                 ' which has ' +\n-                                 str(len(layer.inbound_nodes)) +\n-                                 ' pre-existing inbound connections.')\n-\n-            if len(layer.inbound_nodes[0].output_tensors) != 1:\n+            if len(layer.inbound_nodes[-1].output_tensors) != 1:\n                 raise ValueError('All layers in a Sequential model '\n                                  'should have a single output tensor. '\n                                  'For multi-output layers, '\n                                  'use the functional API.')\n \n-            self.outputs = [layer.inbound_nodes[0].output_tensors[0]]\n+            self.outputs = [layer.inbound_nodes[-1].output_tensors[0]]\n             self.inputs = topology.get_source_inputs(self.outputs[0])\n \n             # We create an input node, which we will keep updated\n\n@@ -257,6 +257,26 @@ def test_sequential_count_params():\n     assert(n == model.count_params())\n \n \n+@keras_test\n+def test_nested_sequential_trainability():\n+    input_dim = 20\n+    num_units = 10\n+    num_classes = 2\n+\n+    inner_model = Sequential()\n+    inner_model.add(Dense(num_units, input_shape=(input_dim,)))\n+\n+    model = Sequential()\n+    model.add(inner_model)\n+    model.add(Dense(num_classes))\n+\n+    assert len(model.trainable_weights) == 4\n+    inner_model.trainable = False\n+    assert len(model.trainable_weights) == 2\n+    inner_model.trainable = True\n+    assert len(model.trainable_weights) == 4\n+\n+\n @keras_test\n def test_rebuild_model():\n     model = Sequential()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d1ee9455afc98e09d2a24f839305293b5388469b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 7 | Files Changed: 3 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 30 | Churn Cumulative: 16566 | Contributors (this commit): 138 | Commits (past 90d): 38 | Contributors (cumulative): 168 | DMM Complexity: 0.9375\n\nDIFF:\n@@ -1442,7 +1442,8 @@ class Model(Container):\n             deduped_out_labels.append(new_label)\n         return deduped_out_labels\n \n-    def fit(self, x=None,\n+    def fit(self,\n+            x=None,\n             y=None,\n             batch_size=None,\n             epochs=1,\n\n@@ -830,9 +830,22 @@ class Sequential(Model):\n         self.sample_weights = self.model.sample_weights\n         self.total_loss = self.model.total_loss\n \n-    def fit(self, x, y, batch_size=32, epochs=10, verbose=1, callbacks=None,\n-            validation_split=0., validation_data=None, shuffle=True,\n-            class_weight=None, sample_weight=None, initial_epoch=0, **kwargs):\n+    def fit(self,\n+            x=None,\n+            y=None,\n+            batch_size=None,\n+            epochs=1,\n+            verbose=1,\n+            callbacks=None,\n+            validation_split=0.,\n+            validation_data=None,\n+            shuffle=True,\n+            class_weight=None,\n+            sample_weight=None,\n+            initial_epoch=0,\n+            steps_per_epoch=None,\n+            validation_steps=None,\n+            **kwargs):\n         \"\"\"Trains the model for a fixed number of epochs.\n \n         # Arguments\n@@ -904,7 +917,9 @@ class Sequential(Model):\n                               shuffle=shuffle,\n                               class_weight=class_weight,\n                               sample_weight=sample_weight,\n-                              initial_epoch=initial_epoch)\n+                              initial_epoch=initial_epoch,\n+                              steps_per_epoch=steps_per_epoch,\n+                              validation_steps=validation_steps)\n \n     def evaluate(self, x, y, batch_size=32, verbose=1,\n                  sample_weight=None):\n\n@@ -247,12 +247,12 @@ def test_EarlyStopping_reuse():\n     stopper = callbacks.EarlyStopping(monitor='acc', patience=patience)\n     weights = model.get_weights()\n \n-    hist = model.fit(data, labels, callbacks=[stopper])\n+    hist = model.fit(data, labels, callbacks=[stopper], epochs=20)\n     assert len(hist.epoch) >= patience\n \n     # This should allow training to go for at least `patience` epochs\n     model.set_weights(weights)\n-    hist = model.fit(data, labels, callbacks=[stopper])\n+    hist = model.fit(data, labels, callbacks=[stopper], epochs=20)\n     assert len(hist.epoch) >= patience\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d9128669720848140be2fb71cd6a67dcac8f0ebe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 101 | Lines Deleted: 69 | Files Changed: 2 | Hunks: 19 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 170 | Churn Cumulative: 15335 | Contributors (this commit): 128 | Commits (past 90d): 29 | Contributors (cumulative): 148 | DMM Complexity: None\n\nDIFF:\n@@ -1461,60 +1461,62 @@ class Model(Container):\n         \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset).\n \n         # Arguments\n-            x: Numpy array of training data,\n-                or list of Numpy arrays if the model has multiple inputs.\n-                If all inputs in the model are named,\n-                you can also pass a dictionary\n-                mapping input names to Numpy arrays.\n-                Can be `None` (default) if feeding from framework-native tensors.\n-            y: Numpy array of target data,\n-                or list of Numpy arrays if the model has multiple outputs.\n-                If all outputs in the model are named,\n-                you can also pass a dictionary\n-                mapping output names to Numpy arrays.\n-                Can be `None` (default) if feeding from framework-native tensors.\n+            x: Numpy array of training data, or a list of Numpy arrays.\n+                If the input in the model is named, you can also pass a\n+                dictionary mapping input name to Numpy array. `x` can be\n+                `None` (default) if feeding from framework-native tensors.\n+            y: Numpy array of target (label) data, or a list of Numpy arrays.\n+                If outputs in the model are named, you can also pass a\n+                dictionary mapping output names to Numpy array. `y` can be\n+                `None` (default) if feeding from framework-native tensors.\n             batch_size: Integer or `None`.\n                 Number of samples per gradient update.\n                 If unspecified, it will default to 32.\n-            epochs: Integer, the number of times to iterate\n-                over the training data arrays.\n+            epochs: Integer. Number of epochs to train the model.\n+                Note that in conjunction with `initial_epoch`, `epochs` is to be\n+                understood as \"final epoch\". The model is not trained for a\n+                number of steps given by `epochs`, but until the epoch `epochs`\n+                is reached.\n             verbose: 0, 1, or 2. Verbosity mode.\n-                0 = silent, 1 = verbose, 2 = one log line per epoch.\n-            callbacks: List of callbacks to be called during training.\n+                0 = silent, 1 = progress bar, 2 = one line per epoch.\n+            callbacks: List of `keras.callbacks.Callback` instances.\n+                List of callbacks to apply during training.\n                 See [callbacks](/callbacks).\n             validation_split: Float between 0 and 1:\n-                fraction of the training data to be used as validation data.\n+                Fraction of the training data to be used as validation data.\n                 The model will set apart this fraction of the training data,\n                 will not train on it, and will evaluate\n                 the loss and any model metrics\n                 on this data at the end of each epoch.\n-            validation_data: Data on which to evaluate\n-                the loss and any model metrics\n-                at the end of each epoch. The model will not\n-                be trained on this data.\n-                This could be a tuple (x_val, y_val)\n-                or a tuple (x_val, y_val, val_sample_weights).\n-            shuffle: Boolean, whether to shuffle the training data\n-                before each epoch. Has no effect when `steps_per_epoch`\n-                is not `None`.\n-            class_weight: Optional dictionary mapping\n-                class indices (integers) to\n-                a weight (float) to apply to the model's loss for the samples\n-                from this class during training.\n-                This can be useful to tell the model to \"pay more attention\" to\n-                samples from an under-represented class.\n-            sample_weight: Optional array of the same length as x, containing\n-                weights to apply to the model's loss for each sample.\n-                In the case of temporal data, you can pass a 2D array\n-                with shape (samples, sequence_length),\n+            validation_data: tuple `(x_val, y_val)` or tuple\n+                `(x_val, y_val, val_sample_weights)` on which to evaluate\n+                the loss and any model metrics at the end of each epoch.\n+                The model will not be trained on this data.\n+                Will override `validation_split`.\n+            shuffle: Boolean (whether to shuffle the training data\n+                before each epoch) or str (for 'batch').\n+                'batch' is a special option for dealing with the\n+                limitations of HDF5 data; it shuffles in batch-sized chunks.\n+                Has no effect when `steps_per_epoch` is not `None`.\n+            class_weight: Optional dictionary mapping class indices (integers)\n+                to a weight (float) value, used for weighting the loss function\n+                (during training only). This can be useful to tell the model to\n+                \"pay more attention\" to samples from an under-represented class.\n+            sample_weight: Optional Numpy array of weights for\n+                the training samples, used for weighting the loss function\n+                (during training only). You can either pass a flat (1D)\n+                Numpy array with the same length as the input samples\n+                (1:1 mapping between weights and samples),\n+                or in the case of temporal data,\n+                you can pass a 2D array with shape `(samples, sequence_length)`,\n                 to apply a different weight to every timestep of every sample.\n                 In this case you should make sure to specify\n-                sample_weight_mode=\"temporal\" in compile().\n+                `sample_weight_mode=\"temporal\"` in `compile()`.\n             initial_epoch: Epoch at which to start training\n-                (useful for resuming a previous training run)\n+                (useful for resuming a previous training run).\n             steps_per_epoch: Total number of steps (batches of samples)\n                 before declaring one epoch finished and starting the\n-                next epoch. When training with Input Tensors such as\n+                next epoch. When training with input tensors such as\n                 TensorFlow data tensors, the default `None` is equal to\n                 the number of unique samples in your dataset divided by\n                 the batch size, or 1 if that cannot be determined.\n@@ -1523,10 +1525,13 @@ class Model(Container):\n                 to validate before stopping.\n \n         # Returns\n-            A `History` instance. Its `history` attribute contains\n-            all information collected during training.\n+            A `History` object. Its `History.history` attribute is\n+            a record of training loss values and metrics values\n+            at successive epochs, as well as validation loss values\n+            and validation metrics values (if applicable).\n \n         # Raises\n+            RuntimeError: If the model was never compiled.\n             ValueError: In case of mismatch between the provided input data\n                 and what the model expects.\n         \"\"\"\n@@ -1643,13 +1648,13 @@ class Model(Container):\n                 If all inputs in the model are named,\n                 you can also pass a dictionary\n                 mapping input names to Numpy arrays.\n-                Can be `None` (default) if feeding from framework-native tensors.\n+                `x` can be `None` (default) if feeding from framework-native tensors.\n             y: Numpy array of target data,\n                 or list of Numpy arrays if the model has multiple outputs.\n                 If all outputs in the model are named,\n                 you can also pass a dictionary\n                 mapping output names to Numpy arrays.\n-                Can be `None` (default) if feeding from framework-native tensors.\n+                `y` can be `None` (default) if feeding from framework-native tensors.\n             batch_size: Integer. If unspecified, it will default to 32.\n             verbose: Verbosity mode, 0 or 1.\n             sample_weight: Array of weights to weight the contribution\n\n@@ -849,43 +849,68 @@ class Sequential(Model):\n         \"\"\"Trains the model for a fixed number of epochs.\n \n         # Arguments\n-            x: input data, as a Numpy array or list of Numpy arrays\n-                (if the model has multiple inputs).\n-            y: labels, as a Numpy array.\n-            batch_size: integer. Number of samples per gradient update.\n-            epochs: integer. Number of epochs to train the model.\n-                Note that in conjunction with initial_epoch, the parameter\n-                epochs is to be understood as \"final epoch\". The model is\n-                not trained for a number of steps given by epochs, but\n-                until the epoch epochs is reached.\n-            verbose: 0 for no logging to stdout,\n-                1 for progress bar logging, 2 for one log line per epoch.\n-            callbacks: list of `keras.callbacks.Callback` instances.\n+            x: Numpy array of training data.\n+                If the input in the model is named, you can also pass a\n+                dictionary mapping the input name to a Numpy array. `x` can be\n+                `None` (default) if feeding from framework-native tensors.\n+            y: Numpy array of target (label) data.\n+                If the output in the model is named, you can also pass a\n+                dictionary mapping the output name to a Numpy array. `y` can be\n+                `None` (default) if feeding from framework-native tensors.\n+            batch_size: Integer or `None`.\n+                Number of samples per gradient update.\n+                If unspecified, it will default to 32.\n+            epochs: Integer. Number of epochs to train the model.\n+                Note that in conjunction with `initial_epoch`, `epochs` is to be\n+                understood as \"final epoch\". The model is not trained for a\n+                number of steps given by `epochs`, but until the epoch `epochs`\n+                is reached.\n+            verbose: 0, 1, or 2. Verbosity mode.\n+                0 = silent, 1 = progress bar, 2 = one line per epoch.\n+            callbacks: List of `keras.callbacks.Callback` instances.\n                 List of callbacks to apply during training.\n                 See [callbacks](/callbacks).\n-            validation_split: float (0. < x < 1).\n-                Fraction of the data to use as held-out validation data.\n-            validation_data: tuple (x_val, y_val) or tuple\n-                (x_val, y_val, val_sample_weights) to be used as held-out\n-                validation data. Will override validation_split.\n-            shuffle: boolean or str (for 'batch').\n-                Whether to shuffle the samples at each epoch.\n+            validation_split: Float between 0 and 1:\n+                Fraction of the training data to be used as validation data.\n+                The model will set apart this fraction of the training data,\n+                will not train on it, and will evaluate\n+                the loss and any model metrics\n+                on this data at the end of each epoch.\n+            validation_data: tuple `(x_val, y_val)` or tuple\n+                `(x_val, y_val, val_sample_weights)` on which to evaluate\n+                the loss and any model metrics at the end of each epoch.\n+                The model will not be trained on this data.\n+                Will override `validation_split`.\n+            shuffle: Boolean (whether to shuffle the training data\n+                before each epoch) or str (for 'batch').\n                 'batch' is a special option for dealing with the\n                 limitations of HDF5 data; it shuffles in batch-sized chunks.\n-            class_weight: dictionary mapping classes to a weight value,\n-                used for scaling the loss function (during training only).\n-            sample_weight: Numpy array of weights for\n-                the training samples, used for scaling the loss function\n+                Has no effect when `steps_per_epoch` is not `None`.\n+            class_weight: Optional dictionary mapping class indices (integers)\n+                to a weight (float) value, used for weighting the loss function\n+                (during training only). This can be useful to tell the model to\n+                \"pay more attention\" to samples from an under-represented class.\n+            sample_weight: Optional Numpy array of weights for\n+                the training samples, used for weighting the loss function\n                 (during training only). You can either pass a flat (1D)\n                 Numpy array with the same length as the input samples\n                 (1:1 mapping between weights and samples),\n                 or in the case of temporal data,\n-                you can pass a 2D array with shape (samples, sequence_length),\n+                you can pass a 2D array with shape `(samples, sequence_length)`,\n                 to apply a different weight to every timestep of every sample.\n                 In this case you should make sure to specify\n-                sample_weight_mode=\"temporal\" in compile().\n+                `sample_weight_mode=\"temporal\"` in `compile()`.\n             initial_epoch: Epoch at which to start training\n                 (useful for resuming a previous training run).\n+            steps_per_epoch: Total number of steps (batches of samples)\n+                before declaring one epoch finished and starting the\n+                next epoch. When training with input tensors such as\n+                TensorFlow data tensors, the default `None` is equal to\n+                the number of unique samples in your dataset divided by\n+                the batch size, or 1 if that cannot be determined.\n+            validation_steps: Only relevant if `steps_per_epoch`\n+                is specified. Total number of steps (batches of samples)\n+                to validate before stopping.\n \n         # Returns\n             A `History` object. Its `History.history` attribute is\n@@ -894,7 +919,9 @@ class Sequential(Model):\n             and validation metrics values (if applicable).\n \n         # Raises\n-            RuntimeError: if the model was never compiled.\n+            RuntimeError: If the model was never compiled.\n+            ValueError: In case of mismatch between the provided input data\n+                and what the model expects.\n         \"\"\"\n         # Legacy support\n         if 'nb_epoch' in kwargs:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d4a14ee54728ac8ea6c5ffbf41f559662dcfba46", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 64 | Lines Deleted: 39 | Files Changed: 2 | Hunks: 20 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 103 | Churn Cumulative: 15438 | Contributors (this commit): 128 | Commits (past 90d): 31 | Contributors (cumulative): 148 | DMM Complexity: None\n\nDIFF:\n@@ -582,7 +582,7 @@ class Model(Container):\n         \"\"\"Configures the model for training.\n \n         # Arguments\n-            optimizer: String (name of optimizer) or optimizer object.\n+            optimizer: String (name of optimizer) or optimizer instance.\n                 See [optimizers](/optimizers).\n             loss: String (name of objective function) or objective function.\n                 See [losses](/losses).\n@@ -622,7 +622,8 @@ class Model(Container):\n                 a single tensor (for a single-output model), a list of tensors,\n                 or a dict mapping output names to target tensors.\n             **kwargs: When using the Theano/CNTK backends, these arguments\n-                are passed into K.function. When using the TensorFlow backend,\n+                are passed into `K.function`.\n+                When using the TensorFlow backend,\n                 these arguments are passed into `tf.Session.run`.\n \n         # Raises\n@@ -1461,38 +1462,48 @@ class Model(Container):\n         \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset).\n \n         # Arguments\n-            x: Numpy array of training data, or a list of Numpy arrays.\n-                If the input in the model is named, you can also pass a\n-                dictionary mapping input name to Numpy array. `x` can be\n-                `None` (default) if feeding from framework-native tensors.\n-            y: Numpy array of target (label) data, or a list of Numpy arrays.\n-                If outputs in the model are named, you can also pass a\n-                dictionary mapping output names to Numpy array. `y` can be\n-                `None` (default) if feeding from framework-native tensors.\n+            x: Numpy array of training data (if the model has a single input),\n+                or list of Numpy arrays (if the model has multiple inputs).\n+                If input layers in the model are named, you can also pass a\n+                dictionary mapping input names to Numpy arrays.\n+                `x` can be `None` (default) if feeding from\n+                framework-native tensors (e.g. TensorFlow data tensors).\n+            y: Numpy array of target (label) data\n+                (if the model has a single output),\n+                or list of Numpy arrays (if the model has multiple outputs).\n+                If output layers in the model are named, you can also pass a\n+                dictionary mapping output names to Numpy arrays.\n+                `y` can be `None` (default) if feeding from\n+                framework-native tensors (e.g. TensorFlow data tensors).\n             batch_size: Integer or `None`.\n                 Number of samples per gradient update.\n                 If unspecified, it will default to 32.\n             epochs: Integer. Number of epochs to train the model.\n-                Note that in conjunction with `initial_epoch`, `epochs` is to be\n-                understood as \"final epoch\". The model is not trained for a\n-                number of steps given by `epochs`, but until the epoch `epochs`\n-                is reached.\n+                An epoch is an iteration over the entire `x` and `y`\n+                data provided.\n+                Note that in conjunction with `initial_epoch`,\n+                `epochs` is to be understood as \"final epoch\".\n+                The model is not trained for a number of iterations\n+                given by `epochs`, but merely until the epoch\n+                of index `epochs` is reached.\n             verbose: 0, 1, or 2. Verbosity mode.\n                 0 = silent, 1 = progress bar, 2 = one line per epoch.\n             callbacks: List of `keras.callbacks.Callback` instances.\n                 List of callbacks to apply during training.\n                 See [callbacks](/callbacks).\n-            validation_split: Float between 0 and 1:\n+            validation_split: Float between 0 and 1.\n                 Fraction of the training data to be used as validation data.\n                 The model will set apart this fraction of the training data,\n                 will not train on it, and will evaluate\n                 the loss and any model metrics\n                 on this data at the end of each epoch.\n+                The validation data is selected from the last samples\n+                in the `x` and `y` data provided, before shuffling.\n             validation_data: tuple `(x_val, y_val)` or tuple\n                 `(x_val, y_val, val_sample_weights)` on which to evaluate\n                 the loss and any model metrics at the end of each epoch.\n                 The model will not be trained on this data.\n-                Will override `validation_split`.\n+                This will override `validation_split`.\n             shuffle: Boolean (whether to shuffle the training data\n                 before each epoch) or str (for 'batch').\n                 'batch' is a special option for dealing with the\n@@ -1500,15 +1511,18 @@ class Model(Container):\n                 Has no effect when `steps_per_epoch` is not `None`.\n             class_weight: Optional dictionary mapping class indices (integers)\n                 to a weight (float) value, used for weighting the loss function\n-                (during training only). This can be useful to tell the model to\n-                \"pay more attention\" to samples from an under-represented class.\n+                (during training only).\n+                This can be useful to tell the model to\n+                \"pay more attention\" to samples from\n+                an under-represented class.\n             sample_weight: Optional Numpy array of weights for\n                 the training samples, used for weighting the loss function\n                 (during training only). You can either pass a flat (1D)\n                 Numpy array with the same length as the input samples\n                 (1:1 mapping between weights and samples),\n                 or in the case of temporal data,\n-                you can pass a 2D array with shape `(samples, sequence_length)`,\n+                you can pass a 2D array with shape\n+                `(samples, sequence_length)`,\n                 to apply a different weight to every timestep of every sample.\n                 In this case you should make sure to specify\n                 `sample_weight_mode=\"temporal\"` in `compile()`.\n\n@@ -784,16 +784,17 @@ class Sequential(Model):\n                 dictionary or a list of modes.\n             weighted_metrics: List of metrics to be evaluated and weighted\n                 by sample_weight or class_weight during training and testing.\n-            target_tensors: By default, Keras will create placeholders for the\n+            target_tensors: By default, Keras will create a placeholder for the\n                 model's target, which will be fed with the target data during\n                 training. If instead you would like to use your own\n-                target tensors (in turn, Keras will not expect external\n+                target tensor (in turn, Keras will not expect external\n                 Numpy data for these targets at training time), you\n-                can specify them via the `target_tensors` argument. It can be\n-                a single tensor (for a single-output model), a list of tensors,\n-                or a dict mapping output names to target tensors.\n+                can specify them via the `target_tensors` argument.\n+                It should be a single tensor\n+                (for a single-output `Sequential` model).\n             **kwargs: When using the Theano/CNTK backends, these arguments\n-                are passed into K.function. When using the TensorFlow backend,\n+                are passed into `K.function`.\n+                When using the TensorFlow backend,\n                 these arguments are passed into `tf.Session.run`.\n \n         # Raises\n@@ -850,21 +851,26 @@ class Sequential(Model):\n \n         # Arguments\n             x: Numpy array of training data.\n-                If the input in the model is named, you can also pass a\n-                dictionary mapping the input name to a Numpy array. `x` can be\n-                `None` (default) if feeding from framework-native tensors.\n+                If the input layer in the model is named, you can also pass a\n+                dictionary mapping the input name to a Numpy array.\n+                `x` can be `None` (default) if feeding from\n+                framework-native tensors (e.g. TensorFlow data tensors).\n             y: Numpy array of target (label) data.\n-                If the output in the model is named, you can also pass a\n-                dictionary mapping the output name to a Numpy array. `y` can be\n-                `None` (default) if feeding from framework-native tensors.\n+                If the output layer in the model is named, you can also pass a\n+                dictionary mapping the output name to a Numpy array.\n+                `y` can be `None` (default) if feeding from\n+                framework-native tensors (e.g. TensorFlow data tensors).\n             batch_size: Integer or `None`.\n                 Number of samples per gradient update.\n                 If unspecified, it will default to 32.\n             epochs: Integer. Number of epochs to train the model.\n-                Note that in conjunction with `initial_epoch`, `epochs` is to be\n-                understood as \"final epoch\". The model is not trained for a\n-                number of steps given by `epochs`, but until the epoch `epochs`\n-                is reached.\n+                An epoch is an iteration over the entire `x` and `y`\n+                data provided.\n+                Note that in conjunction with `initial_epoch`,\n+                `epochs` is to be understood as \"final epoch\".\n+                The model is not trained for a number of iterations\n+                given by `epochs`, but merely until the epoch\n+                of index `epochs` is reached.\n             verbose: 0, 1, or 2. Verbosity mode.\n                 0 = silent, 1 = progress bar, 2 = one line per epoch.\n             callbacks: List of `keras.callbacks.Callback` instances.\n@@ -876,11 +882,13 @@ class Sequential(Model):\n                 will not train on it, and will evaluate\n                 the loss and any model metrics\n                 on this data at the end of each epoch.\n+                The validation data is selected from the last samples\n+                in the `x` and `y` data provided, before shuffling.\n             validation_data: tuple `(x_val, y_val)` or tuple\n                 `(x_val, y_val, val_sample_weights)` on which to evaluate\n                 the loss and any model metrics at the end of each epoch.\n                 The model will not be trained on this data.\n-                Will override `validation_split`.\n+                This will override `validation_split`.\n             shuffle: Boolean (whether to shuffle the training data\n                 before each epoch) or str (for 'batch').\n                 'batch' is a special option for dealing with the\n@@ -888,15 +896,18 @@ class Sequential(Model):\n                 Has no effect when `steps_per_epoch` is not `None`.\n             class_weight: Optional dictionary mapping class indices (integers)\n                 to a weight (float) value, used for weighting the loss function\n-                (during training only). This can be useful to tell the model to\n-                \"pay more attention\" to samples from an under-represented class.\n+                (during training only).\n+                This can be useful to tell the model to\n+                \"pay more attention\" to samples from\n+                an under-represented class.\n             sample_weight: Optional Numpy array of weights for\n                 the training samples, used for weighting the loss function\n                 (during training only). You can either pass a flat (1D)\n                 Numpy array with the same length as the input samples\n                 (1:1 mapping between weights and samples),\n                 or in the case of temporal data,\n-                you can pass a 2D array with shape `(samples, sequence_length)`,\n+                you can pass a 2D array with shape\n+                `(samples, sequence_length)`,\n                 to apply a different weight to every timestep of every sample.\n                 In this case you should make sure to specify\n                 `sample_weight_mode=\"temporal\"` in `compile()`.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e3e97c401aa8251e957b73fba8ed4d108c106f12", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 393 | Contributors (this commit): 15 | Commits (past 90d): 11 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -23,4 +23,4 @@ from .layers import Input\n from .models import Model\r\n from .models import Sequential\r\n \r\n-__version__ = '2.0.9'\r\n+__version__ = '2.1.0'\r\n\n@@ -3,12 +3,12 @@ from setuptools import find_packages\n \n \n setup(name='Keras',\n-      version='2.0.9',\n+      version='2.1.0',\n       description='Deep Learning for Python',\n       author='Francois Chollet',\n       author_email='francois.chollet@gmail.com',\n       url='https://github.com/fchollet/keras',\n-      download_url='https://github.com/fchollet/keras/tarball/2.0.9',\n+      download_url='https://github.com/fchollet/keras/tarball/2.1.0',\n       license='MIT',\n       install_requires=['numpy>=1.9.1',\n                         'scipy>=0.14',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b17169ca5d6cd1c8aeb237fc2bb0555c9e1b6a02", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 18 | Churn Cumulative: 520 | Contributors (this commit): 13 | Commits (past 90d): 4 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -19,6 +19,8 @@ def to_categorical(y, num_classes=None):\n     \"\"\"\n     y = np.array(y, dtype='int')\n     input_shape = y.shape\n+    if input_shape and input_shape[-1] == 1:\n+        input_shape = tuple(input_shape[:-1])\n     y = y.ravel()\n     if not num_classes:\n         num_classes = np.max(y) + 1\n\n@@ -7,18 +7,24 @@ from keras.utils import to_categorical\n \n def test_to_categorical():\n     num_classes = 5\n-    shapes = [(3,), (4, 3), (5, 4, 3)]\n+    shapes = [(3,), (4, 3), (5, 4, 3), (3, 1), (3, 2, 1)]\n+    expected_shapes = [(3, num_classes),\n+                       (4, 3, num_classes),\n+                       (5, 4, 3, num_classes),\n+                       (3, num_classes)]\n     labels = [np.random.randint(0, num_classes, shape) for shape in shapes]\n     one_hots = [to_categorical(label, num_classes) for label in labels]\n-    for label, one_hot in zip(labels, one_hots):\n+    for label, one_hot, expected_shape in zip(labels,\n+                                              one_hots,\n+                                              expected_shapes):\n         # Check shape\n-        assert one_hot.shape == label.shape + (num_classes,)\n+        assert one_hot.shape == expected_shape\n         # Make sure there are only 0s and 1s\n         assert np.array_equal(one_hot, one_hot.astype(bool))\n-        # Make sure there is only one 1 in a row\n+        # Make sure there is exactly one 1 in a row\n         assert np.all(one_hot.sum(axis=-1) == 1)\n         # Get original labels back from one hots\n-        assert np.all(np.argmax(one_hot, -1) == label)\n+        assert np.all(np.argmax(one_hot, -1).reshape(label.shape) == label)\n \n \n if __name__ == '__main__':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#58c219164a0050a38cdafea7a6ff3d7181855326", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 181 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -89,7 +89,7 @@ def multi_gpu_model(model, gpus):\n     # On model saving\n \n     To save the multi-gpu model, use `.save(fname)` or `.save_weights(fname)`\n-    with the template model (the argument you passed to `multi_gpu_model),\n+    with the template model (the argument you passed to `multi_gpu_model`),\n     rather than the model returned by `multi_gpu_model`.\n     \"\"\"\n     if K.backend() != 'tensorflow':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#06453e661dd93348e97a49b6eb8d524c83b9b981", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 4 | Churn Cumulative: 912 | Contributors (this commit): 13 | Commits (past 90d): 4 | Contributors (cumulative): 13 | DMM Complexity: 0.0\n\nDIFF:\n@@ -305,6 +305,10 @@ class Bidirectional(Wrapper):\n         # Properly set learning phase\n         if (getattr(y, '_uses_learning_phase', False) or\n            getattr(y_rev, '_uses_learning_phase', False)):\n+            if self.merge_mode is None:\n+                for out in output:\n+                    out._uses_learning_phase = True\n+            else:\n                 output._uses_learning_phase = True\n         return output\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#21b80642d0d050bf40c66c2841955ffb48506b66", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 9810 | Contributors (this commit): 79 | Commits (past 90d): 11 | Contributors (cumulative): 79 | DMM Complexity: None\n\nDIFF:\n@@ -1072,7 +1072,7 @@ class Sequential(Model):\n         return self.model.test_on_batch(x, y,\n                                         sample_weight=sample_weight)\n \n-    def predict_proba(self, x, batch_size=32, verbose=1):\n+    def predict_proba(self, x, batch_size=32, verbose=0):\n         \"\"\"Generates class probability predictions for the input samples.\n \n         The input samples are processed batch by batch.\n@@ -1094,7 +1094,7 @@ class Sequential(Model):\n                           '(like softmax or sigmoid would).')\n         return preds\n \n-    def predict_classes(self, x, batch_size=32, verbose=1):\n+    def predict_classes(self, x, batch_size=32, verbose=0):\n         \"\"\"Generate class predictions for the input samples.\n \n         The input samples are processed batch by batch.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6d95e7ceb77b94be104750c1e8d60197aa2dfebe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 229 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -153,5 +153,5 @@ test_model.summary()\n \n loss, acc = test_model.evaluate(x_test,\n                                 keras.utils.to_categorical(y_test),\n-                                num_classes)\n+                                batch_size=batch_size)\n print('\\nTest accuracy: {0}'.format(acc))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#31fe12a65d82fffa0ef1443810f3957db3c4d21d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 9814 | Contributors (this commit): 79 | Commits (past 90d): 12 | Contributors (cumulative): 79 | DMM Complexity: None\n\nDIFF:\n@@ -847,7 +847,7 @@ class Sequential(Model):\n             steps_per_epoch=None,\n             validation_steps=None,\n             **kwargs):\n-        \"\"\"Trains the model for a fixed number of epochs.\n+        \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset).\n \n         # Arguments\n             x: Numpy array of training data.\n@@ -876,7 +876,7 @@ class Sequential(Model):\n             callbacks: List of `keras.callbacks.Callback` instances.\n                 List of callbacks to apply during training.\n                 See [callbacks](/callbacks).\n-            validation_split: Float between 0 and 1:\n+            validation_split: Float between 0 and 1.\n                 Fraction of the training data to be used as validation data.\n                 The model will set apart this fraction of the training data,\n                 will not train on it, and will evaluate\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
