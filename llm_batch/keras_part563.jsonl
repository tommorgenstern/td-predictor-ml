{"custom_id": "keras#3fa6cfd8b8e36f441d3ce311fe99f687704eeb94", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 111 | Lines Deleted: 26 | Files Changed: 4 | Hunks: 8 | Methods Changed: 5 | Complexity Δ (Sum/Max): 10/7 | Churn Δ: 137 | Churn Cumulative: 960 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 5 | DMM Complexity: 0.27906976744186046\n\nDIFF:\n@@ -30,15 +30,16 @@ def load_data(path=\"california_housing.npz\", test_split=0.2, seed=113):\n     - Latitude: block group latitude\n     - Longitude: block group longitude\n \n-    This dataset was derived from the 1990 U.S. census, using one row per census\n-    block group. A block group is the smallest geographical unit for which the U.S.\n-    Census Bureau publishes sample data (a block group typically has a population\n-    of 600 to 3,000 people).\n+    This dataset was derived from the 1990 U.S. census, using one row\n+    per census block group. A block group is the smallest geographical\n+    unit for which the U.S. Census Bureau publishes sample data\n+    (a block group typically has a population of 600 to 3,000 people).\n \n-    A household is a group of people residing within a home. Since the average\n-    number of rooms and bedrooms in this dataset are provided per household, these\n-    columns may take surprisingly large values for block groups with few households\n-    and many empty houses, such as vacation resorts.\n+    A household is a group of people residing within a home.\n+    Since the average number of rooms and bedrooms in this dataset are\n+    provided per household, these columns may take surprisingly large\n+    values for block groups with few households and many empty houses,\n+    such as vacation resorts.\n \n     Args:\n         path: path where to cache the dataset locally\n@@ -54,9 +55,10 @@ def load_data(path=\"california_housing.npz\", test_split=0.2, seed=113):\n       containing either the training samples (for `x_train`),\n       or test samples (for `y_train`).\n \n-    **`y_train`, `y_test`**: numpy arrays of shape `(num_samples,)` containing the\n-      target scalars. The targets are float scalars typically between 25,000 and\n-      500,000 that represent the home prices in dollars.\n+    **`y_train`, `y_test`**: numpy arrays of shape `(num_samples,)`\n+        containing the target scalars. The targets are float scalars\n+        typically between 25,000 and 500,000 that represent\n+        the home prices in dollars.\n     \"\"\"\n     assert 0 <= test_split < 1\n     origin_folder = (\n\n@@ -4,5 +4,6 @@ from keras_core.saving.object_registration import get_custom_objects\n from keras_core.saving.object_registration import get_registered_name\n from keras_core.saving.object_registration import get_registered_object\n from keras_core.saving.object_registration import register_keras_serializable\n+from keras_core.saving.saving_api import load_model\n from keras_core.saving.serialization_lib import deserialize_keras_object\n from keras_core.saving.serialization_lib import serialize_keras_object\n\n@@ -0,0 +1,85 @@\n+import os\n+import zipfile\n+\n+from tensorflow.io import gfile\n+\n+from keras_core.api_export import keras_core_export\n+\n+\n+@keras_core_export(\n+    [\"keras_core.saving.load_model\", \"keras_core.models.load_model\"]\n+)\n+def load_model(filepath, custom_objects=None, compile=True, safe_mode=True):\n+    \"\"\"Loads a model saved via `model.save()`.\n+\n+    Args:\n+        filepath: `str` or `pathlib.Path` object, path to the saved model file.\n+        custom_objects: Optional dictionary mapping names\n+            (strings) to custom classes or functions to be\n+            considered during deserialization.\n+        compile: Boolean, whether to compile the model after loading.\n+        safe_mode: Boolean, whether to disallow unsafe `lambda` deserialization.\n+            When `safe_mode=False`, loading an object has the potential to\n+            trigger arbitrary code execution. This argument is only\n+            applicable to the Keras v3 model format. Defaults to True.\n+\n+    Returns:\n+        A Keras model instance. If the original model was compiled,\n+        and the argument `compile=True` is set, then the returned model\n+        will be compiled. Otherwise, the model will be left uncompiled.\n+\n+    Example:\n+\n+    ```python\n+    model = keras_core.Sequential([\n+        keras_core.layers.Dense(5, input_shape=(3,)),\n+        keras_core.layers.Softmax()])\n+    model.save(\"model.keras\")\n+    loaded_model = keras_core.saving.load_model(\"model.keras\")\n+    x = np.random.random((10, 3))\n+    assert np.allclose(model.predict(x), loaded_model.predict(x))\n+    ```\n+\n+    Note that the model variables may have different name values\n+    (`var.name` property, e.g. `\"dense_1/kernel:0\"`) after being reloaded.\n+    It is recommended that you use layer attributes to\n+    access specific variables, e.g. `model.get_layer(\"dense_1\").kernel`.\n+    \"\"\"\n+    from keras_core.saving import saving_lib\n+\n+    is_keras_zip = str(filepath).endswith(\".keras\") and zipfile.is_zipfile(\n+        filepath\n+    )\n+\n+    # Support for remote zip files\n+    if (\n+        saving_lib.is_remote_path(filepath)\n+        and not gfile.isdir(filepath)\n+        and not is_keras_zip\n+    ):\n+        local_path = os.path.join(\n+            saving_lib.get_temp_dir(), os.path.basename(filepath)\n+        )\n+\n+        # Copy from remote to temporary local directory\n+        gfile.copy(filepath, local_path, overwrite=True)\n+\n+        # Switch filepath to local zipfile for loading model\n+        if zipfile.is_zipfile(local_path):\n+            filepath = local_path\n+            is_keras_zip = True\n+\n+    if is_keras_zip:\n+        return saving_lib.load_model(\n+            filepath,\n+            custom_objects=custom_objects,\n+            compile=compile,\n+            safe_mode=safe_mode,\n+        )\n+    else:\n+        raise ValueError(\n+            \"The file you are trying to load does not appear to \"\n+            \"be a valid `.keras` model file. If it is a legacy \"\n+            \"`.h5` or SavedModel file, do note that these formats \"\n+            \"are not supported in Keras Core.\"\n+        )\n\n@@ -352,12 +352,11 @@ class SavingTest(testing.TestCase):\n             mock_warn.call_args_list[0][0][0],\n         )\n \n-    # TODO: need Embedding layer\n-    # def test_compile_overridden_warnings_sequential(self):\n-    #     self._test_compile_overridden_warnings(\"sequential\")\n+    def test_compile_overridden_warnings_sequential(self):\n+        self._test_compile_overridden_warnings(\"sequential\")\n \n-    # def test_compile_overridden_warnings_subclassed(self):\n-    #     self._test_compile_overridden_warnings(\"subclassed\")\n+    def test_compile_overridden_warnings_subclassed(self):\n+        self._test_compile_overridden_warnings(\"subclassed\")\n \n     def test_metadata(self):\n         temp_filepath = Path(\n@@ -390,16 +389,14 @@ class SavingTest(testing.TestCase):\n     #         self.assertIn(str(temp_filepath), mock_re_match.call_args.args)\n     #         self.assertIn(str(temp_filepath), mock_copy.call_args.args)\n \n-    # def test_load_model_api_endpoint(self):\n-    #     temp_filepath = Path(\n-    #         os.path.join(self.get_temp_dir(), \"mymodel.keras\")\n-    #     )\n-    #     model = self._get_functional_model()\n-    #     ref_input = np.random.random((10, 32))\n-    #     ref_output = model.predict(ref_input)\n-    #     model.save(temp_filepath)\n-    #     model = keras_core.models.load_model(temp_filepath)\n-    #     self.assertAllClose(model.predict(ref_input), ref_output, atol=1e-6)\n+    def test_load_model_api_endpoint(self):\n+        temp_filepath = Path(os.path.join(self.get_temp_dir(), \"mymodel.keras\"))\n+        model = self._get_basic_functional_model()\n+        ref_input = np.random.random((2, 4))\n+        ref_output = model.predict(ref_input)\n+        model.save(temp_filepath)\n+        model = keras_core.saving.load_model(temp_filepath)\n+        self.assertAllClose(model.predict(ref_input), ref_output, atol=1e-6)\n \n     def test_save_load_weights_only(self):\n         temp_filepath = Path(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9af6d93937037f9a505677df04e7a3e5428a4831", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 86 | Lines Deleted: 69 | Files Changed: 4 | Hunks: 13 | Methods Changed: 12 | Complexity Δ (Sum/Max): 11/11 | Churn Δ: 155 | Churn Cumulative: 1214 | Contributors (this commit): 7 | Commits (past 90d): 31 | Contributors (cumulative): 13 | DMM Complexity: 0.7741935483870968\n\nDIFF:\n@@ -74,19 +74,13 @@ class AdamWTest(testing.TestCase):\n             optimizer.apply_gradients(zip([grads], [x]))\n \n     def test_clip_norm(self):\n-        # TODO: implement clip_gradients, then uncomment\n-        pass\n-\n-    #     optimizer = AdamW(clipnorm=1)\n-    #     grad = [np.array([100.0, 100.0])]\n-    #     clipped_grad = optimizer._clip_gradients(grad)\n-    #     self.assertAllClose(clipped_grad[0], [2**0.5 / 2, 2**0.5 / 2])\n+        optimizer = AdamW(clipnorm=1)\n+        grad = [np.array([100.0, 100.0])]\n+        clipped_grad = optimizer._clip_gradients(grad)\n+        self.assertAllClose(clipped_grad[0], [2**0.5 / 2, 2**0.5 / 2])\n \n     def test_clip_value(self):\n-        # TODO: implement clip_gradients, then uncomment\n-        pass\n-\n-    #     optimizer = AdamW(clipvalue=1)\n-    #     grad = [np.array([100.0, 100.0])]\n-    #     clipped_grad = optimizer._clip_gradients(grad)\n-    #     self.assertAllClose(clipped_grad[0], [1.0, 1.0])\n+        optimizer = AdamW(clipvalue=1)\n+        grad = [np.array([100.0, 100.0])]\n+        clipped_grad = optimizer._clip_gradients(grad)\n+        self.assertAllClose(clipped_grad[0], [1.0, 1.0])\n\n@@ -8,6 +8,7 @@ from keras_core import initializers\n from keras_core import operations as ops\n from keras_core.api_export import keras_core_export\n from keras_core.optimizers.schedules import learning_rate_schedule\n+from keras_core.saving import serialization_lib\n from keras_core.utils.naming import auto_name\n from keras_core.utils.tracking import Tracker\n \n@@ -329,34 +330,27 @@ class Optimizer:\n \n     def _clip_gradients(self, grads):\n         if self.clipnorm and self.clipnorm > 0:\n-            raise NotImplementedError  # TODO\n-            # clipped_grads = []\n-            # for g in grads:\n-            #     if g is None:\n-            #         clipped_grads.append(g)\n-            #     else:\n-            #         clipped_grads.append(tf.clip_by_norm(g, self.clipnorm))\n-            # return clipped_grads\n+            clipped_grads = []\n+            for g in grads:\n+                if g is None:\n+                    clipped_grads.append(g)\n+                else:\n+                    clipped_grads.append(clip_by_norm(g, self.clipnorm))\n+            return clipped_grads\n \n         if self.global_clipnorm and self.global_clipnorm > 0:\n-            raise NotImplementedError  # TODO\n-            # return tf.clip_by_global_norm(grads, self.global_clipnorm)[0]\n+            return clip_by_global_norm(grads, self.global_clipnorm)\n \n         if self.clipvalue and self.clipvalue > 0:\n-            raise NotImplementedError  # TODO\n-            # clipped_grads = []\n-            # for g in grads:\n-            #     if g is None:\n-            #         clipped_grads.append(g)\n-            #     else:\n-            #         clipped_grads.append(\n-            #             tf.clip_by_value(\n-            #                 g,\n-            #                 clip_value_min=-self.clipvalue,\n-            #                 clip_value_max=self.clipvalue,\n-            #             )\n-            #         )\n-            # return clipped_grads\n+            clipped_grads = []\n+            for g in grads:\n+                if g is None:\n+                    clipped_grads.append(g)\n+                else:\n+                    clipped_grads.append(\n+                        ops.clip(g, -self.clipvalue, self.clipvalue)\n+                    )\n+            return clipped_grads\n         return grads\n \n     def exclude_from_weight_decay(self, var_list=None, var_names=None):\n@@ -491,8 +485,9 @@ class Optimizer:\n         elif ops.is_tensor(self._learning_rate):\n             learning_rate = float(self._learning_rate)\n         elif callable(self._learning_rate):\n-            # TODO: serialize custom object\n-            learning_rate = self._learning_rate\n+            learning_rate = serialization_lib.serialize_keras_object(\n+                self._learning_rate\n+            )\n \n         config = {\n             \"name\": self.name,\n@@ -524,7 +519,9 @@ class Optimizer:\n         \"\"\"\n         if \"learning_rate\" in config:\n             if isinstance(config[\"learning_rate\"], dict):\n-                config[\"learning_rate\"] = learning_rate_schedule.deserialize(\n+                config[\n+                    \"learning_rate\"\n+                ] = serialization_lib.deserialize_keras_object(\n                     config[\"learning_rate\"], custom_objects=custom_objects\n                 )\n         return cls(**config)\n@@ -561,3 +558,41 @@ base_optimizer_keyword_args = \"\"\"name: String. The name to use\n           variables in-place). When using the built-in `fit()` training loop,\n           this happens automatically after the last epoch,\n           and you don't need to do anything.\"\"\"\n+\n+\n+def clip_by_norm(values, clip_norm, axes=None):\n+    # Calculate L2-norm, clip elements by ratio of clip_norm to L2-norm\n+    l2sum = ops.sum(values * values, axes, keepdims=True)\n+    pred = l2sum > 0\n+    # Two-tap tf.where trick to bypass NaN gradients\n+    l2sum_safe = ops.where(pred, l2sum, ops.ones_like(l2sum))\n+    l2norm = ops.where(pred, ops.sqrt(l2sum_safe), l2sum)\n+    intermediate = values * clip_norm\n+    values_clip = intermediate / ops.maximum(l2norm, clip_norm)\n+    return values_clip\n+\n+\n+def global_norm(value_list):\n+    \"\"\"Computes the global norm of multiple tensors.\"\"\"\n+    squared_norms = []\n+    for v in value_list:\n+        if v is not None:\n+            squared_norms.append(ops.sum(ops.square(v)))\n+    squared_norm = ops.sum(ops.stack(squared_norms))\n+    return ops.sqrt(squared_norm)\n+\n+\n+def clip_by_global_norm(value_list, clip_norm):\n+    use_norm = global_norm(value_list)\n+    # Calculate L2-norm, clip elements by ratio of clip_norm to L2-norm\n+    scale_for_finite = clip_norm * ops.minimum(1.0 / use_norm, 1.0 / clip_norm)\n+    # If use_norm is any finite number, this is a no-op. For inf/-inf/NaN,\n+    # this will make scale NaN.\n+    scale = scale_for_finite + (use_norm - use_norm)\n+    values_clipped = []\n+    for v in value_list:\n+        if v is None:\n+            values_clipped.append(None)\n+        else:\n+            values_clipped.append(v * scale)\n+    return values_clipped\n\n@@ -64,19 +64,13 @@ class RMSpropTest(testing.TestCase):\n             optimizer.apply_gradients(zip([grads], [x]))\n \n     def test_clip_norm(self):\n-        # TODO: implement clip_gradients, then uncomment\n-        pass\n-\n-    #     optimizer = RMSprop(clipnorm=1)\n-    #     grad = [np.array([100.0, 100.0])]\n-    #     clipped_grad = optimizer._clip_gradients(grad)\n-    #     self.assertAllClose(clipped_grad[0], [2**0.5 / 2, 2**0.5 / 2])\n+        optimizer = RMSprop(clipnorm=1)\n+        grad = [np.array([100.0, 100.0])]\n+        clipped_grad = optimizer._clip_gradients(grad)\n+        self.assertAllClose(clipped_grad[0], [2**0.5 / 2, 2**0.5 / 2])\n \n     def test_clip_value(self):\n-        # TODO: implement clip_gradients, then uncomment\n-        pass\n-\n-    #     optimizer = RMSprop(clipvalue=1)\n-    #     grad = [np.array([100.0, 100.0])]\n-    #     clipped_grad = optimizer._clip_gradients(grad)\n-    #     self.assertAllClose(clipped_grad[0], [1.0, 1.0])\n+        optimizer = RMSprop(clipvalue=1)\n+        grad = [np.array([100.0, 100.0])]\n+        clipped_grad = optimizer._clip_gradients(grad)\n+        self.assertAllClose(clipped_grad[0], [1.0, 1.0])\n\n@@ -72,19 +72,13 @@ class SGDTest(testing.TestCase):\n             optimizer.apply_gradients(zip([grads], [x]))\n \n     def test_clip_norm(self):\n-        # TODO: implement clip_gradients, then uncomment\n-        pass\n-\n-        #     optimizer = SGD(clipnorm=1)\n-        #     grad = [np.array([100.0, 100.0])]\n-        #     clipped_grad = optimizer._clip_gradients(grad)\n-        #     self.assertAllClose(clipped_grad[0], [2**0.5 / 2, 2**0.5 / 2])\n+        optimizer = SGD(clipnorm=1)\n+        grad = [np.array([100.0, 100.0])]\n+        clipped_grad = optimizer._clip_gradients(grad)\n+        self.assertAllClose(clipped_grad[0], [2**0.5 / 2, 2**0.5 / 2])\n \n     def test_clip_value(self):\n-        # TODO: implement clip_gradients, then uncomment\n-        pass\n-\n-        #     optimizer = SGD(clipvalue=1)\n-        #     grad = [np.array([100.0, 100.0])]\n-        #     clipped_grad = optimizer._clip_gradients(grad)\n-        #     self.assertAllClose(clipped_grad[0], [1.0, 1.0])\n+        optimizer = SGD(clipvalue=1)\n+        grad = [np.array([100.0, 100.0])]\n+        clipped_grad = optimizer._clip_gradients(grad)\n+        self.assertAllClose(clipped_grad[0], [1.0, 1.0])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9976e696d8797bfad42d5d28fc5e1cf7c6992f62", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 496 | Lines Deleted: 0 | Files Changed: 6 | Hunks: 15 | Methods Changed: 19 | Complexity Δ (Sum/Max): 32/14 | Churn Δ: 496 | Churn Cumulative: 2820 | Contributors (this commit): 8 | Commits (past 90d): 61 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,36 +2,44 @@ from keras_core.api_export import keras_core_export\n from keras_core.losses.loss import Loss\n from keras_core.losses.losses import CategoricalHinge\n from keras_core.losses.losses import Hinge\n+from keras_core.losses.losses import KLDivergence\n from keras_core.losses.losses import LossFunctionWrapper\n from keras_core.losses.losses import MeanAbsoluteError\n from keras_core.losses.losses import MeanAbsolutePercentageError\n from keras_core.losses.losses import MeanSquaredError\n from keras_core.losses.losses import MeanSquaredLogarithmicError\n+from keras_core.losses.losses import Poisson\n from keras_core.losses.losses import SquaredHinge\n from keras_core.losses.losses import categorical_hinge\n from keras_core.losses.losses import hinge\n+from keras_core.losses.losses import kl_divergence\n from keras_core.losses.losses import mean_absolute_error\n from keras_core.losses.losses import mean_absolute_percentage_error\n from keras_core.losses.losses import mean_squared_error\n from keras_core.losses.losses import mean_squared_logarithmic_error\n+from keras_core.losses.losses import poisson\n from keras_core.losses.losses import squared_hinge\n from keras_core.saving import serialization_lib\n \n ALL_OBJECTS = {\n     Loss,\n     LossFunctionWrapper,\n+    KLDivergence,\n     MeanSquaredError,\n     MeanAbsoluteError,\n     MeanAbsolutePercentageError,\n     MeanSquaredLogarithmicError,\n     Hinge,\n+    Poisson,\n     SquaredHinge,\n     CategoricalHinge,\n+    kl_divergence,\n     mean_squared_error,\n     mean_absolute_error,\n     mean_absolute_percentage_error,\n     mean_squared_logarithmic_error,\n     hinge,\n+    poisson,\n     squared_hinge,\n     categorical_hinge,\n }\n\n@@ -264,6 +264,54 @@ class CategoricalHinge(LossFunctionWrapper):\n         return Loss.get_config(self)\n \n \n+@keras_core_export(\"keras_core.losses.KLDivergence\")\n+class KLDivergence(LossFunctionWrapper):\n+    \"\"\"Computes Kullback-Leibler divergence loss between `y_true` & `y_pred`.\n+\n+    Formula:\n+\n+    ```python\n+    loss = y_true * log(y_true / y_pred)\n+    ```\n+\n+    Args:\n+        reduction: Type of reduction to apply to loss. For almost all cases\n+            this defaults to `\"sum_over_batch_size\"`. Options are `\"sum\"`,\n+            `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the instance. Defaults to 'kl_divergence'.\n+    \"\"\"\n+\n+    def __init__(self, reduction=\"sum_over_batch_size\", name=\"kl_divergence\"):\n+        super().__init__(kl_divergence, reduction=reduction, name=name)\n+\n+    def get_config(self):\n+        return Loss.get_config(self)\n+\n+\n+@keras_core_export(\"keras_core.losses.Poisson\")\n+class Poisson(LossFunctionWrapper):\n+    \"\"\"Computes the Poisson loss between `y_true` & `y_pred`.\n+\n+    Formula:\n+\n+    ```python\n+    loss = y_pred - y_true * log(y_pred)\n+    ```\n+\n+    Args:\n+        reduction: Type of reduction to apply to loss. For almost all cases\n+            this defaults to `\"sum_over_batch_size\"`. Options are `\"sum\"`,\n+            `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the instance. Defaults to `\"poisson\"`\n+    \"\"\"\n+\n+    def __init__(self, reduction=\"sum_over_batch_size\", name=\"poisson\"):\n+        super().__init__(poisson, reduction=reduction, name=name)\n+\n+    def get_config(self):\n+        return Loss.get_config(self)\n+\n+\n def convert_binary_labels_to_hinge(y_true):\n     \"\"\"Converts binary labels into -1/1 for hinge loss/metric calculation.\"\"\"\n     are_zeros = ops.equal(y_true, 0)\n@@ -582,3 +630,82 @@ def cosine_similarity(y_true, y_pred, axis=-1):\n     y_pred = normalize(y_pred, axis=axis)\n     y_true = normalize(y_true, axis=axis)\n     return -ops.sum(y_true * y_pred, axis=axis)\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.metrics.kl_divergence\",\n+        \"keras_core.losses.kl_divergence\",\n+    ]\n+)\n+def kl_divergence(y_true, y_pred):\n+    \"\"\"Computes Kullback-Leibler divergence loss between `y_true` & `y_pred`.\n+\n+    Formula:\n+\n+    ```python\n+    loss = y_true * log(y_true / y_pred)\n+    ```\n+\n+    Standalone usage:\n+\n+    >>> y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float32)\n+    >>> y_pred = np.random.random(size=(2, 3))\n+    >>> loss = keras_core.losses.kl_divergence(y_true, y_pred)\n+    >>> assert loss.shape == (2,)\n+    >>> y_true = ops.clip(y_true, 1e-7, 1)\n+    >>> y_pred = ops.clip(y_pred, 1e-7, 1)\n+    >>> assert np.array_equal(\n+    ...     loss, np.sum(y_true * np.log(y_true / y_pred), axis=-1))\n+\n+    Args:\n+        y_true: Tensor of true targets.\n+        y_pred: Tensor of predicted targets.\n+\n+    Returns:\n+        KL Divergence loss values with shape = `[batch_size, d0, .. dN-1]`.\n+    \"\"\"\n+    y_pred = ops.convert_to_tensor(y_pred)\n+    y_true = ops.convert_to_tensor(y_true, y_pred.dtype)\n+    y_true = ops.clip(y_true, backend.epsilon(), 1)\n+    y_pred = ops.clip(y_pred, backend.epsilon(), 1)\n+    return ops.sum(y_true * ops.log(y_true / y_pred), axis=-1)\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.metrics.poisson\",\n+        \"keras_core.losses.poisson\",\n+    ]\n+)\n+def poisson(y_true, y_pred):\n+    \"\"\"Computes the Poisson loss between y_true and y_pred.\n+\n+    Formula:\n+\n+    ```python\n+    loss = y_pred - y_true * log(y_pred)\n+    ```\n+\n+    Standalone usage:\n+\n+    >>> y_true = np.random.randint(0, 2, size=(2, 3))\n+    >>> y_pred = np.random.random(size=(2, 3))\n+    >>> loss = keras_core.losses.poisson(y_true, y_pred)\n+    >>> assert loss.shape == (2,)\n+    >>> y_pred = y_pred + 1e-7\n+    >>> assert np.allclose(\n+    ...     loss, np.mean(y_pred - y_true * np.log(y_pred), axis=-1),\n+    ...     atol=1e-5)\n+\n+    Args:\n+        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n+        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n+\n+    Returns:\n+        Poisson loss values with shape = `[batch_size, d0, .. dN-1]`.\n+    \"\"\"\n+    y_pred = ops.convert_to_tensor(y_pred)\n+    y_true = ops.convert_to_tensor(y_true, dtype=y_pred.dtype)\n+    epsilon = ops.convert_to_tensor(backend.epsilon())\n+    return ops.mean(y_pred - y_true * ops.log(y_pred + epsilon), axis=-1)\n\n@@ -461,3 +461,171 @@ class CosineSimilarityTest(testing.TestCase):\n         loss = cosine_obj(self.y_true, self.y_pred)\n         expected_loss = -np.mean(self.expected_loss)\n         self.assertAlmostEqual(loss, expected_loss, 3)\n+\n+\n+class KLDivergenceTest(testing.TestCase):\n+    def setup(self):\n+        self.y_pred = np.asarray(\n+            [0.4, 0.9, 0.12, 0.36, 0.3, 0.4], dtype=np.float32\n+        ).reshape((2, 3))\n+        self.y_true = np.asarray(\n+            [0.5, 0.8, 0.12, 0.7, 0.43, 0.8], dtype=np.float32\n+        ).reshape((2, 3))\n+\n+        self.batch_size = 2\n+        self.expected_losses = np.multiply(\n+            self.y_true, np.log(self.y_true / self.y_pred)\n+        )\n+\n+    def test_config(self):\n+        k_obj = losses.KLDivergence(reduction=\"sum\", name=\"kld\")\n+        self.assertEqual(k_obj.name, \"kld\")\n+        self.assertEqual(k_obj.reduction, \"sum\")\n+\n+    def test_unweighted(self):\n+        self.setup()\n+        k_obj = losses.KLDivergence()\n+\n+        loss = k_obj(self.y_true, self.y_pred)\n+        expected_loss = np.sum(self.expected_losses) / self.batch_size\n+        self.assertAlmostEqual(loss, expected_loss, 3)\n+\n+    def test_scalar_weighted(self):\n+        self.setup()\n+        k_obj = losses.KLDivergence()\n+        sample_weight = 2.3\n+\n+        loss = k_obj(self.y_true, self.y_pred, sample_weight=sample_weight)\n+        expected_loss = (\n+            sample_weight * np.sum(self.expected_losses) / self.batch_size\n+        )\n+        self.assertAlmostEqual(loss, expected_loss, 3)\n+\n+        # Verify we get the same output when the same input is given\n+        loss_2 = k_obj(self.y_true, self.y_pred, sample_weight=sample_weight)\n+        self.assertAlmostEqual(loss, loss_2, 3)\n+\n+    def test_sample_weighted(self):\n+        self.setup()\n+        k_obj = losses.KLDivergence()\n+        sample_weight = np.asarray([1.2, 3.4], dtype=np.float32).reshape((2, 1))\n+        loss = k_obj(self.y_true, self.y_pred, sample_weight=sample_weight)\n+\n+        expected_loss = np.multiply(\n+            self.expected_losses,\n+            np.asarray(\n+                [1.2, 1.2, 1.2, 3.4, 3.4, 3.4], dtype=np.float32\n+            ).reshape(2, 3),\n+        )\n+        expected_loss = np.sum(expected_loss) / self.batch_size\n+        self.assertAlmostEqual(loss, expected_loss, 3)\n+\n+    def test_timestep_weighted(self):\n+        self.setup()\n+        k_obj = losses.KLDivergence()\n+        y_true = self.y_true.reshape(2, 3, 1)\n+        y_pred = self.y_pred.reshape(2, 3, 1)\n+        sample_weight = np.asarray([3, 6, 5, 0, 4, 2]).reshape(2, 3)\n+        expected_losses = np.sum(\n+            np.multiply(y_true, np.log(y_true / y_pred)), axis=-1\n+        )\n+        loss = k_obj(y_true, y_pred, sample_weight=sample_weight)\n+\n+        num_timesteps = 3\n+        expected_loss = np.sum(expected_losses * sample_weight) / (\n+            self.batch_size * num_timesteps\n+        )\n+        self.assertAlmostEqual(loss, expected_loss, 3)\n+\n+    def test_zero_weighted(self):\n+        self.setup()\n+        k_obj = losses.KLDivergence()\n+        loss = k_obj(self.y_true, self.y_pred, sample_weight=0)\n+        self.assertAlmostEqual(loss, 0.0, 3)\n+\n+\n+class PoissonTest(testing.TestCase):\n+    def setup(self):\n+        self.y_pred = np.asarray([1, 9, 2, 5, 2, 6], dtype=np.float32).reshape(\n+            (2, 3)\n+        )\n+        self.y_true = np.asarray([4, 8, 12, 8, 1, 3], dtype=np.float32).reshape(\n+            (2, 3)\n+        )\n+\n+        self.batch_size = 6\n+        self.expected_losses = self.y_pred - np.multiply(\n+            self.y_true, np.log(self.y_pred)\n+        )\n+\n+    def test_config(self):\n+        poisson_obj = losses.Poisson(reduction=\"sum\", name=\"poisson\")\n+        self.assertEqual(poisson_obj.name, \"poisson\")\n+        self.assertEqual(poisson_obj.reduction, \"sum\")\n+\n+    def test_unweighted(self):\n+        self.setup()\n+        poisson_obj = losses.Poisson()\n+\n+        loss = poisson_obj(self.y_true, self.y_pred)\n+        expected_loss = np.sum(self.expected_losses) / self.batch_size\n+        self.assertAlmostEqual(loss, expected_loss, 3)\n+\n+    def test_scalar_weighted(self):\n+        self.setup()\n+        poisson_obj = losses.Poisson()\n+        sample_weight = 2.3\n+        loss = poisson_obj(\n+            self.y_true, self.y_pred, sample_weight=sample_weight\n+        )\n+        expected_loss = (\n+            sample_weight * np.sum(self.expected_losses) / self.batch_size\n+        )\n+        self.assertAlmostEqual(loss, expected_loss, 3)\n+        self.assertAlmostEqual(loss, expected_loss, 3)\n+\n+        # Verify we get the same output when the same input is given\n+        loss_2 = poisson_obj(\n+            self.y_true, self.y_pred, sample_weight=sample_weight\n+        )\n+        self.assertAlmostEqual(loss, loss_2, 3)\n+\n+    def test_sample_weighted(self):\n+        self.setup()\n+        poisson_obj = losses.Poisson()\n+\n+        sample_weight = np.asarray([1.2, 3.4]).reshape((2, 1))\n+        loss = poisson_obj(\n+            self.y_true, self.y_pred, sample_weight=sample_weight\n+        )\n+\n+        expected_loss = np.multiply(\n+            self.expected_losses,\n+            np.asarray([1.2, 1.2, 1.2, 3.4, 3.4, 3.4]).reshape((2, 3)),\n+        )\n+        expected_loss = np.sum(expected_loss) / self.batch_size\n+        self.assertAlmostEqual(loss, expected_loss, 3)\n+\n+    def test_timestep_weighted(self):\n+        self.setup()\n+        poisson_obj = losses.Poisson()\n+        y_true = self.y_true.reshape(2, 3, 1)\n+        y_pred = self.y_pred.reshape(2, 3, 1)\n+        sample_weight = np.asarray([3, 6, 5, 0, 4, 2]).reshape(2, 3, 1)\n+        expected_losses = y_pred - np.multiply(y_true, np.log(y_pred))\n+\n+        loss = poisson_obj(\n+            y_true,\n+            y_pred,\n+            sample_weight=np.asarray(sample_weight).reshape((2, 3)),\n+        )\n+        expected_loss = (\n+            np.sum(expected_losses * sample_weight) / self.batch_size\n+        )\n+        self.assertAlmostEqual(loss, expected_loss, 3)\n+\n+    def test_zero_weighted(self):\n+        self.setup()\n+        poisson_obj = losses.Poisson()\n+        loss = poisson_obj(self.y_true, self.y_pred, sample_weight=0)\n+        self.assertAlmostEqual(loss, 0.0, 3)\n\n@@ -8,6 +8,8 @@ from keras_core.metrics.hinge_metrics import CategoricalHinge\n from keras_core.metrics.hinge_metrics import Hinge\n from keras_core.metrics.hinge_metrics import SquaredHinge\n from keras_core.metrics.metric import Metric\n+from keras_core.metrics.probabilistic_metrics import KLDivergence\n+from keras_core.metrics.probabilistic_metrics import Poisson\n from keras_core.metrics.reduction_metrics import Mean\n from keras_core.metrics.reduction_metrics import MeanMetricWrapper\n from keras_core.metrics.reduction_metrics import Sum\n@@ -26,6 +28,8 @@ ALL_OBJECTS = {\n     Hinge,\n     SquaredHinge,\n     CategoricalHinge,\n+    KLDivergence,\n+    Poisson,\n }\n ALL_OBJECTS_DICT = {cls.__name__: cls for cls in ALL_OBJECTS}\n ALL_OBJECTS_DICT.update(\n\n@@ -0,0 +1,91 @@\n+from keras_core.api_export import keras_core_export\n+from keras_core.losses.losses import kl_divergence\n+from keras_core.losses.losses import poisson\n+from keras_core.metrics import reduction_metrics\n+\n+\n+@keras_core_export(\"keras_core.metrics.KLDivergence\")\n+class KLDivergence(reduction_metrics.MeanMetricWrapper):\n+    \"\"\"Computes Kullback-Leibler divergence metric between `y_true` and\n+    `y_pred`.\n+\n+    Formula:\n+\n+    ```python\n+    metric = y_true * log(y_true / y_pred)\n+    ```\n+\n+    Args:\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = keras_core.metrics.KLDivergence()\n+    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])\n+    >>> m.result()\n+    0.45814306\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]],\n+    ...                sample_weight=[1, 0])\n+    >>> m.result()\n+    0.9162892\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(optimizer='sgd',\n+                  loss='mse',\n+                  metrics=[keras_core.metrics.KLDivergence()])\n+    ```\n+    \"\"\"\n+\n+    def __init__(self, name=\"kl_divergence\", dtype=None):\n+        super().__init__(fn=kl_divergence, name=name, dtype=dtype)\n+\n+    def get_config(self):\n+        return {\"name\": self.name, \"dtype\": self.dtype}\n+\n+\n+@keras_core_export(\"keras_core.metrics.Poisson\")\n+class Poisson(reduction_metrics.MeanMetricWrapper):\n+    \"\"\"Computes the Poisson metric between `y_true` and `y_pred`.\n+\n+    Formula:\n+\n+    ```python\n+    metric = y_pred - y_true * log(y_pred)\n+    ```\n+\n+    Args:\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = keras_core.metrics.Poisson()\n+    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])\n+    >>> m.result()\n+    0.49999997\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],\n+    ...                sample_weight=[1, 0])\n+    >>> m.result()\n+    0.99999994\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(optimizer='sgd',\n+                  loss='mse',\n+                  metrics=[keras_core.metrics.Poisson()])\n+    ```\n+    \"\"\"\n+\n+    def __init__(self, name=\"poisson\", dtype=None):\n+        super().__init__(fn=poisson, name=name, dtype=dtype)\n+\n+    def get_config(self):\n+        return {\"name\": self.name, \"dtype\": self.dtype}\n\n@@ -0,0 +1,98 @@\n+import numpy as np\n+\n+from keras_core import metrics\n+from keras_core import testing\n+\n+\n+class KLDivergenceTest(testing.TestCase):\n+    def setup(self):\n+        self.y_pred = np.asarray(\n+            [0.4, 0.9, 0.12, 0.36, 0.3, 0.4], dtype=np.float32\n+        ).reshape((2, 3))\n+        self.y_true = np.asarray(\n+            [0.5, 0.8, 0.12, 0.7, 0.43, 0.8], dtype=np.float32\n+        ).reshape((2, 3))\n+\n+        self.batch_size = 2\n+        self.expected_results = np.multiply(\n+            self.y_true, np.log(self.y_true / self.y_pred)\n+        )\n+\n+    def test_config(self):\n+        k_obj = metrics.KLDivergence(name=\"kld\", dtype=\"int32\")\n+        self.assertEqual(k_obj.name, \"kld\")\n+        self.assertEqual(k_obj._dtype, \"int32\")\n+\n+        k_obj2 = metrics.KLDivergence.from_config(k_obj.get_config())\n+        self.assertEqual(k_obj2.name, \"kld\")\n+        self.assertEqual(k_obj2._dtype, \"int32\")\n+\n+    def test_unweighted(self):\n+        self.setup()\n+        k_obj = metrics.KLDivergence()\n+\n+        k_obj.update_state(self.y_true, self.y_pred)\n+        result = k_obj.result()\n+        expected_result = np.sum(self.expected_results) / self.batch_size\n+        self.assertAllClose(result, expected_result, atol=1e-3)\n+\n+    def test_weighted(self):\n+        self.setup()\n+        k_obj = metrics.KLDivergence()\n+\n+        sample_weight = np.asarray([1.2, 3.4], dtype=np.float32).reshape((2, 1))\n+        result = k_obj(self.y_true, self.y_pred, sample_weight=sample_weight)\n+\n+        sample_weight = np.asarray(\n+            [1.2, 1.2, 1.2, 3.4, 3.4, 3.4], dtype=np.float32\n+        ).reshape((2, 3))\n+        expected_result = np.multiply(self.expected_results, sample_weight)\n+        expected_result = np.sum(expected_result) / (1.2 + 3.4)\n+        self.assertAllClose(result, expected_result, atol=1e-3)\n+\n+\n+class PoissonTest(testing.TestCase):\n+    def setup(self):\n+        self.y_pred = np.asarray([1, 9, 2, 5, 2, 6], dtype=np.float32).reshape(\n+            (2, 3)\n+        )\n+        self.y_true = np.asarray([4, 8, 12, 8, 1, 3], dtype=np.float32).reshape(\n+            (2, 3)\n+        )\n+        self.batch_size = 6\n+        self.expected_results = self.y_pred - np.multiply(\n+            self.y_true, np.log(self.y_pred)\n+        )\n+\n+    def test_config(self):\n+        poisson_obj = metrics.Poisson(name=\"poisson\", dtype=\"float32\")\n+        self.assertEqual(poisson_obj.name, \"poisson\")\n+        self.assertEqual(poisson_obj._dtype, \"float32\")\n+\n+        poisson_obj2 = metrics.Poisson.from_config(poisson_obj.get_config())\n+        self.assertEqual(poisson_obj2.name, \"poisson\")\n+        self.assertEqual(poisson_obj2._dtype, \"float32\")\n+\n+    def test_unweighted(self):\n+        self.setup()\n+        poisson_obj = metrics.Poisson()\n+        poisson_obj.update_state(self.y_true, self.y_pred)\n+\n+        result = poisson_obj.result()\n+        expected_result = np.sum(self.expected_results) / self.batch_size\n+        self.assertAllClose(result, expected_result, atol=1e-3)\n+\n+    def test_weighted(self):\n+        self.setup()\n+        poisson_obj = metrics.Poisson()\n+        sample_weight = np.asarray([1.2, 3.4], dtype=np.float32).reshape((2, 1))\n+\n+        result = poisson_obj(\n+            self.y_true, self.y_pred, sample_weight=sample_weight\n+        )\n+        sample_weight = np.asarray(\n+            [1.2, 1.2, 1.2, 3.4, 3.4, 3.4], dtype=np.float32\n+        ).reshape((2, 3))\n+        expected_result = np.multiply(self.expected_results, sample_weight)\n+        expected_result = np.sum(expected_result) / np.sum(sample_weight)\n+        self.assertAllClose(result, expected_result, atol=1e-3)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f71b4d85b9cc96f31f3adf56ab58aee6a9e22abf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1010 | Lines Deleted: 102 | Files Changed: 1 | Hunks: 33 | Methods Changed: 20 | Complexity Δ (Sum/Max): 20/20 | Churn Δ: 1112 | Churn Cumulative: 2409 | Contributors (this commit): 6 | Commits (past 90d): 23 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,5 @@\n+import warnings\n+\n from keras_core import backend\n from keras_core import operations as ops\n from keras_core.api_export import keras_core_export\n@@ -43,10 +45,10 @@ class MeanSquaredError(LossFunctionWrapper):\n     ```\n \n     Args:\n-        reduction: Type of reduction to apply to loss. For almost all cases\n-            this defaults to `\"sum_over_batch_size\"`. Options are `\"sum\"`,\n-            `\"sum_over_batch_size\"` or `None`.\n-        name: Optional name for the instance.\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`.\n+            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the loss instance.\n     \"\"\"\n \n     def __init__(\n@@ -69,10 +71,10 @@ class MeanAbsoluteError(LossFunctionWrapper):\n     ```\n \n     Args:\n-        reduction: Type of reduction to apply to loss. For almost all cases\n-            this defaults to `\"sum_over_batch_size\"`. Options are `\"sum\"`,\n-            `\"sum_over_batch_size\"` or `None`.\n-        name: Optional name for the instance.\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`.\n+            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the loss instance.\n     \"\"\"\n \n     def __init__(\n@@ -95,10 +97,10 @@ class MeanAbsolutePercentageError(LossFunctionWrapper):\n     ```\n \n     Args:\n-        reduction: Type of reduction to apply to loss. For almost all cases\n-            this defaults to `\"sum_over_batch_size\"`. Options are `\"sum\"`,\n-            `\"sum_over_batch_size\"` or `None`.\n-        name: Optional name for the instance.\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`.\n+            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the loss instance.\n     \"\"\"\n \n     def __init__(\n@@ -125,10 +127,10 @@ class MeanSquaredLogarithmicError(LossFunctionWrapper):\n     ```\n \n     Args:\n-        reduction: Type of reduction to apply to loss. For almost all cases\n-            this defaults to `\"sum_over_batch_size\"`. Options are `\"sum\"`,\n-            `\"sum_over_batch_size\"` or `None`.\n-        name: Optional name for the instance.\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`.\n+            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the loss instance.\n     \"\"\"\n \n     def __init__(\n@@ -164,10 +166,10 @@ class CosineSimilarity(LossFunctionWrapper):\n     Args:\n         axis: The axis along which the cosine similarity is computed\n             (the features axis). Defaults to -1.\n-        reduction: Type of reduction to apply to loss. Options are `\"sum\"`,\n-            `\"sum_over_batch_size\"` or `None`. Defaults to\n-            `\"sum_over_batch_size\"`.\n-        name: Optional name for the instance.\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`.\n+            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the loss instance.\n     \"\"\"\n \n     def __init__(\n@@ -195,10 +197,10 @@ class Hinge(LossFunctionWrapper):\n     provided we will convert them to -1 or 1.\n \n     Args:\n-        reduction: Type of reduction to apply to loss. For almost all cases\n-            this defaults to `\"sum_over_batch_size\"`. Options are `\"sum\"`,\n-            `\"sum_over_batch_size\"` or `None`.\n-        name: Optional name for the instance. Defaults to `\"hinge\"`\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`.\n+            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the loss instance.\n     \"\"\"\n \n     def __init__(self, reduction=\"sum_over_batch_size\", name=\"hinge\"):\n@@ -222,10 +224,10 @@ class SquaredHinge(LossFunctionWrapper):\n     provided we will convert them to -1 or 1.\n \n     Args:\n-        reduction: Type of reduction to apply to loss. For almost all cases\n-            this defaults to `\"sum_over_batch_size\"`. Options are `\"sum\"`,\n-            `\"sum_over_batch_size\"` or `None`.\n-        name: Optional name for the instance. Defaults to `\"squared_hinge\"`\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`.\n+            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the loss instance.\n     \"\"\"\n \n     def __init__(self, reduction=\"sum_over_batch_size\", name=\"squared_hinge\"):\n@@ -248,11 +250,10 @@ class CategoricalHinge(LossFunctionWrapper):\n     where `neg=maximum((1-y_true)*y_pred)` and `pos=sum(y_true*y_pred)`\n \n     Args:\n-        reduction: Type of reduction to apply to loss. For almost all cases\n-            this defaults to `\"sum_over_batch_size\"`. Options are `\"sum\"`,\n-            `\"sum_over_batch_size\"` or `None`.\n-        name: Optional name for the instance. Defaults to\n-            `\"categorical_hinge\"`\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`.\n+            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the loss instance.\n     \"\"\"\n \n     def __init__(\n@@ -275,10 +276,10 @@ class KLDivergence(LossFunctionWrapper):\n     ```\n \n     Args:\n-        reduction: Type of reduction to apply to loss. For almost all cases\n-            this defaults to `\"sum_over_batch_size\"`. Options are `\"sum\"`,\n-            `\"sum_over_batch_size\"` or `None`.\n-        name: Optional name for the instance. Defaults to 'kl_divergence'.\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`.\n+            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the loss instance.\n     \"\"\"\n \n     def __init__(self, reduction=\"sum_over_batch_size\", name=\"kl_divergence\"):\n@@ -299,10 +300,10 @@ class Poisson(LossFunctionWrapper):\n     ```\n \n     Args:\n-        reduction: Type of reduction to apply to loss. For almost all cases\n-            this defaults to `\"sum_over_batch_size\"`. Options are `\"sum\"`,\n-            `\"sum_over_batch_size\"` or `None`.\n-        name: Optional name for the instance. Defaults to `\"poisson\"`\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`.\n+            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the loss instance.\n     \"\"\"\n \n     def __init__(self, reduction=\"sum_over_batch_size\", name=\"poisson\"):\n@@ -312,6 +313,572 @@ class Poisson(LossFunctionWrapper):\n         return Loss.get_config(self)\n \n \n+@keras_core_export(\"keras_core.losses.BinaryCrossentropy\")\n+class BinaryCrossentropy(LossFunctionWrapper):\n+    \"\"\"Computes the cross-entropy loss between true labels and predicted labels.\n+\n+    Use this cross-entropy loss for binary (0 or 1) classification applications.\n+    The loss function requires the following inputs:\n+\n+    - `y_true` (true label): This is either 0 or 1.\n+    - `y_pred` (predicted value): This is the model's prediction, i.e, a single\n+      floating-point value which either represents a\n+      [logit](https://en.wikipedia.org/wiki/Logit), (i.e, value in [-inf, inf]\n+      when `from_logits=True`) or a probability (i.e, value in [0., 1.] when\n+      `from_logits=False`).\n+\n+    Args:\n+        from_logits: Whether to interpret `y_pred` as a tensor of\n+            [logit](https://en.wikipedia.org/wiki/Logit) values. By default, we\n+            assume that `y_pred` contains probabilities (i.e., values in [0,\n+            1]).\n+        label_smoothing: Float in range [0, 1]. When 0, no smoothing occurs.\n+            When > 0, we compute the loss between the predicted labels\n+            and a smoothed version of the true labels, where the smoothing\n+            squeezes the labels towards 0.5. Larger values of\n+            `label_smoothing` correspond to heavier smoothing.\n+        axis: The axis along which to compute crossentropy (the features\n+            axis).  Defaults to -1.\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`.\n+            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the loss instance.\n+\n+    Examples:\n+\n+    **Recommended Usage:** (set `from_logits=True`)\n+\n+    With `compile()` API:\n+\n+    ```python\n+    model.compile(\n+        loss=keras_core.losses.BinaryCrossentropy(from_logits=True),\n+        ...\n+    )\n+    ```\n+\n+    As a standalone function:\n+\n+    >>> # Example 1: (batch_size = 1, number of samples = 4)\n+    >>> y_true = [0, 1, 0, 0]\n+    >>> y_pred = [-18.6, 0.51, 2.94, -12.8]\n+    >>> bce = keras_core.losses.BinaryCrossentropy(from_logits=True)\n+    >>> bce(y_true, y_pred)\n+    0.865\n+\n+    >>> # Example 2: (batch_size = 2, number of samples = 4)\n+    >>> y_true = [[0, 1], [0, 0]]\n+    >>> y_pred = [[-18.6, 0.51], [2.94, -12.8]]\n+    >>> # Using default 'auto'/'sum_over_batch_size' reduction type.\n+    >>> bce = keras_core.losses.BinaryCrossentropy(from_logits=True)\n+    >>> bce(y_true, y_pred)\n+    0.865\n+    >>> # Using 'sample_weight' attribute\n+    >>> bce(y_true, y_pred, sample_weight=[0.8, 0.2])\n+    0.243\n+    >>> # Using 'sum' reduction` type.\n+    >>> bce = keras_core.losses.BinaryCrossentropy(from_logits=True,\n+    ...     reduction=\"sum\")\n+    >>> bce(y_true, y_pred)\n+    1.730\n+    >>> # Using 'none' reduction type.\n+    >>> bce = keras_core.losses.BinaryCrossentropy(from_logits=True,\n+    ...     reduction=None)\n+    >>> bce(y_true, y_pred)\n+    array([0.235, 1.496], dtype=float32)\n+\n+    **Default Usage:** (set `from_logits=False`)\n+\n+    >>> # Make the following updates to the above \"Recommended Usage\" section\n+    >>> # 1. Set `from_logits=False`\n+    >>> keras_core.losses.BinaryCrossentropy() # OR ...('from_logits=False')\n+    >>> # 2. Update `y_pred` to use probabilities instead of logits\n+    >>> y_pred = [0.6, 0.3, 0.2, 0.8] # OR [[0.6, 0.3], [0.2, 0.8]]\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        from_logits=False,\n+        label_smoothing=0.0,\n+        axis=-1,\n+        reduction=\"sum_over_batch_size\",\n+        name=\"binary_crossentropy\",\n+    ):\n+        super().__init__(\n+            binary_crossentropy,\n+            name=name,\n+            reduction=reduction,\n+            from_logits=from_logits,\n+            label_smoothing=label_smoothing,\n+            axis=axis,\n+        )\n+        self.from_logits = from_logits\n+\n+\n+@keras_core_export(\"keras_core.losses.BinaryFocalCrossentropy\")\n+class BinaryFocalCrossentropy(LossFunctionWrapper):\n+    \"\"\"Computes focal cross-entropy loss between true labels and predictions.\n+\n+    Binary cross-entropy loss is often used for binary (0 or 1) classification\n+    tasks. The loss function requires the following inputs:\n+\n+    - `y_true` (true label): This is either 0 or 1.\n+    - `y_pred` (predicted value): This is the model's prediction, i.e, a single\n+      floating-point value which either represents a\n+      [logit](https://en.wikipedia.org/wiki/Logit), (i.e, value in [-inf, inf]\n+      when `from_logits=True`) or a probability (i.e, value in `[0., 1.]` when\n+      `from_logits=False`).\n+\n+    According to [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf), it\n+    helps to apply a \"focal factor\" to down-weight easy examples and focus more\n+    on hard examples. By default, the focal tensor is computed as follows:\n+\n+    `focal_factor = (1 - output) ** gamma` for class 1\n+    `focal_factor = output ** gamma` for class 0\n+    where `gamma` is a focusing parameter. When `gamma=0`, this function is\n+    equivalent to the binary crossentropy loss.\n+\n+    Args:\n+        apply_class_balancing: A bool, whether to apply weight balancing on the\n+            binary classes 0 and 1.\n+        alpha: A weight balancing factor for class 1, default is `0.25` as\n+            mentioned in reference [Lin et al., 2018](\n+            https://arxiv.org/pdf/1708.02002.pdf).  The weight for class 0 is\n+            `1.0 - alpha`.\n+        gamma: A focusing parameter used to compute the focal factor, default is\n+            `2.0` as mentioned in the reference\n+            [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf).\n+        from_logits: Whether to interpret `y_pred` as a tensor of\n+            [logit](https://en.wikipedia.org/wiki/Logit) values. By default, we\n+            assume that `y_pred` are probabilities (i.e., values in `[0, 1]`).\n+        label_smoothing: Float in `[0, 1]`. When `0`, no smoothing occurs.\n+            When > `0`, we compute the loss between the predicted labels\n+            and a smoothed version of the true labels, where the smoothing\n+            squeezes the labels towards `0.5`.\n+            Larger values of `label_smoothing` correspond to heavier smoothing.\n+        axis: The axis along which to compute crossentropy (the features axis).\n+            Defaults to `-1`.\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`.\n+            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the loss instance.\n+\n+    Examples:\n+\n+    With the `compile()` API:\n+\n+    ```python\n+    model.compile(\n+        loss=keras_core.losses.BinaryFocalCrossentropy(\n+            gamma=2.0, from_logits=True),\n+        ...\n+    )\n+    ```\n+\n+    As a standalone function:\n+\n+    >>> # Example 1: (batch_size = 1, number of samples = 4)\n+    >>> y_true = [0, 1, 0, 0]\n+    >>> y_pred = [-18.6, 0.51, 2.94, -12.8]\n+    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n+    ...    gamma=2, from_logits=True)\n+    >>> loss(y_true, y_pred)\n+    0.691\n+\n+    >>> # Apply class weight\n+    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n+    ...     apply_class_balancing=True, gamma=2, from_logits=True)\n+    >>> loss(y_true, y_pred)\n+    0.51\n+\n+    >>> # Example 2: (batch_size = 2, number of samples = 4)\n+    >>> y_true = [[0, 1], [0, 0]]\n+    >>> y_pred = [[-18.6, 0.51], [2.94, -12.8]]\n+    >>> # Using default 'auto'/'sum_over_batch_size' reduction type.\n+    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n+    ...     gamma=3, from_logits=True)\n+    >>> loss(y_true, y_pred)\n+    0.647\n+\n+    >>> # Apply class weight\n+    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n+    ...      apply_class_balancing=True, gamma=3, from_logits=True)\n+    >>> loss(y_true, y_pred)\n+    0.482\n+\n+    >>> # Using 'sample_weight' attribute with focal effect\n+    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n+    ...     gamma=3, from_logits=True)\n+    >>> loss(y_true, y_pred, sample_weight=[0.8, 0.2])\n+    0.133\n+\n+    >>> # Apply class weight\n+    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n+    ...      apply_class_balancing=True, gamma=3, from_logits=True)\n+    >>> loss(y_true, y_pred, sample_weight=[0.8, 0.2])\n+    0.097\n+\n+    >>> # Using 'sum' reduction` type.\n+    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n+    ...     gamma=4, from_logits=True,\n+    ...     reduction=\"sum\")\n+    >>> loss(y_true, y_pred)\n+    1.222\n+\n+    >>> # Apply class weight\n+    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n+    ...     apply_class_balancing=True, gamma=4, from_logits=True,\n+    ...     reduction=\"sum\")\n+    >>> loss(y_true, y_pred)\n+    0.914\n+\n+    >>> # Using 'none' reduction type.\n+    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n+    ...     gamma=5, from_logits=True,\n+    ...     reduction=None)\n+    >>> loss(y_true, y_pred)\n+    array([0.0017 1.1561], dtype=float32)\n+\n+    >>> # Apply class weight\n+    >>> loss = keras_core.losses.BinaryFocalCrossentropy(\n+    ...     apply_class_balancing=True, gamma=5, from_logits=True,\n+    ...     reduction=None)\n+    >>> loss(y_true, y_pred)\n+    array([0.0004 0.8670], dtype=float32)\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        apply_class_balancing=False,\n+        alpha=0.25,\n+        gamma=2.0,\n+        from_logits=False,\n+        label_smoothing=0.0,\n+        axis=-1,\n+        reduction=\"sum_over_batch_size\",\n+        name=\"binary_focal_crossentropy\",\n+    ):\n+        \"\"\"Initializes `BinaryFocalCrossentropy` instance.\"\"\"\n+        super().__init__(\n+            binary_focal_crossentropy,\n+            apply_class_balancing=apply_class_balancing,\n+            alpha=alpha,\n+            gamma=gamma,\n+            name=name,\n+            reduction=reduction,\n+            from_logits=from_logits,\n+            label_smoothing=label_smoothing,\n+            axis=axis,\n+        )\n+        self.from_logits = from_logits\n+        self.apply_class_balancing = apply_class_balancing\n+        self.alpha = alpha\n+        self.gamma = gamma\n+\n+    def get_config(self):\n+        config = {\n+            \"apply_class_balancing\": self.apply_class_balancing,\n+            \"alpha\": self.alpha,\n+            \"gamma\": self.gamma,\n+        }\n+        base_config = super().get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+@keras_core_export(\"keras_core.losses.CategoricalCrossentropy\")\n+class CategoricalCrossentropy(LossFunctionWrapper):\n+    \"\"\"Computes the crossentropy loss between the labels and predictions.\n+\n+    Use this crossentropy loss function when there are two or more label\n+    classes. We expect labels to be provided in a `one_hot` representation. If\n+    you want to provide labels as integers, please use\n+    `SparseCategoricalCrossentropy` loss.  There should be `# classes` floating\n+    point values per feature.\n+\n+    In the snippet below, there is `# classes` floating pointing values per\n+    example. The shape of both `y_pred` and `y_true` are\n+    `[batch_size, num_classes]`.\n+\n+    Args:\n+        from_logits: Whether `y_pred` is expected to be a logits tensor. By\n+            default, we assume that `y_pred` encodes a probability distribution.\n+        label_smoothing: Float in [0, 1]. When > 0, label values are smoothed,\n+            meaning the confidence on label values are relaxed. For example, if\n+            `0.1`, use `0.1 / num_classes` for non-target labels and\n+            `0.9 + 0.1 / num_classes` for target labels.\n+        axis: The axis along which to compute crossentropy (the features\n+            axis). Defaults to -1.\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`.\n+            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the loss instance.\n+\n+    Examples:\n+\n+    Standalone usage:\n+\n+    >>> y_true = [[0, 1, 0], [0, 0, 1]]\n+    >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n+    >>> # Using 'auto'/'sum_over_batch_size' reduction type.\n+    >>> cce = keras_core.losses.CategoricalCrossentropy()\n+    >>> cce(y_true, y_pred)\n+    1.177\n+\n+    >>> # Calling with 'sample_weight'.\n+    >>> cce(y_true, y_pred, sample_weight=np.array([0.3, 0.7]))\n+    0.814\n+\n+    >>> # Using 'sum' reduction type.\n+    >>> cce = keras_core.losses.CategoricalCrossentropy(\n+    ...     reduction=\"sum\")\n+    >>> cce(y_true, y_pred)\n+    2.354\n+\n+    >>> # Using 'none' reduction type.\n+    >>> cce = keras_core.losses.CategoricalCrossentropy(\n+    ...     reduction=None)\n+    >>> cce(y_true, y_pred)\n+    array([0.0513, 2.303], dtype=float32)\n+\n+    Usage with the `compile()` API:\n+\n+    ```python\n+    model.compile(optimizer='sgd',\n+                  loss=keras_core.losses.CategoricalCrossentropy())\n+    ```\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        from_logits=False,\n+        label_smoothing=0.0,\n+        axis=-1,\n+        reduction=\"sum_over_batch_size\",\n+        name=\"categorical_crossentropy\",\n+    ):\n+        super().__init__(\n+            categorical_crossentropy,\n+            name=name,\n+            reduction=reduction,\n+            from_logits=from_logits,\n+            label_smoothing=label_smoothing,\n+            axis=axis,\n+        )\n+\n+\n+@keras_core_export(\"keras_core.losses.CategoricalFocalCrossentropy\")\n+class CategoricalFocalCrossentropy(LossFunctionWrapper):\n+    \"\"\"Computes the alpha balanced focal crossentropy loss.\n+\n+    Use this crossentropy loss function when there are two or more label\n+    classes and if you want to handle class imbalance without using\n+    `class_weights`. We expect labels to be provided in a `one_hot`\n+    representation.\n+\n+    According to [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf), it\n+    helps to apply a focal factor to down-weight easy examples and focus more on\n+    hard examples. The general formula for the focal loss (FL)\n+    is as follows:\n+\n+    `FL(p_t) = (1 - p_t) ** gamma * log(p_t)`\n+\n+    where `p_t` is defined as follows:\n+    `p_t = output if y_true == 1, else 1 - output`\n+\n+    `(1 - p_t) ** gamma` is the `modulating_factor`, where `gamma` is a focusing\n+    parameter. When `gamma` = 0, there is no focal effect on the cross entropy.\n+    `gamma` reduces the importance given to simple examples in a smooth manner.\n+\n+    The authors use alpha-balanced variant of focal loss (FL) in the paper:\n+    `FL(p_t) = -alpha * (1 - p_t) ** gamma * log(p_t)`\n+\n+    where `alpha` is the weight factor for the classes. If `alpha` = 1, the\n+    loss won't be able to handle class imbalance properly as all\n+    classes will have the same weight. This can be a constant or a list of\n+    constants. If alpha is a list, it must have the same length as the number\n+    of classes.\n+\n+    The formula above can be generalized to:\n+    `FL(p_t) = alpha * (1 - p_t) ** gamma * CrossEntropy(y_true, y_pred)`\n+\n+    where minus comes from `CrossEntropy(y_true, y_pred)` (CE).\n+\n+    Extending this to multi-class case is straightforward:\n+    `FL(p_t) = alpha * (1 - p_t) ** gamma * CategoricalCE(y_true, y_pred)`\n+\n+    In the snippet below, there is `# classes` floating pointing values per\n+    example. The shape of both `y_pred` and `y_true` are\n+    `(batch_size, num_classes)`.\n+\n+    Args:\n+        alpha: A weight balancing factor for all classes, default is `0.25` as\n+            mentioned in the reference. It can be a list of floats or a scalar.\n+            In the multi-class case, alpha may be set by inverse class\n+            frequency by using `compute_class_weight` from `sklearn.utils`.\n+        gamma: A focusing parameter, default is `2.0` as mentioned in the\n+            reference. It helps to gradually reduce the importance given to\n+            simple (easy) examples in a smooth manner.\n+        from_logits: Whether `output` is expected to be a logits tensor. By\n+            default, we consider that `output` encodes a probability\n+            distribution.\n+        label_smoothing: Float in [0, 1]. When > 0, label values are smoothed,\n+            meaning the confidence on label values are relaxed. For example, if\n+            `0.1`, use `0.1 / num_classes` for non-target labels and\n+            `0.9 + 0.1 / num_classes` for target labels.\n+        axis: The axis along which to compute crossentropy (the features\n+            axis). Defaults to -1.\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`.\n+            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the loss instance.\n+\n+    Examples:\n+\n+    Standalone usage:\n+\n+    >>> y_true = [[0., 1., 0.], [0., 0., 1.]]\n+    >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n+    >>> # Using 'auto'/'sum_over_batch_size' reduction type.\n+    >>> cce = keras_core.losses.CategoricalFocalCrossentropy()\n+    >>> cce(y_true, y_pred)\n+    0.23315276\n+\n+    >>> # Calling with 'sample_weight'.\n+    >>> cce(y_true, y_pred, sample_weight=np.array([0.3, 0.7]))\n+    0.1632\n+\n+    >>> # Using 'sum' reduction type.\n+    >>> cce = keras_core.losses.CategoricalFocalCrossentropy(\n+    ...     reduction=\"sum\")\n+    >>> cce(y_true, y_pred)\n+    0.46631\n+\n+    >>> # Using 'none' reduction type.\n+    >>> cce = keras_core.losses.CategoricalFocalCrossentropy(\n+    ...     reduction=None)\n+    >>> cce(y_true, y_pred)\n+    array([3.2058331e-05, 4.6627346e-01], dtype=float32)\n+\n+    Usage with the `compile()` API:\n+\n+    ```python\n+    model.compile(optimizer='adam',\n+                  loss=keras_core.losses.CategoricalFocalCrossentropy())\n+    ```\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        alpha=0.25,\n+        gamma=2.0,\n+        from_logits=False,\n+        label_smoothing=0.0,\n+        axis=-1,\n+        reduction=\"sum_over_batch_size\",\n+        name=\"categorical_focal_crossentropy\",\n+    ):\n+        \"\"\"Initializes `CategoricalFocalCrossentropy` instance.\"\"\"\n+        super().__init__(\n+            categorical_focal_crossentropy,\n+            alpha=alpha,\n+            gamma=gamma,\n+            name=name,\n+            reduction=reduction,\n+            from_logits=from_logits,\n+            label_smoothing=label_smoothing,\n+            axis=axis,\n+        )\n+        self.from_logits = from_logits\n+        self.alpha = alpha\n+        self.gamma = gamma\n+\n+    def get_config(self):\n+        config = {\n+            \"alpha\": self.alpha,\n+            \"gamma\": self.gamma,\n+        }\n+        base_config = super().get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+@keras_core_export(\"keras_core.losses.SparseCategoricalCrossentropy\")\n+class SparseCategoricalCrossentropy(LossFunctionWrapper):\n+    \"\"\"Computes the crossentropy loss between the labels and predictions.\n+\n+    Use this crossentropy loss function when there are two or more label\n+    classes.  We expect labels to be provided as integers. If you want to\n+    provide labels using `one-hot` representation, please use\n+    `CategoricalCrossentropy` loss.  There should be `# classes` floating point\n+    values per feature for `y_pred` and a single floating point value per\n+    feature for `y_true`.\n+\n+    In the snippet below, there is a single floating point value per example for\n+    `y_true` and `# classes` floating pointing values per example for `y_pred`.\n+    The shape of `y_true` is `[batch_size]` and the shape of `y_pred` is\n+    `[batch_size, num_classes]`.\n+\n+    Args:\n+        from_logits: Whether `y_pred` is expected to be a logits tensor. By\n+            default, we assume that `y_pred` encodes a probability distribution.\n+        ignore_class: Optional integer. The ID of a class to be ignored during\n+            loss computation. This is useful, for example, in segmentation\n+            problems featuring a \"void\" class (commonly -1 or 255) in\n+            segmentation maps.\n+            By default (`ignore_class=None`), all classes are considered.\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`.\n+            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+        name: Optional name for the loss instance.\n+\n+    Examples:\n+\n+    >>> y_true = [1, 2]\n+    >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n+    >>> # Using 'auto'/'sum_over_batch_size' reduction type.\n+    >>> scce = keras_core.losses.SparseCategoricalCrossentropy()\n+    >>> scce(y_true, y_pred)\n+    1.177\n+\n+    >>> # Calling with 'sample_weight'.\n+    >>> scce(y_true, y_pred, sample_weight=np.array([0.3, 0.7]))\n+    0.814\n+\n+    >>> # Using 'sum' reduction type.\n+    >>> scce = keras_core.losses.SparseCategoricalCrossentropy(\n+    ...     reduction=\"sum\")\n+    >>> scce(y_true, y_pred)\n+    2.354\n+\n+    >>> # Using 'none' reduction type.\n+    >>> scce = keras_core.losses.SparseCategoricalCrossentropy(\n+    ...     reduction=None)\n+    >>> scce(y_true, y_pred)\n+    array([0.0513, 2.303], dtype=float32)\n+\n+    Usage with the `compile()` API:\n+\n+    ```python\n+    model.compile(optimizer='sgd',\n+                  loss=keras_core.losses.SparseCategoricalCrossentropy())\n+    ```\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        from_logits=False,\n+        ignore_class=None,\n+        reduction=\"sum_over_batch_size\",\n+        name=\"sparse_categorical_crossentropy\",\n+    ):\n+        super().__init__(\n+            sparse_categorical_crossentropy,\n+            name=name,\n+            reduction=reduction,\n+            from_logits=from_logits,\n+            ignore_class=ignore_class,\n+        )\n+\n+\n def convert_binary_labels_to_hinge(y_true):\n     \"\"\"Converts binary labels into -1/1 for hinge loss/metric calculation.\"\"\"\n     are_zeros = ops.equal(y_true, 0)\n@@ -347,12 +914,6 @@ def hinge(y_true, y_pred):\n     loss = mean(maximum(1 - y_true * y_pred, 0), axis=-1)\n     ```\n \n-    Standalone usage:\n-\n-    >>> y_true = np.random.choice([-1, 1], size=(2, 3))\n-    >>> y_pred = np.random.random(size=(2, 3))\n-    >>> loss = keras_core.losses.hinge(y_true, y_pred)\n-\n     Args:\n         y_true: The ground truth values. `y_true` values are expected to be -1\n             or 1. If binary (0 or 1) labels are provided they will be converted\n@@ -361,6 +922,12 @@ def hinge(y_true, y_pred):\n \n     Returns:\n         Hinge loss values with shape = `[batch_size, d0, .. dN-1]`.\n+\n+    Example:\n+\n+    >>> y_true = np.random.choice([-1, 1], size=(2, 3))\n+    >>> y_pred = np.random.random(size=(2, 3))\n+    >>> loss = keras_core.losses.hinge(y_true, y_pred)\n     \"\"\"\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.cast(y_true, dtype=y_pred.dtype)\n@@ -384,12 +951,6 @@ def squared_hinge(y_true, y_pred):\n     loss = mean(square(maximum(1 - y_true * y_pred, 0)), axis=-1)\n     ```\n \n-    Standalone usage:\n-\n-    >>> y_true = np.random.choice([-1, 1], size=(2, 3))\n-    >>> y_pred = np.random.random(size=(2, 3))\n-    >>> loss = keras_core.losses.squared_hinge(y_true, y_pred)\n-\n     Args:\n         y_true: The ground truth values. `y_true` values are expected to be -1\n             or 1. If binary (0 or 1) labels are provided we will convert them\n@@ -398,6 +959,12 @@ def squared_hinge(y_true, y_pred):\n \n     Returns:\n         Squared hinge loss values with shape = `[batch_size, d0, .. dN-1]`.\n+\n+    Example:\n+\n+    >>> y_true = np.random.choice([-1, 1], size=(2, 3))\n+    >>> y_pred = np.random.random(size=(2, 3))\n+    >>> loss = keras_core.losses.squared_hinge(y_true, y_pred)\n     \"\"\"\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.cast(y_true, y_pred.dtype)\n@@ -424,13 +991,6 @@ def categorical_hinge(y_true, y_pred):\n \n     where `neg=maximum((1-y_true)*y_pred)` and `pos=sum(y_true*y_pred)`\n \n-    Standalone usage:\n-\n-    >>> y_true = np.random.randint(0, 3, size=(2,))\n-    >>> y_true = np.eye(np.max(y_true) + 1)[y_true]\n-    >>> y_pred = np.random.random(size=(2, 3))\n-    >>> loss = keras_core.losses.categorical_hinge(y_true, y_pred)\n-\n     Args:\n         y_true: The ground truth values. `y_true` values are expected to be\n             either `{-1, +1}` or `{0, 1}` (i.e. a one-hot-encoded tensor) with\n@@ -439,6 +999,13 @@ def categorical_hinge(y_true, y_pred):\n \n     Returns:\n         Categorical hinge loss values with shape = `[batch_size, d0, .. dN-1]`.\n+\n+    Example:\n+\n+    >>> y_true = np.random.randint(0, 3, size=(2,))\n+    >>> y_true = np.eye(np.max(y_true) + 1)[y_true]\n+    >>> y_pred = np.random.random(size=(2, 3))\n+    >>> loss = keras_core.losses.categorical_hinge(y_true, y_pred)\n     \"\"\"\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.cast(y_true, y_pred.dtype)\n@@ -495,18 +1062,18 @@ def mean_absolute_error(y_true, y_pred):\n     loss = mean(abs(y_true - y_pred), axis=-1)\n     ```\n \n-    Standalone usage:\n-\n-    >>> y_true = np.random.randint(0, 2, size=(2, 3))\n-    >>> y_pred = np.random.random(size=(2, 3))\n-    >>> loss = keras_core.losses.mean_absolute_error(y_true, y_pred)\n-\n     Args:\n         y_true: Ground truth values with shape = `[batch_size, d0, .. dN]`.\n         y_pred: The predicted values with shape = `[batch_size, d0, .. dN]`.\n \n     Returns:\n         Mean absolute error values with shape = `[batch_size, d0, .. dN-1]`.\n+\n+    Example:\n+\n+    >>> y_true = np.random.randint(0, 2, size=(2, 3))\n+    >>> y_pred = np.random.random(size=(2, 3))\n+    >>> loss = keras_core.losses.mean_absolute_error(y_true, y_pred)\n     \"\"\"\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.convert_to_tensor(y_true, dtype=y_pred.dtype)\n@@ -523,18 +1090,16 @@ def mean_absolute_error(y_true, y_pred):\n def mean_absolute_percentage_error(y_true, y_pred):\n     \"\"\"Computes the mean absolute percentage error between `y_true` & `y_pred`.\n \n-    `loss = 100 * mean(abs((y_true - y_pred) / y_true), axis=-1)`\n+    Formula:\n+\n+    ```python\n+    loss = 100 * mean(abs((y_true - y_pred) / y_true), axis=-1)\n+    ```\n \n     Division by zero is prevented by dividing by `maximum(y_true, epsilon)`\n     where `epsilon = keras_core.backend.epsilon()`\n     (default to `1e-7`).\n \n-    Standalone usage:\n-\n-    >>> y_true = np.random.random(size=(2, 3))\n-    >>> y_pred = np.random.random(size=(2, 3))\n-    >>> loss = keras_core.losses.mean_absolute_percentage_error(y_true, y_pred)\n-\n     Args:\n         y_true: Ground truth values with shape = `[batch_size, d0, .. dN]`.\n         y_pred: The predicted values with shape = `[batch_size, d0, .. dN]`.\n@@ -542,6 +1107,12 @@ def mean_absolute_percentage_error(y_true, y_pred):\n     Returns:\n         Mean absolute percentage error values with shape = `[batch_size, d0, ..\n         dN-1]`.\n+\n+    Example:\n+\n+    >>> y_true = np.random.random(size=(2, 3))\n+    >>> y_pred = np.random.random(size=(2, 3))\n+    >>> loss = keras_core.losses.mean_absolute_percentage_error(y_true, y_pred)\n     \"\"\"\n     epsilon = ops.convert_to_tensor(backend.epsilon())\n     y_pred = ops.convert_to_tensor(y_pred)\n@@ -570,19 +1141,19 @@ def mean_squared_logarithmic_error(y_true, y_pred):\n     values and 0 values will be replaced with `keras_core.backend.epsilon()`\n     (default to `1e-7`).\n \n-    Standalone usage:\n-\n-    >>> y_true = np.random.randint(0, 2, size=(2, 3))\n-    >>> y_pred = np.random.random(size=(2, 3))\n-    >>> loss = keras_core.losses.mean_squared_logarithmic_error(y_true, y_pred)\n-\n     Args:\n         y_true: Ground truth values with shape = `[batch_size, d0, .. dN]`.\n         y_pred: The predicted values with shape = `[batch_size, d0, .. dN]`.\n \n     Returns:\n-        Mean squared logarithmic error values. shape = `[batch_size, d0, ..\n+        Mean squared logarithmic error values with shape = `[batch_size, d0, ..\n         dN-1]`.\n+\n+    Example:\n+\n+    >>> y_true = np.random.randint(0, 2, size=(2, 3))\n+    >>> y_pred = np.random.random(size=(2, 3))\n+    >>> loss = keras_core.losses.mean_squared_logarithmic_error(y_true, y_pred)\n     \"\"\"\n     epsilon = ops.convert_to_tensor(backend.epsilon())\n     y_pred = ops.convert_to_tensor(y_pred)\n@@ -610,12 +1181,6 @@ def cosine_similarity(y_true, y_pred, axis=-1):\n     similarity will be 0 regardless of the proximity between predictions\n     and targets.\n \n-    Standalone usage:\n-    >>> y_true = [[0., 1.], [1., 1.], [1., 1.]]\n-    >>> y_pred = [[1., 0.], [1., 1.], [-1., -1.]]\n-    >>> loss = keras_core.losses.cosine_similarity(y_true, y_pred, axis=-1)\n-    [-0., -0.99999994, 0.99999994]\n-\n     Args:\n         y_true: Tensor of true targets.\n         y_pred: Tensor of predicted targets.\n@@ -623,6 +1188,13 @@ def cosine_similarity(y_true, y_pred, axis=-1):\n \n     Returns:\n         Cosine similarity tensor.\n+\n+    Example:\n+\n+    >>> y_true = [[0., 1.], [1., 1.], [1., 1.]]\n+    >>> y_pred = [[1., 0.], [1., 1.], [-1., -1.]]\n+    >>> loss = keras_core.losses.cosine_similarity(y_true, y_pred, axis=-1)\n+    [-0., -0.99999994, 0.99999994]\n     \"\"\"\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.convert_to_tensor(y_true, dtype=y_pred.dtype)\n@@ -647,7 +1219,14 @@ def kl_divergence(y_true, y_pred):\n     loss = y_true * log(y_true / y_pred)\n     ```\n \n-    Standalone usage:\n+    Args:\n+        y_true: Tensor of true targets.\n+        y_pred: Tensor of predicted targets.\n+\n+    Returns:\n+        KL Divergence loss values with shape = `[batch_size, d0, .. dN-1]`.\n+\n+    Example:\n \n     >>> y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float32)\n     >>> y_pred = np.random.random(size=(2, 3))\n@@ -657,13 +1236,6 @@ def kl_divergence(y_true, y_pred):\n     >>> y_pred = ops.clip(y_pred, 1e-7, 1)\n     >>> assert np.array_equal(\n     ...     loss, np.sum(y_true * np.log(y_true / y_pred), axis=-1))\n-\n-    Args:\n-        y_true: Tensor of true targets.\n-        y_pred: Tensor of predicted targets.\n-\n-    Returns:\n-        KL Divergence loss values with shape = `[batch_size, d0, .. dN-1]`.\n     \"\"\"\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.convert_to_tensor(y_true, y_pred.dtype)\n@@ -687,7 +1259,14 @@ def poisson(y_true, y_pred):\n     loss = y_pred - y_true * log(y_pred)\n     ```\n \n-    Standalone usage:\n+    Args:\n+        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n+        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n+\n+    Returns:\n+        Poisson loss values with shape = `[batch_size, d0, .. dN-1]`.\n+\n+    Example:\n \n     >>> y_true = np.random.randint(0, 2, size=(2, 3))\n     >>> y_pred = np.random.random(size=(2, 3))\n@@ -697,15 +1276,344 @@ def poisson(y_true, y_pred):\n     >>> assert np.allclose(\n     ...     loss, np.mean(y_pred - y_true * np.log(y_pred), axis=-1),\n     ...     atol=1e-5)\n-\n-    Args:\n-        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n-        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n-\n-    Returns:\n-        Poisson loss values with shape = `[batch_size, d0, .. dN-1]`.\n     \"\"\"\n     y_pred = ops.convert_to_tensor(y_pred)\n     y_true = ops.convert_to_tensor(y_true, dtype=y_pred.dtype)\n     epsilon = ops.convert_to_tensor(backend.epsilon())\n     return ops.mean(y_pred - y_true * ops.log(y_pred + epsilon), axis=-1)\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.metrics.categorical_crossentropy\",\n+        \"keras_core.losses.categorical_crossentropy\",\n+    ]\n+)\n+def categorical_crossentropy(\n+    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1\n+):\n+    \"\"\"Computes the categorical crossentropy loss.\n+\n+    Args:\n+        y_true: Tensor of one-hot true targets.\n+        y_pred: Tensor of predicted targets.\n+        from_logits: Whether `y_pred` is expected to be a logits tensor. By\n+            default, we assume that `y_pred` encodes a probability distribution.\n+        label_smoothing: Float in [0, 1]. If > `0` then smooth the labels. For\n+            example, if `0.1`, use `0.1 / num_classes` for non-target labels\n+            and `0.9 + 0.1 / num_classes` for target labels.\n+        axis: Defaults to -1. The dimension along which the entropy is\n+            computed.\n+\n+    Returns:\n+        Categorical crossentropy loss value.\n+\n+    Example:\n+\n+    >>> y_true = [[0, 1, 0], [0, 0, 1]]\n+    >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n+    >>> loss = keras_core.losses.categorical_crossentropy(y_true, y_pred)\n+    >>> assert loss.shape == (2,)\n+    >>> loss.numpy()\n+    array([0.0513, 2.303], dtype=float32)\n+    \"\"\"\n+    if isinstance(axis, bool):\n+        raise ValueError(\n+            \"`axis` must be of type `int`. \"\n+            f\"Received: axis={axis} of type {type(axis)}\"\n+        )\n+    y_pred = ops.convert_to_tensor(y_pred)\n+    y_true = ops.cast(y_true, y_pred.dtype)\n+\n+    if y_pred.shape[-1] == 1:\n+        warnings.warn(\n+            \"In loss categorical_crossentropy, expected \"\n+            \"y_pred.shape to be (batch_size, num_classes) \"\n+            f\"with num_classes > 1. Received: y_pred.shape={y_pred.shape}. \"\n+            \"Consider using 'binary_crossentropy' if you only have 2 classes.\",\n+            SyntaxWarning,\n+            stacklevel=2,\n+        )\n+\n+    if label_smoothing:\n+        num_classes = ops.cast(ops.shape(y_true)[-1], y_pred.dtype)\n+        y_true = y_true * (1.0 - label_smoothing) + (\n+            label_smoothing / num_classes\n+        )\n+\n+    return ops.categorical_crossentropy(\n+        y_true, y_pred, from_logits=from_logits, axis=axis\n+    )\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.metrics.categorical_focal_crossentropy\",\n+        \"keras_core.losses.categorical_focal_crossentropy\",\n+    ]\n+)\n+def categorical_focal_crossentropy(\n+    y_true,\n+    y_pred,\n+    alpha=0.25,\n+    gamma=2.0,\n+    from_logits=False,\n+    label_smoothing=0.0,\n+    axis=-1,\n+):\n+    \"\"\"Computes the categorical focal crossentropy loss.\n+\n+    Args:\n+        y_true: Tensor of one-hot true targets.\n+        y_pred: Tensor of predicted targets.\n+        alpha: A weight balancing factor for all classes, default is `0.25` as\n+            mentioned in the reference. It can be a list of floats or a scalar.\n+            In the multi-class case, alpha may be set by inverse class\n+            frequency by using `compute_class_weight` from `sklearn.utils`.\n+        gamma: A focusing parameter, default is `2.0` as mentioned in the\n+            reference. It helps to gradually reduce the importance given to\n+            simple examples in a smooth manner. When `gamma` = 0, there is\n+            no focal effect on the categorical crossentropy.\n+        from_logits: Whether `y_pred` is expected to be a logits tensor. By\n+            default, we assume that `y_pred` encodes a probability\n+            distribution.\n+        label_smoothing: Float in [0, 1]. If > `0` then smooth the labels. For\n+            example, if `0.1`, use `0.1 / num_classes` for non-target labels\n+            and `0.9 + 0.1 / num_classes` for target labels.\n+        axis: Defaults to -1. The dimension along which the entropy is\n+            computed.\n+\n+    Returns:\n+        Categorical focal crossentropy loss value.\n+\n+    Example:\n+\n+    >>> y_true = [[0, 1, 0], [0, 0, 1]]\n+    >>> y_pred = [[0.05, 0.9, 0.05], [0.1, 0.85, 0.05]]\n+    >>> loss = keras_core.losses.categorical_focal_crossentropy(y_true, y_pred)\n+    >>> assert loss.shape == (2,)\n+    >>> loss\n+    array([2.63401289e-04, 6.75912094e-01], dtype=float32)\n+    \"\"\"\n+    if isinstance(axis, bool):\n+        raise ValueError(\n+            \"`axis` must be of type `int`. \"\n+            f\"Received: axis={axis} of type {type(axis)}\"\n+        )\n+    y_pred = ops.convert_to_tensor(y_pred)\n+    y_true = ops.cast(y_true, y_pred.dtype)\n+\n+    if y_pred.shape[-1] == 1:\n+        warnings.warn(\n+            \"In loss categorical_focal_crossentropy, expected \"\n+            \"y_pred.shape to be (batch_size, num_classes) \"\n+            f\"with num_classes > 1. Received: y_pred.shape={y_pred.shape}. \"\n+            \"Consider using 'binary_crossentropy' if you only have 2 classes.\",\n+            SyntaxWarning,\n+            stacklevel=2,\n+        )\n+\n+    if label_smoothing:\n+        num_classes = ops.cast(ops.shape(y_true)[-1], y_pred.dtype)\n+        y_true = y_true * (1.0 - label_smoothing) + (\n+            label_smoothing / num_classes\n+        )\n+\n+    return ops.categorical_focal_crossentropy(\n+        target=y_true,\n+        output=y_pred,\n+        alpha=alpha,\n+        gamma=gamma,\n+        from_logits=from_logits,\n+        axis=axis,\n+    )\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.metrics.sparse_categorical_crossentropy\",\n+        \"keras_core.losses.sparse_categorical_crossentropy\",\n+    ]\n+)\n+def sparse_categorical_crossentropy(\n+    y_true, y_pred, from_logits=False, axis=-1, ignore_class=None\n+):\n+    \"\"\"Computes the sparse categorical crossentropy loss.\n+\n+    Args:\n+        y_true: Ground truth values.\n+        y_pred: The predicted values.\n+        from_logits: Whether `y_pred` is expected to be a logits tensor. By\n+            default, we assume that `y_pred` encodes a probability distribution.\n+        axis: Defaults to -1. The dimension along which the entropy is\n+            computed.\n+        ignore_class: Optional integer. The ID of a class to be ignored during\n+            loss computation. This is useful, for example, in segmentation\n+            problems featuring a \"void\" class (commonly -1 or 255)\n+            in segmentation maps. By default (`ignore_class=None`),\n+            all classes are considered.\n+\n+    Returns:\n+        Sparse categorical crossentropy loss value.\n+\n+    Examples:\n+\n+    >>> y_true = [1, 2]\n+    >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n+    >>> loss = keras_core.losses.sparse_categorical_crossentropy(y_true, y_pred)\n+    >>> assert loss.shape == (2,)\n+    >>> loss\n+    array([0.0513, 2.303], dtype=float32)\n+\n+    >>> y_true = [[[ 0,  2],\n+    ...            [-1, -1]],\n+    ...           [[ 0,  2],\n+    ...            [-1, -1]]]\n+    >>> y_pred = [[[[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]],\n+    ...             [[0.2, 0.5, 0.3], [0.0, 1.0, 0.0]]],\n+    ...           [[[1.0, 0.0, 0.0], [0.0, 0.5, 0.5]],\n+    ...            [[0.2, 0.5, 0.3], [0.0, 1.0, 0.0]]]]\n+    >>> loss = keras_core.losses.sparse_categorical_crossentropy(\n+    ...   y_true, y_pred, ignore_class=-1)\n+    array([[[2.3841855e-07, 2.3841855e-07],\n+            [0.0000000e+00, 0.0000000e+00]],\n+           [[2.3841855e-07, 6.9314730e-01],\n+            [0.0000000e+00, 0.0000000e+00]]], dtype=float32)\n+    \"\"\"\n+    return ops.sparse_categorical_crossentropy(\n+        y_true,\n+        y_pred,\n+        from_logits=from_logits,\n+        ignore_class=ignore_class,\n+        axis=axis,\n+    )\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.metrics.binary_crossentropy\",\n+        \"keras_core.losses.binary_crossentropy\",\n+    ]\n+)\n+def binary_crossentropy(\n+    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1\n+):\n+    \"\"\"Computes the binary crossentropy loss.\n+\n+    Args:\n+        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n+        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n+        from_logits: Whether `y_pred` is expected to be a logits tensor. By\n+            default, we assume that `y_pred` encodes a probability distribution.\n+        label_smoothing: Float in `[0, 1]`. If > `0` then smooth the labels by\n+            squeezing them towards 0.5, that is,\n+            using `1. - 0.5 * label_smoothing` for the target class\n+            and `0.5 * label_smoothing` for the non-target class.\n+        axis: The axis along which the mean is computed. Defaults to -1.\n+\n+    Returns:\n+        Binary crossentropy loss value. shape = `[batch_size, d0, .. dN-1]`.\n+\n+    Example:\n+\n+    >>> y_true = [[0, 1], [0, 0]]\n+    >>> y_pred = [[0.6, 0.4], [0.4, 0.6]]\n+    >>> loss = keras_core.losses.binary_crossentropy(y_true, y_pred)\n+    >>> assert loss.shape == (2,)\n+    >>> loss\n+    array([0.916 , 0.714], dtype=float32)\n+    \"\"\"\n+    y_pred = ops.convert_to_tensor(y_pred)\n+    y_true = ops.cast(y_true, y_pred.dtype)\n+\n+    if label_smoothing:\n+        y_true = y_true * (1.0 - label_smoothing) + 0.5 * label_smoothing\n+\n+    return ops.mean(\n+        ops.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n+        axis=axis,\n+    )\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.metrics.binary_focal_crossentropy\",\n+        \"keras_core.losses.binary_focal_crossentropy\",\n+    ]\n+)\n+def binary_focal_crossentropy(\n+    y_true,\n+    y_pred,\n+    apply_class_balancing=False,\n+    alpha=0.25,\n+    gamma=2.0,\n+    from_logits=False,\n+    label_smoothing=0.0,\n+    axis=-1,\n+):\n+    \"\"\"Computes the binary focal crossentropy loss.\n+\n+    According to [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf), it\n+    helps to apply a focal factor to down-weight easy examples and focus more on\n+    hard examples. By default, the focal tensor is computed as follows:\n+\n+    `focal_factor = (1 - output)**gamma` for class 1\n+    `focal_factor = output**gamma` for class 0\n+    where `gamma` is a focusing parameter. When `gamma` = 0, there is no focal\n+    effect on the binary crossentropy loss.\n+\n+    If `apply_class_balancing == True`, this function also takes into account a\n+    weight balancing factor for the binary classes 0 and 1 as follows:\n+\n+    `weight = alpha` for class 1 (`target == 1`)\n+    `weight = 1 - alpha` for class 0\n+    where `alpha` is a float in the range of `[0, 1]`.\n+\n+    Args:\n+        y_true: Ground truth values, of shape `(batch_size, d0, .. dN)`.\n+        y_pred: The predicted values, of shape `(batch_size, d0, .. dN)`.\n+        apply_class_balancing: A bool, whether to apply weight balancing on the\n+            binary classes 0 and 1.\n+        alpha: A weight balancing factor for class 1, default is `0.25` as\n+            mentioned in the reference. The weight for class 0 is `1.0 - alpha`.\n+        gamma: A focusing parameter, default is `2.0` as mentioned in the\n+            reference.\n+        from_logits: Whether `y_pred` is expected to be a logits tensor. By\n+            default, we assume that `y_pred` encodes a probability distribution.\n+        label_smoothing: Float in `[0, 1]`. If > `0` then smooth the labels by\n+            squeezing them towards 0.5, that is,\n+            using `1. - 0.5 * label_smoothing` for the target class\n+            and `0.5 * label_smoothing` for the non-target class.\n+        axis: The axis along which the mean is computed. Defaults to `-1`.\n+\n+    Returns:\n+        Binary focal crossentropy loss value\n+        with shape = `[batch_size, d0, .. dN-1]`.\n+\n+    Example:\n+\n+    >>> y_true = [[0, 1], [0, 0]]\n+    >>> y_pred = [[0.6, 0.4], [0.4, 0.6]]\n+    >>> loss = keras_core.losses.binary_focal_crossentropy(\n+    ...        y_true, y_pred, gamma=2)\n+    >>> assert loss.shape == (2,)\n+    >>> loss\n+    array([0.330, 0.206], dtype=float32)\n+    \"\"\"\n+    y_pred = ops.convert_to_tensor(y_pred)\n+    y_true = ops.cast(y_true, y_pred.dtype)\n+\n+    if label_smoothing:\n+        y_true = y_true * (1.0 - label_smoothing) + 0.5 * label_smoothing\n+\n+    return ops.mean(\n+        ops.binary_focal_crossentropy(\n+            target=y_true,\n+            output=y_pred,\n+            apply_class_balancing=apply_class_balancing,\n+            alpha=alpha,\n+            gamma=gamma,\n+            from_logits=from_logits,\n+        ),\n+        axis=axis,\n+    )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
