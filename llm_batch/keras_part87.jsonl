{"custom_id": "keras#dbe7662e72986657cc4ac16f15ef0d5a0d9473d9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 0 | Files Changed: 4 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 6/3 | Churn Δ: 44 | Churn Cumulative: 3302 | Contributors (this commit): 28 | Commits (past 90d): 89 | Contributors (cumulative): 46 | DMM Complexity: 1.0\n\nDIFF:\n@@ -685,6 +685,26 @@ def categorical_crossentropy(output, target, from_logits=False):\n         return tf.nn.softmax_cross_entropy_with_logits(output, target)\n \n \n+def sparse_categorical_crossentropy(output, target, from_logits=False):\n+    '''Note: tf.nn.sparse_softmax_cross_entropy_with_logits\n+    expects logits, Keras expects probabilities.\n+    '''\n+    if not from_logits:\n+        output = tf.clip_by_value(output, tf.cast(_EPSILON, dtype=_FLOATX),\n+                                  tf.cast(1.-_EPSILON, dtype=_FLOATX))\n+        output = tf.log(output)\n+\n+    output_shape = output.get_shape()\n+    res = tf.nn.sparse_softmax_cross_entropy_with_logits(\n+        tf.reshape(output, [-1, int(output_shape[-1])]),\n+        cast(flatten(target), 'int32'))\n+    if len(output_shape) == 3:\n+        # if our output includes timesteps we need to reshape\n+        return tf.reshape(res, [-1, int(output_shape[-2])])\n+    else:\n+        return res\n+\n+\n def binary_crossentropy(output, target, from_logits=False):\n     '''Note: tf.nn.sigmoid_cross_entropy_with_logits\n     expects logits, Keras expects probabilities.\n\n@@ -717,6 +717,13 @@ def categorical_crossentropy(output, target, from_logits=False):\n     return T.nnet.categorical_crossentropy(output, target)\n \n \n+def sparse_categorical_crossentropy(output, target, from_logits=False):\n+    target = T.cast(T.flatten(target), 'int32')\n+    target = T.extra_ops.to_one_hot(target, nb_class=output.shape[-1])\n+    target = reshape(target, shape(output))\n+    return categorical_crossentropy(output, target, from_logits)\n+\n+\n def binary_crossentropy(output, target, from_logits=False):\n     if from_logits:\n         output = T.nnet.sigmoid(output)\n\n@@ -36,6 +36,12 @@ def categorical_crossentropy(y_true, y_pred):\n     return K.categorical_crossentropy(y_pred, y_true)\n \n \n+def sparse_categorical_crossentropy(y_true, y_pred):\n+    '''expects a 1-D or 2-D array of integer classes.\n+    '''\n+    return K.sparse_categorical_crossentropy(y_pred, y_true)\n+\n+\n def binary_crossentropy(y_true, y_pred):\n     return K.mean(K.binary_crossentropy(y_pred, y_true), axis=-1)\n \n\n@@ -32,5 +32,16 @@ def test_objective_shapes_2d():\n         assert K.eval(objective_output).shape == (6,)\n \n \n+def test_cce_one_hot():\n+    y_a = K.variable(np.random.randint(0, 7, (5, 6)))\n+    y_b = K.variable(np.random.random((5, 6, 7)))\n+    objective_output = objectives.sparse_categorical_crossentropy(y_a, y_b)\n+    assert K.eval(objective_output).shape == (5, 6)\n+\n+    y_a = K.variable(np.random.randint(0, 7, (6,)))\n+    y_b = K.variable(np.random.random((6, 7)))\n+    assert K.eval(objectives.sparse_categorical_crossentropy(y_a, y_b)).shape == (6,)\n+\n+\n if __name__ == \"__main__\":\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#35da9d6ef247941473dad9954a2fd4b07e2f731d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 1267 | Contributors (this commit): 15 | Commits (past 90d): 39 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -108,18 +108,18 @@ def cast(x, dtype):\n \n def dot(x, y):\n     '''Multiplies 2 tensors.\n-    When attempting to multiply a 2D tensor\n-    with a 3D tensor, reproduces the Theano behavior\n+    When attempting to multiply a ND tensor\n+    with a ND tensor, reproduces the Theano behavior\n     (e.g. (2, 3).(4, 3, 5) = (2, 4, 5))\n     '''\n-    if ndim(x) == 2 and ndim(y) == 3:\n-        slices = []\n-        for i in range(int_shape(y)[0]):\n-            slice_i = tf.matmul(x, y[i, :, :])\n-            slice_i = expand_dims(slice_i, 1)\n-            slices.append(slice_i)\n-        out = tf.concat(1, slices)\n-        return out\n+    if ndim(x) is not None and (ndim(x) > 2 or ndim(y) > 2):\n+        x_shape = (-1,) + int_shape(x)[1:]\n+        y_shape = int_shape(y)\n+        y_permute_dim = list(range(ndim(y)))\n+        y_permute_dim = [y_permute_dim.pop(-2)] + y_permute_dim\n+        xt = tf.reshape(x, [-1, x_shape[-1]])\n+        yt = tf.reshape(tf.transpose(y, perm=y_permute_dim), [y_shape[-2], -1])\n+        return tf.reshape(tf.matmul(xt, yt), x_shape[:-1] + y_shape[:-2] + y_shape[-1:])\n     out = tf.matmul(x, y)\n     return out\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d3615e682e1c65e56427bffbf7527d8c2a91609a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 10 | Files Changed: 3 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 13 | Churn Cumulative: 4233 | Contributors (this commit): 17 | Commits (past 90d): 60 | Contributors (cumulative): 23 | DMM Complexity: 0.0\n\nDIFF:\n@@ -697,7 +697,7 @@ def sparse_categorical_crossentropy(output, target, from_logits=False):\n     output_shape = output.get_shape()\n     res = tf.nn.sparse_softmax_cross_entropy_with_logits(\n         tf.reshape(output, [-1, int(output_shape[-1])]),\n-        cast(flatten(target), 'int32'))\n+        cast(flatten(target), 'int64'))\n     if len(output_shape) == 3:\n         # if our output includes timesteps we need to reshape\n         return tf.reshape(res, [-1, int(output_shape[-2])])\n\n@@ -1812,7 +1812,7 @@ class Container(Layer):\n     @property\n     def input_spec(self):\n         specs = []\n-        for layer in self.input_layers:\n+        for layer in getattr(self, 'input_layers', []):\n             if layer.input_spec is None:\n                 specs.append(None)\n             else:\n\n@@ -31,12 +31,6 @@ def test_parametric_softplus():\n                    input_shape=(2, 3, 4))\n \n \n-def test_thresholded_linear():\n-    from keras.layers.advanced_activations import ThresholdedLinear\n-    layer_test(ThresholdedLinear, kwargs={'theta': 0.5},\n-               input_shape=(2, 3, 4))\n-\n-\n def test_thresholded_relu():\n     from keras.layers.advanced_activations import ThresholdedReLU\n     layer_test(ThresholdedReLU, kwargs={'theta': 0.5},\n@@ -50,5 +44,4 @@ def test_srelu():\n \n \n if __name__ == '__main__':\n-    # pytest.main([__file__])\n-    test_srelu()\n+    pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0d7c8711bd175a48ae0a878fbdaeba8e7db9a0d5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 14 | Churn Cumulative: 3288 | Contributors (this commit): 4 | Commits (past 90d): 20 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -708,8 +708,8 @@ class Layer(object):\n         # reset layer connections\n         self.inbound_nodes = []\n         self.outbound_nodes = []\n-        self.build(input_shape=shape)\n-        input_shape = shape\n+        input_shape = tuple(shape)\n+        self.build(input_shape=input_shape)\n \n         # set Keras tensor metadata\n         input_tensor._uses_learning_phase = False\n@@ -937,6 +937,8 @@ class InputLayer(Layer):\n         if not batch_input_shape:\n             assert input_shape, 'An Input layer should be passed either a `batch_input_shape` or an `input_shape`.'\n             batch_input_shape = (None,) + tuple(input_shape)\n+        else:\n+            batch_input_shape = tuple(batch_input_shape)\n         if not input_dtype:\n             input_dtype = K.floatx()\n \n@@ -2227,8 +2229,6 @@ class Container(Layer):\n             flattened_layers = self.layers\n \n         f = h5py.File(filepath, 'w')\n-        for layer in flattened_layers:\n-            print(layer.name)\n         f.attrs['layer_names'] = [layer.name.encode('utf8') for layer in flattened_layers]\n \n         for layer in flattened_layers:\n\n@@ -190,9 +190,9 @@ def test_multi_input_layer():\n     assert get_source_inputs(c) == [a, b]\n \n     # serialization / deserialization\n-    model_config = model.get_config()\n-    print(json.dumps(model_config, indent=4))\n-    recreated_model = Model.from_config(model_config)\n+    json_config = model.to_json()\n+    recreated_model = model_from_json(json_config)\n+    recreated_model.compile('rmsprop', 'mse')\n \n     print('recreated:')\n     print([layer.name for layer in recreated_model.layers])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b4bdc5a0fae205c0a53871ae94503ab9fa0ae92f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 106 | Lines Deleted: 13 | Files Changed: 3 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 13/13 | Churn Δ: 119 | Churn Cumulative: 9834 | Contributors (this commit): 44 | Commits (past 90d): 54 | Contributors (cumulative): 48 | DMM Complexity: 0.041666666666666664\n\nDIFF:\n@@ -1403,3 +1403,74 @@ class Model(Container):\n                 averages.append(np.average([out[i] for out in all_outs],\n                                 weights=weights))\n             return averages\n+\n+    def predict_generator(self, generator, val_samples):\n+        '''Generate predictions for the input samples from a data generator.\n+        The generator should return the same kind of data as accepted by\n+        `predict_on_batch`.\n+\n+        Arguments:\n+            generator:\n+                generator yielding input samples\n+            val_samples:\n+                total number of samples to generate from `generator`\n+                before returning.\n+\n+        # Returns\n+            A Numpy array of predictions.\n+        '''\n+        processed_samples = 0\n+        wait_time = 0.05\n+        all_outs = []\n+        data_gen_queue, _stop = generator_queue(generator)\n+\n+        while processed_samples < val_samples:\n+            generator_output = None\n+            while not _stop.is_set():\n+                if not data_gen_queue.empty():\n+                    generator_output = data_gen_queue.get()\n+                    break\n+                else:\n+                    time.sleep(wait_time)\n+\n+            if isinstance(generator_output, tuple):\n+                if len(generator_output) == 2:\n+                    x, y = generator_output\n+                    sample_weight = None\n+                elif len(generator_output) == 3:\n+                    x, y, sample_weight = generator_output\n+                else:\n+                    _stop.set()\n+                    raise Exception('output of generator should be a tuple '\n+                                    '(x, y, sample_weight) '\n+                                    'or (x, y). Found: ' + str(generator_output))\n+            else:\n+                x = generator_output\n+\n+            x = standardize_input_data(x, self.input_names)\n+\n+            try:\n+                outs = self.predict_on_batch(x)\n+            except Exception as e:\n+                _stop.set()\n+                raise e\n+\n+            nb_samples = len(x[0])\n+\n+            if type(outs) != list:\n+                outs = [outs]\n+\n+            if len(all_outs) == 0:\n+                for out in outs:\n+                    shape = (val_samples,) + out.shape[1:]\n+                    all_outs.append(np.zeros(shape))\n+\n+            for i, out in enumerate(outs):\n+                all_outs[i][processed_samples:(processed_samples + nb_samples)] = out\n+\n+            processed_samples += nb_samples\n+\n+        _stop.set()\n+        if len(all_outs) == 1:\n+            return all_outs[0]\n+        return all_outs\n\n@@ -659,6 +659,24 @@ class Sequential(Model):\n         return self.model.evaluate_generator(generator,\n                                              val_samples)\n \n+    def predict_generator(self, generator, val_samples):\n+        '''Generate predictions for the input samples from a data generator.\n+        The generator should return the same kind of data as accepted by\n+        `predict_on_batch`.\n+\n+        Arguments:\n+            generator:\n+                generator yielding input samples\n+            val_samples:\n+                total number of samples to generate from `generator`\n+                before returning.\n+\n+        # Returns\n+            A Numpy array of predictions.\n+        '''\n+\n+        return self.model.predict_generator(generator, val_samples)\n+\n     def get_config(self):\n         '''Returns the model configuration\n         as a Python dictionary.\n\n@@ -11,6 +11,8 @@ from keras.layers.core import Dense, Activation, Merge, Lambda\n from keras.utils import np_utils\n from keras.utils.test_utils import get_test_data\n from keras.models import model_from_json, model_from_yaml\n+from keras import objectives\n+from keras.engine.training import make_batches\n \n \n input_dim = 16\n@@ -72,19 +74,15 @@ def test_sequential():\n     (X_train, y_train), (X_test, y_test) = _get_test_data()\n \n     # TODO: factor out\n-    def data_generator(train):\n-        if train:\n-            max_batch_index = len(X_train) // batch_size\n-        else:\n-            max_batch_index = len(X_test) // batch_size\n-        i = 0\n+    def data_generator(x, y, batch_size=50):\n+        index_array = np.arange(len(x))\n         while 1:\n-            if train:\n-                yield (X_train[i * batch_size: (i + 1) * batch_size], y_train[i * batch_size: (i + 1) * batch_size])\n-            else:\n-                yield (X_test[i * batch_size: (i + 1) * batch_size], y_test[i * batch_size: (i + 1) * batch_size])\n-            i += 1\n-            i = i % max_batch_index\n+            batches = make_batches(len(X_test), batch_size)\n+            for batch_index, (batch_start, batch_end) in enumerate(batches):\n+                batch_ids = index_array[batch_start:batch_end]\n+                x_batch = x[batch_ids]\n+                y_batch = y[batch_ids]\n+                yield (x_batch, y_batch)\n \n     model = Sequential()\n     model.add(Dense(nb_hidden, input_shape=(input_dim,)))\n@@ -100,9 +98,15 @@ def test_sequential():\n \n     model.train_on_batch(X_train[:32], y_train[:32])\n \n-    gen_loss = model.evaluate_generator(data_generator(True), 256)\n     loss = model.evaluate(X_test, y_test)\n \n+    prediction = model.predict_generator(data_generator(X_test, y_test), X_test.shape[0])\n+    gen_loss = model.evaluate_generator(data_generator(X_test, y_test, 50), X_test.shape[0])\n+    pred_loss = K.eval(K.mean(objectives.get(model.loss)(K.variable(y_test), K.variable(prediction))))\n+\n+    assert(np.isclose(pred_loss, loss))\n+    assert(np.isclose(gen_loss, loss))\n+\n     model.predict(X_test, verbose=0)\n     model.predict_classes(X_test, verbose=0)\n     model.predict_proba(X_test, verbose=0)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d627fd87813a48b08f4e316c54485006dc9ddde4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 8 | Files Changed: 7 | Hunks: 9 | Methods Changed: 2 | Complexity Δ (Sum/Max): -14/0 | Churn Δ: 24 | Churn Cumulative: 27639 | Contributors (this commit): 89 | Commits (past 90d): 151 | Contributors (cumulative): 139 | DMM Complexity: None\n\nDIFF:\n@@ -412,6 +412,12 @@ class Model(Container):\n                 See [objectives](objectives.md).\n                 If the model has multiple outputs, you can use a different loss\n                 on each output by passing a dictionary or a list of objectives.\n+            metrics: list of metrics to be evaluated by the model\n+                during training and testing.\n+                Typically you will use `metrics=['accuracy']`.\n+                To specify different metrics for different outputs of a\n+                multi-output model, you could also pass a dictionary,\n+                such as `metrics={'output_a': 'accuracy'}`.\n             sample_weight_mode: if you need to do timestep-wise\n                 sample weighting (2D weights), set this to \"temporal\".\n                 \"None\" defaults to sample-wise weights (1D).\n\n@@ -49,7 +49,7 @@ class PReLU(Layer):\n     # Output shape\n         Same shape as the input.\n \n-    # Arguments:\n+    # Arguments\n         init: initialization function for the weights.\n         weights: initial weights, as a list of a single numpy array.\n \n\n@@ -945,7 +945,7 @@ class AveragePooling3D(_Pooling3D):\n class UpSampling1D(Layer):\n     '''Repeat each temporal step `length` times along the time axis.\n \n-    # Arguments:\n+    # Arguments\n         length: integer. Upsampling factor.\n \n     # Input shape\n\n@@ -77,7 +77,7 @@ class Dropout(Layer):\n class Activation(Layer):\n     '''Applies an activation function to an output.\n \n-    # Arguments:\n+    # Arguments\n         activation: name of activation function to use\n             (see: [activations](../activations.md)),\n             or alternatively, a Theano or TensorFlow operation.\n\n@@ -58,7 +58,7 @@ class GaussianDropout(Layer):\n     # Output shape\n         Same shape as input.\n \n-    # References:\n+    # References\n         [Dropout: A Simple Way to Prevent Neural Networks from Overfitting Srivastava, Hinton, et al. 2014](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n     '''\n     def __init__(self, p, **kwargs):\n\n@@ -41,9 +41,10 @@ def time_distributed_dense(x, w, b=None, dropout=None,\n \n class Recurrent(Layer):\n     '''Abstract base class for recurrent layers.\n-    Do not use in a model -- it's not a functional layer!\n+    Do not use in a model -- it's not a valid layer!\n+    Use its children classes `LSTM`, `GRU` and `SimpleRNN` instead.\n \n-    All recurrent layers (GRU, LSTM, SimpleRNN) also\n+    All recurrent layers (`LSTM`, `GRU`, `SimpleRNN`) also\n     follow the specifications of this class and accept\n     the keyword arguments listed below.\n \n\n@@ -294,10 +294,11 @@ class Sequential(Model):\n         # Arguments\n             optimizer: str (name of optimizer) or optimizer object.\n                 See [optimizers](optimizers.md).\n-            metrics: list of str (name of metrics) or\n-                list of metrics functions. See [metrics](metrics.md).\n             loss: str (name of objective function) or objective function.\n                 See [objectives](objectives.md).\n+            metrics: list of metrics to be evaluated by the model\n+                during training and testing.\n+                Typically you will use `metrics=['accuracy']`.\n             sample_weight_mode: if you need to do timestep-wise\n                 sample weighting (2D weights), set this to \"temporal\".\n                 \"None\" defaults to sample-wise weights (1D).\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1baddb9094402d14fd9484426e8bbdb778748ead", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 13 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 14/13 | Churn Δ: 22 | Churn Cumulative: 9046 | Contributors (this commit): 43 | Commits (past 90d): 50 | Contributors (cumulative): 45 | DMM Complexity: None\n\nDIFF:\n@@ -1411,19 +1411,17 @@ class Model(Container):\n             return averages\n \n     def predict_generator(self, generator, val_samples):\n-        '''Generate predictions for the input samples from a data generator.\n+        '''Generates predictions for the input samples from a data generator.\n         The generator should return the same kind of data as accepted by\n         `predict_on_batch`.\n \n-        Arguments:\n-            generator:\n-                generator yielding input samples\n-            val_samples:\n-                total number of samples to generate from `generator`\n+        # Arguments\n+            generator: generator yielding batches of input samples.\n+            val_samples: total number of samples to generate from `generator`\n                 before returning.\n \n         # Returns\n-            A Numpy array of predictions.\n+            Numpy array(s) of predictions.\n         '''\n         processed_samples = 0\n         wait_time = 0.05\n\n@@ -661,15 +661,13 @@ class Sequential(Model):\n                                              val_samples)\n \n     def predict_generator(self, generator, val_samples):\n-        '''Generate predictions for the input samples from a data generator.\n+        '''Generates predictions for the input samples from a data generator.\n         The generator should return the same kind of data as accepted by\n         `predict_on_batch`.\n \n-        Arguments:\n-            generator:\n-                generator yielding input samples\n-            val_samples:\n-                total number of samples to generate from `generator`\n+        # Arguments\n+            generator: generator yielding batches of input samples.\n+            val_samples: total number of samples to generate from `generator`\n                 before returning.\n \n         # Returns\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ed4a95bdada471ef42f80d539a453d81826f78c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 7 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 2574 | Contributors (this commit): 4 | Commits (past 90d): 17 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1591,27 +1591,23 @@ class Container(Layer):\n             node = layer.inbound_nodes[node_index]\n \n             # prevent cycles\n-            if node in seen_nodes:\n-                return\n             seen_nodes.add(node)\n \n-            # basic sanity checks\n-            assert node.outbound_layer == layer\n-            assert node.output_tensors[tensor_index] == tensor\n-\n             node_key = layer.name + '_ib-' + str(node_index)\n             # update container_nodes\n             container_nodes.add(node_key)\n             # update nodes_depths\n-            if node not in nodes_depths:\n+            node_depth = nodes_depths.get(node)\n+            if node_depth is None:\n                 nodes_depths[node] = depth\n             else:\n-                nodes_depths[node] = max(depth, nodes_depths[node])\n+                nodes_depths[node] = max(depth, node_depth)\n             # update layers_depths\n-            if layer not in layers_depths:\n+            layer_depth = layers_depths.get(layer)\n+            if layer_depth is None:\n                 layers_depths[layer] = depth\n             else:\n-                layers_depths[layer] = max(depth, layers_depths[layer])\n+                layers_depths[layer] = max(depth, layer_depth)\n \n             # propagate to all previous tensors connected to this node\n             for i in range(len(node.inbound_layers)):\n@@ -1619,6 +1615,7 @@ class Container(Layer):\n                 layer = node.inbound_layers[i]\n                 node_index = node.node_indices[i]\n                 tensor_index = node.tensor_indices[i]\n+                if not layer.inbound_nodes[node_index] in seen_nodes:\n                     build_map_of_graph(x, copy.copy(seen_nodes), depth + 1,\n                                        layer, node_index, tensor_index)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e1791984105cc734e3dd495ec7e84414fd1bdac9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 14 | Churn Cumulative: 2588 | Contributors (this commit): 4 | Commits (past 90d): 18 | Contributors (cumulative): 4 | DMM Complexity: 0.3333333333333333\n\nDIFF:\n@@ -1570,6 +1570,9 @@ class Container(Layer):\n         nodes_depths = {}  # map {node: depth value}\n         layers_depths = {}  # map {layer: depth value}\n \n+        def make_node_key(node, depth):\n+            return str(id(node)) + '-' + str(depth)\n+\n         def build_map_of_graph(tensor, seen_nodes=set(), depth=0,\n                                layer=None, node_index=None, tensor_index=None):\n             '''This recursively updates the maps nodes_depths,\n@@ -1591,7 +1594,7 @@ class Container(Layer):\n             node = layer.inbound_nodes[node_index]\n \n             # prevent cycles\n-            seen_nodes.add(node)\n+            seen_nodes.add(make_node_key(node, depth))\n \n             node_key = layer.name + '_ib-' + str(node_index)\n             # update container_nodes\n@@ -1615,12 +1618,15 @@ class Container(Layer):\n                 layer = node.inbound_layers[i]\n                 node_index = node.node_indices[i]\n                 tensor_index = node.tensor_indices[i]\n-                if not layer.inbound_nodes[node_index] in seen_nodes:\n-                    build_map_of_graph(x, copy.copy(seen_nodes), depth + 1,\n+                next_node = layer.inbound_nodes[node_index]\n+                node_key = make_node_key(next_node, depth + 1)\n+                if node_key not in seen_nodes:\n+                    build_map_of_graph(x, seen_nodes, depth + 1,\n                                        layer, node_index, tensor_index)\n \n         for x in self.outputs:\n-            build_map_of_graph(x, seen_nodes=set(), depth=0)\n+            seen_nodes = set()\n+            build_map_of_graph(x, seen_nodes, depth=0)\n \n         # build a map {depth: list of nodes with this depth}\n         nodes_by_depth = {}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#63059f6063627d4cd01af927bfa03c1cc4a65277", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1540 | Contributors (this commit): 2 | Commits (past 90d): 11 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1126,7 +1126,7 @@ class Model(Container):\n                                                            sample_weight=sample_weight,\n                                                            check_batch_dim=True)\n         if self.uses_learning_phase:\n-            ins = x + y + sample_weights + [1.]\n+            ins = x + y + sample_weights + [0.]\n         else:\n             ins = x + y + sample_weights\n         outputs = self.test_function(ins)\n@@ -1140,7 +1140,7 @@ class Model(Container):\n         x = standardize_input_data(x, self.input_names,\n                                    self.internal_input_shapes)\n         if self.uses_learning_phase:\n-            ins = x + [1.]\n+            ins = x + [0.]\n         else:\n             ins = x\n         outputs = self.predict_function(ins)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8c73c6f218abc2b7345bab8928e5c5bcf192f4d1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 241 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -12,7 +12,6 @@ Some configurations won't converge.\n - LSTM loss decrease patterns during training can be quite different\n from what you see with CNNs/MLPs/etc.\n '''\n-\n from __future__ import print_function\n import numpy as np\n np.random.seed(1337)  # for reproducibility\n@@ -43,9 +42,8 @@ print('X_test shape:', X_test.shape)\n \n print('Build model...')\n model = Sequential()\n-model.add(Embedding(max_features, 128, input_length=maxlen, dropout=0.5))\n-model.add(LSTM(128, dropout_W=0.5, dropout_U=0.5))  # try using a GRU instead, for fun\n-model.add(Dropout(0.5))\n+model.add(Embedding(max_features, 128, input_length=maxlen, dropout=0.2))\n+model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))  # try using a GRU instead, for fun\n model.add(Dense(1))\n model.add(Activation('sigmoid'))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#201352784088145cb1b320fc7762c81f493df19e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 197 | Lines Deleted: 45 | Files Changed: 1 | Hunks: 83 | Methods Changed: 69 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 242 | Churn Cumulative: 1511 | Contributors (this commit): 15 | Commits (past 90d): 41 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -12,11 +12,18 @@ _LEARNING_PHASE = tf.placeholder(dtype='uint8', name='keras_learning_phase')  #\n \n \n def learning_phase():\n-    # False = test, True = train\n+    '''Returns the learning phase flag.\n+\n+    The learning phase flag is an integer tensor (0 = test, 1 = train)\n+    to be passed as input to any Keras function\n+    that uses a different behavior at train time and test time.\n+    '''\n     return _LEARNING_PHASE\n \n \n def get_session():\n+    '''Returns the TF session in use by the backend.\n+    '''\n     global _SESSION\n     if _SESSION is None:\n         if not os.environ.get('OMP_NUM_THREADS'):\n@@ -28,6 +35,8 @@ def get_session():\n \n \n def set_session(session):\n+    '''Sets the TF session.\n+    '''\n     global _SESSION\n     _SESSION = session\n \n@@ -35,12 +44,36 @@ def set_session(session):\n # VARIABLE MANIPULATION\n \n def variable(value, dtype=_FLOATX, name=None):\n+    '''Instantiates a tensor.\n+\n+    # Arguments\n+        value: numpy array, initial value of the tensor.\n+        dtype: tensor type.\n+        name: optional name string for the tensor.\n+\n+    # Returns\n+        Tensor variable instance.\n+    '''\n     v = tf.Variable(np.asarray(value, dtype=dtype), name=name)\n     get_session().run(v.initializer)\n     return v\n \n \n def placeholder(shape=None, ndim=None, dtype=_FLOATX, name=None):\n+    '''Instantiates a placeholder.\n+\n+    # Arguments:\n+        shape: shape of the placeholder\n+            (integer tuple, may include None entries).\n+        ndim: number of axes of the tensor.\n+            At least one of {`shape`, `ndim`} must be specified.\n+            If both are specified, `shape` is used.\n+        dtype: placeholder type.\n+        name: optional name string for the placeholder.\n+\n+    # Returns:\n+        Placeholder tensor instance.\n+    '''\n     if not shape:\n         if ndim:\n             shape = tuple([None for _ in range(ndim)])\n@@ -51,16 +84,22 @@ def placeholder(shape=None, ndim=None, dtype=_FLOATX, name=None):\n \n \n def shape(x):\n-    # symbolic shape\n+    '''Returns the symbolic shape of a tensor.\n+    '''\n     return tf.shape(x)\n \n \n def int_shape(x):\n+    '''Returns the shape of a tensor as a tuple of\n+    integers or None entries.\n+    '''\n     shape = x.get_shape()\n     return tuple([i.__int__() for i in shape])\n \n \n def ndim(x):\n+    '''Returns the number of axes in a tensor, as an integer.\n+    '''\n     dims = x.get_shape()._dims\n     if dims is not None:\n         return len(dims)\n@@ -68,39 +107,54 @@ def ndim(x):\n \n \n def dtype(x):\n+    '''Returns the dtype of a tensor, as a string.\n+    '''\n     return x.dtype.name\n \n \n def eval(x):\n-    '''Run a graph.\n+    '''Evaluates the value of a tensor.\n+    Returns a Numpy array.\n     '''\n     return x.eval(session=get_session())\n \n \n def zeros(shape, dtype=_FLOATX, name=None):\n+    '''Instantiates an all-zeros tensor variable.\n+    '''\n     return variable(np.zeros(shape), dtype, name)\n \n \n def ones(shape, dtype=_FLOATX, name=None):\n+    '''Instantiates an all-ones tensor variable.\n+    '''\n     return variable(np.ones(shape), dtype, name)\n \n \n-def ones_like(x, name=None):\n-    return tf.ones_like(x, name=name)\n-\n-\n def zeros_like(x, name=None):\n+    '''Instantiates an all-zeros tensor\n+    of the same shape as another tensor.\n+    '''\n     return tf.zeros_like(x, name=name)\n \n \n+def ones_like(x, name=None):\n+    '''Instantiates an all-ones tensor\n+    of the same shape as another tensor.\n+    '''\n+    return tf.ones_like(x, name=name)\n+\n+\n def count_params(x):\n-    '''Return number of scalars in a tensor.\n+    '''Returns the number of scalars in a tensor.\n     '''\n     shape = x.get_shape()\n     return np.prod([shape[i]._value for i in range(len(shape))])\n \n \n def cast(x, dtype):\n+    '''Casts a tensor to a different dtype.\n+    '''\n     return tf.cast(x, dtype)\n \n \n@@ -125,7 +179,8 @@ def dot(x, y):\n \n \n def batch_dot(x, y, axes=None):\n-    '''batchwise dot product\n+    '''Batchwise dot product.\n+\n     batch_dot results in a tensor with less dimensions than the input.\n     If the number of dimensions is reduced to 1, we use `expand_dims` to\n     make sure that ndim is at least 2.\n@@ -160,24 +215,28 @@ def batch_dot(x, y, axes=None):\n \n \n def transpose(x):\n+    '''Transposes a matrix.\n+    '''\n     return tf.transpose(x)\n \n \n def gather(reference, indices):\n-    '''\n+    '''Retrieves the vectors of indices `indices`\n+    in the 2D tensor `reference`.\n+\n     # Arguments\n-        reference: a tensor.\n+        reference: a 2D tensor.\n         indices: an int tensor of indices.\n \n     # Returns\n-        a tensor of same type as `reference`.\n+        A 3D tensor of same type as `reference`.\n     '''\n     return tf.gather(reference, indices)\n \n \n # ELEMENT-WISE OPERATIONS\n \n-def normalize_axis(axis, ndim):\n+def _normalize_axis(axis, ndim):\n     if type(axis) is tuple:\n         axis = list(axis)\n     if type(axis) is list:\n@@ -191,31 +250,37 @@ def normalize_axis(axis, ndim):\n \n \n def max(x, axis=None, keepdims=False):\n-    axis = normalize_axis(axis, ndim(x))\n+    '''Maximum value in a tensor.\n+    '''\n+    axis = _normalize_axis(axis, ndim(x))\n     return tf.reduce_max(x, reduction_indices=axis, keep_dims=keepdims)\n \n \n def min(x, axis=None, keepdims=False):\n-    axis = normalize_axis(axis, ndim(x))\n+    '''Minimum value in a tensor.\n+    '''\n+    axis = _normalize_axis(axis, ndim(x))\n     return tf.reduce_min(x, reduction_indices=axis, keep_dims=keepdims)\n \n \n def sum(x, axis=None, keepdims=False):\n     '''Sum of the values in a tensor, alongside the specified axis.\n     '''\n-    axis = normalize_axis(axis, ndim(x))\n+    axis = _normalize_axis(axis, ndim(x))\n     return tf.reduce_sum(x, reduction_indices=axis, keep_dims=keepdims)\n \n \n def prod(x, axis=None, keepdims=False):\n-    '''Multiply the values in a tensor, alongside the specified axis.\n+    '''Multiplies the values in a tensor, alongside the specified axis.\n     '''\n-    axis = normalize_axis(axis, ndim(x))\n+    axis = _normalize_axis(axis, ndim(x))\n     return tf.reduce_prod(x, reduction_indices=axis, keep_dims=keepdims)\n \n \n def std(x, axis=None, keepdims=False):\n-    axis = normalize_axis(axis, ndim(x))\n+    '''Standard deviation of a tensor, alongside the specificied axis.\n+    '''\n+    axis = _normalize_axis(axis, ndim(x))\n     if x.dtype.base_dtype == tf.bool:\n         x = tf.cast(x, _FLOATX)\n     m = tf.reduce_mean(x, reduction_indices=axis, keep_dims=True)\n@@ -226,7 +291,9 @@ def std(x, axis=None, keepdims=False):\n \n \n def mean(x, axis=None, keepdims=False):\n-    axis = normalize_axis(axis, ndim(x))\n+    '''Mean of a tensor, alongside the specificied axis.\n+    '''\n+    axis = _normalize_axis(axis, ndim(x))\n     if x.dtype.base_dtype == tf.bool:\n         x = tf.cast(x, _FLOATX)\n     return tf.reduce_mean(x, reduction_indices=axis, keep_dims=keepdims)\n@@ -235,61 +302,85 @@ def mean(x, axis=None, keepdims=False):\n def any(x, axis=None, keepdims=False):\n     '''Bitwise reduction (logical OR).\n \n-    Return array of uint8 (0s and 1s).\n+    Returns an uint8 tensor (0s and 1s).\n     '''\n-    axis = normalize_axis(axis, ndim(x))\n+    axis = _normalize_axis(axis, ndim(x))\n     x = tf.cast(x, tf.bool)\n     x = tf.reduce_any(x, reduction_indices=axis, keep_dims=keepdims)\n     return tf.cast(x, tf.uint8)\n \n \n def argmax(x, axis=-1):\n+    '''Returns the index of the maximum value\n+    along a tensor axis.\n+    '''\n     if axis < 0:\n         axis = axis % len(x.get_shape())\n     return tf.argmax(x, axis)\n \n \n def argmin(x, axis=-1):\n+    '''Returns the index of the minimum value\n+    along a tensor axis.\n+    '''\n     if axis < 0:\n         axis = axis % len(x.get_shape())\n     return tf.argmin(x, axis)\n \n \n def square(x):\n+    '''Element-wise square.\n+    '''\n     return tf.square(x)\n \n \n def abs(x):\n+    '''Element-wise absolute value.\n+    '''\n     return tf.abs(x)\n \n \n def sqrt(x):\n+    '''Element-wise square root.\n+    '''\n     x = tf.clip_by_value(x, tf.cast(0., dtype=_FLOATX),\n                          tf.cast(np.inf, dtype=_FLOATX))\n     return tf.sqrt(x)\n \n \n def exp(x):\n+    '''Element-wise exponential.\n+    '''\n     return tf.exp(x)\n \n \n def log(x):\n+    '''Element-wise log.\n+    '''\n     return tf.log(x)\n \n \n def round(x):\n+    '''Element-wise rounding to the closest integer.\n+    '''\n     return tf.round(x)\n \n \n def sign(x):\n+    '''Element-wise sign.\n+    '''\n     return tf.sign(x)\n \n \n def pow(x, a):\n+    '''Element-wise exponentiation.\n+    '''\n     return tf.pow(x, a)\n \n \n def clip(x, min_value, max_value):\n+    '''Element-wise value clipping.\n+    '''\n     if max_value < min_value:\n         max_value = min_value\n     return tf.clip_by_value(x, tf.cast(min_value, dtype=_FLOATX),\n@@ -297,24 +388,36 @@ def clip(x, min_value, max_value):\n \n \n def equal(x, y):\n+    '''Element-wise equality between two tensors.\n+    Returns a bool tensor.\n+    '''\n     return tf.equal(x, y)\n \n \n def not_equal(x, y):\n+    '''Element-wise inequality between two tensors.\n+    Returns a bool tensor.\n+    '''\n     return tf.not_equal(x, y)\n \n \n def maximum(x, y):\n+    '''Element-wise maximum of two tensors.\n+    '''\n     return tf.maximum(x, y)\n \n \n def minimum(x, y):\n+    '''Element-wise minimum of two tensors.\n+    '''\n     return tf.minimum(x, y)\n \n \n # SHAPE OPERATIONS\n \n def concatenate(tensors, axis=-1):\n+    '''Concantes a list of tensors alongside the specified axis.\n+    '''\n     if axis < 0:\n         if len(tensors[0].get_shape()):\n             axis = axis % len(tensors[0].get_shape())\n@@ -324,21 +427,23 @@ def concatenate(tensors, axis=-1):\n \n \n def reshape(x, shape):\n+    '''Reshapes a tensor to the specified shape.\n+    '''\n     return tf.reshape(x, shape)\n \n \n def permute_dimensions(x, pattern):\n-    '''Transpose dimensions.\n+    '''Permutes axes in a tensor.\n \n     # Arguments\n-        pattern: should be a tuple or list of\n-            dimension indices, e.g. [0, 2, 1].\n+        pattern: should be a tuple of\n+            dimension indices, e.g. (0, 2, 1).\n     '''\n     return tf.transpose(x, perm=pattern)\n \n \n def resize_images(X, height_factor, width_factor, dim_ordering):\n-    '''Resize the images contained in a 4D tensor of shape\n+    '''Resizes the images contained in a 4D tensor of shape\n     - [batch, channels, height, width] (for 'th' dim_ordering)\n     - [batch, height, width, channels] (for 'tf' dim_ordering)\n     by a factor of (height_factor, width_factor). Both factors should be\n@@ -373,7 +478,7 @@ def repeat_elements(x, rep, axis):\n \n \n def repeat(x, n):\n-    '''Repeat a 2D tensor:\n+    '''Repeats a 2D tensor:\n \n     if x has shape (samples, dim) and n=2,\n     the output will have shape (samples, 2, dim)\n@@ -401,19 +506,19 @@ def batch_flatten(x):\n \n \n def expand_dims(x, dim=-1):\n-    '''Add a 1-sized dimension at index \"dim\".\n+    '''Adds a 1-sized dimension at index \"dim\".\n     '''\n     return tf.expand_dims(x, dim)\n \n \n def squeeze(x, axis):\n-    '''Remove a 1-dimension from the tensor at index \"axis\".\n+    '''Removes a 1-dimension from the tensor at index \"axis\".\n     '''\n     return tf.squeeze(x, [axis])\n \n \n def temporal_padding(x, padding=1):\n-    '''Pad the middle dimension of a 3D tensor\n+    '''Pads the middle dimension of a 3D tensor\n     with \"padding\" zeros left and right.\n     '''\n     pattern = [[0, 0], [padding, padding], [0, 0]]\n@@ -421,7 +526,7 @@ def temporal_padding(x, padding=1):\n \n \n def spatial_2d_padding(x, padding=(1, 1), dim_ordering='th'):\n-    '''Pad the 2nd and 3rd dimensions of a 4D tensor\n+    '''Pads the 2nd and 3rd dimensions of a 4D tensor\n     with \"padding[0]\" and \"padding[1]\" (resp.) zeros left and right.\n     '''\n     if dim_ordering == 'th':\n@@ -442,12 +547,16 @@ def pack(x):\n \n \n def get_value(x):\n-    '''Technically the same as eval() for TF.\n+    '''Returns the value of a tensor variable,\n+    as a Numpy array.\n     '''\n     return x.eval(session=get_session())\n \n \n def set_value(x, value):\n+    '''Sets the value of a tensor variable,\n+    from a Numpy array.\n+    '''\n     tf.assign(x, np.asarray(value)).op.run(session=get_session())\n \n \n@@ -474,6 +583,13 @@ class Function(object):\n \n \n def function(inputs, outputs, updates=[], **kwargs):\n+    '''Instantiates a Keras function.\n+\n+    # Arguments\n+        inputs: list of placeholder/variable tensors.\n+        outputs: list of output tensors.\n+        updates: list of update tuples (old_tensor, new_tensor).\n+    '''\n     if len(kwargs) > 0:\n         msg = [\n             \"Expected no kwargs, you passed %s\" % len(kwargs),\n@@ -484,6 +600,9 @@ def function(inputs, outputs, updates=[], **kwargs):\n \n \n def gradients(loss, variables):\n+    '''Returns the gradients of `variables` (list of tensor variables)\n+    with regard to `loss`.\n+    '''\n     return tf.gradients(loss, variables)\n \n \n@@ -515,9 +634,10 @@ def rnn(step_function, inputs, initial_states,\n         mask: binary tensor with shape (samples, time, 1),\n             with a zero for every element that is masked.\n         constants: a list of constant values passed at each step.\n-        unroll: with TensorFlow the RNN is always unrolled, so this\n-            does do anything for the time being.\n+        unroll: with TensorFlow the RNN is always unrolled, but with Theano you\n+            can use this boolean flag to unroll the RNN.\n         input_length: not relevant in the TensorFlow implementation.\n+            Must be specified if using unrolling with Theano.\n \n     # Returns\n         A tuple (last_output, outputs, new_states).\n@@ -596,7 +716,7 @@ def rnn(step_function, inputs, initial_states,\n \n \n def switch(condition, then_expression, else_expression):\n-    '''Switch between two operations depending on a scalar value (int or bool).\n+    '''Switches between two operations depending on a scalar value (int or bool).\n     Note that both `then_expression` and `else_expression`\n     should be symbolic tensors of the *same shape*.\n \n@@ -642,7 +762,7 @@ def in_test_phase(x, expression):\n # NN OPERATIONS\n \n def relu(x, alpha=0., max_value=None):\n-    '''ReLU.\n+    '''Rectified linear unit\n \n     # Arguments\n         alpha: slope of negative section.\n@@ -660,17 +780,24 @@ def relu(x, alpha=0., max_value=None):\n \n \n def softmax(x):\n+    '''Softmax of a tensor.\n+    '''\n     return tf.nn.softmax(x)\n \n \n def softplus(x):\n+    '''Softplus of a tensor.\n+    '''\n     return tf.nn.softplus(x)\n \n \n def categorical_crossentropy(output, target, from_logits=False):\n-    '''Note: tf.nn.softmax_cross_entropy_with_logits\n-    expects logits, Keras expects probabilities.\n+    '''Categorical crossentropy between an output tensor\n+    and a target tensor, where the target is a tensor of the same\n+    shape as the output.\n     '''\n+    # Note: tf.nn.softmax_cross_entropy_with_logits\n+    # expects logits, Keras expects probabilities.\n     if not from_logits:\n         # scale preds so that the class probas of each sample sum to 1\n         output /= tf.reduce_sum(output,\n@@ -686,9 +813,11 @@ def categorical_crossentropy(output, target, from_logits=False):\n \n \n def sparse_categorical_crossentropy(output, target, from_logits=False):\n-    '''Note: tf.nn.sparse_softmax_cross_entropy_with_logits\n-    expects logits, Keras expects probabilities.\n+    '''Categorical crossentropy between an output tensor\n+    and a target tensor, where the target is an integer tensor.\n     '''\n+    # Note: tf.nn.softmax_cross_entropy_with_logits\n+    # expects logits, Keras expects probabilities.\n     if not from_logits:\n         output = tf.clip_by_value(output, tf.cast(_EPSILON, dtype=_FLOATX),\n                                   tf.cast(1.-_EPSILON, dtype=_FLOATX))\n@@ -706,9 +835,10 @@ def sparse_categorical_crossentropy(output, target, from_logits=False):\n \n \n def binary_crossentropy(output, target, from_logits=False):\n-    '''Note: tf.nn.sigmoid_cross_entropy_with_logits\n-    expects logits, Keras expects probabilities.\n+    '''Binary crossentropy between an output tensor and a target tensor.\n     '''\n+    # Note: tf.nn.softmax_cross_entropy_with_logits\n+    # expects logits, Keras expects probabilities.\n     if not from_logits:\n         # transform back to logits\n         output = tf.clip_by_value(output, tf.cast(_EPSILON, dtype=_FLOATX),\n@@ -718,10 +848,15 @@ def binary_crossentropy(output, target, from_logits=False):\n \n \n def sigmoid(x):\n+    '''Element-wise sigmoid.\n+    '''\n     return tf.nn.sigmoid(x)\n \n \n def hard_sigmoid(x):\n+    '''Segment-wise linear approximation of sigmoid.\n+    Faster than sigmoid.\n+    '''\n     x = (0.2 * x) + 0.5\n     x = tf.clip_by_value(x, tf.cast(0., dtype=_FLOATX),\n                          tf.cast(1., dtype=_FLOATX))\n@@ -729,10 +864,21 @@ def hard_sigmoid(x):\n \n \n def tanh(x):\n+    '''Element-wise tanh.\n+    '''\n     return tf.nn.tanh(x)\n \n \n def dropout(x, level, seed=None):\n+    '''Sets entries in `x` to zero at random,\n+    while scaling the entire tensor.\n+\n+    # Arguments\n+        x: tensor\n+        level: fraction of the entries in the tensor\n+            that will be set to 0\n+        seed: random seed to ensure determinism.\n+    '''\n     retain_prob = 1. - level\n     if seed is None:\n         seed = np.random.randint(10e6)\n@@ -742,6 +888,8 @@ def dropout(x, level, seed=None):\n \n \n def l2_normalize(x, axis):\n+    '''Normalizes a tensor wrt the L2 norm alonside the specified axis.\n+    '''\n     if axis < 0:\n         axis = axis % len(x.get_shape())\n     return tf.nn.l2_normalize(x, dim=axis)\n@@ -752,11 +900,13 @@ def l2_normalize(x, axis):\n \n def conv2d(x, kernel, strides=(1, 1), border_mode='valid', dim_ordering='th',\n            image_shape=None, filter_shape=None):\n-    '''Runs on cuDNN if available.\n+    '''2D convolution.\n \n     # Arguments\n+        kernel: kernel tensor.\n+        strides: strides tuple.\n         border_mode: string, \"same\" or \"valid\".\n-        dim_ordering: whether to use Theano or TensorFlow dimension ordering\n+        dim_ordering: \"tf\" or \"th\". Whether to use Theano or TensorFlow dimension ordering\n         in inputs/kernels/ouputs.\n     '''\n     if border_mode == 'same':\n@@ -796,12 +946,14 @@ def conv2d(x, kernel, strides=(1, 1), border_mode='valid', dim_ordering='th',\n \n def pool2d(x, pool_size, strides=(1, 1),\n            border_mode='valid', dim_ordering='th', pool_mode='max'):\n-    '''\n+    '''2D Pooling.\n+\n     # Arguments\n         pool_size: tuple of 2 integers.\n         strides: tuple of 2 integers.\n         border_mode: one of \"valid\", \"same\".\n         dim_ordering: one of \"th\", \"tf\".\n+        pool_mode: one of \"max\", \"avg\".\n     '''\n     if border_mode == 'same':\n         padding = 'SAME'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
