{"custom_id": "scrapy#770a8127e8e76d95243c1d586b4bb6113a38870a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 96 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 9 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 96 | Churn Cumulative: 96 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,96 @@\n+from os.path import join, abspath\n+\n+from tests.test_commands import CommandTest\n+\n+\n+class CheckCommandTest(CommandTest):\n+\n+    command = 'check'\n+\n+    def setUp(self):\n+        super(CheckCommandTest, self).setUp()\n+        self.spider_name = 'check_spider'\n+        self.spider = abspath(join(self.proj_mod_path, 'spiders', 'checkspider.py'))\n+\n+    def _write_contract(self, contracts, parse_def):\n+        with open(self.spider, 'w') as file:\n+            file.write(f\"\"\"\n+import scrapy\n+\n+class CheckSpider(scrapy.Spider):\n+    name = '{self.spider_name}'\n+    start_urls = ['http://example.com']\n+\n+    def parse(self, response, **cb_kwargs):\n+        \\\"\\\"\\\"\n+        @url http://www.amazon.com/s?field-keywords=selfish+gene\n+        {contracts}\n+        \\\"\\\"\\\"\n+        {parse_def}\n+            \"\"\")\n+\n+    def _test_contract(self, contracts='', parse_def='pass'):\n+        self._write_contract(contracts, parse_def)\n+        p, out, err = self.proc('check')\n+        self.assertIn('OK', err)\n+        self.assertEqual(p.returncode, 0)\n+\n+    def test_check_returns_requests_contract(self):\n+        contracts = \"\"\"\n+        @returns requests 1\n+        \"\"\"\n+        parse_def = \"\"\"\n+        yield scrapy.Request(url='http://next-url.com')\n+        \"\"\"\n+        self._test_contract(contracts, parse_def)\n+\n+    def test_check_returns_items_contract(self):\n+        contracts = \"\"\"\n+        @returns items 1\n+        \"\"\"\n+        parse_def = \"\"\"\n+        yield {'key1': 'val1', 'key2': 'val2'}\n+        \"\"\"\n+        self._test_contract(contracts, parse_def)\n+\n+    def test_check_cb_kwargs_contract(self):\n+        contracts = \"\"\"\n+        @cb_kwargs {\"arg1\": \"val1\", \"arg2\": \"val2\"}\n+        \"\"\"\n+        parse_def = \"\"\"\n+        if len(cb_kwargs.items()) == 0:\n+            raise Exception(\"Callback args not set\")\n+        \"\"\"\n+        self._test_contract(contracts, parse_def)\n+\n+    def test_check_scrapes_contract(self):\n+        contracts = \"\"\"\n+        @scrapes key1 key2\n+        \"\"\"\n+        parse_def = \"\"\"\n+        yield {'key1': 'val1', 'key2': 'val2'}\n+        \"\"\"\n+        self._test_contract(contracts, parse_def)\n+\n+    def test_check_all_default_contracts(self):\n+        contracts = \"\"\"\n+        @returns items 1\n+        @returns requests 1\n+        @scrapes key1 key2\n+        @cb_kwargs {\"arg1\": \"val1\", \"arg2\": \"val2\"}\n+        \"\"\"\n+        parse_def = \"\"\"\n+        yield {'key1': 'val1', 'key2': 'val2'}\n+        yield scrapy.Request(url='http://next-url.com')\n+        if len(cb_kwargs.items()) == 0:\n+            raise Exception(\"Callback args not set\")\n+        \"\"\"\n+        self._test_contract(contracts, parse_def)\n+\n+    def test_SCRAPY_CHECK_set(self):\n+        parse_def = \"\"\"\n+        import os\n+        if not os.environ.get('SCRAPY_CHECK'):\n+            raise Exception('SCRAPY_CHECK not set')\n+        \"\"\"\n+        self._test_contract(parse_def=parse_def)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1c40dfa7408026bc9ae831000a8614e8f4a9dd0d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 17 | Files Changed: 4 | Hunks: 16 | Methods Changed: 6 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 45 | Churn Cumulative: 3543 | Contributors (this commit): 14 | Commits (past 90d): 57 | Contributors (cumulative): 18 | DMM Complexity: 0.36363636363636365\n\nDIFF:\n@@ -20,7 +20,6 @@ from scrapy.core.http2.stream import Stream, StreamCloseReason\n from scrapy.core.http2.types import H2ConnectionMetadataDict\n from scrapy.http import Request\n \n-\n logger = logging.getLogger(__name__)\n \n \n@@ -30,7 +29,8 @@ class H2ClientProtocol(Protocol):\n         self.conn = H2Connection(config=config)\n \n         # ID of the next request stream\n-        # Following the convention made by hyper-h2 all IDs will be odd\n+        # Following the convention - 'Streams initiated by a client MUST\n+        # use odd-numbered stream identifiers' (RFC 7540)\n         self._stream_id_generator = itertools.count(start=1, step=2)\n \n         # Streams are stored in a dictionary keyed off their stream IDs\n@@ -160,20 +160,25 @@ class H2ClientProtocol(Protocol):\n         \"\"\"Called by Twisted when the transport connection is lost.\n         No need to write anything to transport here.\n         \"\"\"\n+        errors = []\n+        if not reason.check(connectionDone):\n+            logger.warning(\"Connection lost with reason \" + str(reason))\n+            errors.append(reason)\n+\n+        if self._protocol_error:\n+            errors.append(self._protocol_error)\n+\n         for stream in self.streams.values():\n             if stream.request_sent:\n-                stream.close(StreamCloseReason.CONNECTION_LOST, self._protocol_error, from_protocol=True)\n+                stream.close(StreamCloseReason.CONNECTION_LOST, errors, from_protocol=True)\n             else:\n                 stream.close(StreamCloseReason.INACTIVE, from_protocol=True)\n \n         self._active_streams -= len(self.streams)\n         self.streams.clear()\n-        self._send_pending_requests()\n+        self._pending_request_stream_pool.clear()\n         self.conn.close_connection()\n \n-        if not reason.check(connectionDone):\n-            logger.warning(\"Connection lost with reason \" + str(reason))\n-\n     def _handle_events(self, events: list) -> None:\n         \"\"\"Private method which acts as a bridge between the events\n         received from the HTTP/2 data and IH2EventsHandler\n\n@@ -17,11 +17,9 @@ from scrapy.http import Request\n from scrapy.http.headers import Headers\n from scrapy.responsetypes import responsetypes\n \n-\n if TYPE_CHECKING:\n     from scrapy.core.http2.protocol import H2ClientProtocol\n \n-\n logger = logging.getLogger(__name__)\n \n \n@@ -326,7 +324,12 @@ class Stream:\n \n         return expected_size != received_body_size\n \n-    def close(self, reason: StreamCloseReason, error: Optional[Exception] = None, from_protocol: bool = False) -> None:\n+    def close(\n+        self,\n+        reason: StreamCloseReason,\n+        errors: Optional[List[Exception]] = None,\n+        from_protocol: bool = False\n+    ) -> None:\n         \"\"\"Based on the reason sent we will handle each case.\n         \"\"\"\n         if self.stream_closed_server:\n@@ -374,11 +377,14 @@ class Stream:\n             self._response['headers'][':status'] = '499'\n             self._fire_response_deferred()\n \n-        elif reason in (StreamCloseReason.RESET, StreamCloseReason.CONNECTION_LOST):\n+        elif reason is StreamCloseReason.RESET:\n             self._deferred_response.errback(ResponseFailed([\n-                error if error else Failure()\n+                Failure(f'Remote peer {self._protocol.metadata[\"ip_address\"]} sent RST_STREAM')\n             ]))\n \n+        elif reason is StreamCloseReason.CONNECTION_LOST:\n+            self._deferred_response.errback(ResponseFailed(errors))\n+\n         elif reason is StreamCloseReason.INACTIVE:\n             self._deferred_response.errback(InactiveStreamClosed(self._request))\n \n@@ -403,7 +409,7 @@ class Stream:\n \n         response = response_cls(\n             url=self._request.url,\n-            status=self._response['headers'][':status'],\n+            status=int(self._response['headers'][':status']),\n             headers=self._response['headers'],\n             body=body,\n             request=self._request,\n\n@@ -46,15 +46,15 @@ DEFAULT_LOGGING = {\n     'version': 1,\n     'disable_existing_loggers': False,\n     'loggers': {\n+        'hpack': {\n+            'level': 'ERROR',\n+        },\n         'scrapy': {\n             'level': 'DEBUG',\n         },\n         'twisted': {\n             'level': 'ERROR',\n         },\n-        'hpack': {\n-            'level': 'ERROR',\n-        },\n     }\n }\n \n\n@@ -474,7 +474,7 @@ class Https2ClientProtocolTestCase(TestCase):\n \n         # Close the connection now to fire all the extra 10 requests errback\n         # with InactiveStreamClosed\n-        self.client.transport.abortConnection()\n+        self.client.transport.loseConnection()\n \n         return DeferredList(d_list, consumeErrors=True, fireOnOneErrback=True)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#79b4dfc53e431bdad31925aaf16831a0dde536ca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 170 | Lines Deleted: 24 | Files Changed: 4 | Hunks: 10 | Methods Changed: 6 | Complexity Δ (Sum/Max): 11/15 | Churn Δ: 194 | Churn Cumulative: 1194 | Contributors (this commit): 21 | Commits (past 90d): 9 | Contributors (cumulative): 31 | DMM Complexity: 0.8181818181818182\n\nDIFF:\n@@ -1,10 +1,10 @@\n import re\n import os\n-import stat\n import string\n from importlib import import_module\n from os.path import join, exists, abspath\n from shutil import ignore_patterns, move, copy2, copystat\n+from stat import S_IWUSR as OWNER_WRITE_PERMISSION\n \n import scrapy\n from scrapy.commands import ScrapyCommand\n@@ -78,30 +78,12 @@ class Command(ScrapyCommand):\n                 self._copytree(srcname, dstname)\n             else:\n                 copy2(srcname, dstname)\n+                current_permissions = os.stat(dstname).st_mode\n+                os.chmod(dstname, current_permissions | OWNER_WRITE_PERMISSION)\n+\n         copystat(src, dst)\n-        self._set_rw_permissions(dst)\n-\n-    def _set_rw_permissions(self, path):\n-        \"\"\"\n-        Sets permissions of a directory tree to +rw and +rwx for folders.\n-        This is necessary if the start template files come without write\n-        permissions.\n-        \"\"\"\n-        mode_rw = (stat.S_IRUSR\n-                   | stat.S_IWUSR\n-                   | stat.S_IRGRP\n-                   | stat.S_IROTH)\n-\n-        mode_x = (stat.S_IXUSR\n-                  | stat.S_IXGRP\n-                  | stat.S_IXOTH)\n-\n-        os.chmod(path, mode_rw | mode_x)\n-        for root, dirs, files in os.walk(path):\n-            for dir in dirs:\n-                os.chmod(join(root, dir), mode_rw | mode_x)\n-            for file in files:\n-                os.chmod(join(root, file), mode_rw)\n+        current_permissions = os.stat(dst).st_mode\n+        os.chmod(dst, current_permissions | OWNER_WRITE_PERMISSION)\n \n     def run(self, args, opts):\n         if len(args) not in (1, 2):\n\n\nBinary files /dev/null and b/tests/sample_data/read_only_templates/project/module/spiders/__init__.py differ\n\n@@ -6,7 +6,9 @@ import subprocess\n import sys\n import tempfile\n from contextlib import contextmanager\n+from itertools import chain\n from os.path import exists, join, abspath\n+from pathlib import Path\n from shutil import rmtree, copytree\n from tempfile import mkdtemp\n from threading import Timer\n@@ -15,6 +17,7 @@ from twisted.trial import unittest\n \n import scrapy\n from scrapy.commands import ScrapyCommand\n+from scrapy.commands.startproject import IGNORE\n from scrapy.settings import Settings\n from scrapy.utils.python import to_unicode\n from scrapy.utils.test import get_testenv\n@@ -119,8 +122,34 @@ class StartprojectTest(ProjectTest):\n         self.assertEqual(2, self.call('startproject', self.project_name, project_dir, 'another_params'))\n \n \n+def get_permissions_dict(path, renamings=None, ignore=None):\n+    renamings = renamings or tuple()\n+    permissions_dict = {\n+        '.': os.stat(path).st_mode,\n+    }\n+    for root, dirs, files in os.walk(path):\n+        nodes = list(chain(dirs, files))\n+        if ignore:\n+            ignored_names = ignore(root, nodes)\n+            nodes = [node for node in nodes\n+                        if node not in ignored_names]\n+        for node in nodes:\n+            absolute_path = os.path.join(root, node)\n+            relative_path = os.path.relpath(absolute_path, path)\n+            for search_string, replacement in renamings:\n+                relative_path = relative_path.replace(\n+                    search_string,\n+                    replacement\n+                )\n+            permissions = os.stat(absolute_path).st_mode\n+            permissions_dict[relative_path] = permissions\n+    return permissions_dict\n+\n+\n class StartprojectTemplatesTest(ProjectTest):\n \n+    maxDiff = None\n+\n     def setUp(self):\n         super(StartprojectTemplatesTest, self).setUp()\n         self.tmpl = join(self.temp_path, 'templates')\n@@ -139,6 +168,141 @@ class StartprojectTemplatesTest(ProjectTest):\n         self.assertIn(self.tmpl_proj, out)\n         assert exists(join(self.proj_path, 'root_template'))\n \n+    def test_startproject_permissions_from_writable(self):\n+        \"\"\"Check that generated files have the right permissions when the\n+        template folder has the same permissions as in the project, i.e.\n+        everything is writable.\"\"\"\n+        scrapy_path = scrapy.__path__[0]\n+        templates_dir = os.path.join(scrapy_path, 'templates', 'project')\n+        project_name = 'startproject1'\n+        renamings = (\n+            ('module', project_name),\n+            ('.tmpl', ''),\n+        )\n+        expected_permissions = get_permissions_dict(\n+            templates_dir,\n+            renamings,\n+            IGNORE,\n+        )\n+\n+        destination = mkdtemp()\n+        process = subprocess.Popen(\n+            (\n+                sys.executable,\n+                '-m',\n+                'scrapy.cmdline',\n+                'startproject',\n+                project_name,\n+            ),\n+            cwd=destination,\n+            env=self.env,\n+        )\n+        process.wait()\n+\n+        project_dir = os.path.join(destination, project_name)\n+        actual_permissions = get_permissions_dict(project_dir)\n+\n+        self.assertEqual(actual_permissions, expected_permissions)\n+\n+    def test_startproject_permissions_from_read_only(self):\n+        \"\"\"Check that generated files have the right permissions when the\n+        template folder has been made read-only, which is something that some\n+        systems do.\n+\n+        See https://github.com/scrapy/scrapy/pull/4604\n+        \"\"\"\n+        scrapy_path = scrapy.__path__[0]\n+        templates_dir = os.path.join(scrapy_path, 'templates', 'project')\n+        project_name = 'startproject2'\n+        renamings = (\n+            ('module', project_name),\n+            ('.tmpl', ''),\n+        )\n+        expected_permissions = get_permissions_dict(\n+            templates_dir,\n+            renamings,\n+            IGNORE,\n+        )\n+\n+        tests_path = os.path.dirname(__file__)\n+        read_only_templates_dir = os.path.join(\n+            tests_path, 'sample_data', 'read_only_templates'\n+        )\n+        destination = mkdtemp()\n+        process = subprocess.Popen(\n+            (\n+                sys.executable,\n+                '-m',\n+                'scrapy.cmdline',\n+                'startproject',\n+                project_name,\n+                '--set',\n+                'TEMPLATES_DIR={}'.format(read_only_templates_dir),\n+            ),\n+            cwd=destination,\n+            env=self.env,\n+        )\n+        process.wait()\n+\n+        project_dir = os.path.join(destination, project_name)\n+        actual_permissions = get_permissions_dict(project_dir)\n+\n+        self.assertEqual(actual_permissions, expected_permissions)\n+\n+    def test_startproject_permissions_unchanged_in_destination(self):\n+        \"\"\"Check that pre-existing folders and files in the destination folder\n+        do not see their permissions modified.\"\"\"\n+        scrapy_path = scrapy.__path__[0]\n+        templates_dir = os.path.join(scrapy_path, 'templates', 'project')\n+        project_name = 'startproject3'\n+        renamings = (\n+            ('module', project_name),\n+            ('.tmpl', ''),\n+        )\n+        expected_permissions = get_permissions_dict(\n+            templates_dir,\n+            renamings,\n+            IGNORE,\n+        )\n+\n+        destination = mkdtemp()\n+        project_dir = os.path.join(destination, project_name)\n+\n+        existing_nodes = {\n+            oct(permissions)[2:] + extension: permissions\n+            for extension in ('', '.d')\n+            for permissions in (\n+                0o444, 0o555, 0o644, 0o666, 0o755, 0o777,\n+            )\n+        }\n+        os.mkdir(project_dir)\n+        project_dir_path = Path(project_dir)\n+        for node, permissions in existing_nodes.items():\n+            path = project_dir_path / node\n+            if node.endswith('.d'):\n+                path.mkdir(mode=permissions)\n+            else:\n+                path.touch(mode=permissions)\n+            expected_permissions[node] = path.stat().st_mode\n+\n+        process = subprocess.Popen(\n+            (\n+                sys.executable,\n+                '-m',\n+                'scrapy.cmdline',\n+                'startproject',\n+                project_name,\n+                '.',\n+            ),\n+            cwd=project_dir,\n+            env=self.env,\n+        )\n+        process.wait()\n+\n+        actual_permissions = get_permissions_dict(project_dir)\n+\n+        self.assertEqual(actual_permissions, expected_permissions)\n+\n \n class CommandTest(ProjectTest):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a3afff4a0e1b25903d1de5c5501846d1f1288d82", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 898 | Contributors (this commit): 15 | Commits (past 90d): 3 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -131,8 +131,7 @@ def get_permissions_dict(path, renamings=None, ignore=None):\n         nodes = list(chain(dirs, files))\n         if ignore:\n             ignored_names = ignore(root, nodes)\n-            nodes = [node for node in nodes\n-                        if node not in ignored_names]\n+            nodes = [node for node in nodes if node not in ignored_names]\n         for node in nodes:\n             absolute_path = os.path.join(root, node)\n             relative_path = os.path.relpath(absolute_path, path)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e1450799ce2dfa32e248cb0b4069668ee5e6f4ff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 900 | Contributors (this commit): 15 | Commits (past 90d): 4 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -147,8 +147,6 @@ def get_permissions_dict(path, renamings=None, ignore=None):\n \n class StartprojectTemplatesTest(ProjectTest):\n \n-    maxDiff = None\n-\n     def setUp(self):\n         super(StartprojectTemplatesTest, self).setUp()\n         self.tmpl = join(self.temp_path, 'templates')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ca77ca1f751a614b0c9394d1e8c6d0b9cfcdd957", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 16 | Files Changed: 4 | Hunks: 12 | Methods Changed: 6 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 44 | Churn Cumulative: 1243 | Contributors (this commit): 21 | Commits (past 90d): 15 | Contributors (cumulative): 31 | DMM Complexity: 1.0\n\nDIFF:\n@@ -20,7 +20,12 @@ TEMPLATES_TO_RENDER = (\n     ('${project_name}', 'middlewares.py.tmpl'),\n )\n \n-IGNORE = ignore_patterns('*.pyc', '.svn')\n+IGNORE = ignore_patterns('*.pyc', '__pycache__', '.svn')\n+\n+\n+def _make_writable(path):\n+    current_permissions = os.stat(path).st_mode\n+    os.chmod(path, current_permissions | OWNER_WRITE_PERMISSION)\n \n \n class Command(ScrapyCommand):\n@@ -78,12 +83,10 @@ class Command(ScrapyCommand):\n                 self._copytree(srcname, dstname)\n             else:\n                 copy2(srcname, dstname)\n-                current_permissions = os.stat(dstname).st_mode\n-                os.chmod(dstname, current_permissions | OWNER_WRITE_PERMISSION)\n+                _make_writable(dstname)\n \n         copystat(src, dst)\n-        current_permissions = os.stat(dst).st_mode\n-        os.chmod(dst, current_permissions | OWNER_WRITE_PERMISSION)\n+        _make_writable(dst)\n \n     def run(self, args, opts):\n         if len(args) not in (1, 2):\n\n\nBinary files a/tests/sample_data/read_only_templates/project/module/spiders/__init__.py and /dev/null differ\n\n@@ -2,6 +2,7 @@ import inspect\n import json\n import optparse\n import os\n+from stat import S_IWRITE as ANYONE_WRITE_PERMISSION\n import subprocess\n import sys\n import tempfile\n@@ -17,7 +18,7 @@ from twisted.trial import unittest\n \n import scrapy\n from scrapy.commands import ScrapyCommand\n-from scrapy.commands.startproject import IGNORE\n+from scrapy.commands.startproject import IGNORE, _make_writable\n from scrapy.settings import Settings\n from scrapy.utils.python import to_unicode\n from scrapy.utils.test import get_testenv\n@@ -170,14 +171,14 @@ class StartprojectTemplatesTest(ProjectTest):\n         template folder has the same permissions as in the project, i.e.\n         everything is writable.\"\"\"\n         scrapy_path = scrapy.__path__[0]\n-        templates_dir = os.path.join(scrapy_path, 'templates', 'project')\n+        project_template = os.path.join(scrapy_path, 'templates', 'project')\n         project_name = 'startproject1'\n         renamings = (\n             ('module', project_name),\n             ('.tmpl', ''),\n         )\n         expected_permissions = get_permissions_dict(\n-            templates_dir,\n+            project_template,\n             renamings,\n             IGNORE,\n         )\n@@ -209,22 +210,30 @@ class StartprojectTemplatesTest(ProjectTest):\n         See https://github.com/scrapy/scrapy/pull/4604\n         \"\"\"\n         scrapy_path = scrapy.__path__[0]\n-        templates_dir = os.path.join(scrapy_path, 'templates', 'project')\n+        templates_dir = os.path.join(scrapy_path, 'templates')\n+        project_template = os.path.join(templates_dir, 'project')\n         project_name = 'startproject2'\n         renamings = (\n             ('module', project_name),\n             ('.tmpl', ''),\n         )\n         expected_permissions = get_permissions_dict(\n-            templates_dir,\n+            project_template,\n             renamings,\n             IGNORE,\n         )\n \n-        tests_path = os.path.dirname(__file__)\n-        read_only_templates_dir = os.path.join(\n-            tests_path, 'sample_data', 'read_only_templates'\n-        )\n+        def _make_read_only(path):\n+            current_permissions = os.stat(path).st_mode\n+            os.chmod(path, current_permissions & ~ANYONE_WRITE_PERMISSION)\n+\n+        read_only_templates_dir = str(Path(mkdtemp()) / 'templates')\n+        copytree(templates_dir, read_only_templates_dir)\n+\n+        for root, dirs, files in os.walk(read_only_templates_dir):\n+            for node in chain(dirs, files):\n+                _make_read_only(os.path.join(root, node))\n+\n         destination = mkdtemp()\n         process = subprocess.Popen(\n             (\n@@ -250,14 +259,14 @@ class StartprojectTemplatesTest(ProjectTest):\n         \"\"\"Check that pre-existing folders and files in the destination folder\n         do not see their permissions modified.\"\"\"\n         scrapy_path = scrapy.__path__[0]\n-        templates_dir = os.path.join(scrapy_path, 'templates', 'project')\n+        project_template = os.path.join(scrapy_path, 'templates', 'project')\n         project_name = 'startproject3'\n         renamings = (\n             ('module', project_name),\n             ('.tmpl', ''),\n         )\n         expected_permissions = get_permissions_dict(\n-            templates_dir,\n+            project_template,\n             renamings,\n             IGNORE,\n         )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7e386157033e69bccc56e3a34d5545daa0174d44", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 933 | Contributors (this commit): 15 | Commits (past 90d): 6 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -18,7 +18,7 @@ from twisted.trial import unittest\n \n import scrapy\n from scrapy.commands import ScrapyCommand\n-from scrapy.commands.startproject import IGNORE, _make_writable\n+from scrapy.commands.startproject import IGNORE\n from scrapy.settings import Settings\n from scrapy.utils.python import to_unicode\n from scrapy.utils.test import get_testenv\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3e98ed24b6e9189d7fc7b4209d24971068274ddb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 106 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -14,20 +14,20 @@ class CheckCommandTest(CommandTest):\n \n     def _write_contract(self, contracts, parse_def):\n         with open(self.spider, 'w') as file:\n-            file.write(f\"\"\"\n+            file.write(\"\"\"\n import scrapy\n \n class CheckSpider(scrapy.Spider):\n-    name = '{self.spider_name}'\n+    name = '{0}'\n     start_urls = ['http://example.com']\n \n     def parse(self, response, **cb_kwargs):\n         \\\"\\\"\\\"\n         @url http://www.amazon.com/s?field-keywords=selfish+gene\n-        {contracts}\n+        {1}\n         \\\"\\\"\\\"\n-        {parse_def}\n-            \"\"\")\n+        {2}\n+            \"\"\".format(self.spider_name, contracts, parse_def))\n \n     def _test_contract(self, contracts='', parse_def='pass'):\n         self._write_contract(contracts, parse_def)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2ea7d82534cafe5b25b28ef5a78e5b714767d27a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 55 | Lines Deleted: 30 | Files Changed: 4 | Hunks: 25 | Methods Changed: 13 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 85 | Churn Cumulative: 3218 | Contributors (this commit): 2 | Commits (past 90d): 59 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,29 +2,38 @@ import ipaddress\n import itertools\n import logging\n from collections import deque\n-from typing import Dict, Optional\n+from typing import Dict, List, Optional\n \n from h2.config import H2Configuration\n from h2.connection import H2Connection\n from h2.events import (\n-    DataReceived, ResponseReceived, SettingsAcknowledged,\n+    Event, DataReceived, ResponseReceived, SettingsAcknowledged,\n     StreamEnded, StreamReset, WindowUpdated\n )\n from h2.exceptions import ProtocolError\n from twisted.internet.defer import Deferred\n-from twisted.internet.protocol import connectionDone, Protocol\n+from twisted.internet.protocol import connectionDone, Factory, Protocol\n from twisted.internet.ssl import Certificate\n from twisted.python.failure import Failure\n+from twisted.web.client import URI\n \n from scrapy.core.http2.stream import Stream, StreamCloseReason\n from scrapy.core.http2.types import H2ConnectionMetadataDict\n from scrapy.http import Request\n+from scrapy.settings import Settings\n \n logger = logging.getLogger(__name__)\n \n \n class H2ClientProtocol(Protocol):\n-    def __init__(self) -> None:\n+    def __init__(self, uri: URI, settings: Settings) -> None:\n+        \"\"\"\n+        Arguments:\n+            uri -- URI of the base url to which HTTP/2 Connection will be made.\n+                uri is used to verify that incoming client requests have correct\n+                base URL.\n+            settings -- Scrapy project settings\n+        \"\"\"\n         config = H2Configuration(client_side=True, header_encoding='utf-8')\n         self.conn = H2Connection(config=config)\n \n@@ -53,8 +62,9 @@ class H2ClientProtocol(Protocol):\n         self.metadata: H2ConnectionMetadataDict = {\n             'certificate': None,\n             'ip_address': None,\n-            'hostname': None,\n-            'port': None,\n+            'uri': uri,\n+            'default_download_maxsize': settings.getint('DOWNLOAD_MAXSIZE'),\n+            'default_download_warnsize': settings.getint('DOWNLOAD_WARNSIZE'),\n         }\n \n     @property\n@@ -135,8 +145,6 @@ class H2ClientProtocol(Protocol):\n         destination = self.transport.getPeer()\n         logger.debug('Connection made to {}'.format(destination))\n         self.metadata['ip_address'] = ipaddress.ip_address(destination.host)\n-        self.metadata['port'] = destination.port\n-        self.metadata['hostname'] = self.transport.transport.addr[0]\n \n         self.conn.initiate_connection()\n         self._write_to_transport()\n@@ -179,7 +187,7 @@ class H2ClientProtocol(Protocol):\n         self._pending_request_stream_pool.clear()\n         self.conn.close_connection()\n \n-    def _handle_events(self, events: list) -> None:\n+    def _handle_events(self, events: List[Event]) -> None:\n         \"\"\"Private method which acts as a bridge between the events\n         received from the HTTP/2 data and IH2EventsHandler\n \n@@ -232,3 +240,12 @@ class H2ClientProtocol(Protocol):\n             # Send leftover data for all the streams\n             for stream in self.streams.values():\n                 stream.receive_window_update()\n+\n+\n+class H2ClientFactory(Factory):\n+    def __init__(self, uri: URI, settings: Settings) -> None:\n+        self.uri = uri\n+        self.settings = settings\n+\n+    def buildProtocol(self, addr) -> H2ClientProtocol:\n+        return H2ClientProtocol(self.uri, self.settings)\n\n@@ -16,6 +16,7 @@ from scrapy.core.http2.types import H2ResponseDict\n from scrapy.http import Request\n from scrapy.http.headers import Headers\n from scrapy.responsetypes import responsetypes\n+from scrapy.utils.python import to_unicode\n \n if TYPE_CHECKING:\n     from scrapy.core.http2.protocol import H2ClientProtocol\n@@ -34,7 +35,7 @@ class InactiveStreamClosed(ConnectionClosed):\n \n class InvalidHostname(Exception):\n \n-    def __init__(self, request: Request, expected_hostname: Optional[str], expected_netloc: Optional[str]) -> None:\n+    def __init__(self, request: Request, expected_hostname: str, expected_netloc: str) -> None:\n         self.request = request\n         self.expected_hostname = expected_hostname\n         self.expected_netloc = expected_netloc\n@@ -83,10 +84,7 @@ class Stream:\n         self,\n         stream_id: int,\n         request: Request,\n-        protocol: \"H2ClientProtocol\",\n-        download_maxsize: int = 0,\n-        download_warnsize: int = 0,\n-        fail_on_data_loss: bool = True\n+        protocol: \"H2ClientProtocol\"\n     ) -> None:\n         \"\"\"\n         Arguments:\n@@ -98,9 +96,14 @@ class Stream:\n         self._request: Request = request\n         self._protocol: \"H2ClientProtocol\" = protocol\n \n-        self._download_maxsize = self._request.meta.get('download_maxsize', download_maxsize)\n-        self._download_warnsize = self._request.meta.get('download_warnsize', download_warnsize)\n-        self._fail_on_dataloss = self._request.meta.get('download_fail_on_dataloss', fail_on_data_loss)\n+        self._download_maxsize = self._request.meta.get(\n+            'download_maxsize',\n+            self._protocol.metadata['default_download_maxsize']\n+        )\n+        self._download_warnsize = self._request.meta.get(\n+            'download_warnsize',\n+            self._protocol.metadata['default_download_warnsize']\n+        )\n \n         self.request_start_time = None\n \n@@ -174,9 +177,9 @@ class Stream:\n         # Make sure that we are sending the request to the correct URL\n         url = urlparse(self._request.url)\n         return (\n-            url.netloc == self._protocol.metadata['hostname']\n-            or url.netloc == f'{self._protocol.metadata[\"hostname\"]}:{self._protocol.metadata[\"port\"]}'\n-            or url.netloc == f'{self._protocol.metadata[\"ip_address\"]}:{self._protocol.metadata[\"port\"]}'\n+            url.netloc == to_unicode(self._protocol.metadata['uri'].host)\n+            or url.netloc == to_unicode(self._protocol.metadata['uri'].netloc)\n+            or url.netloc == f'{self._protocol.metadata[\"ip_address\"]}:{self._protocol.metadata[\"uri\"].port}'\n         )\n \n     def _get_request_headers(self) -> List[Tuple[str, str]]:\n@@ -391,8 +394,8 @@ class Stream:\n         elif reason is StreamCloseReason.INVALID_HOSTNAME:\n             self._deferred_response.errback(InvalidHostname(\n                 self._request,\n-                self._protocol.metadata['hostname'],\n-                f'{self._protocol.metadata[\"ip_address\"]}:{self._protocol.metadata[\"port\"]}'\n+                to_unicode(self._protocol.metadata['uri'].host),\n+                f'{self._protocol.metadata[\"ip_address\"]}:{self._protocol.metadata[\"uri\"].port}'\n             ))\n \n     def _fire_response_deferred(self, flags: Optional[List[str]] = None) -> None:\n\n@@ -3,6 +3,7 @@ from ipaddress import IPv4Address, IPv6Address\n from typing import Union, Optional\n \n from twisted.internet.ssl import Certificate\n+from twisted.web.client import URI\n # for python < 3.8 -- typing.TypedDict is undefined\n from typing_extensions import TypedDict\n \n@@ -19,14 +20,16 @@ class H2ConnectionMetadataDict(TypedDict):\n     # is updated when HTTP/2 connection is  made successfully\n     ip_address: Optional[Union[IPv4Address, IPv6Address]]\n \n-    # Name of the peer HTTP/2 connection is established\n-    hostname: Optional[str]\n+    # URI of the peer HTTP/2 connection is made\n+    uri: URI\n \n-    port: Optional[int]\n-\n-    # Both ip_address and hostname are used by the Stream before\n+    # Both ip_address and uri are used by the Stream before\n     # initiating the request to verify that the base address\n \n+    # Variables taken from Project Settings\n+    default_download_maxsize: int\n+    default_download_warnsize: int\n+\n \n class H2ResponseDict(TypedDict):\n     # Data received frame by frame from the server is appended\n\n@@ -11,17 +11,18 @@ from h2.exceptions import InvalidBodyLengthError\n from twisted.internet import reactor\n from twisted.internet.defer import inlineCallbacks, DeferredList, CancelledError\n from twisted.internet.endpoints import SSL4ClientEndpoint, SSL4ServerEndpoint\n-from twisted.internet.protocol import Factory\n from twisted.internet.ssl import optionsForClientTLS, PrivateCertificate, Certificate\n from twisted.python.failure import Failure\n from twisted.trial.unittest import TestCase\n+from twisted.web.client import URI\n from twisted.web.http import Request as TxRequest\n from twisted.web.server import Site, NOT_DONE_YET\n from twisted.web.static import File\n \n-from scrapy.core.http2.protocol import H2ClientProtocol\n+from scrapy.core.http2.protocol import H2ClientFactory\n from scrapy.core.http2.stream import InactiveStreamClosed, InvalidHostname\n from scrapy.http import Request, Response, JsonRequest\n+from scrapy.settings import Settings\n from scrapy.utils.python import to_bytes, to_unicode\n from tests.mockserver import ssl_context_factory, LeafResource, Status\n \n@@ -183,7 +184,8 @@ class Https2ClientProtocolTestCase(TestCase):\n             trustRoot=self.client_certificate,\n             acceptableProtocols=[b'h2']\n         )\n-        h2_client_factory = Factory.forProtocol(H2ClientProtocol)\n+        uri = URI.fromBytes(to_bytes(self.get_url('/')))\n+        h2_client_factory = H2ClientFactory(uri, Settings())\n         client_endpoint = SSL4ClientEndpoint(reactor, self.hostname, self.port_number, client_options)\n         self.client = yield client_endpoint.connect(h2_client_factory)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#75bff7b6d33bdc74c1a8eb0e43e4b484473c3062", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 109 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -23,7 +23,7 @@ class CheckSpider(scrapy.Spider):\n \n     def parse(self, response, **cb_kwargs):\n         \\\"\\\"\\\"\n-        @url http://www.amazon.com/s?field-keywords=selfish+gene\n+        @url http://example.com\n         {1}\n         \\\"\\\"\\\"\n         {2}\n@@ -32,6 +32,7 @@ class CheckSpider(scrapy.Spider):\n     def _test_contract(self, contracts='', parse_def='pass'):\n         self._write_contract(contracts, parse_def)\n         p, out, err = self.proc('check')\n+        self.assertNotIn('F', out)\n         self.assertIn('OK', err)\n         self.assertEqual(p.returncode, 0)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9e99be982a2916559219693e6665ec7e9319f0e9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 87 | Lines Deleted: 59 | Files Changed: 20 | Hunks: 28 | Methods Changed: 23 | Complexity Δ (Sum/Max): 2/3 | Churn Δ: 146 | Churn Cumulative: 8135 | Contributors (this commit): 81 | Commits (past 90d): 54 | Contributors (cumulative): 222 | DMM Complexity: 0.7142857142857143\n\nDIFF:\n@@ -19,10 +19,12 @@ def _iter_command_classes(module_name):\n     # scrapy.utils.spider.iter_spider_classes\n     for module in walk_modules(module_name):\n         for obj in vars(module).values():\n-            if inspect.isclass(obj) and \\\n-                    issubclass(obj, ScrapyCommand) and \\\n-                    obj.__module__ == module.__name__ and \\\n-                    not obj == ScrapyCommand:\n+            if (\n+                inspect.isclass(obj)\n+                and issubclass(obj, ScrapyCommand)\n+                and obj.__module__ == module.__name__\n+                and not obj == ScrapyCommand\n+            ):\n                 yield obj\n \n \n\n@@ -26,6 +26,8 @@ class Command(BaseRunSpiderCommand):\n         else:\n             self.crawler_process.start()\n \n-            if self.crawler_process.bootstrap_failed or \\\n-                    (hasattr(self.crawler_process, 'has_exception') and self.crawler_process.has_exception):\n+            if (\n+                self.crawler_process.bootstrap_failed\n+                or hasattr(self.crawler_process, 'has_exception') and self.crawler_process.has_exception\n+            ):\n                 self.exitcode = 1\n\n@@ -19,8 +19,10 @@ class Command(ScrapyCommand):\n         return \"Fetch a URL using the Scrapy downloader\"\n \n     def long_desc(self):\n-        return \"Fetch a URL using the Scrapy downloader and print its content \" \\\n+        return (\n+            \"Fetch a URL using the Scrapy downloader and print its content\"\n             \" to stdout. You may want to use --nolog to disable logging\"\n+        )\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n\n@@ -121,6 +121,7 @@ class Command(ScrapyCommand):\n \n     @property\n     def templates_dir(self):\n-        _templates_base_dir = self.settings['TEMPLATES_DIR'] or \\\n-            join(scrapy.__path__[0], 'templates')\n-        return join(_templates_base_dir, 'spiders')\n+        return join(\n+            self.settings['TEMPLATES_DIR'] or join(scrapy.__path__[0], 'templates'),\n+            'spiders'\n+        )\n\n@@ -137,6 +137,7 @@ class Command(ScrapyCommand):\n \n     @property\n     def templates_dir(self):\n-        _templates_base_dir = self.settings['TEMPLATES_DIR'] or \\\n-            join(scrapy.__path__[0], 'templates')\n-        return join(_templates_base_dir, 'project')\n+        return join(\n+            self.settings['TEMPLATES_DIR'] or join(scrapy.__path__[0], 'templates'),\n+            'project'\n+        )\n\n@@ -8,8 +8,7 @@ class Command(fetch.Command):\n         return \"Open URL in browser, as seen by Scrapy\"\n \n     def long_desc(self):\n-        return \"Fetch a URL using the Scrapy downloader and show its \" \\\n-            \"contents in a browser\"\n+        return \"Fetch a URL using the Scrapy downloader and show its contents in a browser\"\n \n     def add_options(self, parser):\n         super(Command, self).add_options(parser)\n\n@@ -141,10 +141,12 @@ class ExecutionEngine:\n \n     def _needs_backout(self, spider):\n         slot = self.slot\n-        return not self.running \\\n-            or slot.closing \\\n-            or self.downloader.needs_backout() \\\n+        return (\n+            not self.running\n+            or slot.closing\n+            or self.downloader.needs_backout()\n             or self.scraper.slot.needs_backout()\n+        )\n \n     def _next_request_from_scheduler(self, spider):\n         slot = self.slot\n\n@@ -33,10 +33,8 @@ class BaseRedirectMiddleware:\n         if ttl and redirects <= self.max_redirect_times:\n             redirected.meta['redirect_times'] = redirects\n             redirected.meta['redirect_ttl'] = ttl - 1\n-            redirected.meta['redirect_urls'] = request.meta.get('redirect_urls', []) + \\\n-                [request.url]\n-            redirected.meta['redirect_reasons'] = request.meta.get('redirect_reasons', []) + \\\n-                [reason]\n+            redirected.meta['redirect_urls'] = request.meta.get('redirect_urls', []) + [request.url]\n+            redirected.meta['redirect_reasons'] = request.meta.get('redirect_reasons', []) + [reason]\n             redirected.dont_filter = request.dont_filter\n             redirected.priority = request.priority + self.priority_adjust\n             logger.debug(\"Redirecting (%(reason)s) to %(redirected)s from %(request)s\",\n@@ -99,8 +97,11 @@ class MetaRefreshMiddleware(BaseRedirectMiddleware):\n         self._maxdelay = settings.getint('METAREFRESH_MAXDELAY')\n \n     def process_response(self, request, response, spider):\n-        if request.meta.get('dont_redirect', False) or request.method == 'HEAD' or \\\n-                not isinstance(response, HtmlResponse):\n+        if (\n+            request.meta.get('dont_redirect', False)\n+            or request.method == 'HEAD'\n+            or not isinstance(response, HtmlResponse)\n+        ):\n             return response\n \n         interval, url = get_meta_refresh(response,\n\n@@ -60,8 +60,10 @@ class RetryMiddleware:\n         return response\n \n     def process_exception(self, request, exception, spider):\n-        if isinstance(exception, self.EXCEPTIONS_TO_RETRY) \\\n-                and not request.meta.get('dont_retry', False):\n+        if (\n+            isinstance(exception, self.EXCEPTIONS_TO_RETRY)\n+            and not request.meta.get('dont_retry', False)\n+        ):\n             return self._retry(request, exception, spider)\n \n     def _retry(self, request, reason, spider):\n\n@@ -81,8 +81,10 @@ class MemoryUsage:\n             logger.error(\"Memory usage exceeded %(memusage)dM. Shutting down Scrapy...\",\n                          {'memusage': mem}, extra={'crawler': self.crawler})\n             if self.notify_mails:\n-                subj = \"%s terminated: memory usage exceeded %dM at %s\" % \\\n-                        (self.crawler.settings['BOT_NAME'], mem, socket.gethostname())\n+                subj = (\n+                    \"%s terminated: memory usage exceeded %dM at %s\"\n+                    % (self.crawler.settings['BOT_NAME'], mem, socket.gethostname())\n+                )\n                 self._send_report(self.notify_mails, subj)\n                 self.crawler.stats.set_value('memusage/limit_notified', 1)\n \n@@ -102,8 +104,10 @@ class MemoryUsage:\n             logger.warning(\"Memory usage reached %(memusage)dM\",\n                            {'memusage': mem}, extra={'crawler': self.crawler})\n             if self.notify_mails:\n-                subj = \"%s warning: memory usage reached %dM at %s\" % \\\n-                        (self.crawler.settings['BOT_NAME'], mem, socket.gethostname())\n+                subj = (\n+                    \"%s warning: memory usage reached %dM at %s\"\n+                    % (self.crawler.settings['BOT_NAME'], mem, socket.gethostname())\n+                )\n                 self._send_report(self.notify_mails, subj)\n                 self.crawler.stats.set_value('memusage/warning_notified', 1)\n             self.warned = True\n\n@@ -205,8 +205,7 @@ def _get_clickable(clickdata, form):\n \n     # We didn't find it, so now we build an XPath expression out of the other\n     # arguments, because they can be used as such\n-    xpath = u'.//*' + \\\n-            u''.join(u'[@%s=\"%s\"]' % c for c in clickdata.items())\n+    xpath = u'.//*' + u''.join(u'[@%s=\"%s\"]' % c for c in clickdata.items())\n     el = form.xpath(xpath)\n     if len(el) == 1:\n         return (el[0].get('name'), el[0].get('value') or '')\n\n@@ -62,8 +62,11 @@ class TextResponse(Response):\n         return self._declared_encoding() or self._body_inferred_encoding()\n \n     def _declared_encoding(self):\n-        return self._encoding or self._headers_encoding() \\\n+        return (\n+            self._encoding\n+            or self._headers_encoding()\n             or self._body_declared_encoding()\n+        )\n \n     def body_as_unicode(self):\n         \"\"\"Return body as unicode\"\"\"\n\n@@ -21,12 +21,18 @@ class Link:\n         self.nofollow = nofollow\n \n     def __eq__(self, other):\n-        return self.url == other.url and self.text == other.text and \\\n-            self.fragment == other.fragment and self.nofollow == other.nofollow\n+        return (\n+            self.url == other.url\n+            and self.text == other.text\n+            and self.fragment == other.fragment\n+            and self.nofollow == other.nofollow\n+        )\n \n     def __hash__(self):\n         return hash(self.url) ^ hash(self.text) ^ hash(self.fragment) ^ hash(self.nofollow)\n \n     def __repr__(self):\n-        return 'Link(url=%r, text=%r, fragment=%r, nofollow=%r)' % \\\n-            (self.url, self.text, self.fragment, self.nofollow)\n+        return (\n+            'Link(url=%r, text=%r, fragment=%r, nofollow=%r)'\n+            % (self.url, self.text, self.fragment, self.nofollow)\n+        )\n\n@@ -52,8 +52,7 @@ class SettingsAttribute:\n             self.priority = priority\n \n     def __str__(self):\n-        return \"<SettingsAttribute value={self.value!r} \" \\\n-               \"priority={self.priority}>\".format(self=self)\n+        return \"<SettingsAttribute value={self.value!r} priority={self.priority}>\".format(self=self)\n \n     __repr__ = __str__\n \n\n@@ -28,8 +28,7 @@ class Root(Resource):\n \n \n def _getarg(request, name, default=None, type=str):\n-    return type(request.args[name][0]) \\\n-        if name in request.args else default\n+    return type(request.args[name][0]) if name in request.args else default\n \n \n if __name__ == '__main__':\n\n@@ -101,11 +101,13 @@ def get_config(use_closest=True):\n \n \n def get_sources(use_closest=True):\n-    xdg_config_home = os.environ.get('XDG_CONFIG_HOME') or \\\n-        os.path.expanduser('~/.config')\n-    sources = ['/etc/scrapy.cfg', r'c:\\scrapy\\scrapy.cfg',\n+    xdg_config_home = os.environ.get('XDG_CONFIG_HOME') or os.path.expanduser('~/.config')\n+    sources = [\n+        '/etc/scrapy.cfg',\n+        r'c:\\scrapy\\scrapy.cfg',\n         xdg_config_home + '/scrapy.cfg',\n-               os.path.expanduser('~/.scrapy.cfg')]\n+        os.path.expanduser('~/.scrapy.cfg'),\n+    ]\n     if use_closest:\n         sources.append(closest_scrapy_cfg())\n     return sources\n\n@@ -9,8 +9,7 @@ from w3lib.http import basic_auth_header\n \n class CurlParser(argparse.ArgumentParser):\n     def error(self, message):\n-        error_msg = \\\n-            'There was an error parsing the curl command: {}'.format(message)\n+        error_msg = 'There was an error parsing the curl command: {}'.format(message)\n         raise ValueError(error_msg)\n \n \n\n@@ -18,8 +18,7 @@ def install_shutdown_handlers(function, override_sigint=True):\n     from twisted.internet import reactor\n     reactor._handleSignals()\n     signal.signal(signal.SIGTERM, function)\n-    if signal.getsignal(signal.SIGINT) == signal.default_int_handler or \\\n-            override_sigint:\n+    if signal.getsignal(signal.SIGINT) == signal.default_int_handler or override_sigint:\n         signal.signal(signal.SIGINT, function)\n     # Catch Ctrl-Break in windows\n     if hasattr(signal, 'SIGBREAK'):\n\n@@ -47,13 +47,17 @@ def response_httprepr(response):\n     is provided only for reference, since it's not the exact stream of bytes\n     that was received (that's not exposed by Twisted).\n     \"\"\"\n-    s = b\"HTTP/1.1 \" + to_bytes(str(response.status)) + b\" \" + \\\n-        to_bytes(http.RESPONSES.get(response.status, b'')) + b\"\\r\\n\"\n+    values = [\n+        b\"HTTP/1.1 \",\n+        to_bytes(str(response.status)),\n+        b\" \",\n+        to_bytes(http.RESPONSES.get(response.status, b'')),\n+        b\"\\r\\n\",\n+    ]\n     if response.headers:\n-        s += response.headers.to_string() + b\"\\r\\n\"\n-    s += b\"\\r\\n\"\n-    s += response.body\n-    return s\n+        values.extend([response.headers.to_string(), b\"\\r\\n\"])\n+    values.extend([b\"\\r\\n\", response.body])\n+    return b\"\".join(values)\n \n \n def open_in_browser(response, _openfunc=webbrowser.open):\n\n@@ -34,10 +34,12 @@ def iter_spider_classes(module):\n     from scrapy.spiders import Spider\n \n     for obj in vars(module).values():\n-        if inspect.isclass(obj) and \\\n-           issubclass(obj, Spider) and \\\n-           obj.__module__ == module.__name__ and \\\n-           getattr(obj, 'name', None):\n+        if (\n+            inspect.isclass(obj)\n+            and issubclass(obj, Spider)\n+            and obj.__module__ == module.__name__\n+            and getattr(obj, 'name', None)\n+        ):\n             yield obj\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
