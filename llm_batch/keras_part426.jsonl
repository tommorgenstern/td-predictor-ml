{"custom_id": "keras#3042a922d5ac5d985119c51280f481e0e98cd483", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 86 | Lines Deleted: 278 | Files Changed: 5 | Hunks: 24 | Methods Changed: 19 | Complexity Δ (Sum/Max): -25/17 | Churn Δ: 364 | Churn Cumulative: 2652 | Contributors (this commit): 4 | Commits (past 90d): 17 | Contributors (cumulative): 8 | DMM Complexity: 0.4057971014492754\n\nDIFF:\n@@ -1,261 +0,0 @@\n-# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"DTensor specific Keras initializers.\"\"\"\n-\n-import inspect\n-import sys\n-\n-\n-from keras import backend\n-from keras.dtensor import utils\n-from keras.initializers import initializers_v2\n-from keras.utils import generic_utils\n-from keras.utils import tf_inspect\n-\n-\n-class Initializer(initializers_v2.Initializer):\n-  \"\"\"DTensor specific initializer.\n-\n-  Note that the initializer will take an extra argument in `__call__` , which is\n-  the `layout` for the init value.\n-  \"\"\"\n-\n-  def __call__(self, shape, dtype=None, layout=None, **kwargs):\n-    raise NotImplementedError('Initializer subclasses must implement the '\n-                              '`__call__()` method.')\n-\n-\n-def _ensure_keras_seeded():\n-  \"\"\"Make sure the keras.backend global seed generator is set.\n-\n-  This is important for DTensor use case to ensure that each client are\n-  initialized with same seed for tf.random.Generator, so that the value created\n-  are in sync among all the clients.\n-  \"\"\"\n-  if not getattr(backend._SEED_GENERATOR, 'generator', None):  # pylint:disable=protected-access\n-    raise ValueError('When using DTensor APIs, you need to set the global seed '\n-                     'before using any Keras initializers. Please make sure '\n-                     'to call `tf.keras.utils.set_random_seed()` in your code.')\n-\n-\n-class Zeros(initializers_v2.Zeros):\n-\n-  def __call__(self, shape, dtype=None, **kwargs):\n-    layout = kwargs.pop('layout', None)\n-    fn = super(Zeros, self).__call__\n-    return utils.call_with_layout(fn, layout, shape=shape, dtype=dtype)\n-\n-\n-class Ones(initializers_v2.Ones):\n-\n-  def __call__(self, shape, dtype=None, **kwargs):\n-    layout = kwargs.pop('layout', None)\n-    fn = super(Ones, self).__call__\n-    return utils.call_with_layout(fn, layout, shape=shape, dtype=dtype)\n-\n-\n-class Constant(initializers_v2.Constant):\n-\n-  def __call__(self, shape, dtype=None, **kwargs):\n-    layout = kwargs.pop('layout', None)\n-    fn = super(Constant, self).__call__\n-    return utils.call_with_layout(fn, layout, shape=shape, dtype=dtype)\n-\n-\n-# pylint:disable=missing-class-docstring\n-class RandomUniform(initializers_v2.RandomUniform):\n-\n-  def __init__(self, minval=-0.05, maxval=0.05, seed=None):\n-    super().__init__(minval=minval, maxval=maxval, seed=seed)\n-    # Make sure to use the tf.random.Generator which doesn't rely on the\n-    # stateful random ops.\n-    # TODO(scottzhu): Remove this once the backend.use_generator_for_rng is\n-    # default to True.\n-    self._random_generator._force_generator = True\n-\n-  def __call__(self, shape, dtype=None, **kwargs):\n-    layout = kwargs.pop('layout', None)\n-    if layout:\n-      _ensure_keras_seeded()\n-    fn = super(RandomUniform, self).__call__\n-    return utils.call_with_layout(fn, layout, shape=shape, dtype=dtype)\n-\n-\n-class RandomNormal(initializers_v2.RandomNormal):\n-\n-  def __init__(self, mean=0.0, stddev=0.05, seed=None):\n-    super().__init__(mean=mean, stddev=stddev, seed=seed)\n-    self._random_generator._force_generator = True\n-\n-  def __call__(self, shape, dtype=None, **kwargs):\n-    layout = kwargs.pop('layout', None)\n-    if layout:\n-      _ensure_keras_seeded()\n-    fn = super(RandomNormal, self).__call__\n-    return utils.call_with_layout(fn, layout, shape=shape, dtype=dtype)\n-\n-\n-class TruncatedNormal(initializers_v2.TruncatedNormal):\n-\n-  def __init__(self, mean=0.0, stddev=0.05, seed=None):\n-    super().__init__(mean=mean, stddev=stddev, seed=seed)\n-    self._random_generator._force_generator = True\n-\n-  def __call__(self, shape, dtype=None, **kwargs):\n-    layout = kwargs.pop('layout', None)\n-    if layout:\n-      _ensure_keras_seeded()\n-    fn = super(TruncatedNormal, self).__call__\n-    return utils.call_with_layout(fn, layout, shape=shape, dtype=dtype)\n-\n-\n-class Identity(initializers_v2.Identity):\n-\n-  def __call__(self, shape, dtype=None, **kwargs):\n-    layout = kwargs.pop('layout', None)\n-    fn = super(Identity, self).__call__\n-    return utils.call_with_layout(fn, layout, shape=shape, dtype=dtype)\n-\n-\n-class Orthogonal(initializers_v2.Orthogonal):\n-\n-  def __init__(self, gain=1.0, seed=None):\n-    super().__init__(gain=gain, seed=seed)\n-    self._random_generator._force_generator = True\n-\n-  def __call__(self, shape, dtype=None, **kwargs):\n-    layout = kwargs.pop('layout', None)\n-    if layout:\n-      _ensure_keras_seeded()\n-    fn = super(Orthogonal, self).__call__\n-    return utils.call_with_layout(fn, layout, shape=shape, dtype=dtype)\n-\n-\n-class VarianceScaling(initializers_v2.VarianceScaling):\n-\n-  def __init__(self, scale=1.0, mode='fan_in', distribution='truncated_normal',\n-               seed=None):\n-    super().__init__(scale=scale, mode=mode, distribution=distribution,\n-                     seed=seed)\n-    self._random_generator._force_generator = True\n-\n-  def __call__(self, shape, dtype=None, **kwargs):\n-    layout = kwargs.pop('layout', None)\n-    if layout:\n-      _ensure_keras_seeded()\n-    fn = super(VarianceScaling, self).__call__\n-    return utils.call_with_layout(fn, layout, shape=shape, dtype=dtype)\n-\n-\n-class GlorotUniform(VarianceScaling):\n-\n-  def __init__(self, seed=None):\n-    super().__init__(\n-        scale=1.0, mode='fan_avg', distribution='uniform', seed=seed)\n-\n-  def get_config(self):\n-    return {'seed': self.seed}\n-\n-\n-class GlorotNormal(VarianceScaling):\n-\n-  def __init__(self, seed=None):\n-    super().__init__(\n-        scale=1.0, mode='fan_avg', distribution='truncated_normal', seed=seed)\n-\n-  def get_config(self):\n-    return {'seed': self.seed}\n-\n-\n-class LecunNormal(VarianceScaling):\n-\n-  def __init__(self, seed=None):\n-    super().__init__(\n-        scale=1., mode='fan_in', distribution='truncated_normal', seed=seed)\n-\n-  def get_config(self):\n-    return {'seed': self.seed}\n-\n-\n-class LecunUniform(VarianceScaling):\n-\n-  def __init__(self, seed=None):\n-    super().__init__(\n-        scale=1., mode='fan_in', distribution='uniform', seed=seed)\n-\n-  def get_config(self):\n-    return {'seed': self.seed}\n-\n-\n-class HeNormal(VarianceScaling):\n-\n-  def __init__(self, seed=None):\n-    super().__init__(\n-        scale=2., mode='fan_in', distribution='truncated_normal', seed=seed)\n-\n-  def get_config(self):\n-    return {'seed': self.seed}\n-\n-\n-class HeUniform(VarianceScaling):\n-\n-  def __init__(self, seed=None):\n-    super().__init__(\n-        scale=2., mode='fan_in', distribution='uniform', seed=seed)\n-\n-  def get_config(self):\n-    return {'seed': self.seed}\n-\n-\n-# Populate all initializers with their string names.\n-_ALL_INITIALIZERS = {}\n-for name, obj in inspect.getmembers(sys.modules[__name__]):\n-  if inspect.isclass(obj) and issubclass(obj, initializers_v2.Initializer):\n-    _ALL_INITIALIZERS[name] = obj\n-    alternative_name = generic_utils.to_snake_case(name)\n-    _ALL_INITIALIZERS[alternative_name] = obj\n-\n-\n-def serialize(initializer):\n-  return generic_utils.serialize_keras_object(initializer)\n-\n-\n-def deserialize(config, custom_objects=None):\n-  \"\"\"Return an `Initializer` object from its config.\"\"\"\n-  return generic_utils.deserialize_keras_object(\n-      config,\n-      module_objects=_ALL_INITIALIZERS,\n-      custom_objects=custom_objects,\n-      printable_module_name='initializer')\n-\n-\n-def get(identifier):\n-  \"\"\"Retrieve an initializer by the identifier.\"\"\"\n-  # This function is copied from keras, and we only want to inject the logic for\n-  # `deserialize()`.\n-  if identifier is None:\n-    return None\n-  if isinstance(identifier, dict):\n-    return deserialize(identifier)\n-  elif isinstance(identifier, str):\n-    identifier = str(identifier)\n-    return deserialize(identifier)\n-  elif callable(identifier):\n-    if tf_inspect.isclass(identifier):\n-      identifier = identifier()\n-    return identifier\n-  else:\n-    raise ValueError('Could not interpret initializer identifier: ' +\n-                     str(identifier))\n\n@@ -16,8 +16,8 @@\n \n from absl.testing import parameterized\n from keras import backend\n+from keras import initializers\n from keras.dtensor import dtensor_api as dtensor\n-from keras.dtensor import initializers\n from keras.utils import tf_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n\n@@ -19,7 +19,6 @@ import functools\n from keras import backend\n from keras import layers\n from keras.dtensor import dtensor_api as dtensor\n-from keras.dtensor import initializers\n from keras.engine import base_layer\n from keras.engine import base_layer_utils\n from keras.engine import input_spec\n@@ -116,8 +115,6 @@ class Dense(Layer, layers.Dense):\n                kernel_layout=None,\n                bias_layout=None,\n                **kwargs):\n-    kernel_initializer = initializers.get(kernel_initializer)\n-    bias_initializer = initializers.get(bias_initializer)\n     # Note that the Layer is the first parent class here, since we would like\n     # all its methods to take priority than the methods from Dense\n     # (eg, `add_weight()`). We skip the Layer.__init__ here since it doesn't\n@@ -207,9 +204,6 @@ class Conv(Layer, base_conv.Conv):\n                name=None,\n                conv_op=None,\n                **kwargs):\n-    kernel_initializer = initializers.get(kernel_initializer)\n-    bias_initializer = initializers.get(bias_initializer)\n-\n     base_conv.Conv.__init__(\n         self,\n         rank=rank,\n\n@@ -14,6 +14,7 @@\n # ==============================================================================\n \"\"\"Tests for layers.\"\"\"\n \n+from keras import backend\n from keras.dtensor import dtensor_api as dtensor\n from keras.dtensor import layers\n from keras.utils import tf_utils\n@@ -27,6 +28,7 @@ class LayersTest(test_util.DTensorBaseTest):\n \n   def setUp(self):\n     super(LayersTest, self).setUp()\n+    backend.enable_tf_random_generator()\n     tf_utils.set_random_seed(1337)\n     global_ids = test_util.create_device_ids_array((2, 2))\n     local_device_ids = np.ravel(global_ids).tolist()\n\n@@ -13,16 +13,20 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras initializers for TF 2.\"\"\"\n-# pylint: disable=g-classes-have-attributes\n+# pylint: disable=g-classes-have-attributes, missing-docstring, g-direct-tensorflow-import\n \n import math\n from keras import backend\n+from keras.dtensor import utils\n+\n import tensorflow.compat.v2 as tf\n+\n from tensorflow.python.util.tf_export import keras_export\n \n _PARTITION_SHAPE = 'partition_shape'\n _PARTITION_OFFSET = 'partition_offset'\n-_ALLOWED_INITIALIZER_KWARGS = [_PARTITION_SHAPE, _PARTITION_OFFSET]\n+_LAYOUT = 'layout'\n+_ALLOWED_INITIALIZER_KWARGS = [_PARTITION_SHAPE, _PARTITION_OFFSET, _LAYOUT]\n \n \n @keras_export('keras.initializers.Initializer')\n@@ -142,6 +146,9 @@ class Zeros(Initializer):\n       raise ValueError(f'Expected numeric or boolean dtype, got {dtype}.')\n     if _PARTITION_SHAPE in kwargs:\n       shape = kwargs[_PARTITION_SHAPE]\n+    layout = kwargs.pop('layout', None)\n+    if layout:\n+      return utils.call_with_layout(tf.zeros, layout, shape=shape, dtype=dtype)\n     return tf.zeros(shape, dtype)\n \n \n@@ -179,6 +186,9 @@ class Ones(Initializer):\n       raise ValueError(f'Expected numeric or boolean dtype, got {dtype}.')\n     if _PARTITION_SHAPE in kwargs:\n       shape = kwargs[_PARTITION_SHAPE]\n+    layout = kwargs.pop('layout', None)\n+    if layout:\n+      return utils.call_with_layout(tf.ones, layout, shape=shape, dtype=dtype)\n     return tf.ones(shape, dtype)\n \n \n@@ -222,7 +232,10 @@ class Constant(Initializer):\n        (via `tf.keras.backend.set_floatx(float_dtype)`).\n       **kwargs: Additional keyword arguments.\n     \"\"\"\n-    del kwargs\n+    layout = kwargs.pop('layout', None)\n+    if layout:\n+      return utils.call_with_layout(tf.constant, layout, self.value,\n+                                    shape=shape, dtype=dtype)\n     return tf.constant(\n         self.value, dtype=_get_dtype(dtype), shape=shape)\n \n@@ -285,6 +298,13 @@ class RandomUniform(Initializer):\n       raise ValueError(f'Expected float or integer dtype, got {dtype}.')\n     if _PARTITION_SHAPE in kwargs:\n       shape = kwargs[_PARTITION_SHAPE]\n+    layout = kwargs.pop('layout', None)\n+    if layout:\n+      self._random_generator._force_generator = True\n+      _ensure_keras_seeded()\n+      return utils.call_with_layout(\n+          self._random_generator.random_uniform, layout, shape, self.minval,\n+          self.maxval, dtype)\n     return self._random_generator.random_uniform(shape, self.minval,\n                                                  self.maxval, dtype)\n \n@@ -348,6 +368,13 @@ class RandomNormal(Initializer):\n     dtype = _assert_float_dtype(_get_dtype(dtype))\n     if _PARTITION_SHAPE in kwargs:\n       shape = kwargs[_PARTITION_SHAPE]\n+    layout = kwargs.pop('layout', None)\n+    if layout:\n+      self._random_generator._force_generator = True\n+      _ensure_keras_seeded()\n+      return utils.call_with_layout(\n+          self._random_generator.random_normal, layout, shape, self.mean,\n+          self.stddev, dtype)\n     return self._random_generator.random_normal(shape, self.mean, self.stddev,\n                                                 dtype)\n \n@@ -416,6 +443,13 @@ class TruncatedNormal(Initializer):\n     dtype = _assert_float_dtype(_get_dtype(dtype))\n     if _PARTITION_SHAPE in kwargs:\n       shape = kwargs[_PARTITION_SHAPE]\n+    layout = kwargs.pop('layout', None)\n+    if layout:\n+      self._random_generator._force_generator = True\n+      _ensure_keras_seeded()\n+      return utils.call_with_layout(\n+          self._random_generator.truncated_normal, layout, shape, self.mean,\n+          self.stddev, dtype)\n     return self._random_generator.truncated_normal(shape, self.mean,\n                                                    self.stddev, dtype)\n \n@@ -513,10 +547,19 @@ class VarianceScaling(Initializer):\n     \"\"\"\n     _validate_kwargs(self.__class__.__name__, kwargs)\n     dtype = _assert_float_dtype(_get_dtype(dtype))\n-    scale = self.scale\n-    fan_in, fan_out = _compute_fans(shape)\n     if _PARTITION_SHAPE in kwargs:\n       shape = kwargs[_PARTITION_SHAPE]\n+    layout = kwargs.pop('layout', None)\n+    if layout:\n+      self._random_generator._force_generator = True\n+      _ensure_keras_seeded()\n+      return utils.call_with_layout(self._generate_init_val, layout,\n+                                    shape=shape, dtype=dtype)\n+    return self._generate_init_val(shape=shape, dtype=dtype)\n+\n+  def _generate_init_val(self, shape, dtype):\n+    scale = self.scale\n+    fan_in, fan_out = _compute_fans(shape)\n     if self.mode == 'fan_in':\n       scale /= max(1., fan_in)\n     elif self.mode == 'fan_out':\n@@ -607,6 +650,15 @@ class Orthogonal(Initializer):\n       raise ValueError('The tensor to initialize must be '\n                        'at least two-dimensional. Received: '\n                        f'shape={shape} of rank {len(shape)}.')\n+    layout = kwargs.pop('layout', None)\n+    if layout:\n+      self._random_generator._force_generator = True\n+      _ensure_keras_seeded()\n+      return utils.call_with_layout(\n+          self._generate_init_val, layout, shape=shape, dtype=dtype)\n+    return self._generate_init_val(shape, dtype)\n+\n+  def _generate_init_val(self, shape, dtype):\n     # Flatten the input shape with the last dimension remaining\n     # its original shape so it works for conv2d\n     num_rows = 1\n@@ -674,6 +726,13 @@ class Identity(Initializer):\n       raise ValueError(\n           'Identity matrix initializer can only be used for 2D matrices. '\n           f'Received: shape={shape} of rank {len(shape)}.')\n+    layout = kwargs.pop('layout', None)\n+    if layout:\n+      return utils.call_with_layout(\n+          self._generate_init_val, layout, shape=shape, dtype=dtype)\n+    return self._generate_init_val(shape, dtype)\n+\n+  def _generate_init_val(self, shape, dtype):\n     initializer = tf.eye(*shape, dtype=dtype)\n     return self.gain * initializer\n \n@@ -998,10 +1057,24 @@ def _compute_fans(shape):\n \n \n def _validate_kwargs(cls_name, kwargs, support_partition=True):\n-  for kwarg in kwargs:\n-    if kwarg not in _ALLOWED_INITIALIZER_KWARGS:\n-      raise TypeError(f'Unknown keyword arguments: {kwarg}. Allowed keyword '\n-                      f'arguments: {_ALLOWED_INITIALIZER_KWARGS}.')\n-    elif not support_partition:\n+  invalid_kwargs = [k for k in kwargs if k not in _ALLOWED_INITIALIZER_KWARGS]\n+  if invalid_kwargs:\n+    raise TypeError(f'Unknown keyword arguments: {invalid_kwargs}. Allowed '\n+                    f'keyword arguments: {_ALLOWED_INITIALIZER_KWARGS}.')\n+  if not support_partition and (_PARTITION_SHAPE in kwargs or\n+                                _PARTITION_OFFSET in kwargs):\n     raise ValueError(f'{cls_name} initializer doesn\\'t support '\n                      'partition-related arguments.')\n+\n+\n+def _ensure_keras_seeded():\n+  \"\"\"Make sure the keras.backend global seed generator is set.\n+\n+  This is important for DTensor use case to ensure that each client are\n+  initialized with same seed for tf.random.Generator, so that the value created\n+  are in sync among all the clients.\n+  \"\"\"\n+  if not getattr(backend._SEED_GENERATOR, 'generator', None):  # pylint:disable=protected-access\n+    raise ValueError('When using DTensor APIs, you need to set the global seed '\n+                     'before using any Keras initializers. Please make sure '\n+                     'to call `tf.keras.utils.set_random_seed()` in your code.')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#986b2ae807041776ebab966af5525ad21e0270c2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 51 | Lines Deleted: 280 | Files Changed: 3 | Hunks: 16 | Methods Changed: 16 | Complexity Δ (Sum/Max): -8/7 | Churn Δ: 331 | Churn Cumulative: 3863 | Contributors (this commit): 10 | Commits (past 90d): 30 | Contributors (cumulative): 19 | DMM Complexity: 0.0\n\nDIFF:\n@@ -209,7 +209,7 @@ class BinaryAccuracy(base_metric.MeanMetricWrapper):\n \n   def __init__(self, name='binary_accuracy', dtype=None, threshold=0.5):\n     super(BinaryAccuracy, self).__init__(\n-        metrics_utils.binary_matches, name, dtype=dtype, threshold=threshold)\n+        binary_accuracy, name, dtype=dtype, threshold=threshold)\n \n \n @keras_export('keras.metrics.CategoricalAccuracy')\n@@ -261,10 +261,7 @@ class CategoricalAccuracy(base_metric.MeanMetricWrapper):\n \n   def __init__(self, name='categorical_accuracy', dtype=None):\n     super(CategoricalAccuracy, self).__init__(\n-        lambda y_true, y_pred: metrics_utils.sparse_categorical_matches(  # pylint: disable=g-long-lambda\n-            tf.math.argmax(y_true, axis=-1), y_pred),\n-        name,\n-        dtype=dtype)\n+        categorical_accuracy, name, dtype=dtype)\n \n \n @keras_export('keras.metrics.SparseCategoricalAccuracy')\n@@ -315,7 +312,7 @@ class SparseCategoricalAccuracy(base_metric.MeanMetricWrapper):\n \n   def __init__(self, name='sparse_categorical_accuracy', dtype=None):\n     super(SparseCategoricalAccuracy, self).__init__(\n-        metrics_utils.sparse_categorical_matches, name, dtype=dtype)\n+        sparse_categorical_accuracy, name, dtype=dtype)\n \n \n _SPARSE_CATEGORICAL_UPDATE_STATE_DOCSTRING = \"\"\"Accumulates metric statistics.\n@@ -380,11 +377,7 @@ class TopKCategoricalAccuracy(base_metric.MeanMetricWrapper):\n \n   def __init__(self, k=5, name='top_k_categorical_accuracy', dtype=None):\n     super(TopKCategoricalAccuracy, self).__init__(\n-        lambda yt, yp, k: metrics_utils.sparse_top_k_categorical_matches(  # pylint: disable=g-long-lambda\n-            tf.math.argmax(yt, axis=-1), yp, k),\n-        name,\n-        dtype=dtype,\n-        k=k)\n+        top_k_categorical_accuracy, name, dtype=dtype, k=k)\n \n \n @keras_export('keras.metrics.SparseTopKCategoricalAccuracy')\n@@ -422,7 +415,7 @@ class SparseTopKCategoricalAccuracy(base_metric.MeanMetricWrapper):\n \n   def __init__(self, k=5, name='sparse_top_k_categorical_accuracy', dtype=None):\n     super(SparseTopKCategoricalAccuracy, self).__init__(\n-        metrics_utils.sparse_top_k_categorical_matches, name, dtype=dtype, k=k)\n+        sparse_top_k_categorical_accuracy, name, dtype=dtype, k=k)\n \n \n SparseTopKCategoricalAccuracy.update_state.__doc__ = _SPARSE_CATEGORICAL_UPDATE_STATE_DOCSTRING\n@@ -3218,12 +3211,10 @@ def binary_accuracy(y_true, y_pred, threshold=0.5):\n   Returns:\n     Binary accuracy values. shape = `[batch_size, d0, .. dN-1]`\n   \"\"\"\n-  # Note: calls metrics_utils.binary_matches with mean reduction. This maintains\n-  # public facing binary_accuracy behavior and seperates it from the vital\n-  # behavior of the binary_matches method needed in backend dependencies.\n-\n-  return tf.reduce_mean(\n-      metrics_utils.binary_matches(y_true, y_pred, threshold), axis=-1)\n+  y_pred = tf.convert_to_tensor(y_pred)\n+  threshold = tf.cast(threshold, y_pred.dtype)\n+  y_pred = tf.cast(y_pred > threshold, y_pred.dtype)\n+  return backend.mean(tf.equal(y_true, y_pred), axis=-1)\n \n \n @keras_export('keras.metrics.categorical_accuracy')\n@@ -3249,12 +3240,8 @@ def categorical_accuracy(y_true, y_pred):\n   Returns:\n     Categorical accuracy values.\n   \"\"\"\n-  # Note: wraps metrics_utils.categorical_matches. This seperates public facing\n-  # categorical_accuracy behavior from the vital behavior of the\n-  # categorical_matches method needed in backend dependencies.\n-\n-  return metrics_utils.sparse_categorical_matches(\n-      tf.math.argmax(y_true, axis=-1), y_pred)\n+  y_true = tf.math.argmax(y_true, axis=-1)\n+  return sparse_categorical_accuracy(y_true, y_pred)\n \n \n @keras_export('keras.metrics.sparse_categorical_accuracy')\n@@ -3280,18 +3267,22 @@ def sparse_categorical_accuracy(y_true, y_pred):\n   Returns:\n     Sparse categorical accuracy values.\n   \"\"\"\n-  # Note: wraps metrics_utils.sparse_categorical_matches method and checks for\n-  # squeezing to align with expected public facing behavior. This seperates\n-  # public facing sparse_categorical_accuracy behavior from the vital behavior\n-  # of the sparse_categorical_matches method needed in backend dependencies.\n+  y_pred = tf.convert_to_tensor(y_pred)\n+  y_true = tf.convert_to_tensor(y_true)\n+  y_pred_rank = y_pred.shape.ndims\n+  y_true_rank = y_true.shape.ndims\n+  # If the shape of y_true is (num_samples, 1), squeeze to (num_samples,)\n+  if (y_true_rank is not None) and (y_pred_rank is not None) and (len(\n+      backend.int_shape(y_true)) == len(backend.int_shape(y_pred))):\n+    y_true = tf.squeeze(y_true, [-1])\n+  y_pred = tf.math.argmax(y_pred, axis=-1)\n \n-  matches = metrics_utils.sparse_categorical_matches(y_true, y_pred)\n+  # If the predicted output and actual output types don't match, force cast them\n+  # to match.\n+  if backend.dtype(y_pred) != backend.dtype(y_true):\n+    y_pred = tf.cast(y_pred, backend.dtype(y_true))\n \n-  # if shape is (num_samples, 1) squeeze\n-  if matches.shape.ndims > 1 and matches.shape[-1] == 1:\n-    matches = tf.squeeze(matches, [-1])\n-\n-  return matches\n+  return tf.cast(tf.equal(y_true, y_pred), backend.floatx())\n \n \n @keras_export('keras.metrics.top_k_categorical_accuracy')\n@@ -3316,12 +3307,8 @@ def top_k_categorical_accuracy(y_true, y_pred, k=5):\n   Returns:\n     Top K categorical accuracy value.\n   \"\"\"\n-  # Note: wraps metrics_utils.top_k_categorical_matches. This seperates\n-  # public facing top_k_categorical_accuracy behavior from the vital behavior\n-  # of the top_k_categorical_matches method needed in backend dependencies.\n-\n-  return metrics_utils.sparse_top_k_categorical_matches(\n-      tf.math.argmax(y_true, axis=-1), y_pred, k)\n+  y_true = tf.math.argmax(y_true, axis=-1)\n+  return sparse_top_k_categorical_accuracy(y_true, y_pred, k=k)\n \n \n @keras_export('keras.metrics.sparse_top_k_categorical_accuracy')\n@@ -3347,12 +3334,31 @@ def sparse_top_k_categorical_accuracy(y_true, y_pred, k=5):\n   Returns:\n     Sparse top K categorical accuracy value.\n   \"\"\"\n-  # Note: wraps metrics_utils.sparse_top_k_categorical_matches. This seperates\n-  # public facing sparse_top_k_categorical_accuracy behavior from the vital\n-  # behavior of the sparse_top_k_categorical_matches method needed in backend\n-  # dependencies.\n+  reshape_matches = False\n+  y_true = tf.convert_to_tensor(y_true)\n+  y_pred = tf.convert_to_tensor(y_pred)\n+  y_true_rank = y_true.shape.ndims\n+  y_pred_rank = y_pred.shape.ndims\n+  y_true_org_shape = tf.shape(y_true)\n \n-  return metrics_utils.sparse_top_k_categorical_matches(y_true, y_pred, k)\n+  # Flatten y_pred to (batch_size, num_samples) and y_true to (num_samples,)\n+  if (y_true_rank is not None) and (y_pred_rank is not None):\n+    if y_pred_rank > 2:\n+      y_pred = tf.reshape(y_pred, [-1, y_pred.shape[-1]])\n+    if y_true_rank > 1:\n+      reshape_matches = True\n+      y_true = tf.reshape(y_true, [-1])\n+\n+  matches = tf.cast(\n+      tf.math.in_top_k(\n+          predictions=y_pred, targets=tf.cast(y_true, 'int32'), k=k),\n+      dtype=backend.floatx())\n+\n+  # returned matches is expected to have same shape as y_true input\n+  if reshape_matches:\n+    return tf.reshape(matches, shape=y_true_org_shape)\n+\n+  return matches\n \n \n def cosine_similarity(y_true, y_pred, axis=-1):\n\n@@ -839,86 +839,3 @@ def _assert_splits_match(nested_splits_lists):\n       for splits_list in nested_splits_lists[1:]\n       for (s1, s2) in zip(nested_splits_lists[0], splits_list)\n   ]\n-\n-\n-def binary_matches(y_true, y_pred, threshold=0.5):\n-  \"\"\"Creates int Tensor, 1 for label-prediction match, 0 for mismatch.\n-\n-  Args:\n-    y_true: Ground truth values, of shape (batch_size, d0, .. dN).\n-    y_pred: The predicted values, of shape (batch_size, d0, .. dN).\n-    threshold: (Optional) Float representing the threshold for deciding whether\n-      prediction values are 1 or 0.\n-\n-  Returns:\n-    Binary matches, of shape (batch_size, d0, .. dN).\n-  \"\"\"\n-  y_pred = tf.convert_to_tensor(y_pred)\n-  threshold = tf.cast(threshold, y_pred.dtype)\n-  y_pred = tf.cast(y_pred > threshold, y_pred.dtype)\n-  return tf.cast(tf.equal(y_true, y_pred), backend.floatx())\n-\n-\n-def sparse_categorical_matches(y_true, y_pred):\n-  \"\"\"Creates float Tensor, 1.0 for label-prediction match, 0.0 for mismatch.\n-\n-  You can provide logits of classes as `y_pred`, since argmax of\n-  logits and probabilities are same.\n-\n-  Args:\n-    y_true: Integer ground truth values.\n-    y_pred: The prediction values.\n-\n-  Returns:\n-    Match tensor: 1.0 for label-prediction match, 0.0 for mismatch.\n-  \"\"\"\n-  y_pred = tf.convert_to_tensor(y_pred)\n-  y_true = tf.convert_to_tensor(y_true)\n-\n-  # Reshape to ensure comparable to y_true in tf.equal\n-  y_pred = tf.reshape(tf.math.argmax(y_pred, axis=-1), shape=tf.shape(y_true))\n-\n-  # If the predicted output and actual output types don't match, force cast them\n-  # to match.\n-  if backend.dtype(y_pred) != backend.dtype(y_true):\n-    y_pred = tf.cast(y_pred, backend.dtype(y_true))\n-  return tf.cast(tf.equal(y_true, y_pred), backend.floatx())\n-\n-\n-def sparse_top_k_categorical_matches(y_true, y_pred, k=5):\n-  \"\"\"Creates float Tensor, 1.0 for label-TopK_prediction match, 0.0 for mismatch.\n-\n-  Args:\n-    y_true: tensor of true targets.\n-    y_pred: tensor of predicted targets.\n-    k: (Optional) Number of top elements to look at for computing accuracy.\n-      Defaults to 5.\n-\n-  Returns:\n-    Match tensor: 1.0 for label-prediction match, 0.0 for mismatch.\n-  \"\"\"\n-  reshape_matches = False\n-  y_true = tf.convert_to_tensor(y_true)\n-  y_pred = tf.convert_to_tensor(y_pred)\n-  y_true_rank = y_true.shape.ndims\n-  y_pred_rank = y_pred.shape.ndims\n-  y_true_org_shape = tf.shape(y_true)\n-\n-  # Flatten y_pred to (batch_size, num_samples) and y_true to (num_samples,)\n-  if (y_true_rank is not None) and (y_pred_rank is not None):\n-    if y_pred_rank > 2:\n-      y_pred = tf.reshape(y_pred, [-1, y_pred.shape[-1]])\n-    if y_true_rank > 1:\n-      reshape_matches = True\n-      y_true = tf.reshape(y_true, [-1])\n-\n-  matches = tf.cast(\n-      tf.math.in_top_k(\n-          predictions=y_pred, targets=tf.cast(y_true, 'int32'), k=k),\n-      dtype=backend.floatx())\n-\n-  # returned matches is expected to have same shape as y_true input\n-  if reshape_matches:\n-    return tf.reshape(matches, shape=y_true_org_shape)\n-\n-  return matches\n\n@@ -17,9 +17,6 @@\n import tensorflow.compat.v2 as tf\n \n from absl.testing import parameterized\n-import numpy as np\n-\n-from keras import backend\n from keras.testing_infra import test_combinations\n from keras.utils import metrics_utils\n \n@@ -293,154 +290,5 @@ class FilterTopKTest(tf.test.TestCase, parameterized.TestCase):\n     ])\n \n \n-class MatchesMethodsTest(tf.test.TestCase, parameterized.TestCase):\n-\n-  def test_sparse_categorical_matches(self):\n-    with self.cached_session():\n-      matches_method = metrics_utils.sparse_categorical_matches\n-\n-      # Test return tensor is type float\n-      y_true = backend.variable(np.random.randint(0, 7, (6,)))\n-      y_pred = backend.variable(np.random.random((6, 7)))\n-      self.assertEqual(\n-          backend.eval(matches_method(y_true, y_pred)).dtype, backend.floatx())\n-\n-      # Tests that resulting Tensor always has same shape as y_true. Tests from\n-      # 1 dim to 4 dims\n-      dims = []\n-      for _ in range(4):\n-        dims.append(np.random.randint(1, 7))\n-        y_true = backend.variable(np.random.randint(0, 7, dims))\n-        y_pred = backend.variable(np.random.random(dims + [3]))\n-        self.assertEqual(\n-            backend.eval(matches_method(y_true, y_pred)).shape, y_true.shape)\n-\n-      # Test correctness if the shape of y_true is (num_samples,)\n-      y_true = backend.variable([1., 0., 0., 0.])\n-      y_pred = backend.variable([[0.8, 0.2], [0.6, 0.4], [0.7, 0.3], [0.9,\n-                                                                      0.1]])\n-      self.assertAllEqual(\n-          backend.eval(matches_method(y_true, y_pred)), [0., 1., 1., 1.])\n-\n-      # Test correctness if the shape of y_true is (num_samples, 1)\n-      y_true = backend.variable([[1.], [0.], [0.], [0.]])\n-      y_pred = backend.variable([[0.8, 0.2], [0.6, 0.4], [0.7, 0.3], [0.9,\n-                                                                      0.1]])\n-      self.assertAllEqual(\n-          backend.eval(matches_method(y_true, y_pred)),\n-          [[0.], [1.], [1.], [1.]])\n-\n-      # Test correctness if the shape of y_true is (batch_size, seq_length) and\n-      # y_pred is (batch_size, seq_length, num_classes)\n-      y_pred = backend.variable(\n-          np.array([[[0.2, 0.3, 0.1], [0.1, 0.2, 0.7]],\n-                    [[0.3, 0.2, 0.1], [0.7, 0.2, 0.1]]]))\n-      y_true = backend.variable(np.array([[1, 0], [1, 0]]))\n-      self.assertAllEqual(\n-          backend.eval(matches_method(y_true, y_pred)), [[1., 0.], [0., 1.]])\n-\n-  def test_sparse_top_k_categorical_matches(self):\n-    with self.cached_session():\n-      matches_method = metrics_utils.sparse_top_k_categorical_matches\n-\n-      # Test return tensor is type float\n-      y_true = backend.variable(np.random.randint(0, 7, (6,)))\n-      y_pred = backend.variable(np.random.random((6, 7)))\n-      self.assertEqual(\n-          backend.eval(matches_method(y_true, y_pred, 1)).dtype,\n-          backend.floatx())\n-\n-      # Tests that resulting Tensor always has same shape as y_true. Tests from\n-      # 1 dim to 4 dims\n-      dims = []\n-      for _ in range(4):\n-        dims.append(np.random.randint(1, 7))\n-        y_true = backend.variable(np.random.randint(0, 7, dims))\n-        y_pred = backend.variable(np.random.random(dims + [3]))\n-        self.assertEqual(\n-            backend.eval(matches_method(y_true, y_pred, 1)).shape, y_true.shape)\n-\n-      # Test correctness if the shape of y_true is (num_samples,) for k = 1,2,3\n-      y_true = backend.variable([1., 0., 0., 0.])\n-      y_pred = backend.variable([[0.7, 0.2, 0.1], [0.5, 0.3, 0.2],\n-                                 [0.6, 0.3, 0.1], [0.0, 0.1, 0.9]])\n-      self.assertAllEqual(\n-          backend.eval(matches_method(y_true, y_pred, 1)), [0., 1., 1., 0.])\n-      self.assertAllEqual(\n-          backend.eval(matches_method(y_true, y_pred, 2)), [1., 1., 1., 0.])\n-      self.assertAllEqual(\n-          backend.eval(matches_method(y_true, y_pred, 3)), [1., 1., 1., 1.])\n-\n-      # Test correctness if the shape of y_true is (num_samples, 1)\n-      # for k = 1,2,3\n-      y_true = backend.variable([[1.], [0.], [0.], [0.]])\n-      y_pred = backend.variable([[0.7, 0.2, 0.1], [0.5, 0.3, 0.2],\n-                                 [0.6, 0.3, 0.1], [0.0, 0.1, 0.9]])\n-      self.assertAllEqual(\n-          backend.eval(matches_method(y_true, y_pred, 1)),\n-          [[0.], [1.], [1.], [0.]])\n-      self.assertAllEqual(\n-          backend.eval(matches_method(y_true, y_pred, 2)),\n-          [[1.], [1.], [1.], [0.]])\n-      self.assertAllEqual(\n-          backend.eval(matches_method(y_true, y_pred, 3)),\n-          [[1.], [1.], [1.], [1.]])\n-\n-      # Test correctness if the shape of y_true is (batch_size, seq_length) and\n-      # y_pred is (batch_size, seq_length, num_classes) for k = 1,2,3\n-      y_pred = backend.variable(\n-          np.array([[[0.2, 0.3, 0.1], [0.1, 0.2, 0.7]],\n-                    [[0.3, 0.2, 0.1], [0.7, 0.2, 0.1]]]))\n-      y_true = backend.variable(np.array([[1, 0], [1, 0]]))\n-      self.assertAllEqual(\n-          backend.eval(matches_method(y_true, y_pred, 1)), [[1., 0.], [0., 1.]])\n-      self.assertAllEqual(\n-          backend.eval(matches_method(y_true, y_pred, 2)), [[1., 0.], [1., 1.]])\n-      self.assertAllEqual(\n-          backend.eval(matches_method(y_true, y_pred, 3)), [[1., 1.], [1., 1.]])\n-\n-  def test_binary_matches(self):\n-    with self.cached_session():\n-      matches_method = metrics_utils.binary_matches\n-\n-      # Test return tensor is type float\n-      y_true = backend.variable(np.random.random((6, 7)))\n-      y_pred = backend.variable(np.random.random((6, 7)))\n-      self.assertEqual(\n-          backend.eval(matches_method(y_true, y_pred, .5)).dtype,\n-          backend.floatx())\n-\n-      # Tests that resulting Tensor always has same shape as y_true. Tests from\n-      # 1 dim to 4 dims.\n-      dims = []\n-      for _ in range(4):\n-        dims.append(np.random.randint(1, 7))\n-        y_true = y_pred = backend.variable(np.random.random(dims))\n-        self.assertEqual(\n-            backend.eval(matches_method(y_true, y_pred, 0.)).shape,\n-            y_true.shape)\n-\n-      # Testing for correctness shape (num_samples, 1)\n-      y_true = backend.variable([[1.], [0.], [1.], [1.]])\n-      y_pred = backend.variable([[.75], [.2], [.2], [.75]])\n-      self.assertAllEqual(\n-          backend.eval(matches_method(y_true, y_pred, .5)),\n-          [[1.], [1.], [0.], [1.]])\n-\n-      # Testing for correctness shape (num_samples,)\n-      y_true = backend.variable([1., 0., 1., 1.])\n-      y_pred = backend.variable([.75, .2, .2, .75])\n-      self.assertAllEqual(\n-          backend.eval(matches_method(y_true, y_pred, .5)), [1., 1., 0., 1.])\n-\n-      # Testing for correctness batches of sequences\n-      # shape (num_samples, seq_len)\n-      y_true = backend.variable([[1., 0.], [0., 1.], [1., 0.], [1., 0.]])\n-      y_pred = backend.variable([[.75, .2], [.2, .75], [.2, .75], [.75, .2]])\n-      self.assertAllEqual(\n-          backend.eval(matches_method(y_true, y_pred, .5)),\n-          [[1., 1.], [1., 1.], [0., 0.], [1., 1.]])\n-\n-\n if __name__ == '__main__':\n   tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fc00d58f810553a177400e441b903263db96c1e1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 172 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -44,7 +44,7 @@ class DenseFeatures(dense_features.DenseFeatures):\n   ```python\n   price = tf.feature_column.numeric_column('price')\n   keywords_embedded = tf.feature_column.embedding_column(\n-      tf.feature_column.categorical_column_with_hash_bucket(\"keywords\", 10K),\n+      tf.feature_column.categorical_column_with_hash_bucket(\"keywords\", 10000),\n       dimensions=16)\n   columns = [price, keywords_embedded, ...]\n   feature_layer = tf.keras.layers.DenseFeatures(columns)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#581f32291e9eac6ecb6633a89452c61a2cd5fd1f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 15/15 | Churn Δ: 2 | Churn Cumulative: 2088 | Contributors (this commit): 7 | Commits (past 90d): 9 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -1167,7 +1167,7 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertEqual(self._eval_if_tensor(opt.inner_optimizer.momentum), 0.5)\n     self.assertEqual(self.evaluate(opt.loss_scale), 2.)\n     self.assertEqual(opt.dynamic_growth_steps, 3.)\n-    self.assertEqual(opt.dynamic, 4.)\n+    self.assertTrue(opt.dynamic)\n     if lso_type in ('v1', 'v2'):\n       # Deserializing a LossScaleOptimizerV1 results in a V2 LossScaleOptimizer.\n       self.assertEqual(type(opt), loss_scale_optimizer.LossScaleOptimizer)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9c24fc4057303172ad977cebd626da2b7adb63d4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 75 | Lines Deleted: 37 | Files Changed: 1 | Hunks: 30 | Methods Changed: 8 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 112 | Churn Cumulative: 1317 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 0.5555555555555556\n\nDIFF:\n@@ -116,11 +116,7 @@ BLOCK_ARGS = {\n }\n CONV_KERNEL_INITIALIZER = {\n     \"class_name\": \"VarianceScaling\",\n-    \"config\": {\n-        \"scale\": 2.0,\n-        \"mode\": \"fan_out\",\n-        \"distribution\": \"truncated_normal\"\n-    }\n+    \"config\": {\"scale\": 2.0, \"mode\": \"fan_out\", \"distribution\": \"truncated_normal\"},\n }\n \n BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n@@ -195,10 +191,12 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n         A `keras.Model` instance.\n \"\"\"\n \n+\n def Conv2DFixedPadding(filters, kernel_size, strides, name=None):\n     \"\"\"Conv2D block with fixed padding.\"\"\"\n     if name is None:\n-        name = backend.get_uid(\"conv\")\n+        counter = backend.get_uid(\"conv_\")\n+        name = f\"conv_{counter}\"\n \n     def apply(inputs):\n         if strides > 1:\n@@ -212,12 +210,20 @@ def Conv2DFixedPadding(filters, kernel_size, strides, name = None):\n             kernel_initializer=CONV_KERNEL_INITIALIZER,\n             name=name,\n         )(inputs)\n+\n     return apply\n \n-def STEM(bn_momentum: float = 0.0, bn_epsilon: float =1e-5, activation: str =\"relu\", name = None):\n+\n+def STEM(\n+    bn_momentum: float = 0.0,\n+    bn_epsilon: float = 1e-5,\n+    activation: str = \"relu\",\n+    name=None,\n+):\n     \"\"\"ResNet-D type STEM block.\"\"\"\n     if name is None:\n-        name = \"stem\" + str(backend.get_uid(\"stem\"))\n+        counter = backend.get_uid(\"stem_\")\n+        name = f\"stem_{counter}\"\n \n     def apply(inputs):\n         bn_axis = 3 if backend.image_data_format() == \"channels_last\" else 1\n@@ -278,7 +284,8 @@ def SE(in_filters: int, se_ratio: float =0.25,  expand_ratio: int = 1,name = Non\n     \"\"\"Squeeze and Excitation block.\"\"\"\n     bn_axis = 3 if backend.image_data_format() == \"channels_last\" else 1\n     if name is None:\n-        name = \"se\" + str(backend.get_uid(\"se\"))\n+        counter = backend.get_uid(\"se_\")\n+        name = f\"se_{counter}\"\n \n     def apply(inputs):\n         x = layers.GlobalAveragePooling2D(name=name + \"_se_squeeze\")(inputs)\n@@ -330,7 +337,9 @@ def BottleneckBlock(\n ):\n     \"\"\"Bottleneck block variant for residual networks with BN after convolutions.\"\"\"\n     if name is None:\n-        name = \"block_0_\" + str(backend.get_uid(\"block_0_\"))\n+        counter = backend.get_uid(\"block_0_\")\n+        name = f\"block_0_{counter}\"\n+\n     def apply(inputs):\n         bn_axis = 3 if backend.image_data_format() == \"channels_last\" else 1\n \n@@ -430,7 +439,8 @@ def BlockGroup(\n ):\n     \"\"\"Create one group of blocks for the ResNet model.\"\"\"\n     if name is None:\n-        name = \"block_group\" + str(backend.get_uid(\"block_group\"))\n+        counter = backend.get_uid(\"block_group_\")\n+        name = f\"block_group_{counter}\"\n \n     def apply(inputs):\n         # Only the first block per block_group uses projection shortcut and strides.\n@@ -462,6 +472,7 @@ def BlockGroup(\n \n     return apply\n \n+\n def get_survival_probability(init_rate, block_num, total_blocks):\n     \"\"\"Get survival probability based on block number and initial rate.\"\"\"\n     return init_rate * float(block_num) / total_blocks\n@@ -487,6 +498,7 @@ def fixed_padding(inputs, kernel_size):\n \n     return padded_inputs\n \n+\n def ResNetRS(\n     depth: int,\n     input_shape=None,\n@@ -504,7 +516,7 @@ def ResNetRS(\n     input_tensor=None,\n     classes=1000,\n     classifier_activation: Union[str, Callable] = \"softmax\",\n-    include_preprocessing=True\n+    include_preprocessing=True,\n ):\n     \"\"\"Build Resnet-RS model, given provided parameters.\n \n@@ -593,7 +605,8 @@ def ResNetRS(\n         min_size=32,\n         data_format=backend.image_data_format(),\n         require_flatten=include_top,\n-      weights=weights)\n+        weights=weights,\n+    )\n     # Define input tensor\n     if input_tensor is None:\n         img_input = layers.Input(shape=input_shape)\n@@ -602,15 +615,25 @@ def ResNetRS(\n             img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n         else:\n             img_input = input_tensor\n+\n     bn_axis = 3 if backend.image_data_format() == \"channels_last\" else 1\n+\n     x = img_input\n+\n     if include_preprocessing:\n-        x = layers.Rescaling(scale=1. / 255)(x)\n+        num_channels = input_shape[bn_axis - 1]\n+        if num_channels == 3:\n+            x = layers.Rescaling(scale=1.0 / 255)(x)\n+            x = layers.Normalization(\n+                mean=[0.485, 0.456, 0.406],\n+                variance=[0.229**2, 0.224**2, 0.225**2],\n+                axis=bn_axis,\n+            )(x)\n+        else:\n+            x = layers.Rescaling(scale=1.0 / 255)(x)\n \n     # Build stem\n-    x = STEM(bn_momentum=bn_momentum, bn_epsilon=bn_epsilon, activation=activation)(\n-        x\n-    )\n+    x = STEM(bn_momentum=bn_momentum, bn_epsilon=bn_epsilon, activation=activation)(x)\n \n     # Build blocks\n     if block_args is None:\n@@ -642,9 +665,9 @@ def ResNetRS(\n             x = layers.Dropout(dropout_rate, name=\"top_dropout\")(x)\n \n         imagenet_utils.validate_activation(classifier_activation, weights)\n-        x = layers.Dense(\n-            classes, activation=classifier_activation, name=\"predictions\"\n-        )(x)\n+        x = layers.Dense(classes, activation=classifier_activation, name=\"predictions\")(\n+            x\n+        )\n     else:\n         if pooling == \"avg\":\n             x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(x)\n@@ -683,8 +706,10 @@ def ResNetRS(\n \n     return model\n \n-@keras_export('keras.applications.resnet_rs.ResNetRS50',\n-              'keras.applications.ResNetRS50')\n+\n+@keras_export(\n+    \"keras.applications.resnet_rs.ResNetRS50\", \"keras.applications.ResNetRS50\"\n+)\n def ResNetRS50(\n     include_top=True,\n     weights=\"imagenet\",\n@@ -711,8 +736,10 @@ def ResNetRS50(\n         include_preprocessing=include_preprocessing,\n     )\n \n-@keras_export('keras.applications.resnet_rs.ResNetRS101',\n-              'keras.applications.ResNetRS101')\n+\n+@keras_export(\n+    \"keras.applications.resnet_rs.ResNetRS101\", \"keras.applications.ResNetRS101\"\n+)\n def ResNetRS101(\n     include_top=True,\n     weights=\"imagenet\",\n@@ -739,8 +766,10 @@ def ResNetRS101(\n         include_preprocessing=include_preprocessing,\n     )\n \n-@keras_export('keras.applications.resnet_rs.ResNetRS152',\n-              'keras.applications.ResNetRS152')\n+\n+@keras_export(\n+    \"keras.applications.resnet_rs.ResNetRS152\", \"keras.applications.ResNetRS152\"\n+)\n def ResNetRS152(\n     include_top=True,\n     weights=\"imagenet\",\n@@ -767,8 +796,10 @@ def ResNetRS152(\n         include_preprocessing=include_preprocessing,\n     )\n \n-@keras_export('keras.applications.resnet_rs.ResNetRS200',\n-              'keras.applications.ResNetRS200')\n+\n+@keras_export(\n+    \"keras.applications.resnet_rs.ResNetRS200\", \"keras.applications.ResNetRS200\"\n+)\n def ResNetRS200(\n     include_top=True,\n     weights=\"imagenet\",\n@@ -795,8 +826,10 @@ def ResNetRS200(\n         include_preprocessing=include_preprocessing,\n     )\n \n-@keras_export('keras.applications.resnet_rs.ResNetRS270',\n-              'keras.applications.ResNetRS270')\n+\n+@keras_export(\n+    \"keras.applications.resnet_rs.ResNetRS270\", \"keras.applications.ResNetRS270\"\n+)\n def ResNetRS270(\n     include_top=True,\n     weights=\"imagenet\",\n@@ -824,8 +857,10 @@ def ResNetRS270(\n         include_preprocessing=include_preprocessing,\n     )\n \n-@keras_export('keras.applications.resnet_rs.ResNetRS350',\n-              'keras.applications.ResNetRS350')\n+\n+@keras_export(\n+    \"keras.applications.resnet_rs.ResNetRS350\", \"keras.applications.ResNetRS350\"\n+)\n def ResNetRS350(\n     include_top=True,\n     weights=\"imagenet\",\n@@ -853,8 +888,10 @@ def ResNetRS350(\n         include_preprocessing=include_preprocessing,\n     )\n \n-@keras_export('keras.applications.resnet_rs.ResNetRS420',\n-              'keras.applications.ResNetRS420')\n+\n+@keras_export(\n+    \"keras.applications.resnet_rs.ResNetRS420\", \"keras.applications.ResNetRS420\"\n+)\n def ResNetRS420(\n     include_top=True,\n     weights=\"imagenet\",\n@@ -882,7 +919,8 @@ def ResNetRS420(\n         include_preprocessing=include_preprocessing,\n     )\n \n-@keras_export('keras.applications.resnet_rs.preprocess_input')\n+\n+@keras_export(\"keras.applications.resnet_rs.preprocess_input\")\n def preprocess_input(x, data_format=None):\n     \"\"\"A placeholder method for backward compatibility.\n \n@@ -904,10 +942,11 @@ def preprocess_input(x, data_format=None):\n     return x\n \n \n-@keras_export('keras.applications.resnet_rs.decode_predictions')\n+@keras_export(\"keras.applications.resnet_rs.decode_predictions\")\n def decode_predictions(preds, top=5):\n     return imagenet_utils.decode_predictions(preds, top=top)\n \n+\n decode_predictions.__doc__ = imagenet_utils.decode_predictions.__doc__\n \n ResNetRS50.__doc__ = BASE_DOCSTRING.format(name=\"ResNetRS50\")\n@@ -916,4 +955,3 @@ ResNetRS200.__doc__= BASE_DOCSTRING.format(name=\"ResNetRS200\")\n ResNetRS270.__doc__ = BASE_DOCSTRING.format(name=\"ResNetRS270\")\n ResNetRS350.__doc__ = BASE_DOCSTRING.format(name=\"ResNetRS350\")\n ResNetRS420.__doc__ = BASE_DOCSTRING.format(name=\"ResNetRS420\")\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
