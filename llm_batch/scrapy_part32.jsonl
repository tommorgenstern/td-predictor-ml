{"custom_id": "scrapy#4286e5e47c7fd7a0d185749838d3a4a2785558c6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 33 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -107,7 +107,7 @@ class CrawlSpider(BaseSpider):\n             res.extend(self._requests_to_follow(response))\n         if callback:\n             cb_res = callback(response, **cb_kwargs) or ()\n-            cb_res = self.process_results(cb_res, response)\n+            cb_res = self.process_spider_output(cb_res, response)\n             res.extend(cb_res)\n         return res\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d1594df0cbe343257b36b50fdf0fd6ff6e617a8f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 309 | Contributors (this commit): 2 | Commits (past 90d): 13 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -22,7 +22,8 @@ def url_is_from_any_domain(url, domains):\n \n def url_is_from_spider(url, spider):\n     \"\"\"Return True if the url belongs to the given spider\"\"\"\n-    domains = [spider.domain_name] + spider.extra_domain_names\n+    domains = [spider.domain_name]\n+    domains.extend(spider.extra_domain_names)\n     return url_is_from_any_domain(url, domains)\n \n def urljoin_rfc(base, ref):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#66852b8291355d1f55fbf37f027e985287159cd4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 224 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -3,7 +3,7 @@ import libxml2\n from scrapy.http import Response\n from scrapy.xpath.extension import Libxml2Document\n from scrapy.xpath.constructors import xmlDoc_from_html, xmlDoc_from_xml\n-from scrapy.utils.python import flatten\n+from scrapy.utils.python import flatten, unicode_to_str\n from scrapy.utils.misc import extract_regex\n \n class XPathSelector(object):\n@@ -28,7 +28,7 @@ class XPathSelector(object):\n                 self.doc = Libxml2Document(response, constructor=constructor)\n             self.xmlNode = self.doc.xmlDoc\n         elif text:\n-            response = Response(domain=None, url=None, body=str(text))\n+            response = Response(domain=None, url=None, body=unicode_to_str(text))\n             self.doc = Libxml2Document(response, constructor=constructor)\n             self.xmlNode = self.doc.xmlDoc\n         self.expr = expr\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cd148c946e7708dcdd06437e420e5319a9ddf210", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 35 | Files Changed: 3 | Hunks: 4 | Methods Changed: 5 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 63 | Churn Cumulative: 614 | Contributors (this commit): 1 | Commits (past 90d): 27 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,4 @@\n-from scrapy.contrib.adaptors.extraction import extract, extract_unquoted, ExtractImages\n+from scrapy.contrib.adaptors.extraction import extract, ExtractImages\n from scrapy.contrib.adaptors.markup import remove_tags, remove_root, Unquote\n from scrapy.contrib.adaptors.misc import to_unicode, clean_spaces, strip, drop_empty, canonicalize_urls, Delist, Regex\n from scrapy.utils.python import unique, flatten\n\n@@ -10,39 +10,32 @@ from scrapy.utils.url import is_url\n from scrapy.utils.python import flatten\n from scrapy.xpath.selector import XPathSelector, XPathSelectorList\n \n-def _extract(locations, extractor='extract'):\n-    if isinstance(locations, (XPathSelector, XPathSelectorList)):\n-        return flatten(getattr(locations, extractor)())\n-    elif hasattr(locations, '__iter__'):\n-        return flatten([getattr(x, extractor)() if isinstance(x, (XPathSelector, XPathSelectorList)) else x for x in flatten(locations)])\n-    elif isinstance(locations, basestring):\n-        return [locations]\n+def extract(locations, use_unquote=True):\n+    \"\"\"\n+    This adaptor tries to extract data from the given locations.\n+    Any XPathSelector in it will be extracted, and any other data\n+    will be added as-is to the result.\n+\n+    If an XPathSelector is a text/cdata node, and `use_unquote`\n+    is True, that selector will be extracted using the `extract_unquoted`\n+    method; otherwise, the `extract` method will be used.\n+\n+    Input: anything\n+    Output: list of extracted selectors plus anything else in the input\n+    \"\"\"\n+\n+    locations = flatten([locations])\n+\n+    result = []\n+    for location in locations:\n+        if isinstance(location, XPathSelector):\n+            if location.xmlNode.type in ('text', 'cdata') and use_unquote:\n+                result.append(location.extract_unquoted())\n             else:\n-        return []\n-\n-def extract(locations):\n-    \"\"\"\n-    This adaptor extracts a list of strings\n-    from 'locations', which can be either an iterable,\n-    or an XPathSelector/XPathSelectorList.\n-\n-    Input: XPathSelector, XPathSelectorList, iterable, basestring\n-    Output: list of unicodes\n-    \"\"\"\n-    return _extract(locations)\n-\n-def extract_unquoted(locations):\n-    \"\"\"\n-    This adaptor extracts a list of unquoted strings\n-    from 'locations', which can be either an iterable,\n-    or an XPathSelector/XPathSelectorList.\n-    The difference between this and the extract adaptor is\n-    that this adaptor will only extract text nodes and unquote them.\n-\n-    Input: XPathSelector, XPathSelectorList, iterable, basestring\n-    Output: list of unicodes\n-    \"\"\"\n-    return _extract(locations, 'extract_unquoted')\n+                result.append(location.extract())\n+        else:\n+            result.append(location)\n+    return result\n \n class ExtractImages(object):\n     \"\"\"\n\n@@ -75,8 +75,8 @@ class AdaptorsTestCase(unittest.TestCase):\n \n     def test_extract_unquoted(self):\n         x = self.get_selector('example.com', 'http://www.example.com/test_unquoted', 'extr_unquoted.xml', selector=XmlXPathSelector)\n-        self.assertEqual(adaptors.extract_unquoted(x.x('//tag1/text()')), [u'test text & &', u'more test text &amp; &gt;', u'blah&blah'])\n-        self.assertEqual(adaptors.extract_unquoted(x.x('//tag2/text()')), [u'blaheawfds<'])\n+        self.assertEqual(adaptors.extract(x.x('//tag1/text()')), [u'test text & &', u'more test text &amp; &gt;', u'blah&blah'])\n+        self.assertEqual(adaptors.extract(x.x('//tag2/text()')), [u'blaheawfds<'])\n \n     def test_extract_links(self):\n         test_data = \"\"\"<html><body>\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e4852bca78e476873d3acf5a188b98f63cd9d439", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 9 | Methods Changed: 8 | Complexity Δ (Sum/Max): 5/4 | Churn Δ: 46 | Churn Cumulative: 1871 | Contributors (this commit): 2 | Commits (past 90d): 34 | Contributors (cumulative): 4 | DMM Complexity: 0.5\n\nDIFF:\n@@ -10,7 +10,7 @@ from scrapy.utils.url import is_url\n from scrapy.utils.python import flatten\n from scrapy.xpath.selector import XPathSelector, XPathSelectorList\n \n-def extract(locations, use_unquote=True):\n+def extract(locations, adaptor_args=None):\n     \"\"\"\n     This adaptor tries to extract data from the given locations.\n     Any XPathSelector in it will be extracted, and any other data\n@@ -23,8 +23,8 @@ def extract(locations, use_unquote=True):\n     Input: anything\n     Output: list of extracted selectors plus anything else in the input\n     \"\"\"\n-\n     locations = flatten([locations])\n+    use_unquote = adaptor_args.get('use_unquote', True) if adaptor_args else True\n \n     result = []\n     for location in locations:\n\n@@ -51,10 +51,11 @@ class AdaptorPipe(list):\n         return value\n \n     def __add__(self, other):\n-        if isinstance(other, list):\n+        if callable(other):\n+            other = [other]\n+        elif hasattr(other, '__iter__'):\n+            other = list(other)\n         return AdaptorPipe(super(AdaptorPipe, self).__add__(other))\n-        elif callable(other):\n-            return AdaptorPipe(self + [other])\n \n     def __repr__(self):\n         return '<AdaptorPipe %s >' % super(AdaptorPipe, self).__repr__()\n\n@@ -3,10 +3,38 @@ import os\n import unittest\n import re\n \n+from scrapy.item.adaptors import AdaptorPipe\n from scrapy.contrib import adaptors\n from scrapy.http import Response, Headers\n from scrapy.xpath.selector import HtmlXPathSelector, XmlXPathSelector\n \n+class AdaptorPipeTestCase(unittest.TestCase):\n+    def test_pipe_init(self):\n+        self.assertRaises(TypeError, AdaptorPipe, [adaptors.extract, 'a string'])\n+\n+    def test_adaptor_args(self):\n+        def sample_adaptor(value, adaptor_args):\n+            '''Dummy adaptor that joins the received value with the given string'''\n+            sample_text = adaptor_args.get('sample_arg', 'sample text 1')\n+            return '%s \"%s\"' % (value, sample_text)\n+\n+        sample_value = 'hi, this is my text:'\n+        sample_pipe = AdaptorPipe([sample_adaptor])\n+        self.assertEqual(sample_pipe(sample_value), 'hi, this is my text: \"sample text 1\"')\n+        self.assertEqual(sample_pipe(sample_value, sample_arg='foobarfoobar'),\n+            'hi, this is my text: \"foobarfoobar\"')\n+\n+    def test_add(self):\n+        pipe1 = AdaptorPipe([adaptors.extract])\n+        pipe2 = [adaptors.remove_tags]\n+        pipe3 = (adaptors.remove_root, )\n+        sample_callable = dir\n+\n+        self.assertTrue(isinstance(pipe1 + pipe1, AdaptorPipe))\n+        self.assertTrue(isinstance(pipe1 + pipe2, AdaptorPipe))\n+        self.assertTrue(isinstance(pipe1 + pipe3, AdaptorPipe))\n+        self.assertTrue(isinstance(pipe1 + sample_callable, AdaptorPipe))\n+\n class AdaptorsTestCase(unittest.TestCase):\n     def setUp(self):\n         self.samplesdir = os.path.abspath(os.path.join(os.path.dirname(__file__), 'sample_data', 'adaptors'))\n@@ -75,9 +103,16 @@ class AdaptorsTestCase(unittest.TestCase):\n \n     def test_extract_unquoted(self):\n         x = self.get_selector('example.com', 'http://www.example.com/test_unquoted', 'extr_unquoted.xml', selector=XmlXPathSelector)\n+\n+        # test unquoting\n         self.assertEqual(adaptors.extract(x.x('//tag1/text()')), [u'test text & &', u'more test text &amp; &gt;', u'blah&blah'])\n         self.assertEqual(adaptors.extract(x.x('//tag2/text()')), [u'blaheawfds<'])\n \n+        # test without unquoting\n+        self.assertEqual(adaptors.extract(x.x('//tag1/text()'), {'use_unquote': False}),\n+            [u'test text &amp; &amp;', u'<![CDATA[more test text &amp; &gt;]]>', u'blah&amp;blah'])\n+        self.assertEqual(adaptors.extract(x.x('//tag2/text()'), {'use_unquote': False}), [u'blaheawfds&lt;'])\n+\n     def test_extract_links(self):\n         test_data = \"\"\"<html><body>\n                          <div>\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8029275c75875fe396961775c39eda5c8d633264", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 5 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,3 +1 @@\n-from decobot.conf.commands.start import *\n-\n LOGLEVEL='WARNING'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#307f8b3321364c83e42c8e59c7bdc14a527e8ac1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 286 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -88,7 +88,7 @@ def command_settings(cmdname):\n     if basepath:\n         try:\n             module = __import__('%s.%s' % (basepath, cmdname), {}, {}, [''])\n-            update_defaults(settings.defaults, module)\n+            update_defaults(settings.overrides, module)\n         except ImportError:\n             pass\n \n\n@@ -115,10 +115,6 @@ class Command(ScrapyCommand):\n \n         print \"Scrapy %s - Interactive scraping console\\n\" % scrapy.__version__\n \n-        print \"Enabling Scrapy extensions...\",\n-        extensions.load()\n-        print \"done\"\n-\n         def _console_thread():\n             \n             def _get_magic(shell, arg):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#382621b814ca149f8cecf7c082e421978601349c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 331 | Contributors (this commit): 2 | Commits (past 90d): 15 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -55,7 +55,7 @@ class RegexLinkExtractor(LinkExtractor):\n         if self.restrict_xpaths:\n             hxs = HtmlXPathSelector(response)\n             html_slice = ''.join(''.join(html_fragm for html_fragm in hxs.x(xpath_expr).extract()) for xpath_expr in self.restrict_xpaths)\n-            links = self._extract_links(html_slice, response.url)\n+            links = self._extract_links(html_slice, response.url, response.body.get_real_encoding())\n         else:\n             links = LinkExtractor.extract_links(self, response)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#32d842326aa64c8b063e4c5483b05d2229b3cc53", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 181 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -88,7 +88,7 @@ def command_settings(cmdname):\n     if basepath:\n         try:\n             module = __import__('%s.%s' % (basepath, cmdname), {}, {}, [''])\n-            update_defaults(settings.overrides, module)\n+            update_defaults(settings.defaults, module)\n         except ImportError:\n             pass\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8278c0cffccee7f2c5e8c701284d5a6bacce9bbe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 6 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 176 | Contributors (this commit): 2 | Commits (past 90d): 13 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -67,7 +67,7 @@ class CrawlSpider(BaseSpider):\n         return a list of ScrapedItems and/or Requests\"\"\"\n         return []\n \n-    def process_spider_output(self, results, response):\n+    def process_spider_output(self, response, results):\n         \"\"\"This overridable method is called for each result (item or request)\n         returned by the spider, and it's intended to perform any last time\n         processing required before returning the results to the framework core,\n@@ -107,7 +107,7 @@ class CrawlSpider(BaseSpider):\n             res.extend(self._requests_to_follow(response))\n         if callback:\n             cb_res = callback(response, **cb_kwargs) or ()\n-            cb_res = self.process_spider_output(cb_res, response)\n+            cb_res = self.process_spider_output(response, cb_res)\n             res.extend(cb_res)\n         return res\n \n\n@@ -19,7 +19,7 @@ class XMLFeedSpider(BaseSpider):\n     iterator = 'iternodes'\n     itertag = 'item'\n \n-    def process_results(self, results, response):\n+    def process_spider_output(self, response, results):\n         \"\"\"This overridable method is called for each result (item or request)\n         returned by the spider, and it's intended to perform any last time\n         processing required before returning the results to the framework core,\n@@ -40,7 +40,7 @@ class XMLFeedSpider(BaseSpider):\n                 ret = [ret]\n             if not isinstance(ret, (list, tuple)):\n                 raise UsageError('You cannot return an \"%s\" object from a spider' % type(ret).__name__)\n-            for result_item in self.process_results(ret, response):\n+            for result_item in self.process_spider_output(response, ret):\n                 yield result_item\n \n     def parse(self, response):\n@@ -71,7 +71,7 @@ class CSVFeedSpider(BaseSpider):\n     delimiter = None # When this is None, python's csv module's default delimiter is used\n     headers = None\n \n-    def process_results(self, results, response):\n+    def process_spider_output(self, response, results):\n         \"\"\"This method has the same purpose as the one in XMLFeedSpider\"\"\"\n         return results\n \n@@ -86,7 +86,7 @@ class CSVFeedSpider(BaseSpider):\n                 ret = [ret]\n             if not isinstance(ret, (list, tuple)):\n                 raise UsageError('You cannot return an \"%s\" object from a spider' % type(ret).__name__)\n-            for result_item in self.process_results(ret, response):\n+            for result_item in self.process_spider_output(response, ret):\n                 yield result_item\n \n     def parse(self, response):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4a9a48a62951988787424a28066de1dcb43e6596", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 16 | Files Changed: 2 | Hunks: 7 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 37 | Churn Cumulative: 590 | Contributors (this commit): 2 | Commits (past 90d): 23 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -82,7 +82,7 @@ class LinkExtractor(FixedSGMLParser):\n \n     def handle_data(self, data):\n         if self.current_link and not self.current_link.text:\n-            self.current_link.text = data\n+            self.current_link.text = data.strip()\n \n     def matches(self, url):\n         \"\"\"This extractor matches with any url, since\n\n@@ -79,36 +79,41 @@ class RegexLinkExtractorTestCase(unittest.TestCase):\n \n         lx = RegexLinkExtractor()\n         self.assertEqual([link for link in lx.extract_links(self.response)],\n-            [ Link(url='http://example.com/sample1.html', text='sample 1'),\n-              Link(url='http://example.com/sample2.html', text=''),\n-              Link(url='http://example.com/sample3.html', text='sample 3 text'),\n-              Link(url='http://www.google.com/something', text='') ])\n+            [ Link(url='http://example.com/sample1.html', text=u''),\n+              Link(url='http://example.com/sample2.html', text=u'sample 2'),\n+              Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n+              Link(url='http://www.google.com/something', text=u'') ])\n \n         lx = RegexLinkExtractor(allow=('sample', ))\n         self.assertEqual([link for link in lx.extract_links(self.response)],\n-            [ Link(url='http://example.com/sample1.html', text='sample 1'),\n-              Link(url='http://example.com/sample2.html', text=''),\n-              Link(url='http://example.com/sample3.html', text='sample 3 text') ])\n+            [ Link(url='http://example.com/sample1.html', text=u''),\n+              Link(url='http://example.com/sample2.html', text=u'sample 2'),\n+              Link(url='http://example.com/sample3.html', text=u'sample 3 text') ])\n \n         lx = RegexLinkExtractor(allow=('sample', ), unique=False)\n         self.assertEqual([link for link in lx.extract_links(self.response)],\n-            [ Link(url='http://example.com/sample1.html', text='sample 1'),\n-              Link(url='http://example.com/sample2.html', text=''),\n-              Link(url='http://example.com/sample3.html', text='sample 3 text'),\n-              Link(url='http://example.com/sample3.html', text='sample 3 repetition') ])\n+            [ Link(url='http://example.com/sample1.html', text=u''),\n+              Link(url='http://example.com/sample2.html', text=u'sample 2'),\n+              Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n+              Link(url='http://example.com/sample3.html', text=u'sample 3 repetition') ])\n \n         lx = RegexLinkExtractor(allow=('sample', ), deny=('3', ))\n         self.assertEqual([link for link in lx.extract_links(self.response)],\n-            [ Link(url='http://example.com/sample1.html', text='sample 1'),\n-              Link(url='http://example.com/sample2.html', text='') ])\n+            [ Link(url='http://example.com/sample1.html', text=u''),\n+              Link(url='http://example.com/sample2.html', text=u'sample 2') ])\n \n         lx = RegexLinkExtractor(allow_domains=('google.com', ))\n         self.assertEqual([link for link in lx.extract_links(self.response)],\n-            [ Link(url='http://www.google.com/something', text='') ])\n+            [ Link(url='http://www.google.com/something', text=u'') ])\n \n         lx = RegexLinkExtractor(tags=('img', ), attrs=('src', ))\n         self.assertEqual([link for link in lx.extract_links(self.response)],\n-            [ Link(url='http://example.com/sample2.jpg', text='') ])\n+            [ Link(url='http://example.com/sample2.jpg', text=u'') ])\n+\n+        lx = RegexLinkExtractor(restrict_xpaths=('//div[@id=\"subwrapper\"]', ))\n+        self.assertEqual([link for link in lx.extract_links(self.response)],\n+            [ Link(url='http://example.com/sample1.html', text=u''),\n+              Link(url='http://example.com/sample2.html', text=u'sample 2') ])\n \n     def test_matches(self):\n         url1 = 'http://lotsofstuff.com/stuff1/index'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fc76af2eb488792e8c5c9c71902022a9a6240403", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 223 | Contributors (this commit): 2 | Commits (past 90d): 13 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -49,7 +49,7 @@ class LinkExtractor(FixedSGMLParser):\n         ret = []\n         base_url = self.base_url if self.base_url else response_url\n         for link in links:\n-            link.url = urljoin(base_url, link.url).strip()\n+            link.url = urljoin(base_url, link.url)\n             link.url = safe_url_string(link.url, response_encoding)\n             link.text = link.text.decode(response_encoding)\n             ret.append(link)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f39b7b507a597d102e51ef3735b8fd5efc365093", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 8 | Churn Cumulative: 377 | Contributors (this commit): 2 | Commits (past 90d): 12 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -110,10 +110,10 @@ class RegexLinkExtractorTestCase(unittest.TestCase):\n         self.assertEqual([link for link in lx.extract_links(self.response)],\n             [ Link(url='http://example.com/sample2.jpg', text=u'') ])\n \n-        lx = RegexLinkExtractor(restrict_xpaths=('//div[@id=\"subwrapper\"]', ))\n-        self.assertEqual([link for link in lx.extract_links(self.response)],\n-            [ Link(url='http://example.com/sample1.html', text=u''),\n-              Link(url='http://example.com/sample2.html', text=u'sample 2') ])\n+#        lx = RegexLinkExtractor(restrict_xpaths=('//div[@id=\"subwrapper\"]', ))\n+#        self.assertEqual([link for link in lx.extract_links(self.response)],\n+#            [ Link(url='http://example.com/sample1.html', text=u''),\n+#              Link(url='http://example.com/sample2.html', text=u'sample 2') ])\n \n     def test_matches(self):\n         url1 = 'http://lotsofstuff.com/stuff1/index'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6420c407d2c23fe1cd71d39cbee3ccc67511e76f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 21 | Churn Cumulative: 849 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -43,15 +43,18 @@ class SiteDetails(object):\n \n \n class Downloader(object):\n-    \"\"\"Maintain many concurrent downloads and provide an HTTP abstraction\n-    We will have a limited number of connections per domain and scrape many domains in\n+    \"\"\"Maintain many concurrent downloads and provide an HTTP abstraction.\n+    It supports a limited number of connections per domain and many domains in\n     parallel.\n-\n-    request(..) should be called to request resources using http, https or file\n-    protocols.\n     \"\"\"\n \n-    def __init__(self) :\n+    def __init__(self, engine):\n+        \"\"\"Create the downlaoder. \n+        \n+        ``engine`` is the scrapy engine controlling this downloader\n+        \"\"\"\n+\n+        self.engine = engine\n         self.sites = {}\n         self.middleware = DownloaderMiddlewareManager()\n         self.middleware.download_function = self.enqueue\n@@ -113,10 +116,8 @@ class Downloader(object):\n             self._download(request, spider, deferred)\n \n         if site.closed and site.is_idle():\n-            # XXX: Remove scrapyengine reference\n             del self.sites[domain]\n-            from scrapy.core.engine import scrapyengine\n-            scrapyengine.closed_domain(domain=domain)\n+            self.engine.closed_domain(domain)\n \n     def _download(self, request, spider, deferred):\n         log.msg('Activating %s' % request.traceinfo(), log.TRACE)\n\n@@ -56,7 +56,7 @@ class ExecutionEngine(object):\n         Configure execution engine with the given scheduling policy and downloader.\n         \"\"\"\n         self.scheduler = scheduler or Scheduler()\n-        self.downloader = downloader or Downloader()\n+        self.downloader = downloader or Downloader(self)\n         self.spidermiddleware = SpiderMiddlewareManager()\n         self._scraping = {}\n         self.pipeline = ItemPipelineManager()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6568dc4f35b0331cbac564b26eb8ef281386af5c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 18 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -91,7 +91,7 @@ pygments_style = 'sphinx'\n # The style sheet to use for HTML and HTML Help pages. A file of that name\n # must exist either in Sphinx' static/ path, or in one of the custom paths\n # given in html_static_path.\n-html_style = 'default.css'\n+html_style = 'scrapydoc.css'\n \n # The name for this set of Sphinx documents.  If None, it defaults to\n # \"<project> v<release> documentation\".\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5cb9f344ac79a7454069e0cdd851ef2abaebefab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 6 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 188 | Contributors (this commit): 2 | Commits (past 90d): 15 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -67,7 +67,7 @@ class CrawlSpider(BaseSpider):\n         return a list of ScrapedItems and/or Requests\"\"\"\n         return []\n \n-    def process_spider_output(self, response, results):\n+    def process_results(self, response, results):\n         \"\"\"This overridable method is called for each result (item or request)\n         returned by the spider, and it's intended to perform any last time\n         processing required before returning the results to the framework core,\n@@ -107,7 +107,7 @@ class CrawlSpider(BaseSpider):\n             res.extend(self._requests_to_follow(response))\n         if callback:\n             cb_res = callback(response, **cb_kwargs) or ()\n-            cb_res = self.process_spider_output(response, cb_res)\n+            cb_res = self.process_results(response, cb_res)\n             res.extend(cb_res)\n         return res\n \n\n@@ -19,7 +19,7 @@ class XMLFeedSpider(BaseSpider):\n     iterator = 'iternodes'\n     itertag = 'item'\n \n-    def process_spider_output(self, response, results):\n+    def process_results(self, response, results):\n         \"\"\"This overridable method is called for each result (item or request)\n         returned by the spider, and it's intended to perform any last time\n         processing required before returning the results to the framework core,\n@@ -40,7 +40,7 @@ class XMLFeedSpider(BaseSpider):\n                 ret = [ret]\n             if not isinstance(ret, (list, tuple)):\n                 raise UsageError('You cannot return an \"%s\" object from a spider' % type(ret).__name__)\n-            for result_item in self.process_spider_output(response, ret):\n+            for result_item in self.process_results(response, ret):\n                 yield result_item\n \n     def parse(self, response):\n@@ -71,7 +71,7 @@ class CSVFeedSpider(BaseSpider):\n     delimiter = None # When this is None, python's csv module's default delimiter is used\n     headers = None\n \n-    def process_spider_output(self, response, results):\n+    def process_results(self, response, results):\n         \"\"\"This method has the same purpose as the one in XMLFeedSpider\"\"\"\n         return results\n \n@@ -86,7 +86,7 @@ class CSVFeedSpider(BaseSpider):\n                 ret = [ret]\n             if not isinstance(ret, (list, tuple)):\n                 raise UsageError('You cannot return an \"%s\" object from a spider' % type(ret).__name__)\n-            for result_item in self.process_spider_output(response, ret):\n+            for result_item in self.process_results(response, ret):\n                 yield result_item\n \n     def parse(self, response):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c601bbb0837f5de378ab2ded67d1c53b0e1bb238", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 237 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -80,8 +80,8 @@ class Response(object) :\n                            kw.get('url', self.url),\n                            original_url=kw.get('original_url', self.original_url),\n                            headers=kw.get('headers', sameheaders()),\n-                           status=kw.get('status', self.status),\n-                           body=kw.get('body', samebody()))\n+                           status=kw.get('status', self.status))\n+        newresp.body = kw.get('body', samebody())\n         return newresp\n \n     def to_string(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d0e5245fba993659a58bc6d2a5bb6fe9f625bbf0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 244 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -80,8 +80,11 @@ class Response(object) :\n                            kw.get('url', self.url),\n                            original_url=kw.get('original_url', self.original_url),\n                            headers=kw.get('headers', sameheaders()),\n-                           status=kw.get('status', self.status))\n-        newresp.body = kw.get('body', samebody())\n+                           status=kw.get('status', self.status),\n+                           body=kw.get('body'))\n+        # Response.__init__ forbids the use of ResponseBody instances\n+        if 'body' not in kw:\n+            newresp.body = samebody()\n         return newresp\n \n     def to_string(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1feace1bb09d27c82a0e8cc16fc526b51959e278", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 54 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,23 +3,25 @@ from unittest import TestCase, main\n from scrapy.http import Response\n from scrapy.contrib.downloadermiddleware.decompression import DecompressionMiddleware\n \n-class ScrapyDecompressionTest(TestCase):\n-    uncompressed_body = ''\n-    test_responses = {}\n-    middleware = DecompressionMiddleware()\n-\n-    def setUp(self):\n-        self.datadir = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'sample_data', 'compressed')\n+def setUp():\n+    datadir = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'sample_data', 'compressed')\n     formats = ['tar', 'xml.bz2', 'xml.gz', 'zip']\n-        uncompressed_fd = open(os.path.join(self.datadir, 'feed-sample1.xml'), 'r')\n-        self.uncompressed_body = uncompressed_fd.read()\n+\n+    uncompressed_fd = open(os.path.join(datadir, 'feed-sample1.xml'), 'r')\n+    uncompressed_body = uncompressed_fd.read()\n     uncompressed_fd.close()\n \n+    test_responses = {}\n     for format in formats:\n-            fd = open(os.path.join(self.datadir, 'feed-sample1.' + format), 'r')\n+        fd = open(os.path.join(datadir, 'feed-sample1.' + format), 'r')\n         body = fd.read()\n         fd.close()\n-            self.test_responses[format] = Response('foo.com', 'http://foo.com/bar', body=body)\n+        test_responses[format] = Response('foo.com', 'http://foo.com/bar', body=body)\n+    return uncompressed_body, test_responses\n+\n+class ScrapyDecompressionTest(TestCase):\n+    uncompressed_body, test_responses = setUp()\n+    middleware = DecompressionMiddleware()\n \n     def test_tar(self):\n         response, format = self.middleware.extract(self.test_responses['tar'])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8240fc48ad0f58546cef90f56434503734268487", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 15 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 34 | Churn Cumulative: 54 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,20 +1,24 @@\n-# -*- coding: utf-8 -*-\n from unittest import TestCase, main\n-import libxml2\n-from scrapy.http import ResponseBody\n+from scrapy.http import Response, ResponseBody\n \n-class ResponseTest(TestCase):\n-    def test_responsebodyencoding(self):\n-        string = 'кириллический текст'\n-        unicode_string = unicode(string, 'utf-8')\n-        body_cp1251 = unicode_string.encode('cp1251')\n-        body = ResponseBody(body_cp1251, 'cp1251')\n-        self.assertEqual(body.to_unicode(), unicode_string)\n-        self.assertEqual(isinstance(body.to_unicode(), unicode), True)\n-        self.assertEqual(body.to_string('utf-8'), string)\n-        self.assertEqual(isinstance(body.to_string('utf-8'), str), True)\n-        self.assertEqual(body.to_string(), body_cp1251)\n-        self.assertEqual(isinstance(body.to_string('utf-8'), str), True)\n+class ResponseBodyTest(TestCase):\n+    unicode_string = u'\\u043a\\u0438\\u0440\\u0438\\u043b\\u043b\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0439 \\u0442\\u0435\\u043a\\u0441\\u0442'\n+\n+    def test_encoding(self):\n+        original_string = self.unicode_string.encode('cp1251')\n+        cp1251_body     = ResponseBody(original_string, 'cp1251')\n+\n+        # check to_unicode\n+        self.assertTrue(isinstance(cp1251_body.to_unicode(), unicode))\n+        self.assertEqual(cp1251_body.to_unicode(), self.unicode_string)\n+\n+        # check to_string using default encoding (declared when created)\n+        self.assertTrue(isinstance(cp1251_body.to_string(), str))\n+        self.assertEqual(cp1251_body.to_string(), original_string)\n+\n+        # check to_string using arbitrary encoding\n+        self.assertTrue(isinstance(cp1251_body.to_string('utf-8'), str))\n+        self.assertEqual(cp1251_body.to_string('utf-8'), self.unicode_string.encode('utf-8'))\n \n if __name__ == \"__main__\":\n     main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d3ac5d5ab8f106aaabfdc3557a19540025d261af", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 11 | Churn Cumulative: 65 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,17 @@\n from unittest import TestCase, main\n from scrapy.http import Response, ResponseBody\n \n+class ResponseTest(TestCase):\n+    def test_init(self):\n+        # Response requires domain and url\n+        self.assertRaises(Exception, Response)\n+        self.assertRaises(Exception, Response, 'example.com')\n+        self.assertTrue(isinstance(Response('example.com', 'http://example.com/'), Response))\n+        # body can be str or None but not ResponseBody\n+        self.assertTrue(isinstance(Response('example.com', 'http://example.com/', body=None), Response))\n+        self.assertTrue(isinstance(Response('example.com', 'http://example.com/', body='body'), Response))\n+        self.assertRaises(AssertionError, Response, 'example.com', 'http://example.com/', body=ResponseBody('body', 'utf-8'))\n+\n class ResponseBodyTest(TestCase):\n     unicode_string = u'\\u043a\\u0438\\u0440\\u0438\\u043b\\u043b\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0439 \\u0442\\u0435\\u043a\\u0441\\u0442'\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8e38f70a746de67cb3afcc6240c7c6a0521bb383", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 67 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,6 +11,8 @@ class ResponseTest(TestCase):\n         self.assertTrue(isinstance(Response('example.com', 'http://example.com/', body=None), Response))\n         self.assertTrue(isinstance(Response('example.com', 'http://example.com/', body='body'), Response))\n         self.assertRaises(AssertionError, Response, 'example.com', 'http://example.com/', body=ResponseBody('body', 'utf-8'))\n+        # test presence of all optional parameters\n+        self.assertTrue(isinstance(Response('example.com', 'http://example.com/', original_url='http://example.com/None', headers={}, status=200, body=None), Response))\n \n class ResponseBodyTest(TestCase):\n     unicode_string = u'\\u043a\\u0438\\u0440\\u0438\\u043b\\u043b\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0439 \\u0442\\u0435\\u043a\\u0441\\u0442'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4c54305bee3ed6df897be73c277eb1931d2f197d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 135 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -55,20 +55,32 @@ def unique(list_, key=lambda x: x):\n \n \n def str_to_unicode(text, encoding='utf-8'):\n+    \"\"\"Return the unicode representation of text in the given encoding. Unlike\n+    .encode(encoding) this function can be applied directly to a unicode\n+    object without the risk of double-decoding problems (which can happen if\n+    you don't use the default 'ascii' encoding)\n+    \"\"\"\n+    \n     if isinstance(text, str):\n         return text.decode(encoding)\n     elif isinstance(text, unicode):\n         return text\n     else:\n-        raise TypeError('str_to_unicode can only receive a string object')\n+        raise TypeError('str_to_unicode must receive a str or unicode object, got %s' % type(text).__name__)\n \n def unicode_to_str(text, encoding='utf-8'):\n+    \"\"\"Return the str representation of text in the given encoding. Unlike\n+    .encode(encoding) this function can be applied directly to a str\n+    object without the risk of double-decoding problems (which can happen if\n+    you don't use the default 'ascii' encoding)\n+    \"\"\"\n+\n     if isinstance(text, unicode):\n         return text.encode(encoding)\n     elif isinstance(text, str):\n         return text\n     else:\n-        raise TypeError('unicode_to_str can only receive a unicode object')\n+        raise TypeError('unicode_to_str must receive a unicode or str object, got %s' % type(text).__name__)\n \n def re_rsearch(pattern, text, chunk_size=1024):\n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2cfd292f01591d70fbe5c49e4f1923fa9dac30f7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 9 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,6 +6,7 @@ import sys\n from twisted.python import log\n \n from scrapy.conf import settings\n+from scrapy.utils.python import unicode_to_str\n  \n # Logging levels\n levels = {\n@@ -48,7 +49,8 @@ def msg(message, level=INFO, component=BOT_NAME, domain=None):\n     \"\"\" Log message according to the level \"\"\"\n     component = \"%s/%s\" % (BOT_NAME, domain) if domain else component\n     if level <= log_level:\n-        log.msg(\"%s: %s\" % (levels[level], message), system=component)\n+        msg_txt = unicode_to_str(\"%s: %s\" % (levels[level], message))\n+        log.msg(msg_txt, system=component)\n \n def exc(message, level=ERROR, component=BOT_NAME, domain=None):\n     from traceback import format_exc\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#71f7d62c68002f6007038912624aeb95cf73ad49", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 151 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -82,6 +82,6 @@ class AWSMiddleware(object):\n \n     def process_request(self, request, spider):\n         if spider.domain_name == 's3.amazonaws.com' \\\n-                or request.url.hostname.endswith('s3.amazonaws.com'):\n+                or (request.url.hostname and request.url.hostname.endswith('s3.amazonaws.com')):\n             request.headers['Date'] = time.strftime(\"%a, %d %b %Y %H:%M:%S GMT\", time.gmtime())\n             sign_request(request, self.access_key, self.secret_key)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5c15def3a51857450a9ef8753619473ec4cc5421", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 15 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,3 +1,8 @@\n+\"\"\"\n+Mail sending helpers\n+\n+See documentation in docs/topics/email.rst\n+\"\"\"\n import smtplib\n \n from email.MIMEMultipart import MIMEMultipart\n@@ -20,14 +25,6 @@ class MailSender(object):\n             raise NotConfigured(\"MAIL_HOST and MAIL_FROM settings are required\")\n \n     def send(self, to, subject, body, cc=None, attachs=None):\n-        \"\"\"\n-        Send mail to the given recipients\n-        \n-        - to: must be a list of email recipients\n-        - attachs must be a list of tuples: (attach_name, mimetype, file_object)\n-        - body and subjet must be a string\n-        \"\"\"\n-\n         msg = MIMEMultipart()\n         msg['From'] = self.mailfrom\n         msg['To'] = COMMASPACE.join(to)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
