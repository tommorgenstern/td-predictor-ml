{"custom_id": "keras#a56b16fffec6e4a431bf14e13e7dabeeb5904cd8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 8801 | Contributors (this commit): 47 | Commits (past 90d): 11 | Contributors (cumulative): 47 | DMM Complexity: None\n\nDIFF:\n@@ -1174,6 +1174,11 @@ class GRUCell(Layer):\n             Fraction of the units to drop for\n             the linear transformation of the recurrent state.\n         implementation: Implementation mode, either 1 or 2.\n+            Mode 1 will structure its operations as a larger number of\n+            smaller dot products and additions, whereas mode 2 will\n+            batch them into fewer, larger operations. These modes will\n+            have different performance profiles on different hardware and\n+            for different applications.\n     \"\"\"\n \n     def __init__(self, units,\n@@ -1409,6 +1414,11 @@ class GRU(RNN):\n             Fraction of the units to drop for\n             the linear transformation of the recurrent state.\n         implementation: Implementation mode, either 1 or 2.\n+            Mode 1 will structure its operations as a larger number of\n+            smaller dot products and additions, whereas mode 2 will\n+            batch them into fewer, larger operations. These modes will\n+            have different performance profiles on different hardware and\n+            for different applications.\n         return_sequences: Boolean. Whether to return the last output.\n             in the output sequence, or the full sequence.\n         return_state: Boolean. Whether to return the last state\n@@ -1646,6 +1656,11 @@ class LSTMCell(Layer):\n             Fraction of the units to drop for\n             the linear transformation of the recurrent state.\n         implementation: Implementation mode, either 1 or 2.\n+            Mode 1 will structure its operations as a larger number of\n+            smaller dot products and additions, whereas mode 2 will\n+            batch them into fewer, larger operations. These modes will\n+            have different performance profiles on different hardware and\n+            for different applications.\n     \"\"\"\n \n     def __init__(self, units,\n@@ -1904,6 +1919,11 @@ class LSTM(RNN):\n             Fraction of the units to drop for\n             the linear transformation of the recurrent state.\n         implementation: Implementation mode, either 1 or 2.\n+            Mode 1 will structure its operations as a larger number of\n+            smaller dot products and additions, whereas mode 2 will\n+            batch them into fewer, larger operations. These modes will\n+            have different performance profiles on different hardware and\n+            for different applications.\n         return_sequences: Boolean. Whether to return the last output.\n             in the output sequence, or the full sequence.\n         return_state: Boolean. Whether to return the last state\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9634b5a8561d26706ed26a413e999f9aa50c7f47", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 2638 | Contributors (this commit): 44 | Commits (past 90d): 14 | Contributors (cumulative): 44 | DMM Complexity: None\n\nDIFF:\n@@ -319,7 +319,7 @@ def img_to_array(img, data_format=None):\n \n \n def load_img(path, grayscale=False, target_size=None,\n-             interpolation='bilinear'):\n+             interpolation='nearest'):\n     \"\"\"Loads an image into PIL format.\n \n     # Arguments\n@@ -332,7 +332,7 @@ def load_img(path, grayscale=False, target_size=None,\n             Supported methods are \"nearest\", \"bilinear\", and \"bicubic\".\n             If PIL version 1.1.3 or newer is installed, \"lanczos\" is also\n             supported. If PIL version 3.4.0 or newer is installed, \"box\" and\n-            \"hamming\" are also supported. By default, \"bilinear\" is used.\n+            \"hamming\" are also supported. By default, \"nearest\" is used.\n \n     # Returns\n         A PIL Image instance.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ce4947cbaf380589a63def4cc6eb3e460c41254f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 84 | Lines Deleted: 66 | Files Changed: 22 | Hunks: 46 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 150 | Churn Cumulative: 7809 | Contributors (this commit): 59 | Commits (past 90d): 60 | Contributors (cumulative): 124 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,6 @@\n # -*- coding: utf-8 -*-\n '''An implementation of sequence to sequence learning for performing addition\n+\n Input: \"535+61\"\n Output: \"596\"\n Padding is handled by using a repeated sentinel character (space)\n\n@@ -1,6 +1,7 @@\n '''Trains a memory network on the bAbI dataset.\n \n References:\n+\n - Jason Weston, Antoine Bordes, Sumit Chopra, Tomas Mikolov, Alexander M. Rush,\n   \"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\",\n   http://arxiv.org/abs/1502.05698\n\n@@ -1,4 +1,5 @@\n '''Trains two recurrent neural networks based upon a story and a question.\n+\n The resulting merged vector is then queried to answer a range of bAbI tasks.\n \n The results are comparable to those for an LSTM model provided in Weston et al.:\n@@ -31,7 +32,7 @@ QA20 - Agent's Motivations   | 91               | 90.7\n For the resources related to the bAbI project, refer to:\n https://research.facebook.com/researchers/1543934539189348\n \n-Notes:\n+# Notes\n \n - With default word, sentence, and query vector sizes, the GRU model achieves:\n   - 100% test accuracy on QA1 in 20 epochs (2 seconds per epoch on CPU)\n\n@@ -1,9 +1,6 @@\n '''Train a simple deep CNN on the CIFAR10 small images dataset.\n \n-GPU run command with Theano backend (with TensorFlow, the GPU is automatically used):\n-    THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatx=float32 python cifar10_cnn.py\n-\n-It gets down to 0.65 test logloss in 25 epochs, and down to 0.55 after 50 epochs.\n+It gets to 75% validation accuracy in 25 epochs, and 79% after 50 epochs.\n (it's still underfitting at that point, though).\n '''\n \n@@ -19,7 +16,7 @@ import os\n \n batch_size = 32\n num_classes = 10\n-epochs = 200\n+epochs = 100\n data_augmentation = True\n num_predictions = 20\n save_dir = os.path.join(os.getcwd(), 'saved_models')\n\n@@ -3,7 +3,9 @@\n Greater than 91% test accuracy (0.52 val_loss) after 50 epochs\n 48sec per epoch on GTX 1080Ti\n \n-Deep Residual Learning for Image Recognition\n+# Reference\n+\n+- Deep Residual Learning for Image Recognition\n   https://arxiv.org/pdf/1512.03385.pdf\n \"\"\"\n \n\n@@ -1,6 +1,6 @@\n '''Visualization of the filters of VGG16, via gradient ascent in input space.\n \n-This script can run on CPU in a few minutes (with the TensorFlow backend).\n+This script can run on CPU in a few minutes.\n \n Results example: http://i.imgur.com/4nj4KjN.jpg\n '''\n\n@@ -1,4 +1,5 @@\n \"\"\" This script demonstrates the use of a convolutional LSTM network.\n+\n This network is used to predict the next frame of an artificially\n generated movie which contains moving squares.\n \"\"\"\n\n@@ -1,4 +1,4 @@\n-'''Train a Bidirectional LSTM on the IMDB sentiment classification task.\n+'''Trains a Bidirectional LSTM on the IMDB sentiment classification task.\n \n Output after 4 epochs on CPU: ~0.8146\n Time per epoch on CPU (Core i7): ~150s.\n\n@@ -3,9 +3,7 @@\n Gets to 0.89 test accuracy after 2 epochs.\n 90s/epoch on Intel i5 2.4Ghz CPU.\n 10s/epoch on Tesla K40 GPU.\n-\n '''\n-\n from __future__ import print_function\n \n from keras.preprocessing import sequence\n\n@@ -1,7 +1,9 @@\n '''Trains an LSTM model on the IMDB sentiment classification task.\n+\n The dataset is actually too small for LSTM to be of any advantage\n compared to simpler, much faster methods such as TF-IDF + LogReg.\n-Notes:\n+\n+# Notes\n \n - RNNs are tricky. Choice of batch size is important,\n choice of loss and optimizer is critical, etc.\n\n@@ -7,7 +7,7 @@ character-by-character. Note that it is fairly unusual to\n do character-level machine translation, as word-level\n models are more common in this domain.\n \n-# Summary of the algorithm:\n+# Summary of the algorithm\n \n - We start with input sequences from a domain (e.g. English sentences)\n     and correspding target sequences from another domain\n@@ -32,7 +32,7 @@ models are more common in this domain.\n     - Repeat until we generate the end-of-sequence character or we\n         hit the character limit.\n \n-# Data download:\n+# Data download\n \n English to French sentence pairs.\n http://www.manythings.org/anki/fra-eng.zip\n@@ -40,7 +40,7 @@ http://www.manythings.org/anki/fra-eng.zip\n Lots of neat sentence pairs datasets can be found at:\n http://www.manythings.org/anki/\n \n-# References:\n+# References\n \n - Sequence to Sequence Learning with Neural Networks\n     https://arxiv.org/abs/1409.3215\n\n@@ -5,7 +5,8 @@ More documentation about the Keras LSTM model can be found at\n https://keras.io/layers/recurrent/#lstm\n \n The models are trained on an input/output pair, where\n-the input is a generated uniformly distributed random sequence of length = \"input_len\",\n+the input is a generated uniformly distributed\n+random sequence of length = \"input_len\",\n and the output is a moving average of the input with window length = \"tsteps\".\n Both \"input_len\" and \"tsteps\" are defined in the \"editable parameters\" section.\n \n@@ -15,12 +16,14 @@ This memory length is controlled by the \"lahead\" variable (more details below).\n \n The rest of the parameters are:\n - input_len: the length of the generated input sequence\n-- lahead: the input sequence length that the LSTM is trained on for each output point\n+- lahead: the input sequence length that the LSTM\n+  is trained on for each output point\n - batch_size, epochs: same parameters as in the model.fit(...) function\n \n When lahead > 1, the model input is preprocessed to a \"rolling window view\"\n of the data, with the window length = \"lahead\".\n-This is similar to sklearn's \"view_as_windows\" with \"window_shape\" being a single number\n+This is similar to sklearn's \"view_as_windows\"\n+with \"window_shape\" being a single number\n Ref: http://scikit-image.org/docs/0.10.x/api/skimage.util.html#view-as-windows\n \n When lahead < tsteps, only the stateful LSTM converges because its\n\n@@ -1,4 +1,3 @@\n-#!/usr/bin/env python\n # -*- coding: utf-8 -*-\n \"\"\"\n Train an Auxiliary Classifier Generative Adversarial Network (ACGAN) on the\n\n@@ -1,26 +1,32 @@\n-\"\"\"This is an example of using Hierarchical RNN (HRNN) to classify MNIST digits.\n+\"\"\"Example of using Hierarchical RNN (HRNN) to classify MNIST digits.\n \n-HRNNs can learn across multiple levels of temporal hierarchy over a complex sequence.\n-Usually, the first recurrent layer of an HRNN encodes a sentence (e.g. of word vectors)\n-into a  sentence vector. The second recurrent layer then encodes a sequence of\n-such vectors (encoded by the first layer) into a document vector. This\n-document vector is considered to preserve both the word-level and\n-sentence-level structure of the context.\n+HRNNs can learn across multiple levels\n+of temporal hierarchy over a complex sequence.\n+Usually, the first recurrent layer of an HRNN\n+encodes a sentence (e.g. of word vectors)\n+into a  sentence vector.\n+The second recurrent layer then encodes a sequence of\n+such vectors (encoded by the first layer) into a document vector.\n+This document vector is considered to preserve both\n+the word-level and sentence-level structure of the context.\n \n # References\n+\n - [A Hierarchical Neural Autoencoder for Paragraphs and Documents](https://arxiv.org/abs/1506.01057)\n     Encodes paragraphs and documents with HRNN.\n     Results have shown that HRNN outperforms standard\n     RNNs and may play some role in more sophisticated generation tasks like\n     summarization or question answering.\n - [Hierarchical recurrent neural network for skeleton based action recognition](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7298714)\n-        Achieved state-of-the-art results on skeleton based action recognition with 3 levels\n+    Achieved state-of-the-art results on\n+    skeleton based action recognition with 3 levels\n     of bidirectional HRNN combined with fully connected layers.\n \n In the below MNIST example the first LSTM layer first encodes every\n-column of pixels of shape (28, 1) to a column vector of shape (128,). The second LSTM\n-layer encodes then these 28 column vectors of shape (28, 128) to a image vector\n-representing the whole image. A final Dense layer is added for prediction.\n+column of pixels of shape (28, 1) to a column vector of shape (128,).\n+The second LSTM layer encodes then these 28 column vectors of shape (28, 128)\n+to a image vector representing the whole image.\n+A final Dense layer is added for prediction.\n \n After 5 epochs: train acc: 0.9858, val acc: 0.9864\n \"\"\"\n\n@@ -5,7 +5,8 @@ by Tianqi Chen, Ian Goodfellow, and Jonathon Shlens\n arXiv:1511.05641v4 [cs.LG] 23 Apr 2016\n http://arxiv.org/abs/1511.05641\n \n-Notes\n+# Notes\n+\n - What:\n   + Net2Net is a group of methods to transfer knowledge from a teacher neural\n     net to a student net,so that the student net can be trained faster than\n@@ -24,7 +25,8 @@ Notes\n   + Enable 'lifelong learning system' by gradually adjusting model complexity\n     to data availability,and reusing transferable knowledge.\n \n-Experiments\n+# Experiments\n+\n - Teacher model: a basic CNN model trained on MNIST for 3 epochs.\n - Net2WiderNet experiment:\n   + Student model has a wider Conv2D layer and a wider FC layer.\n@@ -43,7 +45,8 @@ Experiments\n     and thus enable full capacity of student models. It is optional\n     when a Dropout layer is used.\n \n-Results\n+# Results\n+\n - Tested with TF backend and 'channels_last' image_data_format.\n - Running on GPU GeForce GTX Titan X Maxwell\n - Performance Comparisons - validation loss values during first 3 epochs:\n\n@@ -1,10 +1,12 @@\n-'''Train a Siamese MLP on pairs of digits from the MNIST dataset.\n+'''Trains a Siamese MLP on pairs of digits from the MNIST dataset.\n \n It follows Hadsell-et-al.'06 [1] by computing the Euclidean distance on the\n output of the shared network and by optimizing the contrastive loss (see paper\n for mode details).\n \n-[1] \"Dimensionality Reduction by Learning an Invariant Mapping\"\n+# References\n+\n+- Dimensionality Reduction by Learning an Invariant Mapping\n     http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n \n Gets to 97.2% test accuracy after 20 epochs.\n\n@@ -9,14 +9,12 @@ input image.  Therefore, if the 'where' is handed from the encoder\n to the corresponding decoder layer, features being decoded can be 'placed' in\r\n the right location, allowing for reconstructions of much higher fidelity.\r\n \r\n-References:\r\n-[1]\r\n-'Visualizing and Understanding Convolutional Networks'\r\n+# References\r\n+\r\n+- Visualizing and Understanding Convolutional Networks\r\n   Matthew D Zeiler, Rob Fergus\r\n   https://arxiv.org/abs/1311.2901v3\r\n-\r\n-[2]\r\n-'Stacked What-Where Auto-encoders'\r\n+- Stacked What-Where Auto-encoders\r\n   Junbo Zhao, Michael Mathieu, Ross Goroshin, Yann LeCun\r\n   https://arxiv.org/abs/1506.02351v8\r\n \r\n@@ -32,17 +30,13 @@ its output is linear or non-linear is determined by the data it is fed.\n However, in order to cap the reconstruction in this example, a hard softmax is\r\n applied as a bias because we know the MNIST digits are mapped to [0, 1].\r\n \r\n-References:\r\n-[3]\r\n-'Deep Residual Learning for Image Recognition'\r\n+# References\r\n+- Deep Residual Learning for Image Recognition\r\n   Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\r\n   https://arxiv.org/abs/1512.03385v1\r\n-\r\n-[4]\r\n-'Identity Mappings in Deep Residual Networks'\r\n+- Identity Mappings in Deep Residual Networks\r\n   Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\r\n   https://arxiv.org/abs/1603.05027v3\r\n-\r\n '''\r\n from __future__ import print_function\r\n import numpy as np\r\n\n@@ -1,11 +1,9 @@\n-'''Transfer learning toy example:\n+'''Transfer learning toy example.\n \n 1 - Train a simple convnet on the MNIST dataset the first 5 digits [0..4].\n 2 - Freeze convolutional layers and fine-tune dense layers\n    for the classification of digits [5..9].\n \n-Run on GPU: THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python mnist_transfer_cnn.py\n-\n Get to 99.8% test accuracy after 5 epochs\n for the first five digits classifier\n and 99.2% for the last five digits after transfer + fine-tuning.\n\n@@ -1,7 +1,8 @@\n '''Neural doodle with Keras\r\n \r\n-Script Usage:\r\n-    # Arguments:\r\n+# Script Usage\r\n+\r\n+## Arguments\r\n ```\r\n --nlabels:              # of regions (colors) in mask images\r\n --style-image:          image to learn style from\r\n@@ -11,7 +12,7 @@ Script Usage:\n --target-image-prefix:  path prefix for generated target images\r\n ```\r\n \r\n-    # Example 1: doodle using a style image, style mask\r\n+## Example 1: doodle using a style image, style mask\r\n and target mask.\r\n ```\r\n python neural_doodle.py --nlabels 4 --style-image Monet/style.png \\\r\n@@ -19,7 +20,7 @@ Script Usage:\n --target-image-prefix generated/monet\r\n ```\r\n \r\n-    # Example 2: doodle using a style image, style mask,\r\n+## Example 2: doodle using a style image, style mask,\r\n target mask and an optional content image.\r\n ```\r\n python neural_doodle.py --nlabels 4 --style-image Renoir/style.png \\\r\n@@ -28,14 +29,16 @@ Script Usage:\n --target-image-prefix generated/renoir\r\n ```\r\n \r\n-References:\r\n-[Dmitry Ulyanov's blog on fast-neural-doodle](http://dmitryulyanov.github.io/feed-forward-neural-doodle/)\r\n-[Torch code for fast-neural-doodle](https://github.com/DmitryUlyanov/fast-neural-doodle)\r\n-[Torch code for online-neural-doodle](https://github.com/DmitryUlyanov/online-neural-doodle)\r\n-[Paper Texture Networks: Feed-forward Synthesis of Textures and Stylized Images](http://arxiv.org/abs/1603.03417)\r\n-[Discussion on parameter tuning](https://github.com/fchollet/keras/issues/3705)\r\n+# References\r\n+\r\n+- [Dmitry Ulyanov's blog on fast-neural-doodle](http://dmitryulyanov.github.io/feed-forward-neural-doodle/)\r\n+- [Torch code for fast-neural-doodle](https://github.com/DmitryUlyanov/fast-neural-doodle)\r\n+- [Torch code for online-neural-doodle](https://github.com/DmitryUlyanov/online-neural-doodle)\r\n+- [Paper Texture Networks: Feed-forward Synthesis of Textures and Stylized Images](http://arxiv.org/abs/1603.03417)\r\n+- [Discussion on parameter tuning](https://github.com/fchollet/keras/issues/3705)\r\n+\r\n+# Resources\r\n \r\n-Resources:\r\n Example images can be downloaded from\r\n https://github.com/DmitryUlyanov/fast-neural-doodle/tree/master/data\r\n '''\r\n\n@@ -4,8 +4,9 @@ Compares the performance of a simple MLP using two\n different activation functions: RELU and SELU\n on the Reuters newswire topic classification task.\n \n-# Reference:\n-  Klambauer, G., Unterthiner, T., Mayr, A., & Hochreiter, S. (2017).\n+# Reference\n+\n+- Klambauer, G., Unterthiner, T., Mayr, A., & Hochreiter, S. (2017).\n   Self-Normalizing Neural Networks. arXiv preprint arXiv:1706.02515.\n   https://arxiv.org/abs/1706.02515\n '''\n\n@@ -1,6 +1,9 @@\n '''This script demonstrates how to build a variational autoencoder with Keras.\n \n-Reference: \"Auto-Encoding Variational Bayes\" https://arxiv.org/abs/1312.6114\n+ #Reference\n+\n+ - Auto-Encoding Variational Bayes\n+   https://arxiv.org/abs/1312.6114\n '''\n import numpy as np\n import matplotlib.pyplot as plt\n\n@@ -1,7 +1,10 @@\n '''This script demonstrates how to build a variational autoencoder\n with Keras and deconvolution layers.\n \n-Reference: \"Auto-Encoding Variational Bayes\" https://arxiv.org/abs/1312.6114\n+# Reference\n+\n+- Auto-Encoding Variational Bayes\n+  https://arxiv.org/abs/1312.6114\n '''\n import numpy as np\n import matplotlib.pyplot as plt\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#eec5da20be0eddb926a74f2c8ef6e690ccf98336", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 68 | Lines Deleted: 14 | Files Changed: 4 | Hunks: 17 | Methods Changed: 14 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 82 | Churn Cumulative: 6915 | Contributors (this commit): 75 | Commits (past 90d): 39 | Contributors (cumulative): 97 | DMM Complexity: 1.0\n\nDIFF:\n@@ -273,13 +273,13 @@ def _check_loss_and_target_compatibility(targets, loss_fns, output_shapes):\n         ValueError: if a loss function or target array\n             is incompatible with an output.\n     \"\"\"\n-    key_losses = {'mean_squared_error',\n-                  'binary_crossentropy',\n-                  'categorical_crossentropy'}\n+    key_losses = {losses.mean_squared_error,\n+                  losses.binary_crossentropy,\n+                  losses.categorical_crossentropy}\n     for y, loss, shape in zip(targets, loss_fns, output_shapes):\n         if loss is None:\n             continue\n-        if loss.__name__ == 'categorical_crossentropy':\n+        if loss is losses.categorical_crossentropy:\n             if y.shape[-1] == 1:\n                 raise ValueError(\n                     'You are passing a target array of shape ' + str(y.shape) +\n@@ -297,7 +297,7 @@ def _check_loss_and_target_compatibility(targets, loss_fns, output_shapes):\n                     'Alternatively, you can use the loss function '\n                     '`sparse_categorical_crossentropy` instead, '\n                     'which does expect integer targets.')\n-        if loss.__name__ in key_losses:\n+        if loss in key_losses:\n             for target_dim, out_dim in zip(y.shape[1:], shape[1:]):\n                 if out_dim is not None and target_dim != out_dim:\n                     raise ValueError(\n@@ -1396,10 +1396,8 @@ class Model(Container):\n \n         output_shapes = []\n         for output_shape, loss_fn in zip(self._feed_output_shapes, self._feed_loss_fns):\n-            if loss_fn.__name__ == 'sparse_categorical_crossentropy':\n+            if loss_fn is losses.sparse_categorical_crossentropy:\n                 output_shapes.append(output_shape[:-1] + (1,))\n-            elif getattr(losses, loss_fn.__name__, None) is None:\n-                output_shapes.append(None)\n             else:\n                 output_shapes.append(output_shape)\n         x = _standardize_input_data(x, self._feed_input_names,\n\n@@ -2,6 +2,7 @@ from __future__ import absolute_import\n import six\n from . import backend as K\n from .utils.generic_utils import deserialize_keras_object\n+from .utils.generic_utils import serialize_keras_object\n \n \n # noinspection SpellCheckingInspection\n@@ -41,6 +42,15 @@ def categorical_hinge(y_true, y_pred):\n \n \n def logcosh(y_true, y_pred):\n+    \"\"\"Logarithm of the hyperbolic cosine of the prediction error.\n+\n+    `log(cosh(x))` is approximately equal to `(x ** 2) / 2` for small `x` and\n+    to `abs(x) - log(2)` for large `x`. This means that 'logcosh' works mostly\n+    like the mean squared error, but will not be so strongly affected by the\n+    occasional wildly incorrect prediction. However, it may return NaNs if the\n+    intermediate value `cosh(y_pred - y_true)` is too large to be represented\n+    in the chosen precision.\n+    \"\"\"\n     def cosh(x):\n         return (K.exp(x) + K.exp(-x)) / 2\n     return K.mean(K.log(cosh(y_pred - y_true)), axis=-1)\n@@ -85,7 +95,7 @@ cosine = cosine_proximity\n \n \n def serialize(loss):\n-    return loss.__name__\n+    return serialize_keras_object(loss)\n \n \n def deserialize(name, custom_objects=None):\n@@ -101,6 +111,8 @@ def get(identifier):\n     if isinstance(identifier, six.string_types):\n         identifier = str(identifier)\n         return deserialize(identifier)\n+    if isinstance(identifier, dict):\n+        return deserialize(identifier)\n     elif callable(identifier):\n         return identifier\n     else:\n\n@@ -6,6 +6,7 @@ import sys\n import scipy.sparse as sparse\n \n import keras\n+from keras import losses\n from keras.layers import Dense, Dropout\n from keras.engine.topology import Input\n from keras.engine.training import Model\n@@ -86,7 +87,7 @@ def test_weighted_masked_objective():\n     def mask_dummy(y_true=None, y_pred=None, weight=None):\n         return K.placeholder(y_true.shape)\n \n-    weighted_function = _weighted_masked_objective(K.categorical_crossentropy)\n+    weighted_function = _weighted_masked_objective(losses.categorical_crossentropy)\n     weighted_function(a, a, None)\n \n \n@@ -470,15 +471,15 @@ def test_trainable_argument():\n @keras_test\n def test_check_not_failing():\n     a = np.random.random((2, 1, 3))\n-    _check_loss_and_target_compatibility([a], [K.categorical_crossentropy], [a.shape])\n-    _check_loss_and_target_compatibility([a], [K.categorical_crossentropy], [(2, None, 3)])\n+    _check_loss_and_target_compatibility([a], [losses.categorical_crossentropy], [a.shape])\n+    _check_loss_and_target_compatibility([a], [losses.categorical_crossentropy], [(2, None, 3)])\n \n \n @keras_test\n def test_check_last_is_one():\n     a = np.random.random((2, 3, 1))\n     with pytest.raises(ValueError) as exc:\n-        _check_loss_and_target_compatibility([a], [K.categorical_crossentropy], [a.shape])\n+        _check_loss_and_target_compatibility([a], [losses.categorical_crossentropy], [a.shape])\n \n     assert 'You are passing a target array' in str(exc)\n \n@@ -487,7 +488,7 @@ def test_check_last_is_one():\n def test_check_bad_shape():\n     a = np.random.random((2, 3, 5))\n     with pytest.raises(ValueError) as exc:\n-        _check_loss_and_target_compatibility([a], [K.categorical_crossentropy], [(2, 3, 6)])\n+        _check_loss_and_target_compatibility([a], [losses.categorical_crossentropy], [(2, 3, 6)])\n \n     assert 'targets to have the same shape' in str(exc)\n \n\n@@ -1,8 +1,10 @@\n import pytest\n import numpy as np\n \n+import keras\n from keras import losses\n from keras import backend as K\n+from keras.utils.generic_utils import custom_object_scope\n \n \n allobj = [losses.mean_squared_error,\n@@ -66,5 +68,46 @@ def test_sparse_categorical_crossentropy():\n     assert np.isclose(expected_loss, np.mean(loss))\n \n \n+class MSE_MAE_loss:\n+    \"\"\"Loss function with internal state, for testing serialization code.\"\"\"\n+    def __init__(self, mse_fraction):\n+        self.mse_fraction = mse_fraction\n+\n+    def __call__(self, y_true, y_pred):\n+        return (self.mse_fraction * losses.mse(y_true, y_pred) +\n+                (1 - self.mse_fraction) * losses.mae(y_true, y_pred))\n+\n+    def get_config(self):\n+        return {'mse_fraction': self.mse_fraction}\n+\n+\n+def test_serializing_loss_class():\n+    orig_loss_class = MSE_MAE_loss(0.3)\n+    with custom_object_scope({'MSE_MAE_loss': MSE_MAE_loss}):\n+        serialized = losses.serialize(orig_loss_class)\n+\n+    with custom_object_scope({'MSE_MAE_loss': MSE_MAE_loss}):\n+        deserialized = losses.deserialize(serialized)\n+    assert isinstance(deserialized, MSE_MAE_loss)\n+    assert deserialized.mse_fraction == 0.3\n+\n+\n+def test_serializing_model_with_loss_class(tmpdir):\n+    model_filename = str(tmpdir / 'custom_loss.hdf')\n+\n+    with custom_object_scope({'MSE_MAE_loss': MSE_MAE_loss}):\n+        loss = MSE_MAE_loss(0.3)\n+        inputs = keras.layers.Input((2,))\n+        outputs = keras.layers.Dense(1, name='model_output')(inputs)\n+        model = keras.models.Model(inputs, outputs)\n+        model.compile(optimizer='sgd', loss={'model_output': loss})\n+        model.fit(np.random.rand(256, 2), np.random.rand(256, 1))\n+        model.save(model_filename)\n+\n+    with custom_object_scope({'MSE_MAE_loss': MSE_MAE_loss}):\n+        loaded_model = keras.models.load_model(model_filename)\n+        loaded_model.predict(np.random.rand(128, 2))\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b285d710f6f3ae8478b9a17e86c6b6aabbd22494", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 5 | Files Changed: 4 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 16230 | Contributors (this commit): 108 | Commits (past 90d): 61 | Contributors (cumulative): 144 | DMM Complexity: 1.0\n\nDIFF:\n@@ -98,6 +98,7 @@ EXCLUDE = {\n     'get',\n     'set_image_dim_ordering',\n     'image_dim_ordering',\n+    'get_variable_shape',\n }\n \n PAGES = [\n\n@@ -493,13 +493,13 @@ def ones_like(x, dtype=None, name=None):\n \n \n def count_params(x):\n-    for _ in get_variable_shape(x):\n+    for _ in x.shape:\n         if _ == C.InferredDimension or _ == C.FreeDimension:\n             raise ValueError('CNTK backend: `count_params` with dynamic '\n                              'shape is not supported. Please provide '\n                              'fixed dimension instead of `None`.')\n \n-    return np.prod(get_variable_shape(x))\n+    return np.prod(int_shape(x))\n \n \n def cast(x, dtype):\n@@ -887,7 +887,7 @@ def binary_crossentropy(target, output, from_logits=False):\n \n \n def get_variable_shape(x):\n-    return x.shape\n+    return int_shape(x)\n \n \n def update(x, new_x):\n\n@@ -889,7 +889,7 @@ def count_params(x):\n                [ 0.,  0.,  0.]], dtype=float32)\n     ```\n     \"\"\"\n-    return np.prod(get_variable_shape(x))\n+    return np.prod(int_shape(x))\n \n \n def cast(x, dtype):\n\n@@ -349,7 +349,8 @@ class TestBackend(object):\n \n     def test_value_manipulation(self):\n         val = np.random.random((4, 2))\n-        for function_name in ['get_value', 'count_params', 'get_variable_shape']:\n+        for function_name in ['get_value', 'count_params',\n+                              'int_shape', 'get_variable_shape']:\n             v_list = [getattr(k, function_name)(k.variable(val))\n                       for k in BACKENDS]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a04284341c26761d4b1a92f3245a535b2b2eaad4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 179 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -58,8 +58,11 @@ def multi_gpu_model(model, gpus):\n         width = 224\n         num_classes = 1000\n \n-        # Instantiate the base model\n-        # (here, we do it on CPU, which is optional).\n+        # Instantiate the base model (or \"template\" model).\n+        # We recommend doing this with under a CPU device scope,\n+        # so that the model's weights are hosted on CPU memory.\n+        # Otherwise they may end up hosted on a GPU, which would\n+        # complicate weight sharing.\n         with tf.device('/cpu:0'):\n             model = Xception(weights=None,\n                              input_shape=(height, width, 3),\n@@ -78,7 +81,16 @@ def multi_gpu_model(model, gpus):\n         # This `fit` call will be distributed on 8 GPUs.\n         # Since the batch size is 256, each GPU will process 32 samples.\n         parallel_model.fit(x, y, epochs=20, batch_size=256)\n+\n+        # Save model via the template model (which shares the same weights):\n+        model.save('my_model.h5')\n     ```\n+\n+    # On model saving\n+\n+    To save the multi-gpu model, use `.save(fname)` or `.save_weights(fname)`\n+    with the template model (the argument you passed to `multi_gpu_model),\n+    rather than the model returned by `multi_gpu_model`.\n     \"\"\"\n     if K.backend() != 'tensorflow':\n         raise ValueError('`multi_gpu_model` is only available '\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a375cfead032ff4c211a28e5ab56502af3e705e0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 272 | Lines Deleted: 64 | Files Changed: 1 | Hunks: 34 | Methods Changed: 4 | Complexity Δ (Sum/Max): 24/24 | Churn Δ: 336 | Churn Cumulative: 632 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: 0.30952380952380953\n\nDIFF:\n@@ -1,20 +1,21 @@\n \"\"\"Trains a ResNet on the CIFAR10 dataset.\n \n-Greater than 91% test accuracy (0.52 val_loss) after 50 epochs\n-48sec per epoch on GTX 1080Ti\n-\n-# Reference\n-\n-- Deep Residual Learning for Image Recognition\n+ResNet v1\n+[a] Deep Residual Learning for Image Recognition\n https://arxiv.org/pdf/1512.03385.pdf\n+\n+ResNet v2\n+[b] Identity Mappings in Deep Residual Networks\n+https://arxiv.org/pdf/1603.05027.pdf\n \"\"\"\n \n from __future__ import print_function\n import keras\n from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n-from keras.layers import MaxPooling2D, AveragePooling2D, Input, Flatten\n+from keras.layers import AveragePooling2D, Input, Flatten\n from keras.optimizers import Adam\n-from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n+from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n+from keras.callbacks import ReduceLROnPlateau\n from keras.preprocessing.image import ImageDataGenerator\n from keras.regularizers import l2\n from keras import backend as K\n@@ -23,17 +24,38 @@ from keras.datasets import cifar10\n import numpy as np\n import os\n \n-# Training params.\n+# Training parameters\n batch_size = 32\n-epochs = 100\n+epochs = 200\n data_augmentation = True\n-\n-# Network architecture params.\n num_classes = 10\n-num_filters = 64\n-num_blocks = 4\n-num_sub_blocks = 2\n-use_max_pool = False\n+\n+# Subtracting pixel mean improves accuracy\n+subtract_pixel_mean = True\n+\n+# Model parameter\n+# ----------------------------------------------------------------------------\n+#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n+# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1090Ti\n+#           |      | %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n+# ----------------------------------------------------------------------------\n+# ResNet20  |  3   | 92.16     | 91.25     | -----     | NA        | 35\n+# ResNet32  |  5   | 92.46     | 92.49     | -----     | NA        | 50\n+# ResNet44  |  7   | 92.50     | 92.83     | -----     | NA        | 70\n+# ResNet56  |  9   | 92.71     | 93.03     | 92.60     | NA        | 90 (100)\n+# ResNet110 |  18  | 92.65     | 93.39     | 93.03     | 93.63     | 165(180)\n+# ---------------------------------------------------------------------------\n+n = 3\n+\n+# Model version\n+# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n+version = 1\n+\n+# Computed depth from supplied model parameter n\n+depth = n * 6 + 2\n+\n+# Model name, depth and version\n+model_type = 'ResNet%d v%d' % (depth, version)\n \n # Load the CIFAR10 data.\n (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n@@ -62,6 +84,13 @@ else:\n # Normalize data.\n x_train = x_train.astype('float32') / 255\n x_test = x_test.astype('float32') / 255\n+\n+# If subtract pixel mean is enabled\n+if subtract_pixel_mean:\n+    x_train_mean = np.mean(x_train, axis=0)\n+    x_train -= x_train_mean\n+    x_test -= x_train_mean\n+\n print('x_train shape:', x_train.shape)\n print(x_train.shape[0], 'train samples')\n print(x_test.shape[0], 'test samples')\n@@ -71,87 +100,255 @@ print('y_train shape:', y_train.shape)\n y_train = keras.utils.to_categorical(y_train, num_classes)\n y_test = keras.utils.to_categorical(y_test, num_classes)\n \n-# Start model definition.\n-inputs = Input(shape=input_shape)\n+\n+def lr_schedule(epoch):\n+    \"\"\"Learning Rate Schedule\n+\n+    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n+    Called automatically every epoch as part of callbacks during training.\n+\n+    # Arguments\n+        epoch (int): The number of epochs\n+\n+    # Returns\n+        lr (float32): learning rate\n+    \"\"\"\n+    lr = 1e-3\n+    if epoch > 180:\n+        lr *= 0.5e-3\n+    elif epoch > 160:\n+        lr *= 1e-3\n+    elif epoch > 120:\n+        lr *= 1e-2\n+    elif epoch > 80:\n+        lr *= 1e-1\n+    print('Learning rate: ', lr)\n+    return lr\n+\n+\n+def resnet_block(inputs,\n+                 num_filters=16,\n+                 kernel_size=3,\n+                 strides=1,\n+                 activation='relu',\n+                 conv_first=True):\n+    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n+\n+    # Arguments\n+        inputs (tensor): input tensor from input image or previous layer\n+        num_filters (int): Conv2D number of filters\n+        kernel_size (int): Conv2D square kernel dimensions\n+        strides (int): Conv2D square stride dimensions\n+        activation (string): activation name\n+        conv_first (bool): conv-bn-activation (True) or\n+            activation-bn-conv (False)\n+\n+    # Returns\n+        x (tensor): tensor as input to the next layer\n+    \"\"\"\n+    if conv_first:\n         x = Conv2D(num_filters,\n-           kernel_size=7,\n+                   kernel_size=kernel_size,\n+                   strides=strides,\n                    padding='same',\n-           strides=2,\n                    kernel_initializer='he_normal',\n                    kernel_regularizer=l2(1e-4))(inputs)\n         x = BatchNormalization()(x)\n+        if activation:\n+            x = Activation(activation)(x)\n+        return x\n+    x = BatchNormalization()(inputs)\n+    if activation:\n         x = Activation('relu')(x)\n+    x = Conv2D(num_filters,\n+               kernel_size=kernel_size,\n+               strides=strides,\n+               padding='same',\n+               kernel_initializer='he_normal',\n+               kernel_regularizer=l2(1e-4))(x)\n+    return x\n \n-# Orig paper uses max pool after 1st conv.\n-# Reaches up 87% acc if use_max_pool = True.\n-# Cifar10 images are already too small at 32x32 to be maxpooled. So, we skip.\n-if use_max_pool:\n-    x = MaxPooling2D(pool_size=3, strides=2, padding='same')(x)\n-    num_blocks = 3\n \n+def resnet_v1(input_shape, depth, num_classes=10):\n+    \"\"\"ResNet Version 1 Model builder [a]\n+\n+    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n+    Last ReLU is after the shortcut connection.\n+    The number of filters doubles when the feature maps size\n+    is halved.\n+    The Number of parameters is approx the same as Table 6 of [a]:\n+    ResNet20 0.27M\n+    ResNet32 0.46M\n+    ResNet44 0.66M\n+    ResNet56 0.85M\n+    ResNet110 1.7M\n+\n+    # Arguments\n+        input_shape (tensor): shape of input image tensor\n+        depth (int): number of core convolutional layers\n+        num_classes (int): number of classes (CIFAR10 has 10)\n+\n+    # Returns\n+        model (Model): Keras model instance\n+    \"\"\"\n+    if (depth - 2) % 6 != 0:\n+        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n+    # Start model definition.\n+    inputs = Input(shape=input_shape)\n+    num_filters = 16\n+    num_sub_blocks = int((depth - 2) / 6)\n+\n+    x = resnet_block(inputs=inputs)\n     # Instantiate convolutional base (stack of blocks).\n-for i in range(num_blocks):\n+    for i in range(3):\n         for j in range(num_sub_blocks):\n             strides = 1\n             is_first_layer_but_not_first_block = j == 0 and i > 0\n             if is_first_layer_but_not_first_block:\n                 strides = 2\n-        y = Conv2D(num_filters,\n-                   kernel_size=3,\n-                   padding='same',\n-                   strides=strides,\n-                   kernel_initializer='he_normal',\n-                   kernel_regularizer=l2(1e-4))(x)\n-        y = BatchNormalization()(y)\n-        y = Activation('relu')(y)\n-        y = Conv2D(num_filters,\n-                   kernel_size=3,\n-                   padding='same',\n-                   kernel_initializer='he_normal',\n-                   kernel_regularizer=l2(1e-4))(y)\n-        y = BatchNormalization()(y)\n+            y = resnet_block(inputs=x,\n+                             num_filters=num_filters,\n+                             strides=strides)\n+            y = resnet_block(inputs=y,\n+                             num_filters=num_filters,\n+                             activation=None)\n             if is_first_layer_but_not_first_block:\n-            x = Conv2D(num_filters,\n+                x = resnet_block(inputs=x,\n+                                 num_filters=num_filters,\n                                  kernel_size=1,\n-                       padding='same',\n-                       strides=2,\n-                       kernel_initializer='he_normal',\n-                       kernel_regularizer=l2(1e-4))(x)\n+                                 strides=strides,\n+                                 activation=None)\n             x = keras.layers.add([x, y])\n             x = Activation('relu')(x)\n-\n         num_filters = 2 * num_filters\n \n     # Add classifier on top.\n-x = AveragePooling2D()(x)\n+    # v1 does not use BN after last shortcut connection-ReLU\n+    x = AveragePooling2D(pool_size=8)(x)\n     y = Flatten()(x)\n     outputs = Dense(num_classes,\n                     activation='softmax',\n                     kernel_initializer='he_normal')(y)\n \n-# Instantiate and compile model.\n+    # Instantiate model.\n     model = Model(inputs=inputs, outputs=outputs)\n+    return model\n+\n+\n+def resnet_v2(input_shape, depth, num_classes=10):\n+    \"\"\"ResNet Version 2 Model builder [b]\n+\n+    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n+    bottleneck layer\n+    First shortcut connection per layer is 1 x 1 Conv2D.\n+    Second and onwards shortcut connection is identity.\n+    Features maps sizes: 16(input), 64(1st sub_block), 128(2nd), 256(3rd)\n+\n+    # Arguments\n+        input_shape (tensor): shape of input image tensor\n+        depth (int): number of core convolutional layers\n+        num_classes (int): number of classes (CIFAR10 has 10)\n+\n+    # Returns\n+        model (Model): Keras model instance\n+    \"\"\"\n+    if (depth - 2) % 9 != 0:\n+        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n+    # Start model definition.\n+    inputs = Input(shape=input_shape)\n+    num_filters_in = 16\n+    num_filters_out = 64\n+    filter_multiplier = 4\n+    num_sub_blocks = int((depth - 2) / 9)\n+\n+    # v2 performs Conv2D on input w/o BN-ReLU\n+    x = Conv2D(num_filters_in,\n+               kernel_size=3,\n+               padding='same',\n+               kernel_initializer='he_normal',\n+               kernel_regularizer=l2(1e-4))(inputs)\n+\n+    # Instantiate convolutional base (stack of blocks).\n+    for i in range(3):\n+        if i > 0:\n+            filter_multiplier = 2\n+        num_filters_out = num_filters_in * filter_multiplier\n+\n+        for j in range(num_sub_blocks):\n+            strides = 1\n+            is_first_layer_but_not_first_block = j == 0 and i > 0\n+            if is_first_layer_but_not_first_block:\n+                strides = 2\n+            y = resnet_block(inputs=x,\n+                             num_filters=num_filters_in,\n+                             kernel_size=1,\n+                             strides=strides,\n+                             conv_first=False)\n+            y = resnet_block(inputs=y,\n+                             num_filters=num_filters_in,\n+                             conv_first=False)\n+            y = resnet_block(inputs=y,\n+                             num_filters=num_filters_out,\n+                             kernel_size=1,\n+                             conv_first=False)\n+            if j == 0:\n+                x = Conv2D(num_filters_out,\n+                           kernel_size=1,\n+                           strides=strides,\n+                           padding='same',\n+                           kernel_initializer='he_normal',\n+                           kernel_regularizer=l2(1e-4))(x)\n+            x = keras.layers.add([x, y])\n+\n+        num_filters_in = num_filters_out\n+\n+    # Add classifier on top.\n+    # v2 has BN-ReLU before Pooling\n+    x = BatchNormalization()(x)\n+    x = Activation('relu')(x)\n+    x = AveragePooling2D(pool_size=8)(x)\n+    y = Flatten()(x)\n+    outputs = Dense(num_classes,\n+                    activation='softmax',\n+                    kernel_initializer='he_normal')(y)\n+\n+    # Instantiate model.\n+    model = Model(inputs=inputs, outputs=outputs)\n+    return model\n+\n+\n+if version == 2:\n+    model = resnet_v2(input_shape=input_shape, depth=depth)\n+else:\n+    model = resnet_v1(input_shape=input_shape, depth=depth)\n+\n model.compile(loss='categorical_crossentropy',\n-              optimizer=Adam(),\n+              optimizer=Adam(lr=lr_schedule(0)),\n               metrics=['accuracy'])\n model.summary()\n+print(model_type)\n \n # Prepare model model saving directory.\n save_dir = os.path.join(os.getcwd(), 'saved_models')\n-model_name = 'cifar10_resnet_model.h5'\n+model_name = 'cifar10_resnet_model.{epoch:02d}.h5'\n if not os.path.isdir(save_dir):\n     os.makedirs(save_dir)\n filepath = os.path.join(save_dir, model_name)\n \n-# Prepare callbacks for model saving and for learning rate decaying.\n+# Prepare callbacks for model saving and for learning rate adjustment.\n checkpoint = ModelCheckpoint(filepath=filepath,\n+                             monitor='val_acc',\n                              verbose=1,\n                              save_best_only=True)\n+\n+lr_scheduler = LearningRateScheduler(lr_schedule)\n+\n lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n                                cooldown=0,\n                                patience=5,\n                                min_lr=0.5e-6)\n-callbacks = [checkpoint, lr_reducer]\n+\n+callbacks = [checkpoint, lr_reducer, lr_scheduler]\n \n # Run training, with or without data augmentation.\n if not data_augmentation:\n@@ -166,24 +363,35 @@ else:\n     print('Using real-time data augmentation.')\n     # This will do preprocessing and realtime data augmentation:\n     datagen = ImageDataGenerator(\n-        featurewise_center=False,  # set input mean to 0 over the dataset\n-        samplewise_center=False,  # set each sample mean to 0\n-        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n-        samplewise_std_normalization=False,  # divide each input by its std\n-        zca_whitening=False,  # apply ZCA whitening\n-        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n-        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n-        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n-        horizontal_flip=True,  # randomly flip images\n-        vertical_flip=False)  # randomly flip images\n+        # set input mean to 0 over the dataset\n+        featurewise_center=False,\n+        # set each sample mean to 0\n+        samplewise_center=False,\n+        # divide inputs by std of dataset\n+        featurewise_std_normalization=False,\n+        # divide each input by its std\n+        samplewise_std_normalization=False,\n+        # apply ZCA whitening\n+        zca_whitening=False,\n+        # randomly rotate images in the range (deg 0 to 180)\n+        rotation_range=0,\n+        # randomly shift images horizontally\n+        width_shift_range=0.1,\n+        # randomly shift images vertically\n+        height_shift_range=0.1,\n+        # randomly flip images\n+        horizontal_flip=True,\n+        # randomly flip images\n+        vertical_flip=False)\n \n     # Compute quantities required for featurewise normalization\n     # (std, mean, and principal components if ZCA whitening is applied).\n     datagen.fit(x_train)\n \n     # Fit the model on the batches generated by datagen.flow().\n+    steps_per_epoch = int(np.ceil(x_train.shape[0] / float(batch_size)))\n     model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n-                        steps_per_epoch=int(np.ceil(x_train.shape[0] / float(batch_size))),\n+                        steps_per_epoch=steps_per_epoch,\n                         validation_data=(x_test, y_test),\n                         epochs=epochs, verbose=1, workers=4,\n                         callbacks=callbacks)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
