{"custom_id": "keras#10e49077a75d444e66f18c4b5c58b99b29da2037", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 61 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 8 | Methods Changed: 6 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 63 | Churn Cumulative: 846 | Contributors (this commit): 4 | Commits (past 90d): 43 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -33,7 +33,7 @@ class InputLayer(Layer):\n         if shape:\n             shape = backend.standardize_shape(shape)\n             batch_shape = (batch_size,) + shape\n-        self.batch_shape = batch_shape\n+        self.batch_shape = tuple(batch_shape)\n         self._dtype = backend.standardize_dtype(dtype)\n \n         if input_tensor is not None:\n\n@@ -1,3 +1,4 @@\n+import json\n import os\n import warnings\n \n@@ -341,11 +342,58 @@ class Model(Trainer, Layer):\n                 stacklevel=2,\n             )\n \n+    def to_json(self, **kwargs):\n+        \"\"\"Returns a JSON string containing the network configuration.\n+\n+        To load a network from a JSON save file, use\n+        `keras.models.model_from_json(json_string, custom_objects={...})`.\n+\n+        Args:\n+            **kwargs: Additional keyword arguments to be passed to\n+                `json.dumps()`.\n+\n+        Returns:\n+            A JSON string.\n+        \"\"\"\n+        from keras_core.saving import serialization_lib\n+\n+        model_config = serialization_lib.serialize_keras_object(self)\n+        return json.dumps(model_config, **kwargs)\n+\n     @traceback_utils.filter_traceback\n     def export(self, filepath):\n         raise NotImplementedError\n \n \n+@keras_core_export(\"keras_core.models.model_from_json\")\n+def model_from_json(json_string, custom_objects=None):\n+    \"\"\"Parses a JSON model configuration string and returns a model instance.\n+\n+    Usage:\n+\n+    >>> model = keras_core.Sequential([\n+    ...     keras_core.layers.Dense(5, input_shape=(3,)),\n+    ...     keras_core.layers.Softmax()])\n+    >>> config = model.to_json()\n+    >>> loaded_model = keras_core.models.model_from_json(config)\n+\n+    Args:\n+        json_string: JSON string encoding a model configuration.\n+        custom_objects: Optional dictionary mapping names\n+            (strings) to custom classes or functions to be\n+            considered during deserialization.\n+\n+    Returns:\n+        A Keras model instance (uncompiled).\n+    \"\"\"\n+    from keras_core.saving import serialization_lib\n+\n+    model_config = json.loads(json_string)\n+    return serialization_lib.deserialize_keras_object(\n+        model_config, custom_objects=custom_objects\n+    )\n+\n+\n def functional_init_arguments(args, kwargs):\n     return (\n         (len(args) == 2)\n\n@@ -3,14 +3,25 @@ from keras_core import testing\n from keras_core.layers.core.input_layer import Input\n from keras_core.models.functional import Functional\n from keras_core.models.model import Model\n+from keras_core.models.model import model_from_json\n \n \n class ModelTest(testing.TestCase):\n-    def test_functional_rerouting(self):\n+    def _get_model(self):\n         input_a = Input(shape=(3,), batch_size=2, name=\"input_a\")\n         input_b = Input(shape=(3,), batch_size=2, name=\"input_b\")\n         x = input_a + input_b\n         x = layers.Dense(5)(x)\n         outputs = layers.Dense(4)(x)\n         model = Model([input_a, input_b], outputs)\n+        return model\n+\n+    def test_functional_rerouting(self):\n+        model = self._get_model()\n         self.assertTrue(isinstance(model, Functional))\n+\n+    def test_json_serialization(self):\n+        model = self._get_model()\n+        json_string = model.to_json()\n+        new_model = model_from_json(json_string)\n+        self.assertEqual(json_string, new_model.to_json())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8c4ae4d9ff48221fab13953d081439b397c0855b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 707 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -658,7 +658,6 @@ def keras_model_summary(name, data, step=None):\n     with summary.experimental.summary_scope(\n         name, \"graph_keras_model\", [data, step]\n     ) as (tag, _):\n-        tensor = ops.convert_to_tensor(json_string, dtype=\"string\")\n         return summary.write(\n-            tag=tag, tensor=tensor, step=step, metadata=summary_metadata\n+            tag=tag, tensor=json_string, step=step, metadata=summary_metadata\n         )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ee226c85653740c924c6a6754dd4bc456713886d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 82 | Lines Deleted: 11 | Files Changed: 3 | Hunks: 11 | Methods Changed: 4 | Complexity Δ (Sum/Max): 8/6 | Churn Δ: 93 | Churn Cumulative: 508 | Contributors (this commit): 3 | Commits (past 90d): 10 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,5 @@\n import numpy as np\n+import torch\n from tensorflow import keras\n \n import keras_core\n@@ -81,10 +82,13 @@ def numerical_test():\n     keras_core_model = build_keras_model(keras_core, NUM_CLASSES)\n \n     # Make sure both model have same weights before training\n-    keras_core_model.set_weights(keras_model.weights)\n+    weights = [weight.numpy() for weight in keras_model.weights]\n+    keras_core_model.set_weights(weights)\n \n     for kw, kcw in zip(keras_model.weights, keras_core_model.weights):\n-        np.testing.assert_allclose(kw, kcw)\n+        if torch.is_tensor(kcw.value):\n+            kcw = kcw.value.detach().numpy()\n+        np.testing.assert_allclose(kw.numpy(), kcw)\n \n     keras_history = train_model(keras_model, x_train, y_train)\n     keras_core_history = train_model(keras_core_model, x_train, y_train)\n\n@@ -5,13 +5,19 @@ from keras_core import layers\n \n model = keras_core.Sequential(\n     [\n-        layers.Dense(1),\n+        layers.Dense(1, activation=\"sigmoid\"),\n     ]\n )\n-model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\"])\n+model.compile(\n+    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"mae\", \"accuracy\"]\n+)\n+x = np.random.rand(100, 10)\n+y = np.random.randint(0, 2, size=(100, 1))\n history = model.fit(\n-    x=np.random.rand(100, 10),\n-    y=np.random.rand(100, 1),\n+    x=x,\n+    y=y,\n     epochs=10,\n     shuffle=False,\n+    validation_data=(x, y),\n )\n+model.predict(x)\n\n@@ -1,5 +1,7 @@\n import warnings\n \n+import numpy as np\n+import tensorflow as tf\n import torch\n \n from keras_core import callbacks as callbacks_module\n@@ -187,11 +189,6 @@ class TorchTrainer(base_trainer.Trainer):\n         callbacks.on_train_end(logs=training_logs)\n         return self.history\n \n-    def predict(\n-        self, x, batch_size=None, verbose=\"auto\", steps=None, callbacks=None\n-    ):\n-        pass\n-\n     def test_step(self, data):\n         data = data[0]\n         x, y, sample_weight = data_adapter_utils.unpack_x_y_sample_weight(data)\n@@ -265,3 +262,67 @@ class TorchTrainer(base_trainer.Trainer):\n         if return_dict:\n             return logs\n         return self._flatten_metrics_in_order(logs)\n+\n+    def predict_step(self, data):\n+        data = data[0]\n+        x, _, _ = data_adapter_utils.unpack_x_y_sample_weight(data)\n+        if self._call_has_training_arg():\n+            y_pred = self(x, training=False)\n+        else:\n+            y_pred = self(x)\n+        return y_pred\n+\n+    def predict(\n+        self, x, batch_size=None, verbose=\"auto\", steps=None, callbacks=None\n+    ):\n+        # Create an iterator that yields batches of input data.\n+        epoch_iterator = EpochIterator(\n+            x=x,\n+            batch_size=batch_size,\n+            steps_per_epoch=steps,\n+            shuffle=False,\n+            steps_per_execution=self.steps_per_execution,\n+        )\n+\n+        # Container that configures and calls callbacks.\n+        if not isinstance(callbacks, callbacks_module.CallbackList):\n+            callbacks = callbacks_module.CallbackList(\n+                callbacks,\n+                add_history=True,\n+                add_progbar=verbose != 0,\n+                verbose=verbose,\n+                epochs=1,\n+                steps=epoch_iterator.num_batches,\n+                model=self,\n+            )\n+\n+        def append_to_outputs(batch_outputs, outputs):\n+            if outputs is None:\n+                outputs = tf.nest.map_structure(\n+                    lambda batch_output: [batch_output],\n+                    batch_outputs,\n+                )\n+            else:\n+                tf.__internal__.nest.map_structure_up_to(\n+                    batch_outputs,\n+                    lambda output, batch_output: output.append(batch_output),\n+                    outputs,\n+                    batch_outputs,\n+                )\n+            return outputs\n+\n+        # Switch the torch Module back to testing mode.\n+        self.eval()\n+\n+        callbacks.on_predict_begin()\n+        outputs = None\n+        for step, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n+            callbacks.on_predict_batch_begin(step)\n+            with torch.no_grad():\n+                batch_outputs = self.predict_step(data)\n+            outputs = append_to_outputs(batch_outputs, outputs)\n+            callbacks.on_predict_batch_end(step, {\"outputs\": batch_outputs})\n+        callbacks.on_predict_end()\n+        return tf.__internal__.nest.map_structure_up_to(\n+            batch_outputs, np.concatenate, outputs\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e7b9de2b89792fac52a45524c943e91fd287213f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 8 | Churn Cumulative: 351 | Contributors (this commit): 5 | Commits (past 90d): 23 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,4 @@\n import numpy as np\n-import torch\n from tensorflow import keras\n \n import keras_core\n@@ -86,9 +85,7 @@ def numerical_test():\n     keras_core_model.set_weights(weights)\n \n     for kw, kcw in zip(keras_model.weights, keras_core_model.weights):\n-        if torch.is_tensor(kcw.value):\n-            kcw = kcw.value.detach().numpy()\n-        np.testing.assert_allclose(kw.numpy(), kcw)\n+        np.testing.assert_allclose(kw.numpy(), kcw.numpy())\n \n     keras_history = train_model(keras_model, x_train, y_train)\n     keras_core_history = train_model(keras_core_model, x_train, y_train)\n\n@@ -47,6 +47,9 @@ class Variable(KerasVariable):\n     def _convert_to_tensor(self, value, dtype=None):\n         return convert_to_tensor(value, dtype=dtype)\n     \n+    def numpy(self):\n+        return self.value.detach().numpy()\n+\n     # Overload native accessor.\n     @classmethod\n     def __torch_function__(cls, func, types, args=(), kwargs=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3cda5ad72712471a950da03c2ba45dd96fe6068a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 9 | Files Changed: 12 | Hunks: 22 | Methods Changed: 15 | Complexity Δ (Sum/Max): 8/4 | Churn Δ: 53 | Churn Cumulative: 5169 | Contributors (this commit): 14 | Commits (past 90d): 216 | Contributors (cumulative): 62 | DMM Complexity: 0.9523809523809523\n\nDIFF:\n@@ -81,7 +81,7 @@ class KerasVariable:\n         return value\n \n     def numpy(self):\n-        return np.array(self.value)\n+        return np.array(self)\n \n     @property\n     def value(self):\n\n@@ -9,6 +9,7 @@ from keras_core.backend.jax.core import Variable\n from keras_core.backend.jax.core import cast\n from keras_core.backend.jax.core import compute_output_spec\n from keras_core.backend.jax.core import cond\n+from keras_core.backend.jax.core import convert_to_numpy\n from keras_core.backend.jax.core import convert_to_tensor\n from keras_core.backend.jax.core import is_tensor\n from keras_core.backend.jax.core import name_scope\n\n@@ -1,5 +1,6 @@\n import jax\n import jax.numpy as jnp\n+import numpy as np\n from tensorflow import nest\n \n from keras_core.backend.common import KerasVariable\n@@ -35,6 +36,10 @@ def convert_to_tensor(x, dtype=None):\n     return jnp.array(x, dtype=dtype)\n \n \n+def convert_to_numpy(x):\n+    return np.array(x)\n+\n+\n def is_tensor(x):\n     if isinstance(x, jnp.ndarray):\n         return True\n\n@@ -9,6 +9,7 @@ from keras_core.backend.tensorflow.core import Variable\n from keras_core.backend.tensorflow.core import cast\n from keras_core.backend.tensorflow.core import compute_output_spec\n from keras_core.backend.tensorflow.core import cond\n+from keras_core.backend.tensorflow.core import convert_to_numpy\n from keras_core.backend.tensorflow.core import convert_to_tensor\n from keras_core.backend.tensorflow.core import is_tensor\n from keras_core.backend.tensorflow.core import name_scope\n\n@@ -1,3 +1,4 @@\n+import numpy as np\n import tensorflow as tf\n from tensorflow.compiler.tf2xla.python.xla import dynamic_update_slice\n \n@@ -75,6 +76,10 @@ def convert_to_tensor(x, dtype=None):\n     return tf.convert_to_tensor(x, dtype=dtype)\n \n \n+def convert_to_numpy(x):\n+    return np.array(x)\n+\n+\n def is_tensor(x):\n     return tf.is_tensor(x)\n \n\n@@ -9,6 +9,7 @@ from keras_core.backend.torch.core import Variable\n from keras_core.backend.torch.core import cast\n from keras_core.backend.torch.core import compute_output_spec\n from keras_core.backend.torch.core import cond\n+from keras_core.backend.torch.core import convert_to_numpy\n from keras_core.backend.torch.core import convert_to_tensor\n from keras_core.backend.torch.core import is_tensor\n from keras_core.backend.torch.core import name_scope\n\n@@ -1,5 +1,6 @@\n import contextlib\n \n+import numpy as np\n import torch\n \n from keras_core.backend.common import KerasVariable\n@@ -47,9 +48,6 @@ class Variable(KerasVariable):\n     def _convert_to_tensor(self, value, dtype=None):\n         return convert_to_tensor(value, dtype=dtype)\n \n-    def numpy(self):\n-        return self.value.detach().numpy()\n-\n     # Overload native accessor.\n     @classmethod\n     def __torch_function__(cls, func, types, args=(), kwargs=None):\n@@ -64,6 +62,11 @@ class Variable(KerasVariable):\n         }\n         return func(*args, **kwargs)\n \n+    def __array__(self, dtype=None):\n+        if self.value.requires_grad:\n+            return self.value.detach().__array__(dtype)\n+        return super().__array__(dtype)\n+\n \n def convert_to_tensor(x, dtype=None):\n     # TODO: Need to address device placement arg of `as_tensor`\n@@ -75,6 +78,12 @@ def convert_to_tensor(x, dtype=None):\n     return torch.as_tensor(x, dtype=dtype)\n \n \n+def convert_to_numpy(x):\n+    if is_tensor(x) and x.requires_grad:\n+        return x.detach().numpy()\n+    return np.array(x)\n+\n+\n def is_tensor(x):\n     return torch.is_tensor(x)\n \n\n@@ -573,6 +573,7 @@ class TensorBoard(Callback):\n                 shape = w_img.shape\n             w_img = ops.reshape(w_img, [shape[0], shape[1], shape[2], 1])\n \n+        w_img = backend.convert_to_numpy(w_img)\n         shape = w_img.shape\n         # Not possible to handle 3D convnets etc.\n         if len(shape) == 4 and shape[-1] in [1, 3, 4]:\n\n@@ -1,6 +1,7 @@\n import numpy as np\n from absl.testing import parameterized\n \n+from keras_core import backend\n from keras_core import layers\n from keras_core import testing\n \n@@ -73,6 +74,7 @@ class BatchNormalizationTest(testing.TestCase, parameterized.TestCase):\n         # Assert the normalization is correct.\n         broadcast_shape = [1] * len(input_shape)\n         broadcast_shape[axis] = input_shape[axis]\n+        out = backend.convert_to_numpy(out)\n         out -= np.reshape(np.array(layer.beta), broadcast_shape)\n         out /= np.reshape(np.array(layer.gamma), broadcast_shape)\n \n@@ -109,6 +111,7 @@ class BatchNormalizationTest(testing.TestCase, parameterized.TestCase):\n         for _ in range(10):\n             out = layer(x, training=True)\n \n+        out = backend.convert_to_numpy(out)\n         out -= np.reshape(np.array(layer.beta), (1, 1, 1, 3))\n         out /= np.reshape(np.array(layer.gamma), (1, 1, 1, 3))\n \n\n@@ -56,7 +56,7 @@ class UpSampling2dTest(testing.TestCase, parameterized.TestCase):\n             expected_out = np.repeat(inputs, length_row, axis=1)\n             expected_out = np.repeat(expected_out, length_col, axis=2)\n \n-        np.testing.assert_allclose(np_output, expected_out)\n+        self.assertAllClose(np_output, expected_out)\n \n     @parameterized.product(\n         data_format=[\"channels_first\", \"channels_last\"],\n\n@@ -83,7 +83,7 @@ class UpSampling3dTest(testing.TestCase, parameterized.TestCase):\n             expected_out = np.repeat(expected_out, length_dim2, axis=2)\n             expected_out = np.repeat(expected_out, length_dim3, axis=3)\n \n-        np.testing.assert_allclose(np_output, expected_out)\n+        self.assertAllClose(np_output, expected_out)\n \n     def test_upsampling_3d_correctness(self):\n         input_shape = (2, 1, 2, 1, 3)\n\n@@ -6,6 +6,7 @@ import unittest\n import numpy as np\n from tensorflow import nest\n \n+from keras_core import backend\n from keras_core import operations as ops\n from keras_core.models import Model\n from keras_core.utils import traceback_utils\n@@ -25,6 +26,10 @@ class TestCase(unittest.TestCase):\n         return temp_dir\n \n     def assertAllClose(self, x1, x2, atol=1e-6, rtol=1e-6, msg=None):\n+        if not isinstance(x1, np.ndarray):\n+            x1 = backend.convert_to_numpy(x1)\n+        if not isinstance(x2, np.ndarray):\n+            x2 = backend.convert_to_numpy(x2)\n         np.testing.assert_allclose(x1, x2, atol=atol, rtol=rtol)\n \n     def assertNotAllClose(self, x1, x2, atol=1e-6, rtol=1e-6, msg=None):\n@@ -248,7 +253,7 @@ class TestCase(unittest.TestCase):\n                     output_dtype = nest.flatten(output)[0].dtype\n                     self.assertEqual(\n                         expected_output_dtype,\n-                        output_dtype,\n+                        backend.standardize_dtype(output_dtype),\n                         msg=\"Unexpected output dtype\",\n                     )\n             if eager:\n@@ -274,8 +279,12 @@ class TestCase(unittest.TestCase):\n \n             model = TestModel(layer)\n             model.compile(optimizer=\"sgd\", loss=\"mse\", jit_compile=True)\n-            input_data = nest.map_structure(lambda x: np.array(x), input_data)\n-            output_data = nest.map_structure(lambda x: np.array(x), output_data)\n+            input_data = nest.map_structure(\n+                lambda x: backend.convert_to_numpy(x), input_data\n+            )\n+            output_data = nest.map_structure(\n+                lambda x: backend.convert_to_numpy(x), output_data\n+            )\n             model.fit(input_data, output_data, verbose=0)\n \n         # Build test.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ac3cad7e048ece86ace19a2c233ebfc4248c509b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 15 | Files Changed: 2 | Hunks: 15 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 33 | Churn Cumulative: 308 | Contributors (this commit): 4 | Commits (past 90d): 14 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -26,19 +26,21 @@ class VariablesTest(test_case.TestCase):\n             dtype=\"float32\",\n         )\n         self.assertEqual(v.dtype, \"float32\")\n-        self.assertEqual(v.value.dtype.name, \"float32\")\n+        self.assertEqual(backend.standardize_dtype(v.value.dtype), \"float32\")\n \n         print(\"open scope\")\n         with AutocastScope(\"float16\"):\n-            self.assertEqual(v.value.dtype.name, \"float16\")\n-        self.assertEqual(v.value.dtype.name, \"float32\")\n+            self.assertEqual(\n+                backend.standardize_dtype(v.value.dtype), \"float16\"\n+            )\n+        self.assertEqual(backend.standardize_dtype(v.value.dtype), \"float32\")\n \n         # Test non-float variables are not affected\n         v = backend.Variable(\n             initializer=initializers.Ones(), shape=(2, 2), dtype=\"int32\"\n         )\n         self.assertEqual(v.dtype, \"int32\")\n-        self.assertEqual(v.value.dtype.name, \"int32\")\n+        self.assertEqual(backend.standardize_dtype(v.value.dtype), \"int32\")\n \n         with AutocastScope(\"float16\"):\n-            self.assertEqual(v.value.dtype.name, \"int32\")\n+            self.assertEqual(backend.standardize_dtype(v.value.dtype), \"int32\")\n\n@@ -1,5 +1,6 @@\n import numpy as np\n \n+from keras_core import backend\n from keras_core import losses as losses_module\n from keras_core import operations as ops\n from keras_core import testing\n@@ -19,19 +20,19 @@ class LossTest(testing.TestCase):\n         # No reduction\n         loss_fn = ExampleLoss(reduction=None)\n         loss = loss_fn(y_true, y_pred)\n-        self.assertEqual(loss.dtype.name, \"float32\")\n+        self.assertEqual(backend.standardize_dtype(loss.dtype), \"float32\")\n         self.assertAllClose((y_true - y_pred) ** 2, loss)\n \n         # sum\n         loss_fn = ExampleLoss(reduction=\"sum\")\n         loss = loss_fn(y_true, y_pred)\n-        self.assertEqual(loss.dtype.name, \"float32\")\n+        self.assertEqual(backend.standardize_dtype(loss.dtype), \"float32\")\n         self.assertAllClose(np.sum((y_true - y_pred) ** 2), loss)\n \n         # sum_over_batch_size\n         loss_fn = ExampleLoss(reduction=\"sum_over_batch_size\")\n         loss = loss_fn(y_true, y_pred)\n-        self.assertEqual(loss.dtype.name, \"float32\")\n+        self.assertEqual(backend.standardize_dtype(loss.dtype), \"float32\")\n         self.assertAllClose(np.sum((y_true - y_pred) ** 2) / 4, loss)\n \n         # bad reduction\n@@ -53,7 +54,7 @@ class LossTest(testing.TestCase):\n \n         loss_fn = ExampleLoss()\n         loss = loss_fn(y_true, y_pred)\n-        self.assertEqual(loss.dtype.name, \"float32\")\n+        self.assertEqual(backend.standardize_dtype(loss.dtype), \"float32\")\n         self.assertAllClose(\n             np.sum((masked_y_true - masked_y_pred) ** 2) / 3, loss\n         )\n@@ -62,7 +63,7 @@ class LossTest(testing.TestCase):\n         mask = np.array([False, False, False, False])\n         y_pred._keras_mask = mask\n         loss = loss_fn(y_true, y_pred)\n-        self.assertEqual(loss.dtype.name, \"float32\")\n+        self.assertEqual(backend.standardize_dtype(loss.dtype), \"float32\")\n         self.assertAllClose(loss, 0)  # No NaN.\n \n     def test_sample_weight(self):\n@@ -72,7 +73,7 @@ class LossTest(testing.TestCase):\n \n         loss_fn = ExampleLoss()\n         loss = loss_fn(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertEqual(loss.dtype.name, \"float32\")\n+        self.assertEqual(backend.standardize_dtype(loss.dtype), \"float32\")\n         self.assertAllClose(\n             np.sum(sample_weight * (y_true - y_pred) ** 2) / 4, loss\n         )\n@@ -80,7 +81,7 @@ class LossTest(testing.TestCase):\n         # Test edge case where every weight is 0.\n         sample_weight = np.array([0.0, 0.0, 0.0, 0.0])\n         loss = loss_fn(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertEqual(loss.dtype.name, \"float32\")\n+        self.assertEqual(backend.standardize_dtype(loss.dtype), \"float32\")\n         self.assertAllClose(loss, 0)  # No NaN.\n \n     def test_mask_and_sample_weight(self):\n@@ -100,7 +101,7 @@ class LossTest(testing.TestCase):\n \n         loss_fn = ExampleLoss()\n         loss = loss_fn(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertEqual(loss.dtype.name, \"float32\")\n+        self.assertEqual(backend.standardize_dtype(loss.dtype), \"float32\")\n         self.assertAllClose(\n             np.sum(masked_sample_weight * (masked_y_true - masked_y_pred) ** 2)\n             / 3,\n@@ -136,7 +137,7 @@ class LossTest(testing.TestCase):\n \n             loss_fn = ExampleLoss()\n             loss = loss_fn(y_true, y_pred, sample_weight=sample_weight)\n-            self.assertEqual(loss.dtype.name, \"float32\")\n+            self.assertEqual(backend.standardize_dtype(loss.dtype), \"float32\")\n             self.assertAllClose(\n                 np.sum(\n                     masked_sample_weight * (masked_y_true - masked_y_pred) ** 2\n@@ -152,7 +153,7 @@ class LossTest(testing.TestCase):\n \n         loss_fn = ExampleLoss()\n         loss = loss_fn(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertEqual(loss.dtype.name, \"float32\")\n+        self.assertEqual(backend.standardize_dtype(loss.dtype), \"float32\")\n         self.assertAllClose(\n             np.sum(sample_weight * (y_true - y_pred) ** 2) / 4,\n             loss,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#24cc859f7f418ced1e6ec22126521f8979dc962d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 6 | Churn Cumulative: 2536 | Contributors (this commit): 5 | Commits (past 90d): 16 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -64,11 +64,15 @@ def max(x, axis=None, keepdims=False, initial=None):\n \n def ones(shape, dtype=\"float32\"):\n     dtype = to_torch_dtype(dtype)\n-    return torch.ones(*shape, dtype=dtype)\n+    if isinstance(shape, int):\n+        shape = (shape,)\n+    return torch.ones(size=shape, dtype=dtype)\n \n \n def zeros(shape, dtype=\"float32\"):\n     dtype = to_torch_dtype(dtype)\n+    if isinstance(shape, int):\n+        shape = (shape,)\n     return torch.zeros(size=shape, dtype=dtype)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9e2f382929d64b16b636d2ed6af16a2f6a1305d8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 229 | Contributors (this commit): 4 | Commits (past 90d): 20 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -72,9 +72,14 @@ def convert_to_tensor(x, dtype=None):\n     # TODO: Need to address device placement arg of `as_tensor`\n     dtype = to_torch_dtype(dtype or getattr(x, \"dtype\", None))\n     if isinstance(x, Variable):\n+        x = x.value\n+    if is_tensor(x):\n         if dtype and dtype != x.dtype:\n-            return x.value.to(dtype)\n-        return x.value\n+            return x.to(dtype)\n+        return x\n+\n+    # Convert to np first in case of any non-numpy, numpy-compatible array.\n+    x = np.array(x)\n     return torch.as_tensor(x, dtype=dtype)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9ce74e0af4de022924edb81c3ee5af04b6007aad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 86 | Lines Deleted: 88 | Files Changed: 22 | Hunks: 52 | Methods Changed: 31 | Complexity Δ (Sum/Max): -5/2 | Churn Δ: 174 | Churn Cumulative: 10590 | Contributors (this commit): 15 | Commits (past 90d): 366 | Contributors (cumulative): 96 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,7 +9,7 @@ import numpy as np\n num_classes = 10\n input_shape = (28, 28, 1)\n learning_rate = 0.01\n-batch_size = 64\n+batch_size = 128\n num_epochs = 1\n \n # Load the data and split it between train and test sets\n\n@@ -26,7 +26,7 @@ y_train = to_categorical(y_train, num_classes)\n y_test = to_categorical(y_test, num_classes)\n \n batch_size = 128\n-epochs = 15\n+epochs = 3\n \n model = keras_core.Sequential(\n     [\n\n@@ -81,7 +81,7 @@ class KerasVariable:\n         return value\n \n     def numpy(self):\n-        return np.array(self)\n+        return np.array(self.value)\n \n     @property\n     def value(self):\n\n@@ -26,21 +26,19 @@ class VariablesTest(test_case.TestCase):\n             dtype=\"float32\",\n         )\n         self.assertEqual(v.dtype, \"float32\")\n-        self.assertEqual(backend.standardize_dtype(v.value.dtype), \"float32\")\n+        self.assertEqual(v.value.dtype.name, \"float32\")\n \n         print(\"open scope\")\n         with AutocastScope(\"float16\"):\n-            self.assertEqual(\n-                backend.standardize_dtype(v.value.dtype), \"float16\"\n-            )\n-        self.assertEqual(backend.standardize_dtype(v.value.dtype), \"float32\")\n+            self.assertEqual(v.value.dtype.name, \"float16\")\n+        self.assertEqual(v.value.dtype.name, \"float32\")\n \n         # Test non-float variables are not affected\n         v = backend.Variable(\n             initializer=initializers.Ones(), shape=(2, 2), dtype=\"int32\"\n         )\n         self.assertEqual(v.dtype, \"int32\")\n-        self.assertEqual(backend.standardize_dtype(v.value.dtype), \"int32\")\n+        self.assertEqual(v.value.dtype.name, \"int32\")\n \n         with AutocastScope(\"float16\"):\n-            self.assertEqual(backend.standardize_dtype(v.value.dtype), \"int32\")\n+            self.assertEqual(v.value.dtype.name, \"int32\")\n\n@@ -9,7 +9,6 @@ from keras_core.backend.jax.core import Variable\n from keras_core.backend.jax.core import cast\n from keras_core.backend.jax.core import compute_output_spec\n from keras_core.backend.jax.core import cond\n-from keras_core.backend.jax.core import convert_to_numpy\n from keras_core.backend.jax.core import convert_to_tensor\n from keras_core.backend.jax.core import is_tensor\n from keras_core.backend.jax.core import name_scope\n\n@@ -1,6 +1,5 @@\n import jax\n import jax.numpy as jnp\n-import numpy as np\n from tensorflow import nest\n \n from keras_core.backend.common import KerasVariable\n@@ -36,10 +35,6 @@ def convert_to_tensor(x, dtype=None):\n     return jnp.array(x, dtype=dtype)\n \n \n-def convert_to_numpy(x):\n-    return np.array(x)\n-\n-\n def is_tensor(x):\n     if isinstance(x, jnp.ndarray):\n         return True\n\n@@ -9,7 +9,6 @@ from keras_core.backend.tensorflow.core import Variable\n from keras_core.backend.tensorflow.core import cast\n from keras_core.backend.tensorflow.core import compute_output_spec\n from keras_core.backend.tensorflow.core import cond\n-from keras_core.backend.tensorflow.core import convert_to_numpy\n from keras_core.backend.tensorflow.core import convert_to_tensor\n from keras_core.backend.tensorflow.core import is_tensor\n from keras_core.backend.tensorflow.core import name_scope\n\n@@ -1,4 +1,3 @@\n-import numpy as np\n import tensorflow as tf\n from tensorflow.compiler.tf2xla.python.xla import dynamic_update_slice\n \n@@ -76,10 +75,6 @@ def convert_to_tensor(x, dtype=None):\n     return tf.convert_to_tensor(x, dtype=dtype)\n \n \n-def convert_to_numpy(x):\n-    return np.array(x)\n-\n-\n def is_tensor(x):\n     return tf.is_tensor(x)\n \n\n@@ -9,7 +9,6 @@ from keras_core.backend.torch.core import Variable\n from keras_core.backend.torch.core import cast\n from keras_core.backend.torch.core import compute_output_spec\n from keras_core.backend.torch.core import cond\n-from keras_core.backend.torch.core import convert_to_numpy\n from keras_core.backend.torch.core import convert_to_tensor\n from keras_core.backend.torch.core import is_tensor\n from keras_core.backend.torch.core import name_scope\n\n@@ -1,6 +1,5 @@\n import contextlib\n \n-import numpy as np\n import torch\n \n from keras_core.backend.common import KerasVariable\n@@ -48,6 +47,9 @@ class Variable(KerasVariable):\n     def _convert_to_tensor(self, value, dtype=None):\n         return convert_to_tensor(value, dtype=dtype)\n     \n+    def numpy(self):\n+        return self.value.detach().numpy()\n+\n     # Overload native accessor.\n     @classmethod\n     def __torch_function__(cls, func, types, args=(), kwargs=None):\n@@ -62,33 +64,17 @@ class Variable(KerasVariable):\n         }\n         return func(*args, **kwargs)\n \n-    def __array__(self, dtype=None):\n-        if self.value.requires_grad:\n-            return self.value.detach().__array__(dtype)\n-        return super().__array__(dtype)\n-\n \n def convert_to_tensor(x, dtype=None):\n     # TODO: Need to address device placement arg of `as_tensor`\n     dtype = to_torch_dtype(dtype or getattr(x, \"dtype\", None))\n     if isinstance(x, Variable):\n-        x = x.value\n-    if is_tensor(x):\n         if dtype and dtype != x.dtype:\n-            return x.to(dtype)\n-        return x\n-\n-    # Convert to np first in case of any non-numpy, numpy-compatible array.\n-    x = np.array(x)\n+            return x.value.to(dtype)\n+        return x.value\n     return torch.as_tensor(x, dtype=dtype)\n \n \n-def convert_to_numpy(x):\n-    if is_tensor(x) and x.requires_grad:\n-        return x.detach().numpy()\n-    return np.array(x)\n-\n-\n def is_tensor(x):\n     return torch.is_tensor(x)\n \n\n@@ -64,15 +64,11 @@ def max(x, axis=None, keepdims=False, initial=None):\n \n def ones(shape, dtype=\"float32\"):\n     dtype = to_torch_dtype(dtype)\n-    if isinstance(shape, int):\n-        shape = (shape,)\n-    return torch.ones(size=shape, dtype=dtype)\n+    return torch.ones(*shape, dtype=dtype)\n \n \n def zeros(shape, dtype=\"float32\"):\n     dtype = to_torch_dtype(dtype)\n-    if isinstance(shape, int):\n-        shape = (shape,)\n     return torch.zeros(size=shape, dtype=dtype)\n \n \n\n@@ -573,7 +573,6 @@ class TensorBoard(Callback):\n                 shape = w_img.shape\n             w_img = ops.reshape(w_img, [shape[0], shape[1], shape[2], 1])\n \n-        w_img = backend.convert_to_numpy(w_img)\n         shape = w_img.shape\n         # Not possible to handle 3D convnets etc.\n         if len(shape) == 4 and shape[-1] in [1, 3, 4]:\n\n@@ -357,16 +357,7 @@ class Layer(BackendLayer, Operation):\n         # Will be added to layer.losses\n         variable.regularizer = regularizer\n         variable.constraint = constraint\n-        if trainable:\n-            self._trainable_variables.append(variable)\n-            # Prevent double-tracking\n-            self._tracker.stored_ids[\"trainable_variables\"].add(id(variable))\n-        else:\n-            self._non_trainable_variables.append(variable)\n-            # Prevent double-tracking\n-            self._tracker.stored_ids[\"non_trainable_variables\"].add(\n-                id(variable)\n-            )\n+        self._track_variable(variable)\n         return variable\n \n     def add_weight(self, *args, **kwargs):\n@@ -896,6 +887,18 @@ class Layer(BackendLayer, Operation):\n         for layer in self._layers:\n             layer._clear_losses()\n \n+    def _track_variable(self, variable):\n+        if variable.trainable:\n+            self._trainable_variables.append(variable)\n+            # Prevent double-tracking\n+            self._tracker.stored_ids[\"trainable_variables\"].add(id(variable))\n+        else:\n+            self._non_trainable_variables.append(variable)\n+            # Prevent double-tracking\n+            self._tracker.stored_ids[\"non_trainable_variables\"].add(\n+                id(variable)\n+            )\n+\n     def add_metric(self):\n         # Permanently disabled\n         raise NotImplementedError\n\n@@ -1,7 +1,6 @@\n import numpy as np\n from absl.testing import parameterized\n \n-from keras_core import backend\n from keras_core import layers\n from keras_core import testing\n \n@@ -74,7 +73,6 @@ class BatchNormalizationTest(testing.TestCase, parameterized.TestCase):\n         # Assert the normalization is correct.\n         broadcast_shape = [1] * len(input_shape)\n         broadcast_shape[axis] = input_shape[axis]\n-        out = backend.convert_to_numpy(out)\n         out -= np.reshape(np.array(layer.beta), broadcast_shape)\n         out /= np.reshape(np.array(layer.gamma), broadcast_shape)\n \n@@ -111,7 +109,6 @@ class BatchNormalizationTest(testing.TestCase, parameterized.TestCase):\n         for _ in range(10):\n             out = layer(x, training=True)\n \n-        out = backend.convert_to_numpy(out)\n         out -= np.reshape(np.array(layer.beta), (1, 1, 1, 3))\n         out /= np.reshape(np.array(layer.gamma), (1, 1, 1, 3))\n \n\n@@ -1,4 +1,5 @@\n import numpy as np\n+import tensorflow as tf\n \n from keras_core import layers\n from keras_core import testing\n@@ -57,3 +58,16 @@ class HashedCrossingTest(testing.TestCase):\n             ),\n             output,\n         )\n+\n+    def test_tf_data_compatibility(self):\n+        layer = layers.HashedCrossing(num_bins=5)\n+        feat1 = np.array([\"A\", \"B\", \"A\", \"B\", \"A\"])\n+        feat2 = np.array([101, 101, 101, 102, 102])\n+        ds = (\n+            tf.data.Dataset.from_tensor_slices((feat1, feat2))\n+            .batch(5)\n+            .map(lambda x1, x2: layer((x1, x2)))\n+        )\n+        for output in ds.take(1):\n+            output = output.numpy()\n+        self.assertAllClose(np.array([1, 4, 1, 1, 3]), output)\n\n@@ -106,13 +106,15 @@ class RandomZoom(Layer):\n             fill_value=fill_value,\n             **kwargs,\n         )\n+        self._allow_non_tensor_positional_args = True\n+        self._convert_input_args = False\n         self.supports_jit = False\n \n     def call(self, inputs, training=True):\n         if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n             inputs = tf.convert_to_tensor(np.array(inputs))\n-        outputs = self.layer.call(inputs)\n-        if backend.backend() != \"tensorflow\":\n+        outputs = self.layer.call(inputs, training=training)\n+        if backend.backend() != \"tensorflow\" and tf.executing_eagerly():\n             outputs = backend.convert_to_tensor(outputs)\n         return outputs\n \n\n@@ -1,4 +1,5 @@\n import numpy as np\n+import tensorflow as tf\n from absl.testing import parameterized\n \n from keras_core import backend\n@@ -84,3 +85,25 @@ class RandomZoomTest(testing.TestCase, parameterized.TestCase):\n             supports_masking=False,\n             run_training_check=False,\n         )\n+\n+    def test_tf_data_compatibility(self):\n+        input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1))\n+        layer = layers.RandomZoom(\n+            height_factor=(0.5, 0.5),\n+            width_factor=(0.8, 0.8),\n+            interpolation=\"nearest\",\n+            fill_mode=\"constant\",\n+        )\n+        ds = tf.data.Dataset.from_tensor_slices(input_image).batch(1).map(layer)\n+        expected_output = np.asarray(\n+            [\n+                [0, 0, 0, 0, 0],\n+                [0, 5, 7, 9, 0],\n+                [0, 10, 12, 14, 0],\n+                [0, 20, 22, 24, 0],\n+                [0, 0, 0, 0, 0],\n+            ]\n+        ).reshape((1, 5, 5, 1))\n+        for output in ds.take(1):\n+            output = output.numpy()\n+        self.assertAllClose(expected_output, output)\n\n@@ -56,7 +56,7 @@ class UpSampling2dTest(testing.TestCase, parameterized.TestCase):\n             expected_out = np.repeat(inputs, length_row, axis=1)\n             expected_out = np.repeat(expected_out, length_col, axis=2)\n \n-        self.assertAllClose(np_output, expected_out)\n+        np.testing.assert_allclose(np_output, expected_out)\n \n     @parameterized.product(\n         data_format=[\"channels_first\", \"channels_last\"],\n\n@@ -83,7 +83,7 @@ class UpSampling3dTest(testing.TestCase, parameterized.TestCase):\n             expected_out = np.repeat(expected_out, length_dim2, axis=2)\n             expected_out = np.repeat(expected_out, length_dim3, axis=3)\n \n-        self.assertAllClose(np_output, expected_out)\n+        np.testing.assert_allclose(np_output, expected_out)\n \n     def test_upsampling_3d_correctness(self):\n         input_shape = (2, 1, 2, 1, 3)\n\n@@ -1,6 +1,5 @@\n import numpy as np\n \n-from keras_core import backend\n from keras_core import losses as losses_module\n from keras_core import operations as ops\n from keras_core import testing\n@@ -20,19 +19,19 @@ class LossTest(testing.TestCase):\n         # No reduction\n         loss_fn = ExampleLoss(reduction=None)\n         loss = loss_fn(y_true, y_pred)\n-        self.assertEqual(backend.standardize_dtype(loss.dtype), \"float32\")\n+        self.assertEqual(loss.dtype.name, \"float32\")\n         self.assertAllClose((y_true - y_pred) ** 2, loss)\n \n         # sum\n         loss_fn = ExampleLoss(reduction=\"sum\")\n         loss = loss_fn(y_true, y_pred)\n-        self.assertEqual(backend.standardize_dtype(loss.dtype), \"float32\")\n+        self.assertEqual(loss.dtype.name, \"float32\")\n         self.assertAllClose(np.sum((y_true - y_pred) ** 2), loss)\n \n         # sum_over_batch_size\n         loss_fn = ExampleLoss(reduction=\"sum_over_batch_size\")\n         loss = loss_fn(y_true, y_pred)\n-        self.assertEqual(backend.standardize_dtype(loss.dtype), \"float32\")\n+        self.assertEqual(loss.dtype.name, \"float32\")\n         self.assertAllClose(np.sum((y_true - y_pred) ** 2) / 4, loss)\n \n         # bad reduction\n@@ -54,7 +53,7 @@ class LossTest(testing.TestCase):\n \n         loss_fn = ExampleLoss()\n         loss = loss_fn(y_true, y_pred)\n-        self.assertEqual(backend.standardize_dtype(loss.dtype), \"float32\")\n+        self.assertEqual(loss.dtype.name, \"float32\")\n         self.assertAllClose(\n             np.sum((masked_y_true - masked_y_pred) ** 2) / 3, loss\n         )\n@@ -63,7 +62,7 @@ class LossTest(testing.TestCase):\n         mask = np.array([False, False, False, False])\n         y_pred._keras_mask = mask\n         loss = loss_fn(y_true, y_pred)\n-        self.assertEqual(backend.standardize_dtype(loss.dtype), \"float32\")\n+        self.assertEqual(loss.dtype.name, \"float32\")\n         self.assertAllClose(loss, 0)  # No NaN.\n \n     def test_sample_weight(self):\n@@ -73,7 +72,7 @@ class LossTest(testing.TestCase):\n \n         loss_fn = ExampleLoss()\n         loss = loss_fn(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertEqual(backend.standardize_dtype(loss.dtype), \"float32\")\n+        self.assertEqual(loss.dtype.name, \"float32\")\n         self.assertAllClose(\n             np.sum(sample_weight * (y_true - y_pred) ** 2) / 4, loss\n         )\n@@ -81,7 +80,7 @@ class LossTest(testing.TestCase):\n         # Test edge case where every weight is 0.\n         sample_weight = np.array([0.0, 0.0, 0.0, 0.0])\n         loss = loss_fn(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertEqual(backend.standardize_dtype(loss.dtype), \"float32\")\n+        self.assertEqual(loss.dtype.name, \"float32\")\n         self.assertAllClose(loss, 0)  # No NaN.\n \n     def test_mask_and_sample_weight(self):\n@@ -101,7 +100,7 @@ class LossTest(testing.TestCase):\n \n         loss_fn = ExampleLoss()\n         loss = loss_fn(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertEqual(backend.standardize_dtype(loss.dtype), \"float32\")\n+        self.assertEqual(loss.dtype.name, \"float32\")\n         self.assertAllClose(\n             np.sum(masked_sample_weight * (masked_y_true - masked_y_pred) ** 2)\n             / 3,\n@@ -137,7 +136,7 @@ class LossTest(testing.TestCase):\n \n             loss_fn = ExampleLoss()\n             loss = loss_fn(y_true, y_pred, sample_weight=sample_weight)\n-            self.assertEqual(backend.standardize_dtype(loss.dtype), \"float32\")\n+            self.assertEqual(loss.dtype.name, \"float32\")\n             self.assertAllClose(\n                 np.sum(\n                     masked_sample_weight * (masked_y_true - masked_y_pred) ** 2\n@@ -153,7 +152,7 @@ class LossTest(testing.TestCase):\n \n         loss_fn = ExampleLoss()\n         loss = loss_fn(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertEqual(backend.standardize_dtype(loss.dtype), \"float32\")\n+        self.assertEqual(loss.dtype.name, \"float32\")\n         self.assertAllClose(\n             np.sum(sample_weight * (y_true - y_pred) ** 2) / 4,\n             loss,\n\n@@ -1,5 +1,8 @@\n \"\"\"\n scatter\n+scatter_update\n+block_update\n+while_loop\n \"\"\"\n \n from keras_core import backend\n\n@@ -6,7 +6,6 @@ import unittest\n import numpy as np\n from tensorflow import nest\n \n-from keras_core import backend\n from keras_core import operations as ops\n from keras_core.models import Model\n from keras_core.utils import traceback_utils\n@@ -26,10 +25,6 @@ class TestCase(unittest.TestCase):\n         return temp_dir\n \n     def assertAllClose(self, x1, x2, atol=1e-6, rtol=1e-6, msg=None):\n-        if not isinstance(x1, np.ndarray):\n-            x1 = backend.convert_to_numpy(x1)\n-        if not isinstance(x2, np.ndarray):\n-            x2 = backend.convert_to_numpy(x2)\n         np.testing.assert_allclose(x1, x2, atol=atol, rtol=rtol)\n \n     def assertNotAllClose(self, x1, x2, atol=1e-6, rtol=1e-6, msg=None):\n@@ -253,7 +248,7 @@ class TestCase(unittest.TestCase):\n                     output_dtype = nest.flatten(output)[0].dtype\n                     self.assertEqual(\n                         expected_output_dtype,\n-                        backend.standardize_dtype(output_dtype),\n+                        output_dtype,\n                         msg=\"Unexpected output dtype\",\n                     )\n             if eager:\n@@ -279,12 +274,8 @@ class TestCase(unittest.TestCase):\n \n             model = TestModel(layer)\n             model.compile(optimizer=\"sgd\", loss=\"mse\", jit_compile=True)\n-            input_data = nest.map_structure(\n-                lambda x: backend.convert_to_numpy(x), input_data\n-            )\n-            output_data = nest.map_structure(\n-                lambda x: backend.convert_to_numpy(x), output_data\n-            )\n+            input_data = nest.map_structure(lambda x: np.array(x), input_data)\n+            output_data = nest.map_structure(lambda x: np.array(x), output_data)\n             model.fit(input_data, output_data, verbose=0)\n \n         # Build test.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ed243a247f73e013ba6a4a18698f8c73dd4a6d60", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 74 | Lines Deleted: 27 | Files Changed: 15 | Hunks: 41 | Methods Changed: 25 | Complexity Δ (Sum/Max): 11/5 | Churn Δ: 101 | Churn Cumulative: 8224 | Contributors (this commit): 14 | Commits (past 90d): 277 | Contributors (cumulative): 75 | DMM Complexity: 0.6333333333333333\n\nDIFF:\n@@ -81,7 +81,7 @@ class KerasVariable:\n         return value\n \n     def numpy(self):\n-        return np.array(self.value)\n+        return np.array(self)\n \n     @property\n     def value(self):\n\n@@ -26,19 +26,21 @@ class VariablesTest(test_case.TestCase):\n             dtype=\"float32\",\n         )\n         self.assertEqual(v.dtype, \"float32\")\n-        self.assertEqual(v.value.dtype.name, \"float32\")\n+        self.assertEqual(backend.standardize_dtype(v.value.dtype), \"float32\")\n \n         print(\"open scope\")\n         with AutocastScope(\"float16\"):\n-            self.assertEqual(v.value.dtype.name, \"float16\")\n-        self.assertEqual(v.value.dtype.name, \"float32\")\n+            self.assertEqual(\n+                backend.standardize_dtype(v.value.dtype), \"float16\"\n+            )\n+        self.assertEqual(backend.standardize_dtype(v.value.dtype), \"float32\")\n \n         # Test non-float variables are not affected\n         v = backend.Variable(\n             initializer=initializers.Ones(), shape=(2, 2), dtype=\"int32\"\n         )\n         self.assertEqual(v.dtype, \"int32\")\n-        self.assertEqual(v.value.dtype.name, \"int32\")\n+        self.assertEqual(backend.standardize_dtype(v.value.dtype), \"int32\")\n \n         with AutocastScope(\"float16\"):\n-            self.assertEqual(v.value.dtype.name, \"int32\")\n+            self.assertEqual(backend.standardize_dtype(v.value.dtype), \"int32\")\n\n@@ -9,6 +9,7 @@ from keras_core.backend.jax.core import Variable\n from keras_core.backend.jax.core import cast\n from keras_core.backend.jax.core import compute_output_spec\n from keras_core.backend.jax.core import cond\n+from keras_core.backend.jax.core import convert_to_numpy\n from keras_core.backend.jax.core import convert_to_tensor\n from keras_core.backend.jax.core import is_tensor\n from keras_core.backend.jax.core import name_scope\n\n@@ -1,5 +1,6 @@\n import jax\n import jax.numpy as jnp\n+import numpy as np\n from tensorflow import nest\n \n from keras_core.backend.common import KerasVariable\n@@ -35,6 +36,10 @@ def convert_to_tensor(x, dtype=None):\n     return jnp.array(x, dtype=dtype)\n \n \n+def convert_to_numpy(x):\n+    return np.array(x)\n+\n+\n def is_tensor(x):\n     if isinstance(x, jnp.ndarray):\n         return True\n\n@@ -9,6 +9,7 @@ from keras_core.backend.tensorflow.core import Variable\n from keras_core.backend.tensorflow.core import cast\n from keras_core.backend.tensorflow.core import compute_output_spec\n from keras_core.backend.tensorflow.core import cond\n+from keras_core.backend.tensorflow.core import convert_to_numpy\n from keras_core.backend.tensorflow.core import convert_to_tensor\n from keras_core.backend.tensorflow.core import is_tensor\n from keras_core.backend.tensorflow.core import name_scope\n\n@@ -1,3 +1,4 @@\n+import numpy as np\n import tensorflow as tf\n from tensorflow.compiler.tf2xla.python.xla import dynamic_update_slice\n \n@@ -75,6 +76,10 @@ def convert_to_tensor(x, dtype=None):\n     return tf.convert_to_tensor(x, dtype=dtype)\n \n \n+def convert_to_numpy(x):\n+    return np.array(x)\n+\n+\n def is_tensor(x):\n     return tf.is_tensor(x)\n \n\n@@ -9,6 +9,7 @@ from keras_core.backend.torch.core import Variable\n from keras_core.backend.torch.core import cast\n from keras_core.backend.torch.core import compute_output_spec\n from keras_core.backend.torch.core import cond\n+from keras_core.backend.torch.core import convert_to_numpy\n from keras_core.backend.torch.core import convert_to_tensor\n from keras_core.backend.torch.core import is_tensor\n from keras_core.backend.torch.core import name_scope\n\n@@ -1,5 +1,6 @@\n import contextlib\n \n+import numpy as np\n import torch\n \n from keras_core.backend.common import KerasVariable\n@@ -47,9 +48,6 @@ class Variable(KerasVariable):\n     def _convert_to_tensor(self, value, dtype=None):\n         return convert_to_tensor(value, dtype=dtype)\n \n-    def numpy(self):\n-        return self.value.detach().numpy()\n-\n     # Overload native accessor.\n     @classmethod\n     def __torch_function__(cls, func, types, args=(), kwargs=None):\n@@ -64,17 +62,33 @@ class Variable(KerasVariable):\n         }\n         return func(*args, **kwargs)\n \n+    def __array__(self, dtype=None):\n+        if self.value.requires_grad:\n+            return self.value.detach().__array__(dtype)\n+        return super().__array__(dtype)\n+\n \n def convert_to_tensor(x, dtype=None):\n     # TODO: Need to address device placement arg of `as_tensor`\n     dtype = to_torch_dtype(dtype or getattr(x, \"dtype\", None))\n     if isinstance(x, Variable):\n+        x = x.value\n+    if is_tensor(x):\n         if dtype and dtype != x.dtype:\n-            return x.value.to(dtype)\n-        return x.value\n+            return x.to(dtype)\n+        return x\n+\n+    # Convert to np first in case of any non-numpy, numpy-compatible array.\n+    x = np.array(x)\n     return torch.as_tensor(x, dtype=dtype)\n \n \n+def convert_to_numpy(x):\n+    if is_tensor(x) and x.requires_grad:\n+        return x.detach().numpy()\n+    return np.array(x)\n+\n+\n def is_tensor(x):\n     return torch.is_tensor(x)\n \n\n@@ -64,11 +64,15 @@ def max(x, axis=None, keepdims=False, initial=None):\n \n def ones(shape, dtype=\"float32\"):\n     dtype = to_torch_dtype(dtype)\n-    return torch.ones(*shape, dtype=dtype)\n+    if isinstance(shape, int):\n+        shape = (shape,)\n+    return torch.ones(size=shape, dtype=dtype)\n \n \n def zeros(shape, dtype=\"float32\"):\n     dtype = to_torch_dtype(dtype)\n+    if isinstance(shape, int):\n+        shape = (shape,)\n     return torch.zeros(size=shape, dtype=dtype)\n \n \n\n@@ -573,6 +573,7 @@ class TensorBoard(Callback):\n                 shape = w_img.shape\n             w_img = ops.reshape(w_img, [shape[0], shape[1], shape[2], 1])\n \n+        w_img = backend.convert_to_numpy(w_img)\n         shape = w_img.shape\n         # Not possible to handle 3D convnets etc.\n         if len(shape) == 4 and shape[-1] in [1, 3, 4]:\n\n@@ -1,6 +1,7 @@\n import numpy as np\n from absl.testing import parameterized\n \n+from keras_core import backend\n from keras_core import layers\n from keras_core import testing\n \n@@ -73,6 +74,7 @@ class BatchNormalizationTest(testing.TestCase, parameterized.TestCase):\n         # Assert the normalization is correct.\n         broadcast_shape = [1] * len(input_shape)\n         broadcast_shape[axis] = input_shape[axis]\n+        out = backend.convert_to_numpy(out)\n         out -= np.reshape(np.array(layer.beta), broadcast_shape)\n         out /= np.reshape(np.array(layer.gamma), broadcast_shape)\n \n@@ -109,6 +111,7 @@ class BatchNormalizationTest(testing.TestCase, parameterized.TestCase):\n         for _ in range(10):\n             out = layer(x, training=True)\n \n+        out = backend.convert_to_numpy(out)\n         out -= np.reshape(np.array(layer.beta), (1, 1, 1, 3))\n         out /= np.reshape(np.array(layer.gamma), (1, 1, 1, 3))\n \n\n@@ -56,7 +56,7 @@ class UpSampling2dTest(testing.TestCase, parameterized.TestCase):\n             expected_out = np.repeat(inputs, length_row, axis=1)\n             expected_out = np.repeat(expected_out, length_col, axis=2)\n \n-        np.testing.assert_allclose(np_output, expected_out)\n+        self.assertAllClose(np_output, expected_out)\n \n     @parameterized.product(\n         data_format=[\"channels_first\", \"channels_last\"],\n\n@@ -83,7 +83,7 @@ class UpSampling3dTest(testing.TestCase, parameterized.TestCase):\n             expected_out = np.repeat(expected_out, length_dim2, axis=2)\n             expected_out = np.repeat(expected_out, length_dim3, axis=3)\n \n-        np.testing.assert_allclose(np_output, expected_out)\n+        self.assertAllClose(np_output, expected_out)\n \n     def test_upsampling_3d_correctness(self):\n         input_shape = (2, 1, 2, 1, 3)\n\n@@ -1,5 +1,6 @@\n import numpy as np\n \n+from keras_core import backend\n from keras_core import losses as losses_module\n from keras_core import operations as ops\n from keras_core import testing\n@@ -19,19 +20,19 @@ class LossTest(testing.TestCase):\n         # No reduction\n         loss_fn = ExampleLoss(reduction=None)\n         loss = loss_fn(y_true, y_pred)\n-        self.assertEqual(loss.dtype.name, \"float32\")\n+        self.assertEqual(backend.standardize_dtype(loss.dtype), \"float32\")\n         self.assertAllClose((y_true - y_pred) ** 2, loss)\n \n         # sum\n         loss_fn = ExampleLoss(reduction=\"sum\")\n         loss = loss_fn(y_true, y_pred)\n-        self.assertEqual(loss.dtype.name, \"float32\")\n+        self.assertEqual(backend.standardize_dtype(loss.dtype), \"float32\")\n         self.assertAllClose(np.sum((y_true - y_pred) ** 2), loss)\n \n         # sum_over_batch_size\n         loss_fn = ExampleLoss(reduction=\"sum_over_batch_size\")\n         loss = loss_fn(y_true, y_pred)\n-        self.assertEqual(loss.dtype.name, \"float32\")\n+        self.assertEqual(backend.standardize_dtype(loss.dtype), \"float32\")\n         self.assertAllClose(np.sum((y_true - y_pred) ** 2) / 4, loss)\n \n         # bad reduction\n@@ -53,7 +54,7 @@ class LossTest(testing.TestCase):\n \n         loss_fn = ExampleLoss()\n         loss = loss_fn(y_true, y_pred)\n-        self.assertEqual(loss.dtype.name, \"float32\")\n+        self.assertEqual(backend.standardize_dtype(loss.dtype), \"float32\")\n         self.assertAllClose(\n             np.sum((masked_y_true - masked_y_pred) ** 2) / 3, loss\n         )\n@@ -62,7 +63,7 @@ class LossTest(testing.TestCase):\n         mask = np.array([False, False, False, False])\n         y_pred._keras_mask = mask\n         loss = loss_fn(y_true, y_pred)\n-        self.assertEqual(loss.dtype.name, \"float32\")\n+        self.assertEqual(backend.standardize_dtype(loss.dtype), \"float32\")\n         self.assertAllClose(loss, 0)  # No NaN.\n \n     def test_sample_weight(self):\n@@ -72,7 +73,7 @@ class LossTest(testing.TestCase):\n \n         loss_fn = ExampleLoss()\n         loss = loss_fn(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertEqual(loss.dtype.name, \"float32\")\n+        self.assertEqual(backend.standardize_dtype(loss.dtype), \"float32\")\n         self.assertAllClose(\n             np.sum(sample_weight * (y_true - y_pred) ** 2) / 4, loss\n         )\n@@ -80,7 +81,7 @@ class LossTest(testing.TestCase):\n         # Test edge case where every weight is 0.\n         sample_weight = np.array([0.0, 0.0, 0.0, 0.0])\n         loss = loss_fn(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertEqual(loss.dtype.name, \"float32\")\n+        self.assertEqual(backend.standardize_dtype(loss.dtype), \"float32\")\n         self.assertAllClose(loss, 0)  # No NaN.\n \n     def test_mask_and_sample_weight(self):\n@@ -100,7 +101,7 @@ class LossTest(testing.TestCase):\n \n         loss_fn = ExampleLoss()\n         loss = loss_fn(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertEqual(loss.dtype.name, \"float32\")\n+        self.assertEqual(backend.standardize_dtype(loss.dtype), \"float32\")\n         self.assertAllClose(\n             np.sum(masked_sample_weight * (masked_y_true - masked_y_pred) ** 2)\n             / 3,\n@@ -136,7 +137,7 @@ class LossTest(testing.TestCase):\n \n             loss_fn = ExampleLoss()\n             loss = loss_fn(y_true, y_pred, sample_weight=sample_weight)\n-            self.assertEqual(loss.dtype.name, \"float32\")\n+            self.assertEqual(backend.standardize_dtype(loss.dtype), \"float32\")\n             self.assertAllClose(\n                 np.sum(\n                     masked_sample_weight * (masked_y_true - masked_y_pred) ** 2\n@@ -152,7 +153,7 @@ class LossTest(testing.TestCase):\n \n         loss_fn = ExampleLoss()\n         loss = loss_fn(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertEqual(loss.dtype.name, \"float32\")\n+        self.assertEqual(backend.standardize_dtype(loss.dtype), \"float32\")\n         self.assertAllClose(\n             np.sum(sample_weight * (y_true - y_pred) ** 2) / 4,\n             loss,\n\n@@ -6,6 +6,7 @@ import unittest\n import numpy as np\n from tensorflow import nest\n \n+from keras_core import backend\n from keras_core import operations as ops\n from keras_core.models import Model\n from keras_core.utils import traceback_utils\n@@ -25,6 +26,10 @@ class TestCase(unittest.TestCase):\n         return temp_dir\n \n     def assertAllClose(self, x1, x2, atol=1e-6, rtol=1e-6, msg=None):\n+        if not isinstance(x1, np.ndarray):\n+            x1 = backend.convert_to_numpy(x1)\n+        if not isinstance(x2, np.ndarray):\n+            x2 = backend.convert_to_numpy(x2)\n         np.testing.assert_allclose(x1, x2, atol=atol, rtol=rtol)\n \n     def assertNotAllClose(self, x1, x2, atol=1e-6, rtol=1e-6, msg=None):\n@@ -248,7 +253,7 @@ class TestCase(unittest.TestCase):\n                     output_dtype = nest.flatten(output)[0].dtype\n                     self.assertEqual(\n                         expected_output_dtype,\n-                        output_dtype,\n+                        backend.standardize_dtype(output_dtype),\n                         msg=\"Unexpected output dtype\",\n                     )\n             if eager:\n@@ -274,8 +279,12 @@ class TestCase(unittest.TestCase):\n \n             model = TestModel(layer)\n             model.compile(optimizer=\"sgd\", loss=\"mse\", jit_compile=True)\n-            input_data = nest.map_structure(lambda x: np.array(x), input_data)\n-            output_data = nest.map_structure(lambda x: np.array(x), output_data)\n+            input_data = nest.map_structure(\n+                lambda x: backend.convert_to_numpy(x), input_data\n+            )\n+            output_data = nest.map_structure(\n+                lambda x: backend.convert_to_numpy(x), output_data\n+            )\n             model.fit(input_data, output_data, verbose=0)\n \n         # Build test.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
