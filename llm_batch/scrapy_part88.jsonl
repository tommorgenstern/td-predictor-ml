{"custom_id": "scrapy#07655d05eaf74fd74a9030fc57f408e69df1fd7c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 652 | Contributors (this commit): 3 | Commits (past 90d): 34 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -25,7 +25,7 @@ COMMANDS_MODULE = ''\n COMMANDS_SETTINGS_MODULE = ''\n \n CONCURRENT_ITEMS = 100\n-\n+CONCURRENT_REQUESTS_PER_SPIDER = 8\n CONCURRENT_SPIDERS = 8\n \n COOKIES_DEBUG = False\n@@ -136,7 +136,6 @@ REQUEST_HANDLERS_BASE = {\n }\n \n REQUESTS_QUEUE_SIZE = 0\n-REQUESTS_PER_SPIDER = 8     # max simultaneous requests per domain\n \n # contrib.middleware.retry.RetryMiddleware default settings\n RETRY_TIMES = 2 # initial response + 2 retries = 3 requests\n\n@@ -27,7 +27,7 @@ class SiteInfo(object):\n         if download_delay:\n             self.max_concurrent_requests = 1\n         elif max_concurrent_requests is None:\n-            self.max_concurrent_requests = settings.getint('REQUESTS_PER_SPIDER')\n+            self.max_concurrent_requests = settings.getint('CONCURRENT_REQUESTS_PER_SPIDER')\n         else:\n             self.max_concurrent_requests =  max_concurrent_requests\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f3e861c89e4204870ca02c6f70428455ca762def", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 26 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -14,7 +14,7 @@ class CookiesMiddlewareTest(TestCase):\n         self.mw = CookiesMiddleware()\n \n     def tearDown(self):\n-        self.mw.spider_closed('scrapytest.org')\n+        self.mw.spider_closed(self.spider)\n         del self.mw\n \n     def test_basic(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#505dfe6f7e5d980c76d30e223e883daa02bfdb52", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 193 | Contributors (this commit): 1 | Commits (past 90d): 8 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -18,7 +18,7 @@ from scrapy.utils.response import open_in_browser\n from scrapy.conf import settings\n from scrapy.core.manager import scrapymanager\n from scrapy.core.engine import scrapyengine\n-from scrapy.http import Request\n+from scrapy.http import Request, TextResponse\n \n def relevant_var(varname):\n     return varname not in ['shelp', 'fetch', 'view', '__builtins__', 'In', \\\n@@ -67,6 +67,7 @@ class Shell(object):\n         item = self.item_class()\n         self.vars['item'] = item\n         if url:\n+            if isinstance(response, TextResponse):\n                 self.vars['xxs'] = XmlXPathSelector(response)\n                 self.vars['hxs'] = HtmlXPathSelector(response)\n             self.vars['url'] = url\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#aeab5370cbefe235c2000a80edefbcfe5d20fc20", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 161 | Lines Deleted: 172 | Files Changed: 17 | Hunks: 102 | Methods Changed: 84 | Complexity Δ (Sum/Max): 19/10 | Churn Δ: 333 | Churn Cumulative: 3299 | Contributors (this commit): 5 | Commits (past 90d): 85 | Contributors (cumulative): 34 | DMM Complexity: 0.14285714285714285\n\nDIFF:\n@@ -10,7 +10,7 @@ from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy.core import signals\n from scrapy.stats import stats\n-from scrapy.stats.signals import stats_domain_opened, stats_domain_closing\n+from scrapy.stats.signals import stats_spider_opened, stats_spider_closing\n from scrapy.conf import settings\n \n class CoreStats(object):\n@@ -22,32 +22,32 @@ class CoreStats(object):\n         stats.set_value('envinfo/logfile', settings['LOG_FILE'])\n         stats.set_value('envinfo/pid', os.getpid())\n \n-        dispatcher.connect(self.stats_domain_opened, signal=stats_domain_opened)\n-        dispatcher.connect(self.stats_domain_closing, signal=stats_domain_closing)\n+        dispatcher.connect(self.stats_spider_opened, signal=stats_spider_opened)\n+        dispatcher.connect(self.stats_spider_closing, signal=stats_spider_closing)\n         dispatcher.connect(self.item_scraped, signal=signals.item_scraped)\n         dispatcher.connect(self.item_passed, signal=signals.item_passed)\n         dispatcher.connect(self.item_dropped, signal=signals.item_dropped)\n \n-    def stats_domain_opened(self, domain):\n-        stats.set_value('start_time', datetime.datetime.utcnow(), domain=domain)\n-        stats.set_value('envinfo/host', stats.get_value('envinfo/host'), domain=domain)\n-        stats.inc_value('domain_count/opened')\n+    def stats_spider_opened(self, spider):\n+        stats.set_value('start_time', datetime.datetime.utcnow(), spider=spider)\n+        stats.set_value('envinfo/host', stats.get_value('envinfo/host'), spider=spider)\n+        stats.inc_value('spider_count/opened')\n \n-    def stats_domain_closing(self, domain, reason):\n-        stats.set_value('finish_time', datetime.datetime.utcnow(), domain=domain)\n-        stats.set_value('finish_status', 'OK' if reason == 'finished' else reason, domain=domain)\n-        stats.inc_value('domain_count/%s' % reason, domain=domain)\n+    def stats_spider_closing(self, spider, reason):\n+        stats.set_value('finish_time', datetime.datetime.utcnow(), spider=spider)\n+        stats.set_value('finish_status', 'OK' if reason == 'finished' else reason, spider=spider)\n+        stats.inc_value('spider_count/%s' % reason, spider=spider)\n \n     def item_scraped(self, item, spider):\n-        stats.inc_value('item_scraped_count', domain=spider.domain_name)\n+        stats.inc_value('item_scraped_count', spider=spider)\n         stats.inc_value('item_scraped_count')\n \n     def item_passed(self, item, spider):\n-        stats.inc_value('item_passed_count', domain=spider.domain_name)\n+        stats.inc_value('item_passed_count', spider=spider)\n         stats.inc_value('item_passed_count')\n \n     def item_dropped(self, item, spider, exception):\n         reason = exception.__class__.__name__\n-        stats.inc_value('item_dropped_count', domain=spider.domain_name)\n-        stats.inc_value('item_dropped_reasons_count/%s' % reason, domain=spider.domain_name)\n+        stats.inc_value('item_dropped_count', spider=spider)\n+        stats.inc_value('item_dropped_reasons_count/%s' % reason, spider=spider)\n         stats.inc_value('item_dropped_count')\n\n@@ -5,37 +5,30 @@ from scrapy.stats import stats\n from scrapy.conf import settings\n \n class DownloaderStats(object):\n-    \"\"\"DownloaderStats store stats of all requests, responses and\n-    exceptions that pass through it.\n-\n-    To use this middleware you must enable the DOWNLOADER_STATS setting.\n-    \"\"\"\n \n     def __init__(self):\n         if not settings.getbool('DOWNLOADER_STATS'):\n             raise NotConfigured\n \n     def process_request(self, request, spider):\n-        domain = spider.domain_name\n         stats.inc_value('downloader/request_count')\n-        stats.inc_value('downloader/request_count', domain=domain)\n-        stats.inc_value('downloader/request_method_count/%s' % request.method, domain=domain)\n+        stats.inc_value('downloader/request_count', spider=spider)\n+        stats.inc_value('downloader/request_method_count/%s' % request.method, spider=spider)\n         reqlen = len(request_httprepr(request))\n-        stats.inc_value('downloader/request_bytes', reqlen, domain=domain)\n+        stats.inc_value('downloader/request_bytes', reqlen, spider=spider)\n         stats.inc_value('downloader/request_bytes', reqlen)\n \n     def process_response(self, request, response, spider):\n-        domain = spider.domain_name\n         stats.inc_value('downloader/response_count')\n-        stats.inc_value('downloader/response_count', domain=domain)\n-        stats.inc_value('downloader/response_status_count/%s' % response.status, domain=domain)\n+        stats.inc_value('downloader/response_count', spider=spider)\n+        stats.inc_value('downloader/response_status_count/%s' % response.status, spider=spider)\n         reslen = len(response_httprepr(response))\n-        stats.inc_value('downloader/response_bytes', reslen, domain=domain)\n+        stats.inc_value('downloader/response_bytes', reslen, spider=spider)\n         stats.inc_value('downloader/response_bytes', reslen)\n         return response\n \n     def process_exception(self, request, exception, spider):\n         ex_class = \"%s.%s\" % (exception.__class__.__module__, exception.__class__.__name__)\n         stats.inc_value('downloader/exception_count')\n-        stats.inc_value('downloader/exception_count', domain=spider.domain_name)\n-        stats.inc_value('downloader/exception_type_count/%s' % ex_class, domain=spider.domain_name)\n+        stats.inc_value('downloader/exception_count', spider=spider)\n+        stats.inc_value('downloader/exception_type_count/%s' % ex_class, spider=spider)\n\n@@ -53,11 +53,11 @@ class ItemSamplerPipeline(object):\n         dispatcher.connect(self.engine_stopped, signal=signals.engine_stopped)\n \n     def process_item(self, spider, item):\n-        sampled = stats.get_value(\"items_sampled\", 0, domain=spider.domain_name)\n+        sampled = stats.get_value(\"items_sampled\", 0, spider=spider)\n         if sampled < items_per_spider:\n             self.items[item.guid] = item\n             sampled += 1\n-            stats.set_value(\"items_sampled\", sampled, domain=spider.domain_name)\n+            stats.set_value(\"items_sampled\", sampled, spider=spider)\n             log.msg(\"Sampled %s\" % item, spider=spider, level=log.INFO)\n             if close_spider and sampled == items_per_spider:\n                 scrapyengine.close_spider(spider)\n@@ -71,7 +71,7 @@ class ItemSamplerPipeline(object):\n                 level=log.WARNING)\n \n     def spider_closed(self, spider, reason):\n-        if reason == 'finished' and not stats.get_value(\"items_sampled\", domain=spider.domain_name):\n+        if reason == 'finished' and not stats.get_value(\"items_sampled\", spider=spider):\n             self.empty_domains.add(spider.domain_name)\n         self.spiders_count += 1\n         log.msg(\"Sampled %d domains so far (%d empty)\" % (self.spiders_count, \\\n@@ -87,7 +87,7 @@ class ItemSamplerMiddleware(object):\n             raise NotConfigured\n \n     def process_spider_input(self, response, spider):\n-        if stats.get_value(\"items_sampled\", domain=spider.domain_name) >= items_per_spider:\n+        if stats.get_value(\"items_sampled\", spider=spider) >= items_per_spider:\n             return []\n         elif max_response_size and max_response_size > len(response_httprepr(response)):  \n             return []\n@@ -100,7 +100,7 @@ class ItemSamplerMiddleware(object):\n             else:\n                 items.append(r)\n \n-        if stats.get_value(\"items_sampled\", domain=spider.domain_name) >= items_per_spider:\n+        if stats.get_value(\"items_sampled\", spider=spider) >= items_per_spider:\n             return []\n         else:\n             # TODO: this needs some revision, as keeping only the first item\n\n@@ -219,7 +219,7 @@ class ImagesPipeline(MediaPipeline):\n         msg = 'Image (%s): Downloaded image from %s referred in <%s>' % \\\n                 (status, request, referer)\n         log.msg(msg, level=log.DEBUG, spider=info.spider)\n-        self.inc_stats(info.spider.domain_name, status)\n+        self.inc_stats(info.spider, status)\n \n         try:\n             key = self.image_key(request.url)\n@@ -258,7 +258,7 @@ class ImagesPipeline(MediaPipeline):\n             referer = request.headers.get('Referer')\n             log.msg('Image (uptodate): Downloaded %s from <%s> referred in <%s>' % \\\n                     (self.MEDIA_NAME, request.url, referer), level=log.DEBUG, spider=info.spider)\n-            self.inc_stats(info.spider.domain_name, 'uptodate')\n+            self.inc_stats(info.spider, 'uptodate')\n \n             checksum = result.get('checksum', None)\n             return {'url': request.url, 'path': key, 'checksum': checksum}\n@@ -295,9 +295,9 @@ class ImagesPipeline(MediaPipeline):\n             thumb_image, thumb_buf = self.convert_image(image, size)\n             yield thumb_key, thumb_image, thumb_buf\n \n-    def inc_stats(self, domain, status):\n-        stats.inc_value('image_count', domain=domain)\n-        stats.inc_value('image_status_count/%s' % status, domain=domain)\n+    def inc_stats(self, spider, status):\n+        stats.inc_value('image_count', spider=spider)\n+        stats.inc_value('image_status_count/%s' % status, spider=spider)\n \n     def convert_image(self, image, size=None):\n         if image.mode != 'RGB':\n\n@@ -18,7 +18,6 @@ class DepthMiddleware(object):\n             stats.set_value('envinfo/request_depth_limit', self.maxdepth)\n \n     def process_spider_output(self, response, result, spider):\n-        domain = spider.domain_name\n         def _filter(request):\n             if isinstance(request, Request):\n                 depth = response.request.meta['depth'] + 1\n@@ -28,14 +27,14 @@ class DepthMiddleware(object):\n                         level=log.DEBUG, spider=spider)\n                     return False\n                 elif self.stats:\n-                    stats.inc_value('request_depth_count/%s' % depth, domain=domain)\n-                    if depth > stats.get_value('request_depth_max', 0, domain=domain):\n-                        stats.set_value('request_depth_max', depth, domain=domain)\n+                    stats.inc_value('request_depth_count/%s' % depth, spider=spider)\n+                    if depth > stats.get_value('request_depth_max', 0, spider=spider):\n+                        stats.set_value('request_depth_max', depth, spider=spider)\n             return True\n \n         # base case (depth=0)\n         if self.stats and 'depth' not in response.request.meta: \n             response.request.meta['depth'] = 0\n-            stats.inc_value('request_depth_count/0', domain=domain)\n+            stats.inc_value('request_depth_count/0', spider=spider)\n \n         return (r for r in result or () if _filter(r))\n\n@@ -1,5 +1,5 @@\n \"\"\"\n-StatsMailer extension sends an email when a domain finishes scraping.\n+StatsMailer extension sends an email when a spider finishes scraping.\n \n Use STATSMAILER_RCPTS setting to enable and give the recipient mail address\n \"\"\"\n@@ -17,12 +17,12 @@ class StatsMailer(object):\n         self.recipients = settings.getlist(\"STATSMAILER_RCPTS\")\n         if not self.recipients:\n             raise NotConfigured\n-        dispatcher.connect(self.stats_domain_closed, signal=signals.stats_domain_closed)\n+        dispatcher.connect(self.stats_spider_closed, signal=signals.stats_spider_closed)\n         \n-    def stats_domain_closed(self, domain, domain_stats):\n+    def stats_spider_closed(self, spider, spider_stats):\n         mail = MailSender()\n         body = \"Global stats\\n\\n\"\n         body += \"\\n\".join(\"%-50s : %s\" % i for i in stats.get_stats().items())\n-        body += \"\\n\\n%s stats\\n\\n\" % domain\n-        body += \"\\n\".join(\"%-50s : %s\" % i for i in domain_stats.items())\n-        mail.send(self.recipients, \"Scrapy stats for: %s\" % domain, body)\n+        body += \"\\n\\n%s stats\\n\\n\" % spider.domain_name\n+        body += \"\\n\".join(\"%-50s : %s\" % i for i in spider_stats.items())\n+        mail.send(self.recipients, \"Scrapy stats for: %s\" % spider.domain_name, body)\n\n@@ -22,9 +22,9 @@ class StatsDump(object):\n         s = banner(self)\n         s += \"<h3>Global stats</h3>\\n\"\n         s += stats_html_table(stats.get_stats())\n-        for domain in stats.list_domains():\n-            s += \"<h3>%s</h3>\\n\" % domain\n-            s += stats_html_table(stats.get_stats(domain))\n+        for spider, spider_stats in stats.iter_spider_stats():\n+            s += \"<h3>%s</h3>\\n\" % spider.domain_name\n+            s += stats_html_table(spider_stats)\n         s += \"</body>\\n\"\n         s += \"</html>\\n\"\n \n\n@@ -45,20 +45,18 @@ class SpiderProfiler(object):\n             r = function(*args, **kwargs)\n             mafter = self._memusage()\n             ct = time() - tbefore\n-            domain = spider.domain_name\n-            tcc = stats.get_value('profiling/total_callback_time', 0, domain=domain)\n-            sct = stats.get_value('profiling/slowest_callback_time', 0, domain=domain)\n-            stats.set_value('profiling/total_callback_time' % spider.domain_name, \\\n-                tcc+ct, domain=domain)\n+            tcc = stats.get_value('profiling/total_callback_time', 0, spider=spider)\n+            sct = stats.get_value('profiling/slowest_callback_time', 0, spider=spider)\n+            stats.set_value('profiling/total_callback_time', tcc+ct, spider=spider)\n             if ct > sct:\n-                stats.set_value('profiling/slowest_callback_time', ct, domain=domain)\n+                stats.set_value('profiling/slowest_callback_time', ct, spider=spider)\n                 stats.set_value('profiling/slowest_callback_name', function.__name__, \\\n-                    domain=domain)\n+                    spider=spider)\n                 stats.set_value('profiling/slowest_callback_url', args[0].url, \\\n-                    domain=domain)\n+                    spider=spider)\n             if self._memusage:\n                 stats.inc_value('profiling/total_mem_allocated_in_callbacks', \\\n-                    count=mafter-mbefore, domain=domain)\n+                    count=mafter-mbefore, spider=spider)\n             return r\n         return new_callback\n \n\n@@ -244,7 +244,7 @@ class ExecutionEngine(object):\n \n         self.downloader.open_spider(spider)\n         self.scraper.open_spider(spider)\n-        stats.open_domain(spider.domain_name)\n+        stats.open_spider(spider)\n \n         send_catch_log(signals.spider_opened, sender=self.__class__, spider=spider)\n \n@@ -305,7 +305,7 @@ class ExecutionEngine(object):\n         reason = self.closing.pop(spider, 'finished')\n         send_catch_log(signal=signals.spider_closed, sender=self.__class__, \\\n             spider=spider, reason=reason)\n-        stats.close_domain(spider.domain_name, reason=reason)\n+        stats.close_spider(spider, reason=reason)\n         dfd = defer.maybeDeferred(spiders.close_spider, spider)\n         dfd.addBoth(log.msg, \"Spider closed (%s)\" % reason, spider=spider)\n         reactor.callLater(0, self._mainloop)\n\n@@ -87,17 +87,17 @@ class Scraper(object):\n     def enqueue_scrape(self, response, request, spider):\n         site = self.sites[spider]\n         dfd = site.add_response_request(response, request)\n-        # FIXME: this can't be called here because the stats domain may be\n+        # FIXME: this can't be called here because the stats spider may be\n         # already closed\n         #stats.max_value('scraper/max_active_size', site.active_size, \\\n-        #    domain=spider.domain_name)\n+        #    spider=spider)\n         def finish_scraping(_):\n             site.finish_response(response)\n             self._scrape_next(spider, site)\n             return _\n         dfd.addBoth(finish_scraping)\n         dfd.addErrback(log.err, 'Scraper bug processing %s' % request, \\\n-            domain=spider.domain_name)\n+            spider=spider)\n         self._scrape_next(spider, site)\n         return dfd\n \n@@ -138,7 +138,7 @@ class Scraper(object):\n             (request.url, referer, _failure)\n         log.msg(msg, log.ERROR, spider=spider)\n         stats.inc_value(\"spider_exceptions/%s\" % _failure.value.__class__.__name__, \\\n-            domain=spider.domain_name)\n+            spider=spider)\n \n     def handle_spider_output(self, result, request, response, spider):\n         if not result:\n@@ -152,7 +152,6 @@ class Scraper(object):\n         from the given spider\n         \"\"\"\n         # TODO: keep closing state internally instead of checking engine\n-        domain = spider.domain_name\n         if spider in self.engine.closing:\n             return\n         elif isinstance(output, Request):\n@@ -165,10 +164,10 @@ class Scraper(object):\n             send_catch_log(signal=signals.item_scraped, sender=self.__class__, \\\n                 item=output, spider=spider, response=response)\n             self.sites[spider].itemproc_size += 1\n-            # FIXME: this can't be called here because the stats domain may be\n+            # FIXME: this can't be called here because the stats spider may be\n             # already closed\n             #stats.max_value('scraper/max_itemproc_size', \\\n-            #        self.sites[domain].itemproc_size, domain=domain)\n+            #        self.sites[spider].itemproc_size, spider=spider)\n             dfd = self.itemproc.process_item(output, spider)\n             dfd.addBoth(self._itemproc_finished, output, spider)\n             return dfd\n@@ -195,7 +194,6 @@ class Scraper(object):\n     def _itemproc_finished(self, output, item, spider):\n         \"\"\"ItemProcessor finished for the given ``item`` and returned ``output``\n         \"\"\"\n-        domain = spider.domain_name\n         self.sites[spider].itemproc_size -= 1\n         if isinstance(output, Failure):\n             ex = output.value\n\n@@ -5,8 +5,8 @@ import pprint\n \n from scrapy.xlib.pydispatch import dispatcher\n \n-from scrapy.stats.signals import stats_domain_opened, stats_domain_closing, \\\n-    stats_domain_closed\n+from scrapy.stats.signals import stats_spider_opened, stats_spider_closing, \\\n+    stats_spider_closed\n from scrapy.utils.signal import send_catch_log\n from scrapy.core import signals\n from scrapy import log\n@@ -19,57 +19,57 @@ class StatsCollector(object):\n         self._stats = {None: {}} # None is for global stats\n         dispatcher.connect(self._engine_stopped, signal=signals.engine_stopped)\n \n-    def get_value(self, key, default=None, domain=None):\n-        return self._stats[domain].get(key, default)\n+    def get_value(self, key, default=None, spider=None):\n+        return self._stats[spider].get(key, default)\n \n-    def get_stats(self, domain=None):\n-        return self._stats[domain]\n+    def get_stats(self, spider=None):\n+        return self._stats[spider]\n \n-    def set_value(self, key, value, domain=None):\n-        self._stats[domain][key] = value\n+    def set_value(self, key, value, spider=None):\n+        self._stats[spider][key] = value\n \n-    def set_stats(self, stats, domain=None):\n-        self._stats[domain] = stats\n+    def set_stats(self, stats, spider=None):\n+        self._stats[spider] = stats\n \n-    def inc_value(self, key, count=1, start=0, domain=None):\n-        d = self._stats[domain]\n+    def inc_value(self, key, count=1, start=0, spider=None):\n+        d = self._stats[spider]\n         d[key] = d.setdefault(key, start) + count\n \n-    def max_value(self, key, value, domain=None):\n-        d = self._stats[domain]\n+    def max_value(self, key, value, spider=None):\n+        d = self._stats[spider]\n         d[key] = max(d.setdefault(key, value), value)\n \n-    def min_value(self, key, value, domain=None):\n-        d = self._stats[domain]\n+    def min_value(self, key, value, spider=None):\n+        d = self._stats[spider]\n         d[key] = min(d.setdefault(key, value), value)\n \n-    def clear_stats(self, domain=None):\n-        self._stats[domain].clear()\n+    def clear_stats(self, spider=None):\n+        self._stats[spider].clear()\n \n-    def list_domains(self):\n-        return [d for d in self._stats.keys() if d is not None]\n+    def iter_spider_stats(self):\n+        return [x for x in self._stats.iteritems() if x[0]]\n \n-    def open_domain(self, domain):\n-        self._stats[domain] = {}\n-        send_catch_log(stats_domain_opened, domain=domain)\n+    def open_spider(self, spider):\n+        self._stats[spider] = {}\n+        send_catch_log(stats_spider_opened, spider=spider)\n \n-    def close_domain(self, domain, reason):\n-        send_catch_log(stats_domain_closing, domain=domain, reason=reason)\n-        stats = self._stats.pop(domain)\n-        send_catch_log(stats_domain_closed, domain=domain, reason=reason, \\\n-            domain_stats=stats)\n+    def close_spider(self, spider, reason):\n+        send_catch_log(stats_spider_closing, spider=spider, reason=reason)\n+        stats = self._stats.pop(spider)\n+        send_catch_log(stats_spider_closed, spider=spider, reason=reason, \\\n+            spider_stats=stats)\n         if self._dump:\n-            log.msg(\"Dumping domain stats:\\n\" + pprint.pformat(stats), \\\n-                domain=domain)\n-        self._persist_stats(stats, domain)\n+            log.msg(\"Dumping spider stats:\\n\" + pprint.pformat(stats), \\\n+                spider=spider)\n+        self._persist_stats(stats, spider)\n \n     def _engine_stopped(self):\n         stats = self.get_stats()\n         if self._dump:\n             log.msg(\"Dumping global stats:\\n\" + pprint.pformat(stats))\n-        self._persist_stats(stats, domain=None)\n+        self._persist_stats(stats, spider=None)\n \n-    def _persist_stats(self, stats, domain=None):\n+    def _persist_stats(self, stats, spider=None):\n         pass\n \n class MemoryStatsCollector(StatsCollector):\n@@ -78,28 +78,29 @@ class MemoryStatsCollector(StatsCollector):\n         super(MemoryStatsCollector, self).__init__()\n         self.domain_stats = {}\n \n-    def _persist_stats(self, stats, domain=None):\n-        self.domain_stats[domain] = stats\n+    def _persist_stats(self, stats, spider=None):\n+        if spider is not None:\n+            self.domain_stats[spider.domain_name] = stats\n \n \n class DummyStatsCollector(StatsCollector):\n \n-    def get_value(self, key, default=None, domain=None):\n+    def get_value(self, key, default=None, spider=None):\n         return default\n \n-    def set_value(self, key, value, domain=None):\n+    def set_value(self, key, value, spider=None):\n         pass\n \n-    def set_stats(self, stats, domain=None):\n+    def set_stats(self, stats, spider=None):\n         pass\n \n-    def inc_value(self, key, count=1, start=0, domain=None):\n+    def inc_value(self, key, count=1, start=0, spider=None):\n         pass\n \n-    def max_value(self, key, value, domain=None):\n+    def max_value(self, key, value, spider=None):\n         pass\n \n-    def min_value(self, key, value, domain=None):\n+    def min_value(self, key, value, spider=None):\n         pass\n \n \n\n@@ -16,8 +16,8 @@ class MysqlStatsCollector(StatsCollector):\n         mysqluri = settings['STATS_MYSQL_URI']\n         self._mysql_conn = mysql_connect(mysqluri, use_unicode=False) if mysqluri else None\n         \n-    def _persist_stats(self, stats, domain=None):\n-        if domain is None: # only store domain-specific stats\n+    def _persist_stats(self, stats, spider=None):\n+        if spider is None: # only store spider-specific stats\n             return\n         if self._mysql_conn is None:\n             return\n@@ -27,5 +27,5 @@ class MysqlStatsCollector(StatsCollector):\n \n         c = self._mysql_conn.cursor()\n         c.execute(\"INSERT INTO %s (domain,stored,data) VALUES (%%s,%%s,%%s)\" % table, \\\n-            (domain, stored, datas))\n+            (spider.domain_name, stored, datas))\n         self._mysql_conn.commit()\n\n@@ -22,27 +22,27 @@ class SimpledbStatsCollector(StatsCollector):\n         self._async = settings.getbool('STATS_SDB_ASYNC')\n         connect_sdb().create_domain(self._sdbdomain)\n \n-    def _persist_stats(self, stats, domain=None):\n-        if domain is None: # only store domain-specific stats\n+    def _persist_stats(self, stats, spider=None):\n+        if spider is None: # only store spider-specific stats\n             return\n         if not self._sdbdomain:\n             return\n         if self._async:\n-            dfd = threads.deferToThread(self._persist_to_sdb, domain, stats.copy())\n+            dfd = threads.deferToThread(self._persist_to_sdb, spider, stats.copy())\n             dfd.addErrback(log.err, 'Error uploading stats to SimpleDB', \\\n-                domain=domain)\n+                spider=spider)\n         else:\n-            self._persist_to_sdb(domain, stats)\n+            self._persist_to_sdb(spider, stats)\n \n-    def _persist_to_sdb(self, domain, stats):\n-        ts = self._get_timestamp(domain).isoformat()\n-        sdb_item_id = \"%s_%s\" % (domain, ts)\n+    def _persist_to_sdb(self, spider, stats):\n+        ts = self._get_timestamp(spider).isoformat()\n+        sdb_item_id = \"%s_%s\" % (spider.domain_name, ts)\n         sdb_item = dict((k, self._to_sdb_value(v, k)) for k, v in stats.iteritems())\n-        sdb_item['domain'] = domain\n+        sdb_item['domain'] = spider.domain_name\n         sdb_item['timestamp'] = self._to_sdb_value(ts)\n         connect_sdb().put_attributes(self._sdbdomain, sdb_item_id, sdb_item)\n \n-    def _get_timestamp(self, domain):\n+    def _get_timestamp(self, spider):\n         return datetime.utcnow()\n \n     def _to_sdb_value(self, obj, key=None):\n\n@@ -1,3 +1,3 @@\n-stats_domain_opened = object()\n-stats_domain_closing = object()\n-stats_domain_closed = object()\n+stats_spider_opened = object()\n+stats_spider_closing = object()\n+stats_spider_closed = object()\n\n@@ -1,6 +1,5 @@\n from unittest import TestCase\n \n-from scrapy.conf import settings\n from scrapy.contrib.downloadermiddleware.stats import DownloaderStats\n from scrapy.http import Request, Response\n from scrapy.spider import BaseSpider\n@@ -10,11 +9,10 @@ from scrapy.stats import stats\n class TestDownloaderStats(TestCase):\n \n     def setUp(self):\n-        self.spider = BaseSpider()\n-        self.spider.domain_name = 'scrapytest.org'\n+        self.spider = BaseSpider('scrapytest.org')\n         self.mw = DownloaderStats()\n \n-        stats.open_domain(self.spider.domain_name)\n+        stats.open_spider(self.spider)\n \n         self.req = Request('scrapytest.org')\n         self.res = Response('scrapytest.org', status=400)\n@@ -22,18 +20,18 @@ class TestDownloaderStats(TestCase):\n     def test_process_request(self):\n         self.mw.process_request(self.req, self.spider)\n         self.assertEqual(stats.get_value('downloader/request_count', \\\n-            domain=self.spider.domain_name), 1)\n+            spider=self.spider), 1)\n         \n     def test_process_response(self):\n         self.mw.process_response(self.req, self.res, self.spider)\n         self.assertEqual(stats.get_value('downloader/response_count', \\\n-            domain=self.spider.domain_name), 1)\n+            spider=self.spider), 1)\n \n     def test_process_exception(self):\n         self.mw.process_exception(self.req, Exception(), self.spider)\n         self.assertEqual(stats.get_value('downloader/exception_count', \\\n-            domain=self.spider.domain_name), 1)\n+            spider=self.spider), 1)\n \n     def tearDown(self):\n-        stats.close_domain(self.spider.domain_name, '')\n+        stats.close_spider(self.spider, '')\n \n\n@@ -14,10 +14,9 @@ class TestDepthMiddleware(TestCase):\n         settings.overrides['DEPTH_LIMIT'] = 1\n         settings.overrides['DEPTH_STATS'] = True\n \n-        self.spider = BaseSpider()\n-        self.spider.domain_name = 'scrapytest.org'\n+        self.spider = BaseSpider('scrapytest.org')\n \n-        stats.open_domain(self.spider.domain_name)\n+        stats.open_spider(self.spider)\n \n         self.mw = DepthMiddleware()\n         self.assertEquals(stats.get_value('envinfo/request_depth_limit'), 1)\n@@ -31,8 +30,7 @@ class TestDepthMiddleware(TestCase):\n         out = list(self.mw.process_spider_output(resp, result, self.spider))\n         self.assertEquals(out, result)\n \n-        rdc = stats.get_value('request_depth_count/1',\n-                              domain=self.spider.domain_name)\n+        rdc = stats.get_value('request_depth_count/1', spider=self.spider)\n         self.assertEquals(rdc, 1)\n \n         req.meta['depth'] = 1\n@@ -40,8 +38,7 @@ class TestDepthMiddleware(TestCase):\n         out2 = list(self.mw.process_spider_output(resp, result, self.spider))\n         self.assertEquals(out2, [])\n \n-        rdm = stats.get_value('request_depth_max',\n-                              domain=self.spider.domain_name)\n+        rdm = stats.get_value('request_depth_max', spider=self.spider)\n         self.assertEquals(rdm, 1)\n  \n     def tearDown(self):\n@@ -49,5 +46,5 @@ class TestDepthMiddleware(TestCase):\n         del settings.overrides['DEPTH_STATS']\n         settings.disabled = True\n \n-        stats.close_domain(self.spider.domain_name, '')\n+        stats.close_spider(self.spider, '')\n \n\n@@ -1,12 +1,16 @@\n import unittest \n \n+from scrapy.spider import BaseSpider\n from scrapy.xlib.pydispatch import dispatcher\n from scrapy.stats.collector import StatsCollector, DummyStatsCollector\n-from scrapy.stats.signals import stats_domain_opened, stats_domain_closing, \\\n-    stats_domain_closed\n+from scrapy.stats.signals import stats_spider_opened, stats_spider_closing, \\\n+    stats_spider_closed\n \n class StatsCollectorTest(unittest.TestCase):\n \n+    def setUp(self):\n+        self.spider = BaseSpider()\n+\n     def test_collector(self):\n         stats = StatsCollector()\n         self.assertEqual(stats.get_stats(), {})\n@@ -43,44 +47,45 @@ class StatsCollectorTest(unittest.TestCase):\n         stats.inc_value('v1')\n         stats.max_value('v2', 100)\n         stats.min_value('v3', 100)\n-        stats.open_domain('a')\n-        stats.set_value('test', 'value', domain='a')\n+        stats.open_spider('a')\n+        stats.set_value('test', 'value', spider=self.spider)\n         self.assertEqual(stats.get_stats(), {})\n         self.assertEqual(stats.get_stats('a'), {})\n \n     def test_signals(self):\n         signals_catched = set()\n \n-        def domain_opened(domain):\n-            assert domain == 'example.com'\n-            signals_catched.add(stats_domain_opened)\n+        def spider_opened(spider):\n+            assert spider is self.spider\n+            signals_catched.add(stats_spider_opened)\n \n-        def domain_closing(domain, reason):\n-            assert domain == 'example.com'\n+        def spider_closing(spider, reason):\n+            assert spider is self.spider\n             assert reason == 'testing'\n-            signals_catched.add(stats_domain_closing)\n+            signals_catched.add(stats_spider_closing)\n \n-        def domain_closed(domain, reason, domain_stats):\n-            assert domain == 'example.com'\n+        def spider_closed(spider, reason, spider_stats):\n+            assert spider is self.spider\n             assert reason == 'testing'\n-            assert domain_stats == {'test': 1}\n-            signals_catched.add(stats_domain_closed)\n+            assert spider_stats == {'test': 1}\n+            signals_catched.add(stats_spider_closed)\n \n-        dispatcher.connect(domain_opened, signal=stats_domain_opened)\n-        dispatcher.connect(domain_closing, signal=stats_domain_closing)\n-        dispatcher.connect(domain_closed, signal=stats_domain_closed)\n+        dispatcher.connect(spider_opened, signal=stats_spider_opened)\n+        dispatcher.connect(spider_closing, signal=stats_spider_closing)\n+        dispatcher.connect(spider_closed, signal=stats_spider_closed)\n \n         stats = StatsCollector()\n-        stats.open_domain('example.com')\n-        stats.set_value('test', 1, domain='example.com')\n-        stats.close_domain('example.com', 'testing')\n-        assert stats_domain_opened in signals_catched\n-        assert stats_domain_closing in signals_catched\n-        assert stats_domain_closed in signals_catched\n+        stats.open_spider(self.spider)\n+        stats.set_value('test', 1, spider=self.spider)\n+        self.assertEqual([(self.spider, {'test': 1})], list(stats.iter_spider_stats()))\n+        stats.close_spider(self.spider, 'testing')\n+        assert stats_spider_opened in signals_catched\n+        assert stats_spider_closing in signals_catched\n+        assert stats_spider_closed in signals_catched\n \n-        dispatcher.disconnect(domain_opened, signal=stats_domain_opened)\n-        dispatcher.disconnect(domain_closing, signal=stats_domain_closing)\n-        dispatcher.disconnect(domain_closed, signal=stats_domain_closed)\n+        dispatcher.disconnect(spider_opened, signal=stats_spider_opened)\n+        dispatcher.disconnect(spider_closing, signal=stats_spider_closing)\n+        dispatcher.disconnect(spider_closed, signal=stats_spider_closed)\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#445d8cd9f038f8252101fbeabac7d7a174667ef9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1250 | Contributors (this commit): 4 | Commits (past 90d): 22 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -260,7 +260,7 @@ class ExecutionEngine(object):\n             dispatcher.send(signal=signals.spider_idle, sender=self.__class__, \\\n                 spider=spider)\n         except DontCloseDomain:\n-            self.next_request(spider)\n+            reactor.callLater(5, self.next_request, spider)\n             return\n         except:\n             log.err(\"Exception catched on spider_idle signal dispatch\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bf55a4708f662f64dc5b1cfd2e8a6a31395bef1c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 19 | Churn Cumulative: 70 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -13,8 +13,7 @@ if sys.version_info < (2,5):\n     sys.exit(1)\n \n # monkey patches to fix external library issues\n-from scrapy.xlib.patches import apply_patches\n-apply_patches()\n+from scrapy.xlib import twisted_250_monkeypatches\n \n # optional_features is a set containing Scrapy optional features\n optional_features = set()\n\n@@ -1,13 +1,9 @@\n \"\"\"\n Monkey patches for supporting Twisted 2.5.0\n+\n+NOTE: This module must not fail if twisted module is not available.\n \"\"\"\n \n-import twisted\n-\n-def apply_patches():\n-    if twisted.__version__ < '8.0.0':\n-        add_missing_blockingCallFromThread()\n-\n # This function comes bundled with Twisted 8.x and above\n def add_missing_blockingCallFromThread():\n     import Queue\n@@ -42,3 +38,11 @@ def add_missing_blockingCallFromThread():\n \n     from twisted.internet import threads\n     threads.blockingCallFromThread = blockingCallFromThread\n+\n+try:\n+    import twisted\n+    if twisted.__version__ < '8.0.0':\n+        add_missing_blockingCallFromThread()\n+except ImportError:\n+    pass\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9ff4dbf636f627dbcda3a0a40ef98d0537a6afa7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f86f62c5d994df2bb8fafe22ebf874dd9e3c1ff9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 19 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 45 | Churn Cumulative: 85 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -1,8 +1,7 @@\n # Scrapy setup.py script \n #\n-# Most of the code here was taken from Django setup.py\n-\n-from distutils.core import setup\n+# It doesn't depend on setuptools, but if setuptools is available it'll use\n+# some of its features, like package dependencies.\n from distutils.command.install_data import install_data\n from distutils.command.install import INSTALL_SCHEMES\n import os\n@@ -80,21 +79,21 @@ version = __import__('scrapy').__version__\n if u'SVN' in version:\n     version = ' '.join(version.split(' ')[:-1])\n \n-setup(\n-    name = 'scrapy',\n-    version = version,\n-    url = 'http://scrapy.org',\n-    description = 'A high-level Python Screen Scraping framework',\n-    long_description = 'Scrapy is a high level scraping and web crawling framework for writing spiders to crawl and parse web pages for all kinds of purposes, from information retrieval to monitoring or testing web sites.',\n-    author = 'Scrapy developers',\n-    maintainer = 'Pablo Hoffman',\n-    maintainer_email = 'pablo@pablohoffman.com',\n-    license = 'BSD',\n-    packages = packages,\n-    cmdclass = cmdclasses,\n-    data_files = data_files,\n-    scripts = ['bin/scrapy-ctl.py'],\n-    classifiers = [\n+setup_args = {\n+    'name': 'scrapy',\n+    'version': version,\n+    'url': 'http://scrapy.org',\n+    'description': 'A high-level Python Screen Scraping framework',\n+    'long_description': 'Scrapy is a high level scraping and web crawling framework for writing spiders to crawl and parse web pages for all kinds of purposes, from information retrieval to monitoring or testing web sites.',\n+    'author': 'Scrapy developers',\n+    'maintainer': 'Pablo Hoffman',\n+    'maintainer_email': 'pablo@pablohoffman.com',\n+    'license': 'BSD',\n+    'packages': packages,\n+    'cmdclass': cmdclasses,\n+    'data_files': data_files,\n+    'scripts': ['bin/scrapy-ctl.py'],\n+    'classifiers': [\n         'Programming Language :: Python',\n         'Programming Language :: Python :: 2.5',\n         'Programming Language :: Python :: 2.6',\n@@ -108,4 +107,12 @@ setup(\n         'Topic :: Software Development :: Libraries :: Python Modules',\n         'Topic :: Internet :: WWW/HTTP',\n     ]\n-)\n+}\n+\n+try:\n+    from setuptools import setup\n+    setup_args['install_requires'] = ['Twisted>=2.5']\n+except ImportError:\n+    from distutils.core import setup\n+ \n+setup(**setup_args)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a49aef2beb4e5c5c1513c36401e53b605d066b88", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 9 | Files Changed: 3 | Hunks: 7 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 19 | Churn Cumulative: 1323 | Contributors (this commit): 4 | Commits (past 90d): 24 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,7 +10,7 @@ from collections import defaultdict\n \n from scrapy.core import signals\n from scrapy.core.engine import scrapyengine\n-from scrapy.core.exceptions import NotConfigured, DontCloseDomain\n+from scrapy.core.exceptions import NotConfigured, DontCloseSpider\n from scrapy.conf import settings\n \n \n@@ -33,7 +33,7 @@ class DelayedCloseDomain(object):\n             lastseen = self.opened_at[spider]\n \n         if time() < lastseen + self.delay:\n-            raise DontCloseDomain\n+            raise DontCloseSpider\n \n     def spider_closed(self, spider):\n         self.opened_at.pop(spider, None)\n\n@@ -16,7 +16,7 @@ from scrapy.conf import settings\n from scrapy.core import signals\n from scrapy.core.downloader import Downloader\n from scrapy.core.scraper import Scraper\n-from scrapy.core.exceptions import IgnoreRequest, DontCloseDomain\n+from scrapy.core.exceptions import IgnoreRequest, DontCloseSpider\n from scrapy.http import Response, Request\n from scrapy.spider import spiders\n from scrapy.utils.misc import load_object\n@@ -251,7 +251,7 @@ class ExecutionEngine(object):\n     def _spider_idle(self, spider):\n         \"\"\"Called when a spider gets idle. This function is called when there\n         are no remaining pages to download or schedule. It can be called\n-        multiple times. If some extension raises a DontCloseDomain exception\n+        multiple times. If some extension raises a DontCloseSpider exception\n         (in the spider_idle signal handler) the spider is not closed until the\n         next loop and this function is guaranteed to be called (at least) once\n         again for this spider.\n@@ -259,11 +259,12 @@ class ExecutionEngine(object):\n         try:\n             dispatcher.send(signal=signals.spider_idle, sender=self.__class__, \\\n                 spider=spider)\n-        except DontCloseDomain:\n+        except DontCloseSpider:\n             reactor.callLater(5, self.next_request, spider)\n             return\n-        except:\n-            log.err(\"Exception catched on spider_idle signal dispatch\")\n+        except Exception, e:\n+            log.msg(\"Exception caught on 'spider_idle' signal dispatch: %r\" % e, \\\n+                level=log.ERROR)\n         if self.spider_is_idle(spider):\n             self.close_spider(spider, reason='finished')\n \n\n@@ -25,8 +25,8 @@ class IgnoreRequest(Exception):\n     def __str__(self):\n         return self.msg\n \n-class DontCloseDomain(Exception):\n-    \"\"\"Request the domain not to be closed yet\"\"\"\n+class DontCloseSpider(Exception):\n+    \"\"\"Request the spider not to be closed yet\"\"\"\n     pass\n \n # Items\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#326090d4a4dd365b5b44088deb264d98b3efcbc2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 263 | Contributors (this commit): 3 | Commits (past 90d): 20 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -42,8 +42,6 @@ DEFAULT_RESPONSE_ENCODING = 'ascii'\n DEPTH_LIMIT = 0\n DEPTH_STATS = True\n \n-SPIDER_SCHEDULER = 'scrapy.contrib.spiderscheduler.FifoSpiderScheduler'\n-\n DOWNLOAD_DELAY = 0\n DOWNLOAD_TIMEOUT = 180      # 3mins\n \n@@ -172,6 +170,8 @@ SPIDER_MIDDLEWARES_BASE = {\n \n SPIDER_MODULES = []\n \n+SPIDER_SCHEDULER = 'scrapy.contrib.spiderscheduler.FifoSpiderScheduler'\n+\n SPIDERPROFILER_ENABLED = False\n \n STATS_CLASS = 'scrapy.stats.collector.MemoryStatsCollector'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1a81ab6f070d5ab7b7aca6803a7c53f7dc0ff5fd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 8 | Churn Cumulative: 8 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,6 +1,6 @@\n \"\"\"\n-DelayedCloseDomain is an extension that keeps open a domain until a\n-configurable amount of idle time is reached\n+SpiderCloseDelay is an extension that keeps a idle spiders open until a\n+configurable amount of idle time has elapsed\n \"\"\"\n \n from time import time\n@@ -14,9 +14,9 @@ from scrapy.core.exceptions import NotConfigured, DontCloseSpider\n from scrapy.conf import settings\n \n \n-class DelayedCloseDomain(object):\n+class SpiderCloseDelay(object):\n     def __init__(self):\n-        self.delay = settings.getint('DOMAIN_CLOSE_DELAY')\n+        self.delay = settings.getint('SPIDER_CLOSE_DELAY')\n         if not self.delay:\n             raise NotConfigured\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
