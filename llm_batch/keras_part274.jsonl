{"custom_id": "keras#6272e662ae0c99ce7cad912992d4c55e22c8a369", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 14 | Methods Changed: 12 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 26 | Churn Cumulative: 518 | Contributors (this commit): 10 | Commits (past 90d): 5 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,7 @@\n \"\"\"Tests for functions in data_utils.py.\n \"\"\"\n import os\n+import time\n import sys\n import tarfile\n import threading\n@@ -136,6 +137,7 @@ class DummySequence(Sequence):\n         self.inner = value\n \n     def __getitem__(self, item):\n+        time.sleep(0.05)\n         return np.ones(self.shape, dtype=np.uint32) * item * self.inner\n \n     def __len__(self):\n@@ -169,7 +171,7 @@ def create_generator_from_sequence_pcs(ds):\n \n def test_generator_enqueuer_threads():\n     enqueuer = GeneratorEnqueuer(create_generator_from_sequence_threads(\n-        DummySequence([3, 200, 200, 3])), use_multiprocessing=False)\n+        DummySequence([3, 10, 10, 3])), use_multiprocessing=False)\n     enqueuer.start(3, 10)\n     gen_output = enqueuer.get()\n     acc = []\n@@ -187,7 +189,7 @@ def test_generator_enqueuer_threads():\n \n def test_generator_enqueuer_processes():\n     enqueuer = GeneratorEnqueuer(create_generator_from_sequence_pcs(\n-        DummySequence([3, 200, 200, 3])), use_multiprocessing=True)\n+        DummySequence([3, 10, 10, 3])), use_multiprocessing=True)\n     enqueuer.start(3, 10)\n     gen_output = enqueuer.get()\n     acc = []\n@@ -200,7 +202,7 @@ def test_generator_enqueuer_processes():\n \n def test_generator_enqueuer_threadsafe():\n     enqueuer = GeneratorEnqueuer(create_generator_from_sequence_pcs(\n-        DummySequence([3, 200, 200, 3])), use_multiprocessing=False)\n+        DummySequence([3, 10, 10, 3])), use_multiprocessing=False)\n     enqueuer.start(3, 10)\n     gen_output = enqueuer.get()\n     with pytest.raises(RuntimeError) as e:\n@@ -228,7 +230,7 @@ def test_generator_enqueuer_fail_processes():\n \n \n def test_ordered_enqueuer_threads():\n-    enqueuer = OrderedEnqueuer(DummySequence([3, 200, 200, 3]),\n+    enqueuer = OrderedEnqueuer(DummySequence([3, 10, 10, 3]),\n                                use_multiprocessing=False)\n     enqueuer.start(3, 10)\n     gen_output = enqueuer.get()\n@@ -241,7 +243,7 @@ def test_ordered_enqueuer_threads():\n \n \n def test_ordered_enqueuer_threads_not_ordered():\n-    enqueuer = OrderedEnqueuer(DummySequence([3, 200, 200, 3]),\n+    enqueuer = OrderedEnqueuer(DummySequence([3, 10, 10, 3]),\n                                use_multiprocessing=False,\n                                shuffle=True)\n     enqueuer.start(3, 10)\n@@ -256,7 +258,7 @@ def test_ordered_enqueuer_threads_not_ordered():\n \n @use_spawn\n def test_ordered_enqueuer_processes():\n-    enqueuer = OrderedEnqueuer(DummySequence([3, 200, 200, 3]),\n+    enqueuer = OrderedEnqueuer(DummySequence([3, 10, 10, 3]),\n                                use_multiprocessing=True)\n     enqueuer.start(3, 10)\n     gen_output = enqueuer.get()\n@@ -278,7 +280,7 @@ def test_ordered_enqueuer_fail_threads():\n \n @use_spawn\n def test_on_epoch_end_processes():\n-    enqueuer = OrderedEnqueuer(DummySequence([3, 200, 200, 3]),\n+    enqueuer = OrderedEnqueuer(DummySequence([3, 10, 10, 3]),\n                                use_multiprocessing=True)\n     enqueuer.start(3, 10)\n     gen_output = enqueuer.get()\n@@ -292,9 +294,9 @@ def test_on_epoch_end_processes():\n \n @use_spawn\n def test_context_switch():\n-    enqueuer = OrderedEnqueuer(DummySequence([3, 200, 200, 3]),\n+    enqueuer = OrderedEnqueuer(DummySequence([3, 10, 10, 3]),\n                                use_multiprocessing=True)\n-    enqueuer2 = OrderedEnqueuer(DummySequence([3, 200, 200, 3], value=15),\n+    enqueuer2 = OrderedEnqueuer(DummySequence([3, 10, 10, 3], value=15),\n                                 use_multiprocessing=True)\n     enqueuer.start(3, 10)\n     enqueuer2.start(3, 10)\n@@ -324,7 +326,7 @@ def test_context_switch():\n \n \n def test_on_epoch_end_threads():\n-    enqueuer = OrderedEnqueuer(DummySequence([3, 200, 200, 3]),\n+    enqueuer = OrderedEnqueuer(DummySequence([3, 10, 10, 3]),\n                                use_multiprocessing=False)\n     enqueuer.start(3, 10)\n     gen_output = enqueuer.get()\n@@ -361,7 +363,7 @@ def create_finite_generator_from_sequence_pcs(ds):\n \n def test_finite_generator_enqueuer_threads():\n     enqueuer = GeneratorEnqueuer(create_finite_generator_from_sequence_threads(\n-        DummySequence([3, 200, 200, 3])), use_multiprocessing=False)\n+        DummySequence([3, 10, 10, 3])), use_multiprocessing=False)\n     enqueuer.start(3, 10)\n     gen_output = enqueuer.get()\n     acc = []\n@@ -373,7 +375,7 @@ def test_finite_generator_enqueuer_threads():\n \n def test_finite_generator_enqueuer_processes():\n     enqueuer = GeneratorEnqueuer(create_finite_generator_from_sequence_pcs(\n-        DummySequence([3, 200, 200, 3])), use_multiprocessing=True)\n+        DummySequence([3, 10, 10, 3])), use_multiprocessing=True)\n     enqueuer.start(3, 10)\n     gen_output = enqueuer.get()\n     acc = []\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#882302d2b0b6a5aad20dc08a98a9384165b3ab0a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 58 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 15 | Methods Changed: 5 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 65 | Churn Cumulative: 233 | Contributors (this commit): 13 | Commits (past 90d): 5 | Contributors (cumulative): 15 | DMM Complexity: 0.35\n\nDIFF:\n@@ -34,7 +34,10 @@ def _check_pydot():\n def model_to_dot(model,\n                  show_shapes=False,\n                  show_layer_names=True,\n-                 rankdir='TB'):\n+                 rankdir='TB',\n+                 expand_nested=False,\n+                 dpi=96,\n+                 subgraph=False):\n     \"\"\"Convert a Keras model to dot format.\n \n     # Arguments\n@@ -45,17 +48,29 @@ def model_to_dot(model,\n             a string specifying the format of the plot:\n             'TB' creates a vertical plot;\n             'LR' creates a horizontal plot.\n+        expand_nested: whether to expand nested models into clusters.\n+        dpi: dot DPI.\n+        subgraph: whether to return a pydot.Cluster instance.\n \n     # Returns\n-        A `pydot.Dot` instance representing the Keras model.\n+        A `pydot.Dot` instance representing the Keras model or\n+        a `pydot.Cluster` instance representing nested model if\n+        `subgraph=True`.\n     \"\"\"\n     from ..layers.wrappers import Wrapper\n+    from ..models import Model\n     from ..models import Sequential\n \n     _check_pydot()\n+    if subgraph:\n+        dot = pydot.Cluster(style='dashed')\n+        dot.set('label', model.name)\n+        dot.set('labeljust', 'l')\n+    else:\n         dot = pydot.Dot()\n         dot.set('rankdir', rankdir)\n         dot.set('concentrate', True)\n+        dot.set('dpi', dpi)\n         dot.set_node_defaults(shape='record')\n \n     if isinstance(model, Sequential):\n@@ -64,13 +79,26 @@ def model_to_dot(model,\n     layers = model._layers\n \n     # Create graph nodes.\n-    for layer in layers:\n+    for i, layer in enumerate(layers):\n         layer_id = str(id(layer))\n \n         # Append a wrapped layer's label to node's label, if it exists.\n         layer_name = layer.name\n         class_name = layer.__class__.__name__\n         if isinstance(layer, Wrapper):\n+            if expand_nested and isinstance(layer.layer, Model):\n+                submodel = model_to_dot(layer.layer, show_shapes,\n+                                        show_layer_names, rankdir, expand_nested,\n+                                        subgraph=True)\n+                model_nodes = submodel.get_nodes()\n+                dot.add_edge(pydot.Edge(layer_id, model_nodes[0].get_name()))\n+                if len(layers) > i + 1:\n+                    next_layer_id = str(id(layers[i + 1]))\n+                    dot.add_edge(pydot.Edge(\n+                        model_nodes[len(model_nodes) - 1].get_name(),\n+                        next_layer_id))\n+                dot.add_subgraph(submodel)\n+            else:\n                 layer_name = '{}({})'.format(layer_name, layer.layer.name)\n                 child_class_name = layer.layer.__class__.__name__\n                 class_name = '{}({})'.format(class_name, child_class_name)\n@@ -107,9 +135,13 @@ def model_to_dot(model,\n             node_key = layer.name + '_ib-' + str(i)\n             if node_key in model._network_nodes:\n                 for inbound_layer in node.inbound_layers:\n+                    if not expand_nested or not (\n+                            isinstance(inbound_layer, Wrapper) and\n+                            isinstance(inbound_layer.layer, Model)):\n                         inbound_layer_id = str(id(inbound_layer))\n                         # Make sure that both nodes exist before connecting them with\n-                    # an edge, as add_edge would otherwise create any missing node.\n+                        # an edge, as add_edge would otherwise\n+                        # create any missing node.\n                         assert dot.get_node(inbound_layer_id)\n                         assert dot.get_node(layer_id)\n                         dot.add_edge(pydot.Edge(inbound_layer_id, layer_id))\n@@ -120,7 +152,9 @@ def plot_model(model,\n                to_file='model.png',\n                show_shapes=False,\n                show_layer_names=True,\n-               rankdir='TB'):\n+               rankdir='TB',\n+               expand_nested=False,\n+               dpi=96):\n     \"\"\"Converts a Keras model to dot format and save to a file.\n \n     # Arguments\n@@ -132,8 +166,11 @@ def plot_model(model,\n             a string specifying the format of the plot:\n             'TB' creates a vertical plot;\n             'LR' creates a horizontal plot.\n+        expand_nested: whether to expand nested models into clusters.\n+        dpi: dot DPI.\n     \"\"\"\n-    dot = model_to_dot(model, show_shapes, show_layer_names, rankdir)\n+    dot = model_to_dot(model, show_shapes, show_layer_names, rankdir,\n+                       expand_nested, dpi)\n     _, extension = os.path.splitext(to_file)\n     if not extension:\n         extension = 'png'\n\n@@ -2,7 +2,9 @@ import pytest\n import os\n import sys\n import numpy as np\n-from keras.layers import Conv2D\n+from keras import Input, Model\n+\n+from keras.layers import Conv2D, Bidirectional\n from keras.layers import Dense\n from keras.layers import Embedding\n from keras.layers import Flatten\n@@ -26,6 +28,18 @@ def test_plot_model():\n     vis_utils.plot_model(model, to_file='model2.png', show_shapes=True)\n     os.remove('model2.png')\n \n+    inner_input = Input(shape=(2, 3), dtype='float32', name='inner_input')\n+    inner_lstm = Bidirectional(LSTM(16, name='inner_lstm'), name='bd')(inner_input)\n+    encoder = Model(inner_input, inner_lstm, name='Encoder_Model')\n+    outer_input = Input(shape=(5, 2, 3), dtype='float32', name='input')\n+    inner_encoder = TimeDistributed(encoder, name='td_encoder')(outer_input)\n+    lstm = LSTM(16, name='outer_lstm')(inner_encoder)\n+    preds = Dense(5, activation='softmax', name='predictions')(lstm)\n+    model = Model(outer_input, preds)\n+    vis_utils.plot_model(model, to_file='model3.png', show_shapes=True,\n+                         expand_nested=True, dpi=300)\n+    os.remove('model3.png')\n+\n \n def test_plot_sequential_embedding():\n     \"\"\"Fixes #11376\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#33487127c84608fe52fab9f7ed48986b1c19d02f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 5263 | Contributors (this commit): 85 | Commits (past 90d): 24 | Contributors (cumulative): 113 | DMM Complexity: 1.0\n\nDIFF:\n@@ -793,7 +793,7 @@ class TensorBoard(Callback):\n                 for weight in layer.weights:\n                     mapped_weight_name = weight.name.replace(':', '_')\n                     tf.summary.histogram(mapped_weight_name, weight)\n-                    if self.write_grads:\n+                    if self.write_grads and weight in layer.trainable_weights:\n                         grads = model.optimizer.get_gradients(model.total_loss,\n                                                               weight)\n \n\n@@ -759,6 +759,7 @@ def test_TensorBoard_convnet(tmpdir):\n         MaxPooling2D(pool_size=2),\n         Conv2D(filters=4, kernel_size=(3, 3),\n                activation='relu', padding='same'),\n+        BatchNormalization(),\n         GlobalAveragePooling2D(),\n         Dense(num_classes, activation='softmax')\n     ])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9148325b9ca9edf94aaebd453b3cf86dba2ef99a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 338 | Lines Deleted: 115 | Files Changed: 5 | Hunks: 26 | Methods Changed: 9 | Complexity Δ (Sum/Max): 33/19 | Churn Δ: 453 | Churn Cumulative: 27026 | Contributors (this commit): 159 | Commits (past 90d): 107 | Contributors (cumulative): 279 | DMM Complexity: 0.25547445255474455\n\nDIFF:\n@@ -570,42 +570,141 @@ def batch_dot(x, y, axes=None):\n     x_shape = int_shape(x)\n     y_shape = int_shape(y)\n \n+    x_ndim = len(x_shape)\n+    y_ndim = len(y_shape)\n+\n+    if x_ndim < 2 or y_ndim < 2:\n+        raise ValueError('Can not do batch_dot on inputs '\n+                         'with rank < 2. '\n+                         'Received inputs with shapes ' +\n+                         str(x_shape) + ' and ' +\n+                         str(y_shape) + '.')\n+\n+    x_batch_size = x_shape[0]\n+    y_batch_size = y_shape[0]\n+\n+    if x_batch_size is not None and y_batch_size is not None:\n+        if x_batch_size != y_batch_size:\n+            raise ValueError('Can not do batch_dot on inputs '\n+                             'with different batch sizes. '\n+                             'Received inputs with shapes ' +\n+                             str(x_shape) + ' and ' +\n+                             str(y_shape) + '.')\n+\n     if isinstance(axes, int):\n-        axes = (axes, axes)\n+        axes = [axes, axes]\n+\n     if axes is None:\n-        # behaves like tf.batch_matmul as default\n-        axes = [len(x_shape) - 1, len(y_shape) - 2]\n+        if y_ndim == 2:\n+            axes = [x_ndim - 1, y_ndim - 1]\n+        else:\n+            axes = [x_ndim - 1, y_ndim - 2]\n+\n     if b_any([isinstance(a, (list, tuple)) for a in axes]):\n         raise ValueError('Multiple target dimensions are not supported. ' +\n                          'Expected: None, int, (int, int), ' +\n                          'Provided: ' + str(axes))\n \n-    if len(x_shape) == 2 and len(y_shape) == 2:\n-        if axes[0] == axes[1]:\n-            result = sum(x * y, axis=axes[0], keepdims=True)\n-            return result if axes[0] == 1 else transpose(result)\n-        else:\n-            return sum(x * transpose(y), axis=axes[0], keepdims=True)\n-    else:\n-        if len(y_shape) == 2:\n-            y = expand_dims(y)\n+    # if tuple, convert to list\n+    axes = list(axes)\n \n-        normalized_axis = []\n-        normalized_axis.append(_normalize_axis(axes[0], x)[0])\n-        normalized_axis.append(_normalize_axis(axes[1], y)[0])\n-        # transpose\n-        i = normalized_axis[0]\n-        while i < len(x.shape) - 1:\n-            x = C.swapaxes(x, i, i + 1)\n-            i += 1\n-        i = normalized_axis[1]\n-        while i > 0:\n-            y = C.swapaxes(y, i, i - 1)\n-            i -= 1\n-        result = C.times(x, y, output_rank=(len(y.shape) - 1)\n-                         if len(y.shape) > 1 else 1)\n-        if len(y_shape) == 2:\n+    # convert negative indices\n+    if axes[0] < 0:\n+        axes[0] += x_ndim\n+    if axes[1] < 0:\n+        axes[1] += y_ndim\n+\n+    if 0 in axes:\n+        raise ValueError('Can not perform batch_dot over axis 0.'\n+                         ' If your inputs are not batched,'\n+                         ' add a dummy batch dimension to your '\n+                         'inputs using K.expand_dims(x, 0)')\n+    d1 = x_shape[axes[0]]\n+    d2 = y_shape[axes[1]]\n+\n+    if d1 is not None and d2 is not None and d1 != d2:\n+        raise ValueError('Can not do batch_dot on inputs with shapes ' +\n+                         str(x_shape) + ' and ' + str(y_shape) +\n+                         ' with axes=' + str(axes) + '. x.shape[%d] != '\n+                         'y.shape[%d] (%d != %d).' % (axes[0], axes[1], d1, d2))\n+\n+    # Input shapes:\n+    # x: (b_size, x1, ..., d, ..., xn)\n+    # y: (b_size, y1, ..., d, ..., yn)\n+    # where d is the dimension to reduce.\n+\n+    # Bring d to the last dimension in x\n+    # x: (b_size, ..., d)\n+\n+    permute_pattern = list(range(x_ndim))\n+    for i in range(axes[0], x_ndim - 1):\n+        permute_pattern[i] = permute_pattern[i + 1]\n+    permute_pattern[-1] = axes[0]\n+\n+    x = permute_dimensions(x, permute_pattern)\n+\n+    # Bring d to the second dimension in y\n+    # y: (b_size, d, ...)\n+    permute_pattern = list(range(y_ndim))\n+\n+    for i in range(axes[1], 1, -1):\n+        permute_pattern[i] = permute_pattern[i - 1]\n+    permute_pattern[1] = axes[1]\n+    y = permute_dimensions(y, permute_pattern)\n+\n+    # Expand to rank 3 if needed\n+    if x_ndim == 2:\n+        x = expand_dims(x, 1)\n+        x_expanded = True\n+    else:\n+        x_expanded = False\n+\n+    if y_ndim == 2:\n+        y = expand_dims(y, -1)\n+        y_expanded = True\n+    else:\n+        y_expanded = False\n+\n+    x_shape = int_shape(x)\n+    y_shape = int_shape(y)\n+\n+    # batch size might be lost at this point\n+    x_batch_size = x_shape[0]\n+    y_batch_size = y_shape[0]\n+\n+    if x_batch_size is None and y_batch_size is None:\n+        dynamic_batch_size = True\n+    elif x_batch_size is not None and y_batch_size is not None:\n+        dynamic_batch_size = False\n+    else:\n+        raise ValueError('Can not perform batch_dot on inputs' +\n+                         ' with both static and dynamic batch sizes.' +\n+                         'You probably attempted to permform the ' +\n+                         'operation on a placeholder and a variable, ' +\n+                         'which is not yet supported on the CNTK backend.')\n+\n+    if dynamic_batch_size:\n+        result = C.times(x, y, output_rank=y_ndim - 2 + int(y_expanded))\n+    else:\n+        result = []\n+\n+        for i in range(x_batch_size):\n+            xi = x[i]\n+            yi = y[i]\n+            if ndim(xi) == ndim(x):  # for older versions of CNTK\n+                xi = squeeze(xi, 0)\n+                yi = squeeze(yi, 0)\n+            result.append(C.times(xi, yi, output_rank=y_ndim - 2 + int(y_expanded)))\n+        result = stack(result, 0)\n+\n+    if x_expanded:\n+        result = squeeze(result, 1)\n+\n+    if y_expanded:\n         result = squeeze(result, -1)\n+\n+    if ndim(result) == 1:\n+        return expand_dims(result)\n     return result\n \n \n@@ -1133,6 +1232,11 @@ def concatenate(tensors, axis=-1):\n     return C.splice(*tensors, axis=axis[0])\n \n \n+def stack(x, axis=0):\n+    x = [expand_dims(t, axis) for t in x]\n+    return concatenate(x, axis)\n+\n+\n def flatten(x):\n     return reshape(x, (-1,))\n \n\n@@ -1090,7 +1090,7 @@ def batch_dot(x, y, axes=None):\n     \"\"\"Batchwise dot product.\n \n     `batch_dot` is used to compute dot product of `x` and `y` when\n-    `x` and `y` are data in batch, i.e. in a shape of\n+    `x` and `y` are data in batches, i.e. in a shape of\n     `(batch_size, :)`.\n     `batch_dot` results in a tensor or variable with less dimensions\n     than the input. If the number of dimensions is reduced to 1,\n@@ -1099,8 +1099,7 @@ def batch_dot(x, y, axes=None):\n     # Arguments\n         x: Keras tensor or variable with `ndim >= 2`.\n         y: Keras tensor or variable with `ndim >= 2`.\n-        axes: list of (or single) int with target dimensions.\n-            The lengths of `axes[0]` and `axes[1]` should be the same.\n+        axes: int or tupe(int, int). Target dimensions to be reduced.\n \n     # Returns\n         A tensor with shape equal to the concatenation of `x`'s shape\n@@ -1114,6 +1113,14 @@ def batch_dot(x, y, axes=None):\n         of `x.dot(y.T)`, although we never have to calculate the off-diagonal\n         elements.\n \n+        Pseudocode:\n+        ```\n+        inner_products = []\n+        for xi, yi in zip(x, y):\n+            inner_products.append(xi.dot(yi))\n+        result = stack(inner_prodcuts)\n+        ```\n+\n         Shape inference:\n         Let `x`'s shape be `(100, 20)` and `y`'s shape be `(100, 30, 20)`.\n         If `axes` is (1, 2), to find the output shape of resultant tensor,\n@@ -1132,52 +1139,105 @@ def batch_dot(x, y, axes=None):\n     ```python\n         >>> x_batch = K.ones(shape=(32, 20, 1))\n         >>> y_batch = K.ones(shape=(32, 30, 20))\n-        >>> xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=[1, 2])\n+        >>> xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=(1, 2))\n         >>> K.int_shape(xy_batch_dot)\n         (32, 1, 30)\n     ```\n     \"\"\"\n+    x_shape = int_shape(x)\n+    y_shape = int_shape(y)\n+\n+    x_ndim = len(x_shape)\n+    y_ndim = len(y_shape)\n+\n+    if x_ndim < 2 or y_ndim < 2:\n+        raise ValueError('Can not do batch_dot on inputs '\n+                         'with rank < 2. '\n+                         'Received inputs with shapes ' +\n+                         str(x_shape) + ' and ' +\n+                         str(y_shape) + '.')\n+\n+    x_batch_size = x_shape[0]\n+    y_batch_size = y_shape[0]\n+\n+    if x_batch_size is not None and y_batch_size is not None:\n+        if x_batch_size != y_batch_size:\n+            raise ValueError('Can not do batch_dot on inputs '\n+                             'with different batch sizes. '\n+                             'Received inputs with shapes ' +\n+                             str(x_shape) + ' and ' +\n+                             str(y_shape) + '.')\n+\n     if isinstance(axes, int):\n-        axes = (axes, axes)\n-    x_ndim = ndim(x)\n-    y_ndim = ndim(y)\n+        axes = [axes, axes]\n+\n     if axes is None:\n-        # behaves like tf.batch_matmul as default\n+        if y_ndim == 2:\n+            axes = [x_ndim - 1, y_ndim - 1]\n+        else:\n             axes = [x_ndim - 1, y_ndim - 2]\n+\n     if py_any([isinstance(a, (list, tuple)) for a in axes]):\n         raise ValueError('Multiple target dimensions are not supported. ' +\n                          'Expected: None, int, (int, int), ' +\n                          'Provided: ' + str(axes))\n-    if x_ndim > y_ndim:\n-        diff = x_ndim - y_ndim\n-        y = tf.reshape(y, tf.concat([tf.shape(y), [1] * (diff)], axis=0))\n-    elif y_ndim > x_ndim:\n-        diff = y_ndim - x_ndim\n-        x = tf.reshape(x, tf.concat([tf.shape(x), [1] * (diff)], axis=0))\n-    else:\n-        diff = 0\n-    if ndim(x) == 2 and ndim(y) == 2:\n-        if axes[0] == axes[1]:\n-            out = tf.reduce_sum(tf.multiply(x, y), axes[0])\n-        else:\n-            out = tf.reduce_sum(tf.multiply(tf.transpose(x, [1, 0]), y), axes[1])\n-    else:\n-        if axes is not None:\n-            adj_x = None if axes[0] == ndim(x) - 1 else True\n-            adj_y = True if axes[1] == ndim(y) - 1 else None\n-        else:\n-            adj_x = None\n-            adj_y = None\n-        out = tf.matmul(x, y, adjoint_a=adj_x, adjoint_b=adj_y)\n-    if diff:\n-        if x_ndim > y_ndim:\n-            idx = x_ndim + y_ndim - 3\n-        else:\n-            idx = x_ndim - 1\n-        out = tf.squeeze(out, list(range(idx, idx + diff)))\n-    if ndim(out) == 1:\n-        out = expand_dims(out, 1)\n-    return out\n+\n+    # if tuple, convert to list\n+    axes = list(axes)\n+\n+    # convert negative indices\n+    if axes[0] < 0:\n+        axes[0] += x_ndim\n+    if axes[1] < 0:\n+        axes[1] += y_ndim\n+\n+    # sanity checks\n+    if 0 in axes:\n+        raise ValueError('Can not perform batch_dot over axis 0.'\n+                         'If your inputs are not batched,'\n+                         ' add a dummy batch dimension to your '\n+                         'inputs using K.expand_dims(x, 0)')\n+\n+    a0, a1 = axes\n+    d1 = x_shape[a0]\n+    d2 = y_shape[a1]\n+\n+    if d1 is not None and d2 is not None and d1 != d2:\n+        raise ValueError('Can not do batch_dot on inputs with shapes ' +\n+                         str(x_shape) + ' and ' + str(y_shape) +\n+                         ' with axes=' + str(axes) + '. x.shape[%d] != '\n+                         'y.shape[%d] (%d != %d).' % (axes[0], axes[1], d1, d2))\n+\n+    # bring the dimensions to be reduced to axis 1\n+    if a0 != 1:\n+        pattern = list(range(x_ndim))\n+        for i in range(a0, 1, -1):\n+            pattern[i] = pattern[i - 1]\n+        pattern[1] = a0\n+        x = permute_dimensions(x, pattern)\n+    if a1 != 1:\n+        pattern = list(range(y_ndim))\n+        for i in range(a1, 1, -1):\n+            pattern[i] = pattern[i - 1]\n+        pattern[1] = a1\n+        y = permute_dimensions(y, pattern)\n+\n+    # reshape to closest broadcastable shape\n+    x_shape = tf.shape(x)\n+    y_shape = tf.shape(y)\n+\n+    new_x_shape = tf.concat([x_shape, tf.ones_like(y_shape[2:])], 0)\n+    new_y_shape = tf.concat([y_shape[:2], tf.ones_like(x_shape[2:]), y_shape[2:]], 0)\n+\n+    x = reshape(x, new_x_shape)\n+    y = reshape(y, new_y_shape)\n+\n+    result = tf.reduce_sum(x * y, 1)\n+\n+    if ndim(result) == 1:\n+        result = tf.expand_dims(result, -1)\n+\n+    return result\n \n \n def transpose(x):\n\n@@ -465,6 +465,9 @@ def batch_dot(x, y, axes=None):\n         axes = (axes, axes)\n     if axes is None:\n         # behaves like tf.batch_matmul as default\n+        if y.ndim == 2:\n+            axes = [x.ndim - 1, y.ndim - 1]\n+        else:\n             axes = [x.ndim - 1, y.ndim - 2]\n     if py_any([isinstance(a, (list, tuple)) for a in axes]):\n         raise ValueError('Multiple target dimensions are not supported. ' +\n@@ -473,14 +476,11 @@ def batch_dot(x, y, axes=None):\n     if isinstance(axes, tuple):\n         axes = list(axes)\n \n-    # workaround because theano doesn't accept axes\n-    # which contains the batch axis (0)\n-    if axes[0] == 0:\n-        x = transpose(x)\n-        axes[0] = x.ndim - 1\n-    if axes[1] == 0:\n-        y = transpose(y)\n-        axes[1] = y.ndim - 1\n+    if 0 in axes:\n+        raise ValueError('Can not perform batch_dot over axis 0.'\n+                         'If your inputs are not batched,'\n+                         ' add a dummy batch dimension to your '\n+                         'inputs using K.expand_dims(x, 0)')\n \n     out = T.batched_tensordot(x, y, axes=axes)\n     if ndim(out) == 1:\n\n@@ -232,6 +232,16 @@ class TestBackend(object):\n                                    WITH_NP, cntk_two_dynamicity=True, axes=1)\n         check_two_tensor_operation('batch_dot', (32, 20), (32, 20),\n                                    WITH_NP, cntk_two_dynamicity=True, axes=(1, 1))\n+        check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 5, 3),\n+                                   WITH_NP, axes=(2, 2))\n+        check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 3),\n+                                   WITH_NP, axes=(2, 1))\n+        check_two_tensor_operation('batch_dot', (4, 2), (4, 2, 3),\n+                                   WITH_NP, axes=(1, 1))\n+        check_two_tensor_operation('batch_dot', (32, 20), (32, 20),\n+                                   WITH_NP, axes=1)\n+        check_two_tensor_operation('batch_dot', (32, 20), (32, 20),\n+                                   WITH_NP, axes=(1, 1))\n \n         check_single_tensor_operation('transpose', (4, 2), WITH_NP)\n         check_single_tensor_operation('reverse', (4, 3, 2), WITH_NP, axes=1)\n@@ -248,21 +258,55 @@ class TestBackend(object):\n                                       shape_or_val=False,\n                                       assert_value_equality=False)\n \n-    @pytest.mark.skipif(K.backend() != 'tensorflow', reason='Not supported.')\n     def test_batch_dot_shape(self):\n-        x_batch = K.ones(shape=(32, 20))\n-        y_batch = K.ones(shape=(32, 20))\n-        xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=1)\n-        assert_allclose(K.eval(xy_batch_dot), np.ones((32, 1)) * 20, atol=1e-05)\n-        xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=0)\n-        assert_allclose(K.eval(xy_batch_dot), np.ones((20, 1)) * 32, atol=1e-05)\n-        # making sure swapping axes when ndim == 2 works\n-        x_batch = K.ones(shape=(32, 20))\n-        y_batch = K.ones(shape=(20, 32))\n-        xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=(0, 1))\n-        assert_allclose(K.eval(xy_batch_dot), np.ones((20, 1)) * 32, atol=1e-05)\n-        xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=(1, 0))\n-        assert_allclose(K.eval(xy_batch_dot), np.ones((32, 1)) * 20, atol=1e-05)\n+        # Note : batch_dot implementation is different for\n+        # placeholders and variables in CNTK backend\n+\n+        test_cases = []\n+        test_cases.append([(None, 3, 4, 5), (None, 2, 3, 4), (2, 3)])\n+        test_cases.append([(None, 3, 4, 5), (None, 2, 4), 2])\n+        test_cases.append([(None, 3, 4), (None, 2, 3, 4), (2, 3)])\n+        test_cases.append([(None, 4, 3), (None, 3, 5), (2, 1)])\n+        test_cases.append([(None, 4), (None, 3, 4), (1, 2)])\n+        test_cases.append([(None, 4), (None, 4), None])\n+\n+        batch_size = 7\n+\n+        def batch_shape(shape):\n+            return (batch_size, ) + shape[1:]\n+\n+        def random(shape):\n+            return np.random.random(batch_shape(shape))\n+\n+        for x_shape, y_shape, axes in test_cases:\n+            x_np = random(x_shape)\n+            y_np = random(y_shape)\n+            z_np = KNP.batch_dot(x_np, y_np, axes)\n+\n+            # test with placeholders\n+            x = K.placeholder(shape=x_shape)\n+            y = K.placeholder(shape=y_shape)\n+            z = K.batch_dot(x, y, axes)\n+\n+            z_shape = K.int_shape(z)\n+            if z_shape is not None:\n+                assert z_shape[1:] == z_np.shape[1:]\n+\n+            f = K.function([x, y], [z])\n+\n+            assert_allclose(f([x_np, y_np])[0], z_np, atol=1e-05)\n+\n+            # test with variables\n+            x = K.variable(x_np)\n+            y = K.variable(y_np)\n+            z = K.batch_dot(x, y, axes)\n+\n+            z_shape = K.int_shape(z)\n+            if z_shape is not None:\n+                assert z_shape == z_np.shape\n+\n+            z = K.eval(z)\n+            assert_allclose(z, z_np, atol=1e-05)\n \n     def test_shape_operations(self):\n         check_two_tensor_operation('concatenate', (4, 3), (4, 2), WITH_NP,\n\n@@ -514,41 +514,56 @@ def dot(x, y):\n \n \n def batch_dot(x, y, axes=None):\n+    if x.ndim < 2 or y.ndim < 2:\n+        raise ValueError('Batch dot requires inputs of rank 2 or more.')\n+\n     if isinstance(axes, int):\n-        axes = (axes, axes)\n+        axes = [axes, axes]\n+    elif isinstance(axes, tuple):\n+        axes = list(axes)\n+\n     if axes is None:\n-        # behaves like tf.batch_matmul as default\n+        if y.ndim == 2:\n+            axes = [x.ndim - 1, y.ndim - 1]\n+        else:\n             axes = [x.ndim - 1, y.ndim - 2]\n+\n     if any([isinstance(a, (list, tuple)) for a in axes]):\n         raise ValueError('Multiple target dimensions are not supported. ' +\n                          'Expected: None, int, (int, int), ' +\n                          'Provided: ' + str(axes))\n-    if x.ndim > y.ndim:\n-        diff = x.ndim - y.ndim\n-        y = np.reshape(y, np.concatenate([np.shape(y), [1] * diff], axis=0))\n-    else:\n-        diff = 0\n-    if ndim(x) == 2 and ndim(y) == 2:\n-        if axes[0] == axes[1]:\n-            out = np.sum(np.multiply(x, y), axes[0])\n-        else:\n-            out = np.sum(np.multiply(np.transpose(x, [1, 0]), y), axes[1])\n-    else:\n-        out = np.tensordot(x, y, axes=axes)\n-        for axis in [axes[0]]:\n-            axis_list = np.arange(len(out.shape) - 1).tolist()\n-            axis_list.insert(0, axis_list.pop(axis))\n-            out = np.transpose(np.diagonal(out, axis1=0, axis2=axis),\n-                               tuple(axis_list))\n-    if diff:\n-        if x.ndim > y.ndim:\n-            idx = x.ndim + y.ndim - 3\n-        else:\n-            idx = x.ndim - 1\n-        out = np.squeeze(out, tuple(range(idx, idx + diff)))\n-    if ndim(out) == 1:\n-        out = expand_dims(out, 1)\n-    return out\n+\n+    # Handle negative axes\n+    if axes[0] < 0:\n+        axes[0] += x.ndim\n+    if axes[1] < 0:\n+        axes[1] += y.ndim\n+\n+    if 0 in axes:\n+        raise ValueError('Can not perform batch dot over axis 0.')\n+\n+    if x.shape[0] != y.shape[0]:\n+        raise ValueError('Can not perform batch dot on inputs'\n+                         ' with different batch sizes.')\n+\n+    d1 = x.shape[axes[0]]\n+    d2 = y.shape[axes[1]]\n+    if d1 != d2:\n+        raise ValueError('Can not do batch_dot on inputs with shapes ' +\n+                         str(x.shape) + ' and ' + str(y.shape) +\n+                         ' with axes=' + str(axes) + '. x.shape[%d] != '\n+                         'y.shape[%d] (%d != %d).' % (axes[0], axes[1], d1, d2))\n+\n+    result = []\n+    axes = [axes[0] - 1, axes[1] - 1]  # ignore batch dimension\n+    for xi, yi in zip(x, y):\n+        result.append(np.tensordot(xi, yi, axes))\n+    result = np.array(result)\n+\n+    if result.ndim == 1:\n+        result = np.expand_dims(result, -1)\n+\n+    return result\n \n \n def transpose(x):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#74264ea3cd5e8bdfe90325f98944297d7f409018", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 37 | Lines Deleted: 9 | Files Changed: 3 | Hunks: 11 | Methods Changed: 6 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 46 | Churn Cumulative: 9357 | Contributors (this commit): 105 | Commits (past 90d): 9 | Contributors (cumulative): 119 | DMM Complexity: 1.0\n\nDIFF:\n@@ -73,6 +73,7 @@ from keras import layers\n from keras.layers import advanced_activations\n from keras.layers import noise\n from keras.layers import wrappers\n+from keras.utils import generic_utils\n from keras import initializers\n from keras import optimizers\n from keras import callbacks\n@@ -352,9 +353,9 @@ ROOT = 'http://keras.io/'\n def get_function_signature(function, method=True):\n     wrapped = getattr(function, '_original_function', None)\n     if wrapped is None:\n-        signature = inspect.getargspec(function)\n+        signature = generic_utils.getargspec(function)\n     else:\n-        signature = inspect.getargspec(wrapped)\n+        signature = generic_utils.getargspec(wrapped)\n     defaults = signature.defaults\n     if method:\n         args = signature.args[1:]\n\n@@ -4,10 +4,10 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-import inspect\n-\n from .. import backend\n from .. import utils\n+from ..utils import generic_utils\n+\n from keras_preprocessing import image\n \n random_rotation = image.random_rotation\n@@ -25,7 +25,7 @@ load_img = image.load_img\n def array_to_img(x, data_format=None, scale=True, dtype=None):\n     if data_format is None:\n         data_format = backend.image_data_format()\n-    if 'dtype' in inspect.getargspec(image.array_to_img).args:\n+    if 'dtype' in generic_utils.getargspec(image.array_to_img).args:\n         if dtype is None:\n             dtype = backend.floatx()\n         return image.array_to_img(x,\n@@ -40,7 +40,7 @@ def array_to_img(x, data_format=None, scale=True, dtype=None):\n def img_to_array(img, data_format=None, dtype=None):\n     if data_format is None:\n         data_format = backend.image_data_format()\n-    if 'dtype' in inspect.getargspec(image.img_to_array).args:\n+    if 'dtype' in generic_utils.getargspec(image.img_to_array).args:\n         if dtype is None:\n             dtype = backend.floatx()\n         return image.img_to_array(img, data_format=data_format, dtype=dtype)\n@@ -142,7 +142,7 @@ class DirectoryIterator(image.DirectoryIterator, Iterator):\n         if data_format is None:\n             data_format = backend.image_data_format()\n         kwargs = {}\n-        if 'dtype' in inspect.getargspec(\n+        if 'dtype' in generic_utils.getargspec(\n                 image.ImageDataGenerator.__init__).args:\n             if dtype is None:\n                 dtype = backend.floatx()\n@@ -210,7 +210,7 @@ class NumpyArrayIterator(image.NumpyArrayIterator, Iterator):\n         if data_format is None:\n             data_format = backend.image_data_format()\n         kwargs = {}\n-        if 'dtype' in inspect.getargspec(\n+        if 'dtype' in generic_utils.getargspec(\n                 image.NumpyArrayIterator.__init__).args:\n             if dtype is None:\n                 dtype = backend.floatx()\n@@ -436,7 +436,7 @@ class ImageDataGenerator(image.ImageDataGenerator):\n         if data_format is None:\n             data_format = backend.image_data_format()\n         kwargs = {}\n-        if 'dtype' in inspect.getargspec(\n+        if 'dtype' in generic_utils.getargspec(\n                 image.ImageDataGenerator.__init__).args:\n             if dtype is None:\n                 dtype = backend.floatx()\n\n@@ -241,6 +241,33 @@ def func_load(code, defaults=None, closure=None, globs=None):\n                                      closure=closure)\n \n \n+def getargspec(fn):\n+    \"\"\"Python 2/3 compatible `getargspec`.\n+\n+    Calls `getfullargspec` and assigns args, varargs,\n+    varkw, and defaults to a python 2/3 compatible `ArgSpec`.\n+    The parameter name 'varkw' is changed to 'keywords' to fit the\n+    `ArgSpec` struct.\n+\n+    # Arguments\n+        fn: the target function to inspect.\n+\n+    # Returns\n+        An ArgSpec with args, varargs, keywords, and defaults parameters\n+        from FullArgSpec.\n+    \"\"\"\n+    if sys.version_info < (3,):\n+        arg_spec = inspect.getargspec(fn)\n+    else:\n+        full_arg_spec = inspect.getfullargspec(fn)\n+        arg_spec = inspect.ArgSpec(\n+            args=full_arg_spec.args,\n+            varargs=full_arg_spec.varargs,\n+            keywords=full_arg_spec.varkw,\n+            defaults=full_arg_spec.defaults)\n+    return arg_spec\n+\n+\n def has_arg(fn, name, accept_all=False):\n     \"\"\"Checks if a callable accepts a given keyword argument.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#df8186833fe47a6fd4b7dbc52fe12f9df73609fb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 30 | Churn Cumulative: 842 | Contributors (this commit): 22 | Commits (past 90d): 13 | Contributors (cumulative): 31 | DMM Complexity: 0.0\n\nDIFF:\n@@ -178,15 +178,21 @@ class H5Dict(object):\n     deserialization.\n \n     # Arguments\n-        path: Either a string (path on disk), a dict, or a HDF5 Group.\n+        path: Either a string (path on disk), a Path, a dict, or a HDF5 Group.\n         mode: File open mode (one of `{\"a\", \"r\", \"w\"}`).\n     \"\"\"\n \n     def __init__(self, path, mode='a'):\n+        def is_path_instance(path):\n+            # We can't use isinstance here because it would require\n+            # us to add pathlib2 to the Python 2 dependencies.\n+            class_name = type(path).__name__\n+            return class_name == 'PosixPath' or class_name == 'WindowsPath'\n+\n         if isinstance(path, h5py.Group):\n             self.data = path\n             self._is_file = False\n-        elif isinstance(path, six.string_types):\n+        elif isinstance(path, six.string_types) or is_path_instance(path):\n             self.data = h5py.File(path, mode=mode)\n             self._is_file = True\n         elif isinstance(path, dict):\n@@ -197,7 +203,7 @@ class H5Dict(object):\n             # Flag to check if a dict is user defined data or a sub group:\n             self.data['_is_group'] = True\n         else:\n-            raise TypeError('Required Group, str or dict. '\n+            raise TypeError('Required Group, str, Path or dict. '\n                             'Received: {}.'.format(type(path)))\n         self.read_only = mode == 'r'\n \n\n@@ -13,8 +13,10 @@ import six\n import h5py\n import tempfile\n try:\n+    from pathlib import Path\n     from unittest.mock import patch\n except:\n+    from pathlib2 import Path\n     from mock import patch\n \n \n@@ -214,5 +216,21 @@ def test_H5Dict_groups():\n     os.remove(h5_path)\n \n \n+def test_H5Dict_accepts_pathlib_Path():\n+    \"\"\"GitHub issue: 11459\"\"\"\n+    _, h5_path = tempfile.mkstemp('.h5')\n+\n+    f = H5Dict(Path(h5_path), mode='w')\n+    f['x'] = 'abcd'\n+    f.close()\n+    del f\n+\n+    f = H5Dict(Path(h5_path), mode='r')\n+    assert f['x'] == 'abcd'\n+    f.close()\n+\n+    os.remove(h5_path)\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#267ccbb4a76913680f4db6b400e05dea7aa84db7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 6548 | Contributors (this commit): 33 | Commits (past 90d): 68 | Contributors (cumulative): 39 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1867,7 +1867,7 @@ class TestBackend(object):\n         num_classes = 20\n         batch_size = 30\n         indices = np.random.randint(0, num_classes, size=(batch_size, input_length))\n-        oh = np.eye(num_classes)[indices]\n+        oh = KNP.one_hot(np.int32(indices), num_classes)\n         koh = K.eval(K.one_hot(K.variable(indices, dtype='int32'), num_classes))\n         assert np.all(koh == oh)\n \n\n@@ -8,6 +8,7 @@ import scipy.signal as signal\n import scipy as sp\n from keras.backend import floatx\n from keras.utils.generic_utils import transpose_shape\n+from keras.utils import to_categorical\n \n \n def normalize_conv(func):\n@@ -684,6 +685,9 @@ def resize_volumes(x, depth_factor, height_factor, width_factor, data_format):\n     return x\n \n \n+def one_hot(indices, num_classes):\n+    return to_categorical(indices, num_classes)\n+\n square = np.square\n abs = np.abs\n exp = np.exp\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#aa0f12ce9eacd70932466d248fde746e0b0dfa0d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 1794 | Contributors (this commit): 31 | Commits (past 90d): 5 | Contributors (cumulative): 31 | DMM Complexity: None\n\nDIFF:\n@@ -73,7 +73,6 @@ from keras import layers\n from keras.layers import advanced_activations\n from keras.layers import noise\n from keras.layers import wrappers\n-from keras.utils import generic_utils\n from keras import initializers\n from keras import optimizers\n from keras import callbacks\n@@ -353,9 +352,9 @@ ROOT = 'http://keras.io/'\n def get_function_signature(function, method=True):\n     wrapped = getattr(function, '_original_function', None)\n     if wrapped is None:\n-        signature = generic_utils.getargspec(function)\n+        signature = inspect.getargspec(function)\n     else:\n-        signature = generic_utils.getargspec(wrapped)\n+        signature = inspect.getargspec(wrapped)\n     defaults = signature.defaults\n     if method:\n         args = signature.args[1:]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a37b41d5ae799c338958b3eeeb884074dfba865f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 47 | Churn Cumulative: 2175 | Contributors (this commit): 31 | Commits (past 90d): 7 | Contributors (cumulative): 33 | DMM Complexity: 0.2857142857142857\n\nDIFF:\n@@ -447,12 +447,16 @@ def count_leading_spaces(s):\n         return 0\n \n \n-def process_list_block(docstring, starting_point, leading_spaces, marker):\n+def process_list_block(docstring, starting_point, section_end,\n+                       leading_spaces, marker):\n     ending_point = docstring.find('\\n\\n', starting_point)\n     block = docstring[starting_point:(None if ending_point == -1 else\n                                       ending_point - 1)]\n     # Place marker for later reinjection.\n-    docstring = docstring.replace(block, marker)\n+    docstring_slice = docstring[starting_point:section_end].replace(block, marker)\n+    docstring = (docstring[:starting_point]\n+                 + docstring_slice\n+                 + docstring[section_end:])\n     lines = block.split('\\n')\n     # Remove the computed number of leading white spaces from each line.\n     lines = [re.sub('^' + ' ' * leading_spaces, '', line) for line in lines]\n@@ -535,12 +539,20 @@ def process_docstring(docstring):\n         anchor = section_idx.group(2)\n         leading_spaces = len(section_idx.group(1))\n         shift += section_idx.end()\n+        next_section_idx = re.search(section_regex, docstring[shift:])\n+        if next_section_idx is None:\n+            section_end = -1\n+        else:\n+            section_end = shift + next_section_idx.start()\n         marker = '$' + anchor.replace(' ', '_') + '$'\n         docstring, content = process_list_block(docstring,\n                                                 shift,\n+                                                section_end,\n                                                 leading_spaces,\n                                                 marker)\n         sections[marker] = content\n+        # `docstring` has changed, so we can't use `next_section_idx` anymore\n+        # we have to recompute it\n         section_idx = re.search(section_regex, docstring[shift:])\n \n     # Format docstring section titles.\n\n@@ -330,5 +330,36 @@ def test_doc_lists():\n     assert docstring == test_doc1['result']\n \n \n+dummy_docstring = \"\"\"Multiplies 2 tensors (and/or variables) and returns a *tensor*.\n+\n+    When attempting to multiply a nD tensor\n+    with a nD tensor, it reproduces the Theano behavior.\n+    (e.g. `(2, 3) * (4, 3, 5) -> (2, 4, 5)`)\n+\n+    # Examples\n+    ```python\n+        # Theano-like behavior example\n+        >>> x = K.random_uniform_variable(shape=(2, 3), low=0, high=1)\n+        >>> y = K.ones((4, 3, 5))\n+        >>> xy = K.dot(x, y)\n+        >>> K.int_shape(xy)\n+        (2, 4, 5)\n+    ```\n+\n+    # Numpy implementation\n+    ```python\n+        def dot(x, y):\n+            return dot(x, y)\n+    ```\n+    \"\"\"\n+\n+\n+def test_doc_multiple_sections_code():\n+    \"\"\" Checks that we can have code blocks in multiple sections.\"\"\"\n+    generated = autogen.process_docstring(dummy_docstring)\n+    assert '# Theano-like behavior example' in generated\n+    assert 'def dot(x, y):' in generated\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#48c1c96ac4cfec5580a5feb7eb7ef7c25c6db234", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 78 | Lines Deleted: 38 | Files Changed: 1 | Hunks: 27 | Methods Changed: 19 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 116 | Churn Cumulative: 10393 | Contributors (this commit): 121 | Commits (past 90d): 17 | Contributors (cumulative): 121 | DMM Complexity: 0.76\n\nDIFF:\n@@ -255,7 +255,8 @@ def _is_current_explicit_device(device_type):\n         device_type: A string containing `GPU` or `CPU` (case-insensitive).\n \n     # Returns\n-        A boolean indicating if the current device scope is explicitly set on the device type.\n+        A boolean indicating if the current device\n+        scope is explicitly set on the device type.\n \n     # Raises\n         ValueError: If the `device_type` string indicates an unsupported device.\n@@ -282,8 +283,10 @@ def _get_available_gpus():\n def _has_nchw_support():\n     \"\"\"Check whether the current scope supports NCHW ops.\n \n-    TensorFlow does not support NCHW on CPU. Therefore we check if we are not explicitly put on\n-    CPU, and have GPUs available. In this case there will be soft-placing on the GPU device.\n+    TensorFlow does not support NCHW on CPU.\n+    Therefore we check if we are not explicitly put on\n+    CPU, and have GPUs available.\n+    In this case there will be soft-placing on the GPU device.\n \n     # Returns\n         bool: if the current scope device placement would support nchw\n@@ -453,19 +456,23 @@ def is_keras_tensor(x):\n         >>> K.is_keras_tensor(np_var) # A numpy array is not a symbolic tensor.\n         ValueError\n         >>> k_var = tf.placeholder('float32', shape=(1,1))\n-        >>> K.is_keras_tensor(k_var) # A variable indirectly created outside of keras is not a Keras tensor.\n+        >>> # A variable indirectly created outside of keras is not a Keras tensor.\n+        >>> K.is_keras_tensor(k_var)\n         False\n         >>> keras_var = K.variable(np_var)\n-        >>> K.is_keras_tensor(keras_var)  # A variable created with the keras backend is not a Keras tensor.\n+        >>> # A variable created with the keras backend is not a Keras tensor.\n+        >>> K.is_keras_tensor(keras_var)\n         False\n         >>> keras_placeholder = K.placeholder(shape=(2, 4, 5))\n-        >>> K.is_keras_tensor(keras_placeholder)  # A placeholder is not a Keras tensor.\n+        >>> # A placeholder is not a Keras tensor.\n+        >>> K.is_keras_tensor(keras_placeholder)\n         False\n         >>> keras_input = Input([10])\n         >>> K.is_keras_tensor(keras_input) # An Input is a Keras tensor.\n         True\n         >>> keras_layer_output = Dense(10)(keras_input)\n-        >>> K.is_keras_tensor(keras_layer_output) # Any Keras layer output is a Keras tensor.\n+        >>> # Any Keras layer output is a Keras tensor.\n+        >>> K.is_keras_tensor(keras_layer_output)\n         True\n     ```\n     \"\"\"\n@@ -1963,9 +1970,12 @@ def batch_normalization(x, mean, var, beta, gamma, axis=-1, epsilon=1e-3):\n         else:\n             tf_data_format = None\n \n-        if tf_data_format == 'NHWC' or tf_data_format == 'NCHW' and _has_nchw_support():\n+        if (tf_data_format == 'NHWC'\n+                or tf_data_format == 'NCHW'\n+                and _has_nchw_support()):\n             # The mean / var / beta / gamma may be processed by broadcast\n-            # so it may have extra axes with 1, it is not needed and should be removed\n+            # so it may have extra axes with 1,\n+            # it is not needed and should be removed\n             if ndim(mean) > 1:\n                 mean = tf.reshape(mean, [-1])\n             if ndim(var) > 1:\n@@ -2063,7 +2073,8 @@ def resize_images(x,\n         A tensor.\n \n     # Raises\n-        ValueError: if `data_format` is neither `\"channels_last\"` or `\"channels_first\"`.\n+        ValueError: if `data_format` is\n+        neither `\"channels_last\"` or `\"channels_first\"`.\n     \"\"\"\n     if data_format == 'channels_first':\n         rows, cols = 2, 3\n@@ -2115,7 +2126,8 @@ def resize_volumes(x, depth_factor, height_factor, width_factor, data_format):\n         A tensor.\n \n     # Raises\n-        ValueError: if `data_format` is neither `\"channels_last\"` or `\"channels_first\"`.\n+        ValueError: if `data_format` is\n+        neither `\"channels_last\"` or `\"channels_first\"`.\n     \"\"\"\n     if data_format == 'channels_first':\n         output = repeat_elements(x, depth_factor, axis=2)\n@@ -2334,7 +2346,8 @@ def spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None):\n         A padded 4D tensor.\n \n     # Raises\n-        ValueError: if `data_format` is neither `\"channels_last\"` or `\"channels_first\"`.\n+        ValueError: if `data_format` is\n+        neither `\"channels_last\"` or `\"channels_first\"`.\n     \"\"\"\n     assert len(padding) == 2\n     assert len(padding[0]) == 2\n@@ -2369,7 +2382,8 @@ def spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None):\n         A padded 5D tensor.\n \n     # Raises\n-        ValueError: if `data_format` is neither `\"channels_last\"` or `\"channels_first\"`.\n+        ValueError: if `data_format` is\n+        neither `\"channels_last\"` or `\"channels_first\"`.\n \n     \"\"\"\n     assert len(padding) == 3\n@@ -2447,9 +2461,10 @@ def slice(x, start, size):\n             along each axis.\n \n     # Returns\n-        Tensor `x[start[0]: start[0] + size[0],\n-                  ...,\n-                  start[-1]: start[-1] + size[-1]]`\n+        A sliced tensor:\n+        ```python\n+        new_x = x[start[0]: start[0] + size[0], ..., start[-1]: start[-1] + size[-1]]\n+        ```\n     \"\"\"\n     return tf.slice(x, start, size)\n \n@@ -2766,11 +2781,13 @@ class Function(object):\n \n             # callable generated by Session._make_callable_from_options accepts\n             # `run_metadata` keyword argument since TF 1.10\n-            if (self.run_metadata and\n-                    StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.10.0')):\n+            if self.run_metadata:\n+                current_version = StrictVersion(tf.__version__.split('-')[0])\n+                if current_version < StrictVersion('1.10.0'):\n                     if py_any(is_tensor(x) for x in inputs):\n                         raise ValueError(\n-                        'In order to feed symbolic tensors to a Keras model and set '\n+                            'In order to feed symbolic tensors '\n+                            'to a Keras model and set '\n                             '`run_metadata`, you need tensorflow 1.10 or higher.')\n                     return self._legacy_call(inputs)\n \n@@ -2800,9 +2817,11 @@ def function(inputs, outputs, updates=None, **kwargs):\n     \"\"\"\n     if kwargs:\n         for key in kwargs:\n-            if not (has_arg(tf.Session.run, key, True) or has_arg(Function.__init__, key, True)):\n-                msg = 'Invalid argument \"%s\" passed to K.function with TensorFlow backend' % key\n-                raise ValueError(msg)\n+            session_has_key = has_arg(tf.Session.run, key, True)\n+            function_has_key = has_arg(Function.__init__, key, True)\n+            if not (session_has_key or function_has_key):\n+                raise ValueError('Invalid argument \"%s\" passed to K.function '\n+                                 'with TensorFlow backend' % key)\n     return Function(inputs, outputs, updates=updates, **kwargs)\n \n \n@@ -3034,10 +3053,12 @@ def rnn(step_function, inputs, initial_states,\n                 tiled_mask_t = tf.tile(mask_t,\n                                        tf.stack([1, tf.shape(output)[1]]))\n                 output = tf.where(tiled_mask_t, output, states[0])\n-                new_states = [\n-                    tf.where(tf.tile(mask_t, tf.stack([1, tf.shape(new_states[i])[1]])),\n-                             new_states[i], states[i]) for i in range(len(states))\n-                ]\n+                tmp = []\n+                for i in range(len(states)):\n+                    multiples = tf.stack([1, tf.shape(new_states[i])[1]])\n+                    tiled = tf.tile(mask_t, multiples)\n+                    tmp.append(tf.where(tiled, new_states[i], states[i]))\n+                new_states = tmp\n                 output_ta_t = output_ta_t.write(time, output)\n                 return (time + 1, output_ta_t) + tuple(new_states)\n         else:\n@@ -3139,7 +3160,8 @@ def switch(condition, then_expression, else_expression):\n             condition = tf.reshape(condition, cond_shape)\n             expr_shape = tf.shape(then_expression)\n             shape_diff = expr_shape - cond_shape\n-            tile_shape = tf.where(shape_diff > 0, expr_shape, tf.ones_like(expr_shape))\n+            zero_expr_shape = tf.ones_like(expr_shape)\n+            tile_shape = tf.where(shape_diff > 0, expr_shape, zero_expr_shape)\n             condition = tf.tile(condition, tile_shape)\n         x = tf.where(condition, then_expression, else_expression)\n     return x\n@@ -4020,7 +4042,8 @@ def pool2d(x, pool_size, strides=(1, 1),\n         A tensor, result of 2D pooling.\n \n     # Raises\n-        ValueError: if `data_format` is neither `\"channels_last\"` or `\"channels_first\"`.\n+        ValueError: if `data_format` is\n+        neither `\"channels_last\"` or `\"channels_first\"`.\n         ValueError: if `pool_mode` is neither `\"max\"` or `\"avg\"`.\n     \"\"\"\n     data_format = normalize_data_format(data_format)\n@@ -4066,7 +4089,8 @@ def pool3d(x, pool_size, strides=(1, 1, 1), padding='valid',\n         A tensor, result of 3D pooling.\n \n     # Raises\n-        ValueError: if `data_format` is neither `\"channels_last\"` or `\"channels_first\"`.\n+        ValueError: if `data_format` is\n+        neither `\"channels_last\"` or `\"channels_first\"`.\n         ValueError: if `pool_mode` is neither `\"max\"` or `\"avg\"`.\n     \"\"\"\n     data_format = normalize_data_format(data_format)\n@@ -4117,7 +4141,8 @@ def bias_add(x, bias, data_format=None):\n     data_format = normalize_data_format(data_format)\n     bias_shape = int_shape(bias)\n     if len(bias_shape) != 1 and len(bias_shape) != ndim(x) - 1:\n-        raise ValueError('Unexpected bias dimensions %d, expect to be 1 or %d dimensions'\n+        raise ValueError('Unexpected bias dimensions %d, '\n+                         'expect to be 1 or %d dimensions'\n                          % (len(bias_shape), ndim(x)))\n     if ndim(x) == 5:\n         if len(bias_shape) == 1:\n@@ -4281,14 +4306,18 @@ def ctc_label_dense_to_sparse(labels, label_lengths):\n                              label_shape)\n     label_ind = tf.boolean_mask(label_array, dense_mask)\n \n-    batch_array = tf.transpose(tf.reshape(tf.tile(tf.range(label_shape[0]),\n-                                                  max_num_labels_tns), reverse(label_shape, 0)))\n+    tmp = tf.tile(tf.range(label_shape[0]), max_num_labels_tns)\n+    batch_array = tf.transpose(tf.reshape(tmp, reverse(label_shape, 0)))\n     batch_ind = tf.boolean_mask(batch_array, dense_mask)\n-    indices = tf.transpose(tf.reshape(concatenate([batch_ind, label_ind], axis=0), [2, -1]))\n+\n+    indices = concatenate([batch_ind, label_ind], axis=0)\n+    indices = tf.transpose(tf.reshape(indices, [2, -1]))\n \n     vals_sparse = tf.gather_nd(labels, indices)\n \n-    return tf.SparseTensor(tf.to_int64(indices), vals_sparse, tf.to_int64(label_shape))\n+    indices = tf.to_int64(indices)\n+    label_shape = tf.to_int64(label_shape)\n+    return tf.SparseTensor(indices, vals_sparse, label_shape)\n \n \n def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n@@ -4361,8 +4390,13 @@ def ctc_decode(y_pred, input_length, greedy=True, beam_width=100,\n             sequence_length=input_length, beam_width=beam_width,\n             top_paths=top_paths)\n \n-    decoded_dense = [tf.sparse_to_dense(st.indices, st.dense_shape, st.values, default_value=-1)\n-                     for st in decoded]\n+    decoded_dense = []\n+    for st in decoded:\n+        dense_tensor = tf.sparse_to_dense(st.indices,\n+                                          st.dense_shape,\n+                                          st.values,\n+                                          default_value=-1)\n+        decoded_dense.append(dense_tensor)\n     return (decoded_dense, log_prob)\n \n \n@@ -4429,7 +4463,8 @@ def local_conv1d(inputs, kernel, kernel_size, strides, data_format=None):\n         data_format: the data format, channels_first or channels_last\n \n     # Returns\n-        the tensor after 1d conv with un-shared weights, with shape (batch_size, output_length, filters)\n+        the tensor after 1d conv with un-shared weights,\n+        with shape (batch_size, output_length, filters)\n \n     # Raises\n         ValueError: If `data_format` is neither\n@@ -4453,7 +4488,12 @@ def local_conv1d(inputs, kernel, kernel_size, strides, data_format=None):\n     return permute_dimensions(output, (1, 0, 2))\n \n \n-def local_conv2d(inputs, kernel, kernel_size, strides, output_shape, data_format=None):\n+def local_conv2d(inputs,\n+                 kernel,\n+                 kernel_size,\n+                 strides,\n+                 output_shape,\n+                 data_format=None):\n     \"\"\"Apply 2D conv with un-shared weights.\n \n     # Arguments\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
