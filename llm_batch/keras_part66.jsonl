{"custom_id": "keras#f800e448a231dfa5d4c4ccffb5882cd08869f9f5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 4091 | Contributors (this commit): 32 | Commits (past 90d): 20 | Contributors (cumulative): 32 | DMM Complexity: 1.0\n\nDIFF:\n@@ -81,6 +81,8 @@ def weighted_objective(fn):\n         # score_array has ndim >= 2\n         score_array = fn(y_true, y_pred)\n         if mask is not None:\n+            # Cast the mask to floatX to avoid float64 upcasting in theano\n+            mask = K.cast(mask, K.floatx())\n             # mask should have the same shape as score_array\n             score_array *= mask\n             #  the loss per batch should be proportional\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bf2f64bfd5ce4f0595fe6e88f98cd4159e7a57f3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 7216 | Contributors (this commit): 36 | Commits (past 90d): 110 | Contributors (cumulative): 36 | DMM Complexity: None\n\nDIFF:\n@@ -1,6 +1,6 @@\n # -*- coding: utf-8 -*-\n from __future__ import absolute_import\n-from __future__ import divisionla\n+from __future__ import division\n \n import numpy as np\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e21a6a9ebff8659594ecd70e8123252715d6b101", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 7218 | Contributors (this commit): 36 | Commits (past 90d): 111 | Contributors (cumulative): 36 | DMM Complexity: None\n\nDIFF:\n@@ -1376,7 +1376,7 @@ class Lambda(Layer):\n         else:\n             output_shape_func = marshal.loads(self._output_shape)\n             output_shape_func = types.FunctionType(output_shape_func, globals())\n-            shape = output_shape_func(self.previous.output_shape)\n+            shape = output_shape_func(self.input_shape)\n             if type(shape) not in {list, tuple}:\n                 raise Exception('output_shape function must return a tuple')\n             return tuple(shape)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e947a56c52923f55e09b25bbdd6ed3bc4d46adce", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 15 | Churn Cumulative: 4106 | Contributors (this commit): 33 | Commits (past 90d): 21 | Contributors (cumulative): 33 | DMM Complexity: 1.0\n\nDIFF:\n@@ -370,8 +370,21 @@ class Model(object):\n         `keras.models.from_json(json_string, custom_objects={})`.\n         '''\n         import json\n+\n+        def get_json_type(obj):\n+\n+            # if obj is any numpy type\n+            if type(obj).__module__ == np.__name__:\n+                return obj.item();\n+\n+            # if obj is a python 'type'\n+            if type(obj).__name__ == type.__name__:\n+                return obj.__name__\n+\n+            raise TypeError('Not JSON Serializable')\n+\n         config = self.get_config()\n-        return json.dumps(config, **kwargs)\n+        return json.dumps(config, default=get_json_type, **kwargs)\n \n     def summary(self):\n         '''Print out a summary of the model architecture,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#09d91fccb9c56af2a18ffbd4dd5fee022b94e04b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 110 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 110 | Churn Cumulative: 110 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,110 @@\n+'''The example demonstrates how to write custom layers for Keras.\n+\n+We build a custom activation layer called 'Antirectifier',\n+which modifies the shape of the tensor that passes through it.\n+We need to specify two methods: `output_shape` and `get_output`.\n+\n+Note that same result can also be achieved via a Lambda layer.\n+\n+Because our custom layer is written with primitives from the Keras\n+backend (`K`), our code can run both on TensorFlow and Theano.\n+'''\n+\n+from __future__ import print_function\n+import numpy as np\n+from keras.models import Sequential\n+from keras.layers.core import Dense, Dropout, Layer, Activation\n+from keras.datasets import mnist\n+from keras import backend as K\n+from keras.utils import np_utils\n+\n+\n+class Antirectifier(Layer):\n+    '''This is the combination of a sample-wise\n+    L2 normalization with the concatenation of the\n+    positive part of the output with the negative part\n+    of the output. The result is a tensor of samples that are\n+    twice as large as the input samples.\n+\n+    It can be used in place of a ReLU.\n+\n+    # Input shape\n+        2D tensor of shape (samples, n)\n+\n+    # Output shape\n+        2D tensor of shape (samples, 2*n)\n+\n+    # Theoretical justification\n+        When applying ReLU, assuming that the distribution\n+        of the previous output is approximately centered around 0.,\n+        you are discarding half of your input. This is inefficient.\n+\n+        Antirectifier allows to return all-positive outputs like ReLU,\n+        without discarding any data.\n+\n+        Further, the samplewise normalization of the output\n+        allows to interpret output features as a probability distribution\n+        (since they are between 0 and 1 and sum to 1 for each sample).\n+\n+        Tests on MNIST show that Antirectifier allows to train networks\n+        with twice less parameters yet with the same\n+        classification performance as an equivalent ReLU-based network.\n+    '''\n+    @property\n+    def output_shape(self):\n+        shape = list(self.input_shape)\n+        assert len(shape) == 2  # only valid for 2D tensors\n+        shape[-1] *= 2\n+        return tuple(shape)\n+\n+    def get_output(self, train):\n+        x = self.get_input(train)\n+        x -= K.mean(x, axis=1, keepdims=True)\n+        x = K.l2_normalize(x, axis=1)\n+        pos = K.relu(x)\n+        neg = K.relu(-x)\n+        return K.concatenate([pos, neg], axis=1)\n+\n+# global parameters\n+batch_size = 128\n+nb_classes = 10\n+nb_epoch = 40\n+\n+# the data, shuffled and split between tran and test sets\n+(X_train, y_train), (X_test, y_test) = mnist.load_data()\n+\n+X_train = X_train.reshape(60000, 784)\n+X_test = X_test.reshape(10000, 784)\n+X_train = X_train.astype('float32')\n+X_test = X_test.astype('float32')\n+X_train /= 255\n+X_test /= 255\n+print(X_train.shape[0], 'train samples')\n+print(X_test.shape[0], 'test samples')\n+\n+# convert class vectors to binary class matrices\n+Y_train = np_utils.to_categorical(y_train, nb_classes)\n+Y_test = np_utils.to_categorical(y_test, nb_classes)\n+\n+# build the model\n+model = Sequential()\n+model.add(Dense(256, input_shape=(784,)))\n+model.add(Antirectifier())\n+model.add(Dropout(0.1))\n+model.add(Dense(256))\n+model.add(Antirectifier())\n+model.add(Dropout(0.1))\n+model.add(Dense(10))\n+model.add(Activation('softmax'))\n+\n+# compile the model\n+model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+# train the model\n+model.fit(X_train, Y_train,\n+          batch_size=batch_size, nb_epoch=nb_epoch,\n+          show_accuracy=True, verbose=1,\n+          validation_data=(X_test, Y_test))\n+\n+# next, compare with an equivalent network\n+# with2x bigger Dense layers and ReLU\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d0b98a2cb5014ce23a16ab98cf3ebec2a28d5612", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 120 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -4,7 +4,7 @@ We build a custom activation layer called 'Antirectifier',\n which modifies the shape of the tensor that passes through it.\n We need to specify two methods: `output_shape` and `get_output`.\n \n-Note that same result can also be achieved via a Lambda layer.\n+Note that the same result can also be achieved via a Lambda layer.\n \n Because our custom layer is written with primitives from the Keras\n backend (`K`), our code can run both on TensorFlow and Theano.\n@@ -22,8 +22,8 @@ from keras.utils import np_utils\n class Antirectifier(Layer):\n     '''This is the combination of a sample-wise\n     L2 normalization with the concatenation of the\n-    positive part of the output with the negative part\n-    of the output. The result is a tensor of samples that are\n+    positive part of the input with the negative part\n+    of the input. The result is a tensor of samples that are\n     twice as large as the input samples.\n \n     It can be used in place of a ReLU.\n@@ -47,8 +47,8 @@ class Antirectifier(Layer):\n         (since they are between 0 and 1 and sum to 1 for each sample).\n \n         Tests on MNIST show that Antirectifier allows to train networks\n-        with twice less parameters yet with the same\n-        classification performance as an equivalent ReLU-based network.\n+        with twice less parameters yet with comparable\n+        classification accuracy as an equivalent ReLU-based network.\n     '''\n     @property\n     def output_shape(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ced84d53bc01e1623da671187588a63dcc9a5a11", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 124 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -42,10 +42,6 @@ class Antirectifier(Layer):\n         Antirectifier allows to return all-positive outputs like ReLU,\n         without discarding any data.\n \n-        Further, the samplewise normalization of the output\n-        allows to interpret output features as a probability distribution\n-        (since they are between 0 and 1 and sum to 1 for each sample).\n-\n         Tests on MNIST show that Antirectifier allows to train networks\n         with twice less parameters yet with comparable\n         classification accuracy as an equivalent ReLU-based network.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#037e592f2ba7c18b71bc9b39f84de11af0252863", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 83 | Lines Deleted: 29 | Files Changed: 7 | Hunks: 38 | Methods Changed: 39 | Complexity Δ (Sum/Max): 23/12 | Churn Δ: 112 | Churn Cumulative: 9986 | Contributors (this commit): 49 | Commits (past 90d): 191 | Contributors (cumulative): 71 | DMM Complexity: 0.8157894736842105\n\nDIFF:\n@@ -287,6 +287,10 @@ def tile(x, n):\n \n \n def flatten(x):\n+    return tf.reshape(x, [-1])\n+\n+\n+def batch_flatten(x):\n     '''Turn a n-D tensor into a 2D tensor where\n     the first dimension is conserved.\n     '''\n@@ -345,12 +349,16 @@ def set_value(x, value):\n class Function(object):\n \n     def __init__(self, inputs, outputs, updates=[]):\n+        assert type(inputs) in {list, tuple}\n+        assert type(outputs) in {list, tuple}\n+        assert type(updates) in {list, tuple}\n         self.inputs = list(inputs)\n         self.outputs = list(outputs)\n         with tf.control_dependencies(self.outputs):\n             self.updates = [tf.assign(p, new_p) for (p, new_p) in updates]\n \n     def __call__(self, inputs):\n+        assert type(inputs) in {list, tuple}\n         names = [v.name for v in self.inputs]\n         feed_dict = dict(zip(names, inputs))\n         session = _get_session()\n@@ -442,7 +450,7 @@ def rnn(step_function, inputs, initial_states,\n     new_states = successive_states[-1]\n \n     outputs = tf.transpose(outputs, (1, 0, 2))\n-    return last_output, outputs, states\n+    return last_output, outputs, new_states\n \n \n def switch(condition, then_expression, else_expression):\n\n@@ -287,6 +287,10 @@ def tile(x, n):\n \n \n def flatten(x):\n+    return T.flatten(x)\n+\n+\n+def batch_flatten(x):\n     '''Turn a n-D tensor into a 2D tensor where\n     the first dimension is conserved.\n     '''\n@@ -378,10 +382,14 @@ def set_value(x, value):\n class Function(object):\n \n     def __init__(self, inputs, outputs, updates=[], **kwargs):\n+        assert type(inputs) in {list, tuple}\n+        assert type(outputs) in {list, tuple}\n+        assert type(updates) in {list, tuple}\n         self.function = theano.function(inputs, outputs, updates=updates,\n                                         allow_input_downcast=True, **kwargs)\n \n     def __call__(self, inputs):\n+        assert type(inputs) in {list, tuple}\n         return self.function(*inputs)\n \n \n\n@@ -9,52 +9,54 @@ def get_fans(shape):\n     return fan_in, fan_out\n \n \n-def uniform(shape, scale=0.05):\n-    return K.variable(np.random.uniform(low=-scale, high=scale, size=shape))\n+def uniform(shape, scale=0.05, name=None):\n+    return K.variable(np.random.uniform(low=-scale, high=scale, size=shape),\n+                      name=name)\n \n \n-def normal(shape, scale=0.05):\n-    return K.variable(np.random.normal(loc=0.0, scale=scale, size=shape))\n+def normal(shape, scale=0.05, name=None):\n+    return K.variable(np.random.normal(loc=0.0, scale=scale, size=shape),\n+                      name=name)\n \n \n-def lecun_uniform(shape):\n+def lecun_uniform(shape, name=None):\n     ''' Reference: LeCun 98, Efficient Backprop\n         http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n     '''\n     fan_in, fan_out = get_fans(shape)\n     scale = np.sqrt(3. / fan_in)\n-    return uniform(shape, scale)\n+    return uniform(shape, scale, name=name)\n \n \n-def glorot_normal(shape):\n+def glorot_normal(shape, name=None):\n     ''' Reference: Glorot & Bengio, AISTATS 2010\n     '''\n     fan_in, fan_out = get_fans(shape)\n     s = np.sqrt(2. / (fan_in + fan_out))\n-    return normal(shape, s)\n+    return normal(shape, s, name=name)\n \n \n-def glorot_uniform(shape):\n+def glorot_uniform(shape, name=None):\n     fan_in, fan_out = get_fans(shape)\n     s = np.sqrt(6. / (fan_in + fan_out))\n-    return uniform(shape, s)\n+    return uniform(shape, s, name=name)\n \n \n-def he_normal(shape):\n+def he_normal(shape, name=None):\n     ''' Reference:  He et al., http://arxiv.org/abs/1502.01852\n     '''\n     fan_in, fan_out = get_fans(shape)\n     s = np.sqrt(2. / fan_in)\n-    return normal(shape, s)\n+    return normal(shape, s, name=name)\n \n \n-def he_uniform(shape):\n+def he_uniform(shape, name=None):\n     fan_in, fan_out = get_fans(shape)\n     s = np.sqrt(6. / fan_in)\n-    return uniform(shape, s)\n+    return uniform(shape, s, name=name)\n \n \n-def orthogonal(shape, scale=1.1):\n+def orthogonal(shape, scale=1.1, name=None):\n     ''' From Lasagne. Reference: Saxe et al., http://arxiv.org/abs/1312.6120\n     '''\n     flat_shape = (shape[0], np.prod(shape[1:]))\n@@ -63,23 +65,23 @@ def orthogonal(shape, scale=1.1):\n     # pick the one with the correct shape\n     q = u if u.shape == flat_shape else v\n     q = q.reshape(shape)\n-    return K.variable(scale * q[:shape[0], :shape[1]])\n+    return K.variable(scale * q[:shape[0], :shape[1]], name=name)\n \n \n-def identity(shape, scale=1):\n+def identity(shape, scale=1, name=None):\n     if len(shape) != 2 or shape[0] != shape[1]:\n         raise Exception('Identity matrix initialization can only be used '\n                         'for 2D square matrices.')\n     else:\n-        return K.variable(scale * np.identity(shape[0]))\n+        return K.variable(scale * np.identity(shape[0]), name=name)\n \n \n-def zero(shape):\n-    return K.zeros(shape)\n+def zero(shape, name=None):\n+    return K.zeros(shape, name=name)\n \n \n-def one(shape):\n-    return K.ones(shape)\n+def one(shape, name=None):\n+    return K.ones(shape, name=name)\n \n \n from .utils.generic_utils import get_from_module\n\n@@ -36,20 +36,34 @@ class Layer(object):\n         allowed_kwargs = {'input_shape',\n                           'trainable',\n                           'batch_input_shape',\n-                          'cache_enabled'}\n+                          'cache_enabled',\n+                          'name'}\n         for kwarg in kwargs:\n             assert kwarg in allowed_kwargs, 'Keyword argument not understood: ' + kwarg\n+\n         if 'input_shape' in kwargs:\n             self.set_input_shape((None,) + tuple(kwargs['input_shape']))\n         if 'batch_input_shape' in kwargs:\n             self.set_input_shape(tuple(kwargs['batch_input_shape']))\n+        self.trainable = True\n         if 'trainable' in kwargs:\n-            self._trainable = kwargs['trainable']\n+            self.trainable = kwargs['trainable']\n+        self.name = self.__class__.__name__.lower()\n+        if 'name' in kwargs:\n+            self.name = kwargs['name']\n         if not hasattr(self, 'params'):\n             self.params = []\n-        self._cache_enabled = True\n+        self.cache_enabled = True\n         if 'cache_enabled' in kwargs:\n-            self._cache_enabled = kwargs['cache_enabled']\n+            self.cache_enabled = kwargs['cache_enabled']\n+\n+    @property\n+    def name(self):\n+        return self._name\n+\n+    @name.setter\n+    def name(self, name):\n+        self._name = name\n \n     @property\n     def cache_enabled(self):\n@@ -234,6 +248,7 @@ class Layer(object):\n         if hasattr(self, '_trainable'):\n             config['trainable'] = self._trainable\n         config['cache_enabled'] = self.cache_enabled\n+        config['custom_name'] = self.name\n         return config\n \n     def get_params(self):\n@@ -819,7 +834,7 @@ class Flatten(Layer):\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n-        return K.flatten(X)\n+        return K.batch_flatten(X)\n \n \n class RepeatVector(Layer):\n\n@@ -71,10 +71,11 @@ def container_from_config(original_layer_dict, custom_objects={}):\n                 kwargs[kwarg] = layer_dict[kwarg]\n         return AutoEncoder(**kwargs)\n \n-    else:\n+    else:  # this is a non-topological layer (e.g. Dense, etc.)\n         layer_dict.pop('name')\n \n         for k, v in layer_dict.items():\n+            # a dictionary argument may be a regularizer or constraint\n             if isinstance(v, dict):\n                 vname = v.pop('name')\n                 if vname in [x for x, y in inspect.getmembers(constraints, predicate=inspect.isclass)]:\n@@ -85,6 +86,9 @@ def container_from_config(original_layer_dict, custom_objects={}):\n                     # not a regularizer of constraint, don't touch it\n                     v['name'] = vname\n \n+        # the \"name\" keyword argument of layers is saved as \"custom_name\"\n+        if 'custom_name' in layer_dict:\n+            layer_dict['name'] = layer_dict.pop('custom_name')\n         base_layer = get_layer(name, layer_dict)\n         return base_layer\n \n\n@@ -130,6 +130,21 @@ def test_maxout_dense():\n     _runner(layer)\n \n \n+def test_naming():\n+    layer = core.Dense(2, input_dim=2)\n+    assert layer.name == 'dense'\n+\n+    model = Sequential()\n+    model.add(core.Dense(2, input_dim=2, name='my_dense'))\n+    model.add(core.Dense(2, name='my_dense'))\n+\n+    assert model.layers[0].name == 'my_dense'\n+    assert model.layers[1].name == 'my_dense'\n+\n+    model.compile(optimizer='rmsprop', loss='mse')\n+    model.train_on_batch(np.random.random((2, 2)), np.random.random((2, 2)))\n+\n+\n @pytest.mark.skipif(K._BACKEND == 'tensorflow',\n                     reason='currently not working with TensorFlow')\n def test_sequences():\n\n@@ -85,6 +85,8 @@ def test_batchnorm_config():\n                                             epsilon=0.1, momentum=0.9)\n     conf = norm.get_config()\n     del conf['cache_enabled']\n+    del conf['trainable']\n+    del conf['custom_name']\n     conf_target = {\"input_shape\": (10, 10),\n                    \"name\": normalization.BatchNormalization.__name__,\n                    \"epsilon\": 0.1, \"mode\": 1, \"momentum\": 0.9}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6bb4cbbf5efaf5ad68be03183b3dc2dda58f48ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 4113 | Contributors (this commit): 33 | Commits (past 90d): 22 | Contributors (cumulative): 33 | DMM Complexity: 0.0\n\nDIFF:\n@@ -156,6 +156,13 @@ def model_from_config(config, custom_objects={}):\n     if 'optimizer' in config:\n         # if it has an optimizer, the model is assumed to be compiled\n         loss = config.get('loss')\n+\n+        # if a custom loss function is passed replace it in loss\n+        for l in loss:\n+            for c in custom_objects:\n+                if loss[l] == c:\n+                    loss[l] = custom_objects[c]\n+\n         class_mode = config.get('class_mode')\n \n         optimizer_params = dict([(k, v) for k, v in config.get('optimizer').items()])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#42cd4d6b629548f2adce63a5d2bd819daa9b19cb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 3 | Churn Cumulative: 4116 | Contributors (this commit): 33 | Commits (past 90d): 23 | Contributors (cumulative): 33 | DMM Complexity: 0.0\n\nDIFF:\n@@ -158,10 +158,13 @@ def model_from_config(config, custom_objects={}):\n         loss = config.get('loss')\n \n         # if a custom loss function is passed replace it in loss\n+        if model_name == 'Graph':\n             for l in loss:\n                 for c in custom_objects:\n                     if loss[l] == c:\n                         loss[l] = custom_objects[c]\n+        elif model_name == 'Sequential' and loss in custom_objects:\n+            loss = custom_objects[loss]\n \n         class_mode = config.get('class_mode')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#314ee54e60802caffa6700c000fc25b8d5f7e85a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 1237 | Contributors (this commit): 13 | Commits (past 90d): 11 | Contributors (cumulative): 13 | DMM Complexity: 0.0\n\nDIFF:\n@@ -513,7 +513,10 @@ class TensorBoard(Callback):\n                 summary_str = result[0]\n                 self.writer.add_summary(summary_str, epoch)\n \n-        for name, value in self.totals.items() + logs.items():\n+        all_values = self.totals.copy()\n+        all_values.update(logs)\n+        \n+        for name, value in all_values.items():\n             if name in ['batch', 'size']:\n                 continue\n             summary = tf.Summary()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a5c07d796aa63733cfc79ccfa8c37cd604aef3a0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 901 | Contributors (this commit): 7 | Commits (past 90d): 28 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -382,9 +382,6 @@ def set_value(x, value):\n class Function(object):\n \n     def __init__(self, inputs, outputs, updates=[], **kwargs):\n-        assert type(inputs) in {list, tuple}\n-        assert type(outputs) in {list, tuple}\n-        assert type(updates) in {list, tuple}\n         self.function = theano.function(inputs, outputs, updates=updates,\n                                         allow_input_downcast=True, **kwargs)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ada6dd29435066a19782ad25dc79827a000a90d3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 259 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 8 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 259 | Churn Cumulative: 259 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,259 @@\n+'''Neural style transfer with Keras.\n+\n+Before running this script, download the weights for the VGG16 model at:\n+https://drive.google.com/file/d/0Bz7KyqmuGsilT0J5dmRCM0ROVHc/view?usp=sharing\n+(source: https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3)\n+and make sure the variable `weights_path` in this script matches the location of the file.\n+\n+Run the script with:\n+```\n+python neural_style.py path_to_your_base_image.jpg path_to_your_reference.jpg prefix_for_results\n+```\n+e.g.:\n+```\n+python neural_style.py img/tuebingen.jpg img/starry_night.jpg results/my_result\n+```\n+\n+It is preferrable to run this script on GPU, for speed.\n+If running on CPU, prefer the TensorFlow backend (much faster).\n+\n+# Details\n+\n+Style transfer consists in generating an image\n+with the same \"content\" as a base image, but with the\n+\"style\" of a different picture (typically artistic).\n+\n+This is achieved through the optimization of a loss function\n+that has 3 components: \"style loss\", \"content loss\",\n+and \"total variation loss\":\n+\n+ - The content loss is a L2 distance between the pixels of the base\n+image and the pixels of the combination image, keeping the generated image\n+close enough to the original one.\n+\n+- The total variation loss imposes local spatial continuity between\n+the pixels of the combination image, giving it visual coherence.\n+\n+- The style loss is where the deep learning keeps in --that one is defined\n+using a deep convolutional neural network. Precisely, it consists in a sum of\n+L2 distances betwen the Gram matrices of the representations of\n+the base image and the style reference image, extracted from\n+different layers of a convnet (trained on ImageNet). The general idea\n+is to capture color/texture information at different spatial\n+scales (fairly large scales --defined by the depth of the layer considered).\n+\n+# Reference:\n+    - [A Neural Algorithm of Artistic Style](http://arxiv.org/abs/1508.06576)\n+'''\n+\n+from __future__ import print_function\n+import cv2\n+import numpy as np\n+from scipy.optimize import fmin_l_bfgs_b\n+import time\n+import argparse\n+import h5py\n+\n+from keras.models import Sequential\n+from keras.layers.convolutional import Convolution2D, ZeroPadding2D, MaxPooling2D\n+from keras.layers.core import Dense, Dropout, Flatten\n+from keras import backend as K\n+\n+parser = argparse.ArgumentParser(description='Neural style transfer with Keras.')\n+parser.add_argument('base_image_path', metavar='base', type=str,\n+                    help='Path to the image to transform.')\n+parser.add_argument('style_reference_image_path', metavar='ref', type=str,\n+                    help='Path to the style reference image.')\n+parser.add_argument('result_prefix', metavar='res_prefix', type=str,\n+                    help='Prefix for the saved results.')\n+\n+# base_image_path = 'tuebingen.jpg'\n+# style_reference_image_path = 'starry_night.jpg'\n+# result_prefix = 'my_result_th'\n+args = parser.parse_args()\n+base_image_path = args.base_image_path\n+style_reference_image_path = args.style_reference_image_path\n+result_prefix = args.result_prefix\n+weights_path = 'vgg16_weights.h5'\n+\n+# these are the weights of the different loss components\n+total_variation_weight = 1.\n+style_weight = 1.\n+content_weight = 0.025\n+\n+# dimensions of the generated picture.\n+img_width = 400\n+img_height = 400\n+assert img_height == img_width, 'Due to the use of the Gram matrix, width and height must match.'\n+\n+# util function to open, resize and format pictures into appropriate tensors\n+def preprocess_image(image_path):\n+    im = cv2.resize(cv2.imread(image_path), (img_width, img_height))\n+    im = im.transpose((2, 0, 1))\n+    im = np.expand_dims(im, axis=0)\n+    return im\n+\n+# util function to convert a tensor into a valid image\n+def deprocess_image(x):\n+    x = x.transpose((1, 2, 0))\n+    x = np.clip(x, 0, 255).astype('uint8')\n+    return x\n+\n+# get tensor representations of our images\n+base_image = K.variable(preprocess_image(base_image_path))\n+style_reference_image = K.variable(preprocess_image(style_reference_image_path))\n+\n+# this will contain our generated image\n+combination_image = K.placeholder((1, 3, img_width, img_height))\n+\n+# combine the 3 images into a single Keras tensor\n+input_tensor = K.concatenate([base_image,\n+                              style_reference_image,\n+                              combination_image], axis=0)\n+\n+# build the VGG16 network with our 3 images as input\n+first_layer = ZeroPadding2D((1, 1), input_shape=(3, img_width, img_height))\n+first_layer.input = input_tensor\n+\n+model = Sequential()\n+model.add(first_layer)\n+model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n+model.add(ZeroPadding2D((1, 1)))\n+model.add(Convolution2D(64, 3, 3, activation='relu'))\n+model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n+\n+model.add(ZeroPadding2D((1, 1)))\n+model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n+model.add(ZeroPadding2D((1, 1)))\n+model.add(Convolution2D(128, 3, 3, activation='relu'))\n+model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n+\n+model.add(ZeroPadding2D((1, 1)))\n+model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n+model.add(ZeroPadding2D((1, 1)))\n+model.add(Convolution2D(256, 3, 3, activation='relu'))\n+model.add(ZeroPadding2D((1, 1)))\n+model.add(Convolution2D(256, 3, 3, activation='relu'))\n+model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n+\n+model.add(ZeroPadding2D((1, 1)))\n+model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n+model.add(ZeroPadding2D((1, 1)))\n+model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n+model.add(ZeroPadding2D((1, 1)))\n+model.add(Convolution2D(512, 3, 3, activation='relu'))\n+model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n+\n+model.add(ZeroPadding2D((1, 1)))\n+model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n+model.add(ZeroPadding2D((1, 1)))\n+model.add(Convolution2D(512, 3, 3, activation='relu'))\n+model.add(ZeroPadding2D((1, 1)))\n+model.add(Convolution2D(512, 3, 3, activation='relu'))\n+model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n+\n+# load the weights of the VGG16 networks\n+# (trained on ImageNet, won the ILSVRC competition in 2014)\n+# note: when there is a complete match between your model definition\n+# and your weight savefile, you can simply call model.load_weights(filename)\n+f = h5py.File(weights_path)\n+for k in range(f.attrs['nb_layers']):\n+    if k >= len(model.layers):\n+        # we don't look at the last (fully-connected) layers in the savefile\n+        break\n+    g = f['layer_{}'.format(k)]\n+    weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n+    model.layers[k].set_weights(weights)\n+f.close()\n+print('Model loaded.')\n+\n+# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n+outputs_dict = dict([(layer.name, layer.get_output()) for layer in model.layers])\n+\n+# compute the neural style loss\n+# first we need to define 4 util functions\n+\n+# the gram matrix of an image tensor (feature-wise outer product)\n+def gram_matrix(x):\n+    assert K.ndim(x) == 3\n+    features = K.batch_flatten(x)\n+    gram = K.dot(features, K.transpose(features))\n+    return gram\n+\n+# the \"style loss\" is designed to maintain\n+# the style of the reference image in the generated image.\n+# It is based on the gram matrices (which capture style) of\n+# feature maps from the style reference image\n+# and from the generated image\n+def style_loss(style, combination):\n+    assert K.ndim(style) == 3\n+    assert K.ndim(combination) == 3\n+    S = gram_matrix(style)\n+    C = gram_matrix(combination)\n+    channels = 3\n+    size = img_width * img_height\n+    return K.sum(K.square(S - C)) / (4. * (channels ** 2) * (size ** 2))\n+\n+# an auxiliary loss function\n+# designed to maintain the \"content\" of the\n+# base image in the generated image\n+def content_loss(base, combination):\n+    return K.sum(K.square(combination - base))\n+\n+# the 3rd loss function, total variation loss,\n+# designed to keep the generated image locally coherent\n+def total_variation_loss(x):\n+    assert K.ndim(x) == 4\n+    a = K.square(x[:, :, :img_width-1, :img_height-1] - x[:, :, 1:, :img_height-1])\n+    b = K.square(x[:, :, :img_width-1, :img_height-1] - x[:, :, :img_width-1, 1:])\n+    return K.sum(K.pow(a + b, 1.25))\n+\n+# combine these loss functions into a single scalar\n+loss = K.variable(0.)\n+layer_features = outputs_dict['conv4_2']\n+base_image_features = layer_features[0, :, :, :]\n+combination_features = layer_features[2, :, :, :]\n+loss += content_weight * content_loss(base_image_features,\n+                                      combination_features)\n+\n+feature_layers = ['conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv5_1']\n+for layer_name in feature_layers:\n+    layer_features = outputs_dict[layer_name]\n+    style_reference_features = layer_features[1, :, :, :]\n+    combination_features = layer_features[2, :, :, :]\n+    sl = style_loss(style_reference_features, combination_features)\n+    loss += (style_weight / len(feature_layers)) * sl\n+loss += total_variation_weight * total_variation_loss(combination_image)\n+\n+# get the gradients of the generated image wrt the loss\n+grads = K.gradients(loss, combination_image)\n+\n+# set up helper functions to extract the loss and gradients\n+# from the computational graph as Numpy arrays\n+f_grads = K.function([combination_image], grads)\n+def eval_grads(x):\n+    x = x.reshape((1, 3, img_width, img_height))\n+    return np.array(f_grads([x])).flatten().astype('float64')\n+\n+f_loss = K.function([combination_image], [loss])\n+def eval_loss(x):\n+    x = x.reshape((1, 3, img_width, img_height))\n+    return f_loss([x])[0].astype('float64')\n+\n+# run scipy-based optimization (L-BFGS) over the pixels of the generated image\n+# so as to minimize the neural style loss\n+x = np.random.uniform(0, 255, (1, 3, img_width, img_height))\n+for i in range(10):\n+    print('Start of iteration', i)\n+    eval_loss_calls = 0\n+    start_time = time.time()\n+    x, min_val, info = fmin_l_bfgs_b(eval_loss, x.flatten(),\n+                                     fprime=eval_grads, maxfun=20)\n+    print('Current loss value:', min_val)\n+    # save current generated image\n+    im = deprocess_image(x.reshape((3, img_width, img_height)))\n+    fname = result_prefix + '_at_iteration_%d.png' % i\n+    cv2.imwrite(fname, im)\n+    end_time = time.time()\n+    print('Image saved as', fname)\n+    print('Iteration %d completed in %ds' % (i, end_time - start_time))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1fdcc370b6d9a71483bc968d5a0f3d259e07a78e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 264 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -17,6 +17,8 @@ python neural_style.py img/tuebingen.jpg img/starry_night.jpg results/my_result\n It is preferrable to run this script on GPU, for speed.\n If running on CPU, prefer the TensorFlow backend (much faster).\n \n+Example result: https://twitter.com/fchollet/status/686631033085677568\n+\n # Details\n \n Style transfer consists in generating an image\n@@ -67,9 +69,6 @@ parser.add_argument('style_reference_image_path', metavar='ref', type=str,\n parser.add_argument('result_prefix', metavar='res_prefix', type=str,\n                     help='Prefix for the saved results.')\n \n-# base_image_path = 'tuebingen.jpg'\n-# style_reference_image_path = 'starry_night.jpg'\n-# result_prefix = 'my_result_th'\n args = parser.parse_args()\n base_image_path = args.base_image_path\n style_reference_image_path = args.style_reference_image_path\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cb13a33a3191d703d439444b9aaa5a5089dae0d9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 274 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -29,10 +29,6 @@ This is achieved through the optimization of a loss function\n that has 3 components: \"style loss\", \"content loss\",\n and \"total variation loss\":\n \n- - The content loss is a L2 distance between the pixels of the base\n-image and the pixels of the combination image, keeping the generated image\n-close enough to the original one.\n-\n - The total variation loss imposes local spatial continuity between\n the pixels of the combination image, giving it visual coherence.\n \n@@ -44,7 +40,11 @@ different layers of a convnet (trained on ImageNet). The general idea\n is to capture color/texture information at different spatial\n scales (fairly large scales --defined by the depth of the layer considered).\n \n-# Reference:\n+ - The content loss is a L2 distance between the features of the base\n+image (extracted from a deep layer) and the features of the combination image,\n+keeping the generated image close enough to the original one.\n+\n+# References\n     - [A Neural Algorithm of Artistic Style](http://arxiv.org/abs/1508.06576)\n '''\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ee6bad63b0fac8d2b76cc00c32e916aeb38513b3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 280 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -49,7 +49,7 @@ keeping the generated image close enough to the original one.\n '''\n \n from __future__ import print_function\n-import cv2\n+from scipy.misc import imread, imresize, imsave\n import numpy as np\n from scipy.optimize import fmin_l_bfgs_b\n import time\n@@ -87,7 +87,7 @@ assert img_height == img_width, 'Due to the use of the Gram matrix, width and he\n \n # util function to open, resize and format pictures into appropriate tensors\n def preprocess_image(image_path):\n-    im = cv2.resize(cv2.imread(image_path), (img_width, img_height))\n+    im = imresize(imread(image_path), (img_width, img_height))\n     im = im.transpose((2, 0, 1))\n     im = np.expand_dims(im, axis=0)\n     return im\n@@ -252,7 +252,7 @@ for i in range(10):\n     # save current generated image\n     im = deprocess_image(x.reshape((3, img_width, img_height)))\n     fname = result_prefix + '_at_iteration_%d.png' % i\n-    cv2.imwrite(fname, im)\n+    imsave(fname, im)\n     end_time = time.time()\n     print('Image saved as', fname)\n     print('Iteration %d completed in %ds' % (i, end_time - start_time))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3d3b8c52e97ab8cbb66612b74e5564f70748b7ed", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 294 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -58,7 +58,6 @@ import h5py\n \n from keras.models import Sequential\n from keras.layers.convolutional import Convolution2D, ZeroPadding2D, MaxPooling2D\n-from keras.layers.core import Dense, Dropout, Flatten\n from keras import backend as K\n \n parser = argparse.ArgumentParser(description='Neural style transfer with Keras.')\n@@ -87,10 +86,10 @@ assert img_height == img_width, 'Due to the use of the Gram matrix, width and he\n \n # util function to open, resize and format pictures into appropriate tensors\n def preprocess_image(image_path):\n-    im = imresize(imread(image_path), (img_width, img_height))\n-    im = im.transpose((2, 0, 1))\n-    im = np.expand_dims(im, axis=0)\n-    return im\n+    img = imresize(imread(image_path), (img_width, img_height))\n+    img = img.transpose((2, 0, 1)).astype('float64')\n+    img = np.expand_dims(img, axis=0)\n+    return img\n \n # util function to convert a tensor into a valid image\n def deprocess_image(x):\n@@ -244,15 +243,14 @@ def eval_loss(x):\n x = np.random.uniform(0, 255, (1, 3, img_width, img_height))\n for i in range(10):\n     print('Start of iteration', i)\n-    eval_loss_calls = 0\n     start_time = time.time()\n     x, min_val, info = fmin_l_bfgs_b(eval_loss, x.flatten(),\n                                      fprime=eval_grads, maxfun=20)\n     print('Current loss value:', min_val)\n     # save current generated image\n-    im = deprocess_image(x.reshape((3, img_width, img_height)))\n+    img = deprocess_image(x.reshape((3, img_width, img_height)))\n     fname = result_prefix + '_at_iteration_%d.png' % i\n-    imsave(fname, im)\n+    imsave(fname, img)\n     end_time = time.time()\n     print('Image saved as', fname)\n     print('Iteration %d completed in %ds' % (i, end_time - start_time))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#58a94a9b058cded717583be4bf99b9365bdb7f2d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 198 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 5 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 198 | Churn Cumulative: 198 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,198 @@\n+'''Deep Dreaming in Keras.\n+\n+Run the script with:\n+```\n+python deep_dream.py path_to_your_base_image.jpg prefix_for_results\n+```\n+e.g.:\n+```\n+python deep_dream.py img/mypic.jpg results/dream\n+```\n+\n+It is preferrable to run this script on GPU, for speed.\n+If running on CPU, prefer the TensorFlow backend (much faster).\n+\n+Example results: http://i.imgur.com/FX6ROg9.jpg\n+'''\n+from __future__ import print_function\n+from scipy.misc import imread, imresize, imsave\n+import numpy as np\n+from scipy.optimize import fmin_l_bfgs_b\n+import time\n+import argparse\n+import h5py\n+\n+from keras.models import Sequential\n+from keras.layers.convolutional import Convolution2D, ZeroPadding2D, MaxPooling2D\n+from keras import backend as K\n+\n+parser = argparse.ArgumentParser(description='Deep Dreams with Keras.')\n+parser.add_argument('base_image_path', metavar='base', type=str,\n+                    help='Path to the image to transform.')\n+parser.add_argument('result_prefix', metavar='res_prefix', type=str,\n+                    help='Prefix for the saved results.')\n+\n+args = parser.parse_args()\n+base_image_path = args.base_image_path\n+result_prefix = args.result_prefix\n+\n+# dimensions of the generated picture.\n+img_width = 600\n+img_height = 600\n+\n+# path to the model weights file.\n+weights_path = 'vgg16_weights.h5'\n+\n+# some settings we found interesting\n+saved_settings = {\n+    'bad_trip': {'features': {'conv4_1': 0.05,\n+                              'conv4_2': 0.01,\n+                              'conv4_3': 0.01},\n+                 'continuity': 0.1,\n+                 'dream_l2': 0.8,\n+                 'jitter': 5},\n+    'dreamy': {'features': {'conv5_1': 0.05,\n+                            'conv5_2': 0.02},\n+               'continuity': 0.1,\n+               'dream_l2': 0.02,\n+               'jitter': 0},\n+}\n+# the settings we will use in this experiment\n+settings = saved_settings['dreamy']\n+\n+# util function to open, resize and format pictures into appropriate tensors\n+def preprocess_image(image_path):\n+    img = imresize(imread(image_path), (img_width, img_height))\n+    img = img.transpose((2, 0, 1)).astype('float64')\n+    img = np.expand_dims(img, axis=0)\n+    return img\n+\n+# util function to convert a tensor into a valid image\n+def deprocess_image(x):\n+    x = x.transpose((1, 2, 0))\n+    x = np.clip(x, 0, 255).astype('uint8')\n+    return x\n+\n+# this will contain our generated image\n+dream = K.placeholder((1, 3, img_width, img_height))\n+\n+# build the VGG16 network with our dream as input\n+first_layer = ZeroPadding2D((1, 1), input_shape=(3, img_width, img_height))\n+first_layer.input = dream\n+\n+model = Sequential()\n+model.add(first_layer)\n+model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n+model.add(ZeroPadding2D((1, 1)))\n+model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n+model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n+\n+model.add(ZeroPadding2D((1, 1)))\n+model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n+model.add(ZeroPadding2D((1, 1)))\n+model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n+model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n+\n+model.add(ZeroPadding2D((1, 1)))\n+model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n+model.add(ZeroPadding2D((1, 1)))\n+model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n+model.add(ZeroPadding2D((1, 1)))\n+model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n+model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n+\n+model.add(ZeroPadding2D((1, 1)))\n+model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n+model.add(ZeroPadding2D((1, 1)))\n+model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n+model.add(ZeroPadding2D((1, 1)))\n+model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n+model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n+\n+model.add(ZeroPadding2D((1, 1)))\n+model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n+model.add(ZeroPadding2D((1, 1)))\n+model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n+model.add(ZeroPadding2D((1, 1)))\n+model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n+model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n+\n+# load the weights of the VGG16 networks\n+# (trained on ImageNet, won the ILSVRC competition in 2014)\n+# note: when there is a complete match between your model definition\n+# and your weight savefile, you can simply call model.load_weights(filename)\n+f = h5py.File(weights_path)\n+for k in range(f.attrs['nb_layers']):\n+    if k >= len(model.layers):\n+        # we don't look at the last (fully-connected) layers in the savefile\n+        break\n+    g = f['layer_{}'.format(k)]\n+    weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n+    model.layers[k].set_weights(weights)\n+f.close()\n+print('Model loaded.')\n+\n+# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n+layer_dict = dict([(layer.name, layer) for layer in model.layers])\n+\n+# continuity loss util function\n+def continuity_loss(x):\n+    assert K.ndim(x) == 4\n+    a = K.square(x[:, :, :img_width-1, :img_height-1] - x[:, :, 1:, :img_height-1])\n+    b = K.square(x[:, :, :img_width-1, :img_height-1] - x[:, :, :img_width-1, 1:])\n+    return K.sum(K.pow(a + b, 1.25))\n+\n+# define the loss\n+loss = K.variable(0.)\n+for layer_name in settings['features']:\n+    # add the L2 norm of the features of a layer to the loss\n+    assert layer_name in layer_dict.keys(), 'Layer ' + layer_name + ' not found in model.'\n+    coeff = settings['features'][layer_name]\n+    x = layer_dict[layer_name].get_output()\n+    shape = layer_dict[layer_name].output_shape\n+    # we avoid border artifacts by only involving non-border pixels in the loss\n+    loss -= coeff * K.sum(K.square(x[:, :, 2: shape[2]-2, 2: shape[3]-2])) / np.prod(shape[1:])\n+\n+# add continuity loss (gives image local coherence, can result in an artful blur)\n+loss += settings['continuity'] * continuity_loss(dream) / (3 * img_width * img_height)\n+# add image L2 norm to loss (prevents pixels from taking very high values, makes image darker)\n+loss += settings['dream_l2'] * K.sum(K.square(dream)) / (3 * img_width * img_height)\n+\n+# feel free to further modify the loss as you see fit, to achieve new effects...\n+\n+# compute the gradients of the dream wrt the loss\n+grads = K.gradients(loss, dream)\n+\n+# set up helper functions to extract the loss and gradients\n+# from the computational graph as Numpy arrays\n+f_grads = K.function([dream], grads)\n+def eval_grads(x):\n+    x = x.reshape((1, 3, img_width, img_height))\n+    return np.array(f_grads([x])).flatten().astype('float64')\n+\n+f_loss = K.function([dream], [loss])\n+def eval_loss(x):\n+    x = x.reshape((1, 3, img_width, img_height))\n+    return f_loss([x])[0].astype('float64')\n+\n+# add a random jitter to the initial image. This will be reverted at decoding time\n+random_jitter = (settings['jitter'] * 2) * (np.random.random((3, img_width, img_height)) - 0.5)\n+x = preprocess_image(base_image_path)\n+x += random_jitter\n+\n+# run scipy-based optimization (L-BFGS) over the pixels of the generated image\n+# so as to minimize the loss\n+for i in range(5):\n+    start_time = time.time()\n+    x, min_val, info = fmin_l_bfgs_b(eval_loss, x.flatten(),\n+                                     fprime=eval_grads, maxfun=7)\n+    print('Current loss value:', min_val)\n+    # decode the dream and save it\n+    x = x.reshape((3, img_width, img_height))\n+    x -= random_jitter\n+    img = deprocess_image(x)\n+    fname = result_prefix + '_at_iteration_%d.png' % i\n+    imsave(fname, img)\n+    end_time = time.time()\n+    print('Image saved as', fname)\n+    print('Iteration %d completed in %ds' % (i, end_time - start_time))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
