{"custom_id": "keras#38a0caa8ae647ad18e3da8a16681df0ad9867582", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 340 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 13 | Methods Changed: 15 | Complexity Δ (Sum/Max): 29/21 | Churn Δ: 349 | Churn Cumulative: 3457 | Contributors (this commit): 14 | Commits (past 90d): 6 | Contributors (cumulative): 21 | DMM Complexity: 1.0\n\nDIFF:\n@@ -71,6 +71,15 @@ class BaseConv(Layer):\n             are not safe to use when doing asynchronous distributed training.\n         bias_constraint: Optional projection function to be applied to the\n             bias after being updated by an `Optimizer`.\n+        lora_rank: Optional integer. If set, the layer's forward pass\n+            will implement LoRA (Low-Rank Adaptation)\n+            with the provided rank. LoRA sets the layer's kernel\n+            to non-trainable and replaces it with a delta over the\n+            original kernel, obtained via multiplying two lower-rank\n+            trainable matrices. This can be useful to reduce the\n+            computation cost of fine-tuning large dense layers.\n+            You can also enable LoRA on an existing layer by calling\n+            `layer.enable_lora(rank)`.\n     \"\"\"\n \n     def __init__(\n@@ -92,16 +101,10 @@ class BaseConv(Layer):\n         activity_regularizer=None,\n         kernel_constraint=None,\n         bias_constraint=None,\n-        trainable=True,\n-        name=None,\n+        lora_rank=None,\n         **kwargs,\n     ):\n-        super().__init__(\n-            trainable=trainable,\n-            name=name,\n-            activity_regularizer=activity_regularizer,\n-            **kwargs,\n-        )\n+        super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n         self.rank = rank\n         self.filters = filters\n         self.groups = groups\n@@ -120,6 +123,8 @@ class BaseConv(Layer):\n         self.bias_regularizer = regularizers.get(bias_regularizer)\n         self.kernel_constraint = constraints.get(kernel_constraint)\n         self.bias_constraint = constraints.get(bias_constraint)\n+        self.lora_rank = lora_rank\n+        self.lora_enabled = False\n         self.input_spec = InputSpec(min_ndim=self.rank + 2)\n         self.data_format = self.data_format\n \n@@ -187,7 +192,7 @@ class BaseConv(Layer):\n         # shape, and make sure the output shape has all positive dimensions.\n         self.compute_output_shape(input_shape)\n \n-        self.kernel = self.add_weight(\n+        self._kernel = self.add_weight(\n             name=\"kernel\",\n             shape=kernel_shape,\n             initializer=self.kernel_initializer,\n@@ -209,6 +214,20 @@ class BaseConv(Layer):\n         else:\n             self.bias = None\n         self.built = True\n+        if self.lora_rank:\n+            self.enable_lora(self.lora_rank)\n+\n+    @property\n+    def kernel(self):\n+        if not self.built:\n+            raise AttributeError(\n+                \"You must build the layer before accessing `kernel`.\"\n+            )\n+        if self.lora_enabled:\n+            return self._kernel + ops.matmul(\n+                self.lora_kernel_a, self.lora_kernel_b\n+            )\n+        return self._kernel\n \n     def convolution_op(self, inputs, kernel):\n         return ops.conv(\n@@ -248,6 +267,63 @@ class BaseConv(Layer):\n             dilation_rate=self.dilation_rate,\n         )\n \n+    def enable_lora(\n+        self, rank, a_initializer=\"he_uniform\", b_initializer=\"zeros\"\n+    ):\n+        if self.kernel_constraint:\n+            raise ValueError(\n+                \"Lora is incompatible with kernel constraints. \"\n+                \"In order to enable lora on this layer, remove the \"\n+                \"`kernel_constraint` argument.\"\n+            )\n+        if not self.built:\n+            raise ValueError(\n+                \"Cannot enable lora on a layer that isn't yet built.\"\n+            )\n+        if self.lora_enabled:\n+            raise ValueError(\n+                \"lora is already enabled. \"\n+                \"This can only be done once per layer.\"\n+            )\n+        self._tracker.unlock()\n+        self.lora_kernel_a = self.add_weight(\n+            name=\"lora_kernel_a\",\n+            shape=self._kernel.shape[:-1] + (rank,),\n+            initializer=initializers.get(a_initializer),\n+            regularizer=self.kernel_regularizer,\n+        )\n+        self.lora_kernel_b = self.add_weight(\n+            name=\"lora_kernel_b\",\n+            shape=(rank, self.filters),\n+            initializer=initializers.get(b_initializer),\n+            regularizer=self.kernel_regularizer,\n+        )\n+        self._kernel.trainable = False\n+        self._tracker.lock()\n+        self.lora_enabled = True\n+        self.lora_rank = rank\n+\n+    def save_own_variables(self, store):\n+        # Do nothing if the layer isn't yet built\n+        if not self.built:\n+            return\n+        store[\"0\"] = self.kernel\n+        if self.use_bias:\n+            store[\"1\"] = self.bias\n+\n+    def load_own_variables(self, store):\n+        if not self.lora_enabled:\n+            self._check_load_own_variables(store)\n+        # Do nothing if the layer isn't yet built\n+        if not self.built:\n+            return\n+        self._kernel.assign(store[\"0\"])\n+        if self.use_bias:\n+            self.bias.assign(store[\"1\"])\n+        if self.lora_enabled:\n+            self.lora_kernel_a.assign(ops.zeros(self.lora_kernel_a.shape))\n+            self.lora_kernel_b.assign(ops.zeros(self.lora_kernel_b.shape))\n+\n     def get_config(self):\n         config = super().get_config()\n         config.update(\n@@ -282,4 +358,40 @@ class BaseConv(Layer):\n                 \"bias_constraint\": constraints.serialize(self.bias_constraint),\n             }\n         )\n+        if self.lora_rank:\n+            config[\"lora_rank\"] = self.lora_rank\n         return config\n+\n+    def _check_load_own_variables(self, store):\n+        all_vars = self._trainable_variables + self._non_trainable_variables\n+        if len(store.keys()) != len(all_vars):\n+            if len(all_vars) == 0 and not self.built:\n+                raise ValueError(\n+                    f\"Layer '{self.name}' was never built \"\n+                    \"and thus it doesn't have any variables. \"\n+                    f\"However the weights file lists {len(store.keys())} \"\n+                    \"variables for this layer.\\n\"\n+                    \"In most cases, this error indicates that either:\\n\\n\"\n+                    \"1. The layer is owned by a parent layer that \"\n+                    \"implements a `build()` method, but calling the \"\n+                    \"parent's `build()` method did NOT create the state of \"\n+                    f\"the child layer '{self.name}'. A `build()` method \"\n+                    \"must create ALL state for the layer, including \"\n+                    \"the state of any children layers.\\n\\n\"\n+                    \"2. You need to implement \"\n+                    \"the `def build_from_config(self, config)` method \"\n+                    f\"on layer '{self.name}', to specify how to rebuild \"\n+                    \"it during loading. \"\n+                    \"In this case, you might also want to implement the \"\n+                    \"method that generates the build config at saving time, \"\n+                    \"`def get_build_config(self)`. \"\n+                    \"The method `build_from_config()` is meant \"\n+                    \"to create the state \"\n+                    \"of the layer (i.e. its variables) upon deserialization.\",\n+                )\n+            raise ValueError(\n+                f\"Layer '{self.name}' expected {len(all_vars)} variables, \"\n+                \"but received \"\n+                f\"{len(store.keys())} variables during loading. \"\n+                f\"Expected: {[v.name for v in all_vars]}\"\n+            )\n\n@@ -1,10 +1,15 @@\n+import os\n+\n import numpy as np\n import pytest\n from absl.testing import parameterized\n from numpy.lib.stride_tricks import as_strided\n \n+from keras import backend\n from keras import constraints\n from keras import layers\n+from keras import models\n+from keras import saving\n from keras import testing\n \n \n@@ -538,6 +543,220 @@ class ConvBasicTest(testing.TestCase, parameterized.TestCase):\n         ):\n             layers.Conv2D(filters=5, kernel_size=(2, 2), groups=2)\n \n+    @parameterized.named_parameters(\n+        {\n+            \"testcase_name\": \"conv1d_kernel_size3_strides1\",\n+            \"conv_cls\": layers.Conv1D,\n+            \"filters\": 6,\n+            \"kernel_size\": 3,\n+            \"strides\": 1,\n+            \"padding\": \"valid\",\n+            \"data_format\": \"channels_last\",\n+            \"dilation_rate\": 1,\n+            \"groups\": 1,\n+            \"input_shape\": (None, 5, 4),\n+            \"output_shape\": (None, 3, 6),\n+        },\n+        {\n+            \"testcase_name\": \"conv1d_kernel_size2_strides2\",\n+            \"conv_cls\": layers.Conv1D,\n+            \"filters\": 6,\n+            \"kernel_size\": 2,\n+            \"strides\": 2,\n+            \"padding\": \"valid\",\n+            \"data_format\": \"channels_last\",\n+            \"dilation_rate\": 1,\n+            \"groups\": 2,\n+            \"input_shape\": (None, 5, 4),\n+            \"output_shape\": (None, 2, 6),\n+        },\n+        {\n+            \"testcase_name\": \"conv2d_kernel_size3_strides1\",\n+            \"conv_cls\": layers.Conv2D,\n+            \"filters\": 6,\n+            \"kernel_size\": 3,\n+            \"strides\": 1,\n+            \"padding\": \"valid\",\n+            \"data_format\": \"channels_last\",\n+            \"dilation_rate\": 1,\n+            \"groups\": 1,\n+            \"input_shape\": (None, 5, 5, 4),\n+            \"output_shape\": (None, 3, 3, 6),\n+        },\n+        {\n+            \"testcase_name\": \"conv2d_kernel_size2_strides2\",\n+            \"conv_cls\": layers.Conv2D,\n+            \"filters\": 6,\n+            \"kernel_size\": 2,\n+            \"strides\": 2,\n+            \"padding\": \"valid\",\n+            \"data_format\": \"channels_last\",\n+            \"dilation_rate\": 1,\n+            \"groups\": 2,\n+            \"input_shape\": (None, 5, 5, 4),\n+            \"output_shape\": (None, 2, 2, 6),\n+        },\n+        {\n+            \"testcase_name\": \"conv3d_kernel_size3_strides1\",\n+            \"conv_cls\": layers.Conv3D,\n+            \"filters\": 6,\n+            \"kernel_size\": 3,\n+            \"strides\": 1,\n+            \"padding\": \"valid\",\n+            \"data_format\": \"channels_last\",\n+            \"dilation_rate\": 1,\n+            \"groups\": 1,\n+            \"input_shape\": (None, 5, 5, 5, 4),\n+            \"output_shape\": (None, 3, 3, 3, 6),\n+        },\n+        {\n+            \"testcase_name\": \"conv3d_kernel_size2_strides2\",\n+            \"conv_cls\": layers.Conv3D,\n+            \"filters\": 6,\n+            \"kernel_size\": 2,\n+            \"strides\": 2,\n+            \"padding\": \"valid\",\n+            \"data_format\": \"channels_last\",\n+            \"dilation_rate\": 1,\n+            \"groups\": 2,\n+            \"input_shape\": (None, 5, 5, 5, 4),\n+            \"output_shape\": (None, 2, 2, 2, 6),\n+        },\n+    )\n+    @pytest.mark.requires_trainable_backend\n+    def test_enable_lora(\n+        self,\n+        conv_cls,\n+        filters,\n+        kernel_size,\n+        strides,\n+        padding,\n+        data_format,\n+        dilation_rate,\n+        groups,\n+        input_shape,\n+        output_shape,\n+    ):\n+        if conv_cls not in (layers.Conv1D, layers.Conv2D, layers.Conv3D):\n+            raise TypeError\n+        layer = conv_cls(\n+            filters=filters,\n+            kernel_size=kernel_size,\n+            strides=strides,\n+            padding=padding,\n+            data_format=data_format,\n+            dilation_rate=dilation_rate,\n+            groups=groups,\n+        )\n+        layer.build(input_shape)\n+        layer.enable_lora(2)\n+        self.assertLen(layer.trainable_weights, 3)\n+        self.assertLen(layer.non_trainable_weights, 1)\n+        if backend.backend() == \"torch\":\n+            self.assertLen(layer.torch_params, 4)\n+        # Try eager call\n+        x = np.random.random((64,) + input_shape[1:])\n+        y = np.random.random((64,) + output_shape[1:])\n+        _ = layer(x[:2])\n+\n+        init_lora_a_kernel_value = layer.lora_kernel_a.numpy()\n+        init_lora_b_kernel_value = layer.lora_kernel_b.numpy()\n+\n+        # Try calling fit()\n+        model = models.Sequential([layer])\n+        model.compile(optimizer=\"sgd\", loss=\"mse\")\n+        model.fit(x, y)\n+\n+        final_lora_a_kernel_value = layer.lora_kernel_a.numpy()\n+        final_lora_b_kernel_value = layer.lora_kernel_b.numpy()\n+        diff_a = np.max(\n+            np.abs(init_lora_a_kernel_value - final_lora_a_kernel_value)\n+        )\n+        diff_b = np.max(\n+            np.abs(init_lora_b_kernel_value - final_lora_b_kernel_value)\n+        )\n+        self.assertGreater(diff_a, 0.0)\n+        self.assertGreater(diff_b, 0.0)\n+\n+        # Try saving and reloading the model\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"lora_model.keras\")\n+        model.save(temp_filepath)\n+\n+        new_model = saving.load_model(temp_filepath)\n+        self.assertTrue(new_model.layers[0].lora_enabled)\n+        self.assertAllClose(model.predict(x), new_model.predict(x))\n+\n+        # Try saving and reloading the model's weights only\n+        temp_filepath = os.path.join(\n+            self.get_temp_dir(), \"lora_model.weights.h5\"\n+        )\n+        model.save_weights(temp_filepath)\n+\n+        # Load the file into a fresh, non-lora model\n+        new_model = models.Sequential(\n+            [\n+                conv_cls(\n+                    filters=filters,\n+                    kernel_size=kernel_size,\n+                    strides=strides,\n+                    padding=padding,\n+                    data_format=data_format,\n+                    dilation_rate=dilation_rate,\n+                    groups=groups,\n+                )\n+            ]\n+        )\n+        new_model.build(input_shape)\n+        new_model.load_weights(temp_filepath)\n+        self.assertAllClose(model.predict(x), new_model.predict(x))\n+\n+        # Try loading a normal checkpoint into a lora model\n+        new_model.save_weights(temp_filepath)\n+        model.load_weights(temp_filepath)\n+        self.assertAllClose(model.predict(x), new_model.predict(x))\n+\n+    @pytest.mark.requires_trainable_backend\n+    def test_lora_weight_name(self):\n+\n+        class MyModel(models.Model):\n+            def __init__(self):\n+                super().__init__(name=\"mymodel\")\n+                self.conv2d = layers.Conv2D(4, 3, name=\"conv2d\")\n+\n+            def build(self, input_shape):\n+                self.conv2d.build(input_shape)\n+\n+            def call(self, x):\n+                return self.conv2d(x)\n+\n+        model = MyModel()\n+        model.build((None, 5, 5, 4))\n+        model.conv2d.enable_lora(2)\n+        self.assertEqual(\n+            model.conv2d.lora_kernel_a.path, \"mymodel/conv2d/lora_kernel_a\"\n+        )\n+\n+    @pytest.mark.requires_trainable_backend\n+    def test_lora_rank_argument(self):\n+        self.run_layer_test(\n+            layers.Conv2D,\n+            init_kwargs={\n+                \"filters\": 5,\n+                \"kernel_size\": 3,\n+                \"activation\": \"sigmoid\",\n+                \"data_format\": \"channels_last\",\n+                \"kernel_regularizer\": \"l2\",\n+                \"lora_rank\": 2,\n+            },\n+            input_shape=(2, 5, 5, 4),\n+            expected_output_shape=(2, 3, 3, 5),\n+            expected_num_trainable_weights=3,\n+            expected_num_non_trainable_weights=1,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=2,  # we have 2 regularizers.\n+            supports_masking=False,\n+        )\n+\n \n class ConvCorrectnessTest(testing.TestCase, parameterized.TestCase):\n     @parameterized.parameters(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c458201d9a0e5b50f3f7b3e638cc20867222b8a7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 516 | Lines Deleted: 380 | Files Changed: 67 | Hunks: 153 | Methods Changed: 41 | Complexity Δ (Sum/Max): 133/72 | Churn Δ: 896 | Churn Cumulative: 28272 | Contributors (this commit): 46 | Commits (past 90d): 365 | Contributors (cumulative): 281 | DMM Complexity: 0.3157894736842105\n\nDIFF:\n@@ -1,5 +1,5 @@\n+from keras import tree\n from keras.api_export import keras_export\n-from keras.utils import tree\n from keras.utils.naming import auto_name\n \n \n\n@@ -4,13 +4,13 @@ import jax.numpy as jnp\n import ml_dtypes\n import numpy as np\n \n+from keras import tree\n from keras.backend.common import KerasVariable\n from keras.backend.common import global_state\n from keras.backend.common import standardize_dtype\n from keras.backend.common.keras_tensor import KerasTensor\n from keras.backend.common.stateless_scope import StatelessScope\n from keras.backend.jax import distribution_lib\n-from keras.utils import tree\n \n SUPPORTS_SPARSE_TENSORS = True\n \n\n@@ -3,8 +3,8 @@ import contextlib\n from jax import lax\n from jax import numpy as jnp\n \n+from keras import tree\n from keras.backend.common import stateless_scope\n-from keras.utils import tree\n \n \n def rnn(\n\n@@ -8,6 +8,7 @@ import numpy as np\n from keras import backend\n from keras import callbacks as callbacks_module\n from keras import optimizers as optimizers_module\n+from keras import tree\n from keras.backend import distribution_lib as jax_distribution_lib\n from keras.distribution import distribution_lib\n from keras.trainers import trainer as base_trainer\n@@ -15,7 +16,6 @@ from keras.trainers.data_adapters import array_slicing\n from keras.trainers.data_adapters import data_adapter_utils\n from keras.trainers.epoch_iterator import EpochIterator\n from keras.utils import traceback_utils\n-from keras.utils import tree\n \n \n class JAXTrainer(base_trainer.Trainer):\n\n@@ -1,11 +1,11 @@\n import numpy as np\n \n+from keras import tree\n from keras.backend.common import KerasVariable\n from keras.backend.common import standardize_dtype\n from keras.backend.common.dtypes import result_type\n from keras.backend.common.keras_tensor import KerasTensor\n from keras.backend.common.stateless_scope import StatelessScope\n-from keras.utils import tree\n \n SUPPORTS_SPARSE_TENSORS = False\n \n\n@@ -1,11 +1,11 @@\n import numpy as np\n \n+from keras import tree\n from keras.backend import config\n from keras.backend import standardize_dtype\n from keras.backend.common import dtypes\n from keras.backend.common.backend_utils import standardize_axis_for_numpy\n from keras.backend.numpy.core import convert_to_tensor\n-from keras.utils import tree\n \n \n def add(x1, x2):\n\n@@ -1,6 +1,6 @@\n import numpy as np\n \n-from keras.utils import tree\n+from keras import tree\n \n \n def rnn(\n\n@@ -2,6 +2,7 @@ import numpy as np\n \n from keras import backend\n from keras import callbacks as callbacks_module\n+from keras import tree\n from keras.backend.common import standardize_dtype\n from keras.backend.common.keras_tensor import KerasTensor\n from keras.backend.numpy.core import is_tensor\n@@ -9,7 +10,6 @@ from keras.trainers import trainer as base_trainer\n from keras.trainers.data_adapters import data_adapter_utils\n from keras.trainers.epoch_iterator import EpochIterator\n from keras.utils import traceback_utils\n-from keras.utils import tree\n \n \n class NumpyTrainer(base_trainer.Trainer):\n\n@@ -2,6 +2,7 @@ import numpy as np\n import tensorflow as tf\n from tensorflow.compiler.tf2xla.python.xla import dynamic_update_slice\n \n+from keras import tree\n from keras.backend.common import KerasVariable\n from keras.backend.common import global_state\n from keras.backend.common import standardize_dtype\n@@ -10,7 +11,6 @@ from keras.backend.common.name_scope import name_scope as base_name_scope\n from keras.backend.common.stateless_scope import StatelessScope\n from keras.backend.common.stateless_scope import in_stateless_scope\n from keras.backend.tensorflow.sparse import sparse_to_dense\n-from keras.utils import tree\n from keras.utils.naming import auto_name\n \n SUPPORTS_SPARSE_TENSORS = True\n\n@@ -1,9 +1,9 @@\n import tensorflow as tf\n \n+from keras import tree\n from keras.backend.tensorflow.trackable import KerasAutoTrackable\n from keras.utils import tf_utils\n from keras.utils import tracking\n-from keras.utils import tree\n \n \n class TFLayer(KerasAutoTrackable):\n\n@@ -10,6 +10,7 @@ import tensorflow as tf\n from tensorflow.experimental import numpy as tfnp\n from tensorflow.python.ops.linalg.sparse import sparse_csr_matrix_ops\n \n+from keras import tree\n from keras.backend import config\n from keras.backend import standardize_dtype\n from keras.backend.common import dtypes\n@@ -18,7 +19,6 @@ from keras.backend.common.backend_utils import to_tuple_or_list\n from keras.backend.tensorflow import sparse\n from keras.backend.tensorflow.core import cast\n from keras.backend.tensorflow.core import convert_to_tensor\n-from keras.utils import tree\n \n \n @sparse.elementwise_binary_union(tf.sparse.add)\n\n@@ -1,6 +1,6 @@\n import tensorflow as tf\n \n-from keras.utils import tree\n+from keras import tree\n \n \n def rnn(\n\n@@ -8,12 +8,12 @@ from tensorflow.python.eager import context as tf_context\n from keras import callbacks as callbacks_module\n from keras import metrics as metrics_module\n from keras import optimizers as optimizers_module\n+from keras import tree\n from keras.trainers import trainer as base_trainer\n from keras.trainers.data_adapters import array_slicing\n from keras.trainers.data_adapters import data_adapter_utils\n from keras.trainers.epoch_iterator import EpochIterator\n from keras.utils import traceback_utils\n-from keras.utils import tree\n \n \n class TensorFlowTrainer(base_trainer.Trainer):\n\n@@ -5,6 +5,7 @@ import ml_dtypes\n import numpy as np\n import torch\n \n+from keras import tree\n from keras.backend.common import KerasVariable\n from keras.backend.common import global_state\n from keras.backend.common import standardize_dtype\n@@ -12,7 +13,6 @@ from keras.backend.common.dtypes import result_type\n from keras.backend.common.keras_tensor import KerasTensor\n from keras.backend.common.stateless_scope import StatelessScope\n from keras.backend.config import floatx\n-from keras.utils import tree\n \n SUPPORTS_SPARSE_TENSORS = False\n \n\n@@ -1,6 +1,7 @@\n import torch\n import torch.nn.functional as tnn\n \n+from keras import tree\n from keras.backend import standardize_data_format\n from keras.backend import standardize_dtype\n from keras.backend.common.backend_utils import (\n@@ -13,7 +14,6 @@ from keras.backend.torch.core import get_device\n from keras.backend.torch.numpy import expand_dims\n from keras.backend.torch.numpy import maximum\n from keras.backend.torch.numpy import where\n-from keras.utils import tree\n from keras.utils.argument_validation import standardize_tuple\n \n \n\n@@ -1,7 +1,7 @@\n import torch\n \n+from keras import tree\n from keras.backend.torch.core import convert_to_tensor\n-from keras.utils import tree\n \n \n def rnn(\n\n@@ -7,12 +7,12 @@ from packaging.version import parse\n from keras import backend\n from keras import callbacks as callbacks_module\n from keras import optimizers as optimizers_module\n+from keras import tree\n from keras.trainers import trainer as base_trainer\n from keras.trainers.data_adapters import array_slicing\n from keras.trainers.data_adapters import data_adapter_utils\n from keras.trainers.epoch_iterator import EpochIterator\n from keras.utils import traceback_utils\n-from keras.utils import tree\n \n \n class TorchTrainer(base_trainer.Trainer):\n\n@@ -1,8 +1,8 @@\n+from keras import tree\n from keras.api_export import keras_export\n from keras.callbacks.callback import Callback\n from keras.callbacks.history import History\n from keras.callbacks.progbar_logger import ProgbarLogger\n-from keras.utils import tree\n \n \n @keras_export(\"keras.callbacks.CallbackList\")\n\n@@ -6,12 +6,12 @@ import warnings\n \n from keras import backend\n from keras import ops\n+from keras import tree\n from keras.api_export import keras_export\n from keras.callbacks.callback import Callback\n from keras.layers import Embedding\n from keras.optimizers import Optimizer\n from keras.utils import file_utils\n-from keras.utils import tree\n \n \n @keras_export(\"keras.callbacks.TensorBoard\")\n\n@@ -7,13 +7,13 @@ import string\n from absl import logging\n \n from keras import backend\n+from keras import tree\n from keras.api_export import keras_export\n from keras.backend.common.stateless_scope import StatelessScope\n from keras.layers import Layer\n from keras.models import Functional\n from keras.models import Sequential\n from keras.utils import io_utils\n-from keras.utils import tree\n from keras.utils.module_utils import tensorflow as tf\n \n \n\n@@ -13,11 +13,11 @@ from keras import models\n from keras import ops\n from keras import random\n from keras import testing\n+from keras import tree\n from keras import utils\n from keras.export import export_lib\n from keras.saving import saving_lib\n from keras.testing.test_utils import named_product\n-from keras.utils import tree\n \n \n class CustomModel(models.Model):\n\n@@ -1,7 +1,7 @@\n+from keras import tree\n from keras.api_export import keras_export\n from keras.backend import KerasTensor\n from keras.layers.layer import Layer\n-from keras.utils import tree\n \n \n @keras_export(\"keras.layers.Identity\")\n\n@@ -2,11 +2,11 @@ import inspect\n import types\n \n from keras import backend\n+from keras import tree\n from keras.api_export import keras_export\n from keras.layers.layer import Layer\n from keras.saving import serialization_lib\n from keras.utils import python_utils\n-from keras.utils import tree\n \n \n @keras_export(\"keras.layers.Lambda\")\n\n@@ -1,6 +1,6 @@\n from keras import backend\n+from keras import tree\n from keras.api_export import keras_export\n-from keras.utils import tree\n \n \n @keras_export([\"keras.InputSpec\", \"keras.layers.InputSpec\"])\n\n@@ -26,6 +26,7 @@ from keras import constraints\n from keras import dtype_policies\n from keras import initializers\n from keras import regularizers\n+from keras import tree\n from keras import utils\n from keras.api_export import keras_export\n from keras.backend import KerasTensor\n@@ -39,7 +40,6 @@ from keras.utils import python_utils\n from keras.utils import summary_utils\n from keras.utils import traceback_utils\n from keras.utils import tracking\n-from keras.utils import tree\n \n if backend.backend() == \"tensorflow\":\n     from keras.backend.tensorflow.layer import TFLayer as BackendLayer\n\n@@ -1,11 +1,11 @@\n from keras import backend\n from keras import layers\n+from keras import tree\n from keras.api_export import keras_export\n from keras.layers.layer import Layer\n from keras.saving import saving_lib\n from keras.saving import serialization_lib\n from keras.utils import backend_utils\n-from keras.utils import tree\n from keras.utils.module_utils import tensorflow as tf\n from keras.utils.naming import auto_name\n \n\n@@ -1,9 +1,9 @@\n import keras.backend\n+from keras import tree\n from keras.layers.layer import Layer\n from keras.random.seed_generator import SeedGenerator\n from keras.utils import backend_utils\n from keras.utils import tracking\n-from keras.utils import tree\n \n \n class TFDataLayer(Layer):\n\n@@ -4,13 +4,13 @@ from keras import constraints\n from keras import initializers\n from keras import ops\n from keras import regularizers\n+from keras import tree\n from keras.layers.input_spec import InputSpec\n from keras.layers.layer import Layer\n from keras.layers.rnn.dropout_rnn_cell import DropoutRNNCell\n from keras.layers.rnn.rnn import RNN\n from keras.ops import operation_utils\n from keras.utils import argument_validation\n-from keras.utils import tree\n \n \n class ConvLSTMCell(Layer, DropoutRNNCell):\n\n@@ -4,12 +4,12 @@ from keras import constraints\n from keras import initializers\n from keras import ops\n from keras import regularizers\n+from keras import tree\n from keras.api_export import keras_export\n from keras.layers.input_spec import InputSpec\n from keras.layers.layer import Layer\n from keras.layers.rnn.dropout_rnn_cell import DropoutRNNCell\n from keras.layers.rnn.rnn import RNN\n-from keras.utils import tree\n \n \n @keras_export(\"keras.layers.GRUCell\")\n\n@@ -4,12 +4,12 @@ from keras import constraints\n from keras import initializers\n from keras import ops\n from keras import regularizers\n+from keras import tree\n from keras.api_export import keras_export\n from keras.layers.input_spec import InputSpec\n from keras.layers.layer import Layer\n from keras.layers.rnn.dropout_rnn_cell import DropoutRNNCell\n from keras.layers.rnn.rnn import RNN\n-from keras.utils import tree\n \n \n @keras_export(\"keras.layers.LSTMCell\")\n\n@@ -1,12 +1,12 @@\n from keras import backend\n from keras import ops\n+from keras import tree\n from keras.api_export import keras_export\n from keras.layers.layer import Layer\n from keras.layers.rnn.dropout_rnn_cell import DropoutRNNCell\n from keras.layers.rnn.stacked_rnn_cells import StackedRNNCells\n from keras.saving import serialization_lib\n from keras.utils import tracking\n-from keras.utils import tree\n \n \n @keras_export(\"keras.layers.RNN\")\n\n@@ -1,8 +1,8 @@\n from keras import ops\n+from keras import tree\n from keras.api_export import keras_export\n from keras.layers.layer import Layer\n from keras.saving import serialization_lib\n-from keras.utils import tree\n \n \n @keras_export(\"keras.layers.StackedRNNCells\")\n\n@@ -9,9 +9,9 @@ from keras import losses\n from keras import metrics as metrics_module\n from keras import models\n from keras import optimizers\n+from keras import tree\n from keras.legacy.saving import serialization\n from keras.saving import object_registration\n-from keras.utils import tree\n \n MODULE_OBJECTS = threading.local()\n \n\n@@ -1,7 +1,7 @@\n from keras import backend\n from keras import ops\n+from keras import tree\n from keras.api_export import keras_export\n-from keras.utils import tree\n from keras.utils.naming import auto_name\n \n \n\n@@ -1,4 +1,5 @@\n from keras import backend\n+from keras import tree\n from keras import utils\n from keras.api_export import keras_export\n from keras.layers import Input\n@@ -7,7 +8,6 @@ from keras.models.functional import Functional\n from keras.models.functional import functional_like_constructor\n from keras.models.sequential import Sequential\n from keras.saving import serialization_lib\n-from keras.utils import tree\n \n \n @keras_export(\"keras.models.clone_model\")\n\n@@ -5,8 +5,8 @@ from absl.testing import parameterized\n from keras import layers\n from keras import models\n from keras import testing\n+from keras import tree\n from keras.models.cloning import clone_model\n-from keras.utils import tree\n \n \n def get_mlp_functional_model(shared_layers=False):\n\n@@ -5,6 +5,7 @@ import warnings\n \n from keras import backend\n from keras import ops\n+from keras import tree\n from keras.backend.common import global_state\n from keras.layers.core.input_layer import Input\n from keras.layers.core.input_layer import InputLayer\n@@ -20,7 +21,6 @@ from keras.ops.node import KerasHistory\n from keras.ops.node import Node\n from keras.saving import serialization_lib\n from keras.utils import tracking\n-from keras.utils import tree\n \n \n class Functional(Function, Model):\n\n@@ -2,6 +2,7 @@ import copy\n import inspect\n import typing\n \n+from keras import tree\n from keras.api_export import keras_export\n from keras.backend.common import global_state\n from keras.layers.core.input_layer import InputLayer\n@@ -11,7 +12,6 @@ from keras.legacy.saving import serialization as legacy_serialization\n from keras.models.functional import Functional\n from keras.models.model import Model\n from keras.saving import serialization_lib\n-from keras.utils import tree\n \n \n @keras_export([\"keras.Sequential\", \"keras.models.Sequential\"])\n\n@@ -17,12 +17,12 @@ custom_gradient\n import numpy as np\n \n from keras import backend\n+from keras import tree\n from keras.api_export import keras_export\n from keras.backend import KerasTensor\n from keras.backend import any_symbolic_tensors\n from keras.ops.operation import Operation\n from keras.utils import traceback_utils\n-from keras.utils import tree\n \n \n class Scatter(Operation):\n\n@@ -12,10 +12,10 @@ from keras import models\n from keras import ops\n from keras import optimizers\n from keras import testing\n+from keras import tree\n from keras.backend.common import dtypes\n from keras.backend.common.keras_tensor import KerasTensor\n from keras.ops import core\n-from keras.utils import tree\n \n \n class CoreOpsStaticShapeTest(testing.TestCase):\n\n@@ -1,10 +1,10 @@\n import collections\n \n+from keras import tree\n from keras.api_export import keras_export\n from keras.backend import KerasTensor\n from keras.backend.config import backend\n from keras.ops.operation import Operation\n-from keras.utils import tree\n \n \n @keras_export(\"keras.Function\")\n\n@@ -1,8 +1,8 @@\n import collections\n \n+from keras import tree\n from keras.backend import KerasTensor\n from keras.ops.symbolic_arguments import SymbolicArguments\n-from keras.utils import tree\n \n \n class Node:\n\n@@ -3,12 +3,12 @@ import textwrap\n \n from keras import backend\n from keras import dtype_policies\n+from keras import tree\n from keras.api_export import keras_export\n from keras.backend.common.keras_tensor import any_symbolic_tensors\n from keras.ops.node import Node\n from keras.utils import python_utils\n from keras.utils import traceback_utils\n-from keras.utils import tree\n from keras.utils.naming import auto_name\n \n \n\n@@ -2,10 +2,10 @@ import math\n \n import numpy as np\n \n+from keras import tree\n from keras.api_export import keras_export\n from keras.backend.common.backend_utils import canonicalize_axis\n from keras.backend.common.backend_utils import to_tuple_or_list\n-from keras.utils import tree\n \n \n def broadcast_shapes(shape1, shape2):\n\n@@ -1,5 +1,5 @@\n+from keras import tree\n from keras.backend import KerasTensor\n-from keras.utils import tree\n \n \n class SymbolicArguments:\n\n@@ -1,7 +1,7 @@\n from keras import testing\n+from keras import tree\n from keras.backend import KerasTensor\n from keras.ops.symbolic_arguments import SymbolicArguments\n-from keras.utils import tree\n \n \n class SymbolicArgumentsTest(testing.TestCase):\n\n@@ -8,6 +8,7 @@ import numpy as np\n from keras import backend\n from keras import distribution\n from keras import ops\n+from keras import tree\n from keras import utils\n from keras.backend.common import is_float_dtype\n from keras.backend.common import standardize_dtype\n@@ -15,7 +16,6 @@ from keras.backend.common.global_state import clear_session\n from keras.backend.common.keras_tensor import KerasTensor\n from keras.models import Model\n from keras.utils import traceback_utils\n-from keras.utils import tree\n \n \n class TestCase(unittest.TestCase):\n\n@@ -2,7 +2,7 @@ from keras import backend\n from keras import losses as losses_module\n from keras import metrics as metrics_module\n from keras import ops\n-from keras.utils import tree\n+from keras import tree\n from keras.utils.naming import get_object_name\n \n \n\n@@ -3,10 +3,10 @@ import math\n \n import numpy as np\n \n+from keras import tree\n from keras.trainers.data_adapters import array_slicing\n from keras.trainers.data_adapters import data_adapter_utils\n from keras.trainers.data_adapters.data_adapter import DataAdapter\n-from keras.utils import tree\n \n \n class ArrayDataAdapter(DataAdapter):\n\n@@ -4,8 +4,8 @@ import math\n import numpy as np\n \n from keras import backend\n+from keras import tree\n from keras.trainers.data_adapters import data_adapter_utils\n-from keras.utils import tree\n \n try:\n     import pandas\n\n@@ -1,8 +1,8 @@\n import numpy as np\n \n from keras import backend\n+from keras import tree\n from keras.api_export import keras_export\n-from keras.utils import tree\n \n NUM_BATCHES_FOR_TENSOR_SPEC = 2\n \n\n@@ -1,8 +1,8 @@\n import itertools\n \n+from keras import tree\n from keras.trainers.data_adapters import data_adapter_utils\n from keras.trainers.data_adapters.data_adapter import DataAdapter\n-from keras.utils import tree\n \n \n class GeneratorDataAdapter(DataAdapter):\n\n@@ -1,6 +1,6 @@\n+from keras import tree\n from keras.trainers.data_adapters import data_adapter_utils\n from keras.trainers.data_adapters.data_adapter import DataAdapter\n-from keras.utils import tree\n \n \n class TFDatasetAdapter(DataAdapter):\n\n@@ -2,9 +2,9 @@ import itertools\n \n import numpy as np\n \n+from keras import tree\n from keras.trainers.data_adapters import data_adapter_utils\n from keras.trainers.data_adapters.data_adapter import DataAdapter\n-from keras.utils import tree\n \n \n class TorchDataLoaderAdapter(DataAdapter):\n\n@@ -5,6 +5,7 @@ from keras import backend\n from keras import metrics as metrics_module\n from keras import ops\n from keras import optimizers\n+from keras import tree\n from keras.optimizers.loss_scale_optimizer import LossScaleOptimizer\n from keras.saving import serialization_lib\n from keras.trainers.compile_utils import CompileLoss\n@@ -12,7 +13,6 @@ from keras.trainers.compile_utils import CompileMetrics\n from keras.trainers.data_adapters import data_adapter_utils\n from keras.utils import traceback_utils\n from keras.utils import tracking\n-from keras.utils import tree\n \n \n class Trainer:\n\n@@ -0,0 +1,10 @@\n+from keras.tree.tree_api import assert_same_structure\n+from keras.tree.tree_api import flatten\n+from keras.tree.tree_api import is_nested\n+from keras.tree.tree_api import lists_to_tuples\n+from keras.tree.tree_api import map_shape_structure\n+from keras.tree.tree_api import map_structure\n+from keras.tree.tree_api import map_structure_up_to\n+from keras.tree.tree_api import pack_sequence_as\n+from keras.tree.tree_api import register_tree_node_class\n+from keras.tree.tree_api import traverse\n\n@@ -0,0 +1,153 @@\n+from keras.utils.module_utils import dmtree\n+\n+\n+def register_tree_node_class(cls):\n+    return cls\n+\n+\n+def is_nested(structure):\n+    dmtree.is_nested(structure)\n+\n+\n+def traverse(func, structure, top_down=True):\n+    return dmtree.traverse(func, structure, top_down=top_down)\n+\n+\n+def flatten(structure):\n+    return dmtree.flatten(structure)\n+\n+\n+def map_structure(func, *structures):\n+    return dmtree.map_structure(func, *structures)\n+\n+\n+def map_structure_up_to(shallow_structure, func, *structures):\n+    return dmtree.map_structure_up_to(shallow_structure, func, *structures)\n+\n+\n+def assert_same_structure(a, b, check_types=True):\n+    return dmtree.assert_same_structure(a, b, check_types=check_types)\n+\n+\n+def pack_sequence_as(structure, flat_sequence, sequence_fn=None):\n+    is_nested_fn = dmtree.is_nested\n+    sequence_fn = sequence_fn or dmtree._sequence_like\n+\n+    def truncate(value, length):\n+        value_str = str(value)\n+        return value_str[:length] + (value_str[length:] and \"...\")\n+\n+    if not is_nested_fn(flat_sequence):\n+        raise TypeError(\n+            \"Attempted to pack value:\\n  {}\\ninto a structure, but found \"\n+            \"incompatible type `{}` instead.\".format(\n+                truncate(flat_sequence, 100), type(flat_sequence)\n+            )\n+        )\n+\n+    if not is_nested_fn(structure):\n+        if len(flat_sequence) != 1:\n+            raise ValueError(\n+                \"The target structure is of type `{}`\\n  {}\\nHowever the input \"\n+                \"is a sequence ({}) of length {}.\\n  {}\\nnest cannot \"\n+                \"guarantee that it is safe to map one to the other.\".format(\n+                    type(structure),\n+                    truncate(structure, 100),\n+                    type(flat_sequence),\n+                    len(flat_sequence),\n+                    truncate(flat_sequence, 100),\n+                )\n+            )\n+        return flat_sequence[0]\n+\n+    try:\n+        final_index, packed = packed_nest_with_indices(\n+            structure, flat_sequence, 0, is_nested_fn, sequence_fn\n+        )\n+        if final_index < len(flat_sequence):\n+            raise IndexError\n+    except IndexError:\n+        flat_structure = dmtree.flatten(structure)\n+        if len(flat_structure) != len(flat_sequence):\n+            # pylint: disable=raise-missing-from\n+            raise ValueError(\n+                \"Could not pack sequence. \"\n+                f\"Structure had {len(flat_structure)} atoms, but \"\n+                f\"flat_sequence had {len(flat_sequence)} items. \"\n+                f\"Structure: {structure}, flat_sequence: {flat_sequence}.\"\n+            )\n+    return sequence_fn(structure, packed)\n+\n+\n+def packed_nest_with_indices(\n+    structure, flat, index, is_nested_fn, sequence_fn=None\n+):\n+    \"\"\"Helper function for pack_sequence_as.\n+\n+    Args:\n+        structure: structure to mimic.\n+        flat: Flattened values to output substructure for.\n+        index: Index at which to start reading from flat.\n+        is_nested_fn: Function used to test if a value should\n+            be treated as a nested structure.\n+        sequence_fn: Function used to generate a new structure instance.\n+\n+    Returns:\n+        The tuple (new_index, child), where:\n+        * new_index - the updated index into `flat`\n+            having processed `structure`.\n+        * packed - the subset of `flat` corresponding to `structure`,\n+            having started at `index`, and packed into the same nested\n+            format.\n+    \"\"\"\n+    packed = []\n+    sequence_fn = sequence_fn or dmtree._sequence_like\n+    for s in yield_value(structure):\n+        if is_nested_fn(s):\n+            new_index, child = packed_nest_with_indices(\n+                s, flat, index, is_nested_fn, sequence_fn\n+            )\n+            packed.append(sequence_fn(s, child))\n+            index = new_index\n+        else:\n+            packed.append(flat[index])\n+            index += 1\n+    return index, packed\n+\n+\n+def yield_value(iterable):\n+    for _, v in dmtree._yield_sorted_items(iterable):\n+        yield v\n+\n+\n+def lists_to_tuples(structure):\n+    def sequence_fn(instance, args):\n+        if isinstance(instance, list):\n+            return tuple(args)\n+        return dmtree._sequence_like(instance, args)\n+\n+    return pack_sequence_as(\n+        structure,\n+        dmtree.flatten(structure),\n+        sequence_fn=sequence_fn,\n+    )\n+\n+\n+def is_shape_tuple(x):\n+    if isinstance(x, (list, tuple)):\n+        if all(isinstance(e, (int, type(None))) for e in x):\n+            return True\n+    return False\n+\n+\n+def map_shape_structure(func, structure):\n+    if is_shape_tuple(structure):\n+        return func(tuple(structure))\n+    if isinstance(structure, list):\n+        return [map_shape_structure(func, e) for e in structure]\n+    if isinstance(structure, tuple):\n+        return tuple(map_shape_structure(func, e) for e in structure)\n+    if isinstance(structure, dict):\n+        return {k: map_shape_structure(func, v) for k, v in structure.items()}\n+    else:\n+        raise ValueError(f\"Cannot map function to unknown object {structure}\")\n\n@@ -3,10 +3,15 @@ import collections.abc\n import types\n \n import optree\n+import optree.utils\n \n-from keras.api_export import keras_export\n from keras.backend.config import backend\n \n+\n+def register_tree_node_class(cls):\n+    return optree.register_pytree_node_class(cls, namespace=\"keras\")\n+\n+\n # Register backend-specific node classes\n if backend() == \"tensorflow\":\n     from tensorflow.python.trackable.data_structures import ListWrapper\n@@ -19,77 +24,13 @@ if backend() == \"tensorflow\":\n     )\n \n \n-@keras_export(\"keras.tree.is_nested\")\n def is_nested(structure):\n-    \"\"\"Checks if a given structure is nested.\n-\n-    Examples:\n-\n-    >>> keras.tree.is_nested(42)\n-    False\n-    >>> keras.tree.is_nested({\"foo\": 42})\n-    True\n-\n-    Args:\n-        structure: A structure to check.\n-\n-    Returns:\n-        `True` if a given structure is nested, i.e. is a sequence, a mapping,\n-        or a namedtuple, and `False` otherwise.\n-    \"\"\"\n     return not optree.tree_is_leaf(\n         structure, none_is_leaf=True, namespace=\"keras\"\n     )\n \n \n-@keras_export(\"keras.tree.traverse\")\n def traverse(func, structure, top_down=True):\n-    \"\"\"Traverses the given nested structure, applying the given function.\n-\n-    The traversal is depth-first. If `top_down` is True (default), parents\n-    are returned before their children (giving the option to avoid traversing\n-    into a sub-tree).\n-\n-    Examples:\n-\n-    >>> v = []\n-    >>> keras.tree.traverse(v.append, [(1, 2), [3], {\"a\": 4}], top_down=True)\n-    [(1, 2), [3], {'a': 4}]\n-    >>> v\n-    [[(1, 2), [3], {'a': 4}], (1, 2), 1, 2, [3], 3, {'a': 4}, 4]\n-\n-    >>> v = []\n-    >>> keras.tree.traverse(v.append, [(1, 2), [3], {\"a\": 4}], top_down=False)\n-    [(1, 2), [3], {'a': 4}]\n-    >>> v\n-    [1, 2, (1, 2), 3, [3], 4, {'a': 4}, [(1, 2), [3], {'a': 4}]]\n-\n-    Args:\n-        func: The function to be applied to each sub-nest of the structure.\n-\n-        When traversing top-down:\n-            If `func(subtree) is None` the traversal continues into the\n-            sub-tree.\n-            If `func(subtree) is not None` the traversal does not continue\n-            into the sub-tree. The sub-tree will be replaced by `func(subtree)`\n-            in the returned structure (to replace the sub-tree with `None`, use\n-            the special value `_MAP_TO_NONE`).\n-\n-        When traversing bottom-up:\n-            If `func(subtree) is None` the traversed sub-tree is returned\n-            unaltered.\n-            If `func(subtree) is not None` the sub-tree will be replaced by\n-            `func(subtree)` in the returned structure (to replace the sub-tree\n-            with None, use the special value `_MAP_TO_NONE`).\n-\n-        structure: The structure to traverse.\n-        top_down: If True, parent structures will be visited before their\n-            children.\n-\n-    Returns:\n-        The structured output from the traversal.\n-    \"\"\"\n-\n     # From https://github.com/google/jax/pull/19695\n     def traverse_children():\n         children, treedef = optree.tree_flatten(\n@@ -118,36 +59,7 @@ def traverse(func, structure, top_down=True):\n     return None if ret is _MAP_TO_NONE else ret\n \n \n-@keras_export(\"keras.tree.flatten\")\n def flatten(structure):\n-    \"\"\"Flattens a possibly nested structure into a list.\n-\n-    In the case of dict instances, the sequence consists of the values,\n-    sorted by key to ensure deterministic behavior. This is true also for\n-    `collections.OrderedDict` instances: their sequence order is\n-    considered. The same convention is followed in `unflatten_as`.\n-    This correctly unflattens dicts and `OrderedDict` after they have been\n-    flattened, or vice-versa.\n-\n-    Dictionaries with non-sortable keys cannot be flattened.\n-\n-    Examples:\n-\n-    >>> keras.tree.flatten([[1, 2, 3], [4, [5], [[6]]]])\n-    [1, 2, 3, 4, 5, 6]\n-    >>> keras.tree.flatten(None)\n-    [None]\n-    >>> keras.tree.flatten(1)\n-    [1]\n-    >>> keras.tree.flatten({100: 'world!', 6: 'Hello'})\n-    ['Hello', 'world!']\n-\n-    Args:\n-        structure: An arbitrarily nested structure.\n-\n-    Returns:\n-        A list, the flattened version of the input `structure`.\n-    \"\"\"\n     # optree.tree_flatten returns a pair (leaves, treespec) where the first\n     # element is a list of leaf values and the second element is a treespec\n     # representing the structure of the pytree.\n@@ -157,79 +69,7 @@ def flatten(structure):\n     return leaves\n \n \n-@keras_export(\"keras.tree.unflatten_as\")\n-def unflatten_as(structure, flat_sequence):\n-    \"\"\"Unflattens a sequence into a given structure.\n-\n-    If `structure` is a scalar, `flat_sequence` must be a single-element list;\n-    in this case the return value is ``flat_sequence[0]``.\n-\n-    If `structure` is or contains a dict instance, the keys will be sorted to\n-    pack the flat sequence in deterministic order. This is true also for\n-    `collections.OrderedDict` instances: their sequence order is considered.\n-    The same convention is followed in `flatten`. This correctly unflattens\n-    dicts and `OrderedDict` after they have been flattened, or vice-versa.\n-\n-    Dictionaries with non-sortable keys cannot be unflattened.\n-\n-    Examples:\n-\n-    >>> keras.tree.unflatten_as([[1, 2], [[3], [4]]], [5, 6, 7, 8])\n-    [[5, 6], [[7], [8]]]\n-    >>> keras.tree.unflatten_as(None, [1])\n-    1\n-    >>> keras.tree.unflatten_as({1: None, 2: None}, ['Hello', 'world!'])\n-    {1: 'Hello', 2: 'world!'}\n-\n-    Args:\n-        structure: Arbitrarily nested structure.\n-        flat_sequence: Sequence to unflatten.\n-\n-    Returns:\n-        `flat_sequence` unflattened into `structure`.\n-    \"\"\"\n-    if not is_nested(flat_sequence):\n-        raise TypeError(\n-            f\"flat_sequence must be a sequence not a {type(flat_sequence)}:\\n\"\n-            f\"{flat_sequence}\"\n-        )\n-    if not is_nested(structure):\n-        if len(flat_sequence) != 1:\n-            raise ValueError(\n-                \"Structure is a scalar but \"\n-                f\"len(flat_sequence) == {len(flat_sequence)} > 1\"\n-            )\n-        return flat_sequence[0]\n-    structure_spec = optree.tree_structure(\n-        structure, none_is_leaf=True, namespace=\"keras\"\n-    )\n-    return structure_spec.unflatten(flat_sequence)\n-\n-\n-@keras_export(\"keras.tree.map_structure\")\n def map_structure(func, *structures):\n-    \"\"\"Maps `func` through given structures.\n-\n-    Examples:\n-\n-    >>> structure = [[1], [2], [3]]\n-    >>> keras.tree.map_structure(lambda v: v**2, structure)\n-    [[1], [4], [9]]\n-    >>> keras.tree.map_structure(lambda x, y: x * y, structure, structure)\n-    [[1], [4], [9]]\n-\n-    >>> Foo = collections.namedtuple('Foo', ['a', 'b'])\n-    >>> structure = Foo(a=1, b=2)\n-    >>> keras.tree.map_structure(lambda v: v * 2, structure)\n-    Foo(a=2, b=4)\n-\n-    Args:\n-        func: A callable that accepts as many arguments as there are structures.\n-        *structures: Arbitrarily nested structures of the same layout.\n-\n-    Returns:\n-        A new structure with the same layout as the given ones.\n-    \"\"\"\n     if not callable(func):\n         raise TypeError(f\"`func` must be callable. Received: func={func}\")\n     if not structures:\n@@ -241,32 +81,7 @@ def map_structure(func, *structures):\n     )\n \n \n-@keras_export(\"keras.tree.map_structure_up_to\")\n def map_structure_up_to(shallow_structure, func, *structures):\n-    \"\"\"Maps `func` through given structures up to `shallow_structure`.\n-\n-    This is a variant of `map_structure` which only maps the given structures\n-    up to `shallow_structure`. All further nested components are retained as-is.\n-\n-    Examples:\n-\n-    >>> shallow_structure = [None, None]\n-    >>> structure = [[1, 1], [2, 2]]\n-    >>> keras.tree.map_structure_up_to(shallow_structure, len, structure)\n-    [2, 2]\n-\n-    >>> shallow_structure = [None, [None, None]]\n-    >>> keras.tree.map_structure_up_to(shallow_structure, str, structure)\n-    ['[1, 1]', ['2', '2']]\n-\n-    Args:\n-        shallow_structure: A structure with layout common to all `structures`.\n-        func: A callable that accepts as many arguments as there are structures.\n-        *structures: Arbitrarily nested structures of the same layout.\n-\n-    Returns:\n-        A new structure with the same layout as `shallow_structure`.\n-    \"\"\"\n     return _map_structure_with_path_up_to(\n         shallow_structure,\n         lambda _, *args: func(*args),  # Discards path.\n@@ -274,31 +89,7 @@ def map_structure_up_to(shallow_structure, func, *structures):\n     )\n \n \n-@keras_export(\"keras.tree.assert_same_structure\")\n def assert_same_structure(a, b, check_types=True):\n-    \"\"\"Asserts that two structures are nested in the same way.\n-\n-    Note that namedtuples with identical name and fields will not be considered\n-    as same structures even `check_types=False`.\n-\n-    Examples:\n-\n-    >>> keras.tree.assert_same_structure([(0, 1)], [(2, 3)])\n-\n-    >>> Foo = collections.namedtuple('Foo', ['a', 'b'])\n-    >>> AlsoFoo = collections.namedtuple('Foo', ['a', 'b'])\n-    >>> keras.tree.assert_same_structure(Foo(0, 1), Foo(2, 3))\n-    >>> keras.tree.assert_same_structure(Foo(0, 1), AlsoFoo(2, 3))\n-    Traceback (most recent call last):\n-        ...\n-    ValueError: `a` and `b` don't have the same structure.\n-    ...\n-\n-    Args:\n-        a: an arbitrarily nested structure.\n-        b: an arbitrarily nested structure.\n-        check_types: if `True` (default) types of leaves are checked as well.\n-    \"\"\"\n     a_structure = optree.tree_structure(a, none_is_leaf=True, namespace=\"keras\")\n     b_structure = optree.tree_structure(b, none_is_leaf=True, namespace=\"keras\")\n     if a_structure != b_structure:\n@@ -323,60 +114,7 @@ def assert_same_structure(a, b, check_types=True):\n             )\n \n \n-@keras_export(\"keras.tree.pack_sequence_as\")\n def pack_sequence_as(structure, flat_sequence, sequence_fn=None):\n-    \"\"\"Returns a given flattened sequence packed into a given structure.\n-\n-    If `structure` is an atom, `flat_sequence` must be a single-item list; in\n-    this case the return value is `flat_sequence[0]`.\n-\n-    If `structure` is or contains a dict instance, the keys will be sorted to\n-    pack the flat sequence in deterministic order. This is true also for\n-    `OrderedDict` instances: their sequence order is considered. The same\n-    convention is followed in `flatten`. This correctly repacks dicts and\n-    `OrderedDicts` after they have been flattened, or vice-versa.\n-\n-    Dictionaries with non-sortable keys cannot be flattened.\n-\n-    Examples:\n-\n-    >>> structure = {\"key3\": \"\", \"key1\": \"\", \"key2\": \"\"}\n-    >>> flat_sequence = [\"value1\", \"value2\", \"value3\"]\n-    >>> keras.tree.pack_sequence_as(structure, flat_sequence)\n-    {\"key3\": \"value3\", \"key1\": \"value1\", \"key2\": \"value2\"}\n-\n-    >>> structure = ((\"a\", \"b\"), (\"c\", \"d\", \"e\"), \"f\")\n-    >>> flat_sequence = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]\n-    >>> keras.tree.pack_sequence_as(structure, flat_sequence)\n-    ((1.0, 2.0), (3.0, 4.0, 5.0), 6.0)\n-\n-    >>> structure = {\"key3\": {\"c\": (\"alpha\", \"beta\"), \"a\": (\"gamma\")},\n-    ... \"key1\": {\"e\": \"val1\", \"d\": \"val2\"}}\n-    >>> flat_sequence = [\"val2\", \"val1\", 3.0, 1.0, 2.0]\n-    >>> keras.tree.pack_sequence_as(structure, flat_sequence)\n-    {'key3': {'c': (1.0, 2.0), 'a': 3.0}, 'key1': {'e': 'val1', 'd': 'val2'}}\n-\n-    >>> structure = [\"a\"]\n-    >>> flat_sequence = [np.array([[1, 2], [3, 4]])]\n-    >>> keras.tree.pack_sequence_as(structure, flat_sequence)\n-    [array([[1, 2],\n-       [3, 4]])]\n-\n-    >>> structure = [\"a\"]\n-    >>> flat_sequence = [keras.ops.ones([2, 2])]\n-    >>> keras.tree.pack_sequence_as(structure, flat_sequence)\n-    [array([[1., 1.],\n-       [1., 1.]]]\n-\n-    Args:\n-        structure: Arbitrarily nested structure.\n-        flat_sequence: Flat sequence to pack.\n-        sequence_fn: Defaults to `_sequence_like`.\n-\n-    Returns:\n-        `flat_sequence` converted to have the same recursive structure as\n-        `structure`.\n-    \"\"\"\n     sequence_fn = sequence_fn or _sequence_like\n \n     def truncate(value, length):\n@@ -425,9 +163,7 @@ def pack_sequence_as(structure, flat_sequence, sequence_fn=None):\n     return sequence_fn(structure, packed)\n \n \n-@keras_export(\"keras.tree.lists_to_tuples\")\n def lists_to_tuples(structure):\n-    \"\"\"Converts `list`s to `tuple`s.\"\"\"\n \n     def sequence_fn(instance, args):\n         if isinstance(instance, list):\n@@ -440,7 +176,6 @@ def lists_to_tuples(structure):\n \n \n def map_shape_structure(func, structure):\n-    \"\"\"Variant of tree.map_structure that operates on shape tuples.\"\"\"\n \n     def is_shape_tuple(x):\n         return isinstance(x, (list, tuple)) and all(\n\n@@ -0,0 +1,275 @@\n+from keras.api_export import keras_export\n+from keras.utils.module_utils import dmtree\n+from keras.utils.module_utils import optree\n+\n+if optree.available:\n+    from keras.tree import optree_impl as tree_impl\n+elif dmtree.available:\n+    from keras.tree import dmtree_impl as tree_impl\n+else:\n+    raise ImportError(\n+        \"To use Keras, you need to have `optree` installed. \"\n+        \"Install it via `pip install optree`\"\n+    )\n+\n+\n+def register_tree_node_class(cls):\n+    return tree_impl.register_tree_node_class(cls)\n+\n+\n+@keras_export(\"keras.tree.is_nested\")\n+def is_nested(structure):\n+    \"\"\"Checks if a given structure is nested.\n+\n+    Examples:\n+\n+    >>> keras.tree.is_nested(42)\n+    False\n+    >>> keras.tree.is_nested({\"foo\": 42})\n+    True\n+\n+    Args:\n+        structure: A structure to check.\n+\n+    Returns:\n+        `True` if a given structure is nested, i.e. is a sequence, a mapping,\n+        or a namedtuple, and `False` otherwise.\n+    \"\"\"\n+    return tree_impl.is_nested(structure)\n+\n+\n+@keras_export(\"keras.tree.traverse\")\n+def traverse(func, structure, top_down=True):\n+    \"\"\"Traverses the given nested structure, applying the given function.\n+\n+    The traversal is depth-first. If `top_down` is True (default), parents\n+    are returned before their children (giving the option to avoid traversing\n+    into a sub-tree).\n+\n+    Examples:\n+\n+    >>> v = []\n+    >>> keras.tree.traverse(v.append, [(1, 2), [3], {\"a\": 4}], top_down=True)\n+    [(1, 2), [3], {'a': 4}]\n+    >>> v\n+    [[(1, 2), [3], {'a': 4}], (1, 2), 1, 2, [3], 3, {'a': 4}, 4]\n+\n+    >>> v = []\n+    >>> keras.tree.traverse(v.append, [(1, 2), [3], {\"a\": 4}], top_down=False)\n+    [(1, 2), [3], {'a': 4}]\n+    >>> v\n+    [1, 2, (1, 2), 3, [3], 4, {'a': 4}, [(1, 2), [3], {'a': 4}]]\n+\n+    Args:\n+        func: The function to be applied to each sub-nest of the structure.\n+\n+        When traversing top-down:\n+            If `func(subtree) is None` the traversal continues into the\n+            sub-tree.\n+            If `func(subtree) is not None` the traversal does not continue\n+            into the sub-tree. The sub-tree will be replaced by `func(subtree)`\n+            in the returned structure (to replace the sub-tree with `None`, use\n+            the special value `_MAP_TO_NONE`).\n+\n+        When traversing bottom-up:\n+            If `func(subtree) is None` the traversed sub-tree is returned\n+            unaltered.\n+            If `func(subtree) is not None` the sub-tree will be replaced by\n+            `func(subtree)` in the returned structure (to replace the sub-tree\n+            with None, use the special value `_MAP_TO_NONE`).\n+\n+        structure: The structure to traverse.\n+        top_down: If True, parent structures will be visited before their\n+            children.\n+\n+    Returns:\n+        The structured output from the traversal.\n+    \"\"\"\n+    return tree_impl.traverse(func, structure, top_down=top_down)\n+\n+\n+@keras_export(\"keras.tree.flatten\")\n+def flatten(structure):\n+    \"\"\"Flattens a possibly nested structure into a list.\n+\n+    In the case of dict instances, the sequence consists of the values,\n+    sorted by key to ensure deterministic behavior. This is true also for\n+    `collections.OrderedDict` instances: their sequence order is\n+    considered. The same convention is followed in `unflatten_as`.\n+    This correctly unflattens dicts and `OrderedDict` after they have been\n+    flattened, or vice-versa.\n+\n+    Dictionaries with non-sortable keys cannot be flattened.\n+\n+    Examples:\n+\n+    >>> keras.tree.flatten([[1, 2, 3], [4, [5], [[6]]]])\n+    [1, 2, 3, 4, 5, 6]\n+    >>> keras.tree.flatten(None)\n+    [None]\n+    >>> keras.tree.flatten(1)\n+    [1]\n+    >>> keras.tree.flatten({100: 'world!', 6: 'Hello'})\n+    ['Hello', 'world!']\n+\n+    Args:\n+        structure: An arbitrarily nested structure.\n+\n+    Returns:\n+        A list, the flattened version of the input `structure`.\n+    \"\"\"\n+    return tree_impl.flatten(structure)\n+\n+\n+@keras_export(\"keras.tree.map_structure\")\n+def map_structure(func, *structures):\n+    \"\"\"Maps `func` through given structures.\n+\n+    Examples:\n+\n+    >>> structure = [[1], [2], [3]]\n+    >>> keras.tree.map_structure(lambda v: v**2, structure)\n+    [[1], [4], [9]]\n+    >>> keras.tree.map_structure(lambda x, y: x * y, structure, structure)\n+    [[1], [4], [9]]\n+\n+    >>> Foo = collections.namedtuple('Foo', ['a', 'b'])\n+    >>> structure = Foo(a=1, b=2)\n+    >>> keras.tree.map_structure(lambda v: v * 2, structure)\n+    Foo(a=2, b=4)\n+\n+    Args:\n+        func: A callable that accepts as many arguments as there are structures.\n+        *structures: Arbitrarily nested structures of the same layout.\n+\n+    Returns:\n+        A new structure with the same layout as the given ones.\n+    \"\"\"\n+    return tree_impl.map_structure(func, *structures)\n+\n+\n+@keras_export(\"keras.tree.map_structure_up_to\")\n+def map_structure_up_to(shallow_structure, func, *structures):\n+    \"\"\"Maps `func` through given structures up to `shallow_structure`.\n+\n+    This is a variant of `map_structure` which only maps the given structures\n+    up to `shallow_structure`. All further nested components are retained as-is.\n+\n+    Examples:\n+\n+    >>> shallow_structure = [None, None]\n+    >>> structure = [[1, 1], [2, 2]]\n+    >>> keras.tree.map_structure_up_to(shallow_structure, len, structure)\n+    [2, 2]\n+\n+    >>> shallow_structure = [None, [None, None]]\n+    >>> keras.tree.map_structure_up_to(shallow_structure, str, structure)\n+    ['[1, 1]', ['2', '2']]\n+\n+    Args:\n+        shallow_structure: A structure with layout common to all `structures`.\n+        func: A callable that accepts as many arguments as there are structures.\n+        *structures: Arbitrarily nested structures of the same layout.\n+\n+    Returns:\n+        A new structure with the same layout as `shallow_structure`.\n+    \"\"\"\n+    return tree_impl.map_structure_up_to(shallow_structure, func, *structures)\n+\n+\n+@keras_export(\"keras.tree.assert_same_structure\")\n+def assert_same_structure(a, b, check_types=True):\n+    \"\"\"Asserts that two structures are nested in the same way.\n+\n+    Note that namedtuples with identical name and fields will not be considered\n+    as same structures even `check_types=False`.\n+\n+    Examples:\n+\n+    >>> keras.tree.assert_same_structure([(0, 1)], [(2, 3)])\n+\n+    >>> Foo = collections.namedtuple('Foo', ['a', 'b'])\n+    >>> AlsoFoo = collections.namedtuple('Foo', ['a', 'b'])\n+    >>> keras.tree.assert_same_structure(Foo(0, 1), Foo(2, 3))\n+    >>> keras.tree.assert_same_structure(Foo(0, 1), AlsoFoo(2, 3))\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: `a` and `b` don't have the same structure.\n+    ...\n+\n+    Args:\n+        a: an arbitrarily nested structure.\n+        b: an arbitrarily nested structure.\n+        check_types: if `True` (default) types of leaves are checked as well.\n+    \"\"\"\n+    return tree_impl.assert_same_structure(a, b, check_types=check_types)\n+\n+\n+@keras_export(\"keras.tree.pack_sequence_as\")\n+def pack_sequence_as(structure, flat_sequence, sequence_fn=None):\n+    \"\"\"Returns a given flattened sequence packed into a given structure.\n+\n+    If `structure` is an atom, `flat_sequence` must be a single-item list; in\n+    this case the return value is `flat_sequence[0]`.\n+\n+    If `structure` is or contains a dict instance, the keys will be sorted to\n+    pack the flat sequence in deterministic order. This is true also for\n+    `OrderedDict` instances: their sequence order is considered. The same\n+    convention is followed in `flatten`. This correctly repacks dicts and\n+    `OrderedDicts` after they have been flattened, or vice-versa.\n+\n+    Dictionaries with non-sortable keys cannot be flattened.\n+\n+    Examples:\n+\n+    >>> structure = {\"key3\": \"\", \"key1\": \"\", \"key2\": \"\"}\n+    >>> flat_sequence = [\"value1\", \"value2\", \"value3\"]\n+    >>> keras.tree.pack_sequence_as(structure, flat_sequence)\n+    {\"key3\": \"value3\", \"key1\": \"value1\", \"key2\": \"value2\"}\n+\n+    >>> structure = ((\"a\", \"b\"), (\"c\", \"d\", \"e\"), \"f\")\n+    >>> flat_sequence = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]\n+    >>> keras.tree.pack_sequence_as(structure, flat_sequence)\n+    ((1.0, 2.0), (3.0, 4.0, 5.0), 6.0)\n+\n+    >>> structure = {\"key3\": {\"c\": (\"alpha\", \"beta\"), \"a\": (\"gamma\")},\n+    ... \"key1\": {\"e\": \"val1\", \"d\": \"val2\"}}\n+    >>> flat_sequence = [\"val2\", \"val1\", 3.0, 1.0, 2.0]\n+    >>> keras.tree.pack_sequence_as(structure, flat_sequence)\n+    {'key3': {'c': (1.0, 2.0), 'a': 3.0}, 'key1': {'e': 'val1', 'd': 'val2'}}\n+\n+    >>> structure = [\"a\"]\n+    >>> flat_sequence = [np.array([[1, 2], [3, 4]])]\n+    >>> keras.tree.pack_sequence_as(structure, flat_sequence)\n+    [array([[1, 2],\n+       [3, 4]])]\n+\n+    >>> structure = [\"a\"]\n+    >>> flat_sequence = [keras.ops.ones([2, 2])]\n+    >>> keras.tree.pack_sequence_as(structure, flat_sequence)\n+    [array([[1., 1.],\n+       [1., 1.]]]\n+\n+    Args:\n+        structure: Arbitrarily nested structure.\n+        flat_sequence: Flat sequence to pack.\n+        sequence_fn: Defaults to `_sequence_like`.\n+\n+    Returns:\n+        `flat_sequence` converted to have the same recursive structure as\n+        `structure`.\n+    \"\"\"\n+    return tree_impl.pack_sequence_as(\n+        structure, flat_sequence, sequence_fn=sequence_fn\n+    )\n+\n+\n+@keras_export(\"keras.tree.lists_to_tuples\")\n+def lists_to_tuples(structure):\n+    return tree_impl.lists_to_tuples(structure)\n+\n+\n+@keras_export(\"keras.tree.map_shape_structure\")\n+def map_shape_structure(func, structure):\n+    \"\"\"Variant of keras.tree.map_structure that operates on shape tuples.\"\"\"\n+    return tree_impl.map_shape_structure(func, structure)\n\n@@ -4,7 +4,7 @@ import numpy as np\n \n from keras import ops\n from keras import testing\n-from keras.utils import tree\n+from keras import tree\n \n STRUCTURE1 = (((1, 2), 3), 4, (5, 6))\n STRUCTURE2 = (((\"foo1\", \"foo2\"), \"foo3\"), \"foo4\", (\"foo5\", \"foo6\"))\n@@ -27,46 +27,19 @@ class TreeTest(testing.TestCase):\n         self.assertFalse(tree.is_nested(np.tanh(ones)))\n         self.assertFalse(tree.is_nested(np.ones((4, 5))))\n \n-    def test_flatten_and_unflatten(self):\n+    def test_flatten(self):\n         structure = ((3, 4), 5, (6, 7, (9, 10), 8))\n         flat = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"]\n \n         self.assertEqual(tree.flatten(structure), [3, 4, 5, 6, 7, 9, 10, 8])\n-        self.assertEqual(\n-            tree.unflatten_as(structure, flat),\n-            ((\"a\", \"b\"), \"c\", (\"d\", \"e\", (\"f\", \"g\"), \"h\")),\n-        )\n         point = collections.namedtuple(\"Point\", [\"x\", \"y\"])\n         structure = (point(x=4, y=2), ((point(x=1, y=0),),))\n         flat = [4, 2, 1, 0]\n         self.assertEqual(tree.flatten(structure), flat)\n-        restructured_from_flat = tree.unflatten_as(structure, flat)\n-        self.assertEqual(restructured_from_flat, structure)\n-        self.assertEqual(restructured_from_flat[0].x, 4)\n-        self.assertEqual(restructured_from_flat[0].y, 2)\n-        self.assertEqual(restructured_from_flat[1][0][0].x, 1)\n-        self.assertEqual(restructured_from_flat[1][0][0].y, 0)\n \n         self.assertEqual([5], tree.flatten(5))\n         self.assertEqual([np.array([5])], tree.flatten(np.array([5])))\n \n-        self.assertEqual(\"a\", tree.unflatten_as(5, [\"a\"]))\n-        self.assertEqual(\n-            np.array([5]), tree.unflatten_as(\"scalar\", [np.array([5])])\n-        )\n-\n-        with self.assertRaisesRegex(ValueError, \"Structure is a scalar\"):\n-            tree.unflatten_as(\"scalar\", [4, 5])\n-        with self.assertRaisesRegex(TypeError, \"flat_sequence\"):\n-            tree.unflatten_as([4, 5], \"bad_sequence\")\n-        with self.assertRaises(ValueError):\n-            tree.unflatten_as([5, 6, [7, 8]], [\"a\", \"b\", \"c\"])\n-\n-        self.assertEqual(\n-            tree.unflatten_as({1: None, 2: None}, [\"Hello\", \"world!\"]),\n-            {1: \"Hello\", 2: \"world!\"},\n-        )\n-\n     def test_flatten_dict_order(self):\n         ordered = collections.OrderedDict(\n             [(\"d\", 3), (\"b\", 1), (\"a\", 0), (\"c\", 2)]\n@@ -77,20 +50,6 @@ class TreeTest(testing.TestCase):\n         self.assertEqual([3, 1, 0, 2], ordered_flat)\n         self.assertEqual([0, 1, 2, 3], plain_flat)\n \n-    def test_unflatten_dict_order(self):\n-        ordered = collections.OrderedDict(\n-            [(\"d\", 0), (\"b\", 0), (\"a\", 0), (\"c\", 0)]\n-        )\n-        plain = {\"d\": 0, \"b\": 0, \"a\": 0, \"c\": 0}\n-        seq = [0, 1, 2, 3]\n-        ordered_reconstruction = tree.unflatten_as(ordered, seq)\n-        plain_reconstruction = tree.unflatten_as(plain, seq)\n-        self.assertEqual(\n-            collections.OrderedDict([(\"d\", 0), (\"b\", 1), (\"a\", 2), (\"c\", 3)]),\n-            ordered_reconstruction,\n-        )\n-        self.assertEqual({\"d\": 3, \"b\": 1, \"a\": 0, \"c\": 2}, plain_reconstruction)\n-\n     def test_map_structure(self):\n         structure2 = (((7, 8), 9), 10, (11, 12))\n         structure1_plus1 = tree.map_structure(lambda x: x + 1, STRUCTURE1)\n\n@@ -3,12 +3,12 @@ import inspect\n import numpy as np\n \n from keras import backend\n+from keras import tree\n from keras.api_export import keras_export\n from keras.layers.layer import Layer\n from keras.saving import serialization_lib\n from keras.utils import jax_utils\n from keras.utils import tracking\n-from keras.utils import tree\n from keras.utils.module_utils import jax\n \n \n\n@@ -13,10 +13,10 @@ from keras import metrics\n from keras import models\n from keras import saving\n from keras import testing\n+from keras import tree\n from keras import utils\n from keras.export import export_lib\n from keras.saving import object_registration\n-from keras.utils import tree\n from keras.utils.jax_layer import FlaxLayer\n from keras.utils.jax_layer import JaxLayer\n \n\n@@ -3,9 +3,9 @@\n import os\n import sys\n \n+from keras import tree\n from keras.api_export import keras_export\n from keras.utils import io_utils\n-from keras.utils import tree\n \n try:\n     # pydot-ng is a fork of pydot that is better maintained.\n\n@@ -41,3 +41,5 @@ gfile = LazyModule(\"tensorflow.io.gfile\", pip_name=\"tensorflow\")\n tensorflow_io = LazyModule(\"tensorflow_io\")\n scipy = LazyModule(\"scipy\")\n jax = LazyModule(\"jax\")\n+optree = LazyModule(\"optree\")\n+dmtree = LazyModule(\"tree\")\n\n@@ -11,9 +11,9 @@ import rich.markup\n import rich.table\n \n from keras import backend\n+from keras import tree\n from keras.utils import dtype_utils\n from keras.utils import io_utils\n-from keras.utils import tree\n \n \n def count_params(weights):\n\n@@ -5,9 +5,9 @@ import types\n from functools import wraps\n \n from keras import backend\n+from keras import tree\n from keras.api_export import keras_export\n from keras.backend.common import global_state\n-from keras.utils import tree\n \n _EXCLUDED_PATHS = (\n     os.path.abspath(os.path.join(__file__, \"..\", \"..\")),\n\n@@ -1,8 +1,6 @@\n from functools import wraps\n \n-import optree\n-import optree.utils\n-\n+from keras import tree\n from keras.backend.common.global_state import get_global_attribute\n from keras.backend.common.global_state import set_global_attribute\n from keras.utils import python_utils\n@@ -135,7 +133,7 @@ class Tracker:\n         self.stored_ids[store_name].add(id(new_value))\n \n \n-@optree.register_pytree_node_class(namespace=\"keras\")\n+@tree.register_tree_node_class\n class TrackedList(list):\n     def __init__(self, values=None, tracker=None):\n         self.tracker = tracker\n@@ -196,7 +194,7 @@ class TrackedList(list):\n         return cls(children)\n \n \n-@optree.register_pytree_node_class(namespace=\"keras\")\n+@tree.register_tree_node_class\n class TrackedDict(dict):\n     def __init__(self, values=None, tracker=None):\n         self.tracker = tracker\n@@ -236,6 +234,8 @@ class TrackedDict(dict):\n         super().clear()\n \n     def tree_flatten(self):\n+        from keras.utils.module_utils import optree\n+\n         # For optree\n         keys, values = optree.utils.unzip2(\n             optree.utils.total_order_sorted(self.items(), key=lambda kv: kv[0])\n@@ -244,11 +244,13 @@ class TrackedDict(dict):\n \n     @classmethod\n     def tree_unflatten(cls, keys, values):\n+        from keras.utils.module_utils import optree\n+\n         # For optree\n         return cls(optree.utils.safe_zip(keys, values))\n \n \n-@optree.register_pytree_node_class(namespace=\"keras\")\n+@tree.register_tree_node_class\n class TrackedSet(set):\n     def __init__(self, values=None, tracker=None):\n         self.tracker = tracker\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
