{"custom_id": "keras#c1c4afe60b1355a6c0e83577791a0423f37a3324", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 38 | Lines Deleted: 12 | Files Changed: 2 | Hunks: 8 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 50 | Churn Cumulative: 3051 | Contributors (this commit): 31 | Commits (past 90d): 10 | Contributors (cumulative): 35 | DMM Complexity: 0.8571428571428571\n\nDIFF:\n@@ -432,7 +432,8 @@ def standardize_weights(y,\n     \"\"\"Performs sample weight validation and standardization.\n \n     Everything gets normalized to a single sample-wise (or timestep-wise)\n-    weight array.\n+    weight array. If both `sample_weights` and `class_weights` are provided,\n+    the weights are multiplied together.\n \n     # Arguments\n         y: Numpy array of model targets to be weighted.\n@@ -478,10 +479,6 @@ def standardize_weights(y,\n                              'sample-wise weights, make sure your '\n                              'sample_weight array is 1D.')\n \n-    if sample_weight is not None and class_weight is not None:\n-        warnings.warn('Found both `sample_weight` and `class_weight`: '\n-                      '`class_weight` argument will be ignored.')\n-\n     if sample_weight is not None:\n         if len(sample_weight.shape) > len(y.shape):\n             raise ValueError('Found a sample_weight with shape' +\n@@ -495,11 +492,13 @@ def standardize_weights(y,\n                              ' for an input with shape ' +\n                              str(y.shape) + '. '\n                              'sample_weight cannot be broadcast.')\n-        return sample_weight\n-    elif isinstance(class_weight, dict):\n+\n+    class_sample_weight = None\n+    if isinstance(class_weight, dict):\n         if len(y.shape) > 2:\n             raise ValueError('`class_weight` not supported for '\n                              '3+ dimensional targets.')\n+        if len(y.shape) == 2:\n             if y.shape[1] > 1:\n                 y_classes = np.argmax(y, axis=1)\n             elif y.shape[1] == 1:\n@@ -507,10 +506,10 @@ def standardize_weights(y,\n         else:\n             y_classes = y\n \n-        weights = np.asarray([class_weight[cls] for cls in y_classes\n-                              if cls in class_weight])\n+        class_sample_weight = np.asarray(\n+            [class_weight[cls] for cls in y_classes if cls in class_weight])\n \n-        if len(weights) != len(y_classes):\n+        if len(class_sample_weight) != len(y_classes):\n             # subtract the sets to pick all missing classes\n             existing_classes = set(y_classes)\n             existing_class_weight = set(class_weight.keys())\n@@ -519,8 +518,15 @@ def standardize_weights(y,\n                              ' The classes %s exist in the data but not in '\n                              '`class_weight`.'\n                              % (existing_classes - existing_class_weight))\n-        return weights\n-    else:\n+\n+    if sample_weight is not None and class_sample_weight is not None:\n+        return sample_weight * class_sample_weight\n+    if sample_weight is not None:\n+        return sample_weight\n+    if class_sample_weight is not None:\n+        return class_sample_weight\n+\n+    # Everything has weight 1 by default.\n     if sample_weight_mode is None:\n         return np.ones((y.shape[0],), dtype=K.floatx())\n     else:\n\n@@ -1575,5 +1575,25 @@ def test_dynamic_set_inputs():\n     assert preds4.shape == (1, 19)\n \n \n+def test_sample_weights():\n+    y = np.array([0, 1, 0, 0, 2])\n+    sample_weights = np.array([0.5, 1., 1., 0., 2.])\n+    class_weights = {0: 0.5, 1: 1., 2: 1.5}\n+\n+    # Only `sample_weights`.\n+    weights = training_utils.standardize_weights(y, sample_weights)\n+    assert np.allclose(weights, sample_weights)\n+\n+    # Only `class_weights`.\n+    weights = training_utils.standardize_weights(y, class_weight=class_weights)\n+    assert np.allclose(weights, np.array([0.5, 1., 0.5, 0.5, 1.5]))\n+\n+    # Both 'sample_weights` and 'class_weights`.\n+    weights = training_utils.standardize_weights(y, sample_weights,\n+                                                 class_weights)\n+    expected = sample_weights * np.array([0.5, 1., 0.5, 0.5, 1.5])\n+    assert np.allclose(weights, expected)\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0cfa5c2709906a7a76f552f71a562f899e408695", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 88 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 88 | Churn Cumulative: 88 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,88 @@\n+# -*- coding: utf-8 -*-\r\n+\r\n+import numpy as np\r\n+import cv2\r\n+import matplotlib.pyplot as plt\r\n+\r\n+from keras.models import Model\r\n+\r\n+import keras.applications.resnet50 as resnet\r\n+from keras.layers import UpSampling2D, Conv2D\r\n+\r\n+\r\n+# Please set an appropriate image file\r\n+INPUT_IMG_FILE = \"dog.jpg\"\r\n+\r\n+################################################################\r\n+# The following parameters can be changed to other models\r\n+# that use global average pooling.\r\n+# e.g.) InceptionResnetV2 / NASNetLarge\r\n+NETWORK_INPUT_SIZE = 224\r\n+MODEL_CLASS = resnet.ResNet50\r\n+PREPROCESS_FN = resnet.preprocess_input\r\n+LAST_CONV_LAYER = \"activation_49\"\r\n+PRED_LAYER = \"fc1000\"\r\n+################################################################\r\n+\r\n+# number of imagenet classes\r\n+N_CLASSES = 1000\r\n+\r\n+\r\n+def load_img(fname, input_size, preprocess_fn):\r\n+    original_img = cv2.imread(fname)[:, :, ::-1]\r\n+    original_size = (original_img.shape[1], original_img.shape[0])\r\n+    img = cv2.resize(original_img, (input_size, input_size))\r\n+    imgs = np.expand_dims(preprocess_fn(img), axis=0)\r\n+    return imgs, original_img, original_size\r\n+\r\n+\r\n+def get_cam_model(model_class,\r\n+                  input_size=224,\r\n+                  last_conv_layer=\"activation_49\",\r\n+                  pred_layer=\"fc1000\"):\r\n+    model = model_class(input_shape=(input_size, input_size, 3))\r\n+\r\n+    final_params = model.get_layer(pred_layer).get_weights()\r\n+    final_params = (final_params[0].reshape(\r\n+        1, 1, -1, N_CLASSES), final_params[1])\r\n+\r\n+    last_conv_output = model.get_layer(last_conv_layer).output\r\n+    x = UpSampling2D(size=(32, 32), interpolation=\"bilinear\")(\r\n+        last_conv_output)\r\n+    x = Conv2D(filters=N_CLASSES, kernel_size=(\r\n+        1, 1), name=\"predictions_2\")(x)\r\n+\r\n+    cam_model = Model(inputs=model.input,\r\n+                      outputs=[model.output, x])\r\n+    cam_model.get_layer(\"predictions_2\").set_weights(final_params)\r\n+    return cam_model\r\n+\r\n+\r\n+def postprocess(preds, cams, top_k=1):\r\n+    idxes = np.argsort(preds[0])[-top_k:]\r\n+    class_activation_map = np.zeros_like(cams[0, :, :, 0])\r\n+    for i in idxes:\r\n+        class_activation_map += cams[0, :, :, i]\r\n+    return class_activation_map\r\n+\r\n+\r\n+# 1. load image\r\n+imgs, original_img, original_size = load_img(INPUT_IMG_FILE,\r\n+                                             input_size=NETWORK_INPUT_SIZE,\r\n+                                             preprocess_fn=resnet.preprocess_input)\r\n+\r\n+# 2. prediction\r\n+model = get_cam_model(resnet.ResNet50,\r\n+                      NETWORK_INPUT_SIZE,\r\n+                      LAST_CONV_LAYER,\r\n+                      PRED_LAYER)\r\n+preds, cams = model.predict(imgs)\r\n+\r\n+# 4. post processing\r\n+class_activation_map = postprocess(preds, cams)\r\n+\r\n+# 5. plot image+cam to original size\r\n+plt.imshow(original_img, alpha=0.5)\r\n+plt.imshow(cv2.resize(class_activation_map,\r\n+                      original_size), cmap='jet', alpha=0.5)\r\n+plt.show()\r\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a9cd4bfc8fcc7ed43e092c84415c261d30e9c79f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 444 | Lines Deleted: 24 | Files Changed: 4 | Hunks: 28 | Methods Changed: 25 | Complexity Δ (Sum/Max): 67/33 | Churn Δ: 468 | Churn Cumulative: 6153 | Contributors (this commit): 36 | Commits (past 90d): 18 | Contributors (cumulative): 55 | DMM Complexity: 0.7948717948717948\n\nDIFF:\n@@ -1086,6 +1086,7 @@ class Network(Layer):\n         from ..models import save_model\n         save_model(self, filepath, overwrite, include_optimizer)\n \n+    @saving.allow_write_to_gcs\n     def save_weights(self, filepath, overwrite=True):\n         \"\"\"Dumps all layer weights to a HDF5 file.\n \n@@ -1118,6 +1119,7 @@ class Network(Layer):\n             saving.save_weights_to_hdf5_group(f, self.layers)\n             f.flush()\n \n+    @saving.allow_read_from_gcs\n     def load_weights(self, filepath, by_name=False,\n                      skip_mismatch=False, reshape=False):\n         \"\"\"Loads all layer weights from a HDF5 save file.\n\n@@ -4,12 +4,17 @@ from __future__ import print_function\n from __future__ import absolute_import\n from __future__ import division\n \n-import numpy as np\n import os\n import json\n import yaml\n+import inspect\n import warnings\n+import tempfile\n from six.moves import zip\n+from six import string_types\n+from functools import wraps\n+\n+import numpy as np\n \n from .. import backend as K\n from .. import optimizers\n@@ -23,6 +28,11 @@ try:\n except ImportError:\n     h5py = None\n \n+try:\n+    from tensorflow.python.lib.io import file_io as tf_file_io\n+except ImportError:\n+    tf_file_io = None\n+\n \n def _serialize_model(model, h5dict, include_optimizer=True):\n     \"\"\"Model serialization logic.\n@@ -333,6 +343,112 @@ def _deserialize_model(h5dict, custom_objects=None, compile=True):\n     return model\n \n \n+def _gcs_copy(source_filepath, target_filepath, overwrite=True):\n+    \"\"\"Copies a file to/from/within Google Cloud Storage (GCS).\n+\n+    # Arguments\n+        source_filepath: String, path to the file on filesystem or object on GCS to\n+            copy from.\n+        target_filepath: String, path to the file on filesystem or object on GCS to\n+            copy to.\n+        overwrite: Whether we should overwrite an existing file/object at the target\n+            location, or instead ask the user with a manual prompt.\n+    \"\"\"\n+    if tf_file_io is None:\n+        raise ImportError('Google Cloud Storage file transfer requires TensorFlow.')\n+    if not overwrite and tf_file_io.file_exists(target_filepath):\n+        proceed = ask_to_proceed_with_overwrite(target_filepath)\n+        if not proceed:\n+            return\n+    with tf_file_io.FileIO(source_filepath, mode='rb') as source_f:\n+        with tf_file_io.FileIO(target_filepath, mode='wb') as target_f:\n+            target_f.write(source_f.read())\n+\n+\n+def _is_gcs_location(filepath):\n+    \"\"\"Checks if `filepath` is referencing a google storage bucket.\n+\n+    # Arguments\n+        filepath: The location to check.\n+    \"\"\"\n+    return isinstance(filepath, string_types) and filepath.startswith('gs://')\n+\n+\n+def allow_write_to_gcs(save_function):\n+    \"\"\"Function decorator to support saving to Google Cloud Storage (GCS).\n+\n+    This decorator parses the `filepath` argument of the `save_function` and\n+    transfers the file to GCS if `filepath` starts with \"gs://\".\n+\n+    Note: the file is temporarily writen to local filesystem before copied to GSC.\n+\n+    # Arguments\n+        save_function: The function to wrap, with requirements:\n+            - second positional argument should indicate the location to save to.\n+            - third positional argument should be the `overwrite` option indicating\n+            whether we should overwrite an existing file/object at the target\n+            location, or instead ask the user with a manual prompt.\n+    \"\"\"\n+    @wraps(save_function)\n+    def save_wrapper(obj, filepath, overwrite=True, *args, **kwargs):\n+        if _is_gcs_location(filepath):\n+            tmp_filepath = os.path.join(tempfile.gettempdir(),\n+                                        os.path.basename(filepath))\n+            save_function(obj, tmp_filepath, True, *args, **kwargs)\n+            try:\n+                _gcs_copy(tmp_filepath, filepath, overwrite)\n+            finally:\n+                os.remove(tmp_filepath)\n+        else:\n+            save_function(obj, filepath, overwrite, *args, **kwargs)\n+\n+    return save_wrapper\n+\n+\n+def allow_read_from_gcs(load_function):\n+    \"\"\"Function decorator to support loading from Google Cloud Storage (GCS).\n+\n+    This decorator parses the `filepath` argument of the `load_function` and\n+    fetches the required object from GCS if `filepath` starts with \"gs://\".\n+\n+    Note: the file is temporarily copied to local filesystem from GCS before loaded.\n+\n+    # Arguments\n+        load_function: The function to wrap, with requirements:\n+            - should have one _named_ argument `filepath` indicating the location to\n+            load from.\n+    \"\"\"\n+    def extract_named_arg(f, name, args, kwargs):\n+        if name in kwargs:\n+            arg = kwargs.pop(name)\n+            return arg, args, kwargs\n+        argnames = inspect.getargspec(f)[0]\n+        for i, (argname, arg) in enumerate(zip(argnames, args)):\n+            if argname == name:\n+                return arg, args[:i] + args[i + 1:], kwargs\n+        else:\n+            raise ValueError('function {} has no argument {}'.format(f, name))\n+\n+    @wraps(load_function)\n+    def load_wrapper(*args, **kwargs):\n+        filepath, _args, _kwargs = extract_named_arg(\n+            load_function, 'filepath', args, kwargs)\n+        if _is_gcs_location(filepath):\n+            tmp_filepath = os.path.join(tempfile.gettempdir(),\n+                                        os.path.basename(filepath))\n+            _gcs_copy(filepath, tmp_filepath)\n+            _kwargs['filepath'] = tmp_filepath\n+            try:\n+                res = load_function(*_args, **_kwargs)\n+            finally:\n+                os.remove(tmp_filepath)\n+            return res\n+        return load_function(*args, **kwargs)\n+\n+    return load_wrapper\n+\n+\n+@allow_write_to_gcs\n def save_model(model, filepath, overwrite=True, include_optimizer=True):\n     \"\"\"Save a model to a HDF5 file.\n \n@@ -385,6 +501,7 @@ def save_model(model, filepath, overwrite=True, include_optimizer=True):\n             h5dict.close()\n \n \n+@allow_read_from_gcs\n def load_model(filepath, custom_objects=None, compile=True):\n     \"\"\"Loads a model saved via `save_model`.\n \n\n@@ -3,6 +3,9 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n+import os\n+from io import BytesIO\n+\n import numpy as np\n from numpy.testing import assert_allclose\n \n@@ -10,6 +13,16 @@ from .generic_utils import has_arg\n from ..engine import Model, Input\n from .. import backend as K\n \n+try:\n+    from tensorflow.python.lib.io import file_io as tf_file_io\n+except ImportError:\n+    tf_file_io = None\n+\n+try:\n+    from unittest.mock import patch, Mock, MagicMock\n+except:\n+    from mock import patch, Mock, MagicMock\n+\n \n def get_test_data(num_train=1000, num_test=500, input_shape=(10,),\n                   output_shape=(2,),\n@@ -116,3 +129,182 @@ def layer_test(layer_cls, kwargs={}, input_shape=None, input_dtype=None,\n \n     # for further checks in the caller function\n     return actual_output\n+\n+\n+class tf_file_io_proxy(object):\n+    \"\"\"Context manager for mock patching `tensorflow.python.lib.io.file_io` in tests.\n+\n+    The purpose of this class is to be able to tests model saving/loading to/from\n+    Google Cloud Storage, for witch the tensorflow `file_io` package is used.\n+\n+    If a `bucket_name` is provided, either as an input argument or by setting the\n+    environment variable GCS_TEST_BUCKET, *NO mocking* will be done and files will be\n+    transferred to the real GCS bucket. For this to work, valid Google application\n+    credentials must be available, see:\n+        https://cloud.google.com/video-intelligence/docs/common/auth\n+    for further details.\n+\n+    If a `bucket_name` is not provided, an identifier of the import of the file_io\n+    module to mock must be provided, using the `file_io_module` argument.\n+    NOTE that only part of the module is mocked and that the same Exceptions\n+    are not raised in mock implementation.\n+\n+    Since the bucket name can be provided using an environment variable, it is\n+    recommended to use method `get_filepath(filename)` in tests to make them\n+    pass with and without a real GCS bucket during testing. See example below.\n+\n+    Arguments:\n+        file_io_module: String identifier of the file_io module import to patch. E.g\n+            'keras.engine.saving.tf_file_io'\n+        bucket_name: String identifier of *a real* GCS bucket (with or without the\n+            'gs://' prefix). A bucket name provided with argument precedes what is\n+            specified using the GCS_TEST_BUCKET environment variable.\n+\n+    Example:\n+    ```python\n+    model = Sequential()\n+    model.add(Dense(2, input_shape=(3,)))\n+\n+    with tf_file_io_proxy('keras.engine.saving.tf_file_io') as file_io_proxy:\n+        gcs_filepath = file_io_proxy.get_filepath(filename='model.h5')\n+        save_model(model, gcs_filepath)\n+        file_io_proxy.assert_exists(gcs_filepath)\n+        new_model_gcs = load_model(gcs_filepath)\n+        file_io_proxy.delete_file(gcs_filepath)  # cleanup\n+    ```\n+    \"\"\"\n+    _gcs_prefix = 'gs://'\n+    _test_bucket_env_key = 'GCS_TEST_BUCKET'\n+\n+    def __init__(self, file_io_module=None, bucket_name=None):\n+        if bucket_name is None:\n+            bucket_name = os.environ.get(self._test_bucket_env_key, None)\n+        if bucket_name is None:\n+            # will mock gcs locally for tests\n+            if file_io_module is None:\n+                raise ValueError('`file_io_module` must be provided for mocking')\n+            self.mock_gcs = True\n+            self.file_io_module = file_io_module\n+            self.local_objects = {}\n+            self.bucket_name = 'mock-bucket'\n+        else:\n+            # will use real bucket for tests\n+            if bucket_name.startswith(self._gcs_prefix):\n+                bucket_name = bucket_name[len(self._gcs_prefix):]\n+            self.bucket_name = bucket_name\n+            if tf_file_io is None:\n+                raise ImportError(\n+                    'tensorflow must be installed to read/write to GCS')\n+            try:\n+                # check that bucket exists and is accessible\n+                tf_file_io.is_directory(self.bucket_path)\n+            except:\n+                raise IOError(\n+                    'could not access provided bucket {}'.format(self.bucket_path))\n+            self.mock_gcs = False\n+            self.file_io_module = None\n+            self.local_objects = None\n+\n+        self.patched_file_io = None\n+        self._is_started = False\n+\n+    @property\n+    def bucket_path(self):\n+        \"\"\"Returns the full GCS bucket path\"\"\"\n+        return self._gcs_prefix + self.bucket_name\n+\n+    def get_filepath(self, filename):\n+        \"\"\"Returns filename appended to bucketpath\"\"\"\n+        return os.path.join(self.bucket_path, filename)\n+\n+    def FileIO(self, name, mode):\n+        \"\"\"Proxy for tensorflow.python.lib.io.file_io.FileIO class. Mocks the class\n+        if a real GCS bucket is not available for testing.\n+        \"\"\"\n+        self._check_started()\n+        if not self.mock_gcs:\n+            return tf_file_io.FileIO(name, mode)\n+\n+        filepath = name\n+        if filepath.startswith(self._gcs_prefix):\n+            mock_fio = MagicMock()\n+            mock_fio.__enter__ = Mock(return_value=mock_fio)\n+            if mode == 'rb':\n+                if filepath not in self.local_objects:\n+                    raise IOError('{} does not exist'.format(filepath))\n+                self.local_objects[filepath].seek(0)\n+                mock_fio.read = self.local_objects[filepath].read\n+            elif mode == 'wb':\n+                self.local_objects[filepath] = BytesIO()\n+                mock_fio.write = self.local_objects[filepath].write\n+            else:\n+                raise ValueError(\n+                    '{} only supports wrapping of FileIO for `mode` \"rb\" or \"wb\"')\n+            return mock_fio\n+\n+        return open(filepath, mode)\n+\n+    def file_exists(self, filename):\n+        \"\"\"Proxy for tensorflow.python.lib.io.file_io.file_exists class. Mocks the\n+        function if a real GCS bucket is not available for testing.\n+        \"\"\"\n+        self._check_started()\n+        if not self.mock_gcs:\n+            return tf_file_io.file_exists(filename)\n+\n+        if filename.startswith(self._gcs_prefix):\n+            return filename in self.local_objects\n+\n+        return os.path.exists(filename)\n+\n+    def delete_file(self, filename):\n+        \"\"\"Proxy for tensorflow.python.lib.io.file_io.delete_file function. Mocks\n+        the function if a real GCS bucket is not available for testing.\n+        \"\"\"\n+        if not self.mock_gcs:\n+            tf_file_io.delete_file(filename)\n+        elif filename.startswith(self._gcs_prefix):\n+            self.local_objects.pop(filename)\n+        else:\n+            os.remove(filename)\n+\n+    def assert_exists(self, filepath):\n+        \"\"\"Convenience method for verifying that a file exists after writing.\"\"\"\n+        self._check_started()\n+        if not self.file_exists(filepath):\n+            raise AssertionError('{} does not exist'.format(filepath))\n+\n+    def _check_started(self):\n+        if not self._is_started:\n+            raise RuntimeError('tf_file_io_proxy is not started')\n+\n+    def start(self):\n+        \"\"\"Start mocking of `self.file_io_module` if real bucket not\n+        available for testing\"\"\"\n+        if self._is_started:\n+            raise RuntimeError('start called on already started tf_file_io_proxy')\n+        if self.mock_gcs:\n+            mock_module = Mock()\n+            mock_module.FileIO = self.FileIO\n+            mock_module.file_exists = self.file_exists\n+            mock_module.delete_file = self.delete_file\n+            patched_file_io = patch(self.file_io_module, new=mock_module)\n+            self.patched_file_io = patched_file_io\n+            self.patched_file_io.start()\n+        self._is_started = True\n+\n+    def stop(self):\n+        \"\"\"Stop mocking of `self.file_io_module` if real bucket not\n+        available for testing\"\"\"\n+        if not self._is_started:\n+            raise RuntimeError('stop called on unstarted tf_file_io_proxy')\n+        if self.mock_gcs:\n+            self.patched_file_io.stop()\n+        self._is_started = False\n+\n+    def __enter__(self):\n+        self.start()\n+        return self\n+\n+    def __exit__(self, exc_type, exc_val, exc_tb):\n+        self.stop()\n\n@@ -1,3 +1,4 @@\n+\n import pytest\n import os\n import h5py\n@@ -18,6 +19,11 @@ from keras import optimizers\n from keras import losses\n from keras import metrics\n from keras.models import save_model, load_model\n+from keras.utils.test_utils import tf_file_io_proxy\n+try:\n+    from unittest.mock import patch\n+except:\n+    from mock import patch\n \n \n skipif_no_tf_gpu = pytest.mark.skipif(\n@@ -40,23 +46,31 @@ def test_sequential_model_saving():\n     model.train_on_batch(x, y)\n \n     out = model.predict(x)\n+\n     _, fname = tempfile.mkstemp('.h5')\n     save_model(model, fname)\n-\n-    new_model = load_model(fname)\n+    new_model_disk = load_model(fname)\n     os.remove(fname)\n \n-    out2 = new_model.predict(x)\n-    assert_allclose(out, out2, atol=1e-05)\n+    with tf_file_io_proxy('keras.engine.saving.tf_file_io') as file_io_proxy:\n+        gcs_filepath = file_io_proxy.get_filepath(filename=fname)\n+        save_model(model, gcs_filepath)\n+        file_io_proxy.assert_exists(gcs_filepath)\n+        new_model_gcs = load_model(gcs_filepath)\n+        file_io_proxy.delete_file(gcs_filepath)  # cleanup\n \n+    x2 = np.random.random((1, 3))\n+    y2 = np.random.random((1, 3, 3))\n+    model.train_on_batch(x2, y2)\n+    out_2 = model.predict(x2)\n+\n+    for new_model in [new_model_disk, new_model_gcs]:\n+        new_out = new_model.predict(x)\n+        assert_allclose(out, new_out, atol=1e-05)\n         # test that new updates are the same with both models\n-    x = np.random.random((1, 3))\n-    y = np.random.random((1, 3, 3))\n-    model.train_on_batch(x, y)\n-    new_model.train_on_batch(x, y)\n-    out = model.predict(x)\n-    out2 = new_model.predict(x)\n-    assert_allclose(out, out2, atol=1e-05)\n+        new_model.train_on_batch(x2, y2)\n+        new_out_2 = new_model.predict(x2)\n+        assert_allclose(out_2, new_out_2, atol=1e-05)\n \n \n def test_sequential_model_saving_2():\n@@ -71,18 +85,25 @@ def test_sequential_model_saving_2():\n     x = np.random.random((1, 3))\n     y = np.random.random((1, 3))\n     model.train_on_batch(x, y)\n-\n     out = model.predict(x)\n+\n+    load_kwargs = {'custom_objects': {'custom_opt': custom_opt,\n+                                      'custom_loss': custom_loss}}\n     _, fname = tempfile.mkstemp('.h5')\n     save_model(model, fname)\n-\n-    model = load_model(fname,\n-                       custom_objects={'custom_opt': custom_opt,\n-                                       'custom_loss': custom_loss})\n+    new_model_disk = load_model(fname, **load_kwargs)\n     os.remove(fname)\n \n-    out2 = model.predict(x)\n-    assert_allclose(out, out2, atol=1e-05)\n+    with tf_file_io_proxy('keras.engine.saving.tf_file_io') as file_io_proxy:\n+        gcs_filepath = file_io_proxy.get_filepath(filename=fname)\n+        save_model(model, gcs_filepath)\n+        file_io_proxy.assert_exists(gcs_filepath)\n+        new_model_gcs = load_model(gcs_filepath, **load_kwargs)\n+        file_io_proxy.delete_file(gcs_filepath)  # cleanup\n+\n+    for new_model in [new_model_disk, new_model_gcs]:\n+        new_out = new_model.predict(x)\n+        assert_allclose(out, new_out, atol=1e-05)\n \n \n def test_functional_model_saving():\n@@ -97,16 +118,23 @@ def test_functional_model_saving():\n     x = np.random.random((1, 3))\n     y = np.random.random((1, 3))\n     model.train_on_batch(x, y)\n-\n     out = model.predict(x)\n+\n     _, fname = tempfile.mkstemp('.h5')\n     save_model(model, fname)\n-\n-    model = load_model(fname)\n+    new_model_disk = load_model(fname)\n     os.remove(fname)\n \n-    out2 = model.predict(x)\n-    assert_allclose(out, out2, atol=1e-05)\n+    with tf_file_io_proxy('keras.engine.saving.tf_file_io') as file_io_proxy:\n+        gcs_filepath = file_io_proxy.get_filepath(filename=fname)\n+        save_model(model, gcs_filepath)\n+        file_io_proxy.assert_exists(gcs_filepath)\n+        new_model_gcs = load_model(gcs_filepath)\n+        file_io_proxy.delete_file(gcs_filepath)  # cleanup\n+\n+    for new_model in [new_model_disk, new_model_gcs]:\n+        new_out = new_model.predict(x)\n+        assert_allclose(out, new_out, atol=1e-05)\n \n \n def test_model_saving_to_pre_created_h5py_file():\n@@ -670,6 +698,87 @@ def test_saving_constant_initializer_with_numpy():\n     os.remove(fname)\n \n \n+def test_save_load_weights_gcs():\n+    model = Sequential()\n+    model.add(Dense(2, input_shape=(3,)))\n+    org_weights = model.get_weights()\n+\n+    with tf_file_io_proxy('keras.engine.saving.tf_file_io') as file_io_proxy:\n+        gcs_filepath = file_io_proxy.get_filepath(\n+            filename='test_save_load_weights_gcs.h5')\n+        # we should not use same filename in several tests to allow for parallel\n+        # execution\n+        model.save_weights(gcs_filepath)\n+        model.set_weights([np.random.random(w.shape) for w in org_weights])\n+        for w, org_w in zip(model.get_weights(), org_weights):\n+            assert not (w == org_w).all()\n+        model.load_weights(gcs_filepath)\n+        for w, org_w in zip(model.get_weights(), org_weights):\n+            assert_allclose(w, org_w)\n+\n+        file_io_proxy.delete_file(gcs_filepath)  # cleanup\n+\n+\n+def test_saving_overwrite_option():\n+    model = Sequential()\n+    model.add(Dense(2, input_shape=(3,)))\n+    org_weights = model.get_weights()\n+    new_weights = [np.random.random(w.shape) for w in org_weights]\n+\n+    _, fname = tempfile.mkstemp('.h5')\n+    save_model(model, fname)\n+    model.set_weights(new_weights)\n+\n+    with patch('keras.engine.saving.ask_to_proceed_with_overwrite') as ask:\n+        ask.return_value = False\n+        save_model(model, fname, overwrite=False)\n+        ask.assert_called_once()\n+        new_model = load_model(fname)\n+        for w, org_w in zip(new_model.get_weights(), org_weights):\n+            assert_allclose(w, org_w)\n+\n+        ask.return_value = True\n+        save_model(model, fname, overwrite=False)\n+        assert ask.call_count == 2\n+        new_model = load_model(fname)\n+        for w, new_w in zip(new_model.get_weights(), new_weights):\n+            assert_allclose(w, new_w)\n+\n+    os.remove(fname)\n+\n+\n+def test_saving_overwrite_option_gcs():\n+    model = Sequential()\n+    model.add(Dense(2, input_shape=(3,)))\n+    org_weights = model.get_weights()\n+    new_weights = [np.random.random(w.shape) for w in org_weights]\n+\n+    with tf_file_io_proxy('keras.engine.saving.tf_file_io') as file_io_proxy:\n+        gcs_filepath = file_io_proxy.get_filepath(\n+            filename='test_saving_overwrite_option_gcs.h5')\n+        # we should not use same filename in several tests to allow for parallel\n+        # execution\n+        save_model(model, gcs_filepath)\n+        model.set_weights(new_weights)\n+\n+        with patch('keras.engine.saving.ask_to_proceed_with_overwrite') as ask:\n+            ask.return_value = False\n+            save_model(model, gcs_filepath, overwrite=False)\n+            ask.assert_called_once()\n+            new_model = load_model(gcs_filepath)\n+            for w, org_w in zip(new_model.get_weights(), org_weights):\n+                assert_allclose(w, org_w)\n+\n+            ask.return_value = True\n+            save_model(model, gcs_filepath, overwrite=False)\n+            assert ask.call_count == 2\n+            new_model = load_model(gcs_filepath)\n+            for w, new_w in zip(new_model.get_weights(), new_weights):\n+                assert_allclose(w, new_w)\n+\n+        file_io_proxy.delete_file(gcs_filepath)  # cleanup\n+\n+\n @pytest.mark.parametrize('implementation', [1, 2], ids=['impl1', 'impl2'])\n @pytest.mark.parametrize('bidirectional',\n                          [False, True],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#aefcd8b3e43c26072dc1a3774a65f506feeaeb8d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 110 | Contributors (this commit): 4 | Commits (past 90d): 9 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -725,7 +725,7 @@ def ctc_decode(y_pred, input_length, greedy=True, beam_width=100, top_paths=1):\n             decoded_dense[i, :len(decoded)] = decoded\n         return decoded_dense[:, :np.max(decoded_length)], log_prob\n     else:\n-        raise \"not supported yet\"\n+        raise NotImplementedError\n \n \n def _remove_repeats(inds):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#994c4bb338ce440a27df5db48b8f5044a3e8f611", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 387 | Lines Deleted: 373 | Files Changed: 2 | Hunks: 9 | Methods Changed: 1 | Complexity Δ (Sum/Max): 19/19 | Churn Δ: 760 | Churn Cumulative: 2621 | Contributors (this commit): 31 | Commits (past 90d): 12 | Contributors (cumulative): 32 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,64 +1,4 @@\n # -*- coding: utf-8 -*-\n-'''\n-General documentation architecture:\n-\n-Home\n-Index\n-\n-- Getting started\n-    Getting started with the sequential model\n-    Getting started with the functional api\n-    FAQ\n-\n-- Models\n-    About Keras models\n-        explain when one should use Sequential or functional API\n-        explain compilation step\n-        explain weight saving, weight loading\n-        explain serialization, deserialization\n-    Sequential\n-    Model (functional API)\n-\n-- Layers\n-    About Keras layers\n-        explain common layer functions: get_weights, set_weights, get_config\n-        explain input_shape\n-        explain usage on non-Keras tensors\n-    Core Layers\n-    Convolutional Layers\n-    Pooling Layers\n-    Locally-connected Layers\n-    Recurrent Layers\n-    Embedding Layers\n-    Merge Layers\n-    Advanced Activations Layers\n-    Normalization Layers\n-    Noise Layers\n-    Layer Wrappers\n-    Writing your own Keras layers\n-\n-- Preprocessing\n-    Sequence Preprocessing\n-    Text Preprocessing\n-    Image Preprocessing\n-\n-Losses\n-Metrics\n-Optimizers\n-Activations\n-Callbacks\n-Datasets\n-Applications\n-Backend\n-Initializers\n-Regularizers\n-Constraints\n-Visualization\n-Scikit-learn API\n-Utils\n-Contributing\n-\n-'''\n from __future__ import print_function\n from __future__ import unicode_literals\n \n@@ -68,22 +8,14 @@ import os\n import shutil\n \n import keras\n-from keras import utils\n-from keras import layers\n-from keras.layers import advanced_activations\n-from keras.layers import noise\n-from keras.layers import wrappers\n-from keras import initializers\n-from keras import optimizers\n-from keras import callbacks\n-from keras import models\n-from keras import losses\n-from keras import metrics\n-from keras import backend\n+from keras import backend as K\n from keras.backend import numpy_backend\n-from keras import constraints\n-from keras import activations\n-from keras import preprocessing\n+\n+from docs.structure import EXCLUDE\n+from docs.structure import PAGES\n+from docs.structure import ROOT\n+from docs.structure import template_np_implementation\n+from docs.structure import template_hidden_np_implementation\n \n import sys\n if sys.version[0] == '2':\n@@ -91,267 +23,6 @@ if sys.version[0] == '2':\n     sys.setdefaultencoding('utf8')\n \n \n-EXCLUDE = {\n-    'Optimizer',\n-    'TFOptimizer',\n-    'Wrapper',\n-    'get_session',\n-    'set_session',\n-    'CallbackList',\n-    'serialize',\n-    'deserialize',\n-    'get',\n-    'set_image_dim_ordering',\n-    'normalize_data_format',\n-    'image_dim_ordering',\n-    'get_variable_shape',\n-    'Constraint'\n-}\n-\n-\n-# For each class to document, it is possible to:\n-# 1) Document only the class: [classA, classB, ...]\n-# 2) Document all its methods: [classA, (classB, \"*\")]\n-# 3) Choose which methods to document (methods listed as strings):\n-# [classA, (classB, [\"method1\", \"method2\", ...]), ...]\n-# 4) Choose which methods to document (methods listed as qualified names):\n-# [classA, (classB, [module.classB.method1, module.classB.method2, ...]), ...]\n-PAGES = [\n-    {\n-        'page': 'models/sequential.md',\n-        'methods': [\n-            models.Sequential.compile,\n-            models.Sequential.fit,\n-            models.Sequential.evaluate,\n-            models.Sequential.predict,\n-            models.Sequential.train_on_batch,\n-            models.Sequential.test_on_batch,\n-            models.Sequential.predict_on_batch,\n-            models.Sequential.fit_generator,\n-            models.Sequential.evaluate_generator,\n-            models.Sequential.predict_generator,\n-            models.Sequential.get_layer,\n-        ],\n-    },\n-    {\n-        'page': 'models/model.md',\n-        'methods': [\n-            models.Model.compile,\n-            models.Model.fit,\n-            models.Model.evaluate,\n-            models.Model.predict,\n-            models.Model.train_on_batch,\n-            models.Model.test_on_batch,\n-            models.Model.predict_on_batch,\n-            models.Model.fit_generator,\n-            models.Model.evaluate_generator,\n-            models.Model.predict_generator,\n-            models.Model.get_layer,\n-        ]\n-    },\n-    {\n-        'page': 'layers/core.md',\n-        'classes': [\n-            layers.Dense,\n-            layers.Activation,\n-            layers.Dropout,\n-            layers.Flatten,\n-            layers.Input,\n-            layers.Reshape,\n-            layers.Permute,\n-            layers.RepeatVector,\n-            layers.Lambda,\n-            layers.ActivityRegularization,\n-            layers.Masking,\n-            layers.SpatialDropout1D,\n-            layers.SpatialDropout2D,\n-            layers.SpatialDropout3D,\n-        ],\n-    },\n-    {\n-        'page': 'layers/convolutional.md',\n-        'classes': [\n-            layers.Conv1D,\n-            layers.Conv2D,\n-            layers.SeparableConv1D,\n-            layers.SeparableConv2D,\n-            layers.DepthwiseConv2D,\n-            layers.Conv2DTranspose,\n-            layers.Conv3D,\n-            layers.Conv3DTranspose,\n-            layers.Cropping1D,\n-            layers.Cropping2D,\n-            layers.Cropping3D,\n-            layers.UpSampling1D,\n-            layers.UpSampling2D,\n-            layers.UpSampling3D,\n-            layers.ZeroPadding1D,\n-            layers.ZeroPadding2D,\n-            layers.ZeroPadding3D,\n-        ],\n-    },\n-    {\n-        'page': 'layers/pooling.md',\n-        'classes': [\n-            layers.MaxPooling1D,\n-            layers.MaxPooling2D,\n-            layers.MaxPooling3D,\n-            layers.AveragePooling1D,\n-            layers.AveragePooling2D,\n-            layers.AveragePooling3D,\n-            layers.GlobalMaxPooling1D,\n-            layers.GlobalAveragePooling1D,\n-            layers.GlobalMaxPooling2D,\n-            layers.GlobalAveragePooling2D,\n-            layers.GlobalMaxPooling3D,\n-            layers.GlobalAveragePooling3D,\n-        ],\n-    },\n-    {\n-        'page': 'layers/local.md',\n-        'classes': [\n-            layers.LocallyConnected1D,\n-            layers.LocallyConnected2D,\n-        ],\n-    },\n-    {\n-        'page': 'layers/recurrent.md',\n-        'classes': [\n-            layers.RNN,\n-            layers.SimpleRNN,\n-            layers.GRU,\n-            layers.LSTM,\n-            layers.ConvLSTM2D,\n-            layers.SimpleRNNCell,\n-            layers.GRUCell,\n-            layers.LSTMCell,\n-            layers.CuDNNGRU,\n-            layers.CuDNNLSTM,\n-        ],\n-    },\n-    {\n-        'page': 'layers/embeddings.md',\n-        'classes': [\n-            layers.Embedding,\n-        ],\n-    },\n-    {\n-        'page': 'layers/normalization.md',\n-        'classes': [\n-            layers.BatchNormalization,\n-        ],\n-    },\n-    {\n-        'page': 'layers/advanced-activations.md',\n-        'all_module_classes': [advanced_activations],\n-    },\n-    {\n-        'page': 'layers/noise.md',\n-        'all_module_classes': [noise],\n-    },\n-    {\n-        'page': 'layers/merge.md',\n-        'classes': [\n-            layers.Add,\n-            layers.Subtract,\n-            layers.Multiply,\n-            layers.Average,\n-            layers.Maximum,\n-            layers.Concatenate,\n-            layers.Dot,\n-        ],\n-        'functions': [\n-            layers.add,\n-            layers.subtract,\n-            layers.multiply,\n-            layers.average,\n-            layers.maximum,\n-            layers.concatenate,\n-            layers.dot,\n-        ]\n-    },\n-    {\n-        'page': 'preprocessing/sequence.md',\n-        'functions': [\n-            preprocessing.sequence.pad_sequences,\n-            preprocessing.sequence.skipgrams,\n-            preprocessing.sequence.make_sampling_table,\n-        ],\n-        'classes': [\n-            preprocessing.sequence.TimeseriesGenerator,\n-        ]\n-    },\n-    {\n-        'page': 'preprocessing/image.md',\n-        'classes': [\n-            (preprocessing.image.ImageDataGenerator, '*')\n-        ]\n-    },\n-    {\n-        'page': 'preprocessing/text.md',\n-        'functions': [\n-            preprocessing.text.hashing_trick,\n-            preprocessing.text.one_hot,\n-            preprocessing.text.text_to_word_sequence,\n-        ],\n-        'classes': [\n-            preprocessing.text.Tokenizer,\n-        ]\n-    },\n-    {\n-        'page': 'layers/wrappers.md',\n-        'all_module_classes': [wrappers],\n-    },\n-    {\n-        'page': 'metrics.md',\n-        'all_module_functions': [metrics],\n-    },\n-    {\n-        'page': 'losses.md',\n-        'all_module_functions': [losses],\n-    },\n-    {\n-        'page': 'initializers.md',\n-        'all_module_functions': [initializers],\n-        'all_module_classes': [initializers],\n-    },\n-    {\n-        'page': 'optimizers.md',\n-        'all_module_classes': [optimizers],\n-    },\n-    {\n-        'page': 'callbacks.md',\n-        'all_module_classes': [callbacks],\n-    },\n-    {\n-        'page': 'activations.md',\n-        'all_module_functions': [activations],\n-    },\n-    {\n-        'page': 'backend.md',\n-        'all_module_functions': [backend],\n-    },\n-    {\n-        'page': 'constraints.md',\n-        'all_module_classes': [constraints],\n-    },\n-    {\n-        'page': 'utils.md',\n-        'functions': [utils.to_categorical,\n-                      utils.normalize,\n-                      utils.get_file,\n-                      utils.print_summary,\n-                      utils.plot_model,\n-                      utils.multi_gpu_model],\n-        'classes': [utils.CustomObjectScope,\n-                    utils.HDF5Matrix,\n-                    utils.Sequence],\n-    },\n-]\n-\n-ROOT = 'http://keras.io/'\n-\n-\n def get_function_signature(function, method=True):\n     wrapped = getattr(function, '_original_function', None)\n     if wrapped is None:\n@@ -571,26 +242,6 @@ def process_docstring(docstring):\n     return docstring\n \n \n-template_np_implementation = \"\"\"# Numpy implementation\n-\n-    ```python\n-{{code}}\n-    ```\n-\"\"\"\n-\n-template_hidden_np_implementation = \"\"\"# Numpy implementation\n-\n-    <details>\n-    <summary>Show the Numpy implementation</summary>\n-\n-    ```python\n-{{code}}\n-    ```\n-\n-    </details>\n-\"\"\"\n-\n-\n def add_np_implementation(function, docstring):\n     np_implementation = getattr(numpy_backend, function.__name__)\n     code = inspect.getsource(np_implementation)\n@@ -608,22 +259,6 @@ def add_np_implementation(function, docstring):\n     return docstring.replace('{{np_implementation}}', section)\n \n \n-print('Cleaning up existing sources directory.')\n-if os.path.exists('sources'):\n-    shutil.rmtree('sources')\n-\n-print('Populating sources directory with templates.')\n-for subdir, dirs, fnames in os.walk('templates'):\n-    for fname in fnames:\n-        new_subdir = subdir.replace('templates', 'sources')\n-        if not os.path.exists(new_subdir):\n-            os.makedirs(new_subdir)\n-        if fname[-3:] == '.md':\n-            fpath = os.path.join(subdir, fname)\n-            new_fpath = fpath.replace('templates', 'sources')\n-            shutil.copy(fpath, new_fpath)\n-\n-\n def read_file(path):\n     with open(path) as f:\n         return f.read()\n@@ -677,7 +312,27 @@ def read_page_data(page_data, type):\n     return data\n \n \n-if __name__ == '__main__':\n+def generate():\n+    if K.backend() != 'tensorflow':\n+        raise ModuleNotFoundError('The documentation must be built '\n+                                  'with the TensorFlow backend because this '\n+                                  'is the only backend with docstrings.')\n+\n+    print('Cleaning up existing sources directory.')\n+    if os.path.exists('sources'):\n+        shutil.rmtree('sources')\n+\n+    print('Populating sources directory with templates.')\n+    for subdir, dirs, fnames in os.walk('templates'):\n+        for fname in fnames:\n+            new_subdir = subdir.replace('templates', 'sources')\n+            if not os.path.exists(new_subdir):\n+                os.makedirs(new_subdir)\n+            if fname[-3:] == '.md':\n+                fpath = os.path.join(subdir, fname)\n+                new_fpath = fpath.replace('templates', 'sources')\n+                shutil.copy(fpath, new_fpath)\n+\n     readme = read_file('../README.md')\n     index = read_file('templates/index.md')\n     index = index.replace('{{autogenerated}}', readme[readme.find('##'):])\n@@ -749,3 +404,7 @@ if __name__ == '__main__':\n             f.write(mkdown)\n \n     shutil.copyfile('../CONTRIBUTING.md', 'sources/contributing.md')\n+\n+\n+if __name__ == '__main__':\n+    generate()\n\n@@ -0,0 +1,355 @@\n+# -*- coding: utf-8 -*-\r\n+'''\r\n+General documentation architecture:\r\n+\r\n+Home\r\n+Index\r\n+\r\n+- Getting started\r\n+    Getting started with the sequential model\r\n+    Getting started with the functional api\r\n+    FAQ\r\n+\r\n+- Models\r\n+    About Keras models\r\n+        explain when one should use Sequential or functional API\r\n+        explain compilation step\r\n+        explain weight saving, weight loading\r\n+        explain serialization, deserialization\r\n+    Sequential\r\n+    Model (functional API)\r\n+\r\n+- Layers\r\n+    About Keras layers\r\n+        explain common layer functions: get_weights, set_weights, get_config\r\n+        explain input_shape\r\n+        explain usage on non-Keras tensors\r\n+    Core Layers\r\n+    Convolutional Layers\r\n+    Pooling Layers\r\n+    Locally-connected Layers\r\n+    Recurrent Layers\r\n+    Embedding Layers\r\n+    Merge Layers\r\n+    Advanced Activations Layers\r\n+    Normalization Layers\r\n+    Noise Layers\r\n+    Layer Wrappers\r\n+    Writing your own Keras layers\r\n+\r\n+- Preprocessing\r\n+    Sequence Preprocessing\r\n+    Text Preprocessing\r\n+    Image Preprocessing\r\n+\r\n+Losses\r\n+Metrics\r\n+Optimizers\r\n+Activations\r\n+Callbacks\r\n+Datasets\r\n+Applications\r\n+Backend\r\n+Initializers\r\n+Regularizers\r\n+Constraints\r\n+Visualization\r\n+Scikit-learn API\r\n+Utils\r\n+Contributing\r\n+\r\n+'''\r\n+from keras import utils\r\n+from keras import layers\r\n+from keras.layers import advanced_activations\r\n+from keras.layers import noise\r\n+from keras.layers import wrappers\r\n+from keras import initializers\r\n+from keras import optimizers\r\n+from keras import callbacks\r\n+from keras import models\r\n+from keras import losses\r\n+from keras import metrics\r\n+from keras import backend\r\n+from keras import constraints\r\n+from keras import activations\r\n+from keras import preprocessing\r\n+\r\n+\r\n+EXCLUDE = {\r\n+    'Optimizer',\r\n+    'TFOptimizer',\r\n+    'Wrapper',\r\n+    'get_session',\r\n+    'set_session',\r\n+    'CallbackList',\r\n+    'serialize',\r\n+    'deserialize',\r\n+    'get',\r\n+    'set_image_dim_ordering',\r\n+    'normalize_data_format',\r\n+    'image_dim_ordering',\r\n+    'get_variable_shape',\r\n+    'Constraint'\r\n+}\r\n+\r\n+# For each class to document, it is possible to:\r\n+# 1) Document only the class: [classA, classB, ...]\r\n+# 2) Document all its methods: [classA, (classB, \"*\")]\r\n+# 3) Choose which methods to document (methods listed as strings):\r\n+# [classA, (classB, [\"method1\", \"method2\", ...]), ...]\r\n+# 4) Choose which methods to document (methods listed as qualified names):\r\n+# [classA, (classB, [module.classB.method1, module.classB.method2, ...]), ...]\r\n+PAGES = [\r\n+    {\r\n+        'page': 'models/sequential.md',\r\n+        'methods': [\r\n+            models.Sequential.compile,\r\n+            models.Sequential.fit,\r\n+            models.Sequential.evaluate,\r\n+            models.Sequential.predict,\r\n+            models.Sequential.train_on_batch,\r\n+            models.Sequential.test_on_batch,\r\n+            models.Sequential.predict_on_batch,\r\n+            models.Sequential.fit_generator,\r\n+            models.Sequential.evaluate_generator,\r\n+            models.Sequential.predict_generator,\r\n+            models.Sequential.get_layer,\r\n+        ],\r\n+    },\r\n+    {\r\n+        'page': 'models/model.md',\r\n+        'methods': [\r\n+            models.Model.compile,\r\n+            models.Model.fit,\r\n+            models.Model.evaluate,\r\n+            models.Model.predict,\r\n+            models.Model.train_on_batch,\r\n+            models.Model.test_on_batch,\r\n+            models.Model.predict_on_batch,\r\n+            models.Model.fit_generator,\r\n+            models.Model.evaluate_generator,\r\n+            models.Model.predict_generator,\r\n+            models.Model.get_layer,\r\n+        ]\r\n+    },\r\n+    {\r\n+        'page': 'layers/core.md',\r\n+        'classes': [\r\n+            layers.Dense,\r\n+            layers.Activation,\r\n+            layers.Dropout,\r\n+            layers.Flatten,\r\n+            layers.Input,\r\n+            layers.Reshape,\r\n+            layers.Permute,\r\n+            layers.RepeatVector,\r\n+            layers.Lambda,\r\n+            layers.ActivityRegularization,\r\n+            layers.Masking,\r\n+            layers.SpatialDropout1D,\r\n+            layers.SpatialDropout2D,\r\n+            layers.SpatialDropout3D,\r\n+        ],\r\n+    },\r\n+    {\r\n+        'page': 'layers/convolutional.md',\r\n+        'classes': [\r\n+            layers.Conv1D,\r\n+            layers.Conv2D,\r\n+            layers.SeparableConv1D,\r\n+            layers.SeparableConv2D,\r\n+            layers.DepthwiseConv2D,\r\n+            layers.Conv2DTranspose,\r\n+            layers.Conv3D,\r\n+            layers.Conv3DTranspose,\r\n+            layers.Cropping1D,\r\n+            layers.Cropping2D,\r\n+            layers.Cropping3D,\r\n+            layers.UpSampling1D,\r\n+            layers.UpSampling2D,\r\n+            layers.UpSampling3D,\r\n+            layers.ZeroPadding1D,\r\n+            layers.ZeroPadding2D,\r\n+            layers.ZeroPadding3D,\r\n+        ],\r\n+    },\r\n+    {\r\n+        'page': 'layers/pooling.md',\r\n+        'classes': [\r\n+            layers.MaxPooling1D,\r\n+            layers.MaxPooling2D,\r\n+            layers.MaxPooling3D,\r\n+            layers.AveragePooling1D,\r\n+            layers.AveragePooling2D,\r\n+            layers.AveragePooling3D,\r\n+            layers.GlobalMaxPooling1D,\r\n+            layers.GlobalAveragePooling1D,\r\n+            layers.GlobalMaxPooling2D,\r\n+            layers.GlobalAveragePooling2D,\r\n+            layers.GlobalMaxPooling3D,\r\n+            layers.GlobalAveragePooling3D,\r\n+        ],\r\n+    },\r\n+    {\r\n+        'page': 'layers/local.md',\r\n+        'classes': [\r\n+            layers.LocallyConnected1D,\r\n+            layers.LocallyConnected2D,\r\n+        ],\r\n+    },\r\n+    {\r\n+        'page': 'layers/recurrent.md',\r\n+        'classes': [\r\n+            layers.RNN,\r\n+            layers.SimpleRNN,\r\n+            layers.GRU,\r\n+            layers.LSTM,\r\n+            layers.ConvLSTM2D,\r\n+            layers.SimpleRNNCell,\r\n+            layers.GRUCell,\r\n+            layers.LSTMCell,\r\n+            layers.CuDNNGRU,\r\n+            layers.CuDNNLSTM,\r\n+        ],\r\n+    },\r\n+    {\r\n+        'page': 'layers/embeddings.md',\r\n+        'classes': [\r\n+            layers.Embedding,\r\n+        ],\r\n+    },\r\n+    {\r\n+        'page': 'layers/normalization.md',\r\n+        'classes': [\r\n+            layers.BatchNormalization,\r\n+        ],\r\n+    },\r\n+    {\r\n+        'page': 'layers/advanced-activations.md',\r\n+        'all_module_classes': [advanced_activations],\r\n+    },\r\n+    {\r\n+        'page': 'layers/noise.md',\r\n+        'all_module_classes': [noise],\r\n+    },\r\n+    {\r\n+        'page': 'layers/merge.md',\r\n+        'classes': [\r\n+            layers.Add,\r\n+            layers.Subtract,\r\n+            layers.Multiply,\r\n+            layers.Average,\r\n+            layers.Maximum,\r\n+            layers.Concatenate,\r\n+            layers.Dot,\r\n+        ],\r\n+        'functions': [\r\n+            layers.add,\r\n+            layers.subtract,\r\n+            layers.multiply,\r\n+            layers.average,\r\n+            layers.maximum,\r\n+            layers.concatenate,\r\n+            layers.dot,\r\n+        ]\r\n+    },\r\n+    {\r\n+        'page': 'preprocessing/sequence.md',\r\n+        'functions': [\r\n+            preprocessing.sequence.pad_sequences,\r\n+            preprocessing.sequence.skipgrams,\r\n+            preprocessing.sequence.make_sampling_table,\r\n+        ],\r\n+        'classes': [\r\n+            preprocessing.sequence.TimeseriesGenerator,\r\n+        ]\r\n+    },\r\n+    {\r\n+        'page': 'preprocessing/image.md',\r\n+        'classes': [\r\n+            (preprocessing.image.ImageDataGenerator, '*')\r\n+        ]\r\n+    },\r\n+    {\r\n+        'page': 'preprocessing/text.md',\r\n+        'functions': [\r\n+            preprocessing.text.hashing_trick,\r\n+            preprocessing.text.one_hot,\r\n+            preprocessing.text.text_to_word_sequence,\r\n+        ],\r\n+        'classes': [\r\n+            preprocessing.text.Tokenizer,\r\n+        ]\r\n+    },\r\n+    {\r\n+        'page': 'layers/wrappers.md',\r\n+        'all_module_classes': [wrappers],\r\n+    },\r\n+    {\r\n+        'page': 'metrics.md',\r\n+        'all_module_functions': [metrics],\r\n+    },\r\n+    {\r\n+        'page': 'losses.md',\r\n+        'all_module_functions': [losses],\r\n+    },\r\n+    {\r\n+        'page': 'initializers.md',\r\n+        'all_module_functions': [initializers],\r\n+        'all_module_classes': [initializers],\r\n+    },\r\n+    {\r\n+        'page': 'optimizers.md',\r\n+        'all_module_classes': [optimizers],\r\n+    },\r\n+    {\r\n+        'page': 'callbacks.md',\r\n+        'all_module_classes': [callbacks],\r\n+    },\r\n+    {\r\n+        'page': 'activations.md',\r\n+        'all_module_functions': [activations],\r\n+    },\r\n+    {\r\n+        'page': 'backend.md',\r\n+        'all_module_functions': [backend],\r\n+    },\r\n+    {\r\n+        'page': 'constraints.md',\r\n+        'all_module_classes': [constraints],\r\n+    },\r\n+    {\r\n+        'page': 'utils.md',\r\n+        'functions': [utils.to_categorical,\r\n+                      utils.normalize,\r\n+                      utils.get_file,\r\n+                      utils.print_summary,\r\n+                      utils.plot_model,\r\n+                      utils.multi_gpu_model],\r\n+        'classes': [utils.CustomObjectScope,\r\n+                    utils.HDF5Matrix,\r\n+                    utils.Sequence],\r\n+    },\r\n+]\r\n+\r\n+ROOT = 'http://keras.io/'\r\n+\r\n+template_np_implementation = \"\"\"# Numpy implementation\r\n+\r\n+    ```python\r\n+{{code}}\r\n+    ```\r\n+\"\"\"\r\n+\r\n+template_hidden_np_implementation = \"\"\"# Numpy implementation\r\n+\r\n+    <details>\r\n+    <summary>Show the Numpy implementation</summary>\r\n+\r\n+    ```python\r\n+{{code}}\r\n+    ```\r\n+\r\n+    </details>\r\n+\"\"\"\r\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
