{"custom_id": "keras#55640456ff1731d09fd9d7d5f5de0437f33ab8fa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1018 | Lines Deleted: 50 | Files Changed: 8 | Hunks: 41 | Methods Changed: 47 | Complexity Δ (Sum/Max): 63/33 | Churn Δ: 1068 | Churn Cumulative: 8647 | Contributors (this commit): 8 | Commits (past 90d): 82 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -25,8 +25,8 @@ def mean(x, axis=None, keepdims=False):\n     return jnp.mean(x, axis=axis, keepdims=keepdims)\n \n \n-def max(x, axis=None, keepdims=False):\n-    return jnp.max(x, axis=axis, keepdims=keepdims)\n+def max(x, axis=None, keepdims=False, initial=None):\n+    return jnp.max(x, axis=axis, keepdims=keepdims, initial=initial)\n \n \n def ones(shape, dtype=\"float32\"):\n@@ -327,8 +327,8 @@ def meshgrid(*x, indexing=\"xy\"):\n     return jnp.meshgrid(*x, indexing=indexing)\n \n \n-def min(x, axis=None, keepdims=False):\n-    return jnp.min(x, axis=axis, keepdims=keepdims)\n+def min(x, axis=None, keepdims=False, initial=None):\n+    return jnp.min(x, axis=axis, keepdims=keepdims, initial=initial)\n \n \n def minimum(x1, x2):\n\n@@ -26,7 +26,22 @@ def mean(x, axis=None, keepdims=False):\n     return tfnp.mean(x, axis=axis, keepdims=keepdims)\n \n \n-def max(x, axis=None, keepdims=False):\n+def max(x, axis=None, keepdims=False, initial=None):\n+    # The TensorFlow numpy API implementation doesn't support `initial` so we\n+    # handle it manually here.\n+    if initial is not None:\n+        return tf.math.maximum(\n+            tfnp.max(x, axis=axis, keepdims=keepdims), initial\n+        )\n+\n+    # TensorFlow returns -inf by default for an empty list, but for consistency\n+    # with other backends and the numpy API we want to throw in this case.\n+    tf.assert_greater(\n+        size(x),\n+        tf.constant(0, dtype=tf.int64),\n+        message=\"Cannot compute the max of an empty tensor.\",\n+    )\n+\n     return tfnp.max(x, axis=axis, keepdims=keepdims)\n \n \n@@ -328,7 +343,22 @@ def meshgrid(*x, indexing=\"xy\"):\n     return tfnp.meshgrid(*x, indexing=indexing)\n \n \n-def min(x, axis=None, keepdims=False):\n+def min(x, axis=None, keepdims=False, initial=None):\n+    # The TensorFlow numpy API implementation doesn't support `initial` so we\n+    # handle it manually here.\n+    if initial is not None:\n+        return tf.math.minimum(\n+            tfnp.min(x, axis=axis, keepdims=keepdims), initial\n+        )\n+\n+    # TensorFlow returns inf by default for an empty list, but for consistency\n+    # with other backends and the numpy API we want to throw in this case.\n+    tf.assert_greater(\n+        size(x),\n+        tf.constant(0, dtype=tf.int64),\n+        message=\"Cannot compute the min of an empty tensor.\",\n+    )\n+\n     return tfnp.min(x, axis=axis, keepdims=keepdims)\n \n \n\n@@ -8,7 +8,11 @@ from keras_core.metrics.accuracy_metrics import TopKCategoricalAccuracy\n from keras_core.metrics.confusion_metrics import FalseNegatives\n from keras_core.metrics.confusion_metrics import FalsePositives\n from keras_core.metrics.confusion_metrics import Precision\n+from keras_core.metrics.confusion_metrics import PrecisionAtRecall\n from keras_core.metrics.confusion_metrics import Recall\n+from keras_core.metrics.confusion_metrics import RecallAtPrecision\n+from keras_core.metrics.confusion_metrics import SensitivityAtSpecificity\n+from keras_core.metrics.confusion_metrics import SpecificityAtSensitivity\n from keras_core.metrics.confusion_metrics import TrueNegatives\n from keras_core.metrics.confusion_metrics import TruePositives\n from keras_core.metrics.hinge_metrics import CategoricalHinge\n@@ -40,7 +44,11 @@ ALL_OBJECTS = {\n     FalseNegatives,\n     FalsePositives,\n     Precision,\n+    PrecisionAtRecall,\n     Recall,\n+    RecallAtPrecision,\n+    SensitivityAtSpecificity,\n+    SpecificityAtSensitivity,\n     TrueNegatives,\n     TruePositives,\n     # Hinge\n\n@@ -69,7 +69,7 @@ class _ConfusionMatrixConditionCount(Metric):\n     def get_config(self):\n         config = {\"thresholds\": self.init_thresholds}\n         base_config = super().get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n+        return {**base_config, **config}\n \n \n @keras_core_export(\"keras_core.metrics.FalsePositives\")\n@@ -402,7 +402,7 @@ class Precision(Metric):\n             \"class_id\": self.class_id,\n         }\n         base_config = super().get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n+        return {**base_config, **config}\n \n \n @keras_core_export(\"keras_core.metrics.Recall\")\n@@ -543,4 +543,512 @@ class Recall(Metric):\n             \"class_id\": self.class_id,\n         }\n         base_config = super().get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n+        return {**base_config, **config}\n+\n+\n+class SensitivitySpecificityBase(Metric):\n+    \"\"\"Abstract base class for computing sensitivity and specificity.\n+\n+    For additional information about specificity and sensitivity, see\n+    [the following](https://en.wikipedia.org/wiki/Sensitivity_and_specificity).\n+    \"\"\"\n+\n+    def __init__(\n+        self, value, num_thresholds=200, class_id=None, name=None, dtype=None\n+    ):\n+        super().__init__(name=name, dtype=dtype)\n+        if num_thresholds <= 0:\n+            raise ValueError(\n+                \"Argument `num_thresholds` must be an integer > 0. \"\n+                f\"Received: num_thresholds={num_thresholds}\"\n+            )\n+        self.value = value\n+        self.class_id = class_id\n+\n+        # Compute `num_thresholds` thresholds in [0, 1]\n+        if num_thresholds == 1:\n+            self.thresholds = [0.5]\n+            self._thresholds_distributed_evenly = False\n+        else:\n+            thresholds = [\n+                (i + 1) * 1.0 / (num_thresholds - 1)\n+                for i in range(num_thresholds - 2)\n+            ]\n+            self.thresholds = [0.0] + thresholds + [1.0]\n+            self._thresholds_distributed_evenly = True\n+\n+        self.true_positives = self.add_variable(\n+            shape=(len(self.thresholds),),\n+            initializer=initializers.Zeros(),\n+            name=\"true_positives\",\n+        )\n+        self.false_positives = self.add_variable(\n+            shape=(len(self.thresholds),),\n+            initializer=initializers.Zeros(),\n+            name=\"false_positives\",\n+        )\n+        self.true_negatives = self.add_variable(\n+            shape=(len(self.thresholds),),\n+            initializer=initializers.Zeros(),\n+            name=\"true_negatives\",\n+        )\n+        self.false_negatives = self.add_variable(\n+            shape=(len(self.thresholds),),\n+            initializer=initializers.Zeros(),\n+            name=\"false_negatives\",\n+        )\n+\n+    def update_state(self, y_true, y_pred, sample_weight=None):\n+        \"\"\"Accumulates confusion matrix statistics.\n+\n+        Args:\n+            y_true: The ground truth values.\n+            y_pred: The predicted values.\n+            sample_weight: Optional weighting of each example. Defaults to 1.\n+                Can be a `Tensor` whose rank is either 0, or the same rank as\n+                `y_true`, and must be broadcastable to `y_true`.\n+        \"\"\"\n+        metrics_utils.update_confusion_matrix_variables(\n+            {\n+                metrics_utils.ConfusionMatrix.TRUE_POSITIVES: self.true_positives,  # noqa: E501\n+                metrics_utils.ConfusionMatrix.TRUE_NEGATIVES: self.true_negatives,  # noqa: E501\n+                metrics_utils.ConfusionMatrix.FALSE_POSITIVES: self.false_positives,  # noqa: E501\n+                metrics_utils.ConfusionMatrix.FALSE_NEGATIVES: self.false_negatives,  # noqa: E501\n+            },\n+            y_true,\n+            y_pred,\n+            thresholds=self.thresholds,\n+            thresholds_distributed_evenly=self._thresholds_distributed_evenly,\n+            class_id=self.class_id,\n+            sample_weight=sample_weight,\n+        )\n+\n+    def reset_state(self):\n+        num_thresholds = len(self.thresholds)\n+        self.true_positives.assign(ops.zeros((num_thresholds,)))\n+        self.false_positives.assign(ops.zeros((num_thresholds,)))\n+        self.true_negatives.assign(ops.zeros((num_thresholds,)))\n+        self.false_negatives.assign(ops.zeros((num_thresholds,)))\n+\n+    def get_config(self):\n+        config = {\"class_id\": self.class_id}\n+        base_config = super().get_config()\n+        return {**base_config, **config}\n+\n+    def _find_max_under_constraint(self, constrained, dependent, predicate):\n+        \"\"\"Returns the maximum of dependent_statistic that satisfies the\n+        constraint.\n+\n+        Args:\n+            constrained: Over these values the constraint is specified. A rank-1\n+                tensor.\n+            dependent: From these values the maximum that satiesfies the\n+                constraint is selected. Values in this tensor and in\n+                `constrained` are linked by having the same threshold at each\n+                position, hence this tensor must have the same shape.\n+            predicate: A binary boolean functor to be applied to arguments\n+                `constrained` and `self.value`, e.g. `ops.greater`.\n+\n+        Returns:\n+            maximal dependent value, if no value satisfies the constraint 0.0.\n+        \"\"\"\n+        feasible = ops.array(ops.nonzero(predicate(constrained, self.value)))\n+\n+        print(feasible)\n+        feasible_exists = ops.greater(ops.size(feasible), 0)\n+        max_dependent = ops.max(ops.take(dependent, feasible), initial=0)\n+\n+        return ops.where(feasible_exists, max_dependent, 0.0)\n+\n+\n+@keras_core_export(\"keras_core.metrics.SensitivityAtSpecificity\")\n+class SensitivityAtSpecificity(SensitivitySpecificityBase):\n+    \"\"\"Computes best sensitivity where specificity is >= specified value.\n+\n+    `Sensitivity` measures the proportion of actual positives that are correctly\n+    identified as such `(tp / (tp + fn))`.\n+    `Specificity` measures the proportion of actual negatives that are correctly\n+    identified as such `(tn / (tn + fp))`.\n+\n+    This metric creates four local variables, `true_positives`,\n+    `true_negatives`, `false_positives` and `false_negatives` that are used to\n+    compute the sensitivity at the given specificity. The threshold for the\n+    given specificity value is computed and used to evaluate the corresponding\n+    sensitivity.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    If `class_id` is specified, we calculate precision by considering only the\n+    entries in the batch for which `class_id` is above the threshold\n+    predictions, and computing the fraction of them for which `class_id` is\n+    indeed a correct label.\n+\n+    For additional information about specificity and sensitivity, see\n+    [the following](https://en.wikipedia.org/wiki/Sensitivity_and_specificity).\n+\n+    Args:\n+        specificity: A scalar value in range `[0, 1]`.\n+        num_thresholds: (Optional) Defaults to 200. The number of thresholds to\n+            use for matching the given specificity.\n+        class_id: (Optional) Integer class ID for which we want binary metrics.\n+            This must be in the half-open interval `[0, num_classes)`, where\n+            `num_classes` is the last dimension of predictions.\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = keras_core.metrics.SensitivityAtSpecificity(0.5)\n+    >>> m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])\n+    >>> m.result()\n+    0.5\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8],\n+    ...                sample_weight=[1, 1, 2, 2, 1])\n+    >>> m.result()\n+    0.333333\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(\n+        optimizer='sgd',\n+        loss='mse',\n+        metrics=[keras_core.metrics.SensitivityAtSpecificity()])\n+    ```\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        specificity,\n+        num_thresholds=200,\n+        class_id=None,\n+        name=None,\n+        dtype=None,\n+    ):\n+        if specificity < 0 or specificity > 1:\n+            raise ValueError(\n+                \"Argument `specificity` must be in the range [0, 1]. \"\n+                f\"Received: specificity={specificity}\"\n+            )\n+        self.specificity = specificity\n+        self.num_thresholds = num_thresholds\n+        super().__init__(\n+            specificity,\n+            num_thresholds=num_thresholds,\n+            class_id=class_id,\n+            name=name,\n+            dtype=dtype,\n+        )\n+\n+    def result(self):\n+        sensitivities = ops.divide(\n+            self.true_positives,\n+            self.true_positives + self.false_negatives + backend.epsilon(),\n+        )\n+        specificities = ops.divide(\n+            self.true_negatives,\n+            self.true_negatives + self.false_positives + backend.epsilon(),\n+        )\n+        return self._find_max_under_constraint(\n+            specificities, sensitivities, ops.greater_equal\n+        )\n+\n+    def get_config(self):\n+        config = {\n+            \"num_thresholds\": self.num_thresholds,\n+            \"specificity\": self.specificity,\n+        }\n+        base_config = super().get_config()\n+        return {**base_config, **config}\n+\n+\n+@keras_core_export(\"keras_core.metrics.SpecificityAtSensitivity\")\n+class SpecificityAtSensitivity(SensitivitySpecificityBase):\n+    \"\"\"Computes best specificity where sensitivity is >= specified value.\n+\n+    `Sensitivity` measures the proportion of actual positives that are correctly\n+    identified as such `(tp / (tp + fn))`.\n+    `Specificity` measures the proportion of actual negatives that are correctly\n+    identified as such `(tn / (tn + fp))`.\n+\n+    This metric creates four local variables, `true_positives`,\n+    `true_negatives`, `false_positives` and `false_negatives` that are used to\n+    compute the specificity at the given sensitivity. The threshold for the\n+    given sensitivity value is computed and used to evaluate the corresponding\n+    specificity.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    If `class_id` is specified, we calculate precision by considering only the\n+    entries in the batch for which `class_id` is above the threshold\n+    predictions, and computing the fraction of them for which `class_id` is\n+    indeed a correct label.\n+\n+    For additional information about specificity and sensitivity, see\n+    [the following](https://en.wikipedia.org/wiki/Sensitivity_and_specificity).\n+\n+    Args:\n+        sensitivity: A scalar value in range `[0, 1]`.\n+        num_thresholds: (Optional) Defaults to 200. The number of thresholds to\n+            use for matching the given sensitivity.\n+        class_id: (Optional) Integer class ID for which we want binary metrics.\n+            This must be in the half-open interval `[0, num_classes)`, where\n+            `num_classes` is the last dimension of predictions.\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = keras_core.metrics.SpecificityAtSensitivity(0.5)\n+    >>> m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])\n+    >>> m.result()\n+    0.66666667\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8],\n+    ...                sample_weight=[1, 1, 2, 2, 2])\n+    >>> m.result()\n+    0.5\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(\n+        optimizer='sgd',\n+        loss='mse',\n+        metrics=[keras_core.metrics.SpecificityAtSensitivity()])\n+    ```\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        sensitivity,\n+        num_thresholds=200,\n+        class_id=None,\n+        name=None,\n+        dtype=None,\n+    ):\n+        if sensitivity < 0 or sensitivity > 1:\n+            raise ValueError(\n+                \"Argument `sensitivity` must be in the range [0, 1]. \"\n+                f\"Received: sensitivity={sensitivity}\"\n+            )\n+        self.sensitivity = sensitivity\n+        self.num_thresholds = num_thresholds\n+        super().__init__(\n+            sensitivity,\n+            num_thresholds=num_thresholds,\n+            class_id=class_id,\n+            name=name,\n+            dtype=dtype,\n+        )\n+\n+    def result(self):\n+        sensitivities = ops.divide(\n+            self.true_positives,\n+            self.true_positives + self.false_negatives + backend.epsilon(),\n+        )\n+        specificities = ops.divide(\n+            self.true_negatives,\n+            self.true_negatives + self.false_positives + backend.epsilon(),\n+        )\n+        return self._find_max_under_constraint(\n+            sensitivities, specificities, ops.greater_equal\n+        )\n+\n+    def get_config(self):\n+        config = {\n+            \"num_thresholds\": self.num_thresholds,\n+            \"sensitivity\": self.sensitivity,\n+        }\n+        base_config = super().get_config()\n+        return {**base_config, **config}\n+\n+\n+@keras_core_export(\"keras_core.metrics.PrecisionAtRecall\")\n+class PrecisionAtRecall(SensitivitySpecificityBase):\n+    \"\"\"Computes best precision where recall is >= specified value.\n+\n+    This metric creates four local variables, `true_positives`,\n+    `true_negatives`, `false_positives` and `false_negatives` that are used to\n+    compute the precision at the given recall. The threshold for the given\n+    recall value is computed and used to evaluate the corresponding precision.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    If `class_id` is specified, we calculate precision by considering only the\n+    entries in the batch for which `class_id` is above the threshold\n+    predictions, and computing the fraction of them for which `class_id` is\n+    indeed a correct label.\n+\n+    Args:\n+        recall: A scalar value in range `[0, 1]`.\n+        num_thresholds: (Optional) Defaults to 200. The number of thresholds to\n+            use for matching the given recall.\n+        class_id: (Optional) Integer class ID for which we want binary metrics.\n+            This must be in the half-open interval `[0, num_classes)`, where\n+            `num_classes` is the last dimension of predictions.\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = keras_core.metrics.PrecisionAtRecall(0.5)\n+    >>> m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])\n+    >>> m.result()\n+    0.5\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8],\n+    ...                sample_weight=[2, 2, 2, 1, 1])\n+    >>> m.result()\n+    0.33333333\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(\n+        optimizer='sgd',\n+        loss='mse',\n+        metrics=[keras_core.metrics.PrecisionAtRecall(recall=0.8)])\n+    ```\n+    \"\"\"\n+\n+    def __init__(\n+        self, recall, num_thresholds=200, class_id=None, name=None, dtype=None\n+    ):\n+        if recall < 0 or recall > 1:\n+            raise ValueError(\n+                \"Argument `recall` must be in the range [0, 1]. \"\n+                f\"Received: recall={recall}\"\n+            )\n+        self.recall = recall\n+        self.num_thresholds = num_thresholds\n+        super().__init__(\n+            value=recall,\n+            num_thresholds=num_thresholds,\n+            class_id=class_id,\n+            name=name,\n+            dtype=dtype,\n+        )\n+\n+    def result(self):\n+        recalls = ops.divide(\n+            self.true_positives,\n+            self.true_positives + self.false_negatives + backend.epsilon(),\n+        )\n+        precisions = ops.divide(\n+            self.true_positives,\n+            self.true_positives + self.false_positives + backend.epsilon(),\n+        )\n+        return self._find_max_under_constraint(\n+            recalls, precisions, ops.greater_equal\n+        )\n+\n+    def get_config(self):\n+        config = {\"num_thresholds\": self.num_thresholds, \"recall\": self.recall}\n+        base_config = super().get_config()\n+        return {**base_config, **config}\n+\n+\n+@keras_core_export(\"keras_core.metrics.RecallAtPrecision\")\n+class RecallAtPrecision(SensitivitySpecificityBase):\n+    \"\"\"Computes best recall where precision is >= specified value.\n+\n+    For a given score-label-distribution the required precision might not\n+    be achievable, in this case 0.0 is returned as recall.\n+\n+    This metric creates four local variables, `true_positives`,\n+    `true_negatives`, `false_positives` and `false_negatives` that are used to\n+    compute the recall at the given precision. The threshold for the given\n+    precision value is computed and used to evaluate the corresponding recall.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    If `class_id` is specified, we calculate precision by considering only the\n+    entries in the batch for which `class_id` is above the threshold\n+    predictions, and computing the fraction of them for which `class_id` is\n+    indeed a correct label.\n+\n+    Args:\n+        precision: A scalar value in range `[0, 1]`.\n+            num_thresholds: (Optional) Defaults to 200. The number of thresholds\n+            to use for matching the given precision.\n+        class_id: (Optional) Integer class ID for which we want binary metrics.\n+            This must be in the half-open interval `[0, num_classes)`, where\n+            `num_classes` is the last dimension of predictions.\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = keras_core.metrics.RecallAtPrecision(0.8)\n+    >>> m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])\n+    >>> m.result()\n+    0.5\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9],\n+    ...                sample_weight=[1, 0, 0, 1])\n+    >>> m.result()\n+    1.0\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(\n+        optimizer='sgd',\n+        loss='mse',\n+        metrics=[keras_core.metrics.RecallAtPrecision(precision=0.8)])\n+    ```\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        precision,\n+        num_thresholds=200,\n+        class_id=None,\n+        name=None,\n+        dtype=None,\n+    ):\n+        if precision < 0 or precision > 1:\n+            raise ValueError(\n+                \"Argument `precision` must be in the range [0, 1]. \"\n+                f\"Received: precision={precision}\"\n+            )\n+        self.precision = precision\n+        self.num_thresholds = num_thresholds\n+        super().__init__(\n+            value=precision,\n+            num_thresholds=num_thresholds,\n+            class_id=class_id,\n+            name=name,\n+            dtype=dtype,\n+        )\n+\n+    def result(self):\n+        recalls = ops.divide(\n+            self.true_positives,\n+            self.true_positives + self.false_negatives + backend.epsilon(),\n+        )\n+        precisions = ops.divide(\n+            self.true_positives,\n+            self.true_positives + self.false_positives + backend.epsilon(),\n+        )\n+        return self._find_max_under_constraint(\n+            precisions, recalls, ops.greater_equal\n+        )\n+\n+    def get_config(self):\n+        config = {\n+            \"num_thresholds\": self.num_thresholds,\n+            \"precision\": self.precision,\n+        }\n+        base_config = super().get_config()\n+        return {**base_config, **config}\n\n@@ -1,7 +1,9 @@\n import numpy as np\n+from absl.testing import parameterized\n from tensorflow.python.ops.numpy_ops import np_config\n \n from keras_core import metrics\n+from keras_core import operations as ops\n from keras_core import testing\n \n # TODO: remove reliance on this (or alternatively, turn it on by default).\n@@ -689,3 +691,413 @@ class RecallTest(testing.TestCase):\n         self.assertAlmostEqual(0.25, r_obj(y_true, y_pred))\n         self.assertAlmostEqual(1, r_obj.true_positives)\n         self.assertAlmostEqual(3, r_obj.false_negatives)\n+\n+\n+class SensitivityAtSpecificityTest(testing.TestCase, parameterized.TestCase):\n+    def test_config(self):\n+        s_obj = metrics.SensitivityAtSpecificity(\n+            0.4,\n+            num_thresholds=100,\n+            class_id=12,\n+            name=\"sensitivity_at_specificity_1\",\n+        )\n+        self.assertEqual(s_obj.name, \"sensitivity_at_specificity_1\")\n+        self.assertLen(s_obj.variables, 4)\n+        self.assertEqual(s_obj.specificity, 0.4)\n+        self.assertEqual(s_obj.num_thresholds, 100)\n+        self.assertEqual(s_obj.class_id, 12)\n+\n+        # Check save and restore config\n+        s_obj2 = metrics.SensitivityAtSpecificity.from_config(\n+            s_obj.get_config()\n+        )\n+        self.assertEqual(s_obj2.name, \"sensitivity_at_specificity_1\")\n+        self.assertLen(s_obj2.variables, 4)\n+        self.assertEqual(s_obj2.specificity, 0.4)\n+        self.assertEqual(s_obj2.num_thresholds, 100)\n+        self.assertEqual(s_obj.class_id, 12)\n+\n+    def test_unweighted_all_correct(self):\n+        s_obj = metrics.SensitivityAtSpecificity(0.7)\n+        inputs = np.random.randint(0, 2, size=(100, 1))\n+        y_pred = np.array(inputs, dtype=\"float32\")\n+        y_true = np.array(inputs)\n+        self.assertAlmostEqual(1, s_obj(y_true, y_pred))\n+\n+    def test_unweighted_high_specificity(self):\n+        s_obj = metrics.SensitivityAtSpecificity(0.8)\n+        pred_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.1, 0.45, 0.5, 0.8, 0.9]\n+        label_values = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n+\n+        y_pred = np.array(pred_values, dtype=\"float32\")\n+        y_true = np.array(label_values)\n+\n+        self.assertAlmostEqual(0.8, s_obj(y_true, y_pred))\n+\n+    def test_unweighted_low_specificity(self):\n+        s_obj = metrics.SensitivityAtSpecificity(0.4)\n+        pred_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.01, 0.02, 0.25, 0.26, 0.26]\n+        label_values = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n+\n+        y_pred = np.array(pred_values, dtype=\"float32\")\n+        y_true = np.array(label_values)\n+\n+        self.assertAlmostEqual(0.6, s_obj(y_true, y_pred))\n+\n+    def test_unweighted_class_id(self):\n+        s_obj = metrics.SpecificityAtSensitivity(0.4, class_id=2)\n+        pred_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.01, 0.02, 0.25, 0.26, 0.26]\n+        label_values = [0, 0, 0, 0, 0, 2, 2, 2, 2, 2]\n+\n+        y_pred = ops.transpose(np.array([pred_values] * 3))\n+        y_true = ops.one_hot(label_values, num_classes=3)\n+\n+        self.assertAlmostEqual(0.6, s_obj(y_true, y_pred))\n+\n+    @parameterized.parameters([\"bool\", \"int32\", \"float32\"])\n+    def test_weighted(self, label_dtype):\n+        s_obj = metrics.SensitivityAtSpecificity(0.4)\n+        pred_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.01, 0.02, 0.25, 0.26, 0.26]\n+        label_values = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n+        weight_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n+\n+        y_pred = np.array(pred_values, dtype=\"float32\")\n+        y_true = ops.cast(label_values, dtype=label_dtype)\n+        weights = np.array(weight_values)\n+\n+        result = s_obj(y_true, y_pred, sample_weight=weights)\n+        self.assertAlmostEqual(0.675, result)\n+\n+    def test_invalid_specificity(self):\n+        with self.assertRaisesRegex(\n+            ValueError, r\"`specificity` must be in the range \\[0, 1\\].\"\n+        ):\n+            metrics.SensitivityAtSpecificity(-1)\n+\n+    def test_invalid_num_thresholds(self):\n+        with self.assertRaisesRegex(\n+            ValueError, \"Argument `num_thresholds` must be an integer > 0\"\n+        ):\n+            metrics.SensitivityAtSpecificity(0.4, num_thresholds=-1)\n+\n+\n+class SpecificityAtSensitivityTest(testing.TestCase, parameterized.TestCase):\n+    def test_config(self):\n+        s_obj = metrics.SpecificityAtSensitivity(\n+            0.4,\n+            num_thresholds=100,\n+            class_id=12,\n+            name=\"specificity_at_sensitivity_1\",\n+        )\n+        self.assertEqual(s_obj.name, \"specificity_at_sensitivity_1\")\n+        self.assertLen(s_obj.variables, 4)\n+        self.assertEqual(s_obj.sensitivity, 0.4)\n+        self.assertEqual(s_obj.num_thresholds, 100)\n+        self.assertEqual(s_obj.class_id, 12)\n+\n+        # Check save and restore config\n+        s_obj2 = metrics.SpecificityAtSensitivity.from_config(\n+            s_obj.get_config()\n+        )\n+        self.assertEqual(s_obj2.name, \"specificity_at_sensitivity_1\")\n+        self.assertLen(s_obj2.variables, 4)\n+        self.assertEqual(s_obj2.sensitivity, 0.4)\n+        self.assertEqual(s_obj2.num_thresholds, 100)\n+        self.assertEqual(s_obj.class_id, 12)\n+\n+    def test_unweighted_all_correct(self):\n+        s_obj = metrics.SpecificityAtSensitivity(0.7)\n+        inputs = np.random.randint(0, 2, size=(100, 1))\n+        y_pred = np.array(inputs, dtype=\"float32\")\n+        y_true = np.array(inputs)\n+\n+        self.assertAlmostEqual(1, s_obj(y_true, y_pred))\n+\n+    def test_unweighted_high_sensitivity(self):\n+        s_obj = metrics.SpecificityAtSensitivity(1.0)\n+        pred_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.01, 0.02, 0.25, 0.26, 0.26]\n+        label_values = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n+\n+        y_pred = np.array(pred_values, dtype=\"float32\")\n+        y_true = np.array(label_values)\n+\n+        self.assertAlmostEqual(0.2, s_obj(y_true, y_pred))\n+\n+    def test_unweighted_low_sensitivity(self):\n+        s_obj = metrics.SpecificityAtSensitivity(0.4)\n+        pred_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.01, 0.02, 0.25, 0.26, 0.26]\n+        label_values = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n+\n+        y_pred = np.array(pred_values, dtype=\"float32\")\n+        y_true = np.array(label_values)\n+\n+        self.assertAlmostEqual(0.6, s_obj(y_true, y_pred))\n+\n+    def test_unweighted_class_id(self):\n+        s_obj = metrics.SpecificityAtSensitivity(0.4, class_id=2)\n+        pred_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.01, 0.02, 0.25, 0.26, 0.26]\n+        label_values = [0, 0, 0, 0, 0, 2, 2, 2, 2, 2]\n+\n+        y_pred = ops.transpose(np.array([pred_values] * 3))\n+        y_true = ops.one_hot(label_values, num_classes=3)\n+\n+        self.assertAlmostEqual(0.6, s_obj(y_true, y_pred))\n+\n+    @parameterized.parameters([\"bool\", \"int32\", \"float32\"])\n+    def test_weighted(self, label_dtype):\n+        s_obj = metrics.SpecificityAtSensitivity(0.4)\n+        pred_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.01, 0.02, 0.25, 0.26, 0.26]\n+        label_values = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n+        weight_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n+\n+        y_pred = np.array(pred_values, dtype=\"float32\")\n+        y_true = ops.cast(label_values, dtype=label_dtype)\n+        weights = np.array(weight_values)\n+\n+        result = s_obj(y_true, y_pred, sample_weight=weights)\n+        self.assertAlmostEqual(0.4, result)\n+\n+    def test_invalid_sensitivity(self):\n+        with self.assertRaisesRegex(\n+            ValueError, r\"`sensitivity` must be in the range \\[0, 1\\].\"\n+        ):\n+            metrics.SpecificityAtSensitivity(-1)\n+\n+    def test_invalid_num_thresholds(self):\n+        with self.assertRaisesRegex(\n+            ValueError, \"Argument `num_thresholds` must be an integer > 0\"\n+        ):\n+            metrics.SpecificityAtSensitivity(0.4, num_thresholds=-1)\n+\n+\n+class PrecisionAtRecallTest(testing.TestCase, parameterized.TestCase):\n+    def test_config(self):\n+        s_obj = metrics.PrecisionAtRecall(\n+            0.4, num_thresholds=100, class_id=12, name=\"precision_at_recall_1\"\n+        )\n+        self.assertEqual(s_obj.name, \"precision_at_recall_1\")\n+        self.assertLen(s_obj.variables, 4)\n+        self.assertEqual(s_obj.recall, 0.4)\n+        self.assertEqual(s_obj.num_thresholds, 100)\n+        self.assertEqual(s_obj.class_id, 12)\n+\n+        # Check save and restore config\n+        s_obj2 = metrics.PrecisionAtRecall.from_config(s_obj.get_config())\n+        self.assertEqual(s_obj2.name, \"precision_at_recall_1\")\n+        self.assertLen(s_obj2.variables, 4)\n+        self.assertEqual(s_obj2.recall, 0.4)\n+        self.assertEqual(s_obj2.num_thresholds, 100)\n+        self.assertEqual(s_obj.class_id, 12)\n+\n+    def test_unweighted_all_correct(self):\n+        s_obj = metrics.PrecisionAtRecall(0.7)\n+        inputs = np.random.randint(0, 2, size=(100, 1))\n+        y_pred = np.array(inputs, dtype=\"float32\")\n+        y_true = np.array(inputs)\n+\n+        self.assertAlmostEqual(1, s_obj(y_true, y_pred))\n+\n+    def test_unweighted_high_recall(self):\n+        s_obj = metrics.PrecisionAtRecall(0.8)\n+        pred_values = [0.0, 0.1, 0.2, 0.5, 0.6, 0.2, 0.5, 0.6, 0.8, 0.9]\n+        label_values = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n+\n+        y_pred = np.array(pred_values, dtype=\"float32\")\n+        y_true = np.array(label_values)\n+\n+        # For 0.5 < decision threshold < 0.6.\n+        self.assertAlmostEqual(2.0 / 3, s_obj(y_true, y_pred))\n+\n+    def test_unweighted_low_recall(self):\n+        s_obj = metrics.PrecisionAtRecall(0.6)\n+        pred_values = [0.0, 0.1, 0.2, 0.5, 0.6, 0.2, 0.5, 0.6, 0.8, 0.9]\n+        label_values = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n+\n+        y_pred = np.array(pred_values, dtype=\"float32\")\n+        y_true = np.array(label_values)\n+\n+        # For 0.2 < decision threshold < 0.5.\n+        self.assertAlmostEqual(0.75, s_obj(y_true, y_pred))\n+\n+    def test_unweighted_class_id(self):\n+        s_obj = metrics.PrecisionAtRecall(0.6, class_id=2)\n+        pred_values = [0.0, 0.1, 0.2, 0.5, 0.6, 0.2, 0.5, 0.6, 0.8, 0.9]\n+        label_values = [0, 0, 0, 0, 0, 2, 2, 2, 2, 2]\n+\n+        y_pred = ops.transpose(np.array([pred_values] * 3))\n+        y_true = ops.one_hot(label_values, num_classes=3)\n+\n+        # For 0.2 < decision threshold < 0.5.\n+        self.assertAlmostEqual(0.75, s_obj(y_true, y_pred))\n+\n+    @parameterized.parameters([\"bool\", \"int32\", \"float32\"])\n+    def test_weighted(self, label_dtype):\n+        s_obj = metrics.PrecisionAtRecall(7.0 / 8)\n+        pred_values = [0.0, 0.1, 0.2, 0.5, 0.6, 0.2, 0.5, 0.6, 0.8, 0.9]\n+        label_values = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n+        weight_values = [2, 1, 2, 1, 2, 1, 2, 2, 1, 2]\n+\n+        y_pred = np.array(pred_values, dtype=\"float32\")\n+        y_true = ops.cast(label_values, dtype=label_dtype)\n+        weights = np.array(weight_values)\n+\n+        result = s_obj(y_true, y_pred, sample_weight=weights)\n+        # For 0.0 < decision threshold < 0.2.\n+        self.assertAlmostEqual(0.7, result)\n+\n+    def test_invalid_sensitivity(self):\n+        with self.assertRaisesRegex(\n+            ValueError, r\"`recall` must be in the range \\[0, 1\\].\"\n+        ):\n+            metrics.PrecisionAtRecall(-1)\n+\n+    def test_invalid_num_thresholds(self):\n+        with self.assertRaisesRegex(\n+            ValueError, \"Argument `num_thresholds` must be an integer > 0\"\n+        ):\n+            metrics.PrecisionAtRecall(0.4, num_thresholds=-1)\n+\n+\n+class RecallAtPrecisionTest(testing.TestCase, parameterized.TestCase):\n+    def test_config(self):\n+        s_obj = metrics.RecallAtPrecision(\n+            0.4, num_thresholds=100, class_id=12, name=\"recall_at_precision_1\"\n+        )\n+        self.assertEqual(s_obj.name, \"recall_at_precision_1\")\n+        self.assertLen(s_obj.variables, 4)\n+        self.assertEqual(s_obj.precision, 0.4)\n+        self.assertEqual(s_obj.num_thresholds, 100)\n+        self.assertEqual(s_obj.class_id, 12)\n+\n+        # Check save and restore config\n+        s_obj2 = metrics.RecallAtPrecision.from_config(s_obj.get_config())\n+        self.assertEqual(s_obj2.name, \"recall_at_precision_1\")\n+        self.assertLen(s_obj2.variables, 4)\n+        self.assertEqual(s_obj2.precision, 0.4)\n+        self.assertEqual(s_obj2.num_thresholds, 100)\n+        self.assertEqual(s_obj.class_id, 12)\n+\n+    def test_unweighted_all_correct(self):\n+        s_obj = metrics.RecallAtPrecision(0.7)\n+        inputs = np.random.randint(0, 2, size=(100, 1))\n+        y_pred = np.array(inputs, dtype=\"float32\")\n+        y_true = np.array(inputs)\n+\n+        self.assertAlmostEqual(1, s_obj(y_true, y_pred))\n+\n+    def test_unweighted_high_precision(self):\n+        s_obj = metrics.RecallAtPrecision(0.75)\n+        pred_values = [\n+            0.05,\n+            0.1,\n+            0.2,\n+            0.3,\n+            0.3,\n+            0.35,\n+            0.4,\n+            0.45,\n+            0.5,\n+            0.6,\n+            0.9,\n+            0.95,\n+        ]\n+        label_values = [0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1]\n+        # precisions: [1/2, 6/11, 1/2, 5/9, 5/8, 5/7, 2/3, 3/5, 3/5, 2/3, 1/2,\n+        # 1].\n+        # recalls:    [1,   1,    5/6, 5/6, 5/6, 5/6, 2/3, 1/2, 1/2, 1/3, 1/6,\n+        # 1/6].\n+        y_pred = np.array(pred_values, dtype=\"float32\")\n+        y_true = np.array(label_values)\n+\n+        # The precision 0.75 can be reached at thresholds 0.4<=t<0.45.\n+        self.assertAlmostEqual(0.5, s_obj(y_true, y_pred))\n+\n+    def test_unweighted_low_precision(self):\n+        s_obj = metrics.RecallAtPrecision(2.0 / 3)\n+        pred_values = [\n+            0.05,\n+            0.1,\n+            0.2,\n+            0.3,\n+            0.3,\n+            0.35,\n+            0.4,\n+            0.45,\n+            0.5,\n+            0.6,\n+            0.9,\n+            0.95,\n+        ]\n+        label_values = [0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1]\n+        # precisions: [1/2, 6/11, 1/2, 5/9, 5/8, 5/7, 2/3, 3/5, 3/5, 2/3, 1/2,\n+        # 1].\n+        # recalls:    [1,   1,    5/6, 5/6, 5/6, 5/6, 2/3, 1/2, 1/2, 1/3, 1/6,\n+        # 1/6].\n+        y_pred = np.array(pred_values, dtype=\"float32\")\n+        y_true = np.array(label_values)\n+\n+        # The precision 5/7 can be reached at thresholds 00.3<=t<0.35.\n+        self.assertAlmostEqual(5.0 / 6, s_obj(y_true, y_pred))\n+\n+    def test_unweighted_class_id(self):\n+        s_obj = metrics.RecallAtPrecision(2.0 / 3, class_id=2)\n+        pred_values = [\n+            0.05,\n+            0.1,\n+            0.2,\n+            0.3,\n+            0.3,\n+            0.35,\n+            0.4,\n+            0.45,\n+            0.5,\n+            0.6,\n+            0.9,\n+            0.95,\n+        ]\n+        label_values = [0, 2, 0, 0, 0, 2, 2, 0, 2, 2, 0, 2]\n+        # precisions: [1/2, 6/11, 1/2, 5/9, 5/8, 5/7, 2/3, 3/5, 3/5, 2/3, 1/2,\n+        # 1].\n+        # recalls:    [1,   1,    5/6, 5/6, 5/6, 5/6, 2/3, 1/2, 1/2, 1/3, 1/6,\n+        # 1/6].\n+        y_pred = ops.transpose(np.array([pred_values] * 3))\n+        y_true = ops.one_hot(label_values, num_classes=3)\n+\n+        # The precision 5/7 can be reached at thresholds 00.3<=t<0.35.\n+        self.assertAlmostEqual(5.0 / 6, s_obj(y_true, y_pred))\n+\n+    @parameterized.parameters([\"bool\", \"int32\", \"float32\"])\n+    def test_weighted(self, label_dtype):\n+        s_obj = metrics.RecallAtPrecision(0.75)\n+        pred_values = [0.1, 0.2, 0.3, 0.5, 0.6, 0.9, 0.9]\n+        label_values = [0, 1, 0, 0, 0, 1, 1]\n+        weight_values = [1, 2, 1, 2, 1, 2, 1]\n+        y_pred = np.array(pred_values, dtype=\"float32\")\n+        y_true = ops.cast(label_values, dtype=label_dtype)\n+        weights = np.array(weight_values)\n+\n+        result = s_obj(y_true, y_pred, sample_weight=weights)\n+        self.assertAlmostEqual(0.6, result)\n+\n+    def test_unachievable_precision(self):\n+        s_obj = metrics.RecallAtPrecision(2.0 / 3)\n+        pred_values = [0.1, 0.2, 0.3, 0.9]\n+        label_values = [1, 1, 0, 0]\n+        y_pred = np.array(pred_values, dtype=\"float32\")\n+        y_true = np.array(label_values)\n+\n+        # The highest possible precision is 1/2 which is below the required\n+        # value, expect 0 recall.\n+        self.assertAlmostEqual(0, s_obj(y_true, y_pred))\n+\n+    def test_invalid_sensitivity(self):\n+        with self.assertRaisesRegex(\n+            ValueError, r\"`precision` must be in the range \\[0, 1\\].\"\n+        ):\n+            metrics.RecallAtPrecision(-1)\n+\n+    def test_invalid_num_thresholds(self):\n+        with self.assertRaisesRegex(\n+            ValueError, \"Argument `num_thresholds` must be an integer > 0\"\n+        ):\n+            metrics.RecallAtPrecision(0.4, num_thresholds=-1)\n\n@@ -74,8 +74,7 @@ def _update_confusion_matrix_variables_optimized(\n       thresholds = [0.0, 1*width, 2*width, 3*width, ..., 1.0]\n     Given a prediction value p, we can map it to its bucket by\n       bucket_index(p) = floor( p * (num_thresholds - 1) )\n-    so we can use ops.unsorted_segmenet_sum() to update the buckets in one\n-    pass.\n+    so we can use ops.segment_sum() to update the buckets in one pass.\n \n     Consider following example:\n     y_true = [0, 0, 1, 1]\n@@ -93,7 +92,7 @@ def _update_confusion_matrix_variables_optimized(\n     # Note the second item \"1.0\" is floored to 0, since the value need to be\n     # strictly larger than the bucket lower bound.\n     # In the implementation, we use ops.ceil() - 1 to achieve this.\n-    tp_bucket_value = ops.unsorted_segmenet_sum(true_labels, bucket_indices,\n+    tp_bucket_value = ops.segment_sum(true_labels, bucket_indices,\n                                                    num_segments=num_thresholds)\n                     = [1, 1, 0]\n     # For [1, 1, 0] here, it means there is 1 true value contributed by bucket\n@@ -109,25 +108,25 @@ def _update_confusion_matrix_variables_optimized(\n     O(T * N).\n \n     Args:\n-      variables_to_update: Dictionary with 'tp', 'fn', 'tn', 'fp' as valid keys\n-        and corresponding variables to update as values.\n+        variables_to_update: Dictionary with 'tp', 'fn', 'tn', 'fp' as valid\n+            keys and corresponding variables to update as values.\n         y_true: A floating point `Tensor` whose shape matches `y_pred`. Will be\n             cast to `bool`.\n-      y_pred: A floating point `Tensor` of arbitrary shape and whose values are\n-        in the range `[0, 1]`.\n+        y_pred: A floating point `Tensor` of arbitrary shape and whose values\n+            are in the range `[0, 1]`.\n         thresholds: A sorted floating point `Tensor` with value in `[0, 1]`.\n-        It need to be evenly distributed (the diff between each element need to\n-        be the same).\n+            It need to be evenly distributed (the diff between each element need\n+            to be the same).\n         multi_label: Optional boolean indicating whether multidimensional\n             prediction/labels should be treated as multilabel responses, or\n             flattened into a single label. When True, the valus of\n-        `variables_to_update` must have a second dimension equal to the number\n-        of labels in y_true and y_pred, and those tensors must not be\n+            `variables_to_update` must have a second dimension equal to the\n+            number of labels in y_true and y_pred, and those tensors must not be\n             RaggedTensors.\n-      sample_weights: Optional `Tensor` whose rank is either 0, or the same rank\n-        as `y_true`, and must be broadcastable to `y_true` (i.e., all dimensions\n-        must be either `1`, or the same as the corresponding `y_true`\n-        dimension).\n+        sample_weights: Optional `Tensor` whose rank is either 0, or the same\n+            rank as `y_true`, and must be broadcastable to `y_true` (i.e., all\n+            dimensions must be either `1`, or the same as the corresponding\n+            `y_true` dimension).\n         label_weights: Optional tensor of non-negative weights for multilabel\n             data. The weights are applied when calculating TP, FP, FN, and TN\n             without explicit multilabel handling (i.e. when the data is to be\n@@ -137,7 +136,7 @@ def _update_confusion_matrix_variables_optimized(\n             imprecisions.  It will change how we handle the leading and tailing\n             bucket.\n     \"\"\"\n-    num_thresholds = thresholds.shape.as_list()[0]\n+    num_thresholds = ops.shape(thresholds)[0]\n \n     if sample_weights is None:\n         sample_weights = 1.0\n@@ -160,7 +159,7 @@ def _update_confusion_matrix_variables_optimized(\n \n     # We shouldn't need this, but in case there are predict value that is out of\n     # the range of [0.0, 1.0]\n-    y_pred = ops.clip(y_pred, clip_value_min=0.0, clip_value_max=1.0)\n+    y_pred = ops.clip(y_pred, x_min=0.0, x_max=1.0)\n \n     y_true = ops.cast(ops.cast(y_true, \"bool\"), y_true.dtype)\n     if not multi_label:\n@@ -198,7 +197,7 @@ def _update_confusion_matrix_variables_optimized(\n                 label_and_bucket_index[0],\n                 label_and_bucket_index[1],\n             )\n-            return ops.unsorted_segmenet_sum(\n+            return ops.segment_sum(\n                 data=label,\n                 segment_ids=bucket_index,\n                 num_segments=num_thresholds,\n@@ -210,21 +209,21 @@ def _update_confusion_matrix_variables_optimized(\n         fp_bucket_v = ops.vectorized_map(\n             gather_bucket, (false_labels, bucket_indices), warn=False\n         )\n-        tp = ops.transpose(ops.cumsum(tp_bucket_v, reverse=True, axis=1))\n-        fp = ops.transpose(ops.cumsum(fp_bucket_v, reverse=True, axis=1))\n+        tp = ops.transpose(ops.cumsum(ops.flip(tp_bucket_v), axis=1))\n+        fp = ops.transpose(ops.cumsum(ops.flip(fp_bucket_v), axis=1))\n     else:\n-        tp_bucket_v = ops.unsorted_segmenet_sum(\n+        tp_bucket_v = ops.segment_sum(\n             data=true_labels,\n             segment_ids=bucket_indices,\n             num_segments=num_thresholds,\n         )\n-        fp_bucket_v = ops.unsorted_segmenet_sum(\n+        fp_bucket_v = ops.segment_sum(\n             data=false_labels,\n             segment_ids=bucket_indices,\n             num_segments=num_thresholds,\n         )\n-        tp = ops.cumsum(tp_bucket_v, reverse=True)\n-        fp = ops.cumsum(fp_bucket_v, reverse=True)\n+        tp = ops.cumsum(ops.flip(tp_bucket_v))\n+        fp = ops.cumsum(ops.flip(fp_bucket_v))\n \n     # fn = sum(true_labels) - tp\n     # tn = sum(false_labels) - fp\n\n@@ -1,7 +1,6 @@\n \"\"\"\n segment_sum\n top_k\n-in_top_k\n \"\"\"\n \n from keras_core import backend\n@@ -10,14 +9,16 @@ from keras_core.operations.operation import Operation\n \n \n class SegmentSum(Operation):\n-    def call(self, x, segment_ids, num_segments=None, sorted=False):\n-        return backend.math.segment_sum(x, segment_ids, num_segments, sorted)\n+    def call(self, data, segment_ids, num_segments=None, sorted=False):\n+        return backend.math.segment_sum(data, segment_ids, num_segments, sorted)\n \n \n-def segment_sum(x, segment_ids, num_segments=None, sorted=False):\n-    if any_symbolic_tensors((x,)):\n-        return SegmentSum().symbolic_call(x, segment_ids, num_segments, sorted)\n-    return backend.math.segment_sum(x, segment_ids, num_segments, sorted)\n+def segment_sum(data, segment_ids, num_segments=None, sorted=False):\n+    if any_symbolic_tensors((data,)):\n+        return SegmentSum().symbolic_call(\n+            data, segment_ids, num_segments, sorted\n+        )\n+    return backend.math.segment_sum(data, segment_ids, num_segments, sorted)\n \n \n class TopK(Operation):\n\n@@ -1881,16 +1881,19 @@ def matmul(x1, x2):\n \n \n class Max(Operation):\n-    def __init__(self, axis=None, keepdims=False):\n+    def __init__(self, axis=None, keepdims=False, initial=None):\n         super().__init__()\n         if isinstance(axis, int):\n             self.axis = [axis]\n         else:\n             self.axis = axis\n         self.keepdims = keepdims\n+        self.initial = initial\n \n     def call(self, x):\n-        return backend.numpy.max(x, axis=self.axis, keepdims=self.keepdims)\n+        return backend.numpy.max(\n+            x, axis=self.axis, keepdims=self.keepdims, initial=self.initial\n+        )\n \n     def compute_output_spec(self, x):\n         return KerasTensor(\n@@ -1899,10 +1902,12 @@ class Max(Operation):\n         )\n \n \n-def max(x, axis=None, keepdims=False):\n+def max(x, axis=None, keepdims=False, initial=None):\n     if any_symbolic_tensors((x,)):\n-        return Max(axis=axis, keepdims=keepdims).symbolic_call(x)\n-    return backend.numpy.max(x, axis=axis, keepdims=keepdims)\n+        return Max(axis=axis, keepdims=keepdims, initial=initial).symbolic_call(\n+            x\n+        )\n+    return backend.numpy.max(x, axis=axis, keepdims=keepdims, initial=initial)\n \n \n class Maximum(Operation):\n@@ -1961,15 +1966,18 @@ def meshgrid(*x, indexing=\"xy\"):\n \n \n class Min(Operation):\n-    def __init__(self, axis=None, keepdims=False):\n+    def __init__(self, axis=None, keepdims=False, initial=None):\n         if isinstance(axis, int):\n             self.axis = [axis]\n         else:\n             self.axis = axis\n         self.keepdims = keepdims\n+        self.initial = initial\n \n     def call(self, x):\n-        return backend.numpy.min(x, axis=self.axis, keepdims=self.keepdims)\n+        return backend.numpy.min(\n+            x, axis=self.axis, keepdims=self.keepdims, initial=self.initial\n+        )\n \n     def compute_output_spec(self, x):\n         return KerasTensor(\n@@ -1978,10 +1986,12 @@ class Min(Operation):\n         )\n \n \n-def min(x, axis=None, keepdims=False):\n+def min(x, axis=None, keepdims=False, initial=None):\n     if any_symbolic_tensors((x,)):\n-        return Min(axis=axis, keepdims=keepdims).symbolic_call(x)\n-    return backend.numpy.min(x, axis=axis, keepdims=keepdims)\n+        return Min(axis=axis, keepdims=keepdims, initial=initial).symbolic_call(\n+            x\n+        )\n+    return backend.numpy.min(x, axis=axis, keepdims=keepdims, initial=initial)\n \n \n class Minimum(Operation):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
