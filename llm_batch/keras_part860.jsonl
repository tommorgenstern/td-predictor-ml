{"custom_id": "keras#c4314a94452517ebc1bda0f4164225e001e34be0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 159 | Contributors (this commit): 6 | Commits (past 90d): 5 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -137,6 +137,7 @@ class MultiHeadAttention(Layer):\n         self._kernel_constraint = constraints.get(kernel_constraint)\n         self._bias_constraint = constraints.get(bias_constraint)\n         self._flash_attention = flash_attention or is_flash_attention_enabled()\n+        self._return_attention_scores = False\n \n         if isinstance(attention_axes, int):\n             attention_axes = (attention_axes,)\n@@ -403,7 +404,6 @@ class MultiHeadAttention(Layer):\n         query,\n         key,\n         value,\n-        return_attention_scores,\n         attention_mask=None,\n         training=None,\n     ):\n@@ -430,7 +430,7 @@ class MultiHeadAttention(Layer):\n         \"\"\"\n \n         # Check for flash attention constraints\n-        if self._flash_attention and return_attention_scores:\n+        if self._flash_attention and self._return_attention_scores:\n             raise ValueError(\n                 \"Returning attention scores is not supported when flash \"\n                 \"attention is enabled. Please disable flash attention to access\"\n@@ -446,7 +446,7 @@ class MultiHeadAttention(Layer):\n         # Determine whether to use dot-product attention\n         use_dot_product_attention = not (\n             self._dropout > 0.0\n-            or return_attention_scores\n+            or self._return_attention_scores\n             or (len(query.shape) != 4)\n         )\n \n@@ -519,6 +519,7 @@ class MultiHeadAttention(Layer):\n         training=None,\n         use_causal_mask=False,\n     ):\n+        self._return_attention_scores = return_attention_scores\n         if key is None:\n             key = value\n         attention_mask = self._compute_attention_mask(\n@@ -545,7 +546,6 @@ class MultiHeadAttention(Layer):\n             query,\n             key,\n             value,\n-            return_attention_scores,\n             attention_mask,\n             training,\n         )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#96e07ec711329d12dff79ec34d1f8f192cbc110e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 61 | Lines Deleted: 43 | Files Changed: 8 | Hunks: 44 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 104 | Churn Cumulative: 1048 | Contributors (this commit): 5 | Commits (past 90d): 20 | Contributors (cumulative): 25 | DMM Complexity: 1.0\n\nDIFF:\n@@ -7,6 +7,9 @@ since your modifications would be overwritten.\n from keras.src.initializers import deserialize\n from keras.src.initializers import get\n from keras.src.initializers import serialize\n+from keras.src.initializers.constant_initializers import STFT\n+from keras.src.initializers.constant_initializers import STFT as STFTInitializer\n+from keras.src.initializers.constant_initializers import STFT as stft\n from keras.src.initializers.constant_initializers import Constant\n from keras.src.initializers.constant_initializers import Constant as constant\n from keras.src.initializers.constant_initializers import Identity\n@@ -16,7 +19,6 @@ from keras.src.initializers.constant_initializers import (\n from keras.src.initializers.constant_initializers import Identity as identity\n from keras.src.initializers.constant_initializers import Ones\n from keras.src.initializers.constant_initializers import Ones as ones\n-from keras.src.initializers.constant_initializers import STFTInitializer\n from keras.src.initializers.constant_initializers import Zeros\n from keras.src.initializers.constant_initializers import Zeros as zeros\n from keras.src.initializers.initializer import Initializer\n@@ -40,13 +42,11 @@ from keras.src.initializers.random_initializers import LecunUniform\n from keras.src.initializers.random_initializers import (\n     LecunUniform as lecun_uniform,\n )\n-from keras.src.initializers.random_initializers import OrthogonalInitializer\n+from keras.src.initializers.random_initializers import Orthogonal\n from keras.src.initializers.random_initializers import (\n-    OrthogonalInitializer as Orthogonal,\n-)\n-from keras.src.initializers.random_initializers import (\n-    OrthogonalInitializer as orthogonal,\n+    Orthogonal as OrthogonalInitializer,\n )\n+from keras.src.initializers.random_initializers import Orthogonal as orthogonal\n from keras.src.initializers.random_initializers import RandomNormal\n from keras.src.initializers.random_initializers import (\n     RandomNormal as random_normal,\n\n@@ -7,6 +7,9 @@ since your modifications would be overwritten.\n from keras.src.initializers import deserialize\n from keras.src.initializers import get\n from keras.src.initializers import serialize\n+from keras.src.initializers.constant_initializers import STFT\n+from keras.src.initializers.constant_initializers import STFT as STFTInitializer\n+from keras.src.initializers.constant_initializers import STFT as stft\n from keras.src.initializers.constant_initializers import Constant\n from keras.src.initializers.constant_initializers import Constant as constant\n from keras.src.initializers.constant_initializers import Identity\n@@ -16,7 +19,6 @@ from keras.src.initializers.constant_initializers import (\n from keras.src.initializers.constant_initializers import Identity as identity\n from keras.src.initializers.constant_initializers import Ones\n from keras.src.initializers.constant_initializers import Ones as ones\n-from keras.src.initializers.constant_initializers import STFTInitializer\n from keras.src.initializers.constant_initializers import Zeros\n from keras.src.initializers.constant_initializers import Zeros as zeros\n from keras.src.initializers.initializer import Initializer\n@@ -40,13 +42,11 @@ from keras.src.initializers.random_initializers import LecunUniform\n from keras.src.initializers.random_initializers import (\n     LecunUniform as lecun_uniform,\n )\n-from keras.src.initializers.random_initializers import OrthogonalInitializer\n+from keras.src.initializers.random_initializers import Orthogonal\n from keras.src.initializers.random_initializers import (\n-    OrthogonalInitializer as Orthogonal,\n-)\n-from keras.src.initializers.random_initializers import (\n-    OrthogonalInitializer as orthogonal,\n+    Orthogonal as OrthogonalInitializer,\n )\n+from keras.src.initializers.random_initializers import Orthogonal as orthogonal\n from keras.src.initializers.random_initializers import RandomNormal\n from keras.src.initializers.random_initializers import (\n     RandomNormal as random_normal,\n\n@@ -5,10 +5,10 @@ import numpy as np\n from keras.src import backend\n from keras.src import ops\n from keras.src.api_export import keras_export\n+from keras.src.initializers.constant_initializers import STFT\n from keras.src.initializers.constant_initializers import Constant\n from keras.src.initializers.constant_initializers import Identity\n from keras.src.initializers.constant_initializers import Ones\n-from keras.src.initializers.constant_initializers import STFTInitializer\n from keras.src.initializers.constant_initializers import Zeros\n from keras.src.initializers.initializer import Initializer\n from keras.src.initializers.random_initializers import GlorotNormal\n@@ -17,7 +17,7 @@ from keras.src.initializers.random_initializers import HeNormal\n from keras.src.initializers.random_initializers import HeUniform\n from keras.src.initializers.random_initializers import LecunNormal\n from keras.src.initializers.random_initializers import LecunUniform\n-from keras.src.initializers.random_initializers import OrthogonalInitializer\n+from keras.src.initializers.random_initializers import Orthogonal\n from keras.src.initializers.random_initializers import RandomNormal\n from keras.src.initializers.random_initializers import RandomUniform\n from keras.src.initializers.random_initializers import TruncatedNormal\n@@ -30,7 +30,7 @@ ALL_OBJECTS = {\n     Constant,\n     Identity,\n     Ones,\n-    STFTInitializer,\n+    STFT,\n     Zeros,\n     GlorotNormal,\n     GlorotUniform,\n@@ -38,11 +38,11 @@ ALL_OBJECTS = {\n     HeUniform,\n     LecunNormal,\n     LecunUniform,\n+    Orthogonal,\n     RandomNormal,\n-    TruncatedNormal,\n     RandomUniform,\n+    TruncatedNormal,\n     VarianceScaling,\n-    OrthogonalInitializer,\n }\n \n ALL_OBJECTS_DICT = {cls.__name__: cls for cls in ALL_OBJECTS}\n@@ -52,11 +52,12 @@ ALL_OBJECTS_DICT.update(\n # Aliases\n ALL_OBJECTS_DICT.update(\n     {\n-        \"uniform\": RandomUniform,\n+        \"IdentityInitializer\": Identity,  # For compatibility\n         \"normal\": RandomNormal,\n-        \"orthogonal\": OrthogonalInitializer,\n-        \"Orthogonal\": OrthogonalInitializer,  # Legacy\n         \"one\": Ones,\n+        \"STFTInitializer\": STFT,  # For compatibility\n+        \"OrthogonalInitializer\": Orthogonal,  # For compatibility\n+        \"uniform\": RandomUniform,\n         \"zero\": Zeros,\n     }\n )\n\n@@ -108,9 +108,9 @@ class Ones(Initializer):\n \n @keras_export(\n     [\n-        \"keras.initializers.IdentityInitializer\",\n         \"keras.initializers.Identity\",\n         \"keras.initializers.identity\",\n+        \"keras.initializers.IdentityInitializer\",\n     ]\n )\n class Identity(Initializer):\n@@ -154,8 +154,14 @@ class Identity(Initializer):\n         return self.gain * ops.eye(*shape, dtype=dtype)\n \n \n-@keras_export([\"keras.initializers.STFTInitializer\"])\n-class STFTInitializer(Initializer):\n+@keras_export(\n+    [\n+        \"keras.initializers.STFT\",\n+        \"keras.initializers.stft\",\n+        \"keras.initializers.STFTInitializer\",\n+    ]\n+)\n+class STFT(Initializer):\n     \"\"\"Initializer of Conv kernels for Short-term Fourier Transformation (STFT).\n \n     Since the formula involves complex numbers, this class compute either the\n@@ -177,7 +183,8 @@ class STFTInitializer(Initializer):\n \n     Args:\n         side: String, `\"real\"` or `\"imag\"` deciding if the kernel will compute\n-            the real side or the imaginary side of the output.\n+            the real side or the imaginary side of the output. Defaults to\n+            `\"real\"`.\n         window: String for the name of the windowing function in the\n             `scipy.signal.windows` module, or array_like for the window values,\n             or `None` for no windowing.\n@@ -188,7 +195,9 @@ class STFTInitializer(Initializer):\n             periodic. Defaults to `False`.\n     \"\"\"\n \n-    def __init__(self, side, window=\"hann\", scaling=\"density\", periodic=False):\n+    def __init__(\n+        self, side=\"real\", window=\"hann\", scaling=\"density\", periodic=False\n+    ):\n         if side not in [\"real\", \"imag\"]:\n             raise ValueError(f\"side should be 'real' or 'imag', not {side}\")\n         if isinstance(window, str):\n\n@@ -69,6 +69,10 @@ class ConstantInitializersTest(testing.TestCase):\n \n         self.run_class_serialization_test(initializer)\n \n+        # Test compatible class_name\n+        initializer = initializers.get(\"IdentityInitializer\")\n+        self.assertIsInstance(initializer, initializers.Identity)\n+\n     def test_stft_initializer(self):\n         shape = (256, 1, 513)\n         time_range = np.arange(256).reshape((-1, 1, 1))\n@@ -82,12 +86,12 @@ class ConstantInitializersTest(testing.TestCase):\n             # of non-small error in jax and torch\n             tol_kwargs = {\"atol\": 1e-4, \"rtol\": 1e-6}\n \n-        initializer = initializers.STFTInitializer(\"real\", None)\n+        initializer = initializers.STFT(\"real\", None)\n         values = backend.convert_to_numpy(initializer(shape))\n         self.assertAllClose(np.cos(args), values, atol=1e-4)\n         self.run_class_serialization_test(initializer)\n \n-        initializer = initializers.STFTInitializer(\n+        initializer = initializers.STFT(\n             \"real\",\n             \"hamming\",\n             None,\n@@ -99,7 +103,7 @@ class ConstantInitializersTest(testing.TestCase):\n         self.assertAllClose(np.cos(args) * window, values, **tol_kwargs)\n         self.run_class_serialization_test(initializer)\n \n-        initializer = initializers.STFTInitializer(\n+        initializer = initializers.STFT(\n             \"imag\",\n             \"tukey\",\n             \"density\",\n@@ -112,7 +116,7 @@ class ConstantInitializersTest(testing.TestCase):\n         self.assertAllClose(np.sin(args) * window, values, **tol_kwargs)\n         self.run_class_serialization_test(initializer)\n \n-        initializer = initializers.STFTInitializer(\n+        initializer = initializers.STFT(\n             \"imag\",\n             list(range(1, 257)),\n             \"spectrum\",\n@@ -125,8 +129,12 @@ class ConstantInitializersTest(testing.TestCase):\n         self.run_class_serialization_test(initializer)\n \n         with self.assertRaises(ValueError):\n-            initializers.STFTInitializer(\"imaginary\")\n+            initializers.STFT(\"imaginary\")\n         with self.assertRaises(ValueError):\n-            initializers.STFTInitializer(\"real\", scaling=\"l2\")\n+            initializers.STFT(\"real\", scaling=\"l2\")\n         with self.assertRaises(ValueError):\n-            initializers.STFTInitializer(\"real\", window=\"unknown\")\n+            initializers.STFT(\"real\", window=\"unknown\")\n+\n+        # Test compatible class_name\n+        initializer = initializers.get(\"STFTInitializer\")\n+        self.assertIsInstance(initializer, initializers.STFT)\n\n@@ -639,12 +639,12 @@ def compute_fans(shape):\n \n @keras_export(\n     [\n-        \"keras.initializers.OrthogonalInitializer\",\n         \"keras.initializers.Orthogonal\",\n         \"keras.initializers.orthogonal\",\n+        \"keras.initializers.OrthogonalInitializer\",\n     ]\n )\n-class OrthogonalInitializer(RandomInitializer):\n+class Orthogonal(RandomInitializer):\n     \"\"\"Initializer that generates an orthogonal matrix.\n \n     If the shape of the tensor to initialize is two-dimensional, it is\n\n@@ -7,7 +7,7 @@ from keras.src import testing\n from keras.src import utils\n \n \n-class InitializersTest(testing.TestCase):\n+class RandomInitializersTest(testing.TestCase):\n     def test_random_normal(self):\n         utils.set_random_seed(1337)\n         shape = (25, 20)\n@@ -124,11 +124,11 @@ class InitializersTest(testing.TestCase):\n         )\n         self.run_class_serialization_test(initializer)\n \n-    def test_orthogonal_initializer(self):\n+    def test_orthogonal(self):\n         shape = (5, 5)\n         gain = 2.0\n         seed = 1234\n-        initializer = initializers.OrthogonalInitializer(gain=gain, seed=seed)\n+        initializer = initializers.Orthogonal(gain=gain, seed=seed)\n         values = initializer(shape=shape)\n         self.assertEqual(initializer.seed, seed)\n         self.assertEqual(initializer.gain, gain)\n@@ -148,9 +148,9 @@ class InitializersTest(testing.TestCase):\n \n         self.run_class_serialization_test(initializer)\n \n-        # Test legacy class_name\n-        initializer = initializers.get(\"Orthogonal\")\n-        self.assertIsInstance(initializer, initializers.OrthogonalInitializer)\n+        # Test compatible class_name\n+        initializer = initializers.get(\"OrthogonalInitializer\")\n+        self.assertIsInstance(initializer, initializers.Orthogonal)\n \n     def test_get_method(self):\n         obj = initializers.get(\"glorot_normal\")\n@@ -214,7 +214,7 @@ class InitializersTest(testing.TestCase):\n \n     def test_serialization_with_seed_generator(self):\n         seed = random.SeedGenerator()\n-        initializer = initializers.OrthogonalInitializer(seed=seed)\n+        initializer = initializers.Orthogonal(seed=seed)\n         self.run_class_serialization_test(initializer)\n \n         seed = random.SeedGenerator()\n\n@@ -217,7 +217,7 @@ class STFTSpectrogram(layers.Layer):\n             self.real_kernel = self.add_weight(\n                 name=\"real_kernel\",\n                 shape=shape,\n-                initializer=initializers.STFTInitializer(\n+                initializer=initializers.STFT(\n                     \"real\", self.window, self.scaling, self.periodic\n                 ),\n             )\n@@ -225,7 +225,7 @@ class STFTSpectrogram(layers.Layer):\n             self.imag_kernel = self.add_weight(\n                 name=\"imag_kernel\",\n                 shape=shape,\n-                initializer=initializers.STFTInitializer(\n+                initializer=initializers.STFT(\n                     \"imag\", self.window, self.scaling, self.periodic\n                 ),\n             )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#65db1a4bbf741118b61db28f0f2962cdf0078b85", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 33 | Churn Cumulative: 388 | Contributors (this commit): 8 | Commits (past 90d): 11 | Contributors (cumulative): 14 | DMM Complexity: 0.9166666666666666\n\nDIFF:\n@@ -454,16 +454,16 @@ class MultiHeadAttention(Layer):\n             if attention_mask is not None:\n                 # Ensure attention_mask has the correct shape for broadcasting\n                 # Expected shape: [batch_size, num_heads, query_seq_len,\n-                # key_seq_len]. This is because masked_softmax is not supported\n-                # in JAX.\n-                while len(attention_mask.shape) < 4:\n+                # key_seq_len].\n+                mask_expansion_axis = -len(self._attention_axes) * 2 - 1\n+                len_attention_scores_shape = 4  # Only accepts 4D inputs\n+                for _ in range(\n+                    len_attention_scores_shape - len(attention_mask.shape)\n+                ):\n                     attention_mask = ops.expand_dims(\n-                        attention_mask, axis=1\n-                    )  # Add dimension for num_heads\n-                if attention_mask.shape[1] != self._num_heads:\n-                    attention_mask = ops.tile(\n-                        attention_mask, [1, self._num_heads, 1, 1]\n+                        attention_mask, axis=mask_expansion_axis\n                     )\n+                attention_mask = ops.cast(attention_mask, dtype=\"bool\")\n             # Directly compute the attention output using dot-product attention\n             attention_output = ops.dot_product_attention(\n                 query=query,\n\n@@ -10,6 +10,8 @@ from keras.src import dtype_policies\n from keras.src import initializers\n from keras.src import layers\n from keras.src import models\n+from keras.src import ops\n+from keras.src import random\n from keras.src import saving\n from keras.src import testing\n from keras.src.layers.attention.attention import disable_flash_attention\n@@ -316,6 +318,21 @@ class MultiHeadAttentionTest(testing.TestCase):\n         )\n         self.assertAllClose(output, output_with_manual_mask)\n \n+    def test_masking_with_different_shapes(self):\n+        x = random.uniform(shape=(2, 5, 8))\n+        mask = ops.tril(ops.ones((5, 5)))  # (5, 5)\n+        layer = layers.MultiHeadAttention(num_heads=2, key_dim=4)\n+        output_1 = layer(query=x, value=x, attention_mask=mask)\n+\n+        mask = ops.tile(mask[None, ...], (2, 1, 1))  # (2, 5, 5)\n+        output_2 = layer(query=x, value=x, attention_mask=mask)\n+\n+        mask = ops.tile(mask[:, None, ...], (1, 2, 1, 1))  # (2, 2, 5, 5)\n+        output_3 = layer(query=x, value=x, attention_mask=mask)\n+\n+        self.assertAllClose(output_1, output_2)\n+        self.assertAllClose(output_1, output_3)\n+\n     def test_correctness(self):\n         query = np.array([[[1.0, 0.0], [0.0, 1.0]]])\n         key = np.array([[[0.0, 1.0], [1.0, 0.0]]])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0861e090086b547e9f6b2984b7333cad1330a799", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 199 | Lines Deleted: 105 | Files Changed: 7 | Hunks: 45 | Methods Changed: 17 | Complexity Δ (Sum/Max): 23/11 | Churn Δ: 304 | Churn Cumulative: 4149 | Contributors (this commit): 18 | Commits (past 90d): 66 | Contributors (cumulative): 66 | DMM Complexity: 0.20833333333333334\n\nDIFF:\n@@ -972,6 +972,53 @@ def psnr(x1, x2, max_val):\n     return psnr\n \n \n+def _can_use_flash_attention(query, key, value, bias, raise_error=False):\n+    # Ref: https://github.com/jax-ml/jax/blob/main/jax/_src/cudnn/fused_attention_stablehlo.py\n+    from jax._src.cudnn.fused_attention_stablehlo import _normalize_layout\n+    from jax._src.cudnn.fused_attention_stablehlo import check_cudnn_version\n+    from jax._src.cudnn.fused_attention_stablehlo import check_layout\n+\n+    try:\n+        # The older version of jax doesn't have `check_compute_capability`\n+        from jax._src.cudnn.fused_attention_stablehlo import (\n+            check_compute_capability,\n+        )\n+    except ImportError:\n+        if raise_error:\n+            raise\n+        return False\n+\n+    try:\n+        # `dot_product_attention` is only available in jax>=0.4.31\n+        if not hasattr(jax.nn, \"dot_product_attention\"):\n+            raise ValueError(\n+                \"Flash attention is not supported in your \"\n+                \"current JAX version. Please update it \"\n+                \"using `pip install -U jax jaxlib`.\"\n+            )\n+        # Check if cuDNN is installed and raise RuntimeError if cuDNN is not\n+        # detected\n+        check_cudnn_version()\n+        # Only support at least Ampere\n+        if not check_compute_capability(\"8.0\"):\n+            raise RuntimeError(\"Require at least Ampere arch to run\")\n+        # Check inputs layout\n+        check_layout(\n+            query,\n+            key,\n+            value,\n+            bias,\n+            q_seqlen=None,\n+            kv_seqlen=None,\n+            layout=_normalize_layout(\"BTNH\"),\n+        )\n+        return True\n+    except:\n+        if raise_error:\n+            raise\n+        return False\n+\n+\n def _apply_masks(logits, mask, is_causal):\n     if mask is None and not is_causal:\n         return logits\n@@ -1021,19 +1068,24 @@ def dot_product_attention(\n     mask=None,\n     scale=None,\n     is_causal=False,\n-    flash_attention=False,\n+    flash_attention=None,\n ):\n     query = convert_to_tensor(query)\n     key = convert_to_tensor(key)\n     value = convert_to_tensor(value)\n-    if len(query.shape) != 4:\n+    if len(query.shape) != 4 or len(key.shape) != 4 or len(value.shape) != 4:\n         raise ValueError(\n             \"`dot_product_attention` only supports 4D inputs. \"\n             f\"Received: query.shape={query.shape}, key.shape={key.shape}, \"\n             f\"value.shape={value.shape}.\"\n         )\n-    is_tpu = jax.devices()[0].platform == \"tpu\"\n-    if is_tpu and flash_attention:\n+    if flash_attention is None:\n+        flash_attention = _can_use_flash_attention(query, key, value, bias)\n+    elif flash_attention is True:\n+        # Use `raise_error=True` to provide more details if the inputs failed to\n+        # use flash attention\n+        _can_use_flash_attention(query, key, value, bias, raise_error=True)\n+    if jax.devices()[0].platform == \"tpu\" and flash_attention:\n         # Use TPU-optimized flash attention from Pallas\n         return flash_attention_tpu(\n             query,\n@@ -1046,7 +1098,6 @@ def dot_product_attention(\n         )\n     # `dot_product_attention` is only available in jax>=0.4.31\n     if hasattr(jax.nn, \"dot_product_attention\"):\n-        implementation = \"cudnn\" if flash_attention else \"xla\"\n         return jax.nn.dot_product_attention(\n             query,\n             key,\n@@ -1055,7 +1106,7 @@ def dot_product_attention(\n             mask=mask,\n             scale=scale,\n             is_causal=is_causal,\n-            implementation=implementation,\n+            implementation=\"cudnn\" if flash_attention else \"xla\",\n         )\n \n     if flash_attention:\n\n@@ -1080,10 +1080,13 @@ def dot_product_attention(\n     mask=None,\n     scale=None,\n     is_causal=False,\n-    flash_attention=False,\n+    flash_attention=None,\n ):\n+    if flash_attention is None:\n+        flash_attention = False\n     if flash_attention:\n-        raise ValueError(\"Flash attention is not implemented in NumPy.\")\n+        raise ValueError(\"Flash attention is not supported in numpy backend.\")\n+\n     # Ref: jax.nn.dot_product_attention\n     # https://github.com/jax-ml/jax/blob/jax-v0.4.32/jax/_src/nn/functions.py#L828\n     # Not support `query_seq_lengths` and `key_value_seq_lengths` args\n\n@@ -999,11 +999,13 @@ def dot_product_attention(\n     mask=None,\n     scale=None,\n     is_causal=False,\n-    flash_attention=False,\n+    flash_attention=None,\n ):\n+    if flash_attention is None:\n+        flash_attention = False\n     if flash_attention:\n         raise ValueError(\n-            \"Flash attention is not supported yet in TensorFlow backend.\"\n+            \"Flash attention is not supported in tensorflow backend.\"\n         )\n \n     # Ref: jax.nn.dot_product_attention\n\n@@ -889,17 +889,30 @@ def _get_large_negative(dtype):\n     return convert_to_tensor(val * -0.7, dtype=dtype)\n \n \n-def is_flash_attention_enabled(query, key, value, mask=None, is_causal=False):\n-    params = torch.backends.cuda.SDPAParams(\n+def _can_use_flash_attention(\n+    query, key, value, mask=None, is_causal=False, debug=False\n+):\n+    try:\n+        spda_params = torch.backends.cuda.SDPAParams(\n             query,\n             key,\n             value,\n             mask,\n-        0.0,\n+            0.0,  # dropout_p\n             is_causal,\n         )\n-    is_enabled = torch.backends.cuda.can_use_flash_attention(params, False)\n-    return is_enabled\n+    except TypeError:\n+        # The signature changed in newer version of torch.\n+        spda_params = torch.backends.cuda.SDPAParams(\n+            query,\n+            key,\n+            value,\n+            mask,\n+            0.0,  # dropout_p\n+            is_causal,\n+            False,  # enable_gqa\n+        )\n+    return torch.backends.cuda.can_use_flash_attention(spda_params, debug)\n \n \n def dot_product_attention(\n@@ -910,7 +923,7 @@ def dot_product_attention(\n     mask=None,\n     scale=None,\n     is_causal=False,\n-    flash_attention=False,\n+    flash_attention=None,\n ):\n     if bias is not None:\n         raise ValueError(\n@@ -919,9 +932,9 @@ def dot_product_attention(\n     query = convert_to_tensor(query)\n     key = convert_to_tensor(key)\n     value = convert_to_tensor(value)\n-    if len(query.shape) != 4:\n+    if len(query.shape) != 4 or len(key.shape) != 4 or len(value.shape) != 4:\n         raise ValueError(\n-            \"`dot_product_attention` only supports 3D and 4D inputs. \"\n+            \"`dot_product_attention` only supports 4D inputs. \"\n             f\"Received: query.shape={query.shape}, key.shape={key.shape}, \"\n             f\"value.shape={value.shape}.\"\n         )\n@@ -937,21 +950,27 @@ def dot_product_attention(\n     key = torch.transpose(key, axis0, axis1)\n     value = torch.transpose(value, axis0, axis1)\n \n-    if flash_attention:\n-        is_enabled = is_flash_attention_enabled(\n+    if flash_attention is None:\n+        flash_attention = _can_use_flash_attention(\n+            query=query, key=key, value=value, mask=mask, is_causal=is_causal\n+        )\n+    elif (\n+        flash_attention is True\n+        and _can_use_flash_attention(\n             query=query,\n             key=key,\n             value=value,\n             mask=mask,\n             is_causal=is_causal,\n+            debug=True,\n         )\n-        if not is_enabled:\n+        is False\n+    ):\n         raise ValueError(\n-                \"Flash attention is not enabled in `torch` backend. \"\n-                \"The dtype of the inputs should be float16/bfloat16 \"\n-                \"and your GPU should support flash attention implementation.\"\n+            \"Flash attention is not supported with the provided inputs. \"\n+            \"Please check the warnings for more details.\"\n         )\n-\n+    if flash_attention:\n         with torch.nn.attention.sdpa_kernel(\n             backends=[torch.nn.attention.SDPBackend.FLASH_ATTENTION],\n         ):\n\n@@ -56,17 +56,13 @@ class MultiHeadAttentionTest(testing.TestCase):\n         )\n \n     def test_basics_with_flash_attention(self):\n-        if backend.backend() in [\n-            \"torch\",\n-            \"tensorflow\",\n-            \"numpy\",\n-        ]:\n+        if backend.backend() in (\"tensorflow\", \"numpy\"):\n             self.skipTest(\n-                \"Not supported in TF and NumPy and supported for \"\n-                \"PyTorch with specific requirements.\"\n+                \"Flash attention is not supported in tensorflow and numpy \"\n+                \"backends.\"\n             )\n \n-        if backend.backend() == \"jax\":\n+        elif backend.backend() == \"torch\":\n             try:\n                 enable_flash_attention()\n                 self.run_layer_test(\n@@ -74,6 +70,7 @@ class MultiHeadAttentionTest(testing.TestCase):\n                     init_kwargs={\n                         \"num_heads\": 2,\n                         \"key_dim\": 2,\n+                        \"dtype\": \"float16\",\n                     },\n                     input_shape={\n                         \"query_shape\": (2, 8, 16),\n@@ -87,45 +84,49 @@ class MultiHeadAttentionTest(testing.TestCase):\n                     supports_masking=True,\n                     run_training_check=False,\n                 )\n-\n+                disable_flash_attention()\n+            except ValueError as e:\n+                self.assertStartsWith(\n+                    e.args[0],\n+                    \"Flash attention is not supported with the provided inputs\",\n+                )\n+        elif backend.backend() == \"jax\":\n+            try:\n+                enable_flash_attention()\n                 self.run_layer_test(\n                     layers.MultiHeadAttention,\n                     init_kwargs={\n                         \"num_heads\": 2,\n-                        \"key_dim\": 2,\n-                        \"value_dim\": 4,\n-                        \"use_bias\": False,\n-                        \"dropout\": 0.5,\n+                        \"key_dim\": 8,  # key_dim % 8 == 0\n+                        \"dtype\": \"float16\",\n                     },\n                     input_shape={\n                         \"query_shape\": (2, 8, 16),\n                         \"value_shape\": (2, 4, 16),\n                     },\n                     expected_output_shape=(2, 8, 16),\n-                    expected_num_trainable_weights=4,\n+                    expected_num_trainable_weights=8,\n                     expected_num_non_trainable_weights=0,\n-                    expected_num_seed_generators=1,\n+                    expected_num_seed_generators=0,\n                     expected_num_losses=0,\n                     supports_masking=True,\n                     run_training_check=False,\n                 )\n                 disable_flash_attention()\n             except ValueError as e:\n-                if e.args[0].startswith(\n-                    \"Flash attention is not supported in your \"\n-                    \"current JAX version\"\n-                ):\n-                    self.skipTest(\n-                        \"JAX version does not have \"\n-                        \"`dot_product_attention` function.\"\n+                self.assertStartsWith(\n+                    e.args[0],\n+                    (\n+                        \"Flash attention is not supported in your current JAX \"\n+                        \"version.\"\n+                    ),\n                 )\n             except RuntimeError as e:\n-                if e.args[0] == \"cuDNN is not detected.\":\n-                    self.skipTest(\"No CuDNN to run flash attention for JAX.\")\n-                elif e.args[0] == \"Require at least Ampere arch to run\":\n-                    self.skipTest(\n-                        \"Requires at least Ampere arch to run flash attention \"\n-                        \"for JAX.\"\n+                if str(e.args[0]).startswith(\"cuDNN\"):\n+                    self.assertStartsWith(e.args[0], \"cuDNN is not detected.\")\n+                elif str(e.args[0]).startswith(\"Require at least\"):\n+                    self.assertStartsWith(\n+                        e.args[0], \"Require at least Ampere arch to run\"\n                     )\n \n     @parameterized.named_parameters(\n\n@@ -2376,7 +2376,7 @@ def dot_product_attention(\n     mask=None,\n     scale=None,\n     is_causal=False,\n-    flash_attention=False,\n+    flash_attention=None,\n ):\n     \"\"\"Scaled dot product attention function.\n \n@@ -2411,6 +2411,10 @@ def dot_product_attention(\n         scale: Optional scale for the logits. If `None`, the scale will be set\n             to `1.0 / sqrt(H)`.\n         is_causal: Whether to apply causal mask.\n+        flash_attention: Whether to use flash attention. If `None`, it will\n+            attempt to use flash attention if the required conditions are met.\n+            Typically, the inputs must be in float16 and bfloat16 dtype and the\n+            input layout requirements may vary depending on the backend.\n \n     Returns:\n         An array of the attention output with the same shape of `query`.\n\n@@ -84,8 +84,7 @@ def _dot_product_attention(\n     padded_logits = _apply_masks(logits, mask, is_causal)\n     padded_logits = padded_logits.astype(np.float32)\n     probs = softmax(padded_logits, axis=-1).astype(key.dtype)\n-    encoded = np.einsum(\"BNTS,BSNH->BTNH\", probs, value)\n-    return encoded\n+    return np.einsum(\"BNTS,BSNH->BTNH\", probs, value)\n \n \n class NNOpsDynamicShapeTest(testing.TestCase):\n@@ -2283,73 +2282,86 @@ class NNOpsCorrectnessTest(testing.TestCase):\n             bias=(None, True),\n             scale=(None, 1.0),\n             mask_and_is_causal=((None, False), (True, False), (None, True)),\n-            flash_attention=(True, False),\n+            flash_attention=(None, True, False),\n         )\n     )\n     def test_dot_product_attention(\n         self, bias, scale, mask_and_is_causal, flash_attention\n     ):\n         mask, is_causal = mask_and_is_causal\n-        query_shape = (2, 3, 4, 5)\n-        key_shape = (2, 6, 4, 5)\n-        mask_shape = (2, 4, 3, 6)\n+        query_shape = (2, 3, 4, 8)\n+        key_shape = (2, 3, 4, 8)\n+        bias_shape = (2, 4, 3, 3)\n         query = np.arange(math.prod(query_shape), dtype=float).reshape(\n             query_shape\n         )\n         key = np.arange(math.prod(key_shape), dtype=float).reshape(key_shape)\n         value = np.arange(math.prod(key_shape), dtype=float).reshape(key_shape)\n         if mask is not None:\n-            mask = np.arange(math.prod(mask_shape)).reshape(mask_shape)\n-            mask = (mask > 10).astype(\"bool\")\n+            mask = np.tril(np.ones((3, 3))).astype(\"bool\")\n+            mask = mask[None, None, ...]\n+            mask = np.tile(mask, (2, 4, 1, 1))\n         if bias is not None:\n             if backend.backend() == \"torch\":\n                 self.skipTest(\n                     \"torch does not support `bias` with `dot_product_attention`\"\n                 )\n-            bias = np.arange(math.prod(mask_shape), dtype=float).reshape(\n-                mask_shape\n+            bias = np.arange(math.prod(bias_shape), dtype=float).reshape(\n+                bias_shape\n             )\n \n-        if flash_attention and backend.backend() in [\n-            \"torch\",\n-            \"tensorflow\",\n-            \"numpy\",\n-        ]:\n+        if flash_attention:\n+            if backend.backend() in (\"tensorflow\", \"numpy\"):\n                 self.skipTest(\n-                \"Not supported in TF and NumPy and supported for \"\n-                \"PyTorch with specific requirements.\"\n+                    \"Flash attention is not supported in tensorflow and numpy \"\n+                    \"backends.\"\n+                )\n+            elif backend.backend() == \"torch\":\n+                import torch\n+\n+                if mask is not None:\n+                    self.skipTest(\n+                        \"Flash attention doesn't support `mask=None` in torch \"\n+                        \"backend.\"\n+                    )\n+                if not torch.cuda.is_available():\n+                    self.skipTest(\n+                        \"Flash attention must be run on CUDA in torch backend.\"\n+                    )\n+                cuda_compute_capability = tuple(\n+                    int(x) for x in torch.cuda.get_device_capability()\n+                )\n+                if cuda_compute_capability < (8, 0):\n+                    self.skipTest(\n+                        \"Flash attention must be run on CUDA compute \"\n+                        \"capability >= 8.0 in torch backend.\"\n+                    )\n+            elif backend.backend() == \"jax\":\n+                import jax\n+                from jax._src import xla_bridge\n+\n+                if \"cuda\" not in xla_bridge.get_backend().platform_version:\n+                    self.skipTest(\n+                        \"Flash attention must be run on CUDA in jax backend.\"\n+                    )\n+                d, *_ = jax.local_devices(backend=\"gpu\")\n+                cuda_compute_capability = tuple(\n+                    int(x) for x in d.compute_capability.split(\".\")\n+                )\n+                if cuda_compute_capability < (8, 0):\n+                    self.skipTest(\n+                        \"Flash attention must be run on CUDA compute \"\n+                        \"capability >= 8.0 in jax backend.\"\n                     )\n \n-        if flash_attention and backend.backend() == \"jax\":\n-            try:\n-                outputs = knn.dot_product_attention(\n-                    query,\n-                    key,\n-                    value,\n-                    bias=bias,\n-                    mask=mask,\n-                    scale=scale,\n-                    is_causal=is_causal,\n-                    flash_attention=flash_attention,\n-                )\n-            except ValueError as e:\n-                if e.args[0].startswith(\n-                    \"Flash attention is not supported in your \"\n-                    \"current JAX version\"\n-                ):\n-                    self.skipTest(\n-                        \"JAX version does not have \"\n-                        \"`dot_product_attention` function.\"\n-                    )\n-            except RuntimeError as e:\n-                if e.args[0] == \"cuDNN is not detected.\":\n-                    self.skipTest(\"No CuDNN to run flash attention for JAX.\")\n-                elif e.args[0] == \"Require at least Ampere arch to run\":\n-                    self.skipTest(\n-                        \"Requires at least Ampere arch to run flash attention \"\n-                        \"for JAX.\"\n-                    )\n-        else:\n+            # Flash attention only supports float16 and bfloat16. We multiply\n+            # 0.1 to avoid overflow.\n+            query = (query * 0.1).astype(\"float16\")\n+            key = (key * 0.1).astype(\"float16\")\n+            value = (value * 0.1).astype(\"float16\")\n+            if bias is not None:\n+                bias = (bias * 0.1).astype(\"float16\")\n+\n         outputs = knn.dot_product_attention(\n             query,\n             key,\n@@ -2370,7 +2382,9 @@ class NNOpsCorrectnessTest(testing.TestCase):\n             scale=scale,\n             is_causal=is_causal,\n         )\n-        self.assertAllClose(outputs, expected)\n+        self.assertAllClose(\n+            outputs, expected, atol=1e-3 if flash_attention else 1e-6\n+        )\n \n \n class NNOpsDtypeTest(testing.TestCase):\n@@ -2831,9 +2845,9 @@ class NNOpsDtypeTest(testing.TestCase):\n     def test_dot_product_attention(self, dtype):\n         # TODO: Get expected output from jax if `jax.nn.dot_product_attention`\n         # is available.\n-        query = knp.ones((2, 3, 3, 4), dtype=dtype)\n-        key = knp.ones((2, 3, 3, 4), dtype=dtype)\n-        value = knp.ones((2, 3, 3, 4), dtype=dtype)\n+        query = knp.ones((2, 3, 3, 8), dtype=dtype)\n+        key = knp.ones((2, 3, 3, 8), dtype=dtype)\n+        value = knp.ones((2, 3, 3, 8), dtype=dtype)\n         expected_dtype = dtype\n \n         self.assertDType(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8d6f4b8239067f4c0f141ab7f200e348618b063b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 1143 | Contributors (this commit): 8 | Commits (past 90d): 7 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -769,8 +769,8 @@ class BinaryFocalCrossentropy(LossFunctionWrapper):\n     As a standalone function:\n \n     >>> # Example 1: (batch_size = 1, number of samples = 4)\n-    >>> y_true = [0, 1, 0, 0]\n-    >>> y_pred = [-18.6, 0.51, 2.94, -12.8]\n+    >>> y_true = np.array([0, 1, 0, 0])\n+    >>> y_pred = np.array([-18.6, 0.51, 2.94, -12.8])\n     >>> loss = keras.losses.BinaryFocalCrossentropy(\n     ...    gamma=2, from_logits=True)\n     >>> loss(y_true, y_pred)\n@@ -783,8 +783,8 @@ class BinaryFocalCrossentropy(LossFunctionWrapper):\n     0.51\n \n     >>> # Example 2: (batch_size = 2, number of samples = 4)\n-    >>> y_true = [[0, 1], [0, 0]]\n-    >>> y_pred = [[-18.6, 0.51], [2.94, -12.8]]\n+    >>> y_true = np.array([[0, 1], [0, 0]])\n+    >>> y_pred = np.array([[-18.6, 0.51], [2.94, -12.8]])\n     >>> # Using default 'auto'/'sum_over_batch_size' reduction type.\n     >>> loss = keras.losses.BinaryFocalCrossentropy(\n     ...     gamma=3, from_logits=True)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cb99d0e036fee02f169e7f7ead51b78fcaa1133f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 148 | Lines Deleted: 0 | Files Changed: 15 | Hunks: 19 | Methods Changed: 13 | Complexity Δ (Sum/Max): 16/5 | Churn Δ: 148 | Churn Cumulative: 4985 | Contributors (this commit): 23 | Commits (past 90d): 145 | Contributors (cumulative): 106 | DMM Complexity: 1.0\n\nDIFF:\n@@ -28,6 +28,7 @@ from keras.src.activations.activations import selu\n from keras.src.activations.activations import sigmoid\n from keras.src.activations.activations import silu\n from keras.src.activations.activations import silu as swish\n+from keras.src.activations.activations import soft_shrink\n from keras.src.activations.activations import softmax\n from keras.src.activations.activations import softplus\n from keras.src.activations.activations import softsign\n\n@@ -94,6 +94,7 @@ from keras.src.ops.nn import separable_conv\n from keras.src.ops.nn import sigmoid\n from keras.src.ops.nn import silu\n from keras.src.ops.nn import silu as swish\n+from keras.src.ops.nn import soft_shrink\n from keras.src.ops.nn import softmax\n from keras.src.ops.nn import softplus\n from keras.src.ops.nn import softsign\n\n@@ -39,6 +39,7 @@ from keras.src.ops.nn import separable_conv\n from keras.src.ops.nn import sigmoid\n from keras.src.ops.nn import silu\n from keras.src.ops.nn import silu as swish\n+from keras.src.ops.nn import soft_shrink\n from keras.src.ops.nn import softmax\n from keras.src.ops.nn import softplus\n from keras.src.ops.nn import softsign\n\n@@ -28,6 +28,7 @@ from keras.src.activations.activations import selu\n from keras.src.activations.activations import sigmoid\n from keras.src.activations.activations import silu\n from keras.src.activations.activations import silu as swish\n+from keras.src.activations.activations import soft_shrink\n from keras.src.activations.activations import softmax\n from keras.src.activations.activations import softplus\n from keras.src.activations.activations import softsign\n\n@@ -94,6 +94,7 @@ from keras.src.ops.nn import separable_conv\n from keras.src.ops.nn import sigmoid\n from keras.src.ops.nn import silu\n from keras.src.ops.nn import silu as swish\n+from keras.src.ops.nn import soft_shrink\n from keras.src.ops.nn import softmax\n from keras.src.ops.nn import softplus\n from keras.src.ops.nn import softsign\n\n@@ -39,6 +39,7 @@ from keras.src.ops.nn import separable_conv\n from keras.src.ops.nn import sigmoid\n from keras.src.ops.nn import silu\n from keras.src.ops.nn import silu as swish\n+from keras.src.ops.nn import soft_shrink\n from keras.src.ops.nn import softmax\n from keras.src.ops.nn import softplus\n from keras.src.ops.nn import softsign\n\n@@ -19,6 +19,7 @@ from keras.src.activations.activations import relu6\n from keras.src.activations.activations import selu\n from keras.src.activations.activations import sigmoid\n from keras.src.activations.activations import silu\n+from keras.src.activations.activations import soft_shrink\n from keras.src.activations.activations import softmax\n from keras.src.activations.activations import softplus\n from keras.src.activations.activations import softsign\n@@ -38,6 +39,7 @@ ALL_OBJECTS = {\n     selu,\n     softplus,\n     softsign,\n+    soft_shrink,\n     silu,\n     gelu,\n     glu,\n\n@@ -259,6 +259,24 @@ def softsign(x):\n     return ops.softsign(x)\n \n \n+@keras_export(\"keras.activations.soft_shrink\")\n+def soft_shrink(x, threshold=0.5):\n+    \"\"\"Soft Shrink activation function.\n+\n+    It is defined as:\n+\n+    `soft_shrink(x) = x - threshold` if `x > threshold`,\n+    `soft_shrink(x) = x + threshold` if `x < -threshold`,\n+    `soft_shrink(x) = 0` otherwise.\n+\n+    Args:\n+        x: Input tensor.\n+        threshold: Threshold value. Defaults to 0.5.\n+\n+    \"\"\"\n+    return ops.soft_shrink(x, threshold=threshold)\n+\n+\n @keras_export([\"keras.activations.silu\", \"keras.activations.swish\"])\n def silu(x):\n     \"\"\"Swish (or Silu) activation function.\n\n@@ -683,6 +683,19 @@ class ActivationsTest(testing.TestCase):\n         expected = hard_shrink(x)\n         self.assertAllClose(result, expected, rtol=1e-05)\n \n+    def test_soft_shrink(self):\n+        def soft_shrink(x, threshold=0.5):\n+            return np.where(\n+                x > threshold,\n+                x - threshold,\n+                np.where(x < -threshold, x + threshold, 0.0),\n+            )\n+\n+        x = np.random.random((2, 5))\n+        result = activations.soft_shrink(x[np.newaxis, :])[0]\n+        expected = soft_shrink(x)\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n     def test_elu(self):\n         x = np.random.random((2, 5))\n         result = activations.elu(x[np.newaxis, :])[0]\n\n@@ -53,6 +53,15 @@ def softsign(x):\n     return jnn.soft_sign(x)\n \n \n+def soft_shrink(x, threshold=0.5):\n+    x = convert_to_tensor(x)\n+    return jnp.where(\n+        x > threshold,\n+        x - threshold,\n+        jnp.where(x < -threshold, x + threshold, 0.0),\n+    )\n+\n+\n def silu(x):\n     x = convert_to_tensor(x)\n     return jnn.silu(x)\n\n@@ -50,6 +50,18 @@ def softsign(x):\n     return x / (np.array(1.0, x.dtype) + np.abs(x))\n \n \n+def soft_shrink(x, threshold=0.5):\n+    return np.where(\n+        x > threshold,\n+        np.array(x - threshold, dtype=x.dtype),\n+        np.where(\n+            x < -threshold,\n+            np.array(x + threshold, dtype=x.dtype),\n+            np.array(0.0, dtype=x.dtype),\n+        ),\n+    )\n+\n+\n def silu(x):\n     x = convert_to_tensor(x)\n     return x * sigmoid(x)\n\n@@ -42,6 +42,14 @@ def softsign(x):\n     return tf.nn.softsign(x)\n \n \n+def soft_shrink(x, threshold=0.5):\n+    return tf.where(\n+        x > threshold,\n+        x - threshold,\n+        tf.where(x < -threshold, x + threshold, tf.zeros_like(x)),\n+    )\n+\n+\n def silu(x):\n     return tf.nn.silu(x)\n \n\n@@ -50,6 +50,11 @@ def softsign(x):\n     return tnn.softsign(x)\n \n \n+def soft_shrink(x, threshold=0.5):\n+    x = convert_to_tensor(x)\n+    return tnn.softshrink(x, lambd=threshold)\n+\n+\n def silu(x):\n     x = convert_to_tensor(x)\n     return tnn.silu(x)\n\n@@ -174,6 +174,48 @@ def softsign(x):\n     return backend.nn.softsign(x)\n \n \n+class SoftShrink(Operation):\n+    def __init__(self, threshold=0.5):\n+        super().__init__()\n+        self.threshold = threshold\n+\n+    def call(self, x):\n+        return backend.nn.soft_shrink(x, self.threshold)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape, dtype=x.dtype)\n+\n+\n+@keras_export([\"keras.ops.soft_shrink\", \"keras.ops.nn.soft_shrink\"])\n+def soft_shrink(x, threshold=0.5):\n+    \"\"\"Soft Shrink activation function.\n+\n+    It is defined as\n+\n+    `f(x) = x - threshold` if `x > threshold`,\n+    `f(x) = x + threshold` if `x < -threshold`,\n+    `f(x) = 0` otherwise.\n+\n+    Args:\n+        x: Input tensor.\n+        threshold: Threshold value. Defaults to 0.5.\n+\n+    Returns:\n+        A tensor with the same shape as `x`.\n+\n+    Example:\n+\n+    >>> x = np.array([-1.0, 0.0, 1.0])\n+    >>> x_soft_shrink = keras.ops.soft_shrink(x)\n+    >>> print(x_soft_shrink)\n+    array([-0.5  0.   0.5], shape=(3,), dtype=float64)\n+\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return SoftShrink(threshold).symbolic_call(x)\n+    return backend.nn.soft_shrink(x, threshold)\n+\n+\n class Silu(Operation):\n     def call(self, x):\n         return backend.nn.silu(x)\n\n@@ -160,6 +160,10 @@ class NNOpsDynamicShapeTest(testing.TestCase):\n         x = KerasTensor([None, 2, 3])\n         self.assertEqual(knn.hard_shrink(x).shape, (None, 2, 3))\n \n+    def test_soft_shrink(self):\n+        x = KerasTensor([None, 2, 3])\n+        self.assertEqual(knn.soft_shrink(x).shape, (None, 2, 3))\n+\n     def test_softmax(self):\n         x = KerasTensor([None, 2, 3])\n         self.assertEqual(knn.softmax(x).shape, (None, 2, 3))\n@@ -825,6 +829,10 @@ class NNOpsStaticShapeTest(testing.TestCase):\n         x = KerasTensor([1, 2, 3])\n         self.assertEqual(knn.hard_shrink(x).shape, (1, 2, 3))\n \n+    def test_soft_shrink(self):\n+        x = KerasTensor([1, 2, 3])\n+        self.assertEqual(knn.soft_shrink(x).shape, (1, 2, 3))\n+\n     def test_softmax(self):\n         x = KerasTensor([1, 2, 3])\n         self.assertEqual(knn.softmax(x).shape, (1, 2, 3))\n@@ -1366,6 +1374,13 @@ class NNOpsCorrectnessTest(testing.TestCase):\n             [0.0, 0.0, 1.0, 2.0, 3.0],\n         )\n \n+    def test_soft_shrink(self):\n+        x = np.array([-0.5, 0, 1, 2, 3], dtype=np.float32)\n+        self.assertAllClose(\n+            knn.soft_shrink(x),\n+            [0.0, 0.0, 0.5, 1.5, 2.5],\n+        )\n+\n     def test_softmax(self):\n         x = np.array([[1, 2, 3], [1, 2, 3]], dtype=np.float32)\n         self.assertAllClose(\n@@ -2524,6 +2539,24 @@ class NNOpsDtypeTest(testing.TestCase):\n             expected_dtype,\n         )\n \n+    @parameterized.named_parameters(named_product(dtype=FLOAT_DTYPES))\n+    def test_soft_shrink(self, dtype):\n+        import torch\n+        import torch.nn.functional as tnn\n+\n+        x = knp.ones((1), dtype=dtype)\n+        x_torch = torch.ones(1, dtype=getattr(torch, dtype))\n+        expected_dtype = standardize_dtype(tnn.softshrink(x_torch).dtype)\n+\n+        self.assertEqual(\n+            standardize_dtype(knn.soft_shrink(x).dtype),\n+            expected_dtype,\n+        )\n+        self.assertEqual(\n+            standardize_dtype(knn.SoftShrink().symbolic_call(x).dtype),\n+            expected_dtype,\n+        )\n+\n     @parameterized.named_parameters(named_product(dtype=FLOAT_DTYPES))\n     def test_glu(self, dtype):\n         import jax.nn as jnn\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
