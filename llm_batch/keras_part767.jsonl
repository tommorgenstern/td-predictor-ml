{"custom_id": "keras#8c126e33fb915d400efee15c58c56dacf90f91ad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 37 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 9 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 45 | Churn Cumulative: 196 | Contributors (this commit): 1 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -158,8 +158,10 @@ class EarlyStopping(Callback):\n             # If no monitor value exists or still in initial warm-up stage.\n             return\n         if self.restore_best_weights and self.best_weights is None:\n-            # Restore the weights after first epoch if no progress is ever made.\n+            # If best weights were never set,\n+            # then the current weights are the best.\n             self.best_weights = self.model.get_weights()\n+            self.best_epoch = epoch\n \n         self.wait += 1\n         if self._is_improvement(current, self.best):\n@@ -175,10 +177,16 @@ class EarlyStopping(Callback):\n                 self.wait = 0\n             return\n \n-        # Only check after the first epoch.\n         if self.wait >= self.patience and epoch > 0:\n+            # Patience has been exceeded: stop training\n             self.stopped_epoch = epoch\n             self.model.stop_training = True\n+\n+    def on_train_end(self, logs=None):\n+        if self.stopped_epoch > 0 and self.verbose > 0:\n+            io_utils.print_msg(\n+                f\"Epoch {self.stopped_epoch + 1}: early stopping\"\n+            )\n         if self.restore_best_weights and self.best_weights is not None:\n             if self.verbose > 0:\n                 io_utils.print_msg(\n@@ -188,12 +196,6 @@ class EarlyStopping(Callback):\n                 )\n             self.model.set_weights(self.best_weights)\n \n-    def on_train_end(self, logs=None):\n-        if self.stopped_epoch > 0 and self.verbose > 0:\n-            io_utils.print_msg(\n-                f\"Epoch {self.stopped_epoch + 1}: early stopping\"\n-            )\n-\n     def get_monitor_value(self, logs):\n         logs = logs or {}\n         monitor_value = logs.get(self.monitor)\n\n@@ -176,6 +176,7 @@ class EarlyStoppingTest(testing.TestCase):\n             early_stop.on_epoch_end(epoch, logs={\"val_loss\": losses[epoch]})\n             if early_stop.model.stop_training:\n                 break\n+        early_stop.on_train_end()\n         # The best configuration is in epoch 2 (loss = 0.1000),\n         # and while patience = 2, we're restoring the best weights,\n         # so we end up at the epoch with the best weights, i.e. epoch 2\n@@ -199,11 +200,37 @@ class EarlyStoppingTest(testing.TestCase):\n             early_stop.on_epoch_end(epoch, logs={\"val_loss\": losses[epoch]})\n             if early_stop.model.stop_training:\n                 break\n+        early_stop.on_train_end()\n         # No epoch improves on the baseline, so we should train for only 5\n         # epochs, and restore the second model.\n         self.assertEqual(epochs_trained, 5)\n         self.assertEqual(early_stop.model.get_weights(), 2)\n \n+        # Check weight restoration when another callback requests a stop.\n+        early_stop = callbacks.EarlyStopping(\n+            monitor=\"val_loss\",\n+            patience=5,\n+            baseline=0.5,\n+            restore_best_weights=True,\n+        )\n+        early_stop.set_model(DummyModel())\n+        losses = [0.9, 0.8, 0.7, 0.71, 0.72, 0.73]\n+        # The best configuration is in the epoch 2 (loss = 0.7000).\n+        epochs_trained = 0\n+        early_stop.on_train_begin()\n+        for epoch in range(len(losses)):\n+            epochs_trained += 1\n+            early_stop.model.set_weight_to_epoch(epoch=epoch)\n+            early_stop.on_epoch_end(epoch, logs={\"val_loss\": losses[epoch]})\n+            if epoch == 3:\n+                early_stop.model.stop_training = True\n+            if early_stop.model.stop_training:\n+                break\n+        early_stop.on_train_end()\n+        # We should restore the second model.\n+        self.assertEqual(epochs_trained, 4)\n+        self.assertEqual(early_stop.model.get_weights(), 2)\n+\n     @pytest.mark.requires_trainable_backend\n     def test_early_stopping_with_start_from_epoch(self):\n         x_train = np.random.random((10, 5))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#03f4a4fe3c0b2be47681e98275c11cd4bc786c8f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 27 | Files Changed: 12 | Hunks: 12 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 54 | Churn Cumulative: 7916 | Contributors (this commit): 15 | Commits (past 90d): 33 | Contributors (cumulative): 95 | DMM Complexity: None\n\nDIFF:\n@@ -37,8 +37,8 @@ class BaseConv(Layer):\n             incompatible with `dilation_rate > 1`.\n         padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n             `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n-            the left/right or up/down of the input such that output has the same\n-            height/width dimension as the input.\n+            the left/right or up/down of the input. When `padding=\"same\"` and\n+            `strides=1`, the output has the same size as the input.\n         data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n             The ordering of the dimensions in the inputs. `\"channels_last\"`\n             corresponds to inputs with shape `(batch, steps, features)`\n\n@@ -49,8 +49,8 @@ class BaseDepthwiseConv(Layer):\n             `strides > 1` is incompatible with `dilation_rate > 1`.\n         padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n             `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n-            the left/right or up/down of the input such that output has the same\n-            height/width dimension as the input.\n+            the left/right or up/down of the input. When `padding=\"same\"` and\n+            `strides=1`, the output has the same size as the input.\n         data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n             The ordering of the dimensions in the inputs. `\"channels_last\"`\n             corresponds to inputs with shape `(batch, steps, features)`\n\n@@ -37,8 +37,8 @@ class BaseSeparableConv(Layer):\n             `stride value != 1` is incompatible with `dilation_rate != 1`.\n         padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n             `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n-            the left/right or up/down of the input such that output has the same\n-            height/width dimension as the input.\n+            the left/right or up/down of the input. When `padding=\"same\"` and\n+            `strides=1`, the output has the same size as the input.\n         data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n             The ordering of the dimensions in the inputs. `\"channels_last\"`\n             corresponds to inputs with shape `(batch, steps, features)`\n\n@@ -23,11 +23,11 @@ class Conv1D(BaseConv):\n             `dilation_rate > 1`.\n         padding: string, `\"valid\"`, `\"same\"` or `\"causal\"`(case-insensitive).\n             `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n-            the left/right or up/down of the input such that output has the same\n-            height/width dimension as the input. `\"causal\"` results in causal\n-            (dilated) convolutions, e.g. `output[t]` does not depend on\n-            `input[t+1:]`. Useful when modeling temporal data where the model\n-            should not violate the temporal order.\n+            the left/right or up/down of the input. When `padding=\"same\"` and\n+            `strides=1`, the output has the same size as the input.\n+            `\"causal\"` results in causal(dilated) convolutions, e.g. `output[t]`\n+            does not depend on`input[t+1:]`. Useful when modeling temporal data\n+            where the model should not violate the temporal order.\n             See [WaveNet: A Generative Model for Raw Audio, section2.1](\n             https://arxiv.org/abs/1609.03499).\n         data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n\n@@ -22,8 +22,8 @@ class Conv2D(BaseConv):\n             `dilation_rate > 1`.\n         padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n             `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n-            the left/right or up/down of the input such that output has the same\n-            height/width dimension as the input.\n+            the left/right or up/down of the input. When `padding=\"same\"` and\n+            `strides=1`, the output has the same size as the input.\n         data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n             The ordering of the dimensions in the inputs. `\"channels_last\"`\n             corresponds to inputs with shape\n\n@@ -27,8 +27,8 @@ class Conv2DTranspose(BaseConvTranspose):\n             `dilation_rate > 1`.\n         padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n             `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n-            the left/right or up/down of the input such that output has the same\n-            height/width dimension as the input.\n+            the left/right or up/down of the input. When `padding=\"same\"` and\n+            `strides=1`, the output has the same size as the input.\n         data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n             The ordering of the dimensions in the inputs. `\"channels_last\"`\n             corresponds to inputs with shape\n\n@@ -22,8 +22,8 @@ class Conv3D(BaseConv):\n             `dilation_rate > 1`.\n         padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n             `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n-            the left/right or up/down of the input such that output has the same\n-            height/width dimension as the input.\n+            the left/right or up/down of the input. When `padding=\"same\"` and\n+            `strides=1`, the output has the same size as the input.\n         data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n             The ordering of the dimensions in the inputs. `\"channels_last\"`\n             corresponds to inputs with shape\n\n@@ -27,8 +27,8 @@ class Conv3DTranspose(BaseConvTranspose):\n             `dilation_rate > 1`.\n         padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n             `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n-            the left/right or up/down of the input such that output has the same\n-            height/width dimension as the input.\n+            the left/right or up/down of the input. When `padding=\"same\"` and\n+            `strides=1`, the output has the same size as the input.\n         data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n             The ordering of the dimensions in the inputs. `\"channels_last\"`\n             corresponds to inputs with shape\n\n@@ -33,8 +33,8 @@ class DepthwiseConv1D(BaseDepthwiseConv):\n             `dilation_rate > 1`.\n         padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n             `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n-            the left/right or up/down of the input such that output has the same\n-            height/width dimension as the input.\n+            the left/right or up/down of the input. When `padding=\"same\"` and\n+            `strides=1`, the output has the same size as the input.\n         depth_multiplier: The number of depthwise convolution output channels\n             for each input channel. The total number of depthwise convolution\n             output channels will be equal to `input_channel * depth_multiplier`.\n\n@@ -33,8 +33,8 @@ class DepthwiseConv2D(BaseDepthwiseConv):\n             `dilation_rate > 1`.\n         padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n             `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n-            the left/right or up/down of the input such that output has the same\n-            height/width dimension as the input.\n+            the left/right or up/down of the input. When `padding=\"same\"` and\n+            `strides=1`, the output has the same size as the input.\n         depth_multiplier: The number of depthwise convolution output channels\n             for each input channel. The total number of depthwise convolution\n             output channels will be equal to `input_channel * depth_multiplier`.\n\n@@ -28,8 +28,8 @@ class SeparableConv1D(BaseSeparableConv):\n             incompatible with `dilation_rate > 1`.\n         padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n             `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n-            the left/right or up/down of the input such that output has the same\n-            height/width dimension as the input.\n+            the left/right or up/down of the input. When `padding=\"same\"` and\n+            `strides=1`, the output has the same size as the input.\n         data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n             The ordering of the dimensions in the inputs. `\"channels_last\"`\n             corresponds to inputs with shape `(batch, steps, features)`\n\n@@ -28,8 +28,8 @@ class SeparableConv2D(BaseSeparableConv):\n             incompatible with `dilation_rate > 1`.\n         padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n             `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n-            the left/right or up/down of the input such that output has the same\n-            height/width dimension as the input.\n+            the left/right or up/down of the input. When `padding=\"same\"` and\n+            `strides=1`, the output has the same size as the input.\n         data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n             The ordering of the dimensions in the inputs. `\"channels_last\"`\n             corresponds to inputs with shape `(batch, steps, features)`\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b4c3a0e163603855f03316b0b97f2c9c25e133eb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 89 | Lines Deleted: 23 | Files Changed: 6 | Hunks: 19 | Methods Changed: 16 | Complexity Δ (Sum/Max): 10/3 | Churn Δ: 112 | Churn Cumulative: 8725 | Contributors (this commit): 7 | Commits (past 90d): 123 | Contributors (cumulative): 29 | DMM Complexity: 0.0\n\nDIFF:\n@@ -583,8 +583,17 @@ def outer(x1, x2):\n     return jnp.outer(x1, x2)\n \n \n-def pad(x, pad_width, mode=\"constant\"):\n-    return jnp.pad(x, pad_width, mode=mode)\n+def pad(x, pad_width, mode=\"constant\", constant_values=None):\n+    kwargs = {}\n+    if constant_values is not None:\n+        if mode != \"constant\":\n+            raise ValueError(\n+                \"Argument 'constant_values' can only be \"\n+                \"provided when mode == 'constant'. \"\n+                f\"Received mode '{mode}'\"\n+            )\n+        kwargs[\"constant_values\"] = constant_values\n+    return jnp.pad(x, pad_width, mode=mode, **kwargs)\n \n \n def prod(x, axis=None, keepdims=False, dtype=None):\n\n@@ -704,8 +704,17 @@ def outer(x1, x2):\n     return np.outer(x1, x2)\n \n \n-def pad(x, pad_width, mode=\"constant\"):\n-    return np.pad(x, pad_width, mode=mode)\n+def pad(x, pad_width, mode=\"constant\", constant_values=None):\n+    kwargs = {}\n+    if constant_values is not None:\n+        if mode != \"constant\":\n+            raise ValueError(\n+                \"Argument 'constant_values' can only be \"\n+                \"provided when mode == 'constant'. \"\n+                f\"Received mode '{mode}'\"\n+            )\n+        kwargs[\"constant_values\"] = constant_values\n+    return np.pad(x, pad_width, mode=mode, **kwargs)\n \n \n def prod(x, axis=None, keepdims=False, dtype=None):\n\n@@ -1060,8 +1060,17 @@ def outer(x1, x2):\n     return tfnp.outer(x1, x2)\n \n \n-def pad(x, pad_width, mode=\"constant\"):\n-    return tfnp.pad(x, pad_width, mode=mode)\n+def pad(x, pad_width, mode=\"constant\", constant_values=None):\n+    kwargs = {}\n+    if constant_values is not None:\n+        if mode != \"constant\":\n+            raise ValueError(\n+                \"Argument 'constant_values' can only be \"\n+                \"provided when mode == 'constant'. \"\n+                f\"Received mode '{mode}'\"\n+            )\n+        kwargs[\"constant_values\"] = constant_values\n+    return tfnp.pad(x, pad_width, mode=mode, **kwargs)\n \n \n def prod(x, axis=None, keepdims=False, dtype=None):\n\n@@ -949,7 +949,16 @@ def outer(x1, x2):\n     return torch.outer(x1.flatten(), x2.flatten())\n \n \n-def pad(x, pad_width, mode=\"constant\"):\n+def pad(x, pad_width, mode=\"constant\", constant_values=None):\n+    kwargs = {}\n+    if constant_values is not None:\n+        if mode != \"constant\":\n+            raise ValueError(\n+                \"Argument 'constant_values' can only be \"\n+                \"provided when mode == 'constant'. \"\n+                f\"Received mode '{mode}'\"\n+            )\n+        kwargs[\"value\"] = constant_values\n     x = convert_to_tensor(x)\n     pad_sum = []\n     pad_width = list(pad_width)[::-1]  # torch uses reverse order\n@@ -964,8 +973,7 @@ def pad(x, pad_width, mode=\"constant\"):\n     if mode == \"symmetric\":\n         mode = \"replicate\"\n     if mode == \"constant\":\n-        return torch.nn.functional.pad(x, pad=pad_sum, mode=mode)\n-\n+        return torch.nn.functional.pad(x, pad=pad_sum, mode=mode, **kwargs)\n     # TODO: reflect and symmetric padding are implemented for padding the\n     # last 3 dimensions of a 4D or 5D input tensor, the last 2 dimensions of a\n     # 3D or 4D input tensor, or the last dimension of a 2D or 3D input tensor.\n\n@@ -4197,10 +4197,15 @@ class Pad(Operation):\n                 pad_width[i] = (pw[0], pw[0])\n         return pad_width\n \n-    def call(self, x):\n-        return backend.numpy.pad(x, pad_width=self.pad_width, mode=self.mode)\n+    def call(self, x, constant_values=None):\n+        return backend.numpy.pad(\n+            x,\n+            pad_width=self.pad_width,\n+            mode=self.mode,\n+            constant_values=constant_values,\n+        )\n \n-    def compute_output_spec(self, x):\n+    def compute_output_spec(self, x, constant_values=None):\n         output_shape = list(x.shape)\n         if len(self.pad_width) == 1:\n             pad_width = [self.pad_width[0] for _ in range(len(output_shape))]\n@@ -4221,7 +4226,7 @@ class Pad(Operation):\n \n \n @keras_export([\"keras.ops.pad\", \"keras.ops.numpy.pad\"])\n-def pad(x, pad_width, mode=\"constant\"):\n+def pad(x, pad_width, mode=\"constant\", constant_values=None):\n     \"\"\"Pad a tensor.\n \n     Args:\n@@ -4237,6 +4242,9 @@ def pad(x, pad_width, mode=\"constant\"):\n             `\"maximum\"`, `\"mean\"`, `\"median\"`, `\"minimum\"`,\n             `\"reflect\"`, `\"symmetric\"`, `\"wrap\"`, `\"empty\"`,\n             `\"circular\"`. Defaults to`\"constant\"`.\n+        constant_values: value to pad with if `mode == \"constant\"`.\n+            Defaults to `0`. A `ValueError` is raised if not None and\n+            `mode != \"constant\"`.\n \n     Note:\n         Torch backend only supports modes `\"constant\"`, `\"reflect\"`,\n@@ -4250,9 +4258,7 @@ def pad(x, pad_width, mode=\"constant\"):\n     Returns:\n         Padded tensor.\n     \"\"\"\n-    if any_symbolic_tensors((x,)):\n-        return Pad(pad_width, mode=mode).symbolic_call(x)\n-    return backend.numpy.pad(x, pad_width, mode=mode)\n+    return Pad(pad_width, mode=mode)(x, constant_values=constant_values)\n \n \n class Prod(Operation):\n\n@@ -3541,27 +3541,50 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n                 \"int32\",\n             ],\n             mode=[\"constant\", \"reflect\", \"symmetric\"],\n+            constant_values=[None, 0, 2],\n         )\n     )\n-    def test_pad(self, dtype, mode):\n+    def test_pad(self, dtype, mode, constant_values):\n         # 2D\n         x = np.ones([2, 3], dtype=dtype)\n         pad_width = ((1, 1), (1, 1))\n+\n+        if mode != \"constant\":\n+            if constant_values is not None:\n+                with self.assertRaisesRegex(\n+                    ValueError,\n+                    \"Argument 'constant_values' can only be \"\n+                    \"provided when mode == 'constant'\",\n+                ):\n+                    knp.pad(\n+                        x, pad_width, mode=mode, constant_values=constant_values\n+                    )\n+                return\n+            # constant_values is None\n+            kwargs = {}\n+        else:\n+            # mode is constant\n+            kwargs = {\"constant_values\": constant_values or 0}\n+\n         self.assertAllClose(\n-            knp.pad(x, pad_width, mode=mode), np.pad(x, pad_width, mode=mode)\n+            knp.pad(x, pad_width, mode=mode, constant_values=constant_values),\n+            np.pad(x, pad_width, mode=mode, **kwargs),\n         )\n         self.assertAllClose(\n-            knp.Pad(pad_width, mode=mode)(x), np.pad(x, pad_width, mode=mode)\n+            knp.Pad(pad_width, mode=mode)(x, constant_values=constant_values),\n+            np.pad(x, pad_width, mode=mode, **kwargs),\n         )\n \n         # 5D (pad last 3D)\n         x = np.ones([2, 3, 4, 5, 6], dtype=dtype)\n         pad_width = ((0, 0), (0, 0), (2, 3), (1, 1), (1, 1))\n         self.assertAllClose(\n-            knp.pad(x, pad_width, mode=mode), np.pad(x, pad_width, mode=mode)\n+            knp.pad(x, pad_width, mode=mode, constant_values=constant_values),\n+            np.pad(x, pad_width, mode=mode, **kwargs),\n         )\n         self.assertAllClose(\n-            knp.Pad(pad_width, mode=mode)(x), np.pad(x, pad_width, mode=mode)\n+            knp.Pad(pad_width, mode=mode)(x, constant_values=constant_values),\n+            np.pad(x, pad_width, mode=mode, **kwargs),\n         )\n \n         # 5D (pad arbitrary dimensions)\n@@ -3573,10 +3596,12 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         x = np.ones([2, 3, 4, 5, 6], dtype=dtype)\n         pad_width = ((1, 1), (2, 1), (3, 2), (4, 3), (5, 4))\n         self.assertAllClose(\n-            knp.pad(x, pad_width, mode=mode), np.pad(x, pad_width, mode=mode)\n+            knp.pad(x, pad_width, mode=mode, constant_values=constant_values),\n+            np.pad(x, pad_width, mode=mode, **kwargs),\n         )\n         self.assertAllClose(\n-            knp.Pad(pad_width, mode=mode)(x), np.pad(x, pad_width, mode=mode)\n+            knp.Pad(pad_width, mode=mode)(x, constant_values=constant_values),\n+            np.pad(x, pad_width, mode=mode, **kwargs),\n         )\n \n     def test_prod(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9620d23ab1d672e0611d3c9bd0f77a11a20b3204", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 18 | Files Changed: 6 | Hunks: 7 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 38 | Churn Cumulative: 8763 | Contributors (this commit): 7 | Commits (past 90d): 129 | Contributors (cumulative): 29 | DMM Complexity: 0.0\n\nDIFF:\n@@ -588,9 +588,9 @@ def pad(x, pad_width, mode=\"constant\", constant_values=None):\n     if constant_values is not None:\n         if mode != \"constant\":\n             raise ValueError(\n-                \"Argument 'constant_values' can only be \"\n-                \"provided when mode == 'constant'. \"\n-                f\"Received mode '{mode}'\"\n+                \"Argument `constant_values` can only be \"\n+                \"provided when `mode == 'constant'`. \"\n+                f\"Received: mode={mode}\"\n             )\n         kwargs[\"constant_values\"] = constant_values\n     return jnp.pad(x, pad_width, mode=mode, **kwargs)\n\n@@ -709,9 +709,9 @@ def pad(x, pad_width, mode=\"constant\", constant_values=None):\n     if constant_values is not None:\n         if mode != \"constant\":\n             raise ValueError(\n-                \"Argument 'constant_values' can only be \"\n-                \"provided when mode == 'constant'. \"\n-                f\"Received mode '{mode}'\"\n+                \"Argument `constant_values` can only be \"\n+                \"provided when `mode == 'constant'`. \"\n+                f\"Received: mode={mode}\"\n             )\n         kwargs[\"constant_values\"] = constant_values\n     return np.pad(x, pad_width, mode=mode, **kwargs)\n\n@@ -1065,9 +1065,9 @@ def pad(x, pad_width, mode=\"constant\", constant_values=None):\n     if constant_values is not None:\n         if mode != \"constant\":\n             raise ValueError(\n-                \"Argument 'constant_values' can only be \"\n-                \"provided when mode == 'constant'. \"\n-                f\"Received mode '{mode}'\"\n+                \"Argument `constant_values` can only be \"\n+                \"provided when `mode == 'constant'`. \"\n+                f\"Received: mode={mode}\"\n             )\n         kwargs[\"constant_values\"] = constant_values\n     return tfnp.pad(x, pad_width, mode=mode, **kwargs)\n\n@@ -954,9 +954,9 @@ def pad(x, pad_width, mode=\"constant\", constant_values=None):\n     if constant_values is not None:\n         if mode != \"constant\":\n             raise ValueError(\n-                \"Argument 'constant_values' can only be \"\n-                \"provided when mode == 'constant'. \"\n-                f\"Received mode '{mode}'\"\n+                \"Argument `constant_values` can only be \"\n+                \"provided when `mode == 'constant'`. \"\n+                f\"Received: mode={mode}\"\n             )\n         kwargs[\"value\"] = constant_values\n     x = convert_to_tensor(x)\n\n@@ -4190,8 +4190,8 @@ class Pad(Operation):\n         for i, pw in enumerate(pad_width):\n             if len(pw) != first_len:\n                 raise ValueError(\n-                    \"`pad_width` should be a list of tuples of length 2 or \"\n-                    f\"1, but received {pad_width}.\"\n+                    \"`pad_width` should be a list of tuples of length \"\n+                    f\"1 or 2. Received: pad_width={pad_width}\"\n                 )\n             if len(pw) == 1:\n                 pad_width[i] = (pw[0], pw[0])\n@@ -4213,8 +4213,10 @@ class Pad(Operation):\n             pad_width = self.pad_width\n         else:\n             raise ValueError(\n-                \"`pad_width` must have the same length as `x.shape`, but \"\n-                f\"received {len(self.pad_width)} and {len(x.shape)}.\"\n+                \"`pad_width` must have the same length as `x.shape`. \"\n+                f\"Received: pad_width={self.pad_width} \"\n+                f\"(of length {len(self.pad_width)}) and x.shape={x.shape} \"\n+                f\"(of length {len(x.shape)})\"\n             )\n \n         for i in range(len(output_shape)):\n\n@@ -3553,8 +3553,8 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             if constant_values is not None:\n                 with self.assertRaisesRegex(\n                     ValueError,\n-                    \"Argument 'constant_values' can only be \"\n-                    \"provided when mode == 'constant'\",\n+                    \"Argument `constant_values` can only be \"\n+                    \"provided when `mode == 'constant'`\",\n                 ):\n                     knp.pad(\n                         x, pad_width, mode=mode, constant_values=constant_values\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0b20c8d2e2da734077dc43bc80d241d004c75424", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 198 | Contributors (this commit): 3 | Commits (past 90d): 12 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1073,9 +1073,9 @@ class Layer(BackendLayer, Operation):\n \n     @property\n     def losses(self):\n-        \"\"\"List of scalar losses added via `add_loss()` during layer call.\"\"\"\n+        \"\"\"List of scalar losses from `add_loss`, regularizers and sublayers.\"\"\"\n         losses = self._get_own_losses()\n-        for layer in self._layers:\n+        for layer in self._flatten_layers(include_self=False):\n             losses.extend(layer._get_own_losses())\n         weight_regularization_losses = []\n         for v in self.trainable_weights:\n\n@@ -328,6 +328,12 @@ class LayerTest(testing.TestCase):\n         self.assertLen(model.losses, 1)\n         self.assertAllClose(model.losses[0], 1.0)\n \n+        # It works recursively in nested models\n+        model = models.Sequential([model])\n+        model(np.ones((1,)))\n+        self.assertLen(model.losses, 1)\n+        self.assertAllClose(model.losses[0], 1.0)\n+\n     def test_training_arg_value_resolution(self):\n         # Check that even if `training` is not passed\n         # to an inner layer, the outer value gets propagated\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8e078e1c5c6be71a393293d01358135dd90d819b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 83 | Lines Deleted: 11 | Files Changed: 4 | Hunks: 11 | Methods Changed: 6 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 94 | Churn Cumulative: 2383 | Contributors (this commit): 16 | Commits (past 90d): 8 | Contributors (cumulative): 25 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,11 +1,70 @@\n+import numpy as np\n+\n from keras.api_export import keras_export\n+from keras.utils.file_utils import get_file\n \n \n @keras_export(\"keras.datasets.boston_housing.load_data\")\n def load_data(path=\"boston_housing.npz\", test_split=0.2, seed=113):\n-    raise NotImplementedError(\n-        \"The Boston Housing dataset is no longer distributed with Keras. \"\n-        \"We recommend that you use instead the \"\n-        \"California Housing dataset, available via \"\n-        \"`keras.datasets.california_housing.load_data()`.\"\n+    \"\"\"Loads the Boston Housing dataset.\n+\n+    This is a dataset taken from the StatLib library which is maintained at\n+    Carnegie Mellon University.\n+\n+    **WARNING:** This dataset has an ethical problem: the authors of this\n+    dataset included a variable, \"B\", that may appear to assume that racial\n+    self-segregation influences house prices. As such, we strongly discourage\n+    the use of this dataset, unless in the context of illustrating ethical\n+    issues in data science and machine learning.\n+\n+    Samples contain 13 attributes of houses at different locations around the\n+    Boston suburbs in the late 1970s. Targets are the median values of\n+    the houses at a location (in k$).\n+\n+    The attributes themselves are defined in the\n+    [StatLib website](http://lib.stat.cmu.edu/datasets/boston).\n+\n+    Args:\n+        path: path where to cache the dataset locally\n+            (relative to `~/.keras/datasets`).\n+        test_split: fraction of the data to reserve as test set.\n+        seed: Random seed for shuffling the data\n+            before computing the test split.\n+\n+    Returns:\n+        Tuple of NumPy arrays: `(x_train, y_train), (x_test, y_test)`.\n+\n+    **x_train, x_test**: NumPy arrays with shape `(num_samples, 13)`\n+        containing either the training samples (for x_train),\n+        or test samples (for y_train).\n+\n+    **y_train, y_test**: NumPy arrays of shape `(num_samples,)` containing the\n+        target scalars. The targets are float scalars typically between 10 and\n+        50 that represent the home prices in k$.\n+    \"\"\"\n+    assert 0 <= test_split < 1\n+    origin_folder = (\n+        \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/\"\n     )\n+    path = get_file(\n+        path,\n+        origin=origin_folder + \"boston_housing.npz\",\n+        file_hash=(  # noqa: E501\n+            \"f553886a1f8d56431e820c5b82552d9d95cfcb96d1e678153f8839538947dff5\"\n+        ),\n+    )\n+    with np.load(path, allow_pickle=True) as f:\n+        x = f[\"x\"]\n+        y = f[\"y\"]\n+\n+    rng = np.random.RandomState(seed)\n+    indices = np.arange(len(x))\n+    rng.shuffle(indices)\n+    x = x[indices]\n+    y = y[indices]\n+\n+    x_train = np.array(x[: int(len(x) * (1 - test_split))])\n+    y_train = np.array(y[: int(len(x) * (1 - test_split))])\n+    x_test = np.array(x[int(len(x) * (1 - test_split)) :])\n+    y_test = np.array(y[int(len(x) * (1 - test_split)) :])\n+    return (x_train, y_train), (x_test, y_test)\n\n@@ -6,8 +6,10 @@ from keras.api_export import keras_export\n from keras.utils.file_utils import get_file\n \n \n-@keras_export(\"keras.datasets.boston_housing.load_data\")\n-def load_data(path=\"california_housing.npz\", test_split=0.2, seed=113):\n+@keras_export(\"keras.datasets.california_housing.load_data\")\n+def load_data(\n+    version=\"large\", path=\"california_housing.npz\", test_split=0.2, seed=113\n+):\n     \"\"\"Loads the California Housing dataset.\n \n     This dataset was obtained from the [StatLib repository](\n@@ -42,6 +44,11 @@ def load_data(path=\"california_housing.npz\", test_split=0.2, seed=113):\n     such as vacation resorts.\n \n     Args:\n+        version: `\"small\"` or `\"large\"`. The small version\n+            contains 600 samples, the large version contains\n+            20,640 samples. The purpose of the small version is\n+            to serve as an approximate replacement for the\n+            deprecated `boston_housing` dataset.\n         path: path where to cache the dataset locally\n             (relative to `~/.keras/datasets`).\n         test_split: fraction of the data to reserve as test set.\n@@ -75,6 +82,15 @@ def load_data(path=\"california_housing.npz\", test_split=0.2, seed=113):\n         x = f[\"x\"]\n         y = f[\"y\"]\n \n+    if version == \"small\":\n+        x = x[:600]\n+        y = y[:600]\n+    elif version != \"large\":\n+        raise ValueError(\n+            \"Argument `version` must be one of 'small', 'large'. \"\n+            f\"Received: version={version}\"\n+        )\n+\n     rng = np.random.RandomState(seed)\n     indices = np.arange(len(x))\n     rng.shuffle(indices)\n\n@@ -233,7 +233,6 @@ class Discretization(TFDataLayer):\n             output_mode=self.output_mode,\n             depth=len(self.bin_boundaries) + 1,\n             dtype=self.compute_dtype,\n-            count_weights=None,\n             backend_module=self.backend,\n         )\n         return outputs\n\n@@ -112,10 +112,9 @@ def encode_categorical_inputs(\n     output_mode,\n     depth,\n     dtype=\"float32\",\n-    count_weights=None,\n     backend_module=None,\n ):\n-    \"\"\"Encodes categoical inputs according to output_mode.\"\"\"\n+    \"\"\"Encodes categorical inputs according to output_mode.\"\"\"\n     backend_module = backend_module or backend\n \n     if output_mode == \"int\":\n@@ -149,5 +148,4 @@ def encode_categorical_inputs(\n             minlength=depth,\n         )\n     bincounts = backend_module.cast(bincounts, dtype)\n-\n     return bincounts\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0a38605c74fc95b70c34487ce559983c551ecd92", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 373 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 10 | Complexity Δ (Sum/Max): 18/18 | Churn Δ: 373 | Churn Cumulative: 373 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,373 @@\n+\"\"\"\n+Title: MobileViT: A mobile-friendly Transformer-based model for image classification\n+Author: [Sayak Paul](https://twitter.com/RisingSayak)\n+Converted to Keras 3 by: [Pavan Kumar Singh](https://github.com/pksX01)\n+Date created: 2021/10/20\n+Last modified: 2023/11/23\n+Description: MobileViT for image classification with combined benefits of convolutions and Transformers.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+In this example, we implement the MobileViT architecture\n+([Mehta et al.](https://arxiv.org/abs/2110.02178)),\n+which combines the benefits of Transformers\n+([Vaswani et al.](https://arxiv.org/abs/1706.03762))\n+and convolutions. With Transformers, we can capture long-range dependencies that result\n+in global representations. With convolutions, we can capture spatial relationships that\n+model locality.\n+\n+Besides combining the properties of Transformers and convolutions, the authors introduce\n+MobileViT as a general-purpose mobile-friendly backbone for different image recognition\n+tasks. Their findings suggest that, performance-wise, MobileViT is better than other\n+models with the same or higher complexity ([MobileNetV3](https://arxiv.org/abs/1905.02244),\n+for example), while being efficient on mobile devices.\n+\"\"\"\n+\n+\"\"\"\n+## Imports\n+\"\"\"\n+import os\n+os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n+\n+import tensorflow as tf\n+\n+from keras import layers\n+import keras as keras\n+\n+import tensorflow_datasets as tfds\n+\n+tfds.disable_progress_bar()\n+\n+\"\"\"\n+## Hyperparameters\n+\"\"\"\n+\n+# Values are from table 4.\n+patch_size = 4  # 2x2, for the Transformer blocks.\n+image_size = 256\n+expansion_factor = 2  # expansion factor for the MobileNetV2 blocks.\n+\n+\"\"\"\n+## MobileViT utilities\n+\n+The MobileViT architecture is comprised of the following blocks:\n+\n+* Strided 3x3 convolutions that process the input image.\n+* [MobileNetV2](https://arxiv.org/abs/1801.04381)-style inverted residual blocks for\n+downsampling the resolution of the intermediate feature maps.\n+* MobileViT blocks that combine the benefits of Transformers and convolutions. It is\n+presented in the figure below (taken from the\n+[original paper](https://arxiv.org/abs/2110.02178)):\n+\n+\n+![](https://i.imgur.com/mANnhI7.png)\n+\"\"\"\n+\n+\n+def conv_block(x, filters=16, kernel_size=3, strides=2):\n+    conv_layer = layers.Conv2D(\n+        filters, kernel_size, strides=strides, activation=keras.activations.swish, padding=\"same\"\n+    )\n+    return conv_layer(x)\n+\n+\n+# Reference: https://git.io/JKgtC\n+\n+\n+def inverted_residual_block(x, expanded_channels, output_channels, strides=1):\n+    m = layers.Conv2D(expanded_channels, 1, padding=\"same\", use_bias=False)(x)\n+    m = layers.BatchNormalization()(m)\n+    m = keras.activations.swish(m)\n+\n+    if strides == 2:\n+        m = layers.ZeroPadding2D()(m)\n+    m = layers.DepthwiseConv2D(\n+        3, strides=strides, padding=\"same\" if strides == 1 else \"valid\", use_bias=False\n+    )(m)\n+    m = layers.BatchNormalization()(m)\n+    m = keras.activations.swish(m)\n+\n+    m = layers.Conv2D(output_channels, 1, padding=\"same\", use_bias=False)(m)\n+    m = layers.BatchNormalization()(m)\n+\n+    if x.shape[-1] == output_channels and strides == 1:\n+        return layers.Add()([m, x])\n+    return m\n+\n+\n+# Reference:\n+# https://keras.io/examples/vision/image_classification_with_vision_transformer/\n+\n+\n+def mlp(x, hidden_units, dropout_rate):\n+    for units in hidden_units:\n+        x = layers.Dense(units, activation=keras.activations.swish)(x)\n+        x = layers.Dropout(dropout_rate)(x)\n+    return x\n+\n+\n+def transformer_block(x, transformer_layers, projection_dim, num_heads=2):\n+    for _ in range(transformer_layers):\n+        # Layer normalization 1.\n+        x1 = layers.LayerNormalization(epsilon=1e-6)(x)\n+        # Create a multi-head attention layer.\n+        attention_output = layers.MultiHeadAttention(\n+            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n+        )(x1, x1)\n+        # Skip connection 1.\n+        x2 = layers.Add()([attention_output, x])\n+        # Layer normalization 2.\n+        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n+        # MLP.\n+        x3 = mlp(\n+            x3,\n+            hidden_units=[x.shape[-1] * 2, x.shape[-1]],\n+            dropout_rate=0.1,\n+        )\n+        # Skip connection 2.\n+        x = layers.Add()([x3, x2])\n+\n+    return x\n+\n+\n+def mobilevit_block(x, num_blocks, projection_dim, strides=1):\n+    # Local projection with convolutions.\n+    local_features = conv_block(x, filters=projection_dim, strides=strides)\n+    local_features = conv_block(\n+        local_features, filters=projection_dim, kernel_size=1, strides=strides\n+    )\n+\n+    # Unfold into patches and then pass through Transformers.\n+    num_patches = int((local_features.shape[1] * local_features.shape[2]) / patch_size)\n+    non_overlapping_patches = layers.Reshape((patch_size, num_patches, projection_dim))(\n+        local_features\n+    )\n+    global_features = transformer_block(\n+        non_overlapping_patches, num_blocks, projection_dim\n+    )\n+\n+    # Fold into conv-like feature-maps.\n+    folded_feature_map = layers.Reshape((*local_features.shape[1:-1], projection_dim))(\n+        global_features\n+    )\n+\n+    # Apply point-wise conv -> concatenate with the input features.\n+    folded_feature_map = conv_block(\n+        folded_feature_map, filters=x.shape[-1], kernel_size=1, strides=strides\n+    )\n+    local_global_features = layers.Concatenate(axis=-1)([x, folded_feature_map])\n+\n+    # Fuse the local and global features using a convoluion layer.\n+    local_global_features = conv_block(\n+        local_global_features, filters=projection_dim, strides=strides\n+    )\n+\n+    return local_global_features\n+\n+\n+\"\"\"\n+**More on the MobileViT block**:\n+\n+* First, the feature representations (A) go through convolution blocks that capture local\n+relationships. The expected shape of a single entry here would be `(h, w, num_channels)`.\n+* Then they get unfolded into another vector with shape `(p, n, num_channels)`,\n+where `p` is the area of a small patch, and `n` is `(h * w) / p`. So, we end up with `n`\n+non-overlapping patches.\n+* This unfolded vector is then passed through a Tranformer block that captures global\n+relationships between the patches.\n+* The output vector (B) is again folded into a vector of shape `(h, w, num_channels)`\n+resembling a feature map coming out of convolutions.\n+\n+Vectors A and B are then passed through two more convolutional layers to fuse the local\n+and global representations. Notice how the spatial resolution of the final vector remains\n+unchanged at this point. The authors also present an explanation of how the MobileViT\n+block resembles a convolution block of a CNN. For more details, please refer to the\n+original paper.\n+\"\"\"\n+\n+\"\"\"\n+Next, we combine these blocks together and implement the MobileViT architecture (XXS\n+variant). The following figure (taken from the original paper) presents a schematic\n+representation of the architecture:\n+\n+![](https://i.ibb.co/sRbVRBN/image.png)\n+\"\"\"\n+\n+\n+def create_mobilevit(num_classes=5):\n+    inputs = keras.Input((image_size, image_size, 3))\n+    x = layers.Rescaling(scale=1.0 / 255)(inputs)\n+\n+    # Initial conv-stem -> MV2 block.\n+    x = conv_block(x, filters=16)\n+    x = inverted_residual_block(\n+        x, expanded_channels=16 * expansion_factor, output_channels=16\n+    )\n+\n+    # Downsampling with MV2 block.\n+    x = inverted_residual_block(\n+        x, expanded_channels=16 * expansion_factor, output_channels=24, strides=2\n+    )\n+    x = inverted_residual_block(\n+        x, expanded_channels=24 * expansion_factor, output_channels=24\n+    )\n+    x = inverted_residual_block(\n+        x, expanded_channels=24 * expansion_factor, output_channels=24\n+    )\n+\n+    # First MV2 -> MobileViT block.\n+    x = inverted_residual_block(\n+        x, expanded_channels=24 * expansion_factor, output_channels=48, strides=2\n+    )\n+    x = mobilevit_block(x, num_blocks=2, projection_dim=64)\n+\n+    # Second MV2 -> MobileViT block.\n+    x = inverted_residual_block(\n+        x, expanded_channels=64 * expansion_factor, output_channels=64, strides=2\n+    )\n+    x = mobilevit_block(x, num_blocks=4, projection_dim=80)\n+\n+    # Third MV2 -> MobileViT block.\n+    x = inverted_residual_block(\n+        x, expanded_channels=80 * expansion_factor, output_channels=80, strides=2\n+    )\n+    x = mobilevit_block(x, num_blocks=3, projection_dim=96)\n+    x = conv_block(x, filters=320, kernel_size=1, strides=1)\n+\n+    # Classification head.\n+    x = layers.GlobalAvgPool2D()(x)\n+    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n+\n+    return keras.Model(inputs, outputs)\n+\n+\n+mobilevit_xxs = create_mobilevit()\n+mobilevit_xxs.summary()\n+\n+\"\"\"\n+## Dataset preparation\n+\n+We will be using the\n+[`tf_flowers`](https://www.tensorflow.org/datasets/catalog/tf_flowers)\n+dataset to demonstrate the model. Unlike other Transformer-based architectures,\n+MobileViT uses a simple augmentation pipeline primarily because it has the properties\n+of a CNN.\n+\"\"\"\n+\n+batch_size = 64\n+auto = tf.data.AUTOTUNE\n+resize_bigger = 280\n+num_classes = 5\n+\n+\n+def preprocess_dataset(is_training=True):\n+    def _pp(image, label):\n+        if is_training:\n+            # Resize to a bigger spatial resolution and take the random\n+            # crops.\n+            image = tf.image.resize(image, (resize_bigger, resize_bigger))\n+            image = tf.image.random_crop(image, (image_size, image_size, 3))\n+            image = tf.image.random_flip_left_right(image)\n+        else:\n+            image = tf.image.resize(image, (image_size, image_size))\n+        label = tf.one_hot(label, depth=num_classes)\n+        return image, label\n+\n+    return _pp\n+\n+\n+def prepare_dataset(dataset, is_training=True):\n+    if is_training:\n+        dataset = dataset.shuffle(batch_size * 10)\n+    dataset = dataset.map(preprocess_dataset(is_training), num_parallel_calls=auto)\n+    return dataset.batch(batch_size).prefetch(auto)\n+\n+\n+\"\"\"\n+The authors use a multi-scale data sampler to help the model learn representations of\n+varied scales. In this example, we discard this part.\n+\"\"\"\n+\n+\"\"\"\n+## Load and prepare the dataset\n+\"\"\"\n+\n+train_dataset, val_dataset = tfds.load(\n+    \"tf_flowers\", split=[\"train[:90%]\", \"train[90%:]\"], as_supervised=True\n+)\n+\n+num_train = train_dataset.cardinality()\n+num_val = val_dataset.cardinality()\n+print(f\"Number of training examples: {num_train}\")\n+print(f\"Number of validation examples: {num_val}\")\n+\n+train_dataset = prepare_dataset(train_dataset, is_training=True)\n+val_dataset = prepare_dataset(val_dataset, is_training=False)\n+\n+\"\"\"\n+## Train a MobileViT (XXS) model\n+\"\"\"\n+\n+learning_rate = 0.002\n+label_smoothing_factor = 0.1\n+epochs = 30\n+\n+optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n+loss_fn = keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing_factor)\n+\n+\n+def run_experiment(epochs=epochs):\n+    mobilevit_xxs = create_mobilevit(num_classes=num_classes)\n+    mobilevit_xxs.compile(optimizer=optimizer, loss=loss_fn, metrics=[\"accuracy\"])\n+\n+    checkpoint_filepath = \"/tmp/checkpoint.weights.h5\"\n+    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n+        checkpoint_filepath,\n+        monitor=\"val_accuracy\",\n+        save_best_only=True,\n+        save_weights_only=True,\n+    )\n+\n+    mobilevit_xxs.fit(\n+        train_dataset,\n+        validation_data=val_dataset,\n+        epochs=epochs,\n+        callbacks=[checkpoint_callback],\n+    )\n+    mobilevit_xxs.load_weights(checkpoint_filepath)\n+    _, accuracy = mobilevit_xxs.evaluate(val_dataset)\n+    print(f\"Validation accuracy: {round(accuracy * 100, 2)}%\")\n+    return mobilevit_xxs\n+\n+\n+mobilevit_xxs = run_experiment()\n+\n+\"\"\"\n+## Results and TFLite conversion\n+\n+With about one million parameters, getting to ~85% top-1 accuracy on 256x256 resolution is\n+a strong result. This MobileViT mobile is fully compatible with TensorFlow Lite (TFLite)\n+and can be converted with the following code:\n+\"\"\"\n+\n+# Serialize the model as a SavedModel.\n+tf.saved_model.save(mobilevit_xxs, \"mobilevit_xxs\")\n+\n+# Convert to TFLite. This form of quantization is called\n+# post-training dynamic-range quantization in TFLite.\n+converter = tf.lite.TFLiteConverter.from_saved_model(\"mobilevit_xxs\")\n+converter.optimizations = [tf.lite.Optimize.DEFAULT]\n+converter.target_spec.supported_ops = [\n+    tf.lite.OpsSet.TFLITE_BUILTINS,  # Enable TensorFlow Lite ops.\n+    tf.lite.OpsSet.SELECT_TF_OPS,  # Enable TensorFlow ops.\n+]\n+tflite_model = converter.convert()\n+open(\"mobilevit_xxs.tflite\", \"wb\").write(tflite_model)\n+\n+\"\"\"\n+To learn more about different quantization recipes available in TFLite and running\n+inference with TFLite models, check out\n+[this official resource](https://www.tensorflow.org/lite/performance/post_training_quantization).\n+\"\"\"\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
