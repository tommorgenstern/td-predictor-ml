{"custom_id": "keras#406774b60ac6b505ae9bf7e8728b00a1523ad4a3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 587 | Lines Deleted: 515 | Files Changed: 28 | Hunks: 206 | Methods Changed: 100 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1102 | Churn Cumulative: 9502 | Contributors (this commit): 12 | Commits (past 90d): 196 | Contributors (cumulative): 150 | DMM Complexity: 0.15384615384615385\n\nDIFF:\n@@ -85,8 +85,8 @@ from keras.utils.generic_utils import serialize_keras_object\n def serialize(optimizer):\n     \"\"\"Serialize the optimizer configuration to JSON compatible python dict.\n \n-    The configuration can be used for persistence and reconstruct the `Optimizer`\n-    instance again.\n+    The configuration can be used for persistence and reconstruct the\n+    `Optimizer` instance again.\n \n     >>> tf.keras.optimizers.serialize(tf.keras.optimizers.SGD())\n     {'class_name': 'SGD', 'config': {'name': 'SGD', 'learning_rate': 0.01,\n@@ -109,7 +109,8 @@ def deserialize(config, custom_objects=None):\n     Args:\n         config: Optimizer configuration dictionary.\n         custom_objects: Optional dictionary mapping names (strings) to custom\n-          objects (classes and functions) to be considered during deserialization.\n+          objects (classes and functions) to be considered during\n+          deserialization.\n \n     Returns:\n         A Keras Optimizer instance.\n@@ -158,9 +159,10 @@ def get(identifier):\n     Args:\n         identifier: Optimizer identifier, one of\n             - String: name of an optimizer\n-            - Dictionary: configuration dictionary. - Keras Optimizer instance (it\n-              will be returned unchanged). - TensorFlow Optimizer instance (it\n-              will be wrapped as a Keras Optimizer).\n+            - Dictionary: configuration dictionary.\n+            - Keras Optimizer instance (it will be returned unchanged).\n+            - TensorFlow Optimizer instance (it will be wrapped as a Keras\n+              Optimizer).\n \n     Returns:\n         A Keras Optimizer instance.\n\n@@ -46,8 +46,9 @@ def exponential_decay(\n                             decay_rate ^ (global_step / decay_steps)\n     ```\n \n-    If the argument `staircase` is `True`, then `global_step / decay_steps` is an\n-    integer division and the decayed learning rate follows a staircase function.\n+    If the argument `staircase` is `True`, then `global_step / decay_steps` is\n+    an integer division and the decayed learning rate follows a staircase\n+    function.\n \n     Example: decay every 100000 steps with a base of 0.96:\n \n@@ -66,15 +67,16 @@ def exponential_decay(\n     ```\n \n     Args:\n-      learning_rate: A scalar `float32` or `float64` `Tensor` or a Python number.\n-        The initial learning rate.\n-      global_step: A scalar `int32` or `int64` `Tensor` or a Python number. Global\n-        step to use for the decay computation.  Must not be negative.\n+      learning_rate: A scalar `float32` or `float64` `Tensor` or a Python\n+        number.  The initial learning rate.\n+      global_step: A scalar `int32` or `int64` `Tensor` or a Python number.\n+        Global step to use for the decay computation.  Must not be negative.\n       decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number. Must\n         be positive.  See the decay computation above.\n       decay_rate: A scalar `float32` or `float64` `Tensor` or a Python number.\n         The decay rate.\n-      staircase: Boolean.  If `True` decay the learning rate at discrete intervals\n+      staircase: Boolean. If `True` decay the learning rate at discrete\n+        intervals\n       name: String. Optional name of the operation.  Defaults to\n         'ExponentialDecay'.\n \n@@ -87,8 +89,9 @@ def exponential_decay(\n \n     @compatibility(eager)\n     When eager execution is enabled, this function returns a function which in\n-    turn returns the decayed learning rate Tensor. This can be useful for changing\n-    the learning rate value across different invocations of optimizer functions.\n+    turn returns the decayed learning rate Tensor. This can be useful for\n+    changing the learning rate value across different invocations of optimizer\n+    functions.\n     @end_compatibility\n     \"\"\"\n     decayed_lr = learning_rate_schedule.ExponentialDecay(\n@@ -112,8 +115,8 @@ def piecewise_constant(x, boundaries, values, name=None):\n     global_step = tf.Variable(0, trainable=False)\n     boundaries = [100000, 110000]\n     values = [1.0, 0.5, 0.1]\n-    learning_rate = tf.compat.v1.train.piecewise_constant(global_step, boundaries,\n-    values)\n+    learning_rate = tf.compat.v1.train.piecewise_constant(\n+        global_step, boundaries, values)\n \n     # Later, whenever we perform an optimization step, we increment global_step.\n     ```\n@@ -123,9 +126,10 @@ def piecewise_constant(x, boundaries, values, name=None):\n         `float64`, `uint8`, `int8`, `int16`, `int32`, `int64`.\n       boundaries: A list of `Tensor`s or `int`s or `float`s with strictly\n         increasing entries, and with all elements having the same type as `x`.\n-      values: A list of `Tensor`s or `float`s or `int`s that specifies the values\n-        for the intervals defined by `boundaries`. It should have one more element\n-        than `boundaries`, and all elements should have the same type.\n+      values: A list of `Tensor`s or `float`s or `int`s that specifies the\n+        values for the intervals defined by `boundaries`. It should have one\n+        more element than `boundaries`, and all elements should have the same\n+        type.\n       name: A string. Optional name of the operation. Defaults to\n         'PiecewiseConstant'.\n \n@@ -141,8 +145,9 @@ def piecewise_constant(x, boundaries, values, name=None):\n \n     @compatibility(eager)\n     When eager execution is enabled, this function returns a function which in\n-    turn returns the decayed learning rate Tensor. This can be useful for changing\n-    the learning rate value across different invocations of optimizer functions.\n+    turn returns the decayed learning rate Tensor. This can be useful for\n+    changing the learning rate value across different invocations of optimizer\n+    functions.\n     @end_compatibility\n     \"\"\"\n     boundaries = tf.nest.map_structure(\n@@ -156,9 +161,9 @@ def piecewise_constant(x, boundaries, values, name=None):\n     # comparisons, for example if floats are converted to integers.\n     for i, b in enumerate(boundaries):\n         if b.dtype.base_dtype != x_recomp.dtype.base_dtype:\n-            # We can promote int32 boundaries to int64 without loss of precision.\n-            # This covers the most common case where the user passes in boundaries\n-            # as an array of Python integers.\n+            # We can promote int32 boundaries to int64 without loss of\n+            # precision.  This covers the most common case where the user passes\n+            # in boundaries as an array of Python integers.\n             if (\n                 b.dtype.base_dtype == tf.int32\n                 and x_recomp.dtype.base_dtype == tf.int64\n@@ -167,8 +172,8 @@ def piecewise_constant(x, boundaries, values, name=None):\n                 boundaries[i] = b\n             else:\n                 raise ValueError(\n-                    f\"`boundaries` ({b.dtype.base_dtype}) must have the same dtype as \"\n-                    f\"x ({x_recomp.dtype.base_dtype}).\"\n+                    f\"`boundaries` ({b.dtype.base_dtype}) must have the same \"\n+                    f\"dtype as x ({x_recomp.dtype.base_dtype}).\"\n                 )\n     for v in values[1:]:\n         if v.dtype.base_dtype != values[0].dtype.base_dtype:\n@@ -204,7 +209,8 @@ def polynomial_decay(\n     `learning_rate` to reach an `end_learning_rate` in the given `decay_steps`.\n \n     It requires a `global_step` value to compute the decayed learning rate. You\n-    can just pass a TensorFlow variable that you increment at each training step.\n+    can just pass a TensorFlow variable that you increment at each training\n+    step.\n \n     The function returns the decayed learning rate.  It is computed as:\n \n@@ -247,10 +253,10 @@ def polynomial_decay(\n     ```\n \n     Args:\n-      learning_rate: A scalar `float32` or `float64` `Tensor` or a Python number.\n-        The initial learning rate.\n-      global_step: A scalar `int32` or `int64` `Tensor` or a Python number. Global\n-        step to use for the decay computation.  Must not be negative.\n+      learning_rate: A scalar `float32` or `float64` `Tensor` or a Python\n+        number.  The initial learning rate.\n+      global_step: A scalar `int32` or `int64` `Tensor` or a Python number.\n+        Global step to use for the decay computation.  Must not be negative.\n       decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number. Must\n         be positive.  See the decay computation above.\n       end_learning_rate: A scalar `float32` or `float64` `Tensor` or a Python\n@@ -270,8 +276,9 @@ def polynomial_decay(\n \n     @compatibility(eager)\n     When eager execution is enabled, this function returns a function which in\n-    turn returns the decayed learning rate Tensor. This can be useful for changing\n-    the learning rate value across different invocations of optimizer functions.\n+    turn returns the decayed learning rate Tensor. This can be useful for\n+    changing the learning rate value across different invocations of optimizer\n+    functions.\n     @end_compatibility\n     \"\"\"\n     decayed_lr = learning_rate_schedule.PolynomialDecay(\n@@ -302,10 +309,10 @@ def natural_exp_decay(\n     \"\"\"Applies natural exponential decay to the initial learning rate.\n \n     When training a model, it is often recommended to lower the learning rate as\n-    the training progresses.  This function applies an exponential decay function\n-    to a provided initial learning rate.  It requires an `global_step` value to\n-    compute the decayed learning rate.  You can just pass a TensorFlow variable\n-    that you increment at each training step.\n+    the training progresses.  This function applies an exponential decay\n+    function to a provided initial learning rate.  It requires an `global_step`\n+    value to compute the decayed learning rate.  You can just pass a TensorFlow\n+    variable that you increment at each training step.\n \n     The function returns the decayed learning rate.  It is computed as:\n \n@@ -317,8 +324,8 @@ def natural_exp_decay(\n     or, if `staircase` is `True`, as:\n \n     ```python\n-    decayed_learning_rate = learning_rate * exp(-decay_rate * floor(global_step /\n-    decay_step))\n+    decayed_learning_rate = learning_rate * exp(-decay_rate * \\\n+        floor(global_step / decay_step))\n     ```\n \n     Example: decay exponentially with a base of 0.96:\n@@ -341,10 +348,10 @@ def natural_exp_decay(\n     ```\n \n     Args:\n-      learning_rate: A scalar `float32` or `float64` `Tensor` or a Python number.\n-        The initial learning rate.\n-      global_step: A Python number. Global step to use for the decay computation.\n-        Must not be negative.\n+      learning_rate: A scalar `float32` or `float64` `Tensor` or a Python\n+        number. The initial learning rate.\n+      global_step: A Python number. Global step to use for the decay\n+        computation. Must not be negative.\n       decay_steps: How often to apply decay.\n       decay_rate: A Python number.  The decay rate.\n       staircase: Whether to apply decay in a discrete staircase, as opposed to\n@@ -361,8 +368,9 @@ def natural_exp_decay(\n \n     @compatibility(eager)\n     When eager execution is enabled, this function returns a function which in\n-    turn returns the decayed learning rate Tensor. This can be useful for changing\n-    the learning rate value across different invocations of optimizer functions.\n+    turn returns the decayed learning rate Tensor. This can be useful for\n+    changing the learning rate value across different invocations of optimizer\n+    functions.\n     @end_compatibility\n     \"\"\"\n     natural_exp_rate = tf.exp(tf.negative(decay_rate))\n@@ -408,8 +416,8 @@ def inverse_time_decay(\n     or, if `staircase` is `True`, as:\n \n     ```python\n-    decayed_learning_rate = learning_rate / (1 + decay_rate * floor(global_step /\n-    decay_step))\n+    decayed_learning_rate = learning_rate / (1 + decay_rate * \\\n+        floor(global_step / decay_step))\n     ```\n \n     Example: decay 1/t with a rate of 0.5:\n@@ -432,10 +440,10 @@ def inverse_time_decay(\n     ```\n \n     Args:\n-      learning_rate: A scalar `float32` or `float64` `Tensor` or a Python number.\n-        The initial learning rate.\n-      global_step: A Python number. Global step to use for the decay computation.\n-        Must not be negative.\n+      learning_rate: A scalar `float32` or `float64` `Tensor` or a Python\n+        number.  The initial learning rate.\n+      global_step: A Python number. Global step to use for the decay\n+        computation. Must not be negative.\n       decay_steps: How often to apply decay.\n       decay_rate: A Python number.  The decay rate.\n       staircase: Whether to apply decay in a discrete staircase, as opposed to\n@@ -452,8 +460,9 @@ def inverse_time_decay(\n \n     @compatibility(eager)\n     When eager execution is enabled, this function returns a function which in\n-    turn returns the decayed learning rate Tensor. This can be useful for changing\n-    the learning rate value across different invocations of optimizer functions.\n+    turn returns the decayed learning rate Tensor. This can be useful for\n+    changing the learning rate value across different invocations of optimizer\n+    functions.\n     @end_compatibility\n     \"\"\"\n     decayed_lr = learning_rate_schedule.InverseTimeDecay(\n@@ -494,10 +503,10 @@ def cosine_decay(learning_rate, global_step, decay_steps, alpha=0.0, name=None):\n     Args:\n       learning_rate: A scalar `float32` or `float64` Tensor or a Python number.\n         The initial learning rate.\n-      global_step: A scalar `int32` or `int64` `Tensor` or a Python number. Global\n-        step to use for the decay computation.\n-      decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number. Number\n-        of steps to decay over.\n+      global_step: A scalar `int32` or `int64` `Tensor` or a Python number.\n+        Global step to use for the decay computation.\n+      decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number.\n+        Number of steps to decay over.\n       alpha: A scalar `float32` or `float64` Tensor or a Python number. Minimum\n         learning rate value as a fraction of learning_rate.\n       name: String. Optional name of the operation.  Defaults to 'CosineDecay'.\n@@ -516,8 +525,9 @@ def cosine_decay(learning_rate, global_step, decay_steps, alpha=0.0, name=None):\n \n     @compatibility(eager)\n     When eager execution is enabled, this function returns a function which in\n-    turn returns the decayed learning rate Tensor. This can be useful for changing\n-    the learning rate value across different invocations of optimizer functions.\n+    turn returns the decayed learning rate Tensor. This can be useful for\n+    changing the learning rate value across different invocations of optimizer\n+    functions.\n     @end_compatibility\n     \"\"\"\n     decayed_lr = learning_rate_schedule.CosineDecay(\n@@ -551,9 +561,9 @@ def cosine_decay_restarts(\n \n     The function returns the decayed learning rate while taking into account\n     possible warm restarts. The learning rate multiplier first decays\n-    from 1 to `alpha` for `first_decay_steps` steps. Then, a warm\n-    restart is performed. Each new warm restart runs for `t_mul` times more steps\n-    and with `m_mul` times smaller initial learning rate.\n+    from 1 to `alpha` for `first_decay_steps` steps. Then, a warm restart is\n+    performed. Each new warm restart runs for `t_mul` times more steps and with\n+    `m_mul` times smaller initial learning rate.\n \n     Example usage:\n     ```python\n@@ -565,12 +575,12 @@ def cosine_decay_restarts(\n     Args:\n       learning_rate: A scalar `float32` or `float64` Tensor or a Python number.\n         The initial learning rate.\n-      global_step: A scalar `int32` or `int64` `Tensor` or a Python number. Global\n-        step to use for the decay computation.\n-      first_decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number.\n-        Number of steps to decay over.\n-      t_mul: A scalar `float32` or `float64` `Tensor` or a Python number. Used to\n-        derive the number of iterations in the i-th period\n+      global_step: A scalar `int32` or `int64` `Tensor` or a Python number.\n+        Global step to use for the decay computation.\n+      first_decay_steps: A scalar `int32` or `int64` `Tensor` or a Python\n+        number. Number of steps to decay over.\n+      t_mul: A scalar `float32` or `float64` `Tensor` or a Python number. Used\n+        to derive the number of iterations in the i-th period\n       m_mul: A scalar `float32` or `float64` `Tensor` or a Python number.\n         Used to derive the initial learning rate of the i-th period:\n       alpha: A scalar `float32` or `float64` Tensor or a Python number. Minimum\n@@ -591,8 +601,9 @@ def cosine_decay_restarts(\n \n     @compatibility(eager)\n     When eager execution is enabled, this function returns a function which in\n-    turn returns the decayed learning rate Tensor. This can be useful for changing\n-    the learning rate value across different invocations of optimizer functions.\n+    turn returns the decayed learning rate Tensor. This can be useful for\n+    changing the learning rate value across different invocations of optimizer\n+    functions.\n     @end_compatibility\n     \"\"\"\n     decayed_lr = learning_rate_schedule.CosineDecayRestarts(\n@@ -627,10 +638,10 @@ def linear_cosine_decay(\n     larger initial learning rates can typically be used.\n \n     When training a model, it is often recommended to lower the learning rate as\n-    the training progresses.  This function applies a linear cosine decay function\n-    to a provided initial learning rate.  It requires a `global_step` value to\n-    compute the decayed learning rate.  You can just pass a TensorFlow variable\n-    that you increment at each training step.\n+    the training progresses.  This function applies a linear cosine decay\n+    function to a provided initial learning rate.  It requires a `global_step`\n+    value to compute the decayed learning rate.  You can just pass a TensorFlow\n+    variable that you increment at each training step.\n \n     The function returns the decayed learning rate.  It is computed as:\n     ```python\n@@ -651,10 +662,10 @@ def linear_cosine_decay(\n     Args:\n       learning_rate: A scalar `float32` or `float64` Tensor or a Python number.\n         The initial learning rate.\n-      global_step: A scalar `int32` or `int64` `Tensor` or a Python number. Global\n-        step to use for the decay computation.\n-      decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number. Number\n-        of steps to decay over.\n+      global_step: A scalar `int32` or `int64` `Tensor` or a Python number.\n+        Global step to use for the decay computation.\n+      decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number.\n+        Number of steps to decay over.\n       num_periods: Number of periods in the cosine part of the decay. See\n         computation above.\n       alpha: See computation above.\n@@ -679,8 +690,9 @@ def linear_cosine_decay(\n \n     @compatibility(eager)\n     When eager execution is enabled, this function returns a function which in\n-    turn returns the decayed learning rate Tensor. This can be useful for changing\n-    the learning rate value across different invocations of optimizer functions.\n+    turn returns the decayed learning rate Tensor. This can be useful for\n+    changing the learning rate value across different invocations of optimizer\n+    functions.\n     @end_compatibility\n     \"\"\"\n     decayed_lr = learning_rate_schedule.LinearCosineDecay(\n@@ -745,10 +757,10 @@ def noisy_linear_cosine_decay(\n     Args:\n       learning_rate: A scalar `float32` or `float64` Tensor or a Python number.\n         The initial learning rate.\n-      global_step: A scalar `int32` or `int64` `Tensor` or a Python number. Global\n-        step to use for the decay computation.\n-      decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number. Number\n-        of steps to decay over.\n+      global_step: A scalar `int32` or `int64` `Tensor` or a Python number.\n+        Global step to use for the decay computation.\n+      decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number.\n+        Number of steps to decay over.\n       initial_variance: initial variance for the noise. See computation above.\n       variance_decay: decay for the noise's variance. See computation above.\n       num_periods: Number of periods in the cosine part of the decay. See\n@@ -775,8 +787,9 @@ def noisy_linear_cosine_decay(\n \n     @compatibility(eager)\n     When eager execution is enabled, this function returns a function which in\n-    turn returns the decayed learning rate Tensor. This can be useful for changing\n-    the learning rate value across different invocations of optimizer functions.\n+    turn returns the decayed learning rate Tensor. This can be useful for\n+    changing the learning rate value across different invocations of optimizer\n+    functions.\n     @end_compatibility\n     \"\"\"\n     decayed_lr = learning_rate_schedule.NoisyLinearCosineDecay(\n\n@@ -27,8 +27,8 @@ from keras.utils import generic_utils\n class Adadelta(optimizer.Optimizer):\n     r\"\"\"Optimizer that implements the Adadelta algorithm.\n \n-    Adadelta optimization is a stochastic gradient descent method that is based on\n-    adaptive learning rate per dimension to address two drawbacks:\n+    Adadelta optimization is a stochastic gradient descent method that is based\n+    on adaptive learning rate per dimension to address two drawbacks:\n \n     - The continual decay of learning rates throughout training.\n     - The need for a manually selected global learning rate.\n@@ -48,7 +48,8 @@ class Adadelta(optimizer.Optimizer):\n         Note that `Adadelta` tends to benefit from higher initial learning rate\n         values compared to other optimizers.\n         To match the exact form in the original paper, use 1.0.\n-      rho: A `Tensor` or a floating point value. The decay rate. Defaults to 0.95.\n+      rho: A `Tensor` or a floating point value. The decay rate. Defaults to\n+        0.95.\n       epsilon: Small floating point value used to maintain numerical stability.\n         Defaults to 1e-7.\n       {{base_optimizer_keyword_args}}\n\n@@ -42,8 +42,8 @@ class Adamax(optimizer.Optimizer):\n     t = 0  # Initialize timestep\n     ```\n \n-    The update rule for parameter `w` with gradient `g` is\n-    described at the end of section 7.1 of the paper (see the referenece section):\n+    The update rule for parameter `w` with gradient `g` is described at the end\n+    of section 7.1 of the paper (see the referenece section):\n \n     ```python\n     t += 1\n\n@@ -136,8 +136,8 @@ class AdamW(optimizer.Optimizer):\n \n         Args:\n           var_list: list of model variables to build AdamW variables on.\n-          exclude_from_weight_decay: list of model variables that will be excluded\n-            from weight decay.\n+          exclude_from_weight_decay: list of model variables that will be\n+            excluded from weight decay.\n         \"\"\"\n         super().build(var_list)\n         if hasattr(self, \"_built\") and self._built:\n\n@@ -27,9 +27,9 @@ from keras.utils import generic_utils\n class Ftrl(optimizer.Optimizer):\n     r\"\"\"Optimizer that implements the FTRL algorithm.\n \n-    \"Follow The Regularized Leader\" (FTRL) is an optimization algorithm developed\n-    at Google for click-through rate prediction in the early 2010s. It is most\n-    suitable for shallow models with large and sparse feature spaces.\n+    \"Follow The Regularized Leader\" (FTRL) is an optimization algorithm\n+    developed at Google for click-through rate prediction in the early 2010s. It\n+    is most suitable for shallow models with large and sparse feature spaces.\n     The algorithm is described by\n     [McMahan et al., 2013](https://research.google.com/pubs/archive/41159.pdf).\n     The Keras version has support for both online L2 regularization\n@@ -73,23 +73,23 @@ class Ftrl(optimizer.Optimizer):\n     Args:\n       learning_rate: A `Tensor`, floating point value, a schedule that is a\n         `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable that\n-        takes no arguments and returns the actual value to use. The learning rate.\n-        Defaults to 0.001.\n-      learning_rate_power: A float value, must be less or equal to zero. Controls\n-        how the learning rate decreases during training. Use zero for a fixed\n-        learning rate.\n-      initial_accumulator_value: The starting value for accumulators. Only zero or\n-        positive values are allowed.\n-      l1_regularization_strength: A float value, must be greater than or equal to\n-        zero. Defaults to 0.0.\n-      l2_regularization_strength: A float value, must be greater than or equal to\n-        zero. Defaults to 0.0.\n-      l2_shrinkage_regularization_strength: A float value, must be greater than or\n-        equal to zero. This differs from L2 above in that the L2 above is a\n+        takes no arguments and returns the actual value to use. The learning\n+        rate.  Defaults to 0.001.\n+      learning_rate_power: A float value, must be less or equal to zero.\n+        Controls how the learning rate decreases during training. Use zero for a\n+        fixed learning rate.\n+      initial_accumulator_value: The starting value for accumulators. Only zero\n+        or positive values are allowed.\n+      l1_regularization_strength: A float value, must be greater than or equal\n+        to zero. Defaults to 0.0.\n+      l2_regularization_strength: A float value, must be greater than or equal\n+        to zero. Defaults to 0.0.\n+      l2_shrinkage_regularization_strength: A float value, must be greater than\n+        or equal to zero. This differs from L2 above in that the L2 above is a\n         stabilization penalty, whereas this L2 shrinkage is a magnitude penalty.\n         When input is sparse shrinkage will only happen on the active weights.\n-      beta: A float value, representing the beta value from the paper. Defaults to\n-        0.0.\n+      beta: A float value, representing the beta value from the paper. Defaults\n+        to 0.0.\n       {{base_optimizer_keyword_args}}\n     \"\"\"\n \n@@ -126,8 +126,9 @@ class Ftrl(optimizer.Optimizer):\n \n         if initial_accumulator_value < 0.0:\n             raise ValueError(\n-                \"`initial_accumulator_value` needs to be positive or zero. Received: \"\n-                f\"initial_accumulator_value={initial_accumulator_value}.\"\n+                \"`initial_accumulator_value` needs to be positive or zero. \"\n+                f\"Received: initial_accumulator_value=\"\n+                f\"{initial_accumulator_value}.\"\n             )\n         if learning_rate_power > 0.0:\n             raise ValueError(\n@@ -137,17 +138,19 @@ class Ftrl(optimizer.Optimizer):\n         if l1_regularization_strength < 0.0:\n             raise ValueError(\n                 \"`l1_regularization_strength` needs to be positive or zero. \"\n-                f\"Received: l1_regularization_strength={l1_regularization_strength}.\"\n+                f\"Received: l1_regularization_strength=\"\n+                f\"{l1_regularization_strength}.\"\n             )\n         if l2_regularization_strength < 0.0:\n             raise ValueError(\n                 \"`l2_regularization_strength` needs to be positive or zero. \"\n-                f\"Received: l2_regularization_strength={l2_regularization_strength}.\"\n+                f\"Received: l2_regularization_strength=\"\n+                f\"{l2_regularization_strength}.\"\n             )\n         if l2_shrinkage_regularization_strength < 0.0:\n             raise ValueError(\n-                \"`l2_shrinkage_regularization_strength` needs to be positive or \"\n-                \"zero. Received: l2_shrinkage_regularization_strength\"\n+                \"`l2_shrinkage_regularization_strength` needs to be positive \"\n+                \"or zero. Received: l2_shrinkage_regularization_strength\"\n                 f\"={l2_shrinkage_regularization_strength}.\"\n             )\n \n\n@@ -52,8 +52,8 @@ class _BaseOptimizer(tf.Module):\n         self.use_ema = use_ema\n         self.jit_compile = jit_compile\n         if not tf.config.list_physical_devices(\"GPU\"):\n-            # Optimizer only benefits from XLA when training on GPU. So if no GPU is\n-            # found, we turn off XLA.\n+            # Optimizer only benefits from XLA when training on GPU. So if no\n+            # GPU is found, we turn off XLA.\n             self.jit_compile = False\n         if use_ema:\n             # Verify the arguments related to EMA.\n@@ -68,7 +68,8 @@ class _BaseOptimizer(tf.Module):\n             ):\n                 raise ValueError(\n                     \"`ema_overwrite_frequency` must be an integer > 1 or None. \"\n-                    f\"Received: ema_overwrite_frequency={ema_overwrite_frequency}\"\n+                    f\"Received: ema_overwrite_frequency=\"\n+                    f\"{ema_overwrite_frequency}\"\n                 )\n         self.ema_momentum = ema_momentum\n         self.ema_overwrite_frequency = ema_overwrite_frequency\n@@ -86,7 +87,8 @@ class _BaseOptimizer(tf.Module):\n     def _create_iteration_variable(self):\n         \"\"\"Create the iterations counter variable.\"\"\"\n         with tf.init_scope():\n-            # Lift the variable creation to init scope to avoid environment issue.\n+            # Lift the variable creation to init scope to avoid environment\n+            # issue.\n             self._iterations = tf.Variable(\n                 0, name=\"iteration\", dtype=tf.int64, trainable=False\n             )\n@@ -114,8 +116,8 @@ class _BaseOptimizer(tf.Module):\n     def _var_key(self, variable):\n         \"\"\"Get a unique identifier of the given variable.\"\"\"\n         # Get the distributed variable if it exists.\n-        # TODO(b/199214315): replace _unique_id with ref() after fixing ref() issues\n-        # on AggregatingVariable.\n+        # TODO(b/199214315): replace _unique_id with ref() after fixing ref()\n+        # issues on AggregatingVariable.\n         return variable._unique_id  # pylint: disable=protected-access\n \n     @abc.abstractmethod\n@@ -138,11 +140,11 @@ class _BaseOptimizer(tf.Module):\n     def _update_step_xla(self, gradient, variable, key):\n         \"\"\"A wrapper of `update_step` to enable XLA acceleration.\n \n-        Due to `tf.function` tracing mechanism, for (gradient, variable) pairs of\n-        the same shape and dtype, the execution graph always invoke the first\n-        pair it has seen. Thus, we need a `key` argument to make each\n-        (gradient, variable) pair unique. In additions, XLA cannot understand\n-        string input, so the key is an integer.\n+        Due to `tf.function` tracing mechanism, for (gradient, variable) pairs\n+        of the same shape and dtype, the execution graph always invoke the first\n+        pair it has seen. Thus, we need a `key` argument to make each (gradient,\n+        variable) pair unique. In additions, XLA cannot understand string input,\n+        so the key is an integer.\n \n         Args:\n           gradient: backpropagated gradient of the given variable.\n@@ -156,14 +158,15 @@ class _BaseOptimizer(tf.Module):\n \n     def _update_step(self, gradient, variable):\n         if getattr(variable, \"_unique_id\", None) is None:\n-            # Variable has no `_unique_id` if called during `model.save()`, in which\n-            # case we do not want to update the variable.\n+            # Variable has no `_unique_id` if called during `model.save()`, in\n+            # which case we do not want to update the variable.\n             return\n         if self._var_key(variable) not in self._index_dict:\n             raise KeyError(\n-                f\"The optimizer cannot recognize variable {variable.name}. This \"\n-                f\"usually means that you're reusing an optimizer previously created \"\n-                f\"for a different model. Try creating a new optimizer instance.\"\n+                f\"The optimizer cannot recognize variable {variable.name}. \"\n+                f\"This usually means that you're reusing an optimizer \"\n+                f\"previously created for a different model. Try creating a \"\n+                \"new optimizer instance.\"\n             )\n         self.update_step(gradient, variable)\n \n@@ -171,12 +174,12 @@ class _BaseOptimizer(tf.Module):\n         \"\"\"Compute gradients of loss on trainable variables.\n \n         Args:\n-          loss: `Tensor` or callable. If a callable, `loss` should take no arguments\n-            and return the value to minimize.\n+          loss: `Tensor` or callable. If a callable, `loss` should take no\n+            arguments and return the value to minimize.\n           var_list: list or tuple of `Variable` objects to update to minimize\n             `loss`.\n-          tape: (Optional) `tf.GradientTape`. If `loss` is provided as a `Tensor`,\n-            the tape that computed the `loss` must be provided.\n+          tape: (Optional) `tf.GradientTape`. If `loss` is provided as a\n+            `Tensor`, the tape that computed the `loss` must be provided.\n \n         Returns:\n           A list of (gradient, variable) pairs. Variable is always present, but\n@@ -217,7 +220,7 @@ class _BaseOptimizer(tf.Module):\n                     clipped_grads.append(\n                         tf.clip_by_value(\n                             g,\n-                            clip_value_min=-self.clipvalue,  # pylint: disable=invalid-unary-operand-type\n+                            clip_value_min=-self.clipvalue,\n                             clip_value_max=self.clipvalue,\n                         )\n                     )\n@@ -317,9 +320,9 @@ class _BaseOptimizer(tf.Module):\n         optimizers need to call `super().build(var_list)`.\n \n         Args:\n-          var_list: List of model variables to build optimizers on. For example, SGD\n-            optimizer with momentum will store one momentum variable corresponding\n-            to each model variable.\n+          var_list: List of model variables to build optimizers on. For example,\n+            SGD optimizer with momentum will store one momentum variable\n+            corresponding to each model variable.\n         \"\"\"\n         if getattr(self, \"_built\", False):\n             return\n@@ -327,8 +330,8 @@ class _BaseOptimizer(tf.Module):\n         if self.use_ema:\n             self._model_variables_moving_average = []\n             for var in var_list:\n-                # Make a copy of the model variables, we will use the copy to store the\n-                # moving average of model variables.\n+                # Make a copy of the model variables, we will use the copy to\n+                # store the moving average of model variables.\n                 self._model_variables_moving_average.append(\n                     self.add_variable_from_reference(\n                         var, \"average\", initial_value=var\n@@ -356,8 +359,8 @@ class _BaseOptimizer(tf.Module):\n         \"\"\"Create an optimizer variable.\n \n         Args:\n-          shape: A list of integers, a tuple of integers, or a 1-D Tensor of type\n-            int32. Defaults to scalar if unspecified.\n+          shape: A list of integers, a tuple of integers, or a 1-D Tensor of\n+            type int32. Defaults to scalar if unspecified.\n           dtype: The DType of the optimizer variable to be created. Defaults to\n             `tf.keras.backend.floatx` if unspecified.\n           initializer: string or callable. Initializer instance.\n@@ -396,8 +399,8 @@ class _BaseOptimizer(tf.Module):\n             variable to be created. If None, the created variable will have the\n             same shape as `model_variable`.\n           initial_value: A Tensor, or Python object convertible to a Tensor,\n-            defaults to None. The initial value of the optimizer variable, if None,\n-            the initial value will be default to 0.\n+            defaults to None. The initial value of the optimizer variable, if\n+            None, the initial value will be default to 0.\n \n         Returns:\n           An optimizer variable.\n@@ -411,7 +414,7 @@ class _BaseOptimizer(tf.Module):\n                 initial_value = tf.zeros(shape, dtype=model_variable.dtype)\n         return tf.Variable(\n             initial_value=initial_value,\n-            name=f\"{variable_name}/{model_variable._shared_name}\",  # pylint: disable=protected-access\n+            name=f\"{variable_name}/{model_variable._shared_name}\",\n             dtype=model_variable.dtype,\n             trainable=False,\n         )\n@@ -425,8 +428,8 @@ class _BaseOptimizer(tf.Module):\n         of using this function.\n \n         Args:\n-          loss: `Tensor` or callable. If a callable, `loss` should take no arguments\n-            and return the value to minimize.\n+          loss: `Tensor` or callable. If a callable, `loss` should take no\n+            arguments and return the value to minimize.\n           var_list: list or tuple of `Variable` objects to update to minimize\n             `loss`.\n           tape: (Optional) `tf.GradientTape`.\n@@ -452,7 +455,8 @@ class _BaseOptimizer(tf.Module):\n         if isinstance(\n             self._learning_rate, learning_rate_schedule.LearningRateSchedule\n         ):\n-            # Compute the current learning rate at the beginning of variable update.\n+            # Compute the current learning rate at the beginning of variable\n+            # update.\n             self._current_learning_rate.assign(\n                 self._learning_rate(self.iterations)\n             )\n@@ -461,7 +465,8 @@ class _BaseOptimizer(tf.Module):\n         scope_name = self._name or \"optimizer\"\n         with tf.name_scope(scope_name):\n             with tf.init_scope():\n-                # Lift variable creation to init scope to avoid environment issues.\n+                # Lift variable creation to init scope to avoid environment\n+                # issues.\n                 self.build(trainable_variables)\n         grads = self._clip_gradients(grads)\n         grads_and_vars = list(zip(grads, trainable_variables))\n@@ -523,8 +528,9 @@ class _BaseOptimizer(tf.Module):\n           var_list: list of model variables.\n         \"\"\"\n         if self.use_ema:\n-            # If the optimizer uses EMA, then when finalizing, we replace the model\n-            # variable value with its moving average stored inside optimizer.\n+            # If the optimizer uses EMA, then when finalizing, we replace the\n+            # model variable value with its moving average stored inside\n+            # optimizer.\n             self._overwrite_model_variables_with_average_value(var_list)\n \n     def _serialize_hyperparameter(self, hyperparameter):\n@@ -700,27 +706,29 @@ class Optimizer(_BaseOptimizer):\n     >>> print([grads[0].numpy(), grads[1].numpy()])\n     [2.0, 2.0]\n     >>> opt.apply_gradients(zip(grads, [var1, var2]))\n-    >>> # Without clipping, we should get [0, 0], but as gradients are clipped to\n+    >>> # Without clipping, we should get [0, 0], but as gradients are clipped\n+    >>> # to\n     >>> # have max value 1, we get [1.0, 1.0].\n     >>> print([var1.numpy(), var2.numpy()])\n     [1.0, 1.0]\n \n     ### Using exponential moving average.\n \n-    Empirically it has been found that using the exponential moving average (EMA)\n-    of the trained parameters of a deep network achieves a better performance than\n-    using its trained parameters directly. Keras optimizers allows users to\n-    compute this moving average and overwrite the model variables at desired time.\n+    Empirically it has been found that using the exponential moving average\n+    (EMA) of the trained parameters of a deep network achieves a better\n+    performance than using its trained parameters directly. Keras optimizers\n+    allows users to compute this moving average and overwrite the model\n+    variables at desired time.\n \n     Example:\n \n     ```python\n-    # Create an SGD optimizer with EMA on. `ema_momentum` controls the decay rate\n-    # of the moving average. `ema_momentum=1` means no decay and the stored moving\n-    # average is always model variable's initial value before training. Reversely,\n-    # `ema_momentum=0` is equivalent to not using EMA. `ema_overwrite_frequency=3`\n-    # means every 3 iterations, we overwrite the trainable variables with their\n-    # moving average values.\n+    # Create an SGD optimizer with EMA on. `ema_momentum` controls the decay\n+    # rate of the moving average. `ema_momentum=1` means no decay and the stored\n+    # moving average is always model variable's initial value before training.\n+    # Reversely, `ema_momentum=0` is equivalent to not using EMA.\n+    # `ema_overwrite_frequency=3` means every 3 iterations, we overwrite the\n+    # trainable variables with their moving average values.\n     opt = tf.keras.optimizers.experimental.SGD(\n         learning_rate=1,\n         use_ema=True,\n@@ -747,19 +755,20 @@ class Optimizer(_BaseOptimizer):\n \n     ```\n     When optimizer is constructed with `use_ema=True`, in custom training loop,\n-    users can explicitly call `finalize_variable_values()` to overwrite trainable\n-    variables with their EMA values. `finalize_variable_values()` is by default\n-    called at the end of `model.fit()`.\n+    users can explicitly call `finalize_variable_values()` to overwrite\n+    trainable variables with their EMA values. `finalize_variable_values()` is\n+    by default called at the end of `model.fit()`.\n \n     ### Use with `tf.distribute.Strategy`\n \n     This optimizer class is `tf.distribute.Strategy` aware, which means it\n     automatically sums gradients across all replicas. To aggregate gradients\n-    yourself, call `apply_gradients` with `skip_aggregate_gradients` set to True.\n-    This is useful if you need to process aggregated gradients.\n+    yourself, call `apply_gradients` with `skip_aggregate_gradients` set to\n+    True.  This is useful if you need to process aggregated gradients.\n \n     ```python\n-    # This example is not runnable, it consists of dummy code for simple tutorial.\n+    # This example is not runnable, it consists of dummy code for simple\n+    # tutorial.\n     strategy = tf.distribute.experimental.TPUStrategy()\n \n     with strategy.scope():\n@@ -836,8 +845,8 @@ class Optimizer(_BaseOptimizer):\n     def aggregate_gradients(self, grads_and_vars):\n         \"\"\"Aggregate gradients on all devices.\n \n-        By default we will perform reduce_sum of gradients across devices. Users can\n-        implement their own aggregation logic by overriding this method.\n+        By default we will perform reduce_sum of gradients across devices. Users\n+        can implement their own aggregation logic by overriding this method.\n \n         Args:\n           grads_and_vars: List of (gradient, variable) pairs.\n@@ -927,14 +936,14 @@ class Optimizer(_BaseOptimizer):\n             _, var_list = zip(*grads_and_vars)\n             self._update_model_variables_moving_average(var_list)\n             if self.ema_overwrite_frequency:\n-                # Only when self.ema_overwrite_frequency is not None, we overwrite the\n-                # model variables.\n+                # Only when self.ema_overwrite_frequency is not None, we\n+                # overwrite the model variables.\n                 should_overwrite_model_vars = (\n                     self.iterations % self.ema_overwrite_frequency == 0\n                 )\n                 tf.cond(\n                     tf.cast(should_overwrite_model_vars, tf.bool),\n-                    true_fn=lambda: self._overwrite_model_variables_with_average_value(  # pylint: disable=g-long-lambda\n+                    true_fn=lambda: self._overwrite_model_variables_with_average_value(\n                         var_list\n                     ),\n                     false_fn=lambda: None,\n@@ -948,8 +957,8 @@ class RestoredOptimizer(Optimizer):\n     def get_config(self):\n         raise NotImplementedError(\n             \"Restoring functional Optimizers from SavedModels is not currently \"\n-            \"supported. Please file a feature request if this limitation bothers \"\n-            \"you.\"\n+            \"supported. Please file a feature request if this limitation \"\n+            \"bothers you.\"\n         )\n \n \n\n@@ -91,8 +91,8 @@ class OptimizerPssTest(tf.test.TestCase, parameterized.TestCase):\n         variables = optimizer.variables\n         for var in variables:\n             if \"iteration\" not in var.name and \"learning_rate\" not in var.name:\n-                # Find a variable not iteration or learning_rate, and verify its value\n-                # is updated (not 0).\n+                # Find a variable not iteration or learning_rate, and verify its\n+                # value is updated (not 0).\n                 self.assertNotAllEqual(var, 0)\n \n     @ds_combinations.generate(\n\n@@ -262,8 +262,8 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n         optimizer.apply_gradients(zip(grads, [var1, var2]))\n         self.assertAllEqual([var1.numpy(), var2.numpy()], [0.0, 0.0])\n \n-        # Third iteration, without EMA, we should see [var1, var2] = [-1.0, -1.0],\n-        # but overwriting results in [var1, var2] = [-0.125, -0.125].\n+        # Third iteration, without EMA, we should see [var1, var2] = [-1.0,\n+        # -1.0], but overwriting results in [var1, var2] = [-0.125, -0.125].\n         optimizer.apply_gradients(zip(grads, [var1, var2]))\n         self.assertAllEqual([var1.numpy(), var2.numpy()], [-0.125, -0.125])\n \n\n@@ -52,10 +52,9 @@ class SGD(optimizer.Optimizer):\n         `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable\n         that takes no arguments and returns the actual value to use. The\n         learning rate. Defaults to 0.001.\n-      momentum: float hyperparameter >= 0 that accelerates gradient descent\n-        in the relevant\n-        direction and dampens oscillations. Defaults to 0, i.e., vanilla gradient\n-        descent.\n+      momentum: float hyperparameter >= 0 that accelerates gradient descent in\n+        the relevant direction and dampens oscillations. Defaults to 0, i.e.,\n+        vanilla gradient descent.\n       nesterov: boolean. Whether to apply Nesterov momentum.\n         Defaults to `False`.\n       {{base_optimizer_keyword_args}}\n\n@@ -85,8 +85,8 @@ class Optimizer:\n             List of gradient tensors.\n \n         Raises:\n-            ValueError: In case any gradient cannot be computed (e.g. if gradient\n-              function not implemented).\n+            ValueError: In case any gradient cannot be computed (e.g. if\n+              gradient function not implemented).\n         \"\"\"\n         grads = backend.gradients(loss, params)\n         if any(g is None for g in grads):\n@@ -113,9 +113,9 @@ class Optimizer:\n         (otherwise the optimizer has no weights).\n \n         Args:\n-            weights: a list of Numpy arrays. The number of arrays and their shape\n-              must match number of the dimensions of the weights of the optimizer\n-              (i.e. it should match the output of `get_weights`).\n+            weights: a list of Numpy arrays. The number of arrays and their\n+              shape must match number of the dimensions of the weights of the\n+              optimizer (i.e. it should match the output of `get_weights`).\n \n         Raises:\n             ValueError: in case of incompatible weight shapes.\n@@ -890,9 +890,9 @@ class TFOptimizer(Optimizer, tf.__internal__.tracking.Trackable):\n             self.updates = []\n \n             if not params:\n-                # After the model vars have been created, the second call to get_updates\n-                # is called with params as an empty list. This ensures that we call\n-                # compute_gradients with params=None.\n+                # After the model vars have been created, the second call to\n+                # get_updates is called with params as an empty list. This\n+                # ensures that we call compute_gradients with params=None.\n                 grads = self.optimizer.compute_gradients(loss)\n             else:\n                 grads = self.optimizer.compute_gradients(loss, params)\n\n@@ -29,8 +29,8 @@ from keras.optimizers.optimizer_v2 import optimizer_v2\n class Adadelta(optimizer_v2.OptimizerV2):\n     r\"\"\"Optimizer that implements the Adadelta algorithm.\n \n-    Adadelta optimization is a stochastic gradient descent method that is based on\n-    adaptive learning rate per dimension to address two drawbacks:\n+    Adadelta optimization is a stochastic gradient descent method that is based\n+    on adaptive learning rate per dimension to address two drawbacks:\n \n     - The continual decay of learning rates throughout training.\n     - The need for a manually selected global learning rate.\n\n@@ -49,8 +49,8 @@ class AdadeltaOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n                     epsilon = 1e-8\n                     if use_callable_params:\n                         adadelta_opt = adadelta.Adadelta(\n-                            learning_rate=lambda: lr,  # pylint: disable=cell-var-from-loop\n-                            rho=lambda: rho,  # pylint: disable=cell-var-from-loop\n+                            learning_rate=lambda: lr,\n+                            rho=lambda: rho,\n                             epsilon=epsilon,\n                         )  # pylint: disable=cell-var-from-loop\n                     else:\n\n@@ -122,9 +122,9 @@ class Adagrad(optimizer_v2.OptimizerV2):\n \n         Args:\n             config: A Python dictionary, typically the output of get_config.\n-            custom_objects: A Python dictionary mapping names to additional Python\n-              objects used to create this optimizer, such as a function used for a\n-              hyperparameter.\n+            custom_objects: A Python dictionary mapping names to additional\n+              Python objects used to create this optimizer, such as a function\n+              used for a hyperparameter.\n \n         Returns:\n             An optimizer instance.\n\n@@ -357,29 +357,33 @@ class NonFusedAdam(optimizer_v2.OptimizerV2):\n         \"\"\"Construct a new Adam optimizer.\n \n         Args:\n-          learning_rate: A `Tensor`, floating point value, or a schedule that is a\n-            `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable that\n-            takes no arguments and returns the actual value to use, The learning\n-            rate. Defaults to 0.001.\n-          beta_1: A float value or a constant float tensor, or a callable that takes\n-            no arguments and returns the actual value to use. The exponential decay\n-            rate for the 1st moment estimates. Defaults to 0.9.\n-          beta_2: A float value or a constant float tensor, or a callable that takes\n-            no arguments and returns the actual value to use, The exponential decay\n-            rate for the 2nd moment estimates. Defaults to 0.999.\n+          learning_rate: A `Tensor`, floating point value, or a schedule that is\n+            a `tf.keras.optimizers.schedules.LearningRateSchedule`, or a\n+            callable that takes no arguments and returns the actual value to\n+            use, The learning rate. Defaults to 0.001.\n+          beta_1: A float value or a constant float tensor, or a callable that\n+            takes no arguments and returns the actual value to use. The\n+            exponential decay rate for the 1st moment estimates. Defaults to\n+            0.9.\n+          beta_2: A float value or a constant float tensor, or a callable that\n+            takes no arguments and returns the actual value to use, The\n+            exponential decay rate for the 2nd moment estimates. Defaults to\n+            0.999.\n           epsilon: A small constant for numerical stability. This epsilon is\n             \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n-            Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to\n-            1e-7.\n-          amsgrad: Boolean. Whether to apply AMSGrad variant of this algorithm from\n-            the paper \"On the Convergence of Adam and beyond\". Defaults to `False`.\n-          name: Optional name for the operations created when applying gradients.\n-            Defaults to \"Adam\".\n-          **kwargs: keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`,\n-            `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is clip\n-            gradients by value, `decay` is included for backward compatibility to\n-            allow time inverse decay of learning rate. `lr` is included for backward\n-            compatibility, recommended to use `learning_rate` instead.\n+            Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults\n+            to 1e-7.\n+          amsgrad: Boolean. Whether to apply AMSGrad variant of this algorithm\n+            from the paper \"On the Convergence of Adam and beyond\". Defaults to\n+            `False`.\n+          name: Optional name for the operations created when applying\n+            gradients.  Defaults to \"Adam\".\n+          **kwargs: keyword arguments. Allowed to be {`clipnorm`, `clipvalue`,\n+            `lr`, `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is\n+            clip gradients by value, `decay` is included for backward\n+            compatibility to allow time inverse decay of learning rate. `lr` is\n+            included for backward compatibility, recommended to use\n+            `learning_rate` instead.\n         \"\"\"\n \n         super().__init__(name, **kwargs)\n\n@@ -163,8 +163,8 @@ class AdamOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n             with tf.Graph().as_default(), self.cached_session(\n                 force_gpu=tf.test.is_gpu_available()\n             ):\n-                # If a GPU is available, tests that all optimizer ops can be placed on\n-                # it (i.e. they have GPU kernels).\n+                # If a GPU is available, tests that all optimizer ops can be\n+                # placed on it (i.e. they have GPU kernels).\n                 var = tf.Variable([[1.0], [2.0]])\n                 indices = tf.constant([0, 1], dtype=index_dtype)\n                 g_sum = lambda: tf.reduce_sum(\n@@ -630,7 +630,8 @@ class AdamOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n         v2 = tf.Variable(1.0)\n         opt = adam.Adam(1.0)\n         opt.minimize(lambda: v1 + v2, var_list=[v1, v2])\n-        # There should be iteration, and two unique slot variables for v1 and v2.\n+        # There should be iteration, and two unique slot variables for v1 and\n+        # v2.\n         self.assertLen(set(v.ref() for v in opt.variables()), 5)\n         self.assertEqual(\n             self.evaluate(opt.variables()[0]), self.evaluate(opt.iterations)\n@@ -733,8 +734,8 @@ class NonFusedAdamOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n             with tf.Graph().as_default(), self.cached_session(\n                 force_gpu=tf.test.is_gpu_available()\n             ):\n-                # If a GPU is available, tests that all optimizer ops can be placed on\n-                # it (i.e. they have GPU kernels).\n+                # If a GPU is available, tests that all optimizer ops can be\n+                # placed on it (i.e. they have GPU kernels).\n                 var = tf.Variable([[1.0], [2.0]])\n                 indices = tf.constant([0, 1], dtype=index_dtype)\n                 g_sum = lambda: tf.reduce_sum(\n\n@@ -133,8 +133,8 @@ class AdamaxOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n             with tf.Graph().as_default(), self.cached_session(\n                 force_gpu=tf.test.is_gpu_available()\n             ):\n-                # If a GPU is available, tests that all optimizer ops can be placed on\n-                # it (i.e. they have GPU kernels).\n+                # If a GPU is available, tests that all optimizer ops can be\n+                # placed on it (i.e. they have GPU kernels).\n                 var = tf.Variable([[1.0], [2.0]])\n                 indices = tf.constant([0, 1], dtype=index_dtype)\n                 g_sum = lambda: tf.reduce_sum(\n@@ -403,7 +403,8 @@ class AdamaxOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n         v2 = tf.Variable(1.0)\n         opt = adamax.Adamax(1.0)\n         opt.minimize(lambda: v1 + v2, var_list=[v1, v2])\n-        # There should be iteration, and two unique slot variables for v1 and v2.\n+        # There should be iteration, and two unique slot variables for v1 and\n+        # v2.\n         self.assertLen({id(v) for v in opt.variables()}, 5)\n \n     def testConstructAdamaxWithLR(self):\n\n@@ -27,9 +27,9 @@ from keras.optimizers.optimizer_v2 import optimizer_v2\n class Ftrl(optimizer_v2.OptimizerV2):\n     r\"\"\"Optimizer that implements the FTRL algorithm.\n \n-    \"Follow The Regularized Leader\" (FTRL) is an optimization algorithm developed\n-    at Google for click-through rate prediction in the early 2010s. It is most\n-    suitable for shallow models with large and sparse feature spaces.\n+    \"Follow The Regularized Leader\" (FTRL) is an optimization algorithm\n+    developed at Google for click-through rate prediction in the early 2010s. It\n+    is most suitable for shallow models with large and sparse feature spaces.\n     The algorithm is described by\n     [McMahan et al., 2013](https://research.google.com/pubs/archive/41159.pdf).\n     The Keras version has support for both online L2 regularization\n@@ -119,28 +119,32 @@ class Ftrl(optimizer_v2.OptimizerV2):\n \n         if initial_accumulator_value < 0.0:\n             raise ValueError(\n-                \"`initial_accumulator_value` needs to be positive or zero. Received: \"\n+                \"`initial_accumulator_value` needs to be \"\n+                \"positive or zero. Received: \"\n                 f\"initial_accumulator_value={initial_accumulator_value}.\"\n             )\n         if learning_rate_power > 0.0:\n             raise ValueError(\n-                \"`learning_rate_power` needs to be negative or zero. Received: \"\n+                \"`learning_rate_power` needs to be \"\n+                \"negative or zero. Received: \"\n                 f\"learning_rate_power={learning_rate_power}.\"\n             )\n         if l1_regularization_strength < 0.0:\n             raise ValueError(\n                 \"`l1_regularization_strength` needs to be positive or zero. \"\n-                f\"Received: l1_regularization_strength={l1_regularization_strength}.\"\n+                f\"Received: l1_regularization_strength=\"\n+                f\"{l1_regularization_strength}.\"\n             )\n         if l2_regularization_strength < 0.0:\n             raise ValueError(\n                 \"`l2_regularization_strength` needs to be positive or zero. \"\n-                f\"Received: l2_regularization_strength={l2_regularization_strength}.\"\n+                f\"Received: l2_regularization_strength=\"\n+                f\"{l2_regularization_strength}.\"\n             )\n         if l2_shrinkage_regularization_strength < 0.0:\n             raise ValueError(\n-                \"`l2_shrinkage_regularization_strength` needs to be positive or \"\n-                \"zero. Received: l2_shrinkage_regularization_strength\"\n+                \"`l2_shrinkage_regularization_strength` needs to be positive \"\n+                \"or zero. Received: l2_shrinkage_regularization_strength\"\n                 f\"={l2_shrinkage_regularization_strength}.\"\n             )\n \n@@ -195,8 +199,8 @@ class Ftrl(optimizer_v2.OptimizerV2):\n             (var_device, var_dtype)\n         ) or self._fallback_apply_state(var_device, var_dtype)\n \n-        # Adjust L2 regularization strength to include beta to avoid the underlying\n-        # TensorFlow ops needing to include it.\n+        # Adjust L2 regularization strength to include beta to avoid the\n+        # underlying TensorFlow ops needing to include it.\n         adjusted_l2_regularization_strength = coefficients[\n             \"l2_regularization_strength\"\n         ] + coefficients[\"beta\"] / (2.0 * coefficients[\"lr_t\"])\n@@ -238,8 +242,8 @@ class Ftrl(optimizer_v2.OptimizerV2):\n             (var_device, var_dtype)\n         ) or self._fallback_apply_state(var_device, var_dtype)\n \n-        # Adjust L2 regularization strength to include beta to avoid the underlying\n-        # TensorFlow ops needing to include it.\n+        # Adjust L2 regularization strength to include beta to avoid the\n+        # underlying TensorFlow ops needing to include it.\n         adjusted_l2_regularization_strength = coefficients[\n             \"l2_regularization_strength\"\n         ] + coefficients[\"beta\"] / (2.0 * coefficients[\"lr_t\"])\n\n@@ -267,9 +267,10 @@ class FtrlOptimizerTest(tf.test.TestCase):\n     def testFtrlWithL1_L2_L2Shrinkage(self):\n         \"\"\"Test the new FTRL op with support for l2 shrinkage.\n \n-        The addition of this parameter which places a constant pressure on weights\n-        towards the origin causes the gradient descent trajectory to differ. The\n-        weights will tend to have smaller magnitudes with this parameter set.\n+        The addition of this parameter which places a constant pressure on\n+        weights towards the origin causes the gradient descent trajectory to\n+        differ. The weights will tend to have smaller magnitudes with this\n+        parameter set.\n         \"\"\"\n         # TODO(tanzheny, omalleyt): Fix test in eager mode.\n         for dtype in [tf.half, tf.float32]:\n@@ -308,7 +309,8 @@ class FtrlOptimizerTest(tf.test.TestCase):\n                 )\n \n     def testFtrlWithL1_L2_L2ShrinkageSparse(self):\n-        \"\"\"Tests the new FTRL op with support for l2 shrinkage on sparse grads.\"\"\"\n+        \"\"\"Tests the new FTRL op with support for l2 shrinkage on sparse\n+        grads.\"\"\"\n         # TODO(tanzheny, omalleyt): Fix test in eager mode.\n         for dtype in [tf.half, tf.float32]:\n             with tf.Graph().as_default(), self.cached_session():\n@@ -390,8 +392,8 @@ class FtrlOptimizerTest(tf.test.TestCase):\n                     update1.run()\n \n                 v0_val, v1_val = self.evaluate([var0, var1])\n-                # var0 is experiencing L2 shrinkage so it should be smaller than var1\n-                # in magnitude.\n+                # var0 is experiencing L2 shrinkage so it should be smaller than\n+                # var1 in magnitude.\n                 self.assertTrue((v0_val**2 < v1_val**2).all())\n                 accum0 = sess.run(opt0.get_slot(var0, \"accumulator\"))\n                 accum1 = sess.run(opt1.get_slot(var1, \"accumulator\"))\n@@ -436,11 +438,12 @@ class FtrlOptimizerTest(tf.test.TestCase):\n         v0_val, v1_val = self.evaluate([var0, var1])\n         return v0_val, v1_val\n \n-    # When variables are initialized with Zero, FTRL-Proximal has two properties:\n+    # When variables are initialized with Zero, FTRL-Proximal has two\n+    # properties:\n     # 1. Without L1&L2 but with fixed learning rate, FTRL-Proximal is identical\n     # with GradientDescent.\n-    # 2. Without L1&L2 but with adaptive learning rate, FTRL-Proximal is identical\n-    # with Adagrad.\n+    # 2. Without L1&L2 but with adaptive learning rate, FTRL-Proximal is\n+    # identical with Adagrad.\n     # So, basing on these two properties, we test if our implementation of\n     # FTRL-Proximal performs same updates as Adagrad or GradientDescent.\n     def testEquivAdagradwithoutRegularization(self):\n\n@@ -51,10 +51,9 @@ class SGD(optimizer_v2.OptimizerV2):\n         `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable\n         that takes no arguments and returns the actual value to use. The\n         learning rate. Defaults to 0.01.\n-      momentum: float hyperparameter >= 0 that accelerates gradient descent\n-        in the relevant\n-        direction and dampens oscillations. Defaults to 0, i.e., vanilla gradient\n-        descent.\n+      momentum: float hyperparameter >= 0 that accelerates gradient descent in\n+        the relevant direction and dampens oscillations. Defaults to 0, i.e.,\n+        vanilla gradient descent.\n       nesterov: boolean. Whether to apply Nesterov momentum.\n         Defaults to `False`.\n       name: Optional name prefix for the operations created when applying\n\n@@ -368,8 +368,8 @@ class MomentumOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n             slot1 = mom_opt.get_slot(var1, \"momentum\")\n             self.assertEqual(slot1.shape, var1.shape)\n \n-            # Step 1: the momentum accumulators where 0. So we should see a normal\n-            # update: v -= grad * learning_rate\n+            # Step 1: the momentum accumulators where 0. So we should see a\n+            # normal update: v -= grad * learning_rate\n             self.evaluate(tf.compat.v1.global_variables_initializer())\n             self.evaluate(mom_update)\n             # Check that the momentum accumulators have been updated.\n@@ -568,8 +568,8 @@ class MomentumOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n                 # Fetch params to validate initial values\n                 self.assertAllClose([1.0, 2.0], self.evaluate(var0))\n                 self.assertAllClose([3.0, 4.0], self.evaluate(var1))\n-                # Step 1: the momentum accumulators where 0. So we should see a normal\n-                # update: v -= grad * learning_rate\n+                # Step 1: the momentum accumulators where 0. So we should see a\n+                # normal update: v -= grad * learning_rate\n                 self.evaluate(mom_update)\n                 # Check that the momentum accumulators have been updated.\n                 self.assertAllCloseAccordingToType(\n@@ -658,8 +658,8 @@ class MomentumOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n                 self.assertAllClose([0, 0], self.evaluate(var0)[1])\n                 self.assertAllClose([1, 1], self.evaluate(var1)[2])\n \n-                # Step 1: the momentum accumulators are 0. So we should see a normal\n-                # update: v -= grad * learning_rate\n+                # Step 1: the momentum accumulators are 0. So we should see a\n+                # normal update: v -= grad * learning_rate\n                 self.evaluate(mom_update)\n                 # Check that the momentum accumulators have been updated.\n                 self.assertAllCloseAccordingToType(\n@@ -749,8 +749,8 @@ class MomentumOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n                 # Fetch params to validate initial values\n                 self.assertAllClose([1.0, 2.0], self.evaluate(var0))\n                 self.assertAllClose([3.0, 4.0], self.evaluate(var1))\n-                # Step 1: the momentum accumulators where 0. So we should see a normal\n-                # update: v -= grad * learning_rate\n+                # Step 1: the momentum accumulators where 0. So we should see a\n+                # normal update: v -= grad * learning_rate\n                 self.evaluate(mom_update1)\n                 # Check that the momentum accumulators have been updated.\n                 self.assertAllCloseAccordingToType(\n@@ -768,7 +768,8 @@ class MomentumOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n                     np.array([3.0 - (0.01 * 2.0), 4.0 - (0.01 * 2.0)]),\n                     self.evaluate(var1),\n                 )\n-                # Step 2: the second momentum accumulators contain the previous update.\n+                # Step 2: the second momentum accumulators contain the previous\n+                # update.\n                 self.evaluate(mom_update2)\n                 # Check that the momentum accumulators have been updated.\n                 self.assertAllCloseAccordingToType(\n\n@@ -152,7 +152,8 @@ class Nadam(optimizer_v2.OptimizerV2):\n         )\n \n     def _prepare(self, var_list):\n-        # Get the value of the momentum cache before starting to apply gradients.\n+        # Get the value of the momentum cache before starting to apply\n+        # gradients.\n         self._m_cache_read = tf.identity(self._m_cache)\n         return super()._prepare(var_list)\n \n\n@@ -104,7 +104,8 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n     \"\"\"Base class for Keras optimizers.\n \n     You should not use this class directly, but instead instantiate one of its\n-    subclasses such as `tf.keras.optimizers.SGD`, `tf.keras.optimizers.Adam`, etc.\n+    subclasses such as `tf.keras.optimizers.SGD`, `tf.keras.optimizers.Adam`,\n+    etc.\n \n     ### Usage\n \n@@ -183,39 +184,40 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n     `tf.keras.losses.Reduction.SUM` for not.\n \n     To aggregate gradients yourself, call `apply_gradients` with\n-    `experimental_aggregate_gradients` set to False. This is useful if you need to\n-    process aggregated gradients.\n+    `experimental_aggregate_gradients` set to False. This is useful if you need\n+    to process aggregated gradients.\n \n     If you are not using these and you want to average gradients, you should use\n-    `tf.math.reduce_sum` to add up your per-example losses and then divide by the\n-    global batch size. Note that when using `tf.distribute.Strategy`, the first\n-    component of a tensor's shape is the *replica-local* batch size, which is off\n-    by a factor equal to the number of replicas being used to compute a single\n-    step. As a result, using `tf.math.reduce_mean` will give the wrong answer,\n-    resulting in gradients that can be many times too big.\n+    `tf.math.reduce_sum` to add up your per-example losses and then divide by\n+    the global batch size. Note that when using `tf.distribute.Strategy`, the\n+    first component of a tensor's shape is the *replica-local* batch size, which\n+    is off by a factor equal to the number of replicas being used to compute a\n+    single step. As a result, using `tf.math.reduce_mean` will give the wrong\n+    answer, resulting in gradients that can be many times too big.\n \n     ### Variable Constraints\n \n     All Keras optimizers respect variable constraints. If constraint function is\n     passed to any variable, the constraint will be applied to the variable after\n     the gradient has been applied to the variable.\n-    Important: If gradient is sparse tensor, variable constraint is not supported.\n+    Important: If gradient is sparse tensor, variable constraint is not\n+    supported.\n \n     ### Thread Compatibility\n \n-    The entire optimizer is currently thread compatible, not thread-safe. The user\n-    needs to perform synchronization if necessary.\n+    The entire optimizer is currently thread compatible, not thread-safe. The\n+    user needs to perform synchronization if necessary.\n \n     ### Slots\n \n     Many optimizer subclasses, such as `Adam` and `Adagrad` allocate and manage\n-    additional variables associated with the variables to train.  These are called\n-    <i>Slots</i>.  Slots have names and you can ask the optimizer for the names of\n-    the slots that it uses.  Once you have a slot name you can ask the optimizer\n-    for the variable it created to hold the slot value.\n+    additional variables associated with the variables to train.  These are\n+    called <i>Slots</i>.  Slots have names and you can ask the optimizer for the\n+    names of the slots that it uses.  Once you have a slot name you can ask the\n+    optimizer for the variable it created to hold the slot value.\n \n-    This can be useful if you want to log debug a training algorithm, report stats\n-    about the slots, etc.\n+    This can be useful if you want to log debug a training algorithm, report\n+    stats about the slots, etc.\n \n     ### Hyperparameters\n \n@@ -278,8 +280,8 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n     If you intend to create your own optimization algorithm, simply inherit from\n     this class and override the following methods:\n \n-      - `_resource_apply_dense` (update variable given gradient tensor is a dense\n-        `tf.Tensor`)\n+      - `_resource_apply_dense` (update variable given gradient tensor is a\n+        dense `tf.Tensor`)\n       - `_resource_apply_sparse` (update variable given gradient tensor is a\n         sparse `tf.IndexedSlices`. The most common way for this to happen\n         is if you are taking the gradient through a `tf.gather`.)\n@@ -330,9 +332,9 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n           name: String. The name to use for momentum accumulator weights created\n             by the optimizer.\n           gradient_aggregator: The function to use to aggregate gradients across\n-            devices (when using `tf.distribute.Strategy`). If `None`, defaults to\n-            summing the gradients across devices. The function should accept and\n-            return a list of `(gradient, variable)` tuples.\n+            devices (when using `tf.distribute.Strategy`). If `None`, defaults\n+            to summing the gradients across devices. The function should accept\n+            and return a list of `(gradient, variable)` tuples.\n           gradient_transformers: Optional. List of functions to use to transform\n             gradients before applying updates to Variables. The functions are\n             applied after `gradient_aggregator`. The functions should accept and\n@@ -342,9 +344,10 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n             If `clipvalue` (float) is set, the gradient of each weight\n             is clipped to be no higher than this value.\n             If `clipnorm` (float) is set, the gradient of each weight\n-            is individually clipped so that its norm is no higher than this value.\n-            If `global_clipnorm` (float) is set the gradient of all weights is\n-            clipped so that their global norm is no higher than this value.\n+            is individually clipped so that its norm is no higher than this\n+            value. If `global_clipnorm` (float) is set the gradient of all\n+            weights is clipped so that their global norm is no higher than this\n+            value.\n \n         Raises:\n           ValueError: in case of any invalid argument.\n@@ -373,7 +376,8 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n                 )\n             if k == \"lr\":\n                 warnings.warn(\n-                    \"The `lr` argument is deprecated, use `learning_rate` instead.\",\n+                    \"The `lr` argument is deprecated, \"\n+                    \"use `learning_rate` instead.\",\n                     stacklevel=2,\n                 )\n \n@@ -403,8 +407,8 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n         self._initial_decay = decay\n \n         self._hypers_created = False\n-        # Store the distribution strategy object if the optimizer is created inside\n-        # strategy scope, so it could be used to create variables later.\n+        # Store the distribution strategy object if the optimizer is created\n+        # inside strategy scope, so it could be used to create variables later.\n         if tf.distribute.has_strategy():\n             self._distribution_strategy = tf.distribute.get_strategy()\n         else:\n@@ -497,7 +501,8 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n         )\n \n     def _transform_loss(self, loss):\n-        \"\"\"Called in `.minimize` to transform loss before computing gradients.\"\"\"\n+        \"\"\"Called in `.minimize` to transform loss before computing\n+        gradients.\"\"\"\n         return loss\n \n     def _get_gradients(self, tape, loss, var_list, grad_loss=None):\n@@ -512,15 +517,15 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n     def _aggregate_gradients(self, grads_and_vars):\n         \"\"\"Called in `apply_gradients` to aggregate gradients across devices.\n \n-        Note that user subclasses may override this, so the interface should not be\n-        changed.\n+        Note that user subclasses may override this, so the interface should not\n+        be changed.\n \n         Args:\n           grads_and_vars: List of (gradient, variable) pairs.\n \n         Returns:\n-          A list of (aggregrated_gradient, variable) pairs. By default, this calls\n-          `self.gradient_aggregator`.\n+          A list of (aggregrated_gradient, variable) pairs. By default, this\n+          calls `self.gradient_aggregator`.\n         \"\"\"\n         return self.gradient_aggregator(grads_and_vars)\n \n@@ -546,23 +551,23 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n         of using this function.\n \n         Args:\n-          loss: `Tensor` or callable. If a callable, `loss` should take no arguments\n-            and return the value to minimize. If a `Tensor`, the `tape` argument\n-            must be passed.\n+          loss: `Tensor` or callable. If a callable, `loss` should take no\n+            arguments and return the value to minimize. If a `Tensor`, the\n+            `tape` argument must be passed.\n           var_list: list or tuple of `Variable` objects to update to minimize\n-            `loss`, or a callable returning the list or tuple of `Variable` objects.\n-            Use callable when the variable list would otherwise be incomplete before\n-            `minimize` since the variables are created at the first time `loss` is\n-            called.\n+            `loss`, or a callable returning the list or tuple of `Variable`\n+            objects.  Use callable when the variable list would otherwise be\n+            incomplete before `minimize` since the variables are created at the\n+            first time `loss` is called.\n           grad_loss: (Optional). A `Tensor` holding the gradient computed for\n             `loss`.\n           name: (Optional) str. Name for the returned operation.\n-          tape: (Optional) `tf.GradientTape`. If `loss` is provided as a `Tensor`,\n-            the tape that computed the `loss` must be provided.\n+          tape: (Optional) `tf.GradientTape`. If `loss` is provided as a\n+            `Tensor`, the tape that computed the `loss` must be provided.\n \n         Returns:\n-          An `Operation` that updates the variables in `var_list`. The `iterations`\n-          will be automatically increased by 1.\n+          An `Operation` that updates the variables in `var_list`. The\n+          `iterations` will be automatically increased by 1.\n \n         Raises:\n           ValueError: If some of the variables are not `Variable` objects.\n@@ -584,16 +589,17 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n \n         Args:\n           loss: `Tensor` or callable. If a callable, `loss` should take no\n-            arguments and return the value to minimize. If a `Tensor`, the `tape`\n-            argument must be passed.\n+            arguments and return the value to minimize. If a `Tensor`, the\n+            `tape` argument must be passed.\n           var_list: list or tuple of `Variable` objects to update to minimize\n-            `loss`, or a callable returning the list or tuple of `Variable` objects.\n-            Use callable when the variable list would otherwise be incomplete before\n-            `minimize` and the variables are created at the first time when `loss`\n-            is called.\n-          grad_loss: Optional. A `Tensor` holding the gradient computed for `loss`.\n-          tape: (Optional) `tf.GradientTape`. If `loss` is provided as a `Tensor`,\n-            the tape that computed the `loss` must be provided.\n+            `loss`, or a callable returning the list or tuple of `Variable`\n+            objects.  Use callable when the variable list would otherwise be\n+            incomplete before `minimize` and the variables are created at the\n+            first time when `loss` is called.\n+          grad_loss: Optional. A `Tensor` holding the gradient computed for\n+            `loss`.\n+          tape: (Optional) `tf.GradientTape`. If `loss` is provided as a\n+            `Tensor`, the tape that computed the `loss` must be provided.\n \n         Returns:\n           A list of (gradient, variable) pairs. Variable is always present, but\n@@ -647,8 +653,8 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n         applies gradients.\n \n         The method sums gradients from all replicas in the presence of\n-        `tf.distribute.Strategy` by default. You can aggregate gradients yourself by\n-        passing `experimental_aggregate_gradients=False`.\n+        `tf.distribute.Strategy` by default. You can aggregate gradients\n+        yourself by passing `experimental_aggregate_gradients=False`.\n \n         Example:\n \n@@ -663,11 +669,12 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n \n         Args:\n           grads_and_vars: List of (gradient, variable) pairs.\n-          name: Optional name for the returned operation. Default to the name passed\n-            to the `Optimizer` constructor.\n-          experimental_aggregate_gradients: Whether to sum gradients from different\n-            replicas in the presence of `tf.distribute.Strategy`. If False, it's\n-            user responsibility to aggregate the gradients. Default to True.\n+          name: Optional name for the returned operation. Default to the name\n+            passed to the `Optimizer` constructor.\n+          experimental_aggregate_gradients: Whether to sum gradients from\n+            different replicas in the presence of `tf.distribute.Strategy`. If\n+            False, it's user responsibility to aggregate the gradients. Default\n+            to True.\n \n         Returns:\n           An `Operation` that applies the specified gradients. The `iterations`\n@@ -687,16 +694,16 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n                 self._create_all_weights(var_list)\n \n             if not grads_and_vars:\n-                # Distribution strategy does not support reducing an empty list of\n-                # gradients\n+                # Distribution strategy does not support reducing an empty list\n+                # of gradients\n                 return tf.no_op()\n \n             if tf.distribute.in_cross_replica_context():\n                 raise RuntimeError(\n-                    \"`apply_gradients() cannot be called in cross-replica context. \"\n-                    \"Use `tf.distribute.Strategy.run` to enter replica \"\n-                    \"context. For more information, please see the docstring of \"\n-                    \"`tf.distribute.get_replica_context`.\"\n+                    \"`apply_gradients() cannot be called in cross-replica \"\n+                    \"context. Use `tf.distribute.Strategy.run` to enter \"\n+                    \"replica context. For more information, please see the \"\n+                    \"docstring of `tf.distribute.get_replica_context`.\"\n                 )\n \n             strategy = tf.distribute.get_strategy()\n@@ -714,9 +721,9 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n                 )\n             ):\n                 raise NotImplementedError(\n-                    \"`experimental_aggregate_gradients=False is not supported for \"\n-                    \"ParameterServerStrategy and CentralStorageStrategy. Used: \"\n-                    f\"strategy={strategy}.\"\n+                    \"`experimental_aggregate_gradients=False is not supported \"\n+                    \"for ParameterServerStrategy and CentralStorageStrategy. \"\n+                    f\"Used: strategy={strategy}.\"\n                 )\n \n             apply_state = self._prepare(var_list)\n@@ -745,15 +752,17 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n             \"\"\"Apply gradient to variable.\"\"\"\n             if isinstance(var, tf.Tensor):\n                 raise NotImplementedError(\n-                    f\"Updating a `Tensor` is not implemented. Received: var={var}.\"\n+                    f\"Updating a `Tensor` is not implemented. \"\n+                    f\"Received: var={var}.\"\n                 )\n \n             apply_kwargs = {}\n             if isinstance(grad, tf.IndexedSlices):\n                 if var.constraint is not None:\n                     raise RuntimeError(\n-                        \"Cannot use a constraint function on a sparse variable. \"\n-                        f\"Received: grad={grad}, var.constraint={var.constraint}.\"\n+                        \"Cannot use a constraint function on a sparse \"\n+                        f\"variable. Received: grad={grad}, \"\n+                        f\"var.constraint={var.constraint}.\"\n                     )\n                 if \"apply_state\" in self._sparse_apply_args:\n                     apply_kwargs[\"apply_state\"] = apply_state\n@@ -776,8 +785,8 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n         update_ops = []\n         with name_scope_only_in_function_or_graph(name or self._name):\n             for grad, var in grads_and_vars:\n-                # Colocate the update with variables to avoid unnecessary communication\n-                # delays. See b/136304694.\n+                # Colocate the update with variables to avoid unnecessary\n+                # communication delays. See b/136304694.\n                 with distribution.extended.colocate_vars_with(var):\n                     with name_scope_only_in_function_or_graph(\n                         \"update\"\n@@ -791,12 +800,13 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n                             group=False,\n                         )\n                         if tf.distribute.in_cross_replica_context():\n-                            # In cross-replica context, extended.update returns a list of\n-                            # update ops from all replicas (group=False).\n+                            # In cross-replica context, extended.update returns\n+                            # a list of update ops from all replicas\n+                            # (group=False).\n                             update_ops.extend(update_op)\n                         else:\n-                            # In replica context, extended.update return the single update op\n-                            # of current replica.\n+                            # In replica context, extended.update return the\n+                            # single update op of current replica.\n                             update_ops.append(update_op)\n \n             any_symbolic = any(\n@@ -804,9 +814,9 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n                 for i in update_ops\n             )\n             if not tf.executing_eagerly() or any_symbolic:\n-                # If the current context is graph mode or any of the update ops are\n-                # symbolic then the step update should be carried out under a graph\n-                # context. (eager updates execute immediately)\n+                # If the current context is graph mode or any of the update ops\n+                # are symbolic then the step update should be carried out under\n+                # a graph context. (eager updates execute immediately)\n                 with backend._current_graph(\n                     update_ops\n                 ).as_default():  # pylint: disable=protected-access\n@@ -905,14 +915,15 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n     def _create_slots_for_sharded_variables(self, var_list):\n         \"\"\"Add ShardedVariables to slots to later reconstruct for checkpointing.\n \n-        ShardedVariables don't have slot variables created for them; their shards\n-        do. This function allows users to call get_slot with a ShardedVariable input\n-        and receive a ShardedVariable output containing the appropriate slot vars.\n+        ShardedVariables don't have slot variables created for them; their\n+        shards do. This function allows users to call get_slot with a\n+        ShardedVariable input and receive a ShardedVariable output containing\n+        the appropriate slot vars.\n \n         Iterate over the variables to find shards, and aggregate the sharded\n-        containers in a set. Add these ShardedVariables to _slots so that get_slot\n-        can retrieve the proper slot variables for their component shards, and\n-        reconstruct those into a ShardedVariable.\n+        containers in a set. Add these ShardedVariables to _slots so that\n+        get_slot can retrieve the proper slot variables for their component\n+        shards, and reconstruct those into a ShardedVariable.\n \n         Args:\n           var_list: list or tuple of `Variable` objects that will be minimized\n@@ -933,12 +944,13 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n             self._slots[sharded_key] = slot_dict\n \n     def _create_all_weights(self, var_list):\n-        \"\"\"Creates all weights, including iterations, hyperparameters and slot vars.\n+        \"\"\"Creates all weights, including iterations, hyperparameters and slot\n+        vars.\n \n         This will add newly created variables to `optimizer.weights`.\n \n-        New variables are only created when this method is called the first time, or\n-        when called with different variables in the var_list.\n+        New variables are only created when this method is called the first\n+        time, or when called with different variables in the var_list.\n \n         Args:\n           var_list: list or tuple of `Variable` objects that will be minimized\n@@ -990,15 +1002,15 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n     def add_slot(self, var, slot_name, initializer=\"zeros\", shape=None):\n         \"\"\"Add a new slot variable for `var`.\n \n-        A slot variable is an additional variable associated with `var` to train.\n-        It is allocated and managed by optimizers, e.g. `Adam`.\n+        A slot variable is an additional variable associated with `var` to\n+        train.  It is allocated and managed by optimizers, e.g. `Adam`.\n \n         Args:\n           var: a `Variable` object.\n           slot_name: name of the slot variable.\n           initializer: initializer of the slot variable\n-          shape: (Optional) shape of the slot variable. If not set, it will default\n-          to the shape of `var`.\n+          shape: (Optional) shape of the slot variable. If not set, it will\n+            default to the shape of `var`.\n \n         Returns:\n           A slot variable.\n@@ -1028,13 +1040,13 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n                 strategy = tf.distribute.get_strategy()\n                 if not strategy.extended.variable_created_in_scope(var):\n                     raise ValueError(\n-                        \"Trying to create optimizer slot variable under the scope for \"\n-                        \"tf.distribute.Strategy ({}), which is different from the scope \"\n-                        \"used for the original variable ({}). Make sure the slot \"\n-                        \"variables are created under the same strategy scope. This may \"\n-                        \"happen if you're restoring from a checkpoint outside the scope.\".format(\n-                            strategy, var\n-                        )\n+                        \"Trying to create optimizer slot variable under the \"\n+                        \"scope for tf.distribute.Strategy ({}), which is \"\n+                        \"different from the scope used for the original \"\n+                        \"variable ({}). Make sure the slot variables are \"\n+                        \"created under the same strategy scope. This may \"\n+                        \"happen if you're restoring from a checkpoint \"\n+                        \"outside the scope.\".format(strategy, var)\n                     )\n \n                 with strategy.extended.colocate_vars_with(var):\n@@ -1063,8 +1075,8 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n         if isinstance(\n             slot_variable, tf.__internal__.distribute.ShardedVariable\n         ):\n-            # Construct a ShardedVariable that points to the input ShardedVariable's\n-            # component shard's slot variables.\n+            # Construct a ShardedVariable that points to the input\n+            # ShardedVariable's component shard's slot variables.\n             shard_vars = []\n             for shard in slot_variable.variables:\n                 slot_shard = self.get_slot(shard, slot_name)\n@@ -1113,9 +1125,9 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n                 if isinstance(value, (tf.Tensor, tf.Variable)) or callable(\n                     value\n                 ):\n-                    # The check for `callable` covers the usage when `value` is a\n-                    # `LearningRateSchedule`, in which case it does not need to create a\n-                    # variable.\n+                    # The check for `callable` covers the usage when `value` is\n+                    # a `LearningRateSchedule`, in which case it does not need\n+                    # to create a variable.\n                     continue\n                 else:\n                     self._hyper[name] = self.add_weight(\n@@ -1196,9 +1208,9 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n \n         Args:\n             config: A Python dictionary, typically the output of get_config.\n-            custom_objects: A Python dictionary mapping names to additional Python\n-              objects used to create this optimizer, such as a function used for a\n-              hyperparameter.\n+            custom_objects: A Python dictionary mapping names to additional\n+              Python objects used to create this optimizer, such as a function\n+              used for a hyperparameter.\n \n         Returns:\n             An optimizer instance.\n@@ -1213,7 +1225,8 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n         return cls(**config)\n \n     def _serialize_hyperparameter(self, hyperparameter_name):\n-        \"\"\"Serialize a hyperparameter that can be a float, callable, or Tensor.\"\"\"\n+        \"\"\"Serialize a hyperparameter that can be a float, callable, or\n+        Tensor.\"\"\"\n         value = self._hyper[hyperparameter_name]\n         if isinstance(value, learning_rate_schedule.LearningRateSchedule):\n             return learning_rate_schedule.serialize(value)\n@@ -1242,9 +1255,9 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n         variables in the order they were created. The returned list can in turn\n         be used to load state into similarly parameterized optimizers.\n \n-        For example, the RMSprop optimizer for this simple model returns a list of\n-        three values-- the iteration count, followed by the root-mean-square value\n-        of the kernel and bias of the single Dense layer:\n+        For example, the RMSprop optimizer for this simple model returns a list\n+        of three values-- the iteration count, followed by the root-mean-square\n+        value of the kernel and bias of the single Dense layer:\n \n         >>> opt = tf.keras.optimizers.RMSprop()\n         >>> m = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])\n@@ -1269,12 +1282,12 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n         This function takes the weight values associated with this\n         optimizer as a list of Numpy arrays. The first value is always the\n         iterations count of the optimizer, followed by the optimizer's state\n-        variables in the order they are created. The passed values are used to set\n-        the new state of the optimizer.\n+        variables in the order they are created. The passed values are used to\n+        set the new state of the optimizer.\n \n         For example, the RMSprop optimizer for this simple model takes a list of\n-        three values-- the iteration count, followed by the root-mean-square value\n-        of the kernel and bias of the single Dense layer:\n+        three values-- the iteration count, followed by the root-mean-square\n+        value of the kernel and bias of the single Dense layer:\n \n         >>> opt = tf.keras.optimizers.RMSprop()\n         >>> m = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])\n@@ -1332,12 +1345,13 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n             if trainable:\n                 raise ValueError(\n                     \"Synchronization value can be set to \"\n-                    \"VariableSynchronization.ON_READ only for non-trainable variables. \"\n-                    \"You have specified trainable=True and \"\n+                    \"VariableSynchronization.ON_READ only for non-trainable \"\n+                    \"variables. You have specified trainable=True and \"\n                     \"synchronization=VariableSynchronization.ON_READ.\"\n                 )\n             else:\n-                # Set trainable to be false when variable is to be synced on read.\n+                # Set trainable to be false when variable is to be synced on\n+                # read.\n                 trainable = False\n         elif trainable is None:\n             trainable = True\n@@ -1405,8 +1419,8 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n \n         Args:\n           grad: a `Tensor` representing the gradient.\n-          handle: a `Tensor` of dtype `resource` which points to the variable to be\n-            updated.\n+          handle: a `Tensor` of dtype `resource` which points to the variable to\n+            be updated.\n           apply_state: A dict which is used across multiple apply calls.\n \n         Returns:\n@@ -1421,20 +1435,20 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n     ):\n         \"\"\"Add ops to apply sparse gradients to `handle`, with repeated indices.\n \n-        Optimizers which override this method must deal with repeated indices. See\n-        the docstring of `_apply_sparse_duplicate_indices` for details. By default\n-        the correct behavior, to sum non-unique indices and their associated\n-        gradients, is enforced by first pre-processing `grad` and `indices` and\n-        passing them on to `_resource_apply_sparse`. Optimizers which deal correctly\n-        with duplicate indices may instead override this method to avoid the\n-        overhead of summing.\n+        Optimizers which override this method must deal with repeated indices.\n+        See the docstring of `_apply_sparse_duplicate_indices` for details. By\n+        default the correct behavior, to sum non-unique indices and their\n+        associated gradients, is enforced by first pre-processing `grad` and\n+        `indices` and passing them on to `_resource_apply_sparse`. Optimizers\n+        which deal correctly with duplicate indices may instead override this\n+        method to avoid the overhead of summing.\n \n         Args:\n           grad: a `Tensor` representing the gradient for the affected indices.\n-          handle: a `Tensor` of dtype `resource` which points to the variable to be\n-            updated.\n-          indices: a `Tensor` of integral type representing the indices for which\n-            the gradient is nonzero. Indices may be repeated.\n+          handle: a `Tensor` of dtype `resource` which points to the variable to\n+            be updated.\n+          indices: a `Tensor` of integral type representing the indices for\n+            which the gradient is nonzero. Indices may be repeated.\n           **kwargs: May optionally contain `apply_state`\n \n         Returns:\n@@ -1450,17 +1464,17 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n     def _resource_apply_sparse(self, grad, handle, indices, apply_state):\n         \"\"\"Add ops to apply sparse gradients to the variable `handle`.\n \n-        Similar to `_apply_sparse`, the `indices` argument to this method has been\n-        de-duplicated. Optimizers which deal correctly with non-unique indices may\n-        instead override `_resource_apply_sparse_duplicate_indices` to avoid this\n-        overhead.\n+        Similar to `_apply_sparse`, the `indices` argument to this method has\n+        been de-duplicated. Optimizers which deal correctly with non-unique\n+        indices may instead override `_resource_apply_sparse_duplicate_indices`\n+        to avoid this overhead.\n \n         Args:\n           grad: a `Tensor` representing the gradient for the affected indices.\n-          handle: a `Tensor` of dtype `resource` which points to the variable to be\n-            updated.\n-          indices: a `Tensor` of integral type representing the indices for which\n-            the gradient is nonzero. Indices are unique.\n+          handle: a `Tensor` of dtype `resource` which points to the variable to\n+            be updated.\n+          indices: a `Tensor` of integral type representing the indices for\n+            which the gradient is nonzero. Indices are unique.\n           apply_state: A dict which is used across multiple apply calls.\n \n         Returns:\n@@ -1510,8 +1524,8 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n         deferred_restorations = self._deferred_slot_restorations.get(\n             slot_name, {}\n         ).pop(variable_key, [])\n-        # Iterate over restores, highest restore UID first to minimize the number\n-        # of assignments.\n+        # Iterate over restores, highest restore UID first to minimize the\n+        # number of assignments.\n         deferred_restorations.sort(\n             key=lambda position: position.restore_uid, reverse=True\n         )\n@@ -1526,16 +1540,16 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n         It is up to the caller to restore the value into the slot variable if a\n         valid slot variable is returned.\n \n-        Called when a variable which has an associated slot variable is created or\n-        restored. When executing eagerly, we create the slot variable with a\n+        Called when a variable which has an associated slot variable is created\n+        or restored. When executing eagerly, we create the slot variable with a\n         restoring initializer.\n \n         No new variables are created when graph building. Instead,\n-        _restore_slot_variable catches these after normal creation and adds restore\n-        ops to the graph. This method is nonetheless important when graph building\n-        for the case when a slot variable has already been created but `variable`\n-        has just been added to a dependency graph (causing us to realize that the\n-        slot variable needs to be restored).\n+        _restore_slot_variable catches these after normal creation and adds\n+        restore ops to the graph. This method is nonetheless important when\n+        graph building for the case when a slot variable has already been\n+        created but `variable` has just been added to a dependency graph\n+        (causing us to realize that the slot variable needs to be restored).\n \n         Args:\n           slot_variable_position: A `trackable._CheckpointPosition` object\n@@ -1544,8 +1558,8 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n           variable: The variable object this slot is being created for.\n \n         Returns:\n-          A slot variable that should have a value restored into it, or None if a\n-          slot variable should not be restored at this time.\n+          A slot variable that should have a value restored into it, or None if\n+          a slot variable should not be restored at this time.\n         \"\"\"\n         variable_key = _var_key(variable)\n         slot_dict = self._slots.get(variable_key, {})\n@@ -1554,22 +1568,22 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n             slot_variable is None\n             and tf.executing_eagerly()\n             and slot_variable_position.is_simple_variable()\n-            # Defer slot variable creation if there is an active variable creator\n-            # scope. Generally we'd like to eagerly create/restore slot variables\n-            # when possible, but this may mean that scopes intended to catch\n-            # `variable` also catch its eagerly created slot variable\n-            # unintentionally (specifically make_template would add a dependency on\n-            # a slot variable if not for this case). Deferring is mostly harmless\n-            # (aside from double initialization), and makes variable creator scopes\n-            # behave the same way they do when graph building.\n+            # Defer slot variable creation if there is an active variable\n+            # creator scope. Generally we'd like to eagerly create/restore slot\n+            # variables when possible, but this may mean that scopes intended to\n+            # catch `variable` also catch its eagerly created slot variable\n+            # unintentionally (specifically make_template would add a dependency\n+            # on a slot variable if not for this case). Deferring is mostly\n+            # harmless (aside from double initialization), and makes variable\n+            # creator scopes behave the same way they do when graph building.\n             #\n-            # One notable case is with distribution strategy, which uses variable\n-            # creator scope but always desires the `variable` and the slot to use\n-            # the same scope, thus we can safely eagerly create/restore slot\n-            # variables.\n+            # One notable case is with distribution strategy, which uses\n+            # variable creator scope but always desires the `variable` and the\n+            # slot to use the same scope, thus we can safely eagerly\n+            # create/restore slot variables.\n             and (\n                 not tf.compat.v1.get_default_graph()._variable_creator_stack\n-                or self._distribution_strategy  # pylint: disable=protected-access\n+                or self._distribution_strategy\n             )\n         ):\n             initializer = (\n@@ -1583,16 +1597,16 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n                 slot_name=slot_name,\n                 shape=slot_variable_position.value_shape(),\n             )\n-            # Slot variables are not owned by any one object (because we don't want to\n-            # save the slot variable if the optimizer is saved without the non-slot\n-            # variable, or if the non-slot variable is saved without the optimizer;\n-            # it's a dependency hypergraph with edges of the form (optimizer, non-slot\n-            # variable, variable)). So we don't _track_ slot variables anywhere, and\n-            # instead special-case this dependency and otherwise pretend it's a normal\n-            # graph.\n+            # Slot variables are not owned by any one object (because we don't\n+            # want to save the slot variable if the optimizer is saved without\n+            # the non-slot variable, or if the non-slot variable is saved\n+            # without the optimizer; it's a dependency hypergraph with edges of\n+            # the form (optimizer, non-slot variable, variable)). So we don't\n+            # _track_ slot variables anywhere, and instead special-case this\n+            # dependency and otherwise pretend it's a normal graph.\n         if slot_variable is not None:\n-            # For sharded variables, we need the logic in get_slot to combine slot\n-            # variables for its shards\n+            # For sharded variables, we need the logic in get_slot to combine\n+            # slot variables for its shards\n             if (slot_variable is variable) and (\n                 isinstance(variable, tf.__internal__.distribute.ShardedVariable)\n             ):\n@@ -1601,10 +1615,10 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n             # existing slot variable, we should restore it.\n             return slot_variable\n         else:\n-            # We didn't make the slot variable. Defer restoring until it gets created\n-            # normally. We keep a list rather than the one with the highest restore\n-            # UID in case slot variables have their own dependencies, in which case\n-            # those could differ between restores.\n+            # We didn't make the slot variable. Defer restoring until it gets\n+            # created normally. We keep a list rather than the one with the\n+            # highest restore UID in case slot variables have their own\n+            # dependencies, in which case those could differ between restores.\n             self._deferred_slot_restorations.setdefault(\n                 slot_name, {}\n             ).setdefault(variable_key, []).append(slot_variable_position)\n@@ -1612,7 +1626,8 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n \n     @contextlib.contextmanager\n     def _distribution_strategy_scope(self):\n-        \"\"\"Returns the `tf.distribute.Strategy` this optimizer was created under.\"\"\"\n+        \"\"\"Returns the `tf.distribute.Strategy` this optimizer was created\n+        under.\"\"\"\n         if self._distribution_strategy and not tf.distribute.has_strategy():\n             with self._distribution_strategy.scope():\n                 yield self._distribution_strategy.scope()\n@@ -1653,8 +1668,8 @@ def _get_slot_key_from_var(var, slot_name):\n class RestoredOptimizer(OptimizerV2):\n     \"\"\"A non-functional Optimizer implementation for checkpoint compatibility.\n \n-    Holds slot variables and hyperparameters when an optimizer is restored from a\n-    SavedModel. These variables may be referenced in functions along with ops\n+    Holds slot variables and hyperparameters when an optimizer is restored from\n+    a SavedModel. These variables may be referenced in functions along with ops\n     created by the original optimizer, but currently we do not support using the\n     optimizer object itself (e.g. through `apply_gradients`).\n     \"\"\"\n@@ -1670,8 +1685,8 @@ class RestoredOptimizer(OptimizerV2):\n         # TODO(allenl): Save and restore the Optimizer's config\n         raise NotImplementedError(\n             \"Restoring functional Optimizers from SavedModels is not currently \"\n-            \"supported. Please file a feature request if this limitation bothers \"\n-            \"you.\"\n+            \"supported. Please file a feature request if this limitation \"\n+            \"bothers you.\"\n         )\n \n \n@@ -1684,7 +1699,7 @@ tf.__internal__.saved_model.load.register_revived_type(\n             version=2,\n             min_producer_version=1,\n             min_consumer_version=1,\n-            setter=RestoredOptimizer._set_hyper,  # pylint: disable=protected-access\n+            setter=RestoredOptimizer._set_hyper,\n         )\n     ],\n )\n\n@@ -459,7 +459,8 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n             loss3 = lambda: 3 * var3 + 5 * var4\n             opt_op_3 = opt1.minimize(loss3, [var3, var4])\n \n-            # Assert set_weights with ValueError since weight list does not match.\n+            # Assert set_weights with ValueError since weight list does not\n+            # match.\n             self.evaluate(tf.compat.v1.global_variables_initializer())\n             weights = opt1.get_weights()\n             with self.assertRaisesRegex(ValueError, \"but the optimizer was\"):\n@@ -727,8 +728,8 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n         test_combinations.combine(mode=[\"graph\", \"eager\"])\n     )\n     def testAggregationTrue(self):\n-        # Test that experimental_aggregate_gradients=True works without distributed\n-        # strategy.\n+        # Test that experimental_aggregate_gradients=True works without\n+        # distributed strategy.\n         var = tf.Variable([1.0, 2.0])\n         opt = gradient_descent.SGD(3.0)\n \n@@ -745,8 +746,8 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n         test_combinations.combine(mode=[\"graph\", \"eager\"])\n     )\n     def testAggregationFalse(self):\n-        # Test that experimental_aggregate_gradients=False works without distributed\n-        # strategy.\n+        # Test that experimental_aggregate_gradients=False works without\n+        # distributed strategy.\n         var = tf.Variable([1.0, 2.0])\n         opt = gradient_descent.SGD(3.0)\n \n@@ -766,8 +767,9 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n         checkpoint = tf.train.Checkpoint(optimizer=opt)\n         path = checkpoint.save(self.get_temp_dir())\n \n-        # Following verifies that the `iterations` can be restored with the absence\n-        # of an `Optimizer` object (using a `Checkpoint` as a placeholder).\n+        # Following verifies that the `iterations` can be restored with the\n+        # absence of an `Optimizer` object (using a `Checkpoint` as a\n+        # placeholder).\n         iterations_var = tf.Variable(0, dtype=tf.int64)\n         optimizer_checkpoint = tf.train.Checkpoint(iter=iterations_var)\n         checkpoint_to_restore = tf.train.Checkpoint(\n@@ -779,7 +781,8 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n \n     @test_combinations.generate(test_combinations.combine(mode=[\"eager\"]))\n     def testSlotWithNonstandardShapeRestoresBasedOnCheckpoint(self):\n-        # First create an optimizer and a slot variable with a non-standard shape.\n+        # First create an optimizer and a slot variable with a non-standard\n+        # shape.\n         x = tf.Variable([[1.0, 2.0], [3.0, 4.0]], dtype=tf.float32)\n         slot_shape = [2, 1]\n         optimizer_1 = optimizer_v2.OptimizerV2(name=\"test\")\n@@ -803,8 +806,8 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n     )\n     def test_gradient_aggregator(self):\n         def gradient_aggregator(grads_and_vars):\n-            # Simulate an all-reduce where the other replica has zeros for gradients,\n-            # by dividing each gradient by 2.\n+            # Simulate an all-reduce where the other replica has zeros for\n+            # gradients, by dividing each gradient by 2.\n             grads = [g for g, _ in grads_and_vars]\n             vars = [\n                 v for _, v in grads_and_vars\n@@ -845,7 +848,8 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n \n     @test_combinations.generate(test_combinations.combine(mode=[\"eager\"]))\n     def test_create_slots_for_sharded_variables(self):\n-        # set names so that ShardedVariable is well-named for slot variable keying.\n+        # set names so that ShardedVariable is well-named for slot variable\n+        # keying.\n         var_a = tf.Variable([1.0], name=\"part_0\")\n         var_b = tf.Variable([2.0], name=\"part_1\")\n         sharded_var = tf.__internal__.distribute.ShardedVariable([var_a, var_b])\n@@ -1254,8 +1258,8 @@ def identify_redundant_ops(graph):\n     \"\"\"Implements basic common subexpression elimination.\n \n     This is not intended to replicate the graph semantics of TensorFlow Graphs\n-    (for instance it does not handle stateful op ordering), nor is it intended to\n-    replace the common subexpression elimination Grappler pass. Rather, it\n+    (for instance it does not handle stateful op ordering), nor is it intended\n+    to replace the common subexpression elimination Grappler pass. Rather, it\n     provides a high level sanity check that clearly redundant ops are not being\n     created.\n \n@@ -1275,8 +1279,8 @@ def identify_redundant_ops(graph):\n         for op_input, name in zip(*get_inputs(op)):\n             input_def = op_input.node_def\n \n-            # Operations can have multiple outputs. We track which is used to prevent\n-            # overzealous elimination.\n+            # Operations can have multiple outputs. We track which is used to\n+            # prevent overzealous elimination.\n             input_def.name = name\n \n             input_def.input[:] = [name_map.get(i, i) for i in input_def.input]\n\n@@ -106,33 +106,36 @@ class RMSprop(optimizer_v2.OptimizerV2):\n         \"\"\"Construct a new RMSprop optimizer.\n \n         Args:\n-          learning_rate: A `Tensor`, floating point value, or a schedule that is a\n-            `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable\n-            that takes no arguments and returns the actual value to use. The\n-            learning rate. Defaults to 0.001.\n-          rho: Discounting factor for the history/coming gradient. Defaults to 0.9.\n+          learning_rate: A `Tensor`, floating point value, or a schedule that is\n+            a `tf.keras.optimizers.schedules.LearningRateSchedule`, or a\n+            callable that takes no arguments and returns the actual value to\n+            use. The learning rate. Defaults to 0.001.\n+          rho: Discounting factor for the history/coming gradient. Defaults to\n+            0.9.\n           momentum: A scalar or a scalar `Tensor`. Defaults to 0.0.\n           epsilon: A small constant for numerical stability. This epsilon is\n             \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n-            Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to\n-            1e-7.\n-          centered: Boolean. If `True`, gradients are normalized by the estimated\n-            variance of the gradient; if False, by the uncentered second moment.\n-            Setting this to `True` may help with training, but is slightly more\n-            expensive in terms of computation and memory. Defaults to `False`.\n+            Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults\n+            to 1e-7.\n+          centered: Boolean. If `True`, gradients are normalized by the\n+            estimated variance of the gradient; if False, by the uncentered\n+            second moment.  Setting this to `True` may help with training, but\n+            is slightly more expensive in terms of computation and memory.\n+            Defaults to `False`.\n           name: Optional name prefix for the operations created when applying\n             gradients. Defaults to \"RMSprop\".\n-          **kwargs: keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`,\n-            `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is clip\n-            gradients by value, `decay` is included for backward compatibility to\n-            allow time inverse decay of learning rate. `lr` is included for backward\n-            compatibility, recommended to use `learning_rate` instead.\n+          **kwargs: keyword arguments. Allowed to be {`clipnorm`, `clipvalue`,\n+            `lr`, `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is\n+            clip gradients by value, `decay` is included for backward\n+            compatibility to allow time inverse decay of learning rate. `lr` is\n+            included for backward compatibility, recommended to use\n+            `learning_rate` instead.\n \n         @compatibility(eager)\n-        When eager execution is enabled, `learning_rate`, `decay`, `momentum`, and\n-        `epsilon` can each be a callable that takes no arguments and returns the\n-        actual value to use. This can be useful for changing these values across\n-        different invocations of optimizer functions.\n+        When eager execution is enabled, `learning_rate`, `decay`, `momentum`,\n+        and `epsilon` can each be a callable that takes no arguments and returns\n+        the actual value to use. This can be useful for changing these values\n+        across different invocations of optimizer functions.\n         @end_compatibility\n         \"\"\"\n         super().__init__(name, **kwargs)\n\n@@ -473,7 +473,8 @@ class RMSpropOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n                     )  # pylint: disable=cell-var-from-loop\n                     return pred * pred\n \n-                # loss = lambda: pred * pred  # pylint: disable=cell-var-from-loop\n+                # loss = lambda: pred * pred  # pylint:\n+                # disable=cell-var-from-loop\n                 sgd_op = rmsprop.RMSprop(\n                     learning_rate=1.0,\n                     rho=0.0,\n@@ -674,7 +675,8 @@ class RMSpropOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n                 ),\n                 self.evaluate(var1),\n             )\n-            # Step 2: the root mean square accumulators contain the previous update.\n+            # Step 2: the root mean square accumulators contain the previous\n+            # update.\n             opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\n             # Check the parameters.\n             self.assertAllCloseAccordingToType(\n@@ -732,7 +734,8 @@ class RMSpropOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n \n         opt = rmsprop.RMSprop(learning_rate=1.0, momentum=0.2, centered=False)\n         opt.minimize(lambda: v1 + v2, var_list=[v1, v2])\n-        # There should be iteration, and two unique slot variables for v1 and v2.\n+        # There should be iteration, and two unique slot variables for v1 and\n+        # v2.\n         self.assertLen(set({id(v) for v in opt.variables()}), 5)\n         self.assertEqual(\n             self.evaluate(opt.variables()[0]), self.evaluate(opt.iterations)\n@@ -740,7 +743,8 @@ class RMSpropOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n \n         opt = rmsprop.RMSprop(learning_rate=1.0, momentum=0.2, centered=True)\n         opt.minimize(lambda: v1 + v2, var_list=[v1, v2])\n-        # There should be iteration, and three unique slot variables for v1 and v2\n+        # There should be iteration, and three unique slot variables for v1 and\n+        # v2\n         self.assertLen(set({id(v) for v in opt.variables()}), 7)\n         self.assertEqual(\n             self.evaluate(opt.variables()[0]), self.evaluate(opt.iterations)\n@@ -784,11 +788,11 @@ class SlotColocationTest(tf.test.TestCase, parameterized.TestCase):\n \n         # Run 1 step through optimizer on GPU.\n         # Slot variables are created the first time optimizer is used on some\n-        # variable. This tests that slot variables will be colocated with the base\n-        # variable.\n+        # variable. This tests that slot variables will be colocated with the\n+        # base variable.\n         with tf.device(\"/device:GPU:0\"):\n-            # Note that for eager execution, minimize expects a function instead of a\n-            # Tensor.\n+            # Note that for eager execution, minimize expects a function instead\n+            # of a Tensor.\n             opt_op = opt.minimize(loss, [var0, var1])\n             self.evaluate(tf.compat.v1.global_variables_initializer())\n             self.evaluate(opt_op)\n\n@@ -79,9 +79,9 @@ def filter_empty_gradients(grads_and_vars):\n     if vars_with_empty_grads:\n         logging.warning(\n             (\n-                \"Gradients do not exist for variables %s when minimizing the loss. \"\n-                \"If you're using `model.compile()`, did you forget to provide a `loss`\"\n-                \"argument?\"\n+                \"Gradients do not exist for variables %s when minimizing . \"\n+                \"the lossIf you're using `model.compile()`, did you forget \"\n+                \"to provide a `loss` argument?\"\n             ),\n             ([v.name for v in vars_with_empty_grads]),\n         )\n\n@@ -244,12 +244,12 @@ class PiecewiseConstantDecay(LearningRateSchedule):\n \n         Args:\n           boundaries: A list of `Tensor`s or `int`s or `float`s with strictly\n-            increasing entries, and with all elements having the same type as the\n-            optimizer step.\n+            increasing entries, and with all elements having the same type as\n+            the optimizer step.\n           values: A list of `Tensor`s or `float`s or `int`s that specifies the\n             values for the intervals defined by `boundaries`. It should have one\n-            more element than `boundaries`, and all elements should have the same\n-            type.\n+            more element than `boundaries`, and all elements should have the\n+            same type.\n           name: A string. Optional name of the operation. Defaults to\n             'PiecewiseConstant'.\n \n@@ -262,7 +262,8 @@ class PiecewiseConstantDecay(LearningRateSchedule):\n             raise ValueError(\n                 \"The length of boundaries should be 1 less than the length of \"\n                 f\"values. Received: boundaries={boundaries} of length \"\n-                f\"{len(boundaries)}, and values={values} of length {len(values)}.\"\n+                f\"{len(boundaries)}, and values={values} \"\n+                f\"of length {len(values)}.\"\n             )\n \n         self.boundaries = boundaries\n@@ -434,7 +435,8 @@ class PolynomialDecay(LearningRateSchedule):\n                 )\n                 decay_steps_recomp = tf.multiply(decay_steps_recomp, multiplier)\n             else:\n-                # Make sure that the global_step used is not bigger than decay_steps.\n+                # Make sure that the global_step used is not bigger than\n+                # decay_steps.\n                 global_step_recomp = tf.minimum(\n                     global_step_recomp, decay_steps_recomp\n                 )\n@@ -528,8 +530,8 @@ class InverseTimeDecay(LearningRateSchedule):\n             Python number.  The initial learning rate.\n           decay_steps: How often to apply decay.\n           decay_rate: A Python number.  The decay rate.\n-          staircase: Whether to apply decay in a discrete staircase, as opposed to\n-            continuous, fashion.\n+          staircase: Whether to apply decay in a discrete staircase, as opposed\n+            to continuous, fashion.\n           name: String.  Optional name of the operation.  Defaults to\n             'InverseTimeDecay'.\n         \"\"\"\n@@ -626,7 +628,8 @@ class CosineDecay(LearningRateSchedule):\n             Number of steps to decay over.\n           alpha: A scalar `float32` or `float64` Tensor or a Python number.\n             Minimum learning rate value as a fraction of initial_learning_rate.\n-          name: String. Optional name of the operation.  Defaults to 'CosineDecay'.\n+          name: String. Optional name of the operation.  Defaults to\n+            'CosineDecay'.\n         \"\"\"\n         super().__init__()\n \n@@ -720,8 +723,8 @@ class CosineDecayRestarts(LearningRateSchedule):\n         \"\"\"Applies cosine decay with restarts to the learning rate.\n \n         Args:\n-          initial_learning_rate: A scalar `float32` or `float64` Tensor or a Python\n-            number. The initial learning rate.\n+          initial_learning_rate: A scalar `float32` or `float64` Tensor or a\n+            Python number. The initial learning rate.\n           first_decay_steps: A scalar `int32` or `int64` `Tensor` or a Python\n             number. Number of steps to decay over.\n           t_mul: A scalar `float32` or `float64` `Tensor` or a Python number.\n@@ -729,7 +732,8 @@ class CosineDecayRestarts(LearningRateSchedule):\n           m_mul: A scalar `float32` or `float64` `Tensor` or a Python number.\n             Used to derive the initial learning rate of the i-th period.\n           alpha: A scalar `float32` or `float64` Tensor or a Python number.\n-            Minimum learning rate value as a fraction of the initial_learning_rate.\n+            Minimum learning rate value as a fraction of the\n+            initial_learning_rate.\n           name: String. Optional name of the operation. Defaults to 'SGDRDecay'.\n         \"\"\"\n         super().__init__()\n@@ -872,8 +876,8 @@ class LinearCosineDecay(LearningRateSchedule):\n         \"\"\"Applies linear cosine decay to the learning rate.\n \n         Args:\n-          initial_learning_rate: A scalar `float32` or `float64` Tensor or a Python\n-            number. The initial learning rate.\n+          initial_learning_rate: A scalar `float32` or `float64` Tensor or a\n+            Python number. The initial learning rate.\n           decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number.\n             Number of steps to decay over.\n           num_periods: Number of periods in the cosine part of the decay.\n@@ -1001,11 +1005,12 @@ class NoisyLinearCosineDecay(LearningRateSchedule):\n         \"\"\"Applies noisy linear cosine decay to the learning rate.\n \n         Args:\n-          initial_learning_rate: A scalar `float32` or `float64` Tensor or a Python\n-            number. The initial learning rate.\n+          initial_learning_rate: A scalar `float32` or `float64` Tensor or a\n+            Python number. The initial learning rate.\n           decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number.\n             Number of steps to decay over.\n-          initial_variance: initial variance for the noise. See computation above.\n+          initial_variance: initial variance for the noise. See computation\n+            above.\n           variance_decay: decay for the noise's variance. See computation above.\n           num_periods: Number of periods in the cosine part of the decay.\n             See computation above.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
