{"custom_id": "requests#27812eb90248c021d7277a8823dd9c7885f497af", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 9549 | Contributors (this commit): 165 | Commits (past 90d): 12 | Contributors (cumulative): 165 | DMM Complexity: None\n\nDIFF:\n@@ -21,6 +21,7 @@ from .structures import CaseInsensitiveDict\n \n from .auth import HTTPBasicAuth\n from .cookies import cookiejar_from_dict, get_cookie_header, _copy_cookie_jar\n+from .packages import idna\n from .packages.urllib3.fields import RequestField\n from .packages.urllib3.filepost import encode_multipart_formdata\n from .packages.urllib3.util import parse_url\n@@ -363,8 +364,8 @@ class PreparedRequest(RequestEncodingMixin, RequestHooksMixin):\n \n         # Only want to apply IDNA to the hostname\n         try:\n-            host = host.encode('idna').decode('utf-8')\n-        except UnicodeError:\n+            host = idna.encode(host).decode('utf-8')\n+        except (UnicodeError, idna.IDNAError):\n             raise InvalidURL('URL has an invalid label.')\n \n         # Carefully reconstruct the network location\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "requests#49b69b57db6fa20291c15c7d5ad9d8efff513740", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 4378 | Contributors (this commit): 63 | Commits (past 90d): 17 | Contributors (cumulative): 73 | DMM Complexity: 1.0\n\nDIFF:\n@@ -34,3 +34,9 @@ try:\n except ImportError:\n     import chardet\n     sys.modules['%s.chardet' % __name__] = chardet\n+\n+try:\n+    from . import idna\n+except ImportError:\n+    import idna\n+    sys.modules['%s.idna' % __name__] = idna\n\n@@ -37,6 +37,7 @@ packages = [\n     'requests',\n     'requests.packages',\n     'requests.packages.chardet',\n+    'requests.packages.idna',\n     'requests.packages.urllib3',\n     'requests.packages.urllib3.packages',\n     'requests.packages.urllib3.contrib',\n\n@@ -1884,6 +1884,7 @@ def test_urllib3_pool_connection_closed(httpbin):\n def test_vendor_aliases():\n     from requests.packages import urllib3\n     from requests.packages import chardet\n+    from requests.packages import idna\n \n     with pytest.raises(ImportError):\n         from requests.packages import webbrowser\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "requests#9337b4d95d23b66f04acdfec1b8a9cd41687d70f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 37 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 37 | Churn Cumulative: 3625 | Contributors (this commit): 46 | Commits (past 90d): 15 | Contributors (cumulative): 46 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1888,3 +1888,40 @@ def test_vendor_aliases():\n \n     with pytest.raises(ImportError):\n         from requests.packages import webbrowser\n+\n+\n+class TestPreparingURLs(object):\n+    @pytest.mark.parametrize(\n+        'url,expected',\n+        (\n+            ('http://google.com', 'http://google.com/'),\n+            (u'http://ジェーピーニック.jp', u'http://xn--hckqz9bzb1cyrb.jp/'),\n+            (\n+                u'http://ジェーピーニック.jp'.encode('utf-8'),\n+                u'http://xn--hckqz9bzb1cyrb.jp/'\n+            ),\n+            (u'http://straße.de/straße', u'http://xn--strae-oqa.de/stra%C3%9Fe'),\n+            (\n+                u'http://straße.de/straße'.encode('utf-8'),\n+                u'http://xn--strae-oqa.de/stra%C3%9Fe'\n+            ),\n+        )\n+    )\n+    def test_preparing_url(self, url, expected):\n+        r = requests.Request(url=url)\n+        p = r.prepare()\n+        assert p.url == expected\n+\n+    @pytest.mark.parametrize(\n+        'url',\n+        (\n+            b\"http://*.google.com\",\n+            b\"http://*\",\n+            u\"http://*.google.com\",\n+            u\"http://*\",\n+        )\n+    )\n+    def test_preparing_bad_url(self, url):\n+        r = requests.Request(url=url)\n+        with pytest.raises(requests.exceptions.InvalidURL):\n+            r.prepare()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "requests#f19d30a67092515349e358e011ada34d852401df", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 505 | Contributors (this commit): 21 | Commits (past 90d): 3 | Contributors (cumulative): 21 | DMM Complexity: None\n\nDIFF:\n@@ -42,6 +42,7 @@ packages = [\n     'requests.packages.urllib3.contrib',\n     'requests.packages.urllib3.util',\n     'requests.packages.urllib3.packages.ssl_match_hostname',\n+    'requests.packages.urllib3.packages.backports',\n ]\n \n requires = []\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "requests#8dd51d22fc42abf98f8e75ff5722f4b0295015e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 13189 | Contributors (this commit): 182 | Commits (past 90d): 29 | Contributors (cumulative): 211 | DMM Complexity: None\n\nDIFF:\n@@ -364,7 +364,7 @@ class PreparedRequest(RequestEncodingMixin, RequestHooksMixin):\n \n         # Only want to apply IDNA to the hostname\n         try:\n-            host = idna.encode(host).decode('utf-8')\n+            host = idna.encode(host, uts46=True).decode('utf-8')\n         except (UnicodeError, idna.IDNAError):\n             raise InvalidURL('URL has an invalid label.')\n \n\n@@ -1900,11 +1900,22 @@ class TestPreparingURLs(object):\n                 u'http://ジェーピーニック.jp'.encode('utf-8'),\n                 u'http://xn--hckqz9bzb1cyrb.jp/'\n             ),\n-            (u'http://straße.de/straße', u'http://xn--strae-oqa.de/stra%C3%9Fe'),\n+            (\n+                u'http://straße.de/straße',\n+                u'http://xn--strae-oqa.de/stra%C3%9Fe'\n+            ),\n             (\n                 u'http://straße.de/straße'.encode('utf-8'),\n                 u'http://xn--strae-oqa.de/stra%C3%9Fe'\n             ),\n+            (\n+                u'http://Königsgäßchen.de/straße',\n+                u'http://xn--knigsgchen-b4a3dun.de/stra%C3%9Fe'\n+            ),\n+            (\n+                u'http://Königsgäßchen.de/straße'.encode('utf-8'),\n+                u'http://xn--knigsgchen-b4a3dun.de/stra%C3%9Fe'\n+            ),\n         )\n     )\n     def test_preparing_url(self, url, expected):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "requests#f5b5c293b910ee997748e3cebf092d589309f684", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 14 | Churn Cumulative: 3760 | Contributors (this commit): 70 | Commits (past 90d): 18 | Contributors (cumulative): 81 | DMM Complexity: 0.9\n\nDIFF:\n@@ -86,7 +86,7 @@ def super_len(o):\n             # let requests chunk it instead.\n             if total_length is not None:\n                 current_position = total_length\n-\n+        else:\n             if hasattr(o, 'seek') and total_length is None:\n                 # StringIO and BytesIO have seek but no useable fileno\n \n\n@@ -51,6 +51,18 @@ class TestSuperLen:\n \n         assert super_len(BoomFile()) == 0\n \n+    @pytest.mark.parametrize('error', [IOError, OSError])\n+    def test_super_len_tell_ioerror(self, error):\n+        \"\"\"Ensure that if tell gives an IOError super_len doesn't fail\"\"\"\n+        class NoLenBoomFile(object):\n+            def tell(self):\n+                raise error()\n+\n+            def seek(self, offset, whence):\n+                pass\n+\n+        assert super_len(NoLenBoomFile()) == 0\n+\n     def test_string(self):\n         assert super_len('Test') == 4\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "requests#e7f5bd634d3fa757f067b297a3d20f1760777a23", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 136 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,4 @@\n-from idna.core import encode, decode, alabel, ulabel, IDNAError\n+from .core import encode, decode, alabel, ulabel, IDNAError\n import codecs\n import re\n \n\n@@ -1,5 +1,5 @@\n-from idna.core import *\n-from idna.codec import *\n+from .core import *\n+from .codec import *\n \n def ToASCII(label):\n     return encode(label)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "requests#d58753344638992a63f4fe8e516c9f55a9a7f027", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 177 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,17 +8,17 @@ import select\n def consume_socket_content(sock, timeout=0.5):\n     chunks = 65536\n     content = b''\n+\n+    while True:\n         more_to_read = select.select([sock], [], [], timeout)[0]\n+        if not more_to_read:\n+            break\n \n-    while more_to_read:\n         new_content = sock.recv(chunks)\n-\n         if not new_content:\n             break\n \n         content += new_content\n-        # stop reading if no new data is received for a while\n-        more_to_read = select.select([sock], [], [], timeout)[0]\n \n     return content\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "requests#548a03acef3238389b1cce95131a0511d5c82211", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 8 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 27 | Churn Cumulative: 13216 | Contributors (this commit): 183 | Commits (past 90d): 26 | Contributors (cumulative): 213 | DMM Complexity: 1.0\n\nDIFF:\n@@ -436,7 +436,7 @@ class PreparedRequest(RequestEncodingMixin, RequestHooksMixin):\n \n         is_stream = all([\n             hasattr(data, '__iter__'),\n-            not isinstance(data, (basestring, list, tuple, dict))\n+            not isinstance(data, (basestring, list, tuple, collections.Mapping))\n         ])\n \n         try:\n\n@@ -656,6 +656,31 @@ class TestRequests:\n         with pytest.raises(ValueError):\n             requests.post(url, files=['bad file data'])\n \n+    def test_post_with_custom_mapping(self, httpbin):\n+        class CustomMapping(collections.MutableMapping):\n+            def __init__(self, *args, **kwargs):\n+                self.data = dict(*args, **kwargs)\n+\n+            def __delitem__(self, key):\n+                del self.data[key]\n+\n+            def __getitem__(self, key):\n+                return self.data[key]\n+\n+            def __setitem__(self, key, value):\n+                self.data[key] = value\n+\n+            def __iter__(self):\n+                return iter(self.data)\n+\n+            def __len__(self):\n+                return len(self.data)\n+\n+        data = CustomMapping({'some': 'data'})\n+        url = httpbin('post')\n+        found_json = requests.post(url, data=data).json().get('form')\n+        assert found_json == {'some': 'data'}\n+\n     def test_conflicting_post_params(self, httpbin):\n         url = httpbin('post')\n         with open('requirements.txt') as f:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "requests#dfd85f2099117adf8058d3f71e196cbfc5254760", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 2649 | Contributors (this commit): 68 | Commits (past 90d): 9 | Contributors (cumulative): 68 | DMM Complexity: 0.0\n\nDIFF:\n@@ -331,9 +331,7 @@ def add_dict_to_cookiejar(cj, cookie_dict):\n     :rtype: CookieJar\n     \"\"\"\n \n-    cj2 = cookiejar_from_dict(cookie_dict)\n-    cj.update(cj2)\n-    return cj\n+    return cookiejar_from_dict(cookie_dict, cj)\n \n \n def get_encodings_from_content(content):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "requests#390a499e81cebe835c1e35b9c917840db85a4ce4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 18 | Churn Cumulative: 1133 | Contributors (this commit): 13 | Commits (past 90d): 5 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,6 +4,7 @@ from io import BytesIO\n \n import pytest\n from requests import compat\n+from requests.cookies import RequestsCookieJar, cookiejar_from_dict\n from requests.structures import CaseInsensitiveDict\n from requests.utils import (\n     address_in_network, dotted_netmask,\n@@ -15,7 +16,7 @@ from requests.utils import (\n     requote_uri, select_proxy, should_bypass_proxies, super_len,\n     to_key_val_list, to_native_string,\n     unquote_header_value, unquote_unreserved,\n-    urldefragauth)\n+    urldefragauth, add_dict_to_cookiejar)\n \n from .compat import StringIO, cStringIO\n \n@@ -501,3 +502,18 @@ def test_should_bypass_proxies(url, expected, monkeypatch):\n     monkeypatch.setenv('no_proxy', '192.168.0.0/24,127.0.0.1,localhost.localdomain,172.16.1.1')\n     monkeypatch.setenv('NO_PROXY', '192.168.0.0/24,127.0.0.1,localhost.localdomain,172.16.1.1')\n     assert should_bypass_proxies(url) == expected\n+\n+@pytest.mark.parametrize(\n+    'cookiejar', (\n+        compat.cookielib.CookieJar(),\n+        RequestsCookieJar()\n+    ))\n+def test_add_dict_to_cookiejar(cookiejar):\n+    \"\"\"Ensure add_dict_to_cookiejar works for\n+    non-RequestsCookieJar CookieJars\n+    \"\"\"\n+    cookiedict = {'test': 'cookies',\n+                  'good': 'cookies'}\n+    cj = add_dict_to_cookiejar(cookiejar, cookiedict)\n+    cookies = dict((cookie.name, cookie.value) for cookie in cj)\n+    assert cookiedict == cookies\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "requests#c97a530638bba86576f3e40b49cb0e061b790bdd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/2 | Churn Δ: 12 | Churn Cumulative: 4754 | Contributors (this commit): 79 | Commits (past 90d): 18 | Contributors (cumulative): 86 | DMM Complexity: 1.0\n\nDIFF:\n@@ -366,7 +366,7 @@ class HTTPAdapter(BaseAdapter):\n         headers = {}\n         username, password = get_auth_from_url(proxy)\n \n-        if username and password:\n+        if username:\n             headers['Proxy-Authorization'] = _basic_auth_str(username,\n                                                              password)\n \n\n@@ -1472,6 +1472,16 @@ class TestRequests:\n         proxies['one'].clear.assert_called_once_with()\n         proxies['two'].clear.assert_called_once_with()\n \n+    def test_proxy_auth(self, httpbin):\n+        adapter = HTTPAdapter()\n+        headers = adapter.proxy_headers(\"http://user:pass@httpbin.org\")\n+        assert headers == {'Proxy-Authorization': 'Basic dXNlcjpwYXNz'}\n+\n+    def test_proxy_auth_empty_pass(self, httpbin):\n+        adapter = HTTPAdapter()\n+        headers = adapter.proxy_headers(\"http://user:@httpbin.org\")\n+        assert headers == {'Proxy-Authorization': 'Basic dXNlcjo='}\n+\n     def test_response_json_when_content_is_None(self, httpbin):\n         r = requests.get(httpbin('/status/204'))\n         # Make sure r.content is None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "requests#40402cd0dd044b640b984d25233e1095fae9ec16", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 51 | Lines Deleted: 5 | Files Changed: 6 | Hunks: 12 | Methods Changed: 5 | Complexity Δ (Sum/Max): 7/4 | Churn Δ: 56 | Churn Cumulative: 19588 | Contributors (this commit): 262 | Commits (past 90d): 48 | Contributors (cumulative): 395 | DMM Complexity: 0.5217391304347826\n\nDIFF:\n@@ -50,6 +50,7 @@ if is_py2:\n     str = unicode\n     basestring = basestring\n     numeric_types = (int, long, float)\n+    integer_types = (int, long)\n \n elif is_py3:\n     from urllib.parse import urlparse, urlunparse, urljoin, urlsplit, urlencode, quote, unquote, quote_plus, unquote_plus, urldefrag\n@@ -64,3 +65,4 @@ elif is_py3:\n     bytes = bytes\n     basestring = (str, bytes)\n     numeric_types = (int, float)\n+    integer_types = (int,)\n\n@@ -100,6 +100,8 @@ class StreamConsumedError(RequestException, TypeError):\n class RetryError(RequestException):\n     \"\"\"Custom retries logic failed\"\"\"\n \n+class UnrewindableBodyError(RequestException):\n+    \"\"\"Requests encountered an error when trying to rewind a body\"\"\"\n \n # Warnings\n \n\n@@ -291,6 +291,8 @@ class PreparedRequest(RequestEncodingMixin, RequestHooksMixin):\n         self.body = None\n         #: dictionary of callback hooks, for internal usage.\n         self.hooks = default_hooks()\n+        #: integer denoting starting position of a readable file-like body.\n+        self._body_position = None\n \n     def prepare(self, method=None, url=None, headers=None, files=None,\n         data=None, params=None, auth=None, cookies=None, hooks=None, json=None):\n@@ -320,6 +322,7 @@ class PreparedRequest(RequestEncodingMixin, RequestHooksMixin):\n         p._cookies = _copy_cookie_jar(self._cookies)\n         p.body = self.body\n         p.hooks = self.hooks\n+        p._body_position = self._body_position\n         return p\n \n     def prepare_method(self, method):\n@@ -447,6 +450,17 @@ class PreparedRequest(RequestEncodingMixin, RequestHooksMixin):\n         if is_stream:\n             body = data\n \n+            if getattr(body, 'tell', None) is not None:\n+                # Record the current file position before reading.\n+                # This will allow us to rewind a file in the event\n+                # of a redirect.\n+                try:\n+                    self._body_position = body.tell()\n+                except (IOError, OSError):\n+                    # This differentiates from None, allowing us to catch\n+                    # a failed `tell()` later when trying to rewind the body\n+                    self._body_position = object()\n+\n             if files:\n                 raise NotImplementedError('Streamed bodies and files are mutually exclusive.')\n \n\n@@ -28,7 +28,7 @@ from .adapters import HTTPAdapter\n \n from .utils import (\n     requote_uri, get_environ_proxies, get_netrc_auth, should_bypass_proxies,\n-    get_auth_from_url\n+    get_auth_from_url, rewind_body\n )\n \n from .status_codes import codes\n@@ -164,6 +164,18 @@ class SessionRedirectMixin(object):\n             proxies = self.rebuild_proxies(prepared_request, proxies)\n             self.rebuild_auth(prepared_request, resp)\n \n+            # A failed tell() sets `_body_position` to `object()`. This non-None\n+            # value ensures `rewindable` will be True, allowing us to raise an\n+            # UnrewindableBodyError, instead of hanging the connection.\n+            rewindable = (\n+                prepared_request._body_position is not None and\n+                ('Content-Length' in headers or 'Transfer-Encoding' in headers)\n+            )\n+\n+            # Attempt to rewind consumed file-like object.\n+            if rewindable:\n+                rewind_body(prepared_request)\n+\n             # Override the original request.\n             req = prepared_request\n \n\n@@ -23,11 +23,13 @@ from . import certs\n # to_native_string is unused here, but imported here for backwards compatibility\n from ._internal_utils import to_native_string\n from .compat import parse_http_list as _parse_list_header\n-from .compat import (quote, urlparse, bytes, str, OrderedDict, unquote,\n-                     getproxies, proxy_bypass, urlunparse, basestring)\n+from .compat import (\n+    quote, urlparse, bytes, str, OrderedDict, unquote, getproxies,\n+    proxy_bypass, urlunparse, basestring, integer_types)\n from .cookies import RequestsCookieJar, cookiejar_from_dict\n from .structures import CaseInsensitiveDict\n-from .exceptions import InvalidURL, InvalidHeader, FileModeWarning\n+from .exceptions import (\n+    InvalidURL, InvalidHeader, FileModeWarning, UnrewindableBodyError)\n \n _hush_pyflakes = (RequestsCookieJar,)\n \n@@ -811,3 +813,17 @@ def urldefragauth(url):\n     netloc = netloc.rsplit('@', 1)[-1]\n \n     return urlunparse((scheme, netloc, path, params, query, ''))\n+\n+def rewind_body(prepared_request):\n+    \"\"\"Move file pointer back to its recorded starting position\n+    so it can be read again on redirect.\n+    \"\"\"\n+    body_seek = getattr(prepared_request.body, 'seek', None)\n+    if body_seek is not None and isinstance(prepared_request._body_position, integer_types):\n+        try:\n+            body_seek(prepared_request._body_position)\n+        except (IOError, OSError):\n+            raise UnrewindableBodyError(\"An error occured when rewinding request \"\n+                                        \"body for redirect.\")\n+    else:\n+        raise UnrewindableBodyError(\"Unable to rewind request body for redirect.\")\n\n@@ -24,7 +24,7 @@ from requests.cookies import (\n from requests.exceptions import (\n     ConnectionError, ConnectTimeout, InvalidSchema, InvalidURL,\n     MissingSchema, ReadTimeout, Timeout, RetryError, TooManyRedirects,\n-    ProxyError, InvalidHeader)\n+    ProxyError, InvalidHeader, UnrewindableBodyError)\n from requests.models import PreparedRequest\n from requests.structures import CaseInsensitiveDict\n from requests.sessions import SessionRedirectMixin\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "requests#9849c27b70b919bc5636745fa2ccddbaf4163ed6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 101 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 16 | Complexity Δ (Sum/Max): 16/16 | Churn Δ: 101 | Churn Cumulative: 3776 | Contributors (this commit): 48 | Commits (past 90d): 18 | Contributors (cumulative): 48 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1386,6 +1386,107 @@ class TestRequests:\n         r3 = next(rg)\n         assert not r3.is_redirect\n \n+    def test_prepare_body_position_non_stream(self):\n+        data = b'the data'\n+        s = requests.Session()\n+        prep = requests.Request('GET', 'http://example.com', data=data).prepare()\n+        assert prep._body_position is None\n+\n+    def test_rewind_body(self):\n+        data = io.BytesIO(b'the data')\n+        s = requests.Session()\n+        prep = requests.Request('GET', 'http://example.com', data=data).prepare()\n+        assert prep._body_position == 0\n+        assert prep.body.read() == b'the data'\n+\n+        # the data has all been read\n+        assert prep.body.read() == b''\n+\n+        # rewind it back\n+        requests.utils.rewind_body(prep)\n+        assert prep.body.read() == b'the data'\n+\n+    def test_rewind_partially_read_body(self):\n+        data = io.BytesIO(b'the data')\n+        s = requests.Session()\n+        data.read(4)  # read some data\n+        prep = requests.Request('GET', 'http://example.com', data=data).prepare()\n+        assert prep._body_position == 4\n+        assert prep.body.read() == b'data'\n+\n+        # the data has all been read\n+        assert prep.body.read() == b''\n+\n+        # rewind it back\n+        requests.utils.rewind_body(prep)\n+        assert prep.body.read() == b'data'\n+\n+    def test_rewind_body_no_seek(self):\n+        class BadFileObj:\n+            def __init__(self, data):\n+                self.data = data\n+\n+            def tell(self):\n+                return 0\n+\n+            def __iter__(self):\n+                return\n+\n+        data = BadFileObj('the data')\n+        s = requests.Session()\n+        prep = requests.Request('GET', 'http://example.com', data=data).prepare()\n+        assert prep._body_position == 0\n+\n+        with pytest.raises(UnrewindableBodyError) as e:\n+            requests.utils.rewind_body(prep)\n+\n+        assert 'Unable to rewind request body' in str(e)\n+\n+    def test_rewind_body_failed_seek(self):\n+        class BadFileObj:\n+            def __init__(self, data):\n+                self.data = data\n+\n+            def tell(self):\n+                return 0\n+\n+            def seek(self, pos):\n+                raise OSError()\n+\n+            def __iter__(self):\n+                return\n+\n+        data = BadFileObj('the data')\n+        s = requests.Session()\n+        prep = requests.Request('GET', 'http://example.com', data=data).prepare()\n+        assert prep._body_position == 0\n+\n+        with pytest.raises(UnrewindableBodyError) as e:\n+            requests.utils.rewind_body(prep)\n+\n+        assert 'error occured when rewinding request body' in str(e)\n+\n+    def test_rewind_body_failed_tell(self):\n+        class BadFileObj:\n+            def __init__(self, data):\n+                self.data = data\n+\n+            def tell(self):\n+                raise OSError()\n+\n+            def __iter__(self):\n+                return\n+\n+        data = BadFileObj('the data')\n+        s = requests.Session()\n+        prep = requests.Request('GET', 'http://example.com', data=data).prepare()\n+        assert prep._body_position is not None\n+\n+        with pytest.raises(UnrewindableBodyError) as e:\n+            requests.utils.rewind_body(prep)\n+\n+        assert 'Unable to rewind request body' in str(e)\n+\n     def _patch_adapter_gzipped_redirect(self, session, url):\n         adapter = session.get_adapter(url=url)\n         org_build_response = adapter.build_response\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "requests#38dd089c5f7a35bf6eed72f4e63820aed9eb7ca3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 16 | Churn Cumulative: 3792 | Contributors (this commit): 48 | Commits (past 90d): 30 | Contributors (cumulative): 48 | DMM Complexity: 1.0\n\nDIFF:\n@@ -165,6 +165,22 @@ class TestRequests:\n         assert r.history[0].status_code == 302\n         assert r.history[0].is_redirect\n \n+    def test_HTTP_307_ALLOW_REDIRECT_POST(self, httpbin):\n+        parts = urlparse(httpbin('post'))\n+        url = \"HTTP://\" + parts.netloc + parts.path\n+        r = requests.post(httpbin('redirect-to'), data='test', params={'url': url, 'status_code': 307})\n+        assert r.status_code == 200\n+        assert r.history[0].status_code == 307\n+        assert r.history[0].is_redirect\n+\n+    def test_HTTP_307_ALLOW_REDIRECT_POST_WITH_SEEKABLE(self, httpbin):\n+        parts = urlparse(httpbin('post'))\n+        url = \"HTTP://\" + parts.netloc + parts.path\n+        r = requests.post(httpbin('redirect-to'), data=io.BytesIO(b'test'), params={'url': url, 'status_code': 307})\n+        assert r.status_code == 200\n+        assert r.history[0].status_code == 307\n+        assert r.history[0].is_redirect\n+\n     def test_HTTP_302_TOO_MANY_REDIRECTS(self, httpbin):\n         try:\n             requests.get(httpbin('relative-redirect', '50'))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "requests#522d37c9597882db3fbd5fb08804b27b5fd8be4d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 3803 | Contributors (this commit): 48 | Commits (past 90d): 20 | Contributors (cumulative): 48 | DMM Complexity: 0.0\n\nDIFF:\n@@ -166,20 +166,19 @@ class TestRequests:\n         assert r.history[0].is_redirect\n \n     def test_HTTP_307_ALLOW_REDIRECT_POST(self, httpbin):\n-        parts = urlparse(httpbin('post'))\n-        url = \"HTTP://\" + parts.netloc + parts.path\n-        r = requests.post(httpbin('redirect-to'), data='test', params={'url': url, 'status_code': 307})\n+        r = requests.post(httpbin('redirect-to'), data='test', params={'url': 'post', 'status_code': 307})\n         assert r.status_code == 200\n         assert r.history[0].status_code == 307\n         assert r.history[0].is_redirect\n+        assert r.json()['data'] == 'test'\n \n     def test_HTTP_307_ALLOW_REDIRECT_POST_WITH_SEEKABLE(self, httpbin):\n-        parts = urlparse(httpbin('post'))\n-        url = \"HTTP://\" + parts.netloc + parts.path\n-        r = requests.post(httpbin('redirect-to'), data=io.BytesIO(b'test'), params={'url': url, 'status_code': 307})\n+        byte_str = b'test'\n+        r = requests.post(httpbin('redirect-to'), data=io.BytesIO(byte_str), params={'url': 'post', 'status_code': 307})\n         assert r.status_code == 200\n         assert r.history[0].status_code == 307\n         assert r.history[0].is_redirect\n+        assert r.json()['data'] == byte_str.decode('utf-8')\n \n     def test_HTTP_302_TOO_MANY_REDIRECTS(self, httpbin):\n         try:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "requests#623d9e7878895eea4e2f7e263b7fc3f01459a37a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1175 | Contributors (this commit): 39 | Commits (past 90d): 1 | Contributors (cumulative): 39 | DMM Complexity: None\n\nDIFF:\n@@ -115,6 +115,7 @@ def put(url, data=None, **kwargs):\n \n     :param url: URL for the new :class:`Request` object.\n     :param data: (optional) Dictionary, bytes, or file-like object to send in the body of the :class:`Request`.\n+    :param json: (optional) json data to send in the body of the :class:`Request`.\n     :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n     :return: :class:`Response <Response>` object\n     :rtype: requests.Response\n@@ -128,6 +129,7 @@ def patch(url, data=None, **kwargs):\n \n     :param url: URL for the new :class:`Request` object.\n     :param data: (optional) Dictionary, bytes, or file-like object to send in the body of the :class:`Request`.\n+    :param json: (optional) json data to send in the body of the :class:`Request`.\n     :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n     :return: :class:`Response <Response>` object\n     :rtype: requests.Response\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "requests#340902153f49a2ed9316cbe1f44c67032dcc8376", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 43 | Files Changed: 1 | Hunks: 12 | Methods Changed: 7 | Complexity Δ (Sum/Max): -30/0 | Churn Δ: 74 | Churn Cumulative: 3877 | Contributors (this commit): 48 | Commits (past 90d): 32 | Contributors (cumulative): 48 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1053,63 +1053,51 @@ class TestRequests:\n         assert r.request.url == pr.request.url\n         assert r.request.headers == pr.request.headers\n \n-    def test_prepared_request_is_pickleable(self):\n+    def test_prepared_request_is_pickleable(self, httpbin):\n         p = requests.Request('GET', httpbin('get')).prepare()\n \n-        # Verify we can pickle the request.\n-        assert pickle.dumps(p)\n-        r = pickle.dumps(p)\n+        # Verify PreparedRequest can be pickled and unpickled\n+        r = pickle.loads(pickle.dumps(p))\n+        assert r.url == p.url\n+        assert r.headers == p.headers\n+        assert r.body == p.body\n \n-        # Verify we can use the pickled request.\n-        assert pickle.loads(r)\n-        r = pickle.loads(r)\n-\n-        # Verify we can use the unpickled request.\n+        # Verify unpickled PreparedRequest sends properly\n         s = requests.Session()\n-        r = s.send(r)\n+        resp = s.send(r)\n+        assert resp.status_code == 200\n \n-        assert r.status_code == 200\n-\n-    def test_prepared_request_with_file_is_pickleable(self):\n-        data = {'a': 0.0}\n-        files = {'b': 'foo'}\n-        r = requests.Request('POST', httpbin('post'), data=data, files=files)\n+    def test_prepared_request_with_file_is_pickleable(self, httpbin):\n+        files = {'file': open(__file__, 'rb')}\n+        r = requests.Request('POST', httpbin('post'), files=files)\n         p = r.prepare()\n \n-        # Verify we can pickle the request.\n-        assert pickle.dumps(p)\n-        r = pickle.dumps(p)\n+        # Verify PreparedRequest can be pickled and unpickled\n+        r = pickle.loads(pickle.dumps(p))\n+        assert r.url == p.url\n+        assert r.headers == p.headers\n+        assert r.body == p.body\n \n-        # Verify we can use the pickled request.\n-        assert pickle.loads(r)\n-        r = pickle.loads(r)\n-\n-        # Verify we can use the unpickled request.\n+        # Verify unpickled PreparedRequest sends properly\n         s = requests.Session()\n-        r = s.send(r)\n+        resp = s.send(r)\n+        assert resp.status_code == 200\n \n-        assert r.status_code == 200\n-\n-    def test_prepared_request_with_hook_is_pickleable(self):\n-        def print_url(r, *args, **kwargs):\n-            print(r.url)\n-\n-        r = requests.Request('POST', httpbin('post'), hooks=dict(response=print_url))\n+    def test_prepared_request_with_hook_is_pickleable(self, httpbin):\n+        r = requests.Request('GET', httpbin('get'), hooks=default_hooks())\n         p = r.prepare()\n \n-        # Verify we can pickle the request.\n-        assert pickle.dumps(p)\n-        r = pickle.dumps(p)\n+        # Verify PreparedRequest can be pickled\n+        r = pickle.loads(pickle.dumps(p))\n+        assert r.url == p.url\n+        assert r.headers == p.headers\n+        assert r.body == p.body\n+        assert r.hooks == p.hooks\n \n-        # Verify we can use the pickled request.\n-        assert pickle.loads(r)\n-        r = pickle.loads(r)\n-\n-        # Verify we can use the unpickled request.\n+        # Verify unpickled PreparedRequest sends properly\n         s = requests.Session()\n-        r = s.send(r)\n-\n-        assert r.status_code == 200\n+        resp = s.send(r)\n+        assert resp.status_code == 200\n \n     def test_cannot_send_unprepared_requests(self, httpbin):\n         r = requests.Request(url=httpbin())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "requests#a49c387b7af58e36f7a1266618595b3078be41fd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 9568 | Contributors (this commit): 167 | Commits (past 90d): 11 | Contributors (cumulative): 167 | DMM Complexity: None\n\nDIFF:\n@@ -818,6 +818,7 @@ class Response(object):\n         \"\"\"Returns the json-encoded content of a response, if any.\n \n         :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n+        :raises ValueError: If the response body does not contain valid json.\n         \"\"\"\n \n         if not self.encoding and self.content and len(self.content) > 3:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "requests#e514920e190bbda3ddd35242bf566e7d752725a1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 8 | Churn Cumulative: 1499 | Contributors (this commit): 44 | Commits (past 90d): 2 | Contributors (cumulative): 44 | DMM Complexity: 1.0\n\nDIFF:\n@@ -15,7 +15,7 @@ import threading\n \n from base64 import b64encode\n \n-from .compat import urlparse, str\n+from .compat import urlparse, str, bytes\n from .cookies import extract_cookies_to_jar\n from ._internal_utils import to_native_string\n from .utils import parse_dict_header\n@@ -28,6 +28,12 @@ CONTENT_TYPE_MULTI_PART = 'multipart/form-data'\n def _basic_auth_str(username, password):\n     \"\"\"Returns a Basic Auth string.\"\"\"\n     \n+    if isinstance(username, bytes):\n+        username = username.decode('latin1')\n+\n+    if isinstance(password, bytes):\n+        password = password.decode('latin1')\n+\n     authstr = 'Basic ' + to_native_string(\n         b64encode(('%s:%s' % (username, password)).encode('latin1')).strip()\n     )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "requests#99fa7becf263473c7bfc1998b41c2c6c80a0f499", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 688 | Lines Deleted: 223 | Files Changed: 20 | Hunks: 169 | Methods Changed: 69 | Complexity Δ (Sum/Max): 73/18 | Churn Δ: 911 | Churn Cumulative: 12092 | Contributors (this commit): 23 | Commits (past 90d): 20 | Contributors (cumulative): 133 | DMM Complexity: 0.6055045871559633\n\nDIFF:\n@@ -32,7 +32,7 @@ except ImportError:\n \n __author__ = 'Andrey Petrov (andrey.petrov@shazow.net)'\n __license__ = 'MIT'\n-__version__ = '1.16'\n+__version__ = '1.19'\n \n __all__ = (\n     'HTTPConnectionPool',\n\n@@ -7,13 +7,8 @@ import socket\n from socket import error as SocketError, timeout as SocketTimeout\n import warnings\n from .packages import six\n-\n-try:  # Python 3\n-    from http.client import HTTPConnection as _HTTPConnection\n-    from http.client import HTTPException  # noqa: unused in this module\n-except ImportError:\n-    from httplib import HTTPConnection as _HTTPConnection\n-    from httplib import HTTPException  # noqa: unused in this module\n+from .packages.six.moves.http_client import HTTPConnection as _HTTPConnection\n+from .packages.six.moves.http_client import HTTPException  # noqa: F401\n \n try:  # Compiled with SSL?\n     import ssl\n@@ -44,8 +39,9 @@ from .packages.ssl_match_hostname import match_hostname, CertificateError\n from .util.ssl_ import (\n     resolve_cert_reqs,\n     resolve_ssl_version,\n-    ssl_wrap_socket,\n     assert_fingerprint,\n+    create_urllib3_context,\n+    ssl_wrap_socket\n )\n \n \n@@ -174,7 +170,13 @@ class HTTPConnection(_HTTPConnection, object):\n         \"\"\"\n         headers = HTTPHeaderDict(headers if headers is not None else {})\n         skip_accept_encoding = 'accept-encoding' in headers\n-        self.putrequest(method, url, skip_accept_encoding=skip_accept_encoding)\n+        skip_host = 'host' in headers\n+        self.putrequest(\n+            method,\n+            url,\n+            skip_accept_encoding=skip_accept_encoding,\n+            skip_host=skip_host\n+        )\n         for header, value in headers.items():\n             self.putheader(header, value)\n         if 'transfer-encoding' not in headers:\n@@ -203,14 +205,18 @@ class HTTPConnection(_HTTPConnection, object):\n class HTTPSConnection(HTTPConnection):\n     default_port = port_by_scheme['https']\n \n+    ssl_version = None\n+\n     def __init__(self, host, port=None, key_file=None, cert_file=None,\n-                 strict=None, timeout=socket._GLOBAL_DEFAULT_TIMEOUT, **kw):\n+                 strict=None, timeout=socket._GLOBAL_DEFAULT_TIMEOUT,\n+                 ssl_context=None, **kw):\n \n         HTTPConnection.__init__(self, host, port, strict=strict,\n                                 timeout=timeout, **kw)\n \n         self.key_file = key_file\n         self.cert_file = cert_file\n+        self.ssl_context = ssl_context\n \n         # Required property for Google AppEngine 1.9.0 which otherwise causes\n         # HTTPS requests to go out as HTTP. (See Issue #356)\n@@ -219,7 +225,19 @@ class HTTPSConnection(HTTPConnection):\n     def connect(self):\n         conn = self._new_conn()\n         self._prepare_conn(conn)\n-        self.sock = ssl.wrap_socket(conn, self.key_file, self.cert_file)\n+\n+        if self.ssl_context is None:\n+            self.ssl_context = create_urllib3_context(\n+                ssl_version=resolve_ssl_version(None),\n+                cert_reqs=resolve_cert_reqs(None),\n+            )\n+\n+        self.sock = ssl_wrap_socket(\n+            sock=conn,\n+            keyfile=self.key_file,\n+            certfile=self.cert_file,\n+            ssl_context=self.ssl_context,\n+        )\n \n \n class VerifiedHTTPSConnection(HTTPSConnection):\n@@ -237,9 +255,18 @@ class VerifiedHTTPSConnection(HTTPSConnection):\n                  cert_reqs=None, ca_certs=None,\n                  assert_hostname=None, assert_fingerprint=None,\n                  ca_cert_dir=None):\n-\n-        if (ca_certs or ca_cert_dir) and cert_reqs is None:\n+        \"\"\"\n+        This method should only be called once, before the connection is used.\n+        \"\"\"\n+        # If cert_reqs is not provided, we can try to guess. If the user gave\n+        # us a cert database, we assume they want to use it: otherwise, if\n+        # they gave us an SSL Context object we should use whatever is set for\n+        # it.\n+        if cert_reqs is None:\n+            if ca_certs or ca_cert_dir:\n                 cert_reqs = 'CERT_REQUIRED'\n+            elif self.ssl_context is not None:\n+                cert_reqs = self.ssl_context.verify_mode\n \n         self.key_file = key_file\n         self.cert_file = cert_file\n@@ -253,9 +280,6 @@ class VerifiedHTTPSConnection(HTTPSConnection):\n         # Add certificate verification\n         conn = self._new_conn()\n \n-        resolved_cert_reqs = resolve_cert_reqs(self.cert_reqs)\n-        resolved_ssl_version = resolve_ssl_version(self.ssl_version)\n-\n         hostname = self.host\n         if getattr(self, '_tunnel_host', None):\n             # _tunnel_host was added in Python 2.6.3\n@@ -281,17 +305,27 @@ class VerifiedHTTPSConnection(HTTPSConnection):\n \n         # Wrap socket using verification with the root certs in\n         # trusted_root_certs\n-        self.sock = ssl_wrap_socket(conn, self.key_file, self.cert_file,\n-                                    cert_reqs=resolved_cert_reqs,\n+        if self.ssl_context is None:\n+            self.ssl_context = create_urllib3_context(\n+                ssl_version=resolve_ssl_version(self.ssl_version),\n+                cert_reqs=resolve_cert_reqs(self.cert_reqs),\n+            )\n+\n+        context = self.ssl_context\n+        context.verify_mode = resolve_cert_reqs(self.cert_reqs)\n+        self.sock = ssl_wrap_socket(\n+            sock=conn,\n+            keyfile=self.key_file,\n+            certfile=self.cert_file,\n             ca_certs=self.ca_certs,\n             ca_cert_dir=self.ca_cert_dir,\n             server_hostname=hostname,\n-                                    ssl_version=resolved_ssl_version)\n+            ssl_context=context)\n \n         if self.assert_fingerprint:\n             assert_fingerprint(self.sock.getpeercert(binary_form=True),\n                                self.assert_fingerprint)\n-        elif resolved_cert_reqs != ssl.CERT_NONE \\\n+        elif context.verify_mode != ssl.CERT_NONE \\\n                 and self.assert_hostname is not False:\n             cert = self.sock.getpeercert()\n             if not cert.get('subjectAltName', ()):\n@@ -304,8 +338,10 @@ class VerifiedHTTPSConnection(HTTPSConnection):\n                 )\n             _match_hostname(cert, self.assert_hostname or hostname)\n \n-        self.is_verified = (resolved_cert_reqs == ssl.CERT_REQUIRED or\n-                            self.assert_fingerprint is not None)\n+        self.is_verified = (\n+            context.verify_mode == ssl.CERT_REQUIRED or\n+            self.assert_fingerprint is not None\n+        )\n \n \n def _match_hostname(cert, asserted_hostname):\n\n@@ -7,13 +7,6 @@ import warnings\n from socket import error as SocketError, timeout as SocketTimeout\n import socket\n \n-try:  # Python 3\n-    from queue import LifoQueue, Empty, Full\n-except ImportError:\n-    from Queue import LifoQueue, Empty, Full\n-    # Queue is imported for side effects on MS Windows\n-    import Queue as _unused_module_Queue  # noqa: unused\n-\n \n from .exceptions import (\n     ClosedPoolError,\n@@ -32,6 +25,7 @@ from .exceptions import (\n )\n from .packages.ssl_match_hostname import CertificateError\n from .packages import six\n+from .packages.six.moves.queue import LifoQueue, Empty, Full\n from .connection import (\n     port_by_scheme,\n     DummyConnection,\n@@ -48,6 +42,10 @@ from .util.timeout import Timeout\n from .util.url import get_host, Url\n \n \n+if six.PY2:\n+    # Queue is imported for side effects on MS Windows\n+    import Queue as _unused_module_Queue  # noqa: F401\n+\n xrange = six.moves.xrange\n \n log = logging.getLogger(__name__)\n@@ -210,7 +208,7 @@ class HTTPConnectionPool(ConnectionPool, RequestMethods):\n         Return a fresh :class:`HTTPConnection`.\n         \"\"\"\n         self.num_connections += 1\n-        log.info(\"Starting new HTTP connection (%d): %s\",\n+        log.debug(\"Starting new HTTP connection (%d): %s\",\n                   self.num_connections, self.host)\n \n         conn = self.ConnectionCls(host=self.host, port=self.port,\n@@ -246,7 +244,7 @@ class HTTPConnectionPool(ConnectionPool, RequestMethods):\n \n         # If this is a persistent connection, check if it got disconnected\n         if conn and is_connection_dropped(conn):\n-            log.info(\"Resetting dropped connection: %s\", self.host)\n+            log.debug(\"Resetting dropped connection: %s\", self.host)\n             conn.close()\n             if getattr(conn, 'auto_open', 1) == 0:\n                 # This is a proxied connection that has been mutated by\n@@ -397,8 +395,9 @@ class HTTPConnectionPool(ConnectionPool, RequestMethods):\n \n         # AppEngine doesn't have a version attr.\n         http_version = getattr(conn, '_http_vsn_str', 'HTTP/?')\n-        log.debug(\"\\\"%s %s %s\\\" %s %s\", method, url, http_version,\n-                  httplib_response.status, httplib_response.length)\n+        log.debug(\"%s://%s:%s \\\"%s %s %s\\\" %s %s\", self.scheme, self.host, self.port,\n+                  method, url, http_version, httplib_response.status,\n+                  httplib_response.length)\n \n         try:\n             assert_header_parsing(httplib_response.msg)\n@@ -600,10 +599,14 @@ class HTTPConnectionPool(ConnectionPool, RequestMethods):\n             # mess.\n             response_conn = conn if not release_conn else None\n \n+            # Pass method to Response for length checking\n+            response_kw['request_method'] = method\n+\n             # Import httplib's response into our own wrapper object\n             response = self.ResponseCls.from_httplib(httplib_response,\n                                                      pool=self,\n                                                      connection=response_conn,\n+                                                     retries=retries,\n                                                      **response_kw)\n \n             # Everything went great!\n@@ -683,7 +686,8 @@ class HTTPConnectionPool(ConnectionPool, RequestMethods):\n                     raise\n                 return response\n \n-            log.info(\"Redirecting %s -> %s\", url, redirect_location)\n+            retries.sleep_for_retry(response)\n+            log.debug(\"Redirecting %s -> %s\", url, redirect_location)\n             return self.urlopen(\n                 method, redirect_location, body, headers,\n                 retries=retries, redirect=redirect,\n@@ -692,7 +696,8 @@ class HTTPConnectionPool(ConnectionPool, RequestMethods):\n                 release_conn=release_conn, **response_kw)\n \n         # Check if we should retry the HTTP response.\n-        if retries.is_forced_retry(method, status_code=response.status):\n+        has_retry_after = bool(response.getheader('Retry-After'))\n+        if retries.is_retry(method, response.status, has_retry_after):\n             try:\n                 retries = retries.increment(method, url, response=response, _pool=self)\n             except MaxRetryError:\n@@ -702,8 +707,8 @@ class HTTPConnectionPool(ConnectionPool, RequestMethods):\n                     response.release_conn()\n                     raise\n                 return response\n-            retries.sleep()\n-            log.info(\"Forced retry: %s\", url)\n+            retries.sleep(response)\n+            log.debug(\"Retry: %s\", url)\n             return self.urlopen(\n                 method, url, body, headers,\n                 retries=retries, redirect=redirect,\n@@ -775,7 +780,6 @@ class HTTPSConnectionPool(HTTPConnectionPool):\n                           assert_hostname=self.assert_hostname,\n                           assert_fingerprint=self.assert_fingerprint)\n             conn.ssl_version = self.ssl_version\n-\n         return conn\n \n     def _prepare_proxy(self, conn):\n@@ -801,7 +805,7 @@ class HTTPSConnectionPool(HTTPConnectionPool):\n         Return a fresh :class:`httplib.HTTPSConnection`.\n         \"\"\"\n         self.num_connections += 1\n-        log.info(\"Starting new HTTPS connection (%d): %s\",\n+        log.debug(\"Starting new HTTPS connection (%d): %s\",\n                   self.num_connections, self.host)\n \n         if not self.ConnectionCls or self.ConnectionCls is DummyConnection:\n@@ -834,7 +838,8 @@ class HTTPSConnectionPool(HTTPConnectionPool):\n             warnings.warn((\n                 'Unverified HTTPS request is being made. '\n                 'Adding certificate verification is strongly advised. See: '\n-                'https://urllib3.readthedocs.io/en/latest/security.html'),\n+                'https://urllib3.readthedocs.io/en/latest/advanced-usage.html'\n+                '#ssl-warnings'),\n                 InsecureRequestWarning)\n \n \n\n@@ -1,7 +1,48 @@\n+\"\"\"\n+This module provides a pool manager that uses Google App Engine's\n+`URLFetch Service <https://cloud.google.com/appengine/docs/python/urlfetch>`_.\n+\n+Example usage::\n+\n+    from urllib3 import PoolManager\n+    from urllib3.contrib.appengine import AppEngineManager, is_appengine_sandbox\n+\n+    if is_appengine_sandbox():\n+        # AppEngineManager uses AppEngine's URLFetch API behind the scenes\n+        http = AppEngineManager()\n+    else:\n+        # PoolManager uses a socket-level API behind the scenes\n+        http = PoolManager()\n+\n+    r = http.request('GET', 'https://google.com/')\n+\n+There are `limitations <https://cloud.google.com/appengine/docs/python/\\\n+urlfetch/#Python_Quotas_and_limits>`_ to the URLFetch service and it may not be\n+the best choice for your application. There are three options for using\n+urllib3 on Google App Engine:\n+\n+1. You can use :class:`AppEngineManager` with URLFetch. URLFetch is\n+   cost-effective in many circumstances as long as your usage is within the\n+   limitations.\n+2. You can use a normal :class:`~urllib3.PoolManager` by enabling sockets.\n+   Sockets also have `limitations and restrictions\n+   <https://cloud.google.com/appengine/docs/python/sockets/\\\n+   #limitations-and-restrictions>`_ and have a lower free quota than URLFetch.\n+   To use sockets, be sure to specify the following in your ``app.yaml``::\n+\n+        env_variables:\n+            GAE_USE_SOCKETS_HTTPLIB : 'true'\n+\n+3. If you are using `App Engine Flexible\n+<https://cloud.google.com/appengine/docs/flexible/>`_, you can use the standard\n+:class:`PoolManager` without any configuration or special environment variables.\n+\"\"\"\n+\n from __future__ import absolute_import\n import logging\n import os\n import warnings\n+from urlparse import urljoin\n \n from ..exceptions import (\n     HTTPError,\n@@ -41,13 +82,12 @@ class AppEngineManager(RequestMethods):\n \n     This manager uses the URLFetch service directly instead of using the\n     emulated httplib, and is subject to URLFetch limitations as described in\n-    the App Engine documentation here:\n+    the App Engine documentation `here\n+    <https://cloud.google.com/appengine/docs/python/urlfetch>`_.\n \n-        https://cloud.google.com/appengine/docs/python/urlfetch\n-\n-    Notably it will raise an AppEnginePlatformError if:\n+    Notably it will raise an :class:`AppEnginePlatformError` if:\n         * URLFetch is not available.\n-        * If you attempt to use this on GAEv2 (Managed VMs), as full socket\n+        * If you attempt to use this on App Engine Flexible, as full socket\n           support is available.\n         * If a request size is more than 10 megabytes.\n         * If a response size is more than 32 megabtyes.\n@@ -56,7 +96,8 @@ class AppEngineManager(RequestMethods):\n     Beyond those cases, it will raise normal urllib3 errors.\n     \"\"\"\n \n-    def __init__(self, headers=None, retries=None, validate_certificate=True):\n+    def __init__(self, headers=None, retries=None, validate_certificate=True,\n+                 urlfetch_retries=True):\n         if not urlfetch:\n             raise AppEnginePlatformError(\n                 \"URLFetch is not available in this environment.\")\n@@ -75,6 +116,7 @@ class AppEngineManager(RequestMethods):\n \n         RequestMethods.__init__(self, headers)\n         self.validate_certificate = validate_certificate\n+        self.urlfetch_retries = urlfetch_retries\n \n         self.retries = retries or Retry.DEFAULT\n \n@@ -92,16 +134,17 @@ class AppEngineManager(RequestMethods):\n         retries = self._get_retries(retries, redirect)\n \n         try:\n+            follow_redirects = (\n+                    redirect and\n+                    retries.redirect != 0 and\n+                    retries.total)\n             response = urlfetch.fetch(\n                 url,\n                 payload=body,\n                 method=method,\n                 headers=headers or {},\n                 allow_truncated=False,\n-                follow_redirects=(\n-                    redirect and\n-                    retries.redirect != 0 and\n-                    retries.total),\n+                follow_redirects=self.urlfetch_retries and follow_redirects,\n                 deadline=self._get_absolute_timeout(timeout),\n                 validate_certificate=self.validate_certificate,\n             )\n@@ -133,19 +176,40 @@ class AppEngineManager(RequestMethods):\n                 \"URLFetch does not support method: %s\" % method, e)\n \n         http_response = self._urlfetch_response_to_http_response(\n-            response, **response_kw)\n+            response, retries=retries, **response_kw)\n \n+        # Handle redirect?\n+        redirect_location = redirect and http_response.get_redirect_location()\n+        if redirect_location:\n             # Check for redirect response\n-        if (http_response.get_redirect_location() and\n-                retries.raise_on_redirect and redirect):\n+            if (self.urlfetch_retries and retries.raise_on_redirect):\n                 raise MaxRetryError(self, url, \"too many redirects\")\n+            else:\n+                if http_response.status == 303:\n+                    method = 'GET'\n+\n+                try:\n+                    retries = retries.increment(method, url, response=http_response, _pool=self)\n+                except MaxRetryError:\n+                    if retries.raise_on_redirect:\n+                        raise MaxRetryError(self, url, \"too many redirects\")\n+                    return http_response\n+\n+                retries.sleep_for_retry(http_response)\n+                log.debug(\"Redirecting %s -> %s\", url, redirect_location)\n+                redirect_url = urljoin(url, redirect_location)\n+                return self.urlopen(\n+                    method, redirect_url, body, headers,\n+                    retries=retries, redirect=redirect,\n+                    timeout=timeout, **response_kw)\n \n         # Check if we should retry the HTTP response.\n-        if retries.is_forced_retry(method, status_code=http_response.status):\n+        has_retry_after = bool(http_response.getheader('Retry-After'))\n+        if retries.is_retry(method, http_response.status, has_retry_after):\n             retries = retries.increment(\n                 method, url, response=http_response, _pool=self)\n-            log.info(\"Forced retry: %s\", url)\n-            retries.sleep()\n+            log.debug(\"Retry: %s\", url)\n+            retries.sleep(http_response)\n             return self.urlopen(\n                 method, url,\n                 body=body, headers=headers,\n@@ -183,12 +247,13 @@ class AppEngineManager(RequestMethods):\n \n     def _get_absolute_timeout(self, timeout):\n         if timeout is Timeout.DEFAULT_TIMEOUT:\n-            return 5  # 5s is the default timeout for URLFetch.\n+            return None  # Defer to URLFetch's default.\n         if isinstance(timeout, Timeout):\n-            if timeout._read is not timeout._connect:\n+            if timeout._read is not None or timeout._connect is not None:\n                 warnings.warn(\n                     \"URLFetch does not support granular timeout settings, \"\n-                    \"reverting to total timeout.\", AppEnginePlatformWarning)\n+                    \"reverting to total or default URLFetch timeout.\",\n+                    AppEnginePlatformWarning)\n             return timeout.total\n         return timeout\n \n\n@@ -5,14 +5,11 @@ Issue #10, see: http://code.google.com/p/urllib3/issues/detail?id=10\n \"\"\"\n from __future__ import absolute_import\n \n-try:\n-    from http.client import HTTPSConnection\n-except ImportError:\n-    from httplib import HTTPSConnection\n from logging import getLogger\n from ntlm import ntlm\n \n-from urllib3 import HTTPSConnectionPool\n+from .. import HTTPSConnectionPool\n+from ..packages.six.moves.http_client import HTTPSConnection\n \n \n log = getLogger(__name__)\n\n@@ -1,17 +1,21 @@\n-'''SSL with SNI_-support for Python 2. Follow these instructions if you would\n+\"\"\"\n+SSL with SNI_-support for Python 2. Follow these instructions if you would\n like to verify SSL certificates in Python 2. Note, the default libraries do\n *not* do certificate checking; you need to do additional work to validate\n certificates yourself.\n \n This needs the following packages installed:\n \n-* pyOpenSSL (tested with 0.13)\n-* ndg-httpsclient (tested with 0.3.2)\n-* pyasn1 (tested with 0.1.6)\n+* pyOpenSSL (tested with 16.0.0)\n+* cryptography (minimum 1.3.4, from pyopenssl)\n+* idna (minimum 2.0, from cryptography)\n+\n+However, pyopenssl depends on cryptography, which depends on idna, so while we\n+use all three directly here we end up having relatively few packages required.\n \n You can install them with the following command:\n \n-    pip install pyopenssl ndg-httpsclient pyasn1\n+    pip install pyopenssl cryptography idna\n \n To activate certificate checking, call\n :func:`~urllib3.contrib.pyopenssl.inject_into_urllib3` from your Python code\n@@ -34,45 +38,38 @@ compression in Python 2 (see `CRIME attack`_).\n If you want to configure the default list of supported cipher suites, you can\n set the ``urllib3.contrib.pyopenssl.DEFAULT_SSL_CIPHER_LIST`` variable.\n \n-Module Variables\n-----------------\n-\n-:var DEFAULT_SSL_CIPHER_LIST: The list of supported SSL/TLS cipher suites.\n-\n .. _sni: https://en.wikipedia.org/wiki/Server_Name_Indication\n .. _crime attack: https://en.wikipedia.org/wiki/CRIME_(security_exploit)\n-\n-'''\n+\"\"\"\n from __future__ import absolute_import\n \n-try:\n-    from ndg.httpsclient.ssl_peer_verification import SUBJ_ALT_NAME_SUPPORT\n-    from ndg.httpsclient.subj_alt_name import SubjectAltName as BaseSubjectAltName\n-except SyntaxError as e:\n-    raise ImportError(e)\n-\n+import idna\n import OpenSSL.SSL\n-from pyasn1.codec.der import decoder as der_decoder\n-from pyasn1.type import univ, constraint\n+from cryptography import x509\n+from cryptography.hazmat.backends.openssl import backend as openssl_backend\n+from cryptography.hazmat.backends.openssl.x509 import _Certificate\n+\n from socket import timeout, error as SocketError\n+from io import BytesIO\n \n try:  # Platform-specific: Python 2\n     from socket import _fileobject\n except ImportError:  # Platform-specific: Python 3\n     _fileobject = None\n-    from urllib3.packages.backports.makefile import backport_makefile\n+    from ..packages.backports.makefile import backport_makefile\n \n+import logging\n import ssl\n import select\n import six\n+import sys\n \n-from .. import connection\n from .. import util\n \n __all__ = ['inject_into_urllib3', 'extract_from_urllib3']\n \n-# SNI only *really* works if we can read the subjectAltName of certificates.\n-HAS_SNI = SUBJ_ALT_NAME_SUPPORT\n+# SNI always works.\n+HAS_SNI = True\n \n # Map from urllib3 to PyOpenSSL compatible parameter-values.\n _openssl_versions = {\n@@ -91,78 +88,120 @@ try:\n except AttributeError:\n     pass\n \n-_openssl_verify = {\n+_stdlib_to_openssl_verify = {\n     ssl.CERT_NONE: OpenSSL.SSL.VERIFY_NONE,\n     ssl.CERT_OPTIONAL: OpenSSL.SSL.VERIFY_PEER,\n     ssl.CERT_REQUIRED:\n         OpenSSL.SSL.VERIFY_PEER + OpenSSL.SSL.VERIFY_FAIL_IF_NO_PEER_CERT,\n }\n-\n-DEFAULT_SSL_CIPHER_LIST = util.ssl_.DEFAULT_CIPHERS.encode('ascii')\n+_openssl_to_stdlib_verify = dict(\n+    (v, k) for k, v in _stdlib_to_openssl_verify.items()\n+)\n \n # OpenSSL will only write 16K at a time\n SSL_WRITE_BLOCKSIZE = 16384\n \n orig_util_HAS_SNI = util.HAS_SNI\n-orig_connection_ssl_wrap_socket = connection.ssl_wrap_socket\n+orig_util_SSLContext = util.ssl_.SSLContext\n+\n+\n+log = logging.getLogger(__name__)\n \n \n def inject_into_urllib3():\n     'Monkey-patch urllib3 with PyOpenSSL-backed SSL-support.'\n \n-    connection.ssl_wrap_socket = ssl_wrap_socket\n+    util.ssl_.SSLContext = PyOpenSSLContext\n     util.HAS_SNI = HAS_SNI\n+    util.ssl_.HAS_SNI = HAS_SNI\n     util.IS_PYOPENSSL = True\n+    util.ssl_.IS_PYOPENSSL = True\n \n \n def extract_from_urllib3():\n     'Undo monkey-patching by :func:`inject_into_urllib3`.'\n \n-    connection.ssl_wrap_socket = orig_connection_ssl_wrap_socket\n+    util.ssl_.SSLContext = orig_util_SSLContext\n     util.HAS_SNI = orig_util_HAS_SNI\n+    util.ssl_.HAS_SNI = orig_util_HAS_SNI\n     util.IS_PYOPENSSL = False\n+    util.ssl_.IS_PYOPENSSL = False\n \n \n-# Note: This is a slightly bug-fixed version of same from ndg-httpsclient.\n-class SubjectAltName(BaseSubjectAltName):\n-    '''ASN.1 implementation for subjectAltNames support'''\n+def _dnsname_to_stdlib(name):\n+    \"\"\"\n+    Converts a dNSName SubjectAlternativeName field to the form used by the\n+    standard library on the given Python version.\n \n-    # There is no limit to how many SAN certificates a certificate may have,\n-    #   however this needs to have some limit so we'll set an arbitrarily high\n-    #   limit.\n-    sizeSpec = univ.SequenceOf.sizeSpec + \\\n-        constraint.ValueSizeConstraint(1, 1024)\n+    Cryptography produces a dNSName as a unicode string that was idna-decoded\n+    from ASCII bytes. We need to idna-encode that string to get it back, and\n+    then on Python 3 we also need to convert to unicode via UTF-8 (the stdlib\n+    uses PyUnicode_FromStringAndSize on it, which decodes via UTF-8).\n+    \"\"\"\n+    def idna_encode(name):\n+        \"\"\"\n+        Borrowed wholesale from the Python Cryptography Project. It turns out\n+        that we can't just safely call `idna.encode`: it can explode for\n+        wildcard names. This avoids that problem.\n+        \"\"\"\n+        for prefix in [u'*.', u'.']:\n+            if name.startswith(prefix):\n+                name = name[len(prefix):]\n+                return prefix.encode('ascii') + idna.encode(name)\n+        return idna.encode(name)\n+\n+    name = idna_encode(name)\n+    if sys.version_info >= (3, 0):\n+        name = name.decode('utf-8')\n+    return name\n \n \n-# Note: This is a slightly bug-fixed version of same from ndg-httpsclient.\n def get_subj_alt_name(peer_cert):\n-    # Search through extensions\n-    dns_name = []\n-    if not SUBJ_ALT_NAME_SUPPORT:\n-        return dns_name\n+    \"\"\"\n+    Given an PyOpenSSL certificate, provides all the subject alternative names.\n+    \"\"\"\n+    # Pass the cert to cryptography, which has much better APIs for this.\n+    # This is technically using private APIs, but should work across all\n+    # relevant versions until PyOpenSSL gets something proper for this.\n+    cert = _Certificate(openssl_backend, peer_cert._x509)\n \n-    general_names = SubjectAltName()\n-    for i in range(peer_cert.get_extension_count()):\n-        ext = peer_cert.get_extension(i)\n-        ext_name = ext.get_short_name()\n-        if ext_name != b'subjectAltName':\n-            continue\n+    # We want to find the SAN extension. Ask Cryptography to locate it (it's\n+    # faster than looping in Python)\n+    try:\n+        ext = cert.extensions.get_extension_for_class(\n+            x509.SubjectAlternativeName\n+        ).value\n+    except x509.ExtensionNotFound:\n+        # No such extension, return the empty list.\n+        return []\n+    except (x509.DuplicateExtension, x509.UnsupportedExtension,\n+            x509.UnsupportedGeneralNameType, UnicodeError) as e:\n+        # A problem has been found with the quality of the certificate. Assume\n+        # no SAN field is present.\n+        log.warning(\n+            \"A problem was encountered with the certificate that prevented \"\n+            \"urllib3 from finding the SubjectAlternativeName field. This can \"\n+            \"affect certificate validation. The error was %s\",\n+            e,\n+        )\n+        return []\n \n-        # PyOpenSSL returns extension data in ASN.1 encoded form\n-        ext_dat = ext.get_data()\n-        decoded_dat = der_decoder.decode(ext_dat,\n-                                         asn1Spec=general_names)\n+    # We want to return dNSName and iPAddress fields. We need to cast the IPs\n+    # back to strings because the match_hostname function wants them as\n+    # strings.\n+    # Sadly the DNS names need to be idna encoded and then, on Python 3, UTF-8\n+    # decoded. This is pretty frustrating, but that's what the standard library\n+    # does with certificates, and so we need to attempt to do the same.\n+    names = [\n+        ('DNS', _dnsname_to_stdlib(name))\n+        for name in ext.get_values_for_type(x509.DNSName)\n+    ]\n+    names.extend(\n+        ('IP Address', str(name))\n+        for name in ext.get_values_for_type(x509.IPAddress)\n+    )\n \n-        for name in decoded_dat:\n-            if not isinstance(name, SubjectAltName):\n-                continue\n-            for entry in range(len(name)):\n-                component = name.getComponentByPosition(entry)\n-                if component.getName() != 'dNSName':\n-                    continue\n-                dns_name.append(str(component.getComponent()))\n-\n-    return dns_name\n+    return names\n \n \n class WrappedSocket(object):\n@@ -282,10 +321,7 @@ class WrappedSocket(object):\n             'subject': (\n                 (('commonName', x509.get_subject().CN),),\n             ),\n-            'subjectAltName': [\n-                ('DNS', value)\n-                for value in get_subj_alt_name(x509)\n-            ]\n+            'subjectAltName': get_subj_alt_name(x509)\n         }\n \n     def _reuse(self):\n@@ -308,41 +344,74 @@ else:  # Platform-specific: Python 3\n WrappedSocket.makefile = makefile\n \n \n-def _verify_callback(cnx, x509, err_no, err_depth, return_code):\n-    return err_no == 0\n+class PyOpenSSLContext(object):\n+    \"\"\"\n+    I am a wrapper class for the PyOpenSSL ``Context`` object. I am responsible\n+    for translating the interface of the standard library ``SSLContext`` object\n+    to calls into PyOpenSSL.\n+    \"\"\"\n+    def __init__(self, protocol):\n+        self.protocol = _openssl_versions[protocol]\n+        self._ctx = OpenSSL.SSL.Context(self.protocol)\n+        self._options = 0\n+        self.check_hostname = False\n \n+    @property\n+    def options(self):\n+        return self._options\n \n-def ssl_wrap_socket(sock, keyfile=None, certfile=None, cert_reqs=None,\n-                    ca_certs=None, server_hostname=None,\n-                    ssl_version=None, ca_cert_dir=None):\n-    ctx = OpenSSL.SSL.Context(_openssl_versions[ssl_version])\n-    if certfile:\n-        keyfile = keyfile or certfile  # Match behaviour of the normal python ssl library\n-        ctx.use_certificate_file(certfile)\n-    if keyfile:\n-        ctx.use_privatekey_file(keyfile)\n-    if cert_reqs != ssl.CERT_NONE:\n-        ctx.set_verify(_openssl_verify[cert_reqs], _verify_callback)\n-    if ca_certs or ca_cert_dir:\n-        try:\n-            ctx.load_verify_locations(ca_certs, ca_cert_dir)\n-        except OpenSSL.SSL.Error as e:\n-            raise ssl.SSLError('bad ca_certs: %r' % ca_certs, e)\n-    else:\n-        ctx.set_default_verify_paths()\n+    @options.setter\n+    def options(self, value):\n+        self._options = value\n+        self._ctx.set_options(value)\n \n-    # Disable TLS compression to mitigate CRIME attack (issue #309)\n-    OP_NO_COMPRESSION = 0x20000\n-    ctx.set_options(OP_NO_COMPRESSION)\n+    @property\n+    def verify_mode(self):\n+        return _openssl_to_stdlib_verify[self._ctx.get_verify_mode()]\n \n-    # Set list of supported ciphersuites.\n-    ctx.set_cipher_list(DEFAULT_SSL_CIPHER_LIST)\n+    @verify_mode.setter\n+    def verify_mode(self, value):\n+        self._ctx.set_verify(\n+            _stdlib_to_openssl_verify[value],\n+            _verify_callback\n+        )\n+\n+    def set_default_verify_paths(self):\n+        self._ctx.set_default_verify_paths()\n+\n+    def set_ciphers(self, ciphers):\n+        if isinstance(ciphers, six.text_type):\n+            ciphers = ciphers.encode('utf-8')\n+        self._ctx.set_cipher_list(ciphers)\n+\n+    def load_verify_locations(self, cafile=None, capath=None, cadata=None):\n+        if cafile is not None:\n+            cafile = cafile.encode('utf-8')\n+        if capath is not None:\n+            capath = capath.encode('utf-8')\n+        self._ctx.load_verify_locations(cafile, capath)\n+        if cadata is not None:\n+            self._ctx.load_verify_locations(BytesIO(cadata))\n+\n+    def load_cert_chain(self, certfile, keyfile=None, password=None):\n+        self._ctx.use_certificate_file(certfile)\n+        if password is not None:\n+            self._ctx.set_passwd_cb(lambda max_length, prompt_twice, userdata: password)\n+        self._ctx.use_privatekey_file(keyfile or certfile)\n+\n+    def wrap_socket(self, sock, server_side=False,\n+                    do_handshake_on_connect=True, suppress_ragged_eofs=True,\n+                    server_hostname=None):\n+        cnx = OpenSSL.SSL.Connection(self._ctx, sock)\n \n-    cnx = OpenSSL.SSL.Connection(ctx, sock)\n         if isinstance(server_hostname, six.text_type):  # Platform-specific: Python 3\n             server_hostname = server_hostname.encode('utf-8')\n+\n+        if server_hostname is not None:\n             cnx.set_tlsext_host_name(server_hostname)\n+\n         cnx.set_connect_state()\n+\n         while True:\n             try:\n                 cnx.do_handshake()\n@@ -356,3 +425,7 @@ def ssl_wrap_socket(sock, keyfile=None, certfile=None, cert_reqs=None,\n             break\n \n         return WrappedSocket(cnx, sock)\n+\n+\n+def _verify_callback(cnx, x509, err_no, err_depth, return_code):\n+    return err_no == 0\n\n@@ -1,17 +1,23 @@\n # -*- coding: utf-8 -*-\n \"\"\"\n-SOCKS support for urllib3\n-~~~~~~~~~~~~~~~~~~~~~~~~~\n-\n-This contrib module contains provisional support for SOCKS proxies from within\n+This module contains provisional support for SOCKS proxies from within\n urllib3. This module supports SOCKS4 (specifically the SOCKS4A variant) and\n SOCKS5. To enable its functionality, either install PySocks or install this\n module with the ``socks`` extra.\n \n+The SOCKS implementation supports the full range of urllib3 features. It also\n+supports the following SOCKS features:\n+\n+- SOCKS4\n+- SOCKS4a\n+- SOCKS5\n+- Usernames and passwords for the SOCKS proxy\n+\n Known Limitations:\n \n - Currently PySocks does not support contacting remote websites via literal\n-  IPv6 addresses. Any such connection attempt will fail.\n+  IPv6 addresses. Any such connection attempt will fail. You must use a domain\n+  name.\n - Currently PySocks does not support IPv6 connections to the SOCKS proxy. Any\n   such connection attempt will fail.\n \"\"\"\n\n@@ -1,4 +1,7 @@\n from __future__ import absolute_import\n+from .packages.six.moves.http_client import (\n+    IncompleteRead as httplib_IncompleteRead\n+)\n # Base Exceptions\n \n \n@@ -193,6 +196,35 @@ class ResponseNotChunked(ProtocolError, ValueError):\n     pass\n \n \n+class BodyNotHttplibCompatible(HTTPError):\n+    \"\"\"\n+    Body should be httplib.HTTPResponse like (have an fp attribute which\n+    returns raw chunks) for read_chunked().\n+    \"\"\"\n+    pass\n+\n+\n+class IncompleteRead(HTTPError, httplib_IncompleteRead):\n+    \"\"\"\n+    Response length doesn't match expected Content-Length\n+\n+    Subclass of http_client.IncompleteRead to allow int value\n+    for `partial` to avoid creating large objects on streamed\n+    reads.\n+    \"\"\"\n+    def __init__(self, partial, expected):\n+        super(IncompleteRead, self).__init__(partial, expected)\n+\n+    def __repr__(self):\n+        return ('IncompleteRead(%i bytes read, '\n+                '%i more expected)' % (self.partial, self.expected))\n+\n+\n+class InvalidHeader(HTTPError):\n+    \"The header provided was somehow invalid.\"\n+    pass\n+\n+\n class ProxySchemeUnknown(AssertionError, ValueError):\n     \"ProxyManager does not support the supplied scheme\"\n     # TODO(t-8ch): Stop inheriting from AssertionError in v2.0.\n\n@@ -130,7 +130,7 @@ class RequestField(object):\n             iterable = header_parts.items()\n \n         for name, value in iterable:\n-            if value:\n+            if value is not None:\n                 parts.append(self._render_part(name, value))\n \n         return '; '.join(parts)\n\n@@ -13,7 +13,7 @@ writer = codecs.lookup('utf-8')[3]\n \n def choose_boundary():\n     \"\"\"\n-    Our embarassingly-simple replacement for mimetools.choose_boundary.\n+    Our embarrassingly-simple replacement for mimetools.choose_boundary.\n     \"\"\"\n     return uuid4().hex\n \n\n@@ -1,5 +1,11 @@\n+import sys\n+\n try:\n-    # Python 3.2+\n+    # Our match_hostname function is the same as 3.5's, so we only want to\n+    # import the match_hostname function if it's at least that good.\n+    if sys.version_info < (3, 5):\n+        raise ImportError(\"Fallback to vendored code\")\n+\n     from ssl import CertificateError, match_hostname\n except ImportError:\n     try:\n\n@@ -4,8 +4,20 @@\n # stdlib.   http://docs.python.org/3/license.html\n \n import re\n+import sys\n+\n+# ipaddress has been backported to 2.6+ in pypi.  If it is installed on the\n+# system, use it to handle IPAddress ServerAltnames (this was added in\n+# python-3.5) otherwise only do DNS matching.  This allows\n+# backports.ssl_match_hostname to continue to be used all the way back to\n+# python-2.4.\n+try:\n+    import ipaddress\n+except ImportError:\n+    ipaddress = None\n+\n+__version__ = '3.5.0.1'\n \n-__version__ = '3.4.0.2'\n \n class CertificateError(ValueError):\n     pass\n@@ -64,6 +76,23 @@ def _dnsname_match(dn, hostname, max_wildcards=1):\n     return pat.match(hostname)\n \n \n+def _to_unicode(obj):\n+    if isinstance(obj, str) and sys.version_info < (3,):\n+        obj = unicode(obj, encoding='ascii', errors='strict')\n+    return obj\n+\n+def _ipaddress_match(ipname, host_ip):\n+    \"\"\"Exact matching of IP addresses.\n+\n+    RFC 6125 explicitly doesn't define an algorithm for this\n+    (section 1.7.2 - \"Out of Scope\").\n+    \"\"\"\n+    # OpenSSL may add a trailing newline to a subjectAltName's IP address\n+    # Divergence from upstream: ipaddress can't handle byte str\n+    ip = ipaddress.ip_address(_to_unicode(ipname).rstrip())\n+    return ip == host_ip\n+\n+\n def match_hostname(cert, hostname):\n     \"\"\"Verify that *cert* (in decoded format as returned by\n     SSLSocket.getpeercert()) matches the *hostname*.  RFC 2818 and RFC 6125\n@@ -73,12 +102,35 @@ def match_hostname(cert, hostname):\n     returns nothing.\n     \"\"\"\n     if not cert:\n-        raise ValueError(\"empty or no certificate\")\n+        raise ValueError(\"empty or no certificate, match_hostname needs a \"\n+                         \"SSL socket or SSL context with either \"\n+                         \"CERT_OPTIONAL or CERT_REQUIRED\")\n+    try:\n+        # Divergence from upstream: ipaddress can't handle byte str\n+        host_ip = ipaddress.ip_address(_to_unicode(hostname))\n+    except ValueError:\n+        # Not an IP address (common case)\n+        host_ip = None\n+    except UnicodeError:\n+        # Divergence from upstream: Have to deal with ipaddress not taking\n+        # byte strings.  addresses should be all ascii, so we consider it not\n+        # an ipaddress in this case\n+        host_ip = None\n+    except AttributeError:\n+        # Divergence from upstream: Make ipaddress library optional\n+        if ipaddress is None:\n+            host_ip = None\n+        else:\n+            raise\n     dnsnames = []\n     san = cert.get('subjectAltName', ())\n     for key, value in san:\n         if key == 'DNS':\n-            if _dnsname_match(value, hostname):\n+            if host_ip is None and _dnsname_match(value, hostname):\n+                return\n+            dnsnames.append(value)\n+        elif key == 'IP Address':\n+            if host_ip is not None and _ipaddress_match(value, host_ip):\n                 return\n             dnsnames.append(value)\n     if not dnsnames:\n\n@@ -3,15 +3,11 @@ import collections\n import functools\n import logging\n \n-try:  # Python 3\n-    from urllib.parse import urljoin\n-except ImportError:\n-    from urlparse import urljoin\n-\n from ._collections import RecentlyUsedContainer\n from .connectionpool import HTTPConnectionPool, HTTPSConnectionPool\n from .connectionpool import port_by_scheme\n from .exceptions import LocationValueError, MaxRetryError, ProxySchemeUnknown\n+from .packages.six.moves.urllib.parse import urljoin\n from .request import RequestMethods\n from .util.url import parse_url\n from .util.retry import Retry\n@@ -23,7 +19,7 @@ __all__ = ['PoolManager', 'ProxyManager', 'proxy_from_url']\n log = logging.getLogger(__name__)\n \n SSL_KEYWORDS = ('key_file', 'cert_file', 'cert_reqs', 'ca_certs',\n-                'ssl_version', 'ca_cert_dir')\n+                'ssl_version', 'ca_cert_dir', 'ssl_context')\n \n # The base fields to use when determining what pool to get a connection from;\n # these do not rely on the ``connection_pool_kw`` and can be determined by the\n\n@@ -1,10 +1,7 @@\n from __future__ import absolute_import\n-try:\n-    from urllib.parse import urlencode\n-except ImportError:\n-    from urllib import urlencode\n \n from .filepost import encode_multipart_formdata\n+from .packages.six.moves.urllib.parse import urlencode\n \n \n __all__ = ['RequestMethods']\n\n@@ -2,18 +2,22 @@ from __future__ import absolute_import\n from contextlib import contextmanager\n import zlib\n import io\n+import logging\n from socket import timeout as SocketTimeout\n from socket import error as SocketError\n \n from ._collections import HTTPHeaderDict\n from .exceptions import (\n-    ProtocolError, DecodeError, ReadTimeoutError, ResponseNotChunked\n+    BodyNotHttplibCompatible, ProtocolError, DecodeError, ReadTimeoutError,\n+    ResponseNotChunked, IncompleteRead, InvalidHeader\n )\n from .packages.six import string_types as basestring, binary_type, PY3\n from .packages.six.moves import http_client as httplib\n from .connection import HTTPException, BaseSSLError\n from .util.response import is_fp_closed, is_response_to_head\n \n+log = logging.getLogger(__name__)\n+\n \n class DeflateDecoder(object):\n \n@@ -89,6 +93,14 @@ class HTTPResponse(io.IOBase):\n         When this HTTPResponse wrapper is generated from an httplib.HTTPResponse\n         object, it's convenient to include the original for debug purposes. It's\n         otherwise unused.\n+\n+    :param retries:\n+        The retries contains the last :class:`~urllib3.util.retry.Retry` that\n+        was used during the request.\n+\n+    :param enforce_content_length:\n+        Enforce content length checking. Body returned by server must match\n+        value of Content-Length header, if present. Otherwise, raise error.\n     \"\"\"\n \n     CONTENT_DECODERS = ['gzip', 'deflate']\n@@ -96,7 +108,8 @@ class HTTPResponse(io.IOBase):\n \n     def __init__(self, body='', headers=None, status=0, version=0, reason=None,\n                  strict=0, preload_content=True, decode_content=True,\n-                 original_response=None, pool=None, connection=None):\n+                 original_response=None, pool=None, connection=None,\n+                 retries=None, enforce_content_length=False, request_method=None):\n \n         if isinstance(headers, HTTPHeaderDict):\n             self.headers = headers\n@@ -107,6 +120,8 @@ class HTTPResponse(io.IOBase):\n         self.reason = reason\n         self.strict = strict\n         self.decode_content = decode_content\n+        self.retries = retries\n+        self.enforce_content_length = enforce_content_length\n \n         self._decoder = None\n         self._body = None\n@@ -132,6 +147,9 @@ class HTTPResponse(io.IOBase):\n         if \"chunked\" in encodings:\n             self.chunked = True\n \n+        # Determine length of response\n+        self.length_remaining = self._init_length(request_method)\n+\n         # If requested, preload the body.\n         if preload_content and not self._body:\n             self._body = self.read(decode_content=decode_content)\n@@ -177,9 +195,57 @@ class HTTPResponse(io.IOBase):\n         \"\"\"\n         return self._fp_bytes_read\n \n+    def _init_length(self, request_method):\n+        \"\"\"\n+        Set initial length value for Response content if available.\n+        \"\"\"\n+        length = self.headers.get('content-length')\n+\n+        if length is not None and self.chunked:\n+            # This Response will fail with an IncompleteRead if it can't be\n+            # received as chunked. This method falls back to attempt reading\n+            # the response before raising an exception.\n+            log.warning(\"Received response with both Content-Length and \"\n+                        \"Transfer-Encoding set. This is expressly forbidden \"\n+                        \"by RFC 7230 sec 3.3.2. Ignoring Content-Length and \"\n+                        \"attempting to process response as Transfer-Encoding: \"\n+                        \"chunked.\")\n+            return None\n+\n+        elif length is not None:\n+            try:\n+                # RFC 7230 section 3.3.2 specifies multiple content lengths can\n+                # be sent in a single Content-Length header\n+                # (e.g. Content-Length: 42, 42). This line ensures the values\n+                # are all valid ints and that as long as the `set` length is 1,\n+                # all values are the same. Otherwise, the header is invalid.\n+                lengths = set([int(val) for val in length.split(',')])\n+                if len(lengths) > 1:\n+                    raise InvalidHeader(\"Content-Length contained multiple \"\n+                                        \"unmatching values (%s)\" % length)\n+                length = lengths.pop()\n+            except ValueError:\n+                length = None\n+            else:\n+                if length < 0:\n+                    length = None\n+\n+        # Convert status to int for comparison\n+        # In some cases, httplib returns a status of \"_UNKNOWN\"\n+        try:\n+            status = int(self.status)\n+        except ValueError:\n+            status = 0\n+\n+        # Check for responses that shouldn't include a body\n+        if status in (204, 304) or 100 <= status < 200 or request_method == 'HEAD':\n+            length = 0\n+\n+        return length\n+\n     def _init_decoder(self):\n         \"\"\"\n-        Set-up the _decoder attribute if necessar.\n+        Set-up the _decoder attribute if necessary.\n         \"\"\"\n         # Note: content-encoding value should be case-insensitive, per RFC 7230\n         # Section 3.2\n@@ -322,9 +388,18 @@ class HTTPResponse(io.IOBase):\n                     # no harm in redundantly calling close.\n                     self._fp.close()\n                     flush_decoder = True\n+                    if self.enforce_content_length and self.length_remaining not in (0, None):\n+                        # This is an edge case that httplib failed to cover due\n+                        # to concerns of backward compatibility. We're\n+                        # addressing it here to make sure IncompleteRead is\n+                        # raised during streaming, so all calls with incorrect\n+                        # Content-Length are caught.\n+                        raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n \n         if data:\n             self._fp_bytes_read += len(data)\n+            if self.length_remaining is not None:\n+                self.length_remaining -= len(data)\n \n             data = self._decode(data, decode_content, flush_decoder)\n \n@@ -349,7 +424,7 @@ class HTTPResponse(io.IOBase):\n             If True, will attempt to decode the body based on the\n             'content-encoding' header.\n         \"\"\"\n-        if self.chunked:\n+        if self.chunked and self.supports_chunked_reads():\n             for line in self.read_chunked(amt, decode_content=decode_content):\n                 yield line\n         else:\n@@ -407,10 +482,10 @@ class HTTPResponse(io.IOBase):\n     def closed(self):\n         if self._fp is None:\n             return True\n+        elif hasattr(self._fp, 'isclosed'):\n+            return self._fp.isclosed()\n         elif hasattr(self._fp, 'closed'):\n             return self._fp.closed\n-        elif hasattr(self._fp, 'isclosed'):  # Python 2\n-            return self._fp.isclosed()\n         else:\n             return True\n \n@@ -440,6 +515,15 @@ class HTTPResponse(io.IOBase):\n             b[:len(temp)] = temp\n             return len(temp)\n \n+    def supports_chunked_reads(self):\n+        \"\"\"\n+        Checks if the underlying file-like object looks like a\n+        httplib.HTTPResponse object. We do this by testing for the fp\n+        attribute. If it is present we assume it returns raw chunks as\n+        processed by read_chunked().\n+        \"\"\"\n+        return hasattr(self._fp, 'fp')\n+\n     def _update_chunk_length(self):\n         # First, we'll figure out length of a chunk and then\n         # we'll try to read it from socket.\n@@ -491,6 +575,10 @@ class HTTPResponse(io.IOBase):\n             raise ResponseNotChunked(\n                 \"Response is not chunked. \"\n                 \"Header 'transfer-encoding: chunked' is missing.\")\n+        if not self.supports_chunked_reads():\n+            raise BodyNotHttplibCompatible(\n+                \"Body should be httplib.HTTPResponse like. \"\n+                \"It should have have an fp attribute which returns raw chunks.\")\n \n         # Don't bother reading the body of a HEAD request.\n         if self._original_response and is_response_to_head(self._original_response):\n\n@@ -12,6 +12,13 @@ def is_fp_closed(obj):\n         The file-like object to check.\n     \"\"\"\n \n+    try:\n+        # Check `isclosed()` first, in case Python3 doesn't set `closed`.\n+        # GH Issue #928\n+        return obj.isclosed()\n+    except AttributeError:\n+        pass\n+\n     try:\n         # Check via the official file-like-object way.\n         return obj.closed\n\n@@ -1,6 +1,10 @@\n from __future__ import absolute_import\n import time\n import logging\n+from collections import namedtuple\n+from itertools import takewhile\n+import email\n+import re\n \n from ..exceptions import (\n     ConnectTimeoutError,\n@@ -8,12 +12,17 @@ from ..exceptions import (\n     ProtocolError,\n     ReadTimeoutError,\n     ResponseError,\n+    InvalidHeader,\n )\n from ..packages import six\n \n \n log = logging.getLogger(__name__)\n \n+# Data structure for representing the metadata of requests that result in a retry.\n+RequestHistory = namedtuple('RequestHistory', [\"method\", \"url\", \"error\",\n+                                               \"status\", \"redirect_location\"])\n+\n \n class Retry(object):\n     \"\"\" Retry configuration.\n@@ -113,18 +122,29 @@ class Retry(object):\n         whether we should raise an exception, or return a response,\n         if status falls in ``status_forcelist`` range and retries have\n         been exhausted.\n+\n+    :param tuple history: The history of the request encountered during\n+        each call to :meth:`~Retry.increment`. The list is in the order\n+        the requests occurred. Each list item is of class :class:`RequestHistory`.\n+\n+    :param bool respect_retry_after_header:\n+        Whether to respect Retry-After header on status codes defined as\n+        :attr:`Retry.RETRY_AFTER_STATUS_CODES` or not.\n+\n     \"\"\"\n \n     DEFAULT_METHOD_WHITELIST = frozenset([\n         'HEAD', 'GET', 'PUT', 'DELETE', 'OPTIONS', 'TRACE'])\n \n+    RETRY_AFTER_STATUS_CODES = frozenset([413, 429, 503])\n+\n     #: Maximum backoff time.\n     BACKOFF_MAX = 120\n \n     def __init__(self, total=10, connect=None, read=None, redirect=None,\n                  method_whitelist=DEFAULT_METHOD_WHITELIST, status_forcelist=None,\n                  backoff_factor=0, raise_on_redirect=True, raise_on_status=True,\n-                 _observed_errors=0):\n+                 history=None, respect_retry_after_header=True):\n \n         self.total = total\n         self.connect = connect\n@@ -140,7 +160,8 @@ class Retry(object):\n         self.backoff_factor = backoff_factor\n         self.raise_on_redirect = raise_on_redirect\n         self.raise_on_status = raise_on_status\n-        self._observed_errors = _observed_errors  # TODO: use .history instead?\n+        self.history = history or tuple()\n+        self.respect_retry_after_header = respect_retry_after_header\n \n     def new(self, **kw):\n         params = dict(\n@@ -151,7 +172,7 @@ class Retry(object):\n             backoff_factor=self.backoff_factor,\n             raise_on_redirect=self.raise_on_redirect,\n             raise_on_status=self.raise_on_status,\n-            _observed_errors=self._observed_errors,\n+            history=self.history,\n         )\n         params.update(kw)\n         return type(self)(**params)\n@@ -175,23 +196,71 @@ class Retry(object):\n \n         :rtype: float\n         \"\"\"\n-        if self._observed_errors <= 1:\n+        # We want to consider only the last consecutive errors sequence (Ignore redirects).\n+        consecutive_errors_len = len(list(takewhile(lambda x: x.redirect_location is None,\n+                                                    reversed(self.history))))\n+        if consecutive_errors_len <= 1:\n             return 0\n \n-        backoff_value = self.backoff_factor * (2 ** (self._observed_errors - 1))\n+        backoff_value = self.backoff_factor * (2 ** (consecutive_errors_len - 1))\n         return min(self.BACKOFF_MAX, backoff_value)\n \n-    def sleep(self):\n-        \"\"\" Sleep between retry attempts using an exponential backoff.\n+    def parse_retry_after(self, retry_after):\n+        # Whitespace: https://tools.ietf.org/html/rfc7230#section-3.2.4\n+        if re.match(r\"^\\s*[0-9]+\\s*$\", retry_after):\n+            seconds = int(retry_after)\n+        else:\n+            retry_date_tuple = email.utils.parsedate(retry_after)\n+            if retry_date_tuple is None:\n+                raise InvalidHeader(\"Invalid Retry-After header: %s\" % retry_after)\n+            retry_date = time.mktime(retry_date_tuple)\n+            seconds = retry_date - time.time()\n \n-        By default, the backoff factor is 0 and this method will return\n-        immediately.\n-        \"\"\"\n+        if seconds < 0:\n+            seconds = 0\n+\n+        return seconds\n+\n+    def get_retry_after(self, response):\n+        \"\"\" Get the value of Retry-After in seconds. \"\"\"\n+\n+        retry_after = response.getheader(\"Retry-After\")\n+\n+        if retry_after is None:\n+            return None\n+\n+        return self.parse_retry_after(retry_after)\n+\n+    def sleep_for_retry(self, response=None):\n+        retry_after = self.get_retry_after(response)\n+        if retry_after:\n+            time.sleep(retry_after)\n+            return True\n+\n+        return False\n+\n+    def _sleep_backoff(self):\n         backoff = self.get_backoff_time()\n         if backoff <= 0:\n             return\n         time.sleep(backoff)\n \n+    def sleep(self, response=None):\n+        \"\"\" Sleep between retry attempts.\n+\n+        This method will respect a server's ``Retry-After`` response header\n+        and sleep the duration of the time requested. If that is not present, it\n+        will use an exponential backoff. By default, the backoff factor is 0 and\n+        this method will return immediately.\n+        \"\"\"\n+\n+        if response:\n+            slept = self.sleep_for_retry(response)\n+            if slept:\n+                return\n+\n+        self._sleep_backoff()\n+\n     def _is_connection_error(self, err):\n         \"\"\" Errors when we're fairly sure that the server did not receive the\n         request, so it should be safe to retry.\n@@ -204,13 +273,17 @@ class Retry(object):\n         \"\"\"\n         return isinstance(err, (ReadTimeoutError, ProtocolError))\n \n-    def is_forced_retry(self, method, status_code):\n+    def is_retry(self, method, status_code, has_retry_after=False):\n         \"\"\" Is this method/status code retryable? (Based on method/codes whitelists)\n         \"\"\"\n         if self.method_whitelist and method.upper() not in self.method_whitelist:\n             return False\n \n-        return self.status_forcelist and status_code in self.status_forcelist\n+        if self.status_forcelist and status_code in self.status_forcelist:\n+            return True\n+\n+        return (self.total and self.respect_retry_after_header and\n+                has_retry_after and (status_code in self.RETRY_AFTER_STATUS_CODES))\n \n     def is_exhausted(self):\n         \"\"\" Are we out of retries? \"\"\"\n@@ -241,11 +314,12 @@ class Retry(object):\n         if total is not None:\n             total -= 1\n \n-        _observed_errors = self._observed_errors\n         connect = self.connect\n         read = self.read\n         redirect = self.redirect\n         cause = 'unknown'\n+        status = None\n+        redirect_location = None\n \n         if error and self._is_connection_error(error):\n             # Connect retry?\n@@ -253,7 +327,6 @@ class Retry(object):\n                 raise six.reraise(type(error), error, _stacktrace)\n             elif connect is not None:\n                 connect -= 1\n-            _observed_errors += 1\n \n         elif error and self._is_read_error(error):\n             # Read retry?\n@@ -261,27 +334,30 @@ class Retry(object):\n                 raise six.reraise(type(error), error, _stacktrace)\n             elif read is not None:\n                 read -= 1\n-            _observed_errors += 1\n \n         elif response and response.get_redirect_location():\n             # Redirect retry?\n             if redirect is not None:\n                 redirect -= 1\n             cause = 'too many redirects'\n+            redirect_location = response.get_redirect_location()\n+            status = response.status\n \n         else:\n             # Incrementing because of a server error like a 500 in\n             # status_forcelist and a the given method is in the whitelist\n-            _observed_errors += 1\n             cause = ResponseError.GENERIC_ERROR\n             if response and response.status:\n                 cause = ResponseError.SPECIFIC_ERROR.format(\n                     status_code=response.status)\n+                status = response.status\n+\n+        history = self.history + (RequestHistory(method, url, error, status, redirect_location),)\n \n         new_retry = self.new(\n             total=total,\n             connect=connect, read=read, redirect=redirect,\n-            _observed_errors=_observed_errors)\n+            history=history)\n \n         if new_retry.is_exhausted():\n             raise MaxRetryError(_pool, url, error or ResponseError(cause))\n\n@@ -11,7 +11,6 @@ from ..exceptions import SSLError, InsecurePlatformWarning, SNIMissingWarning\n \n SSLContext = None\n HAS_SNI = False\n-create_default_context = None\n IS_PYOPENSSL = False\n \n # Maps the length of a digest to a possible hash function producing this digest\n@@ -63,14 +62,25 @@ except ImportError:\n # The general intent is:\n # - Prefer cipher suites that offer perfect forward secrecy (DHE/ECDHE),\n # - prefer ECDHE over DHE for better performance,\n-# - prefer any AES-GCM over any AES-CBC for better performance and security,\n-# - use 3DES as fallback which is secure but slow,\n+# - prefer any AES-GCM and ChaCha20 over any AES-CBC for better performance and\n+#   security,\n+# - prefer AES-GCM over ChaCha20 because hardware-accelerated AES is common,\n # - disable NULL authentication, MD5 MACs and DSS for security reasons.\n-DEFAULT_CIPHERS = (\n-    'ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:ECDH+HIGH:'\n-    'DH+HIGH:ECDH+3DES:DH+3DES:RSA+AESGCM:RSA+AES:RSA+HIGH:RSA+3DES:!aNULL:'\n-    '!eNULL:!MD5'\n-)\n+DEFAULT_CIPHERS = ':'.join([\n+    'ECDH+AESGCM',\n+    'ECDH+CHACHA20',\n+    'DH+AESGCM',\n+    'DH+CHACHA20',\n+    'ECDH+AES256',\n+    'DH+AES256',\n+    'ECDH+AES128',\n+    'DH+AES',\n+    'RSA+AESGCM',\n+    'RSA+AES',\n+    '!aNULL',\n+    '!eNULL',\n+    '!MD5',\n+])\n \n try:\n     from ssl import SSLContext  # Modern SSL?\n@@ -117,8 +127,8 @@ except ImportError:\n                 'urllib3 from configuring SSL appropriately and may cause '\n                 'certain SSL connections to fail. You can upgrade to a newer '\n                 'version of Python to solve this. For more information, see '\n-                'https://urllib3.readthedocs.io/en/latest/security.html'\n-                '#insecureplatformwarning.',\n+                'https://urllib3.readthedocs.io/en/latest/advanced-usage.html'\n+                '#ssl-warnings',\n                 InsecurePlatformWarning\n             )\n             kwargs = {\n@@ -287,6 +297,9 @@ def ssl_wrap_socket(sock, keyfile=None, certfile=None, cert_reqs=None,\n     \"\"\"\n     context = ssl_context\n     if context is None:\n+        # Note: This branch of code and all the variables in it are no longer\n+        # used by urllib3 itself. We should consider deprecating and removing\n+        # this code.\n         context = create_urllib3_context(ssl_version, cert_reqs,\n                                          ciphers=ciphers)\n \n@@ -301,6 +314,9 @@ def ssl_wrap_socket(sock, keyfile=None, certfile=None, cert_reqs=None,\n             if e.errno == errno.ENOENT:\n                 raise SSLError(e)\n             raise\n+    elif getattr(context, 'load_default_certs', None) is not None:\n+        # try to load OS default certs; works well on Windows (require Python3.4+)\n+        context.load_default_certs()\n \n     if certfile:\n         context.load_cert_chain(certfile, keyfile)\n@@ -313,8 +329,8 @@ def ssl_wrap_socket(sock, keyfile=None, certfile=None, cert_reqs=None,\n         'This may cause the server to present an incorrect TLS '\n         'certificate, which can cause validation failures. You can upgrade to '\n         'a newer version of Python to solve this. For more information, see '\n-        'https://urllib3.readthedocs.io/en/latest/security.html'\n-        '#snimissingwarning.',\n+        'https://urllib3.readthedocs.io/en/latest/advanced-usage.html'\n+        '#ssl-warnings',\n         SNIMissingWarning\n     )\n     return context.wrap_socket(sock)\n\n@@ -111,8 +111,8 @@ class Timeout(object):\n         :param name: The name of the timeout attribute to validate. This is\n             used to specify in error messages.\n         :return: The validated and casted version of the given value.\n-        :raises ValueError: If the type is not an integer or a float, or if it\n-            is a numeric value less than zero.\n+        :raises ValueError: If it is a numeric value less than or equal to\n+            zero, or the type is not an integer, float, or None.\n         \"\"\"\n         if value is _Default:\n             return cls.DEFAULT_TIMEOUT\n@@ -120,20 +120,23 @@ class Timeout(object):\n         if value is None or value is cls.DEFAULT_TIMEOUT:\n             return value\n \n+        if isinstance(value, bool):\n+            raise ValueError(\"Timeout cannot be a boolean value. It must \"\n+                             \"be an int, float or None.\")\n         try:\n             float(value)\n         except (TypeError, ValueError):\n             raise ValueError(\"Timeout value %s was %s, but it must be an \"\n-                             \"int or float.\" % (name, value))\n+                             \"int, float or None.\" % (name, value))\n \n         try:\n-            if value < 0:\n+            if value <= 0:\n                 raise ValueError(\"Attempted to set %s timeout to %s, but the \"\n                                  \"timeout cannot be set to a value less \"\n-                                 \"than 0.\" % (name, value))\n+                                 \"than or equal to 0.\" % (name, value))\n         except TypeError:  # Python 3\n             raise ValueError(\"Timeout value %s was %s, but it must be an \"\n-                             \"int or float.\" % (name, value))\n+                             \"int, float or None.\" % (name, value))\n \n         return value\n \n\n@@ -10,14 +10,19 @@ url_attrs = ['scheme', 'auth', 'host', 'port', 'path', 'query', 'fragment']\n class Url(namedtuple('Url', url_attrs)):\n     \"\"\"\n     Datastructure for representing an HTTP URL. Used as a return value for\n-    :func:`parse_url`.\n+    :func:`parse_url`. Both the scheme and host are normalized as they are\n+    both case-insensitive according to RFC 3986.\n     \"\"\"\n-    slots = ()\n+    __slots__ = ()\n \n     def __new__(cls, scheme=None, auth=None, host=None, port=None, path=None,\n                 query=None, fragment=None):\n         if path and not path.startswith('/'):\n             path = '/' + path\n+        if scheme:\n+            scheme = scheme.lower()\n+        if host:\n+            host = host.lower()\n         return super(Url, cls).__new__(cls, scheme, auth, host, port, path,\n                                        query, fragment)\n \n@@ -184,10 +189,14 @@ def parse_url(url):\n             host = _host\n \n         if port:\n-            # If given, ports must be integers.\n+            # If given, ports must be integers. No whitespace, no plus or\n+            # minus prefixes, no non-integer digits such as ^2 (superscript).\n             if not port.isdigit():\n                 raise LocationParseError(url)\n+            try:\n                 port = int(port)\n+            except ValueError:\n+                raise LocationParseError(url)\n         else:\n             # Blank ports are cool, too. (rfc3986#section-3.2.3)\n             port = None\n@@ -211,7 +220,7 @@ def parse_url(url):\n \n def get_host(url):\n     \"\"\"\n-    Deprecated. Use :func:`.parse_url` instead.\n+    Deprecated. Use :func:`parse_url` instead.\n     \"\"\"\n     p = parse_url(url)\n     return p.scheme or 'http', p.hostname, p.port\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
