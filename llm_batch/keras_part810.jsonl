{"custom_id": "keras#afc92f5b33aaa8be5bec729a8398faec747b75ca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 60 | Lines Deleted: 44 | Files Changed: 4 | Hunks: 30 | Methods Changed: 8 | Complexity Δ (Sum/Max): 8/2 | Churn Δ: 104 | Churn Cumulative: 209 | Contributors (this commit): 2 | Commits (past 90d): 12 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -307,9 +307,11 @@ class BaseConv(Layer):\n         # Do nothing if the layer isn't yet built\n         if not self.built:\n             return\n-        store[\"0\"] = self.kernel\n+        target_variables = [self.kernel]\n         if self.use_bias:\n-            store[\"1\"] = self.bias\n+            target_variables.append(self.bias)\n+        for i, variable in enumerate(target_variables):\n+            store[str(i)] = variable\n \n     def load_own_variables(self, store):\n         if not self.lora_enabled:\n@@ -317,9 +319,11 @@ class BaseConv(Layer):\n         # Do nothing if the layer isn't yet built\n         if not self.built:\n             return\n-        self._kernel.assign(store[\"0\"])\n+        target_variables = [self._kernel]\n         if self.use_bias:\n-            self.bias.assign(store[\"1\"])\n+            target_variables.append(self.bias)\n+        for i, variable in enumerate(target_variables):\n+            variable.assign(store[str(i)])\n         if self.lora_enabled:\n             self.lora_kernel_a.assign(ops.zeros(self.lora_kernel_a.shape))\n             self.lora_kernel_b.assign(ops.zeros(self.lora_kernel_b.shape))\n\n@@ -202,24 +202,26 @@ class Dense(Layer):\n         # The keys of the `store` will be saved as determined because the\n         # default ordering will change after quantization\n         kernel_value, kernel_scale = self._get_kernel_with_merged_lora()\n-        store[\"0\"] = kernel_value\n+        target_variables = [kernel_value]\n         if self.use_bias:\n-            store[\"1\"] = self.bias\n+            target_variables.append(self.bias)\n         if isinstance(self.dtype_policy, dtype_policies.QuantizedDTypePolicy):\n             mode = self.dtype_policy.quantization_mode\n             if mode == \"int8\":\n-                store[\"2\"] = kernel_scale\n+                target_variables.append(kernel_scale)\n             elif mode == \"float8\":\n-                store[\"2\"] = self.inputs_scale\n-                store[\"3\"] = self.inputs_amax_history\n-                store[\"4\"] = self.kernel_scale\n-                store[\"5\"] = self.kernel_amax_history\n-                store[\"6\"] = self.outputs_grad_scale\n-                store[\"7\"] = self.outputs_grad_amax_history\n+                target_variables.append(self.inputs_scale)\n+                target_variables.append(self.inputs_amax_history)\n+                target_variables.append(self.kernel_scale)\n+                target_variables.append(self.kernel_amax_history)\n+                target_variables.append(self.outputs_grad_scale)\n+                target_variables.append(self.outputs_grad_amax_history)\n             else:\n                 raise NotImplementedError(\n                     self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode)\n                 )\n+        for i, variable in enumerate(target_variables):\n+            store[str(i)] = variable\n \n     def load_own_variables(self, store):\n         if not self.lora_enabled:\n@@ -229,24 +231,26 @@ class Dense(Layer):\n             return\n         # The keys of the `store` will be saved as determined because the\n         # default ordering will change after quantization\n-        self._kernel.assign(store[\"0\"])\n+        target_variables = [self._kernel]\n         if self.use_bias:\n-            self.bias.assign(store[\"1\"])\n+            target_variables.append(self.bias)\n         if isinstance(self.dtype_policy, dtype_policies.QuantizedDTypePolicy):\n             mode = self.dtype_policy.quantization_mode\n             if mode == \"int8\":\n-                self.kernel_scale.assign(store[\"2\"])\n+                target_variables.append(self.kernel_scale)\n             elif mode == \"float8\":\n-                self.inputs_scale.assign(store[\"2\"])\n-                self.inputs_amax_history.assign(store[\"3\"])\n-                self.kernel_scale.assign(store[\"4\"])\n-                self.kernel_amax_history.assign(store[\"5\"])\n-                self.outputs_grad_scale.assign(store[\"6\"])\n-                self.outputs_grad_amax_history.assign(store[\"7\"])\n+                target_variables.append(self.inputs_scale)\n+                target_variables.append(self.inputs_amax_history)\n+                target_variables.append(self.kernel_scale)\n+                target_variables.append(self.kernel_amax_history)\n+                target_variables.append(self.outputs_grad_scale)\n+                target_variables.append(self.outputs_grad_amax_history)\n             else:\n                 raise NotImplementedError(\n                     self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode)\n                 )\n+        for i, variable in enumerate(target_variables):\n+            variable.assign(store[str(i)])\n         if self.lora_enabled:\n             self.lora_kernel_a.assign(ops.zeros(self.lora_kernel_a.shape))\n             self.lora_kernel_b.assign(ops.zeros(self.lora_kernel_b.shape))\n\n@@ -257,24 +257,26 @@ class EinsumDense(Layer):\n         # The keys of the `store` will be saved as determined because the\n         # default ordering will change after quantization\n         kernel_value, kernel_scale = self._get_kernel_with_merged_lora()\n-        store[\"0\"] = kernel_value\n+        target_variables = [kernel_value]\n         if self.bias is not None:\n-            store[\"1\"] = self.bias\n+            target_variables.append(self.bias)\n         if isinstance(self.dtype_policy, dtype_policies.QuantizedDTypePolicy):\n             mode = self.dtype_policy.quantization_mode\n             if mode == \"int8\":\n-                store[\"2\"] = kernel_scale\n+                target_variables.append(kernel_scale)\n             elif mode == \"float8\":\n-                store[\"2\"] = self.inputs_scale\n-                store[\"3\"] = self.inputs_amax_history\n-                store[\"4\"] = self.kernel_scale\n-                store[\"5\"] = self.kernel_amax_history\n-                store[\"6\"] = self.outputs_grad_scale\n-                store[\"7\"] = self.outputs_grad_amax_history\n+                target_variables.append(self.inputs_scale)\n+                target_variables.append(self.inputs_amax_history)\n+                target_variables.append(self.kernel_scale)\n+                target_variables.append(self.kernel_amax_history)\n+                target_variables.append(self.outputs_grad_scale)\n+                target_variables.append(self.outputs_grad_amax_history)\n             else:\n                 raise NotImplementedError(\n                     self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode)\n                 )\n+        for i, variable in enumerate(target_variables):\n+            store[str(i)] = variable\n \n     def load_own_variables(self, store):\n         if not self.lora_enabled:\n@@ -284,24 +286,26 @@ class EinsumDense(Layer):\n             return\n         # The keys of the `store` will be saved as determined because the\n         # default ordering will change after quantization\n-        self._kernel.assign(store[\"0\"])\n+        target_variables = [self._kernel]\n         if self.bias is not None:\n-            self.bias.assign(store[\"1\"])\n+            target_variables.append(self.bias)\n         if isinstance(self.dtype_policy, dtype_policies.QuantizedDTypePolicy):\n             mode = self.dtype_policy.quantization_mode\n             if mode == \"int8\":\n-                self.kernel_scale.assign(store[\"2\"])\n+                target_variables.append(self.kernel_scale)\n             elif mode == \"float8\":\n-                self.inputs_scale.assign(store[\"2\"])\n-                self.inputs_amax_history.assign(store[\"3\"])\n-                self.kernel_scale.assign(store[\"4\"])\n-                self.kernel_amax_history.assign(store[\"5\"])\n-                self.outputs_grad_scale.assign(store[\"6\"])\n-                self.outputs_grad_amax_history.assign(store[\"7\"])\n+                target_variables.append(self.inputs_scale)\n+                target_variables.append(self.inputs_amax_history)\n+                target_variables.append(self.kernel_scale)\n+                target_variables.append(self.kernel_amax_history)\n+                target_variables.append(self.outputs_grad_scale)\n+                target_variables.append(self.outputs_grad_amax_history)\n             else:\n                 raise NotImplementedError(\n                     self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode)\n                 )\n+        for i, variable in enumerate(target_variables):\n+            variable.assign(store[str(i)])\n         if self.lora_enabled:\n             self.lora_kernel_a.assign(ops.zeros(self.lora_kernel_a.shape))\n             self.lora_kernel_b.assign(ops.zeros(self.lora_kernel_b.shape))\n\n@@ -199,15 +199,17 @@ class Embedding(Layer):\n         embeddings_value, embeddings_scale = (\n             self._get_embeddings_with_merged_lora()\n         )\n-        store[\"0\"] = embeddings_value\n+        target_variables = [embeddings_value]\n         if isinstance(self.dtype_policy, dtype_policies.QuantizedDTypePolicy):\n             mode = self.dtype_policy.quantization_mode\n             if mode == \"int8\":\n-                store[\"1\"] = embeddings_scale\n+                target_variables.append(embeddings_scale)\n             else:\n                 raise NotImplementedError(\n                     self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode)\n                 )\n+        for i, variable in enumerate(target_variables):\n+            store[str(i)] = variable\n \n     def load_own_variables(self, store):\n         if not self.lora_enabled:\n@@ -217,15 +219,17 @@ class Embedding(Layer):\n             return\n         # The keys of the `store` will be saved as determined because the\n         # default ordering will change after quantization\n-        self._embeddings.assign(store[\"0\"])\n+        target_variables = [self._embeddings]\n         if isinstance(self.dtype_policy, dtype_policies.QuantizedDTypePolicy):\n             mode = self.dtype_policy.quantization_mode\n             if mode == \"int8\":\n-                self.embeddings_scale.assign(store[\"1\"])\n+                target_variables.append(self.embeddings_scale)\n             else:\n                 raise NotImplementedError(\n                     self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode)\n                 )\n+        for i, variable in enumerate(target_variables):\n+            variable.assign(store[str(i)])\n         if self.lora_enabled:\n             self.lora_embeddings_a.assign(\n                 ops.zeros(self.lora_embeddings_a.shape)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d7f5d2957d74b89bbe2751ea1ef605a1387731cc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 2 | Churn Cumulative: 247 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -782,6 +782,7 @@ def ctc_beam_search_decode(\n             axis=0,\n             fill_value=_pad,\n         )\n+        if len(unique_inverse.shape) >= 2:\n             unique_inverse = jnp.squeeze(unique_inverse, axis=1)\n \n         emit_scores = jnp.where(masked, -jnp.inf, scores)\n@@ -842,6 +843,7 @@ def ctc_beam_search_decode(\n             axis=0,\n             fill_value=_pad,\n         )\n+        if len(unique_inverse.shape) >= 2:\n             unique_inverse = jnp.squeeze(unique_inverse, axis=1)\n         scores = _merge_scores(unique_inverse, scores)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4919ea1702049a1cc9348f534656908204b3d9f3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 117 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 7 | Methods Changed: 6 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 124 | Churn Cumulative: 244 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -413,10 +413,14 @@ class CompileLoss(losses_module.Loss):\n         reduction=\"sum_over_batch_size\",\n         output_names=None,\n     ):\n-        if loss_weights and not isinstance(loss_weights, (list, tuple, dict)):\n+        if loss_weights and not isinstance(\n+            loss_weights, (list, tuple, dict, float)\n+        ):\n             raise ValueError(\n-                \"Expected `loss_weights` argument to be a list, tuple, or \"\n-                f\"dict. Received instead: loss_weights={loss_weights} \"\n+                \"Expected `loss_weights` argument to be a float \"\n+                \"(single output case) or a list, tuple, or \"\n+                \"dict (multiple output case). \"\n+                f\"Received instead: loss_weights={loss_weights} \"\n                 f\"of type {type(loss_weights)}\"\n             )\n         self._user_loss = loss\n\n@@ -90,7 +90,7 @@ class StructModel(Trainer, layers.Layer):\n         }\n \n \n-class ListModel(Trainer, layers.Layer):\n+class ListInputModel(Trainer, layers.Layer):\n     def __init__(self, units):\n         layers.Layer.__init__(self)\n         Trainer.__init__(self)\n@@ -110,6 +110,25 @@ class ListModel(Trainer, layers.Layer):\n         return self.dense_1(x[0]) + self.dense_2(x[1])\n \n \n+class ListOutputModel(Trainer, layers.Layer):\n+    def __init__(self, units):\n+        layers.Layer.__init__(self)\n+        Trainer.__init__(self)\n+        self.dense_1 = layers.Dense(\n+            units,\n+            use_bias=False,\n+            kernel_initializer=initializers.Ones(),\n+        )\n+        self.dense_2 = layers.Dense(\n+            units,\n+            use_bias=False,\n+            kernel_initializer=initializers.Ones(),\n+        )\n+\n+    def call(self, x):\n+        return [self.dense_1(x), self.dense_2(x)]\n+\n+\n class TrainingTestingLayer(Trainer, layers.Layer):\n     def __init__(self, **kwargs):\n         layers.Layer.__init__(self, **kwargs)\n@@ -265,8 +284,8 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n         self.assertIn(\"mean_squared_error\", history)\n         self.assertAllClose(\n             history[\"mean_squared_error\"],\n-            [14.402393, 10.991339, 8.388159],\n-            atol=6.1051628e-1,\n+            [14.5, 11.5, 8.5],\n+            atol=0.6,  # TODO: abnormal results for certain configs.\n         )\n \n     @parameterized.named_parameters(\n@@ -1164,7 +1183,7 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n \n     @pytest.mark.requires_trainable_backend\n     def test_nested_inputs(self):\n-        model = ListModel(units=2)\n+        model = ListInputModel(units=2)\n         out = model([np.ones((3, 2)), np.ones((3, 3))])\n         self.assertEqual(tuple(out.shape), (3, 2))\n         model.compile(optimizer=\"sgd\", loss=\"mse\", metrics=[\"mse\"])\n@@ -1420,6 +1439,93 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n         history = model.fit(x, y)\n         self.assertGreater(history.history[\"custom\"][0], 0.0)\n \n+    @pytest.mark.requires_trainable_backend\n+    def test_loss_weights(self):\n+        epochs = 3\n+        batch_size = 20\n+        dataset_size = batch_size * 2\n+\n+        # Single output case.\n+        model = ExampleModel(units=3)\n+        model.compile(\n+            optimizer=optimizers.SGD(),\n+            loss=losses.MeanSquaredError(),\n+            metrics=[metrics.MeanSquaredError()],\n+            loss_weights=0.2,\n+        )\n+        x = np.ones((dataset_size, 4))\n+        y = np.zeros((dataset_size, 3))\n+        history = model.fit(\n+            x,\n+            y,\n+            batch_size=batch_size,\n+            epochs=epochs,\n+        )\n+        history = history.history\n+        self.assertIn(\"loss\", history)\n+        self.assertAllClose(\n+            history[\"loss\"],\n+            [3.182979, 3.115617, 3.049681],\n+            atol=1e-3,\n+        )\n+\n+        # Dict output case.\n+        model = StructModel(units=3)\n+        model.compile(\n+            optimizer=optimizers.SGD(),\n+            loss={\n+                \"y_one\": losses.MeanSquaredError(),\n+                \"y_two\": losses.MeanSquaredError(),\n+            },\n+            metrics={\n+                \"y_one\": metrics.MeanSquaredError(),\n+                \"y_two\": metrics.MeanSquaredError(),\n+            },\n+            loss_weights={\"y_one\": 0.1, \"y_two\": 0.2},\n+        )\n+        x1 = np.ones((dataset_size, 4))\n+        x2 = np.ones((dataset_size, 4))\n+        y1 = np.zeros((dataset_size, 3))\n+        y2 = np.zeros((dataset_size, 3))\n+        history = model.fit(\n+            {\"x_one\": x1, \"x_two\": x2},\n+            {\"y_one\": y1, \"y_two\": y2},\n+            batch_size=batch_size,\n+            epochs=epochs,\n+        )\n+        history = history.history\n+        self.assertIn(\"loss\", history)\n+        self.assertAllClose(\n+            history[\"loss\"],\n+            [4.778718, 4.694403, 4.611693],\n+            atol=1e-3,\n+        )\n+\n+        # List output case.\n+        model = ListOutputModel(units=3)\n+        model.compile(\n+            optimizer=optimizers.SGD(),\n+            loss=[losses.MeanSquaredError(), losses.MeanSquaredError()],\n+            metrics=[metrics.MeanSquaredError(), metrics.MeanSquaredError()],\n+            loss_weights=[0.1, 0.2],\n+        )\n+        x = np.ones((dataset_size, 4))\n+        y1 = np.zeros((dataset_size, 3))\n+        y2 = np.zeros((dataset_size, 3))\n+        history = model.fit(\n+            x,\n+            [y1, y2],\n+            batch_size=batch_size,\n+            epochs=epochs,\n+        )\n+        history = history.history\n+        self.assertIn(\"loss\", history)\n+        self.assertAllClose(\n+            history[\"loss\"],\n+            [4.778718, 4.694403, 4.611693],\n+            atol=1e-3,\n+        )\n+\n \n class TrainerDistributeTest(testing.TestCase):\n     @pytest.mark.skipif(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#99fdbf7859ee86f4330f8d353347ddaad14b85e7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 61 | Contributors (this commit): 4 | Commits (past 90d): 9 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1024,6 +1024,8 @@ def vstack(xs):\n \n \n def vectorize(pyfunc, *, excluded=None, signature=None):\n+    if excluded is None:\n+        excluded = set()\n     return jnp.vectorize(pyfunc, excluded=excluded, signature=signature)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#33bf553148dfa9f806b89de0b9eec11497a19bc8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 52 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 60 | Churn Cumulative: 1586 | Contributors (this commit): 7 | Commits (past 90d): 6 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,7 +1,7 @@\n from keras.src.api_export import keras_export\n \n # Unique source of truth for the version number.\n-__version__ = \"3.3.0\"\n+__version__ = \"3.3.1\"\n \n \n @keras_export(\"keras.version\")\n\n@@ -60,6 +60,57 @@ def export_version_string(version, is_nightly=False, rc_index=None):\n         f.write(init_contents)\n \n \n+def ignore_files(_, filenames):\n+    return [f for f in filenames if f.endswith(\"_test.py\")]\n+\n+\n+def copy_source_to_build_directory(root_path):\n+    # Copy sources (`keras/` directory and setup files) to build\n+    # directory\n+    os.chdir(root_path)\n+    os.mkdir(build_directory)\n+    shutil.copytree(\n+        package, os.path.join(build_directory, package), ignore=ignore_files\n+    )\n+    for fname in to_copy:\n+        shutil.copy(fname, os.path.join(f\"{build_directory}\", fname))\n+    os.chdir(build_directory)\n+\n+\n+def build(root_path, is_nightly=False, rc_index=None):\n+    if os.path.exists(build_directory):\n+        raise ValueError(f\"Directory already exists: {build_directory}\")\n+\n+    try:\n+        copy_source_to_build_directory(root_path)\n+        create_legacy_directory()\n+        from keras.src.version import __version__  # noqa: E402\n+\n+        export_version_string(__version__, is_nightly, rc_index)\n+        return build_and_save_output(root_path, __version__)\n+    finally:\n+        # Clean up: remove the build directory (no longer needed)\n+        shutil.rmtree(build_directory)\n+\n+\n+def create_legacy_directory():\n+    shutil.move(os.path.join(\"keras\", \"api\", \"_tf_keras\"), \"keras\")\n+    with open(os.path.join(\"keras\", \"api\", \"__init__.py\")) as f:\n+        contents = f.read()\n+        contents = contents.replace(\"from keras.api import _tf_keras\", \"\")\n+    with open(os.path.join(\"keras\", \"api\", \"__init__.py\"), \"w\") as f:\n+        f.write(contents)\n+\n+    with open(os.path.join(\"keras\", \"_tf_keras\", \"__init__.py\")) as f:\n+        contents = f.read()\n+        contents = contents.replace(\n+            \"from keras.api._tf_keras import keras\",\n+            \"from keras._tf_keras import keras\",\n+        )\n+    with open(os.path.join(\"keras\", \"_tf_keras\", \"__init__.py\"), \"w\") as f:\n+        f.write(contents)\n+\n+\n def build_and_save_output(root_path, __version__):\n     # Build the package\n     os.system(\"python3 -m build\")\n@@ -85,13 +136,6 @@ def build_and_save_output(root_path, __version__):\n     return whl_path\n \n \n-def build(root_path, is_nightly=False, rc_index=None):\n-    from keras.src.version import __version__  # noqa: E402\n-\n-    export_version_string(__version__, is_nightly, rc_index)\n-    return build_and_save_output(root_path, __version__)\n-\n-\n def install_whl(whl_fpath):\n     print(f\"Installing wheel file: {whl_fpath}\")\n     os.system(f\"pip3 install {whl_fpath} --force-reinstall --no-dependencies\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3ebb36fce26c37b5095853a605f07c5f18b57597", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 261 | Lines Deleted: 12 | Files Changed: 12 | Hunks: 24 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 273 | Churn Cumulative: 2352 | Contributors (this commit): 7 | Commits (past 90d): 29 | Contributors (cumulative): 28 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,7 +4,6 @@ This file was autogenerated. Do not edit it by hand,\n since your modifications would be overwritten.\n \"\"\"\n \n-from keras.api import _tf_keras\n from keras.api import activations\n from keras.api import applications\n from keras.api import backend\n\n@@ -6,7 +6,6 @@ since your modifications would be overwritten.\n \n from keras.api import activations\n from keras.api import applications\n-from keras.api import backend\n from keras.api import callbacks\n from keras.api import config\n from keras.api import constraints\n@@ -15,19 +14,15 @@ from keras.api import distribution\n from keras.api import dtype_policies\n from keras.api import export\n from keras.api import initializers\n-from keras.api import layers\n from keras.api import legacy\n-from keras.api import losses\n from keras.api import metrics\n from keras.api import mixed_precision\n from keras.api import models\n from keras.api import ops\n from keras.api import optimizers\n-from keras.api import preprocessing\n from keras.api import quantizers\n from keras.api import random\n from keras.api import regularizers\n-from keras.api import saving\n from keras.api import tree\n from keras.api import utils\n from keras.src.backend.common.keras_tensor import KerasTensor\n@@ -37,6 +32,7 @@ from keras.src.backend.exports import device\n from keras.src.backend.exports import name_scope\n from keras.src.dtype_policies.dtype_policy import DTypePolicy\n from keras.src.dtype_policies.dtype_policy import FloatDTypePolicy\n+from keras.src.dtype_policies.dtype_policy import QuantizedDTypePolicy\n from keras.src.initializers.initializer import Initializer\n from keras.src.layers.core.input_layer import Input\n from keras.src.layers.input_spec import InputSpec\n@@ -48,7 +44,20 @@ from keras.src.models.sequential import Sequential\n from keras.src.ops.function import Function\n from keras.src.ops.operation import Operation\n from keras.src.optimizers.optimizer import Optimizer\n+from keras.src.quantizers.quantizers import AbsMaxQuantizer\n from keras.src.quantizers.quantizers import Quantizer\n from keras.src.regularizers.regularizers import Regularizer\n-from keras.src.version import __version__\n from keras.src.version import version\n+\n+\"\"\"DO NOT EDIT.\n+\n+This file was autogenerated. Do not edit it by hand,\n+since your modifications would be overwritten.\n+\"\"\"\n+\n+\n+from keras._tf_keras.keras import backend\n+from keras._tf_keras.keras import layers\n+from keras._tf_keras.keras import losses\n+from keras._tf_keras.keras import metrics\n+from keras._tf_keras.keras import preprocessing\n\n@@ -18,3 +18,134 @@ from keras.src.backend.config import set_epsilon\n from keras.src.backend.config import set_floatx\n from keras.src.backend.config import set_image_data_format\n from keras.src.utils.naming import get_uid\n+\n+\"\"\"DO NOT EDIT.\n+\n+This file was autogenerated. Do not edit it by hand,\n+since your modifications would be overwritten.\n+\"\"\"\n+\n+\n+from keras.src.legacy.backend import abs\n+from keras.src.legacy.backend import all\n+from keras.src.legacy.backend import any\n+from keras.src.legacy.backend import arange\n+from keras.src.legacy.backend import argmax\n+from keras.src.legacy.backend import argmin\n+from keras.src.legacy.backend import batch_dot\n+from keras.src.legacy.backend import batch_flatten\n+from keras.src.legacy.backend import batch_get_value\n+from keras.src.legacy.backend import batch_normalization\n+from keras.src.legacy.backend import batch_set_value\n+from keras.src.legacy.backend import bias_add\n+from keras.src.legacy.backend import binary_crossentropy\n+from keras.src.legacy.backend import binary_focal_crossentropy\n+from keras.src.legacy.backend import cast\n+from keras.src.legacy.backend import cast_to_floatx\n+from keras.src.legacy.backend import categorical_crossentropy\n+from keras.src.legacy.backend import categorical_focal_crossentropy\n+from keras.src.legacy.backend import clip\n+from keras.src.legacy.backend import concatenate\n+from keras.src.legacy.backend import constant\n+from keras.src.legacy.backend import conv1d\n+from keras.src.legacy.backend import conv2d\n+from keras.src.legacy.backend import conv2d_transpose\n+from keras.src.legacy.backend import conv3d\n+from keras.src.legacy.backend import cos\n+from keras.src.legacy.backend import count_params\n+from keras.src.legacy.backend import ctc_batch_cost\n+from keras.src.legacy.backend import ctc_decode\n+from keras.src.legacy.backend import ctc_label_dense_to_sparse\n+from keras.src.legacy.backend import cumprod\n+from keras.src.legacy.backend import cumsum\n+from keras.src.legacy.backend import depthwise_conv2d\n+from keras.src.legacy.backend import dot\n+from keras.src.legacy.backend import dropout\n+from keras.src.legacy.backend import dtype\n+from keras.src.legacy.backend import elu\n+from keras.src.legacy.backend import equal\n+from keras.src.legacy.backend import eval\n+from keras.src.legacy.backend import exp\n+from keras.src.legacy.backend import expand_dims\n+from keras.src.legacy.backend import eye\n+from keras.src.legacy.backend import flatten\n+from keras.src.legacy.backend import foldl\n+from keras.src.legacy.backend import foldr\n+from keras.src.legacy.backend import gather\n+from keras.src.legacy.backend import get_value\n+from keras.src.legacy.backend import gradients\n+from keras.src.legacy.backend import greater\n+from keras.src.legacy.backend import greater_equal\n+from keras.src.legacy.backend import hard_sigmoid\n+from keras.src.legacy.backend import in_top_k\n+from keras.src.legacy.backend import int_shape\n+from keras.src.legacy.backend import is_sparse\n+from keras.src.legacy.backend import l2_normalize\n+from keras.src.legacy.backend import less\n+from keras.src.legacy.backend import less_equal\n+from keras.src.legacy.backend import log\n+from keras.src.legacy.backend import map_fn\n+from keras.src.legacy.backend import max\n+from keras.src.legacy.backend import maximum\n+from keras.src.legacy.backend import mean\n+from keras.src.legacy.backend import min\n+from keras.src.legacy.backend import minimum\n+from keras.src.legacy.backend import moving_average_update\n+from keras.src.legacy.backend import name_scope\n+from keras.src.legacy.backend import ndim\n+from keras.src.legacy.backend import not_equal\n+from keras.src.legacy.backend import one_hot\n+from keras.src.legacy.backend import ones\n+from keras.src.legacy.backend import ones_like\n+from keras.src.legacy.backend import permute_dimensions\n+from keras.src.legacy.backend import pool2d\n+from keras.src.legacy.backend import pool3d\n+from keras.src.legacy.backend import pow\n+from keras.src.legacy.backend import prod\n+from keras.src.legacy.backend import random_bernoulli\n+from keras.src.legacy.backend import random_normal\n+from keras.src.legacy.backend import random_normal_variable\n+from keras.src.legacy.backend import random_uniform\n+from keras.src.legacy.backend import random_uniform_variable\n+from keras.src.legacy.backend import relu\n+from keras.src.legacy.backend import repeat\n+from keras.src.legacy.backend import repeat_elements\n+from keras.src.legacy.backend import reshape\n+from keras.src.legacy.backend import resize_images\n+from keras.src.legacy.backend import resize_volumes\n+from keras.src.legacy.backend import reverse\n+from keras.src.legacy.backend import rnn\n+from keras.src.legacy.backend import round\n+from keras.src.legacy.backend import separable_conv2d\n+from keras.src.legacy.backend import set_value\n+from keras.src.legacy.backend import shape\n+from keras.src.legacy.backend import sigmoid\n+from keras.src.legacy.backend import sign\n+from keras.src.legacy.backend import sin\n+from keras.src.legacy.backend import softmax\n+from keras.src.legacy.backend import softplus\n+from keras.src.legacy.backend import softsign\n+from keras.src.legacy.backend import sparse_categorical_crossentropy\n+from keras.src.legacy.backend import spatial_2d_padding\n+from keras.src.legacy.backend import spatial_3d_padding\n+from keras.src.legacy.backend import sqrt\n+from keras.src.legacy.backend import square\n+from keras.src.legacy.backend import squeeze\n+from keras.src.legacy.backend import stack\n+from keras.src.legacy.backend import std\n+from keras.src.legacy.backend import stop_gradient\n+from keras.src.legacy.backend import sum\n+from keras.src.legacy.backend import switch\n+from keras.src.legacy.backend import tanh\n+from keras.src.legacy.backend import temporal_padding\n+from keras.src.legacy.backend import tile\n+from keras.src.legacy.backend import to_dense\n+from keras.src.legacy.backend import transpose\n+from keras.src.legacy.backend import truncated_normal\n+from keras.src.legacy.backend import update\n+from keras.src.legacy.backend import update_add\n+from keras.src.legacy.backend import update_sub\n+from keras.src.legacy.backend import var\n+from keras.src.legacy.backend import variable\n+from keras.src.legacy.backend import zeros\n+from keras.src.legacy.backend import zeros_like\n\n@@ -193,3 +193,15 @@ from keras.src.layers.rnn.time_distributed import TimeDistributed\n from keras.src.utils.jax_layer import FlaxLayer\n from keras.src.utils.jax_layer import JaxLayer\n from keras.src.utils.torch_utils import TorchModuleWrapper\n+\n+\"\"\"DO NOT EDIT.\n+\n+This file was autogenerated. Do not edit it by hand,\n+since your modifications would be overwritten.\n+\"\"\"\n+\n+\n+from keras.src.legacy.layers import AlphaDropout\n+from keras.src.legacy.layers import RandomHeight\n+from keras.src.legacy.layers import RandomWidth\n+from keras.src.legacy.layers import ThresholdedReLU\n\n@@ -8,9 +8,9 @@ from keras.src.losses import deserialize\n from keras.src.losses import get\n from keras.src.losses import serialize\n from keras.src.losses.loss import Loss\n-from keras.src.losses.losses import CTC\n from keras.src.losses.losses import BinaryCrossentropy\n from keras.src.losses.losses import BinaryFocalCrossentropy\n+from keras.src.losses.losses import CTC\n from keras.src.losses.losses import CategoricalCrossentropy\n from keras.src.losses.losses import CategoricalFocalCrossentropy\n from keras.src.losses.losses import CategoricalHinge\n@@ -48,3 +48,24 @@ from keras.src.losses.losses import poisson\n from keras.src.losses.losses import sparse_categorical_crossentropy\n from keras.src.losses.losses import squared_hinge\n from keras.src.losses.losses import tversky\n+\n+\"\"\"DO NOT EDIT.\n+\n+This file was autogenerated. Do not edit it by hand,\n+since your modifications would be overwritten.\n+\"\"\"\n+\n+\n+from keras.src.legacy.losses import Reduction\n+from keras.src.losses.losses import kl_divergence as KLD\n+from keras.src.losses.losses import kl_divergence as kld\n+from keras.src.losses.losses import kl_divergence as kullback_leibler_divergence\n+from keras.src.losses.losses import log_cosh as logcosh\n+from keras.src.losses.losses import mean_absolute_error as MAE\n+from keras.src.losses.losses import mean_absolute_error as mae\n+from keras.src.losses.losses import mean_absolute_percentage_error as MAPE\n+from keras.src.losses.losses import mean_absolute_percentage_error as mape\n+from keras.src.losses.losses import mean_squared_error as MSE\n+from keras.src.losses.losses import mean_squared_error as mse\n+from keras.src.losses.losses import mean_squared_logarithmic_error as MSLE\n+from keras.src.losses.losses import mean_squared_logarithmic_error as msle\n\n@@ -74,3 +74,23 @@ from keras.src.metrics.regression_metrics import MeanSquaredError\n from keras.src.metrics.regression_metrics import MeanSquaredLogarithmicError\n from keras.src.metrics.regression_metrics import R2Score\n from keras.src.metrics.regression_metrics import RootMeanSquaredError\n+\n+\"\"\"DO NOT EDIT.\n+\n+This file was autogenerated. Do not edit it by hand,\n+since your modifications would be overwritten.\n+\"\"\"\n+\n+\n+from keras.src.losses.losses import kl_divergence as KLD\n+from keras.src.losses.losses import kl_divergence as kld\n+from keras.src.losses.losses import kl_divergence as kullback_leibler_divergence\n+from keras.src.losses.losses import log_cosh as logcosh\n+from keras.src.losses.losses import mean_absolute_error as MAE\n+from keras.src.losses.losses import mean_absolute_error as mae\n+from keras.src.losses.losses import mean_absolute_percentage_error as MAPE\n+from keras.src.losses.losses import mean_absolute_percentage_error as mape\n+from keras.src.losses.losses import mean_squared_error as MSE\n+from keras.src.losses.losses import mean_squared_error as mse\n+from keras.src.losses.losses import mean_squared_logarithmic_error as MSLE\n+from keras.src.losses.losses import mean_squared_logarithmic_error as msle\n\n@@ -4,10 +4,21 @@ This file was autogenerated. Do not edit it by hand,\n since your modifications would be overwritten.\n \"\"\"\n \n-from keras.api.preprocessing import image\n-from keras.api.preprocessing import sequence\n+from keras._tf_keras.keras.preprocessing import image\n+from keras._tf_keras.keras.preprocessing import sequence\n from keras.src.utils.image_dataset_utils import image_dataset_from_directory\n from keras.src.utils.text_dataset_utils import text_dataset_from_directory\n from keras.src.utils.timeseries_dataset_utils import (\n     timeseries_dataset_from_array,\n )\n+\n+\"\"\"DO NOT EDIT.\n+\n+This file was autogenerated. Do not edit it by hand,\n+since your modifications would be overwritten.\n+\"\"\"\n+\n+\n+from keras._tf_keras.keras.preprocessing import image\n+from keras._tf_keras.keras.preprocessing import sequence\n+from keras._tf_keras.keras.preprocessing import text\n\n@@ -9,3 +9,24 @@ from keras.src.utils.image_utils import img_to_array\n from keras.src.utils.image_utils import load_img\n from keras.src.utils.image_utils import save_img\n from keras.src.utils.image_utils import smart_resize\n+\n+\"\"\"DO NOT EDIT.\n+\n+This file was autogenerated. Do not edit it by hand,\n+since your modifications would be overwritten.\n+\"\"\"\n+\n+\n+from keras.src.legacy.preprocessing.image import DirectoryIterator\n+from keras.src.legacy.preprocessing.image import ImageDataGenerator\n+from keras.src.legacy.preprocessing.image import Iterator\n+from keras.src.legacy.preprocessing.image import NumpyArrayIterator\n+from keras.src.legacy.preprocessing.image import apply_affine_transform\n+from keras.src.legacy.preprocessing.image import apply_brightness_shift\n+from keras.src.legacy.preprocessing.image import apply_channel_shift\n+from keras.src.legacy.preprocessing.image import random_brightness\n+from keras.src.legacy.preprocessing.image import random_channel_shift\n+from keras.src.legacy.preprocessing.image import random_rotation\n+from keras.src.legacy.preprocessing.image import random_shear\n+from keras.src.legacy.preprocessing.image import random_shift\n+from keras.src.legacy.preprocessing.image import random_zoom\n\n@@ -5,3 +5,14 @@ since your modifications would be overwritten.\n \"\"\"\n \n from keras.src.utils.sequence_utils import pad_sequences\n+\n+\"\"\"DO NOT EDIT.\n+\n+This file was autogenerated. Do not edit it by hand,\n+since your modifications would be overwritten.\n+\"\"\"\n+\n+\n+from keras.src.legacy.preprocessing.sequence import TimeseriesGenerator\n+from keras.src.legacy.preprocessing.sequence import make_sampling_table\n+from keras.src.legacy.preprocessing.sequence import skipgrams\n\n@@ -0,0 +1,11 @@\n+\"\"\"DO NOT EDIT.\n+\n+This file was autogenerated. Do not edit it by hand,\n+since your modifications would be overwritten.\n+\"\"\"\n+\n+from keras.src.legacy.preprocessing.text import Tokenizer\n+from keras.src.legacy.preprocessing.text import hashing_trick\n+from keras.src.legacy.preprocessing.text import one_hot\n+from keras.src.legacy.preprocessing.text import text_to_word_sequence\n+from keras.src.legacy.preprocessing.text import tokenizer_from_json\n\n@@ -1,7 +1,7 @@\n from keras.src.api_export import keras_export\n \n # Unique source of truth for the version number.\n-__version__ = \"3.3.1\"\n+__version__ = \"3.3.2\"\n \n \n @keras_export(\"keras.version\")\n\n@@ -84,8 +84,11 @@ def build(root_path, is_nightly=False, rc_index=None):\n     try:\n         copy_source_to_build_directory(root_path)\n         create_legacy_directory()\n-        from keras.src.version import __version__  # noqa: E402\n+        print(os.getcwd())\n \n+        # from keras.src.version import __version__  # noqa: E402\n+\n+        __version__ = \"3.3.2\"\n         export_version_string(__version__, is_nightly, rc_index)\n         return build_and_save_output(root_path, __version__)\n     finally:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a070252632b5c19eb06e7a95b18f2ef0c0550158", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 10 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -135,8 +135,8 @@ def load_data(\n         xs = [[w for w in x if skip_top <= w < num_words] for x in xs]\n \n     idx = len(x_train)\n-    x_train, y_train = xs[:idx], labels[:idx]\n-    x_test, y_test = xs[idx:], labels[idx:]\n+    x_train, y_train = np.array(xs[:idx], dtype=\"object\"), labels[:idx]\n+    x_test, y_test = np.array(xs[idx:], dtype=\"object\"), labels[idx:]\n     return (x_train, y_train), (x_test, y_test)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1ac17cc143b48bdbb23c4bedb82f752601a4dddb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 68 | Lines Deleted: 130 | Files Changed: 11 | Hunks: 34 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 198 | Churn Cumulative: 2878 | Contributors (this commit): 7 | Commits (past 90d): 39 | Contributors (cumulative): 26 | DMM Complexity: 0.2\n\nDIFF:\n@@ -7,6 +7,7 @@ It generates API and formats user and generated APIs.\n \"\"\"\n \n import os\n+import re\n import shutil\n \n import namex\n@@ -78,8 +79,7 @@ def create_legacy_directory(package_dir):\n         for path in os.listdir(os.path.join(src_dir, \"legacy\"))\n         if os.path.isdir(os.path.join(src_dir, \"legacy\", path))\n     ]\n-\n-    for root, _, fnames in os.walk(os.path.join(package_dir, \"_legacy\")):\n+    for root, _, fnames in os.walk(os.path.join(api_dir, \"_legacy\")):\n         for fname in fnames:\n             if fname.endswith(\".py\"):\n                 legacy_fpath = os.path.join(root, fname)\n@@ -110,6 +110,20 @@ def create_legacy_directory(package_dir):\n                             f\"keras.api.{legacy_submodule}\",\n                             f\"keras.api._tf_keras.keras.{legacy_submodule}\",\n                         )\n+                    # Remove duplicate generated comments string.\n+                    legacy_contents = re.sub(r\"\\n\", r\"\\\\n\", legacy_contents)\n+                    legacy_contents = re.sub('\"\"\".*\"\"\"', \"\", legacy_contents)\n+                    legacy_contents = re.sub(r\"\\\\n\", r\"\\n\", legacy_contents)\n+                    # If the same module is in legacy and core_api, use legacy\n+                    legacy_imports = re.findall(\n+                        r\"import (\\w+)\", legacy_contents\n+                    )\n+                    for import_name in legacy_imports:\n+                        core_api_contents = re.sub(\n+                            f\"\\n.* import {import_name}\\n\",\n+                            r\"\\n\",\n+                            core_api_contents,\n+                        )\n                     legacy_contents = core_api_contents + \"\\n\" + legacy_contents\n                 with open(tf_keras_fpath, \"w\") as f:\n                     f.write(legacy_contents)\n\n@@ -4,6 +4,7 @@ This file was autogenerated. Do not edit it by hand,\n since your modifications would be overwritten.\n \"\"\"\n \n+from keras.api import _tf_keras\n from keras.api import activations\n from keras.api import applications\n from keras.api import backend\n\n@@ -15,7 +15,6 @@ from keras.api import dtype_policies\n from keras.api import export\n from keras.api import initializers\n from keras.api import legacy\n-from keras.api import metrics\n from keras.api import mixed_precision\n from keras.api import models\n from keras.api import ops\n@@ -25,6 +24,11 @@ from keras.api import random\n from keras.api import regularizers\n from keras.api import tree\n from keras.api import utils\n+from keras.api._tf_keras.keras import backend\n+from keras.api._tf_keras.keras import layers\n+from keras.api._tf_keras.keras import losses\n+from keras.api._tf_keras.keras import metrics\n+from keras.api._tf_keras.keras import preprocessing\n from keras.src.backend.common.keras_tensor import KerasTensor\n from keras.src.backend.common.stateless_scope import StatelessScope\n from keras.src.backend.exports import Variable\n@@ -32,7 +36,6 @@ from keras.src.backend.exports import device\n from keras.src.backend.exports import name_scope\n from keras.src.dtype_policies.dtype_policy import DTypePolicy\n from keras.src.dtype_policies.dtype_policy import FloatDTypePolicy\n-from keras.src.dtype_policies.dtype_policy import QuantizedDTypePolicy\n from keras.src.initializers.initializer import Initializer\n from keras.src.layers.core.input_layer import Input\n from keras.src.layers.input_spec import InputSpec\n@@ -44,20 +47,7 @@ from keras.src.models.sequential import Sequential\n from keras.src.ops.function import Function\n from keras.src.ops.operation import Operation\n from keras.src.optimizers.optimizer import Optimizer\n-from keras.src.quantizers.quantizers import AbsMaxQuantizer\n from keras.src.quantizers.quantizers import Quantizer\n from keras.src.regularizers.regularizers import Regularizer\n+from keras.src.version import __version__\n from keras.src.version import version\n-\n-\"\"\"DO NOT EDIT.\n-\n-This file was autogenerated. Do not edit it by hand,\n-since your modifications would be overwritten.\n-\"\"\"\n-\n-\n-from keras._tf_keras.keras import backend\n-from keras._tf_keras.keras import layers\n-from keras._tf_keras.keras import losses\n-from keras._tf_keras.keras import metrics\n-from keras._tf_keras.keras import preprocessing\n\n@@ -17,15 +17,6 @@ from keras.src.backend.config import image_data_format\n from keras.src.backend.config import set_epsilon\n from keras.src.backend.config import set_floatx\n from keras.src.backend.config import set_image_data_format\n-from keras.src.utils.naming import get_uid\n-\n-\"\"\"DO NOT EDIT.\n-\n-This file was autogenerated. Do not edit it by hand,\n-since your modifications would be overwritten.\n-\"\"\"\n-\n-\n from keras.src.legacy.backend import abs\n from keras.src.legacy.backend import all\n from keras.src.legacy.backend import any\n@@ -149,3 +140,4 @@ from keras.src.legacy.backend import var\n from keras.src.legacy.backend import variable\n from keras.src.legacy.backend import zeros\n from keras.src.legacy.backend import zeros_like\n+from keras.src.utils.naming import get_uid\n\n@@ -157,7 +157,6 @@ from keras.src.layers.preprocessing.text_vectorization import TextVectorization\n from keras.src.layers.regularization.activity_regularization import (\n     ActivityRegularization,\n )\n-from keras.src.layers.regularization.alpha_dropout import AlphaDropout\n from keras.src.layers.regularization.dropout import Dropout\n from keras.src.layers.regularization.gaussian_dropout import GaussianDropout\n from keras.src.layers.regularization.gaussian_noise import GaussianNoise\n@@ -190,18 +189,10 @@ from keras.src.layers.rnn.simple_rnn import SimpleRNN\n from keras.src.layers.rnn.simple_rnn import SimpleRNNCell\n from keras.src.layers.rnn.stacked_rnn_cells import StackedRNNCells\n from keras.src.layers.rnn.time_distributed import TimeDistributed\n-from keras.src.utils.jax_layer import FlaxLayer\n-from keras.src.utils.jax_layer import JaxLayer\n-from keras.src.utils.torch_utils import TorchModuleWrapper\n-\n-\"\"\"DO NOT EDIT.\n-\n-This file was autogenerated. Do not edit it by hand,\n-since your modifications would be overwritten.\n-\"\"\"\n-\n-\n from keras.src.legacy.layers import AlphaDropout\n from keras.src.legacy.layers import RandomHeight\n from keras.src.legacy.layers import RandomWidth\n from keras.src.legacy.layers import ThresholdedReLU\n+from keras.src.utils.jax_layer import FlaxLayer\n+from keras.src.utils.jax_layer import JaxLayer\n+from keras.src.utils.torch_utils import TorchModuleWrapper\n\n@@ -4,13 +4,14 @@ This file was autogenerated. Do not edit it by hand,\n since your modifications would be overwritten.\n \"\"\"\n \n+from keras.src.legacy.losses import Reduction\n from keras.src.losses import deserialize\n from keras.src.losses import get\n from keras.src.losses import serialize\n from keras.src.losses.loss import Loss\n+from keras.src.losses.losses import CTC\n from keras.src.losses.losses import BinaryCrossentropy\n from keras.src.losses.losses import BinaryFocalCrossentropy\n-from keras.src.losses.losses import CTC\n from keras.src.losses.losses import CategoricalCrossentropy\n from keras.src.losses.losses import CategoricalFocalCrossentropy\n from keras.src.losses.losses import CategoricalHinge\n@@ -38,25 +39,6 @@ from keras.src.losses.losses import ctc\n from keras.src.losses.losses import dice\n from keras.src.losses.losses import hinge\n from keras.src.losses.losses import huber\n-from keras.src.losses.losses import kl_divergence\n-from keras.src.losses.losses import log_cosh\n-from keras.src.losses.losses import mean_absolute_error\n-from keras.src.losses.losses import mean_absolute_percentage_error\n-from keras.src.losses.losses import mean_squared_error\n-from keras.src.losses.losses import mean_squared_logarithmic_error\n-from keras.src.losses.losses import poisson\n-from keras.src.losses.losses import sparse_categorical_crossentropy\n-from keras.src.losses.losses import squared_hinge\n-from keras.src.losses.losses import tversky\n-\n-\"\"\"DO NOT EDIT.\n-\n-This file was autogenerated. Do not edit it by hand,\n-since your modifications would be overwritten.\n-\"\"\"\n-\n-\n-from keras.src.legacy.losses import Reduction\n from keras.src.losses.losses import kl_divergence as KLD\n from keras.src.losses.losses import kl_divergence as kld\n from keras.src.losses.losses import kl_divergence as kullback_leibler_divergence\n@@ -69,3 +51,7 @@ from keras.src.losses.losses import mean_squared_error as MSE\n from keras.src.losses.losses import mean_squared_error as mse\n from keras.src.losses.losses import mean_squared_logarithmic_error as MSLE\n from keras.src.losses.losses import mean_squared_logarithmic_error as msle\n+from keras.src.losses.losses import poisson\n+from keras.src.losses.losses import sparse_categorical_crossentropy\n+from keras.src.losses.losses import squared_hinge\n+from keras.src.losses.losses import tversky\n\n@@ -11,12 +11,18 @@ from keras.src.losses.losses import categorical_focal_crossentropy\n from keras.src.losses.losses import categorical_hinge\n from keras.src.losses.losses import hinge\n from keras.src.losses.losses import huber\n-from keras.src.losses.losses import kl_divergence\n-from keras.src.losses.losses import log_cosh\n-from keras.src.losses.losses import mean_absolute_error\n-from keras.src.losses.losses import mean_absolute_percentage_error\n-from keras.src.losses.losses import mean_squared_error\n-from keras.src.losses.losses import mean_squared_logarithmic_error\n+from keras.src.losses.losses import kl_divergence as KLD\n+from keras.src.losses.losses import kl_divergence as kld\n+from keras.src.losses.losses import kl_divergence as kullback_leibler_divergence\n+from keras.src.losses.losses import log_cosh as logcosh\n+from keras.src.losses.losses import mean_absolute_error as MAE\n+from keras.src.losses.losses import mean_absolute_error as mae\n+from keras.src.losses.losses import mean_absolute_percentage_error as MAPE\n+from keras.src.losses.losses import mean_absolute_percentage_error as mape\n+from keras.src.losses.losses import mean_squared_error as MSE\n+from keras.src.losses.losses import mean_squared_error as mse\n+from keras.src.losses.losses import mean_squared_logarithmic_error as MSLE\n+from keras.src.losses.losses import mean_squared_logarithmic_error as msle\n from keras.src.losses.losses import poisson\n from keras.src.losses.losses import sparse_categorical_crossentropy\n from keras.src.losses.losses import squared_hinge\n@@ -74,23 +80,3 @@ from keras.src.metrics.regression_metrics import MeanSquaredError\n from keras.src.metrics.regression_metrics import MeanSquaredLogarithmicError\n from keras.src.metrics.regression_metrics import R2Score\n from keras.src.metrics.regression_metrics import RootMeanSquaredError\n-\n-\"\"\"DO NOT EDIT.\n-\n-This file was autogenerated. Do not edit it by hand,\n-since your modifications would be overwritten.\n-\"\"\"\n-\n-\n-from keras.src.losses.losses import kl_divergence as KLD\n-from keras.src.losses.losses import kl_divergence as kld\n-from keras.src.losses.losses import kl_divergence as kullback_leibler_divergence\n-from keras.src.losses.losses import log_cosh as logcosh\n-from keras.src.losses.losses import mean_absolute_error as MAE\n-from keras.src.losses.losses import mean_absolute_error as mae\n-from keras.src.losses.losses import mean_absolute_percentage_error as MAPE\n-from keras.src.losses.losses import mean_absolute_percentage_error as mape\n-from keras.src.losses.losses import mean_squared_error as MSE\n-from keras.src.losses.losses import mean_squared_error as mse\n-from keras.src.losses.losses import mean_squared_logarithmic_error as MSLE\n-from keras.src.losses.losses import mean_squared_logarithmic_error as msle\n\n@@ -4,21 +4,11 @@ This file was autogenerated. Do not edit it by hand,\n since your modifications would be overwritten.\n \"\"\"\n \n-from keras._tf_keras.keras.preprocessing import image\n-from keras._tf_keras.keras.preprocessing import sequence\n+from keras.api._tf_keras.keras.preprocessing import image\n+from keras.api._tf_keras.keras.preprocessing import sequence\n+from keras.api._tf_keras.keras.preprocessing import text\n from keras.src.utils.image_dataset_utils import image_dataset_from_directory\n from keras.src.utils.text_dataset_utils import text_dataset_from_directory\n from keras.src.utils.timeseries_dataset_utils import (\n     timeseries_dataset_from_array,\n )\n-\n-\"\"\"DO NOT EDIT.\n-\n-This file was autogenerated. Do not edit it by hand,\n-since your modifications would be overwritten.\n-\"\"\"\n-\n-\n-from keras._tf_keras.keras.preprocessing import image\n-from keras._tf_keras.keras.preprocessing import sequence\n-from keras._tf_keras.keras.preprocessing import text\n\n@@ -4,19 +4,6 @@ This file was autogenerated. Do not edit it by hand,\n since your modifications would be overwritten.\n \"\"\"\n \n-from keras.src.utils.image_utils import array_to_img\n-from keras.src.utils.image_utils import img_to_array\n-from keras.src.utils.image_utils import load_img\n-from keras.src.utils.image_utils import save_img\n-from keras.src.utils.image_utils import smart_resize\n-\n-\"\"\"DO NOT EDIT.\n-\n-This file was autogenerated. Do not edit it by hand,\n-since your modifications would be overwritten.\n-\"\"\"\n-\n-\n from keras.src.legacy.preprocessing.image import DirectoryIterator\n from keras.src.legacy.preprocessing.image import ImageDataGenerator\n from keras.src.legacy.preprocessing.image import Iterator\n@@ -30,3 +17,8 @@ from keras.src.legacy.preprocessing.image import random_rotation\n from keras.src.legacy.preprocessing.image import random_shear\n from keras.src.legacy.preprocessing.image import random_shift\n from keras.src.legacy.preprocessing.image import random_zoom\n+from keras.src.utils.image_utils import array_to_img\n+from keras.src.utils.image_utils import img_to_array\n+from keras.src.utils.image_utils import load_img\n+from keras.src.utils.image_utils import save_img\n+from keras.src.utils.image_utils import smart_resize\n\n@@ -4,15 +4,7 @@ This file was autogenerated. Do not edit it by hand,\n since your modifications would be overwritten.\n \"\"\"\n \n-from keras.src.utils.sequence_utils import pad_sequences\n-\n-\"\"\"DO NOT EDIT.\n-\n-This file was autogenerated. Do not edit it by hand,\n-since your modifications would be overwritten.\n-\"\"\"\n-\n-\n from keras.src.legacy.preprocessing.sequence import TimeseriesGenerator\n from keras.src.legacy.preprocessing.sequence import make_sampling_table\n from keras.src.legacy.preprocessing.sequence import skipgrams\n+from keras.src.utils.sequence_utils import pad_sequences\n\n@@ -83,7 +83,7 @@ def build(root_path, is_nightly=False, rc_index=None):\n \n     try:\n         copy_source_to_build_directory(root_path)\n-        create_legacy_directory()\n+        move_tf_keras_directory()\n         print(os.getcwd())\n \n         # from keras.src.version import __version__  # noqa: E402\n@@ -96,21 +96,25 @@ def build(root_path, is_nightly=False, rc_index=None):\n         shutil.rmtree(build_directory)\n \n \n-def create_legacy_directory():\n-    shutil.move(os.path.join(\"keras\", \"api\", \"_tf_keras\"), \"keras\")\n-    with open(os.path.join(\"keras\", \"api\", \"__init__.py\")) as f:\n+def move_tf_keras_directory():\n+    \"\"\"Move `keras/api/_tf_keras` to `keras/_tf_keras`, update references.\"\"\"\n+    shutil.move(os.path.join(package, \"api\", \"_tf_keras\"), \"keras\")\n+    with open(os.path.join(package, \"api\", \"__init__.py\")) as f:\n         contents = f.read()\n         contents = contents.replace(\"from keras.api import _tf_keras\", \"\")\n-    with open(os.path.join(\"keras\", \"api\", \"__init__.py\"), \"w\") as f:\n+    with open(os.path.join(package, \"api\", \"__init__.py\"), \"w\") as f:\n         f.write(contents)\n-\n-    with open(os.path.join(\"keras\", \"_tf_keras\", \"__init__.py\")) as f:\n+    # Replace `keras.api._tf_keras` with `keras._tf_keras`.\n+    for root, _, fnames in os.walk(os.path.join(package, \"_tf_keras\")):\n+        for fname in fnames:\n+            if fname.endswith(\".py\"):\n+                tf_keras_fpath = os.path.join(root, fname)\n+                with open(tf_keras_fpath) as f:\n                     contents = f.read()\n                     contents = contents.replace(\n-            \"from keras.api._tf_keras import keras\",\n-            \"from keras._tf_keras import keras\",\n+                        \"keras.api._tf_keras\", \"keras._tf_keras\"\n                     )\n-    with open(os.path.join(\"keras\", \"_tf_keras\", \"__init__.py\"), \"w\") as f:\n+                with open(tf_keras_fpath, \"w\") as f:\n                     f.write(contents)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
