{"custom_id": "keras#770e95e6dffbca54ce27d322697a879e67a0bafd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 127 | Lines Deleted: 118 | Files Changed: 54 | Hunks: 134 | Methods Changed: 24 | Complexity Δ (Sum/Max): -6/25 | Churn Δ: 245 | Churn Cumulative: 18502 | Contributors (this commit): 44 | Commits (past 90d): 176 | Contributors (cumulative): 218 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,5 @@\n-import tree\n-\n from keras.api_export import keras_export\n+from keras.utils import tree\n from keras.utils.naming import auto_name\n \n \n\n@@ -3,7 +3,6 @@ import jax.experimental.sparse as jax_sparse\n import jax.numpy as jnp\n import ml_dtypes\n import numpy as np\n-import tree\n \n from keras.backend.common import KerasVariable\n from keras.backend.common import global_state\n@@ -11,6 +10,7 @@ from keras.backend.common import standardize_dtype\n from keras.backend.common.keras_tensor import KerasTensor\n from keras.backend.common.stateless_scope import StatelessScope\n from keras.backend.jax import distribution_lib\n+from keras.utils import tree\n \n SUPPORTS_SPARSE_TENSORS = True\n \n\n@@ -1,11 +1,10 @@\n import contextlib\n \n-import tree\n from jax import lax\n from jax import numpy as jnp\n \n from keras.backend.common import stateless_scope\n-from keras.utils.nest import pack_sequence_as\n+from keras.utils import tree\n \n \n def rnn(\n@@ -86,7 +85,7 @@ def rnn(\n \n         def _get_input_tensor(time):\n             inp = [t_[time] for t_ in processed_input]\n-            return pack_sequence_as(inputs, inp)\n+            return tree.pack_sequence_as(inputs, inp)\n \n         if mask is not None:\n             mask_list = unstack(mask)\n@@ -119,7 +118,7 @@ def rnn(\n                         tiled_mask_t, flat_new_states, flat_states\n                     )\n                 )\n-                states = pack_sequence_as(states, flat_final_states)\n+                states = tree.pack_sequence_as(states, flat_final_states)\n \n                 if return_all_outputs:\n                     successive_outputs.append(output)\n\n@@ -4,7 +4,6 @@ from functools import partial\n \n import jax\n import numpy as np\n-import tree\n \n from keras import backend\n from keras import callbacks as callbacks_module\n@@ -16,6 +15,7 @@ from keras.trainers import trainer as base_trainer\n from keras.trainers.data_adapters import data_adapter_utils\n from keras.trainers.epoch_iterator import EpochIterator\n from keras.utils import traceback_utils\n+from keras.utils import tree\n \n \n class JAXTrainer(base_trainer.Trainer):\n\n@@ -1,12 +1,11 @@\n import numpy as np\n-import tree\n \n from keras.backend.common import KerasVariable\n from keras.backend.common import standardize_dtype\n from keras.backend.common.dtypes import result_type\n from keras.backend.common.keras_tensor import KerasTensor\n from keras.backend.common.stateless_scope import StatelessScope\n-from keras.utils.nest import pack_sequence_as\n+from keras.utils import tree\n \n SUPPORTS_SPARSE_TENSORS = False\n \n@@ -130,7 +129,7 @@ def compute_output_spec(fn, *args, **kwargs):\n                     if e != shape[i]:\n                         shape[i] = None\n                 flat_out.append(KerasTensor(shape, standardize_dtype(x1.dtype)))\n-            outputs = pack_sequence_as(outputs_1, flat_out)\n+            outputs = tree.pack_sequence_as(outputs_1, flat_out)\n \n         def convert_numpy_to_keras_tensor(x):\n             if is_tensor(x):\n\n@@ -1,11 +1,11 @@\n import numpy as np\n-import tree\n \n from keras.backend import config\n from keras.backend import standardize_dtype\n from keras.backend.common import dtypes\n from keras.backend.common.backend_utils import standardize_axis_for_numpy\n from keras.backend.numpy.core import convert_to_tensor\n+from keras.utils import tree\n \n \n def add(x1, x2):\n\n@@ -1,7 +1,6 @@\n import numpy as np\n-import tree\n \n-from keras.utils.nest import pack_sequence_as\n+from keras.utils import tree\n \n \n def rnn(\n@@ -82,7 +81,7 @@ def rnn(\n \n         def _get_input_tensor(time):\n             inp = [t_[time] for t_ in processed_input]\n-            return pack_sequence_as(inputs, inp)\n+            return tree.pack_sequence_as(inputs, inp)\n \n         if mask is not None:\n             mask_list = unstack(mask)\n@@ -115,7 +114,7 @@ def rnn(\n                         tiled_mask_t, flat_new_states, flat_states\n                     )\n                 )\n-                states = pack_sequence_as(states, flat_final_states)\n+                states = tree.pack_sequence_as(states, flat_final_states)\n \n                 if return_all_outputs:\n                     successive_outputs.append(output)\n\n@@ -1,5 +1,4 @@\n import numpy as np\n-import tree\n \n from keras import backend\n from keras import callbacks as callbacks_module\n@@ -10,6 +9,7 @@ from keras.trainers import trainer as base_trainer\n from keras.trainers.data_adapters import data_adapter_utils\n from keras.trainers.epoch_iterator import EpochIterator\n from keras.utils import traceback_utils\n+from keras.utils import tree\n \n \n class NumpyTrainer(base_trainer.Trainer):\n\n@@ -1,7 +1,6 @@\n import tensorflow as tf\n-import tree\n \n-from keras.utils.nest import pack_sequence_as\n+from keras.utils import tree\n \n \n def rnn(\n@@ -156,7 +155,7 @@ def rnn(\n \n         def _get_input_tensor(time):\n             inp = [t_[time] for t_ in processed_input]\n-            return pack_sequence_as(inputs, inp)\n+            return tree.pack_sequence_as(inputs, inp)\n \n         if mask is not None:\n             mask_list = tf.unstack(mask)\n@@ -189,7 +188,7 @@ def rnn(\n                         tiled_mask_t, flat_new_states, flat_states\n                     )\n                 )\n-                states = pack_sequence_as(states, flat_final_states)\n+                states = tree.pack_sequence_as(states, flat_final_states)\n \n                 if return_all_outputs:\n                     successive_outputs.append(output)\n@@ -255,7 +254,7 @@ def rnn(\n         # Get the time(0) input and compute the output for that, the output will\n         # be used to determine the dtype of output tensor array. Don't read from\n         # input_ta due to TensorArray clear_after_read default to True.\n-        input_time_zero = pack_sequence_as(\n+        input_time_zero = tree.pack_sequence_as(\n             inputs, [inp[0] for inp in flattened_inputs]\n         )\n         # output_time_zero is used to determine the cell output shape and its\n@@ -353,7 +352,7 @@ def rnn(\n                 \"\"\"\n                 current_input = tuple(ta.read(time) for ta in input_ta)\n                 # maybe set shape.\n-                current_input = pack_sequence_as(inputs, current_input)\n+                current_input = tree.pack_sequence_as(inputs, current_input)\n                 mask_t = masking_fn(time)\n                 output, new_states = step_function(\n                     current_input, tuple(states) + tuple(constants)\n@@ -375,7 +374,7 @@ def rnn(\n                 flat_final_state = compute_masked_output(\n                     mask_t, flat_new_state, flat_state\n                 )\n-                new_states = pack_sequence_as(new_states, flat_final_state)\n+                new_states = tree.pack_sequence_as(new_states, flat_final_state)\n \n                 ta_index_to_write = time if return_all_outputs else 0\n                 output_ta_t = tuple(\n@@ -408,7 +407,7 @@ def rnn(\n                     Tuple: `(time + 1,output_ta_t) + tuple(new_states)`\n                 \"\"\"\n                 current_input = tuple(ta.read(time) for ta in input_ta)\n-                current_input = pack_sequence_as(inputs, current_input)\n+                current_input = tree.pack_sequence_as(inputs, current_input)\n                 output, new_states = step_function(\n                     current_input, tuple(states) + tuple(constants)\n                 )\n@@ -421,7 +420,9 @@ def rnn(\n                     for ta, out in zip(output_ta_t, flat_output)\n                 )\n \n-                new_states = pack_sequence_as(initial_states, flat_new_state)\n+                new_states = tree.pack_sequence_as(\n+                    initial_states, flat_new_state\n+                )\n                 return (time + 1, output_ta_t) + tuple(new_states)\n \n             final_outputs = tf.while_loop(\n@@ -436,8 +437,8 @@ def rnn(\n         outputs = tuple(o.stack() for o in output_ta)\n         last_output = tuple(o[-1] for o in outputs)\n \n-        outputs = pack_sequence_as(output_time_zero, outputs)\n-        last_output = pack_sequence_as(output_time_zero, last_output)\n+        outputs = tree.pack_sequence_as(output_time_zero, outputs)\n+        last_output = tree.pack_sequence_as(output_time_zero, last_output)\n \n     if not time_major:\n         outputs = tree.map_structure(swap_batch_timestep, outputs)\n\n@@ -3,7 +3,6 @@ import warnings\n \n import numpy as np\n import tensorflow as tf\n-import tree\n from packaging.version import Version\n from tensorflow.python.eager import context as tf_context\n \n@@ -14,6 +13,7 @@ from keras.trainers import trainer as base_trainer\n from keras.trainers.data_adapters import data_adapter_utils\n from keras.trainers.epoch_iterator import EpochIterator\n from keras.utils import traceback_utils\n+from keras.utils import tree\n \n \n class TensorFlowTrainer(base_trainer.Trainer):\n\n@@ -4,7 +4,6 @@ import os\n import ml_dtypes\n import numpy as np\n import torch\n-import tree\n \n from keras.backend.common import KerasVariable\n from keras.backend.common import global_state\n@@ -13,7 +12,7 @@ from keras.backend.common.dtypes import result_type\n from keras.backend.common.keras_tensor import KerasTensor\n from keras.backend.common.stateless_scope import StatelessScope\n from keras.backend.config import floatx\n-from keras.utils.nest import pack_sequence_as\n+from keras.utils import tree\n \n SUPPORTS_SPARSE_TENSORS = False\n \n@@ -322,7 +321,7 @@ def compute_output_spec(fn, *args, **kwargs):\n                     if e != shape[i]:\n                         shape[i] = None\n                 flat_out.append(KerasTensor(shape, standardize_dtype(x1.dtype)))\n-            outputs = pack_sequence_as(outputs_1, flat_out)\n+            outputs = tree.pack_sequence_as(outputs_1, flat_out)\n \n         output_spec = tree.map_structure(convert_torch_to_keras_tensor, outputs)\n     return output_spec\n\n@@ -1,6 +1,5 @@\n import torch\n import torch.nn.functional as tnn\n-import tree\n \n from keras.backend import standardize_data_format\n from keras.backend import standardize_dtype\n@@ -14,6 +13,7 @@ from keras.backend.torch.core import get_device\n from keras.backend.torch.numpy import expand_dims\n from keras.backend.torch.numpy import maximum\n from keras.backend.torch.numpy import where\n+from keras.utils import tree\n from keras.utils.argument_validation import standardize_tuple\n \n \n\n@@ -1,8 +1,7 @@\n import torch\n-import tree\n \n from keras.backend.torch.core import convert_to_tensor\n-from keras.utils.nest import pack_sequence_as\n+from keras.utils import tree\n \n \n def rnn(\n@@ -86,7 +85,7 @@ def rnn(\n \n         def _get_input_tensor(time):\n             inp = [t_[time] for t_ in processed_input]\n-            return pack_sequence_as(inputs, inp)\n+            return tree.pack_sequence_as(inputs, inp)\n \n         if mask is not None:\n             mask_list = torch.unbind(mask)\n@@ -119,7 +118,7 @@ def rnn(\n                         tiled_mask_t, flat_new_states, flat_states\n                     )\n                 )\n-                states = pack_sequence_as(states, flat_final_states)\n+                states = tree.pack_sequence_as(states, flat_final_states)\n \n                 if return_all_outputs:\n                     successive_outputs.append(output)\n@@ -176,7 +175,7 @@ def rnn(\n         )\n \n         # Get the time(0) input and compute the output for that.\n-        input_time_zero = pack_sequence_as(\n+        input_time_zero = tree.pack_sequence_as(\n             inputs, [inp[0] for inp in flattened_inputs]\n         )\n         # output_time_zero is used to determine the cell output shape.\n@@ -265,7 +264,7 @@ def rnn(\n                 \"\"\"\n                 current_input = tuple(ta[time] for ta in input_ta)\n                 # maybe set shape.\n-                current_input = pack_sequence_as(inputs, current_input)\n+                current_input = tree.pack_sequence_as(inputs, current_input)\n                 mask_t = masking_fn(time)\n                 output, new_states = step_function(\n                     current_input, tuple(states) + tuple(constants)\n@@ -287,7 +286,7 @@ def rnn(\n                 flat_final_state = compute_masked_output(\n                     mask_t, flat_new_state, flat_state\n                 )\n-                new_states = pack_sequence_as(new_states, flat_final_state)\n+                new_states = tree.pack_sequence_as(new_states, flat_final_state)\n \n                 ta_index_to_write = time if return_all_outputs else 0\n                 for ta, out in zip(output_ta_t, flat_new_output):\n@@ -325,7 +324,7 @@ def rnn(\n                     Tuple: `(time + 1,output_ta_t) + tuple(new_states)`\n                 \"\"\"\n                 current_input = tuple(ta[time] for ta in input_ta)\n-                current_input = pack_sequence_as(inputs, current_input)\n+                current_input = tree.pack_sequence_as(inputs, current_input)\n                 output, new_states = step_function(\n                     current_input, tuple(states) + tuple(constants)\n                 )\n@@ -336,7 +335,9 @@ def rnn(\n                 for ta, out in zip(output_ta_t, flat_output):\n                     ta[ta_index_to_write] = out\n \n-                new_states = pack_sequence_as(initial_states, flat_new_state)\n+                new_states = tree.pack_sequence_as(\n+                    initial_states, flat_new_state\n+                )\n                 return (time + 1, output_ta_t) + tuple(new_states)\n \n             it = 0\n@@ -361,8 +362,8 @@ def rnn(\n         outputs = tuple(_stack(o) for o in output_ta)\n         last_output = tuple(o[-1] for o in outputs)\n \n-        outputs = pack_sequence_as(output_time_zero, outputs)\n-        last_output = pack_sequence_as(output_time_zero, last_output)\n+        outputs = tree.pack_sequence_as(output_time_zero, outputs)\n+        last_output = tree.pack_sequence_as(output_time_zero, last_output)\n \n     if not time_major:\n         outputs = tree.map_structure(swap_batch_timestep, outputs)\n\n@@ -2,7 +2,6 @@ import warnings\n \n import numpy as np\n import torch\n-import tree\n from packaging.version import parse\n \n from keras import backend\n@@ -12,6 +11,7 @@ from keras.trainers import trainer as base_trainer\n from keras.trainers.data_adapters import data_adapter_utils\n from keras.trainers.epoch_iterator import EpochIterator\n from keras.utils import traceback_utils\n+from keras.utils import tree\n \n \n class TorchTrainer(base_trainer.Trainer):\n\n@@ -1,9 +1,8 @@\n-import tree\n-\n from keras.api_export import keras_export\n from keras.callbacks.callback import Callback\n from keras.callbacks.history import History\n from keras.callbacks.progbar_logger import ProgbarLogger\n+from keras.utils import tree\n \n \n @keras_export(\"keras.callbacks.CallbackList\")\n\n@@ -4,8 +4,6 @@ import sys\n import time\n import warnings\n \n-import tree\n-\n from keras import backend\n from keras import ops\n from keras.api_export import keras_export\n@@ -13,6 +11,7 @@ from keras.callbacks.callback import Callback\n from keras.layers import Embedding\n from keras.optimizers import Optimizer\n from keras.utils import file_utils\n+from keras.utils import tree\n \n \n @keras_export(\"keras.callbacks.TensorBoard\")\n\n@@ -1,8 +1,7 @@\n-import tree\n-\n from keras.api_export import keras_export\n from keras.backend import KerasTensor\n from keras.layers.layer import Layer\n+from keras.utils import tree\n \n \n @keras_export(\"keras.layers.Identity\")\n\n@@ -1,14 +1,13 @@\n import inspect\n import types\n \n-import tree\n-\n from keras import backend\n from keras.api_export import keras_export\n from keras.layers.layer import Layer\n from keras.saving import serialization_lib\n from keras.utils import python_utils\n from keras.utils import shape_utils\n+from keras.utils import tree\n \n \n @keras_export(\"keras.layers.Lambda\")\n\n@@ -1,7 +1,6 @@\n-import tree\n-\n from keras import backend\n from keras.api_export import keras_export\n+from keras.utils import tree\n \n \n @keras_export([\"keras.InputSpec\", \"keras.layers.InputSpec\"])\n\n@@ -21,8 +21,6 @@ import inspect\n import warnings\n from functools import wraps\n \n-import tree\n-\n from keras import backend\n from keras import constraints\n from keras import dtype_policies\n@@ -41,6 +39,7 @@ from keras.utils import python_utils\n from keras.utils import summary_utils\n from keras.utils import traceback_utils\n from keras.utils import tracking\n+from keras.utils import tree\n from keras.utils.shape_utils import map_shape_structure\n \n if backend.backend() == \"tensorflow\":\n\n@@ -1,5 +1,3 @@\n-import tree\n-\n from keras import backend\n from keras import layers\n from keras.api_export import keras_export\n@@ -7,6 +5,7 @@ from keras.layers.layer import Layer\n from keras.saving import saving_lib\n from keras.saving import serialization_lib\n from keras.utils import backend_utils\n+from keras.utils import tree\n from keras.utils.module_utils import tensorflow as tf\n from keras.utils.naming import auto_name\n \n\n@@ -1,10 +1,9 @@\n-import tree\n-\n import keras.backend\n from keras.layers.layer import Layer\n from keras.random.seed_generator import SeedGenerator\n from keras.utils import backend_utils\n from keras.utils import tracking\n+from keras.utils import tree\n \n \n class TFDataLayer(Layer):\n\n@@ -1,5 +1,3 @@\n-import tree\n-\n from keras import activations\n from keras import backend\n from keras import constraints\n@@ -12,6 +10,7 @@ from keras.layers.rnn.dropout_rnn_cell import DropoutRNNCell\n from keras.layers.rnn.rnn import RNN\n from keras.ops import operation_utils\n from keras.utils import argument_validation\n+from keras.utils import tree\n \n \n class ConvLSTMCell(Layer, DropoutRNNCell):\n\n@@ -1,5 +1,3 @@\n-import tree\n-\n from keras import activations\n from keras import backend\n from keras import constraints\n@@ -11,6 +9,7 @@ from keras.layers.input_spec import InputSpec\n from keras.layers.layer import Layer\n from keras.layers.rnn.dropout_rnn_cell import DropoutRNNCell\n from keras.layers.rnn.rnn import RNN\n+from keras.utils import tree\n \n \n @keras_export(\"keras.layers.GRUCell\")\n\n@@ -1,5 +1,3 @@\n-import tree\n-\n from keras import activations\n from keras import backend\n from keras import constraints\n@@ -11,6 +9,7 @@ from keras.layers.input_spec import InputSpec\n from keras.layers.layer import Layer\n from keras.layers.rnn.dropout_rnn_cell import DropoutRNNCell\n from keras.layers.rnn.rnn import RNN\n+from keras.utils import tree\n \n \n @keras_export(\"keras.layers.LSTMCell\")\n\n@@ -1,5 +1,3 @@\n-import tree\n-\n from keras import backend\n from keras import ops\n from keras.api_export import keras_export\n@@ -8,6 +6,7 @@ from keras.layers.rnn.dropout_rnn_cell import DropoutRNNCell\n from keras.layers.rnn.stacked_rnn_cells import StackedRNNCells\n from keras.saving import serialization_lib\n from keras.utils import tracking\n+from keras.utils import tree\n \n \n @keras_export(\"keras.layers.RNN\")\n\n@@ -1,9 +1,8 @@\n-import tree\n-\n from keras import ops\n from keras.api_export import keras_export\n from keras.layers.layer import Layer\n from keras.saving import serialization_lib\n+from keras.utils import tree\n \n \n @keras_export(\"keras.layers.StackedRNNCells\")\n\n@@ -1,7 +1,6 @@\n import json\n import threading\n \n-import tree\n from absl import logging\n \n from keras import backend\n@@ -12,6 +11,7 @@ from keras import models\n from keras import optimizers\n from keras.legacy.saving import serialization\n from keras.saving import object_registration\n+from keras.utils import tree\n \n MODULE_OBJECTS = threading.local()\n \n\n@@ -1,8 +1,7 @@\n-import tree\n-\n from keras import backend\n from keras import ops\n from keras.api_export import keras_export\n+from keras.utils import tree\n from keras.utils.naming import auto_name\n \n \n\n@@ -1,5 +1,3 @@\n-import tree\n-\n from keras import backend\n from keras import utils\n from keras.api_export import keras_export\n@@ -9,6 +7,7 @@ from keras.models.functional import Functional\n from keras.models.functional import functional_like_constructor\n from keras.models.sequential import Sequential\n from keras.saving import serialization_lib\n+from keras.utils import tree\n \n \n @keras_export(\"keras.models.clone_model\")\n\n@@ -1,12 +1,12 @@\n import numpy as np\n import pytest\n-import tree\n from absl.testing import parameterized\n \n from keras import layers\n from keras import models\n from keras import testing\n from keras.models.cloning import clone_model\n+from keras.utils import tree\n \n \n def get_mlp_functional_model(shared_layers=False):\n\n@@ -2,8 +2,6 @@ import copy\n import inspect\n import warnings\n \n-import tree\n-\n from keras import backend\n from keras import ops\n from keras.backend.common import global_state\n@@ -20,7 +18,7 @@ from keras.ops.function import make_node_key\n from keras.ops.node import Node\n from keras.saving import serialization_lib\n from keras.utils import tracking\n-from keras.utils.nest import pack_sequence_as\n+from keras.utils import tree\n \n \n class Functional(Function, Model):\n@@ -758,7 +756,7 @@ def clone_graph_nodes(inputs, outputs):\n             op_id_mapping[id(kt_input._keras_history[0])] = (\n                 cloned_input._keras_history[0]\n             )\n-    cloned_inputs = pack_sequence_as(inputs, cloned_inputs)\n+    cloned_inputs = tree.pack_sequence_as(inputs, cloned_inputs)\n \n     for kt_output in tree.flatten(outputs):\n         cpy = clone_single_keras_tensor(kt_output)\n@@ -766,7 +764,7 @@ def clone_graph_nodes(inputs, outputs):\n         cpy._keras_history = kt_output._keras_history\n         cloned_outputs.append(cpy)\n         kt_id_mapping[id(kt_output)] = cpy\n-    cloned_outputs = pack_sequence_as(outputs, cloned_outputs)\n+    cloned_outputs = tree.pack_sequence_as(outputs, cloned_outputs)\n \n     for node in nodes_to_clone:\n         if id(node.operation) in op_id_mapping:\n\n@@ -1,8 +1,6 @@\n import copy\n import inspect\n \n-import tree\n-\n from keras.api_export import keras_export\n from keras.backend.common import global_state\n from keras.layers.core.input_layer import InputLayer\n@@ -12,6 +10,7 @@ from keras.legacy.saving import serialization as legacy_serialization\n from keras.models.functional import Functional\n from keras.models.model import Model\n from keras.saving import serialization_lib\n+from keras.utils import tree\n \n \n @keras_export([\"keras.Sequential\", \"keras.models.Sequential\"])\n\n@@ -15,7 +15,6 @@ custom_gradient\n \"\"\"\n \n import numpy as np\n-import tree\n \n from keras import backend\n from keras.api_export import keras_export\n@@ -23,6 +22,7 @@ from keras.backend import KerasTensor\n from keras.backend import any_symbolic_tensors\n from keras.ops.operation import Operation\n from keras.utils import traceback_utils\n+from keras.utils import tree\n \n \n class Scatter(Operation):\n\n@@ -3,7 +3,6 @@ from unittest.mock import Mock\n \n import numpy as np\n import pytest\n-import tree\n from absl.testing import parameterized\n \n from keras import backend\n@@ -16,6 +15,7 @@ from keras import testing\n from keras.backend.common.keras_tensor import KerasTensor\n from keras.backend.common.variables import ALLOWED_DTYPES\n from keras.ops import core\n+from keras.utils import tree\n \n \n class CoreOpsStaticShapeTest(testing.TestCase):\n\n@@ -1,12 +1,10 @@\n import collections\n \n-import tree\n-\n from keras.api_export import keras_export\n from keras.backend import KerasTensor\n from keras.backend.config import backend\n from keras.ops.operation import Operation\n-from keras.utils.nest import pack_sequence_as\n+from keras.utils import tree\n \n \n @keras_export(\"keras.Function\")\n@@ -160,7 +158,7 @@ class Function(Operation):\n         for x in self.outputs:\n             output_tensors.append(tensor_dict[id(x)])\n \n-        return pack_sequence_as(self._outputs_struct, output_tensors)\n+        return tree.pack_sequence_as(self._outputs_struct, output_tensors)\n \n     def _assert_input_compatibility(self, inputs):\n         try:\n\n@@ -1,9 +1,8 @@\n import collections\n \n-import tree\n-\n from keras.backend import KerasTensor\n from keras.ops.symbolic_arguments import SymbolicArguments\n+from keras.utils import tree\n \n \n class Node:\n\n@@ -1,8 +1,6 @@\n import inspect\n import textwrap\n \n-import tree\n-\n from keras import backend\n from keras import dtype_policies\n from keras.api_export import keras_export\n@@ -10,6 +8,7 @@ from keras.backend.common.keras_tensor import any_symbolic_tensors\n from keras.ops.node import Node\n from keras.utils import python_utils\n from keras.utils import traceback_utils\n+from keras.utils import tree\n from keras.utils.naming import auto_name\n \n \n\n@@ -1,11 +1,11 @@\n import math\n \n import numpy as np\n-import tree\n \n from keras.api_export import keras_export\n from keras.backend.common.backend_utils import canonicalize_axis\n from keras.backend.common.backend_utils import to_tuple_or_list\n+from keras.utils import tree\n \n \n def broadcast_shapes(shape1, shape2):\n\n@@ -1,6 +1,5 @@\n-import tree\n-\n from keras.backend import KerasTensor\n+from keras.utils import tree\n \n \n class SymbolicArguments:\n\n@@ -1,8 +1,7 @@\n-import tree\n-\n from keras import testing\n from keras.backend import KerasTensor\n from keras.ops.symbolic_arguments import SymbolicArguments\n+from keras.utils import tree\n \n \n class SymbolicArgumentsTest(testing.TestCase):\n\n@@ -4,7 +4,6 @@ import tempfile\n import unittest\n \n import numpy as np\n-import tree\n \n from keras import backend\n from keras import ops\n@@ -15,6 +14,7 @@ from keras.backend.common.global_state import clear_session\n from keras.backend.common.keras_tensor import KerasTensor\n from keras.models import Model\n from keras.utils import traceback_utils\n+from keras.utils import tree\n from keras.utils.shape_utils import map_shape_structure\n \n \n\n@@ -1,9 +1,8 @@\n-import tree\n-\n from keras import backend\n from keras import losses as losses_module\n from keras import metrics as metrics_module\n from keras import ops\n+from keras.utils import tree\n from keras.utils.naming import get_object_name\n \n \n\n@@ -1,13 +1,12 @@\n import math\n \n import numpy as np\n-import tree\n \n from keras import backend\n from keras.trainers.data_adapters import data_adapter_utils\n from keras.trainers.data_adapters.data_adapter import DataAdapter\n+from keras.utils import tree\n from keras.utils.dataset_utils import is_torch_tensor\n-from keras.utils.nest import lists_to_tuples\n \n try:\n     import pandas\n@@ -410,7 +409,7 @@ def convert_to_arrays(arrays):\n         return x\n \n     arrays = tree.map_structure(convert_single_array, arrays)\n-    return lists_to_tuples(arrays)\n+    return tree.lists_to_tuples(arrays)\n \n \n def is_tf_ragged_tensor(x):\n\n@@ -1,10 +1,10 @@\n import math\n \n import numpy as np\n-import tree\n \n from keras import backend\n from keras.api_export import keras_export\n+from keras.utils import tree\n from keras.utils.dataset_utils import is_torch_tensor\n \n try:\n\n@@ -1,11 +1,11 @@\n import itertools\n \n import numpy as np\n-import tree\n \n from keras import backend\n from keras.trainers.data_adapters import data_adapter_utils\n from keras.trainers.data_adapters.data_adapter import DataAdapter\n+from keras.utils import tree\n \n \n class GeneratorDataAdapter(DataAdapter):\n\n@@ -8,12 +8,12 @@ import weakref\n from contextlib import closing\n \n import numpy as np\n-import tree\n \n from keras import backend\n from keras.api_export import keras_export\n from keras.trainers.data_adapters import data_adapter_utils\n from keras.trainers.data_adapters.data_adapter import DataAdapter\n+from keras.utils import tree\n \n \n @keras_export([\"keras.utils.PyDataset\", \"keras.utils.Sequence\"])\n\n@@ -1,7 +1,6 @@\n-import tree\n-\n from keras.trainers.data_adapters import data_adapter_utils\n from keras.trainers.data_adapters.data_adapter import DataAdapter\n+from keras.utils import tree\n \n \n class TFDatasetAdapter(DataAdapter):\n\n@@ -1,9 +1,9 @@\n import numpy as np\n-import tree\n \n from keras import backend\n from keras.trainers.data_adapters import data_adapter_utils\n from keras.trainers.data_adapters.data_adapter import DataAdapter\n+from keras.utils import tree\n \n \n class TorchDataLoaderAdapter(DataAdapter):\n\n@@ -1,8 +1,6 @@\n import platform\n import warnings\n \n-import tree\n-\n from keras import backend\n from keras import metrics as metrics_module\n from keras import ops\n@@ -14,6 +12,7 @@ from keras.trainers.compile_utils import CompileMetrics\n from keras.trainers.data_adapters import data_adapter_utils\n from keras.utils import traceback_utils\n from keras.utils import tracking\n+from keras.utils import tree\n \n \n class Trainer:\n\n@@ -98,12 +98,17 @@ def set_backend(backend):\n     keras.config.set_backend(\"jax\")\n     ```\n \n-    Note that this will **NOT** convert the type of any already\n-    instantiated objects, except for the `keras` module itself.\n+    ⚠️ WARNING ⚠️: Using this function is dangerous and should be done\n+    carefully. Changing the backend will **NOT** convert\n+    the type of any already-instantiated objects.\n     Thus, any layers / tensors / etc. already created will no\n     longer be usable without errors. It is strongly recommended **not**\n     to keep around **any** Keras-originated objects instances created\n     before calling `set_backend()`.\n+\n+    This includes any function or class instance that uses any Keras\n+    functionality. All such code needs to be re-executed after calling\n+    `set_backend()`.\n     \"\"\"\n     os.environ[\"KERAS_BACKEND\"] = backend\n     # Clear module cache.\n\n@@ -9,11 +9,11 @@ import rich.markup\n # See https://github.com/keras-team/keras/issues/448\n # for below imports\n import rich.table\n-import tree\n \n from keras import backend\n from keras.utils import dtype_utils\n from keras.utils import io_utils\n+from keras.utils import tree\n \n \n def count_params(weights):\n\n@@ -4,11 +4,10 @@ import traceback\n import types\n from functools import wraps\n \n-import tree\n-\n from keras import backend\n from keras.api_export import keras_export\n from keras.backend.common import global_state\n+from keras.utils import tree\n \n _EXCLUDED_PATHS = (\n     os.path.abspath(os.path.join(__file__, \"..\", \"..\")),\n\n@@ -1,6 +1,42 @@\n import tree\n \n \n+def is_nested(structure):\n+    return tree.is_nested(structure)\n+\n+\n+def flatten(structure):\n+    return tree.flatten(structure)\n+\n+\n+def map_structure(func, *structures, **kwargs):\n+    return tree.map_structure(func, *structures, **kwargs)\n+\n+\n+def map_structure_up_to(shallow_structure, func, *structures, **kwargs):\n+    return tree.map_structure_up_to(\n+        shallow_structure, func, *structures, **kwargs\n+    )\n+\n+\n+def assert_same_structure(a, b, check_types=True):\n+    return tree.assert_same_structure(a, b, check_types=check_types)\n+\n+\n+def sequence_like(instance, args):\n+    \"\"\"Converts the sequence `args` to the same type as `instance`.\n+\n+    Args:\n+      instance: an instance of `tuple`, `list`, `namedtuple`, `dict`, or\n+          `collections.OrderedDict`.\n+      args: elements to be converted to the `instance` type.\n+\n+    Returns:\n+      `args` with the type of `instance`.\n+    \"\"\"\n+    return tree._sequence_like(instance, args)\n+\n+\n def pack_sequence_as(structure, flat_sequence, sequence_fn=None):\n     \"\"\"Implements sequence packing, i.e. nest.pack_sequence_as().\"\"\"\n     is_nested_fn = tree.is_nested\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#df705d4fc719ab617705197248804d689ad74767", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1697 | Contributors (this commit): 8 | Commits (past 90d): 11 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -4610,7 +4610,7 @@ class Sin(Operation):\n \n @keras_export([\"keras.ops.sin\", \"keras.ops.numpy.sin\"])\n def sin(x):\n-    \"\"\"Trigonomeric sine, element-wise.\n+    \"\"\"Trigonometric sine, element-wise.\n \n     Arguments:\n         x: Input tensor.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6a266b8d9746440088c85646dfde5653b7a606c6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 25 | Churn Cumulative: 2694 | Contributors (this commit): 11 | Commits (past 90d): 13 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -538,10 +538,13 @@ def softmax(x, axis=-1):\n     array([0.09003057, 0.24472847, 0.66524096], shape=(3,), dtype=float64)\n \n     \"\"\"\n-    if isinstance(axis, int) and backend.shape(x)[axis] == 1:\n+    # Don't use `backend.shape` since TensorFlow returns\n+    # symbolic tensors for unknown shape which can trigger\n+    # an error in TensorFlow graph execution.\n+    if isinstance(axis, int) and x.shape[axis] == 1:\n         warnings.warn(\n             f\"You are using a softmax over axis {axis} \"\n-            f\"of a tensor of shape {backend.shape(x)}. This axis \"\n+            f\"of a tensor of shape {x.shape}. This axis \"\n             \"has size 1. The softmax operation will always return \"\n             \"the value 1, which is likely not what you intended. \"\n             \"Did you mean to use a sigmoid instead?\"\n\n@@ -2,10 +2,12 @@ import numpy as np\n import pytest\n from absl.testing import parameterized\n \n+import keras\n from keras import backend\n from keras import layers\n from keras import losses\n from keras import models\n+from keras import ops\n from keras import testing\n from keras.backend.common import standardize_dtype\n from keras.backend.common.keras_tensor import KerasTensor\n@@ -84,6 +86,22 @@ class NNOpsDynamicShapeTest(testing.TestCase, parameterized.TestCase):\n         self.assertEqual(knn.softmax(x, axis=1).shape, (None, 2, 3))\n         self.assertEqual(knn.softmax(x, axis=-1).shape, (None, 2, 3))\n \n+    def test_softmax_in_graph(self):\n+        class SoftmaxLayer(keras.Layer):\n+            def call(self, x):\n+                return ops.softmax(x, axis=-1)\n+\n+        class Model(keras.Model):\n+            def __init__(self):\n+                x = keras.Input(shape=(None,))\n+                y = SoftmaxLayer()(x)\n+                super().__init__(inputs=x, outputs=y)\n+\n+        # Make sure Keras is able to compile the model graph\n+        model = Model()\n+        x = ops.array([[1.0, 2.0, 3.0, 4.0]])\n+        model.predict(x)\n+\n     def test_log_softmax(self):\n         x = KerasTensor([None, 2, 3])\n         self.assertEqual(knn.log_softmax(x).shape, (None, 2, 3))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c591329161302e9a413d2bde065f8fefffa7b921", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 32/32 | Churn Δ: 21 | Churn Cumulative: 358 | Contributors (this commit): 7 | Commits (past 90d): 6 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -519,14 +519,29 @@ class TestCase(unittest.TestCase):\n \n             if run_mixed_precision_check:\n                 layer = layer_cls(**{**init_kwargs, \"dtype\": \"mixed_float16\"})\n+                input_spec = tree.map_structure(\n+                    lambda spec: KerasTensor(\n+                        spec.shape,\n+                        dtype=(\n+                            layer.compute_dtype\n+                            if layer.autocast\n+                            and backend.is_float_dtype(spec.dtype)\n+                            else spec.dtype\n+                        ),\n+                    ),\n+                    keras_tensor_inputs,\n+                )\n                 if isinstance(input_data, dict):\n                     output_data = layer(**input_data, **call_kwargs)\n+                    output_spec = layer.compute_output_spec(**input_spec)\n                 else:\n                     output_data = layer(input_data, **call_kwargs)\n-                for tensor in tree.flatten(output_data):\n+                    output_spec = layer.compute_output_spec(input_spec)\n+                for tensor, spec in zip(\n+                    tree.flatten(output_data), tree.flatten(output_spec)\n+                ):\n                     dtype = standardize_dtype(tensor.dtype)\n-                    if is_float_dtype(dtype):\n-                        self.assertEqual(dtype, \"float16\")\n+                    self.assertEqual(dtype, spec.dtype)\n                 for weight in layer.weights:\n                     dtype = standardize_dtype(weight.dtype)\n                     if is_float_dtype(dtype):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2cb05218baa92accc151ea47f1a198f75508bb35", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 285 | Contributors (this commit): 7 | Commits (past 90d): 13 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -1429,7 +1429,7 @@ class CallSpec:\n                 tensor_args.append(value)\n                 tensor_arg_names.append(name)\n                 tensor_arg_dict[name] = value\n-            elif tree.is_nested(value):\n+            elif tree.is_nested(value) and len(value) > 0:\n                 flat_values = tree.flatten(value)\n                 if all(is_backend_tensor_or_symbolic(x) for x in flat_values):\n                     tensor_args.append(value)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#818c9fadd9cb1748f2b5545e8ef5f141526ec14e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 796 | Lines Deleted: 392 | Files Changed: 12 | Hunks: 65 | Methods Changed: 83 | Complexity Δ (Sum/Max): 90/90 | Churn Δ: 1188 | Churn Cumulative: 5512 | Contributors (this commit): 31 | Commits (past 90d): 69 | Contributors (cumulative): 64 | DMM Complexity: 0.4895833333333333\n\nDIFF:\n@@ -47,21 +47,23 @@ class Variable(KerasVariable):\n def convert_to_tensor(x, dtype=None, sparse=True):\n     if dtype is not None:\n         dtype = standardize_dtype(dtype)\n-    if isinstance(x, (jnp.ndarray, jax.Array)) and dtype == x.dtype:\n+    if isinstance(x, (jnp.ndarray, jax.Array)) and (\n+        dtype is None or x.dtype == dtype\n+    ):\n         # Skip the conversion early if the instance is already a JAX array.\n         # This is important in the multi-process context since jax.array(x) for\n         # an existing distributed jax array will raise error.\n         return x\n \n     if isinstance(x, Variable):\n-        if dtype and dtype != x.dtype:\n+        if dtype is not None and x.dtype != dtype:\n             return x.value.astype(dtype)\n         return x.value\n \n     if isinstance(x, jax_sparse.JAXSparse):\n         if sparse is not None and not sparse:\n             x = x.todense()\n-        elif dtype and dtype != x.dtype:\n+        elif dtype is not None and x.dtype != dtype:\n             return x.astype(dtype)\n         else:\n             return x\n\n@@ -12,6 +12,7 @@ from keras import optimizers as optimizers_module\n from keras.backend import distribution_lib as jax_distribution_lib\n from keras.distribution import distribution_lib\n from keras.trainers import trainer as base_trainer\n+from keras.trainers.data_adapters import array_slicing\n from keras.trainers.data_adapters import data_adapter_utils\n from keras.trainers.epoch_iterator import EpochIterator\n from keras.utils import traceback_utils\n@@ -330,7 +331,7 @@ class JAXTrainer(base_trainer.Trainer):\n                 x,\n                 y,\n                 sample_weight,\n-            ), validation_data = data_adapter_utils.train_validation_split(\n+            ), validation_data = array_slicing.train_validation_split(\n                 (x, y, sample_weight), validation_split=validation_split\n             )\n \n\n@@ -129,6 +129,8 @@ def convert_to_numpy(x):\n         x.set_shape(x_shape)\n     elif isinstance(x, tf.IndexedSlices):\n         x = tf.convert_to_tensor(x)\n+    elif isinstance(x, tf.RaggedTensor):\n+        x = x.to_tensor()\n     return np.asarray(x)\n \n \n@@ -161,6 +163,12 @@ def shape(x):\n \n def cast(x, dtype):\n     dtype = standardize_dtype(dtype)\n+    if isinstance(x, tf.SparseTensor):\n+        x_shape = x.shape\n+        x = tf.cast(x, dtype)\n+        x.set_shape(x_shape)\n+        return x\n+    else:\n         return tf.cast(x, dtype=dtype)\n \n \n\n@@ -10,6 +10,7 @@ from keras import callbacks as callbacks_module\n from keras import metrics as metrics_module\n from keras import optimizers as optimizers_module\n from keras.trainers import trainer as base_trainer\n+from keras.trainers.data_adapters import array_slicing\n from keras.trainers.data_adapters import data_adapter_utils\n from keras.trainers.epoch_iterator import EpochIterator\n from keras.utils import traceback_utils\n@@ -273,7 +274,7 @@ class TensorFlowTrainer(base_trainer.Trainer):\n                 x,\n                 y,\n                 sample_weight,\n-            ), validation_data = data_adapter_utils.train_validation_split(\n+            ), validation_data = array_slicing.train_validation_split(\n                 (x, y, sample_weight), validation_split=validation_split\n             )\n \n\n@@ -8,6 +8,7 @@ from keras import backend\n from keras import callbacks as callbacks_module\n from keras import optimizers as optimizers_module\n from keras.trainers import trainer as base_trainer\n+from keras.trainers.data_adapters import array_slicing\n from keras.trainers.data_adapters import data_adapter_utils\n from keras.trainers.epoch_iterator import EpochIterator\n from keras.utils import traceback_utils\n@@ -193,7 +194,7 @@ class TorchTrainer(base_trainer.Trainer):\n                 x,\n                 y,\n                 sample_weight,\n-            ), validation_data = data_adapter_utils.train_validation_split(\n+            ), validation_data = array_slicing.train_validation_split(\n                 (x, y, sample_weight), validation_split=validation_split\n             )\n \n\n@@ -1,17 +1,12 @@\n+import functools\n import math\n \n import numpy as np\n \n-from keras import backend\n+from keras.trainers.data_adapters import array_slicing\n from keras.trainers.data_adapters import data_adapter_utils\n from keras.trainers.data_adapters.data_adapter import DataAdapter\n from keras.utils import tree\n-from keras.utils.dataset_utils import is_torch_tensor\n-\n-try:\n-    import pandas\n-except ImportError:\n-    pandas = None\n \n \n class ArrayDataAdapter(DataAdapter):\n@@ -33,7 +28,6 @@ class ArrayDataAdapter(DataAdapter):\n                 f\"Received invalid types: x={x}\"\n             )\n \n-        x, y, sample_weight = convert_to_arrays((x, y, sample_weight))\n         if sample_weight is not None:\n             if class_weight is not None:\n                 raise ValueError(\n@@ -41,7 +35,17 @@ class ArrayDataAdapter(DataAdapter):\n                     \"at the same time.\"\n                 )\n             if tree.is_nested(y):\n-                if isinstance(sample_weight, np.ndarray):\n+                if isinstance(sample_weight, (list, tuple, dict)):\n+                    try:\n+                        tree.assert_same_structure(y, sample_weight)\n+                    except ValueError:\n+                        raise ValueError(\n+                            \"You should provide one `sample_weight` array per \"\n+                            \"output in `y`. The two structures did not match:\\n\"\n+                            f\"- y: {y}\\n\"\n+                            f\"- sample_weight: {sample_weight}\\n\"\n+                        )\n+                else:\n                     is_samplewise = len(sample_weight.shape) == 1 or (\n                         len(sample_weight.shape) == 2\n                         and sample_weight.shape[1] == 1\n@@ -59,16 +63,6 @@ class ArrayDataAdapter(DataAdapter):\n                     sample_weight = tree.map_structure(\n                         lambda _: sample_weight, y\n                     )\n-                else:\n-                    try:\n-                        tree.assert_same_structure(y, sample_weight)\n-                    except ValueError:\n-                        raise ValueError(\n-                            \"You should provide one `sample_weight` array per \"\n-                            \"output in `y`. The two structures did not match:\\n\"\n-                            f\"- y: {y}\\n\"\n-                            f\"- sample_weight: {sample_weight}\\n\"\n-                        )\n         if class_weight is not None:\n             if tree.is_nested(y):\n                 raise ValueError(\n@@ -97,29 +91,20 @@ class ArrayDataAdapter(DataAdapter):\n         self._shuffle = shuffle\n \n     def get_numpy_iterator(self):\n-        inputs = self._inputs\n-        if self._shuffle and self._shuffle != \"batch\":\n-            inputs = data_adapter_utils.sync_shuffle(\n-                inputs, num_samples=self._num_samples\n-            )\n-        for i in range(self._size):\n-            start = i * self._batch_size\n-            stop = min((i + 1) * self._batch_size, self._num_samples)\n-            if self._shuffle == \"batch\":\n-\n-                def slice_and_shuffle(x):\n-                    return data_adapter_utils.sync_shuffle(\n-                        x[start:stop], num_samples=(stop - start)\n+        inputs = array_slicing.convert_to_sliceable(\n+            self._inputs, target_backend=\"numpy\"\n         )\n \n-                yield tree.map_structure(slice_and_shuffle, inputs)\n-            else:\n-                yield tree.map_structure(lambda x: x[start:stop], inputs)\n+        def slice_and_convert_to_numpy(sliceable, indices=None):\n+            x = sliceable[indices]\n+            x = sliceable.convert_to_numpy(x)\n+            return x\n+\n+        return self._get_iterator(slice_and_convert_to_numpy, inputs)\n \n     def get_tf_dataset(self):\n         from keras.utils.module_utils import tensorflow as tf\n \n-        inputs = self._inputs\n         shuffle = self._shuffle\n         batch_size = self._batch_size\n         num_samples = self._num_samples\n@@ -203,14 +188,29 @@ class ArrayDataAdapter(DataAdapter):\n             Returns:\n                 A Dataset of input batches matching the batch indices.\n             \"\"\"\n+            inputs = array_slicing.convert_to_sliceable(\n+                self._inputs, target_backend=\"tensorflow\"\n+            )\n+            inputs = tree.lists_to_tuples(inputs)\n+\n             dataset = tf.data.Dataset.zip(\n                 (indices_dataset, tf.data.Dataset.from_tensors(inputs).repeat())\n             )\n \n             def grab_batch(i, data):\n-                return tree.map_structure(\n-                    lambda d: tf.gather(d, i, axis=0), data\n+\n+                def grab_one(x):\n+                    if isinstance(x, array_slicing.TensorflowSparseWrapper):\n+                        return array_slicing.slice_tensorflow_sparse_wrapper(\n+                            x, i\n                         )\n+                    if isinstance(x, (list, tuple, dict)):\n+                        return None\n+                    if tf.is_tensor(x):\n+                        return tf.gather(x, i, axis=0)\n+                    return x\n+\n+                return tree.traverse(grab_one, data)\n \n             dataset = dataset.map(\n                 grab_batch, num_parallel_calls=tf.data.AUTOTUNE\n@@ -230,15 +230,10 @@ class ArrayDataAdapter(DataAdapter):\n             return dataset\n \n         indices_dataset = indices_dataset.flat_map(slice_batch_indices)\n-\n-        dataset = slice_inputs(indices_dataset, inputs)\n-\n         if shuffle == \"batch\":\n+            indices_dataset = indices_dataset.map(tf.random.shuffle)\n \n-            def shuffle_batch(*batch):\n-                return tree.map_structure(tf.random.shuffle, batch)\n-\n-            dataset = dataset.map(shuffle_batch)\n+        dataset = slice_inputs(indices_dataset, self._inputs)\n \n         options = tf.data.Options()\n         options.experimental_distribute.auto_shard_policy = (\n@@ -248,7 +243,19 @@ class ArrayDataAdapter(DataAdapter):\n         return dataset.prefetch(tf.data.AUTOTUNE)\n \n     def get_jax_iterator(self):\n-        return data_adapter_utils.get_jax_iterator(self.get_numpy_iterator())\n+        from keras.backend.jax.core import convert_to_tensor\n+\n+        inputs = array_slicing.convert_to_sliceable(\n+            self._inputs, target_backend=\"jax\"\n+        )\n+\n+        def slice_and_convert_to_jax(sliceable, indices=None):\n+            x = sliceable[indices]\n+            x = sliceable.convert_to_jax_compatible(x)\n+            x = convert_to_tensor(x)\n+            return x\n+\n+        return self._get_iterator(slice_and_convert_to_jax, inputs)\n \n     def get_torch_dataloader(self):\n         import torch\n@@ -260,8 +267,11 @@ class ArrayDataAdapter(DataAdapter):\n                 self.array = array\n \n             def __getitems__(self, indices):\n-                def slice_and_convert(x):\n-                    return convert_to_tensor(np.take(x, indices, axis=0))\n+                def slice_and_convert(sliceable):\n+                    x = sliceable[indices]\n+                    x = sliceable.convert_to_torch_compatible(x)\n+                    x = convert_to_tensor(x)\n+                    return x\n \n                 return tree.map_structure(slice_and_convert, self.array)\n \n@@ -305,11 +315,34 @@ class ArrayDataAdapter(DataAdapter):\n         def no_op_collate(batch):\n             return batch\n \n-        dataset = ArrayDataset(self._inputs)\n+        inputs = array_slicing.convert_to_sliceable(\n+            self._inputs, target_backend=\"torch\"\n+        )\n+        dataset = ArrayDataset(inputs)\n         return torch.utils.data.DataLoader(\n             dataset, batch_sampler=batch_sampler, collate_fn=no_op_collate\n         )\n \n+    def _get_iterator(self, slice_and_convert_fn, inputs):\n+        global_permutation = None\n+        if self._shuffle and self._shuffle != \"batch\":\n+            global_permutation = np.random.permutation(self._num_samples)\n+\n+        for i in range(self._size):\n+            start = i * self._batch_size\n+            stop = min((i + 1) * self._batch_size, self._num_samples)\n+            if self._shuffle == \"batch\":\n+                indices = np.random.permutation(stop - start) + start\n+            elif self._shuffle:\n+                indices = global_permutation[start:stop]\n+            else:\n+                indices = slice(start, stop)\n+\n+            slice_indices_and_convert_fn = functools.partial(\n+                slice_and_convert_fn, indices=indices\n+            )\n+            yield tree.map_structure(slice_indices_and_convert_fn, inputs)\n+\n     @property\n     def num_batches(self):\n         return self._size\n@@ -338,79 +371,6 @@ def can_convert_arrays(arrays):\n         otherwise.\n     \"\"\"\n \n-    def can_convert_single_array(x):\n-        is_none = x is None\n-        known_type = isinstance(x, data_adapter_utils.ARRAY_TYPES)\n-        convertable_type = hasattr(x, \"__array__\")\n-        return is_none or known_type or convertable_type\n-\n     return all(\n-        tree.flatten(tree.map_structure(can_convert_single_array, arrays))\n+        tree.flatten(tree.map_structure(array_slicing.can_slice_array, arrays))\n     )\n-\n-\n-def convert_to_arrays(arrays):\n-    \"\"\"Process array-like inputs.\n-\n-    This function:\n-\n-    - Converts tf.Tensors to NumPy arrays.\n-    - Converts `pandas.Series` to `np.ndarray`\n-    - Converts `list`s to `tuple`s (for `tf.data` support).\n-\n-    Args:\n-        inputs: Structure of `Tensor`s, NumPy arrays, or tensor-like.\n-\n-    Returns:\n-        Structure of NumPy `ndarray`s.\n-    \"\"\"\n-\n-    def convert_single_array(x):\n-        if x is None:\n-            return x\n-        if pandas is not None:\n-            if isinstance(x, pandas.Series):\n-                x = np.expand_dims(x.to_numpy(), axis=-1)\n-            elif isinstance(x, pandas.DataFrame):\n-                x = x.to_numpy()\n-        if is_tf_ragged_tensor(x):\n-            from keras.utils.module_utils import tensorflow as tf\n-\n-            # Convert floats to floatx.\n-            if (\n-                backend.is_float_dtype(x.dtype)\n-                and not backend.standardize_dtype(x.dtype) == backend.floatx()\n-            ):\n-                x = tf.cast(x, backend.floatx())\n-            return x\n-        if not isinstance(x, np.ndarray):\n-            # Using `__array__` should handle `tf.Tensor`, `jax.np.ndarray`,\n-            # `torch.Tensor`, as well as any other tensor-like object that has\n-            # added numpy support.\n-            if hasattr(x, \"__array__\"):\n-                if is_torch_tensor(x):\n-                    x = x.cpu()\n-                x = np.asarray(x)\n-            else:\n-                raise ValueError(\n-                    \"Expected a NumPy array, tf.Tensor, tf.RaggedTensor, \"\n-                    \"jax.np.ndarray, torch.Tensor, Pandas Dataframe, or \"\n-                    \"Pandas Series. Received invalid input: \"\n-                    f\"{x} (of type {type(x)})\"\n-                )\n-        if x.dtype == object:\n-            return x\n-        # Convert floats to floatx.\n-        if (\n-            backend.is_float_dtype(x.dtype)\n-            and not backend.standardize_dtype(x.dtype) == backend.floatx()\n-        ):\n-            x = x.astype(backend.floatx())\n-        return x\n-\n-    arrays = tree.map_structure(convert_single_array, arrays)\n-    return tree.lists_to_tuples(arrays)\n-\n-\n-def is_tf_ragged_tensor(x):\n-    return x.__class__.__name__ == \"RaggedTensor\"\n\n@@ -1,7 +1,8 @@\n import jax\n+import jax.experimental.sparse as jax_sparse\n import numpy as np\n import pandas\n-import pytest\n+import scipy\n import tensorflow as tf\n import torch\n from absl.testing import parameterized\n@@ -13,29 +14,53 @@ from keras.trainers.data_adapters import array_data_adapter\n \n \n class TestArrayDataAdapter(testing.TestCase, parameterized.TestCase):\n-    def make_array(self, array_type, shape, dtype=\"float32\"):\n+    def make_array(self, array_type, shape, dtype):\n         x = np.array([[i] * shape[1] for i in range(shape[0])], dtype=dtype)\n         if array_type == \"np\":\n             return x\n         elif array_type == \"tf\":\n             return tf.constant(x)\n+        elif array_type == \"tf_ragged\":\n+            return tf.RaggedTensor.from_tensor(x)\n+        elif array_type == \"tf_sparse\":\n+            return tf.sparse.from_dense(x)\n         elif array_type == \"jax\":\n             return jax.numpy.array(x)\n+        elif array_type == \"jax_sparse\":\n+            return jax_sparse.BCOO.fromdense(x)\n         elif array_type == \"torch\":\n             return torch.as_tensor(x)\n-        elif array_type == \"pandas\":\n+        elif array_type == \"pandas_data_frame\":\n             return pandas.DataFrame(x)\n+        elif array_type == \"pandas_series\":\n+            return pandas.Series(x[:, 0])\n+        elif array_type == \"scipy_sparse\":\n+            return scipy.sparse.coo_matrix(x)\n \n     @parameterized.named_parameters(\n         named_product(\n-            array_type=[\"np\", \"tf\", \"jax\", \"torch\", \"pandas\"],\n+            array_type=[\n+                \"np\",\n+                \"tf\",\n+                \"tf_ragged\",\n+                \"tf_sparse\",\n+                \"jax\",\n+                \"jax_sparse\",\n+                \"torch\",\n+                \"pandas_data_frame\",\n+                \"pandas_series\",\n+                \"scipy_sparse\",\n+            ],\n+            array_dtype=[\"float32\", \"float64\"],\n             iterator_type=[\"np\", \"tf\", \"jax\", \"torch\"],\n             shuffle=[False, \"batch\", True],\n         )\n     )\n-    def test_basic_flow(self, array_type, iterator_type, shuffle):\n-        x = self.make_array(array_type, (34, 4))\n-        y = self.make_array(array_type, (34, 2))\n+    def test_basic_flow(self, array_type, array_dtype, iterator_type, shuffle):\n+        x = self.make_array(array_type, (34, 4), array_dtype)\n+        y = self.make_array(array_type, (34, 2), \"int32\")\n+        xdim1 = 1 if array_type == \"pandas_series\" else 4\n+        ydim1 = 1 if array_type == \"pandas_series\" else 2\n \n         adapter = array_data_adapter.ArrayDataAdapter(\n             x,\n@@ -55,34 +80,63 @@ class TestArrayDataAdapter(testing.TestCase, parameterized.TestCase):\n             expected_class = np.ndarray\n         elif iterator_type == \"tf\":\n             it = adapter.get_tf_dataset()\n+            if array_type == \"tf_ragged\":\n+                expected_class = tf.RaggedTensor\n+                xdim1 = None\n+                ydim1 = None\n+            elif array_type in (\"tf_sparse\", \"jax_sparse\", \"scipy_sparse\"):\n+                expected_class = tf.SparseTensor\n+            else:\n                 expected_class = tf.Tensor\n         elif iterator_type == \"jax\":\n             it = adapter.get_jax_iterator()\n+            if array_type in (\"tf_sparse\", \"jax_sparse\", \"scipy_sparse\"):\n+                expected_class = jax_sparse.JAXSparse\n+            else:\n                 expected_class = jax.Array\n         elif iterator_type == \"torch\":\n             it = adapter.get_torch_dataloader()\n             expected_class = torch.Tensor\n \n-        sample_order = []\n+        x_order = []\n+        y_order = []\n         for i, batch in enumerate(it):\n             self.assertEqual(len(batch), 2)\n             bx, by = batch\n             self.assertIsInstance(bx, expected_class)\n             self.assertIsInstance(by, expected_class)\n-            self.assertEqual(bx.dtype, by.dtype)\n-            self.assertContainsExactSubsequence(str(bx.dtype), \"float32\")\n+            self.assertEqual(\n+                backend.standardize_dtype(bx.dtype), backend.floatx()\n+            )\n+            self.assertEqual(backend.standardize_dtype(by.dtype), \"int32\")\n             if i < 2:\n-                self.assertEqual(bx.shape, (16, 4))\n-                self.assertEqual(by.shape, (16, 2))\n+                self.assertEqual(bx.shape, (16, xdim1))\n+                self.assertEqual(by.shape, (16, ydim1))\n             else:\n-                self.assertEqual(bx.shape, (2, 4))\n-                self.assertEqual(by.shape, (2, 2))\n-            for i in range(by.shape[0]):\n-                sample_order.append(by[i, 0])\n+                self.assertEqual(bx.shape, (2, xdim1))\n+                self.assertEqual(by.shape, (2, ydim1))\n+\n+            if isinstance(bx, tf.SparseTensor):\n+                bx = tf.sparse.to_dense(bx)\n+                by = tf.sparse.to_dense(by)\n+            if isinstance(bx, jax_sparse.JAXSparse):\n+                bx = bx.todense()\n+                by = by.todense()\n+            x_batch_order = [float(bx[j, 0]) for j in range(bx.shape[0])]\n+            y_batch_order = [float(by[j, 0]) for j in range(by.shape[0])]\n+            x_order.extend(x_batch_order)\n+            y_order.extend(y_batch_order)\n+\n+            if shuffle == \"batch\":\n+                self.assertAllClose(\n+                    sorted(x_batch_order), range(i * 16, i * 16 + bx.shape[0])\n+                )\n+\n+        self.assertAllClose(x_order, y_order)\n         if shuffle:\n-            self.assertNotAllClose(sample_order, list(range(34)))\n+            self.assertNotAllClose(x_order, list(range(34)))\n         else:\n-            self.assertAllClose(sample_order, list(range(34)))\n+            self.assertAllClose(x_order, list(range(34)))\n \n     def test_multi_inputs_and_outputs(self):\n         x1 = np.random.random((34, 1))\n@@ -103,10 +157,8 @@ class TestArrayDataAdapter(testing.TestCase, parameterized.TestCase):\n             self.assertEqual(len(batch), 3)\n             bx, by, bw = batch\n             self.assertIsInstance(bx, dict)\n-            # NOTE: the y list was converted to a tuple for tf.data\n-            # compatibility.\n-            self.assertIsInstance(by, tuple)\n-            self.assertIsInstance(bw, tuple)\n+            self.assertIsInstance(by, list)\n+            self.assertIsInstance(bw, list)\n \n             self.assertIsInstance(bx[\"x1\"], np.ndarray)\n             self.assertIsInstance(bx[\"x2\"], np.ndarray)\n@@ -195,74 +247,3 @@ class TestArrayDataAdapter(testing.TestCase, parameterized.TestCase):\n     def test_errors(self):\n         # TODO\n         pass\n-\n-    @parameterized.named_parameters(\n-        named_product(array_type=[\"np\", \"tf\", \"jax\", \"torch\", \"pandas\"])\n-    )\n-    def test_integer_inputs(self, array_type):\n-        x1 = self.make_array(array_type, (4, 4), dtype=\"float64\")\n-        x2 = self.make_array(array_type, (4, 4), dtype=\"int32\")\n-        y = self.make_array(array_type, (4, 2))\n-\n-        adapter = array_data_adapter.ArrayDataAdapter(\n-            (x1, x2),\n-            y=y,\n-            sample_weight=None,\n-            batch_size=4,\n-            steps=None,\n-            shuffle=False,\n-        )\n-\n-        (x1, x2), y = next(adapter.get_numpy_iterator())\n-        self.assertEqual(x1.dtype, backend.floatx())\n-        self.assertEqual(x2.dtype, \"int32\")\n-\n-    def test_pandas_series(self):\n-        x = pandas.Series(np.ones((10,)))\n-        y = np.ones((10,))\n-\n-        adapter = array_data_adapter.ArrayDataAdapter(\n-            x,\n-            y=y,\n-            sample_weight=None,\n-            batch_size=4,\n-            steps=None,\n-            shuffle=False,\n-        )\n-\n-        self.assertEqual(adapter.num_batches, 3)\n-        self.assertEqual(adapter.batch_size, 4)\n-        self.assertEqual(adapter.has_partial_batch, True)\n-        self.assertEqual(adapter.partial_batch_size, 2)\n-\n-        x, y = next(adapter.get_numpy_iterator())\n-        self.assertEqual(x.dtype, backend.floatx())\n-        self.assertIsInstance(x, np.ndarray)\n-        self.assertEqual(x.shape, (4, 1))\n-\n-    @pytest.mark.skipif(\n-        backend.backend() != \"tensorflow\",\n-        reason=\"Only tensorflow supports raggeds\",\n-    )\n-    def test_tf_ragged(self):\n-        x = tf.ragged.constant([[1, 2], [1, 2, 3], [1, 2], [1], []], \"float64\")\n-        y = np.ones((5,))\n-\n-        adapter = array_data_adapter.ArrayDataAdapter(\n-            x,\n-            y=y,\n-            sample_weight=None,\n-            batch_size=2,\n-            steps=None,\n-            shuffle=False,\n-        )\n-\n-        self.assertEqual(adapter.num_batches, 3)\n-        self.assertEqual(adapter.batch_size, 2)\n-        self.assertEqual(adapter.has_partial_batch, True)\n-        self.assertEqual(adapter.partial_batch_size, 1)\n-\n-        x, y = next(adapter.get_numpy_iterator())\n-        self.assertEqual(x.dtype, backend.floatx())\n-        self.assertIsInstance(x, tf.RaggedTensor)\n-        self.assertEqual(x.shape, (2, None))\n\n@@ -0,0 +1,512 @@\n+import collections\n+import math\n+\n+import numpy as np\n+\n+from keras import backend\n+from keras.trainers.data_adapters import data_adapter_utils\n+from keras.utils import tree\n+\n+try:\n+    import pandas\n+except ImportError:\n+    pandas = None\n+\n+\n+# Leave jax, tf, and torch arrays off this list. Instead we will use\n+# `__array__` to detect these types. Doing so allows us to avoid importing a\n+# backend framework we are not currently using just to do type-checking.\n+ARRAY_TYPES = (np.ndarray,)\n+if pandas:\n+    ARRAY_TYPES = ARRAY_TYPES + (pandas.Series, pandas.DataFrame)\n+\n+\n+class Sliceable:\n+    \"\"\"`Sliceable` wrapping a tensor.\n+\n+    A `Sliceable` implements the subscript operator to slice or index against\n+    the first dimension of the array. It also has conversion methods for each\n+    one of the backends.\n+\n+    Args:\n+        array: the native array or tensor to wrap.\n+\n+    Attributes:\n+        shape: the shape of the full dense native array.\n+    \"\"\"\n+\n+    def __init__(self, array):\n+        self.array = array\n+\n+    def __getitem__(self, indices):\n+        \"\"\"Select elements in the 0th dimension.\n+\n+        Args:\n+            indices: the indices to select. Only needs to support one dimension,\n+                the 0th dimension. Should support a `slice` or a list, tuple,\n+                `np.array` or 1D tensor.\n+        Returns: A slice of `self.array`.\n+        \"\"\"\n+        return self.array[indices]\n+\n+    @classmethod\n+    def cast(cls, x, dtype):\n+        \"\"\"Cast a tensor to a different dtype.\n+\n+        Only called on a full array as provided by the user.\n+\n+        Args:\n+            x: the tensor to cast.\n+        Returns: the cast tensor.\n+        \"\"\"\n+        return x.astype(dtype)\n+\n+    @classmethod\n+    def convert_to_numpy(cls, x):\n+        \"\"\"Convert a tensor to a NumPy array.\n+\n+        Only called after slicing using `__getitem__`.\n+\n+        Args:\n+            x: the tensor to convert.\n+        Returns: the converted tensor.\n+        \"\"\"\n+        return x\n+\n+    @classmethod\n+    def convert_to_tf_dataset_compatible(cls, x):\n+        \"\"\"Convert a tensor to something compatible with `tf.data.Dataset`.\n+\n+        This can be a NumPy array, `tf.Tensor` or any other type of tensor that\n+        `tf.data.Dataset.from_tensors` can consume.\n+        Only called on a full array as provided by the user.\n+\n+        Args:\n+            x: the tensor to convert.\n+        Returns: converted version tensor.\n+        \"\"\"\n+        return x\n+\n+    @classmethod\n+    def convert_to_jax_compatible(cls, x):\n+        \"\"\"Convert a tensor to something that the JAX backend can consume.\n+\n+        This can be a `JAX` array, NumPy array or any other type of tensor that\n+        `keras.backend.jax.core.convert_to_tensor()` can consume.\n+        Only called after slicing using `__getitem__`.\n+        Used to convert sparse tensors and densify ragged tensors.\n+\n+        Args:\n+            x: the tensor to convert.\n+        Returns: the converted tensor.\n+        \"\"\"\n+        return x\n+\n+    @classmethod\n+    def convert_to_torch_compatible(cls, x):\n+        \"\"\"Convert a tensor to something that the Torch backend can consume.\n+\n+        This can be a Torch tensor, NumPy array or any other type of tensor that\n+        `keras.backend.torch.core.convert_to_tensor()` can consume.\n+        Only called after slicing using `__getitem__`.\n+        Used to densify sparse tensors and ragged tensors.\n+\n+        Args:\n+            x: the tensor to convert.\n+        Returns: the converted tensor.\n+        \"\"\"\n+        return x\n+\n+\n+class NumpySliceable(Sliceable):\n+    pass\n+\n+\n+class TensorflowSliceable(Sliceable):\n+    def __getitem__(self, indices):\n+        from keras.utils.module_utils import tensorflow as tf\n+\n+        if isinstance(indices, slice):\n+            return self.array[indices]\n+        else:\n+            return tf.gather(self.array, indices, axis=0)\n+\n+    @classmethod\n+    def cast(cls, x, dtype):\n+        from keras.backend.tensorflow.core import cast\n+\n+        return cast(x, dtype)\n+\n+    @classmethod\n+    def convert_to_numpy(cls, x):\n+        from keras.backend.tensorflow.core import convert_to_numpy\n+\n+        return convert_to_numpy(x)\n+\n+\n+class TensorflowRaggedSliceable(TensorflowSliceable):\n+    @classmethod\n+    def convert_to_jax_compatible(cls, x):\n+        return x.to_tensor()\n+\n+    @classmethod\n+    def convert_to_torch_compatible(cls, x):\n+        return x.to_tensor()\n+\n+\n+class TensorflowSparseSliceable(TensorflowSliceable):\n+    def __init__(self, array):\n+        super().__init__(to_tensorflow_sparse_wrapper(array))\n+\n+    @property\n+    def shape(self):\n+        return self.array.sparse.shape\n+\n+    def __getitem__(self, indices):\n+        return slice_tensorflow_sparse_wrapper(self.array, indices)\n+\n+    @classmethod\n+    def convert_to_tf_dataset_compatible(cls, x):\n+        return to_tensorflow_sparse_wrapper(x)\n+\n+    @classmethod\n+    def convert_to_jax_compatible(cls, x):\n+        return data_adapter_utils.tf_sparse_to_jax_sparse(x)\n+\n+    @classmethod\n+    def convert_to_torch_compatible(cls, x):\n+        from keras.utils.module_utils import tensorflow as tf\n+\n+        return tf.sparse.to_dense(x)\n+\n+\n+class JaxSliceable(Sliceable):\n+    def __getitem__(self, indices):\n+        return self.array[indices, ...]\n+\n+    @classmethod\n+    def convert_to_numpy(cls, x):\n+        from keras.backend.jax.core import convert_to_numpy\n+\n+        return convert_to_numpy(x)\n+\n+\n+class JaxSparseSliceable(JaxSliceable):\n+    @classmethod\n+    def convert_to_tf_dataset_compatible(cls, array):\n+        return to_tensorflow_sparse_wrapper(\n+            data_adapter_utils.jax_sparse_to_tf_sparse(array)\n+        )\n+\n+    @classmethod\n+    def convert_to_torch_compatible(cls, x):\n+        return x.todense()\n+\n+\n+class TorchSliceable(Sliceable):\n+    @classmethod\n+    def cast(cls, x, dtype):\n+        from keras.backend.torch.core import cast\n+\n+        return cast(x, dtype)\n+\n+    @classmethod\n+    def convert_to_numpy(cls, x):\n+        from keras.backend.torch.core import convert_to_numpy\n+\n+        return convert_to_numpy(x)\n+\n+\n+class PandasSliceable(Sliceable):\n+    def __getitem__(self, indices):\n+        return self.array.iloc[indices]\n+\n+    @classmethod\n+    def convert_to_numpy(cls, x):\n+        return x.to_numpy()\n+\n+    @classmethod\n+    def convert_to_tf_dataset_compatible(cls, x):\n+        return cls.convert_to_numpy(x)\n+\n+    @classmethod\n+    def convert_to_jax_compatible(cls, x):\n+        return cls.convert_to_numpy(x)\n+\n+    @classmethod\n+    def convert_to_torch_compatible(cls, x):\n+        return cls.convert_to_numpy(x)\n+\n+\n+class PandasDataFrameSliceable(PandasSliceable):\n+    pass\n+\n+\n+class PandasSeriesSliceable(PandasSliceable):\n+    @classmethod\n+    def convert_to_numpy(cls, x):\n+        return np.expand_dims(x.to_numpy(), axis=-1)\n+\n+\n+class ScipySparseSliceable(Sliceable):\n+    def __init__(self, array):\n+        # The COO representation is not indexable / sliceable and does not lend\n+        # itself to it. Use the CSR representation instead, which is sliceable.\n+        super().__init__(array.tocsr())\n+\n+    @classmethod\n+    def convert_to_numpy(cls, x):\n+        return x.todense()\n+\n+    @classmethod\n+    def convert_to_tf_dataset_compatible(cls, x):\n+        return to_tensorflow_sparse_wrapper(\n+            data_adapter_utils.scipy_sparse_to_tf_sparse(x)\n+        )\n+\n+    @classmethod\n+    def convert_to_jax_compatible(cls, x):\n+        return data_adapter_utils.scipy_sparse_to_jax_sparse(x)\n+\n+    @classmethod\n+    def convert_to_torch_compatible(cls, x):\n+        return x.todense()\n+\n+\n+# `tf.SparseTensor` does not support indexing or `tf.gather`. The COO\n+# representation it uses does not lend itself to indexing. We add some\n+# intermediary tensors to ease the indexing and slicing. We put both indices and\n+# values in `RaggedTensor`s where each row corresponds to a row in the sparse\n+# tensor. This is because the number of values per row is not fixed.\n+# `RaggedTensor`s do support indexing and `tf.gather`, although on CPU only.\n+# We then reconstruct a `SparseTensor` from extracted rows. In theory, there is\n+# no duplication of data for the indices and values, only the addition of row\n+# splits for the ragged representation.\n+# `TensorflowSparseWrapper` is a named tuple which combines the original\n+# `SparseTensor` (used for the shape) and the ragged representations of indices\n+# and values for indexing / slicing. We use a named tuple and not a `Sliceable`\n+# to be able to ingest it in `tf.data.Dataset.from_tensors()` and map it.\n+\n+TensorflowSparseWrapper = collections.namedtuple(\n+    \"TensorflowSparseWrapper\", [\"sparse\", \"ragged_indices\", \"ragged_values\"]\n+)\n+\n+\n+def to_tensorflow_sparse_wrapper(sparse):\n+    from keras.utils.module_utils import tensorflow as tf\n+\n+    row_ids = sparse.indices[:, 0]\n+    row_splits = tf.experimental.RowPartition.from_value_rowids(\n+        row_ids\n+    ).row_splits()\n+\n+    ragged_indices = tf.cast(\n+        tf.RaggedTensor.from_row_splits(sparse.indices, row_splits), tf.int64\n+    )\n+    ragged_values = tf.RaggedTensor.from_row_splits(sparse.values, row_splits)\n+    return TensorflowSparseWrapper(sparse, ragged_indices, ragged_values)\n+\n+\n+def slice_tensorflow_sparse_wrapper(sparse_wrapper, indices):\n+    from keras.utils.module_utils import tensorflow as tf\n+\n+    if isinstance(indices, slice):\n+        sparse_indices = sparse_wrapper.ragged_indices[indices]\n+        sparse_values = sparse_wrapper.ragged_values[indices]\n+        batch_dim = indices.stop - indices.start\n+    else:\n+        sparse_indices = tf.gather(sparse_wrapper.ragged_indices, indices)\n+        sparse_values = tf.gather(sparse_wrapper.ragged_values, indices)\n+        if isinstance(indices, list):\n+            batch_dim = len(indices)\n+        else:\n+            batch_dim = indices.shape[0]\n+            if batch_dim is None:\n+                batch_dim = tf.shape(indices)[0]\n+\n+    row_ids = sparse_indices.value_rowids()\n+    sparse_indices = sparse_indices.flat_values[:, 1:]  # remove first value\n+    sparse_indices = tf.concat(\n+        [tf.expand_dims(row_ids, -1), sparse_indices], axis=1\n+    )\n+\n+    sparse_values = sparse_values.flat_values\n+    sparse_shape = (batch_dim,) + tuple(\n+        sparse_wrapper.sparse.shape.as_list()[1:]\n+    )\n+    return tf.SparseTensor(sparse_indices, sparse_values, sparse_shape)\n+\n+\n+def can_slice_array(x):\n+    return (\n+        x is None\n+        or isinstance(x, ARRAY_TYPES)\n+        or data_adapter_utils.is_tensorflow_tensor(x)\n+        or data_adapter_utils.is_jax_array(x)\n+        or data_adapter_utils.is_torch_tensor(x)\n+        or data_adapter_utils.is_scipy_sparse(x)\n+        or hasattr(x, \"__array__\")\n+    )\n+\n+\n+def convert_to_sliceable(arrays, target_backend=None):\n+    \"\"\"Convert a structure of arrays into `Sliceable` instances\n+\n+    Args:\n+        arrays: the arrays to convert.\n+        target_backend: the target backend for the output:\n+            - `None` indicates that `arrays` will be wrapped into `Sliceable`s\n+              as-is without using a different representation. This is used by\n+              `train_validation_split()`.\n+            - `tensorflow` indicates that\n+              `Sliceable.convert_to_tf_dataset_compatible` will be called. The\n+              returned structure therefore contains arrays, not `Sliceable`s.\n+            - `numpy`, `jax` or `torch` indices that the arrays will eventually\n+              be converted to this backend type after slicing. In this case,\n+              the intermediary `Sliceable`s may use a different representation\n+              from the input `arrays` for better performance.\n+    Returns: the same structure with `Sliceable` instances or arrays.\n+    \"\"\"\n+\n+    def convert_single_array(x):\n+        if x is None:\n+            return x\n+\n+        # Step 1. Determine which Sliceable class to use.\n+        if isinstance(x, np.ndarray):\n+            sliceable_class = NumpySliceable\n+        elif data_adapter_utils.is_tensorflow_tensor(x):\n+            if data_adapter_utils.is_tensorflow_ragged(x):\n+                sliceable_class = TensorflowRaggedSliceable\n+            elif data_adapter_utils.is_tensorflow_sparse(x):\n+                sliceable_class = TensorflowSparseSliceable\n+            else:\n+                sliceable_class = TensorflowSliceable\n+        elif data_adapter_utils.is_jax_array(x):\n+            if data_adapter_utils.is_jax_sparse(x):\n+                sliceable_class = JaxSparseSliceable\n+            else:\n+                sliceable_class = JaxSliceable\n+        elif data_adapter_utils.is_torch_tensor(x):\n+            sliceable_class = TorchSliceable\n+        elif pandas is not None and isinstance(x, pandas.DataFrame):\n+            sliceable_class = PandasDataFrameSliceable\n+        elif pandas is not None and isinstance(x, pandas.Series):\n+            sliceable_class = PandasSeriesSliceable\n+        elif data_adapter_utils.is_scipy_sparse(x):\n+            sliceable_class = ScipySparseSliceable\n+        elif hasattr(x, \"__array__\"):\n+            x = np.asarray(x)\n+            sliceable_class = NumpySliceable\n+        else:\n+            raise ValueError(\n+                \"Expected a NumPy array, tf.Tensor, tf.RaggedTensor, \"\n+                \"tf.SparseTensor, jax.np.ndarray, \"\n+                \"jax.experimental.sparse.JAXSparse, torch.Tensor, \"\n+                \"Pandas Dataframe, or Pandas Series. Received invalid input: \"\n+                f\"{x} (of type {type(x)})\"\n+            )\n+\n+        # Step 2. Normalize floats to floatx.\n+        def is_non_floatx_float(dtype):\n+            return (\n+                not dtype == object\n+                and backend.is_float_dtype(dtype)\n+                and not backend.standardize_dtype(dtype) == backend.floatx()\n+            )\n+\n+        cast_dtype = None\n+        if pandas is not None and isinstance(x, pandas.DataFrame):\n+            if any(is_non_floatx_float(d) for d in x.dtypes.values):\n+                cast_dtype = backend.floatx()\n+        else:\n+            if is_non_floatx_float(x.dtype):\n+                cast_dtype = backend.floatx()\n+\n+        if cast_dtype is not None:\n+            x = sliceable_class.cast(x, cast_dtype)\n+\n+        # Step 3. Apply target backend specific logic and optimizations.\n+        if target_backend is None:\n+            return sliceable_class(x)\n+\n+        if target_backend == \"tensorflow\":\n+            return sliceable_class.convert_to_tf_dataset_compatible(x)\n+\n+        # With dense arrays, with JAX as either input or output, it is faster to\n+        # use NumPy as an intermediary representation, so wrap input array in a\n+        # NumPy array, which should not use extra memory. For the input case,\n+        # see https://github.com/google/jax/issues/1276 for an explanation of\n+        # why slicing a NumPy array is faster than slicing a JAX array.\n+        if sliceable_class == JaxSliceable or (\n+            target_backend == \"jax\"\n+            and sliceable_class in (TensorflowSliceable, TorchSliceable)\n+        ):\n+            x = np.asarray(x)\n+            sliceable_class = NumpySliceable\n+\n+        return sliceable_class(x)\n+\n+    return tree.map_structure(convert_single_array, arrays)\n+\n+\n+def train_validation_split(arrays, validation_split):\n+    \"\"\"Split arrays into train and validation subsets in deterministic order.\n+\n+    The last part of data will become validation data.\n+\n+    Args:\n+        arrays: Tensors to split. Allowed inputs are arbitrarily nested\n+            structures of Tensors and NumPy arrays.\n+        validation_split: Float between 0 and 1. The proportion of the dataset\n+            to include in the validation split. The rest of the dataset will be\n+            included in the training split.\n+\n+    Returns:\n+        `(train_arrays, validation_arrays)`\n+    \"\"\"\n+\n+    flat_arrays = tree.flatten(arrays)\n+    unsplitable = [type(t) for t in flat_arrays if not can_slice_array(t)]\n+    if unsplitable:\n+        raise ValueError(\n+            \"Argument `validation_split` is only supported \"\n+            \"for tensors or NumPy arrays.\"\n+            f\"Found incompatible type in the input: {unsplitable}\"\n+        )\n+\n+    if all(t is None for t in flat_arrays):\n+        return arrays, arrays\n+\n+    first_non_none = None\n+    for t in flat_arrays:\n+        if t is not None:\n+            first_non_none = t\n+            break\n+\n+    # Assumes all arrays have the same batch shape or are `None`.\n+    batch_dim = int(first_non_none.shape[0])\n+    split_at = int(math.floor(batch_dim * (1.0 - validation_split)))\n+\n+    if split_at == 0 or split_at == batch_dim:\n+        raise ValueError(\n+            f\"Training data contains {batch_dim} samples, which is not \"\n+            \"sufficient to split it into a validation and training set as \"\n+            f\"specified by `validation_split={validation_split}`. Either \"\n+            \"provide more data, or a different value for the \"\n+            \"`validation_split` argument.\"\n+        )\n+\n+    def _split(t, start, end):\n+        if t is None:\n+            return t\n+        return t[start:end]\n+\n+    sliceables = convert_to_sliceable(arrays)\n+    train_arrays = tree.map_structure(\n+        lambda x: _split(x, start=0, end=split_at), sliceables\n+    )\n+    val_arrays = tree.map_structure(\n+        lambda x: _split(x, start=split_at, end=batch_dim), sliceables\n+    )\n+    return train_arrays, val_arrays\n\n@@ -1,28 +1,8 @@\n-import math\n-\n import numpy as np\n \n from keras import backend\n from keras.api_export import keras_export\n from keras.utils import tree\n-from keras.utils.dataset_utils import is_torch_tensor\n-\n-try:\n-    import pandas\n-except ImportError:\n-    pandas = None\n-\n-\n-# Leave jax, tf, and torch arrays off this list. Instead we will use\n-# `__array__` to detect these types. Doing so allows us to avoid importing a\n-# backend framework we are not currently using just to do type-checking.\n-ARRAY_TYPES = (np.ndarray,)\n-if backend.backend() == \"tensorflow\":\n-    from keras.utils.module_utils import tensorflow as tf\n-\n-    ARRAY_TYPES = ARRAY_TYPES + (tf.RaggedTensor,)\n-if pandas:\n-    ARRAY_TYPES = ARRAY_TYPES + (pandas.Series, pandas.DataFrame)\n \n \n @keras_export(\"keras.utils.unpack_x_y_sample_weight\")\n@@ -132,79 +112,6 @@ def check_data_cardinality(data):\n         raise ValueError(msg)\n \n \n-def sync_shuffle(data, num_samples=None):\n-    if num_samples is None:\n-        num_samples_set = set(int(i.shape[0]) for i in tree.flatten(data))\n-        assert len(num_samples_set) == 1\n-        num_samples = num_samples_set.pop()\n-    p = np.random.permutation(num_samples)\n-    return tree.map_structure(lambda x: x[p], data)\n-\n-\n-def train_validation_split(arrays, validation_split):\n-    \"\"\"Split arrays into train and validation subsets in deterministic order.\n-\n-    The last part of data will become validation data.\n-\n-    Args:\n-        arrays: Tensors to split. Allowed inputs are arbitrarily nested\n-            structures of Tensors and NumPy arrays.\n-        validation_split: Float between 0 and 1. The proportion of the dataset\n-            to include in the validation split. The rest of the dataset will be\n-            included in the training split.\n-\n-    Returns:\n-        `(train_arrays, validation_arrays)`\n-    \"\"\"\n-\n-    def _can_split(t):\n-        return backend.is_tensor(t) or isinstance(t, ARRAY_TYPES) or t is None\n-\n-    flat_arrays = tree.flatten(arrays)\n-    unsplitable = [type(t) for t in flat_arrays if not _can_split(t)]\n-    if unsplitable:\n-        raise ValueError(\n-            \"Argument `validation_split` is only supported \"\n-            \"for tensors or NumPy arrays.\"\n-            f\"Found incompatible type in the input: {unsplitable}\"\n-        )\n-\n-    if all(t is None for t in flat_arrays):\n-        return arrays, arrays\n-\n-    first_non_none = None\n-    for t in flat_arrays:\n-        if t is not None:\n-            first_non_none = t\n-            break\n-\n-    # Assumes all arrays have the same batch shape or are `None`.\n-    batch_dim = int(first_non_none.shape[0])\n-    split_at = int(math.floor(batch_dim * (1.0 - validation_split)))\n-\n-    if split_at == 0 or split_at == batch_dim:\n-        raise ValueError(\n-            f\"Training data contains {batch_dim} samples, which is not \"\n-            \"sufficient to split it into a validation and training set as \"\n-            f\"specified by `validation_split={validation_split}`. Either \"\n-            \"provide more data, or a different value for the \"\n-            \"`validation_split` argument.\"\n-        )\n-\n-    def _split(t, start, end):\n-        if t is None:\n-            return t\n-        return t[start:end]\n-\n-    train_arrays = tree.map_structure(\n-        lambda x: _split(x, start=0, end=split_at), arrays\n-    )\n-    val_arrays = tree.map_structure(\n-        lambda x: _split(x, start=split_at, end=batch_dim), arrays\n-    )\n-    return train_arrays, val_arrays\n-\n-\n def class_weight_to_sample_weights(y, class_weight):\n     sample_weight = np.ones(shape=(y.shape[0],), dtype=backend.floatx())\n     if len(y.shape) > 1:\n@@ -257,3 +164,95 @@ def get_torch_dataloader(iterable):\n     dataset = ConverterIterableDataset(iterable)\n     # `batch_size=None` indicates that we should not re-batch\n     return torch_data.DataLoader(dataset, batch_size=None)\n+\n+\n+def is_tensorflow_tensor(value):\n+    if hasattr(value, \"__class__\"):\n+        if value.__class__.__name__ in (\"RaggedTensor\", \"SparseTensor\"):\n+            return \"tensorflow.python.\" in str(value.__class__.__module__)\n+        for parent in value.__class__.__mro__:\n+            if parent.__name__ in (\"Tensor\") and \"tensorflow.python.\" in str(\n+                parent.__module__\n+            ):\n+                return True\n+    return False\n+\n+\n+def is_tensorflow_ragged(value):\n+    if hasattr(value, \"__class__\"):\n+        return (\n+            value.__class__.__name__ == \"RaggedTensor\"\n+            and \"tensorflow.python.\" in str(value.__class__.__module__)\n+        )\n+    return False\n+\n+\n+def is_tensorflow_sparse(value):\n+    if hasattr(value, \"__class__\"):\n+        return (\n+            value.__class__.__name__ == \"SparseTensor\"\n+            and \"tensorflow.python.\" in str(value.__class__.__module__)\n+        )\n+    return False\n+\n+\n+def is_jax_array(value):\n+    if hasattr(value, \"__class__\"):\n+        for parent in value.__class__.__mro__:\n+            if parent.__name__ == \"Array\" and str(parent.__module__) == \"jax\":\n+                return True\n+    return is_jax_sparse(value)  # JAX sparse arrays do not extend jax.Array\n+\n+\n+def is_jax_sparse(value):\n+    if hasattr(value, \"__class__\"):\n+        return str(value.__class__.__module__).startswith(\n+            \"jax.experimental.sparse\"\n+        )\n+    return False\n+\n+\n+def is_torch_tensor(value):\n+    if hasattr(value, \"__class__\"):\n+        for parent in value.__class__.__mro__:\n+            if parent.__name__ == \"Tensor\" and str(parent.__module__).endswith(\n+                \"torch\"\n+            ):\n+                return True\n+    return False\n+\n+\n+def is_scipy_sparse(x):\n+    return str(x.__class__.__module__).startswith(\"scipy.sparse\") and hasattr(\n+        x, \"tocoo\"\n+    )\n+\n+\n+def scipy_sparse_to_tf_sparse(x):\n+    from keras.utils.module_utils import tensorflow as tf\n+\n+    coo = x.tocoo()\n+    indices = np.concatenate(\n+        (np.expand_dims(coo.row, 1), np.expand_dims(coo.col, 1)), axis=1\n+    )\n+    return tf.SparseTensor(indices, coo.data, coo.shape)\n+\n+\n+def scipy_sparse_to_jax_sparse(x):\n+    import jax.experimental.sparse as jax_sparse\n+\n+    return jax_sparse.BCOO.from_scipy_sparse(x)\n+\n+\n+def tf_sparse_to_jax_sparse(x):\n+    import jax.experimental.sparse as jax_sparse\n+\n+    values = np.asarray(x.values)\n+    indices = np.asarray(x.indices)\n+    return jax_sparse.BCOO((values, indices), shape=x.shape)\n+\n+\n+def jax_sparse_to_tf_sparse(x):\n+    from keras.utils.module_utils import tensorflow as tf\n+\n+    return tf.SparseTensor(x.indices, x.data, x.shape)\n\n@@ -1,7 +1,5 @@\n import itertools\n \n-import numpy as np\n-\n from keras import backend\n from keras.trainers.data_adapters import data_adapter_utils\n from keras.trainers.data_adapters.data_adapter import DataAdapter\n@@ -44,8 +42,8 @@ class GeneratorDataAdapter(DataAdapter):\n                 return tf.RaggedTensorSpec(shape=shape, dtype=dtype)\n             if (\n                 isinstance(x, tf.SparseTensor)\n-                or is_scipy_sparse(x)\n-                or is_jax_sparse(x)\n+                or data_adapter_utils.is_scipy_sparse(x)\n+                or data_adapter_utils.is_jax_sparse(x)\n             ):\n                 return tf.SparseTensorSpec(shape=shape, dtype=dtype)\n             else:\n@@ -62,10 +60,10 @@ class GeneratorDataAdapter(DataAdapter):\n         from keras.backend.jax.core import convert_to_tensor\n \n         def convert_to_jax(x):\n-            if is_scipy_sparse(x):\n-                return scipy_sparse_to_jax_sparse(x)\n-            elif is_tf_sparse(x):\n-                return tf_sparse_to_jax_sparse(x)\n+            if data_adapter_utils.is_scipy_sparse(x):\n+                return data_adapter_utils.scipy_sparse_to_jax_sparse(x)\n+            elif data_adapter_utils.is_tensorflow_sparse(x):\n+                return data_adapter_utils.tf_sparse_to_jax_sparse(x)\n             return convert_to_tensor(x)\n \n         for batch in self.generator:\n@@ -75,10 +73,10 @@ class GeneratorDataAdapter(DataAdapter):\n         from keras.utils.module_utils import tensorflow as tf\n \n         def convert_to_tf(x):\n-            if is_scipy_sparse(x):\n-                x = scipy_sparse_to_tf_sparse(x)\n-            elif is_jax_sparse(x):\n-                x = jax_sparse_to_tf_sparse(x)\n+            if data_adapter_utils.is_scipy_sparse(x):\n+                x = data_adapter_utils.scipy_sparse_to_tf_sparse(x)\n+            elif data_adapter_utils.is_jax_sparse(x):\n+                x = data_adapter_utils.jax_sparse_to_tf_sparse(x)\n             return x\n \n         def get_tf_iterator():\n@@ -110,58 +108,3 @@ class GeneratorDataAdapter(DataAdapter):\n def peek_and_restore(generator):\n     element = next(generator)\n     return element, itertools.chain([element], generator)\n-\n-\n-def is_scipy_sparse(x):\n-    return x.__class__.__module__.startswith(\"scipy.sparse\") and hasattr(\n-        x, \"tocoo\"\n-    )\n-\n-\n-def is_tf_sparse(x):\n-    return (\n-        x.__class__.__name__ == \"SparseTensor\"\n-        and x.__class__.__module__.startswith(\"tensorflow\")\n-    )\n-\n-\n-def is_jax_sparse(x):\n-    return x.__class__.__module__.startswith(\"jax.experimental.sparse\")\n-\n-\n-def scipy_sparse_to_tf_sparse(x):\n-    from keras.utils.module_utils import tensorflow as tf\n-\n-    coo = x.tocoo()\n-    indices = np.concatenate(\n-        (np.expand_dims(coo.row, 1), np.expand_dims(coo.col, 1)),\n-        axis=1,\n-    )\n-    return tf.SparseTensor(indices, coo.data, coo.shape)\n-\n-\n-def scipy_sparse_to_jax_sparse(x):\n-    import jax.experimental.sparse as jax_sparse\n-\n-    coo = x.tocoo()\n-    indices = np.concatenate(\n-        (np.expand_dims(coo.row, 1), np.expand_dims(coo.col, 1)),\n-        axis=1,\n-    )\n-    return jax_sparse.BCOO((coo.data, indices), shape=coo.shape)\n-\n-\n-def tf_sparse_to_jax_sparse(x):\n-    import jax.experimental.sparse as jax_sparse\n-\n-    from keras.backend.tensorflow.core import convert_to_numpy\n-\n-    values = convert_to_numpy(x.values)\n-    indices = convert_to_numpy(x.indices)\n-    return jax_sparse.BCOO((values, indices), shape=x.shape)\n-\n-\n-def jax_sparse_to_tf_sparse(x):\n-    from keras.utils.module_utils import tensorflow as tf\n-\n-    return tf.SparseTensor(x.indices, x.data, x.shape)\n\n@@ -248,6 +248,8 @@ def _get_next_sample(\n     Yields:\n         data_sample: The next sample.\n     \"\"\"\n+    from keras.trainers.data_adapters.data_adapter_utils import is_torch_tensor\n+\n     try:\n         dataset_iterator = iter(dataset_iterator)\n         first_sample = next(dataset_iterator)\n@@ -293,16 +295,6 @@ def _get_next_sample(\n         yield sample\n \n \n-def is_torch_tensor(value):\n-    if hasattr(value, \"__class__\"):\n-        for parent in value.__class__.__mro__:\n-            if parent.__name__ == \"Tensor\" and str(parent.__module__).endswith(\n-                \"torch\"\n-            ):\n-                return True\n-    return False\n-\n-\n def is_torch_dataset(dataset):\n     if hasattr(dataset, \"__class__\"):\n         for parent in dataset.__class__.__mro__:\n\n@@ -19,6 +19,10 @@ def map_structure_up_to(shallow_structure, func, *structures, **kwargs):\n     )\n \n \n+def traverse(func, structure, top_down=True):\n+    return tree.traverse(func, structure, top_down=top_down)\n+\n+\n def assert_same_structure(a, b, check_types=True):\n     return tree.assert_same_structure(a, b, check_types=check_types)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
