{"custom_id": "keras#fe31a34aa3447c5a7a768a8f61dcfd022ac2467b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 521 | Lines Deleted: 1674 | Files Changed: 13 | Hunks: 150 | Methods Changed: 127 | Complexity Δ (Sum/Max): -128/2 | Churn Δ: 2195 | Churn Cumulative: 18024 | Contributors (this commit): 31 | Commits (past 90d): 99 | Contributors (cumulative): 74 | DMM Complexity: 0.12558869701726844\n\nDIFF:\n@@ -852,21 +852,6 @@ class TrackableWeightHandler(object):\n     backend.get_session().run(self._assign_op, feed_dict)\n \n \n-class StaticTableHandler(TrackableWeightHandler):\n-  \"\"\"Wrapper for handling weight collection for static hash tables.\"\"\"\n-\n-  def __init__(self, getter_lambda):  # pylint: disable=super-init-not-called\n-    self._num_tensors = 2\n-    self._getter = getter_lambda\n-    self._distribute_strategy = tf.distribute.get_strategy()\n-\n-    def raise_error(_):\n-      raise RuntimeError('This layer contains a static lookup table, which '\n-                         'cannot be changed via set_weights().')\n-\n-    self._setter = raise_error\n-\n-\n def no_ragged_support(inputs, layer_name):\n   input_list = tf.nest.flatten(inputs)\n   if any(isinstance(x, tf.RaggedTensor) for x in input_list):\n\n@@ -14,21 +14,20 @@\n # ==============================================================================\n \"\"\"Keras index lookup preprocessing layer.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n # pylint: disable=g-classes-have-attributes\n \n import collections\n-import json\n-import operator\n+import os\n \n-import numpy as np\n from keras import backend\n+from keras.engine import base_layer_utils\n from keras.engine import base_preprocessing_layer\n from keras.layers.preprocessing import category_encoding\n-from keras.layers.preprocessing import table_utils\n from keras.saving.saved_model import layer_serialization\n from keras.utils import layer_utils\n from keras.utils import tf_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n from tensorflow.python.platform import tf_logging as logging\n \n INT = \"int\"\n@@ -41,7 +40,7 @@ _VOCAB_NAME = \"vocab\"\n _IDF_WEIGHTS_NAME = \"idf_weights\"\n \n \n-class _NullInitializer(tf.lookup.TextFileInitializer):\n+class NullInitializer(tf.lookup.KeyValueTensorInitializer):\n   \"\"\"A placeholder initializer for restoring this layer from a SavedModel.\"\"\"\n \n   def __init__(self, key_dtype, value_dtype):\n@@ -51,8 +50,8 @@ class _NullInitializer(tf.lookup.TextFileInitializer):\n       key_dtype: Type of the table keys.\n       value_dtype: Type of the table values.\n     \"\"\"\n-    self._key_dtype = tf.as_dtype(key_dtype)\n-    self._value_dtype = tf.as_dtype(value_dtype)\n+    self._key_dtype = key_dtype\n+    self._value_dtype = value_dtype\n \n   @property\n   def key_dtype(self):\n@@ -68,18 +67,31 @@ class _NullInitializer(tf.lookup.TextFileInitializer):\n     \"\"\"Returns the table initialization op.\"\"\"\n     pass\n \n+\n+class VocabWeightHandler(base_layer_utils.TrackableWeightHandler):\n+  \"\"\"Adds the vocabulary as a layer weight during serialization.\"\"\"\n+\n+  def __init__(self, lookup_layer):\n+    self._layer = lookup_layer\n+    self._dtype = lookup_layer.dtype\n+    self._distribute_strategy = tf.distribute.get_strategy()\n+\n   @property\n-  def _shared_name(self):\n-    \"\"\"Returns a shared name to be used by the table.\"\"\"\n-    shared_name = \"NULL_INITIALIZER_\"\n-    if tf.executing_eagerly():\n-      # Ensure a unique name when eager execution is enabled to avoid spurious\n-      # sharing issues..\n-      shared_name += str(backend.get_uid(shared_name))\n-    return shared_name\n+  def num_tensors(self):\n+    return 1\n+\n+  def set_weights(self, weights):\n+    tokens = tf.convert_to_tensor(weights[0], self._dtype)\n+    self._layer.lookup_table = self._layer._lookup_table_from_tokens(tokens)  # pylint: disable=protected-access\n+\n+  def get_tensors(self):\n+    # Just save the non-config part of the vocab (no special tokens).\n+    tokens = self._layer.get_vocabulary(include_special_tokens=False)\n+    tokens = tf.convert_to_tensor(tokens, self._dtype)\n+    return [tokens]\n \n \n-class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n+class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n   \"\"\"Maps values from a vocabulary to integer indices.\n \n   This layer translates a set of arbitrary hashables into an integer output via\n@@ -101,8 +113,11 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n       no mask term will be added.\n     oov_token: Only used when `invert` is True. The token to return for OOV\n       indices.\n-    vocabulary: An optional list of vocabulary terms. If the list contains the\n-      same token multiple times, an error will be thrown.\n+    vocabulary: Optional. Either an array or a string path to a text file. If\n+      passing an array, can pass a tuple, list, 1D numpy array, or 1D tensor\n+      containing the vocbulary terms. If passing a file path, the file should\n+      contain one line per term in the vocabulary. If this argument is set,\n+      there is no need to `adapt` the layer.\n     invert: Only valid when `output_mode` is `\"int\"`. If True, this layer will\n       map indices to vocabulary items instead of mapping vocabulary items to\n       indices. Default to False.\n@@ -149,7 +164,11 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n     # are creating a 0-element vocab, which doesn't make sense.\n     if max_tokens is not None and max_tokens <= 1:\n       raise ValueError(\"If set, `max_tokens` must be greater than 1. \"\n-                       \"You passed {}\".format(max_tokens))\n+                       \"You passed `max_tokens={}`\".format(max_tokens))\n+\n+    if pad_to_max_tokens and max_tokens is None:\n+      raise ValueError(\"If pad_to_max_tokens is True, must set `max_tokens`. \"\n+                       \"You passed `max_tokens={}`\".format(max_tokens))\n \n     if num_oov_indices < 0:\n       raise ValueError(\"`num_oov_indices` must be greater than or equal to 0. \"\n@@ -174,200 +193,98 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n     self.invert = invert\n     self.max_tokens = max_tokens\n     self.num_oov_indices = num_oov_indices\n+    self.mask_token = mask_token\n+    self.oov_token = oov_token\n     self.output_mode = output_mode\n     self.sparse = sparse\n     self.pad_to_max_tokens = pad_to_max_tokens\n-    self._called = False\n+    self.input_vocabulary = None\n+    # IndexLookupLayerSavedModelSaver will clear the config config vocabulary to\n+    # restore the lookup table ops directly. We persist this hidden option to\n+    # persist the fact that we have have a non-adaptable layer with a manually\n+    # set vocabulary.\n+    self._has_input_vocabulary = kwargs.pop(\"has_input_vocabulary\", False)\n+    self._frozen_vocab_size = None\n \n-    # A note on vocab_size: we need to always keep a non-Tensor representation\n-    # of vocab_size around to use in graph building. Because we might be\n-    # in a tf.function, we can't rely on evaluating the actual tables to\n-    # find the value either.\n-    self._vocab_size = None\n-    # We need to keep track our current vocab size outside of our layer weights\n-    # to support a static output shape when `output_mode != INT`. The bincount\n-    # ops do not set shape on their outputs, which means we have to set it\n-    # ourselves. We persist the current vocab size as a hidden part of the\n-    # config when serializing our model.\n-    if \"vocabulary_size\" in kwargs:\n-      self._vocab_size = kwargs[\"vocabulary_size\"]\n-      del kwargs[\"vocabulary_size\"]\n+    # Drop deprecated config options.\n+    kwargs.pop(\"vocabulary_size\", None)\n+    kwargs.pop(\"has_static_table\", None)\n \n-    restore_from_static_table = kwargs.pop(\"has_static_table\", False)\n+    super().__init__(streaming=False, **kwargs)\n \n-    # Make sure the mask token and oov token are truly of the dtype we want. We\n-    # can ignore strings here, because they have only one dtype.\n-    dtype = kwargs[\"dtype\"]\n-    if dtype == tf.int32:\n-      mask_token = None if mask_token is None else np.int32(mask_token)\n-      oov_token = None if oov_token is None else np.int32(oov_token)\n-    elif dtype == tf.int64:\n-      mask_token = None if mask_token is None else np.int64(mask_token)\n-      oov_token = None if oov_token is None else np.int64(oov_token)\n-    self.mask_token = mask_token\n-    self.oov_token = oov_token\n-\n-    if max_tokens is not None:\n-      available_vocab_size = max_tokens - self._token_start_index()\n-    else:\n-      available_vocab_size = None\n-\n-    super(IndexLookup, self).__init__(\n-        combiner=_IndexLookupCombiner(\n-            vocab_size=available_vocab_size,\n-            mask_value=mask_token,\n-            oov_value=oov_token,\n-            compute_idf=(output_mode == TF_IDF)),\n-        **kwargs)\n-\n-    # We need to save the key dtype so that we know if we're expecting int64\n-    # keys. If we are, we will cast int32 inputs to int64 as well.\n     if invert:\n       self._key_dtype = tf.int64\n-      self._value_dtype = self.dtype\n-      self._mask_key = 0\n-      self._mask_value = mask_token\n-      key_index = tf.lookup.TextFileIndex.LINE_NUMBER\n-      value_index = tf.lookup.TextFileIndex.WHOLE_LINE\n+      self._value_dtype = tf.as_dtype(self.dtype)\n+      mask_key = 0\n+      mask_value = mask_token\n       default_value = self.oov_token\n-      oov_indices = None\n     else:\n-      self._key_dtype = self.dtype\n+      self._key_dtype = tf.as_dtype(self.dtype)\n       self._value_dtype = tf.int64\n-      self._mask_key = mask_token\n-      key_index = tf.lookup.TextFileIndex.WHOLE_LINE\n-      value_index = tf.lookup.TextFileIndex.LINE_NUMBER\n+      mask_key = mask_token\n       # Masks should map to 0 for int output and be dropped otherwise. Max ints\n       # will be dropped from the bincount op.\n-      self._mask_value = 0 if self.output_mode == INT else tf.int64.max\n-      oov_start = self._oov_start_index()\n-      token_start = self._token_start_index()\n+      mask_value = 0 if self.output_mode == INT else tf.int64.max\n       if self.num_oov_indices == 0:\n         # If there are no OOV indices, we map OOV tokens to -1 and error out\n         # during call if we find a negative index.\n         default_value = -1\n-        oov_indices = None\n       elif self.num_oov_indices == 1:\n         # If there is only one OOV index, we can set that index as the default\n         # value of the index_lookup table.\n-        default_value = oov_start\n-        oov_indices = None\n+        default_value = self._oov_start_index()\n       else:\n         # If we hav multiple OOV values, we need to do a further hashing step;\n         # to make this easier, we set the OOV value to -1. (This lets us do a\n         # vectorized add and cast to boolean to determine locations where we\n         # need to do extra hashing.)\n         default_value = -1\n-        oov_indices = list(range(oov_start, token_start))\n-\n-    self._static_vocabulary_path = None\n-    has_vocab_path = (vocabulary is not None and isinstance(vocabulary, str))\n-    if has_vocab_path or restore_from_static_table:\n-      self._has_static_table = True\n-      if vocabulary is None:\n-        # If we're restoring a layer that was saved with a static table\n-        # initializer, we create a fake initializer object to let the code\n-        # progress. The savedmodel restoration code will handle restoring\n-        # the actual data.\n-        initializer = _NullInitializer(self._key_dtype, self._value_dtype)\n-      else:\n-        if not tf.io.gfile.exists(vocabulary):\n-          raise ValueError(\"Vocabulary file %s does not exist.\" % (vocabulary,))\n-        self._static_vocabulary_path = vocabulary\n-        num_tokens = table_utils.num_tokens_in_file(vocabulary)\n-        self._vocab_size = self._token_start_index() + num_tokens\n-\n-        initializer = tf.lookup.TextFileInitializer(\n-            filename=vocabulary,\n-            key_dtype=self._key_dtype,\n-            key_index=key_index,\n-            value_dtype=self._value_dtype,\n-            value_index=value_index,\n-            value_index_offset=self._token_start_index())\n-\n-      self._table = tf.lookup.StaticHashTable(\n-          initializer, default_value=default_value)\n-      self._table_handler = table_utils.TableHandler(\n-          table=self._table,\n-          mask_token=self._mask_key if self.mask_token is not None else None,\n-          mask_value=self._mask_value,\n-          oov_tokens=oov_indices)\n-\n-      tracked_table = self._add_trackable(self._table, trainable=False)\n-\n-    else:\n-      self._has_static_table = False\n-      self._table = tf.lookup.experimental.MutableHashTable(\n-          key_dtype=self._key_dtype,\n-          value_dtype=self._value_dtype,\n-          default_value=default_value,\n-          name=(self._name + \"_index_table\"))\n-      self._table_handler = table_utils.TableHandler(\n-          table=self._table,\n-          oov_tokens=oov_indices)\n-      if vocabulary is not None:\n-        self.set_vocabulary(vocabulary)\n-      tracked_table = self._add_trackable(self._table, trainable=False)\n+    self._default_value = tf.convert_to_tensor(default_value, self._value_dtype)\n+    if self.mask_token is not None:\n+      self._mask_key = tf.convert_to_tensor(mask_key, self._key_dtype)\n+      self._mask_value = tf.convert_to_tensor(mask_value, self._value_dtype)\n \n     if self.output_mode == TF_IDF:\n-      # The TF-IDF weight may have a (None,) tensorshape. This creates\n-      # a 1D variable with arbitrary shape, which we can assign any weight to\n-      # so long as it has 1 dimension. In order to properly initialize this\n-      # weight in Keras, we need to provide a custom callable initializer which\n-      # does not depend on the shape of the weight (as all other initializers\n-      # do) since the weight is not known. Hence the lambda shape, dtype: [0].\n-      if not self.pad_to_max_tokens or max_tokens is None:\n-        initializer = lambda shape, dtype: [0]\n-      else:\n-        initializer = \"zeros\"\n-\n-      # We are adding these here instead of in build() since they do not depend\n-      # on the input shape at all.\n-      idf_shape = (max_tokens,) if self.pad_to_max_tokens else (None,)\n-      self.tf_idf_weights = self._add_state_variable(\n-          name=\"idf\",\n-          shape=tf.TensorShape(idf_shape),\n+      self.idf_weights = tf.Variable(\n+          [0] * self._token_start_index(),\n+          shape=(None,),\n           dtype=backend.floatx(),\n-          initializer=initializer)\n+          trainable=False)\n+      self.idf_weights_const = self.idf_weights.value()\n \n-    # This is a workaround for summary() on this layer. Because the table is\n-    # not mutable during training, the effective number of parameters (and so\n-    # the weight shape) is 0; we add this as an attr so that the parameter\n-    # counting code in the Model object doesn't throw an attribute error.\n-    tracked_table.shape = tf.TensorShape((0,))\n+    if vocabulary is not None:\n+      self.set_vocabulary(vocabulary)\n+    else:\n+      # When restoring from a keras SavedModel, the loading code will expect to\n+      # find and restore a lookup_table attribute on the layer. This table needs\n+      # to be uninitialized as a StaticHashTable cannot be initialized twice.\n+      self.lookup_table = self._uninitialized_lookup_table()\n+      if not self._has_input_vocabulary:\n+        # Add a custom weight handler to return the layers vocab as it's weight.\n+        self._add_trackable(VocabWeightHandler(self), False)\n+        # Set adapt state.\n+        self.token_counts = tf.lookup.experimental.MutableHashTable(\n+            key_dtype=self.dtype, value_dtype=tf.int64, default_value=0)\n+        if self.output_mode == TF_IDF:\n+          self.token_document_counts = tf.lookup.experimental.MutableHashTable(\n+              key_dtype=self.dtype, value_dtype=tf.int64, default_value=0)\n+          self.num_documents = tf.Variable(0, dtype=tf.int64, trainable=False)\n \n   def compute_output_shape(self, input_shape):\n     if self.output_mode == INT:\n       return input_shape\n-    if self._vocab_size and not self.pad_to_max_tokens:\n-      out_depth = self._vocab_size\n-    else:\n+    if self.pad_to_max_tokens:\n       out_depth = self.max_tokens\n+    else:\n+      out_depth = self.vocabulary_size()\n     return tf.TensorShape([input_shape[0], out_depth])\n \n   def compute_output_signature(self, input_spec):\n     output_shape = self.compute_output_shape(input_spec.shape.as_list())\n-    output_dtype = (self._value_dtype if self.output_mode == INT\n-                    else backend.floatx())\n+    output_dtype = (\n+        self._value_dtype if self.output_mode == INT else backend.floatx())\n     return tf.TensorSpec(shape=output_shape, dtype=output_dtype)\n \n-  def adapt(self, data, reset_state=True):\n-    \"\"\"Fits the state of the preprocessing layer to the dataset.\n-\n-    Overrides the default adapt method to apply relevant preprocessing to the\n-    inputs before passing to the combiner.\n-\n-    Args:\n-      data: The data to train on. It can be passed either as a tf.data Dataset,\n-        or as a numpy array.\n-      reset_state: Optional argument specifying whether to clear the state of\n-        the layer at the start of the call to `adapt`. This must be True for\n-        this layer, which does not support repeated calls to `adapt`.\n-    \"\"\"\n-    if not reset_state:\n-      raise ValueError(\"IndexLookup does not support streaming adapts.\")\n-    super(IndexLookup, self).adapt(data, reset_state)\n-\n   def get_vocabulary(self, include_special_tokens=True):\n     \"\"\"Returns the current vocabulary of the layer.\n \n@@ -377,16 +294,16 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n         term's index when calling the layer. If False, the returned vocabulary\n         will not include any mask or OOV tokens.\n     \"\"\"\n-    if self.vocabulary_size() is None:\n-      return []\n-\n-    # The MutableHashTable data will not be sorted, so we will create a inverted\n+    # The lookup table data will not be sorted, so we will create a inverted\n     # lookup here, and use that to lookup a range of indices [0, vocab_size).\n-    keys, values = self._table.export()\n+    if self.lookup_table.size() == 0:\n+      vocab, indices = [], []\n+    else:\n+      keys, values = self.lookup_table.export()\n       vocab, indices = (values, keys) if self.invert else (keys, values)\n-    lookup = collections.defaultdict(\n-        lambda: self.oov_token,\n-        zip(indices.numpy(), self._tensor_vocab_to_numpy(vocab)))\n+      vocab, indices = (self._tensor_vocab_to_numpy(vocab), indices.numpy())\n+    lookup = collections.defaultdict(lambda: self.oov_token,\n+                                     zip(indices, vocab))\n     vocab = [lookup[x] for x in range(self.vocabulary_size())]\n     if self.mask_token is not None and self.output_mode == INT:\n       vocab[0] = self.mask_token\n@@ -400,18 +317,13 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n     Returns:\n       The integer size of the voculary, including optional mask and oov indices.\n     \"\"\"\n-    return self._vocab_size\n+    return int(self.lookup_table.size().numpy()) + self._token_start_index()\n \n   def vocab_size(self):\n     logging.warning(\"vocab_size is deprecated, please use vocabulary_size.\")\n     return self.vocabulary_size()\n \n   def get_config(self):\n-    if self._has_static_table:\n-      vocabulary_path = self._static_vocabulary_path\n-    else:\n-      vocabulary_path = None\n-\n     config = {\n         \"invert\": self.invert,\n         \"max_tokens\": self.max_tokens,\n@@ -420,22 +332,12 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n         \"mask_token\": self.mask_token,\n         \"output_mode\": self.output_mode,\n         \"pad_to_max_tokens\": self.pad_to_max_tokens,\n-        \"vocabulary_size\": self.vocabulary_size(),\n-        \"vocabulary\": vocabulary_path,\n+        \"vocabulary\": self._make_serializable(self.input_vocabulary),\n     }\n-    if self._has_static_table:\n-      config[\"has_static_table\"] = True\n \n-    base_config = super(IndexLookup, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n-  def count_params(self):\n-    # This method counts the number of scalars in the weights of this layer.\n-    # Since this layer doesn't have any /actual/ weights (in that there's\n-    # nothing in this layer that can be trained - we only use the weight\n-    # abstraction for ease of saving!) we return 0.\n-    return 0\n-\n   def set_vocabulary(self, vocabulary, idf_weights=None):\n     \"\"\"Sets vocabulary (and optionally document frequency) data for this layer.\n \n@@ -446,36 +348,39 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n     it.\n \n     Args:\n-      vocabulary: An array, numpy array, or tensor of hashable tokens.\n-      idf_weights: An array, numpy array, or tensor of inverse document\n-        frequency weights with equal length to vocab. Only necessary if the\n-        layer output_mode is TF_IDF.\n+      vocabulary: Either an array or a string path to a text file. If passing an\n+        array, can pass a tuple, list, 1D numpy array, or 1D tensor containing\n+        the vocbulary terms. If passing a file path, the file should contain one\n+        line per term in the vocabulary.\n+      idf_weights: A tuple, list, 1D numpy array, or 1D tensor of inverse\n+        document frequency weights with equal length to vocabulary. Must be set\n+        if `output_mode` is `\"tf_idf\"`. Should not be set otherwise.\n \n     Raises:\n       ValueError: If there are too many inputs, the inputs do not match, or\n         input data is missing.\n       RuntimeError: If the vocabulary cannot be set when this function is\n-        called. This happens when `\"multi_hot\"`, `\"count\"`, and `\"tfidf\"` modes,\n-        if `pad_to_max_tokens` is False and the layer itself has already been\n-        called.\n+        called. This happens when `\"multi_hot\"`, `\"count\"`, and `\"tf_idf\"`\n+        modes, if `pad_to_max_tokens` is False and the layer itself has already\n+        been called.\n       RuntimeError: If a tensor vocabulary is passed outside of eager execution.\n     \"\"\"\n-    if self._has_static_table:\n-      raise RuntimeError(\"Layer {} was created with a static file-based table \"\n-                         \"because a file path was passed to the layer \"\n-                         \"init. Layers created with static file-based tables \"\n-                         \"do not support changing the vocabulary after \"\n-                         \"creation.\".format(self.name))\n+    self.input_vocabulary = vocabulary\n+    self._has_input_vocabulary = True\n \n     if self.output_mode != TF_IDF and idf_weights is not None:\n       raise ValueError(\"`idf_weights` should only be set if output_mode is \"\n                        \"TF_IDF. output_mode is {}.\".format(self.output_mode))\n \n-    if (self.output_mode in [MULTI_HOT, COUNT, TF_IDF] and self._called and\n-        not self.pad_to_max_tokens):\n-      raise RuntimeError(\"When using {} mode and `pad_to_max_tokens` is \"\n-                         \"False, the vocabulary cannot be changed after the \"\n-                         \"layer is called.\".format(self.output_mode))\n+    if isinstance(vocabulary, str):\n+      if not os.path.exists(vocabulary):\n+        raise ValueError(\n+            \"Vocabulary file {} does not exist.\".format(vocabulary))\n+      if self.output_mode == TF_IDF:\n+        raise ValueError(\"output_mode `'tf_idf'` does not support loading a \"\n+                         \"vocabulary from file.\")\n+      self.lookup_table = self._lookup_table_from_file(vocabulary)\n+      return\n \n     if not tf.executing_eagerly() and (tf.is_tensor(vocabulary) or\n                                        tf.is_tensor(idf_weights)):\n@@ -489,8 +394,16 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n     # function to operate on tensors and convert vocabulary to a tensor here.\n     if tf.is_tensor(vocabulary):\n       vocabulary = self._tensor_vocab_to_numpy(vocabulary)\n+    elif isinstance(vocabulary, (list, tuple)):\n+      vocabulary = np.array(vocabulary)\n     if tf.is_tensor(idf_weights):\n       idf_weights = idf_weights.numpy()\n+    elif isinstance(idf_weights, (list, tuple)):\n+      idf_weights = np.array(idf_weights)\n+\n+    if vocabulary.size == 0:\n+      raise ValueError(\n+          \"Cannot set an empty vocabulary, you passed {}.\".format(vocabulary))\n \n     oov_start = self._oov_start_index()\n     token_start = self._token_start_index()\n@@ -542,31 +455,34 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n     else:\n       tokens = vocabulary\n \n-    repeated_tokens = table_utils.find_repeated_tokens(tokens)\n+    repeated_tokens = self._find_repeated_tokens(tokens)\n     if repeated_tokens:\n       raise ValueError(\"The passed vocabulary has at least one repeated \"\n                        \"term. Please uniquify your dataset. The repeated terms \"\n                        \"are {}\".format(repeated_tokens))\n \n     if self.mask_token in tokens:\n+      mask_index = np.argwhere(tokens == self.mask_token)[0]\n       raise ValueError(\"Reserved mask token {} was found in the passed \"\n                        \"vocabulary at index {}. Please either remove the \"\n                        \"reserved token from the vocabulary or change the \"\n                        \"mask token for this layer.\".format(\n-                           self.mask_token, tokens.index(self.mask_token)))\n+                           self.mask_token, mask_index))\n     if self.oov_token in tokens:\n+      oov_index = np.argwhere(tokens == self.oov_token)[0]\n       raise ValueError(\"Reserved OOV token {} was found in the passed \"\n                        \"vocabulary at index {}. Please either remove the \"\n                        \"reserved token from the vocabulary or change the \"\n                        \"OOV token for this layer.\".format(\n-                           self.oov_token, tokens.index(self.oov_token)))\n+                           self.oov_token, oov_index))\n \n-    self._vocab_size = token_start + len(tokens)\n-    if self.max_tokens is not None and self._vocab_size > self.max_tokens:\n+    new_vocab_size = token_start + len(tokens)\n+    if self.max_tokens is not None and (new_vocab_size > self.max_tokens):\n       raise ValueError(\n           \"Attempted to set a vocabulary larger than the maximum vocab size. \"\n           \"Passed vocab size is {}, max vocab size is {}.\".format(\n-              self._vocab_size, self.max_tokens))\n+              new_vocab_size, self.max_tokens))\n+    self.lookup_table = self._lookup_table_from_tokens(tokens)\n \n     if self.output_mode == TF_IDF:\n       if idf_weights is None:\n@@ -581,19 +497,6 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n             \"TF-IDF data must be a 1-index array, but received {}\".format(\n                 type(idf_weights)))\n \n-    # We add the non-special vocab tokens and optionally the mask_token to our\n-    # hash table. OOV tokens are handled with the hash table default value and\n-    # not added directly.\n-    self._table_handler.clear()\n-    indices = np.arange(token_start, len(tokens) + token_start, dtype=np.int64)\n-    if self.invert:\n-      self._table_handler.insert(indices, tokens)\n-    else:\n-      self._table_handler.insert(tokens, indices)\n-    if self.mask_token is not None:\n-      self._table_handler.insert([self._mask_key], [self._mask_value])\n-\n-    if self.output_mode == TF_IDF:\n       # If the passed vocabulary has no special tokens, we need to pad the front\n       # of idf_weights. We don't have real document frequencies for these tokens\n       # so we will use an average of all idf_weights passed in as a reasonable\n@@ -611,97 +514,234 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n         back_padding = self.max_tokens - front_padding - len(idf_weights)\n       else:\n         back_padding = 0\n-      idf_weights = np.pad(\n+      weights = np.pad(\n           idf_weights, (front_padding, back_padding),\n           \"constant\",\n           constant_values=(front_padding_value, back_padding_value))\n-      backend.set_value(self.tf_idf_weights, idf_weights)\n+      weights = tf.convert_to_tensor(weights, dtype=backend.floatx())\n+      self.idf_weights.assign(weights)\n+      self.idf_weights_const = self.idf_weights.value()\n \n-  def _set_state_variables(self, updates):\n-    if not self.built:\n-      raise RuntimeError(\"_set_state_variables() must be called after build().\")\n-    self.set_vocabulary(\n-        updates[_VOCAB_NAME], idf_weights=updates[_IDF_WEIGHTS_NAME])\n+  def update_state(self, data):\n+    if self._has_input_vocabulary:\n+      raise ValueError(\n+          \"Cannot adapt {} layer after setting a static vocabulary via init \"\n+          \"argument or `set_vocabulary`.\".format(self.__class__.__name__))\n+\n+    data = self._standardize_inputs(data, self.dtype)\n+    if data.shape.rank == 0:\n+      data = tf.expand_dims(data, -1)\n+    if data.shape.rank == 1:\n+      data = tf.expand_dims(data, -1)\n+\n+    tokens, counts = self._num_tokens(data)\n+    self.token_counts.insert(tokens, counts + self.token_counts.lookup(tokens))\n+\n+    if self.output_mode == TF_IDF:\n+      # Dedupe each row of our dataset.\n+      deduped_doc_data = tf.map_fn(lambda x: tf.unique(x)[0], data)\n+      # Flatten and count tokens.\n+      tokens, doc_counts = self._num_tokens(deduped_doc_data)\n+      self.token_document_counts.insert(\n+          tokens, doc_counts + self.token_document_counts.lookup(tokens))\n+      if tf_utils.is_ragged(data):\n+        self.num_documents.assign_add(data.nrows())\n+      else:\n+        self.num_documents.assign_add(tf.shape(data, out_type=tf.int64)[0])\n+\n+  def finalize_state(self):\n+    if self._has_input_vocabulary or tf.equal(self.token_counts.size(), 0):\n+      # Finalize idf_weights to a const for call even if we don't need to\n+      # compute a new vocabulary.\n+      if self.output_mode == TF_IDF:\n+        self.idf_weights_const = self.idf_weights.value()\n+      return\n+\n+    # Remove special tokens from our counts.\n+    if self.mask_token is not None:\n+      self.token_counts.remove(\n+          tf.convert_to_tensor([self.mask_token], self.dtype))\n+    if self.oov_token is not None:\n+      self.token_counts.remove(\n+          tf.convert_to_tensor([self.oov_token], self.dtype))\n+\n+    tokens, counts = self.token_counts.export()\n+    # To keep vocabs deterministic, we sort our tokens by count and break ties\n+    # by sorting the tokens themselves. Tensorflow has no ops for sorting\n+    # strings, so we need to use numpy for the sort.\n+    sorted_indices = np.lexsort((tokens.numpy(), counts.numpy()))[::-1]\n+    token_start = self._token_start_index()\n+    if self.max_tokens:\n+      max_learned_tokens = self.max_tokens - token_start\n+      sorted_indices = sorted_indices[:max_learned_tokens]\n+    tokens = tf.gather(tokens, sorted_indices)\n+    self.lookup_table = self._lookup_table_from_tokens(tokens)\n+\n+    if self.output_mode == TF_IDF:\n+      token_document_counts = self.token_document_counts.lookup(tokens)\n+      idf_weights = self._inverse_document_frequency(token_document_counts,\n+                                                     self.num_documents)\n+      idf_weights = tf.cast(idf_weights, backend.floatx())\n+      # Pad the front of idf_weights with the average idf weight for OOV tokens.\n+      # We cannot compute the real idf weight of OOV in a single pass.\n+      idf_weights = tf.pad(\n+          idf_weights, [[self._token_start_index(), 0]],\n+          constant_values=tf.reduce_mean(idf_weights))\n+      self.idf_weights.assign(idf_weights)\n+      self.idf_weights_const = self.idf_weights.value()\n+\n+    # We call this here to save memory, now that we've built our vocabulary, we\n+    # don't want to keep every token we've seen in separate lookup tables.\n+    self.reset_state()\n+\n+  def reset_state(self):  # pylint: disable=method-hidden\n+    if self._has_input_vocabulary:\n+      return\n+\n+    self.token_counts.remove(self.token_counts.export()[0])\n+    if self.output_mode == TF_IDF:\n+      self.token_document_counts.remove(self.token_document_counts.export()[0])\n+      self.num_documents.assign(0)\n \n   def call(self, inputs):\n-    if isinstance(inputs, (list, tuple, np.ndarray)):\n-      inputs = tf.convert_to_tensor(inputs)\n+    self._maybe_freeze_vocab_size()\n \n-    if not self.max_tokens and self._vocab_size is None:\n-      raise ValueError(\"You must set the layer's vocabulary before calling it. \"\n-                       \"Either pass a `vocabulary` argument to the layer, or \"\n-                       \"call `layer.adapt(dataset)` with some sample data.\")\n-    self._called = True\n-    if self._key_dtype == tf.int64 and inputs.dtype == tf.int32:\n-      inputs = tf.cast(inputs, tf.int64)\n-    lookup_result = self._table_handler.lookup(inputs)\n+    inputs = self._standardize_inputs(inputs, self._key_dtype)\n+    original_shape = inputs.shape\n+    # Some ops will not handle scalar input, so uprank to rank 1.\n+    if inputs.shape.rank == 0:\n+      inputs = self._expand_dims(inputs, -1)\n+\n+    if tf_utils.is_sparse(inputs):\n+      lookups = tf.SparseTensor(inputs.indices,\n+                                self._lookup_dense(inputs.values),\n+                                inputs.dense_shape)\n+    elif tf_utils.is_ragged(inputs):\n+      lookups = tf.ragged.map_flat_values(self._lookup_dense, inputs)\n+    else:\n+      lookups = self._lookup_dense(inputs)\n+\n+    if self.output_mode == INT:\n+      # If we received a scalar input, downrank back to a scalar.\n+      if original_shape.rank == 0:\n+        lookups = tf.squeeze(lookups, -1)\n+      return lookups\n+\n+    # One hot will unprank only if the final output dimension is not already 1.\n+    if self.output_mode == ONE_HOT:\n+      if lookups.shape[-1] != 1:\n+        lookups = self._expand_dims(lookups, -1)\n+\n+    # TODO(b/190445202): remove output rank restriction.\n+    if lookups.shape.rank > 2:\n+      raise ValueError(\n+          \"Received input shape {}, which would result in output rank {}. \"\n+          \"Currently only outputs up to rank 2 are supported for \"\n+          \"`output_mode={}`.\".format(original_shape, lookups.shape.rank,\n+                                     self.output_mode))\n+\n+    binary_output = self.output_mode in (MULTI_HOT, ONE_HOT)\n+    if self.pad_to_max_tokens:\n+      out_depth = self.max_tokens\n+    else:\n+      out_depth = self._frozen_vocab_size\n+    if self.sparse:\n+      bincounts = category_encoding.sparse_bincount(lookups, out_depth,\n+                                                    binary_output)\n+    else:\n+      bincounts = category_encoding.dense_bincount(lookups, out_depth,\n+                                                   binary_output)\n+\n+    if self.output_mode == TF_IDF:\n+      return tf.multiply(bincounts, self.idf_weights_const)\n+\n+    return bincounts\n+\n+  def _lookup_dense(self, inputs):\n+    \"\"\"Lookup table values for a dense Tensor, handling masking and OOV.\"\"\"\n+    # Avoid calling lookup on the table when tracing keras.Inputs. This is\n+    # critical for SavedModel, which expects an uninitialized StaticHashTable\n+    # and will trace the model before restoring the table.\n+    if backend.is_keras_tensor(inputs):\n+      lookups = tf.zeros_like(inputs, dtype=self._value_dtype)\n+    else:\n+      lookups = self.lookup_table.lookup(inputs)\n+\n+    if self.mask_token is not None:\n+      mask_locations = tf.equal(inputs, self._mask_key)\n+      lookups = tf.where(mask_locations, self._mask_value, lookups)\n+\n+    if self.invert:\n+      return lookups\n \n     lookup_checks = []\n \n-    if self.num_oov_indices == 0 and not self.invert:\n-      if tf_utils.is_sparse(inputs):\n-        lookup_values = lookup_result.values\n-        input_values = inputs.values\n-      elif tf_utils.is_ragged(inputs):\n-        lookup_values = lookup_result.flat_values\n-        input_values = inputs.flat_values\n-      else:\n-        lookup_values = lookup_result\n-        input_values = inputs\n-      # tf.where needs rank > 0.\n-      if input_values.shape.rank == 0:\n-        input_values = self._expand_dims(input_values, -1)\n-        lookup_values = self._expand_dims(lookup_values, -1)\n-      oov_indices = tf.where(tf.equal(lookup_values, -1))\n-      oov_inputs = tf.gather_nd(input_values, oov_indices)\n+    if self.num_oov_indices == 0:\n+      # If we have zero oov indices, we need to check for oov inputs.\n+      oov_indices = tf.where(tf.equal(lookups, -1))\n+      oov_inputs = tf.gather_nd(inputs, oov_indices)\n       msg = tf.strings.format(\n           \"When `num_oov_indices=0` all inputs should be in vocabulary, \"\n           \"found OOV values {}, consider setting `num_oov_indices=1`.\",\n           (oov_inputs,))\n-      assertion = tf.Assert(\n-          tf.equal(tf.size(oov_indices), 0), [msg])\n+      assertion = tf.Assert(tf.equal(tf.size(oov_indices), 0), [msg])\n       lookup_checks.append(assertion)\n+    elif self.num_oov_indices > 1:\n+      # If we have multiple oov indices, we need a further hashing step.\n+      if self._key_dtype.is_integer:\n+        oov_indices = tf.math.floormod(inputs, self.num_oov_indices)\n+      else:\n+        oov_indices = tf.strings.to_hash_bucket_fast(\n+            inputs, num_buckets=self.num_oov_indices)\n+      oov_indices = oov_indices + self._oov_start_index()\n+      oov_locations = tf.equal(lookups, self._default_value)\n+      lookups = tf.where(oov_locations, oov_indices, lookups)\n \n     with tf.control_dependencies(lookup_checks):\n-      if self.output_mode == INT:\n-        return tf.identity(lookup_result)\n+      return tf.identity(lookups)\n+\n+  def _encode_output(self, lookups):\n+    \"\"\"Encode the lookup result to the final output depending on output_mode.\"\"\"\n+\n+  def _uninitialized_lookup_table(self):\n+    initializer = NullInitializer(self._key_dtype, self._value_dtype)\n+    with tf.init_scope():\n+      return tf.lookup.StaticHashTable(initializer, self._default_value)\n+\n+  def _lookup_table_from_tokens(self, tokens):\n+    token_start = self._token_start_index()\n+    token_end = token_start + tf.size(tokens)\n+    indices = tf.range(token_start, token_end, dtype=tf.int64)\n+    keys, values = (indices, tokens) if self.invert else (tokens, indices)\n+    initializer = tf.lookup.KeyValueTensorInitializer(keys, values,\n+                                                      self._key_dtype,\n+                                                      self._value_dtype)\n+    with tf.init_scope():\n+      return tf.lookup.StaticHashTable(initializer, self._default_value)\n+\n+  def _lookup_table_from_file(self, filename):\n+    if self.invert:\n+      key_index = tf.lookup.TextFileIndex.LINE_NUMBER\n+      value_index = tf.lookup.TextFileIndex.WHOLE_LINE\n     else:\n-        return self._encode_output(lookup_result)\n+      key_index = tf.lookup.TextFileIndex.WHOLE_LINE\n+      value_index = tf.lookup.TextFileIndex.LINE_NUMBER\n+    initializer = tf.lookup.TextFileInitializer(\n+        filename=filename,\n+        key_dtype=self._key_dtype,\n+        key_index=key_index,\n+        value_dtype=self._value_dtype,\n+        value_index=value_index,\n+        value_index_offset=self._token_start_index())\n+    with tf.init_scope():\n+      return tf.lookup.StaticHashTable(initializer, self._default_value)\n \n-  def _encode_output(self, lookup_result):\n-    original_shape = lookup_result.shape\n-    # In all cases, we should uprank scalar input to a single sample.\n-    if lookup_result.shape.rank == 0:\n-      lookup_result = self._expand_dims(lookup_result, -1)\n-    # One hot will unprank only if the final output dimension is not already 1.\n-    if self.output_mode == ONE_HOT:\n-      if lookup_result.shape[-1] != 1:\n-        lookup_result = self._expand_dims(lookup_result, -1)\n-\n-    # TODO(b/190445202): remove output rank restriction.\n-    if lookup_result.shape.rank > 2:\n-      raise ValueError(\n-          \"Received input shape {}, which would result in output rank {}. \"\n-          \"Currently only outputs up to rank 2 are supported for \"\n-          \"`output_mode={}`.\".format(original_shape, lookup_result.shape.rank,\n-                                     self.output_mode))\n-\n-    binary_output = self.output_mode in (MULTI_HOT, ONE_HOT)\n-    if self._vocab_size and not self.pad_to_max_tokens:\n-      out_depth = self._vocab_size\n-    else:\n-      out_depth = self.max_tokens\n-    if self.sparse:\n-      bincounts = category_encoding.sparse_bincount(lookup_result, out_depth,\n-                                                    binary_output)\n-    else:\n-      bincounts = category_encoding.dense_bincount(lookup_result, out_depth,\n-                                                   binary_output)\n-\n-    if self.output_mode == TF_IDF:\n-      return tf.multiply(bincounts, self.tf_idf_weights)\n-\n-    return bincounts\n+  def _standardize_inputs(self, inputs, dtype):\n+    if isinstance(inputs, (list, tuple, np.ndarray)):\n+      inputs = tf.convert_to_tensor(inputs)\n+    if inputs.dtype != dtype:\n+      inputs = tf.cast(inputs, dtype)\n+    return inputs\n \n   def _convert_to_ndarray(self, x):\n     return np.array(x) if isinstance(x, (list, tuple)) else x\n@@ -710,7 +750,15 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n     if tf_utils.is_sparse(inputs):\n       return tf.sparse.expand_dims(inputs, axis)\n     else:\n-      return tf.compat.v1.expand_dims(inputs, axis)\n+      return tf.expand_dims(inputs, axis)\n+\n+  def _make_serializable(self, x):\n+    if tf.is_tensor(x):\n+      x = x.numpy()\n+    if isinstance(x, (np.ndarray)):\n+      x = x.tolist()\n+      x = list(x)\n+    return x\n \n   def _oov_start_index(self):\n     return 1 if self.mask_token is not None and self.output_mode == INT else 0\n@@ -718,6 +766,64 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n   def _token_start_index(self):\n     return self._oov_start_index() + self.num_oov_indices\n \n+  def _maybe_freeze_vocab_size(self):\n+    if self.output_mode == INT or self.pad_to_max_tokens:\n+      return\n+    with tf.init_scope():\n+      new_vocab_size = self.vocabulary_size()\n+    if new_vocab_size == self._token_start_index():\n+      raise RuntimeError(\n+          \"When using `output_mode={}` and `pad_to_max_tokens=False`, you \"\n+          \"must set the layer's vocabulary before calling it. Either pass \"\n+          \"a `vocabulary` argument to the layer, or call `adapt` with some \"\n+          \"sample data.\".format(self.output_mode))\n+    elif (self._frozen_vocab_size is not None and\n+          new_vocab_size != self._frozen_vocab_size):\n+      raise RuntimeError(\n+          \"When using `output_mode={}` and `pad_to_max_tokens=False`, the \"\n+          \"vocabulary size cannot be changed after the layer is called. \"\n+          \"Vocab size is {}, new vocab size is {}\".format(\n+              self.output_mode, self._frozen_vocab_size, new_vocab_size))\n+    self._frozen_vocab_size = new_vocab_size\n+\n+  def _find_repeated_tokens(self, vocabulary):\n+    \"\"\"Return all repeated tokens in a vocabulary.\"\"\"\n+    vocabulary_set = set(vocabulary)\n+    if len(vocabulary) != len(vocabulary_set):\n+      return [\n+          item for item, count in collections.Counter(vocabulary).items()\n+          if count > 1\n+      ]\n+    else:\n+      return []\n+\n+  def _num_tokens(self, data):\n+    \"\"\"Count the number of tokens in a ragged, sparse or dense tensor.\"\"\"\n+    if tf_utils.is_sparse(data):\n+      flat_values = data.values\n+    elif tf_utils.is_ragged(data):\n+      flat_values = data.flat_values\n+    else:\n+      flat_values = tf.reshape(data, [-1])\n+    tokens, _, counts = tf.unique_with_counts(flat_values, out_idx=tf.int64)\n+    return tokens, counts\n+\n+  def _inverse_document_frequency(self, token_document_counts, num_documents):\n+    \"\"\"Computes the inverse-document-frequency (IDF) component of \"tf_idf\".\n+\n+    Uses the default weighting scheme described in\n+    https://en.wikipedia.org/wiki/Tf%E2%80%93idf.\n+\n+    Args:\n+      token_document_counts: An array of the # of documents each token appears\n+        in.\n+      num_documents: An int representing the total number of documents\n+\n+    Returns:\n+      An array of \"inverse document frequency\" weights.\n+    \"\"\"\n+    return tf.math.log(1 + num_documents / (1 + token_document_counts))\n+\n   @property\n   def _trackable_saved_model_saver(self):\n     return layer_serialization.IndexLookupLayerSavedModelSaver(self)\n@@ -726,196 +832,3 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n   def _tensor_vocab_to_numpy(self, vocabulary):\n     \"\"\"Converts a tensor vocabulary to a numpy vocabulary.\"\"\"\n     return vocabulary.numpy()\n-\n-\n-class _IndexLookupAccumulator(\n-    collections.namedtuple(\"Accumulator\",\n-                           [\"data\", \"count_dict\", \"per_doc_count_dict\"])):\n-  pass\n-\n-\n-class _IndexLookupCombiner(base_preprocessing_layer.Combiner):\n-  \"\"\"Combiner for the IndexLookup preprocessing layer.\n-\n-  This class encapsulates the logic for computing a vocabulary based on the\n-  frequency of each token.\n-\n-  Attributes:\n-    vocab_size: (Optional) If set, only the top `vocab_size` tokens (based on\n-      frequency across the dataset) are retained in the vocabulary. If None, or\n-      set to a value greater than the total number of distinct tokens in the\n-      dataset, all tokens are retained.\n-  \"\"\"\n-\n-  def __init__(self,\n-               vocab_size=None,\n-               mask_value=None,\n-               oov_value=None,\n-               compute_idf=False):\n-    self._vocab_size = vocab_size\n-    self._mask_value = mask_value\n-    self._oov_value = oov_value\n-    self._compute_idf = compute_idf\n-\n-  def compute(self, values, accumulator=None):\n-    \"\"\"Compute a step in this computation, returning a new accumulator.\"\"\"\n-    values = base_preprocessing_layer.convert_to_list(\n-        values, sparse_default_value=self._mask_value)\n-\n-    if accumulator is None:\n-      accumulator = self._create_accumulator()\n-\n-    # TODO(momernick): Benchmark improvements to this algorithm.\n-    if not isinstance(values, list):\n-      values = [values]\n-    for document in values:\n-      if not isinstance(document, list):\n-        document = [document]\n-      if self._compute_idf:\n-        current_doc_id = accumulator.data[\"next_doc_id\"]\n-        accumulator.data[\"next_doc_id\"] += 1\n-      for token in document:\n-        accumulator.count_dict[token] += 1\n-        if self._compute_idf:\n-          doc_count = accumulator.per_doc_count_dict[token]\n-          if doc_count[\"last_doc_id\"] != current_doc_id:\n-            doc_count[\"count\"] += 1\n-            doc_count[\"last_doc_id\"] = current_doc_id\n-\n-    return accumulator\n-\n-  def merge(self, accumulators):\n-    \"\"\"Merge several accumulators to a single accumulator.\"\"\"\n-    if not accumulators:\n-      return accumulators\n-\n-    base_accumulator = accumulators[0]\n-    for accumulator in accumulators[1:]:\n-      for token, value in accumulator.count_dict.items():\n-        base_accumulator.count_dict[token] += value\n-\n-      if self._compute_idf:\n-        base_accumulator.data[\"next_doc_id\"] += accumulator.data[\"next_doc_id\"]\n-        if self._compute_idf:\n-          for token, value in accumulator.per_doc_count_dict.items():\n-            # Any newly created token counts in 'base_accumulator''s\n-            # per_doc_count_dict will have a last_doc_id of -1. This is always\n-            # less than the next doc id (which are strictly positive), so any\n-            # future occurrences are guaranteed to be counted.\n-            base_accumulator.per_doc_count_dict[token][\"count\"] += value[\n-                \"count\"]\n-\n-    return base_accumulator\n-\n-  def extract(self, accumulator):\n-    \"\"\"Convert an accumulator into a dict of output values.\n-\n-    Args:\n-      accumulator: An accumulator aggregating over the full dataset.\n-\n-    Returns:\n-      A dict of:\n-        \"vocab\": A list of the retained items in the vocabulary.\n-    \"\"\"\n-    vocab_counts = accumulator.count_dict\n-\n-    # Drop special tokens from our vocab.\n-    if self._mask_value in vocab_counts:\n-      del vocab_counts[self._mask_value]\n-    if self._oov_value in vocab_counts:\n-      del vocab_counts[self._oov_value]\n-    # Data processed by the accumulator could be tensors, numpy arrays or lists.\n-    # For tensor string input, values will have been converted into bytes. We\n-    # need to check the bytes version of special tokens in this case.\n-    if isinstance(self._mask_value, str):\n-      mask_value_bytes = tf.compat.as_bytes(self._mask_value)\n-      if mask_value_bytes in vocab_counts:\n-        del vocab_counts[mask_value_bytes]\n-    if isinstance(self._oov_value, str):\n-      oov_value_bytes = tf.compat.as_bytes(self._oov_value)\n-      if oov_value_bytes in vocab_counts:\n-        del vocab_counts[oov_value_bytes]\n-\n-    sorted_counts = sorted(\n-        vocab_counts.items(), key=operator.itemgetter(1, 0), reverse=True)\n-    vocab_data = (\n-        sorted_counts[:self._vocab_size] if self._vocab_size else sorted_counts)\n-    vocab = [data[0] for data in vocab_data]\n-\n-    if self._compute_idf:\n-      num_documents = accumulator.data[\"next_doc_id\"]\n-      document_counts = accumulator.per_doc_count_dict\n-      doc_counts = [document_counts[token][\"count\"] for token in vocab]\n-      idf_weights = self._inverse_document_frequency(doc_counts, num_documents)\n-    else:\n-      idf_weights = None\n-\n-    return {_VOCAB_NAME: vocab, _IDF_WEIGHTS_NAME: idf_weights}\n-\n-  def restore(self, output):\n-    \"\"\"Create an accumulator based on 'output'.\"\"\"\n-    raise NotImplementedError(\n-        \"IndexLookup does not restore or support streaming updates.\")\n-\n-  def serialize(self, accumulator):\n-    \"\"\"Serialize an accumulator for a remote call.\"\"\"\n-    output_dict = {}\n-    output_dict[\"vocab\"] = list(accumulator.count_dict.keys())\n-    output_dict[\"vocab_counts\"] = list(accumulator.count_dict.values())\n-\n-    if self._compute_idf:\n-      output_dict[\"data\"] = accumulator.data\n-      output_dict[\"idf_vocab\"] = list(accumulator.per_doc_count_dict.keys())\n-      output_dict[\"idf_counts\"] = [\n-          counter[\"count\"]\n-          for counter in accumulator.per_doc_count_dict.values()\n-      ]\n-    return tf.compat.as_bytes(json.dumps(output_dict))\n-\n-  def deserialize(self, encoded_accumulator):\n-    \"\"\"Deserialize an accumulator received from 'serialize()'.\"\"\"\n-    accumulator_dict = json.loads(tf.compat.as_text(encoded_accumulator))\n-\n-    accumulator = self._create_accumulator()\n-    count_dict = dict(\n-        zip(accumulator_dict[\"vocab\"], accumulator_dict[\"vocab_counts\"]))\n-    accumulator.count_dict.update(count_dict)\n-\n-    if self._compute_idf:\n-      accumulator.data = accumulator_dict[\"data\"]\n-      create_dict = lambda x: {\"count\": x, \"last_doc_id\": -1}\n-      idf_count_dicts = [\n-          create_dict(count) for count in accumulator_dict[\"idf_counts\"]\n-      ]\n-      idf_dict = dict(zip(accumulator_dict[\"idf_vocab\"], idf_count_dicts))\n-      accumulator.per_doc_count_dict.update(idf_dict)\n-    return accumulator\n-\n-  def _create_accumulator(self):\n-    \"\"\"Accumulate a sorted array of vocab tokens and corresponding counts.\"\"\"\n-\n-    if self._compute_idf:\n-      create_default_dict = lambda: {\"count\": 0, \"last_doc_id\": -1}\n-      per_doc_count_dict = collections.defaultdict(create_default_dict)\n-      data = {\"next_doc_id\": 0}\n-    else:\n-      per_doc_count_dict = None\n-      data = None\n-\n-    count_dict = collections.defaultdict(int)\n-    return _IndexLookupAccumulator(data, count_dict, per_doc_count_dict)\n-\n-  def _inverse_document_frequency(self, document_counts, num_documents):\n-    \"\"\"Computes the inverse-document-frequency (IDF) component of TF-IDF.\n-\n-    Uses the default weighting scheme described in\n-    https://en.wikipedia.org/wiki/Tf%E2%80%93idf.\n-\n-    Args:\n-      document_counts: An array of the # of documents each token appears in.\n-      num_documents: An int representing the total number of documents\n-\n-    Returns:\n-      An array of \"inverse document frequency\" weights.\n-    \"\"\"\n-    return np.log(1 + num_documents / (1 + np.array(document_counts)))\n\n@@ -240,6 +240,7 @@ def _get_end_to_end_test_cases():\n               np.array([[1138], [1729], [725], [42], [42], [725], [1138], [4]]),\n           \"kwargs\": {\n               \"max_tokens\": 5,\n+              \"pad_to_max_tokens\": True,\n               \"num_oov_indices\": 1,\n               \"mask_token\": 0,\n               \"oov_token\": -1,\n@@ -264,6 +265,7 @@ def _get_end_to_end_test_cases():\n                         [\"and\"], [\"earth\"], [\"michigan\"]]),\n           \"kwargs\": {\n               \"max_tokens\": 5,\n+              \"pad_to_max_tokens\": True,\n               \"num_oov_indices\": 1,\n               \"mask_token\": \"\",\n               \"oov_token\": \"[OOV]\",\n@@ -1043,6 +1045,8 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n     first_output = model.predict(input_array)\n     # Test the second adapt\n     layer.adapt(second_adapt_data)\n+    # We need to recompile the model to retrace our call graph.\n+    model.compile()\n     second_output = model.predict(input_array)\n     self.assertAllEqual(first_expected_output, first_output)\n     self.assertAllEqual(second_expected_output, second_output)\n@@ -1079,6 +1083,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n         output_mode=index_lookup.MULTI_HOT,\n+        vocabulary=[\"foo\"],\n         dtype=tf.string)\n     binary_data = layer(input_data)\n     self.assertAllEqual(binary_data.shape.as_list(), [16, 2])\n@@ -1140,6 +1145,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n         output_mode=index_lookup.COUNT,\n+        vocabulary=[\"foo\"],\n         dtype=tf.string)\n     count_data = layer(input_data)\n     self.assertAllEqual(count_data.shape.as_list(), [16, 2])\n@@ -1204,8 +1210,9 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        output_mode=index_lookup.COUNT,\n+        output_mode=index_lookup.TF_IDF,\n         dtype=tf.string)\n+    layer.set_vocabulary(vocabulary=[\"foo\"], idf_weights=[1.0])\n     layer_output = layer(input_data)\n     self.assertAllEqual(layer_output.shape.as_list(), [16, 2])\n \n@@ -1282,16 +1289,11 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n     expected_vocab = [\"\", \"[OOV]\", \"earth\", \"wind\", \"and\", \"fire\"]\n     self.assertAllEqual(expected_vocab, list_layer.get_vocabulary())\n     expected_vocab_size = 6\n-    self.assertAllEqual(expected_vocab_size, list_layer.vocab_size())\n+    self.assertAllEqual(expected_vocab_size, list_layer.vocabulary_size())\n     self.assertAllEqual(list_layer.get_vocabulary(),\n                         file_layer.get_vocabulary())\n-    self.assertAllEqual(list_layer.vocab_size(), file_layer.vocab_size())\n-\n-    # We expect the weights to be DIFFERENT in these cases.\n-    expected_weights = ([\"\", \"earth\", \"wind\", \"and\", \"fire\"], [0, 2, 3, 4, 5])\n-    sorted_weights = zip_and_sort(expected_weights)\n-    self.assertAllEqual(sorted_weights, zip_and_sort(list_layer.get_weights()))\n-    self.assertAllEqual(0, len(file_layer.get_weights()))\n+    self.assertAllEqual(list_layer.vocabulary_size(),\n+                        file_layer.vocabulary_size())\n \n   def test_file_vocab_and_list_vocab_identical_attrs_multi_oov(self):\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n@@ -1317,15 +1319,11 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n     expected_vocab = [\"\", \"[OOV]\", \"[OOV]\", \"earth\", \"wind\", \"and\", \"fire\"]\n     self.assertAllEqual(expected_vocab, list_layer.get_vocabulary())\n     expected_vocab_size = 7\n-    self.assertAllEqual(expected_vocab_size, list_layer.vocab_size())\n+    self.assertAllEqual(expected_vocab_size, list_layer.vocabulary_size())\n     self.assertAllEqual(list_layer.get_vocabulary(),\n                         file_layer.get_vocabulary())\n-    self.assertAllEqual(list_layer.vocab_size(), file_layer.vocab_size())\n-\n-    expected_weights = ([\"\", \"earth\", \"wind\", \"and\", \"fire\"], [0, 3, 4, 5, 6])\n-    sorted_weights = zip_and_sort(expected_weights)\n-    self.assertAllEqual(sorted_weights, zip_and_sort(list_layer.get_weights()))\n-    self.assertAllEqual(0, len(file_layer.get_weights()))\n+    self.assertAllEqual(list_layer.vocabulary_size(),\n+                        file_layer.vocabulary_size())\n \n   def test_file_vocab_and_list_vocab_identical_attrs_no_mask(self):\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n@@ -1351,15 +1349,11 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n     expected_vocab = [\"[OOV]\", \"[OOV]\", \"earth\", \"wind\", \"and\", \"fire\"]\n     self.assertAllEqual(expected_vocab, list_layer.get_vocabulary())\n     expected_vocab_size = 6\n-    self.assertAllEqual(expected_vocab_size, list_layer.vocab_size())\n+    self.assertAllEqual(expected_vocab_size, list_layer.vocabulary_size())\n     self.assertAllEqual(list_layer.get_vocabulary(),\n                         file_layer.get_vocabulary())\n-    self.assertAllEqual(list_layer.vocab_size(), file_layer.vocab_size())\n-\n-    expected_weights = ([\"earth\", \"wind\", \"and\", \"fire\"], [2, 3, 4, 5])\n-    sorted_weights = zip_and_sort(expected_weights)\n-    self.assertAllEqual(sorted_weights, zip_and_sort(list_layer.get_weights()))\n-    self.assertAllEqual(0, len(file_layer.get_weights()))\n+    self.assertAllEqual(list_layer.vocabulary_size(),\n+                        file_layer.vocabulary_size())\n \n   def test_int_output_file_vocab_no_mask(self):\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n@@ -1454,22 +1448,6 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n     output_dataset = model.predict(input_array)\n     self.assertAllEqual(expected_output, output_dataset)\n \n-  def test_int_output_file_vocab_setting_fails(self):\n-    vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n-\n-    vocab_file = self._write_to_temp_file(\"temp\", vocab_data)\n-\n-    layer = index_lookup.IndexLookup(\n-        vocabulary=vocab_file,\n-        max_tokens=None,\n-        num_oov_indices=1,\n-        mask_token=\"\",\n-        oov_token=\"[OOV]\",\n-        dtype=tf.string)\n-\n-    with self.assertRaisesRegexp(RuntimeError, \"file path\"):\n-      layer.set_vocabulary(vocab_data)\n-\n \n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n@@ -1654,7 +1632,7 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n     with self.assertRaisesRegex(ValueError, \".*Reserved mask.*\"):\n       layer.set_vocabulary(vocab_data)\n \n-  def test_vocab_set_after_call_pad_to_max_false_fails(self):\n+  def test_vocab_size_changed_pad_to_max_false_fails(self):\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n     layer = index_lookup.IndexLookup(\n         max_tokens=None,\n@@ -1665,10 +1643,13 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n         output_mode=index_lookup.MULTI_HOT,\n         dtype=tf.string)\n     layer.set_vocabulary(vocab_data)\n-    # Calling the layer should lock the vocabulary.\n+    # Calling the layer should lock the vocabulary size.\n+    _ = layer([[\"earth\"]])\n+    layer.set_vocabulary(vocab_data[:2])\n+    with self.assertRaisesRegex(RuntimeError,\n+                                \"vocabulary size cannot be changed\"):\n+      # Calling the layer again should cause an error.\n       _ = layer([[\"earth\"]])\n-    with self.assertRaisesRegex(RuntimeError, \"vocabulary cannot be changed\"):\n-      layer.set_vocabulary(vocab_data)\n \n   def test_vocab_with_idf_weights_non_tfidf_output_fails(self):\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n@@ -1988,8 +1969,6 @@ class IndexLookupSavingTest(keras_parameterized.TestCase,\n \n     # Delete the session and graph to ensure that the loaded model is generated\n     # from scratch.\n-    # TODO(b/149526183): Can't clear session when TF2 is disabled.\n-    if tf.__internal__.tf2.enabled():\n     keras.backend.clear_session()\n \n     loaded_model = keras.models.load_model(\n@@ -2023,8 +2002,9 @@ class IndexLookupSavingTest(keras_parameterized.TestCase,\n     output_dataset = model.predict(input_array)\n     self.assertAllEqual(output_dataset, expected_output)\n \n-    # Clone the model.\n+    # Clone the model and set weights.\n     new_model = keras.models.clone_model(model)\n+    new_model.set_weights(model.get_weights())\n \n     # Ensure that the loaded model is unique (so that the save/load is real)\n     self.assertIsNot(model, new_model)\n@@ -2061,8 +2041,6 @@ class IndexLookupSavingTest(keras_parameterized.TestCase,\n \n     # Delete the session and graph to ensure that the loaded model is generated\n     # from scratch.\n-    # TODO(b/149526183): Can't clear session when TF2 is disabled.\n-    if tf.__internal__.tf2.enabled():\n     keras.backend.clear_session()\n \n     loaded_model = tf.saved_model.load(output_path)\n@@ -2103,8 +2081,6 @@ class IndexLookupSavingTest(keras_parameterized.TestCase,\n \n     # Delete the session and graph to ensure that the loaded model is generated\n     # from scratch.\n-    # TODO(b/149526183): Can't clear session when TF2 is disabled.\n-    if tf.__internal__.tf2.enabled():\n     keras.backend.clear_session()\n \n     loaded_model = tf.saved_model.load(output_path)\n@@ -2145,8 +2121,6 @@ class IndexLookupSavingTest(keras_parameterized.TestCase,\n \n     # Delete the session and graph to ensure that the loaded model is generated\n     # from scratch.\n-    # TODO(b/149526183): Can't clear session when TF2 is disabled.\n-    if tf.__internal__.tf2.enabled():\n     keras.backend.clear_session()\n     tf.io.gfile.remove(vocab_file)\n \n@@ -2174,8 +2148,6 @@ class IndexLookupSavingTest(keras_parameterized.TestCase,\n \n     # Delete the session and graph to ensure that the loaded model is generated\n     # from scratch.\n-    # TODO(b/149526183): Can't clear session when TF2 is disabled.\n-    if tf.__internal__.tf2.enabled():\n     keras.backend.clear_session()\n \n     loaded_model = keras.models.load_model(\n@@ -2216,8 +2188,6 @@ class IndexLookupSavingTest(keras_parameterized.TestCase,\n \n     # Delete the session and graph to ensure that the loaded model is generated\n     # from scratch.\n-    # TODO(b/149526183): Can't clear session when TF2 is disabled.\n-    if tf.__internal__.tf2.enabled():\n     keras.backend.clear_session()\n     tf.io.gfile.remove(vocab_file)\n \n@@ -2245,8 +2215,6 @@ class IndexLookupSavingTest(keras_parameterized.TestCase,\n \n     # Delete the session and graph to ensure that the loaded model is generated\n     # from scratch.\n-    # TODO(b/149526183): Can't clear session when TF2 is disabled.\n-    if tf.__internal__.tf2.enabled():\n     keras.backend.clear_session()\n \n     loaded_model = tf.saved_model.load(output_path)\n@@ -2288,8 +2256,6 @@ class IndexLookupSavingTest(keras_parameterized.TestCase,\n \n     # Delete the session and graph to ensure that the loaded model is generated\n     # from scratch.\n-    # TODO(b/149526183): Can't clear session when TF2 is disabled.\n-    if tf.__internal__.tf2.enabled():\n     keras.backend.clear_session()\n     tf.io.gfile.remove(vocab_file)\n \n@@ -2317,8 +2283,6 @@ class IndexLookupSavingTest(keras_parameterized.TestCase,\n \n     # Delete the session and graph to ensure that the loaded model is generated\n     # from scratch.\n-    # TODO(b/149526183): Can't clear session when TF2 is disabled.\n-    if tf.__internal__.tf2.enabled():\n     keras.backend.clear_session()\n \n     loaded_model = keras.models.load_model(\n@@ -2362,266 +2326,5 @@ class IndexLookupSavingTest(keras_parameterized.TestCase,\n     self.assertAllEqual(new_output_dataset, expected_output)\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class IndexLookupStringCombinerTest(\n-    keras_parameterized.TestCase,\n-    preprocessing_test_utils.PreprocessingLayerTest):\n-\n-  def compare_text_accumulators(self, a, b, msg=None):\n-    if a is None or b is None:\n-      self.assertAllEqual(a, b, msg=msg)\n-\n-    self.assertAllEqual(a.count_dict, b.count_dict, msg=msg)\n-\n-  compare_accumulators = compare_text_accumulators\n-\n-  def update_accumulator(self, accumulator, data):\n-    accumulator.count_dict.update(dict(zip(data[\"vocab\"], data[\"counts\"])))\n-\n-    return accumulator\n-\n-  def test_combiner_api_compatibility_int_mode(self):\n-    data = np.array([[\"earth\", \"wind\", \"and\", \"fire\"],\n-                     [\"earth\", \"wind\", \"and\", \"michigan\"]])\n-    combiner = index_lookup._IndexLookupCombiner()\n-    expected_accumulator_output = {\n-        \"vocab\": np.array([\"and\", \"earth\", \"wind\", \"fire\", \"michigan\"]),\n-        \"counts\": np.array([2, 2, 2, 1, 1]),\n-    }\n-    expected_extract_output = {\n-        \"vocab\": np.array([\"wind\", \"earth\", \"and\", \"michigan\", \"fire\"]),\n-        \"idf_weights\": None,\n-    }\n-    expected_accumulator = combiner._create_accumulator()\n-    expected_accumulator = self.update_accumulator(expected_accumulator,\n-                                                   expected_accumulator_output)\n-    self.validate_accumulator_serialize_and_deserialize(combiner, data,\n-                                                        expected_accumulator)\n-    self.validate_accumulator_uniqueness(combiner, data)\n-    self.validate_accumulator_extract(combiner, data, expected_extract_output)\n-\n-  # TODO(askerryryan): Add tests confirming equivalence to behavior of\n-  # existing tf.keras.preprocessing.text.Tokenizer.\n-  @parameterized.named_parameters(\n-      {\n-          \"testcase_name\":\n-              \"top_k_smaller_than_full_vocab\",\n-          \"data\":\n-              np.array([[\"earth\", \"wind\"], [\"fire\", \"wind\"], [\"and\"],\n-                        [\"fire\", \"wind\"]]),\n-          \"vocab_size\":\n-              3,\n-          \"expected_accumulator_output\": {\n-              \"vocab\": np.array([\"wind\", \"fire\", \"earth\", \"and\"]),\n-              \"counts\": np.array([3, 2, 1, 1]),\n-          },\n-          \"expected_extract_output\": {\n-              \"vocab\": np.array([\"wind\", \"fire\", \"earth\"]),\n-              \"idf_weights\": None,\n-          },\n-      },\n-      {\n-          \"testcase_name\":\n-              \"top_k_larger_than_full_vocab\",\n-          \"data\":\n-              np.array([[\"earth\", \"wind\"], [\"fire\", \"wind\"], [\"and\"],\n-                        [\"fire\", \"wind\"]]),\n-          \"vocab_size\":\n-              10,\n-          \"expected_accumulator_output\": {\n-              \"vocab\": np.array([\"wind\", \"fire\", \"earth\", \"and\"]),\n-              \"counts\": np.array([3, 2, 1, 1]),\n-          },\n-          \"expected_extract_output\": {\n-              \"vocab\": np.array([\"wind\", \"fire\", \"earth\", \"and\"]),\n-              \"idf_weights\": None,\n-          },\n-      },\n-      {\n-          \"testcase_name\":\n-              \"no_top_k\",\n-          \"data\":\n-              np.array([[\"earth\", \"wind\"], [\"fire\", \"wind\"], [\"and\"],\n-                        [\"fire\", \"wind\"]]),\n-          \"vocab_size\":\n-              None,\n-          \"expected_accumulator_output\": {\n-              \"vocab\": np.array([\"wind\", \"fire\", \"earth\", \"and\"]),\n-              \"counts\": np.array([3, 2, 1, 1]),\n-          },\n-          \"expected_extract_output\": {\n-              \"vocab\": np.array([\"wind\", \"fire\", \"earth\", \"and\"]),\n-              \"idf_weights\": None,\n-          },\n-      },\n-      {\n-          \"testcase_name\": \"single_element_per_row\",\n-          \"data\": np.array([[\"earth\"], [\"wind\"], [\"fire\"], [\"wind\"], [\"and\"]]),\n-          \"vocab_size\": 3,\n-          \"expected_accumulator_output\": {\n-              \"vocab\": np.array([\"wind\", \"and\", \"earth\", \"fire\"]),\n-              \"counts\": np.array([2, 1, 1, 1]),\n-          },\n-          \"expected_extract_output\": {\n-              \"vocab\": np.array([\"wind\", \"fire\", \"earth\"]),\n-              \"idf_weights\": None,\n-          },\n-      },\n-      # Which tokens are retained are based on global frequency, and thus are\n-      # sensitive to frequency within a document. In contrast, because idf only\n-      # considers the presence of a token in a document, it is insensitive\n-      # to the frequency of the token within the document.\n-      {\n-          \"testcase_name\":\n-              \"retained_tokens_sensitive_to_within_document_frequency\",\n-          \"data\":\n-              np.array([[\"earth\", \"earth\"], [\"wind\", \"wind\"], [\"fire\", \"fire\"],\n-                        [\"wind\", \"wind\"], [\"and\", \"michigan\"]]),\n-          \"vocab_size\":\n-              3,\n-          \"expected_accumulator_output\": {\n-              \"vocab\": np.array([\"wind\", \"earth\", \"fire\", \"and\", \"michigan\"]),\n-              \"counts\": np.array([4, 2, 2, 1, 1]),\n-          },\n-          \"expected_extract_output\": {\n-              \"vocab\": np.array([\"wind\", \"fire\", \"earth\"]),\n-              \"idf_weights\": None,\n-          },\n-      })\n-  def test_combiner_computation(self, data, vocab_size,\n-                                expected_accumulator_output,\n-                                expected_extract_output):\n-    combiner = index_lookup._IndexLookupCombiner(vocab_size=vocab_size)\n-    expected_accumulator = combiner._create_accumulator()\n-    expected_accumulator = self.update_accumulator(expected_accumulator,\n-                                                   expected_accumulator_output)\n-    self.validate_accumulator_computation(combiner, data, expected_accumulator)\n-    self.validate_accumulator_extract(combiner, data, expected_extract_output)\n-\n-\n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class IndexLookupIntCombinerTest(keras_parameterized.TestCase,\n-                                 preprocessing_test_utils.PreprocessingLayerTest\n-                                ):\n-\n-  def compare_text_accumulators(self, a, b, msg=None):\n-    if a is None or b is None:\n-      self.assertAllEqual(a, b, msg=msg)\n-\n-    self.assertAllEqual(a.count_dict, b.count_dict, msg=msg)\n-\n-  compare_accumulators = compare_text_accumulators\n-\n-  def update_accumulator(self, accumulator, data):\n-    accumulator.count_dict.update(dict(zip(data[\"vocab\"], data[\"counts\"])))\n-\n-    return accumulator\n-\n-  def test_combiner_api_compatibility_int_mode(self):\n-    data = np.array([[42, 1138, 725, 1729], [42, 1138, 725, 203]])\n-    combiner = index_lookup._IndexLookupCombiner()\n-    expected_accumulator_output = {\n-        \"vocab\": np.array([1138, 725, 42, 1729, 203]),\n-        \"counts\": np.array([2, 2, 2, 1, 1]),\n-    }\n-    expected_extract_output = {\n-        \"vocab\": np.array([1138, 725, 42, 1729, 203]),\n-        \"idf_weights\": None,\n-    }\n-    expected_accumulator = combiner._create_accumulator()\n-    expected_accumulator = self.update_accumulator(expected_accumulator,\n-                                                   expected_accumulator_output)\n-    self.validate_accumulator_serialize_and_deserialize(combiner, data,\n-                                                        expected_accumulator)\n-    self.validate_accumulator_uniqueness(combiner, data)\n-    self.validate_accumulator_extract(combiner, data, expected_extract_output)\n-\n-  # TODO(askerryryan): Add tests confirming equivalence to behavior of\n-  # existing tf.keras.preprocessing.text.Tokenizer.\n-  @parameterized.named_parameters(\n-      {\n-          \"testcase_name\": \"top_k_smaller_than_full_vocab\",\n-          \"data\": np.array([[42, 1138], [1729, 1138], [725], [1729, 1138]]),\n-          \"vocab_size\": 3,\n-          \"expected_accumulator_output\": {\n-              \"vocab\": np.array([1138, 1729, 725, 42]),\n-              \"counts\": np.array([3, 2, 1, 1]),\n-          },\n-          \"expected_extract_output\": {\n-              \"vocab\": np.array([1138, 1729, 725]),\n-              \"idf_weights\": None,\n-          },\n-      },\n-      {\n-          \"testcase_name\": \"top_k_larger_than_full_vocab\",\n-          \"data\": np.array([[42, 1138], [1729, 1138], [725], [1729, 1138]]),\n-          \"vocab_size\": 10,\n-          \"expected_accumulator_output\": {\n-              \"vocab\": np.array([1138, 1729, 725, 42]),\n-              \"counts\": np.array([3, 2, 1, 1]),\n-          },\n-          \"expected_extract_output\": {\n-              \"vocab\": np.array([1138, 1729, 725, 42]),\n-              \"idf_weights\": None,\n-          },\n-      },\n-      {\n-          \"testcase_name\": \"no_top_k\",\n-          \"data\": np.array([[42, 1138], [1729, 1138], [725], [1729, 1138]]),\n-          \"vocab_size\": None,\n-          \"expected_accumulator_output\": {\n-              \"vocab\": np.array([1138, 1729, 725, 42]),\n-              \"counts\": np.array([3, 2, 1, 1]),\n-          },\n-          \"expected_extract_output\": {\n-              \"vocab\": np.array([1138, 1729, 725, 42]),\n-              \"idf_weights\": None,\n-          },\n-      },\n-      {\n-          \"testcase_name\": \"single_element_per_row\",\n-          \"data\": np.array([[42], [1138], [1729], [1138], [725]]),\n-          \"vocab_size\": 3,\n-          \"expected_accumulator_output\": {\n-              \"vocab\": np.array([1138, 1729, 725, 42]),\n-              \"counts\": np.array([2, 1, 1, 1]),\n-          },\n-          \"expected_extract_output\": {\n-              \"vocab\": np.array([1138, 1729, 725]),\n-              \"idf_weights\": None,\n-          },\n-      },\n-      # Which tokens are retained are based on global frequency, and thus are\n-      # sensitive to frequency within a document. In contrast, because idf only\n-      # considers the presence of a token in a document, it is insensitive\n-      # to the frequency of the token within the document.\n-      {\n-          \"testcase_name\":\n-              \"retained_tokens_sensitive_to_within_document_frequency\",\n-          \"data\":\n-              np.array([[42, 42], [1138, 1138], [1729, 1729], [1138, 1138],\n-                        [725, 203]]),\n-          \"vocab_size\":\n-              3,\n-          \"expected_accumulator_output\": {\n-              \"vocab\": np.array([1138, 42, 1729, 725, 203]),\n-              \"counts\": np.array([4, 2, 2, 1, 1]),\n-          },\n-          \"expected_extract_output\": {\n-              \"vocab\": np.array([1138, 1729, 42]),\n-              \"idf_weights\": None,\n-          },\n-      })\n-  def test_combiner_computation(self, data, vocab_size,\n-                                expected_accumulator_output,\n-                                expected_extract_output):\n-    combiner = index_lookup._IndexLookupCombiner(vocab_size=vocab_size)\n-    expected_accumulator = combiner._create_accumulator()\n-    expected_accumulator = self.update_accumulator(expected_accumulator,\n-                                                   expected_accumulator_output)\n-    self.validate_accumulator_computation(combiner, data, expected_accumulator)\n-    self.validate_accumulator_extract(combiner, data, expected_extract_output)\n-\n-\n if __name__ == \"__main__\":\n   tf.test.main()\n\n@@ -14,10 +14,12 @@\n # ==============================================================================\n \"\"\"Keras string lookup preprocessing layer.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n+# pylint: disable=g-classes-have-attributes\n+\n from keras.engine import base_preprocessing_layer\n from keras.layers.preprocessing import index_lookup\n-from keras.layers.preprocessing import table_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n \n@@ -70,10 +72,11 @@ class IntegerLookup(index_lookup.IndexLookup):\n       If set to None, no mask term will be added. Defaults to None.\n     oov_token: Only used when `invert` is True. The token to return for OOV\n       indices. Defaults to -1.\n-    vocabulary: An optional list of integer tokens, or a path to a text file\n-      containing a vocabulary to load into this layer. The file should contain\n-      one integer token per line. If the list or file contains the same token\n-      multiple times, an error will be thrown.\n+    vocabulary: Optional. Either an array of integers or a string path to a text\n+      file. If passing an array, can pass a tuple, list, 1D numpy array, or 1D\n+      tensor containing the integer vocbulary terms. If passing a file path, the\n+      file should contain one line per term in the vocabulary. If this argument\n+      is set, there is no need to `adapt` the layer.\n     invert: Only valid when `output_mode` is `\"int\"`. If True, this layer will\n       map indices to vocabulary items instead of mapping vocabulary items to\n       indices. Default to False.\n@@ -332,6 +335,10 @@ class IntegerLookup(index_lookup.IndexLookup):\n           \"num_oov_indices must be greater than or equal to 0. You passed %s\" %\n           (num_oov_indices,))\n \n+    # Make sure mask and oov are of the dtype we want.\n+    mask_token = None if mask_token is None else np.int64(mask_token)\n+    oov_token = None if oov_token is None else np.int64(oov_token)\n+\n     super(IntegerLookup, self).__init__(\n         max_tokens=max_tokens,\n         num_oov_indices=num_oov_indices,\n@@ -344,18 +351,3 @@ class IntegerLookup(index_lookup.IndexLookup):\n         pad_to_max_tokens=pad_to_max_tokens,\n         **kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell(\"IntegerLookup\").set(True)\n-\n-  def set_vocabulary(self, vocabulary, idf_weights=None):\n-    if isinstance(vocabulary, str):\n-      if self.output_mode == index_lookup.TF_IDF:\n-        raise RuntimeError(\n-            \"Setting vocabulary directly from a file is not \"\n-            \"supported in TF-IDF mode, since this layer cannot \"\n-            \"read files containing TF-IDF weight data. Please \"\n-            \"read the file using Python and set the vocabulary \"\n-            \"and weights by passing lists or arrays to the \"\n-            \"set_vocabulary function's `vocabulary` and `idf_weights` \"\n-            \"args.\")\n-      vocabulary = table_utils.get_vocabulary_from_file(vocabulary)\n-      vocabulary = [int(v) for v in vocabulary]\n-    super().set_vocabulary(vocabulary, idf_weights=idf_weights)\n\n@@ -405,9 +405,9 @@ class IntegerLookupVocabularyTest(\n     self.assertAllEqual(expected_output, output_dataset)\n \n   def test_no_vocab(self):\n-    with self.assertRaisesRegex(ValueError,\n-                                \"You must set the layer's vocabulary\"):\n-      layer = integer_lookup.IntegerLookup()\n+    with self.assertRaisesRegex(RuntimeError,\n+                                \"you must set the layer's vocabulary\"):\n+      layer = integer_lookup.IntegerLookup(output_mode=\"binary\")\n       layer([[1]])\n \n   def test_one_hot_output(self):\n\n@@ -20,7 +20,6 @@ import tensorflow.compat.v2 as tf\n import numpy as np\n from keras.engine import base_preprocessing_layer\n from keras.layers.preprocessing import index_lookup\n-from keras.layers.preprocessing import table_utils\n from tensorflow.python.util.tf_export import keras_export\n \n \n@@ -69,10 +68,11 @@ class StringLookup(index_lookup.IndexLookup):\n       no mask term will be added. Defaults to `None`.\n     oov_token: Only used when `invert` is True. The token to return for OOV\n       indices. Defaults to `\"[UNK]\"`.\n-    vocabulary: An optional list of tokens, or a path to a text file containing\n-      a vocabulary to load into this layer. The file should contain one token\n-      per line. If the list or file contains the same token multiple times, an\n-      error will be thrown.\n+    vocabulary: Optional. Either an array of strings or a string path to a text\n+      file. If passing an array, can pass a tuple, list, 1D numpy array, or 1D\n+      tensor containing the string vocbulary terms. If passing a file path, the\n+      file should contain one line per term in the vocabulary. If this argument\n+      is set, there is no need to `adapt` the layer.\n     invert: Only valid when `output_mode` is `\"int\"`. If True, this layer will\n       map indices to vocabulary items instead of mapping vocabulary items to\n       indices. Default to False.\n@@ -324,20 +324,6 @@ class StringLookup(index_lookup.IndexLookup):\n     base_config = super(StringLookup, self).get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n-  def set_vocabulary(self, vocabulary, idf_weights=None):\n-    if isinstance(vocabulary, str):\n-      if self.output_mode == index_lookup.TF_IDF:\n-        raise RuntimeError(\"Setting vocabulary directly from a file is not \"\n-                           \"supported in TF-IDF mode, since this layer cannot \"\n-                           \"read files containing TF-IDF weight data. Please \"\n-                           \"read the file using Python and set the vocabulary \"\n-                           \"and weights by passing lists or arrays to the \"\n-                           \"set_vocabulary function's `vocabulary` and \"\n-                           \"`idf_weights` args.\")\n-      vocabulary = table_utils.get_vocabulary_from_file(vocabulary,\n-                                                        self.encoding)\n-    super().set_vocabulary(vocabulary, idf_weights=idf_weights)\n-\n   # Overriden methods from IndexLookup.\n   def _tensor_vocab_to_numpy(self, vocabulary):\n     vocabulary = vocabulary.numpy()\n\n@@ -166,9 +166,9 @@ class StringLookupVocabularyTest(keras_parameterized.TestCase,\n       _ = model.predict(invalid_input)\n \n   def test_no_vocab(self):\n-    with self.assertRaisesRegex(\n-        ValueError, \"You must set the layer's vocabulary\"):\n-      layer = string_lookup.StringLookup()\n+    with self.assertRaisesRegex(RuntimeError,\n+                                \"you must set the layer's vocabulary\"):\n+      layer = string_lookup.StringLookup(output_mode=\"binary\")\n       layer([[\"a\"]])\n \n   def test_one_hot_output(self):\n\n@@ -1,200 +0,0 @@\n-# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Utilities for working with tf.lookup tables in Keras.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n-\n-import collections\n-import os\n-import numpy as np\n-from keras.utils import tf_utils\n-\n-\n-class TableHandler(object):\n-  \"\"\"Wrapper object that holds a lookup table and provides accessors.\"\"\"\n-\n-  def __init__(self,\n-               table,\n-               oov_tokens=None,\n-               mask_token=None,\n-               mask_value=0):\n-    self.table = table\n-    self.mutable = isinstance(table, tf.lookup.experimental.MutableHashTable)\n-    self.mask_token = mask_token\n-    self.mask_value = mask_value\n-\n-    if oov_tokens is None:\n-      self.oov_tokens = oov_tokens\n-    else:\n-      if not isinstance(oov_tokens, (list, tuple, np.ndarray)):\n-        oov_tokens = [oov_tokens]\n-      self.oov_tokens = tf.cast(oov_tokens, table._value_dtype)  # pylint: disable=protected-access\n-\n-  def table_size(self):\n-    return self.table.size().numpy()\n-\n-  def clear(self):\n-    if not self.mutable:\n-      return RuntimeError(\"Unable to clear a statically-backed table.\")\n-\n-    keys, _ = self.table.export()\n-    self.table.remove(keys)\n-\n-  def insert(self, keys, values):\n-    \"\"\"Insert values into the backed table.\"\"\"\n-    if not self.mutable:\n-      raise RuntimeError(\"Unable to insert into a statically-backed table.\")\n-\n-    if len(values) != len(keys):\n-      raise RuntimeError(\"Size mismatch between values and key arrays. \"\n-                         \"Keys had size %s, values had size %s.\" %\n-                         (len(keys), len(values)))\n-    keys = tf.convert_to_tensor(\n-        keys, dtype=self.table._key_dtype)  # pylint: disable=protected-access\n-    values = tf.convert_to_tensor(\n-        values, dtype=self.table._value_dtype)  # pylint: disable=protected-access\n-    if values.shape.ndims != 1:\n-      raise ValueError(\"`values` must be 1-dimensional, got an input with \"\n-                       \" %s dimensions.\" % values.shape.ndims)\n-    self.table.insert(keys, values)\n-\n-  def _replace_oov_buckets(self, inputs, lookups):\n-    \"\"\"Replace the default OOV value with one of the OOV bucket values.\"\"\"\n-    if self.oov_tokens is None:\n-      return lookups\n-\n-    num_oov_elements = self.oov_tokens.shape.num_elements()\n-    if inputs.dtype.is_integer:\n-      oov_indices = tf.math.floormod(inputs, num_oov_elements)\n-    else:\n-      oov_indices = tf.strings.to_hash_bucket_fast(\n-          inputs, num_buckets=num_oov_elements)\n-\n-    oov_values = tf.gather(self.oov_tokens, oov_indices)\n-    oov_locations = tf.equal(lookups, self.table._default_value)  # pylint: disable=protected-access\n-\n-    return tf.where(oov_locations, oov_values, lookups)\n-\n-  def _lookup_and_mask(self, inputs):\n-    \"\"\"Return a lookup with any location with the mask_token masked to 0.\"\"\"\n-    lookups = self.table.lookup(inputs)\n-    # If we don't need to handle masking, return the lookup values directly.\n-    if self.mask_token is None:\n-      return lookups\n-\n-    # Inject 0s wherever the mask token was in the inputs.\n-    mask_locations = tf.equal(inputs, self.mask_token)\n-    return tf.where(\n-        mask_locations,\n-        tf.cast(self.mask_value, self.table._value_dtype),  # pylint: disable=protected-access\n-        lookups)  # pylint: disable=protected-access\n-\n-  def _ragged_lookup(self, inputs):\n-    \"\"\"Perform a table lookup on a ragged tensor.\"\"\"\n-    # The table lookup ops don't natively support ragged tensors, so if we have\n-    # a RT we need to use map_flat_values to look up every element.\n-    indexed_data = tf.ragged.map_flat_values(\n-        self._lookup_and_mask, inputs)\n-    indexed_data = tf.ragged.map_flat_values(\n-        self._replace_oov_buckets, inputs, indexed_data)\n-    # table.lookup is not shape-preserving, so we need to set the shape here.\n-    indexed_data._set_shape(inputs.shape)  # pylint: disable=protected-access\n-    # Composite tensors can pass tensor values through, which will cause\n-    # errors if all operations in the TF graph do so. We can break this chain\n-    # with an identity here.\n-    return tf.identity(indexed_data)\n-\n-  def _sparse_lookup(self, inputs):\n-    \"\"\"Perform a table lookup on a sparse tensor.\"\"\"\n-    values = self._lookup_and_mask(inputs.values)\n-    values = self._replace_oov_buckets(inputs.values, values)\n-    indexed_data = tf.SparseTensor(inputs.indices, values,\n-                                              inputs.dense_shape)\n-    # Composite tensors can pass tensor values through, which will cause\n-    # errors if all operations in the TF graph do so. We can break this chain\n-    # with an identity here.\n-    return tf.identity(indexed_data)\n-\n-  def _tensor_lookup(self, inputs):\n-    \"\"\"Perform a table lookup on a tf.tensor.\"\"\"\n-    values = self._lookup_and_mask(inputs)\n-    indexed_data = self._replace_oov_buckets(inputs, values)\n-    # (b/149446477): output does not preserve input shape.\n-    indexed_data.set_shape(inputs.shape)\n-    return indexed_data\n-\n-  def lookup(self, inputs):\n-    \"\"\"Perform a table lookup.\"\"\"\n-    # Sparse tensors don't play nicely with tensor conversion, so we handle\n-    # them before attempting to convert lists or arrays to tensors.\n-    if isinstance(\n-        inputs, (tf.SparseTensor, tf.compat.v1.SparseTensorValue)):\n-      return self._sparse_lookup(inputs)\n-\n-    if tf_utils.is_ragged(inputs):\n-      if isinstance(inputs, tf.compat.v1.ragged.RaggedTensorValue):\n-        flat_values = tf.convert_to_tensor(\n-            value=inputs.flat_values, name=\"flat_values\")\n-        inputs = tf.RaggedTensor.from_nested_row_splits(\n-            flat_values, inputs.nested_row_splits, validate=False)\n-      return self._ragged_lookup(inputs)\n-\n-    # For normal tensor inputs\n-    inputs = tf.convert_to_tensor(inputs)\n-    return self._tensor_lookup(inputs)\n-\n-\n-def num_tokens_in_file(vocabulary_path):\n-  \"\"\"Count the number of lines in a vocab file to get the number of tokens.\"\"\"\n-  num_tokens = 0\n-  with tf.io.gfile.GFile(vocabulary_path, \"r\") as reader:\n-    text = reader.readline()\n-    while text:\n-      num_tokens += 1\n-      text = reader.readline()\n-\n-  return num_tokens\n-\n-\n-def get_vocabulary_from_file(vocabulary_path, encoding=\"utf-8\"):\n-  \"\"\"Read a vocabulary in from a file.\"\"\"\n-  vocab = []\n-  with tf.io.gfile.GFile(vocabulary_path, \"r\") as reader:\n-    while True:\n-      # Get the next line (incl. \\n), and break if nothing is left to read.\n-      text = reader.readline()\n-      if not text:\n-        break\n-\n-      # Convert the raw text and strip whitespace.\n-      if isinstance(text, str):\n-        token = text\n-      elif isinstance(text, bytes):\n-        token = text.decode(encoding, \"ignore\")\n-      token = token.rstrip(os.linesep)\n-      vocab.append(token)\n-  return vocab\n-\n-\n-def find_repeated_tokens(vocabulary):\n-  \"\"\"Return all repeated tokens in a vocabulary.\"\"\"\n-  vocabulary_set = set(vocabulary)\n-  if len(vocabulary) != len(vocabulary_set):\n-    return [\n-        item for item, count in collections.Counter(vocabulary).items()\n-        if count > 1\n-    ]\n-  else:\n-    return []\n\n@@ -1,434 +0,0 @@\n-# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Tests for Keras lookup table utils.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n-\n-import os\n-import tempfile\n-\n-import numpy as np\n-from keras import keras_parameterized\n-from keras.layers.preprocessing import preprocessing_test_utils\n-from keras.layers.preprocessing import table_utils\n-\n-\n-def get_table(dtype=tf.string, oov_tokens=None):\n-  table = tf.lookup.experimental.MutableHashTable(\n-      key_dtype=dtype,\n-      value_dtype=tf.int64,\n-      default_value=-7,\n-      name=\"index_table\")\n-  return table_utils.TableHandler(table, oov_tokens)\n-\n-\n-def get_static_table(tmpdir,\n-                     vocab_list,\n-                     mask_token=None,\n-                     dtype=tf.string,\n-                     oov_tokens=None):\n-  vocabulary_file = os.path.join(tmpdir, \"tmp_vocab.txt\")\n-\n-  if dtype == tf.string:\n-    with open(vocabulary_file, \"w\") as f:\n-      f.write(\"\\n\".join(vocab_list) + \"\\n\")\n-  else:\n-    with open(vocabulary_file, \"w\") as f:\n-      f.write(\"\\n\".join([str(v) for v in vocab_list]) + \"\\n\")\n-\n-  offset = ((0 if mask_token is None else 1) +\n-            (len(oov_tokens) if oov_tokens is not None else 0))\n-  init = tf.lookup.TextFileInitializer(\n-      vocabulary_file,\n-      dtype,\n-      tf.lookup.TextFileIndex.WHOLE_LINE,\n-      tf.int64,\n-      tf.lookup.TextFileIndex.LINE_NUMBER,\n-      value_index_offset=offset)\n-  table = tf.lookup.StaticHashTable(init, default_value=-7)\n-  return table_utils.TableHandler(\n-      table,\n-      oov_tokens,\n-      mask_token=mask_token)\n-\n-\n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class CategoricalEncodingInputTest(\n-    keras_parameterized.TestCase,\n-    preprocessing_test_utils.PreprocessingLayerTest):\n-\n-  def test_sparse_string_input(self):\n-    vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n-    input_array = tf.SparseTensor(\n-        indices=[[0, 0], [1, 2]],\n-        values=[\"fire\", \"michigan\"],\n-        dense_shape=[3, 4])\n-\n-    expected_indices = [[0, 0], [1, 2]]\n-    expected_values = [5, 1]\n-    expected_dense_shape = [3, 4]\n-\n-    table = get_table(oov_tokens=[1])\n-    table.insert(vocab_data, range(2, len(vocab_data) + 2))\n-    output_data = table.lookup(input_array)\n-\n-    self.assertAllEqual(expected_indices, output_data.indices)\n-    self.assertAllEqual(expected_values, output_data.values)\n-    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)\n-\n-  def test_sparse_int_input(self):\n-    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)\n-    input_array = tf.SparseTensor(\n-        indices=[[0, 0], [1, 2]],\n-        values=np.array([13, 32], dtype=np.int64),\n-        dense_shape=[3, 4])\n-\n-    expected_indices = [[0, 0], [1, 2]]\n-    expected_values = [5, 1]\n-    expected_dense_shape = [3, 4]\n-\n-    table = get_table(dtype=tf.int64, oov_tokens=[1])\n-    table.insert(vocab_data, range(2, len(vocab_data) + 2))\n-    output_data = table.lookup(input_array)\n-\n-    self.assertAllEqual(expected_indices, output_data.indices)\n-    self.assertAllEqual(expected_values, output_data.values)\n-    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)\n-\n-  def test_ragged_string_input(self):\n-    vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n-    input_array = tf.ragged.constant(\n-        [[\"earth\", \"wind\", \"fire\"], [\"fire\", \"and\", \"earth\", \"michigan\"]])\n-    expected_output = [[2, 3, 5], [5, 4, 2, 1]]\n-\n-    table = get_table(oov_tokens=[1])\n-    table.insert(vocab_data, range(2, len(vocab_data) + 2))\n-    output_data = table.lookup(input_array)\n-\n-    self.assertAllEqual(expected_output, output_data)\n-\n-  def test_ragged_int_input(self):\n-    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)\n-    input_array = tf.ragged.constant([[10, 11, 13], [13, 12, 10, 42]],\n-                                              dtype=np.int64)\n-    expected_output = [[2, 3, 5], [5, 4, 2, 1]]\n-\n-    table = get_table(dtype=tf.int64, oov_tokens=[1])\n-    table.insert(vocab_data, range(2, len(vocab_data) + 2))\n-    output_data = table.lookup(input_array)\n-\n-    self.assertAllEqual(expected_output, output_data)\n-\n-  def test_tensor_multi_dim_values_fails(self):\n-    key_data = np.array([0, 1], dtype=np.int64)\n-    value_data = np.array([[11, 12], [21, 22]])\n-\n-    table = get_table(dtype=tf.int64, oov_tokens=[1, 2])\n-\n-    with self.assertRaisesRegex(ValueError, \"must be 1-dimensional\"):\n-      table.insert(key_data, value_data)\n-\n-\n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class CategoricalEncodingMultiOOVTest(\n-    keras_parameterized.TestCase,\n-    preprocessing_test_utils.PreprocessingLayerTest):\n-\n-  def test_sparse_string_input_multi_bucket(self):\n-    vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n-    input_array = tf.SparseTensor(\n-        indices=[[0, 0], [1, 2]], values=[\"fire\", \"ohio\"], dense_shape=[3, 4])\n-\n-    expected_indices = [[0, 0], [1, 2]]\n-    expected_values = [6, 2]\n-    expected_dense_shape = [3, 4]\n-\n-    table = get_table(oov_tokens=[1, 2])\n-    table.insert(vocab_data, range(3, len(vocab_data) + 3))\n-    output_data = table.lookup(input_array)\n-\n-    self.assertAllEqual(expected_indices, output_data.indices)\n-    self.assertAllEqual(expected_values, output_data.values)\n-    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)\n-\n-  def test_sparse_int_input_multi_bucket(self):\n-    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)\n-    input_array = tf.SparseTensor(\n-        indices=[[0, 0], [1, 2]],\n-        values=np.array([13, 132], dtype=np.int64),\n-        dense_shape=[3, 4])\n-\n-    expected_indices = [[0, 0], [1, 2]]\n-    expected_values = [6, 1]\n-    expected_dense_shape = [3, 4]\n-\n-    table = get_table(dtype=tf.int64, oov_tokens=[1, 2])\n-    table.insert(vocab_data, range(3, len(vocab_data) + 3))\n-    output_data = table.lookup(input_array)\n-\n-    self.assertAllEqual(expected_indices, output_data.indices)\n-    self.assertAllEqual(expected_values, output_data.values)\n-    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)\n-\n-  def test_ragged_string_input_multi_bucket(self):\n-    vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n-    input_array = tf.ragged.constant([[\"earth\", \"wind\", \"fire\"],\n-                                               [\"fire\", \"and\", \"earth\",\n-                                                \"ohio\"]])\n-    expected_output = [[3, 4, 6], [6, 5, 3, 2]]\n-\n-    table = get_table(oov_tokens=[1, 2])\n-    table.insert(vocab_data, range(3, len(vocab_data) + 3))\n-    output_data = table.lookup(input_array)\n-\n-    self.assertAllEqual(expected_output, output_data)\n-\n-  def test_ragged_int_input_multi_bucket(self):\n-    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)\n-    input_array = tf.ragged.constant([[10, 11, 13], [13, 12, 10, 132]],\n-                                              dtype=np.int64)\n-    expected_output = [[3, 4, 6], [6, 5, 3, 1]]\n-\n-    table = get_table(dtype=tf.int64, oov_tokens=[1, 2])\n-    table.insert(vocab_data, range(3, len(vocab_data) + 3))\n-    output_data = table.lookup(input_array)\n-\n-    self.assertAllEqual(expected_output, output_data)\n-\n-  def test_tensor_int_input_multi_bucket(self):\n-    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)\n-    input_array = np.array([[13, 132], [13, 133]], dtype=np.int64)\n-    expected_values = [[6, 1], [6, 2]]\n-\n-    table = get_table(dtype=tf.int64, oov_tokens=[1, 2])\n-    table.insert(vocab_data, range(3, len(vocab_data) + 3))\n-    output_data = table.lookup(input_array)\n-\n-    self.assertAllEqual(expected_values, output_data)\n-\n-  def test_tensor_string_input_multi_bucket(self):\n-    vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n-    input_array = [[\"earth\", \"wind\", \"fire\", \"michigan\"],\n-                   [\"fire\", \"and\", \"earth\", \"ohio\"]]\n-    expected_output = [[3, 4, 6, 1], [6, 5, 3, 2]]\n-\n-    table = get_table(oov_tokens=[1, 2])\n-    table.insert(vocab_data, range(3, len(vocab_data) + 3))\n-    output_data = table.lookup(input_array)\n-\n-    self.assertAllEqual(expected_output, output_data)\n-\n-\n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class IndexLookupOutputTest(keras_parameterized.TestCase,\n-                            preprocessing_test_utils.PreprocessingLayerTest):\n-\n-  def test_int_output_default_lookup_value(self):\n-    vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n-    input_array = np.array([[\"earth\", \"wind\", \"and\", \"fire\"],\n-                            [\"fire\", \"and\", \"earth\", \"michigan\"]])\n-    expected_output = [[1, 2, 3, 4], [4, 3, 1, -7]]\n-\n-    table = get_table(oov_tokens=None)\n-    table.insert(vocab_data, range(1, len(vocab_data) + 1))\n-    output_data = table.lookup(input_array)\n-\n-    self.assertAllEqual(expected_output, output_data)\n-\n-  def test_output_shape(self):\n-    vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n-    input_array = np.array([[\"earth\", \"wind\", \"and\", \"fire\"],\n-                            [\"fire\", \"and\", \"earth\", \"michigan\"]])\n-\n-    table = get_table()\n-    table.insert(vocab_data, range(1, len(vocab_data) + 1))\n-    output_data = table.lookup(input_array)\n-\n-    self.assertAllEqual(input_array.shape[1:], output_data.shape[1:])\n-\n-  def test_int_output_no_reserved_zero_default_lookup_value(self):\n-    vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n-    input_array = np.array([[\"earth\", \"wind\", \"and\", \"fire\"],\n-                            [\"fire\", \"and\", \"earth\", \"michigan\"]])\n-    expected_output = [[0, 1, 2, 3], [3, 2, 0, -7]]\n-\n-    table = get_table(oov_tokens=None)\n-    table.insert(vocab_data, range(len(vocab_data)))\n-    output_data = table.lookup(input_array)\n-\n-    self.assertAllEqual(expected_output, output_data)\n-\n-\n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class StaticIndexLookupOutputTest(\n-    keras_parameterized.TestCase,\n-    preprocessing_test_utils.PreprocessingLayerTest):\n-\n-  def test_int_output_default_lookup_value(self):\n-    vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n-    input_array = np.array([[\"earth\", \"wind\", \"and\", \"fire\"],\n-                            [\"fire\", \"and\", \"earth\", \"michigan\"]])\n-    expected_output = [[1, 2, 3, 4], [4, 3, 1, -7]]\n-\n-    table = get_static_table(\n-        tmpdir=self.get_temp_dir(),\n-        vocab_list=vocab_data,\n-        mask_token=\"\",\n-        oov_tokens=None)\n-    output_data = table.lookup(input_array)\n-\n-    self.assertAllEqual(expected_output, output_data)\n-\n-  def test_output_shape(self):\n-    vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n-    input_array = np.array([[\"earth\", \"wind\", \"and\", \"fire\"],\n-                            [\"fire\", \"and\", \"earth\", \"michigan\"]])\n-\n-    table = get_static_table(\n-        tmpdir=self.get_temp_dir(), vocab_list=vocab_data, oov_tokens=None)\n-    output_data = table.lookup(input_array)\n-\n-    self.assertAllEqual(input_array.shape[1:], output_data.shape[1:])\n-\n-  def test_int_output_no_reserved_zero_default_lookup_value(self):\n-    vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n-    input_array = np.array([[\"earth\", \"wind\", \"and\", \"fire\"],\n-                            [\"fire\", \"and\", \"earth\", \"michigan\"]])\n-    expected_output = [[0, 1, 2, 3], [3, 2, 0, -7]]\n-\n-    table = get_static_table(\n-        tmpdir=self.get_temp_dir(), vocab_list=vocab_data, oov_tokens=None)\n-    output_data = table.lookup(input_array)\n-\n-    self.assertAllEqual(expected_output, output_data)\n-\n-\n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class CategoricalEncodingStaticInputTest(\n-    keras_parameterized.TestCase,\n-    preprocessing_test_utils.PreprocessingLayerTest):\n-\n-  def test_sparse_string_input(self):\n-    vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n-    input_array = tf.SparseTensor(\n-        indices=[[0, 0], [1, 2]],\n-        values=[\"fire\", \"michigan\"],\n-        dense_shape=[3, 4])\n-\n-    expected_indices = [[0, 0], [1, 2]]\n-    expected_values = [5, 1]\n-    expected_dense_shape = [3, 4]\n-\n-    table = get_static_table(\n-        tmpdir=self.get_temp_dir(),\n-        vocab_list=vocab_data,\n-        mask_token=\"\",\n-        oov_tokens=[1])\n-    output_data = table.lookup(input_array)\n-\n-    self.assertAllEqual(expected_indices, output_data.indices)\n-    self.assertAllEqual(expected_values, output_data.values)\n-    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)\n-\n-  def test_sparse_int_input(self):\n-    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)\n-    input_array = tf.SparseTensor(\n-        indices=[[0, 0], [1, 2]],\n-        values=np.array([13, 32], dtype=np.int64),\n-        dense_shape=[3, 4])\n-\n-    expected_indices = [[0, 0], [1, 2]]\n-    expected_values = [5, 1]\n-    expected_dense_shape = [3, 4]\n-\n-    table = get_static_table(\n-        tmpdir=self.get_temp_dir(),\n-        vocab_list=vocab_data,\n-        dtype=tf.int64,\n-        mask_token=0,\n-        oov_tokens=[1])\n-    output_data = table.lookup(input_array)\n-\n-    self.assertAllEqual(expected_indices, output_data.indices)\n-    self.assertAllEqual(expected_values, output_data.values)\n-    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)\n-\n-  def test_ragged_string_input(self):\n-    vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n-    input_array = tf.ragged.constant(\n-        [[\"earth\", \"wind\", \"fire\"], [\"fire\", \"and\", \"earth\", \"michigan\"]])\n-    expected_output = [[2, 3, 5], [5, 4, 2, 1]]\n-\n-    table = get_static_table(\n-        tmpdir=self.get_temp_dir(),\n-        vocab_list=vocab_data,\n-        mask_token=\"\",\n-        oov_tokens=[1])\n-    output_data = table.lookup(input_array)\n-\n-    self.assertAllEqual(expected_output, output_data)\n-\n-  def test_ragged_int_input(self):\n-    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)\n-    input_array = tf.ragged.constant([[10, 11, 13], [13, 12, 10, 42]],\n-                                              dtype=np.int64)\n-    expected_output = [[2, 3, 5], [5, 4, 2, 1]]\n-\n-    table = get_static_table(\n-        tmpdir=self.get_temp_dir(),\n-        vocab_list=vocab_data,\n-        dtype=tf.int64,\n-        mask_token=0,\n-        oov_tokens=[1])\n-    output_data = table.lookup(input_array)\n-\n-    self.assertAllEqual(expected_output, output_data)\n-\n-\n-class GetVocabularyFromFileTest(tf.test.TestCase):\n-\n-  def setUp(self):\n-    super(GetVocabularyFromFileTest, self).setUp()\n-    dir_path = tempfile.mkdtemp(prefix=tf.compat.v1.test.get_temp_dir())\n-    self._vocab_path = os.path.join(dir_path, \"vocab\")\n-\n-  def test_only_line_separator_is_stripped(self):\n-    expected = [\"foo\", \" foo\", \"foo \", \" foo \"]\n-    with tf.io.gfile.GFile(self._vocab_path, \"w\") as writer:\n-      for word in expected:\n-        writer.write(word)\n-        writer.write(os.linesep)\n-\n-    actual = actual = table_utils.get_vocabulary_from_file(self._vocab_path)\n-    self.assertAllEqual(expected, actual)\n-\n-  def test_linux_file(self):\n-    content = b\"line1\\nline2\\nline3\"\n-    with tf.io.gfile.GFile(self._vocab_path, \"wb\") as writer:\n-      writer.write(content)\n-\n-    actual = table_utils.get_vocabulary_from_file(self._vocab_path)\n-    self.assertAllEqual([\"line1\", \"line2\", \"line3\"], actual)\n-\n-  def test_windows_file(self):\n-    content = b\"line1\\r\\nline2\\r\\nline3\"\n-    with tf.io.gfile.GFile(self._vocab_path, \"wb\") as writer:\n-      writer.write(content)\n-\n-    actual = table_utils.get_vocabulary_from_file(self._vocab_path)\n-    self.assertAllEqual([\"line1\", \"line2\", \"line3\"], actual)\n-\n-if __name__ == \"__main__\":\n-  tf.test.main()\n\n@@ -59,7 +59,7 @@ _ACCUMULATOR_NUM_DOCUMENTS = \"num_documents\"\n \n @keras_export(\n     \"keras.layers.experimental.preprocessing.TextVectorization\", v1=[])\n-class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n+class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n   \"\"\"Text vectorization layer.\n \n   This layer has basic options for managing text in a Keras model. It\n@@ -149,10 +149,11 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n       `max_tokens` even if the number of unique tokens in the vocabulary is less\n       than max_tokens, resulting in a tensor of shape `(batch_size, max_tokens)`\n       regardless of vocabulary size. Defaults to False.\n-    vocabulary: An optional list of vocabulary terms, or a path to a text file\n-      containing a vocabulary to load into this layer. The file should contain\n-      one token per line. If the list or file contains the same token multiple\n-      times, an error will be thrown.\n+    vocabulary: Optional. Either an array of strings or a string path to a text\n+      file. If passing an array, can pass a tuple, list, 1D numpy array, or 1D\n+      tensor containing the string vocbulary terms. If passing a file path, the\n+      file should contain one line per term in the vocabulary. If this argument\n+      is set, there is no need to `adapt` the layer.\n \n   Example:\n \n@@ -230,6 +231,7 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n                pad_to_max_tokens=False,\n                vocabulary=None,\n                **kwargs):\n+\n     # This layer only applies to string processing, and so should only have\n     # a dtype of 'string'.\n     if \"dtype\" in kwargs and kwargs[\"dtype\"] != tf.string:\n@@ -299,49 +301,30 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n \n     self._output_mode = output_mode\n     self._output_sequence_length = output_sequence_length\n-    vocabulary_size = 0\n-    # IndexLookup needs to keep track the current vocab size outside of its\n-    # layer weights. We persist it as a hidden part of the config during\n-    # serialization.\n-    if \"vocabulary_size\" in kwargs:\n-      vocabulary_size = kwargs[\"vocabulary_size\"]\n-      del kwargs[\"vocabulary_size\"]\n+    # Drop deprecated config options.\n+    kwargs.pop(\"vocabulary_size\", None)\n \n-    super(TextVectorization, self).__init__(\n-        combiner=None,\n-        **kwargs)\n-    base_preprocessing_layer.keras_kpl_gauge.get_cell(\n-        \"TextVectorization\").set(True)\n+    super().__init__(streaming=False, **kwargs)\n+    base_preprocessing_layer.keras_kpl_gauge.get_cell(\"TextVectorization\").set(\n+        True)\n \n     self._index_lookup_layer = string_lookup.StringLookup(\n         max_tokens=max_tokens,\n         vocabulary=vocabulary,\n         pad_to_max_tokens=pad_to_max_tokens,\n         mask_token=\"\",\n-        output_mode=output_mode if output_mode is not None else INT,\n-        vocabulary_size=vocabulary_size)\n-\n-  def _assert_same_type(self, expected_type, values, value_name):\n-    if tf.as_dtype(expected_type) != tf.as_dtype(values.dtype):\n-      raise RuntimeError(\"Expected %s type %s, got %s\" %\n-                         (value_name, expected_type, values.dtype))\n+        output_mode=output_mode if output_mode is not None else INT)\n \n   def compute_output_shape(self, input_shape):\n-    if self._output_mode != INT:\n-      return tf.TensorShape([input_shape[0], self._max_tokens])\n+    if self._output_mode == INT:\n+      return tf.TensorShape([input_shape[0], self._output_sequence_length])\n \n-    if self._output_mode == INT and self._split is None:\n+    if self._split is None:\n       if len(input_shape) <= 1:\n         input_shape = tuple(input_shape) + (1,)\n-      return tf.TensorShape(input_shape)\n-\n-    if self._output_mode == INT and self._split is not None:\n-      input_shape = list(input_shape)\n-      if len(input_shape) <= 1:\n-        input_shape = input_shape + [self._output_sequence_length]\n     else:\n-        input_shape[1] = self._output_sequence_length\n-      return tf.TensorShape(input_shape)\n+      input_shape = tuple(input_shape) + (None,)\n+    return self._index_lookup_layer.compute_output_shape(input_shape)\n \n   def compute_output_signature(self, input_spec):\n     output_shape = self.compute_output_shape(input_spec.shape.as_list())\n@@ -349,52 +332,18 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n                     else backend.floatx())\n     return tf.TensorSpec(shape=output_shape, dtype=output_dtype)\n \n-  def adapt(self, data, reset_state=True):\n-    \"\"\"Fits the state of the preprocessing layer to the dataset.\n-\n-    Overrides the default adapt method to apply relevant preprocessing to the\n-    inputs before passing to the combiner.\n-\n-    Args:\n-      data: The data to train on. It can be passed either as a tf.data Dataset,\n-        as a NumPy array, a string tensor, or as a list of texts.\n-      reset_state: Optional argument specifying whether to clear the state of\n-        the layer at the start of the call to `adapt`. This must be True for\n-        this layer, which does not support repeated calls to `adapt`.\n-    \"\"\"\n-    if not reset_state:\n-      raise ValueError(\"TextVectorization does not support streaming adapts.\")\n-\n-    # Build the layer explicitly with the original data shape instead of relying\n-    # on an implicit call to `build` in the base layer's `adapt`, since\n-    # preprocessing changes the input shape.\n+  def update_state(self, data):\n     if isinstance(data, (list, tuple, np.ndarray)):\n       data = tf.convert_to_tensor(data)\n-\n-    if isinstance(data, tf.Tensor):\n     if data.shape.rank == 1:\n       data = tf.expand_dims(data, axis=-1)\n-      self.build(data.shape)\n-      preprocessed_inputs = self._preprocess(data)\n-    elif isinstance(data, tf.data.Dataset):\n-      # TODO(momernick): Replace this with a more V2-friendly API.\n-      shape = tf.compat.v1.data.get_output_shapes(data)\n-      if not isinstance(shape, tf.TensorShape):\n-        raise ValueError(\"The dataset passed to 'adapt' must contain a single \"\n-                         \"tensor value.\")\n-      if shape.rank == 0:\n-        data = data.map(lambda tensor: tf.expand_dims(tensor, 0))\n-        shape = tf.compat.v1.data.get_output_shapes(data)\n-      if shape.rank == 1:\n-        data = data.map(lambda tensor: tf.expand_dims(tensor, -1))\n-      self.build(tf.compat.v1.data.get_output_shapes(data))\n-      preprocessed_inputs = data.map(self._preprocess)\n-    else:\n-      raise ValueError(\n-          \"adapt() requires a Dataset or an array as input, got {}\".format(\n-              type(data)))\n+    self._index_lookup_layer.update_state(self._preprocess(data))\n \n-    self._index_lookup_layer.adapt(preprocessed_inputs)\n+  def finalize_state(self):\n+    self._index_lookup_layer.finalize_state()\n+\n+  def reset_state(self):  # pylint: disable=method-hidden\n+    self._index_lookup_layer.reset_state()\n \n   def get_vocabulary(self, include_special_tokens=True):\n     \"\"\"Returns the current vocabulary of the layer.\n@@ -427,18 +376,10 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n         \"output_mode\": self._output_mode,\n         \"output_sequence_length\": self._output_sequence_length,\n         \"pad_to_max_tokens\": self._index_lookup_layer.pad_to_max_tokens,\n-        \"vocabulary_size\": self._index_lookup_layer.vocabulary_size(),\n     }\n     base_config = super(TextVectorization, self).get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n-  def count_params(self):\n-    # This method counts the number of scalars in the weights of this layer.\n-    # Since this layer doesn't have any /actual/ weights (in that there's\n-    # nothing in this layer that can be trained - we only use the weight\n-    # abstraction for ease of saving!) we return 0.\n-    return 0\n-\n   def set_vocabulary(self, vocabulary, idf_weights=None):\n     \"\"\"Sets vocabulary (and optionally document frequency) data for this layer.\n \n@@ -449,16 +390,19 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n     it.\n \n     Args:\n-      vocabulary: An array of string tokens, or a path to a file containing one\n-        token per line.\n-      idf_weights: An array of document frequency data with equal length to\n-        vocab. Only necessary if the layer output_mode is TF_IDF.\n+      vocabulary: Either an array or a string path to a text file. If passing an\n+        array, can pass a tuple, list, 1D numpy array, or 1D tensor containing\n+        the vocbulary terms. If passing a file path, the file should contain one\n+        line per term in the vocabulary.\n+      idf_weights: A tuple, list, 1D numpy array, or 1D tensor of inverse\n+        document frequency weights with equal length to vocabulary. Must be set\n+        if `output_mode` is `\"tf_idf\"`. Should not be set otherwise.\n \n     Raises:\n       ValueError: If there are too many inputs, the inputs do not match, or\n         input data is missing.\n       RuntimeError: If the vocabulary cannot be set when this function is\n-        called. This happens when `\"multi_hot\"`, `\"count\"`, and \"tfidf\" modes,\n+        called. This happens when `\"multi_hot\"`, `\"count\"`, and \"tf_idf\" modes,\n         if `pad_to_max_tokens` is False and the layer itself has already been\n         called.\n     \"\"\"\n@@ -470,8 +414,7 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n     # expression to evaluate to False instead of True if the shape is undefined;\n     # the expression needs to evaluate to True in that case.\n     if self._split is not None:\n-      if (input_shape is not None and input_shape.ndims > 1 and\n-          not input_shape[-1] == 1):  # pylint: disable=g-comparison-negation\n+      if input_shape.ndims > 1 and not input_shape[-1] == 1:  # pylint: disable=g-comparison-negation\n         raise RuntimeError(\n             \"When using TextVectorization to tokenize strings, the innermost \"\n             \"dimension of the input array must be 1, got shape \"\n@@ -479,14 +422,6 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n \n     super(TextVectorization, self).build(input_shape)\n \n-  def _set_state_variables(self, updates):\n-    if not self.built:\n-      raise RuntimeError(\"_set_state_variables() must be called after build().\")\n-    if self._output_mode == TF_IDF:\n-      self.set_vocabulary(updates[_VOCAB_NAME], idf_weights=updates[_IDF_NAME])\n-    else:\n-      self.set_vocabulary(updates[_VOCAB_NAME])\n-\n   def _preprocess(self, inputs):\n     if self._standardize == LOWER_AND_STRIP_PUNCTUATION:\n       if tf_utils.is_ragged(inputs):\n\n@@ -142,6 +142,7 @@ def _get_end_to_end_test_cases():\n                         [\"and\"], [\"earth\"], [\"michigan\"]]),\n           \"kwargs\": {\n               \"max_tokens\": 5,\n+              \"pad_to_max_tokens\": True,\n               \"standardize\": None,\n               \"split\": None,\n               \"output_mode\": text_vectorization.MULTI_HOT\n@@ -161,6 +162,7 @@ def _get_end_to_end_test_cases():\n                         [\"earth michigan\"]]),\n           \"kwargs\": {\n               \"max_tokens\": 5,\n+              \"pad_to_max_tokens\": True,\n               \"standardize\": None,\n               \"split\": text_vectorization.SPLIT_ON_WHITESPACE,\n               \"output_mode\": text_vectorization.MULTI_HOT\n@@ -179,6 +181,7 @@ def _get_end_to_end_test_cases():\n                         [\"and\"], [\"earth\"], [\"michigan\"]]),\n           \"kwargs\": {\n               \"max_tokens\": 5,\n+              \"pad_to_max_tokens\": True,\n               \"standardize\": None,\n               \"split\": None,\n               \"output_mode\": text_vectorization.COUNT\n@@ -198,6 +201,7 @@ def _get_end_to_end_test_cases():\n                         [\"earth michigan\"]]),\n           \"kwargs\": {\n               \"max_tokens\": 5,\n+              \"pad_to_max_tokens\": True,\n               \"standardize\": None,\n               \"split\": text_vectorization.SPLIT_ON_WHITESPACE,\n               \"output_mode\": text_vectorization.COUNT\n@@ -216,6 +220,7 @@ def _get_end_to_end_test_cases():\n                         [\"and\"], [\"earth\"], [\"michigan\"]]),\n           \"kwargs\": {\n               \"max_tokens\": 5,\n+              \"pad_to_max_tokens\": True,\n               \"standardize\": None,\n               \"split\": None,\n               \"output_mode\": text_vectorization.TF_IDF\n@@ -236,6 +241,7 @@ def _get_end_to_end_test_cases():\n                         [\"earth michigan\"]]),\n           \"kwargs\": {\n               \"max_tokens\": 5,\n+              \"pad_to_max_tokens\": True,\n               \"standardize\": None,\n               \"split\": text_vectorization.SPLIT_ON_WHITESPACE,\n               \"output_mode\": text_vectorization.TF_IDF\n@@ -453,12 +459,6 @@ class TextVectorizationLayerTest(keras_parameterized.TestCase,\n     output = vectorization(tf.ragged.constant(data, inner_shape=(1,)))\n     self.assertAllEqual(expected, output)\n \n-  def test_layer_list_input(self):\n-    layer = text_vectorization.TextVectorization(vocabulary=[\"a\", \"b\", \"c\"])\n-    output = layer([\"a\", \"b\", \"c\"])\n-    expected_output = [[2], [3], [4]]\n-    self.assertEqual(output.numpy().tolist(), expected_output)\n-\n \n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class TextVectorizationPreprocessingTest(\n@@ -478,6 +478,7 @@ class TextVectorizationPreprocessingTest(\n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n     layer = text_vectorization.TextVectorization(\n         max_tokens=10,\n+        pad_to_max_tokens=True,\n         standardize=text_vectorization.LOWER_AND_STRIP_PUNCTUATION,\n         split=None,\n         ngrams=None,\n@@ -783,7 +784,7 @@ class TextVectorizationPreprocessingTest(\n     self.assertAllEqual(expected_output, output_dataset)\n \n   def test_vocab_setting_with_oov_via_setter(self):\n-    vocab_data = [\"\", \"[UNK]\", \"earth\", \"wind\", \"and\", \"fire\"]\n+    vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n     input_array = np.array([[\"earth\", \"wind\", \"and\", \"fire\"],\n                             [\"fire\", \"and\", \"earth\", \"michigan\"]])\n     expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]\n@@ -1082,35 +1083,6 @@ class TextVectorizationOutputTest(\n     output_dataset = model.predict(input_array)\n     self.assertAllEqual(expected_output, output_dataset)\n \n-  def test_bag_output_hard_maximum_set_state_variables_after_build(self):\n-    state_variables = {\n-        text_vectorization._VOCAB_NAME: [\"earth\", \"wind\", \"and\", \"fire\"]\n-    }\n-    input_array = np.array([[\"earth\", \"wind\", \"and\", \"earth\"],\n-                            [\"ohio\", \"and\", \"earth\", \"michigan\"]])\n-\n-    # pyformat: disable\n-    expected_output = [[0, 1, 1, 1, 0],\n-                       [1, 1, 0, 1, 0]]\n-    # pyformat: enable\n-    max_tokens = 5\n-    expected_output_shape = [None, max_tokens]\n-\n-    input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = text_vectorization.TextVectorization(\n-        max_tokens=max_tokens,\n-        standardize=None,\n-        split=None,\n-        output_mode=text_vectorization.MULTI_HOT,\n-        pad_to_max_tokens=True)\n-    int_data = layer(input_data)\n-    layer._set_state_variables(state_variables)\n-    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())\n-\n-    model = keras.Model(inputs=input_data, outputs=int_data)\n-    output_dataset = model.predict(input_array)\n-    self.assertAllEqual(expected_output, output_dataset)\n-\n   def test_bag_output_hard_maximum_multiple_adapts(self):\n     input_array = np.array([[\"earth\", \"wind\", \"and\", \"earth\"],\n                             [\"ohio\", \"and\", \"earth\", \"michigan\"]])\n@@ -1143,6 +1115,8 @@ class TextVectorizationOutputTest(\n     first_output = model.predict(input_array)\n     # Test the second adapt\n     layer.adapt(second_adapt_data)\n+    # We need to recompile the model to retrace our call graph.\n+    model.compile()\n     second_output = model.predict(input_array)\n     self.assertAllEqual(first_expected_output, first_output)\n     self.assertAllEqual(second_expected_output, second_output)\n@@ -1175,7 +1149,7 @@ class TextVectorizationOutputTest(\n     output_dataset = model.predict(input_array)\n     self.assertAllEqual(expected_output, output_dataset)\n \n-  def test_bag_output_soft_maximum_set_vocabulary_after_call_fails(self):\n+  def test_vocab_size_changed_pad_to_max_false_fails(self):\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n@@ -1187,25 +1161,11 @@ class TextVectorizationOutputTest(\n         pad_to_max_tokens=False)\n     layer.adapt(vocab_data)\n     _ = layer(input_data)\n-    with self.assertRaisesRegex(RuntimeError, \"vocabulary cannot be changed\"):\n-      layer.set_vocabulary(vocab_data)\n \n-  def test_bag_output_soft_maximum_set_state_variables_after_call_fails(self):\n-    state_variables = {\n-        text_vectorization._VOCAB_NAME: [\"earth\", \"wind\", \"and\", \"fire\"]\n-    }\n-\n-    input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = text_vectorization.TextVectorization(\n-        max_tokens=None,\n-        standardize=None,\n-        split=None,\n-        output_mode=text_vectorization.MULTI_HOT,\n-        pad_to_max_tokens=False)\n-    layer.adapt([\"earth\", \"wind\"])\n+    layer.set_vocabulary(vocab_data[:2])\n+    with self.assertRaisesRegex(RuntimeError,\n+                                \"vocabulary size cannot be changed\"):\n       _ = layer(input_data)\n-    with self.assertRaisesRegex(RuntimeError, \"vocabulary cannot be changed\"):\n-      layer._set_state_variables(state_variables)\n \n   def test_count_output_hard_maximum(self):\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n\n@@ -170,6 +170,11 @@ class IndexLookupLayerSavedModelSaver(LayerSavedModelSaver):\n   def python_properties(self):\n     # TODO(kathywu): Add python property validator\n     metadata = self._python_properties_internal()\n-    if metadata['config'].get('has_static_table', False):\n+    # Clear the vocabulary from the config during saving. The vocab will be\n+    # saved as part of the lookup table directly, which correctly handle saving\n+    # vocabulary files as a SavedModel asset.\n     metadata['config']['vocabulary'] = None\n+    # Keep a separate config property to track that a vocabulary was passed in\n+    # and not adapted.\n+    metadata['config']['has_input_vocabulary'] = self.obj._has_input_vocabulary  # pylint: disable=protected-access\n     return metadata\n\n@@ -95,6 +95,8 @@ def count_params(weights):\n       The total number of scalars composing the weights\n   \"\"\"\n   unique_weights = {id(w): w for w in weights}.values()\n+  # Ignore TrackableWeightHandlers, which will not have a shape defined.\n+  unique_weights = [w for w in unique_weights if hasattr(w, 'shape')]\n   weight_shapes = [w.shape.as_list() for w in unique_weights]\n   standardized_weight_shapes = [\n       [0 if w_i is None else w_i for w_i in w] for w in weight_shapes\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
