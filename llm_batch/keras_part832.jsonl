{"custom_id": "keras#d7a8c20d104f26716ff496fe71d4fdefdffc2bbc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 56 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -7,9 +7,9 @@ from keras.src.datasets import imdb\n class ImdbLoadDataTest(testing.TestCase):\n     def test_load_data_default(self):\n         (x_train, y_train), (x_test, y_test) = imdb.load_data()\n-        self.assertIsInstance(x_train, list)\n+        self.assertIsInstance(x_train, np.ndarray)\n         self.assertIsInstance(y_train, np.ndarray)\n-        self.assertIsInstance(x_test, list)\n+        self.assertIsInstance(x_test, np.ndarray)\n         self.assertIsInstance(y_test, np.ndarray)\n \n         # Check lengths\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0ed820f5649bcb27531d73cfc023763712fc8bf9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 111 | Lines Deleted: 23 | Files Changed: 4 | Hunks: 29 | Methods Changed: 13 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 134 | Churn Cumulative: 398 | Contributors (this commit): 6 | Commits (past 90d): 9 | Contributors (cumulative): 15 | DMM Complexity: 0.9555555555555556\n\nDIFF:\n@@ -16,7 +16,7 @@ class JaxOptimizer(base_optimizer.BaseOptimizer):\n     def _backend_apply_gradients(self, grads, trainable_variables):\n         if self.gradient_accumulation_steps:\n             is_update_step = (\n-                self.iterations + 1\n+                self._iterations + 1\n             ) % self.gradient_accumulation_steps == 0\n             steps = self.gradient_accumulation_steps\n \n@@ -47,6 +47,10 @@ class JaxOptimizer(base_optimizer.BaseOptimizer):\n                 lambda: list(grads),\n             )\n \n+            # Apply clipping and weight decay.\n+            grads = self._clip_gradients(grads)\n+            self._apply_weight_decay(trainable_variables)\n+\n             self._backend_update_step(\n                 grads, trainable_variables, self.learning_rate\n             )\n@@ -71,6 +75,10 @@ class JaxOptimizer(base_optimizer.BaseOptimizer):\n                 g_acc.assign(n_g_acc)\n \n         else:\n+            # Apply clipping and weight decay.\n+            grads = self._clip_gradients(grads)\n+            self._apply_weight_decay(trainable_variables)\n+\n             self._backend_update_step(\n                 grads, trainable_variables, self.learning_rate\n             )\n@@ -101,4 +109,4 @@ class JaxOptimizer(base_optimizer.BaseOptimizer):\n                         + var.value * should_not_overwrite_model_vars_int\n                     )\n \n-        self.iterations.assign_add(1)\n+        self._iterations.assign_add(1)\n\n@@ -194,16 +194,19 @@ class OptimizerDistributeTest(testing.TestCase, parameterized.TestCase):\n             self.assertAllClose(\n                 optimizer._accumulated_gradients[0], [[1.0, 1.0], [2.0, 2.0]]\n             )\n-            self.assertAllClose(optimizer.iterations, 1)\n+            self.assertAllClose(optimizer._iterations, 1)\n+            self.assertAllClose(optimizer.iterations, 0)\n             self.strategy.run(lambda: optimizer.apply_gradients([(grads, v)]))\n             self.assertAllClose(v, [[1.0, 2.0], [3.0, 4.0]])\n             self.assertAllClose(\n                 optimizer._accumulated_gradients[0], [[2.0, 2.0], [4.0, 4.0]]\n             )\n-            self.assertAllClose(optimizer.iterations, 2)\n+            self.assertAllClose(optimizer._iterations, 2)\n+            self.assertAllClose(optimizer.iterations, 0)\n             self.strategy.run(lambda: optimizer.apply_gradients([(grads, v)]))\n             self.assertAllClose(v, [[-1.0, 0.0], [-1.0, 0.0]])\n             self.assertAllClose(\n                 optimizer._accumulated_gradients[0], [[0.0, 0.0], [0.0, 0.0]]\n             )\n-            self.assertAllClose(optimizer.iterations, 3)\n+            self.assertAllClose(optimizer._iterations, 3)\n+            self.assertAllClose(optimizer.iterations, 1)\n\n@@ -110,7 +110,7 @@ class BaseOptimizer(KerasSaveable):\n                 aggregation=\"only_first_replica\",\n             )\n         self._track_variable(iterations)\n-        self.iterations = iterations\n+        self._iterations = iterations\n \n         # Create learning rate (schedule or variable)\n         if isinstance(\n@@ -139,6 +139,15 @@ class BaseOptimizer(KerasSaveable):\n             self._track_variable(learning_rate)\n             self._learning_rate = learning_rate\n \n+    @property\n+    def iterations(self):\n+        if self.gradient_accumulation_steps:\n+            return ops.floor_divide(\n+                self._iterations, self.gradient_accumulation_steps\n+            )\n+\n+        return self._iterations\n+\n     def _track_variable(self, variable):\n         self._tracker.add_to_store(\"variables\", variable)\n \n@@ -281,7 +290,7 @@ class BaseOptimizer(KerasSaveable):\n         grads, trainable_variables = zip(*grads_and_vars)\n         self.apply(grads, trainable_variables)\n         # Return iterations for compat with tf.keras.\n-        return self.iterations\n+        return self._iterations\n \n     def apply(self, grads, trainable_variables=None):\n         \"\"\"Update traininable variables according to provided gradient values.\n@@ -343,10 +352,6 @@ class BaseOptimizer(KerasSaveable):\n             if scale is not None:\n                 grads = [g if g is None else g / scale for g in grads]\n \n-            # Apply clipping and weight decay.\n-            grads = self._clip_gradients(grads)\n-            self._apply_weight_decay(trainable_variables)\n-\n             # Apply gradient updates.\n             self._backend_apply_gradients(grads, trainable_variables)\n             # Apply variable constraints after applying gradients.\n@@ -372,7 +377,7 @@ class BaseOptimizer(KerasSaveable):\n         \"\"\"\n         if self.gradient_accumulation_steps:\n             is_update_step = (\n-                self.iterations + 1\n+                self._iterations + 1\n             ) % self.gradient_accumulation_steps == 0\n             # `trainable_variables` might have been filtered in previous\n             # processing steps, so we need to ensure the correct mapping between\n@@ -388,6 +393,11 @@ class BaseOptimizer(KerasSaveable):\n                 grads = [\n                     (g + acc_g) / steps for g, acc_g in zip(grads, acc_grads)\n                 ]\n+\n+                # Apply clipping and weight decay.\n+                grads = self._clip_gradients(grads)\n+                self._apply_weight_decay(trainable_variables)\n+\n                 self._backend_update_step(\n                     grads, trainable_variables, self.learning_rate\n                 )\n@@ -401,6 +411,10 @@ class BaseOptimizer(KerasSaveable):\n                 ),\n             )\n         else:\n+            # Apply clipping and weight decay.\n+            grads = self._clip_gradients(grads)\n+            self._apply_weight_decay(trainable_variables)\n+\n             # Run udpate step.\n             self._backend_update_step(\n                 grads, trainable_variables, self.learning_rate\n@@ -424,7 +438,7 @@ class BaseOptimizer(KerasSaveable):\n                     lambda: None,\n                 )\n         # Update iteration counter.\n-        self.iterations.assign_add(1)\n+        self._iterations.assign_add(1)\n \n     def _backend_update_step(self, grads, trainable_variables, learning_rate):\n         \"\"\"Collective update_step that can be overridden by the backend.\n@@ -586,7 +600,7 @@ class BaseOptimizer(KerasSaveable):\n         if isinstance(\n             self._learning_rate, learning_rate_schedule.LearningRateSchedule\n         ):\n-            return self._learning_rate(self.iterations)\n+            return self._learning_rate(self._iterations)\n         elif callable(self._learning_rate):\n             return self._learning_rate()\n         return self._learning_rate\n@@ -619,7 +633,7 @@ class BaseOptimizer(KerasSaveable):\n                 if self.gradient_accumulation_steps:\n                     # Utilize a stateless manner for JAX compatibility\n                     steps = self.gradient_accumulation_steps\n-                    is_update_step = (self.iterations + 1) % steps == 0\n+                    is_update_step = (self._iterations + 1) % steps == 0\n                     acc_g = self._accumulated_gradients[\n                         self._get_variable_index(v)\n                     ]\n@@ -964,7 +978,10 @@ base_optimizer_keyword_args = \"\"\"name: String. The name to use\n             value of the gradients since the last update. This is known as\n             \"gradient accumulation\". This can be useful\n             when your batch size is very small, in order to reduce gradient\n-            noise at each update step.\n+            noise at each update step. EMA frequency will look at \"accumulated\"\n+            iterations value (optimizer steps // gradient_accumulation_steps).\n+            Learning rate schedules will look at \"real\" iterations value\n+            (optimizer steps).\n \"\"\"\n \n \n\n@@ -195,30 +195,42 @@ class OptimizerTest(testing.TestCase, parameterized.TestCase):\n             learning_rate=1.0, gradient_accumulation_steps=3\n         )\n         self.assertEqual(optimizer.gradient_accumulation_steps, 3)\n+\n+        # Iteration 1\n         optimizer.apply_gradients([(grads, v)])\n         self.assertAllClose(v, [[1.0, 2.0], [3.0, 4.0]])\n         self.assertAllClose(\n             optimizer._accumulated_gradients[0], [[1.0, 1.0], [1.0, 1.0]]\n         )\n-        self.assertAllClose(optimizer.iterations, 1)\n+        self.assertAllClose(optimizer._iterations, 1)\n+        self.assertAllClose(optimizer.iterations, 0)\n+\n+        # Iteration 2\n         optimizer.apply_gradients([(grads, v)])\n         self.assertAllClose(v, [[1.0, 2.0], [3.0, 4.0]])\n         self.assertAllClose(\n             optimizer._accumulated_gradients[0], [[2.0, 2.0], [2.0, 2.0]]\n         )\n-        self.assertAllClose(optimizer.iterations, 2)\n+        self.assertAllClose(optimizer._iterations, 2)\n+        self.assertAllClose(optimizer.iterations, 0)\n+\n+        # Iteration 3\n         optimizer.apply_gradients([(grads, v)])\n         self.assertAllClose(v, [[0.0, 1.0], [2.0, 3.0]])\n         self.assertAllClose(\n             optimizer._accumulated_gradients[0], [[0.0, 0.0], [0.0, 0.0]]\n         )\n-        self.assertAllClose(optimizer.iterations, 3)\n+        self.assertAllClose(optimizer._iterations, 3)\n+        self.assertAllClose(optimizer.iterations, 1)\n+\n+        # Iteration 4\n         optimizer.apply_gradients([(grads, v)])\n         self.assertAllClose(v, [[0.0, 1.0], [2.0, 3.0]])\n         self.assertAllClose(\n             optimizer._accumulated_gradients[0], [[1.0, 1.0], [1.0, 1.0]]\n         )\n-        self.assertAllClose(optimizer.iterations, 4)\n+        self.assertAllClose(optimizer._iterations, 4)\n+        self.assertAllClose(optimizer.iterations, 1)\n \n     @pytest.mark.skipif(backend.backend() != \"tensorflow\", reason=\"Requires TF\")\n     def test_tf_checkpointing(self):\n@@ -281,7 +293,8 @@ class OptimizerTest(testing.TestCase, parameterized.TestCase):\n \n         # Iteration 1\n         optimizer.apply_gradients([(grad_ones, v), (grad_ones, v2)])\n-        self.assertAllClose(optimizer.iterations, 1)\n+        self.assertAllClose(optimizer._iterations, 1)\n+        self.assertAllClose(optimizer.iterations, 0)\n         self.assertAllClose(v, [[1.0, 2.0], [3.0, 4.0]])\n         self.assertAllClose(v2, [[1.0, 2.0], [3.0, 4.0]])\n         self.assertAllClose(\n@@ -290,9 +303,11 @@ class OptimizerTest(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(\n             optimizer._accumulated_gradients[1], [[1.0, 1.0], [1.0, 1.0]]\n         )\n+\n         # Iteration 2\n         optimizer.apply_gradients([(grad_twos, v), (grad_twos, v2)])\n-        self.assertAllClose(optimizer.iterations, 2)\n+        self.assertAllClose(optimizer._iterations, 2)\n+        self.assertAllClose(optimizer.iterations, 1)\n         self.assertAllClose(v, [[2.0, 2.0], [2.0, 2.0]])\n         self.assertAllClose(v2, [[-0.5, 0.5], [1.5, 2.5]])\n         self.assertAllClose(\n@@ -301,9 +316,11 @@ class OptimizerTest(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(\n             optimizer._accumulated_gradients[1], [[0.0, 0.0], [0.0, 0.0]]\n         )\n+\n         # Iteration 3\n         optimizer.apply_gradients([(grad_ones, v), (grad_ones, v2)])\n-        self.assertAllClose(optimizer.iterations, 3)\n+        self.assertAllClose(optimizer._iterations, 3)\n+        self.assertAllClose(optimizer.iterations, 1)\n         self.assertAllClose(v, [[2.0, 2.0], [2.0, 2.0]])\n         self.assertAllClose(v2, [[-0.5, 0.5], [1.5, 2.5]])\n         self.assertAllClose(\n@@ -313,6 +330,49 @@ class OptimizerTest(testing.TestCase, parameterized.TestCase):\n             optimizer._accumulated_gradients[1], [[1.0, 1.0], [1.0, 1.0]]\n         )\n \n+    @parameterized.parameters(\n+        [\n+            (\"adam\",),\n+            (\"sgd\",),\n+            (\"adamw\",),\n+            (\"adagrad\",),\n+            (\"rmsprop\",),\n+            (\"adadelta\",),\n+            (\"adamax\",),\n+            (\"lion\",),\n+            (\"nadam\",),\n+            (\"ftrl\",),\n+            (\"adafactor\",),\n+        ]\n+    )\n+    def test_gradient_accumulation_with_weigth_decay(self, optimizer):\n+        optimizer1 = optimizers.get(\n+            {\"class_name\": optimizer, \"config\": {\"weight_decay\": 0.05}}\n+        )\n+        optimizer3 = optimizers.get(\n+            {\n+                \"class_name\": optimizer,\n+                \"config\": {\n+                    \"weight_decay\": 0.05,\n+                    \"gradient_accumulation_steps\": 3,\n+                },\n+            }\n+        )\n+        variable1 = backend.Variable([[0.9], [0.5]])\n+        variable3 = backend.Variable([[0.9], [0.5]])\n+\n+        for epoch in range(8):\n+            grads3 = np.random.random([3, 2, 1]).astype(\"float32\")\n+\n+            grads1 = backend.convert_to_tensor(grads3.mean(axis=0))\n+            optimizer1.apply_gradients([(grads1, variable1)])\n+\n+            for batch in range(3):\n+                grads3_ = backend.convert_to_tensor(grads3[batch])\n+                optimizer3.apply_gradients([(grads3_, variable3)])\n+\n+        self.assertAllClose(variable1, variable3)\n+\n     def test_setting_lr_to_callable_untracks_lr_var(self):\n         adam = optimizers.Adam(learning_rate=0.001)\n         self.assertLen(adam.variables, 2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dfc4f788720ee58322c927ef061501b48a47eac3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 128 | Lines Deleted: 76 | Files Changed: 3 | Hunks: 32 | Methods Changed: 32 | Complexity Δ (Sum/Max): -1/4 | Churn Δ: 204 | Churn Cumulative: 988 | Contributors (this commit): 5 | Commits (past 90d): 17 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -26,13 +26,17 @@ from keras.src.utils import io_utils\n from keras.src.utils import naming\n from keras.src.utils import plot_model\n from keras.src.utils.model_visualization import check_pydot\n+from keras.src.utils.summary_utils import weight_memory_size\n from keras.src.version import __version__ as keras_version\n \n try:\n     import h5py\n except ImportError:\n     h5py = None\n-\n+try:\n+    import psutil\n+except ImportError:\n+    psutil = None\n try:\n     import huggingface_hub\n except ImportError:\n@@ -45,6 +49,7 @@ _VARS_FNAME = \"model.weights\"  # Will become e.g. \"model.weights.h5\"\n _VARS_FNAME_H5 = _VARS_FNAME + \".h5\"\n _VARS_FNAME_NPZ = _VARS_FNAME + \".npz\"\n _ASSETS_DIRNAME = \"assets\"\n+_MEMORY_UPPER_BOUND = 0.5  # 50%\n \n \n _MODEL_CARD_TEMPLATE = \"\"\"\n@@ -199,18 +204,28 @@ def _save_model_to_fileobj(model, fileobj, weights_format):\n         write_zf = False\n         try:\n             if weights_format == \"h5\":\n-                if isinstance(fileobj, io.BufferedWriter):\n-                    # First, open the .h5 file, then write it to `zf` at the end\n-                    # of the function call.\n+                try:\n+                    if is_memory_sufficient(model):\n+                        # Load the model weights into memory before writing\n+                        # .keras if the system memory is sufficient.\n+                        weights_store = H5IOStore(\n+                            _VARS_FNAME_H5, archive=zf, mode=\"w\"\n+                        )\n+                    else:\n+                        # Try opening the .h5 file, then writing it to `zf` at\n+                        # the end of the function call. This is more memory\n+                        # efficient than writing the weights into memory first.\n                         working_dir = pathlib.Path(fileobj.name).parent\n                         weights_file_path = tempfile.NamedTemporaryFile(\n                             dir=working_dir\n                         )\n-                    weights_store = H5IOStore(weights_file_path.name, mode=\"w\")\n+                        weights_store = H5IOStore(\n+                            weights_file_path.name, mode=\"w\"\n+                        )\n                         write_zf = True\n-                else:\n-                    # Fall back when `fileobj` is an `io.BytesIO`. Typically,\n-                    # this usage is for pickling.\n+                except:\n+                    # If we can't use the local disk for any reason, write the\n+                    # weights into memory first, which consumes more memory.\n                     weights_store = H5IOStore(\n                         _VARS_FNAME_H5, archive=zf, mode=\"w\"\n                     )\n@@ -237,6 +252,8 @@ def _save_model_to_fileobj(model, fileobj, weights_format):\n         except:\n             # Skip the final `zf.write` if any exception is raised\n             write_zf = False\n+            if weights_store:\n+                weights_store.archive = None\n             raise\n         finally:\n             if weights_store:\n@@ -432,10 +449,18 @@ def _load_model_from_fileobj(fileobj, custom_objects, compile, safe_mode):\n         asset_store = None\n         try:\n             if _VARS_FNAME_H5 in all_filenames:\n-                if isinstance(fileobj, io.BufferedReader):\n-                    # First, extract the model.weights.h5 file, then load it\n-                    # using h5py.\n                 try:\n+                    if is_memory_sufficient(model):\n+                        # Load the entire file into memory if the system memory\n+                        # is sufficient.\n+                        io_file = io.BytesIO(\n+                            zf.open(_VARS_FNAME_H5, \"r\").read()\n+                        )\n+                        weights_store = H5IOStore(io_file, mode=\"r\")\n+                    else:\n+                        # Try extracting the model.weights.h5 file, and then\n+                        # loading it using using h5py. This is significantly\n+                        # faster than reading from the zip archive on the fly.\n                         extract_dir = tempfile.TemporaryDirectory(\n                             dir=pathlib.Path(fileobj.name).parent\n                         )\n@@ -444,12 +469,10 @@ def _load_model_from_fileobj(fileobj, custom_objects, compile, safe_mode):\n                             pathlib.Path(extract_dir.name, _VARS_FNAME_H5),\n                             mode=\"r\",\n                         )\n-                    except OSError:\n-                        # Fall back when it is a read-only system\n-                        weights_store = H5IOStore(_VARS_FNAME_H5, zf, mode=\"r\")\n-                else:\n-                    # Fall back when `fileobj` is an `io.BytesIO`. Typically,\n-                    # this usage is for pickling.\n+                except:\n+                    # If we can't use the local disk for any reason, read the\n+                    # weights from the zip archive on the fly, which is less\n+                    # efficient.\n                     weights_store = H5IOStore(_VARS_FNAME_H5, zf, mode=\"r\")\n             elif _VARS_FNAME_NPZ in all_filenames:\n                 weights_store = NpzIOStore(_VARS_FNAME_NPZ, zf, mode=\"r\")\n@@ -1080,3 +1103,20 @@ def get_attr_skiplist(obj_type):\n         f\"saving_attr_skiplist_{obj_type}\", skiplist\n     )\n     return skiplist\n+\n+\n+def is_memory_sufficient(model):\n+    \"\"\"Check if there is sufficient memory to load the model into memory.\n+\n+    If psutil is installed, we can use it to determine whether the memory is\n+    sufficient. Otherwise, we use a predefined value of 1 GB for available\n+    memory.\n+    \"\"\"\n+    if psutil is None:\n+        available_memory = 1024 * 1024 * 1024  # 1 GB in bytes\n+    else:\n+        available_memory = psutil.virtual_memory().available  # In bytes\n+    return (\n+        weight_memory_size(model.variables)\n+        < available_memory * _MEMORY_UPPER_BOUND\n+    )\n\n@@ -10,6 +10,7 @@ from unittest import mock\n \n import numpy as np\n import pytest\n+from absl.testing import parameterized\n \n import keras\n from keras.src import backend\n@@ -245,7 +246,17 @@ def _load_model_fn(filepath):\n     saving_lib.load_model(filepath)\n \n \n-class SavingTest(testing.TestCase):\n+class SavingTest(testing.TestCase, parameterized.TestCase):\n+    def setUp(self):\n+        # Set `_MEMORY_UPPER_BOUND` to zero for testing purpose.\n+        self.original_value = saving_lib._MEMORY_UPPER_BOUND\n+        saving_lib._MEMORY_UPPER_BOUND = 0\n+        return super().setUp()\n+\n+    def tearDown(self) -> None:\n+        saving_lib._MEMORY_UPPER_BOUND = self.original_value\n+        return super().tearDown()\n+\n     def _test_inference_after_instantiation(self, model):\n         x_ref = np.random.random((2, 4))\n         y_ref = model(x_ref)\n@@ -258,28 +269,20 @@ class SavingTest(testing.TestCase):\n             self.assertAllClose(w_ref, w)\n         self.assertAllClose(y_ref, loaded_model(x_ref))\n \n-    def test_inference_after_instantiation_subclassed(self):\n-        model = _get_subclassed_model(compile=False)\n+    @parameterized.named_parameters(\n+        (\"subclassed\", _get_subclassed_model),\n+        (\"basic_sequential\", _get_basic_sequential_model),\n+        (\"basic_functional\", _get_basic_functional_model),\n+        (\"custom_sequential\", _get_custom_sequential_model),\n+        (\"custom_functional\", _get_custom_functional_model),\n+        (\"subclassed_functional\", _get_subclassed_functional_model),\n+    )\n+    def test_inference_after_instantiation(self, model_fn):\n+        model = model_fn(compile=False)\n         self._test_inference_after_instantiation(model)\n \n-    def test_inference_after_instantiation_basic_sequential(self):\n-        model = _get_basic_sequential_model(compile=False)\n-        self._test_inference_after_instantiation(model)\n-\n-    def test_inference_after_instantiation_basic_functional(self):\n-        model = _get_basic_functional_model(compile=False)\n-        self._test_inference_after_instantiation(model)\n-\n-    def test_inference_after_instantiation_custom_sequential(self):\n-        model = _get_custom_sequential_model(compile=False)\n-        self._test_inference_after_instantiation(model)\n-\n-    def test_inference_after_instantiation_custom_functional(self):\n-        model = _get_custom_functional_model(compile=False)\n-        self._test_inference_after_instantiation(model)\n-\n-    def test_inference_after_instantiation_subclassed_functional(self):\n-        model = _get_subclassed_functional_model(compile=False)\n+        # Test small model path\n+        saving_lib._MEMORY_UPPER_BOUND = 1.0\n         self._test_inference_after_instantiation(model)\n \n     def _test_compile_preserved(self, model):\n@@ -314,34 +317,21 @@ class SavingTest(testing.TestCase):\n         for ref_m, m in zip(ref_metrics, new_metrics):\n             self.assertAllClose(ref_m, m)\n \n+    @parameterized.named_parameters(\n+        (\"subclassed\", _get_subclassed_model),\n+        (\"basic_sequential\", _get_basic_sequential_model),\n+        (\"basic_functional\", _get_basic_functional_model),\n+        (\"custom_sequential\", _get_custom_sequential_model),\n+        (\"custom_functional\", _get_custom_functional_model),\n+        (\"subclassed_functional\", _get_subclassed_functional_model),\n+    )\n     @pytest.mark.requires_trainable_backend\n-    def test_compile_preserved_subclassed(self):\n-        model = _get_subclassed_model(compile=True)\n+    def test_compile_preserved(self, model_fn):\n+        model = model_fn(compile=True)\n         self._test_compile_preserved(model)\n \n-    @pytest.mark.requires_trainable_backend\n-    def test_compile_preserved_basic_sequential(self):\n-        model = _get_basic_sequential_model(compile=True)\n-        self._test_compile_preserved(model)\n-\n-    @pytest.mark.requires_trainable_backend\n-    def test_compile_preserved_custom_sequential(self):\n-        model = _get_custom_sequential_model(compile=True)\n-        self._test_compile_preserved(model)\n-\n-    @pytest.mark.requires_trainable_backend\n-    def test_compile_preserved_basic_functional(self):\n-        model = _get_basic_functional_model(compile=True)\n-        self._test_compile_preserved(model)\n-\n-    @pytest.mark.requires_trainable_backend\n-    def test_compile_preserved_custom_functional(self):\n-        model = _get_custom_functional_model(compile=True)\n-        self._test_compile_preserved(model)\n-\n-    @pytest.mark.requires_trainable_backend\n-    def test_compile_preserved_subclassed_functional(self):\n-        model = _get_subclassed_functional_model(compile=True)\n+        # Test small model path\n+        saving_lib._MEMORY_UPPER_BOUND = 1.0\n         self._test_compile_preserved(model)\n \n     def test_saving_preserve_unbuilt_state(self):\n@@ -627,19 +617,27 @@ class SavingTest(testing.TestCase):\n \n         self.assertAllClose(pred1, pred2, atol=1e-5)\n \n-    def test_save_model_exception_raised(self):\n+    @parameterized.named_parameters(\n+        (\"high_memory_config\", True),\n+        (\"low_memory_config\", False),\n+    )\n+    def test_save_model_exception_raised(self, is_memory_sufficient):\n+        if is_memory_sufficient:\n+            saving_lib._MEMORY_UPPER_BOUND = 0.5  # 50%\n+\n         # Assume we have an error in `save_own_variables`.\n         class RaiseErrorLayer(keras.layers.Layer):\n-            def __init__(self, **kwargs):\n+            def __init__(self, units, **kwargs):\n                 super().__init__(**kwargs)\n+                self.dense = keras.layers.Dense(units)\n \n             def call(self, inputs):\n-                return inputs\n+                return self.dense(inputs)\n \n             def save_own_variables(self, store):\n                 raise ValueError\n \n-        model = keras.Sequential([keras.Input([1]), RaiseErrorLayer()])\n+        model = keras.Sequential([keras.Input([1]), RaiseErrorLayer(1)])\n         filepath = f\"{self.get_temp_dir()}/model.keras\"\n         with self.assertRaises(ValueError):\n             saving_lib.save_model(model, filepath)\n@@ -654,19 +652,27 @@ class SavingTest(testing.TestCase):\n         self.assertLen(os.listdir(Path(filepath).parent), 1)\n         self.assertIn(\"model.keras\", os.listdir(Path(filepath).parent))\n \n-    def test_load_model_exception_raised(self):\n+    @parameterized.named_parameters(\n+        (\"high_memory_config\", True),\n+        (\"low_memory_config\", False),\n+    )\n+    def test_load_model_exception_raised(self, is_memory_sufficient):\n+        if is_memory_sufficient:\n+            saving_lib._MEMORY_UPPER_BOUND = 0.5  # 50%\n+\n         # Assume we have an error in `load_own_variables`.\n         class RaiseErrorLayer(keras.layers.Layer):\n-            def __init__(self, **kwargs):\n+            def __init__(self, units, **kwargs):\n                 super().__init__(**kwargs)\n+                self.dense = keras.layers.Dense(units)\n \n             def call(self, inputs):\n-                return inputs\n+                return self.dense(inputs)\n \n             def load_own_variables(self, store):\n                 raise ValueError\n \n-        model = keras.Sequential([keras.Input([1]), RaiseErrorLayer()])\n+        model = keras.Sequential([keras.Input([1]), RaiseErrorLayer(1)])\n         filepath = f\"{self.get_temp_dir()}/model.keras\"\n         saving_lib.save_model(model, filepath)\n         with self.assertRaises(ValueError):\n@@ -679,7 +685,7 @@ class SavingTest(testing.TestCase):\n         self.assertIn(\"model.keras\", os.listdir(Path(filepath).parent))\n \n     def test_load_model_read_only_system(self):\n-        model = keras.Sequential([keras.Input([1]), keras.layers.Dense(2)])\n+        model = keras.Sequential([keras.Input([1]), keras.layers.Dense(32)])\n         filepath = f\"{self.get_temp_dir()}/model.keras\"\n         saving_lib.save_model(model, filepath)\n \n\n@@ -1,3 +1,4 @@\n+import functools\n import math\n import re\n import shutil\n@@ -21,6 +22,14 @@ def count_params(weights):\n     return int(sum(math.prod(p) for p in shapes))\n \n \n+@functools.lru_cache(512)\n+def _compute_memory_size(shape, dtype):\n+    weight_counts = math.prod(shape)\n+    dtype = backend.standardize_dtype(dtype)\n+    per_param_size = dtype_utils.dtype_size(dtype)\n+    return weight_counts * per_param_size\n+\n+\n def weight_memory_size(weights):\n     \"\"\"Compute the memory footprint for weights based on their dtypes.\n \n@@ -33,10 +42,7 @@ def weight_memory_size(weights):\n     unique_weights = {id(w): w for w in weights}.values()\n     total_memory_size = 0\n     for w in unique_weights:\n-        weight_shape = math.prod(w.shape)\n-        dtype = backend.standardize_dtype(w.dtype)\n-        per_param_size = dtype_utils.dtype_size(dtype)\n-        total_memory_size += weight_shape * per_param_size\n+        total_memory_size += _compute_memory_size(w.shape, w.dtype)\n     return total_memory_size / 8\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#556b34faa2ef7b7bc88a282779f2a38c5378369d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 3 | Churn Cumulative: 25 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -428,9 +428,6 @@ class RNN(Layer):\n             output = last_output\n \n         if self.return_state:\n-            if len(states) == 1:\n-                state = states[0]\n-                return output, state\n             return output, *states\n         return output\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8ca7cd8b790f891ec77770fb55c2270465e8b691", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 54 | Lines Deleted: 2 | Files Changed: 4 | Hunks: 6 | Methods Changed: 8 | Complexity Δ (Sum/Max): 8/3 | Churn Δ: 56 | Churn Cumulative: 222 | Contributors (this commit): 4 | Commits (past 90d): 14 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -39,7 +39,7 @@ class Loss(KerasSaveable):\n     def __init__(self, name=None, reduction=\"sum_over_batch_size\", dtype=None):\n         self.name = name or auto_name(self.__class__.__name__)\n         self.reduction = standardize_reduction(reduction)\n-        self._dtype_policy = dtype_policies.get(dtype)\n+        self._dtype_policy = dtype_policies.get(dtype or backend.floatx())\n         self._dtype = self._dtype_policy.compute_dtype\n \n     @property\n\n@@ -18,6 +18,16 @@ class ExampleLoss(Loss):\n \n \n class LossTest(testing.TestCase):\n+    def setUp(self):\n+        self._global_dtype_policy = dtype_policies.dtype_policy.dtype_policy()\n+        self._floatx = backend.floatx()\n+        return super().setUp()\n+\n+    def tearDown(self):\n+        dtype_policies.dtype_policy.set_dtype_policy(self._global_dtype_policy)\n+        backend.set_floatx(self._floatx)\n+        return super().tearDown()\n+\n     def test_squeeze_or_expand(self):\n         x1 = ops.ones((3,))\n         x2 = ops.ones((3, 1))\n@@ -262,3 +272,18 @@ class LossTest(testing.TestCase):\n         # `dtype` setter should raise AttributeError\n         with self.assertRaises(AttributeError):\n             loss.dtype = \"bfloat16\"\n+\n+    def test_default_dtype(self):\n+        y_true = np.array([1.0, 0.0, 1.0, 0.0], dtype=\"float32\")\n+        y_pred = np.array([0.1, 0.2, 0.3, 0.4], dtype=\"float32\")\n+\n+        # Defaults to `keras.config.floatx()` not global `dtype_policy`\n+        dtype_policies.dtype_policy.set_dtype_policy(\"mixed_float16\")\n+        loss_fn = ExampleLoss()\n+        loss = loss_fn(y_true, y_pred)\n+        self.assertDType(loss, \"float32\")\n+\n+        backend.set_floatx(\"float16\")\n+        loss_fn = ExampleLoss()\n+        loss = loss_fn(y_true, y_pred)\n+        self.assertDType(loss, backend.floatx())\n\n@@ -91,7 +91,7 @@ class Metric(KerasSaveable):\n \n     def __init__(self, dtype=None, name=None):\n         self.name = name or auto_name(self.__class__.__name__)\n-        self._dtype_policy = dtype_policies.get(dtype)\n+        self._dtype_policy = dtype_policies.get(dtype or backend.floatx())\n         self._dtype = self._dtype_policy.compute_dtype\n         self._metrics = []\n         self._variables = []\n\n@@ -44,6 +44,16 @@ class ExampleMetric(Metric):\n \n \n class MetricTest(testing.TestCase):\n+    def setUp(self):\n+        self._global_dtype_policy = dtype_policies.dtype_policy.dtype_policy()\n+        self._floatx = backend.floatx()\n+        return super().setUp()\n+\n+    def tearDown(self):\n+        dtype_policies.dtype_policy.set_dtype_policy(self._global_dtype_policy)\n+        backend.set_floatx(self._floatx)\n+        return super().tearDown()\n+\n     def test_end_to_end_flow(self):\n         metric = ExampleMetric(name=\"mse\")\n         self.assertEqual(metric.name, \"mse\")\n@@ -228,3 +238,20 @@ class MetricTest(testing.TestCase):\n         # `dtype` setter should raise AttributeError\n         with self.assertRaises(AttributeError):\n             metric.dtype = \"bfloat16\"\n+\n+    def test_default_dtype(self):\n+        y_true = np.random.random((10, 3))\n+        y_pred = np.random.random((10, 3))\n+\n+        # Defaults to `keras.config.floatx()` not global `dtype_policy`\n+        dtype_policies.dtype_policy.set_dtype_policy(\"mixed_float16\")\n+        metric = ExampleMetric()\n+        metric.update_state(y_true, y_pred)\n+        result = metric.result()\n+        self.assertDType(result, \"float32\")\n+\n+        backend.set_floatx(\"float16\")\n+        metric = ExampleMetric()\n+        metric.update_state(y_true, y_pred)\n+        result = metric.result()\n+        self.assertDType(result, backend.floatx())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8f5ad1e75b3ef27908bd3820e204a4c59b94c938", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 202 | Lines Deleted: 178 | Files Changed: 2 | Hunks: 22 | Methods Changed: 15 | Complexity Δ (Sum/Max): -18/6 | Churn Δ: 380 | Churn Cumulative: 445 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -68,6 +68,31 @@ def _get_model_multi_outputs_dict():\n     return model\n \n \n+def _get_model_multi_outputs_dict_with_single_tensor():\n+    x = Input(shape=(3,), name=\"input_a\")\n+    output = layers.Dense(1, name=\"output_a\")(x)\n+    model = Model(x, {\"output_a\": output, \"output_b\": output})\n+    return model\n+\n+\n+def _get_model_with_custom_compute_loss():\n+\n+    class MyModel(Model):\n+        def __init__(self):\n+            inputs = Input(shape=(3,), name=\"inputs\")\n+            outputs = layers.Dense(1, name=\"a\")(inputs)\n+            super().__init__(inputs=inputs, outputs=outputs)\n+\n+        def compute_loss(self, x, y, y_pred, sample_weight=None, **kwargs):\n+            y_pred = [y_pred, y_pred]  # To list\n+            return super().compute_loss(\n+                x=x, y=y, y_pred=y_pred, sample_weight=sample_weight, **kwargs\n+            )\n+\n+    model = MyModel()\n+    return model\n+\n+\n @pytest.mark.requires_trainable_backend\n class ModelTest(testing.TestCase, parameterized.TestCase):\n     def test_functional_rerouting(self):\n@@ -459,6 +484,38 @@ class ModelTest(testing.TestCase, parameterized.TestCase):\n         )\n         self.assertListEqual(hist_keys, ref_keys)\n \n+    def test_functional_dict_outputs_with_single_tensor(self):\n+        model = _get_model_multi_outputs_dict_with_single_tensor()\n+        self.assertIsInstance(model, Functional)\n+        x = np.random.rand(8, 3)\n+        y1 = np.random.rand(8, 1)\n+        y2 = np.random.randint(0, 2, (8, 1))\n+\n+        # `model` has 2 outputs, but there is actually only 1 output tensor.\n+        self.assertLen(model.outputs, 2)\n+        model.compile(\n+            optimizer=\"sgd\",\n+            loss={\n+                \"output_a\": \"mean_squared_error\",\n+                \"output_b\": \"binary_crossentropy\",\n+            },\n+        )\n+        model.fit(x, (y1, y2), batch_size=2, epochs=1, verbose=0)\n+\n+    def test_functional_list_outputs_with_custom_compute_loss(self):\n+        model = _get_model_with_custom_compute_loss()\n+        self.assertIsInstance(model, Functional)\n+        x = np.random.rand(8, 3)\n+        y1 = np.random.rand(8, 1)\n+        y2 = np.random.randint(0, 2, (8, 1))\n+\n+        # `model` has 1 output, but in `compute_loss` it is separated into 2.\n+        self.assertLen(model.outputs, 1)\n+        model.compile(\n+            optimizer=\"sgd\", loss=[\"mean_squared_error\", \"binary_crossentropy\"]\n+        )\n+        model.fit(x, (y1, y2), batch_size=2, epochs=1, verbose=0)\n+\n     def test_functional_list_outputs_dict_losses_invalid_keys(self):\n         model = _get_model_multi_outputs_list()\n         self.assertIsInstance(model, Functional)\n@@ -474,9 +531,9 @@ class ModelTest(testing.TestCase, parameterized.TestCase):\n         )\n         # Fit the model to make sure compile_metrics are built\n         with self.assertRaisesRegex(\n-            ValueError,\n-            \"In the dict argument `loss`, \"\n-            \"key 'output_c' does not correspond to any model output\",\n+            KeyError,\n+            \"in the `loss` argument, but they can't be found in the \"\n+            \"model's output\",\n         ):\n             model.fit(x, (y1, y2), batch_size=2, epochs=1, verbose=0)\n \n@@ -492,9 +549,9 @@ class ModelTest(testing.TestCase, parameterized.TestCase):\n         )\n         # Fit the model to make sure compile_metrics are built\n         with self.assertRaisesRegex(\n-            ValueError,\n-            \"In the dict argument `loss`, \"\n-            \"key 'output_a' does not correspond to any model output\",\n+            KeyError,\n+            \"in the `loss` argument, but they can't be found in the \"\n+            \"model's output\",\n         ):\n             model.fit(x, (y1, y2), batch_size=2, epochs=1, verbose=0)\n \n@@ -537,9 +594,9 @@ class ModelTest(testing.TestCase, parameterized.TestCase):\n         )\n         # Fit the model to make sure compile_metrics are built\n         with self.assertRaisesRegex(\n-            ValueError,\n-            \"In the dict argument `loss`, \"\n-            \"key 'output_c' does not correspond to any model output\",\n+            KeyError,\n+            \"in the `loss` argument, but they can't be found in the \"\n+            \"model's output\",\n         ):\n             model.fit(x, (y1, y2), batch_size=2, epochs=1, verbose=0)\n \n\n@@ -1,4 +1,3 @@\n-from keras.src import backend\n from keras.src import losses as losses_module\n from keras.src import metrics as metrics_module\n from keras.src import ops\n@@ -429,209 +428,175 @@ class CompileLoss(losses_module.Loss):\n         self.output_names = output_names\n         super().__init__(name=\"compile_loss\", reduction=reduction)\n \n-    def build(self, y_true, y_pred):\n-        if self.output_names:\n-            output_names = self.output_names\n-        elif isinstance(y_pred, dict):\n-            output_names = sorted(list(y_pred.keys()))\n-        elif isinstance(y_pred, (list, tuple)):\n-            num_outputs = len(y_pred)\n-            if all(hasattr(x, \"_keras_history\") for x in y_pred):\n-                output_names = [x._keras_history.operation.name for x in y_pred]\n-            else:\n-                output_names = None\n-        else:\n-            output_names = None\n-            num_outputs = 1\n-        if output_names:\n-            num_outputs = len(output_names)\n+        # Inferred by `y_pred` and `output_names`\n+        self.inferred_output_names = None\n \n-        y_pred = self._flatten_y(y_pred)\n+    def build(self, y_true, y_pred):\n         loss = self._user_loss\n         loss_weights = self._user_loss_weights\n-        flat_losses = []\n-        flat_loss_weights = []\n-\n-        if isinstance(loss, dict):\n-            for name in loss.keys():\n-                if name not in output_names:\n-                    raise ValueError(\n-                        \"In the dict argument `loss`, key \"\n-                        f\"'{name}' does not correspond to any model output. \"\n-                        f\"Received:\\nloss={loss}\"\n-                    )\n-        if num_outputs == 1:\n-            if isinstance(loss, dict):\n-                loss = tree.flatten(loss)\n-            if isinstance(loss, list) and len(loss) == 1:\n-                loss = loss[0]\n-            if not is_function_like(loss):\n-                raise ValueError(\n-                    \"When there is only a single output, the `loss` argument \"\n-                    \"must be a callable. \"\n-                    f\"Received instead:\\nloss={loss} of type {type(loss)}\"\n-                )\n-            if isinstance(y_pred, list) and len(y_pred) == 1:\n-                y_pred = y_pred[0]\n+        output_names = self._get_y_pred_output_names(y_pred)\n+        inferred_output_names = output_names or self.output_names\n \n         if is_function_like(loss) and tree.is_nested(y_pred):\n             # The model has multiple outputs but only one loss fn\n             # was provided. Broadcast loss to all outputs.\n             loss = tree.map_structure(lambda x: loss, y_pred)\n \n-        # Iterate over all possible loss formats:\n-        # plain function, list/tuple, dict\n-        if is_function_like(loss):\n-            flat_losses.append(get_loss(loss, y_true, y_pred))\n-            if loss_weights:\n-                if not isinstance(loss_weights, float):\n-                    raise ValueError(\n-                        \"When there is only a single output, the \"\n-                        \"`loss_weights` argument \"\n-                        \"must be a Python float. \"\n-                        f\"Received instead: loss_weights={loss_weights} of \"\n-                        f\"type {type(loss_weights)}\"\n-                    )\n-                flat_loss_weights.append(loss_weights)\n-            else:\n-                flat_loss_weights.append(1.0)\n-        elif isinstance(loss, (list, tuple)):\n-            loss = tree.flatten(loss)\n-            if len(loss) != len(y_pred):\n-                raise ValueError(\n-                    \"For a model with multiple outputs, \"\n-                    \"when providing the `loss` argument as a list, \"\n-                    \"it should have as many entries as the model has outputs. \"\n-                    f\"Received:\\nloss={loss}\\nof length {len(loss)} \"\n-                    f\"whereas the model has {len(y_pred)} outputs.\"\n-                )\n-            if not all(is_function_like(e) for e in loss):\n-                raise ValueError(\n-                    \"For a model with multiple outputs, \"\n-                    \"when providing the `loss` argument as a list, \"\n-                    \"each list entry should be a callable (the loss function \"\n-                    \"corresponding to that output). \"\n-                    f\"Received: loss={loss}\"\n-                )\n-            flat_losses = [\n-                get_loss(fn, y_true, y_pred) for fn in loss if fn is not None\n-            ]\n-            if loss_weights:\n-                if not isinstance(loss_weights, (list, tuple)):\n-                    raise ValueError(\n-                        \"If the `loss` argument is provided as a list/tuple, \"\n-                        \"the `loss_weight` argument should also be provided as \"\n-                        \"a list/tuple, of equal length. \"\n-                        f\"Received: loss_weights={loss_weights}\"\n-                    )\n-                if len(loss_weights) != len(y_pred):\n-                    raise ValueError(\n-                        \"For a model with multiple outputs, \"\n-                        \"when providing the `loss_weights` argument as a list, \"\n-                        \"it should have as many entries as the model has \"\n-                        f\"outputs. Received: loss_weights={loss_weights} of \"\n-                        f\"length {len(loss_weights)} whereas the model has \"\n-                        f\"{len(y_pred)} outputs.\"\n-                    )\n-                if not all(isinstance(e, (int, float)) for e in loss_weights):\n-                    raise ValueError(\n-                        \"For a model with multiple outputs, when providing \"\n-                        \"the `loss_weights` argument as a list, \"\n-                        \"each list entry should be a Python int or float (the \"\n-                        \"weighting coefficient corresponding to the loss for \"\n-                        f\"that output). Received: loss_weights={loss_weights}\"\n-                    )\n-                flat_loss_weights = list(loss_weights)\n-            else:\n-                flat_loss_weights = [1.0 for _ in loss]\n-        elif isinstance(loss, dict):\n-            if output_names is None:\n+        # Check and filter the keys.\n+        if isinstance(loss, dict):\n+            if inferred_output_names is None:\n                 raise ValueError(\n                     \"Argument `loss` can only be provided as a dict \"\n                     \"when the model also returns a dict of outputs. \"\n                     f\"Received loss={loss}\"\n                 )\n-            for name in loss.keys():\n-                if isinstance(loss[name], list) and len(loss[name]) == 1:\n-                    loss[name] = loss[name][0]\n-                if not is_function_like(loss[name]):\n+        filtered_y_pred_keys = []\n+        filtered_y_true_keys = []\n+        if isinstance(loss, dict):\n+            loss_keys = set(loss.keys())\n+            if inferred_output_names is not None:\n+                y_pred_keys = set(inferred_output_names)\n+                if len(loss_keys - y_pred_keys) > 0:\n+                    raise KeyError(\n+                        f\"There are keys: {list(loss_keys - y_pred_keys)} in \"\n+                        \"the `loss` argument, but they can't be found in \"\n+                        \"the model's output (`y_pred`).\"\n+                    )\n+                filtered_y_pred_keys.extend(list(y_pred_keys - loss_keys))\n+            if isinstance(y_true, dict):\n+                y_true_keys = set(y_true.keys())\n+                if len(loss_keys - y_true_keys) > 0:\n+                    raise KeyError(\n+                        f\"There are keys: {list(loss_keys - y_true_keys)} in \"\n+                        \"the `loss` argument, but they can't be found in \"\n+                        \"`y` (`y_true`).\"\n+                    )\n+                filtered_y_true_keys.extend(list(y_true_keys - loss_keys))\n+        filtered_y_pred_keys = set(filtered_y_pred_keys)\n+        filtered_y_true_keys = set(filtered_y_true_keys)\n+\n+        # Filter unused inputs.\n+        y_true, y_pred = self._filter_unused_inputs(\n+            y_true,\n+            y_pred,\n+            filtered_y_true_keys,\n+            filtered_y_pred_keys,\n+            self.inferred_output_names,\n+        )\n+\n+        # `loss` could be a plain function (or a `Loss` instance), a list, a\n+        # nested list, or a dict. However, in `call`, we want to iterate over\n+        # all losses, so we flatten them into a list regardless of their\n+        # original structure.\n+        flat_losses = tree.flatten(loss)\n+        if loss_weights is None:\n+            flat_loss_weights = [None] * len(flat_losses)\n+        else:\n+            flat_loss_weights = tree.flatten(loss_weights)\n+            for loss_weight in flat_loss_weights:\n+                if not isinstance(loss_weight, (int, float, type(None))):\n+                    raise TypeError(\n+                        \"When providing the `loss_weights` argument, each \"\n+                        \"element should be a Python int, float (the weighting \"\n+                        \"coefficient corresponding to the loss for that \"\n+                        \"output) or `None`.\"\n+                        f\"Received: loss_weights={loss_weights}\"\n+                    )\n+            if len(flat_loss_weights) != len(flat_losses):\n+                raise ValueError(\n+                    \"When providing the `loss_weights` argument, it should \"\n+                    \"have equal length of `loss` argument. \"\n+                    f\"Received: loss_weights length={len(flat_loss_weights)}, \"\n+                    f\"loss legnth={len(flat_losses)}\"\n+                )\n+\n+        y_true = tree.flatten(y_true)\n+        y_pred = tree.flatten(y_pred)\n+        if len(y_pred) != len(flat_losses):\n             raise ValueError(\n                 \"For a model with multiple outputs, \"\n-                        \"when providing the `loss` argument as a dict, \"\n-                        \"each dict entry should be a callable (the loss \"\n-                        \"function corresponding to that output). \"\n-                        f\"At key '{name}', received invalid type:\\n{loss[name]}\"\n+                \"when providing the `loss` argument as a list, \"\n+                \"it should have as many entries as the model has outputs. \"\n+                f\"Received:\\nloss={loss}\\nof length {len(flat_losses)} \"\n+                f\"whereas the model has {len(y_pred)} outputs.\"\n             )\n-            for name, yt, yp in zip(output_names, y_true, y_pred):\n-                if name in loss:\n-                    if loss[name]:\n-                        flat_losses.append(get_loss(loss[name], yt, yp))\n-                    else:\n-                        flat_losses.append(None)\n-                else:\n-                    flat_losses.append(None)\n-            if loss_weights:\n-                if not isinstance(loss_weights, dict):\n-                    raise ValueError(\n-                        \"If the `loss` argument is provided as a dict, \"\n-                        \"the `loss_weight` argument should also be provided as \"\n-                        f\"a dict. Received: loss_weights={loss_weights}\"\n-                    )\n-                for name in loss_weights.keys():\n-                    if name not in output_names:\n-                        raise ValueError(\n-                            \"In the dict argument `loss_weights`, key \"\n-                            f\"'{name}' does not correspond to any model \"\n-                            f\"output. Received: loss_weights={loss_weights}\"\n-                        )\n-                    if not isinstance(loss_weights[name], float):\n-                        raise ValueError(\n-                            \"For a model with multiple outputs, \"\n-                            \"when providing the `loss_weights` argument as a \"\n-                            \"dict, each dict entry should be a Python float \"\n-                            \"(the weighting coefficient corresponding to the \"\n-                            f\"loss for that output). At key '{name}', \"\n-                            f\"received invalid type:\\n{loss_weights[name]}\"\n-                        )\n-                for name in output_names:\n-                    if name in loss_weights:\n-                        flat_loss_weights.append(loss_weights[name])\n-                    else:\n-                        flat_loss_weights.append(1.0)\n-            else:\n-                flat_loss_weights = [1.0 for _ in flat_losses]\n+\n+        # Get the real loss instances.\n+        flat_losses = [\n+            get_loss(identifier, _y_true, _y_pred)\n+            for identifier, _y_true, _y_pred in zip(flat_losses, y_true, y_pred)\n+        ]\n+\n         self.flat_losses = flat_losses\n         self.flat_loss_weights = flat_loss_weights\n+        self.filtered_y_true_keys = filtered_y_true_keys\n+        self.filtered_y_pred_keys = filtered_y_pred_keys\n+        self.inferred_output_names = inferred_output_names\n         self.built = True\n \n+    def _get_y_pred_output_names(self, y_pred):\n+        if isinstance(y_pred, dict):\n+            output_names = sorted(y_pred.keys())\n+        else:\n+            y_pred = tree.flatten(y_pred)\n+            if all(hasattr(x, \"_keras_history\") for x in y_pred):\n+                output_names = [x._keras_history.operation.name for x in y_pred]\n+            else:\n+                output_names = None\n+        return output_names\n+\n+    def _filter_unused_inputs(\n+        self,\n+        y_true,\n+        y_pred,\n+        filtered_y_true_keys,\n+        filtered_y_pred_keys,\n+        output_names,\n+    ):\n+        if len(filtered_y_true_keys) > 0:\n+            if isinstance(y_true, dict):\n+                for k in filtered_y_true_keys:\n+                    y_true.pop(k)\n+        if len(filtered_y_pred_keys) > 0:\n+            if isinstance(y_pred, dict):\n+                for k in filtered_y_pred_keys:\n+                    y_pred.pop(k)\n+            elif output_names is not None:\n+                y_pred = []\n+                for x, output_name in zip(tree.flatten(y_pred), output_names):\n+                    if output_name not in filtered_y_pred_keys:\n+                        y_pred.append(x)\n+        return y_true, y_pred\n+\n     def __call__(self, y_true, y_pred, sample_weight=None):\n         with ops.name_scope(self.name):\n             return self.call(y_true, y_pred, sample_weight)\n \n-    def _flatten_y(self, y):\n-        if isinstance(y, dict) and self.output_names:\n-            result = []\n-            for name in self.output_names:\n-                if name in y:\n-                    result.append(y[name])\n-            return result\n-        return tree.flatten(y)\n-\n     def call(self, y_true, y_pred, sample_weight=None):\n         if not self.built:\n             self.build(y_true, y_pred)\n \n-        y_true = self._flatten_y(y_true)\n-        y_pred = self._flatten_y(y_pred)\n+        # Filter unused inputs.\n+        y_true, y_pred = self._filter_unused_inputs(\n+            y_true,\n+            y_pred,\n+            self.filtered_y_true_keys,\n+            self.filtered_y_pred_keys,\n+            self.inferred_output_names,\n+        )\n \n+        # Flatten the inputs.\n+        y_true = tree.flatten(y_true)\n+        y_pred = tree.flatten(y_pred)\n         if sample_weight is not None:\n-            sample_weight = self._flatten_y(sample_weight)\n+            sample_weight = tree.flatten(sample_weight)\n             # For multi-outputs, repeat sample weights for n outputs.\n             if len(sample_weight) < len(y_true):\n                 sample_weight = [sample_weight[0] for _ in range(len(y_true))]\n         else:\n             sample_weight = [None for _ in y_true]\n \n+        # Iterate all losses in flat form.\n         loss_values = []\n         for loss, y_t, y_p, loss_weight, sample_weight in zip(\n             self.flat_losses,\n@@ -641,9 +606,11 @@ class CompileLoss(losses_module.Loss):\n             sample_weight,\n         ):\n             if loss:\n-                value = loss_weight * ops.cast(\n-                    loss(y_t, y_p, sample_weight), dtype=backend.floatx()\n+                value = ops.cast(\n+                    loss(y_t, y_p, sample_weight), dtype=self.dtype\n                 )\n+                if loss_weight is not None:\n+                    value = ops.multiply(value, loss_weight)\n                 loss_values.append(value)\n         if loss_values:\n             total_loss = sum(loss_values)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
