{"custom_id": "scrapy#9119798a5ce10aaf015d1af647c5b85e312d2386", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 12/12 | Churn Δ: 15 | Churn Cumulative: 294 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -123,6 +123,14 @@ class TestSpider(Spider):\n         \"\"\"\n         return {'url': response.url}\n \n+    def scrapes_multiple_missing_fields(self, response):\n+        \"\"\" returns item with no name\n+        @url http://scrapy.org\n+        @returns items 1 1\n+        @scrapes name url\n+        \"\"\"\n+        return {}\n+\n     def parse_no_url(self, response):\n         \"\"\" method with no url\n         @returns items 1 1\n@@ -256,6 +264,13 @@ class ContractsManagerTest(unittest.TestCase):\n         request.callback(response)\n         self.should_fail()\n \n+        # scrapes_multiple_missing_fields\n+        request = self.conman.from_method(spider.scrapes_multiple_missing_fields, self.results)\n+        request.callback(response)\n+        self.should_fail()\n+        message = 'ContractFail: Missing fields: name, url'\n+        assert message in self.results.failures[-1][-1]\n+\n     def test_custom_contracts(self):\n         self.conman.from_spider(CustomContractSuccessSpider(), self.results)\n         self.should_succeed()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3040f77468d7dd997afd2f8a35590cafaff454af", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 241 | Contributors (this commit): 9 | Commits (past 90d): 1 | Contributors (cumulative): 9 | DMM Complexity: 0.0\n\nDIFF:\n@@ -8,6 +8,7 @@ from os.path import join, dirname, abspath, isabs, exists\n from scrapy.utils.conf import closest_scrapy_cfg, get_config, init_env\n from scrapy.settings import Settings\n from scrapy.exceptions import NotConfigured\n+from scrapy.exceptions import ScrapyDeprecationWarning\n \n ENVVAR = 'SCRAPY_SETTINGS_MODULE'\n DATADIR_CFG_SECTION = 'datadir'\n@@ -70,6 +71,9 @@ def get_project_settings():\n     # XXX: remove this hack\n     pickled_settings = os.environ.get(\"SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE\")\n     if pickled_settings:\n+        warnings.warn(\"Use of environment variable \"\n+                      \"'SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE' \"\n+                      \"is deprecated.\", ScrapyDeprecationWarning)\n         settings.setdict(pickle.loads(pickled_settings), priority='project')\n \n     # XXX: deprecate and remove this functionality\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#da385b56b10aaf3393cc10d13421d45c5a0edbdb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 19 | Churn Cumulative: 155 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,6 @@\n # -*- coding: utf-8 -*-\n \n+import OpenSSL\n import OpenSSL._util as pyOpenSSLutil\n \n from scrapy.utils.python import to_native_str\n@@ -48,3 +49,10 @@ def get_temp_key_info(ssl_object):\n         key_info.append(ffi_buf_to_string(pyOpenSSLutil.lib.OBJ_nid2sn(key_type)))\n     key_info.append('%s bits' % pyOpenSSLutil.lib.EVP_PKEY_bits(temp_key))\n     return ', '.join(key_info)\n+\n+\n+def get_openssl_version():\n+    system_openssl = OpenSSL.SSL.SSLeay_version(\n+        OpenSSL.SSL.SSLEAY_VERSION\n+    ).decode('ascii', errors='replace')\n+    return '{} ({})'.format(OpenSSL.version.__version__, system_openssl)\n\n@@ -4,12 +4,12 @@ import sys\n import cryptography\n import cssselect\n import lxml.etree\n-import OpenSSL\n import parsel\n import twisted\n import w3lib\n \n import scrapy\n+from scrapy.utils.ssl import get_openssl_version\n \n \n def scrapy_components_versions():\n@@ -25,14 +25,7 @@ def scrapy_components_versions():\n         (\"w3lib\", w3lib.__version__),\n         (\"Twisted\", twisted.version.short()),\n         (\"Python\", sys.version.replace(\"\\n\", \"- \")),\n-        (\"pyOpenSSL\", _get_openssl_version()),\n+        (\"pyOpenSSL\", get_openssl_version()),\n         (\"cryptography\", cryptography.__version__),\n         (\"Platform\",  platform.platform()),\n     ]\n-\n-\n-def _get_openssl_version():\n-    openssl = OpenSSL.SSL.SSLeay_version(\n-        OpenSSL.SSL.SSLEAY_VERSION\n-    ).decode('ascii', errors='replace')\n-    return '{} ({})'.format(OpenSSL.version.__version__, openssl)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fa9a9033f0a73e5c3c1b3bd54408e3bab7ea1b91", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 170 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,8 @@\n import logging\n+\n from OpenSSL import SSL\n+from twisted.internet.ssl import AcceptableCiphers\n+from twisted.internet._sslverify import ClientTLSOptions, verifyHostname, VerificationError\n \n from scrapy import twisted_version\n from scrapy.utils.ssl import x509name_to_string, get_temp_key_info\n@@ -22,8 +25,7 @@ openssl_methods = {\n }\n \n \n-if twisted_version >= (14, 0, 0):\n-    # ClientTLSOptions requires a recent-enough version of Twisted.\n+# ClientTLSOptions requires a recent-enough version of Twisted (14.0.0+)\n # Not having ScrapyClientTLSOptions should not matter for older\n # Twisted versions because it is not used in the fallback\n # ScrapyClientContextFactory.\n@@ -38,10 +40,6 @@ if twisted_version >= (14, 0, 0):\n     SSL_CB_HANDSHAKE_START = 0x10\n     SSL_CB_HANDSHAKE_DONE = 0x20\n \n-    from twisted.internet.ssl import AcceptableCiphers\n-    from twisted.internet._sslverify import (ClientTLSOptions,\n-                                             verifyHostname,\n-                                             VerificationError)\n try:\n     # XXX: this import would fail on Debian jessie with system installed\n     # service_identity library, due to lack of cryptography.x509 dependency\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a940a80f5886c631425c4790ac92928926b1cc92", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 184 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -30,16 +30,6 @@ openssl_methods = {\n # Twisted versions because it is not used in the fallback\n # ScrapyClientContextFactory.\n \n-# taken from twisted/twisted/internet/_sslverify.py\n-\n-try:\n-    # XXX: this try-except is not needed in Twisted 17.0.0+ because\n-    # it requires pyOpenSSL 0.16+.\n-    from OpenSSL.SSL import SSL_CB_HANDSHAKE_DONE, SSL_CB_HANDSHAKE_START\n-except ImportError:\n-    SSL_CB_HANDSHAKE_START = 0x10\n-    SSL_CB_HANDSHAKE_DONE = 0x20\n-\n try:\n     # XXX: this import would fail on Debian jessie with system installed\n     # service_identity library, due to lack of cryptography.x509 dependency\n@@ -73,9 +63,9 @@ class ScrapyClientTLSOptions(ClientTLSOptions):\n         self.verbose_logging = verbose_logging\n \n     def _identityVerifyingInfoCallback(self, connection, where, ret):\n-        if where & SSL_CB_HANDSHAKE_START:\n+        if where & SSL.SSL_CB_HANDSHAKE_START:\n             set_tlsext_host_name(connection, self._hostnameBytes)\n-        elif where & SSL_CB_HANDSHAKE_DONE:\n+        elif where & SSL.SSL_CB_HANDSHAKE_DONE:\n             if self.verbose_logging:\n                 if hasattr(connection, 'get_cipher_name'):  # requires pyOPenSSL 0.15\n                     if hasattr(connection, 'get_protocol_version_name'):  # requires pyOPenSSL 16.0.0\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3164543ed1659a11cea1f6a04d8545f9cfebe367", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 34 | Files Changed: 2 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 38 | Churn Cumulative: 492 | Contributors (this commit): 8 | Commits (past 90d): 11 | Contributors (cumulative): 11 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,17 +1,8 @@\n from OpenSSL import SSL\n-from twisted.internet.ssl import ClientContextFactory\n-\n-from scrapy import twisted_version\n-\n-if twisted_version >= (14, 0, 0):\n-\n-    from zope.interface.declarations import implementer\n-\n-    from twisted.internet.ssl import (optionsForClientTLS,\n-                                      CertificateOptions,\n-                                      platformTrust)\n+from twisted.internet.ssl import optionsForClientTLS, CertificateOptions, platformTrust\n from twisted.web.client import BrowserLikePolicyForHTTPS\n from twisted.web.iweb import IPolicyForHTTPS\n+from zope.interface.declarations import implementer\n \n from scrapy.core.downloader.tls import ScrapyClientTLSOptions, DEFAULT_CIPHERS\n \n@@ -92,21 +83,3 @@ if twisted_version >= (14, 0, 0):\n                                    extraCertificateOptions={\n                                         'method': self._ssl_method,\n                                    })\n-\n-else:\n-\n-    class ScrapyClientContextFactory(ClientContextFactory):\n-        \"A SSL context factory which is more permissive against SSL bugs.\"\n-        # see https://github.com/scrapy/scrapy/issues/82\n-        # and https://github.com/scrapy/scrapy/issues/26\n-        # and https://github.com/scrapy/scrapy/issues/981\n-\n-        def __init__(self, method=SSL.SSLv23_METHOD):\n-            self.method = method\n-\n-        def getContext(self, hostname=None, port=None):\n-            ctx = ClientContextFactory.getContext(self)\n-            # Enable all workarounds to SSL bugs as documented by\n-            # https://www.openssl.org/docs/manmaster/man3/SSL_CTX_set_options.html\n-            ctx.set_options(SSL.OP_ALL)\n-            return ctx\n\n@@ -10,12 +10,14 @@ from scrapy.utils.ssl import x509name_to_string, get_temp_key_info\n \n logger = logging.getLogger(__name__)\n \n+\n METHOD_SSLv3 = 'SSLv3'\n METHOD_TLS = 'TLS'\n METHOD_TLSv10 = 'TLSv1.0'\n METHOD_TLSv11 = 'TLSv1.1'\n METHOD_TLSv12 = 'TLSv1.2'\n \n+\n openssl_methods = {\n     METHOD_TLS:    SSL.SSLv23_METHOD,                   # protocol negotiation (recommended)\n     METHOD_SSLv3:  SSL.SSLv3_METHOD,                    # SSL 3 (NOT recommended)\n@@ -25,11 +27,6 @@ openssl_methods = {\n }\n \n \n-# ClientTLSOptions requires a recent-enough version of Twisted (14.0.0+)\n-# Not having ScrapyClientTLSOptions should not matter for older\n-# Twisted versions because it is not used in the fallback\n-# ScrapyClientContextFactory.\n-\n try:\n     # XXX: this import would fail on Debian jessie with system installed\n     # service_identity library, due to lack of cryptography.x509 dependency\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b404941e0de86ddf2e108c42a4847f0341d53d14", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 205 | Contributors (this commit): 5 | Commits (past 90d): 7 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -1,8 +1,9 @@\n import logging\n \n from OpenSSL import SSL\n-from twisted.internet.ssl import AcceptableCiphers\n+from service_identity.exceptions import CertificateError\n from twisted.internet._sslverify import ClientTLSOptions, verifyHostname, VerificationError\n+from twisted.internet.ssl import AcceptableCiphers\n \n from scrapy import twisted_version\n from scrapy.utils.ssl import x509name_to_string, get_temp_key_info\n@@ -27,15 +28,6 @@ openssl_methods = {\n }\n \n \n-try:\n-    # XXX: this import would fail on Debian jessie with system installed\n-    # service_identity library, due to lack of cryptography.x509 dependency\n-    # See https://github.com/pyca/service_identity/issues/21\n-    from service_identity.exceptions import CertificateError\n-    verification_errors = (CertificateError, VerificationError)\n-except ImportError:\n-    verification_errors = VerificationError\n-\n if twisted_version < (17, 0, 0):\n     from twisted.internet._sslverify import _maybeSetHostNameIndication\n     set_tlsext_host_name = _maybeSetHostNameIndication\n@@ -87,7 +79,7 @@ class ScrapyClientTLSOptions(ClientTLSOptions):\n \n             try:\n                 verifyHostname(connection, self._hostnameASCII)\n-            except verification_errors as e:\n+            except (CertificateError, VerificationError) as e:\n                 logger.warning(\n                     'Remote certificate is not valid for hostname \"{}\"; {}'.format(\n                         self._hostnameASCII, e))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d92f1b18580940c27f303f1dfcf47bb66e508ce4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 208 | Contributors (this commit): 5 | Commits (past 90d): 8 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -29,8 +29,7 @@ openssl_methods = {\n \n \n if twisted_version < (17, 0, 0):\n-    from twisted.internet._sslverify import _maybeSetHostNameIndication\n-    set_tlsext_host_name = _maybeSetHostNameIndication\n+    from twisted.internet._sslverify import _maybeSetHostNameIndication as set_tlsext_host_name\n else:\n     def set_tlsext_host_name(connection, hostNameBytes):\n         connection.set_tlsext_host_name(hostNameBytes)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e17c9a48fdd41e838c235be75038fab1ab9bb8e2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 14 | Churn Cumulative: 1625 | Contributors (this commit): 21 | Commits (past 90d): 5 | Contributors (cumulative): 21 | DMM Complexity: 0.0\n\nDIFF:\n@@ -27,7 +27,7 @@ from scrapy.core.downloader.webclient import _parse\n from scrapy.core.downloader.tls import openssl_methods\n from scrapy.utils.misc import load_object, create_instance\n from scrapy.utils.python import to_bytes, to_unicode\n-from scrapy import twisted_version\n+\n \n logger = logging.getLogger(__name__)\n \n@@ -211,18 +211,11 @@ class TunnelingAgent(Agent):\n         self._proxyConf = proxyConf\n         self._contextFactory = contextFactory\n \n-    if twisted_version >= (15, 0, 0):\n     def _getEndpoint(self, uri):\n         return TunnelingTCP4ClientEndpoint(\n             self._reactor, uri.host, uri.port, self._proxyConf,\n             self._contextFactory, self._endpointFactory._connectTimeout,\n             self._endpointFactory._bindAddress)\n-    else:\n-        def _getEndpoint(self, scheme, host, port):\n-            return TunnelingTCP4ClientEndpoint(\n-                self._reactor, host, port, self._proxyConf,\n-                self._contextFactory, self._connectTimeout,\n-                self._bindAddress)\n \n     def _requestWithEndpoint(self, key, endpoint, method, parsedURI,\n                              headers, bodyProducer, requestPath):\n@@ -250,12 +243,7 @@ class ScrapyProxyAgent(Agent):\n         \"\"\"\n         # Cache *all* connections under the same key, since we are only\n         # connecting to a single destination, the proxy:\n-        if twisted_version >= (15, 0, 0):\n         proxyEndpoint = self._getEndpoint(self._proxyURI)\n-        else:\n-            proxyEndpoint = self._getEndpoint(self._proxyURI.scheme,\n-                                              self._proxyURI.host,\n-                                              self._proxyURI.port)\n         key = (\"http-proxy\", self._proxyURI.host, self._proxyURI.port)\n         return self._requestWithEndpoint(key, proxyEndpoint, method,\n                                          URI.fromBytes(uri), headers,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d3737d869b1b49497f53376283b02ac604543e2c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 10 | Churn Cumulative: 1635 | Contributors (this commit): 21 | Commits (past 90d): 6 | Contributors (cumulative): 21 | DMM Complexity: 1.0\n\nDIFF:\n@@ -141,14 +141,8 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n         self._protocol.dataReceived = self._protocolDataReceived\n         respm = TunnelingTCP4ClientEndpoint._responseMatcher.match(self._connectBuffer)\n         if respm and int(respm.group('status')) == 200:\n-            try:\n-                # this sets proper Server Name Indication extension\n-                # but is only available for Twisted>=14.0\n-                sslOptions = self._contextFactory.creatorForNetloc(\n-                    self._tunneledHost, self._tunneledPort)\n-            except AttributeError:\n-                # fall back to non-SNI SSL context factory\n-                sslOptions = self._contextFactory\n+            # set proper Server Name Indication extension\n+            sslOptions = self._contextFactory.creatorForNetloc(self._tunneledHost, self._tunneledPort)\n             self._protocol.transport.startTLS(sslOptions,\n                                               self._protocolFactory)\n             self._tunnelReadyDeferred.callback(self._protocol)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d5dcc5eaef80ef383dff90f19349c0e06f1836a6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 1643 | Contributors (this commit): 21 | Commits (past 90d): 7 | Contributors (cumulative): 21 | DMM Complexity: None\n\nDIFF:\n@@ -13,12 +13,8 @@ from twisted.web.http_headers import Headers as TxHeaders\n from twisted.web.iweb import IBodyProducer, UNKNOWN_LENGTH\n from twisted.internet.error import TimeoutError\n from twisted.web.http import _DataLoss, PotentialDataLoss\n-from twisted.web.client import Agent, ProxyAgent, ResponseDone, \\\n-    HTTPConnectionPool, ResponseFailed\n-try:\n-    from twisted.web.client import URI\n-except ImportError:\n-    from twisted.web.client import _URI as URI\n+from twisted.web.client import (Agent, ProxyAgent, ResponseDone,\n+                                HTTPConnectionPool, ResponseFailed, URI)\n from twisted.internet.endpoints import TCP4ClientEndpoint\n \n from scrapy.http import Headers\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#26fb28b20f661e6d7cfd02c904f4b1dc2a79e1dc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 86 | Lines Deleted: 54 | Files Changed: 1 | Hunks: 25 | Methods Changed: 16 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 140 | Churn Cumulative: 1783 | Contributors (this commit): 21 | Commits (past 90d): 8 | Contributors (cumulative): 21 | DMM Complexity: 0.9714285714285714\n\nDIFF:\n@@ -13,8 +13,7 @@ from twisted.web.http_headers import Headers as TxHeaders\n from twisted.web.iweb import IBodyProducer, UNKNOWN_LENGTH\n from twisted.internet.error import TimeoutError\n from twisted.web.http import _DataLoss, PotentialDataLoss\n-from twisted.web.client import (Agent, ProxyAgent, ResponseDone,\n-                                HTTPConnectionPool, ResponseFailed, URI)\n+from twisted.web.client import Agent, ResponseDone, HTTPConnectionPool, ResponseFailed, URI\n from twisted.internet.endpoints import TCP4ClientEndpoint\n \n from scrapy.http import Headers\n@@ -40,11 +39,19 @@ class HTTP11DownloadHandler(object):\n         self._contextFactoryClass = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])\n         # try method-aware context factory\n         try:\n-            self._contextFactory = create_instance(self._contextFactoryClass, settings=settings, crawler=None,\n-                                                   method=self._sslMethod)\n+            self._contextFactory = create_instance(\n+                self._contextFactoryClass,\n+                settings=settings,\n+                crawler=None,\n+                method=self._sslMethod,\n+            )\n         except TypeError:\n             # use context factory defaults\n-            self._contextFactory = create_instance(self._contextFactoryClass, settings=settings, crawler=None)\n+            self._contextFactory = create_instance(\n+                self._contextFactoryClass,\n+                settings=settings,\n+                crawler=None,\n+            )\n             msg = \"\"\"\n  '%s' does not accept `method` argument (type OpenSSL.SSL method,\\\n  e.g. OpenSSL.SSL.SSLv23_METHOD) and/or `tls_verbose_logging` argument.\\\n@@ -58,10 +65,13 @@ class HTTP11DownloadHandler(object):\n \n     def download_request(self, request, spider):\n         \"\"\"Return a deferred for the HTTP download\"\"\"\n-        agent = ScrapyAgent(contextFactory=self._contextFactory, pool=self._pool,\n+        agent = ScrapyAgent(\n+            contextFactory=self._contextFactory,\n+            pool=self._pool,\n             maxsize=getattr(spider, 'download_maxsize', self._default_maxsize),\n             warnsize=getattr(spider, 'download_warnsize', self._default_warnsize),\n-            fail_on_dataloss=self._fail_on_dataloss)\n+            fail_on_dataloss=self._fail_on_dataloss,\n+        )\n         return agent.download_request(request)\n \n     def close(self):\n@@ -100,11 +110,9 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n \n     _responseMatcher = re.compile(br'HTTP/1\\.. (?P<status>\\d{3})(?P<reason>.{,32})')\n \n-    def __init__(self, reactor, host, port, proxyConf, contextFactory,\n-                 timeout=30, bindAddress=None):\n+    def __init__(self, reactor, host, port, proxyConf, contextFactory, timeout=30, bindAddress=None):\n         proxyHost, proxyPort, self._proxyAuthHeader = proxyConf\n-        super(TunnelingTCP4ClientEndpoint, self).__init__(reactor, proxyHost,\n-            proxyPort, timeout, bindAddress)\n+        super(TunnelingTCP4ClientEndpoint, self).__init__(reactor, proxyHost, proxyPort, timeout, bindAddress)\n         self._tunnelReadyDeferred = defer.Deferred()\n         self._tunneledHost = host\n         self._tunneledPort = port\n@@ -113,8 +121,7 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n \n     def requestTunnel(self, protocol):\n         \"\"\"Asks the proxy to open a tunnel.\"\"\"\n-        tunnelReq = tunnel_request_data(self._tunneledHost, self._tunneledPort,\n-                                        self._proxyAuthHeader)\n+        tunnelReq = tunnel_request_data(self._tunneledHost, self._tunneledPort, self._proxyAuthHeader)\n         protocol.transport.write(tunnelReq)\n         self._protocolDataReceived = protocol.dataReceived\n         protocol.dataReceived = self.processProxyResponse\n@@ -139,8 +146,7 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n         if respm and int(respm.group('status')) == 200:\n             # set proper Server Name Indication extension\n             sslOptions = self._contextFactory.creatorForNetloc(self._tunneledHost, self._tunneledPort)\n-            self._protocol.transport.startTLS(sslOptions,\n-                                              self._protocolFactory)\n+            self._protocol.transport.startTLS(sslOptions, self._protocolFactory)\n             self._tunnelReadyDeferred.callback(self._protocol)\n         else:\n             if respm:\n@@ -158,8 +164,7 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n \n     def connect(self, protocolFactory):\n         self._protocolFactory = protocolFactory\n-        connectDeferred = super(TunnelingTCP4ClientEndpoint,\n-                                self).connect(protocolFactory)\n+        connectDeferred = super(TunnelingTCP4ClientEndpoint, self).connect(protocolFactory)\n         connectDeferred.addCallback(self.requestTunnel)\n         connectDeferred.addErrback(self.connectFailed)\n         return self._tunnelReadyDeferred\n@@ -196,35 +201,46 @@ class TunnelingAgent(Agent):\n \n     def __init__(self, reactor, proxyConf, contextFactory=None,\n                  connectTimeout=None, bindAddress=None, pool=None):\n-        super(TunnelingAgent, self).__init__(reactor, contextFactory,\n-            connectTimeout, bindAddress, pool)\n+        super(TunnelingAgent, self).__init__(reactor, contextFactory, connectTimeout, bindAddress, pool)\n         self._proxyConf = proxyConf\n         self._contextFactory = contextFactory\n \n     def _getEndpoint(self, uri):\n         return TunnelingTCP4ClientEndpoint(\n-            self._reactor, uri.host, uri.port, self._proxyConf,\n-            self._contextFactory, self._endpointFactory._connectTimeout,\n-            self._endpointFactory._bindAddress)\n+            reactor=self._reactor,\n+            host=uri.host,\n+            port=uri.port,\n+            proxyConf=self._proxyConf,\n+            contextFactory=self._contextFactory,\n+            timeout=self._endpointFactory._connectTimeout,\n+            bindAddress=self._endpointFactory._bindAddress,\n+        )\n \n-    def _requestWithEndpoint(self, key, endpoint, method, parsedURI,\n-                             headers, bodyProducer, requestPath):\n+    def _requestWithEndpoint(self, key, endpoint, method, parsedURI, headers, bodyProducer, requestPath):\n         # proxy host and port are required for HTTP pool `key`\n         # otherwise, same remote host connection request could reuse\n         # a cached tunneled connection to a different proxy\n         key = key + self._proxyConf\n-        return super(TunnelingAgent, self)._requestWithEndpoint(key, endpoint, method, parsedURI,\n-            headers, bodyProducer, requestPath)\n+        return super(TunnelingAgent, self)._requestWithEndpoint(\n+            key=key,\n+            endpoint=endpoint,\n+            method=method,\n+            parsedURI=parsedURI,\n+            headers=headers,\n+            bodyProducer=bodyProducer,\n+            requestPath=requestPath,\n+        )\n \n \n class ScrapyProxyAgent(Agent):\n \n-    def __init__(self, reactor, proxyURI,\n-                 connectTimeout=None, bindAddress=None, pool=None):\n-        super(ScrapyProxyAgent, self).__init__(reactor,\n+    def __init__(self, reactor, proxyURI, connectTimeout=None, bindAddress=None, pool=None):\n+        super(ScrapyProxyAgent, self).__init__(\n+            reactor=reactor,\n             connectTimeout=connectTimeout,\n             bindAddress=bindAddress,\n-                                               pool=pool)\n+            pool=pool,\n+        )\n         self._proxyURI = URI.fromBytes(proxyURI)\n \n     def request(self, method, uri, headers=None, bodyProducer=None):\n@@ -233,11 +249,15 @@ class ScrapyProxyAgent(Agent):\n         \"\"\"\n         # Cache *all* connections under the same key, since we are only\n         # connecting to a single destination, the proxy:\n-        proxyEndpoint = self._getEndpoint(self._proxyURI)\n-        key = (\"http-proxy\", self._proxyURI.host, self._proxyURI.port)\n-        return self._requestWithEndpoint(key, proxyEndpoint, method,\n-                                         URI.fromBytes(uri), headers,\n-                                         bodyProducer, uri)\n+        return self._requestWithEndpoint(\n+            key=(\"http-proxy\", self._proxyURI.host, self._proxyURI.port),\n+            endpoint=self._getEndpoint(self._proxyURI),\n+            method=method,\n+            parsedURI=URI.fromBytes(uri),\n+            headers=headers,\n+            bodyProducer=bodyProducer,\n+            requestPath=uri,\n+        )\n \n \n class ScrapyAgent(object):\n@@ -266,17 +286,32 @@ class ScrapyAgent(object):\n             proxyHost = to_unicode(proxyHost)\n             omitConnectTunnel = b'noconnect' in proxyParams\n             if scheme == b'https' and not omitConnectTunnel:\n-                proxyConf = (proxyHost, proxyPort,\n-                             request.headers.get(b'Proxy-Authorization', None))\n-                return self._TunnelingAgent(reactor, proxyConf,\n-                    contextFactory=self._contextFactory, connectTimeout=timeout,\n-                    bindAddress=bindaddress, pool=self._pool)\n+                proxyAuth = request.headers.get(b'Proxy-Authorization', None)\n+                proxyConf = (proxyHost, proxyPort, proxyAuth)\n+                return self._TunnelingAgent(\n+                    reactor=reactor,\n+                    proxyConf=proxyConf,\n+                    contextFactory=self._contextFactory,\n+                    connectTimeout=timeout,\n+                    bindAddress=bindaddress,\n+                    pool=self._pool,\n+                )\n             else:\n-                return self._ProxyAgent(reactor, proxyURI=to_bytes(proxy, encoding='ascii'),\n-                    connectTimeout=timeout, bindAddress=bindaddress, pool=self._pool)\n+                return self._ProxyAgent(\n+                    reactor=reactor,\n+                    proxyURI=to_bytes(proxy, encoding='ascii'),\n+                    connectTimeout=timeout,\n+                    bindAddress=bindaddress,\n+                    pool=self._pool,\n+                )\n \n-        return self._Agent(reactor, contextFactory=self._contextFactory,\n-            connectTimeout=timeout, bindAddress=bindaddress, pool=self._pool)\n+        return self._Agent(\n+            reactor=reactor,\n+            contextFactory=self._contextFactory,\n+            connectTimeout=timeout,\n+            bindAddress=bindaddress,\n+            pool=self._pool,\n+        )\n \n     def download_request(self, request):\n         timeout = request.meta.get('download_timeout') or self._connectTimeout\n@@ -307,8 +342,7 @@ class ScrapyAgent(object):\n         else:\n             bodyproducer = None\n         start_time = time()\n-        d = agent.request(\n-            method, to_bytes(url, encoding='ascii'), headers, bodyproducer)\n+        d = agent.request(method, to_bytes(url, encoding='ascii'), headers, bodyproducer)\n         # set download latency\n         d.addCallback(self._cb_latency, request, start_time)\n         # response body is ready to be consumed\n@@ -363,8 +397,9 @@ class ScrapyAgent(object):\n             txresponse._transport._producer.abortConnection()\n \n         d = defer.Deferred(_cancel)\n-        txresponse.deliverBody(_ResponseReader(\n-            d, txresponse, request, maxsize, warnsize, fail_on_dataloss))\n+        txresponse.deliverBody(\n+            _ResponseReader(d, txresponse, request, maxsize, warnsize, fail_on_dataloss)\n+        )\n \n         # save response for timeouts\n         self._txresponse = txresponse\n@@ -399,8 +434,7 @@ class _RequestBodyProducer(object):\n \n class _ResponseReader(protocol.Protocol):\n \n-    def __init__(self, finished, txresponse, request, maxsize, warnsize,\n-                 fail_on_dataloss):\n+    def __init__(self, finished, txresponse, request, maxsize, warnsize, fail_on_dataloss):\n         self._finished = finished\n         self._txresponse = txresponse\n         self._request = request\n@@ -413,8 +447,7 @@ class _ResponseReader(protocol.Protocol):\n         self._bytes_received = 0\n \n     def dataReceived(self, bodyBytes):\n-        # This maybe called several times after cancel was called with buffered\n-        # data.\n+        # This maybe called several times after cancel was called with buffered data.\n         if self._finished.called:\n             return\n \n@@ -427,8 +460,7 @@ class _ResponseReader(protocol.Protocol):\n                          {'bytes': self._bytes_received,\n                           'maxsize': self._maxsize,\n                           'request': self._request})\n-            # Clear buffer earlier to avoid keeping data in memory for a long\n-            # time.\n+            # Clear buffer earlier to avoid keeping data in memory for a long time.\n             self._bodybuf.truncate(0)\n             self._finished.cancel()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3384db92b4fb2bce66d01ddf5365478a695caad6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 14 | Churn Cumulative: 874 | Contributors (this commit): 48 | Commits (past 90d): 23 | Contributors (cumulative): 54 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,5 @@\n from OpenSSL import SSL\n-from twisted.internet.ssl import optionsForClientTLS, CertificateOptions, platformTrust\n+from twisted.internet.ssl import optionsForClientTLS, CertificateOptions, platformTrust, AcceptableCiphers\n from twisted.web.client import BrowserLikePolicyForHTTPS\n from twisted.web.iweb import IPolicyForHTTPS\n from zope.interface.declarations import implementer\n@@ -19,15 +19,20 @@ class ScrapyClientContextFactory(BrowserLikePolicyForHTTPS):\n      understand the SSLv3, TLSv1, TLSv1.1 and TLSv1.2 protocols.'\n     \"\"\"\n \n-    def __init__(self, method=SSL.SSLv23_METHOD, tls_verbose_logging=False, *args, **kwargs):\n+    def __init__(self, method=SSL.SSLv23_METHOD, tls_verbose_logging=False, tls_ciphers=None, *args, **kwargs):\n         super(ScrapyClientContextFactory, self).__init__(*args, **kwargs)\n         self._ssl_method = method\n         self.tls_verbose_logging = tls_verbose_logging\n+        if tls_ciphers:\n+            self.tls_ciphers = AcceptableCiphers.fromOpenSSLCipherString(tls_ciphers)\n+        else:\n+            self.tls_ciphers = DEFAULT_CIPHERS\n \n     @classmethod\n     def from_settings(cls, settings, method=SSL.SSLv23_METHOD, *args, **kwargs):\n         tls_verbose_logging = settings.getbool('DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING')\n-        return cls(method=method, tls_verbose_logging=tls_verbose_logging, *args, **kwargs)\n+        tls_ciphers = settings['DOWNLOADER_CLIENT_TLS_CIPHERS']\n+        return cls(method=method, tls_verbose_logging=tls_verbose_logging, tls_ciphers=tls_ciphers, *args, **kwargs)\n \n     def getCertificateOptions(self):\n         # setting verify=True will require you to provide CAs\n@@ -45,7 +50,7 @@ class ScrapyClientContextFactory(BrowserLikePolicyForHTTPS):\n                     method=getattr(self, 'method',\n                                    getattr(self, '_ssl_method', None)),\n                     fixBrokenPeers=True,\n-                    acceptableCiphers=DEFAULT_CIPHERS)\n+                    acceptableCiphers=self.tls_ciphers)\n \n     # kept for old-style HTTP/1.0 downloader context twisted calls,\n     # e.g. connectSSL()\n\n@@ -85,6 +85,7 @@ DOWNLOADER = 'scrapy.core.downloader.Downloader'\n \n DOWNLOADER_HTTPCLIENTFACTORY = 'scrapy.core.downloader.webclient.ScrapyHTTPClientFactory'\n DOWNLOADER_CLIENTCONTEXTFACTORY = 'scrapy.core.downloader.contextfactory.ScrapyClientContextFactory'\n+DOWNLOADER_CLIENT_TLS_CIPHERS = 'DEFAULT'\n DOWNLOADER_CLIENT_TLS_METHOD = 'TLS' # Use highest TLS/SSL protocol version supported by the platform,\n                                      # also allowing negotiation\n DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING = False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9a8edf2bf1172e8eec282a70ad46a9dacff76d62", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 101 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 9 | Methods Changed: 10 | Complexity Δ (Sum/Max): 13/7 | Churn Δ: 103 | Churn Cumulative: 1744 | Contributors (this commit): 20 | Commits (past 90d): 11 | Contributors (cumulative): 33 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,6 +3,8 @@ import sys, time, random, os, json\n from six.moves.urllib.parse import urlencode\n from subprocess import Popen, PIPE\n \n+from OpenSSL import SSL\n+\n from twisted.web.server import Site, NOT_DONE_YET\n from twisted.web.resource import Resource\n from twisted.web.static import File\n@@ -222,6 +224,14 @@ def ssl_context_factory(keyfile='keys/localhost.key', certfile='keys/localhost.c\n          )\n \n \n+def broken_ssl_context_factory(keyfile='keys/localhost.key', certfile='keys/localhost.crt', cipher_string='DEFAULT'):\n+    factory = ssl_context_factory(keyfile, certfile)\n+    ctx = factory.getContext()\n+    ctx.set_options(SSL.OP_CIPHER_SERVER_PREFERENCE | SSL.OP_NO_TLSv1_2)\n+    ctx.set_cipher_list(cipher_string)\n+    return factory\n+\n+\n if __name__ == \"__main__\":\n     root = Root()\n     factory = Site(root)\n\n@@ -39,7 +39,7 @@ from scrapy.utils.test import get_crawler, skip_if_no_boto\n from scrapy.utils.python import to_bytes\n from scrapy.exceptions import NotConfigured\n \n-from tests.mockserver import MockServer, ssl_context_factory, Echo\n+from tests.mockserver import MockServer, ssl_context_factory, Echo, broken_ssl_context_factory\n from tests.spiders import SingleRequestSpider\n \n \n@@ -553,6 +553,47 @@ class Https11InvalidDNSPattern(Https11TestCase):\n         super(Https11InvalidDNSPattern, self).setUp()\n \n \n+class Https11BadCiphers(unittest.TestCase):\n+    scheme = 'https'\n+    download_handler_cls = HTTP11DownloadHandler\n+\n+    keyfile = 'keys/localhost.key'\n+    certfile = 'keys/localhost.crt'\n+\n+    def setUp(self):\n+        self.tmpname = self.mktemp()\n+        os.mkdir(self.tmpname)\n+        FilePath(self.tmpname).child(\"file\").setContent(b\"0123456789\")\n+        r = static.File(self.tmpname)\n+        self.site = server.Site(r, timeout=None)\n+        self.wrapper = WrappingFactory(self.site)\n+        self.host = 'localhost'\n+        self.port = reactor.listenSSL(\n+            0, self.wrapper, broken_ssl_context_factory(self.keyfile, self.certfile, cipher_string='CAMELLIA256-SHA'),\n+            interface=self.host)\n+        self.portno = self.port.getHost().port\n+        self.download_handler = self.download_handler_cls(\n+            Settings({'DOWNLOADER_CLIENT_TLS_CIPHERS': 'CAMELLIA256-SHA'}))\n+        self.download_request = self.download_handler.download_request\n+\n+    @defer.inlineCallbacks\n+    def tearDown(self):\n+        yield self.port.stopListening()\n+        if hasattr(self.download_handler, 'close'):\n+            yield self.download_handler.close()\n+        shutil.rmtree(self.tmpname)\n+\n+    def getURL(self, path):\n+        return \"%s://%s:%d/%s\" % (self.scheme, self.host, self.portno, path)\n+\n+    def test_download(self):\n+        request = Request(self.getURL('file'))\n+        d = self.download_request(request, Spider('foo'))\n+        d.addCallback(lambda r: r.body)\n+        d.addCallback(self.assertEqual, b\"0123456789\")\n+        return d\n+\n+\n class Http11MockServerTestCase(unittest.TestCase):\n     \"\"\"HTTP 1.1 test case with MockServer\"\"\"\n \n\n@@ -8,15 +8,18 @@ import shutil\n \n from twisted.trial import unittest\n from twisted.web import server, static, util, resource\n-from twisted.internet import reactor, defer\n+from twisted.internet import reactor, defer, ssl\n from twisted.test.proto_helpers import StringTransport\n from twisted.python.filepath import FilePath\n from twisted.protocols.policies import WrappingFactory\n from twisted.internet.defer import inlineCallbacks\n \n from scrapy.core.downloader import webclient as client\n+from scrapy.core.downloader.contextfactory import ScrapyClientContextFactory\n from scrapy.http import Request, Headers\n+from scrapy.settings import Settings\n from scrapy.utils.python import to_bytes, to_unicode\n+from tests.mockserver import ssl_context_factory, broken_ssl_context_factory\n \n \n def getPage(url, contextFactory=None, response_transform=None, *args, **kwargs):\n@@ -363,3 +366,48 @@ class WebClientTestCase(unittest.TestCase):\n         self.assertEqual(content_encoding, EncodingResource.out_encoding)\n         self.assertEqual(\n             response.body.decode(content_encoding), to_unicode(original_body))\n+\n+\n+class WebClientSSLTestCase(unittest.TestCase):\n+    context_factory = None\n+\n+    def _listen(self, site):\n+        return reactor.listenSSL(\n+            0, site,\n+            contextFactory=self.context_factory or ssl_context_factory(),\n+            interface=\"127.0.0.1\")\n+\n+    def getURL(self, path):\n+        return \"https://127.0.0.1:%d/%s\" % (self.portno, path)\n+\n+    def setUp(self):\n+        self.tmpname = self.mktemp()\n+        os.mkdir(self.tmpname)\n+        FilePath(self.tmpname).child(\"file\").setContent(b\"0123456789\")\n+        r = static.File(self.tmpname)\n+        r.putChild(b\"payload\", PayloadResource())\n+        self.site = server.Site(r, timeout=None)\n+        self.wrapper = WrappingFactory(self.site)\n+        self.port = self._listen(self.wrapper)\n+        self.portno = self.port.getHost().port\n+\n+    @inlineCallbacks\n+    def tearDown(self):\n+        yield self.port.stopListening()\n+        shutil.rmtree(self.tmpname)\n+\n+    def testPayload(self):\n+        s = \"0123456789\" * 10\n+        return getPage(self.getURL(\"payload\"), body=s).addCallback(\n+            self.assertEqual, to_bytes(s))\n+\n+\n+class WebClientBrokenSSLTestCase(WebClientSSLTestCase):\n+    context_factory = broken_ssl_context_factory(cipher_string='CAMELLIA256-SHA')\n+\n+    def testPayload(self):\n+        s = \"0123456789\" * 10\n+        settings = Settings({'DOWNLOADER_CLIENT_TLS_CIPHERS': 'CAMELLIA256-SHA'})\n+        return getPage(self.getURL(\"payload\"), body=s,\n+                       contextFactory=ScrapyClientContextFactory(settings=settings)).addCallback(self.assertEqual,\n+                                                                                                 to_bytes(s))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#aaa5229e5db4f1c014ae69e0fb9e1a933f4952b0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 21 | Files Changed: 5 | Hunks: 18 | Methods Changed: 7 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 53 | Churn Cumulative: 3648 | Contributors (this commit): 30 | Commits (past 90d): 16 | Contributors (cumulative): 56 | DMM Complexity: 1.0\n\nDIFF:\n@@ -54,7 +54,7 @@ class HTTP11DownloadHandler(object):\n             )\n             msg = \"\"\"\n  '%s' does not accept `method` argument (type OpenSSL.SSL method,\\\n- e.g. OpenSSL.SSL.SSLv23_METHOD) and/or `tls_verbose_logging` argument.\\\n+ e.g. OpenSSL.SSL.SSLv23_METHOD) and/or `tls_verbose_logging` argument and/or `tls_ciphers` argument.\\\n  Please upgrade your context factory class to handle them or ignore them.\"\"\" % (\n                 settings['DOWNLOADER_CLIENTCONTEXTFACTORY'],)\n             warnings.warn(msg)\n\n@@ -6,6 +6,11 @@ import OpenSSL._util as pyOpenSSLutil\n from scrapy.utils.python import to_native_str\n \n \n+# The OpenSSL symbol is present since 1.1.1 but it's not currently supported in any version of pyOpenSSL.\n+# Using the binding directly, as this code does, requires cryptography 2.4.\n+SSL_OP_NO_TLSv1_3 = getattr(pyOpenSSLutil.lib, 'SSL_OP_NO_TLSv1_3', 0)\n+\n+\n def ffi_buf_to_string(buf):\n     return to_native_str(pyOpenSSLutil.ffi.string(buf))\n \n\n@@ -4,7 +4,6 @@ from six.moves.urllib.parse import urlencode\n from subprocess import Popen, PIPE\n \n from OpenSSL import SSL\n-\n from twisted.web.server import Site, NOT_DONE_YET\n from twisted.web.resource import Resource\n from twisted.web.static import File\n@@ -15,8 +14,8 @@ from twisted.web.util import redirectTo\n from twisted.internet import reactor, ssl\n from twisted.internet.task import deferLater\n \n-\n from scrapy.utils.python import to_bytes, to_unicode\n+from scrapy.utils.ssl import SSL_OP_NO_TLSv1_3\n \n \n def getarg(request, name, default=None, type=None):\n@@ -217,18 +216,16 @@ class MockServer():\n         return host + path\n \n \n-def ssl_context_factory(keyfile='keys/localhost.key', certfile='keys/localhost.crt'):\n-    return ssl.DefaultOpenSSLContextFactory(\n+def ssl_context_factory(keyfile='keys/localhost.key', certfile='keys/localhost.crt', cipher_string=None):\n+    factory = ssl.DefaultOpenSSLContextFactory(\n          os.path.join(os.path.dirname(__file__), keyfile),\n          os.path.join(os.path.dirname(__file__), certfile),\n          )\n-\n-\n-def broken_ssl_context_factory(keyfile='keys/localhost.key', certfile='keys/localhost.crt', cipher_string='DEFAULT'):\n-    factory = ssl_context_factory(keyfile, certfile)\n+    if cipher_string:\n         ctx = factory.getContext()\n-    ctx.set_options(SSL.OP_CIPHER_SERVER_PREFERENCE | SSL.OP_NO_TLSv1_2)\n-    ctx.set_cipher_list(cipher_string)\n+        # disabling TLS1.2+ because it unconditionally enables some strong ciphers\n+        ctx.set_options(SSL.OP_CIPHER_SERVER_PREFERENCE | SSL.OP_NO_TLSv1_2 | SSL_OP_NO_TLSv1_3)\n+        ctx.set_cipher_list(to_bytes(cipher_string))\n     return factory\n \n \n\n@@ -39,7 +39,7 @@ from scrapy.utils.test import get_crawler, skip_if_no_boto\n from scrapy.utils.python import to_bytes\n from scrapy.exceptions import NotConfigured\n \n-from tests.mockserver import MockServer, ssl_context_factory, Echo, broken_ssl_context_factory\n+from tests.mockserver import MockServer, ssl_context_factory, Echo\n from tests.spiders import SingleRequestSpider\n \n \n@@ -553,7 +553,7 @@ class Https11InvalidDNSPattern(Https11TestCase):\n         super(Https11InvalidDNSPattern, self).setUp()\n \n \n-class Https11BadCiphers(unittest.TestCase):\n+class Https11CustomCiphers(unittest.TestCase):\n     scheme = 'https'\n     download_handler_cls = HTTP11DownloadHandler\n \n@@ -569,7 +569,7 @@ class Https11BadCiphers(unittest.TestCase):\n         self.wrapper = WrappingFactory(self.site)\n         self.host = 'localhost'\n         self.port = reactor.listenSSL(\n-            0, self.wrapper, broken_ssl_context_factory(self.keyfile, self.certfile, cipher_string='CAMELLIA256-SHA'),\n+            0, self.wrapper, ssl_context_factory(self.keyfile, self.certfile, cipher_string='CAMELLIA256-SHA'),\n             interface=self.host)\n         self.portno = self.port.getHost().port\n         self.download_handler = self.download_handler_cls(\n\n@@ -6,9 +6,10 @@ import os\n import six\n import shutil\n \n+import OpenSSL.SSL\n from twisted.trial import unittest\n from twisted.web import server, static, util, resource\n-from twisted.internet import reactor, defer, ssl\n+from twisted.internet import reactor, defer\n from twisted.test.proto_helpers import StringTransport\n from twisted.python.filepath import FilePath\n from twisted.protocols.policies import WrappingFactory\n@@ -18,8 +19,9 @@ from scrapy.core.downloader import webclient as client\n from scrapy.core.downloader.contextfactory import ScrapyClientContextFactory\n from scrapy.http import Request, Headers\n from scrapy.settings import Settings\n+from scrapy.utils.misc import create_instance\n from scrapy.utils.python import to_bytes, to_unicode\n-from tests.mockserver import ssl_context_factory, broken_ssl_context_factory\n+from tests.mockserver import ssl_context_factory\n \n \n def getPage(url, contextFactory=None, response_transform=None, *args, **kwargs):\n@@ -402,12 +404,19 @@ class WebClientSSLTestCase(unittest.TestCase):\n             self.assertEqual, to_bytes(s))\n \n \n-class WebClientBrokenSSLTestCase(WebClientSSLTestCase):\n-    context_factory = broken_ssl_context_factory(cipher_string='CAMELLIA256-SHA')\n+class WebClientCustomCiphersSSLTestCase(WebClientSSLTestCase):\n+    # we try to use a cipher that is not enabled by default in OpenSSL\n+    custom_ciphers = 'CAMELLIA256-SHA'\n+    context_factory = ssl_context_factory(cipher_string=custom_ciphers)\n \n     def testPayload(self):\n         s = \"0123456789\" * 10\n-        settings = Settings({'DOWNLOADER_CLIENT_TLS_CIPHERS': 'CAMELLIA256-SHA'})\n+        settings = Settings({'DOWNLOADER_CLIENT_TLS_CIPHERS': self.custom_ciphers})\n+        client_context_factory = create_instance(ScrapyClientContextFactory, settings=settings, crawler=None)\n         return getPage(self.getURL(\"payload\"), body=s,\n-                       contextFactory=ScrapyClientContextFactory(settings=settings)).addCallback(self.assertEqual,\n-                                                                                                 to_bytes(s))\n+                       contextFactory=client_context_factory).addCallback(self.assertEqual, to_bytes(s))\n+\n+    def testPayloadDefaultCiphers(self):\n+        s = \"0123456789\" * 10\n+        d = getPage(self.getURL(\"payload\"), body=s, contextFactory=ScrapyClientContextFactory())\n+        return self.assertFailure(d, OpenSSL.SSL.Error)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
