{"custom_id": "scrapy#41a4a163e3716bf0c14160373844bcded0e38abd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 15 | Files Changed: 3 | Hunks: 11 | Methods Changed: 8 | Complexity Δ (Sum/Max): 1/2 | Churn Δ: 38 | Churn Cumulative: 3000 | Contributors (this commit): 31 | Commits (past 90d): 29 | Contributors (cumulative): 33 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,8 @@\n import logging\n from typing import TYPE_CHECKING, Any, List\n \n+from scrapy.exceptions import NotConfigured\n+from scrapy.settings import Settings\n from scrapy.utils.conf import build_component_list\n from scrapy.utils.misc import create_instance, load_object\n \n@@ -17,19 +19,23 @@ class AddonManager:\n         self.crawler: \"Crawler\" = crawler\n         self.addons: List[Any] = []\n \n-    def _add(self, addon: Any) -> None:\n+    def _add(self, addon: Any, settings: Settings) -> None:\n         \"\"\"Store an add-on.\"\"\"\n         if isinstance(addon, (type, str)):\n             addon = load_object(addon)\n         if isinstance(addon, type):\n             addon = create_instance(addon, settings=None, crawler=self.crawler)\n+        try:\n+            addon.update_settings(settings)\n             self.addons.append(addon)\n+        except NotConfigured:\n+            pass\n \n-    def load_settings(self, settings) -> None:\n+    def load_settings(self, settings: Settings) -> None:\n         \"\"\"Load add-ons and configurations from a settings object.\n \n         This will load the add-on for every add-on path in the\n-        ``ADDONS`` setting.\n+        ``ADDONS`` setting and execute their ``update_settings`` methods.\n \n         :param settings: The :class:`~scrapy.settings.Settings` object from \\\n             which to read the add-on configuration\n@@ -37,7 +43,7 @@ class AddonManager:\n         \"\"\"\n         addons = build_component_list(settings[\"ADDONS\"])\n         for addon in build_component_list(settings[\"ADDONS\"]):\n-            self._add(addon)\n+            self._add(addon, settings)\n         logger.info(\n             \"Enabled addons:\\n%(addons)s\",\n             {\n@@ -45,13 +51,3 @@ class AddonManager:\n             },\n             extra={\"crawler\": self.crawler},\n         )\n-\n-    def update_settings(self, settings) -> None:\n-        \"\"\"Call ``update_settings()`` of all held add-ons.\n-\n-        :param settings: The :class:`~scrapy.settings.Settings` object to be \\\n-            updated\n-        :type settings: :class:`~scrapy.settings.Settings`\n-        \"\"\"\n-        for addon in self.addons:\n-            addon.update_settings(settings)\n\n@@ -71,7 +71,6 @@ class Crawler:\n \n         self.addons: AddonManager = AddonManager(self)\n         self.addons.load_settings(self.settings)\n-        self.addons.update_settings(self.settings)\n \n         self.signals: SignalManager = SignalManager(self)\n \n\n@@ -4,6 +4,7 @@ from typing import Any, Dict\n \n from scrapy import Spider\n from scrapy.crawler import Crawler, CrawlerRunner\n+from scrapy.exceptions import NotConfigured\n from scrapy.settings import BaseSettings, Settings\n from scrapy.utils.test import get_crawler\n \n@@ -57,6 +58,18 @@ class AddonManagerTest(unittest.TestCase):\n         manager = crawler.addons\n         self.assertIsInstance(manager.addons[0], SimpleAddon)\n \n+    def test_notconfigured(self):\n+        class NotConfiguredAddon:\n+            def update_settings(self, settings):\n+                raise NotConfigured()\n+\n+        settings_dict = {\n+            \"ADDONS\": {NotConfiguredAddon: 0},\n+        }\n+        crawler = get_crawler(settings_dict=settings_dict)\n+        manager = crawler.addons\n+        self.assertFalse(manager.addons)\n+\n     def test_load_settings_order(self):\n         # Get three addons with different settings\n         addonlist = []\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7fdeb5c5c100fd94fd0fe2b8abc0c3fc0ba8c54d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 16 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 36 | Churn Cumulative: 676 | Contributors (this commit): 1 | Commits (past 90d): 16 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -19,18 +19,6 @@ class AddonManager:\n         self.crawler: \"Crawler\" = crawler\n         self.addons: List[Any] = []\n \n-    def _add(self, addon: Any, settings: Settings) -> None:\n-        \"\"\"Store an add-on.\"\"\"\n-        if isinstance(addon, (type, str)):\n-            addon = load_object(addon)\n-        if isinstance(addon, type):\n-            addon = create_instance(addon, settings=None, crawler=self.crawler)\n-        try:\n-            addon.update_settings(settings)\n-            self.addons.append(addon)\n-        except NotConfigured:\n-            pass\n-\n     def load_settings(self, settings: Settings) -> None:\n         \"\"\"Load add-ons and configurations from a settings object.\n \n@@ -41,13 +29,29 @@ class AddonManager:\n             which to read the add-on configuration\n         :type settings: :class:`~scrapy.settings.Settings`\n         \"\"\"\n-        addons = build_component_list(settings[\"ADDONS\"])\n-        for addon in build_component_list(settings[\"ADDONS\"]):\n-            self._add(addon, settings)\n+        enabled = []\n+        for clspath in build_component_list(settings[\"ADDONS\"]):\n+            try:\n+                addoncls = load_object(clspath)\n+                addon = create_instance(\n+                    addoncls, settings=settings, crawler=self.crawler\n+                )\n+                addon.update_settings(settings)\n+                self.addons.append(addon)\n+            except NotConfigured as e:\n+                if e.args:\n+                    clsname = (\n+                        clspath.split(\".\")[-1] if isinstance(clspath, str) else clspath\n+                    )\n+                    logger.warning(\n+                        \"Disabled %(clsname)s: %(eargs)s\",\n+                        {\"clsname\": clsname, \"eargs\": e.args[0]},\n+                        extra={\"crawler\": self.crawler},\n+                    )\n         logger.info(\n             \"Enabled addons:\\n%(addons)s\",\n             {\n-                \"addons\": addons,\n+                \"addons\": enabled,\n             },\n             extra={\"crawler\": self.crawler},\n         )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f803ad63f32b488b3b89a81c6069d3575910e28e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 678 | Contributors (this commit): 1 | Commits (past 90d): 17 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -29,7 +29,7 @@ class AddonManager:\n             which to read the add-on configuration\n         :type settings: :class:`~scrapy.settings.Settings`\n         \"\"\"\n-        enabled = []\n+        enabled: List[Any] = []\n         for clspath in build_component_list(settings[\"ADDONS\"]):\n             try:\n                 addoncls = load_object(clspath)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e58b8078f0d51148c1866d1da3f6e36609b5e2a5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 9 | Files Changed: 3 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 14 | Churn Cumulative: 1106 | Contributors (this commit): 18 | Commits (past 90d): 22 | Contributors (cumulative): 27 | DMM Complexity: 0.0\n\nDIFF:\n@@ -40,12 +40,9 @@ class AddonManager:\n                 self.addons.append(addon)\n             except NotConfigured as e:\n                 if e.args:\n-                    clsname = (\n-                        clspath.split(\".\")[-1] if isinstance(clspath, str) else clspath\n-                    )\n                     logger.warning(\n-                        \"Disabled %(clsname)s: %(eargs)s\",\n-                        {\"clsname\": clsname, \"eargs\": e.args[0]},\n+                        \"Disabled %(clspath)s: %(eargs)s\",\n+                        {\"clspath\": clspath, \"eargs\": e.args[0]},\n                         extra={\"crawler\": self.crawler},\n                     )\n         logger.info(\n\n@@ -46,10 +46,9 @@ class MiddlewareManager:\n                 enabled.append(clspath)\n             except NotConfigured as e:\n                 if e.args:\n-                    clsname = clspath.split(\".\")[-1]\n                     logger.warning(\n-                        \"Disabled %(clsname)s: %(eargs)s\",\n-                        {\"clsname\": clsname, \"eargs\": e.args[0]},\n+                        \"Disabled %(clspath)s: %(eargs)s\",\n+                        {\"clspath\": clspath, \"eargs\": e.args[0]},\n                         extra={\"crawler\": crawler},\n                     )\n \n\n@@ -39,7 +39,7 @@ class MOff:\n         pass\n \n     def __init__(self):\n-        raise NotConfigured\n+        raise NotConfigured(\"foo\")\n \n \n class TestMiddlewareManager(MiddlewareManager):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9f9a2292e08cb0944e1e88e64cef1f4b17486ec8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 21 | Files Changed: 2 | Hunks: 11 | Methods Changed: 9 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 44 | Churn Cumulative: 1129 | Contributors (this commit): 21 | Commits (past 90d): 5 | Contributors (cumulative): 33 | DMM Complexity: 1.0\n\nDIFF:\n@@ -50,11 +50,17 @@ def build_component_list(compdict, custom=None, convert=update_classpath):\n                     \"please provide a real number or None instead\"\n                 )\n \n+    if custom is not None:\n+        warnings.warn(\n+            \"The 'custom' attribute of build_component_list() is deprecated. \"\n+            \"Please merge its value into 'compdict' manually or change your \"\n+            \"code to use Settings.getwithbase().\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n         if isinstance(custom, (list, tuple)):\n             _check_components(custom)\n             return type(custom)(convert(c) for c in custom)\n-\n-    if custom is not None:\n         compdict.update(custom)\n \n     _validate_values(compdict)\n\n@@ -1,6 +1,8 @@\n import unittest\n import warnings\n \n+import pytest\n+\n from scrapy.exceptions import ScrapyDeprecationWarning, UsageError\n from scrapy.settings import BaseSettings, Settings\n from scrapy.utils.conf import (\n@@ -21,6 +23,7 @@ class BuildComponentListTest(unittest.TestCase):\n     def test_backward_compatible_build_dict(self):\n         base = {\"one\": 1, \"two\": 2, \"three\": 3, \"five\": 5, \"six\": None}\n         custom = {\"two\": None, \"three\": 8, \"four\": 4}\n+        with pytest.warns(ScrapyDeprecationWarning, match=\"The 'custom' attribute\"):\n             self.assertEqual(\n                 build_component_list(base, custom, convert=lambda x: x),\n                 [\"one\", \"four\", \"five\", \"three\"],\n@@ -28,12 +31,14 @@ class BuildComponentListTest(unittest.TestCase):\n \n     def test_return_list(self):\n         custom = [\"a\", \"b\", \"c\"]\n+        with pytest.warns(ScrapyDeprecationWarning, match=\"The 'custom' attribute\"):\n             self.assertEqual(\n                 build_component_list(None, custom, convert=lambda x: x), custom\n             )\n \n     def test_map_dict(self):\n         custom = {\"one\": 1, \"two\": 2, \"three\": 3}\n+        with pytest.warns(ScrapyDeprecationWarning, match=\"The 'custom' attribute\"):\n             self.assertEqual(\n                 build_component_list({}, custom, convert=lambda x: x.upper()),\n                 [\"ONE\", \"TWO\", \"THREE\"],\n@@ -41,23 +46,21 @@ class BuildComponentListTest(unittest.TestCase):\n \n     def test_map_list(self):\n         custom = [\"a\", \"b\", \"c\"]\n+        with pytest.warns(ScrapyDeprecationWarning, match=\"The 'custom' attribute\"):\n             self.assertEqual(\n                 build_component_list(None, custom, lambda x: x.upper()), [\"A\", \"B\", \"C\"]\n             )\n \n     def test_duplicate_components_in_dict(self):\n         duplicate_dict = {\"one\": 1, \"two\": 2, \"ONE\": 4}\n-        self.assertRaises(\n-            ValueError,\n-            build_component_list,\n-            {},\n-            duplicate_dict,\n-            convert=lambda x: x.lower(),\n-        )\n+        with self.assertRaises(ValueError):\n+            with pytest.warns(ScrapyDeprecationWarning, match=\"The 'custom' attribute\"):\n+                build_component_list({}, duplicate_dict, convert=lambda x: x.lower())\n \n     def test_duplicate_components_in_list(self):\n         duplicate_list = [\"a\", \"b\", \"a\"]\n         with self.assertRaises(ValueError) as cm:\n+            with pytest.warns(ScrapyDeprecationWarning, match=\"The 'custom' attribute\"):\n                 build_component_list(None, duplicate_list, convert=lambda x: x)\n         self.assertIn(str(duplicate_list), str(cm.exception))\n \n@@ -76,9 +79,8 @@ class BuildComponentListTest(unittest.TestCase):\n         )\n         # Same priority raises ValueError\n         duplicate_bs.set(\"ONE\", duplicate_bs[\"ONE\"], priority=20)\n-        self.assertRaises(\n-            ValueError, build_component_list, duplicate_bs, convert=lambda x: x.lower()\n-        )\n+        with self.assertRaises(ValueError):\n+            build_component_list(duplicate_bs, convert=lambda x: x.lower())\n \n     def test_valid_numbers(self):\n         # work well with None and numeric values\n@@ -92,15 +94,9 @@ class BuildComponentListTest(unittest.TestCase):\n         self.assertEqual(build_component_list(d, convert=lambda x: x), [\"b\", \"c\", \"a\"])\n         # raise exception for invalid values\n         d = {\"one\": \"5\"}\n-        self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)\n-        d = {\"one\": \"1.0\"}\n-        self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)\n-        d = {\"one\": [1, 2, 3]}\n-        self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)\n-        d = {\"one\": {\"a\": \"a\", \"b\": 2}}\n-        self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)\n-        d = {\"one\": \"lorem ipsum\"}\n-        self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)\n+        with self.assertRaises(ValueError):\n+            with pytest.warns(ScrapyDeprecationWarning, match=\"The 'custom' attribute\"):\n+                build_component_list({}, d, convert=lambda x: x)\n \n \n class UtilsConfTestCase(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b9c32a0cfd13dea2a59d20735f33ceff18daac97", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 97 | Files Changed: 5 | Hunks: 10 | Methods Changed: 5 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 97 | Churn Cumulative: 9926 | Contributors (this commit): 56 | Commits (past 90d): 14 | Contributors (cumulative): 114 | DMM Complexity: 0.7631578947368421\n\nDIFF:\n@@ -672,21 +672,12 @@ class FeedExporter:\n         params[\"time\"] = utc_now.replace(microsecond=0).isoformat().replace(\":\", \"-\")\n         params[\"batch_time\"] = utc_now.isoformat().replace(\":\", \"-\")\n         params[\"batch_id\"] = slot.batch_id + 1 if slot is not None else 1\n-        original_params = params.copy()\n         uripar_function = (\n             load_object(uri_params_function)\n             if uri_params_function\n             else lambda params, _: params\n         )\n         new_params = uripar_function(params, spider)\n-        if new_params is None or original_params != params:\n-            warnings.warn(\n-                \"Modifying the params dictionary in-place in the function defined in \"\n-                \"the FEED_URI_PARAMS setting or in the uri_params key of the FEEDS \"\n-                \"setting is deprecated. The function must return a new dictionary \"\n-                \"instead.\",\n-                category=ScrapyDeprecationWarning,\n-            )\n         return new_params if new_params is not None else params\n \n     def _load_filter(self, feed_options):\n\n@@ -10,7 +10,6 @@ from typing import Union\n \n from queuelib import queue\n \n-from scrapy.utils.deprecate import create_deprecated_class\n from scrapy.utils.request import request_from_dict\n \n \n@@ -143,32 +142,3 @@ MarshalFifoDiskQueue = _scrapy_serialization_queue(_MarshalFifoSerializationDisk\n MarshalLifoDiskQueue = _scrapy_serialization_queue(_MarshalLifoSerializationDiskQueue)\n FifoMemoryQueue = _scrapy_non_serialization_queue(queue.FifoMemoryQueue)\n LifoMemoryQueue = _scrapy_non_serialization_queue(queue.LifoMemoryQueue)\n-\n-\n-# deprecated queue classes\n-_subclass_warn_message = \"{cls} inherits from deprecated class {old}\"\n-_instance_warn_message = \"{cls} is deprecated\"\n-PickleFifoDiskQueueNonRequest = create_deprecated_class(\n-    name=\"PickleFifoDiskQueueNonRequest\",\n-    new_class=_PickleFifoSerializationDiskQueue,\n-    subclass_warn_message=_subclass_warn_message,\n-    instance_warn_message=_instance_warn_message,\n-)\n-PickleLifoDiskQueueNonRequest = create_deprecated_class(\n-    name=\"PickleLifoDiskQueueNonRequest\",\n-    new_class=_PickleLifoSerializationDiskQueue,\n-    subclass_warn_message=_subclass_warn_message,\n-    instance_warn_message=_instance_warn_message,\n-)\n-MarshalFifoDiskQueueNonRequest = create_deprecated_class(\n-    name=\"MarshalFifoDiskQueueNonRequest\",\n-    new_class=_MarshalFifoSerializationDiskQueue,\n-    subclass_warn_message=_subclass_warn_message,\n-    instance_warn_message=_instance_warn_message,\n-)\n-MarshalLifoDiskQueueNonRequest = create_deprecated_class(\n-    name=\"MarshalLifoDiskQueueNonRequest\",\n-    new_class=_MarshalLifoSerializationDiskQueue,\n-    subclass_warn_message=_subclass_warn_message,\n-    instance_warn_message=_instance_warn_message,\n-)\n\n@@ -1,27 +0,0 @@\n-import warnings\n-from typing import Optional\n-\n-import scrapy\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-from scrapy.utils.request import request_from_dict as _from_dict\n-\n-warnings.warn(\n-    (\n-        \"Module scrapy.utils.reqser is deprecated, please use request.to_dict method\"\n-        \" and/or scrapy.utils.request.request_from_dict instead\"\n-    ),\n-    category=ScrapyDeprecationWarning,\n-    stacklevel=2,\n-)\n-\n-\n-def request_to_dict(\n-    request: \"scrapy.Request\", spider: Optional[\"scrapy.Spider\"] = None\n-) -> dict:\n-    return request.to_dict(spider=spider)\n-\n-\n-def request_from_dict(\n-    d: dict, spider: Optional[\"scrapy.Spider\"] = None\n-) -> \"scrapy.Request\":\n-    return _from_dict(d, spider=spider)\n\n@@ -3067,9 +3067,6 @@ class URIParamsTest:\n         spider = scrapy.Spider(self.spider_name)\n         spider.crawler = crawler\n \n-        with pytest.warns(\n-            ScrapyDeprecationWarning, match=\"Modifying the params dictionary in-place\"\n-        ):\n         feed_exporter.open_spider(spider)\n \n         self.assertEqual(feed_exporter.slots[0].uri, f\"file:///tmp/{self.spider_name}\")\n\n@@ -1,10 +1,6 @@\n-import sys\n import unittest\n-import warnings\n-from contextlib import suppress\n \n from scrapy import Request, Spider\n-from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import FormRequest, JsonRequest\n from scrapy.utils.request import request_from_dict\n \n@@ -162,30 +158,6 @@ class RequestSerializationTest(unittest.TestCase):\n         self.assertRaises(ValueError, request_from_dict, d, spider=Spider(\"foo\"))\n \n \n-class DeprecatedMethodsRequestSerializationTest(RequestSerializationTest):\n-    def _assert_serializes_ok(self, request, spider=None):\n-        with warnings.catch_warnings(record=True) as caught:\n-            warnings.simplefilter(\"always\")\n-            with suppress(KeyError):\n-                del sys.modules[\n-                    \"scrapy.utils.reqser\"\n-                ]  # delete module to reset the deprecation warning\n-\n-            from scrapy.utils.reqser import request_from_dict as _from_dict\n-            from scrapy.utils.reqser import request_to_dict as _to_dict\n-\n-            request_copy = _from_dict(_to_dict(request, spider), spider)\n-            self._assert_same_request(request, request_copy)\n-\n-            self.assertEqual(len(caught), 1)\n-            self.assertTrue(issubclass(caught[0].category, ScrapyDeprecationWarning))\n-            self.assertEqual(\n-                \"Module scrapy.utils.reqser is deprecated, please use request.to_dict method\"\n-                \" and/or scrapy.utils.request.request_from_dict instead\",\n-                str(caught[0].message),\n-            )\n-\n-\n class TestSpiderMixin:\n     def __mixin_callback(self, response):\n         pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#09c63a178bf28a66b564562125497721bea371b3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 162 | Files Changed: 2 | Hunks: 22 | Methods Changed: 21 | Complexity Δ (Sum/Max): -17/0 | Churn Δ: 180 | Churn Cumulative: 4385 | Contributors (this commit): 36 | Commits (past 90d): 2 | Contributors (cumulative): 53 | DMM Complexity: 0.04838709677419355\n\nDIFF:\n@@ -5,7 +5,6 @@ For more information see docs/topics/architecture.rst\n \n \"\"\"\n import logging\n-import warnings\n from time import time\n from typing import (\n     TYPE_CHECKING,\n@@ -14,7 +13,6 @@ from typing import (\n     Generator,\n     Iterable,\n     Iterator,\n-    List,\n     Optional,\n     Set,\n     Type,\n@@ -29,7 +27,7 @@ from twisted.python.failure import Failure\n from scrapy import signals\n from scrapy.core.downloader import Downloader\n from scrapy.core.scraper import Scraper\n-from scrapy.exceptions import CloseSpider, DontCloseSpider, ScrapyDeprecationWarning\n+from scrapy.exceptions import CloseSpider, DontCloseSpider\n from scrapy.http import Request, Response\n from scrapy.logformatter import LogFormatter\n from scrapy.settings import BaseSettings, Settings\n@@ -213,7 +211,7 @@ class ExecutionEngine:\n         if request is None:\n             return None\n \n-        d = self._download(request, self.spider)\n+        d = self._download(request)\n         d.addBoth(self._handle_downloader_output, request)\n         d.addErrback(\n             lambda f: logger.info(\n@@ -266,13 +264,7 @@ class ExecutionEngine:\n         )\n         return d\n \n-    def spider_is_idle(self, spider: Optional[Spider] = None) -> bool:\n-        if spider is not None:\n-            warnings.warn(\n-                \"Passing a 'spider' argument to ExecutionEngine.spider_is_idle is deprecated\",\n-                category=ScrapyDeprecationWarning,\n-                stacklevel=2,\n-            )\n+    def spider_is_idle(self) -> bool:\n         if self.slot is None:\n             raise RuntimeError(\"Engine slot not assigned\")\n         if not self.scraper.slot.is_idle():  # type: ignore[union-attr]\n@@ -285,18 +277,8 @@ class ExecutionEngine:\n             return False\n         return True\n \n-    def crawl(self, request: Request, spider: Optional[Spider] = None) -> None:\n+    def crawl(self, request: Request) -> None:\n         \"\"\"Inject the request into the spider <-> downloader pipeline\"\"\"\n-        if spider is not None:\n-            warnings.warn(\n-                \"Passing a 'spider' argument to ExecutionEngine.crawl is deprecated\",\n-                category=ScrapyDeprecationWarning,\n-                stacklevel=2,\n-            )\n-            if spider is not self.spider:\n-                raise RuntimeError(\n-                    f\"The spider {spider.name!r} does not match the open spider\"\n-                )\n         if self.spider is None:\n             raise RuntimeError(f\"No open spider to crawl: {request}\")\n         self._schedule_request(request, self.spider)\n@@ -311,39 +293,24 @@ class ExecutionEngine:\n                 signals.request_dropped, request=request, spider=spider\n             )\n \n-    def download(self, request: Request, spider: Optional[Spider] = None) -> Deferred:\n+    def download(self, request: Request) -> Deferred:\n         \"\"\"Return a Deferred which fires with a Response as result, only downloader middlewares are applied\"\"\"\n-        if spider is not None:\n-            warnings.warn(\n-                \"Passing a 'spider' argument to ExecutionEngine.download is deprecated\",\n-                category=ScrapyDeprecationWarning,\n-                stacklevel=2,\n-            )\n-            if spider is not self.spider:\n-                logger.warning(\n-                    \"The spider '%s' does not match the open spider\", spider.name\n-                )\n         if self.spider is None:\n             raise RuntimeError(f\"No open spider to crawl: {request}\")\n-        return self._download(request, spider).addBoth(\n-            self._downloaded, request, spider\n-        )\n+        return self._download(request).addBoth(self._downloaded, request)\n \n     def _downloaded(\n-        self, result: Union[Response, Request], request: Request, spider: Spider\n+        self, result: Union[Response, Request], request: Request\n     ) -> Union[Deferred, Response]:\n         assert self.slot is not None  # typing\n         self.slot.remove_request(request)\n-        return self.download(result, spider) if isinstance(result, Request) else result\n+        return self.download(result) if isinstance(result, Request) else result\n \n-    def _download(self, request: Request, spider: Optional[Spider]) -> Deferred:\n+    def _download(self, request: Request) -> Deferred:\n         assert self.slot is not None  # typing\n \n         self.slot.add_request(request)\n \n-        if spider is None:\n-            spider = self.spider\n-\n         def _on_success(result: Union[Response, Request]) -> Union[Response, Request]:\n             if not isinstance(result, (Response, Request)):\n                 raise TypeError(\n@@ -352,15 +319,17 @@ class ExecutionEngine:\n             if isinstance(result, Response):\n                 if result.request is None:\n                     result.request = request\n-                assert spider is not None\n-                logkws = self.logformatter.crawled(result.request, result, spider)\n+                assert self.spider is not None\n+                logkws = self.logformatter.crawled(result.request, result, self.spider)\n                 if logkws is not None:\n-                    logger.log(*logformatter_adapter(logkws), extra={\"spider\": spider})\n+                    logger.log(\n+                        *logformatter_adapter(logkws), extra={\"spider\": self.spider}\n+                    )\n                 self.signals.send_catch_log(\n                     signal=signals.response_received,\n                     response=result,\n                     request=result.request,\n-                    spider=spider,\n+                    spider=self.spider,\n                 )\n             return result\n \n@@ -369,8 +338,8 @@ class ExecutionEngine:\n             self.slot.nextcall.schedule()\n             return _\n \n-        assert spider is not None\n-        dwld = self.downloader.fetch(request, spider)\n+        assert self.spider is not None\n+        dwld = self.downloader.fetch(request, self.spider)\n         dwld.addCallbacks(_on_success)\n         dwld.addBoth(_on_complete)\n         return dwld\n@@ -485,31 +454,3 @@ class ExecutionEngine:\n         dfd.addBoth(lambda _: self._spider_closed_callback(spider))\n \n         return dfd\n-\n-    @property\n-    def open_spiders(self) -> List[Spider]:\n-        warnings.warn(\n-            \"ExecutionEngine.open_spiders is deprecated, please use ExecutionEngine.spider instead\",\n-            category=ScrapyDeprecationWarning,\n-            stacklevel=2,\n-        )\n-        return [self.spider] if self.spider is not None else []\n-\n-    def has_capacity(self) -> bool:\n-        warnings.warn(\n-            \"ExecutionEngine.has_capacity is deprecated\",\n-            ScrapyDeprecationWarning,\n-            stacklevel=2,\n-        )\n-        return not bool(self.slot)\n-\n-    def schedule(self, request: Request, spider: Spider) -> None:\n-        warnings.warn(\n-            \"ExecutionEngine.schedule is deprecated, please use \"\n-            \"ExecutionEngine.crawl or ExecutionEngine.download instead\",\n-            category=ScrapyDeprecationWarning,\n-            stacklevel=2,\n-        )\n-        if self.slot is None:\n-            raise RuntimeError(\"Engine slot not assigned\")\n-        self._schedule_request(request, spider)\n\n@@ -20,7 +20,6 @@ from threading import Timer\n from urllib.parse import urlparse\n \n import attr\n-import pytest\n from itemadapter import ItemAdapter\n from pydispatch import dispatcher\n from twisted.internet import defer, reactor\n@@ -29,7 +28,7 @@ from twisted.web import server, static, util\n \n from scrapy import signals\n from scrapy.core.engine import ExecutionEngine\n-from scrapy.exceptions import CloseSpider, ScrapyDeprecationWarning\n+from scrapy.exceptions import CloseSpider\n from scrapy.http import Request\n from scrapy.item import Field, Item\n from scrapy.linkextractors import LinkExtractor\n@@ -436,90 +435,6 @@ class EngineTest(unittest.TestCase):\n         finally:\n             yield e.stop()\n \n-    @defer.inlineCallbacks\n-    def test_close_spiders_downloader(self):\n-        with pytest.warns(\n-            ScrapyDeprecationWarning,\n-            match=\"ExecutionEngine.open_spiders is deprecated, \"\n-            \"please use ExecutionEngine.spider instead\",\n-        ):\n-            e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)\n-            yield e.open_spider(TestSpider(), [])\n-            self.assertEqual(len(e.open_spiders), 1)\n-            yield e.close()\n-            self.assertEqual(len(e.open_spiders), 0)\n-\n-    @defer.inlineCallbacks\n-    def test_close_engine_spiders_downloader(self):\n-        with pytest.warns(\n-            ScrapyDeprecationWarning,\n-            match=\"ExecutionEngine.open_spiders is deprecated, \"\n-            \"please use ExecutionEngine.spider instead\",\n-        ):\n-            e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)\n-            yield e.open_spider(TestSpider(), [])\n-            e.start()\n-            self.assertTrue(e.running)\n-            yield e.close()\n-            self.assertFalse(e.running)\n-            self.assertEqual(len(e.open_spiders), 0)\n-\n-    @defer.inlineCallbacks\n-    def test_crawl_deprecated_spider_arg(self):\n-        with pytest.warns(\n-            ScrapyDeprecationWarning,\n-            match=\"Passing a 'spider' argument to \"\n-            \"ExecutionEngine.crawl is deprecated\",\n-        ):\n-            e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)\n-            spider = TestSpider()\n-            yield e.open_spider(spider, [])\n-            e.start()\n-            e.crawl(Request(\"data:,\"), spider)\n-            yield e.close()\n-\n-    @defer.inlineCallbacks\n-    def test_download_deprecated_spider_arg(self):\n-        with pytest.warns(\n-            ScrapyDeprecationWarning,\n-            match=\"Passing a 'spider' argument to \"\n-            \"ExecutionEngine.download is deprecated\",\n-        ):\n-            e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)\n-            spider = TestSpider()\n-            yield e.open_spider(spider, [])\n-            e.start()\n-            e.download(Request(\"data:,\"), spider)\n-            yield e.close()\n-\n-    @defer.inlineCallbacks\n-    def test_deprecated_schedule(self):\n-        with pytest.warns(\n-            ScrapyDeprecationWarning,\n-            match=\"ExecutionEngine.schedule is deprecated, please use \"\n-            \"ExecutionEngine.crawl or ExecutionEngine.download instead\",\n-        ):\n-            e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)\n-            spider = TestSpider()\n-            yield e.open_spider(spider, [])\n-            e.start()\n-            e.schedule(Request(\"data:,\"), spider)\n-            yield e.close()\n-\n-    @defer.inlineCallbacks\n-    def test_deprecated_has_capacity(self):\n-        with pytest.warns(\n-            ScrapyDeprecationWarning, match=\"ExecutionEngine.has_capacity is deprecated\"\n-        ):\n-            e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)\n-            self.assertTrue(e.has_capacity())\n-            spider = TestSpider()\n-            yield e.open_spider(spider, [])\n-            self.assertFalse(e.has_capacity())\n-            e.start()\n-            yield e.close()\n-            self.assertTrue(e.has_capacity())\n-\n     def test_short_timeout(self):\n         args = (\n             sys.executable,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e9b088f1fb430009e4ea4e5b5a3836b4b94364eb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 336 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,5 @@\n import datetime\n+import typing\n import unittest\n \n from scrapy.crawler import Crawler\n@@ -96,7 +97,7 @@ class TestPeriodicLog(unittest.TestCase):\n             ext.spider_closed(spider, reason=\"finished\")\n             return ext, a, b\n \n-        def check(settings: dict, condition: callable):\n+        def check(settings: dict, condition: typing.Callable):\n             ext, a, b = emulate(settings)\n             assert list(a[\"delta\"].keys()) == [\n                 k for k, v in ext.stats._stats.items() if condition(k, v)\n@@ -153,7 +154,7 @@ class TestPeriodicLog(unittest.TestCase):\n             ext.spider_closed(spider, reason=\"finished\")\n             return ext, a, b\n \n-        def check(settings: dict, condition: callable):\n+        def check(settings: dict, condition: typing.Callable):\n             ext, a, b = emulate(settings)\n             assert list(a[\"stats\"].keys()) == [\n                 k for k, v in ext.stats._stats.items() if condition(k, v)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d31829b72f6238a92a42c0990953d5056e8f5778", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 93 | Lines Deleted: 52 | Files Changed: 10 | Hunks: 47 | Methods Changed: 55 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 145 | Churn Cumulative: 3629 | Contributors (this commit): 66 | Commits (past 90d): 24 | Contributors (cumulative): 137 | DMM Complexity: 0.9107142857142857\n\nDIFF:\n@@ -4,6 +4,7 @@ Scrapy core exceptions\n These exceptions are documented in docs/topics/exceptions.rst. Please don't add\n new exceptions here without documenting them there.\n \"\"\"\n+from typing import Any\n \n # Internal\n \n@@ -77,7 +78,7 @@ class NotSupported(Exception):\n class UsageError(Exception):\n     \"\"\"To indicate a command-line usage error\"\"\"\n \n-    def __init__(self, *a, **kw):\n+    def __init__(self, *a: Any, **kw: Any):\n         self.print_help = kw.pop(\"print_help\", True)\n         super().__init__(*a, **kw)\n \n\n@@ -11,7 +11,7 @@ import warnings\n from datetime import datetime\n from pathlib import Path, PureWindowsPath\n from tempfile import NamedTemporaryFile\n-from typing import IO, Any, Callable, List, Optional, Tuple, Union\n+from typing import IO, Any, Callable, Dict, List, Optional, Tuple, Union\n from urllib.parse import unquote, urlparse\n \n from twisted.internet import defer, threads\n@@ -282,15 +282,22 @@ class GCSFeedStorage(BlockingFeedStorage):\n \n \n class FTPFeedStorage(BlockingFeedStorage):\n-    def __init__(self, uri, use_active_mode=False, *, feed_options=None):\n+    def __init__(\n+        self,\n+        uri: str,\n+        use_active_mode: bool = False,\n+        *,\n+        feed_options: Optional[Dict[str, Any]] = None,\n+    ):\n         u = urlparse(uri)\n-        self.host = u.hostname\n-        self.port = int(u.port or \"21\")\n-        self.username = u.username\n-        self.password = unquote(u.password or \"\")\n-        self.path = u.path\n-        self.use_active_mode = use_active_mode\n-        self.overwrite = not feed_options or feed_options.get(\"overwrite\", True)\n+        assert u.hostname\n+        self.host: str = u.hostname\n+        self.port: int = int(u.port or \"21\")\n+        self.username: str = u.username or \"\"\n+        self.password: str = unquote(u.password or \"\")\n+        self.path: str = u.path\n+        self.use_active_mode: bool = use_active_mode\n+        self.overwrite: bool = not feed_options or feed_options.get(\"overwrite\", True)\n \n     @classmethod\n     def from_crawler(cls, crawler, uri, *, feed_options=None):\n\n@@ -5,7 +5,18 @@ import warnings\n from configparser import ConfigParser\n from operator import itemgetter\n from pathlib import Path\n-from typing import Any, Dict, List, Optional, Union\n+from typing import (\n+    Any,\n+    Callable,\n+    Collection,\n+    Dict,\n+    Iterable,\n+    List,\n+    Mapping,\n+    MutableMapping,\n+    Optional,\n+    Union,\n+)\n \n from scrapy.exceptions import ScrapyDeprecationWarning, UsageError\n from scrapy.settings import BaseSettings\n@@ -13,17 +24,21 @@ from scrapy.utils.deprecate import update_classpath\n from scrapy.utils.python import without_none_values\n \n \n-def build_component_list(compdict, custom=None, convert=update_classpath):\n+def build_component_list(\n+    compdict: MutableMapping[Any, Any],\n+    custom: Any = None,\n+    convert: Callable[[Any], Any] = update_classpath,\n+) -> List[Any]:\n     \"\"\"Compose a component list from a { class: order } dictionary.\"\"\"\n \n-    def _check_components(complist):\n+    def _check_components(complist: Collection[Any]) -> None:\n         if len({convert(c) for c in complist}) != len(complist):\n             raise ValueError(\n                 f\"Some paths in {complist!r} convert to the same object, \"\n                 \"please update your settings\"\n             )\n \n-    def _map_keys(compdict):\n+    def _map_keys(compdict: Mapping[Any, Any]) -> Union[BaseSettings, Dict[Any, Any]]:\n         if isinstance(compdict, BaseSettings):\n             compbs = BaseSettings()\n             for k, v in compdict.items():\n@@ -41,7 +56,7 @@ def build_component_list(compdict, custom=None, convert=update_classpath):\n         _check_components(compdict)\n         return {convert(k): v for k, v in compdict.items()}\n \n-    def _validate_values(compdict):\n+    def _validate_values(compdict: Mapping[Any, Any]) -> None:\n         \"\"\"Fail if a value in the components dict is not a real number or None.\"\"\"\n         for name, value in compdict.items():\n             if value is not None and not isinstance(value, numbers.Real):\n@@ -60,7 +75,7 @@ def build_component_list(compdict, custom=None, convert=update_classpath):\n         )\n         if isinstance(custom, (list, tuple)):\n             _check_components(custom)\n-            return type(custom)(convert(c) for c in custom)\n+            return type(custom)(convert(c) for c in custom)  # type: ignore[return-value]\n         compdict.update(custom)\n \n     _validate_values(compdict)\n@@ -68,7 +83,7 @@ def build_component_list(compdict, custom=None, convert=update_classpath):\n     return [k for k, v in sorted(compdict.items(), key=itemgetter(1))]\n \n \n-def arglist_to_dict(arglist):\n+def arglist_to_dict(arglist: List[str]) -> Dict[str, str]:\n     \"\"\"Convert a list of arguments like ['arg1=val1', 'arg2=val2', ...] to a\n     dict\n     \"\"\"\n@@ -91,7 +106,7 @@ def closest_scrapy_cfg(\n     return closest_scrapy_cfg(path.parent, path)\n \n \n-def init_env(project=\"default\", set_syspath=True):\n+def init_env(project: str = \"default\", set_syspath: bool = True) -> None:\n     \"\"\"Initialize environment to use command-line tool from inside a project\n     dir. This sets the Scrapy settings module and modifies the Python path to\n     be able to locate the project module.\n@@ -106,7 +121,7 @@ def init_env(project=\"default\", set_syspath=True):\n             sys.path.append(projdir)\n \n \n-def get_config(use_closest=True):\n+def get_config(use_closest: bool = True) -> ConfigParser:\n     \"\"\"Get Scrapy config file as a ConfigParser\"\"\"\n     sources = get_sources(use_closest)\n     cfg = ConfigParser()\n@@ -114,7 +129,7 @@ def get_config(use_closest=True):\n     return cfg\n \n \n-def get_sources(use_closest=True) -> List[str]:\n+def get_sources(use_closest: bool = True) -> List[str]:\n     xdg_config_home = (\n         os.environ.get(\"XDG_CONFIG_HOME\") or Path(\"~/.config\").expanduser()\n     )\n@@ -129,7 +144,9 @@ def get_sources(use_closest=True) -> List[str]:\n     return sources\n \n \n-def feed_complete_default_values_from_settings(feed, settings):\n+def feed_complete_default_values_from_settings(\n+    feed: Dict[str, Any], settings: BaseSettings\n+) -> Dict[str, Any]:\n     out = feed.copy()\n     out.setdefault(\"batch_item_count\", settings.getint(\"FEED_EXPORT_BATCH_ITEM_COUNT\"))\n     out.setdefault(\"encoding\", settings[\"FEED_EXPORT_ENCODING\"])\n@@ -145,21 +162,21 @@ def feed_complete_default_values_from_settings(feed, settings):\n \n \n def feed_process_params_from_cli(\n-    settings,\n+    settings: BaseSettings,\n     output: List[str],\n-    output_format=None,\n+    output_format: Optional[str] = None,\n     overwrite_output: Optional[List[str]] = None,\n-):\n+) -> Dict[str, Dict[str, Any]]:\n     \"\"\"\n     Receives feed export params (from the 'crawl' or 'runspider' commands),\n     checks for inconsistencies in their quantities and returns a dictionary\n     suitable to be used as the FEEDS setting.\n     \"\"\"\n-    valid_output_formats = without_none_values(\n+    valid_output_formats: Iterable[str] = without_none_values(\n         settings.getwithbase(\"FEED_EXPORTERS\")\n     ).keys()\n \n-    def check_valid_format(output_format):\n+    def check_valid_format(output_format: str) -> None:\n         if output_format not in valid_output_formats:\n             raise UsageError(\n                 f\"Unrecognized output format '{output_format}'. \"\n\n@@ -1,9 +1,10 @@\n import posixpath\n from ftplib import FTP, error_perm\n from posixpath import dirname\n+from typing import IO\n \n \n-def ftp_makedirs_cwd(ftp, path, first_call=True):\n+def ftp_makedirs_cwd(ftp: FTP, path: str, first_call: bool = True) -> None:\n     \"\"\"Set the current directory of the FTP connection given in the ``ftp``\n     argument (as a ftplib.FTP object), creating all parent directories if they\n     don't exist. The ftplib.FTP object must be already connected and logged in.\n@@ -18,8 +19,16 @@ def ftp_makedirs_cwd(ftp, path, first_call=True):\n \n \n def ftp_store_file(\n-    *, path, file, host, port, username, password, use_active_mode=False, overwrite=True\n-):\n+    *,\n+    path: str,\n+    file: IO,\n+    host: str,\n+    port: int,\n+    username: str,\n+    password: str,\n+    use_active_mode: bool = False,\n+    overwrite: bool = True,\n+) -> None:\n     \"\"\"Opens a FTP connection with passed credentials,sets current directory\n     to the directory extracted from given path, then uploads the file to server\n     \"\"\"\n\n@@ -5,7 +5,7 @@ from scrapy.settings import BaseSettings\n \n \n def job_dir(settings: BaseSettings) -> Optional[str]:\n-    path = settings[\"JOBDIR\"]\n+    path: str = settings[\"JOBDIR\"]\n     if path and not Path(path).exists():\n         Path(path).mkdir(parents=True)\n     return path\n\n@@ -1,4 +1,5 @@\n import signal\n+from typing import Callable\n \n signal_names = {}\n for signame in dir(signal):\n@@ -8,7 +9,7 @@ for signame in dir(signal):\n             signal_names[signum] = signame\n \n \n-def install_shutdown_handlers(function, override_sigint=True):\n+def install_shutdown_handlers(function: Callable, override_sigint: bool = True) -> None:\n     \"\"\"Install the given function as a signal handler for all common shutdown\n     signals (such as SIGINT, SIGTERM, etc). If override_sigint is ``False`` the\n     SIGINT handler won't be install if there is already a handler in place\n\n@@ -11,7 +11,7 @@ ENVVAR = \"SCRAPY_SETTINGS_MODULE\"\n DATADIR_CFG_SECTION = \"datadir\"\n \n \n-def inside_project():\n+def inside_project() -> bool:\n     scrapy_module = os.environ.get(ENVVAR)\n     if scrapy_module:\n         try:\n@@ -25,7 +25,7 @@ def inside_project():\n     return bool(closest_scrapy_cfg())\n \n \n-def project_data_dir(project=\"default\") -> str:\n+def project_data_dir(project: str = \"default\") -> str:\n     \"\"\"Return the current project data dir, creating it if it doesn't exist\"\"\"\n     if not inside_project():\n         raise NotConfigured(\"Not inside a project\")\n@@ -44,7 +44,7 @@ def project_data_dir(project=\"default\") -> str:\n     return str(d)\n \n \n-def data_path(path: str, createdir=False) -> str:\n+def data_path(path: str, createdir: bool = False) -> str:\n     \"\"\"\n     Return the given path joined with the .scrapy data directory.\n     If given an absolute path, return it unmodified.\n@@ -60,7 +60,7 @@ def data_path(path: str, createdir=False) -> str:\n     return str(path_obj)\n \n \n-def get_project_settings():\n+def get_project_settings() -> Settings:\n     if ENVVAR not in os.environ:\n         project = os.environ.get(\"SCRAPY_PROJECT\", \"default\")\n         init_env(project)\n\n@@ -4,7 +4,7 @@ Module for processing Sitemaps.\n Note: The main purpose of this module is to provide support for the\n SitemapSpider, its API is subject to change without notice.\n \"\"\"\n-\n+from typing import Any, Dict, Generator, Iterator, Optional\n from urllib.parse import urljoin\n \n import lxml.etree\n@@ -14,7 +14,7 @@ class Sitemap:\n     \"\"\"Class to parse Sitemap (type=urlset) and Sitemap Index\n     (type=sitemapindex) files\"\"\"\n \n-    def __init__(self, xmltext):\n+    def __init__(self, xmltext: str):\n         xmlp = lxml.etree.XMLParser(\n             recover=True, remove_comments=True, resolve_entities=False\n         )\n@@ -22,9 +22,9 @@ class Sitemap:\n         rt = self._root.tag\n         self.type = self._root.tag.split(\"}\", 1)[1] if \"}\" in rt else rt\n \n-    def __iter__(self):\n+    def __iter__(self) -> Iterator[Dict[str, Any]]:\n         for elem in self._root.getchildren():\n-            d = {}\n+            d: Dict[str, Any] = {}\n             for el in elem.getchildren():\n                 tag = el.tag\n                 name = tag.split(\"}\", 1)[1] if \"}\" in tag else tag\n@@ -39,11 +39,13 @@ class Sitemap:\n                 yield d\n \n \n-def sitemap_urls_from_robots(robots_text, base_url=None):\n+def sitemap_urls_from_robots(\n+    robots_text: str, base_url: Optional[str] = None\n+) -> Generator[str, Any, None]:\n     \"\"\"Return an iterator over all sitemap urls contained in the given\n     robots.txt file\n     \"\"\"\n     for line in robots_text.splitlines():\n         if line.lstrip().lower().startswith(\"sitemap:\"):\n             url = line.split(\":\", 1)[1].strip()\n-            yield urljoin(base_url, url)\n+            yield urljoin(base_url or \"\", url)\n\n@@ -1,4 +1,4 @@\n-from typing import Any, Optional, cast\n+from typing import Any, Optional\n \n import OpenSSL._util as pyOpenSSLutil\n import OpenSSL.SSL\n@@ -58,9 +58,6 @@ def get_temp_key_info(ssl_object: Any) -> Optional[str]:\n \n \n def get_openssl_version() -> str:\n-    # https://github.com/python/typeshed/issues/10024\n-    system_openssl_bytes = cast(\n-        bytes, OpenSSL.SSL.SSLeay_version(OpenSSL.SSL.SSLEAY_VERSION)\n-    )\n+    system_openssl_bytes = OpenSSL.SSL.SSLeay_version(OpenSSL.SSL.SSLEAY_VERSION)\n     system_openssl = system_openssl_bytes.decode(\"ascii\", errors=\"replace\")\n     return f\"{OpenSSL.version.__version__} ({system_openssl})\"\n\n@@ -12,9 +12,14 @@ alias to object in that case).\n from collections import defaultdict\n from operator import itemgetter\n from time import time\n-from typing import DefaultDict\n+from typing import TYPE_CHECKING, Any, DefaultDict, Iterable\n from weakref import WeakKeyDictionary\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n+\n NoneType = type(None)\n live_refs: DefaultDict[type, WeakKeyDictionary] = defaultdict(WeakKeyDictionary)\n \n@@ -24,13 +29,14 @@ class object_ref:\n \n     __slots__ = ()\n \n-    def __new__(cls, *args, **kwargs):\n+    def __new__(cls, *args: Any, **kwargs: Any) -> \"Self\":\n         obj = object.__new__(cls)\n         live_refs[cls][obj] = time()\n         return obj\n \n \n-def format_live_refs(ignore=NoneType):\n+# using Any as it's hard to type type(None)\n+def format_live_refs(ignore: Any = NoneType) -> str:\n     \"\"\"Return a tabular representation of tracked objects\"\"\"\n     s = \"Live References\\n\\n\"\n     now = time()\n@@ -44,12 +50,12 @@ def format_live_refs(ignore=NoneType):\n     return s\n \n \n-def print_live_refs(*a, **kw):\n+def print_live_refs(*a: Any, **kw: Any) -> None:\n     \"\"\"Print tracked objects\"\"\"\n     print(format_live_refs(*a, **kw))\n \n \n-def get_oldest(class_name):\n+def get_oldest(class_name: str) -> Any:\n     \"\"\"Get the oldest object for a specific class name\"\"\"\n     for cls, wdict in live_refs.items():\n         if cls.__name__ == class_name:\n@@ -58,8 +64,9 @@ def get_oldest(class_name):\n             return min(wdict.items(), key=itemgetter(1))[0]\n \n \n-def iter_all(class_name):\n+def iter_all(class_name: str) -> Iterable[Any]:\n     \"\"\"Iterate over all objects of the same class by its class name\"\"\"\n     for cls, wdict in live_refs.items():\n         if cls.__name__ == class_name:\n             return wdict.keys()\n+    return []\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
