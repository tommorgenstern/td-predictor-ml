{"custom_id": "keras#3d2db56dd5d917ee9302b37ffe9fa861417ce529", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 16 | Churn Cumulative: 44 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -52,7 +52,7 @@ class SpectralNormalization(Wrapper):\n \n     def build(self, input_shape):\n         super().build(input_shape)\n-        self.input_spec = InputSpec(shape=[None] + list(input_shape[1:]))\n+        self.input_spec = InputSpec(min_ndim=1, axes={-1: input_shape[-1]})\n \n         if hasattr(self.layer, \"kernel\"):\n             self.kernel = self.layer.kernel\n\n@@ -35,6 +35,20 @@ class SpectralNormalizationTest(testing.TestCase):\n             run_training_check=False,\n         )\n \n+    @pytest.mark.requires_trainable_backend\n+    def test_spectralnorm_higher_dim(self):\n+        self.run_layer_test(\n+            layers.SpectralNormalization,\n+            init_kwargs={\"layer\": layers.Dense(2)},\n+            input_data=np.random.uniform(size=(10, 3, 4, 5)),\n+            expected_output_shape=(10, 3, 4, 2),\n+            expected_num_trainable_weights=2,\n+            expected_num_non_trainable_weights=1,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=False,\n+        )\n+\n     def test_invalid_power_iterations(self):\n         with self.assertRaisesRegex(\n             ValueError, \"`power_iterations` should be greater than zero.\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4d33a9acb7296d19270257dc845c2030c28622ae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): -3/2 | Churn Δ: 29 | Churn Cumulative: 888 | Contributors (this commit): 13 | Commits (past 90d): 11 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -453,8 +453,6 @@ class Functional(Function, Model):\n             return [operation.name, new_node_index, tensor_index]\n \n         def map_tensors(tensors):\n-            if isinstance(tensors, backend.KerasTensor):\n-                return [get_tensor_config(tensors)]\n             return tree.map_structure(get_tensor_config, tensors)\n \n         config[\"input_layers\"] = map_tensors(self._inputs_struct)\n@@ -621,10 +619,6 @@ def functional_from_config(cls, config, custom_objects=None):\n \n     input_tensors = map_tensors(functional_config[\"input_layers\"])\n     output_tensors = map_tensors(functional_config[\"output_layers\"])\n-    if isinstance(input_tensors, list) and len(input_tensors) == 1:\n-        input_tensors = input_tensors[0]\n-    if isinstance(output_tensors, list) and len(output_tensors) == 1:\n-        output_tensors = output_tensors[0]\n \n     return cls(\n         inputs=input_tensors,\n\n@@ -11,12 +11,14 @@ from keras.src import layers\n from keras.src import ops\n from keras.src import saving\n from keras.src import testing\n+from keras.src.backend.common.keras_tensor import KerasTensor\n from keras.src.dtype_policies import dtype_policy\n from keras.src.layers.core.input_layer import Input\n from keras.src.layers.input_spec import InputSpec\n from keras.src.models import Functional\n from keras.src.models import Model\n from keras.src.models import Sequential\n+from keras.src.models.model import model_from_json\n \n \n class FunctionalTest(testing.TestCase):\n@@ -273,6 +275,27 @@ class FunctionalTest(testing.TestCase):\n         out_val = model_restored(Input(shape=(3,), batch_size=2))\n         self.assertIsInstance(out_val, out_type)\n \n+    def test_restored_nested_input(self):\n+        input_a = Input(shape=(3,), batch_size=2, name=\"input_a\")\n+        x = layers.Dense(5)(input_a)\n+        outputs = layers.Dense(4)(x)\n+        model = Functional([[input_a]], outputs)\n+\n+        # Serialize and deserialize the model\n+        json_config = model.to_json()\n+        restored_json_config = model_from_json(json_config).to_json()\n+\n+        # Check that the serialized model is the same as the original\n+        self.assertEqual(json_config, restored_json_config)\n+\n+    def test_functional_input_shape_and_type(self):\n+        input = layers.Input((1024, 4))\n+        conv = layers.Conv1D(32, 3)(input)\n+        model = Functional(input, conv)\n+\n+        self.assertIsInstance(model.input, KerasTensor)\n+        self.assertEqual(model.input_shape, (None, 1024, 4))\n+\n     @pytest.mark.requires_trainable_backend\n     def test_layer_getters(self):\n         # Test mixing ops and layers\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ad7bbb8e087e74431c8db537f09c7f207f19d07f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 417 | Lines Deleted: 207 | Files Changed: 25 | Hunks: 130 | Methods Changed: 173 | Complexity Δ (Sum/Max): 28/26 | Churn Δ: 624 | Churn Cumulative: 16807 | Contributors (this commit): 62 | Commits (past 90d): 140 | Contributors (cumulative): 249 | DMM Complexity: 0.05042016806722689\n\nDIFF:\n@@ -133,6 +133,7 @@ from keras.src.ops.numpy import right_shift as right_shift\n from keras.src.ops.numpy import roll as roll\n from keras.src.ops.numpy import rot90 as rot90\n from keras.src.ops.numpy import round as round\n+from keras.src.ops.numpy import searchsorted as searchsorted\n from keras.src.ops.numpy import select as select\n from keras.src.ops.numpy import sign as sign\n from keras.src.ops.numpy import signbit as signbit\n\n@@ -133,6 +133,7 @@ from keras.src.ops.numpy import right_shift as right_shift\n from keras.src.ops.numpy import roll as roll\n from keras.src.ops.numpy import rot90 as rot90\n from keras.src.ops.numpy import round as round\n+from keras.src.ops.numpy import searchsorted as searchsorted\n from keras.src.ops.numpy import select as select\n from keras.src.ops.numpy import sign as sign\n from keras.src.ops.numpy import signbit as signbit\n\n@@ -155,9 +155,9 @@ def log_softmax(x, axis=-1):\n     return jnn.log_softmax(x, axis=axis)\n \n \n-def sparsemax(logits, axis=-1):\n+def sparsemax(x, axis=-1):\n     # Sort logits along the specified axis in descending order\n-    logits = convert_to_tensor(logits)\n+    logits = convert_to_tensor(x)\n     logits_sorted = -1.0 * jnp.sort(logits * -1.0, axis=axis)\n     logits_cumsum = jnp.cumsum(logits_sorted, axis=axis)  # find cumulative sum\n     r = jnp.arange(1, logits.shape[axis] + 1)  # Determine the sparsity\n@@ -250,8 +250,8 @@ def max_pool(\n def average_pool(\n     inputs,\n     pool_size,\n-    strides,\n-    padding,\n+    strides=None,\n+    padding=\"valid\",\n     data_format=None,\n ):\n     data_format = backend.standardize_data_format(data_format)\n@@ -485,7 +485,7 @@ def conv_transpose(\n     )\n \n \n-def one_hot(x, num_classes, axis=-1, dtype=\"float32\", sparse=False):\n+def one_hot(x, num_classes, axis=-1, dtype=None, sparse=False):\n     x = convert_to_tensor(x)\n     if sparse:\n         if axis < 0:\n@@ -513,7 +513,7 @@ def one_hot(x, num_classes, axis=-1, dtype=\"float32\", sparse=False):\n     return jnn.one_hot(x, num_classes, axis=axis, dtype=dtype)\n \n \n-def multi_hot(x, num_classes, axis=-1, dtype=\"float32\", sparse=False):\n+def multi_hot(x, num_classes, axis=-1, dtype=None, sparse=False):\n     x = convert_to_tensor(x)\n     reduction_axis = 1 if len(x.shape) > 1 else 0\n     if sparse:\n\n@@ -630,10 +630,10 @@ def digitize(x, bins):\n     return jnp.digitize(x, bins)\n \n \n-def dot(x, y):\n-    x = convert_to_tensor(x)\n-    y = convert_to_tensor(y)\n-    return jnp.dot(x, y)\n+def dot(x1, x2):\n+    x1 = convert_to_tensor(x1)\n+    x2 = convert_to_tensor(x2)\n+    return jnp.dot(x1, x2)\n \n \n def empty(shape, dtype=None):\n@@ -982,9 +982,9 @@ def ravel(x):\n     return jnp.ravel(x)\n \n \n-def unravel_index(x, shape):\n-    x = convert_to_tensor(x)\n-    return jnp.unravel_index(x, shape)\n+def unravel_index(indices, shape):\n+    indices = convert_to_tensor(indices)\n+    return jnp.unravel_index(indices, shape)\n \n \n @sparse.elementwise_unary(linear=True)\n@@ -1212,7 +1212,7 @@ def vectorize(pyfunc, *, excluded=None, signature=None):\n     return jnp.vectorize(pyfunc, excluded=excluded, signature=signature)\n \n \n-def where(condition, x1, x2):\n+def where(condition, x1=None, x2=None):\n     return jnp.where(condition, x1, x2)\n \n \n@@ -1357,5 +1357,5 @@ def argpartition(x, kth, axis=-1):\n     return jnp.argpartition(x, kth, axis)\n \n \n-def histogram(x, bins, range):\n+def histogram(x, bins=10, range=None):\n     return jnp.histogram(x, bins=bins, range=range)\n\n@@ -343,14 +343,14 @@ def scatter_update(inputs, indices, updates):\n     return inputs\n \n \n-def slice(inputs, start_indices, lengths):\n+def slice(inputs, start_indices, shape):\n     # Validate inputs\n-    assert len(start_indices) == len(lengths)\n+    assert len(start_indices) == len(shape)\n \n     # Generate list of indices arrays for each dimension\n     indices = [\n         np.arange(start, start + length)\n-        for start, length in zip(start_indices, lengths)\n+        for start, length in zip(start_indices, shape)\n     ]\n \n     # Use np.ix_ to create a multidimensional index array\n@@ -407,8 +407,8 @@ def fori_loop(lower, upper, body_fun, init_val):\n     return val\n \n \n-def stop_gradient(x):\n-    return x\n+def stop_gradient(variable):\n+    return variable\n \n \n def unstack(x, num=None, axis=0):\n\n@@ -52,7 +52,7 @@ def segment_max(data, segment_ids, num_segments=None, sorted=False):\n     )\n \n \n-def top_k(x, k, sorted=False):\n+def top_k(x, k, sorted=True):\n     if sorted:\n         # Take the k largest values.\n         sorted_indices = np.argsort(x, axis=-1)[..., ::-1]\n\n@@ -125,11 +125,9 @@ def elu(x, alpha=1.0):\n     )\n \n \n-def selu(\n-    x,\n-    alpha=1.6732632423543772848170429916717,\n-    scale=1.0507009873554804934193349852946,\n-):\n+def selu(x):\n+    alpha = 1.6732632423543772848170429916717\n+    scale = 1.0507009873554804934193349852946\n     x = convert_to_tensor(x)\n     return np.array(scale, x.dtype) * elu(x, alpha)\n \n@@ -196,20 +194,20 @@ def threshold(x, threshold, default_value):\n     return np.where(x > threshold, x, np.array(default_value, dtype=x.dtype))\n \n \n-def softmax(x, axis=None):\n+def softmax(x, axis=-1):\n     exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n     return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n \n \n-def log_softmax(x, axis=None):\n+def log_softmax(x, axis=-1):\n     max_x = np.max(x, axis=axis, keepdims=True)\n     logsumexp = np.log(np.exp(x - max_x).sum(axis=axis, keepdims=True))\n     return x - max_x - logsumexp\n \n \n-def sparsemax(logits, axis=-1):\n+def sparsemax(x, axis=-1):\n     # Sort logits along the specified axis in descending order\n-    logits = convert_to_tensor(logits)\n+    logits = convert_to_tensor(x)\n     logits_sorted = -1.0 * np.sort(-1.0 * logits, axis=axis)\n     logits_cumsum = np.cumsum(logits_sorted, axis=axis)\n     r = np.arange(1, logits.shape[axis] + 1)\n@@ -304,8 +302,8 @@ def max_pool(\n def average_pool(\n     inputs,\n     pool_size,\n-    strides,\n-    padding,\n+    strides=None,\n+    padding=\"valid\",\n     data_format=None,\n ):\n     data_format = backend.standardize_data_format(data_format)\n@@ -543,9 +541,11 @@ def conv_transpose(\n     )\n \n \n-def one_hot(x, num_classes, axis=-1, dtype=\"float32\", sparse=False):\n+def one_hot(x, num_classes, axis=-1, dtype=None, sparse=False):\n     if sparse:\n         raise ValueError(\"Unsupported value `sparse=True` with numpy backend\")\n+    if dtype is None:\n+        dtype = \"float32\"\n     x = convert_to_tensor(x)\n     input_shape = x.shape\n \n@@ -569,7 +569,7 @@ def one_hot(x, num_classes, axis=-1, dtype=\"float32\", sparse=False):\n     return categorical\n \n \n-def multi_hot(x, num_classes, axis=-1, dtype=\"float32\", sparse=False):\n+def multi_hot(x, num_classes, axis=-1, dtype=None, sparse=False):\n     if sparse:\n         raise ValueError(\"Unsupported value `sparse=True` with numpy backend\")\n     x = convert_to_tensor(x)\n\n@@ -173,7 +173,7 @@ def append(x1, x2, axis=None):\n     return np.append(x1, x2, axis=axis)\n \n \n-def arange(start, stop=None, step=None, dtype=None):\n+def arange(start, stop=None, step=1, dtype=None):\n     if dtype is None:\n         dtypes_to_resolve = [\n             getattr(start, \"dtype\", type(start)),\n@@ -537,13 +537,13 @@ def digitize(x, bins):\n     return np.digitize(x, bins).astype(np.int32)\n \n \n-def dot(x, y):\n-    x = convert_to_tensor(x)\n-    y = convert_to_tensor(y)\n-    dtype = dtypes.result_type(x.dtype, y.dtype)\n-    x = x.astype(dtype)\n-    y = y.astype(dtype)\n-    return np.dot(x, y)\n+def dot(x1, x2):\n+    x1 = convert_to_tensor(x1)\n+    x2 = convert_to_tensor(x2)\n+    dtype = dtypes.result_type(x1.dtype, x2.dtype)\n+    x1 = x1.astype(dtype)\n+    x2 = x2.astype(dtype)\n+    return np.dot(x1, x2)\n \n \n def empty(shape, dtype=None):\n@@ -898,10 +898,10 @@ def ravel(x):\n     return np.ravel(x)\n \n \n-def unravel_index(x, shape):\n-    dtype = dtypes.result_type(x.dtype)\n+def unravel_index(indices, shape):\n+    dtype = dtypes.result_type(indices.dtype)\n     return tuple(\n-        indices.astype(dtype) for indices in np.unravel_index(x, shape)\n+        indices.astype(dtype) for indices in np.unravel_index(indices, shape)\n     )\n \n \n@@ -1114,7 +1114,7 @@ def vectorize(pyfunc, *, excluded=None, signature=None):\n     return np.vectorize(pyfunc, excluded=excluded, signature=signature)\n \n \n-def where(condition, x1, x2):\n+def where(condition, x1=None, x2=None):\n     if x1 is not None and x2 is not None:\n         if not isinstance(x1, (int, float)):\n             x1 = convert_to_tensor(x1)\n@@ -1283,5 +1283,5 @@ def argpartition(x, kth, axis=-1):\n     return np.argpartition(x, kth, axis).astype(\"int32\")\n \n \n-def histogram(x, bins, range):\n+def histogram(x, bins=10, range=None):\n     return np.histogram(x, bins=bins, range=range)\n\n@@ -565,21 +565,21 @@ def scatter_update(inputs, indices, updates):\n     )\n \n \n-def slice(inputs, start_indices, lengths):\n+def slice(inputs, start_indices, shape):\n     inputs = get_ov_output(inputs)\n     assert isinstance(start_indices, tuple), (\n         \"`slice` is not supported by openvino backend\"\n-        \" for `start_indices` of type {}\".format(type(lengths))\n+        \" for `start_indices` of type {}\".format(type(shape))\n     )\n-    assert isinstance(lengths, tuple), (\n+    assert isinstance(shape, tuple), (\n         \"`slice` is not supported by openvino backend\"\n-        \" for `lengths` of type {}\".format(type(lengths))\n+        \" for `lengths` of type {}\".format(type(shape))\n     )\n \n     axes = []\n     start = []\n     stop = []\n-    for idx, length in enumerate(lengths):\n+    for idx, length in enumerate(shape):\n         if length is not None and length >= 0:\n             axes.append(idx)\n             start.append(start_indices[idx])\n@@ -621,8 +621,8 @@ def fori_loop(lower, upper, body_fun, init_val):\n     )\n \n \n-def stop_gradient(x):\n-    return x\n+def stop_gradient(variable):\n+    return variable\n \n \n def unstack(x, num=None, axis=0):\n\n@@ -1,4 +1,4 @@\n-def rgb_to_grayscale(image, data_format=\"channels_last\"):\n+def rgb_to_grayscale(images, data_format=None):\n     raise NotImplementedError(\n         \"`rgb_to_grayscale` is not supported with openvino backend\"\n     )\n@@ -19,12 +19,12 @@ def resize(\n \n \n def affine_transform(\n-    image,\n+    images,\n     transform,\n     interpolation=\"bilinear\",\n     fill_mode=\"constant\",\n     fill_value=0,\n-    data_format=\"channels_last\",\n+    data_format=None,\n ):\n     raise NotImplementedError(\n         \"`affine_transform` is not supported with openvino backend\"\n@@ -32,7 +32,7 @@ def affine_transform(\n \n \n def map_coordinates(\n-    input, coordinates, order, fill_mode=\"constant\", fill_value=0.0\n+    inputs, coordinates, order, fill_mode=\"constant\", fill_value=0\n ):\n     raise NotImplementedError(\n         \"`map_coordinates` is not supported with openvino backend\"\n\n@@ -17,7 +17,7 @@ def segment_max(data, segment_ids, num_segments=None, sorted=False):\n     )\n \n \n-def top_k(x, k, sorted=False):\n+def top_k(x, k, sorted=True):\n     raise NotImplementedError(\"`top_k` is not supported with openvino backend\")\n \n \n\n@@ -78,11 +78,9 @@ def elu(x, alpha=1.0):\n     return OpenVINOKerasTensor(ov_opset.elu(x, alpha).output(0))\n \n \n-def selu(\n-    x,\n-    alpha=1.6732632423543772848170429916717,\n-    scale=1.0507009873554804934193349852946,\n-):\n+def selu(x):\n+    alpha = 1.6732632423543772848170429916717\n+    scale = 1.0507009873554804934193349852946\n     x = get_ov_output(x)\n     alpha = get_ov_output(alpha, x.get_element_type())\n     scale = get_ov_output(scale, x.get_element_type())\n@@ -97,7 +95,7 @@ def gelu(x, approximate=True):\n     return OpenVINOKerasTensor(ov_opset.gelu(x, approximate_mode).output(0))\n \n \n-def softmax(x, axis=None):\n+def softmax(x, axis=-1):\n     x = get_ov_output(x)\n     if axis is None:\n         x_shape = ov_opset.shape_of(x)\n@@ -110,7 +108,7 @@ def softmax(x, axis=None):\n     return OpenVINOKerasTensor(ov_opset.softmax(x, axis).output(0))\n \n \n-def log_softmax(x, axis=None):\n+def log_softmax(x, axis=-1):\n     x = get_ov_output(x)\n     if axis is None:\n         x_shape = ov_opset.shape_of(x)\n@@ -138,8 +136,8 @@ def max_pool(\n def average_pool(\n     inputs,\n     pool_size,\n-    strides,\n-    padding,\n+    strides=None,\n+    padding=\"valid\",\n     data_format=None,\n ):\n     raise NotImplementedError(\n@@ -375,13 +373,13 @@ def conv_transpose(\n     )\n \n \n-def one_hot(x, num_classes, axis=-1, dtype=\"float32\", sparse=False):\n+def one_hot(x, num_classes, axis=-1, dtype=None, sparse=False):\n     raise NotImplementedError(\n         \"`one_hot` is not supported with openvino backend\"\n     )\n \n \n-def multi_hot(x, num_classes, axis=-1, dtype=\"float32\", sparse=False):\n+def multi_hot(x, num_classes, axis=-1, dtype=None, sparse=False):\n     raise NotImplementedError(\n         \"`multi_hot` is not supported with openvino backend\"\n     )\n\n@@ -208,7 +208,7 @@ def append(x1, x2, axis=None):\n     return OpenVINOKerasTensor(ov_opset.concat([x1, x2], axis).output(0))\n \n \n-def arange(start, stop=None, step=None, dtype=None):\n+def arange(start, stop=None, step=1, dtype=None):\n     if stop is None:\n         start, stop = get_ov_output(0), get_ov_output(start)\n     else:\n@@ -731,18 +731,18 @@ def digitize(x, bins):\n     )\n \n \n-def dot(x, y):\n+def dot(x1, x2):\n     element_type = None\n-    if isinstance(x, OpenVINOKerasTensor):\n-        element_type = x.output.get_element_type()\n-    if isinstance(y, OpenVINOKerasTensor):\n-        element_type = y.output.get_element_type()\n-    x = get_ov_output(x, element_type)\n-    y = get_ov_output(y, element_type)\n-    x, y = _align_operand_types(x, y, \"dot()\")\n-    if x.get_partial_shape().rank == 0 or y.get_partial_shape().rank == 0:\n-        return OpenVINOKerasTensor(ov_opset.multiply(x, y).output(0))\n-    return OpenVINOKerasTensor(ov_opset.matmul(x, y, False, False).output(0))\n+    if isinstance(x1, OpenVINOKerasTensor):\n+        element_type = x1.output.get_element_type()\n+    if isinstance(x2, OpenVINOKerasTensor):\n+        element_type = x2.output.get_element_type()\n+    x1 = get_ov_output(x1, element_type)\n+    x2 = get_ov_output(x2, element_type)\n+    x1, x2 = _align_operand_types(x1, x2, \"dot()\")\n+    if x1.get_partial_shape().rank == 0 or x2.get_partial_shape().rank == 0:\n+        return OpenVINOKerasTensor(ov_opset.multiply(x1, x2).output(0))\n+    return OpenVINOKerasTensor(ov_opset.matmul(x1, x2, False, False).output(0))\n \n \n def empty(shape, dtype=None):\n@@ -1509,7 +1509,7 @@ def vectorize(pyfunc, *, excluded=None, signature=None):\n     )\n \n \n-def where(condition, x1, x2):\n+def where(condition, x1=None, x2=None):\n     raise NotImplementedError(\"`where` is not supported with openvino backend\")\n \n \n\n@@ -164,9 +164,9 @@ def log_softmax(x, axis=-1):\n     return tf.nn.log_softmax(x, axis=axis)\n \n \n-def sparsemax(logits, axis=-1):\n+def sparsemax(x, axis=-1):\n     # Sort logits along the specified axis in descending order\n-    logits = convert_to_tensor(logits)\n+    logits = convert_to_tensor(x)\n     logits_sorted = tf.sort(logits, direction=\"DESCENDING\", axis=axis)\n     logits_cumsum = tf.cumsum(logits_sorted, axis=axis)\n     r = tf.range(1, tf.shape(logits)[axis] + 1, dtype=logits.dtype)\n@@ -505,7 +505,7 @@ def conv_transpose(\n     )\n \n \n-def one_hot(x, num_classes, axis=-1, dtype=\"float32\", sparse=False):\n+def one_hot(x, num_classes, axis=-1, dtype=None, sparse=False):\n     x = convert_to_tensor(x, dtype=\"int64\")\n     if dtype is None:\n         dtype = \"float32\"\n@@ -541,7 +541,7 @@ def one_hot(x, num_classes, axis=-1, dtype=\"float32\", sparse=False):\n     )\n \n \n-def multi_hot(x, num_classes, axis=-1, dtype=\"float32\", sparse=False):\n+def multi_hot(x, num_classes, axis=-1, dtype=None, sparse=False):\n     reduction_axis = 1 if len(x.shape) > 1 else 0\n     if backend.standardize_dtype(dtype) == \"bool\":\n         if sparse:\n@@ -886,13 +886,7 @@ def batch_normalization(\n     )\n \n \n-def ctc_loss(\n-    target,\n-    output,\n-    target_length,\n-    output_length,\n-    mask_index=0,\n-):\n+def ctc_loss(target, output, target_length, output_length, mask_index=0):\n     target = convert_to_tensor(target)\n     output = convert_to_tensor(output)\n     target = tf.cast(target, dtype=\"int32\")\n\n@@ -1364,23 +1364,23 @@ def digitize(x, bins):\n     return tf.raw_ops.Bucketize(input=x, boundaries=bins)\n \n \n-def dot(x, y):\n-    x = convert_to_tensor(x)\n-    y = convert_to_tensor(y)\n-    result_dtype = dtypes.result_type(x.dtype, y.dtype)\n+def dot(x1, x2):\n+    x1 = convert_to_tensor(x1)\n+    x2 = convert_to_tensor(x2)\n+    result_dtype = dtypes.result_type(x1.dtype, x2.dtype)\n     # GPU only supports float types\n     compute_dtype = dtypes.result_type(result_dtype, float)\n-    x = tf.cast(x, compute_dtype)\n-    y = tf.cast(y, compute_dtype)\n+    x1 = tf.cast(x1, compute_dtype)\n+    x2 = tf.cast(x2, compute_dtype)\n \n-    x_shape = x.shape\n-    y_shape = y.shape\n+    x_shape = x1.shape\n+    y_shape = x2.shape\n     if x_shape.rank == 0 or y_shape.rank == 0:\n-        output = x * y\n+        output = x1 * x2\n     elif y_shape.rank == 1:\n-        output = tf.tensordot(x, y, axes=[[-1], [-1]])\n+        output = tf.tensordot(x1, x2, axes=[[-1], [-1]])\n     else:\n-        output = tf.tensordot(x, y, axes=[[-1], [-2]])\n+        output = tf.tensordot(x1, x2, axes=[[-1], [-2]])\n     return tf.cast(output, result_dtype)\n \n \n@@ -2018,27 +2018,29 @@ def ravel(x):\n     return tf.reshape(x, [-1])\n \n \n-def unravel_index(x, shape):\n-    x = tf.convert_to_tensor(x)\n-    input_dtype = x.dtype\n+def unravel_index(indices, shape):\n+    indices = tf.convert_to_tensor(indices)\n+    input_dtype = indices.dtype\n \n     if None in shape:\n         raise ValueError(\n             f\"`shape` argument cannot contain `None`. Received: shape={shape}\"\n         )\n \n-    if x.ndim == 1:\n+    if indices.ndim == 1:\n         coords = []\n         for dim in reversed(shape):\n-            coords.append(tf.cast(x % dim, input_dtype))\n-            x = x // dim\n+            coords.append(tf.cast(indices % dim, input_dtype))\n+            indices = indices // dim\n         return tuple(reversed(coords))\n \n-    x_shape = x.shape\n+    indices_shape = indices.shape\n     coords = []\n     for dim in shape:\n-        coords.append(tf.reshape(tf.cast(x % dim, input_dtype), x_shape))\n-        x = x // dim\n+        coords.append(\n+            tf.reshape(tf.cast(indices % dim, input_dtype), indices_shape)\n+        )\n+        indices = indices // dim\n \n     return tuple(reversed(coords))\n \n@@ -2587,7 +2589,7 @@ def vectorize(pyfunc, *, excluded=None, signature=None):\n     )\n \n \n-def where(condition, x1, x2):\n+def where(condition, x1=None, x2=None):\n     condition = tf.cast(condition, \"bool\")\n     if x1 is not None and x2 is not None:\n         if not isinstance(x1, (int, float)):\n@@ -2856,7 +2858,7 @@ def argpartition(x, kth, axis=-1):\n     return swapaxes(out, -1, axis)\n \n \n-def histogram(x, bins, range):\n+def histogram(x, bins=10, range=None):\n     \"\"\"Computes a histogram of the data tensor `x`.\n \n     Note: the `tf.histogram_fixed_width()` and\n\n@@ -52,13 +52,13 @@ def _segment_reduction_fn(data, segment_ids, reduction_method, num_segments):\n     return result.type(data.dtype)\n \n \n-def segment_sum(data, segment_ids, num_segments=None, **kwargs):\n+def segment_sum(data, segment_ids, num_segments=None, sorted=False):\n     data = convert_to_tensor(data)\n     segment_ids = convert_to_tensor(segment_ids)\n     return _segment_reduction_fn(data, segment_ids, \"sum\", num_segments)\n \n \n-def segment_max(data, segment_ids, num_segments=None, **kwargs):\n+def segment_max(data, segment_ids, num_segments=None, sorted=False):\n     data = convert_to_tensor(data)\n     segment_ids = convert_to_tensor(segment_ids)\n     return _segment_reduction_fn(data, segment_ids, \"amax\", num_segments)\n\n@@ -191,9 +191,9 @@ def log_softmax(x, axis=-1):\n     return cast(output, dtype)\n \n \n-def sparsemax(logits, axis=-1):\n+def sparsemax(x, axis=-1):\n     # Sort logits along the specified axis in descending order\n-    logits = convert_to_tensor(logits)\n+    logits = convert_to_tensor(x)\n     logits_sorted, _ = torch.sort(logits, dim=axis, descending=True)\n     logits_cumsum = torch.cumsum(logits_sorted, dim=axis)\n     r = torch.arange(\n@@ -656,7 +656,7 @@ def conv_transpose(\n     return outputs\n \n \n-def one_hot(x, num_classes, axis=-1, dtype=\"float32\", sparse=False):\n+def one_hot(x, num_classes, axis=-1, dtype=None, sparse=False):\n     if sparse:\n         raise ValueError(\"Unsupported value `sparse=True` with torch backend\")\n     # Axis is the output axis. By default, PyTorch, outputs to last axis.\n@@ -682,7 +682,7 @@ def one_hot(x, num_classes, axis=-1, dtype=\"float32\", sparse=False):\n     return output\n \n \n-def multi_hot(x, num_classes, axis=-1, dtype=\"float32\", sparse=False):\n+def multi_hot(x, num_classes, axis=-1, dtype=None, sparse=False):\n     if sparse:\n         raise ValueError(\"Unsupported value `sparse=True` with torch backend\")\n     x = convert_to_tensor(x)\n@@ -848,13 +848,7 @@ def batch_normalization(\n     )\n \n \n-def ctc_loss(\n-    target,\n-    output,\n-    target_length,\n-    output_length,\n-    mask_index=0,\n-):\n+def ctc_loss(target, output, target_length, output_length, mask_index=0):\n     target = convert_to_tensor(target)\n     output = convert_to_tensor(output)\n     target_length = convert_to_tensor(target_length)\n\n@@ -614,7 +614,7 @@ def count_nonzero(x, axis=None):\n     return cast(torch.count_nonzero(x, dim=axis).T, \"int32\")\n \n \n-def cross(x1, x2, axisa=-1, axisb=-1, axisc=-1, axis=-1):\n+def cross(x1, x2, axisa=-1, axisb=-1, axisc=-1, axis=None):\n     if axisa != -1 or axisb != -1 or axisc != -1:\n         raise ValueError(\n             \"Torch backend does not support `axisa`, `axisb`, or `axisc`. \"\n@@ -703,10 +703,10 @@ def digitize(x, bins):\n     return cast(torch.bucketize(x, bins, right=True), \"int32\")\n \n \n-def dot(x, y):\n-    x = convert_to_tensor(x)\n-    y = convert_to_tensor(y)\n-    result_dtype = dtypes.result_type(x.dtype, y.dtype)\n+def dot(x1, x2):\n+    x1 = convert_to_tensor(x1)\n+    x2 = convert_to_tensor(x2)\n+    result_dtype = dtypes.result_type(x1.dtype, x2.dtype)\n     # GPU only supports float types\n     compute_dtype = dtypes.result_type(result_dtype, float)\n \n@@ -714,11 +714,11 @@ def dot(x, y):\n     if get_device() == \"cpu\" and compute_dtype == \"float16\":\n         compute_dtype = \"float32\"\n \n-    x = cast(x, compute_dtype)\n-    y = cast(y, compute_dtype)\n-    if x.ndim == 0 or y.ndim == 0:\n-        return cast(torch.multiply(x, y), result_dtype)\n-    return cast(torch.matmul(x, y), result_dtype)\n+    x1 = cast(x1, compute_dtype)\n+    x2 = cast(x2, compute_dtype)\n+    if x1.ndim == 0 or x2.ndim == 0:\n+        return cast(torch.multiply(x1, x2), result_dtype)\n+    return cast(torch.matmul(x1, x2), result_dtype)\n \n \n def empty(shape, dtype=None):\n@@ -1291,10 +1291,12 @@ def ravel(x):\n     return torch.ravel(x)\n \n \n-def unravel_index(x, shape):\n-    x = convert_to_tensor(x)\n-    dtype = dtypes.result_type(x.dtype)\n-    return tuple(cast(idx, dtype) for idx in torch.unravel_index(x, shape))\n+def unravel_index(indices, shape):\n+    indices = convert_to_tensor(indices)\n+    dtype = dtypes.result_type(indices.dtype)\n+    return tuple(\n+        cast(idx, dtype) for idx in torch.unravel_index(indices, shape)\n+    )\n \n \n def real(x):\n@@ -1528,7 +1530,7 @@ def tile(x, repeats):\n     return torch.tile(x, dims=repeats)\n \n \n-def trace(x, offset=None, axis1=None, axis2=None):\n+def trace(x, offset=0, axis1=0, axis2=1):\n     x = convert_to_tensor(x)\n     dtype = standardize_dtype(x.dtype)\n     if dtype != \"int64\":\n@@ -1605,7 +1607,7 @@ def vectorize(pyfunc, *, excluded=None, signature=None):\n     )\n \n \n-def where(condition, x1, x2):\n+def where(condition, x1=None, x2=None):\n     condition = convert_to_tensor(condition, dtype=bool)\n     if x1 is not None and x2 is not None:\n         x1 = convert_to_tensor(x1)\n@@ -1704,7 +1706,7 @@ def sum(x, axis=None, keepdims=False):\n     return cast(torch.sum(x), dtype)\n \n \n-def eye(N, M=None, k=None, dtype=None):\n+def eye(N, M=None, k=0, dtype=None):\n     dtype = to_torch_dtype(dtype or config.floatx())\n     M = N if M is None else M\n     k = 0 if k is None else k\n@@ -1818,6 +1820,6 @@ def argpartition(x, kth, axis=-1):\n     return cast(torch.transpose(out, -1, axis), \"int32\")\n \n \n-def histogram(x, bins, range):\n+def histogram(x, bins=10, range=None):\n     hist_result = torch.histogram(x, bins=bins, range=range)\n     return hist_result.hist, hist_result.bin_edges\n\n@@ -83,12 +83,12 @@ class Scan(Operation):\n         self.reverse = reverse\n         self.unroll = unroll\n \n-    def call(self, f, init, xs, length):\n+    def call(self, f, init, xs=None, length=None):\n         return backend.core.scan(\n             f, init, xs, length, reverse=self.reverse, unroll=self.unroll\n         )\n \n-    def compute_output_spec(self, f, init, xs, length):\n+    def compute_output_spec(self, f, init, xs=None, length=None):\n         if xs is None:\n             n = int(length)\n             x = None\n@@ -185,18 +185,19 @@ def scan(f, init, xs=None, length=None, reverse=False, unroll=1):\n \n \n class AssociativeScan(Operation):\n-    def __init__(self, reverse=False):\n+    def __init__(self, reverse=False, axis=0):\n         super().__init__()\n         self.reverse = reverse\n+        self.axis = axis\n \n-    def call(self, f, elems, axis=0):\n+    def call(self, f, elems):\n         return backend.core.associative_scan(\n-            f, elems, reverse=self.reverse, axis=axis\n+            f, elems, reverse=self.reverse, axis=self.axis\n         )\n \n-    def compute_output_spec(self, f, elems, axis):\n+    def compute_output_spec(self, f, elems):\n         elems_flat = tree.flatten(elems)\n-        lens = [elem.shape[axis] for elem in elems_flat]\n+        lens = [elem.shape[self.axis] for elem in elems_flat]\n         if len(set(lens)) != 1:\n             raise ValueError(\n                 \"Array inputs to associative_scan must have the same \"\n@@ -206,7 +207,8 @@ class AssociativeScan(Operation):\n             )\n \n         x = tree.pack_sequence_as(\n-            elems, [slice_along_axis(x, 0, 1, axis=axis) for x in elems_flat]\n+            elems,\n+            [slice_along_axis(x, 0, 1, axis=self.axis) for x in elems_flat],\n         )\n         y_spec = backend.compute_output_spec(f, x, x)\n \n@@ -274,7 +276,9 @@ def associative_scan(f, elems, reverse=False, axis=0):\n     [[1, 3], [1, 3], [1, 3]]\n     \"\"\"\n     if any_symbolic_tensors((elems,)):\n-        return AssociativeScan(reverse=reverse).symbolic_call(f, elems, axis)\n+        return AssociativeScan(reverse=reverse, axis=axis).symbolic_call(\n+            f, elems\n+        )\n     return backend.core.associative_scan(f, elems, reverse=reverse, axis=axis)\n \n \n@@ -512,7 +516,7 @@ def switch(index, branches, *operands):\n \n \n class WhileLoop(Operation):\n-    def __init__(self, cond, body, maximum_iterations):\n+    def __init__(self, cond, body, maximum_iterations=None):\n         super().__init__()\n         self.cond = cond\n         self.body = body\n@@ -910,14 +914,15 @@ def _saturate_cast(x, dtype, backend_module=None):\n \n \n class ConvertToTensor(Operation):\n-    def __init__(self, dtype, sparse):\n+    def __init__(self, dtype=None, sparse=None, ragged=None):\n         super().__init__()\n         self.dtype = backend.standardize_dtype(dtype)\n         self.sparse = sparse\n+        self.ragged = ragged\n \n     def call(self, x):\n         return backend.core.convert_to_tensor(\n-            x, dtype=self.dtype, sparse=self.sparse\n+            x, dtype=self.dtype, sparse=self.sparse, ragged=self.ragged\n         )\n \n     def compute_output_spec(self, x):\n@@ -925,7 +930,12 @@ class ConvertToTensor(Operation):\n         sparse = (\n             False if self.sparse is not None and not self.sparse else x.sparse\n         )\n-        return backend.KerasTensor(shape=x.shape, dtype=dtype, sparse=sparse)\n+        ragged = (\n+            False if self.ragged is not None and not self.ragged else x.ragged\n+        )\n+        return backend.KerasTensor(\n+            shape=x.shape, dtype=dtype, sparse=sparse, ragged=ragged\n+        )\n \n \n @keras_export(\"keras.ops.convert_to_tensor\")\n@@ -954,7 +964,7 @@ def convert_to_tensor(x, dtype=None, sparse=None, ragged=None):\n     >>> y = keras.ops.convert_to_tensor(x)\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n-        return ConvertToTensor(dtype=dtype, sparse=sparse)(x)\n+        return ConvertToTensor(dtype=dtype, sparse=sparse, ragged=ragged)(x)\n     return backend.core.convert_to_tensor(\n         x, dtype=dtype, sparse=sparse, ragged=ragged\n     )\n\n@@ -1007,12 +1007,12 @@ def _pad_images(\n class CropImages(Operation):\n     def __init__(\n         self,\n-        top_cropping,\n-        left_cropping,\n-        bottom_cropping,\n-        right_cropping,\n-        target_height,\n-        target_width,\n+        top_cropping=None,\n+        left_cropping=None,\n+        bottom_cropping=None,\n+        right_cropping=None,\n+        target_height=None,\n+        target_width=None,\n         data_format=None,\n     ):\n         super().__init__()\n\n@@ -134,7 +134,7 @@ def segment_max(data, segment_ids, num_segments=None, sorted=False):\n \n \n class TopK(Operation):\n-    def __init__(self, k, sorted=False):\n+    def __init__(self, k, sorted=True):\n         super().__init__()\n         self.k = k\n         self.sorted = sorted\n@@ -328,10 +328,6 @@ def extract_sequences(x, sequence_length, sequence_stride):\n \n \n class FFT(Operation):\n-    def __init__(self, axis=-1):\n-        super().__init__()\n-        self.axis = axis\n-\n     def compute_output_spec(self, x):\n         if not isinstance(x, (tuple, list)) or len(x) != 2:\n             raise ValueError(\n@@ -360,7 +356,7 @@ class FFT(Operation):\n         m = real.shape[-1]\n         if m is None:\n             raise ValueError(\n-                f\"Input should have its {self.axis}th axis fully-defined. \"\n+                f\"Input should have its last dimension fully-defined. \"\n                 f\"Received: input.shape = {real.shape}\"\n             )\n \n\n@@ -1160,14 +1160,10 @@ class FFTTest(testing.TestCase):\n         real = KerasTensor(shape=(None,), dtype=\"float32\")\n         imag = KerasTensor(shape=(None,), dtype=\"float32\")\n         with self.assertRaisesRegex(\n-            ValueError, \"Input should have its -1th axis fully-defined\"\n+            ValueError, \"Input should have its last dimension fully-defined\"\n         ):\n             fft_op.compute_output_spec((real, imag))\n \n-    def test_fft_init_default_axis(self):\n-        fft_op = kmath.FFT()\n-        self.assertEqual(fft_op.axis, -1, \"Default axis should be -1\")\n-\n \n class FFT2Test(testing.TestCase):\n     def test_fft2_correct_input(self):\n\n@@ -857,13 +857,13 @@ def hard_shrink(x, threshold=0.5):\n \n \n class Threshold(Operation):\n-    def __init__(self, threshold_value, value):\n+    def __init__(self, threshold, default_value):\n         super().__init__()\n-        self.threshold_value = threshold_value\n-        self.value = value\n+        self.threshold = threshold\n+        self.default_value = default_value\n \n     def call(self, x):\n-        return backend.nn.threshold(x, self.threshold_value, self.value)\n+        return backend.nn.threshold(x, self.threshold, self.default_value)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape, dtype=x.dtype)\n@@ -1569,7 +1569,7 @@ def separable_conv(\n class ConvTranspose(Operation):\n     def __init__(\n         self,\n-        strides,\n+        strides=1,\n         padding=\"valid\",\n         output_padding=None,\n         data_format=None,\n@@ -1622,7 +1622,7 @@ class ConvTranspose(Operation):\n def conv_transpose(\n     inputs,\n     kernel,\n-    strides,\n+    strides=1,\n     padding=\"valid\",\n     output_padding=None,\n     data_format=None,\n@@ -2161,11 +2161,22 @@ def moments(x, axes, keepdims=False, synchronized=False):\n \n \n class BatchNorm(Operation):\n-    def __init__(self, axis, epsilon):\n+    def __init__(self, axis, epsilon=1e-3):\n         super().__init__()\n         self.axis = axis\n         self.epsilon = epsilon\n \n+    def call(self, x, mean, variance, offset=None, scale=None):\n+        return backend.nn.batch_normalization(\n+            x,\n+            mean,\n+            variance,\n+            axis=self.axis,\n+            offset=offset,\n+            scale=scale,\n+            epsilon=self.epsilon,\n+        )\n+\n     def _check_shape(self, name, shape, expected_shape):\n         if shape != expected_shape:\n             raise ValueError(\n@@ -2716,7 +2727,7 @@ def dot_product_attention(\n \n \n class RMSNorm(Operation):\n-    def __init__(self, scale, axis=-1, epsilon=None):\n+    def __init__(self, scale=1, axis=-1, epsilon=None):\n         super().__init__()\n         self.axis = axis\n         self.scale = scale\n@@ -2942,8 +2953,8 @@ class Polar(Operation):\n     def compute_output_spec(self, abs_, angle):\n         return KerasTensor(shape=abs_.shape)\n \n-    def call(self, x):\n-        return _polar(x)\n+    def call(self, abs_, angle):\n+        return _polar(abs_, angle)\n \n \n @keras_export([\"keras.ops.polar\", \"keras.ops.nn.polar\"])\n\n@@ -1207,7 +1207,7 @@ def average(x, axis=None, weights=None):\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Average(axis=axis).symbolic_call(x, weights=weights)\n-    return backend.numpy.average(x, weights=weights, axis=axis)\n+    return backend.numpy.average(x, axis=axis, weights=weights)\n \n \n class Bartlett(Operation):\n@@ -2661,8 +2661,8 @@ class Einsum(Operation):\n         super().__init__()\n         self.subscripts = subscripts\n \n-    def call(self, *operands):\n-        return backend.numpy.einsum(self.subscripts, *operands)\n+    def call(self, *operands, **kwargs):\n+        return backend.numpy.einsum(self.subscripts, *operands, **kwargs)\n \n     def compute_output_spec(self, *operands):\n         \"\"\"Compute the output shape of `einsum`.\n@@ -2836,7 +2836,7 @@ class Einsum(Operation):\n \n \n @keras_export([\"keras.ops.einsum\", \"keras.ops.numpy.einsum\"])\n-def einsum(subscripts, *operands):\n+def einsum(subscripts, *operands, **kwargs):\n     \"\"\"Evaluates the Einstein summation convention on the operands.\n \n     Args:\n@@ -2920,8 +2920,8 @@ def einsum(subscripts, *operands):\n     array([ 30,  80, 130, 180, 230])\n     \"\"\"\n     if any_symbolic_tensors(operands):\n-        return Einsum(subscripts).symbolic_call(*operands)\n-    return backend.numpy.einsum(subscripts, *operands)\n+        return Einsum(subscripts).symbolic_call(*operands, **kwargs)\n+    return backend.numpy.einsum(subscripts, *operands, **kwargs)\n \n \n class Empty(Operation):\n@@ -3611,7 +3611,7 @@ def less_equal(x1, x2):\n \n class Linspace(Operation):\n     def __init__(\n-        self, num=50, endpoint=True, retstep=False, dtype=float, axis=0\n+        self, num=50, endpoint=True, retstep=False, dtype=None, axis=0\n     ):\n         super().__init__()\n         self.num = num\n@@ -3957,7 +3957,7 @@ def logical_or(x1, x2):\n \n \n class Logspace(Operation):\n-    def __init__(self, num=50, endpoint=True, base=10, dtype=float, axis=0):\n+    def __init__(self, num=50, endpoint=True, base=10, dtype=None, axis=0):\n         super().__init__()\n         self.num = num\n         self.endpoint = endpoint\n@@ -5226,7 +5226,7 @@ class SearchSorted(Operation):\n         return KerasTensor(values.shape, dtype=out_type)\n \n \n-@keras_export([\"keras.ops.searchsorted\"])\n+@keras_export([\"keras.ops.searchsorted\", \"keras.ops.numpy.searchsorted\"])\n def searchsorted(sorted_sequence, values, side=\"left\"):\n     \"\"\"Perform a binary search, returning indices for insertion of `values`\n     into `sorted_sequence` that maintain the sorting order.\n@@ -5484,22 +5484,22 @@ class Stack(Operation):\n         super().__init__()\n         self.axis = axis\n \n-    def call(self, xs):\n-        return backend.numpy.stack(xs, axis=self.axis)\n+    def call(self, x):\n+        return backend.numpy.stack(x, axis=self.axis)\n \n-    def compute_output_spec(self, xs):\n-        first_shape = xs[0].shape\n+    def compute_output_spec(self, x):\n+        first_shape = x[0].shape\n         dtypes_to_resolve = []\n-        for x in xs:\n-            if not shape_equal(x.shape, first_shape, axis=[], allow_none=True):\n+        for a in x:\n+            if not shape_equal(a.shape, first_shape, axis=[], allow_none=True):\n                 raise ValueError(\n-                    \"Every value in `xs` must have the same shape. But found \"\n-                    f\"element of shape {x.shape},  which is different from the \"\n+                    \"Every value in `x` must have the same shape. But found \"\n+                    f\"element of shape {a.shape},  which is different from the \"\n                     f\"first element's shape {first_shape}.\"\n                 )\n-            dtypes_to_resolve.append(getattr(x, \"dtype\", type(x)))\n+            dtypes_to_resolve.append(getattr(a, \"dtype\", type(a)))\n \n-        size_on_axis = len(xs)\n+        size_on_axis = len(x)\n         output_shape = list(first_shape)\n         if self.axis == -1:\n             output_shape = output_shape + [size_on_axis]\n\n@@ -0,0 +1,205 @@\n+import inspect\n+\n+from absl.testing import parameterized\n+\n+from keras.api import ops as api_ops_root\n+from keras.src import backend\n+from keras.src import ops\n+from keras.src import testing\n+from keras.src.ops.operation import Operation\n+from keras.src.testing.test_utils import named_product\n+from keras.src.utils.naming import to_snake_case\n+\n+OPS_MODULES = (\"core\", \"image\", \"linalg\", \"math\", \"nn\", \"numpy\")\n+\n+\n+def op_functions_and_classes(ops_module):\n+    \"\"\"Enumerate pairs of op function and op classes in a module.\n+\n+    Will return for instance `(ExpandDims, expand_dims)`, `(Sum, sum)`, ...\n+\n+    Args:\n+        ops_module: the module to explore.\n+\n+    Returns:\n+        iterable returning tuples with function and class pairs.\n+    \"\"\"\n+    # Go through all symbols.\n+    for op_class_name in dir(ops_module):\n+        op_class = getattr(ops_module, op_class_name)\n+        # Find the ones that are classes that extend `Operation`.\n+        if isinstance(op_class, type) and Operation in op_class.__mro__:\n+            # Infer what the corresponding op function name should be.\n+            op_function_name = to_snake_case(op_class_name)\n+            # With some exceptions.\n+            op_function_name = {\n+                \"batch_norm\": \"batch_normalization\",\n+                \"rms_norm\": \"rms_normalization\",\n+                \"search_sorted\": \"searchsorted\",\n+            }.get(op_function_name, op_function_name)\n+            # Check if that function exist. Some classes are abstract super\n+            # classes for multiple operations and should be ignored.\n+            op_function = getattr(ops_module, op_function_name, None)\n+            if op_function is not None:\n+                # We have a pair, return it.\n+                yield op_function, op_class\n+\n+\n+class OperationTest(testing.TestCase):\n+    @parameterized.named_parameters(named_product(module_name=OPS_MODULES))\n+    def test_class_function_consistency(self, module_name):\n+        ops_module = getattr(ops, module_name)\n+        if module_name in (\"core\", \"math\"):\n+            # `core` and `math` are not exported as their own module.\n+            api_ops_module = None\n+        else:\n+            api_ops_module = getattr(api_ops_root, module_name)\n+\n+        for op_function, op_class in op_functions_and_classes(ops_module):\n+            name = op_function.__name__\n+\n+            # ==== Check exports ====\n+            # - op should be exported as e.g. `keras.ops.numpy.sum`\n+            # - op should also be exported as e.g. `keras.ops.sum`\n+\n+            if module_name != \"image\":\n+                # `image` ops are not exported at the top-level.\n+                self.assertIsNotNone(\n+                    getattr(api_ops_root, name, None),\n+                    f\"Not exported as `keras.ops.{name}`\",\n+                )\n+            if api_ops_module is not None:\n+                # `core` and `math` are not exported as their own module.\n+                self.assertIsNotNone(\n+                    getattr(api_ops_module, name, None),\n+                    f\"Not exported as `keras.ops.{module_name}.{name}`\",\n+                )\n+\n+            # ==== Check static parameters ====\n+            # Static parameters are declared in the class' `__init__`.\n+            # Dynamic parameters are declared in the class' `call` method.\n+            # - they should all appear in the op signature with the same name\n+            # - they should have the same default value\n+            # - they should appear in the same order and usually with the\n+            #   dynamic parameters first, and the static parameters last.\n+\n+            dynamic_parameters = list(\n+                inspect.signature(op_class.call).parameters.values()\n+            )[1:]  # Remove self\n+\n+            if op_class.__init__ is Operation.__init__:\n+                # This op class has no static parameters. Do not use the `name`\n+                # and `dtype` parameters from the `__init__` of `Operation`.\n+                static_parameters = []\n+            else:\n+                class_init_signature = inspect.signature(op_class.__init__)\n+                static_parameters = list(\n+                    class_init_signature.parameters.values()\n+                )[1:]  # Remove self\n+\n+            op_signature = inspect.signature(op_function)\n+\n+            for p in dynamic_parameters + static_parameters:\n+                # Check the same name appeas in the op signature\n+                self.assertIn(\n+                    p.name,\n+                    op_signature.parameters,\n+                    f\"Op function `{name}` is missing a parameter that is in \"\n+                    f\"op class `{op_class.__name__}`\",\n+                )\n+                # Check default values are the same\n+                self.assertEqual(\n+                    p.default,\n+                    op_signature.parameters[p.name].default,\n+                    f\"Default mismatch for parameter `{p.name}` between op \"\n+                    f\"function `{name}` and op class `{op_class.__name__}`\",\n+                )\n+\n+            # Check order of parameters.\n+            dynamic_parameter_names = [p.name for p in dynamic_parameters]\n+            static_parameter_names = [p.name for p in static_parameters]\n+\n+            if name in (\n+                \"fori_loop\",\n+                \"while_loop\",\n+                \"batch_normalization\",\n+                \"dot_product_attention\",\n+                \"average\",\n+                \"einsum\",\n+                \"pad\",\n+            ):\n+                # Loose case:\n+                # order of of parameters is preserved but they are interspersed.\n+                op_dynamic_parameter_names = [\n+                    name\n+                    for name in op_signature.parameters.keys()\n+                    if name in dynamic_parameter_names\n+                ]\n+                self.assertEqual(\n+                    op_dynamic_parameter_names,\n+                    dynamic_parameter_names,\n+                    \"Inconsistent dynamic parameter order for op \"\n+                    f\"function `{name}` and op class `{op_class.__name__}`\",\n+                )\n+                op_static_parameter_names = [\n+                    name\n+                    for name in op_signature.parameters.keys()\n+                    if name in static_parameter_names\n+                ]\n+                self.assertEqual(\n+                    op_static_parameter_names,\n+                    static_parameter_names,\n+                    \"Inconsistent static parameter order for op \"\n+                    f\"function `{name}` and op class `{op_class.__name__}`\",\n+                )\n+            else:\n+                # Strict case:\n+                # dynamic parameters first and static parameters at the end.\n+                self.assertEqual(\n+                    list(op_signature.parameters.keys()),\n+                    dynamic_parameter_names + static_parameter_names,\n+                    \"Inconsistent static parameter position for op \"\n+                    f\"function `{name}` and op class `{op_class.__name__}`\",\n+                )\n+\n+    @parameterized.named_parameters(named_product(module_name=OPS_MODULES))\n+    def test_backend_consistency(self, module_name):\n+        ops_module = getattr(ops, module_name)\n+        backend_ops_module = getattr(backend, module_name)\n+\n+        for op_function, _ in op_functions_and_classes(ops_module):\n+            name = op_function.__name__\n+\n+            if hasattr(ops_module, \"_\" + name):\n+                # For an op function `foo`, if there is a function named `_foo`,\n+                # that means we have a backend independent implementation.\n+                continue\n+            if name in (\"view_as_complex\", \"view_as_real\", \"get_item\"):\n+                # These ops have an inlined backend independent implementation.\n+                continue\n+\n+            # ==== Check backend implementation ====\n+            # - op should have an implementation in every backend\n+            # - op implementation should have the same signature (same\n+            #   parameters, same order, same defaults)\n+\n+            backend_op_function = getattr(backend_ops_module, name, None)\n+\n+            if backend.backend() == \"openvino\" and backend_op_function is None:\n+                # Openvino is still missing a number of ops.\n+                continue\n+\n+            self.assertIsNotNone(backend_op_function, f\"Missing op `{name}`\")\n+\n+            if name == \"multi_hot\":\n+                # multi_hot has code to massage the input parameters before\n+                # calling the backend implementation, so the signature is\n+                # different on purpose.\n+                continue\n+\n+            # Signature should match in every way.\n+            self.assertEqual(\n+                inspect.signature(backend_op_function),\n+                inspect.signature(op_function),\n+                f\"Signature mismatch for `{name}`\",\n+            )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
