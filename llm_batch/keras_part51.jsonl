{"custom_id": "keras#5c3aea2202b198ba94f83335f90427034585ccbd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 39 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -4,6 +4,10 @@ import os\n import json\n from .common import epsilon, floatx, set_epsilon, set_floatx\n \n+_keras_dir = os.path.expanduser(os.path.join('~', '.keras'))\n+if not os.path.exists(_keras_dir):\n+    os.makedirs(_keras_dir)\n+\n _BACKEND = 'theano'\n _config_path = os.path.expanduser(os.path.join('~', '.keras', 'keras.json'))\n if os.path.exists(_config_path):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ef3ef71ee6836a69201f908c9ea907f08151bfef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 5 | Churn Cumulative: 411 | Contributors (this commit): 6 | Commits (past 90d): 6 | Contributors (cumulative): 7 | DMM Complexity: 0.75\n\nDIFF:\n@@ -26,6 +26,7 @@ def get_file(fname, origin, untar=False):\n \n     if not os.path.exists(fpath):\n         print('Downloading data from',  origin)\n+        global progbar\n         progbar = None\n \n         def dl_progress(count, block_size, total_size):\n\n@@ -1,6 +1,7 @@\n import numpy as np\n from scipy.linalg import circulant\n \n+from .. import backend as K\n import theano\n import theano.tensor as T\n floatX = theano.config.floatX\n@@ -119,6 +120,9 @@ class NeuralTuringMachine(Recurrent):\n                  inner_rnn='gru', truncate_gradient=-1, return_sequences=False,\n                  init='glorot_uniform', inner_init='orthogonal',\n                  input_dim=None, input_length=None, **kwargs):\n+        if K._BACKEND != 'theano':\n+            raise Exception('NeuralTuringMachine is only available for Theano for the time being. ' +\n+                            'It will be adapted to TensorFlow soon.')\n         self.output_dim = output_dim\n         self.n_slots = n_slots\n         self.m_length = m_length\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#36392c75d74ab575bd986fcd9ee71078eff13b1e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 335 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -4,6 +4,7 @@ from numpy.testing import assert_allclose\n import numpy as np\n import pytest\n \n+if sys.version_info.major == 2:\n     from keras.backend import theano_backend as KTH\n     from keras.backend import tensorflow_backend as KTF\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7ecd6c3c5f4d3008b197a40e89681455bf3d796c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 337 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -38,7 +38,7 @@ def check_two_tensor_operation(function_name, x_input_shape,\n     assert_allclose(zth, ztf, atol=1e-06)\n \n \n-@pytest.mark.skipif(sys.version_info < (2, 7), reason=\"Requires Python 2.7\")\n+@pytest.mark.skipif(sys.version_info.major != 2, reason=\"Requires Python 2.7\")\n class TestBackend(unittest.TestCase):\n \n     def test_linear_operations(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cbee000b66b747e09ea6521ff08fcdfdaecead6c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 9 | Files Changed: 4 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 22 | Churn Cumulative: 1943 | Contributors (this commit): 18 | Commits (past 90d): 41 | Contributors (cumulative): 23 | DMM Complexity: 1.0\n\nDIFF:\n@@ -41,7 +41,7 @@ y_test = np.array(y_test)\n \n print('Build model...')\n model = Graph()\n-model.add_input(name='input', input_shape=(1,), dtype=int)\n+model.add_input(name='input', input_shape=(maxlen,), dtype=int)\n model.add_node(Embedding(max_features, 128, input_length=maxlen),\n                name='embedding', input='input')\n model.add_node(LSTM(64), name='forward', input='embedding')\n\n@@ -381,7 +381,7 @@ def rnn(step_function, inputs, initial_states,\n     successive_states = []\n     successive_outputs = []\n     if go_backwards:\n-        input_list = input_list.reverse()\n+        input_list.reverse()\n     for input in input_list:\n         output, new_states = step_function(input, states)\n         if masking:\n\n@@ -224,12 +224,13 @@ class Graph(Layer):\n         self.input_order.append(name)\n         layer = Layer()  # empty layer\n         layer.set_input_shape(input_shape)\n-        ndim = len(input_shape) + 1\n         if dtype == 'float':\n-            layer.input = K.placeholder(ndim=ndim, name=name)\n+            layer.input = K.placeholder(shape=layer.input_shape, name=name)\n         else:\n-            if ndim == 2:\n-                layer.input = K.placeholder(ndim=2, dtype='int32', name=name)\n+            if len(input_shape) == 1:\n+                layer.input = K.placeholder(shape=layer.input_shape,\n+                                            dtype='int32',\n+                                            name=name)\n             else:\n                 raise Exception('Type \"int\" can only be used with ndim==2 (Embedding).')\n         self.inputs[name] = layer\n\n@@ -17,9 +17,12 @@ class Embedding(Layer):\n     '''\n     input_ndim = 2\n \n-    def __init__(self, input_dim, output_dim, init='uniform', input_length=None,\n-                 W_regularizer=None, activity_regularizer=None, W_constraint=None,\n-                 mask_zero=False, weights=None, **kwargs):\n+    def __init__(self, input_dim, output_dim,\n+                 init='uniform', input_length=None,\n+                 W_regularizer=None, activity_regularizer=None,\n+                 W_constraint=None,\n+                 mask_zero=False,\n+                 weights=None, **kwargs):\n         self.input_dim = input_dim\n         self.output_dim = output_dim\n         self.init = initializations.get(init)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8f2d6d2714aa1b60950a2fc355d39297b7f2cdfb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 11 | Churn Cumulative: 83 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,7 +3,18 @@ from . import backend as K\n \n \n def softmax(x):\n+    ndim = K.ndim(x)\n+    if ndim == 2:\n         return K.softmax(x)\n+    elif ndim == 3:\n+        # apply softmax to each timestep\n+        def step(x, states):\n+            return K.softmax(x), []\n+        last_output, outputs, states = K.rnn(step, x, [], masking=False)\n+        return outputs\n+    else:\n+        raise Exception('Cannot apply softmax to a tensor that is not 2D or 3D. ' +\n+                        'Here, ndim=' + str(ndim))\n \n \n def softplus(x):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#361a7cfe41f7663fd9f0c12345e4ca52b0a8a2a5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 8 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 234 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -12,11 +12,6 @@ Input: \"535+61\"\n Output: \"596\"\n Padding is handled by using a repeated sentinel character (space)\n \n-By default, the JZS1 recurrent neural network is used\n-JZS1 was an \"evolved\" recurrent neural network performing well on arithmetic benchmark in:\n-\"An Empirical Exploration of Recurrent Network Architectures\"\n-http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf\n-\n Input may optionally be inverted, shown to increase performance in many tasks in:\n \"Learning to Execute\"\n http://arxiv.org/abs/1410.4615\n@@ -26,16 +21,16 @@ http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-netwo\n Theoretically it introduces shorter term dependencies between source and target.\n \n Two digits inverted:\n-+ One layer JZS1 (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs\n++ One layer LSTM (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs\n \n Three digits inverted:\n-+ One layer JZS1 (128 HN), 50k training examples = 99% train/test accuracy in 100 epochs\n++ One layer LSTM (128 HN), 50k training examples = 99% train/test accuracy in 100 epochs\n \n Four digits inverted:\n-+ One layer JZS1 (128 HN), 400k training examples = 99% train/test accuracy in 20 epochs\n++ One layer LSTM (128 HN), 400k training examples = 99% train/test accuracy in 20 epochs\n \n Five digits inverted:\n-+ One layer JZS1 (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs\n++ One layer LSTM (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs\n \n \"\"\"\n \n@@ -75,8 +70,8 @@ class colors:\n TRAINING_SIZE = 50000\n DIGITS = 3\n INVERT = True\n-# Try replacing JZS1 with LSTM, GRU, or SimpleRNN\n-RNN = recurrent.JZS1\n+# Try replacing GRU, or SimpleRNN\n+RNN = recurrent.LSTM\n HIDDEN_SIZE = 128\n BATCH_SIZE = 128\n LAYERS = 1\n@@ -123,6 +118,7 @@ indices = np.arange(len(y))\n np.random.shuffle(indices)\n X = X[indices]\n y = y[indices]\n+\n # Explicitly set apart 10% for validation data that we never train over\n split_at = len(X) - len(X) / 10\n (X_train, X_val) = (slice_X(X, 0, split_at), slice_X(X, split_at))\n@@ -136,7 +132,7 @@ model = Sequential()\n # \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE\n # note: in a situation where your input sequences have a variable length,\n # use input_shape=(None, nb_feature).\n-model.add(RNN(HIDDEN_SIZE, input_shape=(None, len(chars))))\n+model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars))))\n # For the decoder's input, we repeat the encoded input for each time step\n model.add(RepeatVector(DIGITS + 1))\n # The decoder RNN could be multiple layers stacked or a single layer\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#df860fdb94c63cf7898315277fe951d1c0ba16a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 72 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -23,8 +23,8 @@ from keras.utils import np_utils\n     Optimizer is replaced with RMSprop which yields more stable and steady\n     improvement.\n \n-    Reaches 0.93 train/test accuracy after 900 epochs (which roughly corresponds\n-    to 1687500 steps in the original paper.)\n+    Reaches 0.93 train/test accuracy after 900 epochs\n+    (which roughly corresponds to 1687500 steps in the original paper.)\n '''\n \n batch_size = 32\n@@ -34,7 +34,6 @@ hidden_units = 100\n \n learning_rate = 1e-6\n clip_norm = 1.0\n-BPTT_truncate = 28*28\n \n # the data, shuffled and split between train and test sets\n (X_train, y_train), (X_test, y_test) = mnist.load_data()\n@@ -58,8 +57,7 @@ model = Sequential()\n model.add(SimpleRNN(output_dim=hidden_units,\n                     init=lambda shape: normal(shape, scale=0.001),\n                     inner_init=lambda shape: identity(shape, scale=1.0),\n-                    activation='relu', truncate_gradient=BPTT_truncate,\n-                    input_shape=(None, 1)))\n+                    activation='relu', input_shape=X_train.shape[1:]))\n model.add(Dense(nb_classes))\n model.add(Activation('softmax'))\n rmsprop = RMSprop(lr=learning_rate)\n@@ -74,7 +72,7 @@ print('IRNN test accuracy:', scores[1])\n \n print('Compare to LSTM...')\n model = Sequential()\n-model.add(LSTM(hidden_units, input_shape=(None, 1)))\n+model.add(LSTM(hidden_units, input_shape=X_train.shape[1:]))\n model.add(Dense(nb_classes))\n model.add(Activation('softmax'))\n rmsprop = RMSprop(lr=learning_rate)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#aaa47f0d2079c138c243e4acf2b68ed1acf0b51a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 99 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -3,12 +3,12 @@ from setuptools import find_packages\n \n \n setup(name='Keras',\n-      version='0.2.0',\n+      version='0.3.0',\n       description='Theano-based Deep Learning library',\n       author='Francois Chollet',\n       author_email='francois.chollet@gmail.com',\n       url='https://github.com/fchollet/keras',\n-      download_url='https://github.com/fchollet/keras/tarball/0.2.0',\n+      download_url='https://github.com/fchollet/keras/tarball/0.3.0',\n       license='MIT',\n       install_requires=['theano', 'pyyaml', 'six'],\n       extras_require={\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2553f07c3c366704dff429aeb939dff5e323d422", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 19 | Files Changed: 6 | Hunks: 19 | Methods Changed: 15 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 53 | Churn Cumulative: 7352 | Contributors (this commit): 36 | Commits (past 90d): 126 | Contributors (cumulative): 54 | DMM Complexity: 0.0\n\nDIFF:\n@@ -217,13 +217,16 @@ class Graph(Layer):\n         else:\n             return dict([(k, v.get_output(train)) for k, v in self.outputs.items()])\n \n-    def add_input(self, name, input_shape, dtype='float'):\n+    def add_input(self, name, input_shape=None, batch_input_shape=None, dtype='float'):\n         if name in self.namespace:\n             raise Exception('Duplicate node identifier: ' + name)\n         self.namespace.add(name)\n         self.input_order.append(name)\n         layer = Layer()  # empty layer\n-        layer.set_input_shape(input_shape)\n+        if input_shape:\n+            layer.set_input_shape((None,) + tuple(input_shape))\n+        elif batch_input_shape:\n+            layer.set_input_shape(batch_input_shape)\n         if dtype == 'float':\n             layer.input = K.placeholder(shape=layer.input_shape, name=name)\n         else:\n\n@@ -19,10 +19,15 @@ import sys\n \n class Layer(object):\n     def __init__(self, **kwargs):\n+        allowed_kwargs = {'input_shape',\n+                          'trainable',\n+                          'batch_input_shape'}\n         for kwarg in kwargs:\n-            assert kwarg in {'input_shape', 'trainable'}, \"Keyword argument not understood: \" + kwarg\n+            assert kwarg in allowed_kwargs, \"Keyword argument not understood: \" + kwarg\n         if 'input_shape' in kwargs:\n-            self.set_input_shape(kwargs['input_shape'])\n+            self.set_input_shape((None,) + tuple(kwargs['input_shape']))\n+        if 'batch_input_shape' in kwargs:\n+            self.set_input_shape(tuple(kwargs['batch_input_shape']))\n         if 'trainable' in kwargs:\n             self._trainable = kwargs['trainable']\n         if not hasattr(self, 'params'):\n@@ -80,7 +85,7 @@ class Layer(object):\n     def set_input_shape(self, input_shape):\n         if type(input_shape) not in [tuple, list]:\n             raise Exception('Invalid input shape - input_shape should be a tuple of int.')\n-        input_shape = (None,) + tuple(input_shape)\n+        input_shape = tuple(input_shape)\n         if hasattr(self, 'input_ndim') and self.input_ndim:\n             if self.input_ndim != len(input_shape):\n                 raise Exception('Invalid input shape - Layer expects input ndim=' +\n\n@@ -40,7 +40,7 @@ class Embedding(Layer):\n         super(Embedding, self).__init__(**kwargs)\n \n     def build(self):\n-        self.input = K.placeholder(shape=(None, self.input_length),\n+        self.input = K.placeholder(shape=(self.input_shape[0], self.input_length),\n                                    dtype='int32')\n         self.W = self.init((self.input_dim, self.output_dim))\n         self.params = [self.W]\n\n@@ -18,7 +18,7 @@ def check_single_tensor_operation(function_name, input_shape, **kwargs):\n     ztf = KTF.eval(getattr(KTF, function_name)(xtf, **kwargs))\n \n     assert zth.shape == ztf.shape\n-    assert_allclose(zth, ztf, atol=1e-06)\n+    assert_allclose(zth, ztf, atol=1e-05)\n \n \n def check_two_tensor_operation(function_name, x_input_shape,\n@@ -35,7 +35,7 @@ def check_two_tensor_operation(function_name, x_input_shape,\n     ztf = KTF.eval(getattr(KTF, function_name)(xtf, ytf, **kwargs))\n \n     assert zth.shape == ztf.shape\n-    assert_allclose(zth, ztf, atol=1e-06)\n+    assert_allclose(zth, ztf, atol=1e-05)\n \n \n @pytest.mark.skipif(sys.version_info.major != 2, reason=\"Requires Python 2.7\")\n@@ -56,7 +56,7 @@ class TestBackend(unittest.TestCase):\n         zth = KTH.eval(KTH.concatenate([xth, yth], axis=-1))\n         ztf = KTF.eval(KTF.concatenate([xtf, ytf], axis=-1))\n         assert zth.shape == ztf.shape\n-        assert_allclose(zth, ztf, atol=1e-06)\n+        assert_allclose(zth, ztf, atol=1e-05)\n \n         check_single_tensor_operation('reshape', (4, 2), shape=(8, 1))\n         check_single_tensor_operation('permute_dimensions', (4, 2, 3),\n@@ -76,7 +76,7 @@ class TestBackend(unittest.TestCase):\n         valth = KTH.get_value(xth)\n         valtf = KTF.get_value(xtf)\n         assert valtf.shape == valth.shape\n-        assert_allclose(valth, valtf, atol=1e-06)\n+        assert_allclose(valth, valtf, atol=1e-05)\n \n         # set_value\n         val = np.random.random((4, 2))\n@@ -86,7 +86,7 @@ class TestBackend(unittest.TestCase):\n         valth = KTH.get_value(xth)\n         valtf = KTF.get_value(xtf)\n         assert valtf.shape == valth.shape\n-        assert_allclose(valth, valtf, atol=1e-06)\n+        assert_allclose(valth, valtf, atol=1e-05)\n \n         # count_params\n         assert KTH.count_params(xth) == KTF.count_params(xtf)\n@@ -149,7 +149,7 @@ class TestBackend(unittest.TestCase):\n         zth = KTH.eval(gradth[0])\n         ztf = KTF.eval(gradtf[0])\n         assert zth.shape == ztf.shape\n-        assert_allclose(zth, ztf, atol=1e-06)\n+        assert_allclose(zth, ztf, atol=1e-05)\n \n     def test_function(self):\n         val = np.random.random((4, 2))\n@@ -171,12 +171,12 @@ class TestBackend(unittest.TestCase):\n         function_outputs_th = fth([input_val])[0]\n         function_outputs_tf = ftf([input_val])[0]\n         assert function_outputs_th.shape == function_outputs_tf.shape\n-        assert_allclose(function_outputs_th, function_outputs_tf, atol=1e-06)\n+        assert_allclose(function_outputs_th, function_outputs_tf, atol=1e-05)\n \n         new_val_th = KTH.get_value(xth)\n         new_val_tf = KTF.get_value(xtf)\n         assert new_val_th.shape == new_val_tf.shape\n-        assert_allclose(new_val_th, new_val_tf, atol=1e-06)\n+        assert_allclose(new_val_th, new_val_tf, atol=1e-05)\n \n     def test_rnn(self):\n         # implement a simple RNN\n@@ -224,9 +224,9 @@ class TestBackend(unittest.TestCase):\n         assert len(new_states) == 1\n         tf_state = KTF.eval(new_states[0])\n \n-        assert_allclose(tf_last_output, th_last_output, atol=1e-06)\n-        assert_allclose(tf_outputs, th_outputs, atol=1e-06)\n-        assert_allclose(tf_state, th_state, atol=1e-06)\n+        assert_allclose(tf_last_output, th_last_output, atol=1e-05)\n+        assert_allclose(tf_outputs, th_outputs, atol=1e-05)\n+        assert_allclose(tf_state, th_state, atol=1e-05)\n \n     def test_switch(self):\n         val = np.random.random()\n@@ -240,7 +240,7 @@ class TestBackend(unittest.TestCase):\n         ztf = KTF.eval(xtf)\n \n         assert zth.shape == ztf.shape\n-        assert_allclose(zth, ztf, atol=1e-06)\n+        assert_allclose(zth, ztf, atol=1e-05)\n \n     def test_nn_operations(self):\n         check_single_tensor_operation('relu', (4, 2), alpha=0.1, max_value=0.5)\n\n@@ -28,6 +28,13 @@ def _runner(layer_class):\n \n             mask = layer.get_output_mask(train)\n \n+    # check statefulness\n+    layer = layer_class(output_dim, return_sequences=False,\n+                        weights=None, batch_input_shape=(nb_samples, timesteps, input_dim))\n+    layer.input = K.variable(np.ones((nb_samples, timesteps, input_dim)))\n+    out = K.eval(layer.get_output(train))\n+    assert(out.shape == (nb_samples, output_dim))\n+\n \n class TestRNNS(unittest.TestCase):\n     \"\"\"\n\n@@ -10,7 +10,7 @@ from keras.layers.recurrent import SimpleRNN\n def check_layer_output_shape(layer, input_data):\n     ndim = len(input_data.shape)\n     layer.input = K.placeholder(ndim=ndim)\n-    layer.set_input_shape(input_data.shape[1:])\n+    layer.set_input_shape(input_data.shape)\n     expected_output_shape = layer.output_shape[1:]\n \n     function = K.function([layer.input], [layer.get_output()])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#905770099cbbc4706f56ddeafe12128da25430ad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 2385 | Contributors (this commit): 10 | Commits (past 90d): 14 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -50,8 +50,8 @@ class Recurrent(MaskedLayer):\n                 raise Exception('When using TensorFlow, you should define ' +\n                                 'explicitely the number of timesteps of ' +\n                                 'your sequences. Make sure the first layer ' +\n-                                'has an \"input_shape\" argument with a defined ' +\n-                                'first dimension.')\n+                                'has a \"batch_input_shape\" argument ' +\n+                                'including the samples axis.')\n \n         mask = self.get_output_mask(train)\n         if mask:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#be39e25b866ef584b4abbc2f97e13fdee40ffffb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 715 | Contributors (this commit): 1 | Commits (past 90d): 9 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -536,8 +536,9 @@ def conv2d(x, kernel, strides=(1, 1), border_mode='valid', dim_ordering='th'):\n     if _on_gpu() and dnn.dnn_available():\n         if border_mode == 'same':\n             assert(strides == (1, 1))\n-            pad_x = (kernel.shape[2] - strides[0]) // 2\n-            pad_y = (kernel.shape[3] - strides[1]) // 2\n+            np_kernel = kernel.eval()\n+            pad_x = (np_kernel.shape[2] - strides[0]) // 2\n+            pad_y = (np_kernel.shape[3] - strides[1]) // 2\n             conv_out = dnn.dnn_conv(img=x,\n                                     kerns=kernel,\n                                     border_mode=(pad_x, pad_y))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6fe65a6a1d3f8220024fcc3fc2d1c5209423a74b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 23 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -12,4 +12,4 @@ good research.\n See http://keras.io/\r\n \"\"\"\r\n \r\n-__version__ = '0.2.0'\r\n+__version__ = '0.3.0'\r\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#54c025ac26c81fc1cc58adf0be8f3d596ec73a3a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 494 | Files Changed: 2 | Hunks: 2 | Methods Changed: 20 | Complexity Δ (Sum/Max): -41/0 | Churn Δ: 494 | Churn Cumulative: 1078 | Contributors (this commit): 3 | Commits (past 90d): 10 | Contributors (cumulative): 6 | DMM Complexity: 0.13692946058091288\n\nDIFF:\n@@ -1,168 +0,0 @@\n-from __future__ import absolute_import\n-from __future__ import print_function\n-import numpy as np\n-np.random.seed(123)\n-import matplotlib.pyplot as plt\n-\n-from theano import function\n-\n-from keras.models import Sequential\n-from keras.layers.core import TimeDistributedDense, Activation\n-from keras.layers.recurrent import LSTM\n-from keras.optimizers import Adam\n-from keras.utils import generic_utils\n-\n-from keras.layers.ntm import NeuralTuringMachine as NTM\n-\n-\"\"\"\n-Copy Problem defined in Graves et. al [0]\n-\n-Training data is made of sequences with length 1 to 20. \n-Test data are sequences of length 100.\n-The model is tested every 500 weight updates.\n-After about 3500 updates, the accuracy jumps from around 50% to >90%.\n-\n-Estimated compile time: 12 min\n-Estimated time to train Neural Turing Machine and 3 layer LSTM on an NVidia GTX 680: 2h\n-\n-[0]: http://arxiv.org/pdf/1410.5401v2.pdf\n-\"\"\"\n-\n-batch_size = 100\n-\n-h_dim = 128\n-n_slots = 128\n-m_length = 20\n-input_dim = 8\n-lr = 1e-3\n-clipvalue = 10\n-\n-##### Neural Turing Machine ######\n-\n-ntm = NTM(h_dim, n_slots=n_slots, m_length=m_length, shift_range=3,\n-          inner_rnn='lstm', return_sequences=True, input_dim=input_dim)\n-model = Sequential()\n-model.add(ntm)\n-model.add(TimeDistributedDense(input_dim))\n-model.add(Activation('sigmoid'))\n-\n-sgd = Adam(lr=lr, clipvalue=clipvalue)\n-model.compile(loss='binary_crossentropy', optimizer=sgd)\n-\n-# LSTM - Run this for comparison\n-\n-sgd2 = Adam(lr=lr, clipvalue=clipvalue)\n-lstm = Sequential()\n-lstm.add(LSTM(input_dim=input_dim, output_dim=h_dim*2, return_sequences=True))\n-lstm.add(LSTM(output_dim=h_dim*2, return_sequences=True))\n-lstm.add(LSTM(output_dim=h_dim*2, return_sequences=True))\n-lstm.add(TimeDistributedDense(input_dim))\n-lstm.add(Activation('sigmoid'))\n-\n-lstm.compile(loss='binary_crossentropy', optimizer=sgd)\n-\n-###### DATASET ########\n-\n-def get_sample(batch_size=128, n_bits=8, max_size=20, min_size=1):\n-    # generate samples with random length\n-    inp = np.zeros((batch_size, 2*max_size-1, n_bits))\n-    out = np.zeros((batch_size, 2*max_size-1, n_bits))\n-    sw = np.zeros((batch_size, 2*max_size-1, 1))\n-    for i in range(batch_size):\n-        t = np.random.randint(low=min_size, high=max_size)\n-        x = np.random.uniform(size=(t, n_bits)) > .5\n-        for j,f in enumerate(x.sum(axis=-1)): # remove fake flags\n-            if f>=n_bits:\n-                x[j, :] = 0.\n-        del_flag = np.ones((1, n_bits))\n-        inp[i, :t+1] = np.concatenate([x, del_flag], axis=0)\n-        out[i, t+1:(2*t+1)] = x\n-        sw[i, t+1:(2*t+1)] = 1\n-    return inp, out, sw\n-\n-def show_pattern(inp, out, sw, file_name='ntm_output.png'):\n-    ''' Helper function to visualize results '''\n-    plt.figure(figsize=(10, 10))\n-    plt.subplot(131)\n-    plt.imshow(inp>.5)\n-    plt.subplot(132)\n-    plt.imshow(out>.5)\n-    plt.subplot(133)\n-    plt.imshow(sw>.5)\n-    plt.savefig(file_name)\n-    plt.close()\n-\n-# Show data example:\n-inp, out, sw = get_sample(1, 8, 20)\n-\n-plt.subplot(131)\n-plt.title('input')\n-plt.imshow(inp[0], cmap='gray')\n-plt.subplot(132)\n-plt.title('desired')\n-plt.imshow(out[0], cmap='gray')\n-plt.subplot(133)\n-plt.title('sample_weight')\n-plt.imshow(sw[0], cmap='gray')\n-\n-# training uses sequences of length 1 to 20. Test uses series of length 100.\n-def test_model(model, file_name, min_size=100):\n-    I, V, sw = get_sample(batch_size=500, n_bits=input_dim, max_size=min_size+1, min_size=min_size)\n-    Y = np.asarray(model.predict(I, batch_size=100) > .5).astype('float64')\n-    acc = (V[:, -min_size:, :] == Y[:, -min_size:, :]).mean() * 100\n-    show_pattern(Y[0], V[0], sw[0], file_name)\n-    return acc\n-\n-##### TRAIN ######\n-nb_epoch = 4000\n-progbar = generic_utils.Progbar(nb_epoch)\n-for e in range(nb_epoch):\n-    I, V, sw = get_sample(n_bits=input_dim, max_size=20, min_size=1, batch_size=100)\n-\n-    loss1 = model.train_on_batch(I, V, sample_weight=sw)\n-    loss2 = lstm.train_on_batch(I, V, sample_weight=sw)\n-\n-    progbar.add(1, values=[(\"NTM\", loss1), (\"LSTM\", loss2)])\n-\n-    if e % 500 == 0:\n-        print(\"\")\n-        acc1 = test_model(model, 'ntm.png')\n-        acc2 = test_model(lstm, 'lstm.png')\n-        print(\"NTM  test acc: {}\".format(acc1))\n-        print(\"LSTM test acc: {}\".format(acc2))\n-\n-##### VISUALIZATION #####\n-X = model.get_input()\n-Y = ntm.get_full_output()[0:3]  # (memory over time, read_vectors, write_vectors)\n-F = function([X], Y, allow_input_downcast=True)\n-\n-inp, out, sw = get_sample(1, 8, 21, 20)\n-mem, read, write = F(inp.astype('float32'))\n-Y = model.predict(inp)\n-\n-plt.figure(figsize=(15, 12))\n-\n-plt.subplot(221)\n-plt.imshow(write[0])\n-plt.xlabel('memory location')\n-plt.ylabel('time')\n-plt.title('write')\n-\n-plt.subplot(222)\n-plt.imshow(read[0])\n-plt.title('read')\n-\n-plt.subplot(223)\n-plt.title('desired')\n-plt.imshow(out[0])\n-\n-plt.subplot(224)\n-plt.imshow(Y[0]>.5)\n-plt.title('output')\n-\n-plt.figure(figsize=(15, 10))\n-plt.subplot(325)\n-plt.ylabel('time')\n-plt.xlabel('location')\n-plt.title('memory evolving in time (avg value per location)')\n-plt.imshow(mem[0].mean(axis=-1))\n\n@@ -1,326 +0,0 @@\n-import numpy as np\n-from scipy.linalg import circulant\n-\n-from .. import backend as K\n-import theano\n-import theano.tensor as T\n-floatX = theano.config.floatX\n-\n-from keras.layers.recurrent import Recurrent, GRU, LSTM\n-from keras.utils.theano_utils import shared_zeros, alloc_zeros_matrix, shared_scalar\n-tol = 1e-4\n-\n-\n-def _update_controller(self, inp, h_tm1, M, mask):\n-    \"\"\" Update inner RNN controler\n-    We have to update the inner RNN inside the Neural Turing Machine, this\n-    is an almost literal copy of keras.layers.recurrent.GRU and \n-    keras.layers.recurrent.LSTM see these clases for further details.\n-    \"\"\"\n-    x = T.concatenate([inp, M], axis=-1)\n-    # get inputs\n-    if self.inner_rnn == 'gru':\n-        x_z = T.dot(x, self.rnn.W_z) + self.rnn.b_z\n-        x_r = T.dot(x, self.rnn.W_r) + self.rnn.b_r\n-        x_h = T.dot(x, self.rnn.W_h) + self.rnn.b_h\n-\n-    elif self.inner_rnn == 'lstm':\n-        xi = T.dot(x, self.rnn.W_i) + self.rnn.b_i\n-        xf = T.dot(x, self.rnn.W_f) + self.rnn.b_f\n-        xc = T.dot(x, self.rnn.W_c) + self.rnn.b_c\n-        xo = T.dot(x, self.rnn.W_o) + self.rnn.b_o\n-\n-    elif self.inner_rnn == 'simple':\n-        x = T.dot(x, self.rnn.W) + self.rnn.b\n-\n-    # update state\n-    if self.inner_rnn == 'gru':\n-        h = self.rnn._step(x_z, x_r, x_h, 1., h_tm1[0],\n-                           self.rnn.U_z,\n-                           self.rnn.U_r,\n-                           self.rnn.U_h)\n-        h = mask[:, None] * h + (1-mask[:, None])*h_tm1[0]\n-        h = (h, )\n-\n-    elif self.inner_rnn == 'lstm':\n-        h = self.rnn._step(xi, xf, xo, xc, 1.,\n-                           h_tm1[1], h_tm1[0],\n-                           self.rnn.U_i, self.rnn.U_f,\n-                           self.rnn.U_o, self.rnn.U_c)\n-        h = h[::-1]\n-        h = tuple([mask[:, None]*h[i] +\n-                   (1-mask[:, None])*h_tm1[i] for i in range(len(h))])\n-\n-    elif self.inner_rnn == 'simple':\n-        h = self.rnn._step(x, 1, h_tm1[0], self.rnn.U)\n-        h = mask[:, None] * h + (1-mask[:, None])*h_tm1[0]\n-        h = (h, )\n-\n-    return h\n-\n-\n-def _circulant(leng, n_shifts):\n-    \"\"\" Generate circulant copies of a vector.\n-    This will generate a tensor with `n_shifts` of rotated versions the\n-    identity matrix. When this tensor is multiplied by a vector\n-    the result are `n_shifts` shifted versions of that vector. Since\n-    everything is done with inner products, this operation is differentiable.\n-\n-    Paramters:\n-    ----------\n-    leng: int > 0, number of memory locations\n-    n_shifts: int > 0, number of allowed shifts (if 1, no shift)\n-\n-    Returns:\n-    --------\n-    shift operation, a tensor with dimensions (n_shifts, leng, leng)\n-    \"\"\"\n-    eye = np.eye(leng)\n-    shifts = range(n_shifts//2, -n_shifts//2, -1)\n-    C = np.asarray([np.roll(eye, s, axis=1) for s in shifts])\n-    return theano.shared(C.astype(theano.config.floatX))\n-\n-\n-def _renorm(x):\n-    return x / (x.sum(axis=1, keepdims=True))\n-\n-\n-def _softmax(x):\n-    wt = x.flatten(ndim=2)\n-    w = T.nnet.softmax(wt)\n-    return w.reshape(x.shape)  # T.clip(s, 0, 1)\n-\n-\n-def _cosine_distance(M, k):\n-    dot = (M * k[:, None, :]).sum(axis=-1)\n-    nM = T.sqrt((M**2).sum(axis=-1))\n-    nk = T.sqrt((k**2).sum(axis=-1, keepdims=True))\n-    return dot / (nM * nk)\n-\n-\n-class NeuralTuringMachine(Recurrent):\n-    \"\"\" Neural Turing Machines\n-    \n-    Parameters:\n-    -----------\n-    shift_range: int, number of available shifts, ex. if 3, avilable shifts are\n-                 (-1, 0, 1)\n-    n_slots: number of memory locations\n-    m_length: memory length at each location\n-    inner_rnn: str, supported values are 'gru' and 'lstm'\n-    output_dim: hidden state size (RNN controller output_dim)\n-\n-    Known issues and TODO:\n-    ----------------------\n-    Theano may complain when n_slots == 1.\n-    Add multiple reading and writing heads.\n-\n-    \"\"\"\n-    def __init__(self, output_dim, n_slots, m_length, shift_range=3,\n-                 inner_rnn='gru', truncate_gradient=-1, return_sequences=False,\n-                 init='glorot_uniform', inner_init='orthogonal',\n-                 input_dim=None, input_length=None, **kwargs):\n-        if K._BACKEND != 'theano':\n-            raise Exception('NeuralTuringMachine is only available for Theano for the time being. ' +\n-                            'It will be adapted to TensorFlow soon.')\n-        self.output_dim = output_dim\n-        self.n_slots = n_slots\n-        self.m_length = m_length\n-        self.shift_range = shift_range\n-        self.init = init\n-        self.inner_init = inner_init\n-        self.inner_rnn = inner_rnn\n-        self.return_sequences = return_sequences\n-        self.truncate_gradient = truncate_gradient\n-\n-        self.input_dim = input_dim\n-        self.input_length = input_length\n-        if self.input_dim:\n-            kwargs['input_shape'] = (self.input_length, self.input_dim)\n-        super(NeuralTuringMachine, self).__init__(**kwargs)\n-\n-    def build(self):\n-        input_leng, input_dim = self.input_shape[1:]\n-        self.input = T.tensor3()\n-\n-        if self.inner_rnn == 'gru':\n-            self.rnn = GRU(\n-                input_dim=input_dim+self.m_length,\n-                input_length=input_leng,\n-                output_dim=self.output_dim, init=self.init,\n-                inner_init=self.inner_init)\n-        elif self.inner_rnn == 'lstm':\n-            self.rnn = LSTM(\n-                input_dim=input_dim+self.m_length,\n-                input_length=input_leng,\n-                output_dim=self.output_dim, init=self.init,\n-                inner_init=self.inner_init)\n-        else:\n-            raise ValueError('this inner_rnn is not implemented yet.')\n-\n-        self.rnn.build()\n-\n-        # initial memory, state, read and write vecotrs\n-        self.M = theano.shared((.001*np.ones((1,)).astype(floatX)))\n-        self.init_h = shared_zeros((self.output_dim))\n-        self.init_wr = self.rnn.init((self.n_slots,))\n-        self.init_ww = self.rnn.init((self.n_slots,))\n-\n-        # write\n-        self.W_e = self.rnn.init((self.output_dim, self.m_length))  # erase\n-        self.b_e = shared_zeros((self.m_length))\n-        self.W_a = self.rnn.init((self.output_dim, self.m_length))  # add\n-        self.b_a = shared_zeros((self.m_length))\n-\n-        # get_w  parameters for reading operation\n-        self.W_k_read = self.rnn.init((self.output_dim, self.m_length))\n-        self.b_k_read = self.rnn.init((self.m_length, ))\n-        self.W_c_read = self.rnn.init((self.output_dim, 3))  # 3 = beta, g, gamma see eq. 5, 7, 9 in Graves et. al 2014\n-        self.b_c_read = shared_zeros((3))\n-        self.W_s_read = self.rnn.init((self.output_dim, self.shift_range))\n-        self.b_s_read = shared_zeros((self.shift_range)) \n-\n-        # get_w  parameters for writing operation\n-        self.W_k_write = self.rnn.init((self.output_dim, self.m_length))\n-        self.b_k_write = self.rnn.init((self.m_length, ))\n-        self.W_c_write = self.rnn.init((self.output_dim, 3))  # 3 = beta, g, gamma see eq. 5, 7, 9\n-        self.b_c_write = shared_zeros((3))\n-        self.W_s_write = self.rnn.init((self.output_dim, self.shift_range))\n-        self.b_s_write = shared_zeros((self.shift_range))\n-\n-        self.C = _circulant(self.n_slots, self.shift_range)\n-\n-        self.params = self.rnn.params + [\n-            self.W_e, self.b_e,\n-            self.W_a, self.b_a,\n-            self.W_k_read, self.b_k_read,\n-            self.W_c_read, self.b_c_read,\n-            self.W_s_read, self.b_s_read,\n-            self.W_k_write, self.b_k_write,\n-            self.W_s_write, self.b_s_write,\n-            self.W_c_write, self.b_c_write,\n-            self.M,\n-            self.init_h, self.init_wr, self.init_ww]\n-\n-        if self.inner_rnn == 'lstm':\n-            self.init_c = shared_zeros((self.output_dim))\n-            self.params = self.params + [self.init_c, ]\n-\n-    def _read(self, w, M):\n-        return (w[:, :, None]*M).sum(axis=1)\n-\n-    def _write(self, w, e, a, M, mask):\n-        Mtilda = M * (1 - w[:, :, None]*e[:, None, :])\n-        Mout = Mtilda + w[:, :, None]*a[:, None, :]\n-        return mask[:, None, None]*Mout + (1-mask[:, None, None])*M\n-\n-    def _get_content_w(self, beta, k, M):\n-        num = beta[:, None] * _cosine_distance(M, k)\n-        return _softmax(num)\n-\n-    def _get_location_w(self, g, s, C, gamma, wc, w_tm1, mask):\n-        wg = g[:, None] * wc + (1-g[:, None])*w_tm1\n-        Cs = (C[None, :, :, :] * wg[:, None, None, :]).sum(axis=3)\n-        wtilda = (Cs * s[:, :, None]).sum(axis=1)\n-        wout = _renorm(wtilda ** gamma[:, None])\n-        return mask[:, None] * wout + (1-mask[:, None])*w_tm1\n-\n-    def _get_controller_output(self, h, W_k, b_k, W_c, b_c, W_s, b_s):\n-        k = T.tanh(T.dot(h, W_k) + b_k)  # + 1e-6\n-        c = T.dot(h, W_c) + b_c\n-        beta = T.nnet.relu(c[:, 0]) + 1e-6\n-        g = T.nnet.sigmoid(c[:, 1])\n-        gamma = T.nnet.relu(c[:, 2]) + 1\n-        s = T.nnet.softmax(T.dot(h, W_s) + b_s)\n-        return k, beta, g, gamma, s\n-\n-    def _get_initial_states(self, batch_size):\n-        init_M = self.M.dimshuffle(0, 'x', 'x').repeat(\n-           batch_size, axis=0).repeat(self.n_slots, axis=1).repeat(\n-               self.m_length, axis=2)\n-\n-        init_h = self.init_h.dimshuffle(('x', 0)).repeat(batch_size, axis=0)\n-        init_wr = self.init_wr.dimshuffle(('x', 0)).repeat(batch_size, axis=0)\n-        init_ww = self.init_ww.dimshuffle(('x', 0)).repeat(batch_size, axis=0)\n-        if self.inner_rnn == 'lstm':\n-            init_c = self.init_c.dimshuffle(('x', 0)).repeat(batch_size, axis=0)\n-            return init_M, T.nnet.softmax(init_wr), T.nnet.softmax(init_ww), init_h, init_c\n-        else:\n-            return init_M, T.nnet.softmax(init_wr), T.nnet.softmax(init_ww), init_h\n-\n-    def _step(self, x, mask, M_tm1, wr_tm1, ww_tm1, *args):\n-        # read\n-        if self.inner_rnn == 'lstm':\n-            h_tm1 = args[0:2][::-1]  # (cell_tm1, h_tm1)\n-        else:\n-            h_tm1 = args[0:1]  # (h_tm1, )\n-        k_read, beta_read, g_read, gamma_read, s_read = self._get_controller_output(\n-            h_tm1[-1], self.W_k_read, self.b_k_read, self.W_c_read, self.b_c_read,\n-            self.W_s_read, self.b_s_read)\n-        wc_read = self._get_content_w(beta_read, k_read, M_tm1)\n-        wr_t = self._get_location_w(g_read, s_read, self.C, gamma_read,\n-                                    wc_read, wr_tm1, mask)\n-        M_read = self._read(wr_t, M_tm1)\n-\n-        # update controller\n-        h_t = _update_controller(self, x, h_tm1, M_read, mask)\n-\n-        # write\n-        k_write, beta_write, g_write, gamma_write, s_write = self._get_controller_output(\n-            h_t[-1], self.W_k_write, self.b_k_write, self.W_c_write,\n-            self.b_c_write, self.W_s_write, self.b_s_write)\n-        wc_write = self._get_content_w(beta_write, k_write, M_tm1)\n-        ww_t = self._get_location_w(g_write, s_write, self.C, gamma_write,\n-                                    wc_write, ww_tm1, mask)\n-        e = T.nnet.sigmoid(T.dot(h_t[-1], self.W_e) + self.b_e)\n-        a = T.tanh(T.dot(h_t[-1], self.W_a) + self.b_a)\n-        M_t = self._write(ww_t, e, a, M_tm1, mask)\n-\n-        return (M_t, wr_t, ww_t) + h_t\n-\n-    def get_output(self, train=False):\n-        outputs = self.get_full_output(train)\n-\n-        if self.return_sequences:\n-            return outputs[-1]\n-        else:\n-            return outputs[-1][:, -1]\n-\n-    @property\n-    def output_shape(self):\n-        input_shape = self.input_shape\n-        if self.return_sequences:\n-            return input_shape[0], input_shape[1], self.output_dim\n-        else:\n-            return input_shape[0], self.output_dim\n-\n-    def get_full_output(self, train=False):\n-        \"\"\"\n-        This method is for research and visualization purposes. Use it as:\n-        X = model.get_input()  # full model\n-        Y = ntm.get_output()    # this layer\n-        F = theano.function([X], Y, allow_input_downcast=True)\n-        [memory, read_address, write_address, rnn_state] = F(x)\n-\n-        if inner_rnn == \"lstm\" use it as:\n-        [memory, read_address, write_address, rnn_cell, rnn_state] = F(x)\n-\n-        \"\"\"\n-        X = self.get_input(train)\n-        padded_mask = self.get_padded_shuffled_mask(train, X, pad=1)[:, :, 0]\n-        X = X.dimshuffle((1, 0, 2))\n-\n-        init_states = self._get_initial_states(X.shape[1])\n-        outputs, updates = theano.scan(self._step,\n-                                       sequences=[X, padded_mask],\n-                                       outputs_info=init_states,\n-                                       non_sequences=self.params,\n-                                       truncate_gradient=self.truncate_gradient)\n-\n-        out = [outputs[0].dimshuffle((1, 0, 2, 3)),\n-               outputs[1].dimshuffle(1, 0, 2),\n-               outputs[2].dimshuffle((1, 0, 2)),\n-               outputs[3].dimshuffle((1, 0, 2))]\n-        if self.inner_rnn == 'lstm':\n-            out + [outputs[4].dimshuffle((1, 0, 2))]\n-        return out\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1c6ab36c63044199c2219e9c9cb1f03e1b54361f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2387 | Contributors (this commit): 10 | Commits (past 90d): 14 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -216,7 +216,7 @@ class GRU(Recurrent):\n                 raise Exception('If a RNN is stateful, a complete ' +\n                                 'input_shape must be provided ' +\n                                 '(including batch size).')\n-            self.states = [K.zeros(input_shape[0], self.output_dim)]\n+            self.states = [K.zeros((input_shape[0], self.output_dim))]\n         else:\n             # initial states: all-zero tensor of shape (output_dim)\n             self.states = [None]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0933147dc89242fb4c89d3bfeedc051d82acc9c5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 2393 | Contributors (this commit): 10 | Commits (past 90d): 15 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -123,7 +123,7 @@ class SimpleRNN(Recurrent):\n                 raise Exception('If a RNN is stateful, a complete ' +\n                                 'input_shape must be provided ' +\n                                 '(including batch size).')\n-            self.states = [K.zeros(input_shape[0], self.output_dim)]\n+            self.states = [K.zeros((input_shape[0], self.output_dim))]\n         else:\n             # initial states: all-zero tensor of shape (output_dim)\n             self.states = [None]\n@@ -295,8 +295,8 @@ class LSTM(Recurrent):\n                 raise Exception('If a RNN is stateful, a complete ' +\n                                 'input_shape must be provided ' +\n                                 '(including batch size).')\n-            self.states = [K.zeros(input_shape[0], self.output_dim),\n-                           K.zeros(input_shape[0], self.output_dim)]\n+            self.states = [K.zeros((input_shape[0], self.output_dim)),\n+                           K.zeros((input_shape[0], self.output_dim))]\n         else:\n             # initial states: 2 all-zero tensor of shape (output_dim)\n             self.states = [None, None]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4781f40eb684c2d6c8dd5d6659eb535b89bb1b04", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 16 | Files Changed: 3 | Hunks: 14 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 34 | Churn Cumulative: 748 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -27,7 +27,9 @@ else:\n     _config = {'floatx': floatx(),\n                'epsilon': epsilon(),\n                'backend': _BACKEND}\n-    json.dump(_config, open(_config_path, 'w'))\n+    with open(_config_path, 'w') as f:\n+        # add new line in order for bash 'cat' display the content correctly\n+        f.write(json.dumps(_config) + '\\n')\n \n if _BACKEND == 'theano':\n     print('Using Theano backend.')\n@@ -36,4 +38,4 @@ elif _BACKEND == 'tensorflow':\n     print('Using TensorFlow backend.')\n     from .tensorflow_backend import *\n else:\n-    raise Exception('Unknown backend: ' + str(backend))\n+    raise Exception('Unknown backend: ' + str(_BACKEND))\n\n@@ -139,7 +139,7 @@ class TestGraph(unittest.TestCase):\n         assert(loss < 4.)\n \n         print('test weight saving')\n-        graph.save_weights('temp.h5', overwrite=True)\n+        graph.save_weights('test_2o_1i_weights_temp.h5', overwrite=True)\n         graph = Graph()\n         graph.add_input(name='input1', input_shape=(32,))\n         graph.add_node(Dense(16), name='dense1', input='input1')\n@@ -148,7 +148,7 @@ class TestGraph(unittest.TestCase):\n         graph.add_output(name='output1', input='dense2')\n         graph.add_output(name='output2', input='dense3')\n         graph.compile('rmsprop', {'output1': 'mse', 'output2': 'mse'})\n-        graph.load_weights('temp.h5')\n+        graph.load_weights('test_2o_1i_weights_temp.h5')\n         nloss = graph.evaluate({'input1': X_test, 'output1': y_test, 'output2': y2_test})\n         print(nloss)\n         assert(loss == nloss)\n\n@@ -61,14 +61,14 @@ class TestSequential(unittest.TestCase):\n         model.get_config(verbose=0)\n \n         print('test weight saving')\n-        model.save_weights('temp.h5', overwrite=True)\n+        model.save_weights('test_sequential_temp.h5', overwrite=True)\n         model = Sequential()\n         model.add(Dense(nb_hidden, input_shape=(input_dim,)))\n         model.add(Activation('relu'))\n         model.add(Dense(nb_class))\n         model.add(Activation('softmax'))\n         model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-        model.load_weights('temp.h5')\n+        model.load_weights('test_sequential_temp.h5')\n \n         nloss = model.evaluate(X_train, y_train, verbose=0)\n         assert(loss == nloss)\n@@ -114,7 +114,7 @@ class TestSequential(unittest.TestCase):\n         model.get_config(verbose=0)\n \n         print('test weight saving')\n-        model.save_weights('temp.h5', overwrite=True)\n+        model.save_weights('test_merge_sum_temp.h5', overwrite=True)\n         left = Sequential()\n         left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n         left.add(Activation('relu'))\n@@ -125,7 +125,7 @@ class TestSequential(unittest.TestCase):\n         model.add(Merge([left, right], mode='sum'))\n         model.add(Dense(nb_class))\n         model.add(Activation('softmax'))\n-        model.load_weights('temp.h5')\n+        model.load_weights('test_merge_sum_temp.h5')\n         model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n         nloss = model.evaluate([X_train, X_train], y_train, verbose=0)\n@@ -205,7 +205,7 @@ class TestSequential(unittest.TestCase):\n         model.get_config(verbose=0)\n \n         print('test weight saving')\n-        model.save_weights('temp.h5', overwrite=True)\n+        model.save_weights('test_merge_concat_temp.h5', overwrite=True)\n         left = Sequential()\n         left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n         left.add(Activation('relu'))\n@@ -221,7 +221,7 @@ class TestSequential(unittest.TestCase):\n         model.add(Activation('softmax'))\n \n         model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-        model.load_weights('temp.h5')\n+        model.load_weights('test_merge_concat_temp.h5')\n \n         nloss = model.evaluate([X_train, X_train], y_train, verbose=0)\n         assert(loss == nloss)\n@@ -268,8 +268,8 @@ class TestSequential(unittest.TestCase):\n         model.predict_proba([X_test, X_test, X_test], verbose=0)\n         model.get_config(verbose=0)\n \n-        model.save_weights('temp.h5', overwrite=True)\n-        model.load_weights('temp.h5')\n+        model.save_weights('test_merge_recursivity_temp.h5', overwrite=True)\n+        model.load_weights('test_merge_recursivity_temp.h5')\n \n         nloss = model.evaluate([X_train, X_train, X_train], y_train, verbose=0)\n         print(nloss)\n@@ -305,8 +305,8 @@ class TestSequential(unittest.TestCase):\n         model.predict_proba(X_test, verbose=0)\n         model.get_config(verbose=0)\n \n-        model.save_weights('temp.h5', overwrite=True)\n-        model.load_weights('temp.h5')\n+        model.save_weights('test_merge_overlap_temp.h5', overwrite=True)\n+        model.load_weights('test_merge_overlap_temp.h5')\n \n         nloss = model.evaluate(X_train, y_train, verbose=0)\n         print(nloss)\n@@ -359,7 +359,7 @@ class TestSequential(unittest.TestCase):\n         model.get_config(verbose=0)\n \n         print('test weight saving')\n-        model.save_weights('temp.h5', overwrite=True)\n+        model.save_weights('test_lambda_temp.h5', overwrite=True)\n         left = Sequential()\n         left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n         left.add(Activation('relu'))\n@@ -371,7 +371,7 @@ class TestSequential(unittest.TestCase):\n                               output_shape=output_shape))\n         model.add(Dense(nb_class))\n         model.add(Lambda(activation))\n-        model.load_weights('temp.h5')\n+        model.load_weights('test_lambda_temp.h5')\n         model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n         nloss = model.evaluate([X_train, X_train], y_train, verbose=0)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
