{"custom_id": "keras#4fab0bf9a8ad03d63af0797b51526a0391e55a87", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 17 | Files Changed: 1 | Hunks: 10 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 42 | Churn Cumulative: 658 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.9166666666666666\n\nDIFF:\n@@ -80,6 +80,7 @@ def relu6(x):\n \n class DepthwiseConv2D(Conv2D):\n     \"\"\"Depthwise separable 2D convolution.\n+\n     Depthwise Separable convolutions consists in performing\n     just the first step in a depthwise spatial convolution\n     (which acts on each input channel separately).\n@@ -284,8 +285,8 @@ def MobileNet(input_shape=None,\n               input_tensor=None,\n               pooling=None,\n               classes=1000):\n-    \"\"\"\n-    Instantiate the MobileNet architecture.\n+    \"\"\"Instantiates the MobileNet architecture.\n+\n     Note that only TensorFlow is supported for now,\n     therefore it only works with the data format\n     `image_data_format='channels_last'` in your Keras config\n@@ -470,8 +471,8 @@ def MobileNet(input_shape=None,\n     # load weights\n     if weights == 'imagenet':\n         if K.image_data_format() == 'channels_first':\n-            raise AttributeError('Weights for Channels Last format are not available')\n-\n+            raise ValueError('Weights for \"channels_last\" format '\n+                             'are not available.')\n         if alpha == 1.0:\n             alpha_text = '1_0'\n         elif alpha == 0.75:\n@@ -493,12 +494,10 @@ def MobileNet(input_shape=None,\n             weights_path = get_file(model_name,\n                                     weigh_path,\n                                     cache_subdir='models')\n-\n         model.load_weights(weights_path)\n \n     if old_data_format:\n         K.set_image_data_format(old_data_format)\n-\n     return model\n \n \n@@ -547,8 +546,10 @@ def _conv_block(input, filters, alpha, kernel=(3, 3), strides=(1, 1)):\n     \"\"\"\n     channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n     filters = int(filters * alpha)\n-\n-    x = Convolution2D(filters, kernel, padding='same', use_bias=False, strides=strides,\n+    x = Convolution2D(filters, kernel,\n+                      padding='same',\n+                      use_bias=False,\n+                      strides=strides,\n                       name='conv1')(input)\n     x = BatchNormalization(axis=channel_axis, name='conv1_bn')(x)\n     x = Activation(relu6, name='conv1_relu')(x)\n@@ -558,9 +559,11 @@ def _conv_block(input, filters, alpha, kernel=(3, 3), strides=(1, 1)):\n \n def _depthwise_conv_block(input, pointwise_conv_filters, alpha,\n                           depth_multiplier=1, strides=(1, 1), id=1):\n-    \"\"\"\n-    Adds a depthwise convolution block (depthwise conv, batch normalization, relu6,\n-    pointwise convolution, batch normalization and relu6).\n+    \"\"\"Adds a depthwise convolution block.\n+\n+    A depthwise convolution block consists of a depthwise conv,\n+    batch normalization, relu6, pointwise convolution,\n+    batch normalization and relu6 activation.\n \n     # Arguments\n         input: Input tensor of shape `(rows, cols, channels)`\n@@ -603,14 +606,19 @@ def _depthwise_conv_block(input, pointwise_conv_filters, alpha,\n     channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n     pointwise_conv_filters = int(pointwise_conv_filters * alpha)\n \n-    x = DepthwiseConv2D(kernel_size=(3, 3), padding='same', depth_multiplier=depth_multiplier,\n-                        strides=strides, use_bias=False, name='conv_dw_%d' % id)(input)\n+    x = DepthwiseConv2D((3, 3),\n+                        padding='same',\n+                        depth_multiplier=depth_multiplier,\n+                        strides=strides,\n+                        use_bias=False,\n+                        name='conv_dw_%d' % id)(input)\n     x = BatchNormalization(axis=channel_axis, name='conv_dw_%d_bn' % id)(x)\n     x = Activation(relu6, name='conv_dw_%d_relu' % id)(x)\n \n-    x = Convolution2D(pointwise_conv_filters, (1, 1), padding='same', use_bias=False, strides=(1, 1),\n+    x = Convolution2D(pointwise_conv_filters, (1, 1),\n+                      padding='same',\n+                      use_bias=False,\n+                      strides=(1, 1),\n                       name='conv_pw_%d' % id)(x)\n     x = BatchNormalization(axis=channel_axis, name='conv_pw_%d_bn' % id)(x)\n-    x = Activation(relu6, name='conv_pw_%d_relu' % id)(x)\n-\n-    return x\n+    return Activation(relu6, name='conv_pw_%d_relu' % id)(x)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e4ab777d073fe648584dde25207540bf772dbc2d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 12678 | Contributors (this commit): 72 | Commits (past 90d): 13 | Contributors (cumulative): 78 | DMM Complexity: None\n\nDIFF:\n@@ -27,7 +27,7 @@ class MaxNorm(Constraint):\n             has shape `(input_dim, output_dim)`,\n             set `axis` to `0` to constrain each weight vector\n             of length `(input_dim,)`.\n-            In a `Convolution2D` layer with `data_format=\"channels_last\"`,\n+            In a `Conv2D` layer with `data_format=\"channels_last\"`,\n             the weight tensor has shape\n             `(rows, cols, input_depth, output_depth)`,\n             set `axis` to `[0, 1, 2]`\n@@ -71,7 +71,7 @@ class UnitNorm(Constraint):\n             has shape `(input_dim, output_dim)`,\n             set `axis` to `0` to constrain each weight vector\n             of length `(input_dim,)`.\n-            In a `Convolution2D` layer with `data_format=\"channels_last\"`,\n+            In a `Conv2D` layer with `data_format=\"channels_last\"`,\n             the weight tensor has shape\n             `(rows, cols, input_depth, output_depth)`,\n             set `axis` to `[0, 1, 2]`\n@@ -112,7 +112,7 @@ class MinMaxNorm(Constraint):\n             has shape `(input_dim, output_dim)`,\n             set `axis` to `0` to constrain each weight vector\n             of length `(input_dim,)`.\n-            In a `Convolution2D` layer with `dim_ordering=\"tf\"`,\n+            In a `Conv2D` layer with `data_format=\"channels_last\"`,\n             the weight tensor has shape\n             `(rows, cols, input_depth, output_depth)`,\n             set `axis` to `[0, 1, 2]`\n\n@@ -456,7 +456,7 @@ class Flatten(Layer):\n \n     ```python\n         model = Sequential()\n-        model.add(Convolution2D(64, 3, 3,\n+        model.add(Conv2D(64, 3, 3,\n                                 border_mode='same',\n                                 input_shape=(3, 32, 32)))\n         # now: model.output_shape == (None, 64, 32, 32)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a09c9f6c2da3b70d3a48a791e178af49811c0933", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 925 | Contributors (this commit): 11 | Commits (past 90d): 9 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -281,8 +281,9 @@ def test_model_methods():\n     out = model.predict([input_a_np, input_b_np], batch_size=4)\n \n     # empty batch\n-    with pytest.raises(StopIteration):\n+    with pytest.raises(ValueError):\n         def gen_data():\n+            while True:\n                 yield (np.asarray([]), np.asarray([]))\n         out = model.evaluate_generator(gen_data(), steps=1)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9a4598da50457317a7ffdc681d2a2998380fa5a1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 65 | Lines Deleted: 41 | Files Changed: 1 | Hunks: 24 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 106 | Churn Cumulative: 764 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 0.4166666666666667\n\nDIFF:\n@@ -3,40 +3,47 @@\n MobileNet is a general architecture and can be used for multiple use cases.\n Depending on the use case, it can use different input layer size and\n different width factors. This allows different width models to reduce\n-the number of multiply-adds and thereby reduce inference cost on mobile devices.\n+the number of multiply-adds and thereby\n+reduce inference cost on mobile devices.\n \n MobileNets support any input size greater than 32 x 32, with larger image sizes\n-offering better performance. The number of parameters and number of multiply-adds\n-can be modified by using the `alpha` parameter, which increases/decreases the number\n-of filters in each layer. By altering the image size and `alpha` parameter,\n+offering better performance.\n+The number of parameters and number of multiply-adds\n+can be modified by using the `alpha` parameter,\n+which increases/decreases the number of filters in each layer.\n+By altering the image size and `alpha` parameter,\n all 16 models from the paper can be built, with ImageNet weights provided.\n \n The paper demonstrates the performance of MobileNets using `alpha` values of\n-1.0 (also called 100 % MobileNet), 0.75, 0.5 and 0.25. For each of these `alpha`\n-values, weights for 4 different input image sizes are provided (224, 192, 160, 128).\n+1.0 (also called 100 % MobileNet), 0.75, 0.5 and 0.25.\n+For each of these `alpha` values, weights for 4 different input image sizes\n+are provided (224, 192, 160, 128).\n \n-The following table describes the size and accuracy of the 100% MobileNet on size 224 x 224\n------------------------------------------------------------------------------------------\n-Width Multiplier (alpha) | ImageNet Accuracy | Million Multiply-Adds | Million Parameters\n------------------------------------------------------------------------------------------\n+The following table describes the size and accuracy of the 100% MobileNet\n+on size 224 x 224:\n+----------------------------------------------------------------------------\n+Width Multiplier (alpha) | ImageNet Acc |  Multiply-Adds (M) |  Params (M)\n+----------------------------------------------------------------------------\n |   1.0 MobileNet-224    |    70.6 %     |        529        |     4.2     |\n |   0.75 MobileNet-224   |    68.4 %     |        325        |     2.6     |\n |   0.50 MobileNet-224   |    63.7 %     |        149        |     1.3     |\n |   0.25 MobileNet-224   |    50.6 %     |        41         |     0.5     |\n------------------------------------------------------------------------------------------\n+----------------------------------------------------------------------------\n \n-The following table describes the performance of the 100 % MobileNet on various input sizes\n------------------------------------------------------------------------------------------\n-        Resolution       | ImageNet Accuracy | Million Multiply-Adds | Million Parameters\n------------------------------------------------------------------------------------------\n+The following table describes the performance of\n+the 100 % MobileNet on various input sizes:\n+------------------------------------------------------------------------\n+      Resolution      | ImageNet Acc | Multiply-Adds (M) | Params (M)\n+------------------------------------------------------------------------\n |  1.0 MobileNet-224  |    70.6 %    |        529        |     4.2     |\n |  1.0 MobileNet-192  |    69.1 %    |        529        |     4.2     |\n |  1.0 MobileNet-160  |    67.2 %    |        529        |     4.2     |\n |  1.0 MobileNet-128  |    64.4 %    |        529        |     4.2     |\n------------------------------------------------------------------------------------------\n+------------------------------------------------------------------------\n \n-The weights for all 16 models are obtained and translated from Tensorflow checkpoints\n-found in https://github.com/tensorflow/models/blob/master/slim/nets/mobilenet_v1.md\n+The weights for all 16 models are obtained and translated\n+from Tensorflow checkpoints found at\n+https://github.com/tensorflow/models/blob/master/slim/nets/mobilenet_v1.md\n \n # Reference\n - [MobileNets: Efficient Convolutional Neural Networks for\n@@ -353,7 +360,8 @@ def MobileNet(input_shape=None,\n \n     if K.backend() != 'tensorflow':\n         raise RuntimeError('Only Tensorflow backend is currently supported, '\n-                           'as other backends do not support depthwise convolution.')\n+                           'as other backends do not support '\n+                           'depthwise convolution.')\n \n     if weights not in {'imagenet', None}:\n         raise ValueError('The `weights` argument should be either '\n@@ -370,7 +378,8 @@ def MobileNet(input_shape=None,\n                              'depth multiplier must be 1')\n \n         if alpha not in [0.25, 0.50, 0.75, 1.0]:\n-            raise ValueError('If imagenet weights are being loaded, alpha can be one of'\n+            raise ValueError('If imagenet weights are being loaded, '\n+                             'alpha can be one of'\n                              '`0.25`, `0.50`, `0.75` or `1.0` only.')\n \n         rows, cols = (0, 1) if K.image_data_format() == 'channels_last' else (1, 2)\n@@ -399,8 +408,8 @@ def MobileNet(input_shape=None,\n         old_data_format = None\n \n     # Determine proper input shape. Note, include_top is False by default, as\n-    # input shape can be anything larger than 32x32 and the same number of parameters\n-    # will be used.\n+    # input shape can be anything larger than 32x32\n+    # and the same number of parameters will be used.\n     input_shape = _obtain_input_shape(input_shape,\n                                       default_size=224,\n                                       min_size=32,\n@@ -416,23 +425,27 @@ def MobileNet(input_shape=None,\n             img_input = input_tensor\n \n     x = _conv_block(img_input, 32, alpha, strides=(2, 2))\n-    x = _depthwise_conv_block(x, 64, alpha, depth_multiplier, id=1)\n+    x = _depthwise_conv_block(x, 64, alpha, depth_multiplier, block_id=1)\n \n-    x = _depthwise_conv_block(x, 128, alpha, depth_multiplier, strides=(2, 2), id=2)\n-    x = _depthwise_conv_block(x, 128, alpha, depth_multiplier, id=3)\n+    x = _depthwise_conv_block(x, 128, alpha, depth_multiplier,\n+                              strides=(2, 2), block_id=2)\n+    x = _depthwise_conv_block(x, 128, alpha, depth_multiplier, block_id=3)\n \n-    x = _depthwise_conv_block(x, 256, alpha, depth_multiplier, strides=(2, 2), id=4)\n-    x = _depthwise_conv_block(x, 256, alpha, depth_multiplier, id=5)\n+    x = _depthwise_conv_block(x, 256, alpha, depth_multiplier,\n+                              strides=(2, 2), block_id=4)\n+    x = _depthwise_conv_block(x, 256, alpha, depth_multiplier, block_id=5)\n \n-    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, strides=(2, 2), id=6)\n-    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, id=7)\n-    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, id=8)\n-    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, id=9)\n-    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, id=10)\n-    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, id=11)\n+    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier,\n+                              strides=(2, 2), block_id=6)\n+    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, block_id=7)\n+    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, block_id=8)\n+    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, block_id=9)\n+    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, block_id=10)\n+    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, block_id=11)\n \n-    x = _depthwise_conv_block(x, 1024, alpha, depth_multiplier, strides=(2, 2), id=12)\n-    x = _depthwise_conv_block(x, 1024, alpha, depth_multiplier, id=13)\n+    x = _depthwise_conv_block(x, 1024, alpha, depth_multiplier,\n+                              strides=(2, 2), block_id=12)\n+    x = _depthwise_conv_block(x, 1024, alpha, depth_multiplier, block_id=13)\n \n     if include_top:\n         if K.image_data_format() == 'channels_first':\n@@ -443,7 +456,8 @@ def MobileNet(input_shape=None,\n         x = GlobalAveragePooling2D()(x)\n         x = Reshape(shape, name='reshape_1')(x)\n         x = Dropout(dropout, name='dropout')(x)\n-        x = Convolution2D(classes, (1, 1), padding='same', name='conv_preds')(x)\n+        x = Convolution2D(classes, (1, 1),\n+                          padding='same', name='conv_preds')(x)\n         x = Activation('softmax', name='act_softmax')(x)\n         x = Reshape((classes,), name='reshape_2')(x)\n     else:\n@@ -543,6 +557,9 @@ def _conv_block(input, filters, alpha, kernel=(3, 3), strides=(1, 1)):\n         or 4D tensor with shape:\n         `(samples, new_rows, new_cols, filters)` if data_format='channels_last'.\n         `rows` and `cols` values might have changed due to stride.\n+\n+    # Returns\n+        Output tensor of block.\n     \"\"\"\n     channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n     filters = int(filters * alpha)\n@@ -552,13 +569,11 @@ def _conv_block(input, filters, alpha, kernel=(3, 3), strides=(1, 1)):\n                       strides=strides,\n                       name='conv1')(input)\n     x = BatchNormalization(axis=channel_axis, name='conv1_bn')(x)\n-    x = Activation(relu6, name='conv1_relu')(x)\n-\n-    return x\n+    return Activation(relu6, name='conv1_relu')(x)\n \n \n def _depthwise_conv_block(input, pointwise_conv_filters, alpha,\n-                          depth_multiplier=1, strides=(1, 1), id=1):\n+                          depth_multiplier=1, strides=(1, 1), block_id=1):\n     \"\"\"Adds a depthwise convolution block.\n \n     A depthwise convolution block consists of a depthwise conv,\n@@ -588,7 +603,7 @@ def _depthwise_conv_block(input, pointwise_conv_filters, alpha,\n             all spatial dimensions.\n             Specifying any stride value != 1 is incompatible with specifying\n             any `dilation_rate` value != 1.\n-        id: Integer, a unique identification designating  the block number.\n+        block_id: Integer, a unique identification designating the block number.\n \n     # Input shape\n         4D tensor with shape:\n@@ -602,6 +617,9 @@ def _depthwise_conv_block(input, pointwise_conv_filters, alpha,\n         or 4D tensor with shape:\n         `(batch, new_rows, new_cols, filters)` if data_format='channels_last'.\n         `rows` and `cols` values might have changed due to stride.\n+\n+    # Returns\n+        Output tensor of block.\n     \"\"\"\n     channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n     pointwise_conv_filters = int(pointwise_conv_filters * alpha)\n@@ -611,14 +629,14 @@ def _depthwise_conv_block(input, pointwise_conv_filters, alpha,\n                         depth_multiplier=depth_multiplier,\n                         strides=strides,\n                         use_bias=False,\n-                        name='conv_dw_%d' % id)(input)\n-    x = BatchNormalization(axis=channel_axis, name='conv_dw_%d_bn' % id)(x)\n-    x = Activation(relu6, name='conv_dw_%d_relu' % id)(x)\n+                        name='conv_dw_%d' % block_id)(input)\n+    x = BatchNormalization(axis=channel_axis, name='conv_dw_%d_bn' % block_id)(x)\n+    x = Activation(relu6, name='conv_dw_%d_relu' % block_id)(x)\n \n     x = Convolution2D(pointwise_conv_filters, (1, 1),\n                       padding='same',\n                       use_bias=False,\n                       strides=(1, 1),\n-                      name='conv_pw_%d' % id)(x)\n-    x = BatchNormalization(axis=channel_axis, name='conv_pw_%d_bn' % id)(x)\n-    return Activation(relu6, name='conv_pw_%d_relu' % id)(x)\n+                      name='conv_pw_%d' % block_id)(x)\n+    x = BatchNormalization(axis=channel_axis, name='conv_pw_%d_bn' % block_id)(x)\n+    return Activation(relu6, name='conv_pw_%d_relu' % block_id)(x)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#98db0285ee5b8a1fcb18eb46793c117d6305e0fb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 776 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -515,11 +515,11 @@ def MobileNet(input_shape=None,\n     return model\n \n \n-def _conv_block(input, filters, alpha, kernel=(3, 3), strides=(1, 1)):\n+def _conv_block(inputs, filters, alpha, kernel=(3, 3), strides=(1, 1)):\n     \"\"\"Adds an initial convolution layer (with batch normalization and relu6).\n \n     # Arguments\n-        input: Input tensor of shape `(rows, cols, 3)`\n+        inputs: Input tensor of shape `(rows, cols, 3)`\n             (with `channels_last` data format) or\n             (3, rows, cols) (with `channels_first` data format).\n             It should have exactly 3 inputs channels,\n@@ -567,12 +567,12 @@ def _conv_block(input, filters, alpha, kernel=(3, 3), strides=(1, 1)):\n                       padding='same',\n                       use_bias=False,\n                       strides=strides,\n-                      name='conv1')(input)\n+                      name='conv1')(inputs)\n     x = BatchNormalization(axis=channel_axis, name='conv1_bn')(x)\n     return Activation(relu6, name='conv1_relu')(x)\n \n \n-def _depthwise_conv_block(input, pointwise_conv_filters, alpha,\n+def _depthwise_conv_block(inputs, pointwise_conv_filters, alpha,\n                           depth_multiplier=1, strides=(1, 1), block_id=1):\n     \"\"\"Adds a depthwise convolution block.\n \n@@ -581,7 +581,7 @@ def _depthwise_conv_block(input, pointwise_conv_filters, alpha,\n     batch normalization and relu6 activation.\n \n     # Arguments\n-        input: Input tensor of shape `(rows, cols, channels)`\n+        inputs: Input tensor of shape `(rows, cols, channels)`\n             (with `channels_last` data format) or\n             (channels, rows, cols) (with `channels_first` data format).\n         pointwise_conv_filters: Integer, the dimensionality of the output space\n@@ -629,7 +629,7 @@ def _depthwise_conv_block(input, pointwise_conv_filters, alpha,\n                         depth_multiplier=depth_multiplier,\n                         strides=strides,\n                         use_bias=False,\n-                        name='conv_dw_%d' % block_id)(input)\n+                        name='conv_dw_%d' % block_id)(inputs)\n     x = BatchNormalization(axis=channel_axis, name='conv_dw_%d_bn' % block_id)(x)\n     x = Activation(relu6, name='conv_dw_%d_relu' % block_id)(x)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#94397e08aeda5f278360929de3abdd33d596f422", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 783 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -61,7 +61,6 @@ from ..layers import Activation\n from ..layers import Dropout\n from ..layers import Reshape\n from ..layers import BatchNormalization\n-from ..layers import Convolution2D\n from ..layers import GlobalAveragePooling2D\n from ..layers import GlobalMaxPooling2D\n from ..layers import Conv2D\n@@ -456,7 +455,7 @@ def MobileNet(input_shape=None,\n         x = GlobalAveragePooling2D()(x)\n         x = Reshape(shape, name='reshape_1')(x)\n         x = Dropout(dropout, name='dropout')(x)\n-        x = Convolution2D(classes, (1, 1),\n+        x = Conv2D(classes, (1, 1),\n                    padding='same', name='conv_preds')(x)\n         x = Activation('softmax', name='act_softmax')(x)\n         x = Reshape((classes,), name='reshape_2')(x)\n@@ -563,7 +562,7 @@ def _conv_block(inputs, filters, alpha, kernel=(3, 3), strides=(1, 1)):\n     \"\"\"\n     channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n     filters = int(filters * alpha)\n-    x = Convolution2D(filters, kernel,\n+    x = Conv2D(filters, kernel,\n                padding='same',\n                use_bias=False,\n                strides=strides,\n@@ -633,7 +632,7 @@ def _depthwise_conv_block(inputs, pointwise_conv_filters, alpha,\n     x = BatchNormalization(axis=channel_axis, name='conv_dw_%d_bn' % block_id)(x)\n     x = Activation(relu6, name='conv_dw_%d_relu' % block_id)(x)\n \n-    x = Convolution2D(pointwise_conv_filters, (1, 1),\n+    x = Conv2D(pointwise_conv_filters, (1, 1),\n                padding='same',\n                use_bias=False,\n                strides=(1, 1),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7866fbaa1a9d89cc168bc950fa47f5de3b173ca8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 9765 | Contributors (this commit): 46 | Commits (past 90d): 8 | Contributors (cumulative): 46 | DMM Complexity: None\n\nDIFF:\n@@ -476,7 +476,7 @@ class Conv3D(_Conv):\n     When using this layer as the first layer in a model,\n     provide the keyword argument `input_shape`\n     (tuple of integers, does not include the sample axis),\n-    e.g. `input_shape=(128, 128, 128, 3)` for 128x128x128 volumes\n+    e.g. `input_shape=(128, 128, 128, 1)` for 128x128x128 volumes\n     with a single channel,\n     in `data_format=\"channels_last\"`.\n \n@@ -484,7 +484,7 @@ class Conv3D(_Conv):\n         filters: Integer, the dimensionality of the output space\n             (i.e. the number output of filters in the convolution).\n         kernel_size: An integer or tuple/list of 3 integers, specifying the\n-            width and height of the 3D convolution window.\n+            depth, height and width of the 3D convolution window.\n             Can be a single integer to specify the same value for\n             all spatial dimensions.\n         strides: An integer or tuple/list of 3 integers,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c6ec39258bd606db8365d4e53209ebf456ccbd49", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 75 | Lines Deleted: 75 | Files Changed: 6 | Hunks: 43 | Methods Changed: 23 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 150 | Churn Cumulative: 15846 | Contributors (this commit): 116 | Commits (past 90d): 76 | Contributors (cumulative): 192 | DMM Complexity: None\n\nDIFF:\n@@ -451,15 +451,15 @@ def shape(x):\n         >>> tf_session = K.get_session()\n         >>> val = np.array([[1, 2], [3, 4]])\n         >>> kvar = K.variable(value=val)\n-        >>> input = keras.backend.placeholder(shape=(2, 4, 5))\n+        >>> inputs = keras.backend.placeholder(shape=(2, 4, 5))\n         >>> K.shape(kvar)\n         <tf.Tensor 'Shape_8:0' shape=(2,) dtype=int32>\n-        >>> K.shape(input)\n+        >>> K.shape(inputs)\n         <tf.Tensor 'Shape_9:0' shape=(3,) dtype=int32>\n         # To get integer shape (Instead, you can use K.int_shape(x))\n         >>> K.shape(kvar).eval(session=tf_session)\n         array([2, 2], dtype=int32)\n-        >>> K.shape(input).eval(session=tf_session)\n+        >>> K.shape(inputs).eval(session=tf_session)\n         array([2, 4, 5], dtype=int32)\n     ```\n     \"\"\"\n@@ -478,8 +478,8 @@ def int_shape(x):\n     # Examples\n     ```python\n         >>> from keras import backend as K\n-        >>> input = K.placeholder(shape=(2, 4, 5))\n-        >>> K.int_shape(input)\n+        >>> inputs = K.placeholder(shape=(2, 4, 5))\n+        >>> K.int_shape(inputs)\n         (2, 4, 5)\n         >>> val = np.array([[1, 2], [3, 4]])\n         >>> kvar = K.variable(value=val)\n@@ -508,10 +508,10 @@ def ndim(x):\n     # Examples\n     ```python\n         >>> from keras import backend as K\n-        >>> input = K.placeholder(shape=(2, 4, 5))\n+        >>> inputs = K.placeholder(shape=(2, 4, 5))\n         >>> val = np.array([[1, 2], [3, 4]])\n         >>> kvar = K.variable(value=val)\n-        >>> K.ndim(input)\n+        >>> K.ndim(inputs)\n         3\n         >>> K.ndim(kvar)\n         2\n@@ -1089,10 +1089,10 @@ def transpose(x):\n     ```\n \n     ```python\n-        >>> input = K.placeholder((2, 3))\n-        >>> input\n+        >>> inputs = K.placeholder((2, 3))\n+        >>> inputs\n         <tf.Tensor 'Placeholder_11:0' shape=(2, 3) dtype=float32>\n-        >>> input_transposed = K.transpose(input)\n+        >>> input_transposed = K.transpose(inputs)\n         >>> input_transposed\n         <tf.Tensor 'transpose_4:0' shape=(3, 2) dtype=float32>\n \n@@ -2327,12 +2327,12 @@ def rnn(step_function, inputs, initial_states,\n     # Arguments\n         step_function: RNN step function.\n             Parameters:\n-                input: tensor with shape `(samples, ...)` (no time dimension),\n+                inputs: tensor with shape `(samples, ...)` (no time dimension),\n                     representing input for the batch of samples at a certain\n                     time step.\n                 states: list of tensors.\n             Returns:\n-                output: tensor with shape `(samples, output_dim)`\n+                outputs: tensor with shape `(samples, output_dim)`\n                     (no time dimension).\n                 new_states: list of tensors, same length and shapes\n                     as 'states'. The first state in the list must be the\n\n@@ -1229,12 +1229,12 @@ def rnn(step_function, inputs, initial_states,\n             (at least 3D).\n         step_function:\n             Parameters:\n-                input: tensor with shape (samples, ...) (no time dimension),\n+                inputs: tensor with shape (samples, ...) (no time dimension),\n                     representing input for the batch of samples at a certain\n                     time step.\n                 states: list of tensors.\n             Returns:\n-                output: tensor with shape (samples, ...) (no time dimension),\n+                outputs: tensor with shape (samples, ...) (no time dimension),\n                 new_states: list of tensors, same length and shapes\n                     as 'states'.\n         initial_states: tensor with shape (samples, ...) (no time dimension),\n@@ -1315,14 +1315,14 @@ def rnn(step_function, inputs, initial_states,\n             if len(initial_states) > 0:\n                 initial_states[0] = T.unbroadcast(initial_states[0], 0, 1)\n \n-            def _step(input, mask, output_tm1, *states):\n-                output, new_states = step_function(input, states)\n+            def _step(inputs, mask, output_tm1, *states):\n+                outputs, new_states = step_function(inputs, states)\n                 # output previous output if masked.\n-                output = T.switch(mask, output, output_tm1)\n+                outputs = T.switch(mask, outputs, output_tm1)\n                 return_states = []\n                 for state, new_state in zip(states, new_states):\n                     return_states.append(T.switch(mask, new_state, state))\n-                return [output] + return_states\n+                return [outputs] + return_states\n \n             results, _ = theano.scan(\n                 _step,\n@@ -1348,8 +1348,8 @@ def rnn(step_function, inputs, initial_states,\n             successive_states = []\n             states = initial_states\n             for i in indices:\n-                output, states = step_function(inputs[i], states + constants)\n-                successive_outputs.append(output)\n+                outputs, states = step_function(inputs[i], states + constants)\n+                successive_outputs.append(outputs)\n                 successive_states.append(states)\n             outputs = T.stack(*successive_outputs)\n             states = []\n@@ -1357,9 +1357,9 @@ def rnn(step_function, inputs, initial_states,\n                 states.append(T.stack(*[states_at_step[i] for states_at_step in successive_states]))\n \n         else:\n-            def _step(input, *states):\n-                output, new_states = step_function(input, states)\n-                return [output] + new_states\n+            def _step(inputs, *states):\n+                outputs, new_states = step_function(inputs, states)\n+                return [outputs] + new_states\n \n             # Theano likes to make shape==1 dimensions in the initial states (outputs_info) broadcastable\n             if len(initial_states) > 0:\n\n@@ -443,9 +443,9 @@ def test_trainable_argument():\n     assert_allclose(out, out_2)\n \n     # test with nesting\n-    input = Input(shape=(3,))\n-    output = model(input)\n-    model = Model(input, output)\n+    inputs = Input(shape=(3,))\n+    outputs = model(inputs)\n+    model = Model(inputs, outputs)\n     model.compile('rmsprop', 'mse')\n     out = model.predict(x)\n     model.train_on_batch(x, y)\n\n@@ -428,29 +428,29 @@ def test_zero_padding_1d():\n     input_dim = 2\n     num_steps = 5\n     shape = (num_samples, num_steps, input_dim)\n-    input = np.ones(shape)\n+    inputs = np.ones(shape)\n \n     # basic test\n     layer_test(convolutional.ZeroPadding1D,\n                kwargs={'padding': 2},\n-               input_shape=input.shape)\n+               input_shape=inputs.shape)\n     layer_test(convolutional.ZeroPadding1D,\n                kwargs={'padding': (1, 2)},\n-               input_shape=input.shape)\n+               input_shape=inputs.shape)\n \n     # correctness test\n     layer = convolutional.ZeroPadding1D(padding=2)\n     layer.build(shape)\n-    output = layer(K.variable(input))\n-    np_output = K.eval(output)\n+    outputs = layer(K.variable(inputs))\n+    np_output = K.eval(outputs)\n     for offset in [0, 1, -1, -2]:\n         assert_allclose(np_output[:, offset, :], 0.)\n     assert_allclose(np_output[:, 2:-2, :], 1.)\n \n     layer = convolutional.ZeroPadding1D(padding=(1, 2))\n     layer.build(shape)\n-    output = layer(K.variable(input))\n-    np_output = K.eval(output)\n+    outputs = layer(K.variable(inputs))\n+    np_output = K.eval(outputs)\n     for left_offset in [0]:\n         assert_allclose(np_output[:, left_offset, :], 0.)\n     for right_offset in [-1, -2]:\n@@ -481,8 +481,8 @@ def test_zero_padding_2d():\n         layer = convolutional.ZeroPadding2D(padding=(2, 2),\n                                             data_format=data_format)\n         layer.build(inputs.shape)\n-        output = layer(K.variable(inputs))\n-        np_output = K.eval(output)\n+        outputs = layer(K.variable(inputs))\n+        np_output = K.eval(outputs)\n         if data_format == 'channels_last':\n             for offset in [0, 1, -1, -2]:\n                 assert_allclose(np_output[:, offset, :, :], 0.)\n@@ -497,8 +497,8 @@ def test_zero_padding_2d():\n         layer = convolutional.ZeroPadding2D(padding=((1, 2), (3, 4)),\n                                             data_format=data_format)\n         layer.build(inputs.shape)\n-        output = layer(K.variable(inputs))\n-        np_output = K.eval(output)\n+        outputs = layer(K.variable(inputs))\n+        np_output = K.eval(outputs)\n         if data_format == 'channels_last':\n             for top_offset in [0]:\n                 assert_allclose(np_output[:, top_offset, :, :], 0.)\n@@ -545,8 +545,8 @@ def test_zero_padding_3d():\n         layer = convolutional.ZeroPadding3D(padding=(2, 2, 2),\n                                             data_format=data_format)\n         layer.build(inputs.shape)\n-        output = layer(K.variable(inputs))\n-        np_output = K.eval(output)\n+        outputs = layer(K.variable(inputs))\n+        np_output = K.eval(outputs)\n         if data_format == 'channels_last':\n             for offset in [0, 1, -1, -2]:\n                 assert_allclose(np_output[:, offset, :, :, :], 0.)\n@@ -563,8 +563,8 @@ def test_zero_padding_3d():\n         layer = convolutional.ZeroPadding3D(padding=((1, 2), (3, 4), (0, 2)),\n                                             data_format=data_format)\n         layer.build(inputs.shape)\n-        output = layer(K.variable(inputs))\n-        np_output = K.eval(output)\n+        outputs = layer(K.variable(inputs))\n+        np_output = K.eval(outputs)\n         if data_format == 'channels_last':\n             for dim1_offset in [0, -1, -2]:\n                 assert_allclose(np_output[:, dim1_offset, :, :, :], 0.)\n@@ -616,8 +616,8 @@ def test_upsampling_2d():\n                     size=(length_row, length_col),\n                     data_format=data_format)\n                 layer.build(inputs.shape)\n-                output = layer(K.variable(inputs))\n-                np_output = K.eval(output)\n+                outputs = layer(K.variable(inputs))\n+                np_output = K.eval(outputs)\n                 if data_format == 'channels_first':\n                     assert np_output.shape[2] == length_row * input_num_row\n                     assert np_output.shape[3] == length_col * input_num_col\n@@ -667,8 +667,8 @@ def test_upsampling_3d():\n                         size=(length_dim1, length_dim2, length_dim3),\n                         data_format=data_format)\n                     layer.build(inputs.shape)\n-                    output = layer(K.variable(inputs))\n-                    np_output = K.eval(output)\n+                    outputs = layer(K.variable(inputs))\n+                    np_output = K.eval(outputs)\n                     if data_format == 'channels_first':\n                         assert np_output.shape[2] == length_dim1 * input_len_dim1\n                         assert np_output.shape[3] == length_dim2 * input_len_dim2\n@@ -729,8 +729,8 @@ def test_cropping_2d():\n         layer = convolutional.Cropping2D(cropping=cropping,\n                                          data_format=data_format)\n         layer.build(inputs.shape)\n-        output = layer(K.variable(inputs))\n-        np_output = K.eval(output)\n+        outputs = layer(K.variable(inputs))\n+        np_output = K.eval(outputs)\n         # compare with numpy\n         if data_format == 'channels_first':\n             expected_out = inputs[:,\n@@ -757,8 +757,8 @@ def test_cropping_2d():\n         layer = convolutional.Cropping2D(cropping=cropping,\n                                          data_format=data_format)\n         layer.build(inputs.shape)\n-        output = layer(K.variable(inputs))\n-        np_output = K.eval(output)\n+        outputs = layer(K.variable(inputs))\n+        np_output = K.eval(outputs)\n         # compare with input\n         assert_allclose(np_output, inputs)\n \n@@ -794,8 +794,8 @@ def test_cropping_3d():\n         layer = convolutional.Cropping3D(cropping=cropping,\n                                          data_format=data_format)\n         layer.build(inputs.shape)\n-        output = layer(K.variable(inputs))\n-        np_output = K.eval(output)\n+        outputs = layer(K.variable(inputs))\n+        np_output = K.eval(outputs)\n         # compare with numpy\n         if data_format == 'channels_first':\n             expected_out = inputs[:,\n@@ -824,8 +824,8 @@ def test_cropping_3d():\n         layer = convolutional.Cropping3D(cropping=cropping,\n                                          data_format=data_format)\n         layer.build(inputs.shape)\n-        output = layer(K.variable(inputs))\n-        np_output = K.eval(output)\n+        outputs = layer(K.variable(inputs))\n+        np_output = K.eval(outputs)\n         # compare with input\n         assert_allclose(np_output, inputs)\n \n\n@@ -158,18 +158,18 @@ def test_Bidirectional():\n         model.fit(x, y, epochs=1, batch_size=1)\n \n         # test with functional API\n-        input = Input((timesteps, dim))\n-        output = wrappers.Bidirectional(rnn(output_dim, dropout=dropout_rate,\n+        inputs = Input((timesteps, dim))\n+        outputs = wrappers.Bidirectional(rnn(output_dim, dropout=dropout_rate,\n                                              recurrent_dropout=dropout_rate),\n-                                        merge_mode=mode)(input)\n-        model = Model(input, output)\n+                                         merge_mode=mode)(inputs)\n+        model = Model(inputs, outputs)\n         model.compile(loss='mse', optimizer='sgd')\n         model.fit(x, y, epochs=1, batch_size=1)\n \n         # Bidirectional and stateful\n-        input = Input(batch_shape=(1, timesteps, dim))\n-        output = wrappers.Bidirectional(rnn(output_dim, stateful=True), merge_mode=mode)(input)\n-        model = Model(input, output)\n+        inputs = Input(batch_shape=(1, timesteps, dim))\n+        outputs = wrappers.Bidirectional(rnn(output_dim, stateful=True), merge_mode=mode)(inputs)\n+        model = Model(inputs, outputs)\n         model.compile(loss='mse', optimizer='sgd')\n         model.fit(x, y, epochs=1, batch_size=1)\n \n\n@@ -78,11 +78,11 @@ def test_sequential_model_saving_2():\n \n @keras_test\n def test_functional_model_saving():\n-    input = Input(shape=(3,))\n-    x = Dense(2)(input)\n-    output = Dense(3)(x)\n+    inputs = Input(shape=(3,))\n+    x = Dense(2)(inputs)\n+    outputs = Dense(3)(x)\n \n-    model = Model(input, output)\n+    model = Model(inputs, outputs)\n     model.compile(loss=losses.MSE,\n                   optimizer=optimizers.RMSprop(lr=0.0001),\n                   metrics=[metrics.categorical_accuracy])\n@@ -103,12 +103,12 @@ def test_functional_model_saving():\n \n @keras_test\n def test_saving_multiple_metrics_outputs():\n-    input = Input(shape=(5,))\n-    x = Dense(5)(input)\n+    inputs = Input(shape=(5,))\n+    x = Dense(5)(inputs)\n     output1 = Dense(1, name='output1')(x)\n     output2 = Dense(1, name='output2')(x)\n \n-    model = Model(inputs=input, outputs=[output1, output2])\n+    model = Model(inputs=inputs, outputs=[output1, output2])\n \n     metrics = {'output1': ['mse', 'binary_accuracy'],\n                'output2': ['mse', 'binary_accuracy']\n@@ -270,11 +270,11 @@ def square_fn(x):\n \n @keras_test\n def test_saving_lambda_custom_objects():\n-    input = Input(shape=(3,))\n-    x = Lambda(lambda x: square_fn(x), output_shape=(3,))(input)\n-    output = Dense(3)(x)\n+    inputs = Input(shape=(3,))\n+    x = Lambda(lambda x: square_fn(x), output_shape=(3,))(inputs)\n+    outputs = Dense(3)(x)\n \n-    model = Model(input, output)\n+    model = Model(inputs, outputs)\n     model.compile(loss=losses.MSE,\n                   optimizer=optimizers.RMSprop(lr=0.0001),\n                   metrics=[metrics.categorical_accuracy])\n@@ -297,10 +297,10 @@ def test_saving_lambda_custom_objects():\n def test_saving_lambda_numpy_array_arguments():\n     mean = np.random.random((4, 2, 3))\n     std = np.abs(np.random.random((4, 2, 3))) + 1e-5\n-    input = Input(shape=(4, 2, 3))\n-    output = Lambda(lambda image, mu, std: (image - mu) / std,\n-                    arguments={'mu': mean, 'std': std})(input)\n-    model = Model(input, output)\n+    inputs = Input(shape=(4, 2, 3))\n+    outputs = Lambda(lambda image, mu, std: (image - mu) / std,\n+                     arguments={'mu': mean, 'std': std})(inputs)\n+    model = Model(inputs, outputs)\n     model.compile(loss='mse', optimizer='sgd', metrics=['acc'])\n \n     _, fname = tempfile.mkstemp('.h5')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5edd96a9f88021acf238a9c8a94ffa8027b4da21", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 21 | Churn Cumulative: 4678 | Contributors (this commit): 55 | Commits (past 90d): 25 | Contributors (cumulative): 55 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1770,11 +1770,12 @@ class Model(Container):\n             for cbk in callbacks:\n                 cbk.validation_data = val_data\n         is_sequence = isinstance(generator, Sequence)\n-        if not is_sequence and use_multiprocessing:\n+        if not is_sequence and use_multiprocessing and workers > 1:\n             warnings.warn(\n                 UserWarning('Using a generator with `use_multiprocessing=True`'\n-                            ' may duplicate your data.Please consider using '\n-                            'the `keras.utils.Sequence` class.'))\n+                            ' and multiple workers may duplicate your data.'\n+                            ' Please consider using the`keras.utils.Sequence'\n+                            ' class.'))\n         enqueuer = None\n \n         try:\n@@ -1920,11 +1921,12 @@ class Model(Container):\n         all_outs = []\n         batch_sizes = []\n         is_sequence = isinstance(generator, Sequence)\n-        if not is_sequence and use_multiprocessing:\n+        if not is_sequence and use_multiprocessing and workers > 1:\n             warnings.warn(\n                 UserWarning('Using a generator with `use_multiprocessing=True`'\n-                            ' may duplicate your data.Please consider using '\n-                            'the `keras.utils.Sequence` class.'))\n+                            ' and multiple workers may duplicate your data.'\n+                            ' Please consider using the`keras.utils.Sequence'\n+                            ' class.'))\n         enqueuer = None\n \n         try:\n@@ -2028,11 +2030,12 @@ class Model(Container):\n         wait_time = 0.01\n         all_outs = []\n         is_sequence = isinstance(generator, Sequence)\n-        if not is_sequence and use_multiprocessing:\n+        if not is_sequence and use_multiprocessing and workers > 1:\n             warnings.warn(\n                 UserWarning('Using a generator with `use_multiprocessing=True`'\n-                            ' may duplicate your data.Please consider using '\n-                            'the `keras.utils.Sequence` class.'))\n+                            ' and multiple workers may duplicate your data.'\n+                            ' Please consider using the`keras.utils.Sequence'\n+                            ' class.'))\n         enqueuer = None\n \n         try:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6c2dea64fca644d427eebfaf320a0acfb7802c08", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 426 | Lines Deleted: 0 | Files Changed: 5 | Hunks: 7 | Methods Changed: 10 | Complexity Δ (Sum/Max): 26/14 | Churn Δ: 426 | Churn Cumulative: 26653 | Contributors (this commit): 131 | Commits (past 90d): 70 | Contributors (cumulative): 220 | DMM Complexity: 0.7625\n\nDIFF:\n@@ -1386,6 +1386,41 @@ def conv3d(x, kernel, strides=(1, 1, 1), padding='valid',\n     return _postprocess_conv3d_output(x, data_format)\n \n \n+def conv3d_transpose(x, kernel, output_shape, strides=(1, 1, 1),\n+                     padding='valid', data_format=None):\n+    if data_format is None:\n+        data_format = image_data_format()\n+    if data_format not in {'channels_first', 'channels_last'}:\n+        raise ValueError('Unknown data_format ' + str(data_format))\n+\n+    x = _preprocess_conv3d_input(x, data_format)\n+    kernel = _preprocess_conv3d_kernel(kernel, data_format)\n+    padding = _preprocess_border_mode(padding)\n+    strides = (1,) + strides\n+    # cntk output_shape does not include batch axis\n+    output_shape = output_shape[1:]\n+    # in keras2, need handle output shape in different format\n+    if data_format == 'channels_last':\n+        shape = list(output_shape)\n+        shape[0] = output_shape[3]\n+        shape[1] = output_shape[0]\n+        shape[2] = output_shape[1]\n+        shape[3] = output_shape[2]\n+        output_shape = tuple(shape)\n+\n+    x = C.convolution_transpose(\n+        kernel,\n+        x,\n+        strides,\n+        auto_padding=[\n+            False,\n+            padding,\n+            padding,\n+            padding],\n+        output_shape=output_shape)\n+    return _postprocess_conv3d_output(x, data_format)\n+\n+\n def pool2d(x, pool_size, strides=(1, 1),\n            padding='valid', data_format=None,\n            pool_mode='max'):\n\n@@ -2911,6 +2911,26 @@ def in_top_k(predictions, targets, k):\n \n # CONVOLUTIONS\n \n+def _preprocess_deconv3d_output_shape(x, shape, data_format):\n+    \"\"\"Get the output_shape for the 3D deconvolution.\n+\n+    # Arguments\n+        x: input tensor.\n+        shape: output shape.\n+        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n+\n+    # Returns\n+        The output shape.\n+    \"\"\"\n+    if data_format == 'channels_first':\n+        shape = (shape[0], shape[2], shape[3], shape[4], shape[1])\n+\n+    if shape[0] is None:\n+        shape = (tf.shape(x)[0], ) + tuple(shape[1:])\n+        shape = tf.stack(list(shape))\n+    return shape\n+\n+\n def _preprocess_deconv_output_shape(x, shape, data_format):\n     \"\"\"Get the output_shape for the deconvolution.\n \n@@ -3286,6 +3306,43 @@ def conv3d(x, kernel, strides=(1, 1, 1), padding='valid',\n     return _postprocess_conv3d_output(x, data_format)\n \n \n+def conv3d_transpose(x, kernel, output_shape, strides=(1, 1, 1),\n+                     padding='valid', data_format=None):\n+    \"\"\"3D deconvolution (i.e. transposed convolution).\n+\n+    # Arguments\n+        x: input tensor.\n+        kernel: kernel tensor.\n+        output_shape: 1D int tensor for the output shape.\n+        strides: strides tuple.\n+        padding: string, \"same\" or \"valid\".\n+        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n+            Whether to use Theano or TensorFlow data format\n+            for inputs/kernels/outputs.\n+\n+    # Returns\n+        A tensor, result of transposed 3D convolution.\n+\n+    # Raises\n+        ValueError: if `data_format` is neither `channels_last` or `channels_first`.\n+    \"\"\"\n+    if data_format is None:\n+        data_format = image_data_format()\n+    if data_format not in {'channels_first', 'channels_last'}:\n+        raise ValueError('Unknown data_format ' + str(data_format))\n+    if isinstance(output_shape, (tuple, list)):\n+        output_shape = tf.stack(output_shape)\n+\n+    x = _preprocess_conv3d_input(x, data_format)\n+    output_shape = _preprocess_deconv3d_output_shape(x, output_shape, data_format)\n+    padding = _preprocess_padding(padding)\n+    strides = (1,) + strides + (1,)\n+\n+    x = tf.nn.conv3d_transpose(x, kernel, output_shape, strides,\n+                               padding=padding)\n+    return _postprocess_conv3d_output(x, data_format)\n+\n+\n def pool2d(x, pool_size, strides=(1, 1),\n            padding='valid', data_format=None,\n            pool_mode='max'):\n\n@@ -1968,6 +1968,63 @@ def conv3d(x, kernel, strides=(1, 1, 1),\n     return conv_out\n \n \n+def conv3d_transpose(x, kernel, output_shape, strides=(1, 1, 1),\n+                     padding='valid', data_format=None):\n+    \"\"\"3D deconvolution (transposed convolution).\n+\n+    # Arguments\n+        kernel: kernel tensor.\n+        output_shape: desired dimensions of output.\n+        strides: strides tuple.\n+        padding: string, \"same\" or \"valid\".\n+        data_format: \"channels_last\" or \"channels_first\".\n+            Whether to use Theano or TensorFlow data format\n+        in inputs/kernels/outputs.\n+\n+    # Raises\n+        ValueError: if using an even kernel size with padding 'same'.\n+    \"\"\"\n+    flip_filters = False\n+    if data_format is None:\n+        data_format = image_data_format()\n+    if data_format not in {'channels_first', 'channels_last'}:\n+        raise ValueError('Unknown data_format ' + data_format)\n+\n+    if data_format == 'channels_last':\n+        output_shape = (output_shape[0],\n+                        output_shape[4],\n+                        output_shape[1],\n+                        output_shape[2],\n+                        output_shape[3])\n+\n+    if hasattr(kernel, '_keras_shape'):\n+        kernel_shape = kernel._keras_shape\n+    else:\n+        # Will only work if `kernel` is a shared variable.\n+        kernel_shape = kernel.eval().shape\n+\n+    if padding == 'same' and kernel_shape[0] % 2 == 0:\n+        raise ValueError('In `Conv3DTranspose`, with padding mode `same`, '\n+                         'even kernel sizes are only supported with Tensorflow. '\n+                         'With Theano, set `kernel_size` to an odd number.')\n+\n+    kernel_shape = _preprocess_conv3d_filter_shape(kernel_shape, data_format)\n+\n+    x = _preprocess_conv3d_input(x, data_format)\n+    kernel = _preprocess_conv3d_kernel(kernel, data_format)\n+\n+    th_padding = _preprocess_padding(padding)\n+    op = T.nnet.abstract_conv.AbstractConv3d_gradInputs(imshp=None,\n+                                                        kshp=kernel_shape,\n+                                                        subsample=strides,\n+                                                        border_mode=th_padding,\n+                                                        filter_flip=not flip_filters)\n+    conv_out = op(kernel, x, output_shape[2:])\n+    conv_out = _postprocess_conv3d_output(conv_out, x, padding,\n+                                          kernel_shape, strides, data_format)\n+    return conv_out\n+\n+\n def pool2d(x, pool_size, strides=(1, 1), padding='valid',\n            data_format=None, pool_mode='max'):\n     if data_format is None:\n\n@@ -806,6 +806,237 @@ class Conv2DTranspose(Conv2D):\n         return config\n \n \n+class Conv3DTranspose(Conv3D):\n+    \"\"\"Transposed convolution layer (sometimes called Deconvolution).\n+\n+    The need for transposed convolutions generally arises\n+    from the desire to use a transformation going in the opposite direction\n+    of a normal convolution, i.e., from something that has the shape of the\n+    output of some convolution to something that has the shape of its input\n+    while maintaining a connectivity pattern that is compatible with\n+    said convolution.\n+\n+    When using this layer as the first layer in a model,\n+    provide the keyword argument `input_shape`\n+    (tuple of integers, does not include the sample axis),\n+    e.g. `input_shape=(128, 128, 128, 3)` for a 128x128x128 volume with 3 channels\n+    if `data_format=\"channels_last\"`.\n+\n+    # Arguments\n+        filters: Integer, the dimensionality of the output space\n+            (i.e. the number of output filters in the convolution).\n+        kernel_size: An integer or tuple/list of 3 integers, specifying the\n+            width and height of the 3D convolution window.\n+            Can be a single integer to specify the same value for\n+            all spatial dimensions.\n+        strides: An integer or tuple/list of 3 integers,\n+            specifying the strides of the convolution along the width and height.\n+            Can be a single integer to specify the same value for\n+            all spatial dimensions.\n+            Specifying any stride value != 1 is incompatible with specifying\n+            any `dilation_rate` value != 1.\n+        padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n+        data_format: A string,\n+            one of `channels_last` (default) or `channels_first`.\n+            The ordering of the dimensions in the inputs.\n+            `channels_last` corresponds to inputs with shape\n+            `(batch, depth, height, width, channels)` while `channels_first`\n+            corresponds to inputs with shape\n+            `(batch, channels, depth, height, width)`.\n+            It defaults to the `image_data_format` value found in your\n+            Keras config file at `~/.keras/keras.json`.\n+            If you never set it, then it will be \"channels_last\".\n+        dilation_rate: an integer or tuple/list of 3 integers, specifying\n+            the dilation rate to use for dilated convolution.\n+            Can be a single integer to specify the same value for\n+            all spatial dimensions.\n+            Currently, specifying any `dilation_rate` value != 1 is\n+            incompatible with specifying any stride value != 1.\n+        activation: Activation function to use\n+            (see [activations](../activations.md)).\n+            If you don't specify anything, no activation is applied\n+            (ie. \"linear\" activation: `a(x) = x`).\n+        use_bias: Boolean, whether the layer uses a bias vector.\n+        kernel_initializer: Initializer for the `kernel` weights matrix\n+            (see [initializers](../initializers.md)).\n+        bias_initializer: Initializer for the bias vector\n+            (see [initializers](../initializers.md)).\n+        kernel_regularizer: Regularizer function applied to\n+            the `kernel` weights matrix\n+            (see [regularizer](../regularizers.md)).\n+        bias_regularizer: Regularizer function applied to the bias vector\n+            (see [regularizer](../regularizers.md)).\n+        activity_regularizer: Regularizer function applied to\n+            the output of the layer (its \"activation\").\n+            (see [regularizer](../regularizers.md)).\n+        kernel_constraint: Constraint function applied to the kernel matrix\n+            (see [constraints](../constraints.md)).\n+        bias_constraint: Constraint function applied to the bias vector\n+            (see [constraints](../constraints.md)).\n+\n+    # Input shape\n+        5D tensor with shape:\n+        `(batch, channels, depth, rows, cols)` if data_format='channels_first'\n+        or 5D tensor with shape:\n+        `(batch, depth, rows, cols, channels)` if data_format='channels_last'.\n+\n+    # Output shape\n+        5D tensor with shape:\n+        `(batch, filters, new_depth, new_rows, new_cols)` if data_format='channels_first'\n+        or 5D tensor with shape:\n+        `(batch, new_depth, new_rows, new_cols, filters)` if data_format='channels_last'.\n+        `depth` and `rows` and `cols` values might have changed due to padding.\n+\n+    # References\n+        - [A guide to convolution arithmetic for deep learning](https://arxiv.org/abs/1603.07285v1)\n+        - [Deconvolutional Networks](http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf)\n+    \"\"\"\n+\n+    def __init__(self, filters,\n+                 kernel_size,\n+                 strides=(1, 1, 1),\n+                 padding='valid',\n+                 data_format=None,\n+                 activation=None,\n+                 use_bias=True,\n+                 kernel_initializer='glorot_uniform',\n+                 bias_initializer='zeros',\n+                 kernel_regularizer=None,\n+                 bias_regularizer=None,\n+                 activity_regularizer=None,\n+                 kernel_constraint=None,\n+                 bias_constraint=None,\n+                 **kwargs):\n+        super(Conv3DTranspose, self).__init__(\n+            filters,\n+            kernel_size,\n+            strides=strides,\n+            padding=padding,\n+            data_format=data_format,\n+            activation=activation,\n+            use_bias=use_bias,\n+            kernel_initializer=kernel_initializer,\n+            bias_initializer=bias_initializer,\n+            kernel_regularizer=kernel_regularizer,\n+            bias_regularizer=bias_regularizer,\n+            activity_regularizer=activity_regularizer,\n+            kernel_constraint=kernel_constraint,\n+            bias_constraint=bias_constraint,\n+            **kwargs)\n+        self.input_spec = InputSpec(ndim=5)\n+\n+    def build(self, input_shape):\n+        if len(input_shape) != 5:\n+            raise ValueError('Inputs should have rank ' +\n+                             str(5) +\n+                             '; Received input shape:', str(input_shape))\n+        if self.data_format == 'channels_first':\n+            channel_axis = 1\n+        else:\n+            channel_axis = -1\n+        if input_shape[channel_axis] is None:\n+            raise ValueError('The channel dimension of the inputs '\n+                             'should be defined. Found `None`.')\n+        input_dim = input_shape[channel_axis]\n+        kernel_shape = self.kernel_size + (self.filters, input_dim)\n+\n+        self.kernel = self.add_weight(shape=kernel_shape,\n+                                      initializer=self.kernel_initializer,\n+                                      name='kernel',\n+                                      regularizer=self.kernel_regularizer,\n+                                      constraint=self.kernel_constraint)\n+        if self.use_bias:\n+            self.bias = self.add_weight(shape=(self.filters,),\n+                                        initializer=self.bias_initializer,\n+                                        name='bias',\n+                                        regularizer=self.bias_regularizer,\n+                                        constraint=self.bias_constraint)\n+        else:\n+            self.bias = None\n+        # Set input spec.\n+        self.input_spec = InputSpec(ndim=5, axes={channel_axis: input_dim})\n+        self.built = True\n+\n+    def call(self, inputs):\n+        input_shape = K.shape(inputs)\n+        batch_size = input_shape[0]\n+        if self.data_format == 'channels_first':\n+            d_axis, h_axis, w_axis = 2, 3, 4\n+        else:\n+            d_axis, h_axis, w_axis = 1, 2, 3\n+\n+        depth = input_shape[d_axis]\n+        height = input_shape[h_axis]\n+        width = input_shape[w_axis]\n+\n+        kernel_d, kernel_h, kernel_w = self.kernel_size\n+        stride_d, stride_h, stride_w = self.strides\n+\n+        # Infer the dynamic output shape:\n+        out_depth = conv_utils.deconv_length(depth,\n+                                             stride_d, kernel_d,\n+                                             self.padding)\n+        out_height = conv_utils.deconv_length(height,\n+                                              stride_h, kernel_h,\n+                                              self.padding)\n+        out_width = conv_utils.deconv_length(width,\n+                                             stride_w, kernel_w,\n+                                             self.padding)\n+\n+        if self.data_format == 'channels_first':\n+            output_shape = (batch_size, self.filters, out_depth, out_height, out_width)\n+        else:\n+            output_shape = (batch_size, out_depth, out_height, out_width, self.filters)\n+\n+        outputs = K.conv3d_transpose(inputs,\n+                                     self.kernel,\n+                                     output_shape,\n+                                     self.strides,\n+                                     padding=self.padding,\n+                                     data_format=self.data_format)\n+\n+        if self.bias:\n+            outputs = K.bias_add(\n+                outputs,\n+                self.bias,\n+                data_format=self.data_format)\n+\n+        if self.activation is not None:\n+            return self.activation(outputs)\n+        return outputs\n+\n+    def compute_output_shape(self, input_shape):\n+        output_shape = list(input_shape)\n+        if self.data_format == 'channels_first':\n+            c_axis, d_axis, h_axis, w_axis = 1, 2, 3, 4\n+        else:\n+            c_axis, d_axis, h_axis, w_axis = 4, 1, 2, 3\n+\n+        kernel_d, kernel_h, kernel_w = self.kernel_size\n+        stride_d, stride_h, stride_w = self.strides\n+\n+        output_shape[c_axis] = self.filters\n+        output_shape[d_axis] = conv_utils.deconv_length(output_shape[d_axis],\n+                                                        stride_d,\n+                                                        kernel_d,\n+                                                        self.padding)\n+        output_shape[h_axis] = conv_utils.deconv_length(output_shape[h_axis],\n+                                                        stride_h,\n+                                                        kernel_h,\n+                                                        self.padding)\n+        output_shape[w_axis] = conv_utils.deconv_length(output_shape[w_axis],\n+                                                        stride_w,\n+                                                        kernel_w,\n+                                                        self.padding)\n+\n+        return tuple(output_shape)\n+\n+    def get_config(self):\n+        config = super(Conv3DTranspose, self).get_config()\n+        config.pop('dilation_rate')\n+        return config\n+\n+\n class SeparableConv2D(Conv2D):\n     \"\"\"Depthwise separable 2D convolution.\n \n@@ -1891,6 +2122,7 @@ Convolution3D = Conv3D\n SeparableConvolution2D = SeparableConv2D\n Convolution2DTranspose = Conv2DTranspose\n Deconvolution2D = Deconv2D = Conv2DTranspose\n+Deconvolution3D = Deconv3D = Conv3DTranspose\n \n # Legacy aliases\n AtrousConv1D = AtrousConvolution1D\n\n@@ -388,6 +388,51 @@ def test_convolution_3d():\n                             stack_size))\n \n \n+@keras_test\n+def test_conv3d_transpose():\n+    filters = 2\n+    stack_size = 3\n+    num_depth = 7\n+    num_row = 5\n+    num_col = 6\n+\n+    for padding in _convolution_paddings:\n+        for strides in [(1, 1, 1), (2, 2, 2)]:\n+            for data_format in ['channels_first', 'channels_last']:\n+                if padding == 'same' and strides != (1, 1, 1):\n+                    continue\n+                layer_test(convolutional.Conv3DTranspose,\n+                           kwargs={'filters': filters,\n+                                   'kernel_size': 3,\n+                                   'padding': padding,\n+                                   'strides': strides,\n+                                   'data_format': data_format},\n+                           input_shape=(None, num_depth, num_row, num_col, stack_size),\n+                           fixed_batch_size=True)\n+\n+    layer_test(convolutional.Conv3DTranspose,\n+               kwargs={'filters': filters,\n+                       'kernel_size': 3,\n+                       'padding': padding,\n+                       'data_format': 'channels_first',\n+                       'activation': None,\n+                       'kernel_regularizer': 'l2',\n+                       'bias_regularizer': 'l2',\n+                       'activity_regularizer': 'l2',\n+                       'kernel_constraint': 'max_norm',\n+                       'bias_constraint': 'max_norm',\n+                       'strides': strides},\n+               input_shape=(None, stack_size, num_depth, num_row, num_col),\n+               fixed_batch_size=True)\n+\n+    # Test invalid use case\n+    with pytest.raises(ValueError):\n+        model = Sequential([convolutional.Conv3DTranspose(filters=filters,\n+                                                          kernel_size=3,\n+                                                          padding=padding,\n+                                                          batch_input_shape=(None, None, 5, None, None))])\n+\n+\n @keras_test\n def test_maxpooling_3d():\n     pool_size = (3, 3, 3)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
