{"custom_id": "keras#5aa3f044a2c530ddfaefb82e17fe5aa22a8570dd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 2798 | Contributors (this commit): 14 | Commits (past 90d): 13 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -2002,8 +2002,8 @@ def spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None):\n     return x\n \n \n-def one_hot(indices, nb_classes):\n-    return C.one_hot(indices, nb_classes)\n+def one_hot(indices, num_classes):\n+    return C.one_hot(indices, num_classes)\n \n \n def get_value(x):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1d8121f9ff8a8f32df99004c33674907c8919602", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 1657 | Contributors (this commit): 28 | Commits (past 90d): 17 | Contributors (cumulative): 32 | DMM Complexity: None\n\nDIFF:\n@@ -35,7 +35,7 @@ python neural_doodle.py --nlabels 4 --style-image Renoir/style.png \\\n - [Torch code for fast-neural-doodle](https://github.com/DmitryUlyanov/fast-neural-doodle)\r\n - [Torch code for online-neural-doodle](https://github.com/DmitryUlyanov/online-neural-doodle)\r\n - [Paper Texture Networks: Feed-forward Synthesis of Textures and Stylized Images](http://arxiv.org/abs/1603.03417)\r\n-- [Discussion on parameter tuning](https://github.com/fchollet/keras/issues/3705)\r\n+- [Discussion on parameter tuning](https://github.com/keras-team/keras/issues/3705)\r\n \r\n # Resources\r\n \r\n\n@@ -7,8 +7,8 @@ setup(name='Keras',\n       description='Deep Learning for Python',\n       author='Francois Chollet',\n       author_email='francois.chollet@gmail.com',\n-      url='https://github.com/fchollet/keras',\n-      download_url='https://github.com/fchollet/keras/tarball/2.1.2',\n+      url='https://github.com/keras-team/keras',\n+      download_url='https://github.com/keras-team/keras/tarball/2.1.2',\n       license='MIT',\n       install_requires=['numpy>=1.9.1',\n                         'scipy>=0.14',\n\n@@ -232,7 +232,7 @@ def test_trainability(layer_class):\n @keras_test\n def test_masking_layer():\n     ''' This test based on a previously failing issue here:\n-    https://github.com/fchollet/keras/issues/1567\n+    https://github.com/keras-team/keras/issues/1567\n     '''\n     inputs = np.random.random((6, 3, 4))\n     targets = np.abs(np.random.random((6, 3, 5)))\n@@ -355,7 +355,7 @@ def test_initial_states_as_other_inputs(layer_class):\n @rnn_test\n def test_specify_state_with_masking(layer_class):\n     ''' This test based on a previously failing issue here:\n-    https://github.com/fchollet/keras/issues/1567\n+    https://github.com/keras-team/keras/issues/1567\n     '''\n     num_states = 2 if layer_class is recurrent.LSTM else 1\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#04e0a10aea5024b6f135d7ee71c38ea5ddf548e0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 222 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -28,7 +28,7 @@ BASE_DIR = ''\n GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n TEXT_DATA_DIR = os.path.join(BASE_DIR, '20_newsgroup')\n MAX_SEQUENCE_LENGTH = 1000\n-MAX_NB_WORDS = 20000\n+MAX_NUM_WORDS = 20000\n EMBEDDING_DIM = 100\n VALIDATION_SPLIT = 0.2\n \n@@ -77,7 +77,7 @@ for name in sorted(os.listdir(TEXT_DATA_DIR)):\n print('Found %s texts.' % len(texts))\n \n # finally, vectorize the text samples into a 2D integer tensor\n-tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n+tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n tokenizer.fit_on_texts(texts)\n sequences = tokenizer.texts_to_sequences(texts)\n \n@@ -105,10 +105,10 @@ y_val = labels[-num_validation_samples:]\n print('Preparing embedding matrix.')\n \n # prepare embedding matrix\n-num_words = min(MAX_NB_WORDS, len(word_index))\n+num_words = min(MAX_NUM_WORDS, len(word_index))\n embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n for word, i in word_index.items():\n-    if i >= MAX_NB_WORDS:\n+    if i >= MAX_NUM_WORDS:\n         continue\n     embedding_vector = embeddings_index.get(word)\n     if embedding_vector is not None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0611d80cd05c1c199f4c41aae523bf8b988adf24", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 82 | Lines Deleted: 8 | Files Changed: 3 | Hunks: 13 | Methods Changed: 7 | Complexity Δ (Sum/Max): 7/4 | Churn Δ: 90 | Churn Cumulative: 18851 | Contributors (this commit): 121 | Commits (past 90d): 27 | Contributors (cumulative): 147 | DMM Complexity: 0.6190476190476191\n\nDIFF:\n@@ -2597,7 +2597,7 @@ class Container(Layer):\n         f.flush()\n         f.close()\n \n-    def load_weights(self, filepath, by_name=False):\n+    def load_weights(self, filepath, by_name=False, skip_mismatch=False):\n         \"\"\"Loads all layer weights from a HDF5 save file.\n \n         If `by_name` is False (default) weights are loaded\n@@ -2616,6 +2616,11 @@ class Container(Layer):\n             filepath: String, path to the weights file to load.\n             by_name: Boolean, whether to load weights by name\n                 or by topological order.\n+            skip_mismatch: Boolean, whether to skip loading of layers\n+                where there is a mismatch in the number of weights,\n+                or a mismatch in the shape of the weight\n+                (only valid when `by_name`=True).\n+\n \n         # Raises\n             ImportError: If h5py is not available.\n@@ -2626,7 +2631,8 @@ class Container(Layer):\n         if 'layer_names' not in f.attrs and 'model_weights' in f:\n             f = f['model_weights']\n         if by_name:\n-            load_weights_from_hdf5_group_by_name(f, self.layers)\n+            load_weights_from_hdf5_group_by_name(\n+                f, self.layers, skip_mismatch=skip_mismatch)\n         else:\n             load_weights_from_hdf5_group(f, self.layers)\n \n@@ -3152,7 +3158,7 @@ def load_weights_from_hdf5_group(f, layers):\n     K.batch_set_value(weight_value_tuples)\n \n \n-def load_weights_from_hdf5_group_by_name(f, layers):\n+def load_weights_from_hdf5_group_by_name(f, layers, skip_mismatch=False):\n     \"\"\"Implements name-based weight loading.\n \n     (instead of topological weight loading).\n@@ -3161,11 +3167,14 @@ def load_weights_from_hdf5_group_by_name(f, layers):\n \n     # Arguments\n         f: A pointer to a HDF5 group.\n-        layers: a list of target layers.\n+        layers: A list of target layers.\n+        skip_mismatch: Boolean, whether to skip loading of layers\n+            where there is a mismatch in the number of weights,\n+            or a mismatch in the shape of the weights.\n \n     # Raises\n         ValueError: in case of mismatch between provided layers\n-            and weights file.\n+            and weights file and skip_mismatch=False.\n     \"\"\"\n     if 'keras_version' in f.attrs:\n         original_keras_version = f.attrs['keras_version'].decode('utf8')\n@@ -3201,6 +3210,12 @@ def load_weights_from_hdf5_group_by_name(f, layers):\n                 original_keras_version,\n                 original_backend)\n             if len(weight_values) != len(symbolic_weights):\n+                if skip_mismatch:\n+                    warnings.warn('Skipping loading of weights for layer {}'.format(layer.name) +\n+                                  ' due to mismatch in number of weights' +\n+                                  ' ({} vs {}).'.format(len(symbolic_weights), len(weight_values)))\n+                    continue\n+                else:\n                     raise ValueError('Layer #' + str(k) +\n                                      ' (named \"' + layer.name +\n                                      '\") expects ' +\n@@ -3210,6 +3225,16 @@ def load_weights_from_hdf5_group_by_name(f, layers):\n                                      ' element(s).')\n             # Set values.\n             for i in range(len(weight_values)):\n+                if skip_mismatch:\n+                    if K.int_shape(symbolic_weights[i]) != weight_values[i].shape:\n+                        warnings.warn('Skipping loading of weights for layer {}'.format(layer.name) +\n+                                      ' due to mismatch in shape' +\n+                                      ' ({} vs {}).'.format(\n+                                          symbolic_weights[i].shape,\n+                                          weight_values[i].shape))\n+                        continue\n+\n                 weight_value_tuples.append((symbolic_weights[i],\n                                             weight_values[i]))\n+\n     K.batch_set_value(weight_value_tuples)\n\n@@ -727,7 +727,7 @@ class Sequential(Model):\n             self.build()\n         self.model.set_weights(weights)\n \n-    def load_weights(self, filepath, by_name=False):\n+    def load_weights(self, filepath, by_name=False, skip_mismatch=False):\n         if h5py is None:\n             raise ImportError('`load_weights` requires h5py.')\n         f = h5py.File(filepath, mode='r')\n@@ -740,7 +740,8 @@ class Sequential(Model):\n         else:\n             layers = self.layers\n         if by_name:\n-            topology.load_weights_from_hdf5_group_by_name(f, layers)\n+            topology.load_weights_from_hdf5_group_by_name(f, layers,\n+                                                          skip_mismatch=skip_mismatch)\n         else:\n             topology.load_weights_from_hdf5_group(f, layers)\n         if hasattr(f, 'close'):\n\n@@ -2,7 +2,7 @@ import pytest\n import os\n import tempfile\n import numpy as np\n-from numpy.testing import assert_allclose\n+from numpy.testing import assert_allclose, assert_raises\n \n from keras import backend as K\n from keras.models import Model, Sequential\n@@ -277,6 +277,54 @@ def test_loading_weights_by_name_2():\n     assert_allclose(np.zeros_like(jessica[1]), jessica[1])  # biases init to 0\n \n \n+@keras_test\n+def test_loading_weights_by_name_skip_mismatch():\n+    \"\"\"\n+    test skipping layers while loading model weights by name on:\n+        - sequential model\n+    \"\"\"\n+\n+    # test with custom optimizer, loss\n+    custom_opt = optimizers.rmsprop\n+    custom_loss = losses.mse\n+\n+    # sequential model\n+    model = Sequential()\n+    model.add(Dense(2, input_shape=(3,), name='rick'))\n+    model.add(Dense(3, name='morty'))\n+    model.compile(loss=custom_loss, optimizer=custom_opt(), metrics=['acc'])\n+\n+    x = np.random.random((1, 3))\n+    y = np.random.random((1, 3))\n+    model.train_on_batch(x, y)\n+\n+    out = model.predict(x)\n+    old_weights = [layer.get_weights() for layer in model.layers]\n+    _, fname = tempfile.mkstemp('.h5')\n+\n+    model.save_weights(fname)\n+\n+    # delete and recreate model\n+    del(model)\n+    model = Sequential()\n+    model.add(Dense(2, input_shape=(3,), name='rick'))\n+    model.add(Dense(4, name='morty'))  # different shape w.r.t. previous model\n+    model.compile(loss=custom_loss, optimizer=custom_opt(), metrics=['acc'])\n+\n+    # load weights from first model\n+    with pytest.warns(UserWarning):  # expect UserWarning for skipping weights\n+        model.load_weights(fname, by_name=True, skip_mismatch=True)\n+    os.remove(fname)\n+\n+    # assert layers 'rick' are equal\n+    for old, new in zip(old_weights[0], model.layers[0].get_weights()):\n+        assert_allclose(old, new, atol=1e-05)\n+\n+    # assert layers 'morty' are not equal, since we skipped loading this layer\n+    for old, new in zip(old_weights[1], model.layers[1].get_weights()):\n+        assert_raises(AssertionError, assert_allclose, old, new, atol=1e-05)\n+\n+\n # a function to be called from the Lambda layer\n def square_fn(x):\n     return x * x\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c45a2c7cba99dadd8727e83cf7070a89deb4b79d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 5 | Churn Cumulative: 2802 | Contributors (this commit): 63 | Commits (past 90d): 5 | Contributors (cumulative): 63 | DMM Complexity: 0.0\n\nDIFF:\n@@ -965,13 +965,14 @@ class CSVLogger(Callback):\n             else:\n                 return k\n \n+        if self.keys is None:\n+            self.keys = sorted(logs.keys())\n+\n         if self.model.stop_training:\n             # We set NA so that csv parsers do not fail for this last epoch.\n             logs = dict([(k, logs[k]) if k in logs else (k, 'NA') for k in self.keys])\n \n         if not self.writer:\n-            self.keys = sorted(logs.keys())\n-\n             class CustomDialect(csv.excel):\n                 delimiter = self.sep\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e39f6ed3ebf4fc5495ddcab4832d9ede6067aba2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 2817 | Contributors (this commit): 35 | Commits (past 90d): 7 | Contributors (cumulative): 37 | DMM Complexity: None\n\nDIFF:\n@@ -443,7 +443,7 @@ class Adam(Optimizer):\n         if self.amsgrad:\n             vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n         else:\n-            vhats = [None for p in params]\n+            vhats = [K.zeros(1) for _ in params]\n         self.weights = [self.iterations] + ms + vs + vhats\n \n         for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n\n@@ -84,7 +84,7 @@ def test_functional_model_saving():\n \n     model = Model(inputs, outputs)\n     model.compile(loss=losses.MSE,\n-                  optimizer=optimizers.RMSprop(lr=0.0001),\n+                  optimizer=optimizers.Adam(),\n                   metrics=[metrics.categorical_accuracy])\n     x = np.random.random((1, 3))\n     y = np.random.random((1, 3))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0d66dc4252f56bec1214874dbf766bdbb2c9ac10", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 5 | Methods Changed: 6 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 47 | Churn Cumulative: 7286 | Contributors (this commit): 79 | Commits (past 90d): 36 | Contributors (cumulative): 95 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2445,6 +2445,6 @@ class Model(Container):\n             else:\n                 return np.concatenate(all_outs[0])\n         if steps_done == 1:\n-            return [out for out in all_outs]\n+            return [out[0] for out in all_outs]\n         else:\n             return [np.concatenate(out) for out in all_outs]\n\n@@ -22,11 +22,12 @@ from keras.callbacks import LambdaCallback\n \n \n class RandomSequence(Sequence):\n-    def __init__(self, batch_size):\n+    def __init__(self, batch_size, sequence_length=12):\n         self.batch_size = batch_size\n+        self.sequence_length = sequence_length\n \n     def __len__(self):\n-        return 12\n+        return self.sequence_length\n \n     def __getitem__(self, idx):\n         return [np.random.random((self.batch_size, 3)), np.random.random((self.batch_size, 3))], [\n@@ -410,6 +411,46 @@ def test_model_methods():\n                                   initial_epoch=0, validation_data=gen_data(),\n                                   callbacks=[tracker_cb])\n \n+    # predict_generator output shape behavior should be consistent\n+    def expected_shape(batch_size, n_batches):\n+        return (batch_size * n_batches, 4), (batch_size * n_batches, 3)\n+\n+    # Multiple outputs and one step.\n+    batch_size = 5\n+    sequence_length = 1\n+    shape_0, shape_1 = expected_shape(batch_size, sequence_length)\n+    out = model.predict_generator(RandomSequence(batch_size,\n+                                                 sequence_length=sequence_length))\n+    assert np.shape(out[0]) == shape_0 and np.shape(out[1]) == shape_1\n+\n+    # Multiple outputs and multiple steps.\n+    batch_size = 5\n+    sequence_length = 2\n+    shape_0, shape_1 = expected_shape(batch_size, sequence_length)\n+    out = model.predict_generator(RandomSequence(batch_size,\n+                                                 sequence_length=sequence_length))\n+    assert np.shape(out[0]) == shape_0 and np.shape(out[1]) == shape_1\n+\n+    # Create a model with a single output.\n+    single_output_model = Model([a, b], a_2)\n+    single_output_model.compile(optimizer, loss, metrics=[], sample_weight_mode=None)\n+\n+    # Single output and one step.\n+    batch_size = 5\n+    sequence_length = 1\n+    shape_0, _ = expected_shape(batch_size, sequence_length)\n+    out = single_output_model.predict_generator(RandomSequence(batch_size,\n+                                                sequence_length=sequence_length))\n+    assert np.shape(out) == shape_0\n+\n+    # Single output and multiple steps.\n+    batch_size = 5\n+    sequence_length = 2\n+    shape_0, _ = expected_shape(batch_size, sequence_length)\n+    out = single_output_model.predict_generator(RandomSequence(batch_size,\n+                                                sequence_length=sequence_length))\n+    assert np.shape(out) == shape_0\n+\n \n @pytest.mark.skipif(sys.version_info < (3,), reason='Cannot catch warnings in python 2')\n @keras_test\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#adbbe01b5deed1da64c37e1767e85e602606e5b8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 208 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -76,8 +76,8 @@ def test_batchnorm_correctness_2d():\n     out -= np.reshape(K.eval(norm.beta), (1, 10, 1))\n     out /= np.reshape(K.eval(norm.gamma), (1, 10, 1))\n \n-    assert_allclose(out.mean(axis=(0, 2)), 0.0, atol=1e-1)\n-    assert_allclose(out.std(axis=(0, 2)), 1.0, atol=1e-1)\n+    assert_allclose(out.mean(axis=(0, 2)), 0.0, atol=1.1e-1)\n+    assert_allclose(out.std(axis=(0, 2)), 1.0, atol=1.1e-1)\n \n \n @keras_test\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#871007dbb0e6211459b9d16244cc3c9683459df7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 5860 | Contributors (this commit): 77 | Commits (past 90d): 29 | Contributors (cumulative): 77 | DMM Complexity: None\n\nDIFF:\n@@ -2106,6 +2106,8 @@ class Model(Container):\n                 output_generator = generator\n \n             callback_model.stop_training = False\n+            # Construct epoch logs.\n+            epoch_logs = {}\n             while epoch < epochs:\n                 callbacks.on_epoch_begin(epoch)\n                 steps_done = 0\n@@ -2152,8 +2154,6 @@ class Model(Container):\n \n                     callbacks.on_batch_end(batch_index, batch_logs)\n \n-                    # Construct epoch logs.\n-                    epoch_logs = {}\n                     batch_index += 1\n                     steps_done += 1\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4cad455ef4da600c96ddc69800bab39d0e52b677", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 21 | Churn Cumulative: 9970 | Contributors (this commit): 50 | Commits (past 90d): 21 | Contributors (cumulative): 59 | DMM Complexity: 0.9166666666666666\n\nDIFF:\n@@ -395,9 +395,10 @@ class RNN(Layer):\n             input_shape = input_shape[0]\n \n         if hasattr(self.cell.state_size, '__len__'):\n-            output_dim = self.cell.state_size[0]\n+            state_size = self.cell.state_size\n         else:\n-            output_dim = self.cell.state_size\n+            state_size = [self.cell.state_size]\n+        output_dim = state_size[0]\n \n         if self.return_sequences:\n             output_shape = (input_shape[0], input_shape[1], output_dim)\n@@ -405,7 +406,7 @@ class RNN(Layer):\n             output_shape = (input_shape[0], output_dim)\n \n         if self.return_state:\n-            state_shape = [(input_shape[0], output_dim) for _ in self.states]\n+            state_shape = [(input_shape[0], dim) for dim in state_size]\n             return [output_shape] + state_shape\n         else:\n             return output_shape\n\n@@ -596,6 +596,20 @@ def test_stacked_rnn_attributes():\n     assert layer.get_losses_for(x) == [y]\n \n \n+@keras_test\n+def test_stacked_rnn_compute_output_shape():\n+    cells = [recurrent.LSTMCell(3),\n+             recurrent.LSTMCell(6)]\n+    layer = recurrent.RNN(cells, return_state=True, return_sequences=True)\n+    output_shape = layer.compute_output_shape((None, timesteps, embedding_dim))\n+    expected_output_shape = [(None, timesteps, 6),\n+                             (None, 6),\n+                             (None, 6),\n+                             (None, 3),\n+                             (None, 3)]\n+    assert output_shape == expected_output_shape\n+\n+\n @rnn_test\n def test_batch_size_equal_one(layer_class):\n     inputs = Input(batch_shape=(1, timesteps, embedding_dim))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#beda1bc5132e6fef10a6853b34f48fefc6dc67c3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 12 | Churn Cumulative: 2704 | Contributors (this commit): 45 | Commits (past 90d): 11 | Contributors (cumulative): 45 | DMM Complexity: 0.0\n\nDIFF:\n@@ -382,8 +382,8 @@ class ImageDataGenerator(object):\n         zca_whitening: apply ZCA whitening.\n         zca_epsilon: epsilon for ZCA whitening. Default is 1e-6.\n         rotation_range: degrees (0 to 180).\n-        width_shift_range: fraction of total width.\n-        height_shift_range: fraction of total height.\n+        width_shift_range: fraction of total width, if < 1, or pixels if >= 1.\n+        height_shift_range: fraction of total height, if < 1, or pixels if >= 1.\n         shear_range: shear intensity (shear angle in radians).\n         zoom_range: amount of zoom. if scalar z, zoom will be randomly picked\n             in the range [1-z, 1+z]. A sequence of two can be passed instead\n@@ -611,12 +611,16 @@ class ImageDataGenerator(object):\n             theta = 0\n \n         if self.height_shift_range:\n-            tx = np.random.uniform(-self.height_shift_range, self.height_shift_range) * x.shape[img_row_axis]\n+            tx = np.random.uniform(-self.height_shift_range, self.height_shift_range)\n+            if self.height_shift_range < 1:\n+                tx *= x.shape[img_row_axis]\n         else:\n             tx = 0\n \n         if self.width_shift_range:\n-            ty = np.random.uniform(-self.width_shift_range, self.width_shift_range) * x.shape[img_col_axis]\n+            ty = np.random.uniform(-self.width_shift_range, self.width_shift_range)\n+            if self.width_shift_range < 1:\n+                ty *= x.shape[img_col_axis]\n         else:\n             ty = 0\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#93f0a1a316359bf8ed6b63b727029e03ddbc9730", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 37 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 37 | Churn Cumulative: 1394 | Contributors (this commit): 20 | Commits (past 90d): 2 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,7 @@\n # -*- coding: utf-8 -*-\n from __future__ import absolute_import\n \n+from .. import activations\n from .. import initializers\n from .. import regularizers\n from .. import constraints\n@@ -208,3 +209,32 @@ class ThresholdedReLU(Layer):\n         config = {'theta': float(self.theta)}\n         base_config = super(ThresholdedReLU, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+class Softmax(Layer):\n+    \"\"\"Softmax activation function.\n+\n+    # Input shape\n+        Arbitrary. Use the keyword argument `input_shape`\n+        (tuple of integers, does not include the samples axis)\n+        when using this layer as the first layer in a model.\n+\n+    # Output shape\n+        Same shape as the input.\n+\n+    # Arguments\n+        axis: Integer, axis along which the softmax normalization is applied.\n+    \"\"\"\n+\n+    def __init__(self, axis=-1, **kwargs):\n+        super(Softmax, self).__init__(**kwargs)\n+        self.supports_masking = True\n+        self.axis = axis\n+\n+    def call(self, inputs):\n+        return activations.softmax(inputs, axis=self.axis)\n+\n+    def get_config(self):\n+        config = {'axis': self.axis}\n+        base_config = super(Softmax, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n\n@@ -36,5 +36,12 @@ def test_thresholded_relu():\n                input_shape=(2, 3, 4))\n \n \n+@keras_test\n+def test_softmax():\n+    for axis in [1, -1]:\n+        layer_test(layers.Softmax, kwargs={'axis': axis},\n+                   input_shape=(2, 3, 4))\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#46182fb2b4a18aa0a5993981cbb3a53c37cf31ed", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 19 | Churn Cumulative: 12239 | Contributors (this commit): 101 | Commits (past 90d): 33 | Contributors (cumulative): 118 | DMM Complexity: 0.46153846153846156\n\nDIFF:\n@@ -2055,8 +2055,16 @@ def arange(start, stop=None, step=1, dtype='int32'):\n \n     \"\"\"\n     # Match the behavior of numpy and Theano by returning an empty seqence.\n-    if stop is None and start < 0:\n+    if stop is None:\n+        try:\n+            if start < 0:\n                 start = 0\n+        except TypeError:\n+            # Handle case where start is a tensor\n+            start = tf.cond(start < 0,\n+                            true_fn=lambda: tf.constant(0, dtype=start.dtype),\n+                            false_fn=lambda: start)\n+\n     result = tf.range(start, limit=stop, delta=step, name='arange')\n     if dtype != 'int32':\n         result = cast(result, dtype)\n\n@@ -1371,6 +1371,15 @@ class TestBackend(object):\n                 t = backend.arange(10, dtype=dtype)\n                 assert backend.dtype(t) == dtype\n \n+        for backend in [KTH, KTF]:\n+            start = backend.constant(1, dtype='int32')\n+            t = backend.arange(start)\n+            assert len(backend.eval(t)) == 1\n+\n+            start = backend.constant(-1, dtype='int32')\n+            t = backend.arange(start)\n+            assert len(backend.eval(t)) == 0\n+\n     def test_in_train_phase(self):\n         for training in [True, False]:\n             check_two_tensor_operation('in_train_phase', (3, 3), (2, 2), [KTH, KTF],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ab68d453c52c413708892cca0f6e59fa965124dd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 9 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 9911 | Contributors (this commit): 81 | Commits (past 90d): 17 | Contributors (cumulative): 81 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1013,7 +1013,7 @@ class Sequential(Model):\n                                    sample_weight=sample_weight,\n                                    steps=steps)\n \n-    def predict(self, x, batch_size=None, verbose=0):\n+    def predict(self, x, batch_size=None, verbose=0, steps=None):\n         \"\"\"Generates output predictions for the input samples.\n \n         The input samples are processed batch by batch.\n@@ -1022,13 +1022,17 @@ class Sequential(Model):\n             x: the input data, as a Numpy array.\n             batch_size: Integer. If unspecified, it will default to 32.\n             verbose: verbosity mode, 0 or 1.\n+            steps: Total number of steps (batches of samples)\n+                before declaring the prediction round finished.\n+                Ignored with the default value of `None`.\n \n         # Returns\n             A Numpy array of predictions.\n         \"\"\"\n         if not self.built:\n             self.build()\n-        return self.model.predict(x, batch_size=batch_size, verbose=verbose)\n+        return self.model.predict(x, batch_size=batch_size, verbose=verbose,\n+                                  steps=steps)\n \n     def predict_on_batch(self, x):\n         \"\"\"Returns predictions for a single batch of samples.\n@@ -1097,7 +1101,7 @@ class Sequential(Model):\n         return self.model.test_on_batch(x, y,\n                                         sample_weight=sample_weight)\n \n-    def predict_proba(self, x, batch_size=None, verbose=0):\n+    def predict_proba(self, x, batch_size=None, verbose=0, steps=None):\n         \"\"\"Generates class probability predictions for the input samples.\n \n         The input samples are processed batch by batch.\n@@ -1107,11 +1111,15 @@ class Sequential(Model):\n                 (if the model has multiple inputs).\n             batch_size: Integer. If unspecified, it will default to 32.\n             verbose: verbosity mode, 0 or 1.\n+            steps: Total number of steps (batches of samples)\n+                before declaring the prediction round finished.\n+                Ignored with the default value of `None`.\n+\n \n         # Returns\n             A Numpy array of probability predictions.\n         \"\"\"\n-        preds = self.predict(x, batch_size, verbose)\n+        preds = self.predict(x, batch_size, verbose, steps=steps)\n         if preds.min() < 0. or preds.max() > 1.:\n             warnings.warn('Network returning invalid probability values. '\n                           'The last layer might not normalize predictions '\n@@ -1119,7 +1127,7 @@ class Sequential(Model):\n                           '(like softmax or sigmoid would).')\n         return preds\n \n-    def predict_classes(self, x, batch_size=None, verbose=0):\n+    def predict_classes(self, x, batch_size=None, verbose=0, steps=None):\n         \"\"\"Generate class predictions for the input samples.\n \n         The input samples are processed batch by batch.\n@@ -1129,11 +1137,15 @@ class Sequential(Model):\n                 (if the model has multiple inputs).\n             batch_size: Integer. If unspecified, it will default to 32.\n             verbose: verbosity mode, 0 or 1.\n+            steps: Total number of steps (batches of samples)\n+                before declaring the prediction round finished.\n+                Ignored with the default value of `None`.\n \n         # Returns\n             A numpy array of class predictions.\n         \"\"\"\n-        proba = self.predict(x, batch_size=batch_size, verbose=verbose)\n+        proba = self.predict(x, batch_size=batch_size, verbose=verbose,\n+                             steps=steps)\n         if proba.shape[-1] > 1:\n             return proba.argmax(axis=-1)\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#816b9ee8317c7c9aa8e77693847a2ec37dde0340", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 115 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 115 | Churn Cumulative: 10085 | Contributors (this commit): 50 | Commits (past 90d): 22 | Contributors (cumulative): 59 | DMM Complexity: 1.0\n\nDIFF:\n@@ -898,6 +898,24 @@ class SimpleRNNCell(Layer):\n                 output._uses_learning_phase = True\n         return output, [output]\n \n+    def get_config(self):\n+        config = {'units': self.units,\n+                  'activation': activations.serialize(self.activation),\n+                  'use_bias': self.use_bias,\n+                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n+                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n+                  'bias_initializer': initializers.serialize(self.bias_initializer),\n+                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n+                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n+                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n+                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n+                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n+                  'bias_constraint': constraints.serialize(self.bias_constraint),\n+                  'dropout': self.dropout,\n+                  'recurrent_dropout': self.recurrent_dropout}\n+        base_config = super(SimpleRNNCell, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n \n class SimpleRNN(RNN):\n     \"\"\"Fully-connected RNN where the output is to be fed back to input.\n@@ -1328,6 +1346,26 @@ class GRUCell(Layer):\n                 h._uses_learning_phase = True\n         return h, [h]\n \n+    def get_config(self):\n+        config = {'units': self.units,\n+                  'activation': activations.serialize(self.activation),\n+                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n+                  'use_bias': self.use_bias,\n+                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n+                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n+                  'bias_initializer': initializers.serialize(self.bias_initializer),\n+                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n+                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n+                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n+                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n+                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n+                  'bias_constraint': constraints.serialize(self.bias_constraint),\n+                  'dropout': self.dropout,\n+                  'recurrent_dropout': self.recurrent_dropout,\n+                  'implementation': self.implementation}\n+        base_config = super(GRUCell, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n \n class GRU(RNN):\n     \"\"\"Gated Recurrent Unit - Cho et al. 2014.\n@@ -1808,6 +1846,27 @@ class LSTMCell(Layer):\n                 h._uses_learning_phase = True\n         return h, [h, c]\n \n+    def get_config(self):\n+        config = {'units': self.units,\n+                  'activation': activations.serialize(self.activation),\n+                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n+                  'use_bias': self.use_bias,\n+                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n+                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n+                  'bias_initializer': initializers.serialize(self.bias_initializer),\n+                  'unit_forget_bias': self.unit_forget_bias,\n+                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n+                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n+                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n+                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n+                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n+                  'bias_constraint': constraints.serialize(self.bias_constraint),\n+                  'dropout': self.dropout,\n+                  'recurrent_dropout': self.recurrent_dropout,\n+                  'implementation': self.implementation}\n+        base_config = super(LSTMCell, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n \n class LSTM(RNN):\n     \"\"\"Long-Short Term Memory layer - Hochreiter 1997.\n\n@@ -32,6 +32,16 @@ def rnn_test(f):\n     ])(f)\n \n \n+@keras_test\n+def rnn_cell_test(f):\n+    f = keras_test(f)\n+    return pytest.mark.parametrize('cell_class', [\n+        recurrent.SimpleRNNCell,\n+        recurrent.GRUCell,\n+        recurrent.LSTMCell\n+    ])(f)\n+\n+\n @rnn_test\n def test_return_sequences(layer_class):\n     layer_test(layer_class,\n@@ -556,6 +566,52 @@ def test_minimal_rnn_cell_layer():\n     assert_allclose(y_np, y_np_2, atol=1e-4)\n \n \n+@rnn_cell_test\n+def test_builtin_rnn_cell_layer(cell_class):\n+    # Test basic case.\n+    x = keras.Input((None, 5))\n+    cell = cell_class(32)\n+    layer = recurrent.RNN(cell)\n+    y = layer(x)\n+    model = keras.models.Model(x, y)\n+    model.compile(optimizer='rmsprop', loss='mse')\n+    model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 32)))\n+\n+    # Test basic case serialization.\n+    x_np = np.random.random((6, 5, 5))\n+    y_np = model.predict(x_np)\n+    weights = model.get_weights()\n+    config = layer.get_config()\n+    layer = recurrent.RNN.from_config(config)\n+    y = layer(x)\n+    model = keras.models.Model(x, y)\n+    model.set_weights(weights)\n+    y_np_2 = model.predict(x_np)\n+    assert_allclose(y_np, y_np_2, atol=1e-4)\n+\n+    # Test stacking.\n+    cells = [cell_class(8),\n+             cell_class(12),\n+             cell_class(32)]\n+    layer = recurrent.RNN(cells)\n+    y = layer(x)\n+    model = keras.models.Model(x, y)\n+    model.compile(optimizer='rmsprop', loss='mse')\n+    model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 32)))\n+\n+    # Test stacked RNN serialization.\n+    x_np = np.random.random((6, 5, 5))\n+    y_np = model.predict(x_np)\n+    weights = model.get_weights()\n+    config = layer.get_config()\n+    layer = recurrent.RNN.from_config(config)\n+    y = layer(x)\n+    model = keras.models.Model(x, y)\n+    model.set_weights(weights)\n+    y_np_2 = model.predict(x_np)\n+    assert_allclose(y_np, y_np_2, atol=1e-4)\n+\n+\n @keras_test\n @pytest.mark.skipif((K.backend() in ['cntk', 'theano']),\n                     reason='Not supported.')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fa1f95f4b70dfee0bfb7d7118ac6d9045183e49a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 2808 | Contributors (this commit): 64 | Commits (past 90d): 6 | Contributors (cumulative): 64 | DMM Complexity: 0.0\n\nDIFF:\n@@ -874,7 +874,6 @@ class ReduceLROnPlateau(Callback):\n             self.best = -np.Inf\n         self.cooldown_counter = 0\n         self.wait = 0\n-        self.lr_epsilon = self.min_lr * 1e-4\n \n     def on_train_begin(self, logs=None):\n         self._reset()\n@@ -901,12 +900,13 @@ class ReduceLROnPlateau(Callback):\n             elif not self.in_cooldown():\n                 if self.wait >= self.patience:\n                     old_lr = float(K.get_value(self.model.optimizer.lr))\n-                    if old_lr > self.min_lr + self.lr_epsilon:\n+                    if old_lr > self.min_lr:\n                         new_lr = old_lr * self.factor\n                         new_lr = max(new_lr, self.min_lr)\n                         K.set_value(self.model.optimizer.lr, new_lr)\n                         if self.verbose > 0:\n-                            print('\\nEpoch %05d: reducing learning rate to %s.' % (epoch + 1, new_lr))\n+                            print('\\nEpoch %05d: ReduceLROnPlateau reducing learning '\n+                                  'rate to %s.' % (epoch + 1, new_lr))\n                         self.cooldown_counter = self.cooldown\n                         self.wait = 0\n                 self.wait += 1\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0249c26bc00cc985f1d13b076d6bf3f9bac8a8ee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 2816 | Contributors (this commit): 64 | Commits (past 90d): 7 | Contributors (cumulative): 64 | DMM Complexity: 1.0\n\nDIFF:\n@@ -560,11 +560,13 @@ class LearningRateScheduler(Callback):\n         schedule: a function that takes an epoch index as input\n             (integer, indexed from 0) and returns a new\n             learning rate as output (float).\n+        verbose: int. 0: quiet, 1: update messages.\n     \"\"\"\n \n-    def __init__(self, schedule):\n+    def __init__(self, schedule, verbose=0):\n         super(LearningRateScheduler, self).__init__()\n         self.schedule = schedule\n+        self.verbose = verbose\n \n     def on_epoch_begin(self, epoch, logs=None):\n         if not hasattr(self.model.optimizer, 'lr'):\n@@ -573,7 +575,9 @@ class LearningRateScheduler(Callback):\n         if not isinstance(lr, (float, np.float32, np.float64)):\n             raise ValueError('The output of the \"schedule\" function '\n                              'should be float.')\n-        K.set_value(self.model.optimizer.lr, lr)\n+        if self.verbose > 0:\n+            print('\\nEpoch %05d: LearningRateScheduler reducing learning '\n+                  'rate to %s.' % (epoch + 1, lr))\n \n \n class TensorBoard(Callback):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#12a060f63462f2e5f838b70cadb2079b4302f449", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 10 | Churn Cumulative: 236 | Contributors (this commit): 7 | Commits (past 90d): 5 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -106,6 +106,16 @@ def test_func_dump_and_load(test_function_type):\n     assert deserialized.__closure__ == test_func.__closure__\n \n \n+def test_func_dump_and_load_closure():\n+    y = 0\n+    test_func = lambda x: x + y\n+    serialized, _, closure = func_dump(test_func)\n+    deserialized = func_load(serialized, closure=closure)\n+    assert deserialized.__code__ == test_func.__code__\n+    assert deserialized.__defaults__ == test_func.__defaults__\n+    assert deserialized.__closure__ == test_func.__closure__\n+\n+\n @pytest.mark.parametrize(\n     'test_func', [activations.softmax, np.argmax, lambda x: x**2])\n def test_func_dump_and_load_backwards_compat(test_func):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c57bba20b0eb7bc1e70af6c71cc23a45de104b02", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 482 | Lines Deleted: 88 | Files Changed: 5 | Hunks: 12 | Methods Changed: 10 | Complexity Δ (Sum/Max): 22/10 | Churn Δ: 570 | Churn Cumulative: 28961 | Contributors (this commit): 150 | Commits (past 90d): 45 | Contributors (cumulative): 252 | DMM Complexity: 0.5988023952095808\n\nDIFF:\n@@ -1502,6 +1502,11 @@ def conv2d(x, kernel, strides=(1, 1), padding='valid',\n     return _postprocess_conv2d_output(x, data_format)\n \n \n+def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,\n+                     padding='valid', data_format=None, dilation_rate=1):\n+    raise NotImplementedError\n+\n+\n def separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1),\n                      padding='valid', data_format=None, dilation_rate=(1, 1)):\n     raise NotImplementedError\n\n@@ -3162,6 +3162,27 @@ def in_top_k(predictions, targets, k):\n # CONVOLUTIONS\n \n \n+def _preprocess_conv1d_input(x, data_format):\n+    \"\"\"Transpose and cast the input before the conv1d.\n+\n+    # Arguments\n+        x: input tensor.\n+        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n+\n+    # Returns\n+        A tensor.\n+    \"\"\"\n+    if dtype(x) == 'float64':\n+        x = tf.cast(x, 'float32')\n+    tf_data_format = 'NHWC'  # to pass TF Conv2dNative operations\n+    if data_format == 'channels_first':\n+        if not _has_nchw_support():\n+            x = tf.transpose(x, (0, 2, 1))  # NCW -> NWC\n+        else:\n+            tf_data_format = 'NCHW'\n+    return x, tf_data_format\n+\n+\n def _preprocess_conv2d_input(x, data_format):\n     \"\"\"Transpose and cast the input before the conv2d.\n \n@@ -3362,6 +3383,57 @@ def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n     return x\n \n \n+def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,\n+                     padding='valid', data_format=None, dilation_rate=1):\n+    \"\"\"1D convolution with separable filters.\n+\n+    # Arguments\n+        x: input tensor\n+        depthwise_kernel: convolution kernel for the depthwise convolution.\n+        pointwise_kernel: kernel for the 1x1 convolution.\n+        strides: stride integer.\n+        padding: string, `\"same\"` or `\"valid\"`.\n+        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n+        dilation_rate: integer dilation rate.\n+\n+    # Returns\n+        Output tensor.\n+\n+    # Raises\n+        ValueError: if `data_format` is neither `channels_last` or `channels_first`.\n+    \"\"\"\n+    if data_format is None:\n+        data_format = image_data_format()\n+    if data_format not in {'channels_first', 'channels_last'}:\n+        raise ValueError('Unknown data_format ' + str(data_format))\n+\n+    x, tf_data_format = _preprocess_conv1d_input(x, data_format)\n+    padding = _preprocess_padding(padding)\n+    if tf_data_format == 'NHWC':\n+        spatial_start_dim = 1\n+        strides = (1, 1) + strides + (1,)\n+    else:\n+        spatial_start_dim = 2\n+        strides = (1, 1, 1) + strides\n+    x = tf.expand_dims(x, spatial_start_dim)\n+    depthwise_kernel = tf.expand_dims(depthwise_kernel, 0)\n+    pointwise_kernel = tf.expand_dims(pointwise_kernel, 0)\n+    dilation_rate = (1,) + dilation_rate\n+\n+    x = tf.nn.separable_conv2d(x, depthwise_kernel, pointwise_kernel,\n+                               strides=strides,\n+                               padding=padding,\n+                               rate=dilation_rate,\n+                               data_format=tf_data_format)\n+\n+    x = tf.squeeze(x, [spatial_start_dim])\n+\n+    if data_format == 'channels_first' and tf_data_format == 'NHWC':\n+        x = tf.transpose(x, (0, 2, 1))  # NWC -> NCW\n+\n+    return x\n+\n+\n def separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1),\n                      padding='valid', data_format=None, dilation_rate=(1, 1)):\n     \"\"\"2D convolution with separable filters.\n\n@@ -1972,6 +1972,11 @@ def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n     return conv_out\n \n \n+def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,\n+                     padding='valid', data_format=None, dilation_rate=1):\n+    raise NotImplementedError\n+\n+\n def separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1),\n                      padding='valid', data_format=None, dilation_rate=(1, 1)):\n     raise NotImplementedError\n\n@@ -1037,7 +1037,347 @@ class Conv3DTranspose(Conv3D):\n         return config\n \n \n-class SeparableConv2D(Conv2D):\n+class _SeparableConv(_Conv):\n+    \"\"\"Abstract nD depthwise separable convolution layer (private).\n+\n+    Separable convolutions consist in first performing\n+    a depthwise spatial convolution\n+    (which acts on each input channel separately)\n+    followed by a pointwise convolution which mixes together the resulting\n+    output channels. The `depth_multiplier` argument controls how many\n+    output channels are generated per input channel in the depthwise step.\n+\n+    Intuitively, separable convolutions can be understood as\n+    a way to factorize a convolution kernel into two smaller kernels,\n+    or as an extreme version of an Inception block.\n+\n+    # Arguments\n+        rank: An integer, the rank of the convolution,\n+            e.g. \"2\" for 2D convolution.\n+        filters: Integer, the dimensionality of the output space\n+            (i.e. the number output of filters in the convolution).\n+        kernel_size: An integer or tuple/list of 2 integers, specifying the\n+            width and height of the 2D convolution window.\n+            Can be a single integer to specify the same value for\n+            all spatial dimensions.\n+        strides: An integer or tuple/list of 2 integers,\n+            specifying the strides of the convolution along the width and height.\n+            Can be a single integer to specify the same value for\n+            all spatial dimensions.\n+            Specifying any stride value != 1 is incompatible with specifying\n+            any `dilation_rate` value != 1.\n+        padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n+        data_format: A string,\n+            one of `channels_last` (default) or `channels_first`.\n+            The ordering of the dimensions in the inputs.\n+            `channels_last` corresponds to inputs with shape\n+            `(batch, height, width, channels)` while `channels_first`\n+            corresponds to inputs with shape\n+            `(batch, channels, height, width)`.\n+            It defaults to the `image_data_format` value found in your\n+            Keras config file at `~/.keras/keras.json`.\n+            If you never set it, then it will be \"channels_last\".\n+        depth_multiplier: The number of depthwise convolution output channels\n+            for each input channel.\n+            The total number of depthwise convolution output\n+            channels will be equal to `filterss_in * depth_multiplier`.\n+        activation: Activation function to use\n+            (see [activations](../activations.md)).\n+            If you don't specify anything, no activation is applied\n+            (ie. \"linear\" activation: `a(x) = x`).\n+        use_bias: Boolean, whether the layer uses a bias vector.\n+        depthwise_initializer: Initializer for the depthwise kernel matrix\n+            (see [initializers](../initializers.md)).\n+        pointwise_initializer: Initializer for the pointwise kernel matrix\n+            (see [initializers](../initializers.md)).\n+        bias_initializer: Initializer for the bias vector\n+            (see [initializers](../initializers.md)).\n+        depthwise_regularizer: Regularizer function applied to\n+            the depthwise kernel matrix\n+            (see [regularizer](../regularizers.md)).\n+        pointwise_regularizer: Regularizer function applied to\n+            the pointwise kernel matrix\n+            (see [regularizer](../regularizers.md)).\n+        bias_regularizer: Regularizer function applied to the bias vector\n+            (see [regularizer](../regularizers.md)).\n+        activity_regularizer: Regularizer function applied to\n+            the output of the layer (its \"activation\").\n+            (see [regularizer](../regularizers.md)).\n+        depthwise_constraint: Constraint function applied to\n+            the depthwise kernel matrix\n+            (see [constraints](../constraints.md)).\n+        pointwise_constraint: Constraint function applied to\n+            the pointwise kernel matrix\n+            (see [constraints](../constraints.md)).\n+        bias_constraint: Constraint function applied to the bias vector\n+            (see [constraints](../constraints.md)).\n+\n+    # Input shape\n+        4D tensor with shape:\n+        `(batch, channels, rows, cols)` if data_format='channels_first'\n+        or 4D tensor with shape:\n+        `(batch, rows, cols, channels)` if data_format='channels_last'.\n+\n+    # Output shape\n+        4D tensor with shape:\n+        `(batch, filters, new_rows, new_cols)` if data_format='channels_first'\n+        or 4D tensor with shape:\n+        `(batch, new_rows, new_cols, filters)` if data_format='channels_last'.\n+        `rows` and `cols` values might have changed due to padding.\n+    \"\"\"\n+\n+    def __init__(self, rank,\n+                 filters,\n+                 kernel_size,\n+                 strides=1,\n+                 padding='valid',\n+                 data_format=None,\n+                 depth_multiplier=1,\n+                 activation=None,\n+                 use_bias=True,\n+                 depthwise_initializer='glorot_uniform',\n+                 pointwise_initializer='glorot_uniform',\n+                 bias_initializer='zeros',\n+                 depthwise_regularizer=None,\n+                 pointwise_regularizer=None,\n+                 bias_regularizer=None,\n+                 activity_regularizer=None,\n+                 depthwise_constraint=None,\n+                 pointwise_constraint=None,\n+                 bias_constraint=None,\n+                 **kwargs):\n+        super(_SeparableConv, self).__init__(\n+            rank=rank,\n+            filters=filters,\n+            kernel_size=kernel_size,\n+            strides=strides,\n+            padding=padding,\n+            data_format=data_format,\n+            activation=activation,\n+            use_bias=use_bias,\n+            bias_regularizer=bias_regularizer,\n+            activity_regularizer=activity_regularizer,\n+            bias_constraint=bias_constraint,\n+            **kwargs)\n+        self.depth_multiplier = depth_multiplier\n+        self.depthwise_initializer = initializers.get(depthwise_initializer)\n+        self.pointwise_initializer = initializers.get(pointwise_initializer)\n+        self.depthwise_regularizer = regularizers.get(depthwise_regularizer)\n+        self.pointwise_regularizer = regularizers.get(pointwise_regularizer)\n+        self.depthwise_constraint = constraints.get(depthwise_constraint)\n+        self.pointwise_constraint = constraints.get(pointwise_constraint)\n+\n+    def build(self, input_shape):\n+        if len(input_shape) < self.rank + 2:\n+            raise ValueError('Inputs to `SeparableConv' + str(self.rank) + 'D` '\n+                             'should have rank ' + str(self.rank + 2) + '. '\n+                             'Received input shape:', str(input_shape))\n+        channel_axis = 1 if self.data_format == 'channels_first' else -1\n+        if input_shape[channel_axis] is None:\n+            raise ValueError('The channel dimension of the inputs '\n+                             'should be defined. Found `None`.')\n+        input_dim = int(input_shape[channel_axis])\n+        depthwise_kernel_shape = self.kernel_size + (input_dim, self.depth_multiplier)\n+        pointwise_kernel_shape = (1,) * self.rank + (self.depth_multiplier * input_dim, self.filters)\n+\n+        self.depthwise_kernel = self.add_weight(\n+            shape=depthwise_kernel_shape,\n+            initializer=self.depthwise_initializer,\n+            name='depthwise_kernel',\n+            regularizer=self.depthwise_regularizer,\n+            constraint=self.depthwise_constraint)\n+        self.pointwise_kernel = self.add_weight(\n+            shape=pointwise_kernel_shape,\n+            initializer=self.pointwise_initializer,\n+            name='pointwise_kernel',\n+            regularizer=self.pointwise_regularizer,\n+            constraint=self.pointwise_constraint)\n+\n+        if self.use_bias:\n+            self.bias = self.add_weight(shape=(self.filters,),\n+                                        initializer=self.bias_initializer,\n+                                        name='bias',\n+                                        regularizer=self.bias_regularizer,\n+                                        constraint=self.bias_constraint)\n+        else:\n+            self.bias = None\n+        # Set input spec.\n+        self.input_spec = InputSpec(ndim=self.rank + 2,\n+                                    axes={channel_axis: input_dim})\n+        self.built = True\n+\n+    def call(self, inputs):\n+        if self.rank == 1:\n+            outputs = K.separable_conv1d(\n+                inputs,\n+                self.depthwise_kernel,\n+                self.pointwise_kernel,\n+                data_format=self.data_format,\n+                strides=self.strides,\n+                padding=self.padding,\n+                dilation_rate=self.dilation_rate)\n+        if self.rank == 2:\n+            outputs = K.separable_conv2d(\n+                inputs,\n+                self.depthwise_kernel,\n+                self.pointwise_kernel,\n+                data_format=self.data_format,\n+                strides=self.strides,\n+                padding=self.padding,\n+                dilation_rate=self.dilation_rate)\n+\n+        if self.bias:\n+            outputs = K.bias_add(\n+                outputs,\n+                self.bias,\n+                data_format=self.data_format)\n+\n+        if self.activation is not None:\n+            return self.activation(outputs)\n+        return outputs\n+\n+    def get_config(self):\n+        config = super(_SeparableConv, self).get_config()\n+        config.pop('rank')\n+        config.pop('kernel_initializer')\n+        config.pop('kernel_regularizer')\n+        config.pop('kernel_constraint')\n+        config['depth_multiplier'] = self.depth_multiplier\n+        config['depthwise_initializer'] = initializers.serialize(self.depthwise_initializer)\n+        config['pointwise_initializer'] = initializers.serialize(self.pointwise_initializer)\n+        config['depthwise_regularizer'] = regularizers.serialize(self.depthwise_regularizer)\n+        config['pointwise_regularizer'] = regularizers.serialize(self.pointwise_regularizer)\n+        config['depthwise_constraint'] = constraints.serialize(self.depthwise_constraint)\n+        config['pointwise_constraint'] = constraints.serialize(self.pointwise_constraint)\n+        return config\n+\n+\n+class SeparableConv1D(_SeparableConv):\n+    \"\"\"Depthwise separable 1D convolution.\n+\n+    Separable convolutions consist in first performing\n+    a depthwise spatial convolution\n+    (which acts on each input channel separately)\n+    followed by a pointwise convolution which mixes together the resulting\n+    output channels. The `depth_multiplier` argument controls how many\n+    output channels are generated per input channel in the depthwise step.\n+\n+    Intuitively, separable convolutions can be understood as\n+    a way to factorize a convolution kernel into two smaller kernels,\n+    or as an extreme version of an Inception block.\n+\n+    # Arguments\n+        filters: Integer, the dimensionality of the output space\n+            (i.e. the number output of filters in the convolution).\n+        kernel_size: An integer or tuple/list of single integer,\n+            specifying the length of the 1D convolution window.\n+        strides: An integer or tuple/list of single integer,\n+            specifying the stride length of the convolution.\n+            Specifying any stride value != 1 is incompatible with specifying\n+            any `dilation_rate` value != 1.\n+        padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n+        data_format: A string,\n+            one of `channels_last` (default) or `channels_first`.\n+            The ordering of the dimensions in the inputs.\n+            `channels_last` corresponds to inputs with shape\n+            `(batch, height, width, channels)` while `channels_first`\n+            corresponds to inputs with shape\n+            `(batch, channels, height, width)`.\n+            It defaults to the `image_data_format` value found in your\n+            Keras config file at `~/.keras/keras.json`.\n+            If you never set it, then it will be \"channels_last\".\n+        depth_multiplier: The number of depthwise convolution output channels\n+            for each input channel.\n+            The total number of depthwise convolution output\n+            channels will be equal to `filterss_in * depth_multiplier`.\n+        activation: Activation function to use\n+            (see [activations](../activations.md)).\n+            If you don't specify anything, no activation is applied\n+            (ie. \"linear\" activation: `a(x) = x`).\n+        use_bias: Boolean, whether the layer uses a bias vector.\n+        depthwise_initializer: Initializer for the depthwise kernel matrix\n+            (see [initializers](../initializers.md)).\n+        pointwise_initializer: Initializer for the pointwise kernel matrix\n+            (see [initializers](../initializers.md)).\n+        bias_initializer: Initializer for the bias vector\n+            (see [initializers](../initializers.md)).\n+        depthwise_regularizer: Regularizer function applied to\n+            the depthwise kernel matrix\n+            (see [regularizer](../regularizers.md)).\n+        pointwise_regularizer: Regularizer function applied to\n+            the pointwise kernel matrix\n+            (see [regularizer](../regularizers.md)).\n+        bias_regularizer: Regularizer function applied to the bias vector\n+            (see [regularizer](../regularizers.md)).\n+        activity_regularizer: Regularizer function applied to\n+            the output of the layer (its \"activation\").\n+            (see [regularizer](../regularizers.md)).\n+        depthwise_constraint: Constraint function applied to\n+            the depthwise kernel matrix\n+            (see [constraints](../constraints.md)).\n+        pointwise_constraint: Constraint function applied to\n+            the pointwise kernel matrix\n+            (see [constraints](../constraints.md)).\n+        bias_constraint: Constraint function applied to the bias vector\n+            (see [constraints](../constraints.md)).\n+\n+    # Input shape\n+        3D tensor with shape:\n+        `(batch, channels, steps)` if data_format='channels_first'\n+        or 3D tensor with shape:\n+        `(batch, steps, channels)` if data_format='channels_last'.\n+\n+    # Output shape\n+        3D tensor with shape:\n+        `(batch, filters, new_steps)` if data_format='channels_first'\n+        or 3D tensor with shape:\n+        `(batch, new_steps, filters)` if data_format='channels_last'.\n+        `new_steps` values might have changed due to padding or strides.\n+    \"\"\"\n+\n+    def __init__(self, filters,\n+                 kernel_size,\n+                 strides=1,\n+                 padding='valid',\n+                 data_format=None,\n+                 depth_multiplier=1,\n+                 activation=None,\n+                 use_bias=True,\n+                 depthwise_initializer='glorot_uniform',\n+                 pointwise_initializer='glorot_uniform',\n+                 bias_initializer='zeros',\n+                 depthwise_regularizer=None,\n+                 pointwise_regularizer=None,\n+                 bias_regularizer=None,\n+                 activity_regularizer=None,\n+                 depthwise_constraint=None,\n+                 pointwise_constraint=None,\n+                 bias_constraint=None,\n+                 **kwargs):\n+        super(SeparableConv1D, self).__init__(\n+            rank=1,\n+            filters=filters,\n+            kernel_size=kernel_size,\n+            strides=strides,\n+            padding=padding,\n+            data_format=data_format,\n+            depth_multiplier=depth_multiplier,\n+            activation=activation,\n+            use_bias=use_bias,\n+            depthwise_initializer=depthwise_initializer,\n+            pointwise_initializer=pointwise_initializer,\n+            bias_initializer=bias_initializer,\n+            depthwise_regularizer=depthwise_regularizer,\n+            pointwise_regularizer=pointwise_regularizer,\n+            bias_regularizer=bias_regularizer,\n+            activity_regularizer=activity_regularizer,\n+            depthwise_constraint=depthwise_constraint,\n+            pointwise_constraint=pointwise_constraint,\n+            bias_constraint=bias_constraint,\n+            **kwargs)\n+\n+\n+class SeparableConv2D(_SeparableConv):\n     \"\"\"Depthwise separable 2D convolution.\n \n     Separable convolutions consist in first performing\n@@ -1145,104 +1485,26 @@ class SeparableConv2D(Conv2D):\n                  bias_constraint=None,\n                  **kwargs):\n         super(SeparableConv2D, self).__init__(\n+            rank=2,\n             filters=filters,\n             kernel_size=kernel_size,\n             strides=strides,\n             padding=padding,\n             data_format=data_format,\n+            depth_multiplier=depth_multiplier,\n             activation=activation,\n             use_bias=use_bias,\n+            depthwise_initializer=depthwise_initializer,\n+            pointwise_initializer=pointwise_initializer,\n+            bias_initializer=bias_initializer,\n+            depthwise_regularizer=depthwise_regularizer,\n+            pointwise_regularizer=pointwise_regularizer,\n             bias_regularizer=bias_regularizer,\n             activity_regularizer=activity_regularizer,\n+            depthwise_constraint=depthwise_constraint,\n+            pointwise_constraint=pointwise_constraint,\n             bias_constraint=bias_constraint,\n             **kwargs)\n-        self.depth_multiplier = depth_multiplier\n-        self.depthwise_initializer = initializers.get(depthwise_initializer)\n-        self.pointwise_initializer = initializers.get(pointwise_initializer)\n-        self.depthwise_regularizer = regularizers.get(depthwise_regularizer)\n-        self.pointwise_regularizer = regularizers.get(pointwise_regularizer)\n-        self.depthwise_constraint = constraints.get(depthwise_constraint)\n-        self.pointwise_constraint = constraints.get(pointwise_constraint)\n-\n-    def build(self, input_shape):\n-        if len(input_shape) < 4:\n-            raise ValueError('Inputs to `SeparableConv2D` should have rank 4. '\n-                             'Received input shape:', str(input_shape))\n-        if self.data_format == 'channels_first':\n-            channel_axis = 1\n-        else:\n-            channel_axis = 3\n-        if input_shape[channel_axis] is None:\n-            raise ValueError('The channel dimension of the inputs to '\n-                             '`SeparableConv2D` '\n-                             'should be defined. Found `None`.')\n-        input_dim = int(input_shape[channel_axis])\n-        depthwise_kernel_shape = (self.kernel_size[0],\n-                                  self.kernel_size[1],\n-                                  input_dim,\n-                                  self.depth_multiplier)\n-        pointwise_kernel_shape = (1, 1,\n-                                  self.depth_multiplier * input_dim,\n-                                  self.filters)\n-\n-        self.depthwise_kernel = self.add_weight(\n-            shape=depthwise_kernel_shape,\n-            initializer=self.depthwise_initializer,\n-            name='depthwise_kernel',\n-            regularizer=self.depthwise_regularizer,\n-            constraint=self.depthwise_constraint)\n-        self.pointwise_kernel = self.add_weight(\n-            shape=pointwise_kernel_shape,\n-            initializer=self.pointwise_initializer,\n-            name='pointwise_kernel',\n-            regularizer=self.pointwise_regularizer,\n-            constraint=self.pointwise_constraint)\n-\n-        if self.use_bias:\n-            self.bias = self.add_weight(shape=(self.filters,),\n-                                        initializer=self.bias_initializer,\n-                                        name='bias',\n-                                        regularizer=self.bias_regularizer,\n-                                        constraint=self.bias_constraint)\n-        else:\n-            self.bias = None\n-        # Set input spec.\n-        self.input_spec = InputSpec(ndim=4, axes={channel_axis: input_dim})\n-        self.built = True\n-\n-    def call(self, inputs):\n-        outputs = K.separable_conv2d(\n-            inputs,\n-            self.depthwise_kernel,\n-            self.pointwise_kernel,\n-            data_format=self.data_format,\n-            strides=self.strides,\n-            padding=self.padding,\n-            dilation_rate=self.dilation_rate)\n-\n-        if self.bias:\n-            outputs = K.bias_add(\n-                outputs,\n-                self.bias,\n-                data_format=self.data_format)\n-\n-        if self.activation is not None:\n-            return self.activation(outputs)\n-        return outputs\n-\n-    def get_config(self):\n-        config = super(SeparableConv2D, self).get_config()\n-        config.pop('kernel_initializer')\n-        config.pop('kernel_regularizer')\n-        config.pop('kernel_constraint')\n-        config['depth_multiplier'] = self.depth_multiplier\n-        config['depthwise_initializer'] = initializers.serialize(self.depthwise_initializer)\n-        config['pointwise_initializer'] = initializers.serialize(self.pointwise_initializer)\n-        config['depthwise_regularizer'] = regularizers.serialize(self.depthwise_regularizer)\n-        config['pointwise_regularizer'] = regularizers.serialize(self.pointwise_regularizer)\n-        config['depthwise_constraint'] = constraints.serialize(self.depthwise_constraint)\n-        config['pointwise_constraint'] = constraints.serialize(self.pointwise_constraint)\n-        return config\n \n \n class UpSampling1D(Layer):\n@@ -2101,6 +2363,7 @@ class Cropping3D(Layer):\n Convolution1D = Conv1D\n Convolution2D = Conv2D\n Convolution3D = Conv3D\n+SeparableConvolution1D = SeparableConv1D\n SeparableConvolution2D = SeparableConv2D\n Convolution2DTranspose = Conv2DTranspose\n Deconvolution2D = Deconv2D = Conv2DTranspose\n\n@@ -229,6 +229,55 @@ def test_conv2d_transpose():\n                                                           batch_input_shape=(None, None, 5, None))])\n \n \n+@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Requires TF backend')\n+@keras_test\n+def test_separable_conv_1d():\n+    num_samples = 2\n+    filters = 6\n+    stack_size = 3\n+    num_step = 9\n+\n+    for padding in _convolution_paddings:\n+        for multiplier in [1, 2]:\n+            for dilation_rate in [1, 2]:\n+                if padding == 'same':\n+                    continue\n+                if dilation_rate != 1:\n+                    continue\n+\n+                layer_test(convolutional.SeparableConv1D,\n+                           kwargs={'filters': filters,\n+                                   'kernel_size': 3,\n+                                   'padding': padding,\n+                                   'strides': 1,\n+                                   'depth_multiplier': multiplier,\n+                                   'dilation_rate': dilation_rate},\n+                           input_shape=(num_samples, num_step, stack_size))\n+\n+    layer_test(convolutional.SeparableConv1D,\n+               kwargs={'filters': filters,\n+                       'kernel_size': 3,\n+                       'padding': padding,\n+                       'data_format': 'channels_first',\n+                       'activation': None,\n+                       'depthwise_regularizer': 'l2',\n+                       'pointwise_regularizer': 'l2',\n+                       'bias_regularizer': 'l2',\n+                       'activity_regularizer': 'l2',\n+                       'pointwise_constraint': 'unit_norm',\n+                       'depthwise_constraint': 'unit_norm',\n+                       'strides': 1,\n+                       'depth_multiplier': multiplier},\n+               input_shape=(num_samples, stack_size, num_step))\n+\n+    # Test invalid use case\n+    with pytest.raises(ValueError):\n+        model = Sequential([convolutional.SeparableConv1D(filters=filters,\n+                                                          kernel_size=3,\n+                                                          padding=padding,\n+                                                          batch_input_shape=(None, 5, None))])\n+\n+\n @pytest.mark.skipif(K.backend() != 'tensorflow', reason='Requires TF backend')\n @keras_test\n def test_separable_conv_2d():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
