{"custom_id": "keras#8401e08334d4b1f102a6ee9479738bacfee0600c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2681 | Lines Deleted: 2360 | Files Changed: 133 | Hunks: 894 | Methods Changed: 277 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5041 | Churn Cumulative: 94233 | Contributors (this commit): 51 | Commits (past 90d): 501 | Contributors (cumulative): 658 | DMM Complexity: 0.42857142857142855\n\nDIFF:\n@@ -16,7 +16,6 @@\n \n import tensorflow.compat.v2 as tf\n \n-# pylint: disable=g-bad-import-order,g-direct-tensorflow-import,disable=g-import-not-at-top\n from tensorflow.python import tf2\n \n # Generic layers.\n\n@@ -67,8 +67,8 @@ class Softmax(Layer):\n         normalization is applied.\n     Call arguments:\n       inputs: The inputs, or logits to the softmax layer.\n-      mask: A boolean mask of the same shape as `inputs`. Defaults to `None`. The\n-        mask specifies 1 to keep and 0 to mask.\n+      mask: A boolean mask of the same shape as `inputs`. Defaults to `None`.\n+        The mask specifies 1 to keep and 0 to mask.\n \n     Returns:\n       softmaxed output with the same shape as `inputs`.\n@@ -81,15 +81,15 @@ class Softmax(Layer):\n \n     def call(self, inputs, mask=None):\n         if mask is not None:\n-            # Since mask is 1.0 for positions we want to keep and 0.0 for\n-            # masked positions, this operation will create a tensor which is 0.0 for\n+            # Since mask is 1.0 for positions we want to keep and 0.0 for masked\n+            # positions, this operation will create a tensor which is 0.0 for\n             # positions we want to attend and -1e.9 for masked positions.\n             adder = (1.0 - tf.cast(mask, inputs.dtype)) * (\n                 _large_compatible_negative(inputs.dtype)\n             )\n \n-            # Since we are adding it to the raw scores before the softmax, this is\n-            # effectively the same as removing these entirely.\n+            # Since we are adding it to the raw scores before the softmax, this\n+            # is effectively the same as removing these entirely.\n             inputs += adder\n         if isinstance(self.axis, (tuple, list)):\n             if len(self.axis) > 1:\n\n@@ -50,8 +50,8 @@ class ThresholdedReLU(Layer):\n         super().__init__(**kwargs)\n         if theta is None:\n             raise ValueError(\n-                \"Theta of a Thresholded ReLU layer cannot be None, expecting a float.\"\n-                f\" Received: {theta}\"\n+                \"Theta of a Thresholded ReLU layer cannot be None, expecting a \"\n+                f\"float. Received: {theta}\"\n             )\n         if theta < 0:\n             raise ValueError(\n\n@@ -29,8 +29,8 @@ from tensorflow.python.util.tf_export import keras_export\n class AdditiveAttention(BaseDenseAttention):\n     \"\"\"Additive attention layer, a.k.a. Bahdanau-style attention.\n \n-    Inputs are `query` tensor of shape `[batch_size, Tq, dim]`, `value` tensor of\n-    shape `[batch_size, Tv, dim]` and `key` tensor of shape\n+    Inputs are `query` tensor of shape `[batch_size, Tq, dim]`, `value` tensor\n+    of shape `[batch_size, Tv, dim]` and `key` tensor of shape\n     `[batch_size, Tv, dim]`. The calculation follows the steps:\n \n     1. Reshape `query` and `key` into shapes `[batch_size, Tq, 1, dim]`\n@@ -44,11 +44,12 @@ class AdditiveAttention(BaseDenseAttention):\n        `return tf.matmul(distribution, value)`.\n \n     Args:\n-      use_scale: If `True`, will create a variable to scale the attention scores.\n-      causal: Boolean. Set to `True` for decoder self-attention. Adds a mask such\n-        that position `i` cannot attend to positions `j > i`. This prevents the\n-        flow of information from the future towards the past.\n-        Defaults to `False`.\n+      use_scale: If `True`, will create a variable to scale the attention\n+        scores.\n+      causal: Boolean. Set to `True` for decoder self-attention. Adds a mask\n+        such that position `i` cannot attend to positions `j > i`. This prevents\n+        the flow of information from the future towards the past.  Defaults to\n+        `False`.\n       dropout: Float between 0 and 1. Fraction of the units to drop for the\n         attention scores. Defaults to 0.0.\n \n\n@@ -65,14 +65,18 @@ class AdditiveAttentionTest(tf.test.TestCase, parameterized.TestCase):\n         )\n         actual = attention_layer._calculate_scores(query=q, key=k)\n \n-        # pylint:disable=line-too-long\n-        # expected000 = 0.5*tanh(1.+1.5) + 0.6*tanh(1.1+1.6) + 0.7*tanh(1.2+1.7) + 0.8*tanh(1.3+1.8) = 2.58044532581\n-        # expected001 = 0.5*tanh(1.+2.5) + 0.6*tanh(1.1+2.6) + 0.7*tanh(1.2+2.7) + 0.8*tanh(1.3+2.8) = 2.59734317449\n-        # expected002 = 0.5*tanh(1.+3.5) + 0.6*tanh(1.1+3.6) + 0.7*tanh(1.2+3.7) + 0.8*tanh(1.3+3.8) = 2.59964024652\n-        # expected010 = 0.5*tanh(2.+1.5) + 0.6*tanh(2.1+1.6) + 0.7*tanh(2.2+1.7) + 0.8*tanh(2.3+1.8) = 2.59734317449\n-        # expected011 = 0.5*tanh(2.+2.5) + 0.6*tanh(2.1+2.6) + 0.7*tanh(2.2+2.7) + 0.8*tanh(2.3+2.8) = 2.59964024652\n-        # expected012 = 0.5*tanh(2.+3.5) + 0.6*tanh(2.1+3.6) + 0.7*tanh(2.2+3.7) + 0.8*tanh(2.3+3.8) = 2.59995130916\n-        # pylint:enable=line-too-long\n+        # expected000 = 0.5*tanh(1.+1.5) + 0.6*tanh(1.1+1.6) + \\\n+        #     0.7*tanh(1.2+1.7) + 0.8*tanh(1.3+1.8) = 2.58044532581\n+        # expected001 = 0.5*tanh(1.+2.5) + 0.6*tanh(1.1+2.6) + \\\n+        #     0.7*tanh(1.2+2.7) + 0.8*tanh(1.3+2.8) = 2.59734317449\n+        # expected002 = 0.5*tanh(1.+3.5) + 0.6*tanh(1.1+3.6) + \\\n+        #     0.7*tanh(1.2+3.7) + 0.8*tanh(1.3+3.8) = 2.59964024652\n+        # expected010 = 0.5*tanh(2.+1.5) + 0.6*tanh(2.1+1.6) + \\\n+        #     0.7*tanh(2.2+1.7) + 0.8*tanh(2.3+1.8) = 2.59734317449\n+        # expected011 = 0.5*tanh(2.+2.5) + 0.6*tanh(2.1+2.6) + \\\n+        #     0.7*tanh(2.2+2.7) + 0.8*tanh(2.3+2.8) = 2.59964024652\n+        # expected012 = 0.5*tanh(2.+3.5) + 0.6*tanh(2.1+3.6) + \\\n+        #     0.7*tanh(2.2+3.7) + 0.8*tanh(2.3+3.8) = 2.59995130916\n         expected = np.array(\n             [\n                 [\n@@ -199,9 +203,10 @@ class AdditiveAttentionTest(tf.test.TestCase, parameterized.TestCase):\n         attention_layer.scale = np.array([[[0.5]]], dtype=np.float32)\n         actual = attention_layer([q, v], mask=[None, v_mask])\n \n-        # pylint:disable=line-too-long\n         # Expected scores of shape [1, 1, 3]\n-        # scores = [[[0.5 * tanh(1.1 + 1.6), 0.5 * tanh(1.1 + 0.7), 0.5 * tanh(1.1 - 0.8)]]]\n+        # scores = [[[0.5 * tanh(1.1 + 1.6),\n+        #             0.5 * tanh(1.1 + 0.7),\n+        #             0.5 * tanh(1.1 - 0.8)]]]\n         #        = [[[0.49550372683, 0.47340300642, 0.14565630622]]]\n         # Expected attention distribution = softmax(scores) with zeros in\n         # positions where v_mask == False.\n@@ -216,7 +221,6 @@ class AdditiveAttentionTest(tf.test.TestCase, parameterized.TestCase):\n         # Expected tensor of shape [1, 1, 1].\n         # expected000 = 0.50552495521 * 1.6 + 0.49447504478 * 0.7 - 0 * 0.8\n         #             = 1.15497245968\n-        # pylint:enable=line-too-long\n         expected = np.array([[[1.15497245968]]], dtype=np.float32)\n         self.assertAllClose(expected, actual)\n \n@@ -235,9 +239,10 @@ class AdditiveAttentionTest(tf.test.TestCase, parameterized.TestCase):\n         attention_layer.scale = np.array([[[0.5]]], dtype=np.float32)\n         actual = attention_layer([q, v, k], mask=[None, v_mask])\n \n-        # pylint:disable=line-too-long\n         # Expected scores of shape [1, 1, 3]\n-        # scores = [[[0.5 * tanh(1.1 + 1.6), 0.5 * tanh(1.1 + 0.7), 0.5 * tanh(1.1 - 0.8)]]]\n+        # scores = [[[0.5 * tanh(1.1 + 1.6),\n+        #             0.5 * tanh(1.1 + 0.7),\n+        #             0.5 * tanh(1.1 - 0.8)]]]\n         #        = [[[0.49550372683, 0.47340300642, 0.14565630622]]]\n         # Expected attention distribution = softmax(scores) with zeros in\n         # positions where v_mask == False.\n@@ -252,7 +257,6 @@ class AdditiveAttentionTest(tf.test.TestCase, parameterized.TestCase):\n         # Expected tensor of shape [1, 1, 1].\n         # expected000 = 0.50552495521 * 0.5 + 0.49447504478 * 0.8 - 0 * 0.3\n         #             = 0.64834251342\n-        # pylint:enable=line-too-long\n         expected = np.array([[[0.64834251342]]], dtype=np.float32)\n         self.assertAllClose(expected, actual)\n \n@@ -271,10 +275,13 @@ class AdditiveAttentionTest(tf.test.TestCase, parameterized.TestCase):\n         attention_layer.scale = np.array([[[0.5]]], dtype=np.float32)\n         actual = attention_layer([q, v], mask=[q_mask, v_mask])\n \n-        # pylint:disable=line-too-long\n         # Expected scores of shape [1, 2, 3]\n-        # scores = [[[0.5 * tanh(1.1 + 1.6), 0.5 * tanh(1.1 + 0.7), 0.5 * tanh(1.1 - 0.8)],\n-        #            [0.5 * tanh(-0.5 + 1.6), 0.5 * tanh(-0.5 + 0.7), 0.5 * tanh(-0.5 - 0.8)]]]\n+        # scores = [[[0.5 * tanh(1.1 + 1.6),\n+        #             0.5 * tanh(1.1 + 0.7),\n+        #             0.5 * tanh(1.1 - 0.8)],\n+        #            [0.5 * tanh(-0.5 + 1.6),\n+        #             0.5 * tanh(-0.5 + 0.7),\n+        #             0.5 * tanh(-0.5 - 0.8)]]]\n         #        = [[[0.49550372683, 0.47340300642, 0.14565630622],\n         #            [0.40024951088, 0.09868766011, -0.43086157965]]]\n         # Expected attention distribution = softmax(scores) with zeros in\n@@ -298,7 +305,6 @@ class AdditiveAttentionTest(tf.test.TestCase, parameterized.TestCase):\n         # expected000 = 0.50552495521 * 1.6 + 0.49447504478 * 0.7 - 0 * 0.8\n         #             = 1.15497245968\n         # expected000 = 0\n-        # pylint:enable=line-too-long\n         expected = np.array([[[1.15497245968], [0.0]]], dtype=np.float32)\n         self.assertAllClose(expected, actual)\n \n\n@@ -29,8 +29,8 @@ from tensorflow.python.util.tf_export import keras_export\n class Attention(BaseDenseAttention):\n     \"\"\"Dot-product attention layer, a.k.a. Luong-style attention.\n \n-    Inputs are `query` tensor of shape `[batch_size, Tq, dim]`, `value` tensor of\n-    shape `[batch_size, Tv, dim]` and `key` tensor of shape\n+    Inputs are `query` tensor of shape `[batch_size, Tq, dim]`, `value` tensor\n+    of shape `[batch_size, Tv, dim]` and `key` tensor of shape\n     `[batch_size, Tv, dim]`. The calculation follows the steps:\n \n     1. Calculate scores with shape `[batch_size, Tq, Tv]` as a `query`-`key` dot\n@@ -44,10 +44,10 @@ class Attention(BaseDenseAttention):\n     Args:\n       use_scale: If `True`, will create a scalar variable to scale the attention\n         scores.\n-      causal: Boolean. Set to `True` for decoder self-attention. Adds a mask such\n-        that position `i` cannot attend to positions `j > i`. This prevents the\n-        flow of information from the future towards the past.\n-        Defaults to `False`.\n+      causal: Boolean. Set to `True` for decoder self-attention. Adds a mask\n+        such that position `i` cannot attend to positions `j > i`. This prevents\n+        the flow of information from the future towards the past.  Defaults to\n+        `False`.\n       dropout: Float between 0 and 1. Fraction of the units to drop for the\n         attention scores. Defaults to 0.0.\n       score_mode: Function to use to compute attention scores, one of\n@@ -142,7 +142,8 @@ class Attention(BaseDenseAttention):\n             )\n \n     def build(self, input_shape):\n-        \"\"\"Creates variable when `use_scale` is True or `score_mode` is `concat`.\"\"\"\n+        \"\"\"Creates variable when `use_scale` is True or `score_mode` is\n+        `concat`.\"\"\"\n         if self.use_scale:\n             self.scale = self.add_weight(\n                 name=\"scale\",\n\n@@ -93,13 +93,18 @@ class AttentionTest(tf.test.TestCase, parameterized.TestCase):\n             attention_layer._calculate_scores(query=q, key=k)\n         )\n \n-        # pylint:disable=line-too-long\n-        # expected000 = tanh(1.+1.5) + tanh(1.1+1.6) + tanh(1.2+1.7) + tanh(1.3+1.8) = 3.96753427840\n-        # expected001 = tanh(1.+2.5) + tanh(1.1+2.6) + tanh(1.2+2.7) + tanh(1.3+2.8) = 3.99558784825\n-        # expected002 = tanh(1.+3.5) + tanh(1.1+3.6) + tanh(1.2+3.7) + tanh(1.3+3.8) = 3.99940254147\n-        # expected010 = tanh(2.+1.5) + tanh(2.1+1.6) + tanh(2.2+1.7) + tanh(2.3+1.8) = 3.99558784825\n-        # expected011 = tanh(2.+2.5) + tanh(2.1+2.6) + tanh(2.2+2.7) + tanh(2.3+2.8) = 3.99940254147\n-        # expected012 = tanh(2.+3.5) + tanh(2.1+3.6) + tanh(2.2+3.7) + tanh(2.3+3.8) = 3.99991913657\n+        # expected000 = tanh(1.+1.5) + tanh(1.1+1.6) + \\\n+        #     tanh(1.2+1.7) + tanh(1.3+1.8) = 3.96753427840\n+        # expected001 = tanh(1.+2.5) + tanh(1.1+2.6) + \\\n+        #     tanh(1.2+2.7) + tanh(1.3+2.8) = 3.99558784825\n+        # expected002 = tanh(1.+3.5) + tanh(1.1+3.6) + \\\n+        #     tanh(1.2+3.7) + tanh(1.3+3.8) = 3.99940254147\n+        # expected010 = tanh(2.+1.5) + tanh(2.1+1.6) + \\\n+        #     tanh(2.2+1.7) + tanh(2.3+1.8) = 3.99558784825\n+        # expected011 = tanh(2.+2.5) + tanh(2.1+2.6) + \\\n+        #     tanh(2.2+2.7) + tanh(2.3+2.8) = 3.99940254147\n+        # expected012 = tanh(2.+3.5) + tanh(2.1+3.6) + \\\n+        #     tanh(2.2+3.7) + tanh(2.3+3.8) = 3.99991913657\n         expected = np.array(\n             [\n                 [\n@@ -365,7 +370,8 @@ class AttentionTest(tf.test.TestCase, parameterized.TestCase):\n             )\n \n         # Expected scores of shape [1, 2, 3]\n-        # scores = [[[1.1*1.6, 1.1*0.7, -1.1*0.8], [-0.5*1.6, -0.5*0.7, 0.5*0.8]]]\n+        # scores = [[[1.1*1.6, 1.1*0.7, -1.1*0.8],\n+        #            [-0.5*1.6, -0.5*0.7, 0.5*0.8]]]\n         #        = [[[1.76, 0.77, -0.88], [-0.8, -0.35, 0.4]]]\n         # Expected attention distribution = softmax(scores) with zeros in\n         # positions where v_mask == False.\n@@ -437,7 +443,9 @@ class AttentionTest(tf.test.TestCase, parameterized.TestCase):\n             )\n \n         # Expected scores of shape [1, 3, 3]\n-        # scores = [[0.25, 0.4, -0.15], [0.4, 0.64, -0.24], [-0.15, -0.24, 0.09]]\n+        # scores = [[0.25, 0.4, -0.15],\n+        #           [0.4, 0.64, -0.24],\n+        #           [-0.15, -0.24, 0.09]]\n         # Expected attention distribution = softmax(scores) lower triangular\n         # => attention_distribution00 = [1., 0., 0.]\n         #    attention_distribution01\n@@ -463,7 +471,8 @@ class AttentionTest(tf.test.TestCase, parameterized.TestCase):\n         # expected000 = 0.5\n         # expected010 = 0.44028635073 * 0.5 + 0.55971364926 * 0.8\n         #             = 0.66791409477\n-        # expected020 = 0.31395396638 * 0.5 +0.28693232061 * 0.8 -0.399113713 * 0.3\n+        # expected020 = 0.31395396638 * 0.5 + \\\n+        #     0.28693232061 * 0.8 -0.399113713 * 0.3\n         #             = 0.26678872577\n         expected = np.array(\n             [[[0.5], [0.66791409477], [0.26678872577]]], dtype=np.float32\n\n@@ -34,9 +34,9 @@ class BaseDenseAttention(base_layer.BaseRandomLayer):\n     reuse the `apply_attention_scores()` method.\n \n     Args:\n-      causal: Boolean. Set to `True` for decoder self-attention. Adds a mask such\n-        that position `i` cannot attend to positions `j > i`. This prevents the\n-        flow of information from the future towards the past.\n+      causal: Boolean. Set to `True` for decoder self-attention. Adds a mask\n+        such that position `i` cannot attend to positions `j > i`. This prevents\n+        the flow of information from the future towards the past.\n       dropout: Float between 0 and 1. Fraction of the units to drop for the\n         attention scores.\n \n@@ -90,11 +90,11 @@ class BaseDenseAttention(base_layer.BaseRandomLayer):\n \n         To use this method in your attention layer, follow the steps:\n \n-        * Use `query` tensor of shape `[batch_size, Tq]` and `key` tensor of shape\n-          `[batch_size, Tv]` to calculate the attention `scores`.\n+        * Use `query` tensor of shape `[batch_size, Tq]` and `key` tensor of\n+          shape `[batch_size, Tv]` to calculate the attention `scores`.\n         * Pass `scores` and `value` tensors to this method. The method applies\n-          `scores_mask`, calculates `attention_distribution = softmax(scores)`, then\n-          returns `matmul(attention_distribution, value).\n+          `scores_mask`, calculates `attention_distribution = softmax(scores)`,\n+          then returns `matmul(attention_distribution, value).\n         * Apply `query_mask` and return the result.\n \n         Args:\n@@ -102,8 +102,9 @@ class BaseDenseAttention(base_layer.BaseRandomLayer):\n           value: Value tensor of shape `[batch_size, Tv, dim]`.\n           scores_mask: A boolean mask `Tensor` of shape `[batch_size, 1, Tv]` or\n             `[batch_size, Tq, Tv]`. If given, scores at positions where\n-            `scores_mask==False` do not contribute to the result. It must contain\n-            at least one `True` value in each line along the last dimension.\n+            `scores_mask==False` do not contribute to the result. It must\n+            contain at least one `True` value in each line along the last\n+            dimension.\n           training: Python boolean indicating whether the layer should behave in\n             training mode (adding dropout) or in inference mode (no dropout).\n \n@@ -114,8 +115,8 @@ class BaseDenseAttention(base_layer.BaseRandomLayer):\n         \"\"\"\n         if scores_mask is not None:\n             padding_mask = tf.logical_not(scores_mask)\n-            # Bias so padding positions do not contribute to attention distribution.\n-            # Note 65504. is the max float16 value.\n+            # Bias so padding positions do not contribute to attention\n+            # distribution.  Note 65504. is the max float16 value.\n             if scores.dtype is tf.float16:\n                 scores -= 65504.0 * tf.cast(padding_mask, dtype=scores.dtype)\n             else:\n@@ -148,8 +149,8 @@ class BaseDenseAttention(base_layer.BaseRandomLayer):\n             v_mask = tf.expand_dims(v_mask, axis=-2)\n         if self.causal:\n             # Creates a lower triangular mask, so position i cannot attend to\n-            # positions j>i. This prevents the flow of information from the future\n-            # into the past.\n+            # positions j>i. This prevents the flow of information from the\n+            # future into the past.\n             scores_shape = tf.shape(scores)\n             # causal_mask_shape = [1, Tq, Tv].\n             causal_mask_shape = tf.concat(\n@@ -208,7 +209,8 @@ class BaseDenseAttention(base_layer.BaseRandomLayer):\n             if len(mask) < 2 or len(mask) > len(inputs):\n                 raise ValueError(\n                     f\"{class_name} layer mask must be a list of length 2, \"\n-                    f\"namely [query_mask, value_mask]. Received length: {len(mask)}.\"\n+                    \"namely [query_mask, value_mask]. \"\n+                    f\"Received length: {len(mask)}.\"\n                 )\n \n     def get_config(self):\n\n@@ -71,8 +71,8 @@ class BaseDenseAttentionTest(tf.test.TestCase, parameterized.TestCase):\n             scores=scores, value=v, scores_mask=scores_mask\n         )\n \n-        # Expected softmax scores = softmax(scores) with zeros in positions where\n-        # v_mask == False.\n+        # Expected softmax scores = softmax(scores) with zeros in positions\n+        # where v_mask == False.\n         # => softmax_scores000 = exp(1)/(exp(1) + exp(0)) = 0.73105857863\n         #    softmax_scores001 = exp(0)/(exp(1) + exp(0)) = 0.26894142137\n         #    softmax_scores002 = 0\n\n@@ -51,8 +51,8 @@ def _build_attention_equation(rank, attn_axes):\n     num_heads, <query attention dims>, <key attention dims>)`\n     (2) Combination:\n     `(<batch dims>, num_heads, <query attention dims>, <key attention dims>),\n-    (<batch dims>, <value attention dims>, num_heads, channels) -> (<batch dims>,\n-    <query attention dims>, num_heads, channels)`\n+    (<batch dims>, <value attention dims>, num_heads, channels) -> (<batch\n+    dims>, <query attention dims>, num_heads, channels)`\n \n     Args:\n       rank: Rank of query, key, value tensors.\n@@ -130,8 +130,8 @@ def _get_output_shape(output_rank, known_last_dims):\n class MultiHeadAttention(Layer):\n     \"\"\"MultiHeadAttention layer.\n \n-    This is an implementation of multi-headed attention as described in the paper\n-    \"Attention is all you Need\" (Vaswani et al., 2017).\n+    This is an implementation of multi-headed attention as described in the\n+    paper \"Attention is all you Need\" (Vaswani et al., 2017).\n     If `query`, `key,` `value` are the same, then\n     this is self-attention. Each timestep in `query` attends to the\n     corresponding sequence in `key`, and returns a fixed-width vector.\n@@ -153,8 +153,8 @@ class MultiHeadAttention(Layer):\n     When using MultiHeadAttention inside a custom Layer, the custom Layer must\n     implement `build()` and call MultiHeadAttention's `_build_from_signature()`.\n     This enables weights to be restored correctly when the model is loaded.\n-    TODO(b/172609172): link to documentation about calling custom build functions\n-    when used in a custom Layer.\n+    TODO(b/172609172): link to documentation about calling custom build\n+    functions when used in a custom Layer.\n \n     Examples:\n \n@@ -173,7 +173,8 @@ class MultiHeadAttention(Layer):\n \n     Performs 2D self-attention over a 5D input tensor on axes 2 and 3.\n \n-    >>> layer = MultiHeadAttention(num_heads=2, key_dim=2, attention_axes=(2, 3))\n+    >>> layer = MultiHeadAttention(\n+    ...     num_heads=2, key_dim=2, attention_axes=(2, 3))\n     >>> input_tensor = tf.keras.Input(shape=[5, 3, 4, 16])\n     >>> output_tensor = layer(input_tensor, input_tensor)\n     >>> print(output_tensor.shape)\n@@ -185,8 +186,9 @@ class MultiHeadAttention(Layer):\n       value_dim: Size of each attention head for value.\n       dropout: Dropout probability.\n       use_bias: Boolean, whether the dense layers use bias vectors/matrices.\n-      output_shape: The expected shape of an output tensor, besides the batch and\n-        sequence dims. If not specified, projects back to the key feature dim.\n+      output_shape: The expected shape of an output tensor, besides the batch\n+        and sequence dims. If not specified, projects back to the key feature\n+        dim.\n       attention_axes: axes over which the attention is applied. `None` means\n         attention over all axes, but batch, heads, and features.\n       kernel_initializer: Initializer for dense layer kernels.\n@@ -208,8 +210,8 @@ class MultiHeadAttention(Layer):\n         indicates no attention. Broadcasting can happen for the missing batch\n         dimensions and the head dimension.\n       return_attention_scores: A boolean to indicate whether the output should\n-        be `(attention_output, attention_scores)` if `True`, or `attention_output`\n-        if `False`. Defaults to `False`.\n+        be `(attention_output, attention_scores)` if `True`, or\n+        `attention_output` if `False`. Defaults to `False`.\n       training: Python boolean indicating whether the layer should behave in\n         training mode (adding dropout) or in inference mode (no dropout).\n         Defaults to either using the training mode of the parent layer/model,\n@@ -304,8 +306,8 @@ class MultiHeadAttention(Layer):\n         layer = cls(**config)\n         if None in [query_shape, key_shape, value_shape]:\n             logging.warning(\n-                \"One of dimensions of the input shape is missing. It should have been\"\n-                \" memorized when the layer was serialized. \"\n+                \"One of dimensions of the input shape is missing. It \"\n+                \"should have been memorized when the layer was serialized. \"\n                 \"%s is created without weights.\",\n                 str(cls),\n             )\n@@ -318,7 +320,8 @@ class MultiHeadAttention(Layer):\n     def _build_from_signature(self, query, value, key=None):\n         \"\"\"Builds layers and variables.\n \n-        Once the method is called, self._built_from_signature will be set to True.\n+        Once the method is called, self._built_from_signature will be set to\n+        True.\n \n         Args:\n           query: Query tensor or TensorShape.\n@@ -383,9 +386,9 @@ class MultiHeadAttention(Layer):\n                 **self._get_common_kwargs_for_sublayer()\n             )\n \n-            # Builds the attention computations for multi-head dot product attention.\n-            # These computations could be wrapped into the keras attention layer once\n-            # it supports mult-head einsum computations.\n+            # Builds the attention computations for multi-head dot product\n+            # attention.  These computations could be wrapped into the keras\n+            # attention layer once it supports mult-head einsum computations.\n             self._build_attention(output_rank)\n             self._output_dense = self._make_output_dense(\n                 free_dims,\n@@ -401,8 +404,8 @@ class MultiHeadAttention(Layer):\n             kernel_constraint=self._kernel_constraint,\n             bias_constraint=self._bias_constraint,\n         )\n-        # Create new clone of kernel/bias initializer, so that we don't reuse the\n-        # initializer instance, which could lead to same init value since\n+        # Create new clone of kernel/bias initializer, so that we don't reuse\n+        # the initializer instance, which could lead to same init value since\n         # initializer is stateless.\n         kernel_initializer = self._kernel_initializer.__class__.from_config(\n             self._kernel_initializer.get_config()\n@@ -475,7 +478,8 @@ class MultiHeadAttention(Layer):\n         # `attention_scores` = [B, N, T, S]\n         if attention_mask is not None:\n             # The expand dim happens starting from the `num_heads` dimension,\n-            # (<batch_dims>, num_heads, <query_attention_dims, key_attention_dims>)\n+            # (<batch_dims>, num_heads, <query_attention_dims,\n+            # key_attention_dims>)\n             mask_expansion_axis = -len(self._attention_axes) * 2 - 1\n             for _ in range(\n                 len(attention_scores.shape) - len(attention_mask.shape)\n@@ -491,8 +495,8 @@ class MultiHeadAttention(Layer):\n         \"\"\"Applies Dot-product attention with query, key, value tensors.\n \n         This function defines the computation inside `call` with projected\n-        multi-head Q, K, V inputs. Users can override this function for customized\n-        attention implementation.\n+        multi-head Q, K, V inputs. Users can override this function for\n+        customized attention implementation.\n \n         Args:\n           query: Projected query `Tensor` of shape `(B, T, N, key_dim)`.\n\n@@ -30,7 +30,8 @@ class MultiHeadAttentionTest(test_combinations.TestCase):\n         (\"key_value_different_proj\", 32, 60, [40, 60]),\n     )\n     def test_non_masked_attention(self, value_dim, output_shape, output_dims):\n-        \"\"\"Test that the attention layer can be created without a mask tensor.\"\"\"\n+        \"\"\"Test that the attention layer can be created without a mask\n+        tensor.\"\"\"\n         test_layer = keras.layers.MultiHeadAttention(\n             num_heads=12,\n             key_dim=64,\n@@ -92,19 +93,20 @@ class MultiHeadAttentionTest(test_combinations.TestCase):\n         from_data = 10 * np.random.random_sample((batch_size, 4, 8))\n         to_data = 10 * np.random.random_sample((batch_size, 2, 8))\n \n-        # Invoke the data with a random set of mask data. This should mask at least\n-        # one element.\n+        # Invoke the data with a random set of mask data. This should mask at\n+        # least one element.\n         mask_data = np.random.randint(2, size=(batch_size, 4, 2))\n         masked_output_data = model.predict([from_data, to_data, mask_data])\n \n-        # Invoke the same data, but with a null mask (where no elements are masked).\n+        # Invoke the same data, but with a null mask (where no elements are\n+        # masked).\n         null_mask_data = np.ones((batch_size, 4, 2))\n         unmasked_output_data = model.predict(\n             [from_data, to_data, null_mask_data]\n         )\n \n-        # Because one data is masked and one is not, the outputs should not be the\n-        # same.\n+        # Because one data is masked and one is not, the outputs should not be\n+        # the same.\n         self.assertNotAllClose(masked_output_data, unmasked_output_data)\n \n         # Tests the layer with three inputs: Q, K, V.\n@@ -120,8 +122,8 @@ class MultiHeadAttentionTest(test_combinations.TestCase):\n         unmasked_output_data = model.predict(\n             [from_data, to_data, to_data, null_mask_data]\n         )\n-        # Because one data is masked and one is not, the outputs should not be the\n-        # same.\n+        # Because one data is masked and one is not, the outputs should not be\n+        # the same.\n         self.assertNotAllClose(masked_output_data, unmasked_output_data)\n \n         if use_bias:\n@@ -143,8 +145,8 @@ class MultiHeadAttentionTest(test_combinations.TestCase):\n         output = test_layer(query, query)\n         self.assertEqual(output.shape.as_list(), [None, 40, 80])\n \n-        # Make sure the sub layers have different kernel init value, and not reusing\n-        # the initializers.\n+        # Make sure the sub layers have different kernel init value, and not\n+        # reusing the initializers.\n         self.assertNotAllClose(\n             keras.backend.eval(test_layer._query_dense.kernel),\n             keras.backend.eval(test_layer._key_dense.kernel),\n@@ -177,19 +179,20 @@ class MultiHeadAttentionTest(test_combinations.TestCase):\n         from_data = 10 * np.random.random_sample((batch_size, 4, 8))\n         to_data = 10 * np.random.random_sample((batch_size, 2, 8))\n \n-        # Invoke the data with a random set of mask data. This should mask at least\n-        # one element.\n+        # Invoke the data with a random set of mask data. This should mask at\n+        # least one element.\n         mask_data = np.random.randint(2, size=(batch_size, 4, 2))\n         masked_output_data = model.predict([from_data, to_data, mask_data])\n \n-        # Invoke the same data, but with a null mask (where no elements are masked).\n+        # Invoke the same data, but with a null mask (where no elements are\n+        # masked).\n         null_mask_data = np.ones((batch_size, 4, 2))\n         unmasked_output_data = model.predict(\n             [from_data, to_data, null_mask_data]\n         )\n \n-        # Because one data is masked and one is not, the outputs should not be the\n-        # same.\n+        # Because one data is masked and one is not, the outputs should not be\n+        # the same.\n         self.assertNotAllClose(masked_output_data, unmasked_output_data)\n \n         # Create a model containing attention scores.\n@@ -242,13 +245,14 @@ class MultiHeadAttentionTest(test_combinations.TestCase):\n         query = 10 * np.random.random_sample(query_shape)\n         value = 10 * np.random.random_sample(value_shape)\n \n-        # Invoke the data with a random set of mask data. This should mask at least\n-        # one element.\n+        # Invoke the data with a random set of mask data. This should mask at\n+        # least one element.\n         mask_data = np.random.randint(2, size=mask_shape).astype(\"bool\")\n-        # Invoke the same data, but with a null mask (where no elements are masked).\n+        # Invoke the same data, but with a null mask (where no elements are\n+        # masked).\n         null_mask_data = np.ones(mask_shape)\n-        # Because one data is masked and one is not, the outputs should not be the\n-        # same.\n+        # Because one data is masked and one is not, the outputs should not be\n+        # the same.\n         query_tensor = keras.Input(query_shape[1:], name=\"query\")\n         value_tensor = keras.Input(value_shape[1:], name=\"value\")\n         mask_tensor = keras.Input(mask_shape[1:], name=\"mask\")\n\n@@ -38,7 +38,8 @@ class Conv(Layer):\n     once (except the `trainable` attribute).\n \n     Args:\n-      rank: An integer, the rank of the convolution, e.g. \"2\" for 2D convolution.\n+      rank: An integer, the rank of the convolution, e.g. \"2\" for 2D\n+        convolution.\n       filters: Integer, the dimensionality of the output space (i.e. the number\n         of filters in the convolution). Could be \"None\", eg in the case of\n         depth wise convolution.\n@@ -50,10 +51,12 @@ class Conv(Layer):\n         any `dilation_rate` value != 1.\n       padding: One of `\"valid\"`,  `\"same\"`, or `\"causal\"` (case-insensitive).\n         `\"valid\"` means no padding. `\"same\"` results in padding with zeros\n-        evenly to the left/right or up/down of the input such that output has the\n-        same height/width dimension as the input. `\"causal\"` results in causal\n-        (dilated) convolutions, e.g. `output[t]` does not depend on `input[t+1:]`.\n-      data_format: A string, one of `channels_last` (default) or `channels_first`.\n+        evenly to the left/right or up/down of the input such that output has\n+        the same height/width dimension as the input. `\"causal\"` results in\n+        causal (dilated) convolutions, e.g. `output[t]` does not depend on\n+        `input[t+1:]`.\n+      data_format: A string, one of `channels_last` (default) or\n+        `channels_first`.\n         The ordering of the dimensions in the inputs.\n         `channels_last` corresponds to inputs with shape\n         `(batch_size, ..., channels)` while `channels_first` corresponds to\n@@ -70,8 +73,8 @@ class Conv(Layer):\n       activation: Activation function to use.\n         If you don't specify anything, no activation is applied.\n       use_bias: Boolean, whether the layer uses a bias.\n-      kernel_initializer: An initializer for the convolution kernel. If None, the\n-        default initializer (glorot_uniform) will be used.\n+      kernel_initializer: An initializer for the convolution kernel. If None,\n+        the default initializer (glorot_uniform) will be used.\n       bias_initializer: An initializer for the bias vector. If None, the default\n         initializer (zeros) will be used.\n       kernel_regularizer: Optional regularizer for the convolution kernel.\n@@ -162,8 +165,8 @@ class Conv(Layer):\n     def _validate_init(self):\n         if self.filters is not None and self.filters % self.groups != 0:\n             raise ValueError(\n-                \"The number of filters must be evenly divisible by the number of \"\n-                \"groups. Received: groups={}, filters={}\".format(\n+                \"The number of filters must be evenly divisible by the \"\n+                \"number of groups. Received: groups={}, filters={}\".format(\n                     self.groups, self.filters\n                 )\n             )\n@@ -199,9 +202,9 @@ class Conv(Layer):\n         input_channel = self._get_input_channel(input_shape)\n         if input_channel % self.groups != 0:\n             raise ValueError(\n-                \"The number of input channels must be evenly divisible by the number \"\n-                \"of groups. Received groups={}, but the input has {} channels \"\n-                \"(full input shape is {}).\".format(\n+                \"The number of input channels must be evenly divisible by \"\n+                \"the number of groups. Received groups={}, but the input \"\n+                \"has {} channels (full input shape is {}).\".format(\n                     self.groups, input_channel, input_shape\n                 )\n             )\n@@ -210,8 +213,8 @@ class Conv(Layer):\n             self.filters,\n         )\n \n-        # compute_output_shape contains some validation logic for the input shape,\n-        # and make sure the output shape has all positive dimensions.\n+        # compute_output_shape contains some validation logic for the input\n+        # shape, and make sure the output shape has all positive dimensions.\n         self.compute_output_shape(input_shape)\n \n         self.kernel = self.add_weight(\n@@ -259,9 +262,9 @@ class Conv(Layer):\n             name=self.__class__.__name__,\n         )\n \n-    # TODO(b/213173659): remove this when grouped convolutions are fully supported\n-    # on the CPU for compiled functions. For now, we need this as a workaround for\n-    # CPU support.\n+    # TODO(b/213173659): remove this when grouped convolutions are fully\n+    # supported on the CPU for compiled functions. For now, we need this as a\n+    # workaround for CPU support.\n     @tf.function(jit_compile=True)\n     def _jit_compiled_convolution_op(self, inputs, kernel):\n         return self.convolution_op(inputs, kernel)\n@@ -313,7 +316,7 @@ class Conv(Layer):\n \n     def _spatial_output_shape(self, spatial_input_shape):\n         return [\n-            conv_utils.conv_output_length(  # pylint: disable=g-complex-comprehension\n+            conv_utils.conv_output_length(\n                 length,\n                 self.kernel_size[i],\n                 padding=self.padding,\n\n@@ -26,10 +26,10 @@ import tensorflow.compat.v2 as tf\n class DepthwiseConv(Conv):\n     \"\"\"Depthwise convolution.\n \n-    Depthwise convolution is a type of convolution in which each input channel is\n-    convolved with a different kernel (called a depthwise kernel). You\n-    can understand depthwise convolution as the first step in a depthwise\n-    separable convolution.\n+    Depthwise convolution is a type of convolution in which each input channel\n+    is convolved with a different kernel (called a depthwise kernel). You can\n+    understand depthwise convolution as the first step in a depthwise separable\n+    convolution.\n \n     It is implemented via the following steps:\n \n@@ -41,32 +41,33 @@ class DepthwiseConv(Conv):\n     Unlike a regular convolution, depthwise convolution does not mix\n     information across different input channels.\n \n-    The `depth_multiplier` argument determines how many filter are applied to one\n-    input channel. As such, it controls the amount of output channels that are\n-    generated per input channel in the depthwise step.\n+    The `depth_multiplier` argument determines how many filter are applied to\n+    one input channel. As such, it controls the amount of output channels that\n+    are generated per input channel in the depthwise step.\n \n     Args:\n       kernel_size: A tuple or list of integers specifying the spatial dimensions\n-        of the filters. Can be a single integer to specify the same value for all\n-        spatial dimensions.\n+        of the filters. Can be a single integer to specify the same value for\n+        all spatial dimensions.\n       strides: A tuple or list of integers specifying the strides of the\n         convolution. Can be a single integer to specify the same value for all\n         spatial dimensions. Specifying any `stride` value != 1 is incompatible\n         with specifying any `dilation_rate` value != 1.\n-      padding: One of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means no\n-        padding. `\"same\"` results in padding with zeros evenly to the left/right\n-        or up/down of the input such that output has the same height/width\n-        dimension as the input.\n+      padding: One of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means\n+        no padding. `\"same\"` results in padding with zeros evenly to the\n+        left/right or up/down of the input such that output has the same\n+        height/width dimension as the input.\n       depth_multiplier: The number of depthwise convolution output channels for\n         each input channel. The total number of depthwise convolution output\n         channels will be equal to `filters_in * depth_multiplier`.\n-      data_format: A string, one of `channels_last` (default) or `channels_first`.\n-        The ordering of the dimensions in the inputs. `channels_last` corresponds\n-        to inputs with shape `(batch_size, height, width, channels)` while\n-        `channels_first` corresponds to inputs with shape `(batch_size, channels,\n-        height, width)`. It defaults to the `image_data_format` value found in\n-        your Keras config file at `~/.keras/keras.json`. If you never set it, then\n-        it will be 'channels_last'.\n+      data_format: A string, one of `channels_last` (default) or\n+        `channels_first`.  The ordering of the dimensions in the inputs.\n+        `channels_last` corresponds to inputs with shape `(batch_size, height,\n+        width, channels)` while `channels_first` corresponds to inputs with\n+        shape `(batch_size, channels, height, width)`. It defaults to the\n+        `image_data_format` value found in your Keras config file at\n+        `~/.keras/keras.json`. If you never set it, then it will be\n+        'channels_last'.\n       dilation_rate: An integer or tuple/list of 2 integers, specifying the\n         dilation rate to use for dilated convolution. Currently, specifying any\n         `dilation_rate` value != 1 is incompatible with specifying any `strides`\n@@ -78,10 +79,10 @@ class DepthwiseConv(Conv):\n         `keras.initializers`). If None, the default initializer\n         ('glorot_uniform') will be used.\n       bias_initializer: Initializer for the bias vector (see\n-        `keras.initializers`). If None, the default initializer ('zeros') will be\n-        used.\n-      depthwise_regularizer: Regularizer function applied to the depthwise kernel\n-        matrix (see `keras.regularizers`).\n+        `keras.initializers`). If None, the default initializer ('zeros') will\n+        be used.\n+      depthwise_regularizer: Regularizer function applied to the depthwise\n+        kernel matrix (see `keras.regularizers`).\n       bias_regularizer: Regularizer function applied to the bias vector (see\n         `keras.regularizers`).\n       activity_regularizer: Regularizer function applied to the output of the\n@@ -102,8 +103,8 @@ class DepthwiseConv(Conv):\n         new_cols]` if `data_format='channels_first'`\n         or 4D tensor with shape: `[batch_size,\n         new_rows, new_cols, channels * depth_multiplier]` if\n-        `data_format='channels_last'`. `rows` and `cols` values might have changed\n-        due to padding.\n+        `data_format='channels_last'`. `rows` and `cols` values might have\n+        changed due to padding.\n \n     Returns:\n       A tensor of rank 4 representing\n\n@@ -31,26 +31,28 @@ class SeparableConv(Conv):\n     channels, followed by a pointwise convolution that mixes channels.\n     If `use_bias` is True and a bias initializer is provided,\n     it adds a bias vector to the output.\n-    It then optionally applies an activation function to produce the final output.\n+    It then optionally applies an activation function to produce the final\n+    output.\n \n     Args:\n-      rank: An integer, the rank of the convolution, e.g. \"2\" for 2D convolution.\n+      rank: An integer, the rank of the convolution, e.g. \"2\" for 2D\n+        convolution.\n       filters: Integer, the dimensionality of the output space (i.e. the number\n         of filters in the convolution).\n       kernel_size: A tuple or list of integers specifying the spatial\n         dimensions of the filters. Can be a single integer to specify the same\n         value for all spatial dimensions.\n       strides: A tuple or list of integers specifying the strides\n-        of the convolution. Can be a single integer to specify the same value for\n-        all spatial dimensions.\n+        of the convolution. Can be a single integer to specify the same value\n+        for all spatial dimensions.\n         Specifying any `stride` value != 1 is incompatible with specifying\n         any `dilation_rate` value != 1.\n       padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n-        `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly\n-        to the left/right or up/down of the input such that output has the same\n-        height/width dimension as the input.\n-      data_format: A string, one of `channels_last` (default) or `channels_first`.\n-        The ordering of the dimensions in the inputs.\n+        `\"valid\"` means no padding. `\"same\"` results in padding with zeros\n+        evenly to the left/right or up/down of the input such that output has\n+        the same height/width dimension as the input.\n+      data_format: A string, one of `channels_last` (default) or\n+        `channels_first`.  The ordering of the dimensions in the inputs.\n         `channels_last` corresponds to inputs with shape\n         `(batch_size, ..., channels)` while `channels_first` corresponds to\n         inputs with shape `(batch_size, channels, ...)`.\n\n@@ -44,8 +44,8 @@ class Conv1D(Conv):\n \n     Examples:\n \n-    >>> # The inputs are 128-length vectors with 10 timesteps, and the batch size\n-    >>> # is 4.\n+    >>> # The inputs are 128-length vectors with 10 timesteps, and the\n+    >>> # batch size is 4.\n     >>> input_shape = (4, 10, 128)\n     >>> x = tf.random.normal(input_shape)\n     >>> y = tf.keras.layers.Conv1D(\n@@ -73,9 +73,9 @@ class Conv1D(Conv):\n         Specifying any stride value != 1 is incompatible with specifying\n         any `dilation_rate` value != 1.\n       padding: One of `\"valid\"`, `\"same\"` or `\"causal\"` (case-insensitive).\n-        `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly\n-        to the left/right or up/down of the input such that output has the same\n-        height/width dimension as the input.\n+        `\"valid\"` means no padding. `\"same\"` results in padding with zeros\n+        evenly to the left/right or up/down of the input such that output has\n+        the same height/width dimension as the input.\n         `\"causal\"` results in causal (dilated) convolutions, e.g. `output[t]`\n         does not depend on `input[t+1:]`. Useful when modeling temporal data\n         where the model should not violate the temporal order.\n\n@@ -54,15 +54,15 @@ class Conv1DTranspose(Conv1D):\n         time dimension. Specifying a stride value != 1 is incompatible with\n         specifying a `dilation_rate` value != 1. Defaults to 1.\n       padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n-        `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly\n-        to the left/right or up/down of the input such that output has the same\n-        height/width dimension as the input.\n+        `\"valid\"` means no padding. `\"same\"` results in padding with zeros\n+        evenly to the left/right or up/down of the input such that output has\n+        the same height/width dimension as the input.\n       output_padding: An integer specifying the amount of padding along\n         the time dimension of the output tensor.\n         The amount of output padding must be lower than the stride.\n         If set to `None` (default), the output shape is inferred.\n-      data_format: A string, one of `channels_last` (default) or `channels_first`.\n-        The ordering of the dimensions in the inputs.\n+      data_format: A string, one of `channels_last` (default) or\n+        `channels_first`.  The ordering of the dimensions in the inputs.\n         `channels_last` corresponds to inputs with shape\n         `(batch_size, length, channels)` while `channels_first` corresponds to\n         inputs with shape `(batch_size, channels, length)`.\n\n@@ -57,7 +57,10 @@ class Conv2D(Conv):\n     >>> input_shape = (4, 28, 28, 3)\n     >>> x = tf.random.normal(input_shape)\n     >>> y = tf.keras.layers.Conv2D(\n-    ... 2, 3, activation='relu', dilation_rate=2, input_shape=input_shape[1:])(x)\n+    ...     2, 3,\n+    ...     activation='relu',\n+    ...     dilation_rate=2,\n+    ...     input_shape=input_shape[1:])(x)\n     >>> print(y.shape)\n     (4, 24, 24, 2)\n \n@@ -79,36 +82,38 @@ class Conv2D(Conv):\n \n \n     Args:\n-      filters: Integer, the dimensionality of the output space (i.e. the number of\n-        output filters in the convolution).\n+      filters: Integer, the dimensionality of the output space (i.e. the number\n+        of output filters in the convolution).\n       kernel_size: An integer or tuple/list of 2 integers, specifying the height\n-        and width of the 2D convolution window. Can be a single integer to specify\n-        the same value for all spatial dimensions.\n+        and width of the 2D convolution window. Can be a single integer to\n+        specify the same value for all spatial dimensions.\n       strides: An integer or tuple/list of 2 integers, specifying the strides of\n         the convolution along the height and width. Can be a single integer to\n         specify the same value for all spatial dimensions. Specifying any stride\n-        value != 1 is incompatible with specifying any `dilation_rate` value != 1.\n+        value != 1 is incompatible with specifying any `dilation_rate` value !=\n+        1.\n       padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n-        `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly\n-        to the left/right or up/down of the input. When `padding=\"same\"` and\n-        `strides=1`, the output has the same size as the input.\n-      data_format: A string, one of `channels_last` (default) or `channels_first`.\n-        The ordering of the dimensions in the inputs. `channels_last` corresponds\n-        to inputs with shape `(batch_size, height, width, channels)` while\n-        `channels_first` corresponds to inputs with shape `(batch_size, channels,\n-        height, width)`. It defaults to the `image_data_format` value found in\n-        your Keras config file at `~/.keras/keras.json`. If you never set it, then\n-        it will be `channels_last`.\n+        `\"valid\"` means no padding. `\"same\"` results in padding with zeros\n+        evenly to the left/right or up/down of the input. When `padding=\"same\"`\n+        and `strides=1`, the output has the same size as the input.\n+      data_format: A string, one of `channels_last` (default) or\n+        `channels_first`.  The ordering of the dimensions in the inputs.\n+        `channels_last` corresponds to inputs with shape `(batch_size, height,\n+        width, channels)` while `channels_first` corresponds to inputs with\n+        shape `(batch_size, channels, height, width)`. It defaults to the\n+        `image_data_format` value found in your Keras config file at\n+        `~/.keras/keras.json`. If you never set it, then it will be\n+        `channels_last`.\n       dilation_rate: an integer or tuple/list of 2 integers, specifying the\n         dilation rate to use for dilated convolution. Can be a single integer to\n         specify the same value for all spatial dimensions. Currently, specifying\n-        any `dilation_rate` value != 1 is incompatible with specifying any stride\n-        value != 1.\n+        any `dilation_rate` value != 1 is incompatible with specifying any\n+        stride value != 1.\n       groups: A positive integer specifying the number of groups in which the\n-        input is split along the channel axis. Each group is convolved separately\n-        with `filters / groups` filters. The output is the concatenation of all\n-        the `groups` results along the channel axis. Input channels and `filters`\n-        must both be divisible by `groups`.\n+        input is split along the channel axis. Each group is convolved\n+        separately with `filters / groups` filters. The output is the\n+        concatenation of all the `groups` results along the channel axis. Input\n+        channels and `filters` must both be divisible by `groups`.\n       activation: Activation function to use. If you don't specify anything, no\n         activation is applied (see `keras.activations`).\n       use_bias: Boolean, whether the layer uses a bias vector.\n\n@@ -62,9 +62,9 @@ class Conv2DTranspose(Conv2D):\n         Specifying any stride value != 1 is incompatible with specifying\n         any `dilation_rate` value != 1.\n       padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n-        `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly\n-        to the left/right or up/down of the input such that output has the same\n-        height/width dimension as the input.\n+        `\"valid\"` means no padding. `\"same\"` results in padding with zeros\n+        evenly to the left/right or up/down of the input such that output has\n+        the same height/width dimension as the input.\n       output_padding: An integer or tuple/list of 2 integers,\n         specifying the amount of padding along the height and width\n         of the output tensor.\n@@ -115,10 +115,12 @@ class Conv2DTranspose(Conv2D):\n \n     Output shape:\n       4D tensor with shape:\n-      `(batch_size, filters, new_rows, new_cols)` if data_format='channels_first'\n+      `(batch_size, filters, new_rows, new_cols)` if\n+      data_format='channels_first'\n       or 4D tensor with shape:\n-      `(batch_size, new_rows, new_cols, filters)` if data_format='channels_last'.\n-      `rows` and `cols` values might have changed due to padding.\n+      `(batch_size, new_rows, new_cols, filters)` if\n+      data_format='channels_last'.  `rows` and `cols` values might have changed\n+      due to padding.\n       If `output_padding` is specified:\n       ```\n       new_rows = ((rows - 1) * strides[0] + kernel_size[0] - 2 * padding[0] +\n@@ -247,9 +249,9 @@ class Conv2DTranspose(Conv2D):\n             h_axis, w_axis = 1, 2\n \n         # Use the constant height and weight when possible.\n-        # TODO(scottzhu): Extract this into a utility function that can be applied\n-        # to all convolutional layers, which currently lost the static shape\n-        # information due to tf.shape().\n+        # TODO(scottzhu): Extract this into a utility function that can be\n+        # applied to all convolutional layers, which currently lost the static\n+        # shape information due to tf.shape().\n         height, width = None, None\n         if inputs.shape.rank is not None:\n             dims = inputs.shape.as_list()\n\n@@ -53,8 +53,8 @@ class Conv3D(Conv):\n     >>> print(y.shape)\n     (4, 26, 26, 26, 2)\n \n-    >>> # With extended batch shape [4, 7], e.g. a batch of 4 videos of 3D frames,\n-    >>> # with 7 frames per video.\n+    >>> # With extended batch shape [4, 7], e.g. a batch of 4 videos of\n+    >>> # 3D frames, with 7 frames per video.\n     >>> input_shape = (4, 7, 28, 28, 28, 1)\n     >>> x = tf.random.normal(input_shape)\n     >>> y = tf.keras.layers.Conv3D(\n@@ -63,37 +63,39 @@ class Conv3D(Conv):\n     (4, 7, 26, 26, 26, 2)\n \n     Args:\n-      filters: Integer, the dimensionality of the output space (i.e. the number of\n-        output filters in the convolution).\n+      filters: Integer, the dimensionality of the output space (i.e. the number\n+        of output filters in the convolution).\n       kernel_size: An integer or tuple/list of 3 integers, specifying the depth,\n-        height and width of the 3D convolution window. Can be a single integer to\n-        specify the same value for all spatial dimensions.\n+        height and width of the 3D convolution window. Can be a single integer\n+        to specify the same value for all spatial dimensions.\n       strides: An integer or tuple/list of 3 integers, specifying the strides of\n         the convolution along each spatial dimension. Can be a single integer to\n         specify the same value for all spatial dimensions. Specifying any stride\n-        value != 1 is incompatible with specifying any `dilation_rate` value != 1.\n+        value != 1 is incompatible with specifying any `dilation_rate` value !=\n+        1.\n       padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n-        `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly\n-        to the left/right or up/down of the input such that output has the same\n-        height/width dimension as the input.\n-      data_format: A string, one of `channels_last` (default) or `channels_first`.\n-        The ordering of the dimensions in the inputs. `channels_last` corresponds\n-        to inputs with shape `batch_shape + (spatial_dim1, spatial_dim2,\n-        spatial_dim3, channels)` while `channels_first` corresponds to inputs with\n-        shape `batch_shape + (channels, spatial_dim1, spatial_dim2,\n-        spatial_dim3)`. It defaults to the `image_data_format` value found in your\n-        Keras config file at `~/.keras/keras.json`. If you never set it, then it\n-        will be \"channels_last\".\n+        `\"valid\"` means no padding. `\"same\"` results in padding with zeros\n+        evenly to the left/right or up/down of the input such that output has\n+        the same height/width dimension as the input.\n+      data_format: A string, one of `channels_last` (default) or\n+        `channels_first`.  The ordering of the dimensions in the inputs.\n+        `channels_last` corresponds to inputs with shape `batch_shape +\n+        (spatial_dim1, spatial_dim2, spatial_dim3, channels)` while\n+        `channels_first` corresponds to inputs with shape `batch_shape +\n+        (channels, spatial_dim1, spatial_dim2, spatial_dim3)`. It defaults to\n+        the `image_data_format` value found in your Keras config file at\n+        `~/.keras/keras.json`. If you never set it, then it will be\n+        \"channels_last\".\n       dilation_rate: an integer or tuple/list of 3 integers, specifying the\n         dilation rate to use for dilated convolution. Can be a single integer to\n         specify the same value for all spatial dimensions. Currently, specifying\n-        any `dilation_rate` value != 1 is incompatible with specifying any stride\n-        value != 1.\n+        any `dilation_rate` value != 1 is incompatible with specifying any\n+        stride value != 1.\n       groups: A positive integer specifying the number of groups in which the\n-        input is split along the channel axis. Each group is convolved separately\n-        with `filters / groups` filters. The output is the concatenation of all\n-        the `groups` results along the channel axis. Input channels and `filters`\n-        must both be divisible by `groups`.\n+        input is split along the channel axis. Each group is convolved\n+        separately with `filters / groups` filters. The output is the\n+        concatenation of all the `groups` results along the channel axis. Input\n+        channels and `filters` must both be divisible by `groups`.\n       activation: Activation function to use. If you don't specify anything, no\n         activation is applied (see `keras.activations`).\n       use_bias: Boolean, whether the layer uses a bias vector.\n@@ -122,9 +124,9 @@ class Conv3D(Conv):\n       5+D tensor with shape: `batch_shape + (filters, new_conv_dim1,\n         new_conv_dim2, new_conv_dim3)` if data_format='channels_first'\n       or 5+D tensor with shape: `batch_shape + (new_conv_dim1, new_conv_dim2,\n-        new_conv_dim3, filters)` if data_format='channels_last'. `new_conv_dim1`,\n-        `new_conv_dim2` and `new_conv_dim3` values might have changed due to\n-        padding.\n+        new_conv_dim3, filters)` if data_format='channels_last'.\n+        `new_conv_dim1`, `new_conv_dim2` and `new_conv_dim3` values might have\n+        changed due to padding.\n \n     Returns:\n       A tensor of rank 5+ representing\n\n@@ -44,8 +44,8 @@ class Conv3DTranspose(Conv3D):\n     When using this layer as the first layer in a model,\n     provide the keyword argument `input_shape`\n     (tuple of integers or `None`, does not include the sample axis),\n-    e.g. `input_shape=(128, 128, 128, 3)` for a 128x128x128 volume with 3 channels\n-    if `data_format=\"channels_last\"`.\n+    e.g. `input_shape=(128, 128, 128, 3)` for a 128x128x128 volume with 3\n+    channels if `data_format=\"channels_last\"`.\n \n     Args:\n       filters: Integer, the dimensionality of the output space\n@@ -62,9 +62,9 @@ class Conv3DTranspose(Conv3D):\n         Specifying any stride value != 1 is incompatible with specifying\n         any `dilation_rate` value != 1.\n       padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n-        `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly\n-        to the left/right or up/down of the input such that output has the same\n-        height/width dimension as the input.\n+        `\"valid\"` means no padding. `\"same\"` results in padding with zeros\n+        evenly to the left/right or up/down of the input such that output has\n+        the same height/width dimension as the input.\n       output_padding: An integer or tuple/list of 3 integers,\n         specifying the amount of padding along the depth, height, and\n         width.\n@@ -112,9 +112,11 @@ class Conv3DTranspose(Conv3D):\n \n     Input shape:\n       5D tensor with shape:\n-      `(batch_size, channels, depth, rows, cols)` if data_format='channels_first'\n+      `(batch_size, channels, depth, rows, cols)` if\n+      data_format='channels_first'\n       or 5D tensor with shape:\n-      `(batch_size, depth, rows, cols, channels)` if data_format='channels_last'.\n+      `(batch_size, depth, rows, cols, channels)` if\n+      data_format='channels_last'.\n \n     Output shape:\n       5D tensor with shape:\n\n@@ -642,7 +642,8 @@ class ConvSequentialTest(test_combinations.TestCase):\n             input_shape = (5, None, None, 2)\n             inputs = keras.Input(shape=input_shape)\n             x = layer(inputs)\n-            # Won't raise error here with None values in input shape (b/144282043).\n+            # Won't raise error here with None values in input shape\n+            # (b/144282043).\n             layer(x)\n \n \n\n@@ -27,10 +27,10 @@ from tensorflow.python.util.tf_export import keras_export\n class DepthwiseConv1D(DepthwiseConv):\n     \"\"\"Depthwise 1D convolution.\n \n-    Depthwise convolution is a type of convolution in which each input channel is\n-    convolved with a different kernel (called a depthwise kernel). You\n-    can understand depthwise convolution as the first step in a depthwise\n-    separable convolution.\n+    Depthwise convolution is a type of convolution in which each input channel\n+    is convolved with a different kernel (called a depthwise kernel). You can\n+    understand depthwise convolution as the first step in a depthwise separable\n+    convolution.\n \n     It is implemented via the following steps:\n \n@@ -42,35 +42,36 @@ class DepthwiseConv1D(DepthwiseConv):\n     Unlike a regular 1D convolution, depthwise convolution does not mix\n     information across different input channels.\n \n-    The `depth_multiplier` argument determines how many filter are applied to one\n-    input channel. As such, it controls the amount of output channels that are\n-    generated per input channel in the depthwise step.\n+    The `depth_multiplier` argument determines how many filter are applied to\n+    one input channel. As such, it controls the amount of output channels that\n+    are generated per input channel in the depthwise step.\n \n     Args:\n       kernel_size: An integer, specifying the height and width of the 1D\n-        convolution window. Can be a single integer to specify the same value for\n-        all spatial dimensions.\n+        convolution window. Can be a single integer to specify the same value\n+        for all spatial dimensions.\n       strides: An integer, specifying the strides of the convolution along the\n         height and width. Can be a single integer to specify the same value for\n         all spatial dimensions. Specifying any stride value != 1 is incompatible\n         with specifying any `dilation_rate` value != 1.\n-      padding: one of `'valid'` or `'same'` (case-insensitive). `\"valid\"` means no\n-        padding. `\"same\"` results in padding with zeros evenly to the left/right\n-        or up/down of the input such that output has the same height/width\n-        dimension as the input.\n+      padding: one of `'valid'` or `'same'` (case-insensitive). `\"valid\"` means\n+        no padding. `\"same\"` results in padding with zeros evenly to the\n+        left/right or up/down of the input such that output has the same\n+        height/width dimension as the input.\n       depth_multiplier: The number of depthwise convolution output channels for\n         each input channel. The total number of depthwise convolution output\n         channels will be equal to `filters_in * depth_multiplier`.\n-      data_format: A string, one of `channels_last` (default) or `channels_first`.\n-        The ordering of the dimensions in the inputs. `channels_last` corresponds\n-        to inputs with shape `(batch_size, height, width, channels)` while\n-        `channels_first` corresponds to inputs with shape `(batch_size, channels,\n-        height, width)`. It defaults to the `image_data_format` value found in\n-        your Keras config file at `~/.keras/keras.json`. If you never set it, then\n-        it will be 'channels_last'.\n+      data_format: A string, one of `channels_last` (default) or\n+        `channels_first`.  The ordering of the dimensions in the inputs.\n+        `channels_last` corresponds to inputs with shape `(batch_size, height,\n+        width, channels)` while `channels_first` corresponds to inputs with\n+        shape `(batch_size, channels, height, width)`. It defaults to the\n+        `image_data_format` value found in your Keras config file at\n+        `~/.keras/keras.json`. If you never set it, then it will be\n+        'channels_last'.\n       dilation_rate: A single integer, specifying the dilation rate to use for\n-        dilated convolution. Currently, specifying any `dilation_rate` value != 1\n-        is incompatible with specifying any stride value != 1.\n+        dilated convolution. Currently, specifying any `dilation_rate`\n+        value != 1 is incompatible with specifying any stride value != 1.\n       activation: Activation function to use. If you don't specify anything, no\n         activation is applied (see `keras.activations`).\n       use_bias: Boolean, whether the layer uses a bias vector.\n@@ -78,10 +79,10 @@ class DepthwiseConv1D(DepthwiseConv):\n         `keras.initializers`). If None, the default initializer\n         ('glorot_uniform') will be used.\n       bias_initializer: Initializer for the bias vector (see\n-        `keras.initializers`). If None, the default initializer ('zeros') will be\n-        used.\n-      depthwise_regularizer: Regularizer function applied to the depthwise kernel\n-        matrix (see `keras.regularizers`).\n+        `keras.initializers`). If None, the default initializer ('zeros') will\n+        be used.\n+      depthwise_regularizer: Regularizer function applied to the depthwise\n+        kernel matrix (see `keras.regularizers`).\n       bias_regularizer: Regularizer function applied to the bias vector (see\n         `keras.regularizers`).\n       activity_regularizer: Regularizer function applied to the output of the\n@@ -102,8 +103,8 @@ class DepthwiseConv1D(DepthwiseConv):\n         new_cols]` if `data_format='channels_first'`\n         or 4D tensor with shape: `[batch_size,\n         new_rows, new_cols, channels * depth_multiplier]` if\n-        `data_format='channels_last'`. `rows` and `cols` values might have changed\n-        due to padding.\n+        `data_format='channels_last'`. `rows` and `cols` values might have\n+        changed due to padding.\n \n     Returns:\n       A tensor of rank 4 representing\n\n@@ -27,10 +27,10 @@ from tensorflow.python.util.tf_export import keras_export\n class DepthwiseConv2D(DepthwiseConv):\n     \"\"\"Depthwise 2D convolution.\n \n-    Depthwise convolution is a type of convolution in which each input channel is\n-    convolved with a different kernel (called a depthwise kernel). You\n-    can understand depthwise convolution as the first step in a depthwise\n-    separable convolution.\n+    Depthwise convolution is a type of convolution in which each input channel\n+    is convolved with a different kernel (called a depthwise kernel). You can\n+    understand depthwise convolution as the first step in a depthwise separable\n+    convolution.\n \n     It is implemented via the following steps:\n \n@@ -42,32 +42,34 @@ class DepthwiseConv2D(DepthwiseConv):\n     Unlike a regular 2D convolution, depthwise convolution does not mix\n     information across different input channels.\n \n-    The `depth_multiplier` argument determines how many filter are applied to one\n-    input channel. As such, it controls the amount of output channels that are\n-    generated per input channel in the depthwise step.\n+    The `depth_multiplier` argument determines how many filter are applied to\n+    one input channel. As such, it controls the amount of output channels that\n+    are generated per input channel in the depthwise step.\n \n     Args:\n       kernel_size: An integer or tuple/list of 2 integers, specifying the height\n-        and width of the 2D convolution window. Can be a single integer to specify\n-        the same value for all spatial dimensions.\n+        and width of the 2D convolution window. Can be a single integer to\n+        specify the same value for all spatial dimensions.\n       strides: An integer or tuple/list of 2 integers, specifying the strides of\n         the convolution along the height and width. Can be a single integer to\n         specify the same value for all spatial dimensions. Specifying any stride\n-        value != 1 is incompatible with specifying any `dilation_rate` value != 1.\n-      padding: one of `'valid'` or `'same'` (case-insensitive). `\"valid\"` means no\n-        padding. `\"same\"` results in padding with zeros evenly to the left/right\n-        or up/down of the input such that output has the same height/width\n-        dimension as the input.\n+        value != 1 is incompatible with specifying any `dilation_rate` value !=\n+        1.\n+      padding: one of `'valid'` or `'same'` (case-insensitive). `\"valid\"` means\n+        no padding. `\"same\"` results in padding with zeros evenly to the\n+        left/right or up/down of the input such that output has the same\n+        height/width dimension as the input.\n       depth_multiplier: The number of depthwise convolution output channels for\n         each input channel. The total number of depthwise convolution output\n         channels will be equal to `filters_in * depth_multiplier`.\n-      data_format: A string, one of `channels_last` (default) or `channels_first`.\n-        The ordering of the dimensions in the inputs. `channels_last` corresponds\n-        to inputs with shape `(batch_size, height, width, channels)` while\n-        `channels_first` corresponds to inputs with shape `(batch_size, channels,\n-        height, width)`. It defaults to the `image_data_format` value found in\n-        your Keras config file at `~/.keras/keras.json`. If you never set it, then\n-        it will be 'channels_last'.\n+      data_format: A string, one of `channels_last` (default) or\n+        `channels_first`. The ordering of the dimensions in the inputs.\n+        `channels_last` corresponds to inputs with shape `(batch_size, height,\n+        width, channels)` while `channels_first` corresponds to inputs with\n+        shape `(batch_size, channels, height, width)`. It defaults to the\n+        `image_data_format` value found in your Keras config file at\n+        `~/.keras/keras.json`. If you never set it, then it will be\n+        'channels_last'.\n       dilation_rate: An integer or tuple/list of 2 integers, specifying the\n         dilation rate to use for dilated convolution. Currently, specifying any\n         `dilation_rate` value != 1 is incompatible with specifying any `strides`\n@@ -79,10 +81,10 @@ class DepthwiseConv2D(DepthwiseConv):\n         `keras.initializers`). If None, the default initializer\n         ('glorot_uniform') will be used.\n       bias_initializer: Initializer for the bias vector (see\n-        `keras.initializers`). If None, the default initializer ('zeros') will be\n-        used.\n-      depthwise_regularizer: Regularizer function applied to the depthwise kernel\n-        matrix (see `keras.regularizers`).\n+        `keras.initializers`). If None, the default initializer ('zeros') will\n+        be used.\n+      depthwise_regularizer: Regularizer function applied to the depthwise\n+        kernel matrix (see `keras.regularizers`).\n       bias_regularizer: Regularizer function applied to the bias vector (see\n         `keras.regularizers`).\n       activity_regularizer: Regularizer function applied to the output of the\n@@ -103,8 +105,8 @@ class DepthwiseConv2D(DepthwiseConv):\n         new_cols]` if `data_format='channels_first'`\n         or 4D tensor with shape: `[batch_size,\n         new_rows, new_cols, channels * depth_multiplier]` if\n-        `data_format='channels_last'`. `rows` and `cols` values might have changed\n-        due to padding.\n+        `data_format='channels_last'`. `rows` and `cols` values might have\n+        changed due to padding.\n \n     Returns:\n       A tensor of rank 4 representing\n\n@@ -36,7 +36,8 @@ class SeparableConv1D(SeparableConv):\n     channels, followed by a pointwise convolution that mixes channels.\n     If `use_bias` is True and a bias initializer is provided,\n     it adds a bias vector to the output.\n-    It then optionally applies an activation function to produce the final output.\n+    It then optionally applies an activation function to produce the final\n+    output.\n \n     Args:\n       filters: Integer, the dimensionality of the output space (i.e. the number\n@@ -48,12 +49,13 @@ class SeparableConv1D(SeparableConv):\n         Specifying any `stride` value != 1 is incompatible with specifying\n         any `dilation_rate` value != 1.\n       padding: One of `\"valid\"`, `\"same\"`, or `\"causal\"` (case-insensitive).\n-        `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly\n-        to the left/right or up/down of the input such that output has the same\n-        height/width dimension as the input. `\"causal\"` results in causal\n-        (dilated) convolutions, e.g. `output[t]` does not depend on `input[t+1:]`.\n-      data_format: A string, one of `channels_last` (default) or `channels_first`.\n-        The ordering of the dimensions in the inputs.\n+        `\"valid\"` means no padding. `\"same\"` results in padding with zeros\n+        evenly to the left/right or up/down of the input such that output has\n+        the same height/width dimension as the input. `\"causal\"` results in\n+        causal (dilated) convolutions, e.g. `output[t]` does not depend on\n+        `input[t+1:]`.\n+      data_format: A string, one of `channels_last` (default) or\n+        `channels_first`.  The ordering of the dimensions in the inputs.\n         `channels_last` corresponds to inputs with shape\n         `(batch_size, length, channels)` while `channels_first` corresponds to\n         inputs with shape `(batch_size, channels, length)`.\n@@ -174,7 +176,8 @@ class SeparableConv1D(SeparableConv):\n             spatial_start_dim = 2\n \n         # Explicitly broadcast inputs and kernels to 4D.\n-        # TODO(fchollet): refactor when a native separable_conv1d op is available.\n+        # TODO(fchollet): refactor when a native separable_conv1d op is\n+        # available.\n         inputs = tf.expand_dims(inputs, spatial_start_dim)\n         depthwise_kernel = tf.expand_dims(self.depthwise_kernel, 0)\n         pointwise_kernel = tf.expand_dims(self.pointwise_kernel, 0)\n\n@@ -58,9 +58,9 @@ class SeparableConv2D(SeparableConv):\n         Specifying any stride value != 1 is incompatible with specifying\n         any `dilation_rate` value != 1.\n       padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n-        `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly\n-        to the left/right or up/down of the input such that output has the same\n-        height/width dimension as the input.\n+        `\"valid\"` means no padding. `\"same\"` results in padding with zeros\n+        evenly to the left/right or up/down of the input such that output has\n+        the same height/width dimension as the input.\n       data_format: A string,\n         one of `channels_last` (default) or `channels_first`.\n         The ordering of the dimensions in the inputs.\n@@ -115,10 +115,12 @@ class SeparableConv2D(SeparableConv):\n \n     Output shape:\n       4D tensor with shape:\n-      `(batch_size, filters, new_rows, new_cols)` if data_format='channels_first'\n+      `(batch_size, filters, new_rows, new_cols)` if\n+      data_format='channels_first'\n       or 4D tensor with shape:\n-      `(batch_size, new_rows, new_cols, filters)` if data_format='channels_last'.\n-      `rows` and `cols` values might have changed due to padding.\n+      `(batch_size, new_rows, new_cols, filters)` if\n+      data_format='channels_last'.  `rows` and `cols` values might have changed\n+      due to padding.\n \n     Returns:\n       A tensor of rank 4 representing\n\n@@ -95,9 +95,9 @@ class DropoutLayersTest(test_combinations.TestCase):\n         model = keras.Model(inputs, outputs)\n         train = model(np.ones((20, 5, 10)), training=True)\n         predict = model(np.ones((20, 5, 10)))\n-        # Make sure the weights from tf.random.Generator is not present in the model\n-        # which will cause weight loading issue for existing application models if\n-        # it contains dropout layer.\n+        # Make sure the weights from tf.random.Generator is not present in the\n+        # model which will cause weight loading issue for existing application\n+        # models if it contains dropout layer.\n         self.assertEmpty(layer.get_weights())\n         self.assertEmpty(model.get_weights())\n \n@@ -321,8 +321,8 @@ class TestStatefulLambda(test_combinations.TestCase):\n         def lambda_fn(x, v):\n             return x * v\n \n-        # While it is generally not advised to mix Variables with Lambda layers, if\n-        # the variables are explicitly set as attributes then they are still\n+        # While it is generally not advised to mix Variables with Lambda layers,\n+        # if the variables are explicitly set as attributes then they are still\n         # tracked. This is consistent with the base Layer behavior.\n         layer = keras.layers.Lambda(lambda_fn, arguments={\"v\": v})\n         self.assertLen(layer.trainable_weights, 0)\n@@ -415,8 +415,8 @@ class TestStatefulLambda(test_combinations.TestCase):\n     @test_combinations.run_all_keras_modes\n     @test_combinations.run_with_all_model_types\n     def test_lambda_skip_state_variable_from_initializer(self):\n-        # Force the initializers to use the tf.random.Generator, which will contain\n-        # the state variable.\n+        # Force the initializers to use the tf.random.Generator, which will\n+        # contain the state variable.\n         kernel_initializer = initializers.RandomNormalV2()\n         kernel_initializer._random_generator._rng_type = (\n             kernel_initializer._random_generator.RNG_STATEFUL\n@@ -428,8 +428,8 @@ class TestStatefulLambda(test_combinations.TestCase):\n         def lambda_fn(x):\n             return dense(x + 1)  # Dense layer is built on first call\n \n-        # While it is generally not advised to mix Variables with Lambda layers, if\n-        # the variables are explicitly set as attributes then they are still\n+        # While it is generally not advised to mix Variables with Lambda layers,\n+        # if the variables are explicitly set as attributes then they are still\n         # tracked. This is consistent with the base Layer behavior.\n         layer = keras.layers.Lambda(lambda_fn)\n         layer.dense = dense\n\n@@ -43,11 +43,11 @@ class Dense(Layer):\n     Note: If the input to the layer has a rank greater than 2, then `Dense`\n     computes the dot product between the `inputs` and the `kernel` along the\n     last axis of the `inputs` and axis 0 of the `kernel` (using `tf.tensordot`).\n-    For example, if input has dimensions `(batch_size, d0, d1)`,\n-    then we create a `kernel` with shape `(d1, units)`, and the `kernel` operates\n-    along axis 2 of the `input`, on every sub-tensor of shape `(1, 1, d1)`\n-    (there are `batch_size * d0` such sub-tensors).\n-    The output in this case will have shape `(batch_size, d0, units)`.\n+    For example, if input has dimensions `(batch_size, d0, d1)`, then we create\n+    a `kernel` with shape `(d1, units)`, and the `kernel` operates along axis 2\n+    of the `input`, on every sub-tensor of shape `(1, 1, d1)` (there are\n+    `batch_size * d0` such sub-tensors).  The output in this case will have\n+    shape `(batch_size, d0, units)`.\n \n     Besides, layer attributes cannot be modified after the layer has been called\n     once (except the `trainable` attribute).\n@@ -180,9 +180,9 @@ class Dense(Layer):\n \n         is_ragged = isinstance(inputs, tf.RaggedTensor)\n         if is_ragged:\n-            # In case we encounter a RaggedTensor with a fixed last dimension (last\n-            # dimension not ragged), we can flatten the input and restore the ragged\n-            # dimensions at the end.\n+            # In case we encounter a RaggedTensor with a fixed last dimension\n+            # (last dimension not ragged), we can flatten the input and restore\n+            # the ragged dimensions at the end.\n             if tf.compat.dimension_value(inputs.shape[-1]) is None:\n                 raise ValueError(\n                     \"Dense layer only supports RaggedTensors when the \"\n@@ -208,22 +208,24 @@ class Dense(Layer):\n \n         rank = inputs.shape.rank\n         if rank == 2 or rank is None:\n-            # We use embedding_lookup_sparse as a more efficient matmul operation for\n-            # large sparse input tensors. The op will result in a sparse gradient, as\n-            # opposed to sparse_ops.sparse_tensor_dense_matmul which results in dense\n+            # We use embedding_lookup_sparse as a more efficient matmul\n+            # operation for large sparse input tensors. The op will result in a\n+            # sparse gradient, as opposed to\n+            # sparse_ops.sparse_tensor_dense_matmul which results in dense\n             # gradients. This can lead to sigfinicant speedups, see b/171762937.\n             if isinstance(inputs, tf.SparseTensor):\n-                # We need to fill empty rows, as the op assumes at least one id per row.\n+                # We need to fill empty rows, as the op assumes at least one id\n+                # per row.\n                 inputs, _ = tf.sparse.fill_empty_rows(inputs, 0)\n-                # We need to do some munging of our input to use the embedding lookup as\n-                # a matrix multiply. We split our input matrix into separate ids and\n-                # weights tensors. The values of the ids tensor should be the column\n-                # indices of our input matrix and the values of the weights tensor\n-                # can continue to the actual matrix weights.\n-                # The column arrangement of ids and weights\n-                # will be summed over and does not matter. See the documentation for\n-                # sparse_ops.sparse_tensor_dense_matmul a more detailed explanation\n-                # of the inputs to both ops.\n+                # We need to do some munging of our input to use the embedding\n+                # lookup as a matrix multiply. We split our input matrix into\n+                # separate ids and weights tensors. The values of the ids tensor\n+                # should be the column indices of our input matrix and the\n+                # values of the weights tensor can continue to the actual matrix\n+                # weights.  The column arrangement of ids and weights will be\n+                # summed over and does not matter. See the documentation for\n+                # sparse_ops.sparse_tensor_dense_matmul a more detailed\n+                # explanation of the inputs to both ops.\n                 ids = tf.SparseTensor(\n                     indices=inputs.indices,\n                     values=inputs.indices[:, 1],\n\n@@ -38,8 +38,8 @@ class EinsumDense(Layer):\n     Args:\n       equation: An equation describing the einsum to perform. This equation must\n         be a valid einsum string of the form `ab,bc->ac`, `...ab,bc->...ac`, or\n-        `ab...,bc->ac...` where 'ab', 'bc', and 'ac' can be any valid einsum axis\n-        expression sequence.\n+        `ab...,bc->ac...` where 'ab', 'bc', and 'ac' can be any valid einsum\n+        axis expression sequence.\n       output_shape: The expected shape of the output tensor (excluding the batch\n         dimension and any dimensions represented by ellipses). You can specify\n         None for any dimension that is unknown or can be inferred from the input\n@@ -47,8 +47,8 @@ class EinsumDense(Layer):\n       activation: Activation function to use. If you don't specify anything, no\n         activation is applied (that is, a \"linear\" activation: `a(x) = x`).\n       bias_axes: A string containing the output dimension(s) to apply a bias to.\n-        Each character in the `bias_axes` string should correspond to a character\n-        in the output portion of the `equation` string.\n+        Each character in the `bias_axes` string should correspond to a\n+        character in the output portion of the `equation` string.\n       kernel_initializer: Initializer for the `kernel` weights matrix.\n       bias_initializer: Initializer for the bias vector.\n       kernel_regularizer: Regularizer function applied to the `kernel` weights\n@@ -81,8 +81,8 @@ class EinsumDense(Layer):\n     This example shows how to instantiate a layer that applies the same dense\n     operation to every element in a sequence. Here, the `output_shape` has two\n     values (since there are two non-batch dimensions in the output); the first\n-    dimension in the `output_shape` is `None`, because the sequence dimension `b`\n-    has an unknown shape.\n+    dimension in the `output_shape` is `None`, because the sequence dimension\n+    `b` has an unknown shape.\n \n     >>> layer = tf.keras.layers.EinsumDense(\"abc,cd->abd\",\n     ...                                     output_shape=(None, 64),\n@@ -99,9 +99,9 @@ class EinsumDense(Layer):\n     instead of specifying the batch and sequence dimensions.\n \n     Because we are using ellipsis notation and have specified only one axis, the\n-    `output_shape` arg is a single value. When instantiated in this way, the layer\n-    can handle any number of sequence dimensions - including the case where no\n-    sequence dimension exists.\n+    `output_shape` arg is a single value. When instantiated in this way, the\n+    layer can handle any number of sequence dimensions - including the case\n+    where no sequence dimension exists.\n \n     >>> layer = tf.keras.layers.EinsumDense(\"...x,xy->...y\",\n     ...                                     output_shape=64,\n@@ -266,16 +266,16 @@ def _analyze_split_string(\n \n     if elided > 0 and left_elided:\n         for i in range(1, elided):\n-            # We already inserted the 0th input dimension at dim 0, so we need to\n-            # start at location 1 here.\n+            # We already inserted the 0th input dimension at dim 0, so we need\n+            # to start at location 1 here.\n             output_shape.insert(1, input_shape[i])\n     elif elided > 0 and not left_elided:\n         for i in range(len(input_shape) - elided, len(input_shape)):\n             output_shape.append(input_shape[i])\n \n     if left_elided:\n-        # If we have beginning dimensions elided, we need to use negative indexing\n-        # to determine where in the input dimension our values are.\n+        # If we have beginning dimensions elided, we need to use negative\n+        # indexing to determine where in the input dimension our values are.\n         input_dim_map = {\n             dim: (i + elided) - len(input_shape)\n             for i, dim in enumerate(input_spec)\n@@ -307,9 +307,9 @@ def _analyze_split_string(\n     for dim in output_spec:\n         if dim not in input_spec and dim not in weight_spec:\n             raise ValueError(\n-                f\"Dimension '{dim}' was specified in the output '{output_spec}' but \"\n-                f\"has no corresponding dim in the input spec '{input_spec}' or \"\n-                f\"weight spec '{output_spec}'\"\n+                f\"Dimension '{dim}' was specified in the output \"\n+                f\"'{output_spec}' but has no corresponding dim in the input \"\n+                f\"spec '{input_spec}' or weight spec '{output_spec}'\"\n             )\n \n     weight_shape = []\n@@ -321,8 +321,9 @@ def _analyze_split_string(\n         else:\n             raise ValueError(\n                 f\"Weight dimension '{dim}' did not have a match in either \"\n-                f\"the input spec '{input_spec}' or the output spec '{output_spec}'. \"\n-                \"For this layer, the weight must be fully specified.\"\n+                f\"the input spec '{input_spec}' or the output \"\n+                f\"spec '{output_spec}'. For this layer, the weight must \"\n+                \"be fully specified.\"\n             )\n \n     if bias_axes is not None:\n\n@@ -277,7 +277,8 @@ class TestEinsumDenseLayer(test_combinations.TestCase):\n         expected_bias_shape,\n         expected_output_shape,\n     ):\n-        # Keras elides the 0-dimension of the input shape when constructing inputs.\n+        # Keras elides the 0-dimension of the input shape when constructing\n+        # inputs.\n         non_batch_input_shape = list(input_shape)[1:]\n \n         input_tensor = keras.Input(shape=non_batch_input_shape)\n\n@@ -67,15 +67,13 @@ class Embedding(Layer):\n         the `embeddings` matrix (see `keras.regularizers`).\n       embeddings_constraint: Constraint function applied to\n         the `embeddings` matrix (see `keras.constraints`).\n-      mask_zero: Boolean, whether or not the input value 0 is a special \"padding\"\n-        value that should be masked out.\n-        This is useful when using recurrent layers\n-        which may take variable length input.\n-        If this is `True`, then all subsequent layers\n-        in the model need to support masking or an exception will be raised.\n-        If mask_zero is set to True, as a consequence, index 0 cannot be\n-        used in the vocabulary (input_dim should equal size of\n-        vocabulary + 1).\n+      mask_zero: Boolean, whether or not the input value 0 is a special\n+        \"padding\" value that should be masked out. This is useful when using\n+        recurrent layers which may take variable length input. If this is\n+        `True`, then all subsequent layers in the model need to support masking\n+        or an exception will be raised. If mask_zero is set to True, as a\n+        consequence, index 0 cannot be used in the vocabulary (input_dim should\n+        equal size of vocabulary + 1).\n       input_length: Length of input sequences, when it is constant.\n         This argument is required if you are going to connect\n         `Flatten` then `Dense` layers upstream\n@@ -131,19 +129,20 @@ class Embedding(Layer):\n         if input_dim <= 0 or output_dim <= 0:\n             raise ValueError(\n                 \"Both `input_dim` and `output_dim` should be positive, \"\n-                f\"Received input_dim = {input_dim} and output_dim = {output_dim}\"\n+                f\"Received input_dim = {input_dim} \"\n+                f\"and output_dim = {output_dim}\"\n             )\n         if (\n             not base_layer_utils.v2_dtype_behavior_enabled()\n             and \"dtype\" not in kwargs\n         ):\n-            # In TF1, the dtype defaults to the input dtype which is typically int32,\n-            # so explicitly set it to floatx\n+            # In TF1, the dtype defaults to the input dtype which is typically\n+            # int32, so explicitly set it to floatx\n             kwargs[\"dtype\"] = backend.floatx()\n-        # We set autocast to False, as we do not want to cast floating- point inputs\n-        # to self.dtype. In call(), we cast to int32, and casting to self.dtype\n-        # before casting to int32 might cause the int32 values to be different due\n-        # to a loss of precision.\n+        # We set autocast to False, as we do not want to cast floating- point\n+        # inputs to self.dtype. In call(), we cast to int32, and casting to\n+        # self.dtype before casting to int32 might cause the int32 values to be\n+        # different due to a loss of precision.\n         kwargs[\"autocast\"] = False\n         super().__init__(**kwargs)\n \n@@ -186,15 +185,15 @@ class Embedding(Layer):\n                 in_lens = [self.input_length]\n             if len(in_lens) != len(input_shape) - 1:\n                 raise ValueError(\n-                    f'\"input_length\" is {self.input_length}, but received input has '\n-                    f\"shape {input_shape}\"\n+                    f'\"input_length\" is {self.input_length}, but received '\n+                    f\"input has shape {input_shape}\"\n                 )\n             else:\n                 for i, (s1, s2) in enumerate(zip(in_lens, input_shape[1:])):\n                     if s1 is not None and s2 is not None and s1 != s2:\n                         raise ValueError(\n-                            f'\"input_length\" is {self.input_length}, but received input '\n-                            f\"has shape {input_shape}\"\n+                            f'\"input_length\" is {self.input_length}, but '\n+                            f\"received input has shape {input_shape}\"\n                         )\n                     elif s1 is None:\n                         in_lens[i] = s2\n@@ -209,8 +208,8 @@ class Embedding(Layer):\n             self._dtype_policy.compute_dtype\n             != self._dtype_policy.variable_dtype\n         ):\n-            # Instead of casting the variable as in most layers, cast the output, as\n-            # this is mathematically equivalent but is faster.\n+            # Instead of casting the variable as in most layers, cast the\n+            # output, as this is mathematically equivalent but is faster.\n             out = tf.cast(out, self._dtype_policy.compute_dtype)\n         return out\n \n\n@@ -36,7 +36,8 @@ class Lambda(Layer):\n     as a `Layer` when constructing `Sequential`\n     and Functional API models. `Lambda` layers are best suited for simple\n     operations or quick experimentation. For more advanced use cases, follow\n-    [this guide](https://www.tensorflow.org/guide/keras/custom_layers_and_models)\n+    [this guide](\n+    https://www.tensorflow.org/guide/keras/custom_layers_and_models)\n     for subclassing `tf.keras.layers.Layer`.\n \n     WARNING: `tf.keras.layers.Lambda` layers have (de)serialization limitations!\n@@ -97,7 +98,8 @@ class Lambda(Layer):\n     ```\n \n       In general, Lambda layers can be convenient for simple stateless\n-      computation, but anything more complex should use a subclass Layer instead.\n+      computation, but anything more complex should use a subclass Layer\n+      instead.\n \n     Args:\n       function: The function to be evaluated. Takes input tensor as first\n@@ -105,12 +107,12 @@ class Lambda(Layer):\n       output_shape: Expected output shape from function. This argument can be\n         inferred if not explicitly provided. Can be a tuple or function. If a\n         tuple, it only specifies the first dimension onward;\n-        sample dimension is assumed either the same as the input: `output_shape =\n-          (input_shape[0], ) + output_shape` or, the input is `None` and\n-        the sample dimension is also `None`: `output_shape = (None, ) +\n-          output_shape` If a function, it specifies the entire shape as a function\n-          of the\n-        input shape: `output_shape = f(input_shape)`\n+        sample dimension is assumed either the same as the input:\n+        `output_shape = (input_shape[0], ) + output_shape` or, the input is\n+        `None` and the sample dimension is also `None`:\n+        `output_shape = (None, ) + output_shape` If a function, it specifies the\n+        entire shape as a function of the input shape:\n+        `output_shape = f(input_shape)`\n       mask: Either None (indicating no masking) or a callable with the same\n         signature as the `compute_mask` layer method, or a tensor that will be\n         returned as output mask regardless of what the input is.\n@@ -147,16 +149,17 @@ class Lambda(Layer):\n     def compute_output_shape(self, input_shape):\n         if self._output_shape is None:\n             # Make use of existing autocomputation but provide Lambda-specific\n-            # error message. This is always safe to run even when the outer context\n-            # is Graph mode because Lambda layers don't have side effects such as\n-            # `add_loss`.\n+            # error message. This is always safe to run even when the outer\n+            # context is Graph mode because Lambda layers don't have side\n+            # effects such as `add_loss`.\n             with tf.__internal__.eager_context.eager_mode():\n                 try:\n                     return super().compute_output_shape(input_shape)\n                 except NotImplementedError:\n                     raise NotImplementedError(\n-                        \"We could not automatically infer the shape of the Lambda's \"\n-                        \"output. Please specify `output_shape` for this Lambda.\"\n+                        \"We could not automatically infer the shape of \"\n+                        \"the Lambda's output. Please specify `output_shape` \"\n+                        \"for this Lambda.\"\n                     )\n \n         if callable(self._output_shape):\n@@ -180,7 +183,8 @@ class Lambda(Layer):\n         return tf.nest.map_structure(_add_batch, output_shapes)\n \n     def call(self, inputs, mask=None, training=None):\n-        # We must copy for thread safety, but it only needs to be a shallow copy.\n+        # We must copy for thread safety, but it only needs to be a shallow\n+        # copy.\n         kwargs = {k: v for k, v in self.arguments.items()}\n         if self._fn_expects_mask_arg:\n             kwargs[\"mask\"] = mask\n@@ -203,9 +207,9 @@ class Lambda(Layer):\n \n     def _check_variables(self, created_variables, accessed_variables):\n         if not created_variables and not accessed_variables:\n-            # In the common case that a Lambda layer does not touch a Variable, we\n-            # don't want to incur the runtime cost of assembling any state used for\n-            # checking only to immediately discard it.\n+            # In the common case that a Lambda layer does not touch a Variable,\n+            # we don't want to incur the runtime cost of assembling any state\n+            # used for checking only to immediately discard it.\n             return\n \n         # Filter out the state variable in the tf.random.Generator, which is\n@@ -257,8 +261,8 @@ class Lambda(Layer):\n             self._already_warned = True\n \n     def _warn(self, msg):\n-        # This method will be overridden in a unit test to raise an error, because\n-        # self.assertWarns is not universally implemented.\n+        # This method will be overridden in a unit test to raise an error,\n+        # because self.assertWarns is not universally implemented.\n         return tf_logging.warning(msg)\n \n     def compute_mask(self, inputs, mask=None):\n@@ -392,6 +396,7 @@ class Lambda(Layer):\n             supported_types = [\"function\", \"lambda\", \"raw\"]\n             raise TypeError(\n                 f\"Unsupported value for `function_type` argument. Received: \"\n-                f\"function_type={function_type}. Expected one of {supported_types}\"\n+                f\"function_type={function_type}. \"\n+                f\"Expected one of {supported_types}\"\n             )\n         return function\n\n@@ -13,7 +13,6 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Contains the TFOpLambda layer.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import,g-bad-import-order\n import tensorflow.compat.v2 as tf\n \n # pylint: enable=g-bad-import-order\n@@ -109,7 +108,8 @@ class ClassMethod(Layer):\n \n \n class KerasOpDispatcher(tf.__internal__.dispatch.GlobalOpDispatcher):\n-    \"\"\"A global dispatcher that allows building a functional model with TF Ops.\"\"\"\n+    \"\"\"A global dispatcher that allows building a functional model with TF\n+    Ops.\"\"\"\n \n     def handle(self, op, args, kwargs):\n         \"\"\"Handle the specified operation with the specified arguments.\"\"\"\n@@ -283,9 +283,9 @@ class TFOpLambda(Layer):\n \n     def _check_variables(self, created_variables, accessed_variables):\n         if not created_variables and not accessed_variables:\n-            # In the common case that a Lambda layer does not touch a Variable, we\n-            # don't want to incur the runtime cost of assembling any state used for\n-            # checking only to immediately discard it.\n+            # In the common case that a Lambda layer does not touch a Variable,\n+            # we don't want to incur the runtime cost of assembling any state\n+            # used for checking only to immediately discard it.\n             return\n \n         tracked_weights = set(v.ref() for v in self.weights)\n@@ -298,11 +298,13 @@ class TFOpLambda(Layer):\n             )\n             raise ValueError(\n                 \"The following Variables were created within a Lambda layer \"\n-                f\"({self.name}) but are not tracked by said layer: {variable_str}\\n\"\n+                f\"({self.name}) but are not tracked by said layer: \"\n+                f\"{variable_str}\\n\"\n                 \"The layer cannot safely ensure proper Variable reuse \"\n-                \"across multiple calls, and consequently this behavior is disallowed \"\n-                \"for safety reasons. Lambda layers are not well suited for stateful \"\n-                \"computation; instead, writing a subclassed Layer is the recommend \"\n+                \"across multiple calls, and consequently this behavior \"\n+                \"is disallowed for safety reasons. Lambda layers are \"\n+                \"not well suited for stateful computation; instead, \"\n+                \"writing a subclassed Layer is the recommend \"\n                 \"way to define layers with Variables.\"\n             )\n \n@@ -316,22 +318,22 @@ class TFOpLambda(Layer):\n             self._warn(\n                 \"The following Variables were used in a Lambda layer's call \"\n                 f\"({self.name}), but are not present in its tracked objects: \"\n-                f\"{variable_str}. This is a strong indication that the Lambda layer \"\n-                \"should be rewritten as a subclassed Layer.\"\n+                f\"{variable_str}. This is a strong indication that the Lambda \"\n+                \"layer should be rewritten as a subclassed Layer.\"\n             )\n             self._already_warned = True\n \n     def _warn(self, msg):\n-        # This method will be overridden in a unit test to raise an error, because\n-        # self.assertWarns is not universally implemented.\n+        # This method will be overridden in a unit test to raise an error,\n+        # because self.assertWarns is not universally implemented.\n         return tf_logging.warning(msg)\n \n     def get_config(self):\n         if not self.symbol:\n             raise ValueError(\n-                f\"This Keras op layer was generated from {self.function}, a method \"\n-                \"that is not publicly exposed in the TensorFlow API. This \"\n-                \"may have happened if the method was explicitly \"\n+                f\"This Keras op layer was generated from {self.function}, a \"\n+                \"method that is not publicly exposed in the TensorFlow API. \"\n+                \"This may have happened if the method was explicitly \"\n                 \"decorated to add dispatching support, and it was used \"\n                 \"during Functional model construction. \"\n                 \"To ensure cross-version compatibility of Keras models \"\n@@ -368,7 +370,8 @@ def _delegate_property(\n     intermediate values in the model.\n \n     Args:\n-      keras_tensor_cls: The KerasTensor subclass that should expose the property.\n+      keras_tensor_cls: The KerasTensor subclass that should expose the\n+        property.\n       property_name: The name of the property to expose and delegate to the\n         represented (Composite)Tensor.\n     \"\"\"\n@@ -387,12 +390,13 @@ def _delegate_method(\n \n     Calling this function times with the same arguments should be a no-op.\n \n-    This method exposes an instance method on the KerasTensor class that will use\n-    an `InstanceMethod` layer to run the desired method on the represented\n+    This method exposes an instance method on the KerasTensor class that will\n+    use an `InstanceMethod` layer to run the desired method on the represented\n     intermediate values in the model.\n \n     Args:\n-      keras_tensor_cls: The KerasTensor subclass that should expose the property.\n+      keras_tensor_cls: The KerasTensor subclass that should expose the\n+        property.\n       method_name: The name of the method to expose and delegate to the\n         represented (Composite)Tensor.\n     \"\"\"\n@@ -449,7 +453,8 @@ for sparse_method in [\n \n \n class TFClassMethodDispatcher(tf.__internal__.dispatch.OpDispatcher):\n-    \"\"\"A class method dispatcher that allows building a functional model with TF class methods.\"\"\"\n+    \"\"\"A class method dispatcher that allows building a functional model with TF\n+    class methods.\"\"\"\n \n     def __init__(self, cls, method_name):\n         self.cls = cls\n@@ -513,9 +518,9 @@ class SlicingOpLambda(TFOpLambda):\n             # because dicts are flattened by nest while slices aren't.\n             # So, map_structure would only see the individual elements in the\n             # dict.\n-            # This can't use map_structure_up_to either because the 'shallowness' of\n-            # the shallow tree would have to vary depending on if only one dim or\n-            # multiple are being sliced.\n+            # This can't use map_structure_up_to either because the\n+            # 'shallowness' of the shallow tree would have to vary depending on\n+            # if only one dim or multiple are being sliced.\n             new_args = []\n             for arg in args:\n                 arg = _dict_to_slice(arg)\n@@ -557,7 +562,8 @@ def _dict_to_slice(x):\n \n \n class TFSlicingOpDispatcher(tf.__internal__.dispatch.OpDispatcher):\n-    \"\"\"A global dispatcher that allows building a functional model with TF Ops.\"\"\"\n+    \"\"\"A global dispatcher that allows building a functional model with TF\n+    Ops.\"\"\"\n \n     def __init__(self, op):\n         self.op = op\n\n@@ -30,22 +30,22 @@ _SUPPORTED_RBF_KERNEL_TYPES = [\"gaussian\", \"laplacian\"]\n class RandomFourierFeatures(base_layer.Layer):\n     r\"\"\"Layer that projects its inputs into a random feature space.\n \n-    This layer implements a mapping from input space to a space with `output_dim`\n-    dimensions, which approximates shift-invariant kernels. A kernel function\n-    `K(x, y)` is shift-invariant if `K(x, y) == k(x - y)` for some function `k`.\n-    Many popular Radial Basis Functions (RBF), including Gaussian and\n-    Laplacian kernels, are shift-invariant.\n+    This layer implements a mapping from input space to a space with\n+    `output_dim` dimensions, which approximates shift-invariant kernels. A\n+    kernel function `K(x, y)` is shift-invariant if `K(x, y) == k(x - y)` for\n+    some function `k`.  Many popular Radial Basis Functions (RBF), including\n+    Gaussian and Laplacian kernels, are shift-invariant.\n \n     The implementation of this layer is based on the following paper:\n     [\"Random Features for Large-Scale Kernel Machines\"](\n       https://people.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf)\n     by Ali Rahimi and Ben Recht.\n \n-    The distribution from which the parameters of the random features map (layer)\n-    are sampled determines which shift-invariant kernel the layer approximates\n-    (see paper for more details). You can use the distribution of your\n-    choice. The layer supports out-of-the-box\n-    approximations of the following two RBF kernels:\n+    The distribution from which the parameters of the random features map\n+    (layer) are sampled determines which shift-invariant kernel the layer\n+    approximates (see paper for more details). You can use the distribution of\n+    your choice. The layer supports out-of-the-box approximations of the\n+    following two RBF kernels:\n \n     - Gaussian: `K(x, y) == exp(- square(x - y) / (2 * square(scale)))`\n     - Laplacian: `K(x, y) = exp(-abs(x - y) / scale))`\n@@ -56,15 +56,16 @@ class RandomFourierFeatures(base_layer.Layer):\n \n     **Usage:** Typically, this layer is used to \"kernelize\" linear models by\n     applying a non-linear transformation (this layer) to the input features and\n-    then training a linear model on top of the transformed features. Depending on\n-    the loss function of the linear model, the composition of this layer and the\n-    linear model results to models that are equivalent (up to approximation) to\n-    kernel SVMs (for hinge loss), kernel logistic regression (for logistic loss),\n-    kernel linear regression (for squared loss), etc.\n+    then training a linear model on top of the transformed features. Depending\n+    on the loss function of the linear model, the composition of this layer and\n+    the linear model results to models that are equivalent (up to approximation)\n+    to kernel SVMs (for hinge loss), kernel logistic regression (for logistic\n+    loss), kernel linear regression (for squared loss), etc.\n \n     Examples:\n \n-    A kernel multinomial logistic regression model with Gaussian kernel for MNIST:\n+    A kernel multinomial logistic regression model with Gaussian kernel for\n+    MNIST:\n \n     ```python\n     model = keras.Sequential([\n@@ -111,23 +112,23 @@ class RandomFourierFeatures(base_layer.Layer):\n     ```\n \n     Args:\n-      output_dim: Positive integer, the dimension of the layer's output, i.e., the\n-        number of random features used to approximate the kernel.\n+      output_dim: Positive integer, the dimension of the layer's output, i.e.,\n+        the number of random features used to approximate the kernel.\n       kernel_initializer: Determines the distribution of the parameters of the\n-        random features map (and therefore the kernel approximated by the layer).\n-        It can be either a string identifier or a Keras `Initializer` instance.\n-        Currently only 'gaussian' and 'laplacian' are supported string\n-        identifiers (case insensitive). Note that the kernel matrix is not\n-        trainable.\n+        random features map (and therefore the kernel approximated by the\n+        layer).  It can be either a string identifier or a Keras `Initializer`\n+        instance.  Currently only 'gaussian' and 'laplacian' are supported\n+        string identifiers (case insensitive). Note that the kernel matrix is\n+        not trainable.\n       scale: For Gaussian and Laplacian kernels, this corresponds to a scaling\n-        factor of the corresponding kernel approximated by the layer (see concrete\n-        definitions above). When provided, it should be a positive float. If None,\n-        a default value is used: if the kernel initializer is set to \"gaussian\",\n-        `scale` defaults to `sqrt(input_dim / 2)`, otherwise, it defaults to 1.0.\n-        Both the approximation error of the kernel and the classification quality\n-        are sensitive to this parameter. If `trainable` is set to `True`, this\n-        parameter is learned end-to-end during training and the provided value\n-        serves as the initial value.\n+        factor of the corresponding kernel approximated by the layer (see\n+        concrete definitions above). When provided, it should be a positive\n+        float. If None, a default value is used: if the kernel initializer is\n+        set to \"gaussian\", `scale` defaults to `sqrt(input_dim / 2)`, otherwise,\n+        it defaults to 1.0.  Both the approximation error of the kernel and the\n+        classification quality are sensitive to this parameter. If `trainable`\n+        is set to `True`, this parameter is learned end-to-end during training\n+        and the provided value serves as the initial value.\n         **Note:** When features from this layer are fed to a linear model,\n           by making `scale` trainable, the resulting optimization problem is\n           no longer convex (even if the loss function used by the linear model\n@@ -148,7 +149,8 @@ class RandomFourierFeatures(base_layer.Layer):\n     ):\n         if output_dim <= 0:\n             raise ValueError(\n-                f\"`output_dim` should be a positive integer. Received: {output_dim}\"\n+                \"`output_dim` should be a positive integer. \"\n+                f\"Received: {output_dim}\"\n             )\n         if isinstance(kernel_initializer, str):\n             if kernel_initializer.lower() not in _SUPPORTED_RBF_KERNEL_TYPES:\n@@ -168,8 +170,8 @@ class RandomFourierFeatures(base_layer.Layer):\n \n     def build(self, input_shape):\n         input_shape = tf.TensorShape(input_shape)\n-        # TODO(pmol): Allow higher dimension inputs. Currently the input is expected\n-        # to have shape [batch_size, dimension].\n+        # TODO(pmol): Allow higher dimension inputs. Currently the input is\n+        # expected to have shape [batch_size, dimension].\n         if input_shape.rank != 2:\n             raise ValueError(\n                 \"The rank of the input tensor should be 2. \"\n\n@@ -356,8 +356,9 @@ class RandomFourierFeaturesTest(tf.test.TestCase, parameterized.TestCase):\n         output_y2 = math.sqrt(2.0 / 2000.0) * rff_layer2(y)\n \n         # Compute the inner products of the outputs (on inputs x and y) for both\n-        # layers. For any fixed random features layer rff_layer, and inputs x, y,\n-        # rff_layer(x)^T * rff_layer(y) ~= K(x,y) up to a normalization factor.\n+        # layers. For any fixed random features layer rff_layer, and inputs x,\n+        # y, rff_layer(x)^T * rff_layer(y) ~= K(x,y) up to a normalization\n+        # factor.\n         approx_kernel1 = kernelized_utils.inner_product(output_x1, output_y1)\n         approx_kernel2 = kernelized_utils.inner_product(output_x2, output_y2)\n         self._assert_all_close(approx_kernel1, approx_kernel2, atol=0.08)\n@@ -389,8 +390,8 @@ class RandomFourierFeaturesTest(tf.test.TestCase, parameterized.TestCase):\n         output_y = math.sqrt(2.0 / small_output_dim) * rff_layer(y)\n \n         # The inner products of the outputs (on inputs x and y) approximates the\n-        # real value of the RBF kernel but poorly since the output dimension of the\n-        # layer is small.\n+        # real value of the RBF kernel but poorly since the output dimension of\n+        # the layer is small.\n         exact_kernel_value = exact_kernel_fn(x, y)\n         approx_kernel_value = kernelized_utils.inner_product(output_x, output_y)\n         abs_error = tf.abs(exact_kernel_value - approx_kernel_value)\n\n@@ -54,10 +54,10 @@ class LocallyConnected1D(Layer):\n     ```\n \n     Args:\n-        filters: Integer, the dimensionality of the output space (i.e. the number\n-          of output filters in the convolution).\n-        kernel_size: An integer or tuple/list of a single integer, specifying the\n-          length of the 1D convolution window.\n+        filters: Integer, the dimensionality of the output space (i.e. the\n+          number of output filters in the convolution).\n+        kernel_size: An integer or tuple/list of a single integer, specifying\n+          the length of the 1D convolution window.\n         strides: An integer or tuple/list of a single integer, specifying the\n           stride length of the convolution.\n         padding: Currently only supports `\"valid\"` (case-insensitive). `\"same\"`\n@@ -69,9 +69,8 @@ class LocallyConnected1D(Layer):\n           `(batch, channels, length)`. It defaults to the `image_data_format`\n           value found in your Keras config file at `~/.keras/keras.json`. If you\n           never set it, then it will be \"channels_last\".\n-        activation: Activation function to use. If you don't specify anything, no\n-          activation is applied\n-            (ie. \"linear\" activation: `a(x) = x`).\n+        activation: Activation function to use. If you don't specify anything,\n+          no activation is applied (ie. \"linear\" activation: `a(x) = x`).\n         use_bias: Boolean, whether the layer uses a bias vector.\n         kernel_initializer: Initializer for the `kernel` weights matrix.\n         bias_initializer: Initializer for the bias vector.\n@@ -95,15 +94,16 @@ class LocallyConnected1D(Layer):\n             `3`: large, sparse models,  where \"large\" stands for large\n               input/output activations (i.e. many `filters`, `input_filters`,\n               large `input_size`, `output_size`), and \"sparse\" stands for few\n-              connections between inputs and outputs, i.e. small ratio `filters *\n-              input_filters * kernel_size / (input_size * strides)`, where inputs\n-              to and outputs of the layer are assumed to have shapes `(input_size,\n-              input_filters)`, `(output_size, filters)` respectively.  It is\n-              recommended to benchmark each in the setting of interest to pick the\n-              most efficient one (in terms of speed and memory usage). Correct\n-              choice of implementation can lead to dramatic speed improvements\n-              (e.g. 50X), potentially at the expense of RAM.  Also, only\n-              `padding=\"valid\"` is supported by `implementation=1`.\n+              connections between inputs and outputs, i.e. small ratio\n+              `filters * input_filters * kernel_size / (input_size * strides)`,\n+              where inputs to and outputs of the layer are assumed to have\n+              shapes `(input_size, input_filters)`, `(output_size, filters)`\n+              respectively.  It is recommended to benchmark each in the setting\n+              of interest to pick the most efficient one (in terms of speed and\n+              memory usage). Correct choice of implementation can lead to\n+              dramatic speed improvements (e.g. 50X), potentially at the expense\n+              of RAM.  Also, only `padding=\"valid\"` is supported by\n+              `implementation=1`.\n     Input shape:\n         3D tensor with shape: `(batch_size, steps, input_dim)`\n     Output shape:\n\n@@ -58,27 +58,27 @@ class LocallyConnected2D(Layer):\n     ```\n \n     Args:\n-        filters: Integer, the dimensionality of the output space (i.e. the number\n-          of output filters in the convolution).\n-        kernel_size: An integer or tuple/list of 2 integers, specifying the width\n-          and height of the 2D convolution window. Can be a single integer to\n-          specify the same value for all spatial dimensions.\n-        strides: An integer or tuple/list of 2 integers, specifying the strides of\n-          the convolution along the width and height. Can be a single integer to\n-          specify the same value for all spatial dimensions.\n+        filters: Integer, the dimensionality of the output space (i.e. the\n+          number of output filters in the convolution).\n+        kernel_size: An integer or tuple/list of 2 integers, specifying the\n+          width and height of the 2D convolution window. Can be a single integer\n+          to specify the same value for all spatial dimensions.\n+        strides: An integer or tuple/list of 2 integers, specifying the strides\n+          of the convolution along the width and height. Can be a single integer\n+          to specify the same value for all spatial dimensions.\n         padding: Currently only support `\"valid\"` (case-insensitive). `\"same\"`\n           will be supported in future. `\"valid\"` means no padding.\n         data_format: A string, one of `channels_last` (default) or\n           `channels_first`. The ordering of the dimensions in the inputs.\n-          `channels_last` corresponds to inputs with shape `(batch, height, width,\n-          channels)` while `channels_first` corresponds to inputs with shape\n+          `channels_last` corresponds to inputs with shape `(batch, height,\n+            width, channels)` while `channels_first` corresponds to inputs with\n+            shape\n           `(batch, channels, height, width)`. It defaults to the\n           `image_data_format` value found in your Keras config file at\n           `~/.keras/keras.json`. If you never set it, then it will be\n           \"channels_last\".\n-        activation: Activation function to use. If you don't specify anything, no\n-          activation is applied\n-            (ie. \"linear\" activation: `a(x) = x`).\n+        activation: Activation function to use. If you don't specify anything,\n+          no activation is applied (ie. \"linear\" activation: `a(x) = x`).\n         use_bias: Boolean, whether the layer uses a bias vector.\n         kernel_initializer: Initializer for the `kernel` weights matrix.\n         bias_initializer: Initializer for the bias vector.\n@@ -107,11 +107,11 @@ class LocallyConnected2D(Layer):\n               (np.prod(input_size) * np.prod(strides))`, where inputs to and\n               outputs of the layer are assumed to have shapes `input_size +\n               (input_filters,)`, `output_size + (filters,)` respectively. It is\n-              recommended to benchmark each in the setting of interest to pick the\n-              most efficient one (in terms of speed and memory usage). Correct\n-              choice of implementation can lead to dramatic speed improvements\n-              (e.g. 50X), potentially at the expense of RAM.  Also, only\n-              `padding=\"valid\"` is supported by `implementation=1`.\n+              recommended to benchmark each in the setting of interest to pick\n+              the most efficient one (in terms of speed and memory usage).\n+              Correct choice of implementation can lead to dramatic speed\n+              improvements (e.g. 50X), potentially at the expense of RAM. Also,\n+              only `padding=\"valid\"` is supported by `implementation=1`.\n     Input shape:\n         4D tensor with shape: `(samples, channels, rows, cols)` if\n           data_format='channels_first'\n@@ -121,8 +121,8 @@ class LocallyConnected2D(Layer):\n         4D tensor with shape: `(samples, filters, new_rows, new_cols)` if\n           data_format='channels_first'\n         or 4D tensor with shape: `(samples, new_rows, new_cols, filters)` if\n-          data_format='channels_last'. `rows` and `cols` values might have changed\n-          due to padding.\n+          data_format='channels_last'. `rows` and `cols` values might have\n+          changed due to padding.\n     \"\"\"\n \n     def __init__(\n\n@@ -25,10 +25,11 @@ def get_locallyconnected_mask(\n ):\n     \"\"\"Return a mask representing connectivity of a locally-connected operation.\n \n-    This method returns a masking numpy array of 0s and 1s (of type `np.float32`)\n-    that, when element-wise multiplied with a fully-connected weight tensor, masks\n-    out the weights between disconnected input-output pairs and thus implements\n-    local connectivity through a sparse fully-connected weight tensor.\n+    This method returns a masking numpy array of 0s and 1s (of type\n+    `np.float32`) that, when element-wise multiplied with a fully-connected\n+    weight tensor, masks out the weights between disconnected input-output pairs\n+    and thus implements local connectivity through a sparse fully-connected\n+    weight tensor.\n \n     Assume an unshared convolution with given parameters is applied to an input\n     having N spatial dimensions with `input_shape = (d_in1, ..., d_inN)`\n@@ -36,10 +37,10 @@ def get_locallyconnected_mask(\n     by layer parameters such as `strides`).\n \n     This method returns a mask which can be broadcast-multiplied (element-wise)\n-    with a 2*(N+1)-D weight matrix (equivalent to a fully-connected layer between\n-    (N+1)-D activations (N spatial + 1 channel dimensions for input and output)\n-    to make it perform an unshared convolution with given `kernel_shape`,\n-    `strides`, `padding` and `data_format`.\n+    with a 2*(N+1)-D weight matrix (equivalent to a fully-connected layer\n+    between (N+1)-D activations (N spatial + 1 channel dimensions for input and\n+    output) to make it perform an unshared convolution with given\n+    `kernel_shape`, `strides`, `padding` and `data_format`.\n \n     Args:\n       input_shape: tuple of size N: `(d_in1, ..., d_inN)` spatial shape of the\n@@ -98,19 +99,18 @@ def local_conv_matmul(inputs, kernel, kernel_mask, output_shape):\n         inputs: (N+2)-D tensor with shape `(batch_size, channels_in, d_in1, ...,\n           d_inN)` or `(batch_size, d_in1, ..., d_inN, channels_in)`.\n         kernel: the unshared weights for N-D convolution,\n-            an (N+2)-D tensor of shape: `(d_in1, ..., d_inN, channels_in, d_out2,\n-              ..., d_outN, channels_out)` or `(channels_in, d_in1, ..., d_inN,\n-              channels_out, d_out2, ..., d_outN)`, with the ordering of channels\n-              and spatial dimensions matching that of the input. Each entry is the\n-              weight between a particular input and output location, similarly to\n-              a fully-connected weight matrix.\n+            an (N+2)-D tensor of shape: `(d_in1, ..., d_inN, channels_in,\n+            d_out2, ..., d_outN, channels_out)` or `(channels_in, d_in1, ...,\n+            d_inN, channels_out, d_out2, ..., d_outN)`, with the ordering of\n+            channels and spatial dimensions matching that of the input. Each\n+            entry is the weight between a particular input and output location,\n+            similarly to a fully-connected weight matrix.\n         kernel_mask: a float 0/1 mask tensor of shape: `(d_in1, ..., d_inN, 1,\n           d_out2, ..., d_outN, 1)` or `(1, d_in1, ..., d_inN, 1, d_out2, ...,\n-          d_outN)`, with the ordering of singleton and spatial dimensions matching\n-          that of the input. Mask represents the connectivity pattern of the layer\n-          and is\n-             precomputed elsewhere based on layer parameters: stride, padding, and\n-               the receptive field shape.\n+          d_outN)`, with the ordering of singleton and spatial dimensions\n+          matching that of the input. Mask represents the connectivity pattern\n+          of the layer and is precomputed elsewhere based on layer parameters:\n+          stride, padding, and the receptive field shape.\n         output_shape: a tuple of (N+2) elements representing the output shape:\n           `(batch_size, channels_out, d_out1, ..., d_outN)` or `(batch_size,\n           d_out1, ..., d_outN, channels_out)`, with the ordering of channels and\n\n@@ -141,7 +141,8 @@ class _Merge(Layer):\n                 return self._merge_function(reshaped_inputs)\n             else:\n                 # Transpose all inputs so that batch size is the last dimension.\n-                # (batch_size, dim1, dim2, ... ) -> (dim1, dim2, ... , batch_size)\n+                # (batch_size, dim1, dim2, ... ) -> (dim1, dim2, ... ,\n+                # batch_size)\n                 transposed = False\n                 for x in inputs:\n                     x_ndim = backend.ndim(x)\n@@ -167,12 +168,14 @@ class _Merge(Layer):\n                         reshaped_inputs.append(tf.transpose(x, perm=dims))\n                         transposed = True\n                     else:\n-                        # We don't transpose inputs if they are 1D vectors or scalars.\n+                        # We don't transpose inputs if they are 1D vectors or\n+                        # scalars.\n                         reshaped_inputs.append(x)\n                 y = self._merge_function(reshaped_inputs)\n                 y_ndim = backend.ndim(y)\n                 if transposed:\n-                    # If inputs have been transposed, we have to transpose the output too.\n+                    # If inputs have been transposed, we have to transpose the\n+                    # output too.\n                     if y_ndim is None:\n                         y_shape = tf.shape(y)\n                         y_ndim = tf.shape(y_shape)[0]\n\n@@ -118,8 +118,8 @@ class Concatenate(_Merge):\n             # Get the only rank for the set.\n             (rank,) = ranks\n             for axis in range(rank):\n-                # Skip the Nones in the shape since they are dynamic, also the axis for\n-                # concat has been removed above.\n+                # Skip the Nones in the shape since they are dynamic, also the\n+                # axis for concat has been removed above.\n                 unique_dims = set(\n                     shape[axis]\n                     for shape in shape_set\n@@ -137,8 +137,9 @@ class Concatenate(_Merge):\n             not isinstance(input_shape[0], (tuple, list))\n         ):\n             # The tf_utils.shape_type_conversion decorator turns tensorshapes\n-            # into tuples, so we need to verify that `input_shape` is a list/tuple,\n-            # *and* that the individual elements are themselves shape tuples.\n+            # into tuples, so we need to verify that `input_shape` is a\n+            # list/tuple, *and* that the individual elements are themselves\n+            # shape tuples.\n             raise ValueError(\n                 \"A `Concatenate` layer should be called on a list of inputs. \"\n                 f\"Received: input_shape={input_shape}\"\n\n@@ -80,9 +80,9 @@ class Dot(_Merge):\n         Args:\n           axes: Integer or tuple of integers,\n             axis or axes along which to take the dot product. If a tuple, should\n-            be two integers corresponding to the desired axis from the first input\n-            and the desired axis from the second input, respectively. Note that the\n-            size of the two selected axes must match.\n+            be two integers corresponding to the desired axis from the first\n+            input and the desired axis from the second input, respectively. Note\n+            that the size of the two selected axes must match.\n           normalize: Whether to L2-normalize samples along the\n             dot product axis before taking the dot product.\n             If set to True, then the output of the dot product\n@@ -103,8 +103,8 @@ class Dot(_Merge):\n                 )\n             if not isinstance(axes[0], int) or not isinstance(axes[1], int):\n                 raise ValueError(\n-                    \"Invalid format for argument `axes`: list elements should be \"\n-                    f\"integers. Received: axes={axes}\"\n+                    \"Invalid format for argument `axes`: list elements should \"\n+                    f\"be integers. Received: axes={axes}\"\n                 )\n         self.axes = axes\n         self.normalize = normalize\n\n@@ -64,9 +64,11 @@ def multiply(inputs, **kwargs):\n     Usage in a functional model:\n \n     >>> input1 = tf.keras.layers.Input(shape=(16,))\n-    >>> x1 = tf.keras.layers.Dense(8, activation='relu')(input1) #shape=(None, 8)\n+    >>> x1 = tf.keras.layers.Dense(\n+    ...     8, activation='relu')(input1) #shape=(None, 8)\n     >>> input2 = tf.keras.layers.Input(shape=(32,))\n-    >>> x2 = tf.keras.layers.Dense(8, activation='relu')(input2) #shape=(None, 8)\n+    >>> x2 = tf.keras.layers.Dense(\n+    ...     8, activation='relu')(input2) #shape=(None, 8)\n     >>> out = tf.keras.layers.multiply([x1,x2]) #shape=(None, 8)\n     >>> out = tf.keras.layers.Dense(4)(out)\n     >>> model = tf.keras.models.Model(inputs=[input1, input2], outputs=out)\n\n@@ -25,9 +25,8 @@ from tensorflow.python.util.tf_export import keras_export\n class Subtract(_Merge):\n     \"\"\"Layer that subtracts two inputs.\n \n-    It takes as input a list of tensors of size 2,\n-    both of the same shape, and returns a single tensor, (inputs[0] - inputs[1]),\n-    also of the same shape.\n+    It takes as input a list of tensors of size 2, both of the same shape, and\n+    returns a single tensor, (inputs[0] - inputs[1]), also of the same shape.\n \n     Examples:\n \n\n@@ -76,11 +76,11 @@ class BatchNormalizationBase(Layer):\n         `data_format=\"channels_first\"`, set `axis=1` in `BatchNormalization`.\n       momentum: Momentum for the moving average.\n       epsilon: Small float added to variance to avoid dividing by zero.\n-      center: If True, add offset of `beta` to normalized tensor. If False, `beta`\n-        is ignored.\n-      scale: If True, multiply by `gamma`. If False, `gamma` is not used. When the\n-        next layer is linear (also e.g. `nn.relu`), this can be disabled since the\n-        scaling will be done by the next layer.\n+      center: If True, add offset of `beta` to normalized tensor. If False,\n+        `beta` is ignored.\n+      scale: If True, multiply by `gamma`. If False, `gamma` is not used. When\n+        the next layer is linear (also e.g. `nn.relu`), this can be disabled\n+        since the scaling will be done by the next layer.\n       beta_initializer: Initializer for the beta weight.\n       gamma_initializer: Initializer for the gamma weight.\n       moving_mean_initializer: Initializer for the moving mean.\n@@ -91,7 +91,8 @@ class BatchNormalizationBase(Layer):\n       gamma_constraint: Optional constraint for the gamma weight.\n       renorm: Whether to use [Batch Renormalization](\n         https://arxiv.org/abs/1702.03275). This adds extra variables during\n-          training. The inference is the same for either value of this parameter.\n+          training. The inference is the same for either value of this\n+          parameter.\n       renorm_clipping: A dictionary that may map keys 'rmax', 'rmin', 'dmax' to\n         scalar `Tensors` used to clip the renorm correction. The correction `(r,\n         d)` is used as `corrected_value = normalized_value * r + d`, with `r`\n@@ -100,23 +101,23 @@ class BatchNormalizationBase(Layer):\n       renorm_momentum: Momentum used to update the moving means and standard\n         deviations with renorm. Unlike `momentum`, this affects training and\n         should be neither too small (which would add noise) nor too large (which\n-        would give stale estimates). Note that `momentum` is still applied to get\n-        the means and variances for inference.\n-      fused: if `True`, use a faster, fused implementation, or raise a ValueError\n-        if the fused implementation cannot be used. If `None`, use the faster\n-        implementation if possible. If False, do not used the fused\n-        implementation.\n-        Note that in TensorFlow 1.x, the meaning of `fused=True` is different: if\n-          `False`, the layer uses the system-recommended implementation.\n+        would give stale estimates). Note that `momentum` is still applied to\n+        get the means and variances for inference.\n+      fused: if `True`, use a faster, fused implementation, or raise a\n+        ValueError if the fused implementation cannot be used. If `None`, use\n+        the faster implementation if possible. If False, do not used the fused\n+        implementation. Note that in TensorFlow 1.x, the meaning of\n+        `fused=True` is different: if `False`, the layer uses the\n+        system-recommended implementation.\n       trainable: Boolean, if `True` the variables will be marked as trainable.\n       virtual_batch_size: An `int`. By default, `virtual_batch_size` is `None`,\n-        which means batch normalization is performed across the whole batch. When\n-        `virtual_batch_size` is not `None`, instead perform \"Ghost Batch\n+        which means batch normalization is performed across the whole batch.\n+        When `virtual_batch_size` is not `None`, instead perform \"Ghost Batch\n         Normalization\", which creates virtual sub-batches which are each\n         normalized separately (with shared gamma, beta, and moving statistics).\n         Must divide the actual batch size during execution.\n-      adjustment: A function taking the `Tensor` containing the (dynamic) shape of\n-        the input tensor and returning a pair (scale, bias) to apply to the\n+      adjustment: A function taking the `Tensor` containing the (dynamic) shape\n+        of the input tensor and returning a pair (scale, bias) to apply to the\n         normalized values (before gamma and beta), only during training. For\n         example, if `axis=-1`,\n           `adjustment = lambda shape: (\n@@ -132,10 +133,10 @@ class BatchNormalizationBase(Layer):\n       inputs: Input tensor (of any rank).\n       training: Python boolean indicating whether the layer should behave in\n         training mode or in inference mode.\n-        - `training=True`: The layer will normalize its inputs using the mean and\n-          variance of the current batch of inputs.\n-        - `training=False`: The layer will normalize its inputs using the mean and\n-          variance of its moving statistics, learned during training.\n+        - `training=True`: The layer will normalize its inputs using the mean\n+          and variance of the current batch of inputs.\n+        - `training=False`: The layer will normalize its inputs using the mean\n+          and variance of its moving statistics, learned during training.\n \n     Input shape: Arbitrary. Use the keyword argument `input_shape` (tuple of\n       integers, does not include the samples axis) when using this layer as the\n@@ -206,8 +207,9 @@ class BatchNormalizationBase(Layer):\n         if self._USE_V2_BEHAVIOR:\n             if fused:\n                 self._raise_if_fused_cannot_be_used()\n-            # We leave fused as None if self._fused_can_be_used()==True, since we\n-            # still may set it to False in self.build() if the input rank is not 4.\n+            # We leave fused as None if self._fused_can_be_used()==True, since\n+            # we still may set it to False in self.build() if the input rank is\n+            # not 4.\n             elif fused is None and not self._fused_can_be_used():\n                 fused = False\n         elif fused is None:\n@@ -232,17 +234,17 @@ class BatchNormalizationBase(Layer):\n     def _raise_if_fused_cannot_be_used(self):\n         \"\"\"Raises a ValueError if fused implementation cannot be used.\n \n-        In addition to the checks done in this function, the input tensors rank must\n-        be 4 or 5. The input rank check can only be done once the input shape is\n-        known.\n+        In addition to the checks done in this function, the input tensors rank\n+        must be 4 or 5. The input rank check can only be done once the input\n+        shape is known.\n         \"\"\"\n         # Note the ValueErrors in this function are caught and not reraised in\n         # _fused_can_be_used(). No other exception besides ValueError should be\n         # raised here.\n \n-        # Currently fused batch norm doesn't support renorm. It also only supports a\n-        # channel dimension on axis 1 or 3 (rank=4) / 1 or 4 (rank5), when no\n-        # virtual batch size or adjustment is used.\n+        # Currently fused batch norm doesn't support renorm. It also only\n+        # supports a channel dimension on axis 1 or 3 (rank=4) / 1 or 4 (rank5),\n+        # when no virtual batch size or adjustment is used.\n         if self.renorm:\n             raise ValueError(\n                 \"Passing both `fused=True` and `renorm=True` is \"\n@@ -250,8 +252,8 @@ class BatchNormalizationBase(Layer):\n             )\n         axis = [self.axis] if isinstance(self.axis, int) else self.axis\n         # Axis -3 is equivalent to 1, and axis -1 is equivalent to 3, when the\n-        # input rank is 4. Similarly, the valid axis is -4, -1, 1, 4 when the rank\n-        # is 5. The combination of ranks and axes will be checked later.\n+        # input rank is 4. Similarly, the valid axis is -4, -1, 1, 4 when the\n+        # rank is 5. The combination of ranks and axes will be checked later.\n         if len(axis) > 1 or axis[0] not in (-4, -3, -1, 1, 3, 4):\n             raise ValueError(\n                 \"Passing `fused=True` is only supported when axis is 1 \"\n@@ -303,8 +305,8 @@ class BatchNormalizationBase(Layer):\n         if not tf.distribute.has_strategy():\n             return False\n         strategy = tf.distribute.get_strategy()\n-        # TODO(b/195085185): remove experimental_enable_get_next_as_optional after\n-        # migrating all users.\n+        # TODO(b/195085185): remove experimental_enable_get_next_as_optional\n+        # after migrating all users.\n         return getattr(\n             strategy.extended,\n             \"enable_partial_batch_handling\",\n@@ -323,9 +325,9 @@ class BatchNormalizationBase(Layer):\n         if self.virtual_batch_size is not None:\n             if self.virtual_batch_size <= 0:\n                 raise ValueError(\n-                    f\"`virtual_batch_size` must be a positive integer that divides the \"\n-                    f\"true batch size of the input tensor. Received: \"\n-                    f\"virtual_batch_size={self.virtual_batch_size}\"\n+                    f\"`virtual_batch_size` must be a positive integer that \"\n+                    f\"divides the true batch size of the input tensor. \"\n+                    f\"Received: virtual_batch_size={self.virtual_batch_size}\"\n                 )\n             # If using virtual batches, the first dimension must be the batch\n             # dimension and cannot be the batch norm axis\n@@ -342,8 +344,8 @@ class BatchNormalizationBase(Layer):\n                 )\n \n         if self.fused in (None, True):\n-            # TODO(yaozhang): if input is not 4D, reshape it to 4D and reshape the\n-            # output back to its original shape accordingly.\n+            # TODO(yaozhang): if input is not 4D, reshape it to 4D and reshape\n+            # the output back to its original shape accordingly.\n             if self._USE_V2_BEHAVIOR:\n                 if self.fused is None:\n                     self.fused = rank in (4, 5)\n@@ -357,11 +359,12 @@ class BatchNormalizationBase(Layer):\n                 assert self.fused is not None\n                 self.fused = rank in (4, 5) and self._fused_can_be_used()\n             # TODO(chrisying): fused batch norm is currently not supported for\n-            # multi-axis batch norm and by extension virtual batches. In some cases,\n-            # it might be possible to use fused batch norm but would require reshaping\n-            # the Tensor to 4D with the axis in 1 or 3 (preferred 1) which is\n-            # particularly tricky. A compromise might be to just support the most\n-            # common use case (turning 5D w/ virtual batch to NCHW)\n+            # multi-axis batch norm and by extension virtual batches. In some\n+            # cases, it might be possible to use fused batch norm but would\n+            # require reshaping the Tensor to 4D with the axis in 1 or 3\n+            # (preferred 1) which is particularly tricky. A compromise might be\n+            # to just support the most common use case (turning 5D w/ virtual\n+            # batch to NCHW)\n \n         if self.fused:\n             if self.axis == [1] and rank == 4:\n@@ -373,21 +376,21 @@ class BatchNormalizationBase(Layer):\n             elif self.axis == [4] and rank == 5:\n                 self._data_format = \"NDHWC\"\n             elif rank == 5:\n-                # 5D tensors that can be passed in but should not use fused batch norm\n-                # due to unsupported axis.\n+                # 5D tensors that can be passed in but should not use fused\n+                # batch norm due to unsupported axis.\n                 self.fused = False\n             else:\n                 if rank == 4:\n                     raise ValueError(\n-                        \"Unsupported axis. The use of `fused=True` is only possible with \"\n-                        \"`axis=1` or `axis=3` for 4D input tensors. Received: \"\n-                        f\"axis={tuple(self.axis)}\"\n+                        \"Unsupported axis. The use of `fused=True` is only \"\n+                        \"possible with `axis=1` or `axis=3` for 4D input \"\n+                        f\"tensors. Received: axis={tuple(self.axis)}\"\n                     )\n                 else:\n                     raise ValueError(\n-                        \"Unsupported axis. The use of `fused=True` is only possible with \"\n-                        \"`axis=1` or `axis=4` for 5D input tensors. Received: \"\n-                        f\"axis={tuple(self.axis)}\"\n+                        \"Unsupported axis. The use of `fused=True` is only \"\n+                        \"possible with `axis=1` or `axis=4` for 5D input \"\n+                        f\"tensors. Received: axis={tuple(self.axis)}\"\n                     )\n \n         axis_to_dim = {x: input_shape.dims[x].value for x in self.axis}\n@@ -404,7 +407,8 @@ class BatchNormalizationBase(Layer):\n             # Single axis batch norm (most common/default use-case)\n             param_shape = (list(axis_to_dim.values())[0],)\n         else:\n-            # Parameter shape is the original shape but with 1 in all non-axis dims\n+            # Parameter shape is the original shape but with 1 in all non-axis\n+            # dims\n             param_shape = [\n                 axis_to_dim[i] if i in axis_to_dim else 1 for i in range(rank)\n             ]\n@@ -451,7 +455,8 @@ class BatchNormalizationBase(Layer):\n                 )\n \n         try:\n-            # Disable variable partitioning when creating the moving mean and variance\n+            # Disable variable partitioning when creating the moving mean and\n+            # variance\n             if hasattr(self, \"_scope\") and self._scope:\n                 partitioner = self._scope.partitioner\n                 self._scope.set_partitioner(None)\n@@ -480,8 +485,9 @@ class BatchNormalizationBase(Layer):\n             )\n \n             if self.renorm:\n-                # In batch renormalization we track the inference moving stddev instead\n-                # of the moving variance to more closely align with the paper.\n+                # In batch renormalization we track the inference moving stddev\n+                # instead of the moving variance to more closely align with the\n+                # paper.\n                 def moving_stddev_initializer(*args, **kwargs):\n                     return tf.sqrt(\n                         self.moving_variance_initializer(*args, **kwargs)\n@@ -501,13 +507,14 @@ class BatchNormalizationBase(Layer):\n                         experimental_autocast=False,\n                     )\n \n-                # Create variables to maintain the moving mean and standard deviation.\n-                # These are used in training and thus are different from the moving\n-                # averages above. The renorm variables are colocated with moving_mean\n-                # and moving_stddev.\n-                # NOTE: below, the outer `with device` block causes the current device\n-                # stack to be cleared. The nested ones use a `lambda` to set the desired\n-                # device and ignore any devices that may be set by the custom getter.\n+                # Create variables to maintain the moving mean and standard\n+                # deviation.  These are used in training and thus are different\n+                # from the moving averages above. The renorm variables are\n+                # colocated with moving_mean and moving_stddev.\n+                # NOTE: below, the outer `with device` block causes the current\n+                # device stack to be cleared. The nested ones use a `lambda` to\n+                # set the desired device and ignore any devices that may be set\n+                # by the custom getter.\n                 def _renorm_variable(name, shape, initializer=\"zeros\"):\n                     \"\"\"Create a renorm variable.\"\"\"\n                     var = self.add_weight(\n@@ -579,18 +586,18 @@ class BatchNormalizationBase(Layer):\n         beta = self.beta if self.center else self._beta_const\n         gamma = self.gamma if self.scale else self._gamma_const\n \n-        # TODO(b/129279393): Support zero batch input in non DistributionStrategy\n-        # code as well.\n+        # TODO(b/129279393): Support zero batch input in non\n+        # DistributionStrategy code as well.\n         if self._support_zero_size_input():\n-            # Keras assumes that batch dimension is the first dimension for Batch\n-            # Normalization.\n+            # Keras assumes that batch dimension is the first dimension for\n+            # Batch Normalization.\n             input_batch_size = tf.shape(inputs)[0]\n         else:\n             input_batch_size = None\n \n-        # TODO(rmlarsen): Support using fused avg updates for non-eager execution\n-        # after fixing graph pattern matching and enabling fused_batch_norm to\n-        # take exponential_avg_factor as a tensor input.\n+        # TODO(rmlarsen): Support using fused avg updates for non-eager\n+        # execution after fixing graph pattern matching and enabling\n+        # fused_batch_norm to take exponential_avg_factor as a tensor input.\n         use_fused_avg_updates = (\n             tf.compat.v1.executing_eagerly_outside_functions()\n             and isinstance(self.momentum, (float, int))\n@@ -684,7 +691,8 @@ class BatchNormalizationBase(Layer):\n                     )\n \n             def variance_update():\n-                \"\"\"Update self.moving_variance with the most recent data point.\"\"\"\n+                \"\"\"Update self.moving_variance with the most recent data\n+                point.\"\"\"\n                 if use_fused_avg_updates:\n                     if input_batch_size is not None:\n                         new_variance = control_flow_util.smart_cond(\n@@ -746,7 +754,8 @@ class BatchNormalizationBase(Layer):\n         )\n \n         def _update_renorm_variable(var, value, inputs_size):\n-            \"\"\"Updates a moving average and weight, returns the unbiased value.\"\"\"\n+            \"\"\"Updates a moving average and weight, returns the unbiased\n+            value.\"\"\"\n             value = tf.identity(value)\n \n             def _do_update():\n@@ -785,8 +794,8 @@ class BatchNormalizationBase(Layer):\n         mean, variance = self._calculate_mean_and_var(\n             inputs, reduction_axes, keep_dims\n         )\n-        # TODO(b/129279393): Support zero batch input in non DistributionStrategy\n-        # code as well.\n+        # TODO(b/129279393): Support zero batch input in non\n+        # DistributionStrategy code as well.\n         if self._support_zero_size_input():\n             input_batch_size = tf.shape(inputs)[0]\n             mean = tf.where(\n@@ -804,8 +813,8 @@ class BatchNormalizationBase(Layer):\n             if isinstance(training, int):\n                 training = bool(training)\n             if not self.trainable:\n-                # When the layer is not trainable, it overrides the value passed from\n-                # model.\n+                # When the layer is not trainable, it overrides the value passed\n+                # from model.\n                 training = False\n         return training\n \n@@ -814,8 +823,8 @@ class BatchNormalizationBase(Layer):\n         training = self._get_training_value(training)\n \n         if self.virtual_batch_size is not None:\n-            # Virtual batches (aka ghost batches) can be simulated by reshaping the\n-            # Tensor and reusing the existing batch norm implementation\n+            # Virtual batches (aka ghost batches) can be simulated by reshaping\n+            # the Tensor and reusing the existing batch norm implementation\n             original_shape = tf.shape(inputs)\n             original_shape = tf.concat(\n                 [tf.constant([-1]), original_shape[1:]], axis=0\n@@ -828,7 +837,8 @@ class BatchNormalizationBase(Layer):\n                 axis=0,\n             )\n \n-            # Will cause errors if virtual_batch_size does not divide the batch size\n+            # Will cause errors if virtual_batch_size does not divide the batch\n+            # size\n             inputs = tf.reshape(inputs, expanded_shape)\n \n             def undo_virtual_batching(outputs):\n@@ -838,16 +848,17 @@ class BatchNormalizationBase(Layer):\n         if self.fused:\n             outputs = self._fused_batch_norm(inputs, training=training)\n             if self.virtual_batch_size is not None:\n-                # Currently never reaches here since fused_batch_norm does not support\n-                # virtual batching\n+                # Currently never reaches here since fused_batch_norm does not\n+                # support virtual batching\n                 outputs = undo_virtual_batching(outputs)\n             return outputs\n \n         inputs_dtype = inputs.dtype.base_dtype\n         if inputs_dtype in (tf.float16, tf.bfloat16):\n-            # Do all math in float32 if given 16-bit inputs for numeric stability.\n-            # In particular, it's very easy for variance to overflow in float16 and\n-            # for safety we also choose to cast bfloat16 to float32.\n+            # Do all math in float32 if given 16-bit inputs for numeric\n+            # stability.  In particular, it's very easy for variance to overflow\n+            # in float16 and for safety we also choose to cast bfloat16 to\n+            # float32.\n             inputs = tf.cast(inputs, tf.float32)\n \n         # Compute the axes along which to reduce the mean / variance\n@@ -857,8 +868,8 @@ class BatchNormalizationBase(Layer):\n         if self.virtual_batch_size is not None:\n             del reduction_axes[1]  # Do not reduce along virtual batch dim\n \n-        # Broadcasting only necessary for single-axis batch norm where the axis is\n-        # not the last dimension\n+        # Broadcasting only necessary for single-axis batch norm where the axis\n+        # is not the last dimension\n         broadcast_shape = [1] * ndims\n         broadcast_shape[self.axis[0]] = input_shape.dims[self.axis[0]].value\n \n@@ -881,7 +892,8 @@ class BatchNormalizationBase(Layer):\n                 offset += then_offset\n             return (scale, offset)\n \n-        # Determine a boolean value for `training`: could be True, False, or None.\n+        # Determine a boolean value for `training`: could be True, False, or\n+        # None.\n         training_value = control_flow_util.constant_value(training)\n         if (\n             training_value == False\n@@ -901,8 +913,9 @@ class BatchNormalizationBase(Layer):\n                     adj_scale, adj_bias, scale, offset\n                 )\n \n-            # Some of the computations here are not necessary when training==False\n-            # but not a constant. However, this makes the code simpler.\n+            # Some of the computations here are not necessary when\n+            # training==False but not a constant. However, this makes the code\n+            # simpler.\n             keep_dims = (\n                 self.virtual_batch_size is not None or len(self.axis) > 1\n             )\n@@ -928,18 +941,19 @@ class BatchNormalizationBase(Layer):\n \n             if self.virtual_batch_size is not None:\n                 # This isn't strictly correct since in ghost batch norm, you are\n-                # supposed to sequentially update the moving_mean and moving_variance\n-                # with each sub-batch. However, since the moving statistics are only\n-                # used during evaluation, it is more efficient to just update in one\n-                # step and should not make a significant difference in the result.\n+                # supposed to sequentially update the moving_mean and\n+                # moving_variance with each sub-batch. However, since the moving\n+                # statistics are only used during evaluation, it is more\n+                # efficient to just update in one step and should not make a\n+                # significant difference in the result.\n                 new_mean = tf.reduce_mean(mean, axis=1, keepdims=True)\n                 new_variance = tf.reduce_mean(variance, axis=1, keepdims=True)\n             else:\n                 new_mean, new_variance = mean, variance\n \n             if self._support_zero_size_input():\n-                # Keras assumes that batch dimension is the first dimension for Batch\n-                # Normalization.\n+                # Keras assumes that batch dimension is the first dimension for\n+                # Batch Normalization.\n                 input_batch_size = tf.shape(inputs)[0]\n             else:\n                 input_batch_size = None\n@@ -953,9 +967,10 @@ class BatchNormalizationBase(Layer):\n                 ) = self._renorm_correction_and_moments(\n                     new_mean, new_variance, training, input_batch_size\n                 )\n-                # When training, the normalized values (say, x) will be transformed as\n-                # x * gamma + beta without renorm, and (x * r + d) * gamma + beta\n-                # = x * (r * gamma) + (d * gamma + beta) with renorm.\n+                # When training, the normalized values (say, x) will be\n+                # transformed as x * gamma + beta without renorm, and (x * r +\n+                # d) * gamma + beta = x * (r * gamma) + (d * gamma + beta) with\n+                # renorm.\n                 r = _broadcast(tf.stop_gradient(r, name=\"renorm_r\"))\n                 d = _broadcast(tf.stop_gradient(d, name=\"renorm_d\"))\n                 scale, offset = _compose_transforms(r, d, scale, offset)\n@@ -977,15 +992,15 @@ class BatchNormalizationBase(Layer):\n                 \"\"\"Update the moving variance.\"\"\"\n \n                 def true_branch_renorm():\n-                    # We apply epsilon as part of the moving_stddev to mirror the training\n-                    # code path.\n+                    # We apply epsilon as part of the moving_stddev to mirror\n+                    # the training code path.\n                     moving_stddev = _do_update(\n                         self.moving_stddev, tf.sqrt(new_variance + self.epsilon)\n                     )\n                     return self._assign_new_value(\n                         self.moving_variance,\n-                        # Apply relu in case floating point rounding causes it to go\n-                        # negative.\n+                        # Apply relu in case floating point rounding causes it\n+                        # to go negative.\n                         backend.relu(\n                             moving_stddev * moving_stddev - self.epsilon\n                         ),\n@@ -1053,8 +1068,8 @@ class BatchNormalizationBase(Layer):\n             \"beta_constraint\": constraints.serialize(self.beta_constraint),\n             \"gamma_constraint\": constraints.serialize(self.gamma_constraint),\n         }\n-        # Only add TensorFlow-specific parameters if they are set, so as to preserve\n-        # model compatibility with external Keras.\n+        # Only add TensorFlow-specific parameters if they are set, so as to\n+        # preserve model compatibility with external Keras.\n         if self.renorm:\n             config[\"renorm\"] = True\n             config[\"renorm_clipping\"] = self.renorm_clipping\n@@ -1078,16 +1093,16 @@ class BatchNormalizationBase(Layer):\n class SyncBatchNormalization(BatchNormalizationBase):\n     r\"\"\"Normalize and scale inputs or activations synchronously across replicas.\n \n-    Applies batch normalization to activations of the previous layer at each batch\n-    by synchronizing the global batch statistics across all devices that are\n-    training the model. For specific details about batch normalization please\n-    refer to the `tf.keras.layers.BatchNormalization` layer docs.\n+    Applies batch normalization to activations of the previous layer at each\n+    batch by synchronizing the global batch statistics across all devices that\n+    are training the model. For specific details about batch normalization\n+    please refer to the `tf.keras.layers.BatchNormalization` layer docs.\n \n     If this layer is used when using tf.distribute strategy to train models\n     across devices/workers, there will be an allreduce call to aggregate batch\n     statistics across all replicas at every training step. Without tf.distribute\n-    strategy, this layer behaves as a regular `tf.keras.layers.BatchNormalization`\n-    layer.\n+    strategy, this layer behaves as a regular\n+    `tf.keras.layers.BatchNormalization` layer.\n \n     Example usage:\n \n@@ -1187,9 +1202,10 @@ class SyncBatchNormalization(BatchNormalizationBase):\n     def _calculate_mean_and_var(self, x, axes, keep_dims):\n \n         with backend.name_scope(\"moments\"):\n-            # The dynamic range of fp16 is too limited to support the collection of\n-            # sufficient statistics. As a workaround we simply perform the operations\n-            # on 32-bit floats before converting the mean and variance back to fp16\n+            # The dynamic range of fp16 is too limited to support the collection\n+            # of sufficient statistics. As a workaround we simply perform the\n+            # operations on 32-bit floats before converting the mean and\n+            # variance back to fp16\n             y = tf.cast(x, tf.float32) if x.dtype == tf.float16 else x\n             replica_ctx = tf.distribute.get_replica_context()\n             if replica_ctx:\n@@ -1198,9 +1214,10 @@ class SyncBatchNormalization(BatchNormalizationBase):\n                     tf.square(y), axis=axes, keepdims=True\n                 )\n                 batch_size = tf.cast(tf.shape(y)[axes[0]], tf.float32)\n-                # TODO(b/163099951): batch the all-reduces once we sort out the ordering\n-                # issue for NCCL. We don't have a mechanism to launch NCCL in the same\n-                # order in each replica nowadays, so we limit NCCL to batch all-reduces.\n+                # TODO(b/163099951): batch the all-reduces once we sort out the\n+                # ordering issue for NCCL. We don't have a mechanism to launch\n+                # NCCL in the same order in each replica nowadays, so we limit\n+                # NCCL to batch all-reduces.\n                 y_sum = replica_ctx.all_reduce(\n                     tf.distribute.ReduceOp.SUM, local_sum\n                 )\n@@ -1222,7 +1239,8 @@ class SyncBatchNormalization(BatchNormalizationBase):\n                 # var = E(x^2) - E(x)^2\n                 variance = y_squared_mean - tf.square(mean)\n             else:\n-                # Compute true mean while keeping the dims for proper broadcasting.\n+                # Compute true mean while keeping the dims for proper\n+                # broadcasting.\n                 mean = tf.reduce_mean(y, axes, keepdims=True, name=\"mean\")\n                 # sample variance, not unbiased variance\n                 # Note: stop_gradient does not change the gradient that gets\n@@ -1292,11 +1310,11 @@ class BatchNormalization(BatchNormalizationBase):\n         `data_format=\"channels_first\"`, set `axis=1` in `BatchNormalization`.\n       momentum: Momentum for the moving average.\n       epsilon: Small float added to variance to avoid dividing by zero.\n-      center: If True, add offset of `beta` to normalized tensor. If False, `beta`\n-        is ignored.\n-      scale: If True, multiply by `gamma`. If False, `gamma` is not used. When the\n-        next layer is linear (also e.g. `nn.relu`), this can be disabled since the\n-        scaling will be done by the next layer.\n+      center: If True, add offset of `beta` to normalized tensor. If False,\n+        `beta` is ignored.\n+      scale: If True, multiply by `gamma`. If False, `gamma` is not used. When\n+        the next layer is linear (also e.g. `nn.relu`), this can be disabled\n+        since the scaling will be done by the next layer.\n       beta_initializer: Initializer for the beta weight.\n       gamma_initializer: Initializer for the gamma weight.\n       moving_mean_initializer: Initializer for the moving mean.\n@@ -1310,10 +1328,10 @@ class BatchNormalization(BatchNormalizationBase):\n       inputs: Input tensor (of any rank).\n       training: Python boolean indicating whether the layer should behave in\n         training mode or in inference mode.\n-        - `training=True`: The layer will normalize its inputs using the mean and\n-          variance of the current batch of inputs.\n-        - `training=False`: The layer will normalize its inputs using the mean and\n-          variance of its moving statistics, learned during training.\n+        - `training=True`: The layer will normalize its inputs using the mean\n+          and variance of the current batch of inputs.\n+        - `training=False`: The layer will normalize its inputs using the mean\n+          and variance of its moving statistics, learned during training.\n \n     Input shape:\n       Arbitrary. Use the keyword argument `input_shape` (tuple of\n\n@@ -258,9 +258,9 @@ class BatchNormalizationTest(test_combinations.TestCase):\n         )\n         layer(x, training=True)\n         self.assertTrue(layer.fused)\n-        # Since fused is used, Bessel's correction is used. The variance of [0, 2]\n-        # is 2 with Bessel's correction. Since the momentum is 0.5, the variance is\n-        # 2 * 0.5 == 1.\n+        # Since fused is used, Bessel's correction is used. The variance of [0,\n+        # 2] is 2 with Bessel's correction. Since the momentum is 0.5, the\n+        # variance is 2 * 0.5 == 1.\n         self.assertAllEqual(self.evaluate(layer.moving_variance), [1.0])\n \n         x = tf.constant([0.0, 2.0], shape=[2, 1, 1, 1, 1])\n@@ -269,9 +269,9 @@ class BatchNormalizationTest(test_combinations.TestCase):\n         )\n         layer(x, training=True)\n         self.assertTrue(layer.fused)\n-        # Since fused is used, Bessel's correction is used. The variance of [0, 2]\n-        # is 2 with Bessel's correction. Since the momentum is 0.5, the variance is\n-        # 2 * 0.5 == 1.\n+        # Since fused is used, Bessel's correction is used. The variance of [0,\n+        # 2] is 2 with Bessel's correction. Since the momentum is 0.5, the\n+        # variance is 2 * 0.5 == 1.\n         self.assertAllEqual(self.evaluate(layer.moving_variance), [1.0])\n \n \n\n@@ -120,24 +120,26 @@ class LayerNormalization(Layer):\n     Normalization layer with group size set to 1.\n \n     Args:\n-      axis: Integer or List/Tuple. The axis or axes to normalize across. Typically\n-        this is the features axis/axes. The left-out axes are typically the batch\n-        axis/axes. This argument defaults to `-1`, the last dimension in the\n-        input.\n+      axis: Integer or List/Tuple. The axis or axes to normalize across.\n+        Typically this is the features axis/axes. The left-out axes are\n+        typically the batch axis/axes. This argument defaults to `-1`, the last\n+        dimension in the input.\n       epsilon: Small float added to variance to avoid dividing by zero. Defaults\n         to 1e-3\n-      center: If True, add offset of `beta` to normalized tensor. If False, `beta`\n-        is ignored. Defaults to True.\n-      scale: If True, multiply by `gamma`. If False, `gamma` is not used. Defaults\n-        to True. When the next layer is linear (also e.g. `nn.relu`), this can be\n-        disabled since the scaling will be done by the next layer.\n+      center: If True, add offset of `beta` to normalized tensor. If False,\n+        `beta` is ignored. Defaults to True.\n+      scale: If True, multiply by `gamma`. If False, `gamma` is not used.\n+        Defaults to True. When the next layer is linear (also e.g. `nn.relu`),\n+        this can be disabled since the scaling will be done by the next layer.\n       beta_initializer: Initializer for the beta weight. Defaults to zeros.\n       gamma_initializer: Initializer for the gamma weight. Defaults to ones.\n-      beta_regularizer: Optional regularizer for the beta weight. None by default.\n+      beta_regularizer: Optional regularizer for the beta weight. None by\n+        default.\n       gamma_regularizer: Optional regularizer for the gamma weight. None by\n         default.\n       beta_constraint: Optional constraint for the beta weight. None by default.\n-      gamma_constraint: Optional constraint for the gamma weight. None by default.\n+      gamma_constraint: Optional constraint for the gamma weight. None by\n+        default.\n \n     Input shape:\n       Arbitrary. Use the keyword argument `input_shape` (tuple of\n@@ -189,8 +191,8 @@ class LayerNormalization(Layer):\n \n         self.supports_masking = True\n \n-        # Indicates whether a faster fused implementation can be used. This will be\n-        # set to True or False in build()\"\n+        # Indicates whether a faster fused implementation can be used. This will\n+        # be set to True or False in build()\"\n         self._fused = None\n \n     def _fused_can_be_used(self, ndims):\n@@ -205,10 +207,10 @@ class LayerNormalization(Layer):\n         if axis[-1] == ndims - 1 and axis[-1] - axis[0] == len(axis) - 1:\n             can_use_fused = True\n \n-        # fused_batch_norm will silently raise epsilon to be at least 1.001e-5, so\n-        # we cannot used the fused version if epsilon is below that value. Also, the\n-        # variable dtype must be float32, as fused_batch_norm only supports float32\n-        # variables.\n+        # fused_batch_norm will silently raise epsilon to be at least 1.001e-5,\n+        # so we cannot used the fused version if epsilon is below that value.\n+        # Also, the variable dtype must be float32, as fused_batch_norm only\n+        # supports float32 variables.\n         if self.epsilon < 1.001e-5 or self.dtype != \"float32\":\n             can_use_fused = False\n \n@@ -281,8 +283,8 @@ class LayerNormalization(Layer):\n                 input_dtype in (\"float16\", \"bfloat16\")\n                 and self.dtype == \"float32\"\n             ):\n-                # If mixed precision is used, cast inputs to float32 so that this is at\n-                # least as numerically stable as the fused version.\n+                # If mixed precision is used, cast inputs to float32 so that\n+                # this is at least as numerically stable as the fused version.\n                 inputs = tf.cast(inputs, \"float32\")\n \n             # Calculate the moments on the last axis (layer activations).\n@@ -290,7 +292,8 @@ class LayerNormalization(Layer):\n \n             scale, offset = _broadcast(self.gamma), _broadcast(self.beta)\n \n-            # Compute layer normalization using the batch_normalization function.\n+            # Compute layer normalization using the batch_normalization\n+            # function.\n             outputs = tf.nn.batch_normalization(\n                 inputs,\n                 mean,\n@@ -319,10 +322,11 @@ class LayerNormalization(Layer):\n \n             inputs = tf.reshape(inputs, squeezed_shape)\n \n-            # self.gamma and self.beta have the wrong shape for fused_batch_norm, so\n-            # we cannot pass them as the scale and offset parameters. Therefore, we\n-            # create two constant tensors in correct shapes for fused_batch_norm and\n-            # later construct a separate calculation on the scale and offset.\n+            # self.gamma and self.beta have the wrong shape for\n+            # fused_batch_norm, so we cannot pass them as the scale and offset\n+            # parameters. Therefore, we create two constant tensors in correct\n+            # shapes for fused_batch_norm and later construct a separate\n+            # calculation on the scale and offset.\n             scale = tf.ones([pre_dim], dtype=self.dtype)\n             offset = tf.zeros([pre_dim], dtype=self.dtype)\n \n\n@@ -191,7 +191,8 @@ class LayerNormalizationTest(test_combinations.TestCase):\n     def testInvalidAxis(self):\n         with self.assertRaisesRegex(\n             ValueError,\n-            r\"Invalid value for `axis` argument. Expected 0 <= axis < inputs.rank\",\n+            r\"Invalid value for `axis` argument. \"\n+            r\"Expected 0 <= axis < inputs.rank\",\n         ):\n             layer_norm = layer_normalization.LayerNormalization(axis=3)\n             layer_norm.build(input_shape=(2, 2, 2))\n@@ -242,10 +243,10 @@ class LayerNormalizationNumericsTest(test_combinations.TestCase):\n         \"\"\"Tests the forward pass of layer layer_normalization.\n \n         Args:\n-          batch_input_shape: The input shape that will be used to test, including\n-            the batch dimension.\n-          axis: A list of axes to normalize. Will be passed to the `axis` argument\n-            of Layerlayer_normalization.\n+          batch_input_shape: The input shape that will be used to test,\n+            including the batch dimension.\n+          axis: A list of axes to normalize. Will be passed to the `axis`\n+            argument of Layerlayer_normalization.\n           fp64_tol: The relative and absolute tolerance for float64.\n           fp32_tol: The relative and absolute tolerance for float32.\n           fp16_tol: The relative and absolute tolerance for float16.\n@@ -284,16 +285,16 @@ class LayerNormalizationNumericsTest(test_combinations.TestCase):\n                     assert dtype == \"float16\"\n                     tol = fp16_tol\n \n-                # We use absolute tolerances in addition to relative tolerances, because\n-                # some of the values are very close to zero.\n+                # We use absolute tolerances in addition to relative tolerances,\n+                # because some of the values are very close to zero.\n                 self.assertAllClose(expected, actual, rtol=tol, atol=tol)\n \n     @test_combinations.generate(\n         test_combinations.combine(mode=[\"graph\", \"eager\"])\n     )\n     def test_forward(self):\n-        # For numeric stability, we ensure the axis's dimension(s) have at least 4\n-        # elements.\n+        # For numeric stability, we ensure the axis's dimension(s) have at least\n+        # 4 elements.\n         self._test_forward_pass((4, 3), (0,))\n         self._test_forward_pass((3, 4), (1,))\n         self._test_forward_pass((4, 3, 2), (0,))\n@@ -315,10 +316,10 @@ class LayerNormalizationNumericsTest(test_combinations.TestCase):\n         \"\"\"Tests the backwards pass of layer layer_normalization.\n \n         Args:\n-          batch_input_shape: The input shape that will be used to test, including\n-            the batch dimension.\n-          axis: A list of axes to normalize. Will be passed to the `axis` argument\n-            of Layerlayer_normalization.\n+          batch_input_shape: The input shape that will be used to test,\n+            including the batch dimension.\n+          axis: A list of axes to normalize. Will be passed to the `axis`\n+            argument of Layerlayer_normalization.\n           fp64_tol: The relative and absolute tolerance for float64.\n           fp32_tol: The relative and absolute tolerance for float32.\n           fp16_tol: The relative and absolute tolerance for float16.\n@@ -334,10 +335,10 @@ class LayerNormalizationNumericsTest(test_combinations.TestCase):\n         x = np.random.normal(size=batch_input_shape)\n \n         for epsilon in 1e-12, 1e-3:\n-            # Float64 must come first in this list, as we use the float64 numerical\n-            # gradients to compare to the float32 and float16 symbolic gradients as\n-            # well. Computing float32/float16 numerical gradients is too numerically\n-            # unstable.\n+            # Float64 must come first in this list, as we use the float64\n+            # numerical gradients to compare to the float32 and float16 symbolic\n+            # gradients as well. Computing float32/float16 numerical gradients\n+            # is too numerically unstable.\n             for dtype in \"float64\", \"float32\", \"float16\":\n                 norm = layer_normalization.LayerNormalization(\n                     axis=axis,\n@@ -351,10 +352,11 @@ class LayerNormalizationNumericsTest(test_combinations.TestCase):\n \n                 # pylint: disable=cell-var-from-loop\n                 def forward_fn(x, beta, gamma):\n-                    # We must monkey-patch the attributes of `norm` with the function\n-                    # arguments, so that the gradient checker will properly compute their\n-                    # gradients. The gradient checker computes gradients with respect to\n-                    # the input arguments of `f`.\n+                    # We must monkey-patch the attributes of `norm` with the\n+                    # function arguments, so that the gradient checker will\n+                    # properly compute their gradients. The gradient checker\n+                    # computes gradients with respect to the input arguments of\n+                    # `f`.\n                     with tf.compat.v1.test.mock.patch.object(\n                         norm, \"beta\", beta\n                     ):\n@@ -374,8 +376,8 @@ class LayerNormalizationNumericsTest(test_combinations.TestCase):\n                 ) = results\n \n                 if dtype == \"float64\":\n-                    # We use the float64 numeric gradients as the reference, to compare\n-                    # against the symbolic gradients for all dtypes.\n+                    # We use the float64 numeric gradients as the reference, to\n+                    # compare against the symbolic gradients for all dtypes.\n                     x_grad_ref = x_grad_n\n                     beta_grad_ref = beta_grad_n\n                     gamma_grad_ref = gamma_grad_n\n@@ -386,8 +388,8 @@ class LayerNormalizationNumericsTest(test_combinations.TestCase):\n                     assert dtype == \"float16\"\n                     tol = fp16_tol\n \n-                # We use absolute tolerances in addition to relative tolerances, because\n-                # some of the values are very close to zero.\n+                # We use absolute tolerances in addition to relative tolerances,\n+                # because some of the values are very close to zero.\n                 self.assertAllClose(x_grad_t, x_grad_ref, rtol=tol, atol=tol)\n                 self.assertAllClose(\n                     beta_grad_t, beta_grad_ref, rtol=tol, atol=tol\n@@ -396,11 +398,12 @@ class LayerNormalizationNumericsTest(test_combinations.TestCase):\n                     gamma_grad_t, gamma_grad_ref, rtol=tol, atol=tol\n                 )\n \n-    # The gradient_checker_v2 does not work properly with LayerNorm in graph mode.\n+    # The gradient_checker_v2 does not work properly with LayerNorm in graph\n+    # mode.\n     @test_utils.run_v2_only\n     def test_backward(self):\n-        # For numeric stability, we ensure the axis's dimension(s) have at least 4\n-        # elements.\n+        # For numeric stability, we ensure the axis's dimension(s) have at least\n+        # 4 elements.\n         self._test_backward_pass((4, 3), (0,))\n         self._test_backward_pass((2, 4, 2), (1,))\n         self._test_backward_pass((2, 3, 4), (2,))\n\n@@ -40,10 +40,10 @@ class UnitNormalization(base_layer.Layer):\n     1.0\n \n     Args:\n-      axis: Integer or list/tuple. The axis or axes to normalize across. Typically\n-        this is the features axis or axes. The left-out axes are typically the\n-        batch axis or axes. Defaults to `-1`, the last dimension in\n-        the input.\n+      axis: Integer or list/tuple. The axis or axes to normalize across.\n+        Typically this is the features axis or axes. The left-out axes are\n+        typically the batch axis or axes. Defaults to `-1`, the last dimension\n+        in the input.\n     \"\"\"\n \n     def __init__(self, axis=-1, **kwargs):\n\n@@ -25,8 +25,8 @@ from tensorflow.python.util.tf_export import keras_export\n class AveragePooling3D(Pooling3D):\n     \"\"\"Average pooling operation for 3D data (spatial or spatio-temporal).\n \n-    Downsamples the input along its spatial dimensions (depth, height, and width)\n-    by taking the average value over an input window\n+    Downsamples the input along its spatial dimensions (depth, height, and\n+    width) by taking the average value over an input window\n     (of size defined by `pool_size`) for each channel of the input.\n     The window is shifted by `strides` along each dimension.\n \n\n@@ -53,7 +53,8 @@ class AveragePoolingTest(tf.test.TestCase, parameterized.TestCase):\n         # This part of the test can only run on GPU but doesn't appear\n         # to be properly assigned to a GPU when running in eager mode.\n         if not tf.executing_eagerly():\n-            # Only runs on GPU with CUDA, channels_first is not supported on CPU.\n+            # Only runs on GPU with CUDA, channels_first is not supported on\n+            # CPU.\n             # TODO(b/62340061): Support channels_first on CPU.\n             if tf.test.is_gpu_available(cuda_only=True):\n                 test_utils.layer_test(\n\n@@ -29,7 +29,8 @@ class Pooling2D(Layer):\n \n     Args:\n       pool_function: The pooling function to apply, e.g. `tf.nn.max_pool2d`.\n-      pool_size: An integer or tuple/list of 2 integers: (pool_height, pool_width)\n+      pool_size: An integer or tuple/list of 2 integers:\n+        (pool_height, pool_width)\n         specifying the size of the pooling window.\n         Can be a single integer to specify the same value for\n         all spatial dimensions.\n@@ -39,7 +40,8 @@ class Pooling2D(Layer):\n         all spatial dimensions.\n       padding: A string. The padding method, either 'valid' or 'same'.\n         Case-insensitive.\n-      data_format: A string, one of `channels_last` (default) or `channels_first`.\n+      data_format: A string, one of `channels_last` (default) or\n+        `channels_first`.\n         The ordering of the dimensions in the inputs.\n         `channels_last` corresponds to inputs with shape\n         `(batch, height, width, channels)` while `channels_first` corresponds to\n\n@@ -40,7 +40,8 @@ class Pooling3D(Layer):\n         all spatial dimensions.\n       padding: A string. The padding method, either 'valid' or 'same'.\n         Case-insensitive.\n-      data_format: A string, one of `channels_last` (default) or `channels_first`.\n+      data_format: A string, one of `channels_last` (default) or\n+        `channels_first`.\n         The ordering of the dimensions in the inputs.\n         `channels_last` corresponds to inputs with shape\n         `(batch, depth, height, width, channels)`\n\n@@ -25,10 +25,10 @@ from tensorflow.python.util.tf_export import keras_export\n class MaxPooling3D(Pooling3D):\n     \"\"\"Max pooling operation for 3D data (spatial or spatio-temporal).\n \n-    Downsamples the input along its spatial dimensions (depth, height, and width)\n-    by taking the maximum value over an input window\n-    (of size defined by `pool_size`) for each channel of the input.\n-    The window is shifted by `strides` along each dimension.\n+    Downsamples the input along its spatial dimensions (depth, height, and\n+    width) by taking the maximum value over an input window (of size defined by\n+    `pool_size`) for each channel of the input.  The window is shifted by\n+    `strides` along each dimension.\n \n     Args:\n       pool_size: Tuple of 3 integers,\n\n@@ -12,7 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Benchmark for KPL implementation of categorical hash columns with dense inputs.\"\"\"\n+\"\"\"Benchmark for KPL implementation of categorical hash columns with dense\n+inputs.\"\"\"\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -12,7 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Benchmark for KPL implementation of categorical hash columns with varying-length inputs.\"\"\"\n+\"\"\"Benchmark for KPL implementation of categorical hash columns with\n+varying-length inputs.\"\"\"\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -12,7 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Benchmark for KPL implementation of vocabulary columns from files with dense inputs.\"\"\"\n+\"\"\"Benchmark for KPL implementation of vocabulary columns from files with dense\n+inputs.\"\"\"\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -12,7 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Benchmark for KPL implementation of vocabulary columns from files with varying-length inputs.\"\"\"\n+\"\"\"Benchmark for KPL implementation of vocabulary columns from files with\n+varying-length inputs.\"\"\"\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -12,7 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Benchmark for KPL implementation of vocabulary columns from lists with dense inputs.\"\"\"\n+\"\"\"Benchmark for KPL implementation of vocabulary columns from lists with dense\n+inputs.\"\"\"\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -12,7 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Benchmark for KPL implementation of vocabulary columns + indicator from lists with dense inputs.\"\"\"\n+\"\"\"Benchmark for KPL implementation of vocabulary columns + indicator from lists\n+with dense inputs.\"\"\"\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -12,7 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Benchmark for KPL implementation of vocabulary columns + indicator from lists with varying-length inputs.\"\"\"\n+\"\"\"Benchmark for KPL implementation of vocabulary columns + indicator from lists\n+with varying-length inputs.\"\"\"\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -12,7 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Benchmark for KPL implementation of vocabulary columns from lists with varying-length inputs.\"\"\"\n+\"\"\"Benchmark for KPL implementation of vocabulary columns from lists with\n+varying-length inputs.\"\"\"\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -12,7 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Benchmark for KPL implementation of embedding column with varying-length inputs.\"\"\"\n+\"\"\"Benchmark for KPL implementation of embedding column with varying-length\n+inputs.\"\"\"\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -12,7 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Benchmark for KPL implementation of categorical cross hash columns with dense inputs.\"\"\"\n+\"\"\"Benchmark for KPL implementation of categorical cross hash columns with dense\n+inputs.\"\"\"\n \n \n import keras\n\n@@ -12,7 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Benchmark for KPL implementation of weighted embedding column with varying-length inputs.\"\"\"\n+\"\"\"Benchmark for KPL implementation of weighted embedding column with\n+varying-length inputs.\"\"\"\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -42,8 +42,8 @@ class CategoryEncoding(base_layer.Layer):\n     This layer provides options for condensing data into a categorical encoding\n     when the total number of tokens are known in advance. It accepts integer\n     values as inputs, and it outputs a dense or sparse representation of those\n-    inputs. For integer inputs where the total number of tokens is not known, use\n-    `tf.keras.layers.IntegerLookup` instead.\n+    inputs. For integer inputs where the total number of tokens is not known,\n+    use `tf.keras.layers.IntegerLookup` instead.\n \n     For an overview and full list of preprocessing layers, see the preprocessing\n     [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n@@ -85,9 +85,9 @@ class CategoryEncoding(base_layer.Layer):\n              [0. , 0.2, 0. , 0.4]], dtype=float32)>\n \n     Args:\n-      num_tokens: The total number of tokens the layer should support. All inputs\n-        to the layer must integers in the range `0 <= value < num_tokens`, or an\n-        error will be thrown.\n+      num_tokens: The total number of tokens the layer should support. All\n+        inputs to the layer must integers in the range `0 <= value <\n+        num_tokens`, or an error will be thrown.\n       output_mode: Specification for the output of the layer.\n         Defaults to `\"multi_hot\"`. Values can be `\"one_hot\"`, `\"multi_hot\"` or\n         `\"count\"`, configuring the layer as follows:\n@@ -97,10 +97,10 @@ class CategoryEncoding(base_layer.Layer):\n             last dimension is not size 1, will append a new dimension for the\n             encoded output.\n           - `\"multi_hot\"`: Encodes each sample in the input into a single array\n-            of `num_tokens` size, containing a 1 for each vocabulary term present\n-            in the sample. Treats the last dimension as the sample dimension, if\n-            input shape is `(..., sample_length)`, output shape will be\n-            `(..., num_tokens)`.\n+            of `num_tokens` size, containing a 1 for each vocabulary term\n+            present in the sample. Treats the last dimension as the sample\n+            dimension, if input shape is `(..., sample_length)`, output shape\n+            will be `(..., num_tokens)`.\n           - `\"count\"`: Like `\"multi_hot\"`, but the int array contains a count of\n             the number of times the token at that index appeared in the sample.\n         For all output modes, currently only output up to rank 2 is supported.\n@@ -110,15 +110,15 @@ class CategoryEncoding(base_layer.Layer):\n     Call arguments:\n       inputs: A 1D or 2D tensor of integer inputs.\n       count_weights: A tensor in the same shape as `inputs` indicating the\n-        weight for each sample value when summing up in `count` mode. Not used in\n-        `\"multi_hot\"` or `\"one_hot\"` modes.\n+        weight for each sample value when summing up in `count` mode. Not used\n+        in `\"multi_hot\"` or `\"one_hot\"` modes.\n     \"\"\"\n \n     def __init__(\n         self, num_tokens=None, output_mode=\"multi_hot\", sparse=False, **kwargs\n     ):\n-        # max_tokens is an old name for the num_tokens arg we continue to support\n-        # because of usage.\n+        # max_tokens is an old name for the num_tokens arg we continue to\n+        # support because of usage.\n         if \"max_tokens\" in kwargs:\n             logging.warning(\n                 \"max_tokens is deprecated, please use num_tokens instead.\"\n@@ -192,8 +192,8 @@ class CategoryEncoding(base_layer.Layer):\n         if count_weights is not None:\n             if self.output_mode != COUNT:\n                 raise ValueError(\n-                    \"`count_weights` is not used when `output_mode` is not `'count'`. \"\n-                    \"Received `count_weights={}`.\".format(count_weights)\n+                    \"`count_weights` is not used when `output_mode` is not \"\n+                    \"`'count'`. Received `count_weights={count_weights}`.\"\n                 )\n             count_weights = utils.ensure_tensor(\n                 count_weights, self.compute_dtype\n\n@@ -297,7 +297,8 @@ class CategoryEncodingInputTest(\n         int_data = encoder_layer(input_data)\n         self.assertAllEqual(expected_output_shape, int_data.shape.as_list())\n         model = keras.Model(inputs=input_data, outputs=int_data)\n-        # Call predict once on valid input to compile a graph and test control flow.\n+        # Call predict once on valid input to compile a graph and test control\n+        # flow.\n         _ = model.predict(valid_array, steps=1)\n         with self.assertRaisesRegex(\n             tf.errors.InvalidArgumentError,\n@@ -315,7 +316,8 @@ class CategoryEncodingInputTest(\n         int_data = encoder_layer(input_data)\n         self.assertAllEqual(expected_output_shape, int_data.shape.as_list())\n         model = keras.Model(inputs=input_data, outputs=int_data)\n-        # Call predict once on valid input to compile a graph and test control flow.\n+        # Call predict once on valid input to compile a graph and test control\n+        # flow.\n         _ = model.predict(valid_array, steps=1)\n         with self.assertRaisesRegex(\n             tf.errors.InvalidArgumentError,\n\n@@ -46,7 +46,8 @@ def summarize(values, epsilon):\n \n     Args:\n         values: 1D `np.ndarray` to be summarized.\n-        epsilon: A `'float32'` that determines the approximate desired precision.\n+        epsilon: A `'float32'` that determines the approximate desired\n+          precision.\n \n     Returns:\n         A 2D `np.ndarray` that is a summary of the inputs. First column is the\n@@ -69,15 +70,16 @@ def summarize(values, epsilon):\n def compress(summary, epsilon):\n     \"\"\"Compress a summary to within `epsilon` accuracy.\n \n-    The compression step is needed to keep the summary sizes small after merging,\n-    and also used to return the final target boundaries. It finds the new bins\n-    based on interpolating cumulative weight percentages from the large summary.\n-    Taking the difference of the cumulative weights from the previous bin's\n-    cumulative weight will give the new weight for that bin.\n+    The compression step is needed to keep the summary sizes small after\n+    merging, and also used to return the final target boundaries. It finds the\n+    new bins based on interpolating cumulative weight percentages from the large\n+    summary.  Taking the difference of the cumulative weights from the previous\n+    bin's cumulative weight will give the new weight for that bin.\n \n     Args:\n         summary: 2D `np.ndarray` summary to be compressed.\n-        epsilon: A `'float32'` that determines the approxmiate desired precision.\n+        epsilon: A `'float32'` that determines the approxmiate desired\n+          precision.\n \n     Returns:\n         A 2D `np.ndarray` that is a compressed summary. First column is the\n@@ -153,30 +155,30 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n     Arguments:\n       bin_boundaries: A list of bin boundaries. The leftmost and rightmost bins\n         will always extend to `-inf` and `inf`, so `bin_boundaries=[0., 1., 2.]`\n-        generates bins `(-inf, 0.)`, `[0., 1.)`, `[1., 2.)`, and `[2., +inf)`. If\n-        this option is set, `adapt()` should not be called.\n+        generates bins `(-inf, 0.)`, `[0., 1.)`, `[1., 2.)`, and `[2., +inf)`.\n+        If this option is set, `adapt()` should not be called.\n       num_bins: The integer number of bins to compute. If this option is set,\n         `adapt()` should be called to learn the bin boundaries.\n       epsilon: Error tolerance, typically a small fraction close to zero (e.g.\n         0.01). Higher values of epsilon increase the quantile approximation, and\n         hence result in more unequal buckets, but could improve performance\n         and resource consumption.\n-      output_mode: Specification for the output of the layer. Defaults to `\"int\"`.\n-        Values can be `\"int\"`, `\"one_hot\"`, `\"multi_hot\"`, or `\"count\"`\n-        configuring the layer as follows:\n+      output_mode: Specification for the output of the layer. Defaults to\n+        `\"int\"`.  Values can be `\"int\"`, `\"one_hot\"`, `\"multi_hot\"`, or\n+        `\"count\"` configuring the layer as follows:\n           - `\"int\"`: Return the discritized bin indices directly.\n           - `\"one_hot\"`: Encodes each individual element in the input into an\n             array the same size as `num_bins`, containing a 1 at the input's bin\n-            index. If the last dimension is size 1, will encode on that dimension.\n-            If the last dimension is not size 1, will append a new dimension for\n-            the encoded output.\n+            index. If the last dimension is size 1, will encode on that\n+            dimension.  If the last dimension is not size 1, will append a new\n+            dimension for the encoded output.\n           - `\"multi_hot\"`: Encodes each sample in the input into a single array\n             the same size as `num_bins`, containing a 1 for each bin index\n             index present in the sample. Treats the last dimension as the sample\n-            dimension, if input shape is `(..., sample_length)`, output shape will\n-            be `(..., num_tokens)`.\n-          - `\"count\"`: As `\"multi_hot\"`, but the int array contains a count of the\n-            number of times the bin index appeared in the sample.\n+            dimension, if input shape is `(..., sample_length)`, output shape\n+            will be `(..., num_tokens)`.\n+          - `\"count\"`: As `\"multi_hot\"`, but the int array contains a count of\n+            the number of times the bin index appeared in the sample.\n       sparse: Boolean. Only applicable to `\"one_hot\"`, `\"multi_hot\"`,\n         and `\"count\"` output modes. If True, returns a `SparseTensor` instead of\n         a dense `Tensor`. Defaults to False.\n@@ -210,11 +212,12 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n         sparse=False,\n         **kwargs,\n     ):\n-        # bins is a deprecated arg for setting bin_boundaries or num_bins that still\n-        # has some usage.\n+        # bins is a deprecated arg for setting bin_boundaries or num_bins that\n+        # still has some usage.\n         if \"bins\" in kwargs:\n             logging.warning(\n-                \"bins is deprecated, please use bin_boundaries or num_bins instead.\"\n+                \"bins is deprecated, \"\n+                \"please use bin_boundaries or num_bins instead.\"\n             )\n             if isinstance(kwargs[\"bins\"], int) and num_bins is None:\n                 num_bins = kwargs[\"bins\"]\n@@ -230,7 +233,8 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n         elif (\n             output_mode == \"int\" and not tf.as_dtype(kwargs[\"dtype\"]).is_integer\n         ):\n-            # Compat for when dtype was always floating and ignored by the layer.\n+            # Compat for when dtype was always floating and ignored by the\n+            # layer.\n             kwargs[\"dtype\"] = tf.int64\n \n         super().__init__(**kwargs)\n@@ -292,8 +296,8 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n         if self.input_bin_boundaries is not None:\n             return\n \n-        # Summary contains two equal length vectors of bins at index 0 and weights\n-        # at index 1.\n+        # Summary contains two equal length vectors of bins at index 0 and\n+        # weights at index 1.\n         self.summary = self.add_weight(\n             name=\"summary\",\n             shape=(2, None),\n@@ -309,25 +313,28 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n     def adapt(self, data, batch_size=None, steps=None):\n         \"\"\"Computes bin boundaries from quantiles in a input dataset.\n \n-        Calling `adapt()` on a `Discretization` layer is an alternative to passing\n-        in a `bin_boundaries` argument during construction. A `Discretization` layer\n-        should always be either adapted over a dataset or passed `bin_boundaries`.\n+        Calling `adapt()` on a `Discretization` layer is an alternative to\n+        passing in a `bin_boundaries` argument during construction. A\n+        `Discretization` layer should always be either adapted over a dataset or\n+        passed `bin_boundaries`.\n \n         During `adapt()`, the layer will estimate the quantile boundaries of the\n-        input dataset. The number of quantiles can be controlled via the `num_bins`\n-        argument, and the error tolerance for quantile boundaries can be controlled\n-        via the `epsilon` argument.\n+        input dataset. The number of quantiles can be controlled via the\n+        `num_bins` argument, and the error tolerance for quantile boundaries can\n+        be controlled via the `epsilon` argument.\n \n-        In order to make `Discretization` efficient in any distribution context, the\n-        computed boundaries are kept static with respect to any compiled `tf.Graph`s\n-        that call the layer. As a consequence, if the layer is adapted a second\n-        time, any models using the layer should be re-compiled. For more information\n-        see `tf.keras.layers.experimental.preprocessing.PreprocessingLayer.adapt`.\n+        In order to make `Discretization` efficient in any distribution context,\n+        the computed boundaries are kept static with respect to any compiled\n+        `tf.Graph`s that call the layer. As a consequence, if the layer is\n+        adapted a second time, any models using the layer should be re-compiled.\n+        For more information see\n+        `tf.keras.layers.experimental.preprocessing.PreprocessingLayer.adapt`.\n \n-        `adapt()` is meant only as a single machine utility to compute layer state.\n-        To analyze a dataset that cannot fit on a single machine, see\n-        [Tensorflow Transform](https://www.tensorflow.org/tfx/transform/get_started)\n-        for a multi-machine, map-reduce solution.\n+        `adapt()` is meant only as a single machine utility to compute layer\n+        state.  To analyze a dataset that cannot fit on a single machine, see\n+        [Tensorflow Transform](\n+        https://www.tensorflow.org/tfx/transform/get_started) for a\n+        multi-machine, map-reduce solution.\n \n         Arguments:\n           data: The data to train on. It can be passed either as a\n@@ -354,8 +361,8 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n     def update_state(self, data):\n         if self.input_bin_boundaries is not None:\n             raise ValueError(\n-                \"Cannot adapt a Discretization layer that has been initialized with \"\n-                \"`bin_boundaries`, use `num_bins` instead. You passed \"\n+                \"Cannot adapt a Discretization layer that has been initialized \"\n+                \"with `bin_boundaries`, use `num_bins` instead. You passed \"\n                 \"`bin_boundaries={}`.\".format(self.input_bin_boundaries)\n             )\n \n\n@@ -33,8 +33,8 @@ ONE_HOT = utils.ONE_HOT\n class HashedCrossing(base_layer.Layer):\n     \"\"\"A preprocessing layer which crosses features using the \"hashing trick\".\n \n-    This layer performs crosses of categorical features using the \"hasing trick\".\n-    Conceptually, the transformation can be thought of as:\n+    This layer performs crosses of categorical features using the \"hasing\n+    trick\".  Conceptually, the transformation can be thought of as:\n     hash(concatenation of features) % `num_bins`.\n \n     This layer currently only performs crosses of scalar inputs and batches of\n@@ -46,8 +46,9 @@ class HashedCrossing(base_layer.Layer):\n \n     Args:\n       num_bins: Number of hash bins.\n-      output_mode: Specification for the output of the layer. Defaults to `\"int\"`.\n-        Values can be `\"int\"`, or `\"one_hot\"` configuring the layer as follows:\n+      output_mode: Specification for the output of the layer. Defaults to\n+        `\"int\"`.  Values can be `\"int\"`, or `\"one_hot\"` configuring the layer as\n+        follows:\n           - `\"int\"`: Return the integer bin indices directly.\n           - `\"one_hot\"`: Encodes each individual element in the input into an\n             array the same size as `num_bins`, containing a 1 at the input's bin\n@@ -118,8 +119,8 @@ class HashedCrossing(base_layer.Layer):\n         self.sparse = sparse\n \n     def call(self, inputs):\n-        # Convert all inputs to tensors and check shape. This layer only supports\n-        # sclars and batches of scalars for the initial version.\n+        # Convert all inputs to tensors and check shape. This layer only\n+        # supports sclars and batches of scalars for the initial version.\n         self._check_at_least_two_inputs(inputs)\n         inputs = [utils.ensure_tensor(x) for x in inputs]\n         self._check_input_shape_and_type(inputs)\n@@ -137,9 +138,9 @@ class HashedCrossing(base_layer.Layer):\n \n         # Fix output shape and downrank to match input rank.\n         if rank == 2:\n-            # tf.sparse.cross_hashed output shape will always be None on the last\n-            # dimension. Given our input shape restrictions, we want to force shape 1\n-            # instead.\n+            # tf.sparse.cross_hashed output shape will always be None on the\n+            # last dimension. Given our input shape restrictions, we want to\n+            # force shape 1 instead.\n             outputs = tf.reshape(outputs, [-1, 1])\n         elif rank == 1:\n             outputs = tf.reshape(outputs, [-1])\n@@ -184,8 +185,8 @@ class HashedCrossing(base_layer.Layer):\n     def _check_at_least_two_inputs(self, inputs):\n         if not isinstance(inputs, (list, tuple)):\n             raise ValueError(\n-                \"`HashedCrossing` should be called on a list or tuple of inputs. \"\n-                f\"Received: inputs={inputs}\"\n+                \"`HashedCrossing` should be called on a list or tuple of \"\n+                f\"inputs. Received: inputs={inputs}\"\n             )\n         if len(inputs) < 2:\n             raise ValueError(\n@@ -198,8 +199,9 @@ class HashedCrossing(base_layer.Layer):\n         rank = len(first_shape)\n         if rank > 2 or (rank == 2 and first_shape[-1] != 1):\n             raise ValueError(\n-                \"All `HashedCrossing` inputs should have shape `[]`, `[batch_size]` \"\n-                f\"or `[batch_size, 1]`. Received: inputs={inputs}\"\n+                \"All `HashedCrossing` inputs should have shape `[]`, \"\n+                \"`[batch_size]` or `[batch_size, 1]`. \"\n+                f\"Received: inputs={inputs}\"\n             )\n         if not all(x.shape.as_list() == first_shape for x in inputs[1:]):\n             raise ValueError(\n\n@@ -47,8 +47,8 @@ class Hashing(base_layer.Layer):\n     stable across invocations, regardless of device and context, by mixing the\n     input bits thoroughly.\n \n-    If you want to obfuscate the hashed output, you can also pass a random `salt`\n-    argument in the constructor. In that case, the layer will use the\n+    If you want to obfuscate the hashed output, you can also pass a random\n+    `salt` argument in the constructor. In that case, the layer will use the\n     [SipHash64](https://github.com/google/highwayhash) hash function, with\n     the `salt` value serving as additional input to the hash function.\n \n@@ -104,9 +104,9 @@ class Hashing(base_layer.Layer):\n              [0]])>\n \n     Args:\n-      num_bins: Number of hash bins. Note that this includes the `mask_value` bin,\n-        so the effective number of bins is `(num_bins - 1)` if `mask_value` is\n-        set.\n+      num_bins: Number of hash bins. Note that this includes the `mask_value`\n+        bin, so the effective number of bins is `(num_bins - 1)` if `mask_value`\n+        is set.\n       mask_value: A value that represents masked inputs, which are mapped to\n         index 0. Defaults to None, meaning no mask term will be added and the\n         hashing will start at index 0.\n@@ -115,23 +115,24 @@ class Hashing(base_layer.Layer):\n         used as an additional input (known as a \"salt\" in cryptography).\n         These should be non-zero. Defaults to `None` (in that\n         case, the FarmHash64 hash function is used). It also supports\n-        tuple/list of 2 unsigned integer numbers, see reference paper for details.\n-      output_mode: Specification for the output of the layer. Defaults to `\"int\"`.\n-        Values can be `\"int\"`, `\"one_hot\"`, `\"multi_hot\"`, or `\"count\"`\n-        configuring the layer as follows:\n+        tuple/list of 2 unsigned integer numbers, see reference paper for\n+        details.\n+      output_mode: Specification for the output of the layer. Defaults to\n+        `\"int\"`.  Values can be `\"int\"`, `\"one_hot\"`, `\"multi_hot\"`, or\n+        `\"count\"` configuring the layer as follows:\n           - `\"int\"`: Return the integer bin indices directly.\n           - `\"one_hot\"`: Encodes each individual element in the input into an\n             array the same size as `num_bins`, containing a 1 at the input's bin\n-            index. If the last dimension is size 1, will encode on that dimension.\n-            If the last dimension is not size 1, will append a new dimension for\n-            the encoded output.\n+            index. If the last dimension is size 1, will encode on that\n+            dimension.  If the last dimension is not size 1, will append a new\n+            dimension for the encoded output.\n           - `\"multi_hot\"`: Encodes each sample in the input into a single array\n             the same size as `num_bins`, containing a 1 for each bin index\n             index present in the sample. Treats the last dimension as the sample\n-            dimension, if input shape is `(..., sample_length)`, output shape will\n-            be `(..., num_tokens)`.\n-          - `\"count\"`: As `\"multi_hot\"`, but the int array contains a count of the\n-            number of times the bin index appeared in the sample.\n+            dimension, if input shape is `(..., sample_length)`, output shape\n+            will be `(..., num_tokens)`.\n+          - `\"count\"`: As `\"multi_hot\"`, but the int array contains a count of\n+            the number of times the bin index appeared in the sample.\n       sparse: Boolean. Only applicable to `\"one_hot\"`, `\"multi_hot\"`,\n         and `\"count\"` output modes. If True, returns a `SparseTensor` instead of\n         a dense `Tensor`. Defaults to False.\n@@ -163,8 +164,8 @@ class Hashing(base_layer.Layer):\n     ):\n         if num_bins is None or num_bins <= 0:\n             raise ValueError(\n-                f\"The `num_bins` for `Hashing` cannot be `None` or non-positive \"\n-                f\"values. Received: num_bins={num_bins}.\"\n+                f\"The `num_bins` for `Hashing` cannot be `None` or \"\n+                f\"non-positive values. Received: num_bins={num_bins}.\"\n             )\n \n         # By default, output int64 when output_mode='int' and floats otherwise.\n@@ -175,7 +176,8 @@ class Hashing(base_layer.Layer):\n         elif (\n             output_mode == \"int\" and not tf.as_dtype(kwargs[\"dtype\"]).is_integer\n         ):\n-            # Compat for when dtype was always floating and ignored by the layer.\n+            # Compat for when dtype was always floating and ignored by the\n+            # layer.\n             kwargs[\"dtype\"] = tf.int64\n \n         super().__init__(**kwargs)\n@@ -221,8 +223,9 @@ class Hashing(base_layer.Layer):\n                 self.salt = [salt, salt]\n             else:\n                 raise ValueError(\n-                    f\"The `salt` argument for `Hashing` can only be a tuple of size 2 \"\n-                    f\"integers, or a single integer. Received: salt={salt}.\"\n+                    \"The `salt` argument for `Hashing` can only be a tuple of \"\n+                    \"size 2 integers, or a single integer. \"\n+                    f\"Received: salt={salt}.\"\n                 )\n \n     def call(self, inputs):\n\n@@ -67,8 +67,8 @@ class HashingTest(test_combinations.TestCase):\n         )\n         empty_mask_output = empty_mask_layer(inp)\n         omar_mask_output = omar_mask_layer(inp)\n-        # Outputs should be one more than test_hash_dense_input_farmhash (the zeroth\n-        # bin is now reserved for masks).\n+        # Outputs should be one more than test_hash_dense_input_farmhash (the\n+        # zeroth bin is now reserved for masks).\n         self.assertAllClose([[1], [1], [2], [1], [1]], empty_mask_output)\n         # 'omar' should map to 0.\n         self.assertAllClose([[0], [1], [2], [1], [1]], omar_mask_output)\n\n@@ -59,12 +59,14 @@ class Resizing(base_layer.Layer):\n     \"\"\"A preprocessing layer which resizes images.\n \n     This layer resizes an image input to a target height and width. The input\n-    should be a 4D (batched) or 3D (unbatched) tensor in `\"channels_last\"` format.\n-    Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and of\n-    interger or floating point dtype. By default, the layer will output floats.\n+    should be a 4D (batched) or 3D (unbatched) tensor in `\"channels_last\"`\n+    format.  Input pixel values can be of any range (e.g. `[0., 1.)` or `[0,\n+    255]`) and of interger or floating point dtype. By default, the layer will\n+    output floats.\n \n     This layer can be called on tf.RaggedTensor batches of input images of\n-    distinct sizes, and will resize the outputs to dense tensors of uniform size.\n+    distinct sizes, and will resize the outputs to dense tensors of uniform\n+    size.\n \n     For an overview and full list of preprocessing layers, see the preprocessing\n     [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n@@ -77,10 +79,10 @@ class Resizing(base_layer.Layer):\n         `\"lanczos5\"`, `\"gaussian\"`, `\"mitchellcubic\"`.\n       crop_to_aspect_ratio: If True, resize the images without aspect\n         ratio distortion. When the original aspect ratio differs from the target\n-        aspect ratio, the output image will be cropped so as to return the largest\n-        possible window in the image (of size `(height, width)`) that matches\n-        the target aspect ratio. By default (`crop_to_aspect_ratio=False`),\n-        aspect ratio may not be preserved.\n+        aspect ratio, the output image will be cropped so as to return the\n+        largest possible window in the image (of size `(height, width)`) that\n+        matches the target aspect ratio. By default\n+        (`crop_to_aspect_ratio=False`), aspect ratio may not be preserved.\n     \"\"\"\n \n     def __init__(\n@@ -102,9 +104,9 @@ class Resizing(base_layer.Layer):\n         base_preprocessing_layer.keras_kpl_gauge.get_cell(\"Resizing\").set(True)\n \n     def call(self, inputs):\n-        # tf.image.resize will always output float32 and operate more efficiently on\n-        # float32 unless interpolation is nearest, in which case ouput type matches\n-        # input type.\n+        # tf.image.resize will always output float32 and operate more\n+        # efficiently on float32 unless interpolation is nearest, in which case\n+        # ouput type matches input type.\n         if self.interpolation == \"nearest\":\n             input_dtype = self.compute_dtype\n         else:\n@@ -160,12 +162,13 @@ class CenterCrop(base_layer.Layer):\n     \"\"\"A preprocessing layer which crops images.\n \n     This layers crops the central portion of the images to a target size. If an\n-    image is smaller than the target size, it will be resized and cropped so as to\n-    return the largest possible window in the image that matches the target aspect\n-    ratio.\n+    image is smaller than the target size, it will be resized and cropped so as\n+    to return the largest possible window in the image that matches the target\n+    aspect ratio.\n \n     Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and\n-    of interger or floating point dtype. By default, the layer will output floats.\n+    of interger or floating point dtype. By default, the layer will output\n+    floats.\n \n     For an overview and full list of preprocessing layers, see the preprocessing\n     [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n@@ -238,8 +241,8 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n     \"\"\"Abstract base layer for image augmentaion.\n \n     This layer contains base functionalities for preprocessing layers which\n-    augment image related data, eg. image and in future, label and bounding boxes.\n-    The subclasses could avoid making certain mistakes and reduce code\n+    augment image related data, eg. image and in future, label and bounding\n+    boxes.  The subclasses could avoid making certain mistakes and reduce code\n     duplications.\n \n     This layer requires you to implement one method: `augment_image()`, which\n@@ -249,14 +252,14 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n     `augment_label()`, which handles label augmentation if the layer supports\n     that.\n \n-    `augment_bounding_boxes()`, which handles the bounding box augmentation, if the\n-    layer supports that.\n+    `augment_bounding_boxes()`, which handles the bounding box augmentation, if\n+    the layer supports that.\n \n     `get_random_transformation()`, which should produce a random transformation\n-    setting. The tranformation object, which could be any type, will be passed to\n-    `augment_image`, `augment_label` and `augment_bounding_boxes`, to coodinate\n-    the randomness behavior, eg, in the RandomFlip layer, the image and\n-    bounding_boxes should be changed in the same way.\n+    setting. The tranformation object, which could be any type, will be passed\n+    to `augment_image`, `augment_label` and `augment_bounding_boxes`, to\n+    coodinate the randomness behavior, eg, in the RandomFlip layer, the image\n+    and bounding_boxes should be changed in the same way.\n \n     The `call()` method support two formats of inputs:\n     1. Single image tensor with 3D (HWC) or 4D (NHWC) format.\n@@ -267,9 +270,9 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n     The output of the `call()` will be in two formats, which will be the same\n     structure as the inputs.\n \n-    The `call()` will handle the logic detecting the training/inference\n-    mode, unpack the inputs, forward to the correct function, and pack the output\n-    back to the same structure as the inputs.\n+    The `call()` will handle the logic detecting the training/inference mode,\n+    unpack the inputs, forward to the correct function, and pack the output back\n+    to the same structure as the inputs.\n \n     By default the `call()` method leverages the `tf.vectorized_map()` function.\n     Auto-vectorization can be disabled by setting `self.auto_vectorize = False`\n@@ -299,8 +302,8 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n     ```\n \n     Note that since the randomness is also a common functionnality, this layer\n-    also includes a tf.keras.backend.RandomGenerator, which can be used to produce\n-    the random numbers.  The random number generator is stored in the\n+    also includes a tf.keras.backend.RandomGenerator, which can be used to\n+    produce the random numbers.  The random number generator is stored in the\n     `self._random_generator` attribute.\n     \"\"\"\n \n@@ -312,10 +315,10 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n     def auto_vectorize(self):\n         \"\"\"Control whether automatic vectorization occurs.\n \n-        By default the `call()` method leverages the `tf.vectorized_map()` function.\n-        Auto-vectorization can be disabled by setting `self.auto_vectorize = False`\n-        in your `__init__()` method.  When disabled, `call()` instead relies\n-        on `tf.map_fn()`. For example:\n+        By default the `call()` method leverages the `tf.vectorized_map()`\n+        function.  Auto-vectorization can be disabled by setting\n+        `self.auto_vectorize = False` in your `__init__()` method.  When\n+        disabled, `call()` instead relies on `tf.map_fn()`. For example:\n \n         ```python\n         class SubclassLayer(BaseImageAugmentationLayer):\n@@ -342,10 +345,11 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n         \"\"\"Augment a single image during training.\n \n         Args:\n-          image: 3D image input tensor to the layer. Forwarded from `layer.call()`.\n+          image: 3D image input tensor to the layer. Forwarded from\n+            `layer.call()`.\n           transformation: The transformation object produced by\n-            `get_random_transformation`. Used to coordinate the randomness between\n-            image, label and bounding box.\n+            `get_random_transformation`. Used to coordinate the randomness\n+            between image, label and bounding box.\n \n         Returns:\n           output 3D tensor, which will be forward to `layer.call()`.\n@@ -359,8 +363,8 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n         Args:\n           label: 1D label to the layer. Forwarded from `layer.call()`.\n           transformation: The transformation object produced by\n-            `get_random_transformation`. Used to coordinate the randomness between\n-            image, label and bounding box.\n+            `get_random_transformation`. Used to coordinate the randomness\n+            between image, label and bounding box.\n \n         Returns:\n           output 1D tensor, which will be forward to `layer.call()`.\n@@ -374,8 +378,8 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n         Args:\n           target: 1D label to the layer. Forwarded from `layer.call()`.\n           transformation: The transformation object produced by\n-            `get_random_transformation`. Used to coordinate the randomness between\n-            image, label and bounding box.\n+            `get_random_transformation`. Used to coordinate the randomness\n+            between image, label and bounding box.\n \n         Returns:\n           output 1D tensor, which will be forward to `layer.call()`.\n@@ -389,11 +393,13 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n         \"\"\"Augment bounding boxes for one image during training.\n \n         Args:\n-          image: 3D image input tensor to the layer. Forwarded from `layer.call()`.\n-          bounding_boxes: 2D bounding boxes to the layer. Forwarded from `call()`.\n+          image: 3D image input tensor to the layer. Forwarded from\n+            `layer.call()`.\n+          bounding_boxes: 2D bounding boxes to the layer. Forwarded from\n+            `call()`.\n           transformation: The transformation object produced by\n-            `get_random_transformation`. Used to coordinate the randomness between\n-            image, label and bounding box.\n+            `get_random_transformation`. Used to coordinate the randomness\n+            between image, label and bounding box.\n \n         Returns:\n           output 2D tensor, which will be forward to `layer.call()`.\n@@ -406,7 +412,8 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n     ):\n         \"\"\"Produce random transformation config for one single input.\n \n-        This is used to produce same randomness between image/label/bounding_box.\n+        This is used to produce same randomness between\n+        image/label/bounding_box.\n \n         Args:\n           image: 3D image tensor from inputs.\n@@ -509,17 +516,18 @@ class RandomCrop(BaseImageAugmentationLayer):\n     \"\"\"A preprocessing layer which randomly crops images during training.\n \n     During training, this layer will randomly choose a location to crop images\n-    down to a target size. The layer will crop all the images in the same batch to\n-    the same cropping location.\n+    down to a target size. The layer will crop all the images in the same batch\n+    to the same cropping location.\n \n     At inference time, and during training if an input image is smaller than the\n-    target size, the input will be resized and cropped so as to return the largest\n-    possible window in the image that matches the target aspect ratio. If you need\n-    to apply random cropping at inference time, set `training` to True when\n-    calling the layer.\n+    target size, the input will be resized and cropped so as to return the\n+    largest possible window in the image that matches the target aspect ratio.\n+    If you need to apply random cropping at inference time, set `training` to\n+    True when calling the layer.\n \n     Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and\n-    of interger or floating point dtype. By default, the layer will output floats.\n+    of interger or floating point dtype. By default, the layer will output\n+    floats.\n \n     For an overview and full list of preprocessing layers, see the preprocessing\n     [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n@@ -557,7 +565,8 @@ class RandomCrop(BaseImageAugmentationLayer):\n             inputs = self._ensure_inputs_are_compute_dtype(inputs)\n             inputs, is_dict, targets = self._format_inputs(inputs)\n             output = inputs\n-            # self._resize() returns valid results for both batched and unbatched\n+            # self._resize() returns valid results for both batched and\n+            # unbatched\n             output[\"images\"] = self._resize(inputs[\"images\"])\n             return self._format_output(output, is_dict, targets)\n \n@@ -618,16 +627,16 @@ class RandomCrop(BaseImageAugmentationLayer):\n class Rescaling(base_layer.Layer):\n     \"\"\"A preprocessing layer which rescales input values to a new range.\n \n-    This layer rescales every value of an input (often an image) by multiplying by\n-    `scale` and adding `offset`.\n+    This layer rescales every value of an input (often an image) by multiplying\n+    by `scale` and adding `offset`.\n \n     For instance:\n \n     1. To rescale an input in the ``[0, 255]`` range\n     to be in the `[0, 1]` range, you would pass `scale=1./255`.\n \n-    2. To rescale an input in the ``[0, 255]`` range to be in the `[-1, 1]` range,\n-    you would pass `scale=1./127.5, offset=-1`.\n+    2. To rescale an input in the ``[0, 255]`` range to be in the `[-1, 1]`\n+    range, you would pass `scale=1./127.5, offset=-1`.\n \n     The rescaling is applied both during training and inference. Inputs can be\n     of integer or floating point dtype, and by default the layer will output\n@@ -689,7 +698,8 @@ class RandomFlip(BaseImageAugmentationLayer):\n     input. Call the layer with `training=True` to flip the input.\n \n     Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and\n-    of interger or floating point dtype. By default, the layer will output floats.\n+    of interger or floating point dtype. By default, the layer will output\n+    floats.\n \n     For an overview and full list of preprocessing layers, see the preprocessing\n     [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n@@ -813,26 +823,26 @@ class RandomTranslation(BaseImageAugmentationLayer):\n     filling empty space according to `fill_mode`.\n \n     Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and\n-    of interger or floating point dtype. By default, the layer will output floats.\n+    of interger or floating point dtype. By default, the layer will output\n+    floats.\n \n     For an overview and full list of preprocessing layers, see the preprocessing\n     [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n \n     Args:\n-      height_factor: a float represented as fraction of value, or a tuple of size\n-        2 representing lower and upper bound for shifting vertically. A negative\n-        value means shifting image up, while a positive value means shifting image\n-        down. When represented as a single positive float, this value is used for\n-        both the upper and lower bound. For instance, `height_factor=(-0.2, 0.3)`\n-        results in an output shifted by a random amount in the range\n-        `[-20%, +30%]`.\n-        `height_factor=0.2` results in an output height shifted by a random amount\n-        in the range `[-20%, +20%]`.\n-      width_factor: a float represented as fraction of value, or a tuple of size 2\n-        representing lower and upper bound for shifting horizontally. A negative\n-        value means shifting image left, while a positive value means shifting\n-        image right. When represented as a single positive float, this value is\n-        used for both the upper and lower bound. For instance,\n+      height_factor: a float represented as fraction of value, or a tuple of\n+        size 2 representing lower and upper bound for shifting vertically. A\n+        negative value means shifting image up, while a positive value means\n+        shifting image down. When represented as a single positive float, this\n+        value is used for both the upper and lower bound. For instance,\n+        `height_factor=(-0.2, 0.3)` results in an output shifted by a random\n+        amount in the range `[-20%, +30%]`.  `height_factor=0.2` results in an\n+        output height shifted by a random amount in the range `[-20%, +20%]`.\n+      width_factor: a float represented as fraction of value, or a tuple of size\n+        2 representing lower and upper bound for shifting horizontally. A\n+        negative value means shifting image left, while a positive value means\n+        shifting image right. When represented as a single positive float, this\n+        value is used for both the upper and lower bound. For instance,\n         `width_factor=(-0.2, 0.3)` results in an output shifted left by 20%, and\n         shifted right by 30%. `width_factor=0.2` results in an output height\n         shifted left or right by 20%.\n@@ -844,13 +854,13 @@ class RandomTranslation(BaseImageAugmentationLayer):\n           filling all values beyond the edge with the same constant value k = 0.\n         - *wrap*: `(a b c d | a b c d | a b c d)` The input is extended by\n           wrapping around to the opposite edge.\n-        - *nearest*: `(a a a a | a b c d | d d d d)` The input is extended by the\n-          nearest pixel.\n+        - *nearest*: `(a a a a | a b c d | d d d d)` The input is extended by\n+          the nearest pixel.\n       interpolation: Interpolation mode. Supported values: `\"nearest\"`,\n         `\"bilinear\"`.\n       seed: Integer. Used to create a random seed.\n-      fill_value: a float represents the value to be filled outside the boundaries\n-        when `fill_mode=\"constant\"`.\n+      fill_value: a float represents the value to be filled outside the\n+        boundaries when `fill_mode=\"constant\"`.\n \n     Input shape:\n       3D (unbatched) or 4D (batched) tensor with shape:\n@@ -921,8 +931,8 @@ class RandomTranslation(BaseImageAugmentationLayer):\n     @tf.function\n     def augment_image(self, image, transformation):\n         \"\"\"Translated inputs with random ops.\"\"\"\n-        # The transform op only accepts rank 4 inputs, so if we have an unbatched\n-        # image, we need to temporarily expand dims to a batch.\n+        # The transform op only accepts rank 4 inputs, so if we have an\n+        # unbatched image, we need to temporarily expand dims to a batch.\n         original_shape = image.shape\n         inputs = tf.expand_dims(image, 0)\n \n@@ -972,8 +982,8 @@ class RandomTranslation(BaseImageAugmentationLayer):\n         }\n \n     def _batch_augment(self, inputs):\n-        # Change to vectorized_map for better performance, as well as work around\n-        # issue for different tensorspec between inputs and outputs.\n+        # Change to vectorized_map for better performance, as well as work\n+        # around issue for different tensorspec between inputs and outputs.\n         return tf.vectorized_map(self._augment, inputs)\n \n     def augment_label(self, label, transformation):\n@@ -1004,8 +1014,8 @@ def get_translation_matrix(translations, name=None):\n       name: The name of the op.\n \n     Returns:\n-      A tensor of shape `(num_images, 8)` projective transforms which can be given\n-        to `transform`.\n+      A tensor of shape `(num_images, 8)` projective transforms which can be\n+        given to `transform`.\n     \"\"\"\n     with backend.name_scope(name or \"translation_matrix\"):\n         num_translations = tf.shape(translations)[0]\n@@ -1042,19 +1052,20 @@ def transform(\n \n     Args:\n       images: A tensor of shape\n-        `(num_images, num_rows, num_columns, num_channels)` (NHWC). The rank must\n-        be statically known (the shape is not `TensorShape(None)`).\n+        `(num_images, num_rows, num_columns, num_channels)` (NHWC). The rank\n+        must be statically known (the shape is not `TensorShape(None)`).\n       transforms: Projective transform matrix/matrices. A vector of length 8 or\n-        tensor of size N x 8. If one row of transforms is [a0, a1, a2, b0, b1, b2,\n-        c0, c1], then it maps the *output* point `(x, y)` to a transformed *input*\n-        point `(x', y') = ((a0 x + a1 y + a2) / k, (b0 x + b1 y + b2) / k)`, where\n-        `k = c0 x + c1 y + 1`. The transforms are *inverted* compared to the\n-        transform mapping input points to output points. Note that gradients are\n-        not backpropagated into transformation parameters.\n+        tensor of size N x 8. If one row of transforms is [a0, a1, a2, b0, b1,\n+        b2, c0, c1], then it maps the *output* point `(x, y)` to a transformed\n+        *input* point\n+        `(x', y') = ((a0 x + a1 y + a2) / k, (b0 x + b1 y + b2) / k)`,\n+        where `k = c0 x + c1 y + 1`. The transforms are *inverted* compared\n+        to the transform mapping input points to output points. Note that\n+        gradients are not backpropagated into transformation parameters.\n       fill_mode: Points outside the boundaries of the input are filled according\n         to the given mode (one of `{\"constant\", \"reflect\", \"wrap\", \"nearest\"}`).\n-      fill_value: a float represents the value to be filled outside the boundaries\n-        when `fill_mode=\"constant\"`.\n+      fill_value: a float represents the value to be filled outside the\n+        boundaries when `fill_mode=\"constant\"`.\n       interpolation: Interpolation mode. Supported values: `\"nearest\"`,\n         `\"bilinear\"`.\n       output_shape: Output dimension after the transform, `[height, width]`.\n@@ -1130,18 +1141,18 @@ def get_rotation_matrix(angles, image_height, image_width, name=None):\n     \"\"\"Returns projective transform(s) for the given angle(s).\n \n     Args:\n-      angles: A scalar angle to rotate all images by, or (for batches of images) a\n-        vector with an angle to rotate each image in the batch. The rank must be\n-        statically known (the shape is not `TensorShape(None)`).\n+      angles: A scalar angle to rotate all images by, or (for batches of images)\n+        a vector with an angle to rotate each image in the batch. The rank must\n+        be statically known (the shape is not `TensorShape(None)`).\n       image_height: Height of the image(s) to be transformed.\n       image_width: Width of the image(s) to be transformed.\n       name: The name of the op.\n \n     Returns:\n-      A tensor of shape (num_images, 8). Projective transforms which can be given\n-        to operation `image_projective_transform_v2`. If one row of transforms is\n-         [a0, a1, a2, b0, b1, b2, c0, c1], then it maps the *output* point\n-         `(x, y)` to a transformed *input* point\n+      A tensor of shape (num_images, 8). Projective transforms which can be\n+        given to operation `image_projective_transform_v2`. If one row of\n+        transforms is [a0, a1, a2, b0, b1, b2, c0, c1], then it maps the\n+        *output* point `(x, y)` to a transformed *input* point\n         `(x', y') = ((a0 x + a1 y + a2) / k, (b0 x + b1 y + b2) / k)`,\n         where `k = c0 x + c1 y + 1`.\n     \"\"\"\n@@ -1191,7 +1202,8 @@ class RandomRotation(BaseImageAugmentationLayer):\n     rotations at inference time, set `training` to True when calling the layer.\n \n     Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and\n-    of interger or floating point dtype. By default, the layer will output floats.\n+    of interger or floating point dtype. By default, the layer will output\n+    floats.\n \n     For an overview and full list of preprocessing layers, see the preprocessing\n     [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n@@ -1211,8 +1223,9 @@ class RandomRotation(BaseImageAugmentationLayer):\n         while a negative value means clock-wise. When represented as a single\n         float, this value is used for both the upper and lower bound. For\n         instance, `factor=(-0.2, 0.3)` results in an output rotation by a random\n-        amount in the range `[-20% * 2pi, 30% * 2pi]`. `factor=0.2` results in an\n-        output rotating by a random amount in the range `[-20% * 2pi, 20% * 2pi]`.\n+        amount in the range `[-20% * 2pi, 30% * 2pi]`. `factor=0.2` results in\n+        an output rotating by a random amount in the range\n+        `[-20% * 2pi, 20% * 2pi]`.\n       fill_mode: Points outside the boundaries of the input are filled according\n         to the given mode (one of `{\"constant\", \"reflect\", \"wrap\", \"nearest\"}`).\n         - *reflect*: `(d c b a | a b c d | d c b a)` The input is extended by\n@@ -1221,13 +1234,13 @@ class RandomRotation(BaseImageAugmentationLayer):\n           filling all values beyond the edge with the same constant value k = 0.\n         - *wrap*: `(a b c d | a b c d | a b c d)` The input is extended by\n           wrapping around to the opposite edge.\n-        - *nearest*: `(a a a a | a b c d | d d d d)` The input is extended by the\n-          nearest pixel.\n+        - *nearest*: `(a a a a | a b c d | d d d d)` The input is extended by\n+          the nearest pixel.\n       interpolation: Interpolation mode. Supported values: `\"nearest\"`,\n         `\"bilinear\"`.\n       seed: Integer. Used to create a random seed.\n-      fill_value: a float represents the value to be filled outside the boundaries\n-        when `fill_mode=\"constant\"`.\n+      fill_value: a float represents the value to be filled outside the\n+        boundaries when `fill_mode=\"constant\"`.\n     \"\"\"\n \n     def __init__(\n@@ -1295,8 +1308,8 @@ class RandomRotation(BaseImageAugmentationLayer):\n         h = image_shape[H_AXIS]\n         w = image_shape[W_AXIS]\n         bbox_dtype = bounding_boxes.dtype\n-        # origin coordinates, all the points on the image are rotated around this\n-        # point\n+        # origin coordinates, all the points on the image are rotated around\n+        # this point\n         origin_x, origin_y = int(h / 2), int(w / 2)\n         angle = transformation[\"angle\"]\n         angle = -angle\n@@ -1376,22 +1389,23 @@ class RandomZoom(BaseImageAugmentationLayer):\n     independently, filling empty space according to `fill_mode`.\n \n     Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and\n-    of interger or floating point dtype. By default, the layer will output floats.\n+    of interger or floating point dtype. By default, the layer will output\n+    floats.\n \n     For an overview and full list of preprocessing layers, see the preprocessing\n     [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n \n     Args:\n-      height_factor: a float represented as fraction of value, or a tuple of size\n-        2 representing lower and upper bound for zooming vertically. When\n+      height_factor: a float represented as fraction of value, or a tuple of\n+        size 2 representing lower and upper bound for zooming vertically. When\n         represented as a single float, this value is used for both the upper and\n         lower bound. A positive value means zooming out, while a negative value\n         means zooming in. For instance, `height_factor=(0.2, 0.3)` result in an\n         output zoomed out by a random amount in the range `[+20%, +30%]`.\n         `height_factor=(-0.3, -0.2)` result in an output zoomed in by a random\n         amount in the range `[+20%, +30%]`.\n-      width_factor: a float represented as fraction of value, or a tuple of size 2\n-        representing lower and upper bound for zooming horizontally. When\n+      width_factor: a float represented as fraction of value, or a tuple of size\n+        2 representing lower and upper bound for zooming horizontally. When\n         represented as a single float, this value is used for both the upper and\n         lower bound. For instance, `width_factor=(0.2, 0.3)` result in an output\n         zooming out between 20% to 30%. `width_factor=(-0.3, -0.2)` result in an\n@@ -1405,13 +1419,13 @@ class RandomZoom(BaseImageAugmentationLayer):\n           filling all values beyond the edge with the same constant value k = 0.\n         - *wrap*: `(a b c d | a b c d | a b c d)` The input is extended by\n           wrapping around to the opposite edge.\n-        - *nearest*: `(a a a a | a b c d | d d d d)` The input is extended by the\n-          nearest pixel.\n+        - *nearest*: `(a a a a | a b c d | d d d d)` The input is extended by\n+          the nearest pixel.\n       interpolation: Interpolation mode. Supported values: `\"nearest\"`,\n         `\"bilinear\"`.\n       seed: Integer. Used to create a random seed.\n-      fill_value: a float represents the value to be filled outside the boundaries\n-        when `fill_mode=\"constant\"`.\n+      fill_value: a float represents the value to be filled outside the\n+        boundaries when `fill_mode=\"constant\"`.\n \n     Example:\n \n@@ -1547,8 +1561,8 @@ def get_zoom_matrix(zooms, image_height, image_width, name=None):\n     \"\"\"Returns projective transform(s) for the given zoom(s).\n \n     Args:\n-      zooms: A matrix of 2-element lists representing `[zx, zy]` to zoom for each\n-        image (for a batch of images).\n+      zooms: A matrix of 2-element lists representing `[zx, zy]` to zoom for\n+        each image (for a batch of images).\n       image_height: Height of the image(s) to be transformed.\n       image_width: Width of the image(s) to be transformed.\n       name: The name of the op.\n@@ -1594,17 +1608,17 @@ def get_zoom_matrix(zooms, image_height, image_width, name=None):\n class RandomContrast(BaseImageAugmentationLayer):\n     \"\"\"A preprocessing layer which randomly adjusts contrast during training.\n \n-    This layer will randomly adjust the contrast of an image or images by a random\n-    factor. Contrast is adjusted independently for each channel of each image\n-    during training.\n+    This layer will randomly adjust the contrast of an image or images by a\n+    random factor. Contrast is adjusted independently for each channel of each\n+    image during training.\n \n     For each channel, this layer computes the mean of the image pixels in the\n     channel and then adjusts each component `x` of each pixel to\n     `(x - mean) * contrast_factor + mean`.\n \n     Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and\n-    in integer or floating point dtype. By default, the layer will output floats.\n-    The output value will be clipped to the range `[0, 255]`, the valid\n+    in integer or floating point dtype. By default, the layer will output\n+    floats. The output value will be clipped to the range `[0, 255]`, the valid\n     range of RGB colors.\n \n     For an overview and full list of preprocessing layers, see the preprocessing\n@@ -1621,10 +1635,10 @@ class RandomContrast(BaseImageAugmentationLayer):\n     Arguments:\n       factor: a positive float represented as fraction of value, or a tuple of\n         size 2 representing lower and upper bound. When represented as a single\n-        float, lower = upper. The contrast factor will be randomly picked between\n-        `[1.0 - lower, 1.0 + upper]`. For any pixel x in the channel, the output\n-        will be `(x - mean) * factor + mean` where `mean` is the mean value of the\n-        channel.\n+        float, lower = upper. The contrast factor will be randomly picked\n+        between `[1.0 - lower, 1.0 + upper]`. For any pixel x in the channel,\n+        the output will be `(x - mean) * factor + mean` where `mean` is the mean\n+        value of the channel.\n       seed: Integer. Used to create a random seed.\n     \"\"\"\n \n@@ -1704,10 +1718,10 @@ class RandomBrightness(BaseImageAugmentationLayer):\n         is provided, eg, 0.2, then -0.2 will be used for lower bound and 0.2\n         will be used for upper bound.\n       value_range: Optional list/tuple of 2 floats for the lower and upper limit\n-        of the values of the input data. Defaults to [0.0, 255.0]. Can be changed\n-        to e.g. [0.0, 1.0] if the image input has been scaled before this layer.\n-        The brightness adjustment will be scaled to this range, and the\n-        output values will be clipped to this range.\n+        of the values of the input data. Defaults to [0.0, 255.0]. Can be\n+        changed to e.g. [0.0, 1.0] if the image input has been scaled before\n+        this layer.  The brightness adjustment will be scaled to this range, and\n+        the output values will be clipped to this range.\n       seed: optional integer, for fixed RNG behavior.\n \n     Inputs: 3D (HWC) or 4D (NHWC) tensor, with float or int dtype. Input pixel\n@@ -1854,14 +1868,15 @@ class RandomHeight(BaseImageAugmentationLayer):\n     [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n \n     Args:\n-      factor: A positive float (fraction of original height), or a tuple of size 2\n-        representing lower and upper bound for resizing vertically. When\n+      factor: A positive float (fraction of original height), or a tuple of size\n+        2 representing lower and upper bound for resizing vertically. When\n         represented as a single float, this value is used for both the upper and\n         lower bound. For instance, `factor=(0.2, 0.3)` results in an output with\n         height changed by a random amount in the range `[20%, 30%]`.\n-        `factor=(-0.2, 0.3)` results in an output with height changed by a random\n-        amount in the range `[-20%, +30%]`. `factor=0.2` results in an output with\n-        height changed by a random amount in the range `[-20%, +20%]`.\n+        `factor=(-0.2, 0.3)` results in an output with height changed by a\n+        random amount in the range `[-20%, +30%]`. `factor=0.2` results in an\n+        output with height changed by a random amount in the range\n+        `[-20%, +20%]`.\n       interpolation: String, the interpolation method. Defaults to `\"bilinear\"`.\n         Supports `\"bilinear\"`, `\"nearest\"`, `\"bicubic\"`, `\"area\"`,\n         `\"lanczos3\"`, `\"lanczos5\"`, `\"gaussian\"`, `\"mitchellcubic\"`.\n@@ -1928,8 +1943,8 @@ class RandomHeight(BaseImageAugmentationLayer):\n         return result\n \n     def augment_image(self, image, transformation):\n-        # The batch dimension of the input=image is not modified. The output would\n-        # be accurate for both unbatched and batched input\n+        # The batch dimension of the input=image is not modified. The output\n+        # would be accurate for both unbatched and batched input\n         inputs_shape = tf.shape(image)\n         img_wd = inputs_shape[W_AXIS]\n         adjusted_height = transformation[\"height\"]\n@@ -1970,8 +1985,8 @@ class RandomWidth(BaseImageAugmentationLayer):\n     This layer will randomly adjusts the width of a batch of images of a\n     batch of images by a random factor. The input should be a 3D (unbatched) or\n     4D (batched) tensor in the `\"channels_last\"` image data format. Input pixel\n-    values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and of interger or\n-    floating point dtype. By default, the layer will output floats.\n+    values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and of interger\n+    or floating point dtype. By default, the layer will output floats.\n \n     By default, this layer is inactive during inference.\n \n@@ -1979,14 +1994,14 @@ class RandomWidth(BaseImageAugmentationLayer):\n     [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n \n     Args:\n-      factor: A positive float (fraction of original width), or a tuple of size 2\n-        representing lower and upper bound for resizing vertically. When\n+      factor: A positive float (fraction of original width), or a tuple of size\n+        2 representing lower and upper bound for resizing vertically. When\n         represented as a single float, this value is used for both the upper and\n         lower bound. For instance, `factor=(0.2, 0.3)` results in an output with\n-        width changed by a random amount in the range `[20%, 30%]`. `factor=(-0.2,\n-        0.3)` results in an output with width changed by a random amount in the\n-        range `[-20%, +30%]`. `factor=0.2` results in an output with width changed\n-        by a random amount in the range `[-20%, +20%]`.\n+        width changed by a random amount in the range `[20%, 30%]`.\n+        `factor=(-0.2, 0.3)` results in an output with width changed by a random\n+        amount in the range `[-20%, +30%]`. `factor=0.2` results in an output\n+        with width changed by a random amount in the range `[-20%, +20%]`.\n       interpolation: String, the interpolation method. Defaults to `bilinear`.\n         Supports `\"bilinear\"`, `\"nearest\"`, `\"bicubic\"`, `\"area\"`, `\"lanczos3\"`,\n         `\"lanczos5\"`, `\"gaussian\"`, `\"mitchellcubic\"`.\n@@ -2040,8 +2055,8 @@ class RandomWidth(BaseImageAugmentationLayer):\n         return result\n \n     def augment_image(self, image, transformation):\n-        # The batch dimension of the input=image is not modified. The output would\n-        # be accurate for both unbatched and batched input\n+        # The batch dimension of the input=image is not modified. The output\n+        # would be accurate for both unbatched and batched input\n         inputs = utils.ensure_tensor(image)\n         inputs_shape = tf.shape(inputs)\n         img_hd = inputs_shape[H_AXIS]\n\n@@ -323,7 +323,8 @@ class CenterCropTest(test_combinations.TestCase):\n         with test_utils.use_gpu():\n             layer = image_preprocessing.CenterCrop(height, width)\n             actual_output = layer(inp)\n-            # In this case, output should equal resizing with crop_to_aspect ratio.\n+            # In this case, output should equal resizing with crop_to_aspect\n+            # ratio.\n             resize_layer = image_preprocessing.Resizing(\n                 height, width, crop_to_aspect_ratio=True\n             )\n@@ -390,7 +391,8 @@ class RandomCropTest(test_combinations.TestCase):\n         with test_utils.use_gpu():\n             layer = image_preprocessing.RandomCrop(height, width)\n             actual_output = layer(inp)\n-            # In this case, output should equal resizing with crop_to_aspect ratio.\n+            # In this case, output should equal resizing with crop_to_aspect\n+            # ratio.\n             resize_layer = image_preprocessing.Resizing(\n                 height, width, crop_to_aspect_ratio=True\n             )\n@@ -845,7 +847,8 @@ class RandomContrastTest(test_combinations.TestCase):\n \n     def test_output_value_clip(self):\n         input_images = np.random.random((5, 8, 3)).astype(np.float32) * 255.0\n-        # Give a factor range [1.0, 11.0] so that it will produce large contrast.\n+        # Give a factor range [1.0, 11.0] so that it will produce large\n+        # contrast.\n         layer = image_preprocessing.RandomContrast((0.0, 10.0))\n         output = layer(input_images)\n         self.assertLessEqual(tf.reduce_max(output), 255.0)\n@@ -1936,7 +1939,8 @@ class RandomRotationTest(test_combinations.TestCase):\n             self.assertAllClose(expected_output, actual_output)\n \n     def test_distribution_strategy(self):\n-        \"\"\"Tests that RandomRotation can be created within distribution strategies.\"\"\"\n+        \"\"\"Tests that RandomRotation can be created within distribution\n+        strategies.\"\"\"\n         input_images = np.random.random((2, 5, 8, 3)).astype(np.float32)\n         with test_utils.use_gpu():\n             strat = tf.distribute.MirroredStrategy(devices=[\"cpu\", \"gpu\"])\n@@ -2256,8 +2260,9 @@ class RandomHeightTest(test_combinations.TestCase):\n                     dtype\n                 )\n                 layer = image_preprocessing.RandomHeight(factor=(1.0, 1.0))\n-                # Return type of RandomHeight() is float32 if `interpolation` is not\n-                # set to `ResizeMethod.NEAREST_NEIGHBOR`; cast `layer` to desired dtype.\n+                # Return type of RandomHeight() is float32 if `interpolation` is\n+                # not set to `ResizeMethod.NEAREST_NEIGHBOR`; cast `layer` to\n+                # desired dtype.\n                 output_image = tf.cast(\n                     layer(np.expand_dims(input_image, axis=0)), dtype=dtype\n                 )\n@@ -2412,8 +2417,9 @@ class RandomWidthTest(test_combinations.TestCase):\n                     dtype\n                 )\n                 layer = image_preprocessing.RandomWidth(factor=(1.0, 1.0))\n-                # Return type of RandomWidth() is float32 if `interpolation` is not\n-                # set to `ResizeMethod.NEAREST_NEIGHBOR`; cast `layer` to desired dtype.\n+                # Return type of RandomWidth() is float32 if `interpolation` is\n+                # not set to `ResizeMethod.NEAREST_NEIGHBOR`; cast `layer` to\n+                # desired dtype.\n                 output_image = tf.cast(\n                     layer(np.expand_dims(input_image, axis=0)), dtype=dtype\n                 )\n\n@@ -96,9 +96,9 @@ class VocabWeightHandler(base_layer_utils.TrackableWeightHandler):\n class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n     \"\"\"Maps values from a vocabulary to integer indices.\n \n-    This layer translates a set of arbitrary hashables into an integer output via\n-    a table-based lookup, with optional out-of-vocabulary handling. This is the\n-    basis layer for both IntegerLookup and StringLookup; it holds the common\n+    This layer translates a set of arbitrary hashables into an integer output\n+    via a table-based lookup, with optional out-of-vocabulary handling. This is\n+    the basis layer for both IntegerLookup and StringLookup; it holds the common\n     logic but is not intended to be exported as part of the Keras API.\n \n     Args:\n@@ -106,13 +106,14 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n         there is no cap on the size of the vocabulary. Note that this size\n         includes the OOV and mask tokens.\n       num_oov_indices: The number of out-of-vocabulary tokens to use. If this\n-        value is more than 1, OOV inputs are hashed to determine their OOV value.\n-        If this value is 0, OOV inputs will cause an error when calling the layer.\n+        value is more than 1, OOV inputs are hashed to determine their OOV\n+        value. If this value is 0, OOV inputs will cause an error when calling\n+        the layer.\n       mask_token: A token that represents masked inputs. When `output_mode` is\n         `\"int\"`, the token is included in vocabulary and mapped to index 0. In\n         other output modes, the token will not appear in the vocabulary and\n-        instances of the mask token in the input will be dropped. If set to None,\n-        no mask term will be added.\n+        instances of the mask token in the input will be dropped. If set to\n+        None, no mask term will be added.\n       oov_token: Only used when `invert` is True. The token to return for OOV\n         indices.\n       vocabulary: Optional. Either an array or a string path to a text file. If\n@@ -120,43 +121,44 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n         containing the vocbulary terms. If passing a file path, the file should\n         contain one line per term in the vocabulary. If this argument is set,\n         there is no need to `adapt` the layer.\n-      vocabulary_dtype: The dtype of the vocabulary terms. For example, `\"int64\"`\n-        or `\"string\"`.\n-      idf_weights: Only valid when `output_mode` is `\"tf_idf\"`. A tuple, list, 1D\n-        numpy array, or 1D tensor or the same length as the vocabulary, containing\n-        the floating point inverse document frequency weights, which will be\n-        multiplied by per sample term counts for the final `tf_idf` weight. If the\n-        `vocabulary` argument is set, and `output_mode` is `\"tf_idf\"`, this\n-        argument must be supplied.\n+      vocabulary_dtype: The dtype of the vocabulary terms. For example,\n+        `\"int64\"` or `\"string\"`.\n+      idf_weights: Only valid when `output_mode` is `\"tf_idf\"`. A tuple, list,\n+        1D numpy array, or 1D tensor or the same length as the vocabulary,\n+        containing the floating point inverse document frequency weights, which\n+        will be multiplied by per sample term counts for the final `tf_idf`\n+        weight. If the `vocabulary` argument is set, and `output_mode` is\n+        `\"tf_idf\"`, this argument must be supplied.\n       invert: Only valid when `output_mode` is `\"int\"`. If True, this layer will\n         map indices to vocabulary items instead of mapping vocabulary items to\n         indices. Default to False.\n-      output_mode: Specification for the output of the layer. Defaults to `\"int\"`.\n-        Values can be `\"int\"`, `\"one_hot\"`, `\"multi_hot\"`, `\"count\"`, or\n-        `\"tf_idf\"` configuring the layer as follows:\n+      output_mode: Specification for the output of the layer. Defaults to\n+        `\"int\"`.  Values can be `\"int\"`, `\"one_hot\"`, `\"multi_hot\"`, `\"count\"`,\n+        or `\"tf_idf\"` configuring the layer as follows:\n           - `\"int\"`: Return the raw integer indices of the input tokens.\n           - `\"one_hot\"`: Encodes each individual element in the input into an\n             array the same size as the vocabulary, containing a 1 at the element\n-            index. If the last dimension is size 1, will encode on that dimension.\n-            If the last dimension is not size 1, will append a new dimension for\n-            the encoded output.\n+            index. If the last dimension is size 1, will encode on that\n+            dimension.  If the last dimension is not size 1, will append a new\n+            dimension for the encoded output.\n           - `\"multi_hot\"`: Encodes each sample in the input into a single array\n             the same size as the vocabulary, containing a 1 for each vocabulary\n             term present in the sample. Treats the last dimension as the sample\n             dimension, if input shape is (..., sample_length), output shape will\n             be (..., num_tokens).\n-          - `\"count\"`: As `\"multi_hot\"`, but the int array contains a count of the\n-            number of times the token at that index appeared in the sample.\n+          - `\"count\"`: As `\"multi_hot\"`, but the int array contains a count of\n+            the number of times the token at that index appeared in the sample.\n           - `\"tf_idf\"`: As `\"multi_hot\"`, but the TF-IDF algorithm is applied to\n             find the value in each token slot.\n       pad_to_max_tokens: Only valid when `output_mode` is `\"multi_hot\"`,\n         `\"count\"`, or `\"tf_idf\"`. If True, the output will have its feature axis\n         padded to `max_tokens` even if the number of unique tokens in the\n         vocabulary is less than max_tokens, resulting in a tensor of shape\n-        [batch_size, max_tokens] regardless of vocabulary size. Defaults to False.\n+        [batch_size, max_tokens] regardless of vocabulary size. Defaults to\n+        False.\n       sparse: Boolean. Only applicable to `\"one_hot\"`, `\"multi_hot\"`, `\"count\"`\n-        and `\"tf-idf\"` output modes. If True, returns a `SparseTensor` instead of\n-        a dense `Tensor`. Defaults to False.\n+        and `\"tf-idf\"` output modes. If True, returns a `SparseTensor` instead\n+        of a dense `Tensor`. Defaults to False.\n     \"\"\"\n \n     def __init__(\n@@ -241,9 +243,10 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n \n         self.input_vocabulary = vocabulary\n         self.input_idf_weights = idf_weights\n-        # VocabularySavedModelSaver will clear the config vocabulary to restore the\n-        # lookup table ops directly. We persist this hidden option to persist the\n-        # fact that we have have a non-adaptable layer with a manually set vocab.\n+        # VocabularySavedModelSaver will clear the config vocabulary to restore\n+        # the lookup table ops directly. We persist this hidden option to\n+        # persist the fact that we have have a non-adaptable layer with a\n+        # manually set vocab.\n         self._has_input_vocabulary = kwargs.pop(\n             \"has_input_vocabulary\", (vocabulary is not None)\n         )\n@@ -281,22 +284,22 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n             self._key_dtype = tf.as_dtype(self.vocabulary_dtype)\n             self._value_dtype = self.dtype if output_mode == INT else tf.int64\n             mask_key = mask_token\n-            # Masks should map to 0 for int output and be dropped otherwise. Max ints\n-            # will be dropped from the bincount op.\n+            # Masks should map to 0 for int output and be dropped otherwise. Max\n+            # ints will be dropped from the bincount op.\n             mask_value = 0 if self.output_mode == INT else self._value_dtype.max\n             if self.num_oov_indices == 0:\n-                # If there are no OOV indices, we map OOV tokens to -1 and error out\n-                # during call if we find a negative index.\n+                # If there are no OOV indices, we map OOV tokens to -1 and error\n+                # out during call if we find a negative index.\n                 self._default_value = -1\n             elif self.num_oov_indices == 1:\n-                # If there is only one OOV index, we can set that index as the default\n-                # value of the index_lookup table.\n+                # If there is only one OOV index, we can set that index as the\n+                # default value of the index_lookup table.\n                 self._default_value = self._oov_start_index()\n             else:\n-                # If we have multiple OOV values, we need to do a further hashing step;\n-                # to make this easier, we set the OOV value to -1. (This lets us do a\n-                # vectorized add and cast to boolean to determine locations where we\n-                # need to do extra hashing.)\n+                # If we have multiple OOV values, we need to do a further\n+                # hashing step; to make this easier, we set the OOV value to -1.\n+                # (This lets us do a vectorized add and cast to boolean to\n+                # determine locations where we need to do extra hashing.)\n                 self._default_value = -1\n         if self.mask_token is not None:\n             self._mask_key = tf.convert_to_tensor(mask_key, self._key_dtype)\n@@ -316,14 +319,16 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n         if vocabulary is not None:\n             self.set_vocabulary(vocabulary, idf_weights)\n         else:\n-            # When restoring from a keras SavedModel, the loading code will expect to\n-            # find and restore a lookup_table attribute on the layer. This table needs\n-            # to be uninitialized as a StaticHashTable cannot be initialized twice.\n+            # When restoring from a keras SavedModel, the loading code will\n+            # expect to find and restore a lookup_table attribute on the layer.\n+            # This table needs to be uninitialized as a StaticHashTable cannot\n+            # be initialized twice.\n             self.lookup_table = self._uninitialized_lookup_table()\n \n         # Only set up adapt state if we did not receive a vocab on construction.\n         if not self._has_input_vocabulary:\n-            # Add a custom weight handler to return the layers vocab as it's weight.\n+            # Add a custom weight handler to return the layers vocab as it's\n+            # weight.\n             self._add_trackable(VocabWeightHandler(self), False)\n             # Set adapt state.\n             self.token_counts = tf.lookup.experimental.MutableHashTable(\n@@ -364,13 +369,14 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n         \"\"\"Returns the current vocabulary of the layer.\n \n         Args:\n-          include_special_tokens: If True, the returned vocabulary will include mask\n-            and OOV tokens, and a term's index in the vocabulary will equal the\n-            term's index when calling the layer. If False, the returned vocabulary\n-            will not include any mask or OOV tokens.\n+          include_special_tokens: If True, the returned vocabulary will include\n+            mask and OOV tokens, and a term's index in the vocabulary will equal\n+            the term's index when calling the layer. If False, the returned\n+            vocabulary will not include any mask or OOV tokens.\n         \"\"\"\n         # The lookup table data will not be sorted, so we will create a inverted\n-        # lookup here, and use that to lookup a range of indices [0, vocab_size).\n+        # lookup here, and use that to lookup a range of indices [0,\n+        # vocab_size).\n         if self.lookup_table.size() == 0:\n             vocab, indices = [], []\n         else:\n@@ -394,7 +400,8 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n         \"\"\"Gets the current size of the layer's vocabulary.\n \n         Returns:\n-          The integer size of the vocabulary, including optional mask and oov indices.\n+          The integer size of the vocabulary, including optional mask and oov\n+          indices.\n         \"\"\"\n         if tf.executing_eagerly():\n             return (\n@@ -430,28 +437,29 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n         \"\"\"Sets vocabulary (and optionally document frequency) data for this layer.\n \n         This method sets the vocabulary and idf weights for this layer directly,\n-        instead of analyzing a dataset through `adapt`. It should be used whenever\n-        the vocab (and optionally document frequency) information is already known.\n-        If vocabulary data is already present in the layer, this method will replace\n-        it.\n+        instead of analyzing a dataset through `adapt`. It should be used\n+        whenever the vocab (and optionally document frequency) information is\n+        already known.  If vocabulary data is already present in the layer, this\n+        method will replace it.\n \n         Args:\n-          vocabulary: Either an array or a string path to a text file. If passing an\n-            array, can pass a tuple, list, 1D numpy array, or 1D tensor containing\n-            the vocbulary terms. If passing a file path, the file should contain one\n-            line per term in the vocabulary.\n+          vocabulary: Either an array or a string path to a text file. If\n+            passing an array, can pass a tuple, list, 1D numpy array, or 1D\n+            tensor containing the vocbulary terms. If passing a file path, the\n+            file should contain one line per term in the vocabulary.\n           idf_weights: A tuple, list, 1D numpy array, or 1D tensor of inverse\n-            document frequency weights with equal length to vocabulary. Must be set\n-            if `output_mode` is `\"tf_idf\"`. Should not be set otherwise.\n+            document frequency weights with equal length to vocabulary. Must be\n+            set if `output_mode` is `\"tf_idf\"`. Should not be set otherwise.\n \n         Raises:\n           ValueError: If there are too many inputs, the inputs do not match, or\n             input data is missing.\n           RuntimeError: If the vocabulary cannot be set when this function is\n             called. This happens when `\"multi_hot\"`, `\"count\"`, and `\"tf_idf\"`\n-            modes, if `pad_to_max_tokens` is False and the layer itself has already\n-            been called.\n-          RuntimeError: If a tensor vocabulary is passed outside of eager execution.\n+            modes, if `pad_to_max_tokens` is False and the layer itself has\n+            already been called.\n+          RuntimeError: If a tensor vocabulary is passed outside of eager\n+            execution.\n         \"\"\"\n         if self.output_mode != TF_IDF and idf_weights is not None:\n             raise ValueError(\n@@ -477,15 +485,15 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n             tf.is_tensor(vocabulary) or tf.is_tensor(idf_weights)\n         ):\n             raise RuntimeError(\n-                \"Cannot set a tensor vocabulary on {} layer {} when not executing \"\n-                \"eagerly. Create this layer or call `set_vocabulary` outside of \"\n-                \"any `tf.function`s and with eager execution enabled.\".format(\n-                    self.__class__.__name__, self.name\n-                )\n+                \"Cannot set a tensor vocabulary on {} layer {} when not \"\n+                \"executing eagerly. Create this layer or call `set_vocabulary` \"\n+                \"outside of any `tf.function`s and with eager execution \"\n+                \"enabled.\".format(self.__class__.__name__, self.name)\n             )\n \n-        # TODO(mattdangerw): for better performance we should rewrite this entire\n-        # function to operate on tensors and convert vocabulary to a tensor here.\n+        # TODO(mattdangerw): for better performance we should rewrite this\n+        # entire function to operate on tensors and convert vocabulary to a\n+        # tensor here.\n         if tf.is_tensor(vocabulary):\n             vocabulary = self._tensor_vocab_to_numpy(vocabulary)\n         elif isinstance(vocabulary, (list, tuple)):\n@@ -526,11 +534,12 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n         if self.mask_token is not None and self.mask_token in tokens:\n             mask_index = np.argwhere(vocabulary == self.mask_token)[-1]\n             raise ValueError(\n-                \"Found reserved mask token at unexpected location in `vocabulary`. \"\n-                \"Note that passed `vocabulary` does not need to include the OOV and \"\n-                \"mask tokens. Either remove all mask and OOV tokens, or include them \"\n-                \"only at the start of the vocabulary in precisely this order: \"\n-                f\"{special_tokens}. Received: mask_token={self.mask_token} at \"\n+                \"Found reserved mask token at unexpected location in \"\n+                \"`vocabulary`. Note that passed `vocabulary` does not need to \"\n+                \"include the OOV and mask tokens. Either remove all mask and \"\n+                \"OOV tokens, or include them only at the start of the \"\n+                f\"vocabulary in precisely this order: {special_tokens}. \"\n+                f\"Received: mask_token={self.mask_token} at \"\n                 f\"vocabulary index {mask_index}\"\n             )\n         # Only error out for oov_token when invert=True. When invert=False,\n@@ -542,19 +551,20 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n         ):\n             oov_index = np.argwhere(vocabulary == self.oov_token)[-1]\n             raise ValueError(\n-                \"Found reserved OOV token at unexpected location in `vocabulary`. \"\n-                \"Note that passed `vocabulary` does not need to include the OOV and \"\n-                \"mask tokens. Either remove all mask and OOV tokens, or include them \"\n-                \"only at the start of the vocabulary in precisely this order: \"\n-                f\"{special_tokens}. Received: oov_token={self.oov_token} at \"\n+                \"Found reserved OOV token at unexpected location in \"\n+                \"`vocabulary`. Note that passed `vocabulary` does not need to \"\n+                \"include the OOV and mask tokens. Either remove all mask and \"\n+                \"OOV tokens, or include them only at the start of the \"\n+                f\"vocabulary in precisely this order: {special_tokens}. \"\n+                f\"Received: oov_token={self.oov_token} at \"\n                 f\"vocabulary index {oov_index}\"\n             )\n \n         new_vocab_size = token_start + len(tokens)\n         if self.max_tokens is not None and (new_vocab_size > self.max_tokens):\n             raise ValueError(\n-                \"Attempted to set a vocabulary larger than the maximum vocab size. \"\n-                \"Passed vocab size is {}, max vocab size is {}.\".format(\n+                \"Attempted to set a vocabulary larger than the maximum vocab \"\n+                \"size. Passed vocab size is {}, max vocab size is {}.\".format(\n                     new_vocab_size, self.max_tokens\n                 )\n             )\n@@ -575,23 +585,23 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n             idf_weights = self._convert_to_ndarray(idf_weights)\n             if idf_weights.ndim != 1:\n                 raise ValueError(\n-                    \"TF-IDF data must be a 1-index array, but received {}\".format(\n-                        type(idf_weights)\n-                    )\n+                    \"TF-IDF data must be a 1-index array, \"\n+                    \"but received {}\".format(type(idf_weights))\n                 )\n \n-            # If the passed vocabulary has no special tokens, we need to pad the front\n-            # of idf_weights. We don't have real document frequencies for these tokens\n-            # so we will use an average of all idf_weights passed in as a reasonable\n-            # default.\n+            # If the passed vocabulary has no special tokens, we need to pad the\n+            # front of idf_weights. We don't have real document frequencies for\n+            # these tokens so we will use an average of all idf_weights passed\n+            # in as a reasonable default.\n             if found_special_tokens:\n                 front_padding = 0\n                 front_padding_value = 0\n             else:\n                 front_padding = token_start\n                 front_padding_value = np.average(idf_weights)\n-            # If pad_to_max_tokens is true, and max_tokens is greater than our total\n-            # vocab size, we need to pad the back of idf_weights with zeros as well.\n+            # If pad_to_max_tokens is true, and max_tokens is greater than our\n+            # total vocab size, we need to pad the back of idf_weights with\n+            # zeros as well.\n             back_padding_value = 0\n             if self.pad_to_max_tokens and self.max_tokens is not None:\n                 back_padding = (\n@@ -612,15 +622,17 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n     def update_state(self, data):\n         if self._has_input_vocabulary:\n             raise ValueError(\n-                \"Cannot adapt {} layer after setting a static vocabulary via init \"\n-                \"argument or `set_vocabulary`.\".format(self.__class__.__name__)\n+                \"Cannot adapt {} layer after setting a static vocabulary via \"\n+                \"init argument \"\n+                \"or `set_vocabulary`.\".format(self.__class__.__name__)\n             )\n \n         data = utils.ensure_tensor(data, dtype=self.vocabulary_dtype)\n         if data.shape.rank == 0:\n             data = tf.expand_dims(data, 0)\n         if data.shape.rank == 1:\n-            # Expand dims on axis 0 for tf-idf. A 1-d tensor is a single document.\n+            # Expand dims on axis 0 for tf-idf. A 1-d tensor is a single\n+            # document.\n             data = tf.expand_dims(data, 0)\n \n         tokens, counts = self._num_tokens(data)\n@@ -662,9 +674,9 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n             )\n \n         tokens, counts = self.token_counts.export()\n-        # To keep vocabs deterministic, we sort our tokens by count and break ties\n-        # by sorting the tokens themselves. Tensorflow has no ops for sorting\n-        # strings, so we need to use numpy for the sort.\n+        # To keep vocabs deterministic, we sort our tokens by count and break\n+        # ties by sorting the tokens themselves. Tensorflow has no ops for\n+        # sorting strings, so we need to use numpy for the sort.\n         sorted_indices = np.lexsort((tokens.numpy(), counts.numpy()))[::-1]\n         token_start = self._token_start_index()\n         if self.max_tokens:\n@@ -679,8 +691,9 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n                 token_document_counts, self.num_documents\n             )\n             idf_weights = tf.cast(idf_weights, self.compute_dtype)\n-            # Pad the front of idf_weights with the average idf weight for OOV tokens.\n-            # We cannot compute the real idf weight of OOV in a single pass.\n+            # Pad the front of idf_weights with the average idf weight for OOV\n+            # tokens.  We cannot compute the real idf weight of OOV in a single\n+            # pass.\n             idf_weights = tf.pad(\n                 idf_weights,\n                 [[self._token_start_index(), 0]],\n@@ -696,8 +709,9 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n             self.idf_weights.assign(idf_weights)\n             self.idf_weights_const = self.idf_weights.value()\n \n-        # We call this here to save memory, now that we've built our vocabulary, we\n-        # don't want to keep every token we've seen in separate lookup tables.\n+        # We call this here to save memory, now that we've built our vocabulary,\n+        # we don't want to keep every token we've seen in separate lookup\n+        # tables.\n         self.reset_state()\n \n     def reset_state(self):  # pylint: disable=method-hidden\n@@ -756,11 +770,11 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n \n     def _lookup_dense(self, inputs):\n         \"\"\"Lookup table values for a dense Tensor, handling masking and OOV.\"\"\"\n-        # When executing eagerly and tracing keras.Inputs, do not call lookup. This\n-        # is critical for restoring SavedModel, which will first trace layer.call\n-        # and then attempt to restore the table. We need the table to be\n-        # uninitialized for the restore to work, but calling the table uninitialized\n-        # would error.\n+        # When executing eagerly and tracing keras.Inputs, do not call lookup.\n+        # This is critical for restoring SavedModel, which will first trace\n+        # layer.call and then attempt to restore the table. We need the table to\n+        # be uninitialized for the restore to work, but calling the table\n+        # uninitialized would error.\n         if tf.executing_eagerly() and backend.is_keras_tensor(inputs):\n             lookups = tf.zeros_like(inputs, dtype=self._value_dtype)\n         else:\n@@ -863,26 +877,25 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n         with tf.init_scope():\n             if not tf.executing_eagerly():\n                 raise RuntimeError(\n-                    \"When using `output_mode={}` eager execution must be enabled.\".format(\n-                        self.output_mode\n-                    )\n+                    \"When using `output_mode={}` eager execution must \"\n+                    \"be enabled.\".format(self.output_mode)\n                 )\n             new_vocab_size = self.vocabulary_size()\n         if new_vocab_size == self._token_start_index():\n             raise RuntimeError(\n-                \"When using `output_mode={}` and `pad_to_max_tokens=False`, you \"\n-                \"must set the layer's vocabulary before calling it. Either pass \"\n-                \"a `vocabulary` argument to the layer, or call `adapt` with some \"\n-                \"sample data.\".format(self.output_mode)\n+                \"When using `output_mode={}` and `pad_to_max_tokens=False`, \"\n+                \"you must set the layer's vocabulary before calling it. Either \"\n+                \"pass a `vocabulary` argument to the layer, or call `adapt` \"\n+                \"with some sample data.\".format(self.output_mode)\n             )\n         elif (\n             self._frozen_vocab_size is not None\n             and new_vocab_size != self._frozen_vocab_size\n         ):\n             raise RuntimeError(\n-                \"When using `output_mode={}` and `pad_to_max_tokens=False`, the \"\n-                \"vocabulary size cannot be changed after the layer is called. \"\n-                \"Vocab size is {}, new vocab size is {}\".format(\n+                \"When using `output_mode={}` and `pad_to_max_tokens=False`, \"\n+                \"the vocabulary size cannot be changed after the layer is \"\n+                \"called. Vocab size is {}, new vocab size is {}\".format(\n                     self.output_mode, self._frozen_vocab_size, new_vocab_size\n                 )\n             )\n@@ -918,8 +931,8 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n         https://en.wikipedia.org/wiki/Tf%E2%80%93idf.\n \n         Args:\n-          token_document_counts: An array of the # of documents each token appears\n-            in.\n+          token_document_counts: An array of the # of documents each token\n+            appears in.\n           num_documents: An int representing the total number of documents\n \n         Returns:\n\n@@ -144,7 +144,8 @@ class IndexLookupDistributionTest(\n         self.assertAllEqual(expected_output, output_dataset)\n \n     def test_tpu_with_multiple_oov(self, strategy):\n-        # TODO(b/180614455): remove this check when MLIR bridge is always enabled.\n+        # TODO(b/180614455): remove this check when MLIR bridge is always\n+        # enabled.\n         if backend.is_tpu_strategy(strategy):\n             self.skipTest(\"This test needs MLIR bridge on TPU.\")\n \n\n@@ -41,8 +41,8 @@ def _get_end_to_end_test_cases():\n     test_cases = (\n         {\n             \"testcase_name\": \"test_strings_soft_vocab_cap\",\n-            # Create an array where 'earth' is the most frequent term, followed by\n-            # 'wind', then 'and', then 'fire'. This ensures that the vocab\n+            # Create an array where 'earth' is the most frequent term, followed\n+            # by 'wind', then 'and', then 'fire'. This ensures that the vocab\n             # accumulator is sorting by frequency.\n             \"vocab_data\": np.array(\n                 [\n@@ -82,8 +82,8 @@ def _get_end_to_end_test_cases():\n         },\n         {\n             \"testcase_name\": \"test_inverse_strings_soft_vocab_cap\",\n-            # Create an array where 'earth' is the most frequent term, followed by\n-            # 'wind', then 'and', then 'fire'. This ensures that the vocab\n+            # Create an array where 'earth' is the most frequent term, followed\n+            # by 'wind', then 'and', then 'fire'. This ensures that the vocab\n             # accumulator is sorting by frequency.\n             \"vocab_data\": np.array(\n                 [\n@@ -124,8 +124,8 @@ def _get_end_to_end_test_cases():\n         },\n         {\n             \"testcase_name\": \"test_strings_with_special_tokens\",\n-            # Mask and oov values in the vocab data should be dropped, and mapped\n-            # to 0 and 1 respectively when calling the layer.\n+            # Mask and oov values in the vocab data should be dropped, and\n+            # mapped to 0 and 1 respectively when calling the layer.\n             \"vocab_data\": np.array(\n                 [\n                     [\"fire\"],\n@@ -217,8 +217,8 @@ def _get_end_to_end_test_cases():\n         },\n         {\n             \"testcase_name\": \"test_ints_with_special_tokens\",\n-            # Mask and oov values in the vocab data should be dropped, and mapped\n-            # to 0 and 1 respectively when calling the layer.\n+            # Mask and oov values in the vocab data should be dropped, and\n+            # mapped to 0 and 1 respectively when calling the layer.\n             \"vocab_data\": np.array(\n                 [\n                     [42],\n@@ -267,8 +267,8 @@ def _get_end_to_end_test_cases():\n         },\n         {\n             \"testcase_name\": \"test_strings_hard_vocab_cap\",\n-            # Create an array where 'earth' is the most frequent term, followed by\n-            # 'wind', then 'and', then 'fire'. This ensures that the vocab\n+            # Create an array where 'earth' is the most frequent term, followed\n+            # by 'wind', then 'and', then 'fire'. This ensures that the vocab\n             # accumulator is sorting by frequency.\n             \"vocab_data\": np.array(\n                 [\n@@ -308,8 +308,8 @@ def _get_end_to_end_test_cases():\n         },\n         {\n             \"testcase_name\": \"test_inverse_strings_hard_vocab_cap\",\n-            # Create an array where 'earth' is the most frequent term, followed by\n-            # 'wind', then 'and', then 'fire'. This ensures that the vocab\n+            # Create an array where 'earth' is the most frequent term, followed\n+            # by 'wind', then 'and', then 'fire'. This ensures that the vocab\n             # accumulator is sorting by frequency.\n             \"vocab_data\": np.array(\n                 [\n@@ -520,12 +520,12 @@ class IndexLookupLayerTest(\n             # together. When the results have different shapes on the non-concat\n             # axis (which can happen in the output_mode = INT case for\n             # IndexLookup), the concatenation fails. In real use cases, this may\n-            # not be an issue because users are likely to pipe the preprocessing layer\n-            # into other keras layers instead of predicting it directly. A workaround\n-            # for these unit tests is to have the dataset only contain one batch, so\n-            # no concatenation needs to happen with the result. For consistency with\n-            # numpy input, we should make `predict` join differently shaped results\n-            # together sensibly, with 0 padding.\n+            # not be an issue because users are likely to pipe the preprocessing\n+            # layer into other keras layers instead of predicting it directly. A\n+            # workaround for these unit tests is to have the dataset only\n+            # contain one batch, so no concatenation needs to happen with the\n+            # result. For consistency with numpy input, we should make `predict`\n+            # join differently shaped results together sensibly, with 0 padding.\n             input_data = tf.data.Dataset.from_tensor_slices(input_data).batch(\n                 input_shape[0]\n             )\n@@ -2233,8 +2233,8 @@ class IndexLookupSavingTest(\n         output_path = os.path.join(self.get_temp_dir(), \"tf_keras_saved_model\")\n         model.save(output_path, save_format=\"tf\")\n \n-        # Delete the session and graph to ensure that the loaded model is generated\n-        # from scratch.\n+        # Delete the session and graph to ensure that the loaded model is\n+        # generated from scratch.\n         keras.backend.clear_session()\n \n         loaded_model = keras.models.load_model(\n@@ -2317,8 +2317,8 @@ class IndexLookupSavingTest(\n         output_path = os.path.join(self.get_temp_dir(), \"tf_keras_saved_model\")\n         tf.saved_model.save(obj=model, export_dir=output_path)\n \n-        # Delete the session and graph to ensure that the loaded model is generated\n-        # from scratch.\n+        # Delete the session and graph to ensure that the loaded model is\n+        # generated from scratch.\n         keras.backend.clear_session()\n \n         loaded_model = tf.saved_model.load(output_path)\n@@ -2362,8 +2362,8 @@ class IndexLookupSavingTest(\n         output_path = os.path.join(self.get_temp_dir(), \"tf_keras_saved_model\")\n         model.save(output_path, save_format=\"tf\")\n \n-        # Delete the session and graph to ensure that the loaded model is generated\n-        # from scratch.\n+        # Delete the session and graph to ensure that the loaded model is\n+        # generated from scratch.\n         keras.backend.clear_session()\n \n         loaded_model = tf.saved_model.load(output_path)\n@@ -2407,8 +2407,8 @@ class IndexLookupSavingTest(\n         output_path = os.path.join(self.get_temp_dir(), \"tf_keras_saved_model\")\n         model.save(output_path, save_format=\"tf\")\n \n-        # Delete the session and graph to ensure that the loaded model is generated\n-        # from scratch.\n+        # Delete the session and graph to ensure that the loaded model is\n+        # generated from scratch.\n         keras.backend.clear_session()\n         tf.io.gfile.remove(vocab_file)\n \n@@ -2438,8 +2438,8 @@ class IndexLookupSavingTest(\n         )\n         model_2.save(output_path, save_format=\"tf\")\n \n-        # Delete the session and graph to ensure that the loaded model is generated\n-        # from scratch.\n+        # Delete the session and graph to ensure that the loaded model is\n+        # generated from scratch.\n         keras.backend.clear_session()\n \n         loaded_model = keras.models.load_model(\n@@ -2485,8 +2485,8 @@ class IndexLookupSavingTest(\n         output_path = os.path.join(self.get_temp_dir(), \"tf_keras_saved_model\")\n         model.save(output_path, save_format=\"tf\")\n \n-        # Delete the session and graph to ensure that the loaded model is generated\n-        # from scratch.\n+        # Delete the session and graph to ensure that the loaded model is\n+        # generated from scratch.\n         keras.backend.clear_session()\n         tf.io.gfile.remove(vocab_file)\n \n@@ -2516,8 +2516,8 @@ class IndexLookupSavingTest(\n         )\n         tf.saved_model.save(model_2, output_path)\n \n-        # Delete the session and graph to ensure that the loaded model is generated\n-        # from scratch.\n+        # Delete the session and graph to ensure that the loaded model is\n+        # generated from scratch.\n         keras.backend.clear_session()\n \n         loaded_model = tf.saved_model.load(output_path)\n@@ -2563,8 +2563,8 @@ class IndexLookupSavingTest(\n         output_path = os.path.join(self.get_temp_dir(), \"tf_keras_saved_model\")\n         model.save(output_path, save_format=\"tf\")\n \n-        # Delete the session and graph to ensure that the loaded model is generated\n-        # from scratch.\n+        # Delete the session and graph to ensure that the loaded model is\n+        # generated from scratch.\n         keras.backend.clear_session()\n         tf.io.gfile.remove(vocab_file)\n \n@@ -2594,8 +2594,8 @@ class IndexLookupSavingTest(\n         )\n         model_2.save(output_path, save_format=\"tf\")\n \n-        # Delete the session and graph to ensure that the loaded model is generated\n-        # from scratch.\n+        # Delete the session and graph to ensure that the loaded model is\n+        # generated from scratch.\n         keras.backend.clear_session()\n \n         loaded_model = keras.models.load_model(\n@@ -2679,9 +2679,10 @@ class EagerExecutionDisabled(\n     test_combinations.TestCase, preprocessing_test_utils.PreprocessingLayerTest\n ):\n     def test_lookup(self):\n-        # We need this test for model_to_estimator followed by export_saved_model,\n-        # which will call the layer in a legacy session. This could also happen\n-        # directly if a user calls disable_v2_behavior or disable_eager_execution.\n+        # We need this test for model_to_estimator followed by\n+        # export_saved_model, which will call the layer in a legacy session.\n+        # This could also happen directly if a user calls disable_v2_behavior or\n+        # disable_eager_execution.\n         with tf.compat.v1.Session():\n             with test_utils.run_eagerly_scope(False):\n                 vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n@@ -2699,8 +2700,8 @@ class EagerExecutionDisabled(\n                 )\n                 int_data = layer(input_data)\n                 model = keras.Model(inputs=input_data, outputs=int_data)\n-                # In a TF1 session the user will need to make sure all tables are\n-                # initialized themselves.\n+                # In a TF1 session the user will need to make sure all tables\n+                # are initialized themselves.\n                 tf.compat.v1.tables_initializer().run()\n                 output_dataset = model(input_array)\n                 self.assertAllEqual(output_dataset, expected_output)\n\n@@ -33,48 +33,48 @@ from tensorflow.python.util.tf_export import keras_export\n class IntegerLookup(index_lookup.IndexLookup):\n     \"\"\"A preprocessing layer which maps integer features to contiguous ranges.\n \n-    This layer maps a set of arbitrary integer input tokens into indexed\n-    integer output via a table-based vocabulary lookup. The layer's output indices\n-    will be contiguously arranged up to the maximum vocab size, even if the input\n+    This layer maps a set of arbitrary integer input tokens into indexed integer\n+    output via a table-based vocabulary lookup. The layer's output indices will\n+    be contiguously arranged up to the maximum vocab size, even if the input\n     tokens are non-continguous or unbounded. The layer supports multiple options\n     for encoding the output via `output_mode`, and has optional support for\n     out-of-vocabulary (OOV) tokens and masking.\n \n     The vocabulary for the layer must be either supplied on construction or\n     learned via `adapt()`. During `adapt()`, the layer will analyze a data set,\n-    determine the frequency of individual integer tokens, and create a vocabulary\n-    from them. If the vocabulary is capped in size, the most frequent tokens will\n-    be used to create the vocabulary and all others will be treated as OOV.\n+    determine the frequency of individual integer tokens, and create a\n+    vocabulary from them. If the vocabulary is capped in size, the most frequent\n+    tokens will be used to create the vocabulary and all others will be treated\n+    as OOV.\n \n-    There are two possible output modes for the layer.\n-    When `output_mode` is `\"int\"`,\n-    input integers are converted to their index in the vocabulary (an integer).\n-    When `output_mode` is `\"multi_hot\"`, `\"count\"`, or `\"tf_idf\"`, input integers\n-    are encoded into an array where each dimension corresponds to an element in\n-    the vocabulary.\n+    There are two possible output modes for the layer.  When `output_mode` is\n+    `\"int\"`, input integers are converted to their index in the vocabulary (an\n+    integer).  When `output_mode` is `\"multi_hot\"`, `\"count\"`, or `\"tf_idf\"`,\n+    input integers are encoded into an array where each dimension corresponds to\n+    an element in the vocabulary.\n \n     The vocabulary can optionally contain a mask token as well as an OOV token\n     (which can optionally occupy multiple indices in the vocabulary, as set\n     by `num_oov_indices`).\n-    The position of these tokens in the vocabulary is fixed. When `output_mode` is\n-    `\"int\"`, the vocabulary will begin with the mask token at index 0, followed by\n-    OOV indices, followed by the rest of the vocabulary. When `output_mode` is\n-    `\"multi_hot\"`, `\"count\"`, or `\"tf_idf\"` the vocabulary will begin with OOV\n-    indices and instances of the mask token will be dropped.\n+    The position of these tokens in the vocabulary is fixed. When `output_mode`\n+    is `\"int\"`, the vocabulary will begin with the mask token at index 0,\n+    followed by OOV indices, followed by the rest of the vocabulary. When\n+    `output_mode` is `\"multi_hot\"`, `\"count\"`, or `\"tf_idf\"` the vocabulary will\n+    begin with OOV indices and instances of the mask token will be dropped.\n \n     For an overview and full list of preprocessing layers, see the preprocessing\n     [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n \n     Args:\n-      max_tokens: Maximum size of the vocabulary for this layer. This should only\n-        be specified when adapting the vocabulary or when setting\n+      max_tokens: Maximum size of the vocabulary for this layer. This should\n+        only be specified when adapting the vocabulary or when setting\n         `pad_to_max_tokens=True`. If None, there is no cap on the size of the\n-        vocabulary. Note that this size includes the OOV and mask tokens. Defaults\n-        to None.\n+        vocabulary. Note that this size includes the OOV and mask tokens.\n+        Defaults to None.\n       num_oov_indices: The number of out-of-vocabulary tokens to use. If this\n         value is more than 1, OOV inputs are modulated to determine their OOV\n-        value. If this value is 0, OOV inputs will cause an error when calling the\n-        layer. Defaults to 1.\n+        value. If this value is 0, OOV inputs will cause an error when calling\n+        the layer. Defaults to 1.\n       mask_token: An integer token that represents masked inputs. When\n         `output_mode` is `\"int\"`, the token is included in vocabulary and mapped\n         to index 0. In other output modes, the token will not appear in the\n@@ -82,38 +82,38 @@ class IntegerLookup(index_lookup.IndexLookup):\n         If set to None, no mask term will be added. Defaults to None.\n       oov_token: Only used when `invert` is True. The token to return for OOV\n         indices. Defaults to -1.\n-      vocabulary: Optional. Either an array of integers or a string path to a text\n-        file. If passing an array, can pass a tuple, list, 1D numpy array, or 1D\n-        tensor containing the integer vocbulary terms. If passing a file path, the\n-        file should contain one line per term in the vocabulary. If this argument\n-        is set, there is no need to `adapt()` the layer.\n+      vocabulary: Optional. Either an array of integers or a string path to a\n+        text file. If passing an array, can pass a tuple, list, 1D numpy array,\n+        or 1D tensor containing the integer vocbulary terms. If passing a file\n+        path, the file should contain one line per term in the vocabulary. If\n+        this argument is set, there is no need to `adapt()` the layer.\n       vocabulary_dtype: The dtype of the vocabulary terms, for example\n         `\"int64\"` or `\"int32\"`. Defaults to `\"int64\"`.\n-      idf_weights: Only valid when `output_mode` is `\"tf_idf\"`. A tuple, list, 1D\n-        numpy array, or 1D tensor or the same length as the vocabulary, containing\n-        the floating point inverse document frequency weights, which will be\n-        multiplied by per sample term counts for the final `tf_idf` weight. If the\n-        `vocabulary` argument is set, and `output_mode` is `\"tf_idf\"`, this\n-        argument must be supplied.\n+      idf_weights: Only valid when `output_mode` is `\"tf_idf\"`. A tuple, list,\n+        1D numpy array, or 1D tensor or the same length as the vocabulary,\n+        containing the floating point inverse document frequency weights, which\n+        will be multiplied by per sample term counts for the final `tf_idf`\n+        weight. If the `vocabulary` argument is set, and `output_mode` is\n+        `\"tf_idf\"`, this argument must be supplied.\n       invert: Only valid when `output_mode` is `\"int\"`. If True, this layer will\n         map indices to vocabulary items instead of mapping vocabulary items to\n         indices. Default to False.\n-      output_mode: Specification for the output of the layer. Defaults to `\"int\"`.\n-        Values can be `\"int\"`, `\"one_hot\"`, `\"multi_hot\"`, `\"count\"`, or\n-        `\"tf_idf\"` configuring the layer as follows:\n+      output_mode: Specification for the output of the layer. Defaults to\n+        `\"int\"`.  Values can be `\"int\"`, `\"one_hot\"`, `\"multi_hot\"`, `\"count\"`,\n+        or `\"tf_idf\"` configuring the layer as follows:\n           - `\"int\"`: Return the vocabulary indices of the input tokens.\n           - `\"one_hot\"`: Encodes each individual element in the input into an\n             array the same size as the vocabulary, containing a 1 at the element\n-            index. If the last dimension is size 1, will encode on that dimension.\n-            If the last dimension is not size 1, will append a new dimension for\n-            the encoded output.\n+            index. If the last dimension is size 1, will encode on that\n+            dimension.  If the last dimension is not size 1, will append a new\n+            dimension for the encoded output.\n           - `\"multi_hot\"`: Encodes each sample in the input into a single array\n             the same size as the vocabulary, containing a 1 for each vocabulary\n             term present in the sample. Treats the last dimension as the sample\n             dimension, if input shape is (..., sample_length), output shape will\n             be (..., num_tokens).\n-          - `\"count\"`: As `\"multi_hot\"`, but the int array contains a count of the\n-            number of times the token at that index appeared in the sample.\n+          - `\"count\"`: As `\"multi_hot\"`, but the int array contains a count of\n+            the number of times the token at that index appeared in the sample.\n           - `\"tf_idf\"`: As `\"multi_hot\"`, but the TF-IDF algorithm is applied to\n             find the value in each token slot.\n         For `\"int\"` output, any shape of input and output is supported. For all\n@@ -122,7 +122,8 @@ class IntegerLookup(index_lookup.IndexLookup):\n         `\"count\"`, or `\"tf_idf\"`. If True, the output will have its feature axis\n         padded to `max_tokens` even if the number of unique tokens in the\n         vocabulary is less than max_tokens, resulting in a tensor of shape\n-        [batch_size, max_tokens] regardless of vocabulary size. Defaults to False.\n+        [batch_size, max_tokens] regardless of vocabulary size. Defaults to\n+        False.\n       sparse: Boolean. Only applicable when `output_mode` is `\"multi_hot\"`,\n         `\"count\"`, or `\"tf_idf\"`. If True, returns a `SparseTensor` instead of a\n         dense `Tensor`. Defaults to False.\n@@ -143,8 +144,8 @@ class IntegerLookup(index_lookup.IndexLookup):\n \n     **Creating a lookup layer with an adapted vocabulary**\n \n-    This example creates a lookup layer and generates the vocabulary by analyzing\n-    the dataset.\n+    This example creates a lookup layer and generates the vocabulary by\n+    analyzing the dataset.\n \n     >>> data = tf.constant([[12, 1138, 42], [42, 1000, 36]])\n     >>> layer = tf.keras.layers.IntegerLookup()\n@@ -167,14 +168,15 @@ class IntegerLookup(index_lookup.IndexLookup):\n \n     **Lookups with multiple OOV indices**\n \n-    This example demonstrates how to use a lookup layer with multiple OOV indices.\n-    When a layer is created with more than one OOV index, any OOV tokens are\n-    hashed into the number of OOV buckets, distributing OOV tokens in a\n-    deterministic fashion across the set.\n+    This example demonstrates how to use a lookup layer with multiple OOV\n+    indices.  When a layer is created with more than one OOV index, any OOV\n+    tokens are hashed into the number of OOV buckets, distributing OOV tokens in\n+    a deterministic fashion across the set.\n \n     >>> vocab = [12, 36, 1138, 42]\n     >>> data = tf.constant([[12, 1138, 42], [37, 1000, 36]])\n-    >>> layer = tf.keras.layers.IntegerLookup(vocabulary=vocab, num_oov_indices=2)\n+    >>> layer = tf.keras.layers.IntegerLookup(\n+    ...     vocabulary=vocab, num_oov_indices=2)\n     >>> layer(data)\n     <tf.Tensor: shape=(2, 3), dtype=int64, numpy=\n     array([[2, 4, 5],\n@@ -182,8 +184,8 @@ class IntegerLookup(index_lookup.IndexLookup):\n \n     Note that the output for OOV token 37 is 1, while the output for OOV token\n     1000 is 0. The in-vocab terms have their output index increased by 1 from\n-    earlier examples (12 maps to 2, etc) in order to make space for the extra OOV\n-    token.\n+    earlier examples (12 maps to 2, etc) in order to make space for the extra\n+    OOV token.\n \n     **One-hot output**\n \n@@ -208,7 +210,8 @@ class IntegerLookup(index_lookup.IndexLookup):\n     `num_oov_indices` dimensions in the multi_hot encoding represent OOV tokens\n \n     >>> vocab = [12, 36, 1138, 42]\n-    >>> data = tf.constant([[12, 1138, 42, 42], [42, 7, 36, 7]]) # Note OOV tokens\n+    >>> data = tf.constant([[12, 1138, 42, 42],\n+    ...                     [42, 7, 36, 7]]) # Note OOV tokens\n     >>> layer = tf.keras.layers.IntegerLookup(\n     ...     vocabulary=vocab, output_mode='multi_hot')\n     >>> layer(data)\n@@ -218,11 +221,12 @@ class IntegerLookup(index_lookup.IndexLookup):\n \n     **Token count output**\n \n-    Configure the layer with `output_mode='count'`. As with multi_hot output, the\n-    first `num_oov_indices` dimensions in the output represent OOV tokens.\n+    Configure the layer with `output_mode='count'`. As with multi_hot output,\n+    the first `num_oov_indices` dimensions in the output represent OOV tokens.\n \n     >>> vocab = [12, 36, 1138, 42]\n-    >>> data = tf.constant([[12, 1138, 42, 42], [42, 7, 36, 7]]) # Note OOV tokens\n+    >>> data = tf.constant([[12, 1138, 42, 42],\n+    ...                     [42, 7, 36, 7]]) # Note OOV tokens\n     >>> layer = tf.keras.layers.IntegerLookup(\n     ...     vocabulary=vocab, output_mode='count')\n     >>> layer(data)\n@@ -232,17 +236,18 @@ class IntegerLookup(index_lookup.IndexLookup):\n \n     **TF-IDF output**\n \n-    Configure the layer with `output_mode='tf_idf'`. As with multi_hot output, the\n-    first `num_oov_indices` dimensions in the output represent OOV tokens.\n+    Configure the layer with `output_mode='tf_idf'`. As with multi_hot output,\n+    the first `num_oov_indices` dimensions in the output represent OOV tokens.\n \n     Each token bin will output `token_count * idf_weight`, where the idf weights\n-    are the inverse document frequency weights per token. These should be provided\n-    along with the vocabulary. Note that the `idf_weight` for OOV tokens will\n-    default to the average of all idf weights passed in.\n+    are the inverse document frequency weights per token. These should be\n+    provided along with the vocabulary. Note that the `idf_weight` for OOV\n+    tokens will default to the average of all idf weights passed in.\n \n     >>> vocab = [12, 36, 1138, 42]\n     >>> idf_weights = [0.25, 0.75, 0.6, 0.4]\n-    >>> data = tf.constant([[12, 1138, 42, 42], [42, 7, 36, 7]]) # Note OOV tokens\n+    >>> data = tf.constant([[12, 1138, 42, 42],\n+    ...                     [42, 7, 36, 7]]) # Note OOV tokens\n     >>> layer = tf.keras.layers.IntegerLookup(\n     ...     output_mode='tf_idf', vocabulary=vocab, idf_weights=idf_weights)\n     >>> layer(data)\n@@ -255,7 +260,8 @@ class IntegerLookup(index_lookup.IndexLookup):\n \n     >>> vocab = [-1, 12, 36, 1138, 42]\n     >>> idf_weights = [0.9, 0.25, 0.75, 0.6, 0.4]\n-    >>> data = tf.constant([[12, 1138, 42, 42], [42, 7, 36, 7]]) # Note OOV tokens\n+    >>> data = tf.constant([[12, 1138, 42, 42],\n+    ...                     [42, 7, 36, 7]]) # Note OOV tokens\n     >>> layer = tf.keras.layers.IntegerLookup(\n     ...     output_mode='tf_idf', vocabulary=vocab, idf_weights=idf_weights)\n     >>> layer(data)\n@@ -263,15 +269,15 @@ class IntegerLookup(index_lookup.IndexLookup):\n       array([[0.  , 0.25, 0.  , 0.6 , 0.8 ],\n              [1.8 , 0.  , 0.75, 0.  , 0.4 ]], dtype=float32)>\n \n-    When adapting the layer in tf_idf mode, each input sample will be considered a\n-    document, and idf weight per token will be calculated as\n+    When adapting the layer in tf_idf mode, each input sample will be considered\n+    a document, and idf weight per token will be calculated as\n     `log(1 + num_documents / (1 + token_document_count))`.\n \n     **Inverse lookup**\n \n-    This example demonstrates how to map indices to tokens using this layer. (You\n-    can also use `adapt()` with `inverse=True`, but for simplicity we'll pass the\n-    vocab in this example.)\n+    This example demonstrates how to map indices to tokens using this layer.\n+    (You can also use `adapt()` with `inverse=True`, but for simplicity we'll\n+    pass the vocab in this example.)\n \n     >>> vocab = [12, 36, 1138, 42]\n     >>> data = tf.constant([[1, 3, 4], [4, 0, 2]])\n@@ -329,8 +335,8 @@ class IntegerLookup(index_lookup.IndexLookup):\n             )\n \n         # Legacy versions of the IntegerLookup layer set layer dtype to int64,\n-        # instead of the output type. If we see this and output mode is not \"int\",\n-        # clear the setting so we don't switch types for old SavedModels.\n+        # instead of the output type. If we see this and output mode is not\n+        # \"int\", clear the setting so we don't switch types for old SavedModels.\n         if (\n             output_mode != \"int\"\n             and \"dtype\" in kwargs\n@@ -405,29 +411,32 @@ class IntegerLookup(index_lookup.IndexLookup):\n     def adapt(self, data, batch_size=None, steps=None):\n         \"\"\"Computes a vocabulary of interger terms from tokens in a dataset.\n \n-        Calling `adapt()` on an `IntegerLookup` layer is an alternative to passing\n-        in a precomputed vocabulary  on construction via the `vocabulary` argument.\n-        An `IntegerLookup` layer should always be either adapted over a dataset or\n-        supplied with a vocabulary.\n+        Calling `adapt()` on an `IntegerLookup` layer is an alternative to\n+        passing in a precomputed vocabulary  on construction via the\n+        `vocabulary` argument.  An `IntegerLookup` layer should always be either\n+        adapted over a dataset or supplied with a vocabulary.\n \n-        During `adapt()`, the layer will build a vocabulary of all integer tokens\n-        seen in the dataset, sorted by occurrence count, with ties broken by sort\n-        order of the tokens (high to low). At the end of `adapt()`, if `max_tokens`\n-        is set, the vocabulary wil be truncated to `max_tokens` size. For example,\n-        adapting a layer with `max_tokens=1000` will compute the 1000 most frequent\n-        tokens occurring in the input dataset. If `output_mode='tf-idf'`, `adapt()`\n-        will also learn the document frequencies of each token in the input dataset.\n+        During `adapt()`, the layer will build a vocabulary of all integer\n+        tokens seen in the dataset, sorted by occurrence count, with ties broken\n+        by sort order of the tokens (high to low). At the end of `adapt()`, if\n+        `max_tokens` is set, the vocabulary wil be truncated to `max_tokens`\n+        size. For example, adapting a layer with `max_tokens=1000` will compute\n+        the 1000 most frequent tokens occurring in the input dataset. If\n+        `output_mode='tf-idf'`, `adapt()` will also learn the document\n+        frequencies of each token in the input dataset.\n \n-        In order to make `StringLookup` efficient in any distribution context, the\n-        vocabulary is kept static with respect to any compiled `tf.Graph`s that\n-        call the layer. As a consequence, if the layer is adapted a second time,\n-        any models using the layer should be re-compiled. For more information\n-        see `tf.keras.layers.experimental.preprocessing.PreprocessingLayer.adapt`.\n+        In order to make `StringLookup` efficient in any distribution context,\n+        the vocabulary is kept static with respect to any compiled `tf.Graph`s\n+        that call the layer. As a consequence, if the layer is adapted a second\n+        time, any models using the layer should be re-compiled. For more\n+        information see\n+        `tf.keras.layers.experimental.preprocessing.PreprocessingLayer.adapt`.\n         \n-        `adapt()` is meant only as a single machine utility to compute layer state.\n-        To analyze a dataset that cannot fit on a single machine, see\n-        [Tensorflow Transform](https://www.tensorflow.org/tfx/transform/get_started)\n-        for a multi-machine, map-reduce solution.\n+        `adapt()` is meant only as a single machine utility to compute layer\n+        state.  To analyze a dataset that cannot fit on a single machine, see\n+        [Tensorflow Transform](\n+        https://www.tensorflow.org/tfx/transform/get_started) for a\n+        multi-machine, map-reduce solution.\n \n         Arguments:\n           data: The data to train on. It can be passed either as a\n\n@@ -103,13 +103,14 @@ class IntegerLookupLayerTest(\n             # dataset batch separately, then tries to concatenate the results\n             # together. When the results have different shapes on the non-concat\n             # axis (which can happen in the output_mode = INT case for\n-            # IntegerLookup), the concatenation fails. In real use cases, this may\n-            # not be an issue because users are likely to pipe the preprocessing layer\n-            # into other keras layers instead of predicting it directly. A workaround\n-            # for these unit tests is to have the dataset only contain one batch, so\n-            # no concatenation needs to happen with the result. For consistency with\n-            # numpy input, we should make `predict` join differently shaped results\n-            # together sensibly, with 0 padding.\n+            # IntegerLookup), the concatenation fails. In real use cases, this\n+            # may not be an issue because users are likely to pipe the\n+            # preprocessing layer into other keras layers instead of predicting\n+            # it directly. A workaround for these unit tests is to have the\n+            # dataset only contain one batch, so no concatenation needs to\n+            # happen with the result. For consistency with numpy input, we\n+            # should make `predict` join differently shaped results together\n+            # sensibly, with 0 padding.\n             input_data = tf.data.Dataset.from_tensor_slices(input_data).batch(\n                 input_shape[0]\n             )\n@@ -634,8 +635,8 @@ class IntegerLookupSavingTest(\n         output_path = os.path.join(self.get_temp_dir(), \"tf_keras_saved_model\")\n         model.save(output_path, save_format=\"tf\")\n \n-        # Delete the session and graph to ensure that the loaded model is generated\n-        # from scratch.\n+        # Delete the session and graph to ensure that the loaded model is\n+        # generated from scratch.\n         # TODO(b/149526183): Can't clear session when TF2 is disabled.\n         if tf.__internal__.tf2.enabled():\n             keras.backend.clear_session()\n\n@@ -33,8 +33,9 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n     \"\"\"A preprocessing layer which normalizes continuous features.\n \n     This layer will shift and scale inputs into a distribution centered around\n-    0 with standard deviation 1. It accomplishes this by precomputing the mean and\n-    variance of the data, and calling `(input - mean) / sqrt(var)` at runtime.\n+    0 with standard deviation 1. It accomplishes this by precomputing the mean\n+    and variance of the data, and calling `(input - mean) / sqrt(var)` at\n+    runtime.\n \n     The mean and variance values for the layer must be either supplied on\n     construction or learned via `adapt()`. `adapt()` will compute the mean and\n@@ -48,21 +49,21 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n         axis: Integer, tuple of integers, or None. The axis or axes that should\n           have a separate mean and variance for each index in the shape. For\n           example, if shape is `(None, 5)` and `axis=1`, the layer will track 5\n-          separate mean and variance values for the last axis. If `axis` is set to\n-          `None`, the layer will normalize all elements in the input by a scalar\n-          mean and variance. Defaults to -1, where the last axis of the input is\n-          assumed to be a feature dimension and is normalized per index. Note that\n-          in the specific case of batched scalar inputs where the only axis is the\n-          batch axis, the default will normalize each index in the batch\n-          separately. In this case, consider passing `axis=None`.\n+          separate mean and variance values for the last axis. If `axis` is set\n+          to `None`, the layer will normalize all elements in the input by a\n+          scalar mean and variance. Defaults to -1, where the last axis of the\n+          input is assumed to be a feature dimension and is normalized per\n+          index. Note that in the specific case of batched scalar inputs where\n+          the only axis is the batch axis, the default will normalize each index\n+          in the batch separately. In this case, consider passing `axis=None`.\n         mean: The mean value(s) to use during normalization. The passed value(s)\n           will be broadcast to the shape of the kept axes above; if the value(s)\n-          cannot be broadcast, an error will be raised when this layer's `build()`\n-          method is called.\n+          cannot be broadcast, an error will be raised when this layer's\n+          `build()` method is called.\n         variance: The variance value(s) to use during normalization. The passed\n           value(s) will be broadcast to the shape of the kept axes above; if the\n-          value(s) cannot be broadcast, an error will be raised when this layer's\n-          `build()` method is called.\n+          value(s) cannot be broadcast, an error will be raised when this\n+          layer's `build()` method is called.\n         invert: If True, this layer will apply the inverse transformation\n           to its inputs: it would turn a normalized input back into its\n           original form.\n@@ -183,7 +184,8 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n         for d in self._keep_axis:\n             if input_shape[d] is None:\n                 raise ValueError(\n-                    \"All `axis` values to be kept must have known shape. Got axis: {}, \"\n+                    \"All `axis` values to be kept must have known shape. \"\n+                    \"Got axis: {}, \"\n                     \"input shape: {}, with unknown axis at index: {}\".format(\n                         self.axis, input_shape, d\n                     )\n@@ -224,8 +226,8 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n             )\n             self.finalize_state()\n         else:\n-            # In the no adapt case, make constant tensors for mean and variance with\n-            # proper broadcast shape for use during call.\n+            # In the no adapt case, make constant tensors for mean and variance\n+            # with proper broadcast shape for use during call.\n             mean = self.input_mean * np.ones(mean_and_var_shape)\n             variance = self.input_variance * np.ones(mean_and_var_shape)\n             mean = tf.reshape(mean, self._broadcast_shape)\n@@ -237,26 +239,27 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n     def adapt(self, data, batch_size=None, steps=None):\n         \"\"\"Computes the mean and variance of values in a dataset.\n \n-        Calling `adapt()` on a `Normalization` layer is an alternative to passing in\n-        `mean` and `variance` arguments during layer construction. A `Normalization`\n-        layer should always either be adapted over a dataset or passed `mean` and\n-        `variance`.\n+        Calling `adapt()` on a `Normalization` layer is an alternative to\n+        passing in `mean` and `variance` arguments during layer construction. A\n+        `Normalization` layer should always either be adapted over a dataset or\n+        passed `mean` and `variance`.\n \n-        During `adapt()`, the layer will compute a `mean` and `variance` separately\n-        for each position in each axis specified by the `axis` argument. To\n-        calculate a single `mean` and `variance` over the input data, simply pass\n-        `axis=None`.\n+        During `adapt()`, the layer will compute a `mean` and `variance`\n+        separately for each position in each axis specified by the `axis`\n+        argument. To calculate a single `mean` and `variance` over the input\n+        data, simply pass `axis=None`.\n \n-        In order to make `Normalization` efficient in any distribution context, the\n-        computed mean and variance are kept static with respect to any compiled\n-        `tf.Graph`s that call the layer. As a consequence, if the layer is adapted a\n-        second time, any models using the layer should be re-compiled. For more\n-        information see\n+        In order to make `Normalization` efficient in any distribution context,\n+        the computed mean and variance are kept static with respect to any\n+        compiled `tf.Graph`s that call the layer. As a consequence, if the layer\n+        is adapted a second time, any models using the layer should be\n+        re-compiled. For more information see\n         `tf.keras.layers.experimental.preprocessing.PreprocessingLayer.adapt`.\n \n-        `adapt()` is meant only as a single machine utility to compute layer state.\n-        To analyze a dataset that cannot fit on a single machine, see\n-        [Tensorflow Transform](https://www.tensorflow.org/tfx/transform/get_started)\n+        `adapt()` is meant only as a single machine utility to compute layer\n+        state.  To analyze a dataset that cannot fit on a single machine, see\n+        [Tensorflow Transform](\n+        https://www.tensorflow.org/tfx/transform/get_started)\n         for a multi-machine, map-reduce solution.\n \n         Arguments:\n@@ -285,7 +288,8 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n         if self.input_mean is not None:\n             raise ValueError(\n                 \"Cannot `adapt` a Normalization layer that is initialized with \"\n-                \"static `mean` and `variance`, you passed mean {} and variance {}.\".format(\n+                \"static `mean` and `variance`, \"\n+                \"you passed mean {} and variance {}.\".format(\n                     self.input_mean, self.input_variance\n                 )\n             )\n@@ -313,7 +317,8 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n             self.adapt_mean * existing_weight + batch_mean * batch_weight\n         )\n         # The variance is computed using the lack-of-fit sum of squares\n-        # formula (see https://en.wikipedia.org/wiki/Lack-of-fit_sum_of_squares).\n+        # formula (see\n+        # https://en.wikipedia.org/wiki/Lack-of-fit_sum_of_squares).\n         total_variance = (\n             self.adapt_variance + (self.adapt_mean - total_mean) ** 2\n         ) * existing_weight + (\n\n@@ -218,7 +218,8 @@ class NormalizationTest(\n     def test_output_dtype(self):\n         if not tf.__internal__.tf2.enabled():\n             self.skipTest(\"set_global_policy only supported in TF2.\")\n-        # Output should respect an explicit dtype, and default to the global policy.\n+        # Output should respect an explicit dtype, and default to the global\n+        # policy.\n         policy.set_global_policy(\"float64\")\n         input_data = keras.Input(batch_size=16, shape=(1,))\n         layer = normalization.Normalization(\n\n@@ -36,7 +36,8 @@ class PreprocessingStage(\n     a single `adapt()` call on the preprocessing stage.\n \n     Args:\n-      layers: List of layers. Can include layers that aren't preprocessing layers.\n+      layers: List of layers. Can include layers that aren't preprocessing\n+        layers.\n       name: String. Optional name for the preprocessing stage object.\n     \"\"\"\n \n@@ -54,12 +55,12 @@ class PreprocessingStage(\n             data, (tf.data.Dataset, np.ndarray, tf.__internal__.EagerTensor)\n         ):\n             raise ValueError(\n-                f\"`adapt()` requires a batched Dataset, an EagerTensor, or a Numpy \"\n-                f\"array as input. Received data={data}\"\n+                f\"`adapt()` requires a batched Dataset, an EagerTensor, or a \"\n+                f\"Numpy array as input. Received data={data}\"\n             )\n         if isinstance(data, tf.data.Dataset):\n-            # Validate the datasets to try and ensure we haven't been passed one with\n-            # infinite size. That would cause an infinite loop here.\n+            # Validate the datasets to try and ensure we haven't been passed one\n+            # with infinite size. That would cause an infinite loop here.\n             if tf_utils.dataset_is_infinite(data):\n                 raise ValueError(\n                     \"The dataset passed to `adapt()` has an infinite number of \"\n@@ -76,7 +77,8 @@ class PreprocessingStage(\n                 \"\"\"Maps `PreprocessingStage` inputs to inputs at `current_layer_index`.\n \n                 Args:\n-                  x: Batch of inputs seen in entry of the `PreprocessingStage` instance.\n+                  x: Batch of inputs seen in entry of the `PreprocessingStage`\n+                    instance.\n \n                 Returns:\n                   Batch of inputs to be processed by layer\n@@ -133,17 +135,17 @@ class FunctionalPreprocessingStage(\n     >>> stage = FunctionalPreprocessingStage(inputs, outputs)\n \n     Args:\n-      inputs: An input tensor (must be created via `tf.keras.Input()`), or a list,\n-        a dict, or a nested structure of input tensors.\n-      outputs: An output tensor, or a list, a dict or a nested structure of output\n-        tensors.\n+      inputs: An input tensor (must be created via `tf.keras.Input()`), or a\n+        list, a dict, or a nested structure of input tensors.\n+      outputs: An output tensor, or a list, a dict or a nested structure of\n+        output tensors.\n       name: String, optional. Name of the preprocessing stage.\n     \"\"\"\n \n     def fit(self, *args, **kwargs):\n         raise ValueError(\n-            \"Preprocessing stage is not a complete model, and hence should not be \"\n-            \"`fit`. Instead, you may feed data to `adapt` the stage to set \"\n+            \"Preprocessing stage is not a complete model, and hence should not \"\n+            \"be `fit`. Instead, you may feed data to `adapt` the stage to set \"\n             \"appropriate states of the layers in the stage.\"\n         )\n \n@@ -151,14 +153,14 @@ class FunctionalPreprocessingStage(\n         \"\"\"Adapt the state of the layers of the preprocessing stage to the data.\n \n         Args:\n-          data: A batched Dataset object, a NumPy array, an EagerTensor, or a list,\n-            dict or nested structure of Numpy Arrays or EagerTensors. The elements\n-            of Dataset object need to conform with inputs of the stage. The first\n-            dimension of NumPy arrays or EagerTensors are understood to be batch\n-            dimension. Data to be iterated over to adapt the state of the layers in\n-            this preprocessing stage.\n-          reset_state: Whether this call to `adapt` should reset the state of the\n-            layers in this preprocessing stage.\n+          data: A batched Dataset object, a NumPy array, an EagerTensor, or a\n+            list, dict or nested structure of Numpy Arrays or EagerTensors. The\n+            elements of Dataset object need to conform with inputs of the stage.\n+            The first dimension of NumPy arrays or EagerTensors are understood\n+            to be batch dimension. Data to be iterated over to adapt the state\n+            of the layers in this preprocessing stage.\n+          reset_state: Whether this call to `adapt` should reset the state of\n+            the layers in this preprocessing stage.\n \n         Examples:\n \n@@ -184,16 +186,16 @@ class FunctionalPreprocessingStage(\n                 for datum in data\n             ):\n                 raise ValueError(\n-                    \"`adapt()` requires a batched Dataset, a list of EagerTensors \"\n-                    \"or Numpy arrays as input, got {}\".format(type(data))\n+                    \"`adapt()` requires a batched Dataset, a list of \"\n+                    f\"EagerTensors or Numpy arrays as input, got {type(data)}\"\n                 )\n             ds_input = [\n                 tf.data.Dataset.from_tensor_slices(x).batch(1) for x in data\n             ]\n \n         if isinstance(data, tf.data.Dataset):\n-            # Validate the datasets to try and ensure we haven't been passed one with\n-            # infinite size. That would cause an infinite loop here.\n+            # Validate the datasets to try and ensure we haven't been passed one\n+            # with infinite size. That would cause an infinite loop here.\n             if tf_utils.dataset_is_infinite(data):\n                 raise ValueError(\n                     \"The dataset passed to `adapt()` has an infinite number of \"\n\n@@ -66,7 +66,8 @@ class PreprocessingLayerTest(tf.test.TestCase):\n     compare_accumulators = assertAllCloseOrEqual\n \n     def validate_accumulator_computation(self, combiner, data, expected):\n-        \"\"\"Validate that various combinations of compute and merge are identical.\"\"\"\n+        \"\"\"Validate that various combinations of compute and merge are\n+        identical.\"\"\"\n         if len(data) < 4:\n             raise AssertionError(\n                 f\"Data must have at least 4 elements. Received \"\n@@ -151,8 +152,8 @@ class PreprocessingLayerTest(tf.test.TestCase):\n         self.compare_accumulators(\n             all_merge,\n             single_merge,\n-            msg=\"Calling merge with a data length of 1 should not change the data \"\n-            \"output.\",\n+            msg=\"Calling merge with a data length of 1 should not change \"\n+            \"the data output.\",\n         )\n \n         self.compare_accumulators(\n\n@@ -117,8 +117,9 @@ def encode_categorical_inputs(\n     # TODO(b/190445202): remove output rank restriction.\n     if inputs.shape.rank > 2:\n         raise ValueError(\n-            f\"When output_mode is not `'int'`, maximum supported output rank is 2. \"\n-            f\"Received output_mode {output_mode} and input shape {original_shape}, \"\n+            f\"When output_mode is not `'int'`, maximum supported output rank \"\n+            f\"is 2. Received output_mode {output_mode} and input shape \"\n+            f\"{original_shape}, \"\n             f\"which would result in output rank {inputs.shape.rank}.\"\n         )\n \n\n@@ -39,10 +39,10 @@ class StringLookup(index_lookup.IndexLookup):\n \n     The vocabulary for the layer must be either supplied on construction or\n     learned via `adapt()`. During `adapt()`, the layer will analyze a data set,\n-    determine the frequency of individual strings tokens, and create a vocabulary\n-    from them. If the vocabulary is capped in size, the most frequent tokens will\n-    be used to create the vocabulary and all others will be treated as\n-    out-of-vocabulary (OOV).\n+    determine the frequency of individual strings tokens, and create a\n+    vocabulary from them. If the vocabulary is capped in size, the most frequent\n+    tokens will be used to create the vocabulary and all others will be treated\n+    as out-of-vocabulary (OOV).\n \n     There are two possible output modes for the layer.\n     When `output_mode` is `\"int\"`,\n@@ -54,62 +54,62 @@ class StringLookup(index_lookup.IndexLookup):\n     The vocabulary can optionally contain a mask token as well as an OOV token\n     (which can optionally occupy multiple indices in the vocabulary, as set\n     by `num_oov_indices`).\n-    The position of these tokens in the vocabulary is fixed. When `output_mode` is\n-    `\"int\"`, the vocabulary will begin with the mask token (if set), followed by\n-    OOV indices, followed by the rest of the vocabulary. When `output_mode` is\n-    `\"multi_hot\"`, `\"count\"`, or `\"tf_idf\"` the vocabulary will begin with OOV\n-    indices and instances of the mask token will be dropped.\n+    The position of these tokens in the vocabulary is fixed. When `output_mode`\n+    is `\"int\"`, the vocabulary will begin with the mask token (if set), followed\n+    by OOV indices, followed by the rest of the vocabulary. When `output_mode`\n+    is `\"multi_hot\"`, `\"count\"`, or `\"tf_idf\"` the vocabulary will begin with\n+    OOV indices and instances of the mask token will be dropped.\n \n     For an overview and full list of preprocessing layers, see the preprocessing\n     [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n \n     Args:\n-      max_tokens: Maximum size of the vocabulary for this layer. This should only\n-        be specified when adapting the vocabulary or when setting\n+      max_tokens: Maximum size of the vocabulary for this layer. This should\n+        only be specified when adapting the vocabulary or when setting\n         `pad_to_max_tokens=True`. If None, there is no cap on the size of the\n-        vocabulary. Note that this size includes the OOV and mask tokens. Defaults\n-        to None.\n+        vocabulary. Note that this size includes the OOV and mask tokens.\n+        Defaults to None.\n       num_oov_indices: The number of out-of-vocabulary tokens to use. If this\n-        value is more than 1, OOV inputs are hashed to determine their OOV value.\n-        If this value is 0, OOV inputs will cause an error when calling the layer.\n-        Defaults to 1.\n+        value is more than 1, OOV inputs are hashed to determine their OOV\n+        value. If this value is 0, OOV inputs will cause an error when calling\n+        the layer.  Defaults to 1.\n       mask_token: A token that represents masked inputs. When `output_mode` is\n         `\"int\"`, the token is included in vocabulary and mapped to index 0. In\n         other output modes, the token will not appear in the vocabulary and\n-        instances of the mask token in the input will be dropped. If set to None,\n-        no mask term will be added. Defaults to `None`.\n+        instances of the mask token in the input will be dropped. If set to\n+        None, no mask term will be added. Defaults to `None`.\n       oov_token: Only used when `invert` is True. The token to return for OOV\n         indices. Defaults to `\"[UNK]\"`.\n-      vocabulary: Optional. Either an array of strings or a string path to a text\n-        file. If passing an array, can pass a tuple, list, 1D numpy array, or 1D\n-        tensor containing the string vocbulary terms. If passing a file path, the\n-        file should contain one line per term in the vocabulary. If this argument\n-        is set, there is no need to `adapt()` the layer.\n-      idf_weights: Only valid when `output_mode` is `\"tf_idf\"`. A tuple, list, 1D\n-        numpy array, or 1D tensor or the same length as the vocabulary, containing\n-        the floating point inverse document frequency weights, which will be\n-        multiplied by per sample term counts for the final `tf_idf` weight. If the\n-        `vocabulary` argument is set, and `output_mode` is `\"tf_idf\"`, this\n-        argument must be supplied.\n+      vocabulary: Optional. Either an array of strings or a string path to a\n+        text file. If passing an array, can pass a tuple, list, 1D numpy array,\n+        or 1D tensor containing the string vocbulary terms. If passing a file\n+        path, the file should contain one line per term in the vocabulary. If\n+        this argument is set, there is no need to `adapt()` the layer.\n+      idf_weights: Only valid when `output_mode` is `\"tf_idf\"`. A tuple, list,\n+        1D numpy array, or 1D tensor or the same length as the vocabulary,\n+        containing the floating point inverse document frequency weights, which\n+        will be multiplied by per sample term counts for the final `tf_idf`\n+        weight. If the `vocabulary` argument is set, and `output_mode` is\n+        `\"tf_idf\"`, this argument must be supplied.\n       invert: Only valid when `output_mode` is `\"int\"`. If True, this layer will\n         map indices to vocabulary items instead of mapping vocabulary items to\n         indices. Default to False.\n-      output_mode: Specification for the output of the layer. Defaults to `\"int\"`.\n-        Values can be `\"int\"`, `\"one_hot\"`, `\"multi_hot\"`, `\"count\"`, or\n-        `\"tf_idf\"` configuring the layer as follows:\n+      output_mode: Specification for the output of the layer. Defaults to\n+        `\"int\"`.  Values can be `\"int\"`, `\"one_hot\"`, `\"multi_hot\"`, `\"count\"`,\n+        or `\"tf_idf\"` configuring the layer as follows:\n           - `\"int\"`: Return the raw integer indices of the input tokens.\n           - `\"one_hot\"`: Encodes each individual element in the input into an\n             array the same size as the vocabulary, containing a 1 at the element\n-            index. If the last dimension is size 1, will encode on that dimension.\n-            If the last dimension is not size 1, will append a new dimension for\n-            the encoded output.\n+            index. If the last dimension is size 1, will encode on that\n+            dimension. If the last dimension is not size 1, will append a new\n+            dimension for the encoded output.\n           - `\"multi_hot\"`: Encodes each sample in the input into a single array\n             the same size as the vocabulary, containing a 1 for each vocabulary\n             term present in the sample. Treats the last dimension as the sample\n             dimension, if input shape is (..., sample_length), output shape will\n             be (..., num_tokens).\n-          - `\"count\"`: As `\"multi_hot\"`, but the int array contains a count of the\n-            number of times the token at that index appeared in the sample.\n+          - `\"count\"`: As `\"multi_hot\"`, but the int array contains a count of\n+            the number of times the token at that index appeared in the sample.\n           - `\"tf_idf\"`: As `\"multi_hot\"`, but the TF-IDF algorithm is applied to\n             find the value in each token slot.\n         For `\"int\"` output, any shape of input and output is supported. For all\n@@ -118,7 +118,8 @@ class StringLookup(index_lookup.IndexLookup):\n         `\"count\"`, or `\"tf_idf\"`. If True, the output will have its feature axis\n         padded to `max_tokens` even if the number of unique tokens in the\n         vocabulary is less than max_tokens, resulting in a tensor of shape\n-        [batch_size, max_tokens] regardless of vocabulary size. Defaults to False.\n+        [batch_size, max_tokens] regardless of vocabulary size. Defaults to\n+        False.\n       sparse: Boolean. Only applicable when `output_mode` is `\"multi_hot\"`,\n         `\"count\"`, or `\"tf_idf\"`. If True, returns a `SparseTensor` instead of a\n         dense `Tensor`. Defaults to False.\n@@ -139,8 +140,8 @@ class StringLookup(index_lookup.IndexLookup):\n \n     **Creating a lookup layer with an adapted vocabulary**\n \n-    This example creates a lookup layer and generates the vocabulary by analyzing\n-    the dataset.\n+    This example creates a lookup layer and generates the vocabulary by\n+    analyzing the dataset.\n \n     >>> data = tf.constant([[\"a\", \"c\", \"d\"], [\"d\", \"z\", \"b\"]])\n     >>> layer = tf.keras.layers.StringLookup()\n@@ -162,14 +163,15 @@ class StringLookup(index_lookup.IndexLookup):\n \n     **Lookups with multiple OOV indices**\n \n-    This example demonstrates how to use a lookup layer with multiple OOV indices.\n-    When a layer is created with more than one OOV index, any OOV values are\n-    hashed into the number of OOV buckets, distributing OOV values in a\n-    deterministic fashion across the set.\n+    This example demonstrates how to use a lookup layer with multiple OOV\n+    indices.  When a layer is created with more than one OOV index, any OOV\n+    values are hashed into the number of OOV buckets, distributing OOV values in\n+    a deterministic fashion across the set.\n \n     >>> vocab = [\"a\", \"b\", \"c\", \"d\"]\n     >>> data = tf.constant([[\"a\", \"c\", \"d\"], [\"m\", \"z\", \"b\"]])\n-    >>> layer = tf.keras.layers.StringLookup(vocabulary=vocab, num_oov_indices=2)\n+    >>> layer = tf.keras.layers.StringLookup(vocabulary=vocab,\n+    ...                                      num_oov_indices=2)\n     >>> layer(data)\n     <tf.Tensor: shape=(2, 3), dtype=int64, numpy=\n     array([[2, 4, 5],\n@@ -213,8 +215,8 @@ class StringLookup(index_lookup.IndexLookup):\n \n     **Token count output**\n \n-    Configure the layer with `output_mode='count'`. As with multi_hot output, the\n-    first `num_oov_indices` dimensions in the output represent OOV values.\n+    Configure the layer with `output_mode='count'`. As with multi_hot output,\n+    the first `num_oov_indices` dimensions in the output represent OOV values.\n \n     >>> vocab = [\"a\", \"b\", \"c\", \"d\"]\n     >>> data = tf.constant([[\"a\", \"c\", \"d\", \"d\"], [\"d\", \"z\", \"b\", \"z\"]])\n@@ -227,13 +229,13 @@ class StringLookup(index_lookup.IndexLookup):\n \n     **TF-IDF output**\n \n-    Configure the layer with `output_mode=\"tf_idf\"`. As with multi_hot output, the\n-    first `num_oov_indices` dimensions in the output represent OOV values.\n+    Configure the layer with `output_mode=\"tf_idf\"`. As with multi_hot output,\n+    the first `num_oov_indices` dimensions in the output represent OOV values.\n \n     Each token bin will output `token_count * idf_weight`, where the idf weights\n-    are the inverse document frequency weights per token. These should be provided\n-    along with the vocabulary. Note that the `idf_weight` for OOV values will\n-    default to the average of all idf weights passed in.\n+    are the inverse document frequency weights per token. These should be\n+    provided along with the vocabulary. Note that the `idf_weight` for OOV\n+    values will default to the average of all idf weights passed in.\n \n     >>> vocab = [\"a\", \"b\", \"c\", \"d\"]\n     >>> idf_weights = [0.25, 0.75, 0.6, 0.4]\n@@ -264,9 +266,9 @@ class StringLookup(index_lookup.IndexLookup):\n \n     **Inverse lookup**\n \n-    This example demonstrates how to map indices to strings using this layer. (You\n-    can also use `adapt()` with `inverse=True`, but for simplicity we'll pass the\n-    vocab in this example.)\n+    This example demonstrates how to map indices to strings using this layer.\n+    (You can also use `adapt()` with `inverse=True`, but for simplicity we'll\n+    pass the vocab in this example.)\n \n     >>> vocab = [\"a\", \"b\", \"c\", \"d\"]\n     >>> data = tf.constant([[1, 3, 4], [4, 0, 2]])\n@@ -298,7 +300,8 @@ class StringLookup(index_lookup.IndexLookup):\n     since 1000 was not in the vocabulary - it got represented as an OOV, and all\n     OOV values are returned as `\"[UNK]\"` in the inverse layer. Also, note that\n     for the inverse to work, you must have already set the forward layer\n-    vocabulary either directly or via `adapt()` before calling `get_vocabulary()`.\n+    vocabulary either directly or via `adapt()` before calling\n+    `get_vocabulary()`.\n     \"\"\"\n \n     def __init__(\n@@ -357,29 +360,32 @@ class StringLookup(index_lookup.IndexLookup):\n     def adapt(self, data, batch_size=None, steps=None):\n         \"\"\"Computes a vocabulary of string terms from tokens in a dataset.\n \n-        Calling `adapt()` on a `StringLookup` layer is an alternative to passing in\n-        a precomputed vocabulary on construction via the `vocabulary` argument. A\n-        `StringLookup` layer should always be either adapted over a dataset or\n-        supplied with a vocabulary.\n+        Calling `adapt()` on a `StringLookup` layer is an alternative to passing\n+        in a precomputed vocabulary on construction via the `vocabulary`\n+        argument. A `StringLookup` layer should always be either adapted over a\n+        dataset or supplied with a vocabulary.\n \n         During `adapt()`, the layer will build a vocabulary of all string tokens\n-        seen in the dataset, sorted by occurrence count, with ties broken by sort\n-        order of the tokens (high to low). At the end of `adapt()`, if `max_tokens`\n-        is set, the vocabulary wil be truncated to `max_tokens` size. For example,\n-        adapting a layer with `max_tokens=1000` will compute the 1000 most frequent\n-        tokens occurring in the input dataset. If `output_mode='tf-idf'`, `adapt()`\n-        will also learn the document frequencies of each token in the input dataset.\n+        seen in the dataset, sorted by occurrence count, with ties broken by\n+        sort order of the tokens (high to low). At the end of `adapt()`, if\n+        `max_tokens` is set, the vocabulary wil be truncated to `max_tokens`\n+        size. For example, adapting a layer with `max_tokens=1000` will compute\n+        the 1000 most frequent tokens occurring in the input dataset. If\n+        `output_mode='tf-idf'`, `adapt()` will also learn the document\n+        frequencies of each token in the input dataset.\n \n-        In order to make `StringLookup` efficient in any distribution context, the\n-        vocabulary is kept static with respect to any compiled `tf.Graph`s that\n-        call the layer. As a consequence, if the layer is adapted a second time,\n-        any models using the layer should be re-compiled. For more information\n-        see `tf.keras.layers.experimental.preprocessing.PreprocessingLayer.adapt`.\n+        In order to make `StringLookup` efficient in any distribution context,\n+        the vocabulary is kept static with respect to any compiled `tf.Graph`s\n+        that call the layer. As a consequence, if the layer is adapted a second\n+        time, any models using the layer should be re-compiled. For more\n+        information see\n+        `tf.keras.layers.experimental.preprocessing.PreprocessingLayer.adapt`.\n \n-        `adapt()` is meant only as a single machine utility to compute layer state.\n-        To analyze a dataset that cannot fit on a single machine, see\n-        [Tensorflow Transform](https://www.tensorflow.org/tfx/transform/get_started)\n-        for a multi-machine, map-reduce solution.\n+        `adapt()` is meant only as a single machine utility to compute layer\n+        state.  To analyze a dataset that cannot fit on a single machine, see\n+        [Tensorflow Transform](\n+        https://www.tensorflow.org/tfx/transform/get_started) for a\n+        multi-machine, map-reduce solution.\n \n         Arguments:\n           data: The data to train on. It can be passed either as a\n\n@@ -31,8 +31,8 @@ def _get_end_to_end_test_cases():\n     test_cases = (\n         {\n             \"testcase_name\": \"test_strings_soft_vocab_cap\",\n-            # Create an array where 'earth' is the most frequent term, followed by\n-            # 'wind', then 'and', then 'fire'. This ensures that the vocab\n+            # Create an array where 'earth' is the most frequent term, followed\n+            # by 'wind', then 'and', then 'fire'. This ensures that the vocab\n             # accumulator is sorting by frequency.\n             \"vocab_data\": np.array(\n                 [\n@@ -105,13 +105,14 @@ class StringLookupLayerTest(\n             # dataset batch separately, then tries to concatenate the results\n             # together. When the results have different shapes on the non-concat\n             # axis (which can happen in the output_mode = INT case for\n-            # StringLookup), the concatenation fails. In real use cases, this may\n-            # not be an issue because users are likely to pipe the preprocessing layer\n-            # into other keras layers instead of predicting it directly. A workaround\n-            # for these unit tests is to have the dataset only contain one batch, so\n-            # no concatenation needs to happen with the result. For consistency with\n-            # numpy input, we should make `predict` join differently shaped results\n-            # together sensibly, with 0 padding.\n+            # StringLookup), the concatenation fails. In real use cases, this\n+            # may not be an issue because users are likely to pipe the\n+            # preprocessing layer into other keras layers instead of predicting\n+            # it directly. A workaround for these unit tests is to have the\n+            # dataset only contain one batch, so no concatenation needs to\n+            # happen with the result. For consistency with numpy input, we\n+            # should make `predict` join differently shaped results together\n+            # sensibly, with 0 padding.\n             input_data = tf.data.Dataset.from_tensor_slices(input_data).batch(\n                 input_shape[0]\n             )\n\n@@ -54,21 +54,21 @@ DEFAULT_STRIP_REGEX = r'[!\"#$%&()\\*\\+,-\\./:;<=>?@\\[\\\\\\]^_`{|}~\\']'\n class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n     \"\"\"A preprocessing layer which maps text features to integer sequences.\n \n-    This layer has basic options for managing text in a Keras model. It transforms\n-    a batch of strings (one example = one string) into either a list of token\n-    indices (one example = 1D tensor of integer token indices) or a dense\n-    representation (one example = 1D tensor of float values representing data\n-    about the example's tokens). This layer is meant to handle natural language\n-    inputs. To handle simple string inputs (categorical strings or pre-tokenized\n-    strings) see `tf.keras.layers.StringLookup`.\n+    This layer has basic options for managing text in a Keras model. It\n+    transforms a batch of strings (one example = one string) into either a list\n+    of token indices (one example = 1D tensor of integer token indices) or a\n+    dense representation (one example = 1D tensor of float values representing\n+    data about the example's tokens). This layer is meant to handle natural\n+    language inputs. To handle simple string inputs (categorical strings or\n+    pre-tokenized strings) see `tf.keras.layers.StringLookup`.\n \n     The vocabulary for the layer must be either supplied on construction or\n     learned via `adapt()`. When this layer is adapted, it will analyze the\n     dataset, determine the frequency of individual string values, and create a\n     vocabulary from them. This vocabulary can have unlimited size or be capped,\n     depending on the configuration options for this layer; if there are more\n-    unique values in the input than the maximum vocabulary size, the most frequent\n-    terms will be used to create the vocabulary.\n+    unique values in the input than the maximum vocabulary size, the most\n+    frequent terms will be used to create the vocabulary.\n \n     The processing of each example contains the following steps:\n \n@@ -102,11 +102,11 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n     [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n \n     Args:\n-      max_tokens: Maximum size of the vocabulary for this layer. This should only\n-        be specified when adapting a vocabulary or when setting\n+      max_tokens: Maximum size of the vocabulary for this layer. This should\n+        only be specified when adapting a vocabulary or when setting\n         `pad_to_max_tokens=True`. Note that this vocabulary\n-        contains 1 OOV token, so the effective number of tokens is `(max_tokens -\n-        1 - (1 if output_mode == \"int\" else 0))`.\n+        contains 1 OOV token, so the effective number of tokens is\n+        `(max_tokens - 1 - (1 if output_mode == \"int\" else 0))`.\n       standardize: Optional specification for standardization to apply to the\n         input text. Values can be:\n           - `None`: No standardization.\n@@ -122,53 +122,54 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n           - `\"character\"`: Split on each unicode character.\n           - Callable: Standardized inputs will passed to the callable function,\n             which should split and returned.\n-      ngrams: Optional specification for ngrams to create from the possibly-split\n-        input text. Values can be None, an integer or tuple of integers; passing\n-        an integer will create ngrams up to that integer, and passing a tuple of\n-        integers will create ngrams for the specified values in the tuple. Passing\n-        None means that no ngrams will be created.\n-      output_mode: Optional specification for the output of the layer. Values can\n-        be `\"int\"`, `\"multi_hot\"`, `\"count\"` or `\"tf_idf\"`, configuring the layer\n-        as follows:\n+      ngrams: Optional specification for ngrams to create from the\n+        possibly-split input text. Values can be None, an integer or tuple of\n+        integers; passing an integer will create ngrams up to that integer, and\n+        passing a tuple of integers will create ngrams for the specified values\n+        in the tuple. Passing None means that no ngrams will be created.\n+      output_mode: Optional specification for the output of the layer. Values\n+        can be `\"int\"`, `\"multi_hot\"`, `\"count\"` or `\"tf_idf\"`, configuring the\n+        layer as follows:\n           - `\"int\"`: Outputs integer indices, one integer index per split string\n             token. When `output_mode == \"int\"`, 0 is reserved for masked\n             locations; this reduces the vocab size to\n             `max_tokens - 2` instead of `max_tokens - 1`.\n           - `\"multi_hot\"`: Outputs a single int array per batch, of either\n-            vocab_size or max_tokens size, containing 1s in all elements where the\n-            token mapped to that index exists at least once in the batch item.\n+            vocab_size or max_tokens size, containing 1s in all elements where\n+            the token mapped to that index exists at least once in the batch\n+            item.\n           - `\"count\"`: Like `\"multi_hot\"`, but the int array contains a count of\n             the number of times the token at that index appeared in the\n             batch item.\n-          - `\"tf_idf\"`: Like `\"multi_hot\"`, but the TF-IDF algorithm is applied to\n-            find the value in each token slot.\n+          - `\"tf_idf\"`: Like `\"multi_hot\"`, but the TF-IDF algorithm is applied\n+            to find the value in each token slot.\n         For `\"int\"` output, any shape of input and output is supported. For all\n-        other output modes, currently only rank 1 inputs (and rank 2 outputs after\n-        splitting) are supported.\n-      output_sequence_length: Only valid in INT mode. If set, the output will have\n-        its time dimension padded or truncated to exactly `output_sequence_length`\n-        values, resulting in a tensor of shape\n+        other output modes, currently only rank 1 inputs (and rank 2 outputs\n+        after splitting) are supported.\n+      output_sequence_length: Only valid in INT mode. If set, the output will\n+        have its time dimension padded or truncated to exactly\n+        `output_sequence_length` values, resulting in a tensor of shape\n         `(batch_size, output_sequence_length)` regardless of how many tokens\n         resulted from the splitting step. Defaults to None.\n       pad_to_max_tokens: Only valid in  `\"multi_hot\"`, `\"count\"`, and `\"tf_idf\"`\n         modes. If True, the output will have its feature axis padded to\n-        `max_tokens` even if the number of unique tokens in the vocabulary is less\n-        than max_tokens, resulting in a tensor of shape `(batch_size, max_tokens)`\n-        regardless of vocabulary size. Defaults to False.\n-      vocabulary: Optional. Either an array of strings or a string path to a text\n-        file. If passing an array, can pass a tuple, list, 1D numpy array, or 1D\n-        tensor containing the string vocbulary terms. If passing a file path, the\n-        file should contain one line per term in the vocabulary. If this argument\n-        is set, there is no need to `adapt()` the layer.\n-      idf_weights: Only valid when `output_mode` is `\"tf_idf\"`. A tuple, list, 1D\n-        numpy array, or 1D tensor or the same length as the vocabulary, containing\n-        the floating point inverse document frequency weights, which will be\n-        multiplied by per sample term counts for the final `tf_idf` weight. If the\n-        `vocabulary` argument is set, and `output_mode` is `\"tf_idf\"`, this\n-        argument must be supplied.\n-      ragged: Boolean. Only applicable to `\"int\"` output mode. If True, returns a\n-        `RaggedTensor` instead of a dense `Tensor`, where each sequence may have a\n-        different length after string splitting. Defaults to False.\n+        `max_tokens` even if the number of unique tokens in the vocabulary is\n+        less than max_tokens, resulting in a tensor of shape `(batch_size,\n+        max_tokens)` regardless of vocabulary size. Defaults to False.\n+      vocabulary: Optional. Either an array of strings or a string path to a\n+        text file. If passing an array, can pass a tuple, list, 1D numpy array,\n+        or 1D tensor containing the string vocbulary terms. If passing a file\n+        path, the file should contain one line per term in the vocabulary. If\n+        this argument is set, there is no need to `adapt()` the layer.\n+      idf_weights: Only valid when `output_mode` is `\"tf_idf\"`. A tuple, list,\n+        1D numpy array, or 1D tensor or the same length as the vocabulary,\n+        containing the floating point inverse document frequency weights, which\n+        will be multiplied by per sample term counts for the final `tf_idf`\n+        weight. If the `vocabulary` argument is set, and `output_mode` is\n+        `\"tf_idf\"`, this argument must be supplied.\n+      ragged: Boolean. Only applicable to `\"int\"` output mode. If True, returns\n+        a `RaggedTensor` instead of a dense `Tensor`, where each sequence may\n+        have a different length after string splitting. Defaults to False.\n       sparse: Boolean. Only applicable to `\"multi_hot\"`, `\"count\"`, and\n         `\"tf_idf\"` output modes. If True, returns a `SparseTensor` instead of a\n         dense `Tensor`. Defaults to False.\n@@ -188,9 +189,10 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n     ...  output_mode='int',\n     ...  output_sequence_length=max_len)\n     >>>\n-    >>> # Now that the vocab layer has been created, call `adapt` on the text-only\n-    >>> # dataset to create the vocabulary. You don't have to batch, but for large\n-    >>> # datasets this means we're not keeping spare copies of the dataset.\n+    >>> # Now that the vocab layer has been created, call `adapt` on the\n+    >>> # text-only dataset to create the vocabulary. You don't have to batch,\n+    >>> # but for large datasets this means we're not keeping spare copies of\n+    >>> # the dataset.\n     >>> vectorize_layer.adapt(text_dataset.batch(64))\n     >>>\n     >>> # Create the model that uses the vectorize text layer\n@@ -202,12 +204,12 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n     >>> model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n     >>>\n     >>> # The first layer in our model is the vectorization layer. After this\n-    >>> # layer, we have a tensor of shape (batch_size, max_len) containing vocab\n-    >>> # indices.\n+    >>> # layer, we have a tensor of shape (batch_size, max_len) containing\n+    >>> # vocab indices.\n     >>> model.add(vectorize_layer)\n     >>>\n-    >>> # Now, the model can map strings to integers, and you can add an embedding\n-    >>> # layer to map these integers to learned embeddings.\n+    >>> # Now, the model can map strings to integers, and you can add an\n+    >>> # embedding layer to map these integers to learned embeddings.\n     >>> input_data = [[\"foo qux bar\"], [\"qux baz\"]]\n     >>> model.predict(input_data)\n     array([[2, 1, 4, 0],\n@@ -232,7 +234,8 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n     >>>\n     >>> # Because we've passed the vocabulary directly, we don't need to adapt\n     >>> # the layer - the vocabulary is already set. The vocabulary contains the\n-    >>> # padding token ('') and OOV token ('[UNK]') as well as the passed tokens.\n+    >>> # padding token ('') and OOV token ('[UNK]') as well as the passed\n+    >>> # tokens.\n     >>> vectorize_layer.get_vocabulary()\n     ['', '[UNK]', 'earth', 'wind', 'and', 'fire']\n \n@@ -265,7 +268,8 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n             kwargs[\"dtype\"] = tf.string\n \n         # 'standardize' must be one of\n-        # (None, LOWER_AND_STRIP_PUNCTUATION, LOWER, STRIP_PUNCTUATION, callable)\n+        # (None, LOWER_AND_STRIP_PUNCTUATION, LOWER, STRIP_PUNCTUATION,\n+        # callable)\n         layer_utils.validate_string_arg(\n             standardize,\n             allowable_strings=(\n@@ -329,8 +333,9 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n \n         if output_mode != INT and output_sequence_length is not None:\n             raise ValueError(\n-                f\"`output_sequence_length` must not be set if `output_mode` is not \"\n-                f\"'int'. Received output_sequence_length={output_sequence_length}.\"\n+                f\"`output_sequence_length` must not be set if `output_mode` is \"\n+                f\"not 'int'. \"\n+                f\"Received output_sequence_length={output_sequence_length}.\"\n             )\n \n         if ragged and output_mode != INT:\n@@ -360,9 +365,10 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n         self._output_mode = output_mode\n         self._output_sequence_length = output_sequence_length\n \n-        # VocabularySavedModelSaver will clear the config vocabulary to restore the\n-        # lookup table ops directly. We persist this hidden option to persist the\n-        # fact that we have have a non-adaptable layer with a manually set vocab.\n+        # VocabularySavedModelSaver will clear the config vocabulary to restore\n+        # the lookup table ops directly. We persist this hidden option to\n+        # persist the fact that we have have a non-adaptable layer with a\n+        # manually set vocab.\n         self._has_input_vocabulary = kwargs.pop(\n             \"has_input_vocabulary\", (vocabulary is not None)\n         )\n@@ -412,27 +418,30 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n \n         Calling `adapt()` on a `TextVectorization` layer is an alternative to\n         passing in a precomputed vocabulary on construction via the `vocabulary`\n-        argument. A `TextVectorization` layer should always be either adapted over a\n-        dataset or supplied with a vocabulary.\n+        argument. A `TextVectorization` layer should always be either adapted\n+        over a dataset or supplied with a vocabulary.\n \n         During `adapt()`, the layer will build a vocabulary of all string tokens\n-        seen in the dataset, sorted by occurrence count, with ties broken by sort\n-        order of the tokens (high to low). At the end of `adapt()`, if `max_tokens`\n-        is set, the vocabulary wil be truncated to `max_tokens` size. For example,\n-        adapting a layer with `max_tokens=1000` will compute the 1000 most frequent\n-        tokens occurring in the input dataset. If `output_mode='tf-idf'`, `adapt()`\n-        will also learn the document frequencies of each token in the input dataset.\n+        seen in the dataset, sorted by occurrence count, with ties broken by\n+        sort order of the tokens (high to low). At the end of `adapt()`, if\n+        `max_tokens` is set, the vocabulary wil be truncated to `max_tokens`\n+        size. For example, adapting a layer with `max_tokens=1000` will compute\n+        the 1000 most frequent tokens occurring in the input dataset. If\n+        `output_mode='tf-idf'`, `adapt()` will also learn the document\n+        frequencies of each token in the input dataset.\n \n-        In order to make `TextVectorization` efficient in any distribution context,\n-        the vocabulary is kept static with respect to any compiled `tf.Graph`s that\n-        call the layer. As a consequence, if the layer is adapted a second time,\n-        any models using the layer should be re-compiled. For more information\n-        see `tf.keras.layers.experimental.preprocessing.PreprocessingLayer.adapt`.\n+        In order to make `TextVectorization` efficient in any distribution\n+        context, the vocabulary is kept static with respect to any compiled\n+        `tf.Graph`s that call the layer. As a consequence, if the layer is\n+        adapted a second time, any models using the layer should be re-compiled.\n+        For more information see\n+        `tf.keras.layers.experimental.preprocessing.PreprocessingLayer.adapt`.\n \n-        `adapt()` is meant only as a single machine utility to compute layer state.\n-        To analyze a dataset that cannot fit on a single machine, see\n-        [Tensorflow Transform](https://www.tensorflow.org/tfx/transform/get_started)\n-        for a multi-machine, map-reduce solution.\n+        `adapt()` is meant only as a single machine utility to compute layer\n+        state.  To analyze a dataset that cannot fit on a single machine, see\n+        [Tensorflow Transform](\n+        https://www.tensorflow.org/tfx/transform/get_started) for a\n+        multi-machine, map-reduce solution.\n \n         Arguments:\n           data: The data to train on. It can be passed either as a\n@@ -470,9 +479,9 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n \n         Args:\n           include_special_tokens: If True, the returned vocabulary will include\n-            the padding and OOV tokens, and a term's index in the vocabulary will\n-            equal the term's index when calling the layer. If False, the returned\n-            vocabulary will not include any padding or OOV tokens.\n+            the padding and OOV tokens, and a term's index in the vocabulary\n+            will equal the term's index when calling the layer. If False, the\n+            returned vocabulary will not include any padding or OOV tokens.\n         \"\"\"\n         return self._lookup_layer.get_vocabulary(include_special_tokens)\n \n@@ -508,27 +517,27 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n         \"\"\"Sets vocabulary (and optionally document frequency) data for this layer.\n \n         This method sets the vocabulary and idf weights for this layer directly,\n-        instead of analyzing a dataset through 'adapt'. It should be used whenever\n-        the vocab (and optionally document frequency) information is already known.\n-        If vocabulary data is already present in the layer, this method will replace\n-        it.\n+        instead of analyzing a dataset through 'adapt'. It should be used\n+        whenever the vocab (and optionally document frequency) information is\n+        already known.  If vocabulary data is already present in the layer, this\n+        method will replace it.\n \n         Args:\n-          vocabulary: Either an array or a string path to a text file. If passing an\n-            array, can pass a tuple, list, 1D numpy array, or 1D tensor containing\n-            the vocbulary terms. If passing a file path, the file should contain one\n-            line per term in the vocabulary.\n+          vocabulary: Either an array or a string path to a text file. If\n+            passing an array, can pass a tuple, list, 1D numpy array, or 1D\n+            tensor containing the vocbulary terms. If passing a file path, the\n+            file should contain one line per term in the vocabulary.\n           idf_weights: A tuple, list, 1D numpy array, or 1D tensor of inverse\n-            document frequency weights with equal length to vocabulary. Must be set\n-            if `output_mode` is `\"tf_idf\"`. Should not be set otherwise.\n+            document frequency weights with equal length to vocabulary. Must be\n+            set if `output_mode` is `\"tf_idf\"`. Should not be set otherwise.\n \n         Raises:\n           ValueError: If there are too many inputs, the inputs do not match, or\n             input data is missing.\n           RuntimeError: If the vocabulary cannot be set when this function is\n-            called. This happens when `\"multi_hot\"`, `\"count\"`, and \"tf_idf\" modes,\n-            if `pad_to_max_tokens` is False and the layer itself has already been\n-            called.\n+            called. This happens when `\"multi_hot\"`, `\"count\"`, and \"tf_idf\"\n+            modes, if `pad_to_max_tokens` is False and the layer itself has\n+            already been called.\n         \"\"\"\n         self._lookup_layer.set_vocabulary(vocabulary, idf_weights=idf_weights)\n \n@@ -545,21 +554,23 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n             inputs = self._standardize(inputs)\n \n         if self._split is not None:\n-            # If we are splitting, we validate that the 1st axis is of dimension 1 and\n-            # so can be squeezed out. We do this here instead of after splitting for\n-            # performance reasons - it's more expensive to squeeze a ragged tensor.\n+            # If we are splitting, we validate that the 1st axis is of dimension\n+            # 1 and so can be squeezed out. We do this here instead of after\n+            # splitting for performance reasons - it's more expensive to squeeze\n+            # a ragged tensor.\n             if inputs.shape.rank > 1:\n                 if inputs.shape[-1] != 1:\n                     raise ValueError(\n-                        \"When using `TextVectorization` to tokenize strings, the input \"\n-                        \"rank must be 1 or the last shape dimension must be 1. Received: \"\n-                        f\"inputs.shape={inputs.shape} with rank={inputs.shape.rank}\"\n+                        \"When using `TextVectorization` to tokenize strings, \"\n+                        \"the input rank must be 1 or the last shape dimension \"\n+                        f\"must be 1. Received: inputs.shape={inputs.shape} \"\n+                        f\"with rank={inputs.shape.rank}\"\n                     )\n                 else:\n                     inputs = tf.squeeze(inputs, axis=-1)\n             if self._split == WHITESPACE:\n-                # This treats multiple whitespaces as one whitespace, and strips leading\n-                # and trailing whitespace.\n+                # This treats multiple whitespaces as one whitespace, and strips\n+                # leading and trailing whitespace.\n                 inputs = tf.strings.split(inputs)\n             elif self._split == CHARACTER:\n                 inputs = tf.strings.unicode_split(inputs, \"UTF-8\")\n@@ -576,8 +587,8 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n                 )\n \n         # Note that 'inputs' here can be either ragged or dense depending on the\n-        # configuration choices for this Layer. The strings.ngrams op, however, does\n-        # support both ragged and dense inputs.\n+        # configuration choices for this Layer. The strings.ngrams op, however,\n+        # does support both ragged and dense inputs.\n         if self._ngrams is not None:\n             inputs = tf.strings.ngrams(\n                 inputs, ngram_width=self._ngrams, separator=\" \"\n@@ -597,7 +608,8 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n \n         lookup_data = self._lookup_layer(inputs)\n \n-        # For any non-int output, we can return directly from the underlying layer.\n+        # For any non-int output, we can return directly from the underlying\n+        # layer.\n         if self._output_mode != INT:\n             return lookup_data\n \n@@ -607,8 +619,8 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n         # If we have a ragged tensor, we can pad during the conversion to dense.\n         if tf_utils.is_ragged(lookup_data):\n             shape = lookup_data.shape.as_list()\n-            # If output sequence length is None, to_tensor will pad the last dimension\n-            # to the bounding shape of the ragged dimension.\n+            # If output sequence length is None, to_tensor will pad the last\n+            # dimension to the bounding shape of the ragged dimension.\n             shape[-1] = self._output_sequence_length\n             return lookup_data.to_tensor(default_value=0, shape=shape)\n \n@@ -617,8 +629,9 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n             # Maybe trim the output.\n             lookup_data = lookup_data[..., : self._output_sequence_length]\n \n-            # Maybe pad the output. We need to be careful to use dynamic shape here as\n-            # required_space_to_batch_paddings requires a fully known shape.\n+            # Maybe pad the output. We need to be careful to use dynamic shape\n+            # here as required_space_to_batch_paddings requires a fully known\n+            # shape.\n             shape = tf.shape(lookup_data)\n             padded_shape = tf.concat(\n                 (shape[:-1], [self._output_sequence_length]), 0\n\n@@ -80,7 +80,8 @@ class TextVectorizationDistributionTest(\n         self.assertAllEqual(expected_output, output_dataset)\n \n     def test_distribution_strategy_output_with_adapt(self, strategy):\n-        # TODO(b/180614455): remove this check when MLIR bridge is always enabled.\n+        # TODO(b/180614455): remove this check when MLIR bridge is always\n+        # enabled.\n         if backend.is_tpu_strategy(strategy):\n             self.skipTest(\"This test needs MLIR bridge on TPU.\")\n \n\n@@ -37,9 +37,9 @@ def _get_end_to_end_test_cases():\n     test_cases = (\n         {\n             \"testcase_name\": \"test_simple_tokens_int_mode\",\n-            # Create an array where 'earth' is the most frequent term, followed by\n-            # 'wind', then 'and', then 'fire'. This ensures that the vocab\n-            # is sorting by frequency.\n+            # Create an array where 'earth' is the most frequent term, followed\n+            # by 'wind', then 'and', then 'fire'. This ensures that the vocab is\n+            # sorting by frequency.\n             \"vocab_data\": np.array(\n                 [\n                     [\"fire\"],\n@@ -76,9 +76,9 @@ def _get_end_to_end_test_cases():\n         },\n         {\n             \"testcase_name\": \"test_simple_tokens_int_mode_hard_cap\",\n-            # Create an array where 'earth' is the most frequent term, followed by\n-            # 'wind', then 'and', then 'fire'. This ensures that the vocab\n-            # is sorting by frequency.\n+            # Create an array where 'earth' is the most frequent term, followed\n+            # by 'wind', then 'and', then 'fire'. This ensures that the vocab is\n+            # sorting by frequency.\n             \"vocab_data\": np.array(\n                 [\n                     [\"fire\"],\n@@ -115,8 +115,8 @@ def _get_end_to_end_test_cases():\n         },\n         {\n             \"testcase_name\": \"test_special_tokens_int_mode\",\n-            # Mask tokens in the vocab data should be ignored, and mapped to 0 in\n-            # from the input data.\n+            # Mask tokens in the vocab data should be ignored, and mapped to 0\n+            # in from the input data.\n             \"vocab_data\": np.array(\n                 [\n                     [\"fire\"],\n@@ -463,13 +463,14 @@ class TextVectorizationLayerTest(\n             # dataset batch separately, then tries to concatenate the results\n             # together. When the results have different shapes on the non-concat\n             # axis (which can happen in the output_mode = INT case for\n-            # TextVectorization), the concatenation fails. In real use cases, this may\n-            # not be an issue because users are likely to pipe the preprocessing layer\n-            # into other keras layers instead of predicting it directly. A workaround\n-            # for these unit tests is to have the dataset only contain one batch, so\n-            # no concatenation needs to happen with the result. For consistency with\n-            # numpy input, we should make `predict` join differently shaped results\n-            # together sensibly, with 0 padding.\n+            # TextVectorization), the concatenation fails. In real use cases,\n+            # this may not be an issue because users are likely to pipe the\n+            # preprocessing layer into other keras layers instead of predicting\n+            # it directly. A workaround for these unit tests is to have the\n+            # dataset only contain one batch, so no concatenation needs to\n+            # happen with the result. For consistency with numpy input, we\n+            # should make `predict` join differently shaped results together\n+            # sensibly, with 0 padding.\n             input_data = tf.data.Dataset.from_tensor_slices(input_data).batch(\n                 input_shape[0]\n             )\n@@ -738,8 +739,8 @@ class TextVectorizationPreprocessingTest(\n         )\n         int_data = layer(input_data)\n         model = keras.Model(inputs=input_data, outputs=int_data)\n-        # We are testing that model.summary() can be called without erroring out.\n-        # (b/145726907)\n+        # We are testing that model.summary() can be called without erroring\n+        # out. (b/145726907)\n         model.summary()\n \n     @parameterized.parameters([list, np.array, tf.constant, tf.ragged.constant])\n@@ -1291,9 +1292,9 @@ class TextVectorizationOutputTest(\n \n     def test_int_output_densifies_with_zeros(self):\n         vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n-        # Create an input array that has 5 elements in the first example and 4 in\n-        # the second. This should output a 2x5 tensor with a padding value in the\n-        # second example.\n+        # Create an input array that has 5 elements in the first example and 4\n+        # in the second. This should output a 2x5 tensor with a padding value in\n+        # the second example.\n         input_array = np.array(\n             [[\"earth wind and also fire\"], [\"fire and earth michigan\"]]\n         )\n@@ -1321,8 +1322,8 @@ class TextVectorizationOutputTest(\n \n     def test_int_output_ragged(self):\n         vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n-        # Create an input array that has 5 elements in the first example and 4 in\n-        # the second.\n+        # Create an input array that has 5 elements in the first example and 4\n+        # in the second.\n         input_array = np.array(\n             [[\"earth wind and also fire\"], [\"fire and earth michigan\"]]\n         )\n@@ -1348,9 +1349,9 @@ class TextVectorizationOutputTest(\n \n     def test_int_output_densifies_with_zeros_and_pads(self):\n         vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n-        # Create an input array that has 5 elements in the first example and 4 in\n-        # the second. This should output a 2x6 tensor with a padding value in the\n-        # second example, since output_sequence_length is set to 6.\n+        # Create an input array that has 5 elements in the first example and 4\n+        # in the second. This should output a 2x6 tensor with a padding value in\n+        # the second example, since output_sequence_length is set to 6.\n         input_array = np.array(\n             [[\"earth wind and also fire\"], [\"fire and earth michigan\"]]\n         )\n@@ -1378,9 +1379,9 @@ class TextVectorizationOutputTest(\n \n     def test_int_output_densifies_with_zeros_and_strips(self):\n         vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n-        # Create an input array that has 5 elements in the first example and 4 in\n-        # the second. This should output a 2x3 tensor with a padding value in the\n-        # second example, since output_sequence_length is set to 3.\n+        # Create an input array that has 5 elements in the first example and 4\n+        # in the second. This should output a 2x3 tensor with a padding value in\n+        # the second example, since output_sequence_length is set to 3.\n         input_array = np.array(\n             [[\"earth wind and also fire\"], [\"fire and earth michigan\"]]\n         )\n@@ -1407,9 +1408,9 @@ class TextVectorizationOutputTest(\n \n     def test_int_output_dynamically_strips_and_pads(self):\n         vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n-        # Create an input array that has 5 elements in the first example and 4 in\n-        # the second. This should output a 2x3 tensor with a padding value in the\n-        # second example, since output_sequence_length is set to 3.\n+        # Create an input array that has 5 elements in the first example and 4\n+        # in the second. This should output a 2x3 tensor with a padding value in\n+        # the second example, since output_sequence_length is set to 3.\n         input_array = np.array(\n             [[\"earth wind and also fire\"], [\"fire and earth michigan\"]]\n         )\n@@ -1435,8 +1436,8 @@ class TextVectorizationOutputTest(\n         self.assertAllEqual(expected_output, output_dataset)\n \n         # Create an input array that has 1 element in the first example and 2 in\n-        # the second. This should output a 2x3 tensor with a padding value in the\n-        # second example, since output_sequence_length is set to 3.\n+        # the second. This should output a 2x3 tensor with a padding value in\n+        # the second example, since output_sequence_length is set to 3.\n         input_array_2 = np.array([[\"wind\"], [\"fire and\"]])\n         expected_output_2 = [[3, 0, 0], [5, 4, 0]]\n         output_dataset = model.predict(input_array_2)\n@@ -2241,8 +2242,8 @@ class TextVectorizationSavingTest(\n \n         model.save(output_path, save_format=\"tf\")\n \n-        # Delete the session and graph to ensure that the loaded model is generated\n-        # from scratch.\n+        # Delete the session and graph to ensure that the loaded model is\n+        # generated from scratch.\n         keras.backend.clear_session()\n \n         loaded_model = keras.models.load_model(output_path)\n@@ -2285,8 +2286,8 @@ class TextVectorizationSavingTest(\n         output_path = os.path.join(self.get_temp_dir(), \"tf_keras_saved_model\")\n         outer_model.save(output_path, save_format=\"tf\")\n \n-        # Delete the session and graph to ensure that the loaded model is generated\n-        # from scratch.\n+        # Delete the session and graph to ensure that the loaded model is\n+        # generated from scratch.\n         keras.backend.clear_session()\n \n         loaded_model = keras.models.load_model(output_path)\n@@ -2330,8 +2331,8 @@ class TextVectorizationSavingTest(\n \n         model.save(output_path, save_format=\"tf\")\n \n-        # Delete the session and graph to ensure that the loaded model is generated\n-        # from scratch.\n+        # Delete the session and graph to ensure that the loaded model is\n+        # generated from scratch.\n         keras.backend.clear_session()\n \n         loaded_model = keras.models.load_model(output_path)\n\n@@ -90,9 +90,9 @@ class Dropout(base_layer.BaseRandomLayer):\n         self._random_generator._maybe_init()  # pylint: disable=protected-access\n \n     def _get_noise_shape(self, inputs):\n-        # Subclasses of `Dropout` may implement `_get_noise_shape(self, inputs)`,\n-        # which will override `self.noise_shape`, and allows for custom noise\n-        # shapes with dynamically sized inputs.\n+        # Subclasses of `Dropout` may implement `_get_noise_shape(self,\n+        # inputs)`, which will override `self.noise_shape`, and allows for\n+        # custom noise shapes with dynamically sized inputs.\n         if self.noise_shape is None:\n             return None\n \n\n@@ -57,9 +57,9 @@ class DropoutTest(test_combinations.TestCase):\n         model = keras.Model(inputs, outputs)\n         train = model(np.ones((20, 5, 10)), training=True)\n         predict = model(np.ones((20, 5, 10)))\n-        # Make sure the weights from tf.random.Generator is not present in the model\n-        # which will cause weight loading issue for existing application models if\n-        # it contains dropout layer.\n+        # Make sure the weights from tf.random.Generator is not present in the\n+        # model which will cause weight loading issue for existing application\n+        # models if it contains dropout layer.\n         self.assertEmpty(layer.get_weights())\n         self.assertEmpty(model.get_weights())\n \n\n@@ -37,11 +37,12 @@ class SpatialDropout2D(Dropout):\n \n     Args:\n       rate: Float between 0 and 1. Fraction of the input units to drop.\n-      data_format: 'channels_first' or 'channels_last'. In 'channels_first' mode,\n-        the channels dimension (the depth) is at index 1, in 'channels_last' mode\n-        is it at index 3. It defaults to the `image_data_format` value found in\n-        your Keras config file at `~/.keras/keras.json`. If you never set it, then\n-        it will be \"channels_last\".\n+      data_format: 'channels_first' or 'channels_last'. In 'channels_first'\n+        mode, the channels dimension (the depth) is at index 1, in\n+        'channels_last' mode is it at index 3. It defaults to the\n+        `image_data_format` value found in your Keras config file at\n+        `~/.keras/keras.json`. If you never set it, then it will be\n+        \"channels_last\".\n     Call arguments:\n       inputs: A 4D tensor.\n       training: Python boolean indicating whether the layer should behave in\n\n@@ -37,11 +37,12 @@ class SpatialDropout3D(Dropout):\n \n     Args:\n       rate: Float between 0 and 1. Fraction of the input units to drop.\n-      data_format: 'channels_first' or 'channels_last'. In 'channels_first' mode,\n-        the channels dimension (the depth) is at index 1, in 'channels_last' mode\n-        is it at index 4. It defaults to the `image_data_format` value found in\n-        your Keras config file at `~/.keras/keras.json`. If you never set it, then\n-        it will be \"channels_last\".\n+      data_format: 'channels_first' or 'channels_last'. In 'channels_first'\n+        mode, the channels dimension (the depth) is at index 1, in\n+        'channels_last' mode is it at index 4. It defaults to the\n+        `image_data_format` value found in your Keras config file at\n+        `~/.keras/keras.json`. If you never set it, then it will be\n+        \"channels_last\".\n     Call arguments:\n       inputs: A 5D tensor.\n       training: Python boolean indicating whether the layer should behave in\n\n@@ -59,8 +59,8 @@ class Cropping3D(Layer):\n     Input shape:\n       5D tensor with shape:\n       - If `data_format` is `\"channels_last\"`:\n-        `(batch_size, first_axis_to_crop, second_axis_to_crop, third_axis_to_crop,\n-          depth)`\n+        `(batch_size, first_axis_to_crop, second_axis_to_crop,\n+        third_axis_to_crop, depth)`\n       - If `data_format` is `\"channels_first\"`:\n         `(batch_size, depth, first_axis_to_crop, second_axis_to_crop,\n           third_axis_to_crop)`\n@@ -68,8 +68,8 @@ class Cropping3D(Layer):\n     Output shape:\n       5D tensor with shape:\n       - If `data_format` is `\"channels_last\"`:\n-        `(batch_size, first_cropped_axis, second_cropped_axis, third_cropped_axis,\n-          depth)`\n+        `(batch_size, first_cropped_axis, second_cropped_axis,\n+        third_cropped_axis, depth)`\n       - If `data_format` is `\"channels_first\"`:\n         `(batch_size, depth, first_cropped_axis, second_cropped_axis,\n           third_cropped_axis)`\n@@ -106,7 +106,8 @@ class Cropping3D(Layer):\n             raise ValueError(\n                 \"`cropping` should be either an int, \"\n                 \"a tuple of 3 ints \"\n-                \"(symmetric_dim1_crop, symmetric_dim2_crop, symmetric_dim3_crop), \"\n+                \"(symmetric_dim1_crop, symmetric_dim2_crop, \"\n+                \"symmetric_dim3_crop), \"\n                 \"or a tuple of 3 tuples of 2 ints \"\n                 \"((left_dim1_crop, right_dim1_crop),\"\n                 \" (left_dim2_crop, right_dim2_crop),\"\n@@ -301,13 +302,7 @@ class Cropping3D(Layer):\n                 :,\n                 self.cropping[0][0] : -self.cropping[0][1],\n                 self.cropping[1][0] : -self.cropping[1][1],\n-                self.cropping[2][\n-                    0\n-                ] : -self.cropping[  # pylint: disable=invalid-unary-operand-type\n-                    2\n-                ][\n-                    1\n-                ],\n+                self.cropping[2][0] : -self.cropping[2][1],\n                 :,\n             ]  # pylint: disable=invalid-unary-operand-type\n         # pylint: enable=invalid-unary-operand-type\n\n@@ -76,7 +76,8 @@ class Flatten(Layer):\n \n         if tf.executing_eagerly():\n             # Full static shape is guaranteed to be available.\n-            # Performance: Using `constant_op` is much faster than passing a list.\n+            # Performance: Using `constant_op` is much faster than passing a\n+            # list.\n             flattened_shape = tf.constant([inputs.shape[0], -1])\n             return tf.reshape(inputs, flattened_shape)\n         else:\n@@ -87,7 +88,8 @@ class Flatten(Layer):\n             else:\n                 batch_dim = tf.compat.dimension_value(input_shape[0])\n                 non_batch_dims = input_shape[1:]\n-                # Reshape in a way that preserves as much shape info as possible.\n+                # Reshape in a way that preserves as much shape info as\n+                # possible.\n                 if non_batch_dims.is_fully_defined():\n                     last_dim = int(\n                         functools.reduce(operator.mul, non_batch_dims)\n\n@@ -61,8 +61,8 @@ class Permute(Layer):\n         if sorted(dims) != list(range(1, len(dims) + 1)):\n             raise ValueError(\n                 \"Invalid permutation argument `dims` for Permute Layer. \"\n-                \"The set of indices in `dims` must be consecutive and start from 1. \"\n-                f\"Received dims={dims}\"\n+                \"The set of indices in `dims` must be consecutive and start \"\n+                f\"from 1. Received dims={dims}\"\n             )\n         self.input_spec = InputSpec(ndim=len(self.dims) + 1)\n \n\n@@ -28,9 +28,9 @@ class Reshape(Layer):\n \n     Input shape:\n       Arbitrary, although all dimensions in the input shape must be known/fixed.\n-      Use the keyword argument `input_shape` (tuple of integers, does not include\n-      the samples/batch size axis) when using this layer as the first layer\n-      in a model.\n+      Use the keyword argument `input_shape` (tuple of integers, does not\n+      include the samples/batch size axis) when using this layer as the first\n+      layer in a model.\n \n     Output shape:\n       `(batch_size,) + target_shape`\n@@ -74,8 +74,9 @@ class Reshape(Layer):\n \n         Args:\n           input_shape: Shape of array being reshaped\n-          output_shape: Desired shape of the array with at most a single -1 which\n-            indicates a dimension that should be derived from the input shape.\n+          output_shape: Desired shape of the array with at most a single -1\n+            which indicates a dimension that should be derived from the input\n+            shape.\n \n         Returns:\n           The new output shape with a -1 replaced with its computed value.\n@@ -100,8 +101,8 @@ class Reshape(Layer):\n                     unknown = index\n                 else:\n                     raise ValueError(\n-                        f\"There must be at most one unknown dimension in output_shape. \"\n-                        f\"Received: output_shape={output_shape}.\"\n+                        f\"There must be at most one unknown dimension in \"\n+                        f\"output_shape. Received: output_shape={output_shape}.\"\n                     )\n             else:\n                 known *= dim\n@@ -133,8 +134,9 @@ class Reshape(Layer):\n     def call(self, inputs):\n         result = tf.reshape(inputs, (tf.shape(inputs)[0],) + self.target_shape)\n         if not tf.executing_eagerly():\n-            # Set the static shape for the result since it might lost during array_ops\n-            # reshape, eg, some `None` dim in the result could be inferred.\n+            # Set the static shape for the result since it might lost during\n+            # array_ops reshape, eg, some `None` dim in the result could be\n+            # inferred.\n             result.set_shape(self.compute_output_shape(inputs.shape))\n         return result\n \n\n@@ -66,7 +66,8 @@ class UpSampling2D(Layer):\n         Keras config file at `~/.keras/keras.json`.\n         If you never set it, then it will be \"channels_last\".\n       interpolation: A string, one of `\"area\"`, `\"bicubic\"`, `\"bilinear\"`,\n-        `\"gaussian\"`, `\"lanczos3\"`, `\"lanczos5\"`, `\"mitchellcubic\"`, `\"nearest\"`.\n+        `\"gaussian\"`, `\"lanczos3\"`, `\"lanczos5\"`, `\"mitchellcubic\"`,\n+        `\"nearest\"`.\n \n     Input shape:\n       4D tensor with shape:\n\n@@ -63,9 +63,11 @@ class UpSampling3D(Layer):\n     Output shape:\n       5D tensor with shape:\n       - If `data_format` is `\"channels_last\"`:\n-          `(batch_size, upsampled_dim1, upsampled_dim2, upsampled_dim3, channels)`\n+          `(batch_size, upsampled_dim1, upsampled_dim2, upsampled_dim3,\n+          channels)`\n       - If `data_format` is `\"channels_first\"`:\n-          `(batch_size, channels, upsampled_dim1, upsampled_dim2, upsampled_dim3)`\n+          `(batch_size, channels, upsampled_dim1, upsampled_dim2,\n+          upsampled_dim3)`\n     \"\"\"\n \n     def __init__(self, size=(2, 2, 2), data_format=None, **kwargs):\n\n@@ -62,8 +62,8 @@ class ZeroPadding3D(Layer):\n     Input shape:\n       5D tensor with shape:\n       - If `data_format` is `\"channels_last\"`:\n-          `(batch_size, first_axis_to_pad, second_axis_to_pad, third_axis_to_pad,\n-            depth)`\n+          `(batch_size, first_axis_to_pad, second_axis_to_pad,\n+          third_axis_to_pad, depth)`\n       - If `data_format` is `\"channels_first\"`:\n           `(batch_size, depth, first_axis_to_pad, second_axis_to_pad,\n           third_axis_to_pad)`\n@@ -71,8 +71,8 @@ class ZeroPadding3D(Layer):\n     Output shape:\n       5D tensor with shape:\n       - If `data_format` is `\"channels_last\"`:\n-          `(batch_size, first_padded_axis, second_padded_axis, third_axis_to_pad,\n-            depth)`\n+          `(batch_size, first_padded_axis, second_padded_axis,\n+          third_axis_to_pad, depth)`\n       - If `data_format` is `\"channels_first\"`:\n           `(batch_size, depth, first_padded_axis, second_padded_axis,\n             third_axis_to_pad)`\n\n@@ -13,7 +13,6 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras recurrent layers.\"\"\"\n-# pylint: disable=g-bad-import-order,g-direct-tensorflow-import,disable=g-import-not-at-top\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -81,8 +81,8 @@ class AbstractRNNCell(base_layer.Layer):\n         \"\"\"The function that contains the logic for one RNN step calculation.\n \n         Args:\n-          inputs: the input tensor, which is a slide from the overall RNN input by\n-            the time dimension (usually the second dimension).\n+          inputs: the input tensor, which is a slide from the overall RNN input\n+            by the time dimension (usually the second dimension).\n           states: the state tensor from previous step, which has the same shape\n             as `(batch, state_size)`. In the case of timestep 0, it will be the\n             initial state user specified, or zero filled tensor otherwise.\n@@ -98,8 +98,8 @@ class AbstractRNNCell(base_layer.Layer):\n     def state_size(self):\n         \"\"\"size(s) of state(s) used by this cell.\n \n-        It can be represented by an Integer, a TensorShape or a tuple of Integers\n-        or TensorShapes.\n+        It can be represented by an Integer, a TensorShape or a tuple of\n+        Integers or TensorShapes.\n         \"\"\"\n         raise NotImplementedError\n \n\n@@ -32,21 +32,21 @@ class ConvLSTMCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n \n     Args:\n       rank: Integer, rank of the convolution, e.g. \"2\" for 2D convolutions.\n-      filters: Integer, the dimensionality of the output space (i.e. the number of\n-        output filters in the convolution).\n+      filters: Integer, the dimensionality of the output space (i.e. the number\n+        of output filters in the convolution).\n       kernel_size: An integer or tuple/list of n integers, specifying the\n         dimensions of the convolution window.\n       strides: An integer or tuple/list of n integers, specifying the strides of\n         the convolution. Specifying any stride value != 1 is incompatible with\n         specifying any `dilation_rate` value != 1.\n-      padding: One of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means no\n-        padding. `\"same\"` results in padding evenly to the left/right or up/down\n-        of the input such that output has the same height/width dimension as the\n-        input.\n-      data_format: A string, one of `channels_last` (default) or `channels_first`.\n-        It defaults to the `image_data_format` value found in your Keras config\n-        file at `~/.keras/keras.json`. If you never set it, then it will be\n-        \"channels_last\".\n+      padding: One of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means\n+        no padding. `\"same\"` results in padding evenly to the left/right or\n+        up/down of the input such that output has the same height/width\n+        dimension as the input.\n+      data_format: A string, one of `channels_last` (default) or\n+        `channels_first`.  It defaults to the `image_data_format` value found in\n+        your Keras config file at `~/.keras/keras.json`. If you never set it,\n+        then it will be \"channels_last\".\n       dilation_rate: An integer or tuple/list of n integers, specifying the\n         dilation rate to use for dilated convolution. Currently, specifying any\n         `dilation_rate` value != 1 is incompatible with specifying any `strides`\n@@ -61,9 +61,9 @@ class ConvLSTMCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n       recurrent_initializer: Initializer for the `recurrent_kernel` weights\n         matrix, used for the linear transformation of the recurrent state.\n       bias_initializer: Initializer for the bias vector.\n-      unit_forget_bias: Boolean. If True, add 1 to the bias of the forget gate at\n-        initialization. Use in combination with `bias_initializer=\"zeros\"`. This\n-        is recommended in [Jozefowicz et al., 2015](\n+      unit_forget_bias: Boolean. If True, add 1 to the bias of the forget gate\n+      at initialization. Use in combination with `bias_initializer=\"zeros\"`.\n+      This is recommended in [Jozefowicz et al., 2015](\n       http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n       kernel_regularizer: Regularizer function applied to the `kernel` weights\n         matrix.\n@@ -72,13 +72,13 @@ class ConvLSTMCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n       bias_regularizer: Regularizer function applied to the bias vector.\n       kernel_constraint: Constraint function applied to the `kernel` weights\n         matrix.\n-      recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n-        weights matrix.\n+      recurrent_constraint: Constraint function applied to the\n+        `recurrent_kernel` weights matrix.\n       bias_constraint: Constraint function applied to the bias vector.\n-      dropout: Float between 0 and 1. Fraction of the units to drop for the linear\n-        transformation of the inputs.\n-      recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n-        the linear transformation of the recurrent state.\n+      dropout: Float between 0 and 1. Fraction of the units to drop for the\n+        linear transformation of the inputs.\n+      recurrent_dropout: Float between 0 and 1. Fraction of the units to drop\n+        for the linear transformation of the recurrent state.\n     Call arguments:\n       inputs: A (2+ `rank`)D tensor.\n       states:  List of state tensors corresponding to the previous timestep.\n@@ -161,8 +161,9 @@ class ConvLSTMCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n             channel_axis = -1\n         if input_shape[channel_axis] is None:\n             raise ValueError(\n-                \"The channel dimension of the inputs (last axis) should be defined. \"\n-                f\"Found None. Full input shape received: input_shape={input_shape}\"\n+                \"The channel dimension of the inputs (last axis) should be \"\n+                \"defined. Found None. Full input shape received: \"\n+                f\"input_shape={input_shape}\"\n             )\n         input_dim = input_shape[channel_axis]\n         self.kernel_shape = self.kernel_size + (input_dim, self.filters * 4)\n\n@@ -35,14 +35,14 @@ class ConvRNN(RNN):\n         `call(input_at_t, states_at_t)` method, returning `(output_at_t,\n         states_at_t_plus_1)`. The call method of the cell can also take the\n         optional argument `constants`, see section \"Note on passing external\n-        constants\" below. - a `state_size` attribute. This can be a single integer\n-        (single state) in which case it is the number of channels of the recurrent\n-        state (which should be the same as the number of channels of the cell\n-        output). This can also be a list/tuple of integers (one size per state).\n-        In this case, the first entry (`state_size[0]`) should be the same as the\n-        size of the cell output.\n-      return_sequences: Boolean. Whether to return the last output. in the output\n-        sequence, or the full sequence.\n+        constants\" below. - a `state_size` attribute. This can be a single\n+        integer (single state) in which case it is the number of channels of the\n+        recurrent state (which should be the same as the number of channels of\n+        the cell output). This can also be a list/tuple of integers (one size\n+        per state).  In this case, the first entry (`state_size[0]`) should be\n+        the same as the size of the cell output.\n+      return_sequences: Boolean. Whether to return the last output. in the\n+        output sequence, or the full sequence.\n       return_state: Boolean. Whether to return the last state in addition to the\n         output.\n       go_backwards: Boolean (default False). If True, process the input sequence\n@@ -59,8 +59,8 @@ class ConvRNN(RNN):\n       training: Python boolean indicating whether the layer should behave in\n         training mode or in inference mode. This argument is passed to the cell\n         when calling it. This is for use with cells that use dropout.\n-      initial_state: List of initial state tensors to be passed to the first call\n-        of the cell.\n+      initial_state: List of initial state tensors to be passed to the first\n+        call of the cell.\n       constants: List of constant tensors to be passed to the cell at each\n         timestep.\n     Input shape:\n@@ -69,13 +69,13 @@ class ConvRNN(RNN):\n       if data_format='channels_first' or shape: `(samples, timesteps,\n         img_dimensions..., channels)` if data_format='channels_last'.\n     Output shape:\n-      - If `return_state`: a list of tensors. The first tensor is the output. The\n-        remaining tensors are the last states,\n+      - If `return_state`: a list of tensors. The first tensor is the output.\n+        The remaining tensors are the last states,\n         each (2 + `rank`)D tensor with shape: `(samples, filters,\n           new_img_dimensions...)` if data_format='channels_first'\n         or shape: `(samples, new_img_dimensions..., filters)` if\n-          data_format='channels_last'. img_dimension values might have changed due\n-          to padding.\n+          data_format='channels_last'. img_dimension values might have changed\n+          due to padding.\n       - If `return_sequences`: (3 + `rank`)D tensor with shape: `(samples,\n         timesteps, filters, new_img_dimensions...)` if\n         data_format='channels_first'\n@@ -85,37 +85,39 @@ class ConvRNN(RNN):\n         new_img_dimensions...)` if data_format='channels_first'\n         or shape: `(samples, new_img_dimensions..., filters)` if\n           data_format='channels_last'.\n-    Masking: This layer supports masking for input data with a variable number of\n-      timesteps.\n+    Masking: This layer supports masking for input data with a variable number\n+      of timesteps.\n     Note on using statefulness in RNNs: You can set RNN layers to be 'stateful',\n       which means that the states computed for the samples in one batch will be\n       reused as initial states for the samples in the next batch. This assumes a\n       one-to-one mapping between samples in different successive batches.\n-      To enable statefulness: - Specify `stateful=True` in the layer constructor.\n+      To enable statefulness: - Specify `stateful=True` in the layer\n+      constructor.\n         - Specify a fixed batch size for your model, by passing\n-            - If sequential model: `batch_input_shape=(...)` to the first layer in\n-              your model.\n-            - If functional model with 1 or more Input layers: `batch_shape=(...)`\n-              to all the first layers in your model. This is the expected shape of\n-              your inputs *including the batch size*. It should be a tuple of\n-              integers, e.g. `(32, 10, 100, 100, 32)`. for rank 2 convolution Note\n-              that the image dimensions should be specified too. - Specify\n-              `shuffle=False` when calling fit(). To reset the states of your\n-              model, call `.reset_states()` on either a specific layer, or on your\n-              entire model.\n+            - If sequential model: `batch_input_shape=(...)` to the first layer\n+              in your model.\n+            - If functional model with 1 or more Input layers:\n+              `batch_shape=(...)` to all the first layers in your model. This is\n+              the expected shape of your inputs *including the batch size*. It\n+              should be a tuple of integers, e.g. `(32, 10, 100, 100, 32)`. for\n+              rank 2 convolution Note that the image dimensions should be\n+              specified too. - Specify `shuffle=False` when calling fit(). To\n+              reset the states of your model, call `.reset_states()` on either a\n+              specific layer, or on your entire model.\n     Note on specifying the initial state of RNNs: You can specify the initial\n       state of RNN layers symbolically by calling them with the keyword argument\n-      `initial_state`. The value of `initial_state` should be a tensor or list of\n-      tensors representing the initial state of the RNN layer. You can specify the\n-      initial state of RNN layers numerically by calling `reset_states` with the\n-      keyword argument `states`. The value of `states` should be a numpy array or\n-      list of numpy arrays representing the initial state of the RNN layer.\n-    Note on passing external constants to RNNs: You can pass \"external\" constants\n-      to the cell using the `constants` keyword argument of `RNN.__call__` (as\n-      well as `RNN.call`) method. This requires that the `cell.call` method\n-      accepts the same keyword argument `constants`. Such constants can be used to\n-      condition the cell transformation on additional static inputs (not changing\n-      over time), a.k.a. an attention mechanism.\n+      `initial_state`. The value of `initial_state` should be a tensor or list\n+      of tensors representing the initial state of the RNN layer. You can\n+      specify the initial state of RNN layers numerically by calling\n+      `reset_states` with the keyword argument `states`. The value of `states`\n+      should be a numpy array or list of numpy arrays representing the initial\n+      state of the RNN layer.\n+    Note on passing external constants to RNNs: You can pass \"external\"\n+      constants to the cell using the `constants` keyword argument of\n+      `RNN.__call__` (as well as `RNN.call`) method. This requires that the\n+      `cell.call` method accepts the same keyword argument `constants`. Such\n+      constants can be used to condition the cell transformation on additional\n+      static inputs (not changing over time), a.k.a. an attention mechanism.\n     \"\"\"\n \n     def __init__(\n@@ -169,7 +171,7 @@ class ConvRNN(RNN):\n \n         norm_img_dims = tuple(\n             [\n-                conv_utils.conv_output_length(  # pylint: disable=g-complex-comprehension\n+                conv_utils.conv_output_length(\n                     img_dims[idx],\n                     cell.kernel_size[idx],\n                     padding=cell.padding,\n@@ -433,8 +435,8 @@ class ConvRNN(RNN):\n                     dim = self.cell.state_size\n                 if value.shape != get_tuple_shape(dim):\n                     raise ValueError(\n-                        f\"State {index} is incompatible with layer {self.name}: \"\n-                        f\"expected shape={get_tuple_shape(dim)}, \"\n+                        \"State {index} is incompatible with layer \"\n+                        f\"{self.name}: expected shape={get_tuple_shape(dim)}, \"\n                         f\"found shape={value.shape}\"\n                     )\n                 backend.set_value(state, value)\n\n@@ -35,9 +35,9 @@ class _CuDNNRNN(RNN):\n       stateful: Boolean (default False). If True, the last state\n           for each sample at index i in a batch will be used as initial\n           state for the sample of index i in the following batch.\n-      time_major: Boolean (default False). If true, the inputs and outputs will be\n-          in shape `(timesteps, batch, ...)`, whereas in the False case, it will\n-          be `(batch, timesteps, ...)`.\n+      time_major: Boolean (default False). If true, the inputs and outputs will\n+          be in shape `(timesteps, batch, ...)`, whereas in the False case, it\n+          will be `(batch, timesteps, ...)`.\n     \"\"\"\n \n     def __init__(\n\n@@ -87,8 +87,8 @@ class RNN(base_layer.Layer):\n         for each sample at index i in a batch will be used as initial\n         state for the sample of index i in the following batch.\n       unroll: Boolean (default `False`).\n-        If True, the network will be unrolled, else a symbolic loop will be used.\n-        Unrolling can speed-up a RNN, although it tends to be more\n+        If True, the network will be unrolled, else a symbolic loop will be\n+        used. Unrolling can speed-up a RNN, although it tends to be more\n         memory-intensive. Unrolling is only suitable for short sequences.\n       time_major: The shape format of the `inputs` and `outputs` tensors.\n         If True, the inputs and outputs will be in shape\n@@ -250,7 +250,8 @@ class RNN(base_layer.Layer):\n                 f\"Received: cell={cell}\"\n             )\n         # If True, the output for masked timestep will be zeros, whereas in the\n-        # False case, output from previous timestep is returned for masked timestep.\n+        # False case, output from previous timestep is returned for masked\n+        # timestep.\n         self.zero_output_for_mask = kwargs.pop(\"zero_output_for_mask\", False)\n \n         if \"input_shape\" not in kwargs and (\n@@ -272,9 +273,9 @@ class RNN(base_layer.Layer):\n         self.time_major = time_major\n \n         self.supports_masking = True\n-        # The input shape is unknown yet, it could have nested tensor inputs, and\n-        # the input spec will be the list of specs for nested inputs, the structure\n-        # of the input_spec will be the same as the input.\n+        # The input shape is unknown yet, it could have nested tensor inputs,\n+        # and the input spec will be the list of specs for nested inputs, the\n+        # structure of the input_spec will be the same as the input.\n         self.input_spec = None\n         self.state_spec = None\n         self._states = None\n@@ -291,10 +292,11 @@ class RNN(base_layer.Layer):\n     @property\n     def _use_input_spec_as_call_signature(self):\n         if self.unroll:\n-            # When the RNN layer is unrolled, the time step shape cannot be unknown.\n-            # The input spec does not define the time step (because this layer can be\n-            # called with any time step value, as long as it is not None), so it\n-            # cannot be used as the call function signature when saving to SavedModel.\n+            # When the RNN layer is unrolled, the time step shape cannot be\n+            # unknown.  The input spec does not define the time step (because\n+            # this layer can be called with any time step value, as long as it\n+            # is not None), so it cannot be used as the call function signature\n+            # when saving to SavedModel.\n             return False\n         return super()._use_input_spec_as_call_signature\n \n@@ -316,8 +318,8 @@ class RNN(base_layer.Layer):\n         if isinstance(input_shape, list):\n             input_shape = input_shape[0]\n         # Check whether the input shape contains any nested shapes. It could be\n-        # (tensor_shape(1, 2), tensor_shape(3, 4)) or (1, 2, 3) which is from numpy\n-        # inputs.\n+        # (tensor_shape(1, 2), tensor_shape(3, 4)) or (1, 2, 3) which is from\n+        # numpy inputs.\n         try:\n             input_shape = tf.TensorShape(input_shape)\n         except (ValueError, TypeError):\n@@ -393,8 +395,8 @@ class RNN(base_layer.Layer):\n             input_shape = input_shape[0]\n             # The input_shape here could be a nest structure.\n \n-        # do the tensor_shape to shapes here. The input could be single tensor, or a\n-        # nested structure of tensors.\n+        # do the tensor_shape to shapes here. The input could be single tensor,\n+        # or a nested structure of tensors.\n         def get_input_spec(shape):\n             \"\"\"Convert input shape to InputSpec.\"\"\"\n             if isinstance(shape, tf.TensorShape):\n@@ -420,8 +422,8 @@ class RNN(base_layer.Layer):\n             return InputSpec(shape=tuple(state_spec_shape))\n \n         # Check whether the input shape contains any nested shapes. It could be\n-        # (tensor_shape(1, 2), tensor_shape(3, 4)) or (1, 2, 3) which is from numpy\n-        # inputs.\n+        # (tensor_shape(1, 2), tensor_shape(3, 4)) or (1, 2, 3) which is from\n+        # numpy inputs.\n         try:\n             input_shape = tf.TensorShape(input_shape)\n         except (ValueError, TypeError):\n@@ -485,11 +487,12 @@ class RNN(base_layer.Layer):\n \n         Args:\n           cell_state_sizes: list, the `state_size` attribute from the cell.\n-          init_state_specs: list, the `state_spec` from the initial_state that is\n-            passed in `call()`.\n+          init_state_specs: list, the `state_spec` from the initial_state that\n+            is passed in `call()`.\n \n         Raises:\n-          ValueError: When initial state spec is not compatible with the state size.\n+          ValueError: When initial state spec is not compatible with the state\n+            size.\n         \"\"\"\n         validation_error = ValueError(\n             \"An `initial_state` was passed that is not compatible with \"\n@@ -516,8 +519,8 @@ class RNN(base_layer.Layer):\n         get_initial_state_fn = getattr(self.cell, \"get_initial_state\", None)\n \n         if tf.nest.is_nested(inputs):\n-            # The input are nested sequences. Use the first element in the seq to get\n-            # batch size and dtype.\n+            # The input are nested sequences. Use the first element in the seq\n+            # to get batch size and dtype.\n             inputs = tf.nest.flatten(inputs)[0]\n \n         input_shape = tf.shape(inputs)\n@@ -531,10 +534,12 @@ class RNN(base_layer.Layer):\n             init_state = rnn_utils.generate_zero_filled_state(\n                 batch_size, self.cell.state_size, dtype\n             )\n-        # Keras RNN expect the states in a list, even if it's a single state tensor.\n+        # Keras RNN expect the states in a list, even if it's a single state\n+        # tensor.\n         if not tf.nest.is_nested(init_state):\n             init_state = [init_state]\n-        # Force the state to be a list in case it is a namedtuple eg LSTMStateTuple.\n+        # Force the state to be a list in case it is a namedtuple eg\n+        # LSTMStateTuple.\n         return list(init_state)\n \n     def __call__(self, inputs, initial_state=None, constants=None, **kwargs):\n@@ -565,8 +570,8 @@ class RNN(base_layer.Layer):\n             ]\n             self._num_constants = len(constants)\n             additional_specs += self.constants_spec\n-        # additional_inputs can be empty if initial_state or constants are provided\n-        # but empty (e.g. the cell is stateless).\n+        # additional_inputs can be empty if initial_state or constants are\n+        # provided but empty (e.g. the cell is stateless).\n         flat_additional_inputs = tf.nest.flatten(additional_inputs)\n         is_keras_tensor = (\n             backend.is_keras_tensor(flat_additional_inputs[0])\n@@ -577,21 +582,23 @@ class RNN(base_layer.Layer):\n             if backend.is_keras_tensor(tensor) != is_keras_tensor:\n                 raise ValueError(\n                     \"The initial state or constants of an RNN layer cannot be \"\n-                    \"specified via a mix of Keras tensors and non-Keras tensors \"\n-                    '(a \"Keras tensor\" is a tensor that was returned by a Keras layer '\n-                    \" or by `Input` during Functional model construction). \"\n-                    f\"Received: initial_state={initial_state}, constants={constants}\"\n+                    \"specified via a mix of Keras tensors and non-Keras \"\n+                    'tensors (a \"Keras tensor\" is a tensor that was returned '\n+                    \"by a Keras layer  or by `Input` during Functional \"\n+                    \"model construction). Received: \"\n+                    f\"initial_state={initial_state}, constants={constants}\"\n                 )\n \n         if is_keras_tensor:\n             # Compute the full input spec, including state and constants\n             full_input = [inputs] + additional_inputs\n             if self.built:\n-                # Keep the input_spec since it has been populated in build() method.\n+                # Keep the input_spec since it has been populated in build()\n+                # method.\n                 full_input_spec = self.input_spec + additional_specs\n             else:\n-                # The original input_spec is None since there could be a nested tensor\n-                # input. Update the input_spec to match the inputs.\n+                # The original input_spec is None since there could be a nested\n+                # tensor input. Update the input_spec to match the inputs.\n                 full_input_spec = (\n                     generic_utils.to_list(\n                         tf.nest.map_structure(lambda _: None, inputs)\n@@ -601,9 +608,9 @@ class RNN(base_layer.Layer):\n             # Perform the call with temporarily replaced input_spec\n             self.input_spec = full_input_spec\n             output = super().__call__(full_input, **kwargs)\n-            # Remove the additional_specs from input spec and keep the rest. It is\n-            # important to keep since the input spec was populated by build(), and\n-            # will be reused in the stateful=True.\n+            # Remove the additional_specs from input spec and keep the rest. It\n+            # is important to keep since the input spec was populated by\n+            # build(), and will be reused in the stateful=True.\n             self.input_spec = self.input_spec[: -len(additional_specs)]\n             return output\n         else:\n@@ -642,7 +649,8 @@ class RNN(base_layer.Layer):\n             mask = tf.nest.flatten(mask)[0]\n \n         if tf.nest.is_nested(inputs):\n-            # In the case of nested input, use the first element for shape check.\n+            # In the case of nested input, use the first element for shape\n+            # check.\n             input_shape = backend.int_shape(tf.nest.flatten(inputs)[0])\n         else:\n             input_shape = backend.int_shape(inputs)\n@@ -666,7 +674,8 @@ class RNN(base_layer.Layer):\n         if generic_utils.has_arg(self.cell.call, \"training\"):\n             kwargs[\"training\"] = training\n \n-        # TF RNN cells expect single tensor as state instead of list wrapped tensor.\n+        # TF RNN cells expect single tensor as state instead of list wrapped\n+        # tensor.\n         is_tf_rnn_cell = getattr(self.cell, \"_is_tf_rnn_cell\", None) is not None\n         # Use the __call__ function for callable objects, eg layers, so that it\n         # will have the proper name scopes for the ops, etc.\n@@ -773,9 +782,9 @@ class RNN(base_layer.Layer):\n \n         if self.stateful:\n             if initial_state is not None:\n-                # When layer is stateful and initial_state is provided, check if the\n-                # recorded state is same as the default value (zeros). Use the recorded\n-                # state if it is not same as the default.\n+                # When layer is stateful and initial_state is provided, check if\n+                # the recorded state is same as the default value (zeros). Use\n+                # the recorded state if it is not same as the default.\n                 non_zero_count = tf.add_n(\n                     [\n                         tf.math.count_nonzero(s)\n@@ -792,7 +801,8 @@ class RNN(base_layer.Layer):\n             else:\n                 initial_state = self.states\n             initial_state = tf.nest.map_structure(\n-                # When the layer has a inferred dtype, use the dtype from the cell.\n+                # When the layer has a inferred dtype, use the dtype from the\n+                # cell.\n                 lambda v: tf.cast(\n                     v, self.compute_dtype or self.cell.compute_dtype\n                 ),\n@@ -837,9 +847,10 @@ class RNN(base_layer.Layer):\n \n         Can only be used when RNN layer is constructed with `stateful` = `True`.\n         Args:\n-          states: Numpy arrays that contains the value for the initial state, which\n-            will be feed to cell at the first time step. When the value is None,\n-            zero filled numpy array will be created based on the cell state size.\n+          states: Numpy arrays that contains the value for the initial state,\n+            which will be feed to cell at the first time step. When the value is\n+            None, zero filled numpy array will be created based on the cell\n+            state size.\n \n         Raises:\n           AttributeError: When the RNN layer is not stateful.\n@@ -853,9 +864,10 @@ class RNN(base_layer.Layer):\n         if self.input_spec is not None:\n             spec_shape = tf.nest.flatten(self.input_spec[0])[0].shape\n         if spec_shape is None:\n-            # It is possible to have spec shape to be None, eg when construct a RNN\n-            # with a custom cell, or standard RNN layers (LSTM/GRU) which we only know\n-            # it has 3 dim input, but not its full shape spec before build().\n+            # It is possible to have spec shape to be None, eg when construct a\n+            # RNN with a custom cell, or standard RNN layers (LSTM/GRU) which we\n+            # only know it has 3 dim input, but not its full shape spec before\n+            # build().\n             batch_size = None\n         else:\n             batch_size = spec_shape[1] if self.time_major else spec_shape[0]\n@@ -879,8 +891,8 @@ class RNN(base_layer.Layer):\n                     self.cell.get_initial_state(\n                         inputs=None,\n                         batch_size=batch_size,\n-                        # Use variable_dtype instead of compute_dtype, since the state is\n-                        # stored in a variable\n+                        # Use variable_dtype instead of compute_dtype, since the\n+                        # state is stored in a variable\n                         dtype=self.variable_dtype or backend.floatx(),\n                     )\n                 )\n\n@@ -849,8 +849,9 @@ class RNNTest(test_combinations.TestCase):\n \n     def test_dropout_mask_reuse(self):\n         # The layer is created with recurrent_initializer = zero, so that the\n-        # the recurrent state won't affect the output. By doing this, we can verify\n-        # the output and see if the same mask is applied to for each timestep.\n+        # the recurrent state won't affect the output. By doing this, we can\n+        # verify the output and see if the same mask is applied to for each\n+        # timestep.\n         layer_1 = keras.layers.SimpleRNN(\n             3,\n             dropout=0.5,\n@@ -1516,7 +1517,8 @@ class RNNTest(test_combinations.TestCase):\n             self.assertAllClose(result_1, result_2)\n \n     def test_unroll_single_step(self):\n-        \"\"\"Even if the time dimension is only one, we should be able to unroll.\"\"\"\n+        \"\"\"Even if the time dimension is only one, we should be able to\n+        unroll.\"\"\"\n         cell = keras.layers.SimpleRNNCell(5)\n         x = keras.Input((1, 5))\n         layer = keras.layers.RNN(cell, return_sequences=True, unroll=True)\n@@ -1656,13 +1658,14 @@ class RNNTest(test_combinations.TestCase):\n         model.reset_states()\n         predict_3 = model.predict(test_inputs)\n \n-        # predict 1 and 2 should be different since the batch 2 should use the state\n-        # from batch 1 as the initial state.\n+        # predict 1 and 2 should be different since the batch 2 should use the\n+        # state from batch 1 as the initial state.\n         self.assertNotAllClose(predict_1, predict_2)\n         self.assertAllClose(predict_1, predict_3)\n \n-        # Create a new model with same weights but without initial states. Make sure\n-        # the predict value is different from the model with non-zero initial state.\n+        # Create a new model with same weights but without initial states. Make\n+        # sure the predict value is different from the model with non-zero\n+        # initial state.\n         model_2 = make_model(stateful=True, with_initial_state=False)\n         model_2.layers[1].set_weights(layer_weights)\n \n@@ -1672,8 +1675,8 @@ class RNNTest(test_combinations.TestCase):\n         self.assertNotAllClose(predict_1, predict_4)\n         self.assertNotAllClose(predict_4, predict_5)\n \n-        # Create models with stateful=False, and make sure they handle init state\n-        # correctly.\n+        # Create models with stateful=False, and make sure they handle init\n+        # state correctly.\n         model_3 = make_model(stateful=False, with_initial_state=True)\n         model_3.layers[1].set_weights(layer_weights)\n \n@@ -1723,7 +1726,8 @@ class RNNTest(test_combinations.TestCase):\n         ]\n     )\n     def test_state_spec_with_stack_cell(self, cell):\n-        # See https://github.com/tensorflow/tensorflow/issues/27817 for more detail.\n+        # See https://github.com/tensorflow/tensorflow/issues/27817 for more\n+        # detail.\n         batch = 12\n         timesteps = 10\n         input_dim = 8\n@@ -1899,16 +1903,17 @@ class RNNTest(test_combinations.TestCase):\n         dense_data = ragged_data.to_tensor()\n         output_dense = model_2.predict(dense_data, steps=1)\n \n-        # Note that the raw output for dense and ragged input when go_backward=True\n-        # will be different. Consider following input\n+        # Note that the raw output for dense and ragged input when\n+        # go_backward=True will be different. Consider following input\n         # [[a, b, 0], [c, 0, 0], [d, e, f]] where 0s are masked value.\n-        # The dense output will be [[0, b, a], [0, 0, c], [f, e, d]] since it will\n-        # process the whole sequence from the end.\n-        # While ragged output will be [[b, a], [c], [f, e, d]] since it just ignore\n-        # the 0s. And if we densify the ragged output, it will by default inserting\n-        # 0s to the end (rather than from the beginning), which make the output to\n-        # be [[b, a, 0], [c, 0, 0], [f, e, d]]. With this, we need to verify that\n-        # reverse(ragged_output.to_tensor()) == reverse(dense_output)\n+        # The dense output will be [[0, b, a], [0, 0, c], [f, e, d]] since it\n+        # will process the whole sequence from the end.\n+        # While ragged output will be [[b, a], [c], [f, e, d]] since it just\n+        # ignore the 0s. And if we densify the ragged output, it will by default\n+        # inserting 0s to the end (rather than from the beginning), which make\n+        # the output to be [[b, a, 0], [c, 0, 0], [f, e, d]]. With this, we need\n+        # to verify that reverse(ragged_output.to_tensor()) ==\n+        # reverse(dense_output)\n         output_dense = keras.backend.reverse(output_dense, [1])\n         output_dense = tf.RaggedTensor.from_tensor(\n             output_dense, lengths=row_lengths\n\n@@ -46,9 +46,9 @@ class Bidirectional(Wrapper):\n         Note that the recommended way to create new RNN layers is to write a\n         custom RNN cell and use it with `keras.layers.RNN`, instead of\n         subclassing `keras.layers.Layer` directly.\n-        - When the `returns_sequences` is true, the output of the masked timestep\n-        will be zero regardless of the layer's original `zero_output_for_mask`\n-        value.\n+        - When the `returns_sequences` is true, the output of the masked\n+        timestep will be zero regardless of the layer's original\n+        `zero_output_for_mask` value.\n       merge_mode: Mode by which outputs of the forward and backward RNNs will be\n         combined. One of {'sum', 'mul', 'concat', 'ave', None}. If None, the\n         outputs will not be combined, they will be returned as a list. Default\n@@ -83,7 +83,8 @@ class Bidirectional(Wrapper):\n \n     ```python\n     model = Sequential()\n-    model.add(Bidirectional(LSTM(10, return_sequences=True), input_shape=(5, 10)))\n+    model.add(Bidirectional(LSTM(10, return_sequences=True),\n+                                 input_shape=(5, 10)))\n     model.add(Bidirectional(LSTM(10)))\n     model.add(Dense(5))\n     model.add(Activation('softmax'))\n@@ -117,8 +118,8 @@ class Bidirectional(Wrapper):\n             )\n         if backward_layer is not None and not isinstance(backward_layer, Layer):\n             raise ValueError(\n-                \"`backward_layer` need to be a `tf.keras.layers.Layer` instance. \"\n-                f\"Received: {backward_layer}\"\n+                \"`backward_layer` need to be a `tf.keras.layers.Layer` \"\n+                f\"instance. Received: {backward_layer}\"\n             )\n         if merge_mode not in [\"sum\", \"mul\", \"ave\", \"concat\", None]:\n             raise ValueError(\n@@ -126,14 +127,14 @@ class Bidirectional(Wrapper):\n                 \"Merge mode should be one of \"\n                 '{\"sum\", \"mul\", \"ave\", \"concat\", None}'\n             )\n-        # We don't want to track `layer` since we're already tracking the two copies\n-        # of it we actually run.\n+        # We don't want to track `layer` since we're already tracking the two\n+        # copies of it we actually run.\n         self._setattr_tracking = False\n         super().__init__(layer, **kwargs)\n         self._setattr_tracking = True\n \n-        # Recreate the forward layer from the original layer config, so that it will\n-        # not carry over any state from the layer.\n+        # Recreate the forward layer from the original layer config, so that it\n+        # will not carry over any state from the layer.\n         self.forward_layer = self._recreate_layer_from_config(layer)\n \n         if backward_layer is None:\n@@ -142,9 +143,9 @@ class Bidirectional(Wrapper):\n             )\n         else:\n             self.backward_layer = backward_layer\n-            # Keep the custom backward layer config, so that we can save it later. The\n-            # layer's name might be updated below with prefix 'backward_', and we want\n-            # to preserve the original config.\n+            # Keep the custom backward layer config, so that we can save it\n+            # later. The layer's name might be updated below with prefix\n+            # 'backward_', and we want to preserve the original config.\n             self._backward_layer_config = generic_utils.serialize_keras_object(\n                 backward_layer\n             )\n@@ -187,8 +188,10 @@ class Bidirectional(Wrapper):\n             raise ValueError(\n                 \"Forward layer and backward layer should have different \"\n                 \"`go_backwards` value.\"\n-                f\"forward_layer.go_backwards = {self.forward_layer.go_backwards},\"\n-                f\"backward_layer.go_backwards = {self.backward_layer.go_backwards}\"\n+                f\"forward_layer.go_backwards = \"\n+                f\"{self.forward_layer.go_backwards},\"\n+                f\"backward_layer.go_backwards = \"\n+                f\"{self.backward_layer.go_backwards}\"\n             )\n \n         common_attributes = (\"stateful\", \"return_sequences\", \"return_state\")\n@@ -197,17 +200,18 @@ class Bidirectional(Wrapper):\n             backward_value = getattr(self.backward_layer, a)\n             if forward_value != backward_value:\n                 raise ValueError(\n-                    \"Forward layer and backward layer are expected to have the same \"\n-                    f'value for attribute \"{a}\", got \"{forward_value}\" for forward '\n-                    f'layer and \"{backward_value}\" for backward layer'\n+                    \"Forward layer and backward layer are expected to have \"\n+                    f'the same value for attribute \"{a}\", got '\n+                    f'\"{forward_value}\" for forward layer and '\n+                    f'\"{backward_value}\" for backward layer'\n                 )\n \n     def _recreate_layer_from_config(self, layer, go_backwards=False):\n-        # When recreating the layer from its config, it is possible that the layer\n-        # is a RNN layer that contains custom cells. In this case we inspect the\n-        # layer and pass the custom cell class as part of the `custom_objects`\n-        # argument when calling `from_config`.\n-        # See https://github.com/tensorflow/tensorflow/issues/26581 for more detail.\n+        # When recreating the layer from its config, it is possible that the\n+        # layer is a RNN layer that contains custom cells. In this case we\n+        # inspect the layer and pass the custom cell class as part of the\n+        # `custom_objects` argument when calling `from_config`.  See\n+        # https://github.com/tensorflow/tensorflow/issues/26581 for more detail.\n         config = layer.get_config()\n         if go_backwards:\n             config[\"go_backwards\"] = not config[\"go_backwards\"]\n@@ -258,7 +262,8 @@ class Bidirectional(Wrapper):\n         return output_shape\n \n     def __call__(self, inputs, initial_state=None, constants=None, **kwargs):\n-        \"\"\"`Bidirectional.__call__` implements the same API as the wrapped `RNN`.\"\"\"\n+        \"\"\"`Bidirectional.__call__` implements the same API as the wrapped\n+        `RNN`.\"\"\"\n         inputs, initial_state, constants = rnn_utils.standardize_args(\n             inputs, initial_state, constants, self._num_constants\n         )\n@@ -325,8 +330,8 @@ class Bidirectional(Wrapper):\n         if is_keras_tensor:\n             # Compute the full input spec, including state\n             full_input = [inputs] + additional_inputs\n-            # The original input_spec is None since there could be a nested tensor\n-            # input. Update the input_spec to match the inputs.\n+            # The original input_spec is None since there could be a nested\n+            # tensor input. Update the input_spec to match the inputs.\n             full_input_spec = [\n                 None for _ in range(len(tf.nest.flatten(inputs)))\n             ] + additional_specs\n@@ -362,9 +367,10 @@ class Bidirectional(Wrapper):\n \n         if generic_utils.has_arg(self.layer.call, \"initial_state\"):\n             if isinstance(inputs, list) and len(inputs) > 1:\n-                # initial_states are keras tensors, which means they are passed in\n-                # together with inputs as list. The initial_states need to be split into\n-                # forward and backward section, and be feed to layers accordingly.\n+                # initial_states are keras tensors, which means they are passed\n+                # in together with inputs as list. The initial_states need to be\n+                # split into forward and backward section, and be feed to layers\n+                # accordingly.\n                 forward_inputs = [inputs[0]]\n                 backward_inputs = [inputs[0]]\n                 pivot = (len(inputs) - self._num_constants) // 2 + 1\n@@ -383,9 +389,10 @@ class Bidirectional(Wrapper):\n                 if \"constants\" in kwargs:\n                     kwargs[\"constants\"] = None\n             elif initial_state is not None:\n-                # initial_states are not keras tensors, eg eager tensor from np array.\n-                # They are only passed in from kwarg initial_state, and should be passed\n-                # to forward/backward layer via kwarg initial_state as well.\n+                # initial_states are not keras tensors, eg eager tensor from np\n+                # array.  They are only passed in from kwarg initial_state, and\n+                # should be passed to forward/backward layer via kwarg\n+                # initial_state as well.\n                 forward_inputs, backward_inputs = inputs, inputs\n                 half = len(initial_state) // 2\n                 forward_state = initial_state[:half]\n@@ -426,7 +433,8 @@ class Bidirectional(Wrapper):\n             output = [y, y_rev]\n         else:\n             raise ValueError(\n-                f\"Unrecognized value for `merge_mode`. Received: {self.merge_mode}\"\n+                \"Unrecognized value for `merge_mode`. \"\n+                f\"Received: {self.merge_mode}\"\n                 'Expected values are [\"concat\", \"sum\", \"ave\", \"mul\"]'\n             )\n \n\n@@ -326,8 +326,8 @@ class BidirectionalTest(tf.test.TestCase, parameterized.TestCase):\n     def test_Bidirectional_with_time_major_input(self, time_major):\n         batch_size, time, input_dim = 2, 3, 1\n         inputs = tf.zeros((batch_size, time, input_dim))\n-        # length is [1 2]. Within the batch, the first element has 1 step, and the\n-        # second element as 2 steps.\n+        # length is [1 2]. Within the batch, the first element has 1 step, and\n+        # the second element as 2 steps.\n         lengths = tf.range(1, 1 + batch_size)\n         mask = tf.sequence_mask(lengths, maxlen=time, dtype=tf.float32)\n \n@@ -355,8 +355,8 @@ class BidirectionalTest(tf.test.TestCase, parameterized.TestCase):\n         if time_major:\n             keras_outputs = tf.transpose(keras_outputs, [1, 0, 2])\n \n-        # expect the first element in batch has 1 step and second element in batch\n-        # has 2 steps.\n+        # expect the first element in batch has 1 step and second element in\n+        # batch has 2 steps.\n         expected_result = np.array(\n             [\n                 [[1.0, 1.0], [0.0, 0.0], [0.0, 0.0]],\n@@ -430,7 +430,8 @@ class BidirectionalTest(tf.test.TestCase, parameterized.TestCase):\n             model.predict(inputs)\n \n     def test_Bidirectional_state_reuse_with_np_input(self):\n-        # See https://github.com/tensorflow/tensorflow/issues/28761 for more detail.\n+        # See https://github.com/tensorflow/tensorflow/issues/28761 for more\n+        # detail.\n         rnn = keras.layers.LSTM\n         samples = 2\n         dim = 5\n@@ -620,7 +621,8 @@ class BidirectionalTest(tf.test.TestCase, parameterized.TestCase):\n             rnn(3, return_state=True), merge_mode=None\n         )\n         output_shape = wrapper.compute_output_shape(input_shape)\n-        # 1 for forward output and 1 for backward output,  and the rest for states\n+        # 1 for forward output and 1 for backward output,  and the rest for\n+        # states\n         self.assertLen(output_shape, 2 + num_state)\n         for shape in output_shape:\n             self.assertEqual(shape.as_list(), [None, 3])\n@@ -659,7 +661,8 @@ class BidirectionalTest(tf.test.TestCase, parameterized.TestCase):\n \n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm MIOpen does not support padded input yet.\",\n+        skip_message=\"Skipping as ROCm MIOpen does not support padded \"\n+        \"input yet.\",\n     )\n     def test_Bidirectional_last_output_with_masking(self):\n         rnn = keras.layers.LSTM\n@@ -669,8 +672,8 @@ class BidirectionalTest(tf.test.TestCase, parameterized.TestCase):\n         units = 3\n         merge_mode = \"concat\"\n         x = np.random.rand(samples, timesteps, dim)\n-        # clear the first record's timestep 2. Last output should be same as state,\n-        # not zeroed.\n+        # clear the first record's timestep 2. Last output should be same as\n+        # state, not zeroed.\n         x[0, 2] = 0\n \n         with self.cached_session():\n@@ -691,7 +694,8 @@ class BidirectionalTest(tf.test.TestCase, parameterized.TestCase):\n     @parameterized.parameters([keras.layers.LSTM, keras.layers.GRU])\n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm MIOpen does not support padded input yet.\",\n+        skip_message=\"Skipping as ROCm MIOpen does not support padded \"\n+        \"input yet.\",\n     )\n     def test_Bidirectional_sequence_output_with_masking(self, rnn):\n         samples = 2\n@@ -700,8 +704,8 @@ class BidirectionalTest(tf.test.TestCase, parameterized.TestCase):\n         units = 3\n         merge_mode = \"concat\"\n         x = np.random.rand(samples, timesteps, dim)\n-        # clear the first record's timestep 2, and expect the output of timestep 2\n-        # is also 0s.\n+        # clear the first record's timestep 2, and expect the output of timestep\n+        # 2 is also 0s.\n         x[0, 2] = 0\n \n         with self.cached_session():\n@@ -919,7 +923,8 @@ class BidirectionalTest(tf.test.TestCase, parameterized.TestCase):\n     @parameterized.parameters([\"ave\", \"concat\", \"mul\"])\n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm RNN does not support ragged tensors yet.\",\n+        skip_message=\"Skipping as ROCm RNN does not support ragged \"\n+        \"tensors yet.\",\n     )\n     def test_Bidirectional_ragged_input(self, merge_mode):\n         np.random.seed(100)\n@@ -958,8 +963,8 @@ class BidirectionalTest(tf.test.TestCase, parameterized.TestCase):\n             )\n \n             # TODO(kaftan): after KerasTensor refactor TF op layers should work\n-            # with many composite tensors, and this shouldn't need to be a lambda\n-            # layer.\n+            # with many composite tensors, and this shouldn't need to be a\n+            # lambda layer.\n             reverse_layer = core.Lambda(tf.reverse, arguments=dict(axis=[1]))\n             f_backward = keras.backend.function(\n                 [inputs], reverse_layer(layer.backward_layer(inputs))\n\n@@ -60,26 +60,27 @@ class _RNNCellWrapper(AbstractRNNCell):\n         Args:\n           inputs: A tensor with wrapped cell's input.\n           state: A tensor or tuple of tensors with wrapped cell's state.\n-          cell_call_fn: Wrapped cell's method to use for step computation (cell's\n-            `__call__` or 'call' method).\n+          cell_call_fn: Wrapped cell's method to use for step computation\n+            (cell's `__call__` or 'call' method).\n           **kwargs: Additional arguments.\n \n         Returns:\n           A pair containing:\n           - Output: A tensor with cell's output.\n-          - New state: A tensor or tuple of tensors with new wrapped cell's state.\n+          - New state: A tensor or tuple of tensors with new wrapped cell's\n+            state.\n         \"\"\"\n         raise NotImplementedError\n \n     def call(self, inputs, state, **kwargs):\n         \"\"\"Runs the RNN cell step computation.\n \n-        When `call` is being used, we assume that the wrapper object has been built,\n-        and therefore the wrapped cells has been built via its `build` method and\n-        its `call` method can be used directly.\n+        When `call` is being used, we assume that the wrapper object has been\n+        built, and therefore the wrapped cells has been built via its `build`\n+        method and its `call` method can be used directly.\n \n-        This allows to use the wrapped cell and the non-wrapped cell equivalently\n-        when using `call` and `build`.\n+        This allows to use the wrapped cell and the non-wrapped cell\n+        equivalently when using `call` and `build`.\n \n         Args:\n           inputs: A tensor with wrapped cell's input.\n@@ -90,7 +91,8 @@ class _RNNCellWrapper(AbstractRNNCell):\n           A pair containing:\n \n           - Output: A tensor with cell's output.\n-          - New state: A tensor or tuple of tensors with new wrapped cell's state.\n+          - New state: A tensor or tuple of tensors with new wrapped cell's\n+            state.\n         \"\"\"\n         return self._call_wrapped_cell(\n             inputs, state, cell_call_fn=self.cell.call, **kwargs\n@@ -159,10 +161,11 @@ class DropoutWrapper(_RNNCellWrapper):\n     ):\n         \"\"\"Create a cell with added input, state, and/or output dropout.\n \n-        If `variational_recurrent` is set to `True` (**NOT** the default behavior),\n-        then the same dropout mask is applied at every step, as described in:\n-        [A Theoretically Grounded Application of Dropout in Recurrent\n-        Neural Networks. Y. Gal, Z. Ghahramani](https://arxiv.org/abs/1512.05287).\n+        If `variational_recurrent` is set to `True` (**NOT** the default\n+        behavior), then the same dropout mask is applied at every step, as\n+        described in: [A Theoretically Grounded Application of Dropout in\n+        Recurrent Neural Networks. Y. Gal, Z.\n+        Ghahramani](https://arxiv.org/abs/1512.05287).\n \n         Otherwise a different dropout mask is applied at every time step.\n \n@@ -174,44 +177,52 @@ class DropoutWrapper(_RNNCellWrapper):\n         Args:\n           cell: an RNNCell, a projection to output_size is added to it.\n           input_keep_prob: unit Tensor or float between 0 and 1, input keep\n-            probability; if it is constant and 1, no input dropout will be added.\n+            probability; if it is constant and 1, no input dropout will be\n+            added.\n           output_keep_prob: unit Tensor or float between 0 and 1, output keep\n-            probability; if it is constant and 1, no output dropout will be added.\n+            probability; if it is constant and 1, no output dropout will be\n+            added.\n           state_keep_prob: unit Tensor or float between 0 and 1, output keep\n-            probability; if it is constant and 1, no output dropout will be added.\n-            State dropout is performed on the outgoing states of the cell. **Note**\n-            the state components to which dropout is applied when `state_keep_prob`\n-            is in `(0, 1)` are also determined by the argument\n-            `dropout_state_filter_visitor` (e.g. by default dropout is never applied\n-            to the `c` component of an `LSTMStateTuple`).\n+            probability; if it is constant and 1, no output dropout will be\n+            added.  State dropout is performed on the outgoing states of the\n+            cell. **Note** the state components to which dropout is applied when\n+            `state_keep_prob` is in `(0, 1)` are also determined by the argument\n+            `dropout_state_filter_visitor` (e.g. by default dropout is never\n+            applied to the `c` component of an `LSTMStateTuple`).\n           variational_recurrent: Python bool.  If `True`, then the same dropout\n-            pattern is applied across all time steps per run call. If this parameter\n-            is set, `input_size` **must** be provided.\n-          input_size: (optional) (possibly nested tuple of) `TensorShape` objects\n-            containing the depth(s) of the input tensors expected to be passed in to\n-            the `DropoutWrapper`.  Required and used **iff** `variational_recurrent\n-            = True` and `input_keep_prob < 1`.\n+            pattern is applied across all time steps per run call. If this\n+            parameter is set, `input_size` **must** be provided.\n+          input_size: (optional) (possibly nested tuple of) `TensorShape`\n+            objects containing the depth(s) of the input tensors expected to be\n+            passed in to the `DropoutWrapper`.  Required and used **iff**\n+            `variational_recurrent = True` and `input_keep_prob < 1`.\n           dtype: (optional) The `dtype` of the input, state, and output tensors.\n             Required and used **iff** `variational_recurrent = True`.\n           seed: (optional) integer, the randomness seed.\n-          dropout_state_filter_visitor: (optional), default: (see below).  Function\n-            that takes any hierarchical level of the state and returns a scalar or\n-            depth=1 structure of Python booleans describing which terms in the state\n-            should be dropped out.  In addition, if the function returns `True`,\n-            dropout is applied across this sublevel.  If the function returns\n-            `False`, dropout is not applied across this entire sublevel.\n-            Default behavior: perform dropout on all terms except the memory (`c`)\n-              state of `LSTMCellState` objects, and don't try to apply dropout to\n-            `TensorArray` objects: ```\n+          dropout_state_filter_visitor: (optional), default: (see below).\n+            Function that takes any hierarchical level of the state and returns\n+            a scalar or depth=1 structure of Python booleans describing which\n+            terms in the state should be dropped out.  In addition, if the\n+            function returns `True`, dropout is applied across this sublevel.\n+            If the function returns `False`, dropout is not applied across this\n+            entire sublevel.  Default behavior: perform dropout on all terms\n+            except the memory (`c`) state of `LSTMCellState` objects, and don't\n+            try to apply dropout to\n+            `TensorArray` objects:\n+            ```\n             def dropout_state_filter_visitor(s):\n-              if isinstance(s, LSTMCellState): # Never perform dropout on the c\n-                state. return LSTMCellState(c=False, h=True)\n-              elif isinstance(s, TensorArray): return False return True ```\n+              # Never perform dropout on the c state.\n+              if isinstance(s, LSTMCellState):\n+                return LSTMCellState(c=False, h=True)\n+              elif isinstance(s, TensorArray):\n+                return False\n+              return True\n+            ```\n           **kwargs: dict of keyword arguments for base layer.\n \n         Raises:\n-          TypeError: if `cell` is not an `RNNCell`, or `keep_state_fn` is provided\n-            but not `callable`.\n+          TypeError: if `cell` is not an `RNNCell`, or `keep_state_fn` is\n+            provided but not `callable`.\n           ValueError: if any of the keep_probs are not between 0 and 1.\n         \"\"\"\n         if isinstance(cell, lstm.LSTMCell):\n@@ -287,8 +298,8 @@ class DropoutWrapper(_RNNCellWrapper):\n             ):\n                 if input_size is None:\n                     raise ValueError(\n-                        \"When variational_recurrent=True and input_keep_prob < 1.0 or \"\n-                        \"is unknown, input_size must be provided\"\n+                        \"When variational_recurrent=True and input_keep_prob < \"\n+                        \"1.0 or is unknown, input_size must be provided\"\n                     )\n                 self._recurrent_input_noise = _enumerated_map_structure_up_to(\n                     input_size,\n@@ -386,15 +397,16 @@ class DropoutWrapper(_RNNCellWrapper):\n         Args:\n           inputs: A tensor with wrapped cell's input.\n           state: A tensor or tuple of tensors with wrapped cell's state.\n-          cell_call_fn: Wrapped cell's method to use for step computation (cell's\n-            `__call__` or 'call' method).\n+          cell_call_fn: Wrapped cell's method to use for step computation\n+            (cell's `__call__` or 'call' method).\n           **kwargs: Additional arguments.\n \n         Returns:\n           A pair containing:\n \n           - Output: A tensor with cell's output.\n-          - New state: A tensor or tuple of tensors with new wrapped cell's state.\n+          - New state: A tensor or tuple of tensors with new wrapped cell's\n+            state.\n         \"\"\"\n \n         def _should_dropout(p):\n@@ -487,10 +499,10 @@ class ResidualWrapper(_RNNCellWrapper):\n \n         Args:\n           cell: An instance of `RNNCell`.\n-          residual_fn: (Optional) The function to map raw cell inputs and raw cell\n-            outputs to the actual cell outputs of the residual network.\n-            Defaults to calling nest.map_structure on (lambda i, o: i + o), inputs\n-              and outputs.\n+          residual_fn: (Optional) The function to map raw cell inputs and raw\n+            cell outputs to the actual cell outputs of the residual network.\n+            Defaults to calling nest.map_structure on (lambda i, o: i + o),\n+            inputs and outputs.\n           **kwargs: dict of keyword arguments for base layer.\n         \"\"\"\n         super().__init__(cell, **kwargs)\n@@ -502,8 +514,8 @@ class ResidualWrapper(_RNNCellWrapper):\n         Args:\n           inputs: cell inputs.\n           state: cell state.\n-          cell_call_fn: Wrapped cell's method to use for step computation (cell's\n-            `__call__` or 'call' method).\n+          cell_call_fn: Wrapped cell's method to use for step computation\n+            (cell's `__call__` or 'call' method).\n           **kwargs: Additional arguments passed to the wrapped cell's `call`.\n \n         Returns:\n@@ -511,7 +523,8 @@ class ResidualWrapper(_RNNCellWrapper):\n \n         Raises:\n           TypeError: If cell inputs and outputs have different structure (type).\n-          ValueError: If cell inputs and outputs have different structure (value).\n+          ValueError: If cell inputs and outputs have different structure\n+            (value).\n         \"\"\"\n         outputs, new_state = cell_call_fn(inputs, state, **kwargs)\n \n\n@@ -28,24 +28,24 @@ class ConvLSTM1D(ConvLSTM):\n     and recurrent transformations are both convolutional.\n \n     Args:\n-      filters: Integer, the dimensionality of the output space (i.e. the number of\n-        output filters in the convolution).\n+      filters: Integer, the dimensionality of the output space (i.e. the number\n+        of output filters in the convolution).\n       kernel_size: An integer or tuple/list of n integers, specifying the\n         dimensions of the convolution window.\n       strides: An integer or tuple/list of n integers, specifying the strides of\n         the convolution. Specifying any stride value != 1 is incompatible with\n         specifying any `dilation_rate` value != 1.\n-      padding: One of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means no\n-        padding. `\"same\"` results in padding evenly to the left/right or up/down\n-        of the input such that output has the same height/width dimension as the\n-        input.\n-      data_format: A string, one of `channels_last` (default) or `channels_first`.\n-        The ordering of the dimensions in the inputs. `channels_last` corresponds\n-        to inputs with shape `(batch, time, ..., channels)` while `channels_first`\n-        corresponds to inputs with shape `(batch, time, channels, ...)`. It\n-        defaults to the `image_data_format` value found in your Keras config file\n-        at `~/.keras/keras.json`. If you never set it, then it will be\n-        \"channels_last\".\n+      padding: One of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means\n+        no padding. `\"same\"` results in padding evenly to the left/right or\n+        up/down of the input such that output has the same height/width\n+        dimension as the input.\n+      data_format: A string, one of `channels_last` (default) or\n+        `channels_first`.  The ordering of the dimensions in the inputs.\n+        `channels_last` corresponds to inputs with shape `(batch, time, ...,\n+        channels)` while `channels_first` corresponds to inputs with shape\n+        `(batch, time, channels, ...)`. It defaults to the `image_data_format`\n+        value found in your Keras config file at `~/.keras/keras.json`. If you\n+        never set it, then it will be \"channels_last\".\n       dilation_rate: An integer or tuple/list of n integers, specifying the\n         dilation rate to use for dilated convolution. Currently, specifying any\n         `dilation_rate` value != 1 is incompatible with specifying any `strides`\n@@ -59,9 +59,9 @@ class ConvLSTM1D(ConvLSTM):\n       recurrent_initializer: Initializer for the `recurrent_kernel` weights\n         matrix, used for the linear transformation of the recurrent state.\n       bias_initializer: Initializer for the bias vector.\n-      unit_forget_bias: Boolean. If True, add 1 to the bias of the forget gate at\n-        initialization. Use in combination with `bias_initializer=\"zeros\"`. This\n-        is recommended in [Jozefowicz et al., 2015](\n+      unit_forget_bias: Boolean. If True, add 1 to the bias of the forget gate\n+        at initialization. Use in combination with `bias_initializer=\"zeros\"`.\n+        This is recommended in [Jozefowicz et al., 2015](\n         http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n       kernel_regularizer: Regularizer function applied to the `kernel` weights\n         matrix.\n@@ -71,8 +71,8 @@ class ConvLSTM1D(ConvLSTM):\n       activity_regularizer: Regularizer function applied to.\n       kernel_constraint: Constraint function applied to the `kernel` weights\n         matrix.\n-      recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n-        weights matrix.\n+      recurrent_constraint: Constraint function applied to the\n+        `recurrent_kernel` weights matrix.\n       bias_constraint: Constraint function applied to the bias vector.\n       return_sequences: Boolean. Whether to return the last output in the output\n         sequence, or the full sequence. (default False)\n@@ -83,27 +83,27 @@ class ConvLSTM1D(ConvLSTM):\n       stateful: Boolean (default False). If True, the last state for each sample\n         at index i in a batch will be used as initial state for the sample of\n         index i in the following batch.\n-      dropout: Float between 0 and 1. Fraction of the units to drop for the linear\n-        transformation of the inputs.\n-      recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n-        the linear transformation of the recurrent state.\n+      dropout: Float between 0 and 1. Fraction of the units to drop for the\n+        linear transformation of the inputs.\n+      recurrent_dropout: Float between 0 and 1. Fraction of the units to drop\n+        for the linear transformation of the recurrent state.\n     Call arguments:\n       inputs: A 4D tensor.\n       mask: Binary tensor of shape `(samples, timesteps)` indicating whether a\n         given timestep should be masked.\n       training: Python boolean indicating whether the layer should behave in\n         training mode or in inference mode. This argument is passed to the cell\n-        when calling it. This is only relevant if `dropout` or `recurrent_dropout`\n-        are set.\n-      initial_state: List of initial state tensors to be passed to the first call\n-        of the cell.\n+        when calling it. This is only relevant if `dropout` or\n+        `recurrent_dropout` are set.\n+      initial_state: List of initial state tensors to be passed to the first\n+        call of the cell.\n     Input shape: - If data_format='channels_first'\n           4D tensor with shape: `(samples, time, channels, rows)` - If\n             data_format='channels_last'\n           4D tensor with shape: `(samples, time, rows, channels)`\n     Output shape:\n-      - If `return_state`: a list of tensors. The first tensor is the output. The\n-        remaining tensors are the last states,\n+      - If `return_state`: a list of tensors. The first tensor is the output.\n+        The remaining tensors are the last states,\n         each 3D tensor with shape: `(samples, filters, new_rows)` if\n           data_format='channels_first'\n         or shape: `(samples, new_rows, filters)` if data_format='channels_last'.\n\n@@ -28,24 +28,24 @@ class ConvLSTM2D(ConvLSTM):\n     and recurrent transformations are both convolutional.\n \n     Args:\n-      filters: Integer, the dimensionality of the output space (i.e. the number of\n-        output filters in the convolution).\n+      filters: Integer, the dimensionality of the output space (i.e. the number\n+        of output filters in the convolution).\n       kernel_size: An integer or tuple/list of n integers, specifying the\n         dimensions of the convolution window.\n       strides: An integer or tuple/list of n integers, specifying the strides of\n         the convolution. Specifying any stride value != 1 is incompatible with\n         specifying any `dilation_rate` value != 1.\n-      padding: One of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means no\n-        padding. `\"same\"` results in padding evenly to the left/right or up/down\n-        of the input such that output has the same height/width dimension as the\n-        input.\n-      data_format: A string, one of `channels_last` (default) or `channels_first`.\n-        The ordering of the dimensions in the inputs. `channels_last` corresponds\n-        to inputs with shape `(batch, time, ..., channels)` while `channels_first`\n-        corresponds to inputs with shape `(batch, time, channels, ...)`. It\n-        defaults to the `image_data_format` value found in your Keras config file\n-        at `~/.keras/keras.json`. If you never set it, then it will be\n-        \"channels_last\".\n+      padding: One of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means\n+        no padding. `\"same\"` results in padding evenly to the left/right or\n+        up/down of the input such that output has the same height/width\n+        dimension as the input.\n+      data_format: A string, one of `channels_last` (default) or\n+        `channels_first`.  The ordering of the dimensions in the inputs.\n+        `channels_last` corresponds to inputs with shape `(batch, time, ...,\n+        channels)` while `channels_first` corresponds to inputs with shape\n+        `(batch, time, channels, ...)`. It defaults to the `image_data_format`\n+        value found in your Keras config file at `~/.keras/keras.json`. If you\n+        never set it, then it will be \"channels_last\".\n       dilation_rate: An integer or tuple/list of n integers, specifying the\n         dilation rate to use for dilated convolution. Currently, specifying any\n         `dilation_rate` value != 1 is incompatible with specifying any `strides`\n@@ -59,9 +59,9 @@ class ConvLSTM2D(ConvLSTM):\n       recurrent_initializer: Initializer for the `recurrent_kernel` weights\n         matrix, used for the linear transformation of the recurrent state.\n       bias_initializer: Initializer for the bias vector.\n-      unit_forget_bias: Boolean. If True, add 1 to the bias of the forget gate at\n-        initialization. Use in combination with `bias_initializer=\"zeros\"`. This\n-        is recommended in [Jozefowicz et al., 2015](\n+      unit_forget_bias: Boolean. If True, add 1 to the bias of the forget gate\n+        at initialization. Use in combination with `bias_initializer=\"zeros\"`.\n+        This is recommended in [Jozefowicz et al., 2015](\n         http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n       kernel_regularizer: Regularizer function applied to the `kernel` weights\n         matrix.\n@@ -71,8 +71,8 @@ class ConvLSTM2D(ConvLSTM):\n       activity_regularizer: Regularizer function applied to.\n       kernel_constraint: Constraint function applied to the `kernel` weights\n         matrix.\n-      recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n-        weights matrix.\n+      recurrent_constraint: Constraint function applied to the\n+        `recurrent_kernel` weights matrix.\n       bias_constraint: Constraint function applied to the bias vector.\n       return_sequences: Boolean. Whether to return the last output in the output\n         sequence, or the full sequence. (default False)\n@@ -83,32 +83,32 @@ class ConvLSTM2D(ConvLSTM):\n       stateful: Boolean (default False). If True, the last state for each sample\n         at index i in a batch will be used as initial state for the sample of\n         index i in the following batch.\n-      dropout: Float between 0 and 1. Fraction of the units to drop for the linear\n-        transformation of the inputs.\n-      recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n-        the linear transformation of the recurrent state.\n+      dropout: Float between 0 and 1. Fraction of the units to drop for the\n+        linear transformation of the inputs.\n+      recurrent_dropout: Float between 0 and 1. Fraction of the units to drop\n+        for the linear transformation of the recurrent state.\n     Call arguments:\n       inputs: A 5D tensor.\n       mask: Binary tensor of shape `(samples, timesteps)` indicating whether a\n         given timestep should be masked.\n       training: Python boolean indicating whether the layer should behave in\n         training mode or in inference mode. This argument is passed to the cell\n-        when calling it. This is only relevant if `dropout` or `recurrent_dropout`\n-        are set.\n-      initial_state: List of initial state tensors to be passed to the first call\n-        of the cell.\n+        when calling it. This is only relevant if `dropout` or\n+        `recurrent_dropout` are set.\n+      initial_state: List of initial state tensors to be passed to the first\n+        call of the cell.\n     Input shape: - If data_format='channels_first'\n           5D tensor with shape: `(samples, time, channels, rows, cols)` - If\n             data_format='channels_last'\n           5D tensor with shape: `(samples, time, rows, cols, channels)`\n     Output shape:\n-      - If `return_state`: a list of tensors. The first tensor is the output. The\n-        remaining tensors are the last states,\n+      - If `return_state`: a list of tensors. The first tensor is the output.\n+        The remaining tensors are the last states,\n         each 4D tensor with shape: `(samples, filters, new_rows, new_cols)` if\n           data_format='channels_first'\n         or shape: `(samples, new_rows, new_cols, filters)` if\n-          data_format='channels_last'. `rows` and `cols` values might have changed\n-          due to padding.\n+          data_format='channels_last'. `rows` and `cols` values might have\n+          changed due to padding.\n       - If `return_sequences`: 5D tensor with shape: `(samples, timesteps,\n         filters, new_rows, new_cols)` if data_format='channels_first'\n         or shape: `(samples, timesteps, new_rows, new_cols, filters)` if\n\n@@ -28,24 +28,24 @@ class ConvLSTM3D(ConvLSTM):\n     and recurrent transformations are both convolutional.\n \n     Args:\n-      filters: Integer, the dimensionality of the output space (i.e. the number of\n-        output filters in the convolution).\n+      filters: Integer, the dimensionality of the output space (i.e. the number\n+        of output filters in the convolution).\n       kernel_size: An integer or tuple/list of n integers, specifying the\n         dimensions of the convolution window.\n       strides: An integer or tuple/list of n integers, specifying the strides of\n         the convolution. Specifying any stride value != 1 is incompatible with\n         specifying any `dilation_rate` value != 1.\n-      padding: One of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means no\n-        padding. `\"same\"` results in padding evenly to the left/right or up/down\n-        of the input such that output has the same height/width dimension as the\n-        input.\n-      data_format: A string, one of `channels_last` (default) or `channels_first`.\n-        The ordering of the dimensions in the inputs. `channels_last` corresponds\n-        to inputs with shape `(batch, time, ..., channels)` while `channels_first`\n-        corresponds to inputs with shape `(batch, time, channels, ...)`. It\n-        defaults to the `image_data_format` value found in your Keras config file\n-        at `~/.keras/keras.json`. If you never set it, then it will be\n-        \"channels_last\".\n+      padding: One of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means\n+        no padding. `\"same\"` results in padding evenly to the left/right or\n+        up/down of the input such that output has the same height/width\n+        dimension as the input.\n+      data_format: A string, one of `channels_last` (default) or\n+        `channels_first`. The ordering of the dimensions in the inputs.\n+        `channels_last` corresponds to inputs with shape `(batch, time, ...,\n+        channels)` while `channels_first` corresponds to inputs with shape\n+        `(batch, time, channels, ...)`. It defaults to the `image_data_format`\n+        value found in your Keras config file at `~/.keras/keras.json`. If you\n+        never set it, then it will be \"channels_last\".\n       dilation_rate: An integer or tuple/list of n integers, specifying the\n         dilation rate to use for dilated convolution. Currently, specifying any\n         `dilation_rate` value != 1 is incompatible with specifying any `strides`\n@@ -59,9 +59,9 @@ class ConvLSTM3D(ConvLSTM):\n       recurrent_initializer: Initializer for the `recurrent_kernel` weights\n         matrix, used for the linear transformation of the recurrent state.\n       bias_initializer: Initializer for the bias vector.\n-      unit_forget_bias: Boolean. If True, add 1 to the bias of the forget gate at\n-        initialization. Use in combination with `bias_initializer=\"zeros\"`. This\n-        is recommended in [Jozefowicz et al., 2015](\n+      unit_forget_bias: Boolean. If True, add 1 to the bias of the forget gate\n+        at initialization. Use in combination with `bias_initializer=\"zeros\"`.\n+        This is recommended in [Jozefowicz et al., 2015](\n         http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n       kernel_regularizer: Regularizer function applied to the `kernel` weights\n         matrix.\n@@ -71,8 +71,8 @@ class ConvLSTM3D(ConvLSTM):\n       activity_regularizer: Regularizer function applied to.\n       kernel_constraint: Constraint function applied to the `kernel` weights\n         matrix.\n-      recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n-        weights matrix.\n+      recurrent_constraint: Constraint function applied to the\n+        `recurrent_kernel` weights matrix.\n       bias_constraint: Constraint function applied to the bias vector.\n       return_sequences: Boolean. Whether to return the last output in the output\n         sequence, or the full sequence. (default False)\n@@ -83,27 +83,27 @@ class ConvLSTM3D(ConvLSTM):\n       stateful: Boolean (default False). If True, the last state for each sample\n         at index i in a batch will be used as initial state for the sample of\n         index i in the following batch.\n-      dropout: Float between 0 and 1. Fraction of the units to drop for the linear\n-        transformation of the inputs.\n-      recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n-        the linear transformation of the recurrent state.\n+      dropout: Float between 0 and 1. Fraction of the units to drop for the\n+        linear transformation of the inputs.\n+      recurrent_dropout: Float between 0 and 1. Fraction of the units to drop\n+        for the linear transformation of the recurrent state.\n     Call arguments:\n       inputs: A 6D tensor.\n       mask: Binary tensor of shape `(samples, timesteps)` indicating whether a\n         given timestep should be masked.\n       training: Python boolean indicating whether the layer should behave in\n         training mode or in inference mode. This argument is passed to the cell\n-        when calling it. This is only relevant if `dropout` or `recurrent_dropout`\n-        are set.\n-      initial_state: List of initial state tensors to be passed to the first call\n-        of the cell.\n+        when calling it. This is only relevant if `dropout` or\n+        `recurrent_dropout` are set.\n+      initial_state: List of initial state tensors to be passed to the first\n+        call of the cell.\n     Input shape: - If data_format='channels_first'\n           6D tensor with shape: `(samples, time, channels, rows, cols, depth)` -\n             If data_format='channels_last'\n           5D tensor with shape: `(samples, time, rows, cols, depth, channels)`\n     Output shape:\n-      - If `return_state`: a list of tensors. The first tensor is the output. The\n-        remaining tensors are the last states,\n+      - If `return_state`: a list of tensors. The first tensor is the output.\n+        The remaining tensors are the last states,\n         each 5D tensor with shape: `(samples, filters, new_rows, new_cols,\n           new_depth)` if data_format='channels_first'\n         or shape: `(samples, new_rows, new_cols, new_depth, filters)` if\n\n@@ -37,8 +37,8 @@ class CuDNNGRU(_CuDNNRNN):\n \n     Args:\n         units: Positive integer, dimensionality of the output space.\n-        kernel_initializer: Initializer for the `kernel` weights matrix, used for\n-          the linear transformation of the inputs.\n+        kernel_initializer: Initializer for the `kernel` weights matrix, used\n+          for the linear transformation of the inputs.\n         recurrent_initializer: Initializer for the `recurrent_kernel` weights\n           matrix, used for the linear transformation of the recurrent state.\n         bias_initializer: Initializer for the bias vector.\n@@ -54,15 +54,15 @@ class CuDNNGRU(_CuDNNRNN):\n         recurrent_constraint: Constraint function applied to the\n           `recurrent_kernel` weights matrix.\n         bias_constraint: Constraint function applied to the bias vector.\n-        return_sequences: Boolean. Whether to return the last output in the output\n-          sequence, or the full sequence.\n-        return_state: Boolean. Whether to return the last state in addition to the\n-          output.\n-        go_backwards: Boolean (default False). If True, process the input sequence\n-          backwards and return the reversed sequence.\n-        stateful: Boolean (default False). If True, the last state for each sample\n-          at index i in a batch will be used as initial state for the sample of\n-          index i in the following batch.\n+        return_sequences: Boolean. Whether to return the last output in the\n+          output sequence, or the full sequence.\n+        return_state: Boolean. Whether to return the last state in addition to\n+          the output.\n+        go_backwards: Boolean (default False). If True, process the input\n+          sequence backwards and return the reversed sequence.\n+        stateful: Boolean (default False). If True, the last state for each\n+          sample at index i in a batch will be used as initial state for the\n+          sample of index i in the following batch.\n     \"\"\"\n \n     def __init__(\n\n@@ -37,8 +37,8 @@ class CuDNNLSTM(_CuDNNRNN):\n \n     Args:\n         units: Positive integer, dimensionality of the output space.\n-        kernel_initializer: Initializer for the `kernel` weights matrix, used for\n-          the linear transformation of the inputs.\n+        kernel_initializer: Initializer for the `kernel` weights matrix, used\n+          for the linear transformation of the inputs.\n         unit_forget_bias: Boolean. If True, add 1 to the bias of the forget gate\n           at initialization. Setting it to true will also force\n           `bias_initializer=\"zeros\"`. This is recommended in [Jozefowicz et\n@@ -60,13 +60,13 @@ class CuDNNLSTM(_CuDNNRNN):\n         bias_constraint: Constraint function applied to the bias vector.\n         return_sequences: Boolean. Whether to return the last output. in the\n           output sequence, or the full sequence.\n-        return_state: Boolean. Whether to return the last state in addition to the\n-          output.\n-        go_backwards: Boolean (default False). If True, process the input sequence\n-          backwards and return the reversed sequence.\n-        stateful: Boolean (default False). If True, the last state for each sample\n-          at index i in a batch will be used as initial state for the sample of\n-          index i in the following batch.\n+        return_state: Boolean. Whether to return the last state in addition to\n+          the output.\n+        go_backwards: Boolean (default False). If True, process the input\n+          sequence backwards and return the reversed sequence.\n+        stateful: Boolean (default False). If True, the last state for each\n+          sample at index i in a batch will be used as initial state for the\n+          sample of index i in the following batch.\n     \"\"\"\n \n     def __init__(\n\n@@ -390,8 +390,9 @@ class CuDNNV1OnlyTest(test_combinations.TestCase):\n     def test_load_weights_between_noncudnn_rnn_time_distributed(\n         self, rnn_type, to_cudnn\n     ):\n-        # Similar test as test_load_weights_between_noncudnn_rnn() but has different\n-        # rank of input due to usage of TimeDistributed. Issue: #10356.\n+        # Similar test as test_load_weights_between_noncudnn_rnn() but has\n+        # different rank of input due to usage of TimeDistributed. Issue:\n+        # #10356.\n         input_size = 10\n         steps = 6\n         timesteps = 6\n\n@@ -25,9 +25,9 @@ from tensorflow.tools.docs import doc_controls\n class DropoutRNNCellMixin:\n     \"\"\"Object that hold dropout related fields for RNN Cell.\n \n-    This class is not a standalone RNN cell. It suppose to be used with a RNN cell\n-    by multiple inheritance. Any cell that mix with class should have following\n-    fields:\n+    This class is not a standalone RNN cell. It suppose to be used with a RNN\n+    cell by multiple inheritance. Any cell that mix with class should have\n+    following fields:\n       dropout: a float number within range [0, 1). The ratio that the input\n         tensor need to dropout.\n       recurrent_dropout: a float number within range [0, 1). The ratio that the\n@@ -51,14 +51,14 @@ class DropoutRNNCellMixin:\n         tensors will be generated differently than in the \"graph function\" case,\n         and they will be cached.\n \n-        Also note that in graph mode, we still cache those masks only because the\n-        RNN could be created with `unroll=True`. In that case, the `cell.call()`\n-        function will be invoked multiple times, and we want to ensure same mask\n-        is used every time.\n+        Also note that in graph mode, we still cache those masks only because\n+        the RNN could be created with `unroll=True`. In that case, the\n+        `cell.call()` function will be invoked multiple times, and we want to\n+        ensure same mask is used every time.\n \n-        Also the caches are created without tracking. Since they are not picklable\n-        by python when deepcopy, we don't want `layer._obj_reference_counts_dict`\n-        to track it by default.\n+        Also the caches are created without tracking. Since they are not\n+        picklable by python when deepcopy, we don't want\n+        `layer._obj_reference_counts_dict` to track it by default.\n         \"\"\"\n         self._dropout_mask_cache = backend.ContextValueCache(\n             self._create_dropout_mask\n@@ -70,22 +70,22 @@ class DropoutRNNCellMixin:\n     def reset_dropout_mask(self):\n         \"\"\"Reset the cached dropout masks if any.\n \n-        This is important for the RNN layer to invoke this in it `call()` method so\n-        that the cached mask is cleared before calling the `cell.call()`. The mask\n-        should be cached across the timestep within the same batch, but shouldn't\n-        be cached between batches. Otherwise it will introduce unreasonable bias\n-        against certain index of data within the batch.\n+        This is important for the RNN layer to invoke this in it `call()` method\n+        so that the cached mask is cleared before calling the `cell.call()`. The\n+        mask should be cached across the timestep within the same batch, but\n+        shouldn't be cached between batches. Otherwise it will introduce\n+        unreasonable bias against certain index of data within the batch.\n         \"\"\"\n         self._dropout_mask_cache.clear()\n \n     def reset_recurrent_dropout_mask(self):\n         \"\"\"Reset the cached recurrent dropout masks if any.\n \n-        This is important for the RNN layer to invoke this in it call() method so\n-        that the cached mask is cleared before calling the cell.call(). The mask\n-        should be cached across the timestep within the same batch, but shouldn't\n-        be cached between batches. Otherwise it will introduce unreasonable bias\n-        against certain index of data within the batch.\n+        This is important for the RNN layer to invoke this in it call() method\n+        so that the cached mask is cleared before calling the cell.call(). The\n+        mask should be cached across the timestep within the same batch, but\n+        shouldn't be cached between batches. Otherwise it will introduce\n+        unreasonable bias against certain index of data within the batch.\n         \"\"\"\n         self._recurrent_dropout_mask_cache.clear()\n \n@@ -116,10 +116,10 @@ class DropoutRNNCellMixin:\n         Args:\n           inputs: The input tensor whose shape will be used to generate dropout\n             mask.\n-          training: Boolean tensor, whether its in training mode, dropout will be\n-            ignored in non-training mode.\n-          count: Int, how many dropout mask will be generated. It is useful for cell\n-            that has internal weights fused together.\n+          training: Boolean tensor, whether its in training mode, dropout will\n+            be ignored in non-training mode.\n+          count: Int, how many dropout mask will be generated. It is useful for\n+            cell that has internal weights fused together.\n         Returns:\n           List of mask tensor, generated or cached mask based on context.\n         \"\"\"\n@@ -137,10 +137,10 @@ class DropoutRNNCellMixin:\n         Args:\n           inputs: The input tensor whose shape will be used to generate dropout\n             mask.\n-          training: Boolean tensor, whether its in training mode, dropout will be\n-            ignored in non-training mode.\n-          count: Int, how many dropout mask will be generated. It is useful for cell\n-            that has internal weights fused together.\n+          training: Boolean tensor, whether its in training mode, dropout will\n+            be ignored in non-training mode.\n+          count: Int, how many dropout mask will be generated. It is useful for\n+            cell that has internal weights fused together.\n         Returns:\n           List of mask tensor, generated or cached mask based on context.\n         \"\"\"\n@@ -150,8 +150,8 @@ class DropoutRNNCellMixin:\n         return self._recurrent_dropout_mask_cache.setdefault(kwargs=init_kwargs)\n \n     def __getstate__(self):\n-        # Used for deepcopy. The caching can't be pickled by python, since it will\n-        # contain tensor and graph.\n+        # Used for deepcopy. The caching can't be pickled by python, since it\n+        # will contain tensor and graph.\n         state = super().__getstate__()\n         state.pop(\"_dropout_mask_cache\", None)\n         state.pop(\"_recurrent_dropout_mask_cache\", None)\n\n@@ -81,34 +81,34 @@ class GRUCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n         used for the linear transformation of the inputs. Default:\n         `glorot_uniform`.\n       recurrent_initializer: Initializer for the `recurrent_kernel`\n-        weights matrix, used for the linear transformation of the recurrent state.\n-        Default: `orthogonal`.\n+        weights matrix, used for the linear transformation of the recurrent\n+        state.  Default: `orthogonal`.\n       bias_initializer: Initializer for the bias vector. Default: `zeros`.\n       kernel_regularizer: Regularizer function applied to the `kernel` weights\n         matrix. Default: `None`.\n       recurrent_regularizer: Regularizer function applied to the\n         `recurrent_kernel` weights matrix. Default: `None`.\n-      bias_regularizer: Regularizer function applied to the bias vector. Default:\n-        `None`.\n+      bias_regularizer: Regularizer function applied to the bias vector.\n+        Default: `None`.\n       kernel_constraint: Constraint function applied to the `kernel` weights\n         matrix. Default: `None`.\n-      recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n-        weights matrix. Default: `None`.\n+      recurrent_constraint: Constraint function applied to the\n+        `recurrent_kernel` weights matrix. Default: `None`.\n       bias_constraint: Constraint function applied to the bias vector. Default:\n         `None`.\n       dropout: Float between 0 and 1. Fraction of the units to drop for the\n         linear transformation of the inputs. Default: 0.\n-      recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n-        the linear transformation of the recurrent state. Default: 0.\n+      recurrent_dropout: Float between 0 and 1. Fraction of the units to drop\n+        for the linear transformation of the recurrent state. Default: 0.\n       reset_after: GRU convention (whether to apply reset gate after or\n         before matrix multiplication). False = \"before\",\n         True = \"after\" (default and cuDNN compatible).\n \n     Call arguments:\n       inputs: A 2D tensor, with shape of `[batch, feature]`.\n-      states: A 2D tensor with shape of `[batch, units]`, which is the state from\n-        the previous time step. For timestep 0, the initial state provided by user\n-        will be feed to cell.\n+      states: A 2D tensor with shape of `[batch, units]`, which is the state\n+        from the previous time step. For timestep 0, the initial state provided\n+        by user will be feed to cell.\n       training: Python boolean indicating whether the layer should behave in\n         training mode or in inference mode. Only relevant when `dropout` or\n         `recurrent_dropout` is used.\n@@ -205,9 +205,9 @@ class GRUCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n                 bias_shape = (3 * self.units,)\n             else:\n                 # separate biases for input and recurrent kernels\n-                # Note: the shape is intentionally different from CuDNNGRU biases\n-                # `(2 * 3 * self.units,)`, so that we can distinguish the classes\n-                # when loading and converting saved weights.\n+                # Note: the shape is intentionally different from CuDNNGRU\n+                # biases `(2 * 3 * self.units,)`, so that we can distinguish the\n+                # classes when loading and converting saved weights.\n                 bias_shape = (2, 3 * self.units)\n             self.bias = self.add_weight(\n                 shape=bias_shape,\n@@ -413,9 +413,9 @@ class GRU(DropoutRNNCellMixin, RNN, base_layer.BaseRandomLayer):\n     7. Inputs, if use masking, are strictly right-padded.\n     8. Eager execution is enabled in the outermost context.\n \n-    There are two variants of the GRU implementation. The default one is based on\n-    [v3](https://arxiv.org/abs/1406.1078v3) and has reset gate applied to hidden\n-    state before matrix multiplication. The other one is based on\n+    There are two variants of the GRU implementation. The default one is based\n+    on [v3](https://arxiv.org/abs/1406.1078v3) and has reset gate applied to\n+    hidden state before matrix multiplication. The other one is based on\n     [original](https://arxiv.org/abs/1406.1078v1) and has the order reversed.\n \n     The second variant is compatible with CuDNNGRU (GPU-only) and allows\n@@ -460,20 +460,20 @@ class GRU(DropoutRNNCellMixin, RNN, base_layer.BaseRandomLayer):\n         matrix. Default: `None`.\n       recurrent_regularizer: Regularizer function applied to the\n         `recurrent_kernel` weights matrix. Default: `None`.\n-      bias_regularizer: Regularizer function applied to the bias vector. Default:\n-        `None`.\n+      bias_regularizer: Regularizer function applied to the bias vector.\n+        Default: `None`.\n       activity_regularizer: Regularizer function applied to the output of the\n         layer (its \"activation\"). Default: `None`.\n       kernel_constraint: Constraint function applied to the `kernel` weights\n         matrix. Default: `None`.\n-      recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n-        weights matrix. Default: `None`.\n+      recurrent_constraint: Constraint function applied to the\n+        `recurrent_kernel` weights matrix. Default: `None`.\n       bias_constraint: Constraint function applied to the bias vector. Default:\n         `None`.\n-      dropout: Float between 0 and 1. Fraction of the units to drop for the linear\n-        transformation of the inputs. Default: 0.\n-      recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n-        the linear transformation of the recurrent state. Default: 0.\n+      dropout: Float between 0 and 1. Fraction of the units to drop for the\n+        linear transformation of the inputs. Default: 0.\n+      recurrent_dropout: Float between 0 and 1. Fraction of the units to drop\n+        for the linear transformation of the recurrent state. Default: 0.\n       return_sequences: Boolean. Whether to return the last output\n         in the output sequence, or the full sequence. Default: `False`.\n       return_state: Boolean. Whether to return the last state in addition to the\n@@ -607,8 +607,8 @@ class GRU(DropoutRNNCellMixin, RNN, base_layer.BaseRandomLayer):\n             and tf.compat.v1.executing_eagerly_outside_functions()\n         )\n         if tf.config.list_logical_devices(\"GPU\"):\n-            # Only show the message when there is GPU available, user will not care\n-            # about the cuDNN if there isn't any GPU.\n+            # Only show the message when there is GPU available, user will not\n+            # care about the cuDNN if there isn't any GPU.\n             if self._could_use_gpu_kernel:\n                 logging.debug(gru_lstm_utils.CUDNN_AVAILABLE_MSG % self.name)\n             else:\n@@ -812,7 +812,8 @@ class GRU(DropoutRNNCellMixin, RNN, base_layer.BaseRandomLayer):\n     ):\n         # Use the new defun approach for backend implementation swap.\n         # Note that different implementations need to have same function\n-        # signature, eg, the tensor parameters need to have same shape and dtypes.\n+        # signature, eg, the tensor parameters need to have same shape and\n+        # dtypes.\n \n         self.reset_dropout_mask()\n         dropout_mask = self.get_dropout_mask_for_cell(inputs, training, count=3)\n@@ -865,7 +866,8 @@ class GRU(DropoutRNNCellMixin, RNN, base_layer.BaseRandomLayer):\n             if tf.executing_eagerly():\n                 device_type = gru_lstm_utils.get_context_device_type()\n                 can_use_gpu = (\n-                    # Either user specified GPU or unspecified but GPU is available.\n+                    # Either user specified GPU or unspecified but GPU is\n+                    # available.\n                     (\n                         device_type == gru_lstm_utils.GPU_DEVICE_NAME\n                         or (\n@@ -928,19 +930,19 @@ def standard_gru(\n       init_h: Initial state tensor for the cell output.\n       kernel: Weights for cell kernel.\n       recurrent_kernel: Weights for cell recurrent kernel.\n-      bias: Weights for cell kernel bias and recurrent bias. The bias contains the\n-        combined input_bias and recurrent_bias.\n+      bias: Weights for cell kernel bias and recurrent bias. The bias contains\n+        the combined input_bias and recurrent_bias.\n       mask: Binary tensor of shape `(samples, timesteps)` indicating whether\n         a given timestep should be masked. An individual `True` entry indicates\n-        that the corresponding timestep should be utilized, while a `False` entry\n-        indicates that the corresponding timestep should be ignored.\n+        that the corresponding timestep should be utilized, while a `False`\n+        entry indicates that the corresponding timestep should be ignored.\n       time_major: Boolean, whether the inputs are in the format of\n         [time, batch, feature] or [batch, time, feature].\n       go_backwards: Boolean (default False). If True, process the input sequence\n         backwards and return the reversed sequence.\n-      sequence_lengths: The lengths of all sequences coming from a variable length\n-        input, such as ragged tensors. If the input has a fixed timestep size,\n-        this should be None.\n+      sequence_lengths: The lengths of all sequences coming from a variable\n+        length input, such as ragged tensors. If the input has a fixed timestep\n+        size, this should be None.\n       zero_output_for_mask: Boolean, whether to output zero for masked timestep.\n       return_sequences: Boolean. If True, return the recurrent outputs for all\n         timesteps in the sequence. If False, only return the output for the\n@@ -1044,9 +1046,10 @@ def gpu_gru(\n     bias = tf.split(backend.flatten(bias), 6)\n \n     if tf.sysconfig.get_build_info()[\"is_cuda_build\"]:\n-        # Note that the gate order for cuDNN is different from the canonical format.\n-        # canonical format is [z, r, h], whereas cuDNN is [r, z, h]. The swap need\n-        # to be done for kernel, recurrent_kernel, input_bias, recurrent_bias.\n+        # Note that the gate order for cuDNN is different from the canonical\n+        # format.  canonical format is [z, r, h], whereas cuDNN is [r, z, h].\n+        # The swap need to be done for kernel, recurrent_kernel, input_bias,\n+        # recurrent_bias.\n         # z is update gate weights.\n         # r is reset gate weights.\n         # h is output gate weights.\n@@ -1112,11 +1115,11 @@ def gpu_gru(\n     h = tf.squeeze(h, axis=seq_axis)\n \n     # In the case of variable length input, the cudnn kernel will fill zeros for\n-    # the output, whereas the default keras behavior is to bring over the previous\n-    # output for t-1, so that in the return_sequence=False case, user can quickly\n-    # get the final effect output instead just 0s at the last timestep.\n-    # In order to mimic the default keras behavior, we copy the final h state as\n-    # the last_output, since it is numerically same as the output.\n+    # the output, whereas the default keras behavior is to bring over the\n+    # previous output for t-1, so that in the return_sequence=False case, user\n+    # can quickly get the final effect output instead just 0s at the last\n+    # timestep.  In order to mimic the default keras behavior, we copy the final\n+    # h state as the last_output, since it is numerically same as the output.\n     if sequence_lengths is not None:\n         last_output = h\n \n@@ -1165,15 +1168,15 @@ def gru_with_backend_selection(\n         is used in this case.\n       mask: Boolean tensor for mask out the steps within sequence.\n         An individual `True` entry indicates that the corresponding timestep\n-        should be utilized, while a `False` entry indicates that the corresponding\n-        timestep should be ignored.\n+        should be utilized, while a `False` entry indicates that the\n+        corresponding timestep should be ignored.\n       time_major: Boolean, whether the inputs are in the format of\n         [time, batch, feature] or [batch, time, feature].\n       go_backwards: Boolean (default False). If True, process the input sequence\n         backwards and return the reversed sequence.\n-      sequence_lengths: The lengths of all sequences coming from a variable length\n-        input, such as ragged tensors. If the input has a fixed timestep size,\n-        this should be None.\n+      sequence_lengths: The lengths of all sequences coming from a variable\n+        length input, such as ragged tensors. If the input has a fixed timestep\n+        size, this should be None.\n       zero_output_for_mask: Boolean, whether to output zero for masked timestep.\n       return_sequences: Boolean. If True, return the recurrent outputs for all\n         timesteps in the sequence. If False, only return the output for the\n\n@@ -66,7 +66,8 @@ class RNNV2Test(test_combinations.TestCase):\n \n     @parameterized.parameters([lstm.LSTM, gru.GRU])\n     def test_reset_dropout_mask_between_batch(self, layer):\n-        # See https://github.com/tensorflow/tensorflow/issues/29187 for more details\n+        # See https://github.com/tensorflow/tensorflow/issues/29187 for more\n+        # details\n         batch_size = 8\n         timestep = 12\n         embedding_dim = 10\n\n@@ -100,7 +100,8 @@ class DefunWrapper:\n def canonical_to_params(weights, biases, shape, transpose_weights=False):\n     \"\"\"Utility function convert variable to cuDNN compatible parameter.\n \n-    Note that Keras weights for kernels are different from the cuDNN format. Eg.:\n+    Note that Keras weights for kernels are different from the cuDNN format.\n+    Eg.:\n \n     ```\n       Keras                 cuDNN\n@@ -142,8 +143,8 @@ def is_sequence_right_padded(mask):\n     Mixture of mask/unmasked data: [[True, False, True, False, False]].\n \n     Note that for the mixed data example above, the actually data RNN should see\n-    are those 2 Trues (index 0 and 2), the index 1 False should be ignored and not\n-    pollute the internal states.\n+    are those 2 Trues (index 0 and 2), the index 1 False should be ignored and\n+    not pollute the internal states.\n \n     Args:\n       mask: the Boolean tensor with shape [batch, timestep]\n@@ -158,11 +159,11 @@ def is_sequence_right_padded(mask):\n \n \n def has_fully_masked_sequence(mask):\n-    # See https://github.com/tensorflow/tensorflow/issues/33148 for more details.\n-    # Cudnn kernel will error out if the input sequence contains any fully masked\n-    # data. We walk around this issue by rerouting the computation to standard\n-    # kernel, until the issue on cudnn side has been fixed.\n-    # For a fully masked sequence, it will contain all Falses. To make it easy to\n+    # See https://github.com/tensorflow/tensorflow/issues/33148 for more\n+    # details.  Cudnn kernel will error out if the input sequence contains any\n+    # fully masked data. We walk around this issue by rerouting the computation\n+    # to standard kernel, until the issue on cudnn side has been fixed.  For a\n+    # fully masked sequence, it will contain all Falses. To make it easy to\n     # check, we inverse the boolean, check if any of the sequence has all True.\n     return tf.reduce_any(tf.reduce_all(tf.logical_not(mask), axis=1))\n \n@@ -185,15 +186,15 @@ def calculate_sequence_by_mask(mask, time_major):\n     Consider the following example:\n       a = [[True, True, False, False],\n            [True, True, True, False]]\n-    It is a (2, 4) tensor, and the corresponding sequence length result should be\n-    1D tensor with value [2, 3]. Note that the masking tensor must be right\n+    It is a (2, 4) tensor, and the corresponding sequence length result should\n+    be 1D tensor with value [2, 3]. Note that the masking tensor must be right\n     padded that could be checked by, e.g., `is_sequence_right_padded()`.\n \n     Args:\n       mask: Boolean tensor with shape [batch, timestep] or [timestep, batch] if\n         time_major=True.\n-      time_major: Boolean, which indicates whether the mask is time major or batch\n-        major.\n+      time_major: Boolean, which indicates whether the mask is time major or\n+        batch major.\n     Returns:\n       sequence_length: 1D int32 tensor.\n     \"\"\"\n@@ -250,7 +251,8 @@ def function_register(func, *args, **kwargs):\n       a `ConcreteFunction` object specialized to inputs and execution context.\n \n     Raises:\n-      ValueError: When the input function is not a defun wrapped python function.\n+      ValueError: When the input function is not a defun wrapped python\n+        function.\n     \"\"\"\n     concrete_func = func.get_concrete_function(*args, **kwargs)\n     concrete_func.add_to_graph()\n\n@@ -212,7 +212,8 @@ class GRUGraphRewriteTest(test_combinations.TestCase):\n \n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm MIOpen does not support padded input yet.\",\n+        skip_message=\"Skipping as ROCm MIOpen does not support padded \"\n+        \"input yet.\",\n     )\n     def test_with_masking_layer_GRU(self):\n         layer_class = keras.layers.GRU\n@@ -230,7 +231,8 @@ class GRUGraphRewriteTest(test_combinations.TestCase):\n \n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm MIOpen does not support padded input yet.\",\n+        skip_message=\"Skipping as ROCm MIOpen does not support padded \"\n+        \"input yet.\",\n     )\n     def test_masking_with_stacking_GRU(self):\n         inputs = np.random.random((2, 3, 4))\n@@ -280,7 +282,8 @@ class GRUGraphRewriteTest(test_combinations.TestCase):\n \n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm MIOpen does not support padded input yet.\",\n+        skip_message=\"Skipping as ROCm MIOpen does not support padded \"\n+        \"input yet.\",\n     )\n     def test_return_states_GRU(self):\n         layer_class = keras.layers.GRU\n@@ -366,7 +369,8 @@ class GRUGraphRewriteTest(test_combinations.TestCase):\n \n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm MIOpen does not support padded input yet.\",\n+        skip_message=\"Skipping as ROCm MIOpen does not support padded \"\n+        \"input yet.\",\n     )\n     def test_statefulness_GRU(self):\n         num_samples = 2\n@@ -476,7 +480,8 @@ class GRUGraphRewriteTest(test_combinations.TestCase):\n \n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm MIOpen does not support padded input yet.\",\n+        skip_message=\"Skipping as ROCm MIOpen does not support padded \"\n+        \"input yet.\",\n     )\n     @test_utils.run_v2_only\n     def test_explicit_device_with_go_backward_and_mask(self):\n@@ -624,12 +629,13 @@ class GRUGraphRewriteTest(test_combinations.TestCase):\n \n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm MIOpen does not support padded input yet.\",\n+        skip_message=\"Skipping as ROCm MIOpen does not support padded \"\n+        \"input yet.\",\n     )\n     @test_utils.run_v2_only\n     def test_GRU_runtime_with_mask(self):\n-        # Masking will affect which backend is selected based on whether the mask\n-        # is strictly right padded.\n+        # Masking will affect which backend is selected based on whether the\n+        # mask is strictly right padded.\n         layer = keras.layers.GRU(self.rnn_state_size, return_runtime=True)\n \n         inputs = keras.layers.Input(\n@@ -806,7 +812,8 @@ class GRULayerTest(test_combinations.TestCase):\n \n     def test_recurrent_dropout_with_implementation_restriction(self):\n         layer = keras.layers.GRU(2, recurrent_dropout=0.1, implementation=2)\n-        # The implementation is force to 1 due to the limit of recurrent_dropout.\n+        # The implementation is force to 1 due to the limit of\n+        # recurrent_dropout.\n         self.assertEqual(layer.implementation, 1)\n \n     @parameterized.parameters([0, 1, 2])\n\n@@ -60,8 +60,8 @@ class GRUCell(gru.GRUCell):\n       recurrent_constraint: Constraint function applied to\n         the `recurrent_kernel` weights matrix.\n       bias_constraint: Constraint function applied to the bias vector.\n-      dropout: Float between 0 and 1.\n-        Fraction of the units to drop for the linear transformation of the inputs.\n+      dropout: Float between 0 and 1. Fraction of the units to drop for the\n+        linear transformation of the inputs.\n       recurrent_dropout: Float between 0 and 1.\n         Fraction of the units to drop for\n         the linear transformation of the recurrent state.\n@@ -146,8 +146,8 @@ class GRU(RNN):\n       use_bias: Boolean, whether the layer uses a bias vector.\n       kernel_initializer: Initializer for the `kernel` weights matrix,\n         used for the linear transformation of the inputs.\n-      recurrent_initializer: Initializer for the `recurrent_kernel`\n-        weights matrix, used for the linear transformation of the recurrent state.\n+      recurrent_initializer: Initializer for the `recurrent_kernel` weights\n+        matrix, used for the linear transformation of the recurrent state.\n       bias_initializer: Initializer for the bias vector.\n       kernel_regularizer: Regularizer function applied to\n         the `kernel` weights matrix.\n\n@@ -41,7 +41,8 @@ _config = tf.compat.v1.ConfigProto(graph_options=_graph_options)\n class GRUGraphRewriteTest(test_combinations.TestCase):\n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm MIOpen does not support padded input yet.\",\n+        skip_message=\"Skipping as ROCm MIOpen does not support padded \"\n+        \"input yet.\",\n     )\n     @test_utils.run_v2_only\n     def test_gru_feature_parity_v1_v2(self):\n@@ -143,7 +144,8 @@ class GRUGraphRewriteTest(test_combinations.TestCase):\n \n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm MIOpen does not support padded input yet.\",\n+        skip_message=\"Skipping as ROCm MIOpen does not support padded \"\n+        \"input yet.\",\n     )\n     @test_utils.run_v2_only\n     def test_explicit_device_with_go_backward_and_mask_v1(self):\n\n@@ -86,8 +86,9 @@ def assert_like_rnncell(cell_name, cell):\n class _RNNCellWrapperV1(RNNCell):\n     \"\"\"Base class for cells wrappers V1 compatibility.\n \n-    This class along with `_RNNCellWrapperV2` allows to define cells wrappers that\n-    are compatible with V1 and V2, and defines helper methods for this purpose.\n+    This class along with `_RNNCellWrapperV2` allows to define cells wrappers\n+    that are compatible with V1 and V2, and defines helper methods for this\n+    purpose.\n     \"\"\"\n \n     def __init__(self, cell, *args, **kwargs):\n@@ -105,14 +106,15 @@ class _RNNCellWrapperV1(RNNCell):\n         Args:\n           inputs: A tensor with wrapped cell's input.\n           state: A tensor or tuple of tensors with wrapped cell's state.\n-          cell_call_fn: Wrapped cell's method to use for step computation (cell's\n-            `__call__` or 'call' method).\n+          cell_call_fn: Wrapped cell's method to use for step computation\n+            (cell's `__call__` or 'call' method).\n           **kwargs: Additional arguments.\n \n         Returns:\n           A pair containing:\n           - Output: A tensor with cell's output.\n-          - New state: A tensor or tuple of tensors with new wrapped cell's state.\n+          - New state: A tensor or tuple of tensors with new wrapped cell's\n+            state.\n         \"\"\"\n         raise NotImplementedError\n \n@@ -123,8 +125,8 @@ class _RNNCellWrapperV1(RNNCell):\n         method. We directly use the wrapped cell's `__call__` in the overridden\n         wrapper `__call__` method.\n \n-        This allows to use the wrapped cell and the non-wrapped cell equivalently\n-        when using `__call__`.\n+        This allows to use the wrapped cell and the non-wrapped cell\n+        equivalently when using `__call__`.\n \n         Args:\n           inputs: A tensor with wrapped cell's input.\n@@ -136,7 +138,8 @@ class _RNNCellWrapperV1(RNNCell):\n           A pair containing:\n \n           - Output: A tensor with cell's output.\n-          - New state: A tensor or tuple of tensors with new wrapped cell's state.\n+          - New state: A tensor or tuple of tensors with new wrapped cell's\n+            state.\n         \"\"\"\n         return self._call_wrapped_cell(\n             inputs, state, cell_call_fn=self.cell.__call__, scope=scope\n@@ -199,10 +202,11 @@ class DropoutWrapper(_RNNCellWrapperV1):\n     ):\n         \"\"\"Create a cell with added input, state, and/or output dropout.\n \n-        If `variational_recurrent` is set to `True` (**NOT** the default behavior),\n-        then the same dropout mask is applied at every step, as described in:\n-        [A Theoretically Grounded Application of Dropout in Recurrent\n-        Neural Networks. Y. Gal, Z. Ghahramani](https://arxiv.org/abs/1512.05287).\n+        If `variational_recurrent` is set to `True` (**NOT** the default\n+        behavior), then the same dropout mask is applied at every step, as\n+        described in: [A Theoretically Grounded Application of Dropout in\n+        Recurrent Neural Networks. Y. Gal, Z.\n+        Ghahramani](https://arxiv.org/abs/1512.05287).\n \n         Otherwise a different dropout mask is applied at every time step.\n \n@@ -214,44 +218,51 @@ class DropoutWrapper(_RNNCellWrapperV1):\n         Args:\n           cell: an RNNCell, a projection to output_size is added to it.\n           input_keep_prob: unit Tensor or float between 0 and 1, input keep\n-            probability; if it is constant and 1, no input dropout will be added.\n+            probability; if it is constant and 1, no input dropout will be\n+            added.\n           output_keep_prob: unit Tensor or float between 0 and 1, output keep\n-            probability; if it is constant and 1, no output dropout will be added.\n+            probability; if it is constant and 1, no output dropout will be\n+            added.\n           state_keep_prob: unit Tensor or float between 0 and 1, output keep\n-            probability; if it is constant and 1, no output dropout will be added.\n-            State dropout is performed on the outgoing states of the cell. **Note**\n-            the state components to which dropout is applied when `state_keep_prob`\n-            is in `(0, 1)` are also determined by the argument\n-            `dropout_state_filter_visitor` (e.g. by default dropout is never applied\n-            to the `c` component of an `LSTMStateTuple`).\n+            probability; if it is constant and 1, no output dropout will be\n+            added. State dropout is performed on the outgoing states of the\n+            cell. **Note** the state components to which dropout is applied when\n+            `state_keep_prob` is in `(0, 1)` are also determined by the argument\n+            `dropout_state_filter_visitor` (e.g. by default dropout is never\n+            applied to the `c` component of an `LSTMStateTuple`).\n           variational_recurrent: Python bool.  If `True`, then the same dropout\n-            pattern is applied across all time steps per run call. If this parameter\n-            is set, `input_size` **must** be provided.\n-          input_size: (optional) (possibly nested tuple of) `TensorShape` objects\n-            containing the depth(s) of the input tensors expected to be passed in to\n-            the `DropoutWrapper`.  Required and used **iff** `variational_recurrent\n-            = True` and `input_keep_prob < 1`.\n+            pattern is applied across all time steps per run call. If this\n+            parameter is set, `input_size` **must** be provided.\n+          input_size: (optional) (possibly nested tuple of) `TensorShape`\n+            objects containing the depth(s) of the input tensors expected to be\n+            passed in to the `DropoutWrapper`.  Required and used **iff**\n+            `variational_recurrent = True` and `input_keep_prob < 1`.\n           dtype: (optional) The `dtype` of the input, state, and output tensors.\n             Required and used **iff** `variational_recurrent = True`.\n           seed: (optional) integer, the randomness seed.\n-          dropout_state_filter_visitor: (optional), default: (see below).  Function\n-            that takes any hierarchical level of the state and returns a scalar or\n-            depth=1 structure of Python booleans describing which terms in the state\n-            should be dropped out.  In addition, if the function returns `True`,\n-            dropout is applied across this sublevel.  If the function returns\n-            `False`, dropout is not applied across this entire sublevel.\n-            Default behavior: perform dropout on all terms except the memory (`c`)\n-              state of `LSTMCellState` objects, and don't try to apply dropout to\n-            `TensorArray` objects: ```\n+          dropout_state_filter_visitor: (optional), default: (see below).\n+            Function that takes any hierarchical level of the state and returns\n+            a scalar or depth=1 structure of Python booleans describing which\n+            terms in the state should be dropped out.  In addition, if the\n+            function returns `True`, dropout is applied across this sublevel.\n+            If the function returns `False`, dropout is not applied across this\n+            entire sublevel.  Default behavior: perform dropout on all terms\n+            except the memory (`c`) state of `LSTMCellState` objects, and don't\n+            try to apply dropout to `TensorArray` objects:\n+            ```\n             def dropout_state_filter_visitor(s):\n-              if isinstance(s, LSTMCellState): # Never perform dropout on the c\n-                state. return LSTMCellState(c=False, h=True)\n-              elif isinstance(s, TensorArray): return False return True ```\n+              # Never perform dropout on the c state.\n+              if isinstance(s, LSTMCellState):\n+                return LSTMCellState(c=False, h=True)\n+              elif isinstance(s, TensorArray):\n+                return False\n+              return True\n+            ```\n           **kwargs: dict of keyword arguments for base layer.\n \n         Raises:\n-          TypeError: if `cell` is not an `RNNCell`, or `keep_state_fn` is provided\n-            but not `callable`.\n+          TypeError: if `cell` is not an `RNNCell`, or `keep_state_fn` is\n+            provided but not `callable`.\n           ValueError: if any of the keep_probs are not between 0 and 1.\n         \"\"\"\n         super().__init__(cell, dtype=dtype, **kwargs)\n@@ -321,8 +332,8 @@ class DropoutWrapper(_RNNCellWrapperV1):\n             ):\n                 if input_size is None:\n                     raise ValueError(\n-                        \"When variational_recurrent=True and input_keep_prob < 1.0 or \"\n-                        \"is unknown, input_size must be provided\"\n+                        \"When variational_recurrent=True and input_keep_prob \"\n+                        \"< 1.0 or is unknown, input_size must be provided\"\n                     )\n                 self._recurrent_input_noise = _enumerated_map_structure_up_to(\n                     input_size,\n@@ -428,15 +439,16 @@ class DropoutWrapper(_RNNCellWrapperV1):\n         Args:\n           inputs: A tensor with wrapped cell's input.\n           state: A tensor or tuple of tensors with wrapped cell's state.\n-          cell_call_fn: Wrapped cell's method to use for step computation (cell's\n-            `__call__` or 'call' method).\n+          cell_call_fn: Wrapped cell's method to use for step computation\n+            (cell's `__call__` or 'call' method).\n           **kwargs: Additional arguments.\n \n         Returns:\n           A pair containing:\n \n           - Output: A tensor with cell's output.\n-          - New state: A tensor or tuple of tensors with new wrapped cell's state.\n+          - New state: A tensor or tuple of tensors with new wrapped cell's\n+            state.\n         \"\"\"\n \n         def _should_dropout(p):\n@@ -530,10 +542,10 @@ class ResidualWrapper(_RNNCellWrapperV1):\n \n         Args:\n           cell: An instance of `RNNCell`.\n-          residual_fn: (Optional) The function to map raw cell inputs and raw cell\n-            outputs to the actual cell outputs of the residual network.\n-            Defaults to calling nest.map_structure on (lambda i, o: i + o), inputs\n-              and outputs.\n+          residual_fn: (Optional) The function to map raw cell inputs and raw\n+            cell outputs to the actual cell outputs of the residual network.\n+            Defaults to calling nest.map_structure on (lambda i, o: i + o),\n+            inputs and outputs.\n           **kwargs: dict of keyword arguments for base layer.\n         \"\"\"\n         super().__init__(cell, **kwargs)\n@@ -545,8 +557,8 @@ class ResidualWrapper(_RNNCellWrapperV1):\n         Args:\n           inputs: cell inputs.\n           state: cell state.\n-          cell_call_fn: Wrapped cell's method to use for step computation (cell's\n-            `__call__` or 'call' method).\n+          cell_call_fn: Wrapped cell's method to use for step computation\n+            (cell's `__call__` or 'call' method).\n           **kwargs: Additional arguments passed to the wrapped cell's `call`.\n \n         Returns:\n@@ -554,7 +566,8 @@ class ResidualWrapper(_RNNCellWrapperV1):\n \n         Raises:\n           TypeError: If cell inputs and outputs have different structure (type).\n-          ValueError: If cell inputs and outputs have different structure (value).\n+          ValueError: If cell inputs and outputs have different structure\n+            (value).\n         \"\"\"\n         outputs, new_state = cell_call_fn(inputs, state, **kwargs)\n \n\n@@ -169,10 +169,11 @@ class RNNCell(base_layer.Layer):\n \n     def __init__(self, trainable=True, name=None, dtype=None, **kwargs):\n         super().__init__(trainable=trainable, name=name, dtype=dtype, **kwargs)\n-        # Attribute that indicates whether the cell is a TF RNN cell, due the slight\n-        # difference between TF and Keras RNN cell. Notably the state is not wrapped\n-        # in a list for TF cell where they are single tensor state, whereas keras\n-        # cell will wrap the state into a list, and call() will have to unwrap them.\n+        # Attribute that indicates whether the cell is a TF RNN cell, due the\n+        # slight difference between TF and Keras RNN cell. Notably the state is\n+        # not wrapped in a list for TF cell where they are single tensor state,\n+        # whereas keras cell will wrap the state into a list, and call() will\n+        # have to unwrap them.\n         self._is_tf_rnn_cell = True\n \n     def __call__(self, inputs, state, scope=None):\n@@ -180,18 +181,18 @@ class RNNCell(base_layer.Layer):\n \n         Args:\n           inputs: `2-D` tensor with shape `[batch_size, input_size]`.\n-          state: if `self.state_size` is an integer, this should be a `2-D Tensor`\n-            with shape `[batch_size, self.state_size]`.  Otherwise, if\n-            `self.state_size` is a tuple of integers, this should be a tuple with\n-            shapes `[batch_size, s] for s in self.state_size`.\n+          state: if `self.state_size` is an integer, this should be a\n+            `2-D Tensor` with shape `[batch_size, self.state_size]`. Otherwise,\n+            if `self.state_size` is a tuple of integers, this should be a tuple\n+            with shapes `[batch_size, s] for s in self.state_size`.\n           scope: VariableScope for the created subgraph; defaults to class name.\n \n         Returns:\n           A pair containing:\n \n           - Output: A `2-D` tensor with shape `[batch_size, self.output_size]`.\n-          - New state: Either a single `2-D` tensor, or a tuple of tensors matching\n-            the arity and shapes of `state`.\n+          - New state: Either a single `2-D` tensor, or a tuple of tensors\n+            matching the arity and shapes of `state`.\n         \"\"\"\n         if scope is not None:\n             with tf.compat.v1.variable_scope(\n@@ -233,8 +234,8 @@ class RNNCell(base_layer.Layer):\n     def state_size(self):\n         \"\"\"size(s) of state(s) used by this cell.\n \n-        It can be represented by an Integer, a TensorShape or a tuple of Integers\n-        or TensorShapes.\n+        It can be represented by an Integer, a TensorShape or a tuple of\n+        Integers or TensorShapes.\n         \"\"\"\n         raise NotImplementedError(\"Abstract method\")\n \n@@ -250,7 +251,8 @@ class RNNCell(base_layer.Layer):\n \n     def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n         if inputs is not None:\n-            # Validate the given batch_size and dtype against inputs if provided.\n+            # Validate the given batch_size and dtype against inputs if\n+            # provided.\n             inputs = tf.convert_to_tensor(inputs, name=\"inputs\")\n             if batch_size is not None:\n                 if tf.is_tensor(batch_size):\n@@ -262,14 +264,16 @@ class RNNCell(base_layer.Layer):\n                 if inputs.shape.dims[0].value != static_batch_size:\n                     raise ValueError(\n                         \"batch size from input tensor is different from the \"\n-                        f\"input param. Input tensor batch: {inputs.shape.dims[0].value}, \"\n+                        f\"input param. Input tensor batch: \"\n+                        f\"{inputs.shape.dims[0].value}, \"\n                         f\"batch_size: {batch_size}\"\n                     )\n \n             if dtype is not None and inputs.dtype != dtype:\n                 raise ValueError(\n                     \"dtype from input tensor is different from the \"\n-                    f\"input param. Input tensor dtype: {inputs.dtype}, dtype: {dtype}\"\n+                    f\"input param. Input tensor dtype: {inputs.dtype}, \"\n+                    f\"dtype: {dtype}\"\n                 )\n \n             batch_size = (\n@@ -278,8 +282,8 @@ class RNNCell(base_layer.Layer):\n             dtype = inputs.dtype\n         if batch_size is None or dtype is None:\n             raise ValueError(\n-                \"batch_size and dtype cannot be None while constructing initial \"\n-                f\"state: batch_size={batch_size}, dtype={dtype}\"\n+                \"batch_size and dtype cannot be None while constructing \"\n+                f\"initial state: batch_size={batch_size}, dtype={dtype}\"\n             )\n         return self.zero_state(batch_size, dtype)\n \n@@ -298,8 +302,8 @@ class RNNCell(base_layer.Layer):\n           a nested list or tuple (of the same structure) of `2-D` tensors with\n           the shapes `[batch_size, s]` for each s in `state_size`.\n         \"\"\"\n-        # Try to use the last cached zero_state. This is done to avoid recreating\n-        # zeros, especially when eager execution is enabled.\n+        # Try to use the last cached zero_state. This is done to avoid\n+        # recreating zeros, especially when eager execution is enabled.\n         state_size = self.state_size\n         is_eager = tf.executing_eagerly()\n         if is_eager and _hasattr(self, \"_last_zero_state\"):\n@@ -327,8 +331,9 @@ class RNNCell(base_layer.Layer):\n \n     @property\n     def _use_input_spec_as_call_signature(self):\n-        # We do not store the shape information for the state argument in the call\n-        # function for legacy RNN cells, so do not generate an input signature.\n+        # We do not store the shape information for the state argument in the\n+        # call function for legacy RNN cells, so do not generate an input\n+        # signature.\n         return False\n \n \n@@ -336,11 +341,11 @@ class LayerRNNCell(RNNCell):\n     \"\"\"Subclass of RNNCells that act like proper `tf.Layer` objects.\n \n     For backwards compatibility purposes, most `RNNCell` instances allow their\n-    `call` methods to instantiate variables via `tf.compat.v1.get_variable`.  The\n-    underlying\n-    variable scope thus keeps track of any variables, and returning cached\n-    versions.  This is atypical of `tf.layer` objects, which separate this\n-    part of layer building into a `build` method that is only called once.\n+    `call` methods to instantiate variables via `tf.compat.v1.get_variable`.\n+    The underlying variable scope thus keeps track of any variables, and\n+    returning cached versions.  This is atypical of `tf.layer` objects, which\n+    separate this part of layer building into a `build` method that is only\n+    called once.\n \n     Here we provide a subclass for `RNNCell` objects that act exactly as\n     `Layer` objects do.  They must provide a `build` method and their\n@@ -352,10 +357,10 @@ class LayerRNNCell(RNNCell):\n \n         Args:\n           inputs: `2-D` tensor with shape `[batch_size, input_size]`.\n-          state: if `self.state_size` is an integer, this should be a `2-D Tensor`\n-            with shape `[batch_size, self.state_size]`.  Otherwise, if\n-            `self.state_size` is a tuple of integers, this should be a tuple with\n-            shapes `[batch_size, s] for s in self.state_size`.\n+          state: if `self.state_size` is an integer, this should be a `2-D\n+            Tensor` with shape `[batch_size, self.state_size]`.  Otherwise, if\n+            `self.state_size` is a tuple of integers, this should be a tuple\n+            with shapes `[batch_size, s] for s in self.state_size`.\n           scope: optional cell scope.\n           *args: Additional positional arguments.\n           **kwargs: Additional keyword arguments.\n@@ -364,8 +369,8 @@ class LayerRNNCell(RNNCell):\n           A pair containing:\n \n           - Output: A `2-D` tensor with shape `[batch_size, self.output_size]`.\n-          - New state: Either a single `2-D` tensor, or a tuple of tensors matching\n-            the arity and shapes of `state`.\n+          - New state: Either a single `2-D` tensor, or a tuple of tensors\n+            matching the arity and shapes of `state`.\n         \"\"\"\n         # Bypass RNNCell's variable capturing semantics for LayerRNNCell.\n         # Instead, it is up to subclasses to provide a proper build\n@@ -387,8 +392,8 @@ class BasicRNNCell(LayerRNNCell):\n       num_units: int, The number of units in the RNN cell.\n       activation: Nonlinearity to use.  Default: `tanh`. It could also be string\n         that is within Keras activation function names.\n-      reuse: (optional) Python boolean describing whether to reuse variables in an\n-        existing scope.  If not `True`, and the existing scope already has the\n+      reuse: (optional) Python boolean describing whether to reuse variables in\n+        an existing scope. If not `True`, and the existing scope already has the\n         given variables, an error is raised.\n       name: String, the name of the layer. Layers with the same name will share\n         weights, but to avoid mistakes we require reuse=True in such cases.\n@@ -464,7 +469,8 @@ class BasicRNNCell(LayerRNNCell):\n         self.built = True\n \n     def call(self, inputs, state):\n-        \"\"\"Most basic RNN: output = new_state = act(W * input + U * state + B).\"\"\"\n+        \"\"\"Most basic RNN: output = new_state = act(W * input + U * state +\n+        B).\"\"\"\n         _check_rnn_cell_input_dtypes([inputs, state])\n         gate_inputs = tf.matmul(tf.concat([inputs, state], 1), self._kernel)\n         gate_inputs = tf.nn.bias_add(gate_inputs, self._bias)\n@@ -493,9 +499,9 @@ class GRUCell(LayerRNNCell):\n     Args:\n       num_units: int, The number of units in the GRU cell.\n       activation: Nonlinearity to use.  Default: `tanh`.\n-      reuse: (optional) Python boolean describing whether to reuse variables in an\n-        existing scope.  If not `True`, and the existing scope already has the\n-        given variables, an error is raised.\n+      reuse: (optional) Python boolean describing whether to reuse variables in\n+        an existing scope. If not `True`, and the existing scope already has\n+        the given variables, an error is raised.\n       kernel_initializer: (optional) The initializer to use for the weight and\n         projection matrices.\n       bias_initializer: (optional) The initializer to use for the bias.\n@@ -505,9 +511,8 @@ class GRUCell(LayerRNNCell):\n         the first input). Required when `build` is called before `call`.\n       **kwargs: Dict, keyword named properties for common layer attributes, like\n         `trainable` etc when constructing the cell from configs of get_config().\n-        References: Learning Phrase Representations using RNN Encoder Decoder for\n-          Statistical\n-      Machine Translation: [Cho et al., 2014]\n+        References: Learning Phrase Representations using RNN Encoder Decoder\n+        for Statistical Machine Translation: [Cho et al., 2014]\n         (https://aclanthology.coli.uni-saarland.de/papers/D14-1179/d14-1179)\n         ([pdf](http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf))\n     \"\"\"\n@@ -702,24 +707,28 @@ class BasicLSTMCell(LayerRNNCell):\n \n         Args:\n           num_units: int, The number of units in the LSTM cell.\n-          forget_bias: float, The bias added to forget gates (see above). Must set\n-            to `0.0` manually when restoring from CudnnLSTM-trained checkpoints.\n-          state_is_tuple: If True, accepted and returned states are 2-tuples of the\n-            `c_state` and `m_state`.  If False, they are concatenated along the\n-            column axis.  The latter behavior will soon be deprecated.\n-          activation: Activation function of the inner states.  Default: `tanh`. It\n-            could also be string that is within Keras activation function names.\n-          reuse: (optional) Python boolean describing whether to reuse variables in\n-            an existing scope.  If not `True`, and the existing scope already has\n-            the given variables, an error is raised.\n-          name: String, the name of the layer. Layers with the same name will share\n-            weights, but to avoid mistakes we require reuse=True in such cases.\n-          dtype: Default dtype of the layer (default of `None` means use the type of\n-            the first input). Required when `build` is called before `call`.\n-          **kwargs: Dict, keyword named properties for common layer attributes, like\n-            `trainable` etc when constructing the cell from configs of get_config().\n-            When restoring from CudnnLSTM-trained checkpoints, must use\n-            `CudnnCompatibleLSTMCell` instead.\n+          forget_bias: float, The bias added to forget gates (see above). Must\n+            set to `0.0` manually when restoring from CudnnLSTM-trained\n+            checkpoints.\n+          state_is_tuple: If True, accepted and returned states are 2-tuples of\n+            the `c_state` and `m_state`.  If False, they are concatenated along\n+            the column axis.  The latter behavior will soon be deprecated.\n+          activation: Activation function of the inner states.  Default: `tanh`.\n+            It could also be string that is within Keras activation function\n+            names.\n+          reuse: (optional) Python boolean describing whether to reuse variables\n+            in an existing scope.  If not `True`, and the existing scope already\n+            has the given variables, an error is raised.\n+          name: String, the name of the layer. Layers with the same name will\n+            share weights, but to avoid mistakes we require reuse=True in such\n+            cases.\n+          dtype: Default dtype of the layer (default of `None` means use the\n+            type of the first input). Required when `build` is called before\n+            `call`.\n+          **kwargs: Dict, keyword named properties for common layer attributes,\n+            like `trainable` etc when constructing the cell from configs of\n+            get_config().  When restoring from CudnnLSTM-trained checkpoints,\n+            must use `CudnnCompatibleLSTMCell` instead.\n         \"\"\"\n         warnings.warn(\n             \"`tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be \"\n@@ -795,8 +804,8 @@ class BasicLSTMCell(LayerRNNCell):\n         Args:\n           inputs: `2-D` tensor with shape `[batch_size, input_size]`.\n           state: An `LSTMStateTuple` of state tensors, each shaped `[batch_size,\n-            num_units]`, if `state_is_tuple` has been set to `True`.  Otherwise, a\n-            `Tensor` shaped `[batch_size, 2 * num_units]`.\n+            num_units]`, if `state_is_tuple` has been set to `True`.  Otherwise,\n+            a `Tensor` shaped `[batch_size, 2 * num_units]`.\n \n         Returns:\n           A pair containing the new hidden state, and the new state (either a\n@@ -903,39 +912,42 @@ class LSTMCell(LayerRNNCell):\n         Args:\n           num_units: int, The number of units in the LSTM cell.\n           use_peepholes: bool, set True to enable diagonal/peephole connections.\n-          cell_clip: (optional) A float value, if provided the cell state is clipped\n-            by this value prior to the cell output activation.\n+          cell_clip: (optional) A float value, if provided the cell state is\n+            clipped by this value prior to the cell output activation.\n           initializer: (optional) The initializer to use for the weight and\n             projection matrices.\n           num_proj: (optional) int, The output dimensionality for the projection\n             matrices.  If None, no projection is performed.\n-          proj_clip: (optional) A float value.  If `num_proj > 0` and `proj_clip` is\n-            provided, then the projected values are clipped elementwise to within\n-            `[-proj_clip, proj_clip]`.\n+          proj_clip: (optional) A float value.  If `num_proj > 0` and\n+            `proj_clip` is provided, then the projected values are clipped\n+            elementwise to within `[-proj_clip, proj_clip]`.\n           num_unit_shards: Deprecated, will be removed by Jan. 2017. Use a\n             variable_scope partitioner instead.\n           num_proj_shards: Deprecated, will be removed by Jan. 2017. Use a\n             variable_scope partitioner instead.\n-          forget_bias: Biases of the forget gate are initialized by default to 1 in\n-            order to reduce the scale of forgetting at the beginning of the\n-            training. Must set it manually to `0.0` when restoring from CudnnLSTM\n-            trained checkpoints.\n-          state_is_tuple: If True, accepted and returned states are 2-tuples of the\n-            `c_state` and `m_state`.  If False, they are concatenated along the\n-            column axis.  This latter behavior will soon be deprecated.\n-          activation: Activation function of the inner states.  Default: `tanh`. It\n-            could also be string that is within Keras activation function names.\n-          reuse: (optional) Python boolean describing whether to reuse variables in\n-            an existing scope.  If not `True`, and the existing scope already has\n-            the given variables, an error is raised.\n-          name: String, the name of the layer. Layers with the same name will share\n-            weights, but to avoid mistakes we require reuse=True in such cases.\n-          dtype: Default dtype of the layer (default of `None` means use the type of\n-            the first input). Required when `build` is called before `call`.\n-          **kwargs: Dict, keyword named properties for common layer attributes, like\n-            `trainable` etc when constructing the cell from configs of get_config().\n-            When restoring from CudnnLSTM-trained checkpoints, use\n-            `CudnnCompatibleLSTMCell` instead.\n+          forget_bias: Biases of the forget gate are initialized by default to 1\n+            in order to reduce the scale of forgetting at the beginning of the\n+            training. Must set it manually to `0.0` when restoring from\n+            CudnnLSTM trained checkpoints.\n+          state_is_tuple: If True, accepted and returned states are 2-tuples of\n+            the `c_state` and `m_state`.  If False, they are concatenated along\n+            the column axis.  This latter behavior will soon be deprecated.\n+          activation: Activation function of the inner states.  Default: `tanh`.\n+            It could also be string that is within Keras activation function\n+            names.\n+          reuse: (optional) Python boolean describing whether to reuse variables\n+            in an existing scope.  If not `True`, and the existing scope already\n+            has the given variables, an error is raised.\n+          name: String, the name of the layer. Layers with the same name will\n+            share weights, but to avoid mistakes we require reuse=True in such\n+            cases.\n+          dtype: Default dtype of the layer (default of `None` means use the\n+            type of the first input). Required when `build` is called before\n+            `call`.\n+          **kwargs: Dict, keyword named properties for common layer attributes,\n+            like `trainable` etc when constructing the cell from configs of\n+            get_config().  When restoring from CudnnLSTM-trained checkpoints,\n+            use `CudnnCompatibleLSTMCell` instead.\n         \"\"\"\n         warnings.warn(\n             \"`tf.nn.rnn_cell.LSTMCell` is deprecated and will be \"\n@@ -1075,9 +1087,10 @@ class LSTMCell(LayerRNNCell):\n \n         Args:\n           inputs: input Tensor, must be 2-D, `[batch, input_size]`.\n-          state: if `state_is_tuple` is False, this must be a state Tensor, `2-D,\n-            [batch, state_size]`.  If `state_is_tuple` is True, this must be a tuple\n-            of state Tensors, both `2-D`, with column sizes `c_state` and `m_state`.\n+          state: if `state_is_tuple` is False, this must be a state Tensor,\n+            `2-D, [batch, state_size]`.  If `state_is_tuple` is True, this must\n+            be a tuple of state Tensors, both `2-D`, with column sizes `c_state`\n+            and `m_state`.\n \n         Returns:\n           A tuple containing:\n@@ -1087,8 +1100,9 @@ class LSTMCell(LayerRNNCell):\n             Here output_dim is:\n                num_proj if num_proj was set,\n                num_units otherwise.\n-          - Tensor(s) representing the new state of LSTM after reading `inputs` when\n-            the previous state was `state`.  Same type and shape(s) as `state`.\n+          - Tensor(s) representing the new state of LSTM after reading `inputs`\n+            when the previous state was `state`.  Same type and shape(s) as\n+            `state`.\n \n         Raises:\n           ValueError: If input size cannot be inferred from inputs via\n@@ -1193,13 +1207,15 @@ class MultiRNNCell(RNNCell):\n \n         Args:\n           cells: list of RNNCells that will be composed in this order.\n-          state_is_tuple: If True, accepted and returned states are n-tuples, where\n-            `n = len(cells)`.  If False, the states are all concatenated along the\n-            column axis.  This latter behavior will soon be deprecated.\n+          state_is_tuple: If True, accepted and returned states are n-tuples,\n+            where `n = len(cells)`.  If False, the states are all concatenated\n+            along the column axis.  This latter behavior will soon be\n+            deprecated.\n \n         Raises:\n-          ValueError: if cells is empty (not allowed), or at least one of the cells\n-            returns a state tuple but the flag `state_is_tuple` is `False`.\n+          ValueError: if cells is empty (not allowed), or at least one of the\n+            cells returns a state tuple but the flag `state_is_tuple` is\n+            `False`.\n         \"\"\"\n         logging.warning(\n             \"`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class \"\n@@ -1257,7 +1273,8 @@ class MultiRNNCell(RNNCell):\n                 )\n             else:\n                 # We know here that state_size of each cell is not a tuple and\n-                # presumably does not contain TensorArrays or anything else fancy\n+                # presumably does not contain TensorArrays or anything else\n+                # fancy\n                 return super().zero_state(batch_size, dtype)\n \n     @property\n@@ -1294,7 +1311,8 @@ class MultiRNNCell(RNNCell):\n                 if self._state_is_tuple:\n                     if not tf.nest.is_nested(state):\n                         raise ValueError(\n-                            f\"Expected state to be a tuple of length {len(self.state_size)}\"\n+                            f\"Expected state to be a tuple of length \"\n+                            f\"{len(self.state_size)}\"\n                             f\", but received: {state}\"\n                         )\n                     cur_state = state[i]\n\n@@ -76,8 +76,8 @@ class LSTMCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n         (`tanh`). If you pass `None`, no activation is applied (ie. \"linear\"\n         activation: `a(x) = x`).\n       recurrent_activation: Activation function to use for the recurrent step.\n-        Default: sigmoid (`sigmoid`). If you pass `None`, no activation is applied\n-        (ie. \"linear\" activation: `a(x) = x`).\n+        Default: sigmoid (`sigmoid`). If you pass `None`, no activation is\n+        applied (ie. \"linear\" activation: `a(x) = x`).\n       use_bias: Boolean, (default `True`), whether the layer uses a bias vector.\n       kernel_initializer: Initializer for the `kernel` weights matrix, used for\n         the linear transformation of the inputs. Default: `glorot_uniform`.\n@@ -93,18 +93,18 @@ class LSTMCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n         matrix. Default: `None`.\n       recurrent_regularizer: Regularizer function applied to\n         the `recurrent_kernel` weights matrix. Default: `None`.\n-      bias_regularizer: Regularizer function applied to the bias vector. Default:\n-        `None`.\n+      bias_regularizer: Regularizer function applied to the bias vector.\n+        Default: `None`.\n       kernel_constraint: Constraint function applied to the `kernel` weights\n         matrix. Default: `None`.\n-      recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n-        weights matrix. Default: `None`.\n+      recurrent_constraint: Constraint function applied to the\n+        `recurrent_kernel` weights matrix. Default: `None`.\n       bias_constraint: Constraint function applied to the bias vector. Default:\n         `None`.\n-      dropout: Float between 0 and 1. Fraction of the units to drop for the linear\n-        transformation of the inputs. Default: 0.\n-      recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n-        the linear transformation of the recurrent state. Default: 0.\n+      dropout: Float between 0 and 1. Fraction of the units to drop for the\n+        linear transformation of the inputs. Default: 0.\n+      recurrent_dropout: Float between 0 and 1. Fraction of the units to drop\n+        for the linear transformation of the recurrent state. Default: 0.\n \n     Call arguments:\n       inputs: A 2D tensor, with shape of `[batch, feature]`.\n@@ -439,29 +439,29 @@ class LSTM(DropoutRNNCellMixin, RNN, base_layer.BaseRandomLayer):\n         matrix. Default: `None`.\n       recurrent_regularizer: Regularizer function applied to the\n         `recurrent_kernel` weights matrix. Default: `None`.\n-      bias_regularizer: Regularizer function applied to the bias vector. Default:\n-        `None`.\n+      bias_regularizer: Regularizer function applied to the bias vector.\n+        Default: `None`.\n       activity_regularizer: Regularizer function applied to the output of the\n         layer (its \"activation\"). Default: `None`.\n       kernel_constraint: Constraint function applied to the `kernel` weights\n         matrix. Default: `None`.\n-      recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n-        weights matrix. Default: `None`.\n+      recurrent_constraint: Constraint function applied to the\n+        `recurrent_kernel` weights matrix. Default: `None`.\n       bias_constraint: Constraint function applied to the bias vector. Default:\n         `None`.\n-      dropout: Float between 0 and 1. Fraction of the units to drop for the linear\n-        transformation of the inputs. Default: 0.\n-      recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n-        the linear transformation of the recurrent state. Default: 0.\n+      dropout: Float between 0 and 1. Fraction of the units to drop for the\n+        linear transformation of the inputs. Default: 0.\n+      recurrent_dropout: Float between 0 and 1. Fraction of the units to drop\n+        for the linear transformation of the recurrent state. Default: 0.\n       return_sequences: Boolean. Whether to return the last output in the output\n         sequence, or the full sequence. Default: `False`.\n       return_state: Boolean. Whether to return the last state in addition to the\n         output. Default: `False`.\n-      go_backwards: Boolean (default `False`). If True, process the input sequence\n-        backwards and return the reversed sequence.\n-      stateful: Boolean (default `False`). If True, the last state for each sample\n-        at index i in a batch will be used as initial state for the sample of\n-        index i in the following batch.\n+      go_backwards: Boolean (default `False`). If True, process the input\n+        sequence backwards and return the reversed sequence.\n+      stateful: Boolean (default `False`). If True, the last state for each\n+      sample at index i in a batch will be used as initial state for the sample\n+        of index i in the following batch.\n       time_major: The shape format of the `inputs` and `outputs` tensors.\n         If True, the inputs and outputs will be in shape\n         `[timesteps, batch, feature]`, whereas in the False case, it will be\n@@ -471,17 +471,17 @@ class LSTM(DropoutRNNCellMixin, RNN, base_layer.BaseRandomLayer):\n         default this function accepts input and emits output in batch-major\n         form.\n       unroll: Boolean (default `False`). If True, the network will be unrolled,\n-        else a symbolic loop will be used. Unrolling can speed-up a RNN, although\n-        it tends to be more memory-intensive. Unrolling is only suitable for short\n-        sequences.\n+        else a symbolic loop will be used. Unrolling can speed-up a RNN,\n+        although it tends to be more memory-intensive. Unrolling is only\n+        suitable for short sequences.\n \n     Call arguments:\n       inputs: A 3D tensor with shape `[batch, timesteps, feature]`.\n       mask: Binary tensor of shape `[batch, timesteps]` indicating whether\n         a given timestep should be masked (optional, defaults to `None`).\n         An individual `True` entry indicates that the corresponding timestep\n-        should be utilized, while a `False` entry indicates that the corresponding\n-        timestep should be ignored.\n+        should be utilized, while a `False` entry indicates that the\n+        corresponding timestep should be ignored.\n       training: Python boolean indicating whether the layer should behave in\n         training mode or in inference mode. This argument is passed to the cell\n         when calling it. This is only relevant if `dropout` or\n@@ -580,8 +580,8 @@ class LSTM(DropoutRNNCellMixin, RNN, base_layer.BaseRandomLayer):\n             and tf.compat.v1.executing_eagerly_outside_functions()\n         )\n         if tf.config.list_logical_devices(\"GPU\"):\n-            # Only show the message when there is GPU available, user will not care\n-            # about the cuDNN if there isn't any GPU.\n+            # Only show the message when there is GPU available, user will not\n+            # care about the cuDNN if there isn't any GPU.\n             if self._could_use_gpu_kernel:\n                 logging.debug(gru_lstm_utils.CUDNN_AVAILABLE_MSG % self.name)\n             else:\n@@ -639,9 +639,9 @@ class LSTM(DropoutRNNCellMixin, RNN, base_layer.BaseRandomLayer):\n         else:\n             # Use the new defun approach for backend implementation swap.\n             # Note that different implementations need to have same function\n-            # signature, eg, the tensor parameters need to have same shape and dtypes.\n-            # Since the cuDNN has an extra set of bias, those bias will be passed to\n-            # both normal and cuDNN implementations.\n+            # signature, eg, the tensor parameters need to have same shape and\n+            # dtypes. Since the cuDNN has an extra set of bias, those bias will\n+            # be passed to both normal and cuDNN implementations.\n             self.reset_dropout_mask()\n             dropout_mask = self.get_dropout_mask_for_cell(\n                 inputs, training, count=4\n@@ -709,7 +709,8 @@ class LSTM(DropoutRNNCellMixin, RNN, base_layer.BaseRandomLayer):\n                 if tf.executing_eagerly():\n                     device_type = gru_lstm_utils.get_context_device_type()\n                     can_use_gpu = (\n-                        # Either user specified GPU or unspecified but GPU is available.\n+                        # Either user specified GPU or unspecified but GPU is\n+                        # available.\n                         (\n                             device_type == gru_lstm_utils.GPU_DEVICE_NAME\n                             or (\n@@ -724,8 +725,8 @@ class LSTM(DropoutRNNCellMixin, RNN, base_layer.BaseRandomLayer):\n                             )\n                         )\n                     )\n-                    # Under eager context, check the device placement and prefer the\n-                    # GPU implementation when GPU is available.\n+                    # Under eager context, check the device placement and prefer\n+                    # the GPU implementation when GPU is available.\n                     if can_use_gpu:\n                         last_output, outputs, new_h, new_c, runtime = gpu_lstm(\n                             **gpu_lstm_kwargs\n@@ -914,8 +915,9 @@ def standard_lstm(\n     removed since cuDNN implementation does not support that.\n \n     Note that the first half of the bias tensor should be ignored by this impl.\n-    The cuDNN impl need an extra set of input gate bias. In order to make the both\n-    function take same shape of parameter, that extra set of bias is also feed\n+    The cuDNN impl need an extra set of input gate bias. In order to make the\n+    both function take same shape of parameter, that extra set of bias is also\n+    feed\n     here.\n \n     Args:\n@@ -928,15 +930,15 @@ def standard_lstm(\n         is used in this case.\n       mask: Boolean tensor for mask out the steps within sequence.\n         An individual `True` entry indicates that the corresponding timestep\n-        should be utilized, while a `False` entry indicates that the corresponding\n-        timestep should be ignored.\n+        should be utilized, while a `False` entry indicates that the\n+        corresponding timestep should be ignored.\n       time_major: boolean, whether the inputs are in the format of\n         [time, batch, feature] or [batch, time, feature].\n       go_backwards: Boolean (default False). If True, process the input sequence\n         backwards and return the reversed sequence.\n-      sequence_lengths: The lengths of all sequences coming from a variable length\n-        input, such as ragged tensors. If the input has a fixed timestep size,\n-        this should be None.\n+      sequence_lengths: The lengths of all sequences coming from a variable\n+        length input, such as ragged tensors. If the input has a fixed timestep\n+        size, this should be None.\n       zero_output_for_mask: Boolean, whether to output zero for masked timestep.\n       return_sequences: Boolean. If True, return the recurrent outputs for all\n         timesteps in the sequence. If False, only return the output for the\n@@ -1013,10 +1015,11 @@ def gpu_lstm(\n     sequence_lengths,\n     return_sequences,\n ):\n-    \"\"\"LSTM with either cuDNN or ROCm implementation which is only available for GPU.\n+    \"\"\"LSTM with either cuDNN or ROCm implementation which is only available for\n+    GPU.\n \n-    Note that currently only right padded data is supported, or the result will be\n-    polluted by the unmasked data which should be filtered.\n+    Note that currently only right padded data is supported, or the result will\n+    be polluted by the unmasked data which should be filtered.\n \n     Args:\n       inputs: Input tensor of LSTM layer.\n@@ -1027,16 +1030,16 @@ def gpu_lstm(\n       bias: Weights for cell kernel bias and recurrent bias. Only recurrent bias\n         is used in this case.\n       mask: Boolean tensor for mask out the steps within sequence. An individual\n-        `True` entry indicates that the corresponding timestep should be utilized,\n-        while a `False` entry indicates that the corresponding timestep should be\n-        ignored.\n+        `True` entry indicates that the corresponding timestep should be\n+        utilized, while a `False` entry indicates that the corresponding\n+        timestep should be ignored.\n       time_major: Boolean, whether the inputs are in the format of [time, batch,\n         feature] or [batch, time, feature].\n       go_backwards: Boolean (default False). If True, process the input sequence\n         backwards and return the reversed sequence.\n-      sequence_lengths: The lengths of all sequences coming from a variable length\n-        input, such as ragged tensors. If the input has a fixed timestep size,\n-        this should be None.\n+      sequence_lengths: The lengths of all sequences coming from a variable\n+        length input, such as ragged tensors. If the input has a fixed timestep\n+        size, this should be None.\n       return_sequences: Boolean. If True, return the recurrent outputs for all\n         timesteps in the sequence. If False, only return the output for the\n         last timestep, matching the CPU function output format.\n@@ -1075,8 +1078,8 @@ def gpu_lstm(\n     full_bias = tf.concat((tf.zeros_like(bias), bias), 0)\n \n     if tf.sysconfig.get_build_info()[\"is_rocm_build\"]:\n-        # ROCm MIOpen's weight sequence for LSTM is different from both canonical\n-        # and Cudnn format\n+        # ROCm MIOpen's weight sequence for LSTM is different from both\n+        # canonical and Cudnn format\n         # MIOpen: [i, f, o, c] Cudnn/Canonical: [i, f, c, o]\n         # i is input gate weights.\n         # f is forget gate weights.\n@@ -1148,11 +1151,11 @@ def gpu_lstm(\n     c = tf.squeeze(c, axis=seq_axis)\n \n     # In the case of variable length input, the cudnn kernel will fill zeros for\n-    # the output, whereas the default keras behavior is to bring over the previous\n-    # output for t-1, so that in the return_sequence=False case, user can quickly\n-    # get the final effect output instead just 0s at the last timestep.\n-    # In order to mimic the default keras behavior, we copy the final h state as\n-    # the last_output, since it is numerically same as the output.\n+    # the output, whereas the default keras behavior is to bring over the\n+    # previous output for t-1, so that in the return_sequence=False case, user\n+    # can quickly get the final effect output instead just 0s at the last\n+    # timestep.  In order to mimic the default keras behavior, we copy the final\n+    # h state as the last_output, since it is numerically same as the output.\n     if sequence_lengths is not None:\n         last_output = h\n \n@@ -1204,15 +1207,15 @@ def lstm_with_backend_selection(\n         is used in this case.\n       mask: Boolean tensor for mask out the steps within sequence.\n         An individual `True` entry indicates that the corresponding timestep\n-        should be utilized, while a `False` entry indicates that the corresponding\n-        timestep should be ignored.\n+        should be utilized, while a `False` entry indicates that the\n+        corresponding timestep should be ignored.\n       time_major: Boolean, whether the inputs are in the format of\n         [time, batch, feature] or [batch, time, feature].\n       go_backwards: Boolean (default False). If True, process the input sequence\n         backwards and return the reversed sequence.\n-      sequence_lengths: The lengths of all sequences coming from a variable length\n-        input, such as ragged tensors. If the input has a fixed timestep size,\n-        this should be None.\n+      sequence_lengths: The lengths of all sequences coming from a variable\n+        length input, such as ragged tensors. If the input has a fixed timestep\n+        size, this should be None.\n       zero_output_for_mask: Boolean, whether to output zero for masked timestep.\n       return_sequences: Boolean. If True, return the recurrent outputs for all\n         timesteps in the sequence. If False, only return the output for the\n\n@@ -264,7 +264,8 @@ class LSTMGraphRewriteTest(test_combinations.TestCase):\n \n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm MIOpen does not support padded input yet.\",\n+        skip_message=\"Skipping as ROCm MIOpen does not support padded \"\n+        \"input yet.\",\n     )\n     def test_return_state(self):\n         num_states = 2\n@@ -347,7 +348,8 @@ class LSTMGraphRewriteTest(test_combinations.TestCase):\n     @parameterized.named_parameters((\"v0\", 0), (\"v1\", 1), (\"v2\", 2))\n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm MIOpen does not support padded input yet.\",\n+        skip_message=\"Skipping as ROCm MIOpen does not support padded \"\n+        \"input yet.\",\n     )\n     def test_implementation_mode_LSTM(self, implementation_mode):\n         num_samples = 2\n@@ -393,7 +395,8 @@ class LSTMGraphRewriteTest(test_combinations.TestCase):\n \n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm MIOpen does not support padded input yet.\",\n+        skip_message=\"Skipping as ROCm MIOpen does not support padded \"\n+        \"input yet.\",\n     )\n     def test_masking_with_stacking_LSTM(self):\n         inputs = np.random.random((2, 3, 4))\n@@ -528,7 +531,8 @@ class LSTMGraphRewriteTest(test_combinations.TestCase):\n \n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm MIOpen does not support padded input yet.\",\n+        skip_message=\"Skipping as ROCm MIOpen does not support padded \"\n+        \"input yet.\",\n     )\n     def test_statefulness_LSTM(self):\n         num_samples = 2\n@@ -675,7 +679,8 @@ class LSTMGraphRewriteTest(test_combinations.TestCase):\n \n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm MIOpen does not support padded input yet.\",\n+        skip_message=\"Skipping as ROCm MIOpen does not support padded \"\n+        \"input yet.\",\n     )\n     @test_utils.run_v2_only\n     def test_explicit_device_with_go_backward_and_mask(self):\n@@ -828,12 +833,13 @@ class LSTMGraphRewriteTest(test_combinations.TestCase):\n \n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm MIOpen does not support padded input yet.\",\n+        skip_message=\"Skipping as ROCm MIOpen does not support padded \"\n+        \"input yet.\",\n     )\n     @test_utils.run_v2_only\n     def test_LSTM_runtime_with_mask(self):\n-        # Masking will affect which backend is selected based on whether the mask\n-        # is strictly right padded.\n+        # Masking will affect which backend is selected based on whether the\n+        # mask is strictly right padded.\n         layer = keras.layers.LSTM(self.rnn_state_size, return_runtime=True)\n \n         inputs = keras.layers.Input(\n@@ -998,7 +1004,8 @@ class LSTMLayerTest(test_combinations.TestCase):\n \n     def test_recurrent_dropout_with_implementation_restriction(self):\n         layer = keras.layers.LSTM(2, recurrent_dropout=0.1, implementation=2)\n-        # The implementation is force to 1 due to the limit of recurrent_dropout.\n+        # The implementation is force to 1 due to the limit of\n+        # recurrent_dropout.\n         self.assertEqual(layer.implementation, 1)\n \n     @parameterized.parameters([0, 1, 2])\n\n@@ -43,7 +43,8 @@ _config = tf.compat.v1.ConfigProto(graph_options=_graph_options)\n class LSTMGraphRewriteTest(test_combinations.TestCase):\n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm MIOpen does not support padded input yet.\",\n+        skip_message=\"Skipping as ROCm MIOpen does not support padded \"\n+        \"input yet.\",\n     )\n     @test_utils.run_v2_only\n     def test_lstm_feature_parity_v1_v2(self):\n@@ -170,7 +171,8 @@ class LSTMGraphRewriteTest(test_combinations.TestCase):\n \n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm MIOpen does not support padded input yet.\",\n+        skip_message=\"Skipping as ROCm MIOpen does not support padded \"\n+        \"input yet.\",\n     )\n     @test_utils.run_v2_only\n     def test_explicit_device_with_go_backward_and_mask_v1(self):\n\n@@ -45,17 +45,19 @@ def standardize_args(inputs, initial_state, constants, num_constants):\n     \"\"\"\n     if isinstance(inputs, list):\n         # There are several situations here:\n-        # In the graph mode, __call__ will be only called once. The initial_state\n-        # and constants could be in inputs (from file loading).\n+        # In the graph mode, __call__ will be only called once. The\n+        # initial_state and constants could be in inputs (from file loading).\n         # In the eager mode, __call__ will be called twice, once during\n         # rnn_layer(inputs=input_t, constants=c_t, ...), and second time will be\n-        # model.fit/train_on_batch/predict with real np data. In the second case,\n-        # the inputs will contain initial_state and constants as eager tensor.\n+        # model.fit/train_on_batch/predict with real np data. In the second\n+        # case, the inputs will contain initial_state and constants as eager\n+        # tensor.\n         #\n         # For either case, the real input is the first item in the list, which\n-        # could be a nested structure itself. Then followed by initial_states, which\n-        # could be a list of items, or list of list if the initial_state is complex\n-        # structure, and finally followed by constants which is a flat list.\n+        # could be a nested structure itself. Then followed by initial_states,\n+        # which could be a list of items, or list of list if the initial_state\n+        # is complex structure, and finally followed by constants which is a\n+        # flat list.\n         assert initial_state is None and constants is None\n         if num_constants:\n             constants = inputs[-num_constants:]\n@@ -100,8 +102,8 @@ def generate_zero_filled_state(batch_size_tensor, state_size, dtype):\n     \"\"\"Generate a zero filled tensor with shape [batch_size, state_size].\"\"\"\n     if batch_size_tensor is None or dtype is None:\n         raise ValueError(\n-            \"batch_size and dtype cannot be None while constructing initial state. \"\n-            f\"Received: batch_size={batch_size_tensor}, dtype={dtype}\"\n+            \"batch_size and dtype cannot be None while constructing initial \"\n+            f\"state. Received: batch_size={batch_size_tensor}, dtype={dtype}\"\n         )\n \n     def create_zeros(unnested_state_size):\n@@ -118,15 +120,15 @@ def generate_zero_filled_state(batch_size_tensor, state_size, dtype):\n def caching_device(rnn_cell):\n     \"\"\"Returns the caching device for the RNN variable.\n \n-    This is useful for distributed training, when variable is not located as same\n-    device as the training worker. By enabling the device cache, this allows\n-    worker to read the variable once and cache locally, rather than read it every\n-    time step from remote when it is needed.\n+    This is useful for distributed training, when variable is not located as\n+    same device as the training worker. By enabling the device cache, this\n+    allows worker to read the variable once and cache locally, rather than read\n+    it every time step from remote when it is needed.\n \n-    Note that this is assuming the variable that cell needs for each time step is\n-    having the same value in the forward path, and only gets updated in the\n-    backprop. It is true for all the default cells (SimpleRNN, GRU, LSTM). If the\n-    cell body relies on any variable that gets updated every time step, then\n+    Note that this is assuming the variable that cell needs for each time step\n+    is having the same value in the forward path, and only gets updated in the\n+    backprop. It is true for all the default cells (SimpleRNN, GRU, LSTM). If\n+    the cell body relies on any variable that gets updated every time step, then\n     caching device will cause it to read the stall value.\n \n     Args:\n@@ -137,10 +139,10 @@ def caching_device(rnn_cell):\n         return None\n     if not getattr(rnn_cell, \"_enable_caching_device\", False):\n         return None\n-    # Don't set a caching device when running in a loop, since it is possible that\n-    # train steps could be wrapped in a tf.while_loop. In that scenario caching\n-    # prevents forward computations in loop iterations from re-reading the\n-    # updated weights.\n+    # Don't set a caching device when running in a loop, since it is possible\n+    # that train steps could be wrapped in a tf.while_loop. In that scenario\n+    # caching prevents forward computations in loop iterations from re-reading\n+    # the updated weights.\n     if control_flow_util.IsInWhileLoop(tf.compat.v1.get_default_graph()):\n         logging.warning(\n             \"Variable read device caching has been disabled because the \"\n@@ -180,7 +182,8 @@ def config_for_enable_caching_device(rnn_cell):\n \n     Returns:\n       A dict which contains the JSON config for enable_caching_device value or\n-      empty dict if the enable_caching_device value is same as the default value.\n+      empty dict if the enable_caching_device value is same as the default\n+      value.\n     \"\"\"\n     default_enable_caching_device = (\n         tf.compat.v1.executing_eagerly_outside_functions()\n\n@@ -53,31 +53,31 @@ class SimpleRNNCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n         used for the linear transformation of the inputs. Default:\n         `glorot_uniform`.\n       recurrent_initializer: Initializer for the `recurrent_kernel`\n-        weights matrix, used for the linear transformation of the recurrent state.\n-        Default: `orthogonal`.\n+        weights matrix, used for the linear transformation of the recurrent\n+        state.  Default: `orthogonal`.\n       bias_initializer: Initializer for the bias vector. Default: `zeros`.\n       kernel_regularizer: Regularizer function applied to the `kernel` weights\n         matrix. Default: `None`.\n       recurrent_regularizer: Regularizer function applied to the\n         `recurrent_kernel` weights matrix. Default: `None`.\n-      bias_regularizer: Regularizer function applied to the bias vector. Default:\n-        `None`.\n+      bias_regularizer: Regularizer function applied to the bias vector.\n+        Default: `None`.\n       kernel_constraint: Constraint function applied to the `kernel` weights\n         matrix. Default: `None`.\n-      recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n-        weights matrix. Default: `None`.\n+      recurrent_constraint: Constraint function applied to the\n+        `recurrent_kernel` weights matrix. Default: `None`.\n       bias_constraint: Constraint function applied to the bias vector. Default:\n         `None`.\n-      dropout: Float between 0 and 1. Fraction of the units to drop for the linear\n-        transformation of the inputs. Default: 0.\n-      recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n-        the linear transformation of the recurrent state. Default: 0.\n+      dropout: Float between 0 and 1. Fraction of the units to drop for the\n+        linear transformation of the inputs. Default: 0.\n+      recurrent_dropout: Float between 0 and 1. Fraction of the units to drop\n+        for the linear transformation of the recurrent state. Default: 0.\n \n     Call arguments:\n       inputs: A 2D tensor, with shape of `[batch, feature]`.\n-      states: A 2D tensor with shape of `[batch, units]`, which is the state from\n-        the previous time step. For timestep 0, the initial state provided by user\n-        will be feed to cell.\n+      states: A 2D tensor with shape of `[batch, units]`, which is the state\n+        from the previous time step. For timestep 0, the initial state provided\n+        by user will be feed to cell.\n       training: Python boolean indicating whether the layer should behave in\n         training mode or in inference mode. Only relevant when `dropout` or\n         `recurrent_dropout` is used.\n@@ -265,26 +265,26 @@ class SimpleRNN(RNN):\n         used for the linear transformation of the inputs. Default:\n         `glorot_uniform`.\n       recurrent_initializer: Initializer for the `recurrent_kernel`\n-        weights matrix, used for the linear transformation of the recurrent state.\n-        Default: `orthogonal`.\n+        weights matrix, used for the linear transformation of the recurrent\n+        state.  Default: `orthogonal`.\n       bias_initializer: Initializer for the bias vector. Default: `zeros`.\n       kernel_regularizer: Regularizer function applied to the `kernel` weights\n         matrix. Default: `None`.\n       recurrent_regularizer: Regularizer function applied to the\n         `recurrent_kernel` weights matrix. Default: `None`.\n-      bias_regularizer: Regularizer function applied to the bias vector. Default:\n-        `None`.\n+      bias_regularizer: Regularizer function applied to the bias vector.\n+        Default: `None`.\n       activity_regularizer: Regularizer function applied to the output of the\n         layer (its \"activation\"). Default: `None`.\n       kernel_constraint: Constraint function applied to the `kernel` weights\n         matrix. Default: `None`.\n-      recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n-        weights matrix.  Default: `None`.\n+      recurrent_constraint: Constraint function applied to the\n+        `recurrent_kernel` weights matrix.  Default: `None`.\n       bias_constraint: Constraint function applied to the bias vector. Default:\n         `None`.\n       dropout: Float between 0 and 1.\n-        Fraction of the units to drop for the linear transformation of the inputs.\n-        Default: 0.\n+        Fraction of the units to drop for the linear transformation of the\n+        inputs. Default: 0.\n       recurrent_dropout: Float between 0 and 1.\n         Fraction of the units to drop for the linear transformation of the\n         recurrent state. Default: 0.\n@@ -309,8 +309,8 @@ class SimpleRNN(RNN):\n       inputs: A 3D tensor, with shape `[batch, timesteps, feature]`.\n       mask: Binary tensor of shape `[batch, timesteps]` indicating whether\n         a given timestep should be masked. An individual `True` entry indicates\n-        that the corresponding timestep should be utilized, while a `False` entry\n-        indicates that the corresponding timestep should be ignored.\n+        that the corresponding timestep should be utilized, while a `False`\n+        entry indicates that the corresponding timestep should be ignored.\n       training: Python boolean indicating whether the layer should behave in\n         training mode or in inference mode. This argument is passed to the cell\n         when calling it. This is only relevant if `dropout` or\n\n@@ -67,10 +67,11 @@ class StackedRNNCells(base_layer.Layer):\n                     f\"Received cell without a `state_size`: {cell}\"\n                 )\n         self.cells = cells\n-        # reverse_state_order determines whether the state size will be in a reverse\n-        # order of the cells' state. User might want to set this to True to keep the\n-        # existing behavior. This is only useful when use RNN(return_state=True)\n-        # since the state will be returned as the same order of state_size.\n+        # reverse_state_order determines whether the state size will be in a\n+        # reverse order of the cells' state. User might want to set this to True\n+        # to keep the existing behavior. This is only useful when use\n+        # RNN(return_state=True) since the state will be returned as the same\n+        # order of state_size.\n         self.reverse_state_order = kwargs.pop(\"reverse_state_order\", False)\n         if self.reverse_state_order:\n             logging.warning(\n@@ -135,7 +136,8 @@ class StackedRNNCells(base_layer.Layer):\n         new_nested_states = []\n         for cell, states in zip(self.cells, nested_states):\n             states = states if tf.nest.is_nested(states) else [states]\n-            # TF cell does not wrap the state into list when there is only one state.\n+            # TF cell does not wrap the state into list when there is only one\n+            # state.\n             is_tf_rnn_cell = getattr(cell, \"_is_tf_rnn_cell\", None) is not None\n             states = (\n                 states[0] if len(states) == 1 and is_tf_rnn_cell else states\n@@ -144,8 +146,8 @@ class StackedRNNCells(base_layer.Layer):\n                 kwargs[\"training\"] = training\n             else:\n                 kwargs.pop(\"training\", None)\n-            # Use the __call__ function for callable objects, eg layers, so that it\n-            # will have the proper name scopes for the ops, etc.\n+            # Use the __call__ function for callable objects, eg layers, so that\n+            # it will have the proper name scopes for the ops, etc.\n             cell_call_fn = cell.__call__ if callable(cell) else cell.call\n             if generic_utils.has_arg(cell.call, \"constants\"):\n                 inputs, states = cell_call_fn(\n\n@@ -34,8 +34,8 @@ class TimeDistributed(Wrapper):\n     Every input should be at least 3D, and the dimension of index one of the\n     first input will be considered to be the temporal dimension.\n \n-    Consider a batch of 32 video samples, where each sample is a 128x128 RGB image\n-    with `channels_last` data format, across 10 timesteps.\n+    Consider a batch of 32 video samples, where each sample is a 128x128 RGB\n+    image with `channels_last` data format, across 10 timesteps.\n     The batch input shape is `(32, 10, 128, 128, 3)`.\n \n     You can then use `TimeDistributed` to apply the same `Conv2D` layer to each\n@@ -47,8 +47,8 @@ class TimeDistributed(Wrapper):\n     >>> outputs.shape\n     TensorShape([None, 10, 126, 126, 64])\n \n-    Because `TimeDistributed` applies the same instance of `Conv2D` to each of the\n-    timestamps, the same set of weights are used at each timestamp.\n+    Because `TimeDistributed` applies the same instance of `Conv2D` to each of\n+    the timestamps, the same set of weights are used at each timestamp.\n \n     Args:\n       layer: a `tf.keras.layers.Layer` instance.\n@@ -85,8 +85,8 @@ class TimeDistributed(Wrapper):\n     def _get_shape_tuple(self, init_tuple, tensor, start_idx, int_shape=None):\n         \"\"\"Finds non-specific dimensions in the static shapes.\n \n-        The static shapes are replaced with the corresponding dynamic shapes of the\n-        tensor.\n+        The static shapes are replaced with the corresponding dynamic shapes of\n+        the tensor.\n         Args:\n           init_tuple: a tuple, the first part of the output shape\n           tensor: the tensor from which to get the (static and dynamic) shapes\n@@ -263,9 +263,9 @@ class TimeDistributed(Wrapper):\n                     y, tf.reshape, y, output_shape\n                 )\n                 if not tf.executing_eagerly():\n-                    # Set the static shape for the result since it might be lost during\n-                    # array_ops reshape, eg, some `None` dim in the result could be\n-                    # inferred.\n+                    # Set the static shape for the result since it might be lost\n+                    # during array_ops reshape, eg, some `None` dim in the\n+                    # result could be inferred.\n                     tf.__internal__.nest.map_structure_up_to(\n                         y,\n                         lambda tensor, shape: tensor.set_shape(shape),\n@@ -295,19 +295,19 @@ class TimeDistributed(Wrapper):\n \n         Args:\n           inputs: Tensor with shape [batch size, timesteps, ...] indicating the\n-            input to TimeDistributed. If static shape information is available for\n-            \"batch size\", `mask` is returned unmodified.\n+            input to TimeDistributed. If static shape information is available\n+            for \"batch size\", `mask` is returned unmodified.\n           mask: Either None (indicating no masking) or a Tensor indicating the\n             input mask for TimeDistributed. The shape can be static or dynamic.\n \n         Returns:\n-          Either None (no masking), or a [batch size, timesteps, ...] Tensor with\n-          an output mask for the TimeDistributed layer with the shape beyond the\n-          second dimension being the value of the input mask shape(if the computed\n-          output mask is none), an output mask with the shape beyond the first\n-          dimension being the value of the mask shape(if mask is not None) or\n-          output mask with the shape beyond the first dimension being the\n-          value of the computed output shape.\n+          Either None (no masking), or a [batch size, timesteps, ...] Tensor\n+          with an output mask for the TimeDistributed layer with the shape\n+          beyond the second dimension being the value of the input mask shape(if\n+          the computed output mask is none), an output mask with the shape\n+          beyond the first dimension being the value of the mask shape(if mask\n+          is not None) or output mask with the shape beyond the first dimension\n+          being the value of the computed output shape.\n \n         \"\"\"\n         # cases need to call the layer.compute_mask when input_mask is None:\n@@ -325,8 +325,9 @@ class TimeDistributed(Wrapper):\n             tf.nest.flatten(is_ragged_input)\n         )\n         if batch_size and not self._always_use_reshape or any(is_ragged_input):\n-            # batch size matters, we currently do not handle mask explicitly, or if\n-            # the layer always uses reshape approach, or the input is a ragged tensor.\n+            # batch size matters, we currently do not handle mask explicitly, or\n+            # if the layer always uses reshape approach, or the input is a\n+            # ragged tensor.\n             return mask\n         inner_mask = mask\n         if inner_mask is not None:\n\n@@ -296,7 +296,8 @@ class TimeDistributedTest(test_combinations.TestCase):\n         td1 = keras.layers.TimeDistributed(keras.layers.Dense(5))\n         self.assertTrue(td1._always_use_reshape)\n \n-        # Built-in layers that are stateful don't use the reshape implementation.\n+        # Built-in layers that are stateful don't use the reshape\n+        # implementation.\n         td2 = keras.layers.TimeDistributed(\n             keras.layers.RNN(keras.layers.SimpleRNNCell(10), stateful=True)\n         )\n\n@@ -16,8 +16,6 @@\n \n import tensorflow.compat.v2 as tf\n \n-# pylint: disable=g-bad-import-order,g-direct-tensorflow-import,unused-import,wildcard-import\n-\n import threading\n from keras.engine import base_layer\n from keras.engine import input_layer\n\n@@ -74,8 +74,8 @@ class LayerSerializationTest(parameterized.TestCase, tf.test.TestCase):\n             bias_regularizer=\"l2\",\n         )\n         config = keras.layers.serialize(layer)\n-        # Because we're passing an unknown class here, deserialization should fail\n-        # unless we add SerializableInt to the custom object dict.\n+        # Because we're passing an unknown class here, deserialization should\n+        # fail unless we add SerializableInt to the custom object dict.\n         with self.assertRaisesRegex(\n             ValueError, \"Unknown config_item: SerializableInt.*\"\n         ):\n@@ -89,8 +89,8 @@ class LayerSerializationTest(parameterized.TestCase, tf.test.TestCase):\n             bias_regularizer=\"l2\",\n         )\n         config = keras.layers.serialize(layer)\n-        # Because we're passing an unknown class here, deserialization should fail\n-        # unless we add SerializableInt to the custom object dict.\n+        # Because we're passing an unknown class here, deserialization should\n+        # fail unless we add SerializableInt to the custom object dict.\n         new_layer = keras.layers.deserialize(\n             config, custom_objects={\"SerializableInt\": SerializableInt}\n         )\n\n@@ -609,8 +609,8 @@ class AutoLambdaTest(test_combinations.TestCase):\n             tf.constant(stop, shape=(batch_size,)),\n             tf.constant(step, shape=(batch_size,)),\n         ]\n-        # Slice the innermost dim. only grab one index from the second-to-innermost\n-        # dim, removing that dim from the shape.\n+        # Slice the innermost dim. only grab one index from the\n+        # second-to-innermost dim, removing that dim from the shape.\n         expected = tf.stack(\n             [\n                 tf.stack([tf.range(8)[start:stop:step] for _ in range(4)])\n@@ -757,7 +757,8 @@ class InputInEagerTest(test_combinations.TestCase):\n         x = keras.Input(shape=(1,))\n         ident = tf.identity(x)\n \n-        # This is now a graph tensor, and should be able to continue in graphland\n+        # This is now a graph tensor, and should be able to continue in\n+        # graphland\n         self.assertIn(\"Identity\", ident.name)\n \n     def test_size(self):\n@@ -765,7 +766,8 @@ class InputInEagerTest(test_combinations.TestCase):\n         self.assertAllEqual(x.get_shape().as_list(), [None, 3])\n         sz = tf.size(x)\n \n-        # This is now a graph tensor, and should be able to continue in graphland\n+        # This is now a graph tensor, and should be able to continue in\n+        # graphland\n         self.assertIn(\"Size\", sz.name)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
