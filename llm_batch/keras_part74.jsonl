{"custom_id": "keras#96483326d8e1bd7ba7d9368d8e7552ec19b27a74", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 16 | Files Changed: 3 | Hunks: 10 | Methods Changed: 4 | Complexity Δ (Sum/Max): 14/12 | Churn Δ: 58 | Churn Cumulative: 3715 | Contributors (this commit): 23 | Commits (past 90d): 52 | Contributors (cumulative): 34 | DMM Complexity: 0.0\n\nDIFF:\n@@ -105,7 +105,7 @@ class Embedding(Layer):\n         X = self.get_input(train)\n         retain_p = 1. - self.p\n         if train and self.p > 0:\n-            B = K.random_binomial((self.input_dim), p=retain_p)\n+            B = K.random_binomial((self.input_dim,), p=retain_p)\n         else:\n             B = K.ones((self.input_dim)) * retain_p\n         out = K.gather(self.W * K.expand_dims(B), X) # we zero-out rows of W at random\n\n@@ -78,6 +78,10 @@ class Recurrent(MaskedLayer):\n \n         To reset the states of your model, call `.reset_states()` on either\n         a specific layer, or on your entire model.\n+\n+    # Note on using dropout with TensorFlow\n+        When using the TensorFlow backend, specify a fixed batch size for your model\n+        following the notes on statefulness RNNs.\n     '''\n     input_ndim = 3\n \n@@ -252,8 +256,7 @@ class SimpleRNN(Recurrent):\n         input_shape = self.input_shape\n         if not input_shape[0]:\n             raise Exception('If a RNN is stateful, a complete ' +\n-                            'input_shape must be provided ' +\n-                            '(including batch size).')\n+                            'input_shape must be provided (including batch size).')\n         if hasattr(self, 'states'):\n             K.set_value(self.states[0],\n                         np.zeros((input_shape[0], self.output_dim)))\n@@ -272,8 +275,11 @@ class SimpleRNN(Recurrent):\n \n     def get_constants(self, X, train=False):\n         nb_samples = K.shape(X)[0]\n-        if K._BACKEND == 'tensorflow':\n-            nb_samples = int(nb_samples)\n+        if K._BACKEND == 'tensorflow' and train and self.p_W > 0 and self.p_U > 0:\n+            if not self.input_shape[0]:\n+                raise Exception('For RNN dropout in tensorflow, a complete ' +\n+                                'input_shape must be provided (including batch size).')\n+            nb_samples = self.input_shape[0]\n         retain_p_W = 1. - self.p_W\n         retain_p_U = 1. - self.p_U\n         if train and self.p_W > 0 and self.p_U > 0:\n@@ -391,8 +397,7 @@ class GRU(Recurrent):\n         input_shape = self.input_shape\n         if not input_shape[0]:\n             raise Exception('If a RNN is stateful, a complete ' +\n-                            'input_shape must be provided ' +\n-                            '(including batch size).')\n+                            'input_shape must be provided (including batch size).')\n         if hasattr(self, 'states'):\n             K.set_value(self.states[0],\n                         np.zeros((input_shape[0], self.output_dim)))\n@@ -418,8 +423,11 @@ class GRU(Recurrent):\n \n     def get_constants(self, X, train=False):\n         nb_samples = K.shape(X)[0]\n-        if K._BACKEND == 'tensorflow':\n-            nb_samples = int(nb_samples)\n+        if K._BACKEND == 'tensorflow' and train and self.p_W > 0 and self.p_U > 0:\n+            if not self.input_shape[0]:\n+                raise Exception('For RNN dropout in tensorflow, a complete ' +\n+                                'input_shape must be provided (including batch size).')\n+            nb_samples = self.input_shape[0]\n         retain_p_W = 1. - self.p_W\n         retain_p_U = 1. - self.p_U\n         if train and self.p_W > 0 and self.p_U > 0:\n@@ -553,8 +561,7 @@ class LSTM(Recurrent):\n         input_shape = self.input_shape\n         if not input_shape[0]:\n             raise Exception('If a RNN is stateful, a complete ' +\n-                            'input_shape must be provided ' +\n-                            '(including batch size).')\n+                            'input_shape must be provided (including batch size).')\n         if hasattr(self, 'states'):\n             K.set_value(self.states[0],\n                         np.zeros((input_shape[0], self.output_dim)))\n@@ -585,8 +592,11 @@ class LSTM(Recurrent):\n \n     def get_constants(self, X, train=False):\n         nb_samples = K.shape(X)[0]\n-        if K._BACKEND == 'tensorflow':\n-            nb_samples = int(nb_samples)\n+        if K._BACKEND == 'tensorflow' and train and self.p_W > 0 and self.p_U > 0:\n+            if not self.input_shape[0]:\n+                raise Exception('For RNN dropout in tensorflow, a complete ' +\n+                                'input_shape must be provided (including batch size).')\n+            nb_samples = self.input_shape[0]\n         retain_p_W = 1. - self.p_W\n         retain_p_U = 1. - self.p_U\n         if train and self.p_W > 0 and self.p_U > 0:\n\n@@ -18,11 +18,27 @@ def _runner(layer_class):\n     All the recurrent layers share the same interface,\n     so we can run through them with a single function.\n     \"\"\"\n-    for p in [0., 0.5]:\n     for ret_seq in [True, False]:\n         layer = layer_class(output_dim, return_sequences=ret_seq,\n-                                weights=None, input_shape=(timesteps, embedding_dim),\n-                                p_W=p, p_U=p)\n+                            weights=None, input_shape=(timesteps, embedding_dim))\n+        layer.input = K.variable(np.ones((nb_samples, timesteps, embedding_dim)))\n+        layer.get_config()\n+\n+        for train in [True, False]:\n+            out = K.eval(layer.get_output(train))\n+            # Make sure the output has the desired shape\n+            if ret_seq:\n+                assert(out.shape == (nb_samples, timesteps, output_dim))\n+            else:\n+                assert(out.shape == (nb_samples, output_dim))\n+\n+            mask = layer.get_output_mask(train)\n+\n+    # check dropout\n+    for ret_seq in [True, False]:\n+        layer = layer_class(output_dim, return_sequences=ret_seq, weights=None, \n+                            batch_input_shape=(nb_samples, timesteps, embedding_dim),\n+                            p_W=0.5, p_U=0.5)\n         layer.input = K.variable(np.ones((nb_samples, timesteps, embedding_dim)))\n         layer.get_config()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bc60832dcff74d8493f1e97fcb0d344f4fefd54f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 8 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 4431 | Contributors (this commit): 29 | Commits (past 90d): 42 | Contributors (cumulative): 33 | DMM Complexity: 0.0\n\nDIFF:\n@@ -536,12 +536,13 @@ class ModelTest(Callback):\n     def on_epoch_begin(self, epoch, logs={}):\n         if epoch % self.test_every_X_epochs != 0:\n             return\n-        model_output = self.model.predict(self.Xt, \n-            batch_size=self.batch_size, verbose=self.verbose)\n+        model_output = self.model.predict(self.Xt, batch_size=self.batch_size, \n+                                          verbose=self.verbose)\n         MC_model_output = []\n         for _ in xrange(self.T):\n             MC_model_output += [self.model.predict_stochastic(self.Xt, \n-                batch_size=self.batch_size, verbose=self.verbose)]\n+                                                              batch_size=self.batch_size, \n+                                                              verbose=self.verbose)]\n         MC_model_output = np.array(MC_model_output)\n         MC_model_output_mean = np.mean(MC_model_output, 0)\n \n\n@@ -234,13 +234,13 @@ class SimpleRNN(Recurrent):\n         self.U = self.inner_init((self.output_dim, self.output_dim))\n         self.b = K.zeros((self.output_dim,))\n \n-        self.regularizers = []\n         def appendRegulariser(input_regulariser, param, regularizers_list):\n             regulariser = regularizers.get(input_regulariser)\n             if regulariser:\n                 regulariser.set_param(param)\n                 regularizers_list.append(regulariser)\n \n+        self.regularizers = []\n         appendRegulariser(self.W_regularizer, self.W, self.regularizers)\n         appendRegulariser(self.U_regularizer, self.U, self.regularizers)\n         appendRegulariser(self.b_regularizer, self.b, self.regularizers)\n@@ -365,13 +365,13 @@ class GRU(Recurrent):\n         self.U_h = self.inner_init((self.output_dim, self.output_dim))\n         self.b_h = K.zeros((self.output_dim,))\n \n-        self.regularizers = []\n         def appendRegulariser(input_regulariser, param, regularizers_list):\n             regulariser = regularizers.get(input_regulariser)\n             if regulariser:\n                 regulariser.set_param(param)\n                 regularizers_list.append(regulariser)\n \n+        self.regularizers = []\n         for W in [self.W_z, self.W_r, self.W_h]:\n             appendRegulariser(self.W_regularizer, W, self.regularizers)\n         for U in [self.U_z, self.U_r, self.U_h]:\n@@ -533,13 +533,13 @@ class LSTM(Recurrent):\n         self.U_o = self.inner_init((self.output_dim, self.output_dim))\n         self.b_o = K.zeros((self.output_dim,))\n \n-        self.regularizers = []\n         def appendRegulariser(input_regulariser, param, regularizers_list):\n             regulariser = regularizers.get(input_regulariser)\n             if regulariser:\n                 regulariser.set_param(param)\n                 regularizers_list.append(regulariser)\n \n+        self.regularizers = []\n         for W in [self.W_i, self.W_f, self.W_i, self.W_o]:\n             appendRegulariser(self.W_regularizer, W, self.regularizers)\n         for U in [self.U_i, self.U_f, self.U_i, self.U_o]:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bec270121498b68a00f6a3c3e43640efaa2de698", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 996 | Contributors (this commit): 8 | Commits (past 90d): 34 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -714,4 +714,3 @@ def random_binomial(shape, p=0.0, dtype=_FLOATX, seed=None):\n         seed = np.random.randint(10e6)\n     return tf.select(tf.random_uniform(shape, dtype=dtype, seed=seed) <= p, \n                      tf.ones(shape), tf.zeros(shape))\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#941c3f6ae8182b2a3b9fb75c052268254118e264", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 57 | Lines Deleted: 50 | Files Changed: 3 | Hunks: 38 | Methods Changed: 16 | Complexity Δ (Sum/Max): -9/0 | Churn Δ: 107 | Churn Cumulative: 7955 | Contributors (this commit): 50 | Commits (past 90d): 79 | Contributors (cumulative): 67 | DMM Complexity: None\n\nDIFF:\n@@ -43,7 +43,7 @@ class Embedding(Layer):\n           This argument is required if you are going to connect\n           `Flatten` then `Dense` layers upstream\n           (without it, the shape of the dense outputs cannot be computed).\n-      p: float between 0 and 1. Fraction of the embeddings to drop.\n+      dropout: float between 0 and 1. Fraction of the embeddings to drop.\n \n     # References\n         - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n@@ -55,13 +55,13 @@ class Embedding(Layer):\n                  W_regularizer=None, activity_regularizer=None,\n                  W_constraint=None,\n                  mask_zero=False,\n-                 weights=None, p=0., **kwargs):\n+                 weights=None, dropout=0., **kwargs):\n         self.input_dim = input_dim\n         self.output_dim = output_dim\n         self.init = initializations.get(init)\n         self.input_length = input_length\n         self.mask_zero = mask_zero\n-        self.p = p\n+        self.dropout = dropout\n \n         self.W_constraint = constraints.get(W_constraint)\n         self.constraints = [self.W_constraint]\n@@ -103,8 +103,8 @@ class Embedding(Layer):\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n-        retain_p = 1. - self.p\n-        if train and self.p > 0:\n+        retain_p = 1. - self.dropout\n+        if train and self.dropout > 0:\n             B = K.random_binomial((self.input_dim,), p=retain_p)\n         else:\n             B = K.ones((self.input_dim)) * retain_p\n@@ -121,6 +121,6 @@ class Embedding(Layer):\n                   \"activity_regularizer\": self.activity_regularizer.get_config() if self.activity_regularizer else None,\n                   \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n                   \"W_constraint\": self.W_constraint.get_config() if self.W_constraint else None,\n-                  \"p\": self.p}\n+                  \"dropout\": self.dropout}\n         base_config = super(Embedding, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n\n@@ -199,8 +199,8 @@ class SimpleRNN(Recurrent):\n             (eg. L1 or L2 regularization), applied to the recurrent weights matrices.\n         b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n             applied to the bias.\n-        p_W: float between 0 and 1. Fraction of the input units to drop for inputs.\n-        p_U: float between 0 and 1. Fraction of the input units to drop for recurrent connections.\n+        dropout_W: float between 0 and 1. Fraction of the input units to drop for input gates.\n+        dropout_U: float between 0 and 1. Fraction of the input units to drop for recurrent connections.\n \n     # References\n         - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n@@ -209,7 +209,7 @@ class SimpleRNN(Recurrent):\n                  init='glorot_uniform', inner_init='orthogonal',\n                  activation='sigmoid', \n                  W_regularizer=None, U_regularizer=None, b_regularizer=None, \n-                 p_W=0., p_U=0., **kwargs):\n+                 dropout_W=0., dropout_U=0., **kwargs):\n         self.output_dim = output_dim\n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n@@ -217,7 +217,7 @@ class SimpleRNN(Recurrent):\n         self.W_regularizer = W_regularizer\n         self.U_regularizer = U_regularizer\n         self.b_regularizer = b_regularizer\n-        self.p_W, self.p_U = p_W, p_U\n+        self.dropout_W, self.dropout_U = dropout_W, dropout_U\n         super(SimpleRNN, self).__init__(**kwargs)\n \n     def build(self):\n@@ -234,16 +234,16 @@ class SimpleRNN(Recurrent):\n         self.U = self.inner_init((self.output_dim, self.output_dim))\n         self.b = K.zeros((self.output_dim,))\n \n-        def appendRegulariser(input_regulariser, param, regularizers_list):\n+        def append_regulariser(input_regulariser, param, regularizers_list):\n             regulariser = regularizers.get(input_regulariser)\n             if regulariser:\n                 regulariser.set_param(param)\n                 regularizers_list.append(regulariser)\n \n         self.regularizers = []\n-        appendRegulariser(self.W_regularizer, self.W, self.regularizers)\n-        appendRegulariser(self.U_regularizer, self.U, self.regularizers)\n-        appendRegulariser(self.b_regularizer, self.b, self.regularizers)\n+        append_regulariser(self.W_regularizer, self.W, self.regularizers)\n+        append_regulariser(self.U_regularizer, self.U, self.regularizers)\n+        append_regulariser(self.b_regularizer, self.b, self.regularizers)\n \n         self.trainable_weights = [self.W, self.U, self.b]\n \n@@ -274,15 +274,15 @@ class SimpleRNN(Recurrent):\n         return output, [output]\n \n     def get_constants(self, X, train=False):\n+        retain_p_W = 1. - self.dropout_W\n+        retain_p_U = 1. - self.dropout_U\n+        if train and self.dropout_W > 0 and self.dropout_U > 0:\n             nb_samples = K.shape(X)[0]\n-        if K._BACKEND == 'tensorflow' and train and self.p_W > 0 and self.p_U > 0:\n+            if K._BACKEND == 'tensorflow':\n                 if not self.input_shape[0]:\n                     raise Exception('For RNN dropout in tensorflow, a complete ' +\n                                     'input_shape must be provided (including batch size).')\n                 nb_samples = self.input_shape[0]\n-        retain_p_W = 1. - self.p_W\n-        retain_p_U = 1. - self.p_U\n-        if train and self.p_W > 0 and self.p_U > 0:\n             B_W = K.random_binomial((nb_samples, self.input_dim), p=retain_p_W)\n             B_U = K.random_binomial((nb_samples, self.output_dim), p=retain_p_U)\n         else:\n@@ -298,8 +298,8 @@ class SimpleRNN(Recurrent):\n                   \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n                   \"U_regularizer\": self.U_regularizer.get_config() if self.U_regularizer else None,\n                   \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n-                  \"p_W\": self.p_W, \n-                  \"p_U\": self.p_U}\n+                  \"dropout_W\": self.dropout_W, \n+                  \"dropout_U\": self.dropout_U}\n         base_config = super(SimpleRNN, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -323,8 +323,8 @@ class GRU(Recurrent):\n             (eg. L1 or L2 regularization), applied to the recurrent weights matrices.\n         b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n             applied to the bias.\n-        p_W: float between 0 and 1. Fraction of the input units to drop for inputs.\n-        p_U: float between 0 and 1. Fraction of the input units to drop for recurrent connections.\n+        dropout_W: float between 0 and 1. Fraction of the input units to drop for input gates.\n+        dropout_U: float between 0 and 1. Fraction of the input units to drop for recurrent connections.\n \n     # References\n         - [On the Properties of Neural Machine Translation: Encoder–Decoder Approaches](http://www.aclweb.org/anthology/W14-4012)\n@@ -335,7 +335,7 @@ class GRU(Recurrent):\n                  init='glorot_uniform', inner_init='orthogonal',\n                  activation='sigmoid', inner_activation='hard_sigmoid',\n                  W_regularizer=None, U_regularizer=None, b_regularizer=None, \n-                 p_W=0., p_U=0., **kwargs):\n+                 dropout_W=0., dropout_U=0., **kwargs):\n         self.output_dim = output_dim\n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n@@ -344,7 +344,7 @@ class GRU(Recurrent):\n         self.W_regularizer = W_regularizer\n         self.U_regularizer = U_regularizer\n         self.b_regularizer = b_regularizer\n-        self.p_W, self.p_U = p_W, p_U\n+        self.dropout_W, self.dropout_U = dropout_W, dropout_U\n         super(GRU, self).__init__(**kwargs)\n \n     def build(self):\n@@ -365,7 +365,7 @@ class GRU(Recurrent):\n         self.U_h = self.inner_init((self.output_dim, self.output_dim))\n         self.b_h = K.zeros((self.output_dim,))\n \n-        def appendRegulariser(input_regulariser, param, regularizers_list):\n+        def append_regulariser(input_regulariser, param, regularizers_list):\n             regulariser = regularizers.get(input_regulariser)\n             if regulariser:\n                 regulariser.set_param(param)\n@@ -373,11 +373,11 @@ class GRU(Recurrent):\n \n         self.regularizers = []\n         for W in [self.W_z, self.W_r, self.W_h]:\n-            appendRegulariser(self.W_regularizer, W, self.regularizers)\n+            append_regulariser(self.W_regularizer, W, self.regularizers)\n         for U in [self.U_z, self.U_r, self.U_h]:\n-            appendRegulariser(self.U_regularizer, U, self.regularizers)\n+            append_regulariser(self.U_regularizer, U, self.regularizers)\n         for b in [self.b_z, self.b_r, self.b_h]:\n-            appendRegulariser(self.b_regularizer, b, self.regularizers)\n+            append_regulariser(self.b_regularizer, b, self.regularizers)\n \n         self.trainable_weights = [self.W_z, self.U_z, self.b_z,\n                                   self.W_r, self.U_r, self.b_r,\n@@ -422,15 +422,15 @@ class GRU(Recurrent):\n         return h, [h]\n \n     def get_constants(self, X, train=False):\n+        retain_p_W = 1. - self.dropout_W\n+        retain_p_U = 1. - self.dropout_U\n+        if train and self.dropout_W > 0 and self.dropout_U > 0:\n             nb_samples = K.shape(X)[0]\n-        if K._BACKEND == 'tensorflow' and train and self.p_W > 0 and self.p_U > 0:\n+            if K._BACKEND == 'tensorflow':\n                 if not self.input_shape[0]:\n                     raise Exception('For RNN dropout in tensorflow, a complete ' +\n                                     'input_shape must be provided (including batch size).')\n                 nb_samples = self.input_shape[0]\n-        retain_p_W = 1. - self.p_W\n-        retain_p_U = 1. - self.p_U\n-        if train and self.p_W > 0 and self.p_U > 0:\n             B_W = K.random_binomial((3, nb_samples, self.input_dim), p=retain_p_W)\n             B_U = K.random_binomial((3, nb_samples, self.output_dim), p=retain_p_U)\n         else:\n@@ -447,8 +447,8 @@ class GRU(Recurrent):\n                   \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n                   \"U_regularizer\": self.U_regularizer.get_config() if self.U_regularizer else None,\n                   \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n-                  \"p_W\": self.p_W, \n-                  \"p_U\": self.p_U}\n+                  \"dropout_W\": self.dropout_W, \n+                  \"dropout_U\": self.dropout_U}\n         base_config = super(GRU, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -478,8 +478,8 @@ class LSTM(Recurrent):\n             (eg. L1 or L2 regularization), applied to the recurrent weights matrices.\n         b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n             applied to the bias.\n-        p_W: float between 0 and 1. Fraction of the input units to drop for inputs.\n-        p_U: float between 0 and 1. Fraction of the input units to drop for recurrent connections.\n+        dropout_W: float between 0 and 1. Fraction of the input units to drop for input gates.\n+        dropout_U: float between 0 and 1. Fraction of the input units to drop for recurrent connections.\n \n     # References\n         - [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) (original 1997 paper)\n@@ -492,7 +492,7 @@ class LSTM(Recurrent):\n                  forget_bias_init='one', activation='tanh',\n                  inner_activation='hard_sigmoid', \n                  W_regularizer=None, U_regularizer=None, b_regularizer=None, \n-                 p_W=0., p_U=0., **kwargs):\n+                 dropout_W=0., dropout_U=0., **kwargs):\n         self.output_dim = output_dim\n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n@@ -502,7 +502,7 @@ class LSTM(Recurrent):\n         self.W_regularizer = W_regularizer\n         self.U_regularizer = U_regularizer\n         self.b_regularizer = b_regularizer\n-        self.p_W, self.p_U = p_W, p_U\n+        self.dropout_W, self.dropout_U = dropout_W, dropout_U\n         super(LSTM, self).__init__(**kwargs)\n \n     def build(self):\n@@ -533,7 +533,7 @@ class LSTM(Recurrent):\n         self.U_o = self.inner_init((self.output_dim, self.output_dim))\n         self.b_o = K.zeros((self.output_dim,))\n \n-        def appendRegulariser(input_regulariser, param, regularizers_list):\n+        def append_regulariser(input_regulariser, param, regularizers_list):\n             regulariser = regularizers.get(input_regulariser)\n             if regulariser:\n                 regulariser.set_param(param)\n@@ -541,11 +541,11 @@ class LSTM(Recurrent):\n \n         self.regularizers = []\n         for W in [self.W_i, self.W_f, self.W_i, self.W_o]:\n-            appendRegulariser(self.W_regularizer, W, self.regularizers)\n+            append_regulariser(self.W_regularizer, W, self.regularizers)\n         for U in [self.U_i, self.U_f, self.U_i, self.U_o]:\n-            appendRegulariser(self.U_regularizer, U, self.regularizers)\n+            append_regulariser(self.U_regularizer, U, self.regularizers)\n         for b in [self.b_i, self.b_f, self.b_i, self.b_o]:\n-            appendRegulariser(self.b_regularizer, b, self.regularizers)\n+            append_regulariser(self.b_regularizer, b, self.regularizers)\n \n         self.trainable_weights = [self.W_i, self.U_i, self.b_i,\n                                   self.W_c, self.U_c, self.b_c,\n@@ -591,15 +591,15 @@ class LSTM(Recurrent):\n         return h, [h, c]\n \n     def get_constants(self, X, train=False):\n+        retain_p_W = 1. - self.dropout_W\n+        retain_p_U = 1. - self.dropout_U\n+        if train and self.dropout_W > 0 and self.dropout_U > 0:\n             nb_samples = K.shape(X)[0]\n-        if K._BACKEND == 'tensorflow' and train and self.p_W > 0 and self.p_U > 0:\n+            if K._BACKEND == 'tensorflow':\n                 if not self.input_shape[0]:\n                     raise Exception('For RNN dropout in tensorflow, a complete ' +\n                                     'input_shape must be provided (including batch size).')\n                 nb_samples = self.input_shape[0]\n-        retain_p_W = 1. - self.p_W\n-        retain_p_U = 1. - self.p_U\n-        if train and self.p_W > 0 and self.p_U > 0:\n             B_W = K.random_binomial((4, nb_samples, self.input_dim), p=retain_p_W)\n             B_U = K.random_binomial((4, nb_samples, self.output_dim), p=retain_p_U)\n         else:\n@@ -617,7 +617,7 @@ class LSTM(Recurrent):\n                   \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n                   \"U_regularizer\": self.U_regularizer.get_config() if self.U_regularizer else None,\n                   \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n-                  \"p_W\": self.p_W, \n-                  \"p_U\": self.p_U}\n+                  \"dropout_W\": self.dropout_W, \n+                  \"dropout_U\": self.dropout_U}\n         base_config = super(LSTM, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n\n@@ -662,8 +662,11 @@ class Sequential(Model, containers.Sequential):\n         return self._predict_loop(self._predict, X, batch_size, verbose)[0]\n \n     def predict_stochastic(self, X, batch_size=128, verbose=0):\n-        '''Generate stochastic output predictions for the input samples\n-        batch by batch.\n+        '''Generate output predictions for the input samples\n+        batch by batch, using stochastic forward passes. If \n+        dropout is used at training, during prediction network \n+        units will be dropped at random as well. This procedure\n+        can be used for MC dropout (see [ModelTest callback](callbacks.md)).\n \n         # Arguments\n             X: the input data, as a numpy array.\n@@ -672,6 +675,10 @@ class Sequential(Model, containers.Sequential):\n \n         # Returns\n             A numpy array of predictions.\n+    \n+        # References\n+            - [Dropout: A simple way to prevent neural networks from overfitting](http://jmlr.org/papers/v15/srivastava14a.html)\n+            - [Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning](http://arxiv.org/abs/1506.02142)\n         '''\n         X = standardize_X(X)\n         return self._predict_loop(self._predict_stochastic, X, batch_size, verbose)[0]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5406bd3ad22028ffb555f3c1e864b7daffa2b60f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 241 | Contributors (this commit): 5 | Commits (past 90d): 15 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -38,7 +38,7 @@ def _runner(layer_class):\n     for ret_seq in [True, False]:\n         layer = layer_class(output_dim, return_sequences=ret_seq, weights=None, \n                             batch_input_shape=(nb_samples, timesteps, embedding_dim),\n-                            p_W=0.5, p_U=0.5)\n+                            dropout_W=0.5, dropout_U=0.5)\n         layer.input = K.variable(np.ones((nb_samples, timesteps, embedding_dim)))\n         layer.get_config()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a2fdc323818af677f88870890669f41ded0c1827", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 21 | Files Changed: 1 | Hunks: 11 | Methods Changed: 2 | Complexity Δ (Sum/Max): 10/10 | Churn Δ: 54 | Churn Cumulative: 3156 | Contributors (this commit): 19 | Commits (past 90d): 30 | Contributors (cumulative): 19 | DMM Complexity: 1.0\n\nDIFF:\n@@ -267,6 +267,10 @@ class SimpleRNN(Recurrent):\n         # states only contains the previous output.\n         assert len(states) == 3 # 1 state and 2 constants\n         prev_output = states[0]\n+        if self.dropout_W == 0 and self.dropout_U == 0: \n+            # this uses less memory when dropout is disabled\n+            B_W, B_U = 1, 1\n+        else:\n             B_W = states[1]\n             B_U = states[2]\n         h = K.dot(x * B_W, self.W) + self.b\n@@ -276,7 +280,7 @@ class SimpleRNN(Recurrent):\n     def get_constants(self, X, train=False):\n         retain_p_W = 1. - self.dropout_W\n         retain_p_U = 1. - self.dropout_U\n-        if train and self.dropout_W > 0 and self.dropout_U > 0:\n+        if train and (self.dropout_W > 0 or self.dropout_U > 0):\n             nb_samples = K.shape(X)[0]\n             if K._BACKEND == 'tensorflow':\n                 if not self.input_shape[0]:\n@@ -407,24 +411,28 @@ class GRU(Recurrent):\n     def step(self, x, states):\n         assert len(states) == 3 # 1 state and 2 constants\n         h_tm1 = states[0]\n-        B_W = states[1]\n-        B_U = states[2]\n+        if self.dropout_W == 0 and self.dropout_U == 0: \n+            # this uses less memory when dropout is disabled\n+            B_W, B_U = [1] * 3, [1] * 3\n+        else:\n+            B_W = [K.gather(states[1], i) for i in range(3)]\n+            B_U = [K.gather(states[2], i) for i in range(3)]\n \n-        x_z = K.dot(x * K.gather(B_W, 0), self.W_z) + self.b_z\n-        x_r = K.dot(x * K.gather(B_W, 1), self.W_r) + self.b_r\n-        x_h = K.dot(x * K.gather(B_W, 2), self.W_h) + self.b_h\n+        x_z = K.dot(x * B_W[0], self.W_z) + self.b_z\n+        x_r = K.dot(x * B_W[1], self.W_r) + self.b_r\n+        x_h = K.dot(x * B_W[2], self.W_h) + self.b_h\n \n-        z = self.inner_activation(x_z + K.dot(h_tm1 * K.gather(B_U, 0), self.U_z))\n-        r = self.inner_activation(x_r + K.dot(h_tm1 * K.gather(B_U, 1), self.U_r))\n+        z = self.inner_activation(x_z + K.dot(h_tm1 * B_U[0], self.U_z))\n+        r = self.inner_activation(x_r + K.dot(h_tm1 * B_U[1], self.U_r))\n \n-        hh = self.activation(x_h + K.dot(r * h_tm1 * K.gather(B_U, 2), self.U_h))\n+        hh = self.activation(x_h + K.dot(r * h_tm1 * B_U[2], self.U_h))\n         h = z * h_tm1 + (1 - z) * hh\n         return h, [h]\n \n     def get_constants(self, X, train=False):\n         retain_p_W = 1. - self.dropout_W\n         retain_p_U = 1. - self.dropout_U\n-        if train and self.dropout_W > 0 and self.dropout_U > 0:\n+        if train and (self.dropout_W > 0 or self.dropout_U > 0):\n             nb_samples = K.shape(X)[0]\n             if K._BACKEND == 'tensorflow':\n                 if not self.input_shape[0]:\n@@ -575,25 +583,29 @@ class LSTM(Recurrent):\n         assert len(states) == 4 # 2 states and 2 constants\n         h_tm1 = states[0]\n         c_tm1 = states[1]\n-        B_W = states[2]\n-        B_U = states[3]\n+        if self.dropout_W == 0 and self.dropout_U == 0: \n+            # this uses less memory when dropout is disabled\n+            B_W, B_U = [1] * 4, [1] * 4\n+        else:\n+            B_W = [K.gather(states[2], i) for i in range(4)]\n+            B_U = [K.gather(states[3], i) for i in range(4)]\n \n-        x_i = K.dot(x * K.gather(B_W, 0), self.W_i) + self.b_i\n-        x_f = K.dot(x * K.gather(B_W, 1), self.W_f) + self.b_f\n-        x_c = K.dot(x * K.gather(B_W, 2), self.W_c) + self.b_c\n-        x_o = K.dot(x * K.gather(B_W, 3), self.W_o) + self.b_o\n+        x_i = K.dot(x * B_W[0], self.W_i) + self.b_i\n+        x_f = K.dot(x * B_W[1], self.W_f) + self.b_f\n+        x_c = K.dot(x * B_W[2], self.W_c) + self.b_c\n+        x_o = K.dot(x * B_W[3], self.W_o) + self.b_o\n \n-        i = self.inner_activation(x_i + K.dot(h_tm1 * K.gather(B_U, 0), self.U_i))\n-        f = self.inner_activation(x_f + K.dot(h_tm1 * K.gather(B_U, 1), self.U_f))\n-        c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1 * K.gather(B_U, 2), self.U_c))\n-        o = self.inner_activation(x_o + K.dot(h_tm1 * K.gather(B_U, 3), self.U_o))\n+        i = self.inner_activation(x_i + K.dot(h_tm1 * B_U[0], self.U_i))\n+        f = self.inner_activation(x_f + K.dot(h_tm1 * B_U[1], self.U_f))\n+        c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1 * B_U[2], self.U_c))\n+        o = self.inner_activation(x_o + K.dot(h_tm1 * B_U[3], self.U_o))\n         h = o * self.activation(c)\n         return h, [h, c]\n \n     def get_constants(self, X, train=False):\n         retain_p_W = 1. - self.dropout_W\n         retain_p_U = 1. - self.dropout_U\n-        if train and self.dropout_W > 0 and self.dropout_U > 0:\n+        if train and (self.dropout_W > 0 or self.dropout_U > 0):\n             nb_samples = K.shape(X)[0]\n             if K._BACKEND == 'tensorflow':\n                 if not self.input_shape[0]:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#860e4e9177cb637f5dd72a8d6d2b648387a40a6a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 56 | Lines Deleted: 54 | Files Changed: 2 | Hunks: 44 | Methods Changed: 14 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 110 | Churn Cumulative: 6091 | Contributors (this commit): 42 | Commits (past 90d): 65 | Contributors (cumulative): 46 | DMM Complexity: 1.0\n\nDIFF:\n@@ -218,8 +218,11 @@ def get_function_name(o):\n         return o.__name__\n \n \n-def mk_gen_queue(generator, max_q_size=10,\n+def generator_queue(generator, max_q_size=10,\n                     wait_time=0.05, nb_worker=1):\n+    '''Builds a threading queue out of a data generator.\n+    Used in `fit_generator`, `evaluate_generator`.\n+    '''\n     q = queue.Queue()\n     _stop = threading.Event()\n \n@@ -413,7 +416,6 @@ class Model(object):\n                 config['loss'] = dict([(k, get_function_name(v)) for k, v in self.loss.items()])\n             else:\n                 config['loss'] = get_function_name(self.loss)\n-                \n         if verbose:\n             pp = pprint.PrettyPrinter(indent=4)\n             pp.pprint(config)\n@@ -884,9 +886,14 @@ class Sequential(Model, containers.Sequential):\n             self.layers[k].set_weights(weights)\n         f.close()\n \n-    def verify_gen_output(self, generator_output, stop):\n+    def _check_generator_output(self, generator_output, stop):\n         '''Validates the output of a generator. On error, calls\n-        stop.set() with the passed Event object stop\n+        stop.set().\n+\n+        # Arguments\n+            generator_output: output of a data generator.\n+            stop: threading event to be called to\n+                interrupt training/evaluation.\n         '''\n         if not hasattr(generator_output, '__len__'):\n             stop.set()\n@@ -931,18 +938,18 @@ class Sequential(Model, containers.Sequential):\n             val_samples:\n                 total number of samples to generate from `generator`\n                 to use in validation.\n-\n-            Other argumens are as for `fit`.\n+            show_accuracy: whether to display accuracy in logs.\n+            verbose: verbosity mode, 0 (silent), 1 (per-batch logs),\n+                or 2 (per-epoch logs).\n         '''\n         done_samples = 0\n         all_outs = None\n         weights = []\n-        q, _stop = mk_gen_queue(generator, **kwargs)\n+        q, _stop = generator_queue(generator, **kwargs)\n \n         while done_samples < val_samples:\n-            X, y, sample_weight = self.verify_gen_output(q.get(), _stop)\n+            X, y, sample_weight = self._check_generator_output(q.get(), _stop)\n             do_samples = len(X[0])\n-            # TODO: verbosity/output control\n             outs = self.evaluate(X, y, batch_size=do_samples,\n                                  sample_weight=sample_weight,\n                                  show_accuracy=show_accuracy,\n@@ -970,7 +977,7 @@ class Sequential(Model, containers.Sequential):\n \n     def fit_generator(self, generator, samples_per_epoch, nb_epoch,\n                       verbose=1, show_accuracy=False, callbacks=[],\n-                      validation_data=None, validation_samples=None,\n+                      validation_data=None, nb_val_samples=None,\n                       class_weight=None,\n                       nb_worker=1, nb_val_worker=None):\n         '''Fit a model on data generated batch-by-batch by a Python generator.\n@@ -1005,9 +1012,9 @@ class Sequential(Model, containers.Sequential):\n                 (input_data, target_data, sample weights). If generator,\n                 it is assumed to yield tuples of 2 or 3 elements as above.\n                 The generator will be called at the end of every epoch until\n-                at least `validation_samples` examples have been obtained,\n+                at least `nb_val_samples` examples have been obtained,\n                 with these examples used for validation.\n-            validation_samples: number of samples to use from validation\n+            nb_val_samples: number of samples to use from validation\n                 generator at the end of every epoch. If `None` defaults\n                 to `samples_per_epoch/5`\n             class_weight: dictionary mapping class indices to a weight\n@@ -1025,7 +1032,6 @@ class Sequential(Model, containers.Sequential):\n                 `nb_worker`.\n \n         # Returns\n-\n             A `History` object.\n \n         # Examples\n@@ -1045,10 +1051,8 @@ class Sequential(Model, containers.Sequential):\n                                 samples_per_epoch=10000, nb_epoch=10)\n         ```\n         '''\n-\n         # TODO: make into kwargs?\n         max_data_q_size = 10  # maximum number of batches in queue\n-\n         wait_time = 0.05  # in seconds\n         epoch = 0\n \n@@ -1057,8 +1061,9 @@ class Sequential(Model, containers.Sequential):\n         # avoid any explicit version checks\n         val_gen = (hasattr(validation_data, 'next') or\n                    hasattr(validation_data, '__next__'))\n-        if validation_samples is None:\n-            validation_samples = samples_per_epoch/5\n+        if val_gen and not nb_val_samples:\n+            raise Exception('When using a generator for validation data, '\n+                            'you must specify a value for \"nb_val_samples\".')\n         if nb_val_worker is None:\n             nb_val_worker = nb_worker\n \n@@ -1086,12 +1091,10 @@ class Sequential(Model, containers.Sequential):\n         callbacks.on_train_begin()\n \n         # start generator thread storing batches into a queue\n-        data_gen_queue, _data_stop =\\\n-            mk_gen_queue(generator, max_q_size=max_data_q_size,\n+        data_gen_queue, _data_stop = generator_queue(generator, max_q_size=max_data_q_size,\n                                                      wait_time=wait_time)\n-\n         if do_validation and not val_gen:\n-            X_val, y_val, sample_weight_val = self.verify_gen_output(validation_data,\n+            X_val, y_val, sample_weight_val = self._check_generator_output(validation_data,\n                                                                            _data_stop)\n             self.validation_data = X_val + [y_val, sample_weight_val]\n         else:\n@@ -1111,9 +1114,8 @@ class Sequential(Model, containers.Sequential):\n                     else:\n                         time.sleep(wait_time)\n \n-                X, y, sample_weight = self.verify_gen_output(generator_output,\n+                X, y, sample_weight = self._check_generator_output(generator_output,\n                                                                    _data_stop)\n-\n                 batch_logs = {}\n                 batch_size = len(X[0])\n                 batch_logs['batch'] = batch_index\n@@ -1139,7 +1141,7 @@ class Sequential(Model, containers.Sequential):\n                 if samples_seen >= samples_per_epoch and do_validation:\n                     if val_gen:\n                         val_outs = self.evaluate_generator(validation_data,\n-                                                           validation_samples,\n+                                                           nb_val_samples,\n                                                            show_accuracy=show_accuracy,\n                                                            verbose=0)\n                     else:\n@@ -1423,18 +1425,18 @@ class Graph(Model, containers.Graph):\n         self.set_weights(weights)\n         f.close()\n \n-    def evaluate_generator(self, generator, val_samples,\n+    def evaluate_generator(self, generator, nb_val_samples,\n                            verbose=1, **kwargs):\n         '''Evaluates the model on a generator. The generator should\n         return the same kind of data with every yield as accepted\n-        by `evaluate`\n+        by `evaluate`.\n \n         Arguments:\n             generator:\n                 generator yielding dictionaries of the kind accepted\n                 by `evaluate`, or tuples of such dictionaries and\n                 associated dictionaries of sample weights.\n-            val_samples:\n+            nb_val_samples:\n                 total number of samples to generate from `generator`\n                 to use in validation.\n \n@@ -1443,10 +1445,10 @@ class Graph(Model, containers.Graph):\n         done_samples = 0\n         all_outs = []\n         weights = []\n-        q, _stop = mk_gen_queue(generator, **kwargs)\n+        q, _stop = generator_queue(generator, **kwargs)\n \n-        while done_samples < val_samples:\n-            data, sample_weight = self.verify_gen_output(q.get(), _stop)\n+        while done_samples < nb_val_samples:\n+            data, sample_weight = self._check_generator_output(q.get(), _stop)\n             do_samples = len(data[next(iter(data.keys()))])\n             outs = self.evaluate(data, batch_size=do_samples,\n                                  sample_weight=sample_weight,\n@@ -1460,10 +1462,10 @@ class Graph(Model, containers.Graph):\n         return np.average(np.asarray(all_outs),\n                           weights=weights)\n \n-    def verify_gen_output(self, generator_output, stop):\n+    def _check_generator_output(self, generator_output, stop):\n         '''Verifies the output of a generator to make sure\n         it is consistent with requirements. Also standardizes\n-        the output\n+        the output.\n         '''\n         if type(generator_output) in [list, tuple]:\n             if len(generator_output) == 2:\n@@ -1494,7 +1496,7 @@ class Graph(Model, containers.Graph):\n \n     def fit_generator(self, generator, samples_per_epoch, nb_epoch,\n                       verbose=1, callbacks=[],\n-                      validation_data=None, validation_samples=None,\n+                      validation_data=None, nb_val_samples=None,\n                       class_weight={},\n                       nb_worker=1, nb_val_worker=None):\n         '''Fit a model on data generated batch-by-batch by a Python generator.\n@@ -1522,10 +1524,10 @@ class Graph(Model, containers.Graph):\n                 held-out validation data, or a generator yielding such\n                 dictionaries. All arrays should contain the same number\n                 of samples. If a generator, will be called until more than\n-                `validation_samples` examples have been generated at the\n+                `nb_val_samples` examples have been generated at the\n                 end of every epoch. These examples will then be used\n                 as the validation data.\n-            validation_samples: number of samples to use from validation\n+            nb_val_samples: number of samples to use from validation\n                 generator at the end of every epoch. If `None` defaults\n                 to `samples_per_epoch/5`\n             class_weight: dictionary mapping class indices to a weight\n@@ -1543,7 +1545,6 @@ class Graph(Model, containers.Graph):\n \n \n         # Returns\n-\n             A `History` object.\n \n         # Examples\n@@ -1571,8 +1572,9 @@ class Graph(Model, containers.Graph):\n         do_validation = bool(validation_data)\n         val_gen = (hasattr(validation_data, 'next') or\n                    hasattr(validation_data, '__next__'))\n-        if validation_samples is None:\n-            validation_samples = samples_per_epoch/5\n+        if val_gen and not nb_val_samples:\n+            raise Exception('When using a generator for validation data, '\n+                            'you must specify a value for \"nb_val_samples\".')\n         if nb_val_worker is None:\n             nb_val_worker = nb_worker\n \n@@ -1599,13 +1601,11 @@ class Graph(Model, containers.Graph):\n         callbacks.on_train_begin()\n \n         # start generator thread storing batches into a queue\n-        data_gen_queue, _data_stop =\\\n-            mk_gen_queue(generator, max_q_size=max_data_q_size,\n+        data_gen_queue, _data_stop = generator_queue(generator, max_q_size=max_data_q_size,\n                                                      wait_time=wait_time)\n-\n         if do_validation and not val_gen:\n             # TODO: _data_stop not really sensical here\n-            data_val, sample_weight_val = self.verify_gen_output(validation_data, _data_stop)\n+            data_val, sample_weight_val = self._check_generator_output(validation_data, _data_stop)\n             sample_weight_val_l = [sample_weight_val[name] for name in self.output_order]\n             y_val = [standardize_y(data_val[name]) for name in self.output_order]\n             self.validation_data = [data_val[name] for name in self.input_order] + y_val + sample_weight_val_l\n@@ -1625,7 +1625,7 @@ class Graph(Model, containers.Graph):\n                     else:\n                         time.sleep(wait_time)\n \n-                data, sample_weight = self.verify_gen_output(generator_output,\n+                data, sample_weight = self._check_generator_output(generator_output,\n                                                                    _data_stop)\n                 batch_logs = {}\n                 batch_size = len(data[list(data.keys())[0]])\n@@ -1650,7 +1650,7 @@ class Graph(Model, containers.Graph):\n                 if samples_seen >= samples_per_epoch and do_validation:\n                     if val_gen:\n                         val_outs = self.evaluate_generator(validation_data,\n-                                                           validation_samples,\n+                                                           nb_val_samples,\n                                                            verbose=0)\n                     else:\n                         val_outs = self.evaluate(data_val,\n\n@@ -1,6 +1,7 @@\n from __future__ import absolute_import\n from __future__ import print_function\n import pytest\n+import os\n import numpy as np\n np.random.seed(1337)\n \n@@ -11,8 +12,6 @@ from keras.layers import containers\n from keras.utils import np_utils\n from keras.utils.test_utils import get_test_data\n \n-import os\n-\n \n input_dim = 32\n nb_hidden = 16\n@@ -69,8 +68,10 @@ def test_sequential_fit_generator():\n     model.fit_generator(data_generator(True), len(X_train), nb_epoch, show_accuracy=True)\n     model.fit_generator(data_generator(True), len(X_train), nb_epoch, show_accuracy=False, validation_data=(X_test, y_test))\n     model.fit_generator(data_generator(True), len(X_train), nb_epoch, show_accuracy=True, validation_data=(X_test, y_test))\n-    model.fit_generator(data_generator(True), len(X_train), nb_epoch, show_accuracy=False, validation_data=data_generator(False))\n-    model.fit_generator(data_generator(True), len(X_train), nb_epoch, show_accuracy=True, validation_data=data_generator(False))\n+    model.fit_generator(data_generator(True), len(X_train), nb_epoch, show_accuracy=False,\n+                        validation_data=data_generator(False), nb_val_samples=batch_size * 3)\n+    model.fit_generator(data_generator(True), len(X_train), nb_epoch, show_accuracy=True,\n+                        validation_data=data_generator(False), nb_val_samples=batch_size * 3)\n \n     loss = model.evaluate(X_train, y_train, verbose=0)\n     assert(loss < 0.9)\n@@ -642,8 +643,10 @@ def test_graph_fit_generator():\n     graph.fit_generator(data_generator_graph(True), 1000, nb_epoch=4)\n     graph.fit_generator(data_generator_graph(True), 1000, nb_epoch=4, validation_data={'input1': X_test_graph, 'output1': y_test_graph})\n     graph.fit_generator(data_generator_graph(True), 1000, nb_epoch=4, validation_data={'input1': X_test_graph, 'output1': y_test_graph})\n-    graph.fit_generator(data_generator_graph(True), 1000, nb_epoch=4, validation_data=data_generator_graph(False))\n-    graph.fit_generator(data_generator_graph(True), 1000, nb_epoch=4, validation_data=data_generator_graph(False))\n+    graph.fit_generator(data_generator_graph(True), 1000, nb_epoch=4,\n+                        validation_data=data_generator_graph(False), nb_val_samples=batch_size * 3)\n+    graph.fit_generator(data_generator_graph(True), 1000, nb_epoch=4,\n+                        validation_data=data_generator_graph(False), nb_val_samples=batch_size * 3)\n \n     gen_loss = graph.evaluate_generator(data_generator_graph(True), 128, verbose=0)\n     assert(gen_loss < 3.)\n@@ -987,7 +990,6 @@ def test_count_params():\n \n if __name__ == '__main__':\n     # pytest.main([__file__])\n-    # test_sequential()\n-    # test_sequential_fit_generator()\n-    # test_graph_fit_generator()\n-    pass\n+    test_sequential()\n+    test_sequential_fit_generator()\n+    test_graph_fit_generator()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
