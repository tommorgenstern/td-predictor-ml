{"custom_id": "keras#096b848e63072328f8812a45061a239f52ba303c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 78 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -20,7 +20,7 @@ _BACKEND = \"tensorflow\"\n def floatx():\n     \"\"\"Return the default float type, as a string.\n \n-    E.g. `'float16'`, `'float32'`, `'float64'`.\n+    E.g. `'bfloat16'`, `'float16'`, `'float32'`, `'float64'`.\n \n     Returns:\n         String, the current default float type.\n@@ -45,7 +45,7 @@ def set_floatx(value):\n     `keras.mixed_precision.set_dtype_policy('mixed_float16')`.\n \n     Args:\n-        value: String; `'float16'`, `'float32'`, or `'float64'`.\n+        value: String; `'bfloat16'`, `'float16'`, `'float32'`, or `'float64'`.\n \n     Examples:\n     >>> keras.config.floatx()\n@@ -62,7 +62,7 @@ def set_floatx(value):\n         ValueError: In case of invalid value.\n     \"\"\"\n     global _FLOATX\n-    accepted_dtypes = {\"float16\", \"float32\", \"float64\"}\n+    accepted_dtypes = {\"bfloat16\", \"float16\", \"float32\", \"float64\"}\n     if value not in accepted_dtypes:\n         raise ValueError(\n             f\"Unknown `floatx` value: {value}. \"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6e6bb0796ee5122fd34d21d7d6cb124a4a4dba84", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 9 | Files Changed: 3 | Hunks: 9 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 2682 | Contributors (this commit): 15 | Commits (past 90d): 7 | Contributors (cumulative): 26 | DMM Complexity: 1.0\n\nDIFF:\n@@ -146,7 +146,9 @@ class Dense(Layer):\n         output_shape[-1] = self.units\n         return tuple(output_shape)\n \n-    def enable_lora(self, rank):\n+    def enable_lora(\n+        self, rank, a_initializer=\"he_uniform\", b_initializer=\"zeros\"\n+    ):\n         if self.kernel_constraint:\n             raise ValueError(\n                 \"Lora is incompatible with kernel constraints. \"\n@@ -166,11 +168,11 @@ class Dense(Layer):\n         self.lora_kernel_a = self.add_weight(\n             name=\"lora_kernel_a\",\n             shape=(self.kernel.shape[0], rank),\n-            initializer=\"zeros\",\n+            initializer=initializers.get(a_initializer),\n             regularizer=self.kernel_regularizer,\n         )\n         self.lora_kernel_b = self.add_weight(\n-            name=\"lora_kernel_b\",\n+            name=initializers.get(b_initializer),\n             shape=(rank, self.kernel.shape[1]),\n             initializer=\"zeros\",\n             regularizer=self.kernel_regularizer,\n\n@@ -222,7 +222,9 @@ class EinsumDense(Layer):\n             x = self.activation(x)\n         return x\n \n-    def enable_lora(self, rank):\n+    def enable_lora(\n+        self, rank, a_initializer=\"he_uniform\", b_initializer=\"zeros\"\n+    ):\n         if self.kernel_constraint:\n             raise ValueError(\n                 \"Lora is incompatible with kernel constraints. \"\n@@ -243,13 +245,13 @@ class EinsumDense(Layer):\n         self.lora_kernel_a = self.add_weight(\n             name=\"lora_kernel_a\",\n             shape=(self.kernel.shape[:-1] + (rank,)),\n-            initializer=\"zeros\",\n+            initializer=initializers.get(a_initializer),\n             regularizer=self.kernel_regularizer,\n         )\n         self.lora_kernel_b = self.add_weight(\n             name=\"lora_kernel_b\",\n             shape=(rank, self.kernel.shape[-1]),\n-            initializer=\"zeros\",\n+            initializer=initializers.get(b_initializer),\n             regularizer=self.kernel_regularizer,\n         )\n         self.kernel.trainable = False\n\n@@ -125,7 +125,9 @@ class Embedding(Layer):\n     def compute_output_shape(self, input_shape):\n         return input_shape + (self.output_dim,)\n \n-    def enable_lora(self, rank):\n+    def enable_lora(\n+        self, rank, a_initializer=\"he_uniform\", b_initializer=\"zeros\"\n+    ):\n         if self.embeddings_constraint:\n             raise ValueError(\n                 \"Lora is incompatible with embedding constraints. \"\n@@ -145,13 +147,13 @@ class Embedding(Layer):\n         self.lora_embeddings_a = self.add_weight(\n             name=\"lora_embeddings_a\",\n             shape=(self.embeddings.shape[0], rank),\n-            initializer=\"zeros\",\n+            initializer=initializers.get(a_initializer),\n             regularizer=self.embeddings_regularizer,\n         )\n         self.lora_embeddings_b = self.add_weight(\n             name=\"lora_embeddings_b\",\n             shape=(rank, self.embeddings.shape[1]),\n-            initializer=\"zeros\",\n+            initializer=initializers.get(b_initializer),\n             regularizer=self.embeddings_regularizer,\n         )\n         self.embeddings.trainable = False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fba9920891db2096aed9872c61c9ea710fdcbc52", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 17 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,7 @@\n from keras.api_export import keras_export\n \n # Unique source of truth for the version number.\n-__version__ = \"3.0.3\"\n+__version__ = \"3.0.4\"\n \n \n @keras_export(\"keras.version\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2c4decfdd6513729fe20d6227c9e255d0d0dec66", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 13 | Files Changed: 4 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 33 | Churn Cumulative: 3378 | Contributors (this commit): 15 | Commits (past 90d): 13 | Contributors (cumulative): 31 | DMM Complexity: 1.0\n\nDIFF:\n@@ -172,9 +172,9 @@ class Dense(Layer):\n             regularizer=self.kernel_regularizer,\n         )\n         self.lora_kernel_b = self.add_weight(\n-            name=initializers.get(b_initializer),\n+            name=\"lora_kernel_b\",\n             shape=(rank, self.kernel.shape[1]),\n-            initializer=\"zeros\",\n+            initializer=initializers.get(b_initializer),\n             regularizer=self.kernel_regularizer,\n         )\n         self.kernel.trainable = False\n@@ -185,9 +185,7 @@ class Dense(Layer):\n         if not self.lora_enabled:\n             return super().save_own_variables(store)\n \n-        kernel_value = ops.convert_to_numpy(\n-            self.kernel + ops.matmul(self.lora_kernel_a, self.lora_kernel_b)\n-        )\n+        kernel_value = ops.convert_to_numpy(self.kernel)\n         store[\"0\"] = kernel_value\n         if self.use_bias:\n             store[\"1\"] = self.bias.numpy()\n\n@@ -262,9 +262,7 @@ class EinsumDense(Layer):\n         if not self.lora_enabled:\n             return super().save_own_variables(store)\n \n-        kernel_value = ops.convert_to_numpy(\n-            self.kernel + ops.matmul(self.lora_kernel_a, self.lora_kernel_b)\n-        )\n+        kernel_value = ops.convert_to_numpy(self.kernel)\n         store[\"0\"] = kernel_value\n         if self.bias is not None:\n             store[\"1\"] = self.bias.numpy()\n\n@@ -298,6 +298,9 @@ class EinsumDenseTest(testing.TestCase, parameterized.TestCase):\n         y = np.random.random((64, 8, 32))\n         _ = layer(x[:2])\n \n+        init_lora_a_kernel_value = layer.lora_kernel_a.numpy()\n+        init_lora_b_kernel_value = layer.lora_kernel_b.numpy()\n+\n         # Try calling fit()\n         model = models.Sequential(\n             [\n@@ -305,7 +308,18 @@ class EinsumDenseTest(testing.TestCase, parameterized.TestCase):\n             ]\n         )\n         model.compile(optimizer=\"sgd\", loss=\"mse\")\n-        model.fit(x, y)\n+        model.fit(x, y, epochs=2)\n+\n+        final_lora_a_kernel_value = layer.lora_kernel_a.numpy()\n+        final_lora_b_kernel_value = layer.lora_kernel_b.numpy()\n+        diff_a = np.max(\n+            np.abs(init_lora_a_kernel_value - final_lora_a_kernel_value)\n+        )\n+        diff_b = np.max(\n+            np.abs(init_lora_b_kernel_value - final_lora_b_kernel_value)\n+        )\n+        self.assertGreater(diff_a, 0.0)\n+        self.assertGreater(diff_b, 0.0)\n \n         # Try saving and reloading the model\n         temp_filepath = os.path.join(self.get_temp_dir(), \"lora_model.keras\")\n\n@@ -164,10 +164,7 @@ class Embedding(Layer):\n         if not self.lora_enabled:\n             return super().save_own_variables(store)\n \n-        embeddings_value = ops.convert_to_numpy(\n-            self.embeddings\n-            + ops.matmul(self.lora_embeddings_a, self.lora_embeddings_b)\n-        )\n+        embeddings_value = ops.convert_to_numpy(self.embeddings)\n         store[\"0\"] = embeddings_value\n \n     def load_own_variables(self, store):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#07500c3a2d8da5909caf2c01661a11c2cb380468", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 10 | Churn Cumulative: 6105 | Contributors (this commit): 9 | Commits (past 90d): 44 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -892,9 +892,7 @@ def min(x, axis=None, keepdims=False, initial=None):\n     if axis is None:\n         result = torch.min(x)\n     else:\n-        if isinstance(axis, list):\n-            axis = axis[-1]\n-        result = torch.min(x, dim=axis, keepdim=keepdims)\n+        result = amin(x, axis=axis, keepdims=keepdims)\n \n     if isinstance(getattr(result, \"values\", None), torch.Tensor):\n         result = result.values\n\n@@ -3429,6 +3429,12 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(knp.min(x), np.min(x))\n         self.assertAllClose(knp.Min()(x), np.min(x))\n \n+        self.assertAllClose(knp.min(x, axis=(0, 1)), np.min(x, (0, 1)))\n+        self.assertAllClose(knp.Min((0, 1))(x), np.min(x, (0, 1)))\n+\n+        self.assertAllClose(knp.min(x, axis=()), np.min(x, axis=()))\n+        self.assertAllClose(knp.Min(())(x), np.min(x, axis=()))\n+\n         self.assertAllClose(knp.min(x, 0), np.min(x, 0))\n         self.assertAllClose(knp.Min(0)(x), np.min(x, 0))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ea38ae3e7abd9c90f4580f82d2c0a7b03d5c757d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 28 | Churn Cumulative: 735 | Contributors (this commit): 7 | Commits (past 90d): 6 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -193,6 +193,9 @@ class DenseTest(testing.TestCase):\n         y = np.random.random((64, 16))\n         _ = layer(x[:2])\n \n+        init_lora_a_kernel_value = layer.lora_kernel_a.numpy()\n+        init_lora_b_kernel_value = layer.lora_kernel_b.numpy()\n+\n         # Try calling fit()\n         model = models.Sequential(\n             [\n@@ -202,6 +205,17 @@ class DenseTest(testing.TestCase):\n         model.compile(optimizer=\"sgd\", loss=\"mse\")\n         model.fit(x, y)\n \n+        final_lora_a_kernel_value = layer.lora_kernel_a.numpy()\n+        final_lora_b_kernel_value = layer.lora_kernel_b.numpy()\n+        diff_a = np.max(\n+            np.abs(init_lora_a_kernel_value - final_lora_a_kernel_value)\n+        )\n+        diff_b = np.max(\n+            np.abs(init_lora_b_kernel_value - final_lora_b_kernel_value)\n+        )\n+        self.assertGreater(diff_a, 0.0)\n+        self.assertGreater(diff_b, 0.0)\n+\n         # Try saving and reloading the model\n         temp_filepath = os.path.join(self.get_temp_dir(), \"lora_model.keras\")\n         model.save(temp_filepath)\n\n@@ -117,6 +117,9 @@ class EmbeddingTest(test_case.TestCase):\n         y = np.random.random((64, 3, 16))\n         _ = layer(x[:2])\n \n+        init_lora_a_embeddings_value = layer.lora_embeddings_a.numpy()\n+        init_lora_b_embeddings_value = layer.lora_embeddings_b.numpy()\n+\n         # Try calling fit()\n         model = models.Sequential(\n             [\n@@ -126,6 +129,17 @@ class EmbeddingTest(test_case.TestCase):\n         model.compile(optimizer=\"sgd\", loss=\"mse\")\n         model.fit(x, y)\n \n+        final_lora_a_embeddings_value = layer.lora_embeddings_a.numpy()\n+        final_lora_b_embeddings_value = layer.lora_embeddings_b.numpy()\n+        diff_a = np.max(\n+            np.abs(init_lora_a_embeddings_value - final_lora_a_embeddings_value)\n+        )\n+        diff_b = np.max(\n+            np.abs(init_lora_b_embeddings_value - final_lora_b_embeddings_value)\n+        )\n+        self.assertGreater(diff_a, 0.0)\n+        self.assertGreater(diff_b, 0.0)\n+\n         # Try saving and reloading the model\n         temp_filepath = os.path.join(self.get_temp_dir(), \"lora_model.keras\")\n         model.save(temp_filepath)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dad5342c44a6ecd69442a82b4a8f70a2428b35cf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 6 | Complexity Δ (Sum/Max): 6/3 | Churn Δ: 44 | Churn Cumulative: 779 | Contributors (this commit): 7 | Commits (past 90d): 8 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -263,3 +263,24 @@ class DenseTest(testing.TestCase):\n             expected_num_losses=2,  # we have 2 regularizers.\n             supports_masking=True,\n         )\n+\n+    def test_enable_lora_with_kernel_constraint(self):\n+        layer = layers.Dense(units=2, kernel_constraint=\"max_norm\")\n+        with self.assertRaisesRegex(\n+            ValueError, \"incompatible with kernel constraints\"\n+        ):\n+            layer.enable_lora(rank=2)\n+\n+    def test_enable_lora_on_unbuilt_layer(self):\n+        layer = layers.Dense(units=2)\n+        with self.assertRaisesRegex(\n+            ValueError, \"Cannot enable lora on a layer that isn't yet built\"\n+        ):\n+            layer.enable_lora(rank=2)\n+\n+    def test_enable_lora_when_already_enabled(self):\n+        layer = layers.Dense(units=2)\n+        layer.build((None, 2))\n+        layer.enable_lora(rank=2)\n+        with self.assertRaisesRegex(ValueError, \"lora is already enabled\"):\n+            layer.enable_lora(rank=2)\n\n@@ -183,3 +183,26 @@ class EmbeddingTest(test_case.TestCase):\n             expected_num_losses=0,\n             supports_masking=False,\n         )\n+\n+    def test_enable_lora_with_embeddings_constraint(self):\n+        layer = layers.Embedding(\n+            input_dim=10, output_dim=16, embeddings_constraint=\"max_norm\"\n+        )\n+        with self.assertRaisesRegex(\n+            ValueError, \"incompatible with embedding constraints\"\n+        ):\n+            layer.enable_lora(rank=2)\n+\n+    def test_enable_lora_on_unbuilt_layer(self):\n+        layer = layers.Embedding(input_dim=10, output_dim=16)\n+        with self.assertRaisesRegex(\n+            ValueError, \"Cannot enable lora on a layer that isn't yet built\"\n+        ):\n+            layer.enable_lora(rank=2)\n+\n+    def test_enable_lora_when_already_enabled(self):\n+        layer = layers.Embedding(input_dim=10, output_dim=16)\n+        layer.build()\n+        layer.enable_lora(rank=2)\n+        with self.assertRaisesRegex(ValueError, \"lora is already enabled\"):\n+            layer.enable_lora(rank=2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dfadf6af43d3fa6b49e6a402d773bc1a04b8768d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 247 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 27 | Complexity Δ (Sum/Max): 29/27 | Churn Δ: 249 | Churn Cumulative: 694 | Contributors (this commit): 5 | Commits (past 90d): 6 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -381,6 +381,10 @@ def extract_sequences(x, sequence_length, sequence_stride):\n \n \n class FFT(Operation):\n+    def __init__(self, axis=-1):\n+        super().__init__()\n+        self.axis = axis\n+\n     def compute_output_spec(self, x):\n         if not isinstance(x, (tuple, list)) or len(x) != 2:\n             raise ValueError(\n@@ -449,6 +453,10 @@ def fft(x):\n \n \n class FFT2(Operation):\n+    def __init__(self):\n+        super().__init__()\n+        self.axes = (-2, -1)\n+\n     def compute_output_spec(self, x):\n         if not isinstance(x, (tuple, list)) or len(x) != 2:\n             raise ValueError(\n@@ -473,8 +481,8 @@ class FFT2(Operation):\n             )\n \n         # The axes along which we are calculating FFT should be fully-defined.\n-        m = real.shape[-1]\n-        n = real.shape[-2]\n+        m = real.shape[self.axes[0]]\n+        n = real.shape[self.axes[1]]\n         if m is None or n is None:\n             raise ValueError(\n                 f\"Input should have its {self.axes} axes fully-defined. \"\n\n@@ -976,3 +976,240 @@ class ExtractSequencesOpTest(testing.TestCase):\n             (input_shape[1] - sequence_length) // sequence_stride\n         ) + 1\n         return (input_shape[0], num_sequences, sequence_length)\n+\n+\n+class SegmentSumTest(testing.TestCase):\n+    def test_segment_sum_call(self):\n+        data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=np.float32)\n+        segment_ids = np.array([0, 0, 1], dtype=np.int32)\n+        num_segments = 2\n+        sorted_segments = False\n+        segment_sum_op = kmath.SegmentSum(\n+            num_segments=num_segments, sorted=sorted_segments\n+        )\n+        output = segment_sum_op.call(data, segment_ids)\n+        expected_output = np.array([[5, 7, 9], [7, 8, 9]], dtype=np.float32)\n+        self.assertAllClose(output, expected_output)\n+\n+\n+class SegmentMaxTest(testing.TestCase):\n+    def test_segment_max_call(self):\n+        data = np.array([[1, 4, 7], [2, 5, 8], [3, 6, 9]], dtype=np.float32)\n+        segment_ids = np.array([0, 0, 1], dtype=np.int32)\n+        num_segments = 2\n+        sorted_segments = False\n+        segment_max_op = kmath.SegmentMax(\n+            num_segments=num_segments, sorted=sorted_segments\n+        )\n+        output = segment_max_op.call(data, segment_ids)\n+        expected_output = np.array([[2, 5, 8], [3, 6, 9]], dtype=np.float32)\n+        self.assertAllClose(output, expected_output)\n+\n+\n+class TopKTest(testing.TestCase):\n+    def test_top_k_call_values(self):\n+        data = np.array([[1, 3, 2], [4, 6, 5]], dtype=np.float32)\n+        k = 2\n+        sorted_flag = True\n+        top_k_op = kmath.TopK(k=k, sorted=sorted_flag)\n+        values, _ = top_k_op.call(data)\n+        expected_values = np.array([[3, 2], [6, 5]], dtype=np.float32)\n+        self.assertAllClose(values, expected_values)\n+\n+    def test_top_k_call_indices(self):\n+        data = np.array([[1, 3, 2], [4, 6, 5]], dtype=np.float32)\n+        k = 2\n+        sorted_flag = True\n+        top_k_op = kmath.TopK(k=k, sorted=sorted_flag)\n+        _, indices = top_k_op.call(data)\n+        expected_indices = np.array([[1, 2], [1, 2]], dtype=np.int32)\n+        self.assertAllClose(indices, expected_indices)\n+\n+\n+class InTopKTest(testing.TestCase):\n+    def test_in_top_k_call(self):\n+        targets = np.array([2, 0, 1], dtype=np.int32)\n+        predictions = np.array(\n+            [[0.1, 0.2, 0.7], [1.0, 0.2, 0.3], [0.2, 0.6, 0.2]],\n+            dtype=np.float32,\n+        )\n+        k = 2\n+        in_top_k_op = kmath.InTopK(k=k)\n+        output = in_top_k_op.call(targets, predictions)\n+        expected_output = np.array([True, True, True], dtype=bool)\n+        self.assertAllEqual(output, expected_output)\n+\n+\n+class LogsumexpTest(testing.TestCase):\n+    def test_logsumexp_call(self):\n+        x = np.array([[1, 2], [3, 4]], dtype=np.float32)\n+        axis = 0\n+        keepdims = True\n+        logsumexp_op = kmath.Logsumexp(axis=axis, keepdims=keepdims)\n+        output = logsumexp_op.call(x)\n+        expected_output = np.log(\n+            np.sum(np.exp(x), axis=axis, keepdims=keepdims)\n+        )\n+        self.assertAllClose(output, expected_output)\n+\n+\n+class FFTTest(testing.TestCase):\n+    def test_fft_input_not_tuple_or_list(self):\n+        fft_op = kmath.FFT()\n+        with self.assertRaisesRegex(\n+            ValueError, \"Input `x` should be a tuple of two tensors\"\n+        ):\n+            fft_op.compute_output_spec(np.array([1, 2, 3]))\n+\n+    def test_fft_input_parts_different_shapes(self):\n+        fft_op = kmath.FFT()\n+        real = np.array([1, 2, 3])\n+        imag = np.array([1, 2])\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Both the real and imaginary parts should have the same shape\",\n+        ):\n+            fft_op.compute_output_spec((real, imag))\n+\n+    def test_fft_input_not_1d(self):\n+        fft_op = kmath.FFT()\n+        real = np.array(1)\n+        imag = np.array(1)\n+        with self.assertRaisesRegex(ValueError, \"Input should have rank >= 1\"):\n+            fft_op.compute_output_spec((real, imag))\n+\n+    def test_fft_last_axis_not_fully_defined(self):\n+        fft_op = kmath.FFT()\n+        real = KerasTensor(shape=(None,), dtype=\"float32\")\n+        imag = KerasTensor(shape=(None,), dtype=\"float32\")\n+        with self.assertRaisesRegex(\n+            ValueError, \"Input should have its -1th axis fully-defined\"\n+        ):\n+            fft_op.compute_output_spec((real, imag))\n+\n+    def test_fft_init_default_axis(self):\n+        fft_op = kmath.FFT()\n+        self.assertEqual(fft_op.axis, -1, \"Default axis should be -1\")\n+\n+\n+class SolveTest(testing.TestCase):\n+    def test_solve_call(self):\n+        solve_op = kmath.Solve()\n+        a = np.array([[3, 2], [1, 2]], dtype=np.float32)\n+        b = np.array([[9, 8], [5, 4]], dtype=np.float32)\n+        output = solve_op.call(a, b)\n+        expected_output = np.linalg.solve(a, b)\n+        np.testing.assert_allclose(output, expected_output, atol=1e-6)\n+\n+\n+class FFT2Test(testing.TestCase):\n+    def test_fft2_correct_input(self):\n+        fft2_op = kmath.FFT2()\n+        real_part = np.random.rand(2, 3, 4)\n+        imag_part = np.random.rand(2, 3, 4)\n+        # This should not raise any errors\n+        fft2_op.compute_output_spec((real_part, imag_part))\n+\n+    def test_fft2_incorrect_input_type(self):\n+        fft2_op = kmath.FFT2()\n+        incorrect_input = np.array([1, 2, 3])  # Not a tuple or list\n+        with self.assertRaisesRegex(\n+            ValueError, \"should be a tuple of two tensors\"\n+        ):\n+            fft2_op.compute_output_spec(incorrect_input)\n+\n+    def test_fft2_mismatched_shapes(self):\n+        fft2_op = kmath.FFT2()\n+        real_part = np.random.rand(2, 3, 4)\n+        imag_part = np.random.rand(2, 3)  # Mismatched shape\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Both the real and imaginary parts should have the same shape\",\n+        ):\n+            fft2_op.compute_output_spec((real_part, imag_part))\n+\n+    def test_fft2_low_rank(self):\n+        fft2_op = kmath.FFT2()\n+        low_rank_input = np.random.rand(3)  # Rank of 1\n+        with self.assertRaisesRegex(ValueError, \"Input should have rank >= 2\"):\n+            fft2_op.compute_output_spec((low_rank_input, low_rank_input))\n+\n+    def test_fft2_undefined_dimensions(self):\n+        fft2_op = kmath.FFT2()\n+        real_part = KerasTensor(shape=(None, None, 3), dtype=\"float32\")\n+        imag_part = KerasTensor(shape=(None, None, 3), dtype=\"float32\")\n+        with self.assertRaisesRegex(\n+            ValueError, \"Input should have its .* axes fully-defined\"\n+        ):\n+            fft2_op.compute_output_spec((real_part, imag_part))\n+\n+\n+class RFFTTest(testing.TestCase):\n+    def test_rfft_low_rank_input(self):\n+        rfft_op = kmath.RFFT()\n+        low_rank_input = np.array(5)\n+        with self.assertRaisesRegex(ValueError, \"Input should have rank >= 1\"):\n+            rfft_op.compute_output_spec(low_rank_input)\n+\n+    def test_rfft_defined_fft_length(self):\n+        fft_length = 10\n+        rfft_op = kmath.RFFT(fft_length=fft_length)\n+        input_tensor = np.random.rand(3, 8)\n+\n+        expected_last_dimension = fft_length // 2 + 1\n+        expected_shape = input_tensor.shape[:-1] + (expected_last_dimension,)\n+\n+        output_tensors = rfft_op.compute_output_spec(input_tensor)\n+        for output_tensor in output_tensors:\n+            self.assertEqual(output_tensor.shape, expected_shape)\n+\n+        def test_rfft_undefined_fft_length_defined_last_dim(self):\n+            rfft_op = kmath.RFFT()\n+            input_tensor = np.random.rand(3, 8)\n+            expected_last_dimension = input_tensor.shape[-1] // 2 + 1\n+            expected_shape = input_tensor.shape[:-1] + (\n+                expected_last_dimension,\n+            )\n+            output_tensors = rfft_op.compute_output_spec(input_tensor)\n+            for output_tensor in output_tensors:\n+                self.assertEqual(output_tensor.shape, expected_shape)\n+\n+    def test_rfft_undefined_fft_length_undefined_last_dim(self):\n+        rfft_op = kmath.RFFT()\n+        input_tensor = KerasTensor(shape=(None, None), dtype=\"float32\")\n+        expected_shape = input_tensor.shape[:-1] + (None,)\n+        output_tensors = rfft_op.compute_output_spec(input_tensor)\n+        for output_tensor in output_tensors:\n+            self.assertEqual(output_tensor.shape, expected_shape)\n+\n+\n+class ISTFTTest(testing.TestCase):\n+    def test_istft_incorrect_input_type(self):\n+        istft_op = kmath.ISTFT(\n+            sequence_length=5, sequence_stride=2, fft_length=10\n+        )\n+        incorrect_input = np.array([1, 2, 3])\n+        with self.assertRaisesRegex(\n+            ValueError, \"should be a tuple of two tensors\"\n+        ):\n+            istft_op.compute_output_spec(incorrect_input)\n+\n+    def test_istft_mismatched_shapes(self):\n+        istft_op = kmath.ISTFT(\n+            sequence_length=5, sequence_stride=2, fft_length=10\n+        )\n+        real_part = np.random.rand(2, 3, 4)\n+        imag_part = np.random.rand(2, 3)\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Both the real and imaginary parts should have the same shape\",\n+        ):\n+            istft_op.compute_output_spec((real_part, imag_part))\n+\n+    def test_istft_low_rank_input(self):\n+        istft_op = kmath.ISTFT(\n+            sequence_length=5, sequence_stride=2, fft_length=10\n+        )\n+        low_rank_input = np.random.rand(3)\n+        with self.assertRaisesRegex(ValueError, \"Input should have rank >= 2\"):\n+            istft_op.compute_output_spec((low_rank_input, low_rank_input))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d9fa84d2cee5bf459efc1e70adc773141d8da267", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 8 | Files Changed: 5 | Hunks: 6 | Methods Changed: 7 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 14 | Churn Cumulative: 1432 | Contributors (this commit): 10 | Commits (past 90d): 5 | Contributors (cumulative): 32 | DMM Complexity: 0.0\n\nDIFF:\n@@ -91,7 +91,7 @@ class AveragePooling2D(BasePooling):\n         pool_size,\n         strides=None,\n         padding=\"valid\",\n-        data_format=\"channels_last\",\n+        data_format=None,\n         name=None,\n         **kwargs\n     ):\n\n@@ -67,7 +67,7 @@ class AveragePooling3D(BasePooling):\n         pool_size,\n         strides=None,\n         padding=\"valid\",\n-        data_format=\"channels_last\",\n+        data_format=None,\n         name=None,\n         **kwargs\n     ):\n\n@@ -1,4 +1,4 @@\n-from keras.backend import image_data_format\n+from keras import backend\n from keras.layers.input_spec import InputSpec\n from keras.layers.layer import Layer\n \n@@ -11,9 +11,7 @@ class BaseGlobalPooling(Layer):\n     ):\n         super().__init__(**kwargs)\n \n-        self.data_format = (\n-            image_data_format() if data_format is None else data_format\n-        )\n+        self.data_format = backend.standardize_data_format(data_format)\n         self.keepdims = keepdims\n         self.input_spec = InputSpec(ndim=pool_dimensions + 2)\n \n\n@@ -70,7 +70,7 @@ class UpSampling2D(Layer):\n         self, size=(2, 2), data_format=None, interpolation=\"nearest\", **kwargs\n     ):\n         super().__init__(**kwargs)\n-        self.data_format = backend.config.standardize_data_format(data_format)\n+        self.data_format = backend.standardize_data_format(data_format)\n         self.size = argument_validation.standardize_tuple(size, 2, \"size\")\n         self.interpolation = interpolation.lower()\n         self.input_spec = InputSpec(ndim=4)\n\n@@ -55,7 +55,7 @@ class UpSampling3D(Layer):\n \n     def __init__(self, size=(2, 2, 2), data_format=None, **kwargs):\n         super().__init__(**kwargs)\n-        self.data_format = backend.config.standardize_data_format(data_format)\n+        self.data_format = backend.standardize_data_format(data_format)\n         self.size = argument_validation.standardize_tuple(size, 3, \"size\")\n         self.input_spec = InputSpec(ndim=5)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9ff93f3e9cf66ca2dc95f7e11c4378e9583a2503", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 6116 | Contributors (this commit): 9 | Commits (past 90d): 45 | Contributors (cumulative): 14 | DMM Complexity: 0.625\n\nDIFF:\n@@ -1217,9 +1217,12 @@ def swapaxes(x, axis1, axis2):\n def take(x, indices, axis=None):\n     x = convert_to_tensor(x)\n     indices = convert_to_tensor(indices).long()\n-    if x.ndim == 2 and (axis is None or axis == 0):\n+    if x.ndim == 2 and axis == 0:\n         # This case is equivalent to embedding lookup.\n         return torch.nn.functional.embedding(indices, x)\n+    if axis is None:\n+        x = torch.reshape(x, (-1,))\n+        axis = 0\n     if axis is not None:\n         # make sure axis is non-negative\n         axis = len(x.shape) + axis if axis < 0 else axis\n\n@@ -2476,6 +2476,12 @@ class NumpyTwoInputOpsCorretnessTest(testing.TestCase, parameterized.TestCase):\n             knp.take(x, indices, axis=-2),\n             np.take(x, indices, axis=-2),\n         )\n+        # test with axis=None & x.ndim=2\n+        x = np.array(([1, 2], [3, 4]))\n+        indices = np.array([2, 3])\n+        self.assertAllClose(\n+            knp.take(x, indices, axis=None), np.take(x, indices, axis=None)\n+        )\n \n     @parameterized.named_parameters(\n         named_product(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f97e3c7963c3c82e6a8bcea49e20ad6063054ed7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 430 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -1099,7 +1099,7 @@ class SolveTest(testing.TestCase):\n         b = np.array([[9, 8], [5, 4]], dtype=np.float32)\n         output = solve_op.call(a, b)\n         expected_output = np.linalg.solve(a, b)\n-        np.testing.assert_allclose(output, expected_output, atol=1e-6)\n+        self.assertAllClose(output, expected_output, atol=1e-6)\n \n \n class FFT2Test(testing.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d9c69a024f3ffae0e73ac0d8f4be54fa36308c6a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 213 | Contributors (this commit): 6 | Commits (past 90d): 5 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -632,6 +632,9 @@ class JAXTrainer(base_trainer.Trainer):\n         non_trainable_variables = [\n             v.value for v in self.non_trainable_variables\n         ]\n+        self._purge_model_variables(\n+            optimizer_variables=False, metric_variables=False\n+        )\n         state = (trainable_variables, non_trainable_variables)\n         outputs = None\n         for step, x in epoch_iterator.enumerate_epoch():\n@@ -641,7 +644,16 @@ class JAXTrainer(base_trainer.Trainer):\n             callbacks.on_predict_batch_end(step, {\"outputs\": batch_outputs})\n             if self.stop_predicting:\n                 break\n+\n+        self._jax_state = {\n+            # I wouldn't recommend modifying non-trainable model state\n+            # during predict(), but it's allowed.\n+            \"trainable_variables\": state[0],\n+            \"non_trainable_variables\": state[1],\n+        }\n+        self.jax_state_sync()\n         callbacks.on_predict_end()\n+        self._jax_state = None\n         return tree.map_structure_up_to(batch_outputs, np.concatenate, outputs)\n \n     def train_on_batch(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#960fa94fbd90cbfc5a8b908aea406f3f8def0afc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 12 | Files Changed: 3 | Hunks: 14 | Methods Changed: 8 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 57 | Churn Cumulative: 631 | Contributors (this commit): 7 | Commits (past 90d): 12 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -181,17 +181,17 @@ class JAXTrainer(base_trainer.Trainer):\n             trainable_variables, non_trainable_variables, x, **kwargs\n         )\n         (\n-            trainable_variables,\n+            _,\n             non_trainable_variables,\n             _,\n             _,\n         ) = self._enforce_jax_state_sharding(\n-            trainable_variables=trainable_variables,\n+            trainable_variables=None,\n             non_trainable_variables=non_trainable_variables,\n             optimizer_variables=None,\n             metrics_variables=None,\n         )\n-        return outputs, (trainable_variables, non_trainable_variables)\n+        return outputs, non_trainable_variables\n \n     def make_train_function(self, force=False):\n         if self.train_function is not None and not force:\n@@ -266,10 +266,10 @@ class JAXTrainer(base_trainer.Trainer):\n             return self.predict_step(state, data)\n \n         def multi_predict_steps(state, data):\n-            outputs, state = one_predict_step(state, data[:1])\n+            outputs, trainable_variables = one_predict_step(state, data[:1])\n \n             for single_step_data in data[1:]:\n-                step_outputs, state = one_predict_step(\n+                step_outputs, trainable_variables = one_predict_step(\n                     state,\n                     [single_step_data],\n                 )\n@@ -278,7 +278,7 @@ class JAXTrainer(base_trainer.Trainer):\n                     outputs,\n                     step_outputs,\n                 )\n-            return outputs, state\n+            return outputs, trainable_variables\n \n         if self.steps_per_execution > 1:\n             predict_step = multi_predict_steps\n@@ -633,13 +633,13 @@ class JAXTrainer(base_trainer.Trainer):\n             v.value for v in self.non_trainable_variables\n         ]\n         self._purge_model_variables(\n-            optimizer_variables=False, metric_variables=False\n+            trainable_variables=False, optimizer_variables=False, metric_variables=False\n         )\n-        state = (trainable_variables, non_trainable_variables)\n         outputs = None\n         for step, x in epoch_iterator.enumerate_epoch():\n+            state = (trainable_variables, non_trainable_variables)\n             callbacks.on_predict_batch_begin(step)\n-            batch_outputs, state = self.predict_function(state, x)\n+            batch_outputs, non_trainable_variables = self.predict_function(state, x)\n             outputs = append_to_outputs(batch_outputs, outputs)\n             callbacks.on_predict_batch_end(step, {\"outputs\": batch_outputs})\n             if self.stop_predicting:\n@@ -648,8 +648,7 @@ class JAXTrainer(base_trainer.Trainer):\n         self._jax_state = {\n             # I wouldn't recommend modifying non-trainable model state\n             # during predict(), but it's allowed.\n-            \"trainable_variables\": state[0],\n-            \"non_trainable_variables\": state[1],\n+            \"non_trainable_variables\": non_trainable_variables,\n         }\n         self.jax_state_sync()\n         callbacks.on_predict_end()\n@@ -777,7 +776,11 @@ class JAXTrainer(base_trainer.Trainer):\n             v.value for v in self.non_trainable_variables\n         ]\n         state = (trainable_variables, non_trainable_variables)\n-        batch_outputs, state = self.predict_function(state, [(x,)])\n+        batch_outputs, non_trainable_variables = self.predict_function(state, [(x,)])\n+        self._jax_state = {\n+            \"non_trainable_variables\": non_trainable_variables,\n+        }\n+        self.jax_state_sync()\n         batch_outputs = tree.map_structure(lambda x: np.array(x), batch_outputs)\n         return batch_outputs\n \n\n@@ -0,0 +1,9 @@\n+from keras.random.random import categorical \n+from keras.random.random import dropout\n+from keras.random.random import gamma\n+from keras.random.random import normal\n+from keras.random.random import randint\n+from keras.random.random import shuffle\n+from keras.random.random import truncated_normal\n+from keras.random.random import uniform\n+from keras.random.seed_generator import SeedGenerator\n\n@@ -1153,3 +1153,24 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n         self.assertGreaterEqual(\n             np.min(backend.convert_to_numpy(model.layers[0].kernel)), 0.0\n         )\n+\n+    @pytest.mark.requires_trainable_backend\n+    def test_rng_updated_during_predict(self):\n+\n+        class TestTimeDropout(layers.Layer):\n+            def __init__(self):\n+                super().__init__()\n+                self.random_generator = keras.random.SeedGenerator()\n+\n+            def call(self, x):\n+                return keras.random.dropout(x, rate=0.5, seed=self.random_generator)\n+\n+        inputs = layers.Input((20,))\n+        outputs = TestTimeDropout()(inputs)\n+        model = keras.Model(inputs, outputs)\n+        model.compile(optimizer=\"rmsprop\", loss=\"mse\")\n+\n+        x = np.ones((32, 20))\n+        out_1 = model.predict(x)\n+        out_2 = model.predict(x)\n+        self.assertGreater(np.mean(np.abs(out_1 - out_2)), 0.01)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4955cb27b503bb346ac6f685d71bfa1686aa1e3a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 180 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,7 @@\n import keras  # isort: skip, keep it on top for torch test\n \n import numpy as np\n-from tensorflow import keras as tf_keras\n+import tf_keras\n \n NUM_CLASSES = 10\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b9db6e44c760207506648e66b6a5a1f79948e7f2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 9 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 33 | Churn Cumulative: 245 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -68,37 +68,48 @@ class PyDatasetAdapterTest(testing.TestCase, parameterized.TestCase):\n         named_product(\n             [\n                 {\n-                    \"testcase_name\": \"multi_on\",\n+                    \"testcase_name\": \"multiprocessing\",\n                     \"workers\": 2,\n                     \"use_multiprocessing\": True,\n                     \"max_queue_size\": 10,\n+                    \"dataset_type\": \"np\",\n                 },\n                 {\n-                    \"testcase_name\": \"multi_off\",\n+                    \"testcase_name\": \"multithreading\",\n                     \"workers\": 2,\n                     \"use_multiprocessing\": False,\n                     \"max_queue_size\": 10,\n+                    \"dataset_type\": \"np\",\n                 },\n                 {\n-                    \"testcase_name\": \"multi_off_zero\",\n-                    \"workers\": 0,\n-                    \"use_multiprocessing\": False,\n-                    \"max_queue_size\": 0,\n+                    \"testcase_name\": \"single_np\",\n+                    \"dataset_type\": \"np\",\n+                },\n+                {\n+                    \"testcase_name\": \"single_tf\",\n+                    \"dataset_type\": \"tf\",\n+                },\n+                {\n+                    \"testcase_name\": \"single_jax\",\n+                    \"dataset_type\": \"jax\",\n+                },\n+                {\n+                    \"testcase_name\": \"single_torch\",\n+                    \"dataset_type\": \"torch\",\n                 },\n             ],\n-            shuffle=[True, False],\n-            dataset_type=[\"np\", \"tf\", \"jax\", \"torch\"],\n             iterator_type=[\"np\", \"tf\", \"jax\", \"torch\"],\n+            shuffle=[True, False],\n         )\n     )\n     def test_basic_flow(\n         self,\n         shuffle,\n-        workers,\n-        use_multiprocessing,\n-        max_queue_size,\n         dataset_type,\n         iterator_type,\n+        workers=0,\n+        use_multiprocessing=False,\n+        max_queue_size=0,\n     ):\n         set_random_seed(1337)\n         x = np.random.random((64, 4)).astype(\"float32\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1d0008bc701aa94e0fdbad2603d2fed98ce37a87", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 639 | Contributors (this commit): 7 | Commits (past 90d): 13 | Contributors (cumulative): 11 | DMM Complexity: 0.5\n\nDIFF:\n@@ -633,13 +633,17 @@ class JAXTrainer(base_trainer.Trainer):\n             v.value for v in self.non_trainable_variables\n         ]\n         self._purge_model_variables(\n-            trainable_variables=False, optimizer_variables=False, metric_variables=False\n+            trainable_variables=False,\n+            optimizer_variables=False,\n+            metric_variables=False,\n         )\n         outputs = None\n         for step, x in epoch_iterator.enumerate_epoch():\n             state = (trainable_variables, non_trainable_variables)\n             callbacks.on_predict_batch_begin(step)\n-            batch_outputs, non_trainable_variables = self.predict_function(state, x)\n+            batch_outputs, non_trainable_variables = self.predict_function(\n+                state, x\n+            )\n             outputs = append_to_outputs(batch_outputs, outputs)\n             callbacks.on_predict_batch_end(step, {\"outputs\": batch_outputs})\n             if self.stop_predicting:\n@@ -776,7 +780,9 @@ class JAXTrainer(base_trainer.Trainer):\n             v.value for v in self.non_trainable_variables\n         ]\n         state = (trainable_variables, non_trainable_variables)\n-        batch_outputs, non_trainable_variables = self.predict_function(state, [(x,)])\n+        batch_outputs, non_trainable_variables = self.predict_function(\n+            state, [(x,)]\n+        )\n         self._jax_state = {\n             \"non_trainable_variables\": non_trainable_variables,\n         }\n\n@@ -1156,14 +1156,15 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n \n     @pytest.mark.requires_trainable_backend\n     def test_rng_updated_during_predict(self):\n-\n         class TestTimeDropout(layers.Layer):\n             def __init__(self):\n                 super().__init__()\n                 self.random_generator = keras.random.SeedGenerator()\n \n             def call(self, x):\n-                return keras.random.dropout(x, rate=0.5, seed=self.random_generator)\n+                return keras.random.dropout(\n+                    x, rate=0.5, seed=self.random_generator\n+                )\n \n         inputs = layers.Input((20,))\n         outputs = TestTimeDropout()(inputs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bf1f463e0e77c77ac1f9e16f77d5064e4ed4a29a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 41 | Files Changed: 9 | Hunks: 34 | Methods Changed: 23 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 82 | Churn Cumulative: 6667 | Contributors (this commit): 14 | Commits (past 90d): 103 | Contributors (cumulative): 41 | DMM Complexity: None\n\nDIFF:\n@@ -48,10 +48,10 @@ class KerasTensor:\n     def ndim(self):\n         return len(self.shape)\n \n-    def reshape(self, new_shape):\n+    def reshape(self, newshape):\n         from keras import ops\n \n-        return ops.Reshape(new_shape)(self)\n+        return ops.Reshape(newshape)(self)\n \n     def squeeze(self, axis=None):\n         from keras import ops\n\n@@ -647,8 +647,8 @@ def repeat(x, repeats, axis=None):\n     return jnp.repeat(x, repeats, axis=axis)\n \n \n-def reshape(x, new_shape):\n-    return jnp.reshape(x, new_shape)\n+def reshape(x, newshape):\n+    return jnp.reshape(x, newshape)\n \n \n def roll(x, shift, axis=None):\n\n@@ -770,8 +770,8 @@ def repeat(x, repeats, axis=None):\n     return np.repeat(x, repeats, axis=axis)\n \n \n-def reshape(x, new_shape):\n-    return np.reshape(x, new_shape)\n+def reshape(x, newshape):\n+    return np.reshape(x, newshape)\n \n \n def roll(x, shift, axis=None):\n\n@@ -1339,18 +1339,18 @@ def repeat(x, repeats, axis=None):\n     return tf.repeat(x, repeats, axis=axis)\n \n \n-def reshape(x, new_shape):\n+def reshape(x, newshape):\n     x = convert_to_tensor(x)\n     if isinstance(x, tf.SparseTensor):\n         from keras.ops.operation_utils import compute_reshape_output_shape\n \n         output_shape = compute_reshape_output_shape(\n-            x.shape, new_shape, \"new_shape\"\n+            x.shape, newshape, \"newshape\"\n         )\n-        output = tf.sparse.reshape(x, new_shape)\n+        output = tf.sparse.reshape(x, newshape)\n         output.set_shape(output_shape)\n         return output\n-    return tf.reshape(x, new_shape)\n+    return tf.reshape(x, newshape)\n \n \n def roll(x, shift, axis=None):\n\n@@ -1122,11 +1122,11 @@ def repeat(x, repeats, axis=None):\n     return torch.repeat_interleave(x, repeats, dim=axis)\n \n \n-def reshape(x, new_shape):\n-    if not isinstance(new_shape, (list, tuple)):\n-        new_shape = (new_shape,)\n+def reshape(x, newshape):\n+    if not isinstance(newshape, (list, tuple)):\n+        newshape = (newshape,)\n     x = convert_to_tensor(x)\n-    return torch.reshape(x, new_shape)\n+    return torch.reshape(x, newshape)\n \n \n def roll(x, shift, axis=None):\n\n@@ -165,7 +165,7 @@ def categorical_accuracy(y_true, y_pred):\n         y_pred = ops.cast(y_pred, dtype=y_true.dtype)\n     matches = ops.cast(ops.equal(y_true, y_pred), backend.floatx())\n     if reshape_matches:\n-        matches = ops.reshape(matches, new_shape=y_true_org_shape)\n+        matches = ops.reshape(matches, y_true_org_shape)\n     return matches\n \n \n@@ -251,7 +251,7 @@ def sparse_categorical_accuracy(y_true, y_pred):\n         y_pred = ops.cast(y_pred, y_true.dtype)\n     matches = ops.cast(ops.equal(y_true, y_pred), backend.floatx())\n     if reshape_matches:\n-        matches = ops.reshape(matches, new_shape=y_true_org_shape)\n+        matches = ops.reshape(matches, y_true_org_shape)\n     # if shape is (num_samples, 1) squeeze\n     if len(matches.shape) > 1 and matches.shape[-1] == 1:\n         matches = ops.squeeze(matches, -1)\n@@ -337,7 +337,7 @@ def top_k_categorical_accuracy(y_true, y_pred, k=5):\n \n     # returned matches is expected to have same shape as y_true input\n     if reshape_matches:\n-        matches = ops.reshape(matches, new_shape=y_true_org_shape)\n+        matches = ops.reshape(matches, y_true_org_shape)\n \n     return matches\n \n@@ -415,7 +415,7 @@ def sparse_top_k_categorical_accuracy(y_true, y_pred, k=5):\n \n     # returned matches is expected to have same shape as y_true input\n     if reshape_matches:\n-        matches = ops.reshape(matches, new_shape=y_true_org_shape)\n+        matches = ops.reshape(matches, y_true_org_shape)\n \n     return matches\n \n\n@@ -4463,28 +4463,28 @@ def repeat(x, repeats, axis=None):\n \n \n class Reshape(Operation):\n-    def __init__(self, new_shape):\n+    def __init__(self, newshape):\n         super().__init__()\n-        self.new_shape = new_shape\n+        self.newshape = newshape\n \n     def call(self, x):\n-        return backend.numpy.reshape(x, self.new_shape)\n+        return backend.numpy.reshape(x, self.newshape)\n \n     def compute_output_spec(self, x):\n         output_shape = operation_utils.compute_reshape_output_shape(\n-            x.shape, self.new_shape, \"new_shape\"\n+            x.shape, self.newshape, \"newshape\"\n         )\n         sparse = getattr(x, \"sparse\", False)\n         return KerasTensor(output_shape, dtype=x.dtype, sparse=sparse)\n \n \n @keras_export([\"keras.ops.reshape\", \"keras.ops.numpy.reshape\"])\n-def reshape(x, new_shape):\n+def reshape(x, newshape):\n     \"\"\"Gives a new shape to a tensor without changing its data.\n \n     Args:\n         x: Input tensor.\n-        new_shape: The new shape should be compatible with the original shape.\n+        newshape: The new shape should be compatible with the original shape.\n             One shape dimension can be -1 in which case the value is\n             inferred from the length of the array and remaining dimensions.\n \n@@ -4492,8 +4492,8 @@ def reshape(x, new_shape):\n         The reshaped tensor.\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n-        return Reshape(new_shape).symbolic_call(x)\n-    return backend.numpy.reshape(x, new_shape)\n+        return Reshape(newshape).symbolic_call(x)\n+    return backend.numpy.reshape(x, newshape)\n \n \n class Roll(Operation):\n\n@@ -272,36 +272,36 @@ def compute_matmul_output_shape(shape1, shape2):\n     return tuple(output_shape)\n \n \n-def compute_reshape_output_shape(input_shape, new_shape, new_shape_arg_name):\n-    \"\"\"Converts `-1` in `new_shape` to either an actual dimension or `None`.\n+def compute_reshape_output_shape(input_shape, newshape, newshape_arg_name):\n+    \"\"\"Converts `-1` in `newshape` to either an actual dimension or `None`.\n \n     This utility does not special case the 0th dimension (batch size).\n     \"\"\"\n-    unknown_dim_count = new_shape.count(-1)\n+    unknown_dim_count = newshape.count(-1)\n     if unknown_dim_count > 1:\n         raise ValueError(\n             \"There must be at most one unknown dimension (-1) in \"\n-            f\"{new_shape_arg_name}. Received: {new_shape_arg_name}={new_shape}.\"\n+            f\"{newshape_arg_name}. Received: {newshape_arg_name}={newshape}.\"\n         )\n \n     # If there is a None in input_shape, we can't infer what the -1 is\n     if None in input_shape:\n-        return tuple(dim if dim != -1 else None for dim in new_shape)\n+        return tuple(dim if dim != -1 else None for dim in newshape)\n \n     input_size = math.prod(input_shape)\n-    # If the new_shape fully defined, return it\n+    # If the `newshape` is fully defined, return it\n     if unknown_dim_count == 0:\n-        if input_size != math.prod(new_shape):\n+        if input_size != math.prod(newshape):\n             raise ValueError(\n                 \"The total size of the tensor must be unchanged. Received: \"\n-                f\"input_shape={input_shape}, {new_shape_arg_name}={new_shape}\"\n+                f\"input_shape={input_shape}, {newshape_arg_name}={newshape}\"\n             )\n-        return new_shape\n+        return newshape\n \n-    # We have one -1 in new_shape, compute the actual value\n+    # We have one -1 in `newshape`, compute the actual value\n     known_output_size = 1\n     unknown_dim_index = None\n-    for index, dim in enumerate(new_shape):\n+    for index, dim in enumerate(newshape):\n         if dim == -1:\n             unknown_dim_index = index\n         else:\n@@ -311,11 +311,11 @@ def compute_reshape_output_shape(input_shape, new_shape, new_shape_arg_name):\n         raise ValueError(\n             \"The total size of the tensor must be unchanged, however, the \"\n             \"input size cannot by divided by the specified dimensions in \"\n-            f\"{new_shape_arg_name}. Received: input_shape={input_shape}, \"\n-            f\"{new_shape_arg_name}={new_shape}\"\n+            f\"{newshape_arg_name}. Received: input_shape={input_shape}, \"\n+            f\"{newshape_arg_name}={newshape}\"\n         )\n \n-    output_shape = list(new_shape)\n+    output_shape = list(newshape)\n     output_shape[unknown_dim_index] = input_size // known_output_size\n     return tuple(output_shape)\n \n\n@@ -139,7 +139,7 @@ class OperationUtilsTest(testing.TestCase):\n         input_shape = (1, 4, 4, 1)\n         target_shape = (16, 1)\n         output_shape = operation_utils.compute_reshape_output_shape(\n-            input_shape, new_shape=target_shape, new_shape_arg_name=\"New shape\"\n+            input_shape, newshape=target_shape, newshape_arg_name=\"New shape\"\n         )\n         self.assertEqual(output_shape, target_shape)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
