{"custom_id": "keras#a1161885d5d7a4a37ba58c7df5e7073850c6d1ea", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 8 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 92 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -15,12 +15,14 @@ if sys.version_info < (3,):\n else:\n     maketrans = str.maketrans\n \n+\n def base_filter():\n     f = string.punctuation\n     f = f.replace(\"'\", '')\n     f += '\\t\\n'\n     return f\n \n+\n def text_to_word_sequence(text, filters=base_filter(), lower=True, split=\" \"):\n     '''prune: sequence of characters to filter out\n     '''\n@@ -69,13 +71,12 @@ class Tokenizer(object):\n         wcounts = list(self.word_counts.items())\n         wcounts.sort(key=lambda x: x[1], reverse=True)\n         sorted_voc = [wc[0] for wc in wcounts]\n-        self.word_index = dict(list(zip(sorted_voc, list(range(0, len(sorted_voc))))))\n+        self.word_index = dict(list(zip(sorted_voc, list(range(1, len(sorted_voc) + 1)))))\n \n         self.index_docs = {}\n         for w, c in list(self.word_docs.items()):\n             self.index_docs[self.word_index[w]] = c\n \n-\n     def fit_on_sequences(self, sequences):\n         '''\n             required before using sequences_to_matrix\n@@ -91,7 +92,6 @@ class Tokenizer(object):\n                 else:\n                     self.index_docs[i] += 1\n \n-\n     def texts_to_sequences(self, texts):\n         '''\n             Transform each text in texts in a sequence of integers.\n@@ -126,7 +126,6 @@ class Tokenizer(object):\n                         vect.append(i)\n             yield vect\n \n-\n     def texts_to_matrix(self, texts, mode=\"binary\"):\n         '''\n             modes: binary, count, tfidf, freq\n@@ -140,7 +139,7 @@ class Tokenizer(object):\n         '''\n         if not self.nb_words:\n             if self.word_index:\n-                nb_words = len(self.word_index)\n+                nb_words = len(self.word_index) + 1\n             else:\n                 raise Exception(\"Specify a dimension (nb_words argument), or fit on some text data first\")\n         else:\n@@ -175,11 +174,3 @@ class Tokenizer(object):\n                 else:\n                     raise Exception(\"Unknown vectorization mode: \" + str(mode))\n         return X\n-\n-\n-\n-                \n-\n-\n-\n-    \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#024a88f986b3a6896f4770d5e14668ba5923a1b4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 562 | Contributors (this commit): 9 | Commits (past 90d): 21 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -250,8 +250,8 @@ class UpSample2D(Layer):\n \n     def get_output(self, train):\n         X = self.get_input(train)\n-        Y = theano.tensor.extra_ops.repeat(X, self.upsample_size[0], axis = -2)\n-        output = theano.tensor.extra_ops.repeat(Y, self.upsample_size[1], axis = -1)\n+        Y = theano.tensor.extra_ops.repeat(X, self.upsample_size[0], axis = 2)\n+        output = theano.tensor.extra_ops.repeat(Y, self.upsample_size[1], axis = 3)\n         return output\n \n     def get_config(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#965a8cae03fbd4a6ac29b3af62140151e5aefb43", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 52 | Files Changed: 1 | Hunks: 6 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 105 | Churn Cumulative: 667 | Contributors (this commit): 9 | Commits (past 90d): 22 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -93,50 +93,6 @@ class Convolution1D(Layer):\n                 \"b_constraint\": self.b_constraint.get_config() if self.b_constraint else None}\n \n \n-class MaxPooling1D(Layer):\n-    def __init__(self, pool_length=2, stride=None, ignore_border=True):\n-        super(MaxPooling1D, self).__init__()\n-        self.pool_length = pool_length\n-        self.stride = stride\n-        if self.stride:\n-            self.st = (self.stride, 1)\n-        else:\n-            self.st = None\n-\n-        self.input = T.tensor3()\n-        self.poolsize = (pool_length, 1)\n-        self.ignore_border = ignore_border\n-\n-    def get_output(self, train):\n-        X = self.get_input(train)\n-        X = T.reshape(X, (X.shape[0], X.shape[1], X.shape[2], 1)).dimshuffle(0, 2, 1, 3)\n-        output = T.signal.downsample.max_pool_2d(X, ds=self.poolsize, st=self.st, ignore_border=self.ignore_border)\n-        output = output.dimshuffle(0, 2, 1, 3)\n-        return T.reshape(output, (output.shape[0], output.shape[1], output.shape[2]))\n-\n-    def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n-                \"stride\": self.stride,\n-                \"pool_length\": self.pool_length,\n-                \"ignore_border\": self.ignore_border,\n-                \"subsample_length\": self.subsample_length}\n-\n-class UpSample1D(Layer):\n-    def __init__(self, upsample_length=2):\n-        super(UpSample1D,self).__init__()\n-        self.upsample_length = upsample_length\n-        self.input = T.tensor3()\n-\n-    def get_output(self, train):\n-        X = self.get_input(train)\n-        output = theano.tensor.extra_ops.repeat(X, self.upsample_length, axis=1)\n-        return output\n-\n-    def get_config(self):\n-        return {\"name\":self.__class__.__name__,\n-                \"upsample_length\":self.upsample_length}\n-                \n-\n class Convolution2D(Layer):\n     def __init__(self, nb_filter, stack_size, nb_row, nb_col,\n                  init='glorot_uniform', activation='linear', weights=None,\n@@ -222,6 +178,35 @@ class Convolution2D(Layer):\n                 \"b_constraint\": self.b_constraint.get_config() if self.b_constraint else None}\n \n \n+class MaxPooling1D(Layer):\n+    def __init__(self, pool_length=2, stride=None, ignore_border=True):\n+        super(MaxPooling1D, self).__init__()\n+        self.pool_length = pool_length\n+        self.stride = stride\n+        if self.stride:\n+            self.st = (self.stride, 1)\n+        else:\n+            self.st = None\n+\n+        self.input = T.tensor3()\n+        self.poolsize = (pool_length, 1)\n+        self.ignore_border = ignore_border\n+\n+    def get_output(self, train):\n+        X = self.get_input(train)\n+        X = T.reshape(X, (X.shape[0], X.shape[1], X.shape[2], 1)).dimshuffle(0, 2, 1, 3)\n+        output = T.signal.downsample.max_pool_2d(X, ds=self.poolsize, st=self.st, ignore_border=self.ignore_border)\n+        output = output.dimshuffle(0, 2, 1, 3)\n+        return T.reshape(output, (output.shape[0], output.shape[1], output.shape[2]))\n+\n+    def get_config(self):\n+        return {\"name\": self.__class__.__name__,\n+                \"stride\": self.stride,\n+                \"pool_length\": self.pool_length,\n+                \"ignore_border\": self.ignore_border,\n+                \"subsample_length\": self.subsample_length}\n+\n+\n class MaxPooling2D(Layer):\n     def __init__(self, poolsize=(2, 2), stride=None, ignore_border=True):\n         super(MaxPooling2D, self).__init__()\n@@ -241,22 +226,38 @@ class MaxPooling2D(Layer):\n                 \"ignore_border\": self.ignore_border,\n                 \"stride\": self.stride}\n \n-class UpSample2D(Layer):\n-    def __init__(self, upsample_size=(2, 2)):\n-        super(UpSample2D,self).__init__()\n-        self.input = T.tensor4()\n-        self.upsample_size = upsample_size\n \n+class UpSample1D(Layer):\n+    def __init__(self, length=2):\n+        super(UpSample1D, self).__init__()\n+        self.length = length\n+        self.input = T.tensor3()\n \n     def get_output(self, train):\n         X = self.get_input(train)\n-        Y = theano.tensor.extra_ops.repeat(X, self.upsample_size[0], axis = 2)\n-        output = theano.tensor.extra_ops.repeat(Y, self.upsample_size[1], axis = 3)\n+        output = theano.tensor.extra_ops.repeat(X, self.length, axis=1)\n         return output\n \n     def get_config(self):\n         return {\"name\": self.__class__.__name__,\n-                \"upsample_size\":self.upsample_size}\n+                \"length\": self.length}\n+\n+\n+class UpSample2D(Layer):\n+    def __init__(self, size=(2, 2)):\n+        super(UpSample2D, self).__init__()\n+        self.input = T.tensor4()\n+        self.size = size\n+\n+    def get_output(self, train):\n+        X = self.get_input(train)\n+        Y = theano.tensor.extra_ops.repeat(X, self.size[0], axis=2)\n+        output = theano.tensor.extra_ops.repeat(Y, self.size[1], axis=3)\n+        return output\n+\n+    def get_config(self):\n+        return {\"name\": self.__class__.__name__,\n+                \"size\": self.size}\n \n \n class ZeroPadding2D(Layer):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a02ccc2a7842534c97f1cebcd95cd70e5167e0c8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 679 | Contributors (this commit): 9 | Commits (past 90d): 23 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -261,20 +261,20 @@ class UpSample2D(Layer):\n \n \n class ZeroPadding2D(Layer):\n-    def __init__(self, width=1):\n+    def __init__(self, pad=(1, 1)):\n         super(ZeroPadding2D, self).__init__()\n-        self.width = width\n+        self.pad = pad\n         self.input = T.tensor4()\n \n     def get_output(self, train):\n         X = self.get_input(train)\n-        width = self.width\n+        pad = self.pad\n         in_shape = X.shape\n-        out_shape = (in_shape[0], in_shape[1], in_shape[2] + 2 * width, in_shape[3] + 2 * width)\n+        out_shape = (in_shape[0], in_shape[1], in_shape[2] + 2 * pad[0], in_shape[3] + 2 * pad[1])\n         out = T.zeros(out_shape)\n-        indices = (slice(None), slice(None), slice(width, in_shape[2] + width), slice(width, in_shape[3] + width))\n+        indices = (slice(None), slice(None), slice(pad[0], in_shape[2] + pad[0]), slice(pad[1], in_shape[3] + pad[1]))\n         return T.set_subtensor(out[indices], X)\n \n     def get_config(self):\n         return {\"name\": self.__class__.__name__,\n-                \"width\": self.width}\n+                \"pad\": self.pad}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#af6b50b64cba5fc50914590022308f6f8c0c7bd9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 3 | Churn Cumulative: 89 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -5,8 +5,7 @@ from collections import defaultdict\n \n \n class HDF5Matrix():\n-    def __init__(self):\n-        self.refs = defaultdict(int)\n+    refs = defaultdict(int)\n \n     def __init__(self, datapath, dataset, start, end, normalizer=None):\n         if datapath not in list(self.refs.keys()):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4de2b58842d233cfa38c36e1dcf18562422a3c18", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 2641 | Contributors (this commit): 21 | Commits (past 90d): 86 | Contributors (cumulative): 21 | DMM Complexity: None\n\nDIFF:\n@@ -573,8 +573,8 @@ class Graph(Model, containers.Graph):\n             ys_train.append(y_train)\n             ys_test.append(y_test)\n \n-            if hasattr(self.layers[-1], \"get_output_mask\"):\n-                mask = self.layers[-1].get_output_mask()\n+            if hasattr(output, \"get_output_mask\"):\n+                mask = output.get_output_mask()\n             else:\n                 mask = None\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#97174dd298cf4b5be459e79b0181a124650d9148", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 138 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -23,7 +23,7 @@ class BatchNormalization(Layer):\n         self.epsilon = epsilon\n         self.mode = mode\n         self.momentum = momentum\n-        self.input = ndim_tensor(len(self.input_shape))\n+        self.input = ndim_tensor(len(self.input_shape) + 1)\n \n         self.gamma = self.init((self.input_shape))\n         self.beta = shared_zeros(self.input_shape)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#588ce7a7e24e5245f43c927ed96e1881ae3ed3ad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 152 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 152 | Churn Cumulative: 152 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,152 @@\n+# -*- coding: utf-8 -*-\n+from __future__ import print_function\n+from keras.models import Sequential, slice_X\n+from keras.layers.core import Activation, Dense, RepeatVector\n+from keras.layers import recurrent\n+from sklearn.utils import shuffle\n+import numpy as np\n+\n+\"\"\"\n+An implementation of sequence to sequence learning for performing addition\n+Input: \"535+61\"\n+Output: \"596\"\n+Padding is handled by using a repeated sentinel character (space)\n+\n+By default, the JZS1 recurrent neural network is used\n+JZS1 was an \"evolved\" recurrent neural network performing well on arithmetic benchmark in:\n+\"An Empirical Exploration of Recurrent Network Architectures\"\n+http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf\n+\n+Input may optionally be inverted, shown to increase performance in many tasks in:\n+\"Learning to Execute\"\n+http://arxiv.org/abs/1410.4615\n+and\n+\"Sequence to Sequence Learning with Neural Networks\"\n+http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n+Theoretically it introduces shorter term dependencies between source and target.\n+\n+Two digits inverted:\n++ One layer JZS1 (128 HN) with 55 iterations = 99% train/test accuracy\n+Three digits inverted:\n++ One layer JZS1 (128 HN) with 19 iterations = 99% train/test accuracy\n+Four digits inverted:\n++ One layer JZS1 (128 HN) with 20 iterations = 99% train/test accuracy\n+Five digits inverted:\n++ One layer JZS1 (128 HN) with 28 iterations = 99% train/test accuracy\n+\"\"\"\n+\n+\n+class CharacterTable(object):\n+    \"\"\"\n+    Given a set of characters:\n+    + Encode them to a one hot integer representation\n+    + Decode the one hot integer representation to their character output\n+    + Decode a vector of probabilties to their character output\n+    \"\"\"\n+    def __init__(self, chars, maxlen):\n+        self.chars = sorted(set(chars))\n+        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n+        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n+        self.maxlen = maxlen\n+\n+    def encode(self, C, maxlen=None):\n+        maxlen = maxlen if maxlen else self.maxlen\n+        X = np.zeros((maxlen, len(self.chars)))\n+        for i, c in enumerate(C):\n+            X[i, self.char_indices[c]] = 1\n+        return X\n+\n+    def decode(self, X, calc_argmax=True):\n+        if calc_argmax:\n+            X = X.argmax(axis=-1)\n+        return ''.join(self.indices_char[x] for x in X)\n+\n+# Parameters for the model and dataset\n+# Note: Training size is number of queries to generate, not final number of unique queries\n+TRAINING_SIZE = 800000\n+DIGITS = 3\n+INVERT = True\n+# Try replacing JZS1 with LSTM, GRU, or SimpleRNN\n+RNN = recurrent.JZS1\n+HIDDEN_SIZE = 128\n+BATCH_SIZE = 128\n+LAYERS = 1\n+MAXLEN = DIGITS + 1 + DIGITS\n+\n+chars = '0123456789+ '\n+ctable = CharacterTable(chars, MAXLEN)\n+\n+questions = []\n+expected = []\n+seen = set()\n+print('Generating data...')\n+for i in xrange(TRAINING_SIZE):\n+    f = lambda: int(''.join(np.random.choice(list('0123456789')) for i in xrange(np.random.randint(1, DIGITS + 1))))\n+    a, b = f(), f()\n+    # Skip any addition questions we've already seen\n+    # Also skip any such that X+Y == Y+X (hence the sorting)\n+    key = tuple(sorted((a, b)))\n+    if key in seen:\n+        continue\n+    seen.add(key)\n+    # Pad the data with spaces such that it is always MAXLEN\n+    q = '{}+{}'.format(a, b)\n+    query = q + ' ' * (MAXLEN - len(q))\n+    ans = str(a + b)\n+    # Answers can be of maximum size DIGITS + 1\n+    ans += ' ' * (DIGITS + 1 - len(ans))\n+    if INVERT:\n+        query = query[::-1]\n+    questions.append(query)\n+    expected.append(ans)\n+print('Total addition questions:', len(questions))\n+\n+print('Vectorization...')\n+X = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n+y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool)\n+for i, sentence in enumerate(questions):\n+    X[i] = ctable.encode(sentence, maxlen=MAXLEN)\n+for i, sentence in enumerate(expected):\n+    y[i] = ctable.encode(sentence, maxlen=DIGITS + 1)\n+\n+# Shuffle (X, y) in unison as the later parts of X will almost all be larger digits\n+X, y = shuffle(X, y)\n+# Explicitly set apart 10% for validation data that we never train over\n+split_at = len(X) - len(X) / 10\n+(X_train, X_val) = (slice_X(X, 0, split_at), slice_X(X, split_at))\n+(y_train, y_val) = (y[:split_at], y[split_at:])\n+\n+print('Build model...')\n+model = Sequential()\n+# \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE\n+model.add(RNN(len(chars), HIDDEN_SIZE))\n+# For the decoder's input, we repeat the encoded input for each time step\n+model.add(RepeatVector(DIGITS + 1))\n+# The decoder RNN could be multiple layers stacked or a single layer\n+for _ in xrange(LAYERS):\n+    model.add(RNN(HIDDEN_SIZE, HIDDEN_SIZE, return_sequences=True))\n+# For each of step of the output sequence, decide which character should be chosen\n+model.add(Dense(HIDDEN_SIZE, len(chars)))\n+model.add(Activation('softmax'))\n+\n+model.compile(loss='categorical_crossentropy', optimizer='adam')\n+\n+# Train the model each generation and show predictions against the validation dataset\n+for iteration in range(1, 60):\n+    print()\n+    print('-' * 50)\n+    print('Iteration', iteration)\n+    model.fit(X, y, batch_size=BATCH_SIZE, nb_epoch=1, validation_data=(X_val, y_val), show_accuracy=True)\n+    ###\n+    # Select 10 samples from the validation set at random so we can visualize errors\n+    for i in xrange(10):\n+        ind = np.random.randint(0, len(X_val))\n+        rowX, rowy = X_val[np.array([ind])], y_val[np.array([ind])]\n+        preds = model.predict_classes(rowX, verbose=0)\n+        q = ctable.decode(rowX[0])\n+        correct = ctable.decode(rowy[0])\n+        guess = ctable.decode(preds[0], calc_argmax=False)\n+        print('Q', q[::-1] if INVERT else q)\n+        print('T', correct)\n+        print('☑' if correct == guess else '☒', guess)\n+        print('---')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ca0ebcc6276ba978764af2751afca380fa33c9e3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 501 | Contributors (this commit): 5 | Commits (past 90d): 20 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -217,7 +217,7 @@ class Graph(Layer):\n             if input in self.nodes:\n                 self.outputs[name] = self.nodes[input]\n             elif input in self.inputs:\n-                self.ouputs[name] = self.inputs[input]\n+                self.outputs[name] = self.inputs[input]\n         if inputs:\n             to_merge = []\n             for n in inputs:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d5455154f2a9f055e601e14dd5e8e3fea2ac4721", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 10 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 30 | Churn Cumulative: 182 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -25,14 +25,21 @@ and\n http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n Theoretically it introduces shorter term dependencies between source and target.\n \n+\n Two digits inverted:\n-+ One layer JZS1 (128 HN) with 55 iterations = 99% train/test accuracy\n++ One layer JZS1 (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs\n+\n Three digits inverted:\n-+ One layer JZS1 (128 HN) with 19 iterations = 99% train/test accuracy\n++ One layer JZS1 (128 HN), 50k training examples = 99% train/test accuracy in 100 epochs\n+\n+\n Four digits inverted:\n-+ One layer JZS1 (128 HN) with 20 iterations = 99% train/test accuracy\n++ One layer JZS1 (128 HN), 400k training examples = 99% train/test accuracy in 20 epochs\n+\n+\n Five digits inverted:\n-+ One layer JZS1 (128 HN) with 28 iterations = 99% train/test accuracy\n++ One layer JZS1 (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs\n+\n \"\"\"\n \n \n@@ -61,9 +68,14 @@ class CharacterTable(object):\n             X = X.argmax(axis=-1)\n         return ''.join(self.indices_char[x] for x in X)\n \n+\n+class colors:\n+    ok = '\\033[92m'\n+    fail = '\\033[91m'\n+    close = '\\033[0m'\n+\n # Parameters for the model and dataset\n-# Note: Training size is number of queries to generate, not final number of unique queries\n-TRAINING_SIZE = 800000\n+TRAINING_SIZE = 50000\n DIGITS = 3\n INVERT = True\n # Try replacing JZS1 with LSTM, GRU, or SimpleRNN\n@@ -80,7 +92,7 @@ questions = []\n expected = []\n seen = set()\n print('Generating data...')\n-for i in xrange(TRAINING_SIZE):\n+while len(questions) < TRAINING_SIZE:\n     f = lambda: int(''.join(np.random.choice(list('0123456789')) for i in xrange(np.random.randint(1, DIGITS + 1))))\n     a, b = f(), f()\n     # Skip any addition questions we've already seen\n@@ -132,7 +144,7 @@ model.add(Activation('softmax'))\n model.compile(loss='categorical_crossentropy', optimizer='adam')\n \n # Train the model each generation and show predictions against the validation dataset\n-for iteration in range(1, 60):\n+for iteration in range(1, 200):\n     print()\n     print('-' * 50)\n     print('Iteration', iteration)\n@@ -148,5 +160,5 @@ for iteration in range(1, 60):\n         guess = ctable.decode(preds[0], calc_argmax=False)\n         print('Q', q[::-1] if INVERT else q)\n         print('T', correct)\n-        print('☑' if correct == guess else '☒', guess)\n+        print(colors.ok + '☑' + colors.close if correct == guess else colors.fail + '☒' + colors.close, guess)\n         print('---')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7c966439fa0122f44299d86384ef999c8ac01333", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 58 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 7 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 67 | Churn Cumulative: 67 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -4,19 +4,24 @@ from keras.datasets import mnist\n from keras.models import Sequential\n from keras.layers.core import Dense, Activation\n from keras.utils import np_utils\n-from keras.wrappers.scikit_learn import KerasClassifier\n+from keras.wrappers.scikit_learn import *\n import numpy as np\n \n-nb_classes = 10\n batch_size = 128\n nb_epoch = 1\n \n+nb_classes = 10\n max_train_samples = 5000\n max_test_samples = 1000\n \n np.random.seed(1337) # for reproducibility\n \n-# the data, shuffled and split between tran and test sets\n+############################################\n+# scikit-learn classification wrapper test #\n+############################################\n+print('Beginning scikit-learn classification wrapper test')\n+\n+print('Loading data')\n (X_train, y_train), (X_test, y_test) = mnist.load_data()\n \n X_train = X_train.reshape(60000, 784)[:max_train_samples]\n@@ -29,11 +34,6 @@ X_test /= 255\n Y_train = np_utils.to_categorical(y_train, nb_classes)[:max_train_samples]\n Y_test = np_utils.to_categorical(y_test, nb_classes)[:max_test_samples]\n \n-#############################\n-# scikit-learn wrapper test #\n-#############################\n-print('Beginning scikit-learn wrapper test')\n-\n print('Defining model')\n model = Sequential()\n model.add(Dense(784, 50))\n@@ -63,7 +63,7 @@ print('Testing get params')\n print(classifier.get_params())\n \n print('Testing set params')\n-classifier.set_params(optimizer='sgd', loss='mse')\n+classifier.set_params(optimizer='sgd', loss='binary_crossentropy')\n print(classifier.get_params())\n \n print('Testing attributes')\n@@ -73,5 +73,54 @@ print('Config')\n print(classifier.config_)\n print('Weights')\n print(classifier.weights_)\n+print('Compiled model')\n+print(classifier.compiled_model_)\n+\n+########################################\n+# scikit-learn regression wrapper test #\n+########################################\n+print('Beginning scikit-learn regression wrapper test')\n+\n+print('Generating data')\n+X_train = np.random((5000, 100))\n+X_test = np.random((1000, 100))\n+y_train = np.random(5000)\n+y_test = np.random(1000)\n+\n+print('Defining model')\n+model = Sequential()\n+model.add(Dense(100, 50))\n+model.add(Activation('relu'))\n+model.add(Dense(50, 10))\n+model.add(Activation('linear'))\n+\n+print('Creating wrapper')\n+regressor = KerasRegressor(model)\n+\n+print('Fitting model')\n+regressor.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch)\n+\n+print('Testing score function')\n+score = regressor.score(X_train, y_train)\n+print('Score: ', score)\n+\n+print('Testing predict function')\n+preds = regressor.predict(X_test)\n+print('Preds.shape: ', preds.shape)\n+\n+print('Testing get params')\n+print(regressor.get_params())\n+\n+print('Testing set params')\n+regressor.set_params(optimizer='sgd', loss='mean_absolute_error')\n+print(regressor.get_params())\n+\n+print('Testing attributes')\n+print('Config')\n+print(regressor.config_)\n+print('Weights')\n+print(regressor.weights_)\n+print('Compiled model')\n+print(regressor.compiled_model_)\n \n print('Test script complete.')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c515dc90d4be04454193e1b774018de28c6bcf48", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 503 | Contributors (this commit): 5 | Commits (past 90d): 21 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -129,7 +129,7 @@ class Graph(Layer):\n         else:\n             if not connection_map:\n                 raise Exception('Cannot attach multi-input layer: no connection_map provided.')\n-            for k, v in connection_map:\n+            for k, v in connection_map.items():\n                 if k in self.inputs and v in layer.outputs:\n                     self.inputs[k].set_previous(layer.outputs[v])\n                 else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#78feed7fa98f2f7fc03e3e21b69b7b2997f0cef2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 18 | Churn Cumulative: 264 | Contributors (this commit): 4 | Commits (past 90d): 15 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -5,7 +5,7 @@ import theano\n import copy\n \n from ..layers.advanced_activations import LeakyReLU, PReLU\n-from ..layers.core import Dense, Merge, Dropout, Activation, Reshape, Flatten, RepeatVector, Layer\n+from ..layers.core import Dense, Merge, Dropout, Activation, Reshape, Flatten, RepeatVector, Layer, AutoEncoder\n from ..layers.core import ActivityRegularization, TimeDistributedDense, AutoEncoder, MaxoutDense\n from ..layers.convolutional import Convolution1D, Convolution2D, MaxPooling1D, MaxPooling2D, ZeroPadding2D\n from ..layers.embeddings import Embedding, WordContextProduct\n@@ -58,6 +58,14 @@ def container_from_config(original_layer_dict):\n             graph_layer.add_output(**output)\n         return graph_layer\n \n+    elif name == 'AutoEncoder':\n+        kwargs = {'encoder': container_from_config(layer_dict.get('encoder_config')),\n+                  'decoder': container_from_config(layer_dict.get('decoder_config'))}\n+        for kwarg in ['output_reconstruction', 'weights']:\n+            if kwarg in layer_dict:\n+                kwargs[kwarg] = layer_dict[kwarg]\n+        return AutoEncoder(**kwargs)\n+\n     else:\n         layer_dict.pop('name')\n \n\n@@ -1,7 +1,7 @@\n from __future__ import absolute_import\n from __future__ import print_function\n from keras.datasets import mnist\n-from keras.models import Sequential\n+from keras.models import Sequential, model_from_config\n from keras.layers.core import AutoEncoder, Dense, Activation, TimeDistributedDense, Flatten\n from keras.layers.recurrent import LSTM\n from keras.layers.embeddings import Embedding\n@@ -57,6 +57,7 @@ print('\\nclassical_score:', classical_score)\n # autoencoder model test #\n ##########################\n \n+\n def build_lstm_autoencoder(autoencoder, X_train, X_test):\n     X_train = X_train[:, np.newaxis, :]\n     X_test = X_test[:, np.newaxis, :]\n@@ -95,7 +96,6 @@ for autoencoder_type in ['classical', 'lstm']:\n         print(\"Error: unknown autoencoder type!\")\n         exit(-1)\n \n-    autoencoder.get_config(verbose=1)\n     autoencoder.compile(loss='mean_squared_error', optimizer='adam')\n     # Do NOT use validation data with return output_reconstruction=True\n     autoencoder.fit(X_train, X_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=1)\n@@ -128,3 +128,7 @@ for autoencoder_type in ['classical', 'lstm']:\n \n     print('Loss change:', (score[0] - classical_score[0])/classical_score[0], '%')\n     print('Accuracy change:', (score[1] - classical_score[1])/classical_score[1], '%')\n+\n+    # check serialization\n+    config = autoencoder.get_config(verbose=1)\n+    autoencoder = model_from_config(config)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4afb5b60d6fc209b8cb2ec018c6fdb2fbf64dca2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 432 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -234,7 +234,7 @@ class KerasRegressor(BaseWrapper):\n         preds : array-like, shape = (n_samples)\n             Predictions.\n         \"\"\"\n-        return self.compiled_model_.predict(X, batch_size=batch_size, verbose=verbose)\n+        return self.compiled_model_.predict(X, batch_size=batch_size, verbose=verbose).ravel()\n \n     def score(self, X, y, batch_size=128, verbose=0):\n         \"\"\"\n\n@@ -82,16 +82,16 @@ print(classifier.compiled_model_)\n print('Beginning scikit-learn regression wrapper test')\n \n print('Generating data')\n-X_train = np.random((5000, 100))\n-X_test = np.random((1000, 100))\n-y_train = np.random(5000)\n-y_test = np.random(1000)\n+X_train = np.random.random((5000, 100))\n+X_test = np.random.random((1000, 100))\n+y_train = np.random.random(5000)\n+y_test = np.random.random(1000)\n \n print('Defining model')\n model = Sequential()\n model.add(Dense(100, 50))\n model.add(Activation('relu'))\n-model.add(Dense(50, 10))\n+model.add(Dense(50, 1))\n model.add(Activation('linear'))\n \n print('Creating wrapper')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a478930d25543f9f2de7cd76c3c051df08b78475", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 17 | Churn Cumulative: 149 | Contributors (this commit): 4 | Commits (past 90d): 8 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,4 @@\n+from .. import initializations\n from ..layers.core import Layer, MaskedLayer\n from ..utils.theano_utils import shared_zeros, shared_ones, sharedX\n import theano.tensor as T\n@@ -24,12 +25,16 @@ class PReLU(MaskedLayer):\n             Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\n                 http://arxiv.org/pdf/1502.01852v1.pdf\n     '''\n-    def __init__(self, input_shape):\n+    def __init__(self, input_shape, init='zero', weights=None):\n         super(PReLU, self).__init__()\n-        self.alphas = shared_zeros(input_shape)\n+        self.init = initializations.get(init)\n+        self.alphas = self.init(input_shape)\n         self.params = [self.alphas]\n         self.input_shape = input_shape\n \n+        if weights is not None:\n+            self.set_weights(weights)\n+\n     def get_output(self, train):\n         X = self.get_input(train)\n         pos = ((X + abs(X)) / 2.0)\n@@ -38,7 +43,8 @@ class PReLU(MaskedLayer):\n \n     def get_config(self):\n         return {\"name\": self.__class__.__name__,\n-                \"input_shape\": self.input_shape}\n+                \"input_shape\": self.input_shape,\n+                \"init\": self.init.__name__}\n \n \n class ParametricSoftplus(MaskedLayer):\n@@ -49,7 +55,7 @@ class ParametricSoftplus(MaskedLayer):\n             Inferring Nonlinear Neuronal Computation Based on Physiologically Plausible Inputs\n             http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003143\n     '''\n-    def __init__(self, input_shape, alpha_init=0.2, beta_init=5.0):\n+    def __init__(self, input_shape, alpha_init=0.2, beta_init=5.0, weights=None):\n \n         super(ParametricSoftplus, self).__init__()\n         self.alpha_init = alpha_init\n@@ -59,6 +65,9 @@ class ParametricSoftplus(MaskedLayer):\n         self.params = [self.alphas, self.betas]\n         self.input_shape = input_shape\n \n+        if weights is not None:\n+            self.set_weights(weights)\n+\n     def get_output(self, train):\n         X = self.get_input(train)\n         return T.nnet.softplus(self.betas * X) * self.alphas\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a3f3a020a268ace69c25574cd269a0f4ffedd18d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 12 | Churn Cumulative: 691 | Contributors (this commit): 10 | Commits (past 90d): 24 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -178,6 +178,7 @@ class Convolution2D(Layer):\n     def get_output(self, train):\n         X = self.get_input(train)\n         border_mode = self.border_mode\n+        if dnn.dnn_available():\n             if border_mode == 'same':\n                 assert(self.subsample == (1, 1))\n                 pad_x = (self.nb_row - self.subsample[0]) // 2\n@@ -190,6 +191,17 @@ class Convolution2D(Layer):\n                                         kerns=self.W,\n                                         border_mode=border_mode,\n                                         subsample=self.subsample)\n+        else:\n+            if border_mode == 'same':\n+                border_mode = 'full'\n+\n+            conv_out = T.nnet.conv.conv2d(X, self.W,\n+                                          border_mode=border_mode,\n+                                          subsample=self.subsample)\n+            if self.border_mode == 'same':\n+                shift_x = (self.nb_row - 1) // 2\n+                shift_y = (self.nb_col - 1) // 2\n+                conv_out = conv_out[:, :, shift_x:X.shape[2] + shift_x, shift_y:X.shape[3] + shift_y]\n \n         return self.activation(conv_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dcbe14cd9a475ba896306db28f2739c833f96290", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 66 | Lines Deleted: 61 | Files Changed: 2 | Hunks: 27 | Methods Changed: 14 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 127 | Churn Cumulative: 559 | Contributors (this commit): 3 | Commits (past 90d): 11 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,11 +11,36 @@ class BaseWrapper(object):\n     Base class for the Keras scikit-learn wrapper.\n \n     Warning: This class should not be used directly. Use derived classes instead.\n+\n+    Parameters\n+    ----------\n+    train_batch_size : int, optional\n+        Number of training samples evaluated at a time.\n+    test_batch_size : int, optional\n+        Number of test samples evaluated at a time.\n+    nb_epochs : int, optional\n+        Number of training epochs.\n+    shuffle : boolean, optional\n+        Whether to shuffle the samples at each epoch.\n+    show_accuracy : boolean, optional\n+        Whether to display class accuracy in the logs at each epoch.\n+    validation_split : float [0, 1], optional\n+        Fraction of the data to use as held-out validation data.\n+    validation_data : tuple (X, y), optional\n+        Data to be used as held-out validation data. Will override validation_split.\n+    callbacks : list, optional\n+        List of callbacks to apply during training.\n+    verbose : int, optional\n+        Verbosity level.\n     \"\"\"\n     __metaclass__ = abc.ABCMeta\n \n     @abc.abstractmethod\n-    def __init__(self, model, optimizer, loss):\n+    def __init__(self, model, optimizer, loss,\n+                 train_batch_size=128, test_batch_size=128,\n+                 nb_epoch=100, shuffle=True, show_accuracy=False,\n+                 validation_split=0, validation_data=None, callbacks=None,\n+                 verbose=0,):\n         self.model = model\n         self.optimizer = optimizer\n         self.loss = loss\n@@ -24,6 +49,17 @@ class BaseWrapper(object):\n         self.config_ = []\n         self.weights_ = []\n \n+        self.train_batch_size = train_batch_size\n+        self.test_batch_size = test_batch_size\n+        self.nb_epoch = nb_epoch\n+        self.shuffle = shuffle\n+        self.show_accuracy = show_accuracy\n+        self.validation_split = validation_split\n+        self.validation_data = validation_data\n+        self.callbacks = [] if callbacks is None else callbacks\n+\n+        self.verbose = verbose\n+\n     def get_params(self, deep=True):\n         \"\"\"\n         Get parameters for this estimator.\n@@ -58,8 +94,7 @@ class BaseWrapper(object):\n             setattr(self, parameter, value)\n         return self\n \n-    def fit(self, X, y, batch_size=128, nb_epoch=100, verbose=0, shuffle=True, show_accuracy=False,\n-            validation_split=0, validation_data=None, callbacks=[]):\n+    def fit(self, X, y):\n         \"\"\"\n         Fit the model according to the given training data.\n \n@@ -74,22 +109,6 @@ class BaseWrapper(object):\n             and n_features is the number of features.\n         y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n             True labels for X.\n-        batch_size : int, optional\n-            Number of training samples evaluated at a time.\n-        nb_epochs : int, optional\n-            Number of training epochs.\n-        verbose : int, optional\n-            Verbosity level.\n-        shuffle : boolean, optional\n-            Whether to shuffle the samples at each epoch.\n-        show_accuracy : boolean, optional\n-            Whether to display class accuracy in the logs at each epoch.\n-        validation_split : float [0, 1], optional\n-            Fraction of the data to use as held-out validation data.\n-        validation_data : tuple (X, y), optional\n-            Data to be used as held-out validation data. Will override validation_split.\n-        callbacks : list, optional\n-            List of callbacks to apply during training.\n \n         Returns\n         -------\n@@ -105,10 +124,11 @@ class BaseWrapper(object):\n \n         self.compiled_model_ = copy.deepcopy(self.model)\n         self.compiled_model_.compile(optimizer=self.optimizer, loss=self.loss)\n-        history = self.compiled_model_.fit(X, y, batch_size=batch_size, nb_epoch=nb_epoch, verbose=verbose,\n-                                           shuffle=shuffle, show_accuracy=show_accuracy,\n-                                           validation_split=validation_split, validation_data=validation_data,\n-                                           callbacks=callbacks)\n+        history = self.compiled_model_.fit(\n+            X, y, batch_size=self.train_batch_size, nb_epoch=self.nb_epoch, verbose=self.verbose,\n+            shuffle=self.shuffle, show_accuracy=self.show_accuracy,\n+            validation_split=self.validation_split, validation_data=self.validation_data,\n+            callbacks=self.callbacks)\n \n         self.config_ = self.model.get_config()\n         self.weights_ = self.model.get_weights()\n@@ -129,10 +149,10 @@ class KerasClassifier(BaseWrapper):\n     loss : string\n         Loss function used by the model during compilation/training.\n     \"\"\"\n-    def __init__(self, model, optimizer='adam', loss='categorical_crossentropy'):\n-        super(KerasClassifier, self).__init__(model, optimizer, loss)\n+    def __init__(self, model, optimizer='adam', loss='categorical_crossentropy', **kwargs):\n+        super(KerasClassifier, self).__init__(model, optimizer, loss, **kwargs)\n \n-    def predict(self, X, batch_size=128, verbose=0):\n+    def predict(self, X):\n         \"\"\"\n         Returns the class predictions for the given test data.\n \n@@ -141,19 +161,16 @@ class KerasClassifier(BaseWrapper):\n         X : array-like, shape = (n_samples, n_features)\n             Test samples where n_samples in the number of samples\n             and n_features is the number of features.\n-        batch_size : int, optional\n-            Number of test samples evaluated at a time.\n-        verbose : int, optional\n-            Verbosity level.\n \n         Returns\n         -------\n         preds : array-like, shape = (n_samples)\n             Class predictions.\n         \"\"\"\n-        return self.compiled_model_.predict_classes(X, batch_size=batch_size, verbose=verbose)\n+        return self.compiled_model_.predict_classes(\n+            X, batch_size=self.test_batch_size, verbose=self.verbose)\n \n-    def predict_proba(self, X, batch_size=128, verbose=0):\n+    def predict_proba(self, X):\n         \"\"\"\n         Returns class probability estimates for the given test data.\n \n@@ -162,19 +179,16 @@ class KerasClassifier(BaseWrapper):\n         X : array-like, shape = (n_samples, n_features)\n             Test samples where n_samples in the number of samples\n             and n_features is the number of features.\n-        batch_size : int, optional\n-            Number of test samples evaluated at a time.\n-        verbose : int, optional\n-            Verbosity level.\n \n         Returns\n         -------\n         proba : array-like, shape = (n_samples, n_outputs)\n             Class probability estimates.\n         \"\"\"\n-        return self.compiled_model_.predict_proba(X, batch_size=batch_size, verbose=verbose)\n+        return self.compiled_model_.predict_proba(\n+            X, batch_size=self.test_batch_size, verbose=self.verbose)\n \n-    def score(self, X, y, batch_size=128, verbose=0):\n+    def score(self, X, y):\n         \"\"\"\n         Returns the mean accuracy on the given test data and labels.\n \n@@ -185,17 +199,14 @@ class KerasClassifier(BaseWrapper):\n             and n_features is the number of features.\n         y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n             True labels for X.\n-        batch_size : int, optional\n-            Number of test samples evaluated at a time.\n-        verbose : int, optional\n-            Verbosity level.\n \n         Returns\n         -------\n         score : float\n             Mean accuracy of predictions on X wrt. y.\n         \"\"\"\n-        loss, accuracy = self.compiled_model_.evaluate(X, y, batch_size=batch_size, show_accuracy=True, verbose=verbose)\n+        loss, accuracy = self.compiled_model_.evaluate(\n+            X, y, batch_size=self.test_batch_size, show_accuracy=True, verbose=self.verbose)\n         return accuracy\n \n \n@@ -212,10 +223,10 @@ class KerasRegressor(BaseWrapper):\n     loss : string\n         Loss function used by the model during compilation/training.\n     \"\"\"\n-    def __init__(self, model, optimizer='adam', loss='mean_squared_error'):\n-        super(KerasRegressor, self).__init__(model, optimizer, loss)\n+    def __init__(self, model, optimizer='adam', loss='mean_squared_error', **kwargs):\n+        super(KerasRegressor, self).__init__(model, optimizer, loss, **kwargs)\n \n-    def predict(self, X, batch_size=128, verbose=0):\n+    def predict(self, X):\n         \"\"\"\n         Returns predictions for the given test data.\n \n@@ -224,19 +235,16 @@ class KerasRegressor(BaseWrapper):\n         X : array-like, shape = (n_samples, n_features)\n             Test samples where n_samples in the number of samples\n             and n_features is the number of features.\n-        batch_size : int, optional\n-            Number of test samples evaluated at a time.\n-        verbose : int, optional\n-            Verbosity level.\n \n         Returns\n         -------\n         preds : array-like, shape = (n_samples)\n             Predictions.\n         \"\"\"\n-        return self.compiled_model_.predict(X, batch_size=batch_size, verbose=verbose).ravel()\n+        return self.compiled_model_.predict(\n+            X, batch_size=self.test_batch_size, verbose=self.verbose).ravel()\n \n-    def score(self, X, y, batch_size=128, verbose=0):\n+    def score(self, X, y):\n         \"\"\"\n         Returns the mean accuracy on the given test data and labels.\n \n@@ -247,15 +255,12 @@ class KerasRegressor(BaseWrapper):\n             and n_features is the number of features.\n         y : array-like, shape = (n_samples)\n             True labels for X.\n-        batch_size : int, optional\n-            Number of test samples evaluated at a time.\n-        verbose : int, optional\n-            Verbosity level.\n \n         Returns\n         -------\n         score : float\n             Loss from predictions on X wrt. y.\n         \"\"\"\n-        loss = self.compiled_model_.evaluate(X, y, batch_size=batch_size, show_accuracy=False, verbose=verbose)\n+        loss = self.compiled_model_.evaluate(\n+            X, y, batch_size=self.test_batch_size, show_accuracy=False, verbose=self.verbose)\n         return loss\n\n@@ -42,10 +42,10 @@ model.add(Dense(50, 10))\n model.add(Activation('softmax'))\n \n print('Creating wrapper')\n-classifier = KerasClassifier(model)\n+classifier = KerasClassifier(model, train_batch_size=batch_size, nb_epoch=nb_epoch)\n \n print('Fitting model')\n-classifier.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch)\n+classifier.fit(X_train, Y_train)\n \n print('Testing score function')\n score = classifier.score(X_train, Y_train)\n@@ -95,10 +95,10 @@ model.add(Dense(50, 1))\n model.add(Activation('linear'))\n \n print('Creating wrapper')\n-regressor = KerasRegressor(model)\n+regressor = KerasRegressor(model, train_batch_size=batch_size, nb_epoch=nb_epoch)\n \n print('Fitting model')\n-regressor.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch)\n+regressor.fit(X_train, y_train)\n \n print('Testing score function')\n score = regressor.score(X_train, y_train)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
