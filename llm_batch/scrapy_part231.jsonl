{"custom_id": "scrapy#f6205778f31dae3ac0f60528a7aa4ef3ef6a181a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 15 | Files Changed: 1 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 30 | Churn Cumulative: 475 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -28,6 +28,16 @@ class ReferrerPolicy(object):\n     def referrer(self, response, request):\n         raise NotImplementedError()\n \n+    def stripped_referrer(self, req_or_resp):\n+        stripped = self.strip_url(req_or_resp)\n+        if stripped is not None:\n+            return urlunparse(stripped)\n+\n+    def origin_referrer(self, req_or_resp):\n+        stripped = self.strip_url(req_or_resp, origin_only=True)\n+        if stripped is not None:\n+            return urlunparse(stripped)\n+\n     def strip_url(self, req_or_resp, origin_only=False):\n         \"\"\"\n         https://www.w3.org/TR/referrer-policy/#strip-url\n@@ -113,9 +123,7 @@ class NoReferrerWhenDowngradePolicy(ReferrerPolicy):\n         if urlparse_cached(response).scheme in ('https', 'ftps') and \\\n             urlparse_cached(request).scheme in ('http',):\n                 return None\n-        stripped = self.strip_url(response)\n-        if stripped is not None:\n-            return urlunparse(stripped)\n+        return self.stripped_referrer(response)\n \n \n class SameOriginPolicy(ReferrerPolicy):\n@@ -132,9 +140,7 @@ class SameOriginPolicy(ReferrerPolicy):\n \n     def referrer(self, response, request):\n         if self.origin(response) == self.origin(request):\n-            stripped = self.strip_url(response)\n-            if stripped is not None:\n-                return urlunparse(stripped)\n+            return self.stripped_referrer(response)\n \n \n class OriginPolicy(ReferrerPolicy):\n@@ -149,9 +155,7 @@ class OriginPolicy(ReferrerPolicy):\n     name = POLICY_ORIGIN\n \n     def referrer(self, response, request):\n-        origin = self.strip_url(response, origin_only=True)\n-        if origin is not None:\n-            return urlunparse(origin)\n+        return self.origin_referrer(response)\n \n \n class OriginWhenCrossOriginPolicy(ReferrerPolicy):\n@@ -170,9 +174,7 @@ class OriginWhenCrossOriginPolicy(ReferrerPolicy):\n     def referrer(self, response, request):\n         origin = self.origin(response)\n         if origin == self.origin(request):\n-            stripped = self.strip_url(response)\n-            if stripped is not None:\n-                return urlunparse(stripped)\n+            return self.stripped_referrer(response)\n         else:\n             return urlunparse(origin + ('', '', ''))\n \n@@ -193,9 +195,7 @@ class UnsafeUrlPolicy(ReferrerPolicy):\n     name = POLICY_UNSAFE_URL\n \n     def referrer(self, response, request):\n-        stripped = self.strip_url(response)\n-        if stripped is not None:\n-            return urlunparse(stripped)\n+        return self.stripped_referrer(response)\n \n \n class LegacyPolicy(ReferrerPolicy):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#842ce131aa666dce0db61aeee701c420050a5a9c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 19 | Churn Cumulative: 998 | Contributors (this commit): 35 | Commits (past 90d): 18 | Contributors (cumulative): 37 | DMM Complexity: 1.0\n\nDIFF:\n@@ -234,6 +234,7 @@ REDIRECT_MAX_TIMES = 20  # uses Firefox default setting\n REDIRECT_PRIORITY_ADJUST = +2\n \n REFERER_ENABLED = True\n+REFERER_POLICY = 'scrapy.spidermiddlewares.referer.DefaultReferrerPolicy'\n \n RETRY_ENABLED = True\n RETRY_TIMES = 2  # initial response + 2 retries = 3 requests\n\n@@ -8,6 +8,7 @@ from scrapy.http import Request\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.python import to_native_str\n from scrapy.utils.httpobj import urlparse_cached\n+from scrapy.utils.misc import load_object\n \n \n LOCAL_SCHEMES = ('about', 'blob', 'data', 'filesystem',)\n@@ -221,16 +222,27 @@ _policy_classes = {p.name: p for p in (\n \n class RefererMiddleware(object):\n \n-    def __init__(self, policy_class=DefaultReferrerPolicy):\n-        self.default_policy = policy_class\n+    def __init__(self, settings={}):\n+        policy = settings.get('REFERER_POLICY')\n+        if policy is not None:\n+            try:\n+                self.default_policy = load_object(policy)\n+            except ValueError:\n+                try:\n+                    self.default_policy = _policy_classes[policy]\n+                except:\n+                    raise NotConfigured(\"Unknown referrer policy name %r\" % policy)\n+        else:\n+            self.default_policy = DefaultReferrerPolicy\n \n     @classmethod\n     def from_crawler(cls, crawler):\n         if not crawler.settings.getbool('REFERER_ENABLED'):\n             raise NotConfigured\n-        return cls()\n+        return cls(crawler.settings)\n \n     def policy(self, response, request):\n+        # policy set in request's meta dict takes precedence over default policy\n         policy_name = request.meta.get('referrer_policy')\n         if policy_name is None:\n             policy_name = to_native_str(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e72b6e33611f61dbb358f1ff97c5c195a7db696f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 146 | Lines Deleted: 63 | Files Changed: 1 | Hunks: 29 | Methods Changed: 14 | Complexity Δ (Sum/Max): -8/0 | Churn Δ: 209 | Churn Cumulative: 531 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,34 +1,56 @@\n from unittest import TestCase\n \n+from scrapy.exceptions import NotConfigured\n from scrapy.http import Response, Request\n+from scrapy.settings import Settings\n from scrapy.spiders import Spider\n from scrapy.spidermiddlewares.referer import RefererMiddleware, \\\n     POLICY_NO_REFERRER, POLICY_NO_REFERRER_WHEN_DOWNGRADE, \\\n     POLICY_SAME_ORIGIN, POLICY_ORIGIN, POLICY_ORIGIN_WHEN_CROSS_ORIGIN, \\\n-    POLICY_UNSAFE_URL\n+    POLICY_SCRAPY_DEFAULT, POLICY_UNSAFE_URL, \\\n+    DefaultReferrerPolicy, \\\n+    NoReferrerPolicy, NoReferrerWhenDowngradePolicy, \\\n+    OriginWhenCrossOriginPolicy, OriginPolicy, \\\n+    SameOriginPolicy, UnsafeUrlPolicy\n \n \n class TestRefererMiddleware(TestCase):\n \n+    req_meta = {}\n+    resp_headers = {}\n+    settings = {}\n+    scenarii = [\n+        ('http://scrapytest.org', 'http://scrapytest.org/',  b'http://scrapytest.org'),\n+    ]\n+\n     def setUp(self):\n         self.spider = Spider('foo')\n-        self.mw = RefererMiddleware()\n+        settings = Settings(self.settings)\n+        self.mw = RefererMiddleware(settings)\n \n-    def test_process_spider_output(self):\n-        res = Response('http://scrapytest.org')\n-        reqs = [Request('http://scrapytest.org/')]\n+    def get_request(self, target):\n+        return Request(target, meta=self.req_meta)\n \n-        out = list(self.mw.process_spider_output(res, reqs, self.spider))\n-        self.assertEquals(out[0].headers.get('Referer'),\n-                          b'http://scrapytest.org')\n+    def get_response(self, origin):\n+        return Response(origin, headers=self.resp_headers)\n \n-    def test_policy_default(self):\n+    def test(self):\n+\n+        for origin, target, referrer in self.scenarii:\n+            response = self.get_response(origin)\n+            request = self.get_request(target)\n+\n+            out = list(self.mw.process_spider_output(response, [request], self.spider))\n+            self.assertEquals(out[0].headers.get('Referer'), referrer)\n+\n+\n+class MixinDefault(object):\n     \"\"\"\n     Based on https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade\n \n     with some additional filtering of s3://\n     \"\"\"\n-        for origin, target, referrer in [\n+    scenarii = [\n         ('https://example.com/',    'https://scrapy.org/',  b'https://example.com/'),\n         ('http://example.com/',     'http://scrapy.org/',   b'http://example.com/'),\n         ('http://example.com/',     'https://scrapy.org/',  b'http://example.com/'),\n@@ -44,31 +66,21 @@ class TestRefererMiddleware(TestCase):\n         # no referrer leak for s3 origins\n         ('s3://mybucket/path/to/data.csv',  'https://scrapy.org/', None),\n         ('s3://mybucket/path/to/data.csv',  'http://scrapy.org/',  None),\n-            ]:\n-            response = Response(origin)\n-            request = Request(target)\n+    ]\n \n-            out = list(self.mw.process_spider_output(response, [request], self.spider))\n-            self.assertEquals(out[0].headers.get('Referer'), referrer)\n \n-    def test_policy_no_referrer(self):\n-\n-        for origin, target, referrer in [\n+class MixinNoReferrer(object):\n+    scenarii = [\n         ('https://example.com/page.html',       'https://example.com/', None),\n         ('http://www.example.com/',             'https://scrapy.org/',  None),\n         ('http://www.example.com/',             'http://scrapy.org/',   None),\n         ('https://www.example.com/',            'http://scrapy.org/',   None),\n         ('file:///home/path/to/somefile.html',  'http://scrapy.org/',   None),\n-            ]:\n-            response = Response(origin)\n-            request = Request(target, meta={'referrer_policy': POLICY_NO_REFERRER})\n+    ]\n \n-            out = list(self.mw.process_spider_output(response, [request], self.spider))\n-            self.assertEquals(out[0].headers.get('Referer'), referrer)\n \n-    def test_policy_no_referrer_when_downgrade(self):\n-\n-        for origin, target, referrer in [\n+class MixinNoReferrerWhenDowngrade(object):\n+    scenarii = [\n         # TLS to TLS: send non-empty referrer\n         ('https://example.com/page.html',       'https://not.example.com/', b'https://example.com/page.html'),\n         ('https://example.com/page.html',       'https://scrapy.org/',      b'https://example.com/page.html'),\n@@ -93,16 +105,11 @@ class TestRefererMiddleware(TestCase):\n \n         # test for user/password stripping\n         ('http://user:password@example.com/page.html', 'https://not.example.com/', b'http://example.com/page.html'),\n-            ]:\n-            response = Response(origin)\n-            request = Request(target, meta={'referrer_policy': POLICY_NO_REFERRER_WHEN_DOWNGRADE})\n+    ]\n \n-            out = list(self.mw.process_spider_output(response, [request], self.spider))\n-            self.assertEquals(out[0].headers.get('Referer'), referrer)\n \n-    def test_policy_same_origin(self):\n-\n-        for origin, target, referrer in [\n+class MixinSameOrigin(object):\n+    scenarii = [\n         # Same origin (protocol, host, port): send referrer\n         ('https://example.com/page.html',       'https://example.com/not-page.html',        b'https://example.com/page.html'),\n         ('http://example.com/page.html',        'http://example.com/not-page.html',         b'http://example.com/page.html'),\n@@ -131,16 +138,11 @@ class TestRefererMiddleware(TestCase):\n         # test for user/password stripping\n         ('https://user:password@example.com/page.html', 'https://example.com/not-page.html',    b'https://example.com/page.html'),\n         ('https://user:password@example.com/page.html', 'http://example.com/not-page.html',     None),\n-            ]:\n-            response = Response(origin)\n-            request = Request(target, meta={'referrer_policy': POLICY_SAME_ORIGIN})\n+    ]\n \n-            out = list(self.mw.process_spider_output(response, [request], self.spider))\n-            self.assertEquals(out[0].headers.get('Referer'), referrer)\n \n-    def test_policy_origin(self):\n-\n-        for origin, target, referrer in [\n+class MixinOrigin(object):\n+    scenarii = [\n         # TLS or non-TLS to TLS or non-TLS: referrer origin is sent (yes, even for downgrades)\n         ('https://example.com/page.html',   'https://example.com/not-page.html',    b'https://example.com/'),\n         ('https://example.com/page.html',   'https://scrapy.org',                   b'https://example.com/'),\n@@ -149,16 +151,11 @@ class TestRefererMiddleware(TestCase):\n \n         # test for user/password stripping\n         ('https://user:password@example.com/page.html', 'http://scrapy.org', b'https://example.com/'),\n-            ]:\n-            response = Response(origin)\n-            request = Request(target, meta={'referrer_policy': POLICY_ORIGIN})\n+    ]\n \n-            out = list(self.mw.process_spider_output(response, [request], self.spider))\n-            self.assertEquals(out[0].headers.get('Referer'), referrer)\n \n-    def test_policy_origin_when_cross_origin(self):\n-\n-        for origin, target, referrer in [\n+class MixinOriginWhenCrossOrigin(object):\n+    scenarii = [\n         # Same origin (protocol, host, port): send referrer\n         ('https://example.com/page.html',       'https://example.com/not-page.html',        b'https://example.com/page.html'),\n         ('http://example.com/page.html',        'http://example.com/not-page.html',         b'http://example.com/page.html'),\n@@ -189,16 +186,11 @@ class TestRefererMiddleware(TestCase):\n         ('https://user:password@example5.com/page.html', 'https://example5.com/not-page.html',  b'https://example5.com/page.html'),\n         # TLS to non-TLS downgrade: send origin\n         ('https://user:password@example5.com/page.html', 'http://example5.com/not-page.html',   b'https://example5.com/'),\n-            ]:\n-            response = Response(origin)\n-            request = Request(target, meta={'referrer_policy': POLICY_ORIGIN_WHEN_CROSS_ORIGIN})\n+    ]\n \n-            out = list(self.mw.process_spider_output(response, [request], self.spider))\n-            self.assertEquals(out[0].headers.get('Referer'), referrer)\n \n-    def test_policy_unsafe_url(self):\n-\n-        for origin, target, referrer in [\n+class MixinUnsafeUrl(object):\n+    scenarii = [\n         # TLS to TLS: send referrer\n         ('https://example.com/sekrit.html',     'http://not.example.com/',      b'https://example.com/sekrit.html'),\n         ('https://example1.com/page.html',      'https://not.example1.com/',    b'https://example1.com/page.html'),\n@@ -225,9 +217,100 @@ class TestRefererMiddleware(TestCase):\n         # test for user/password stripping\n         ('http://user:password@example4.com/page.html',     'https://not.example4.com/',    b'http://example4.com/page.html'),\n         ('https://user:password@example4.com/page.html',    'http://scrapy.org/',           b'https://example4.com/page.html'),\n-            ]:\n-            response = Response(origin)\n-            request = Request(target, meta={'referrer_policy': POLICY_UNSAFE_URL})\n+    ]\n \n-            out = list(self.mw.process_spider_output(response, [request], self.spider))\n-            self.assertEquals(out[0].headers.get('Referer'), referrer)\n+\n+class TestRefererMiddlewareDefault(MixinDefault, TestRefererMiddleware):\n+    pass\n+\n+\n+# --- Tests using settings to set policy using class path\n+class TestRefererMiddlewareSettingsNoReferrer(MixinNoReferrer, TestRefererMiddleware):\n+    settings = {'REFERER_POLICY': 'scrapy.spidermiddlewares.referer.NoReferrerPolicy'}\n+\n+\n+class TestRefererMiddlewareSettingsNoReferrerWhenDowngrade(MixinNoReferrerWhenDowngrade, TestRefererMiddleware):\n+    settings = {'REFERER_POLICY': 'scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy'}\n+\n+\n+class TestRefererMiddlewareSettingsSameOrigin(MixinSameOrigin, TestRefererMiddleware):\n+    settings = {'REFERER_POLICY': 'scrapy.spidermiddlewares.referer.SameOriginPolicy'}\n+\n+\n+class TestRefererMiddlewareSettingsOrigin(MixinOrigin, TestRefererMiddleware):\n+    settings = {'REFERER_POLICY': 'scrapy.spidermiddlewares.referer.OriginPolicy'}\n+\n+\n+class TestRefererMiddlewareSettingsOriginWhenCrossOrigin(MixinOriginWhenCrossOrigin, TestRefererMiddleware):\n+    settings = {'REFERER_POLICY': 'scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy'}\n+\n+\n+class TestRefererMiddlewareSettingsUnsafeUrl(MixinUnsafeUrl, TestRefererMiddleware):\n+    settings = {'REFERER_POLICY': 'scrapy.spidermiddlewares.referer.UnsafeUrlPolicy'}\n+\n+\n+# --- Tests using Request meta dict to set policy\n+class TestRefererMiddlewareDefaultMeta(MixinDefault, TestRefererMiddleware):\n+    req_meta = {'referrer_policy': POLICY_SCRAPY_DEFAULT}\n+\n+\n+class TestRefererMiddlewareNoReferrer(MixinNoReferrer, TestRefererMiddleware):\n+    req_meta = {'referrer_policy': POLICY_NO_REFERRER}\n+\n+\n+class TestRefererMiddlewareNoReferrerWhenDowngrade(MixinNoReferrerWhenDowngrade, TestRefererMiddleware):\n+    req_meta = {'referrer_policy': POLICY_NO_REFERRER_WHEN_DOWNGRADE}\n+\n+\n+class TestRefererMiddlewareSameOrigin(MixinSameOrigin, TestRefererMiddleware):\n+    req_meta = {'referrer_policy': POLICY_SAME_ORIGIN}\n+\n+\n+class TestRefererMiddlewareOrigin(MixinOrigin, TestRefererMiddleware):\n+    req_meta = {'referrer_policy': POLICY_ORIGIN}\n+\n+\n+class TestRefererMiddlewareOriginWhenCrossOrigin(MixinOriginWhenCrossOrigin, TestRefererMiddleware):\n+    req_meta = {'referrer_policy': POLICY_ORIGIN_WHEN_CROSS_ORIGIN}\n+\n+\n+class TestRefererMiddlewareUnsafeUrl(MixinUnsafeUrl, TestRefererMiddleware):\n+    req_meta = {'referrer_policy': POLICY_UNSAFE_URL}\n+\n+\n+\n+class TestRefererMiddlewareMetaPredecence001(MixinUnsafeUrl, TestRefererMiddleware):\n+    settings = {'REFERER_POLICY': 'scrapy.spidermiddlewares.referer.SameOriginPolicy'}\n+    req_meta = {'referrer_policy': POLICY_UNSAFE_URL}\n+\n+\n+class TestRefererMiddlewareMetaPredecence002(MixinNoReferrer, TestRefererMiddleware):\n+    settings = {'REFERER_POLICY': 'scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy'}\n+    req_meta = {'referrer_policy': POLICY_NO_REFERRER}\n+\n+\n+class TestRefererMiddlewareMetaPredecence003(MixinUnsafeUrl, TestRefererMiddleware):\n+    settings = {'REFERER_POLICY': 'scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy'}\n+    req_meta = {'referrer_policy': POLICY_UNSAFE_URL}\n+\n+\n+class TestRefererMiddlewareSettingsPolicyByName(TestCase):\n+\n+    def test_valid_name(self):\n+        for s, p in [\n+                (POLICY_SCRAPY_DEFAULT, DefaultReferrerPolicy),\n+                (POLICY_NO_REFERRER, NoReferrerPolicy),\n+                (POLICY_NO_REFERRER_WHEN_DOWNGRADE, NoReferrerWhenDowngradePolicy),\n+                (POLICY_SAME_ORIGIN, SameOriginPolicy),\n+                (POLICY_ORIGIN, OriginPolicy),\n+                (POLICY_ORIGIN_WHEN_CROSS_ORIGIN, OriginWhenCrossOriginPolicy),\n+                (POLICY_UNSAFE_URL, UnsafeUrlPolicy),\n+            ]:\n+            settings = Settings({'REFERER_POLICY': s})\n+            mw = RefererMiddleware(settings)\n+            self.assertEquals(mw.default_policy, p)\n+\n+    def test_invalid_name(self):\n+        settings = Settings({'REFERER_POLICY': 'some-custom-unknown-policy'})\n+        with self.assertRaises(NotConfigured):\n+            mw = RefererMiddleware(settings)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0344f57fefc0877bf9048a084002fc719335e31c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 16 | Churn Cumulative: 1040 | Contributors (this commit): 4 | Commits (past 90d): 13 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -229,7 +229,7 @@ class RefererMiddleware(object):\n                 self.default_policy = load_object(policy)\n             except ValueError:\n                 try:\n-                    self.default_policy = _policy_classes[policy]\n+                    self.default_policy = _policy_classes[policy.lower()]\n                 except:\n                     raise NotConfigured(\"Unknown referrer policy name %r\" % policy)\n         else:\n\n@@ -310,6 +310,20 @@ class TestRefererMiddlewareSettingsPolicyByName(TestCase):\n             mw = RefererMiddleware(settings)\n             self.assertEquals(mw.default_policy, p)\n \n+    def test_valid_name_casevariants(self):\n+        for s, p in [\n+                (POLICY_SCRAPY_DEFAULT, DefaultReferrerPolicy),\n+                (POLICY_NO_REFERRER, NoReferrerPolicy),\n+                (POLICY_NO_REFERRER_WHEN_DOWNGRADE, NoReferrerWhenDowngradePolicy),\n+                (POLICY_SAME_ORIGIN, SameOriginPolicy),\n+                (POLICY_ORIGIN, OriginPolicy),\n+                (POLICY_ORIGIN_WHEN_CROSS_ORIGIN, OriginWhenCrossOriginPolicy),\n+                (POLICY_UNSAFE_URL, UnsafeUrlPolicy),\n+            ]:\n+            settings = Settings({'REFERER_POLICY': s.upper()})\n+            mw = RefererMiddleware(settings)\n+            self.assertEquals(mw.default_policy, p)\n+\n     def test_invalid_name(self):\n         settings = Settings({'REFERER_POLICY': 'some-custom-unknown-policy'})\n         with self.assertRaises(NotConfigured):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ec8b4c1a9bfd004960d730b41078b9f0633627d8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 501 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -222,7 +222,9 @@ _policy_classes = {p.name: p for p in (\n \n class RefererMiddleware(object):\n \n-    def __init__(self, settings={}):\n+    def __init__(self, settings=None):\n+        self.default_policy = DefaultReferrerPolicy\n+        if settings is not None:\n             policy = settings.get('REFERER_POLICY')\n             if policy is not None:\n                 try:\n@@ -232,8 +234,6 @@ class RefererMiddleware(object):\n                         self.default_policy = _policy_classes[policy.lower()]\n                     except:\n                         raise NotConfigured(\"Unknown referrer policy name %r\" % policy)\n-        else:\n-            self.default_policy = DefaultReferrerPolicy\n \n     @classmethod\n     def from_crawler(cls, crawler):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e50e670eff2fe1b109f6b5dd026c4c706a93585c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 34 | Churn Cumulative: 1080 | Contributors (this commit): 4 | Commits (past 90d): 16 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -227,9 +227,12 @@ class RefererMiddleware(object):\n         if settings is not None:\n             policy = settings.get('REFERER_POLICY')\n             if policy is not None:\n+                # expect a string for the path to the policy class\n                 try:\n                     self.default_policy = load_object(policy)\n                 except ValueError:\n+                    # otherwise try to interpret the string as standard\n+                    # https://www.w3.org/TR/referrer-policy/#referrer-policies\n                     try:\n                         self.default_policy = _policy_classes[policy.lower()]\n                     except:\n@@ -239,6 +242,7 @@ class RefererMiddleware(object):\n     def from_crawler(cls, crawler):\n         if not crawler.settings.getbool('REFERER_ENABLED'):\n             raise NotConfigured\n+\n         return cls(crawler.settings)\n \n     def policy(self, response, request):\n\n@@ -11,7 +11,7 @@ from scrapy.spidermiddlewares.referer import RefererMiddleware, \\\n     DefaultReferrerPolicy, \\\n     NoReferrerPolicy, NoReferrerWhenDowngradePolicy, \\\n     OriginWhenCrossOriginPolicy, OriginPolicy, \\\n-    SameOriginPolicy, UnsafeUrlPolicy\n+    SameOriginPolicy, UnsafeUrlPolicy, ReferrerPolicy\n \n \n class TestRefererMiddleware(TestCase):\n@@ -249,6 +249,33 @@ class TestRefererMiddlewareSettingsUnsafeUrl(MixinUnsafeUrl, TestRefererMiddlewa\n     settings = {'REFERER_POLICY': 'scrapy.spidermiddlewares.referer.UnsafeUrlPolicy'}\n \n \n+class CustomPythonOrgPolicy(ReferrerPolicy):\n+    \"\"\"\n+    A dummy policy that returns referrer as http(s)://python.org\n+    depending on the scheme of the target URL.\n+    \"\"\"\n+    def referrer(self, response, request):\n+        from scrapy.utils.httpobj import urlparse_cached\n+\n+        scheme = urlparse_cached(request).scheme\n+        if scheme == 'https':\n+            return b'https://python.org/'\n+        elif scheme == 'http':\n+            return b'http://python.org/'\n+\n+\n+class TestRefererMiddlewareSettingsCustomPolicy(TestRefererMiddleware):\n+    settings = {'REFERER_POLICY': 'tests.test_spidermiddleware_referer.CustomPythonOrgPolicy'}\n+    scenarii = [\n+        ('https://example.com/',    'https://scrapy.org/',  b'https://python.org/'),\n+        ('http://example.com/',     'http://scrapy.org/',   b'http://python.org/'),\n+        ('http://example.com/',     'https://scrapy.org/',  b'https://python.org/'),\n+        ('https://example.com/',    'http://scrapy.org/',   b'http://python.org/'),\n+        ('file:///home/path/to/somefile.html',  'https://scrapy.org/', b'https://python.org/'),\n+        ('file:///home/path/to/somefile.html',  'http://scrapy.org/',  b'http://python.org/'),\n+\n+    ]\n+\n # --- Tests using Request meta dict to set policy\n class TestRefererMiddlewareDefaultMeta(MixinDefault, TestRefererMiddleware):\n     req_meta = {'referrer_policy': POLICY_SCRAPY_DEFAULT}\n@@ -278,7 +305,6 @@ class TestRefererMiddlewareUnsafeUrl(MixinUnsafeUrl, TestRefererMiddleware):\n     req_meta = {'referrer_policy': POLICY_UNSAFE_URL}\n \n \n-\n class TestRefererMiddlewareMetaPredecence001(MixinUnsafeUrl, TestRefererMiddleware):\n     settings = {'REFERER_POLICY': 'scrapy.spidermiddlewares.referer.SameOriginPolicy'}\n     req_meta = {'referrer_policy': POLICY_UNSAFE_URL}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d3d4d66ce8e5d01aa1fd6013a6d63337bb931460", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 588 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -354,3 +354,16 @@ class TestRefererMiddlewareSettingsPolicyByName(TestCase):\n         settings = Settings({'REFERER_POLICY': 'some-custom-unknown-policy'})\n         with self.assertRaises(NotConfigured):\n             mw = RefererMiddleware(settings)\n+\n+\n+class TestRefererMiddlewarePolicyHeaderPredecence001(MixinUnsafeUrl, TestRefererMiddleware):\n+    settings = {'REFERER_POLICY': 'scrapy.spidermiddlewares.referer.SameOriginPolicy'}\n+    resp_headers = {'Referrer-Policy': POLICY_UNSAFE_URL.upper()}\n+\n+class TestRefererMiddlewarePolicyHeaderPredecence002(MixinNoReferrer, TestRefererMiddleware):\n+    settings = {'REFERER_POLICY': 'scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy'}\n+    resp_headers = {'Referrer-Policy': POLICY_NO_REFERRER.swapcase()}\n+\n+class TestRefererMiddlewarePolicyHeaderPredecence003(MixinNoReferrerWhenDowngrade, TestRefererMiddleware):\n+    settings = {'REFERER_POLICY': 'scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy'}\n+    resp_headers = {'Referrer-Policy': POLICY_NO_REFERRER_WHEN_DOWNGRADE.title()}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#285d5bc03a7ebe8eaa558a5c24ff0693353c3e87", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 23 | Churn Cumulative: 528 | Contributors (this commit): 2 | Commits (past 90d): 12 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,8 +4,9 @@ originated it.\n \"\"\"\n from six.moves.urllib.parse import ParseResult, urlunparse\n \n-from scrapy.http import Request\n+from scrapy.http import Request, Response\n from scrapy.exceptions import NotConfigured\n+from scrapy import signals\n from scrapy.utils.python import to_native_str\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.misc import load_object\n@@ -242,8 +243,9 @@ class RefererMiddleware(object):\n     def from_crawler(cls, crawler):\n         if not crawler.settings.getbool('REFERER_ENABLED'):\n             raise NotConfigured\n-\n-        return cls(crawler.settings)\n+        mw = cls(crawler.settings)\n+        crawler.signals.connect(mw.request_scheduled, signal=signals.request_scheduled)\n+        return mw\n \n     def policy(self, response, request):\n         # policy set in request's meta dict takes precedence over default policy\n@@ -264,3 +266,18 @@ class RefererMiddleware(object):\n             return r\n         return (_set_referer(r) for r in result or ())\n \n+    def request_scheduled(self, request, spider):\n+        # check redirected request to patch \"Referer\" header if necessary\n+        redirected_urls = request.meta.get('redirect_urls', [])\n+        if redirected_urls:\n+            request_referrer = request.headers.get('Referer')\n+            # we don't patch the referrer value if there is none\n+            if request_referrer is not None:\n+                faked_response = Response(redirected_urls[0])\n+                policy_referrer = self.policy(faked_response,\n+                    request).referrer(faked_response, request)\n+                if policy_referrer != request_referrer:\n+                    if policy_referrer is None:\n+                        request.headers.pop('Referer')\n+                    else:\n+                        request.headers['Referer'] = policy_referrer\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5dd7311cd48e676147138746229d7ab2b429b8a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 133 | Lines Deleted: 36 | Files Changed: 3 | Hunks: 11 | Methods Changed: 23 | Complexity Δ (Sum/Max): 19/17 | Churn Δ: 169 | Churn Cumulative: 2295 | Contributors (this commit): 17 | Commits (past 90d): 19 | Contributors (cumulative): 24 | DMM Complexity: 0.958904109589041\n\nDIFF:\n@@ -2,14 +2,13 @@\n RefererMiddleware: populates Request referer field, based on the Response which\n originated it.\n \"\"\"\n-from six.moves.urllib.parse import ParseResult, urlunparse\n-\n from scrapy.http import Request, Response\n from scrapy.exceptions import NotConfigured\n from scrapy import signals\n from scrapy.utils.python import to_native_str\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.misc import load_object\n+from scrapy.utils.url import strip_url_credentials\n \n \n LOCAL_SCHEMES = ('about', 'blob', 'data', 'filesystem',)\n@@ -31,14 +30,10 @@ class ReferrerPolicy(object):\n         raise NotImplementedError()\n \n     def stripped_referrer(self, req_or_resp):\n-        stripped = self.strip_url(req_or_resp)\n-        if stripped is not None:\n-            return urlunparse(stripped)\n+        return self.strip_url(req_or_resp)\n \n     def origin_referrer(self, req_or_resp):\n-        stripped = self.strip_url(req_or_resp, origin_only=True)\n-        if stripped is not None:\n-            return urlunparse(stripped)\n+        return self.strip_url(req_or_resp, origin_only=True)\n \n     def strip_url(self, req_or_resp, origin_only=False):\n         \"\"\"\n@@ -56,33 +51,13 @@ class ReferrerPolicy(object):\n         \"\"\"\n         if req_or_resp.url is None or not req_or_resp.url:\n             return None\n-        parsed = urlparse_cached(req_or_resp)\n-\n-        if parsed.scheme in self.NOREFERRER_SCHEMES:\n-            return None\n-\n-        netloc = parsed.netloc\n-        # strip username and password if present\n-        if parsed.username or parsed.password:\n-            netloc = netloc.replace('{p.username}:{p.password}@'.format(p=parsed), '')\n-\n-        # strip standard protocol numbers\n-        # Note: strictly speaking, standard port numbers should only be\n-        # stripped when comparing origins\n-        if parsed.port:\n-            if (parsed.scheme, parsed.port) in (('http', 80), ('https', 443)):\n-                netloc = netloc.replace(':{p.port}'.format(p=parsed), '')\n-\n-        return ParseResult(parsed.scheme,\n-                           netloc,\n-                           '/' if origin_only else parsed.path,\n-                           '' if origin_only else parsed.params,\n-                           '' if origin_only else parsed.query,\n-                           '')\n+        parsed_url = urlparse_cached(req_or_resp)\n+        if parsed_url.scheme not in self.NOREFERRER_SCHEMES:\n+            return strip_url_credentials(parsed_url, origin_only=origin_only)\n \n     def origin(self, req_or_resp):\n-        \"\"\"Return (scheme, host, path) tuple for a request or response URL.\"\"\"\n-        return tuple(self.strip_url(req_or_resp, origin_only=True)[:3])\n+        \"\"\"Return serialized origin (scheme, host, path) for a request or response URL.\"\"\"\n+        return self.strip_url(req_or_resp, origin_only=True)\n \n \n class NoReferrerPolicy(ReferrerPolicy):\n@@ -178,7 +153,7 @@ class OriginWhenCrossOriginPolicy(ReferrerPolicy):\n         if origin == self.origin(request):\n             return self.stripped_referrer(response)\n         else:\n-            return urlunparse(origin + ('', '', ''))\n+            return origin\n \n \n class UnsafeUrlPolicy(ReferrerPolicy):\n\n@@ -7,7 +7,7 @@ to the w3lib.url module. Always import those from there instead.\n \"\"\"\n import posixpath\n import re\n-from six.moves.urllib.parse import (ParseResult, urldefrag, urlparse)\n+from six.moves.urllib.parse import (ParseResult, urldefrag, urlparse, urlunparse)\n \n # scrapy.utils.url was moved to w3lib.url and import * ensures this\n # move doesn't break old code\n@@ -103,3 +103,35 @@ def guess_scheme(url):\n         return any_to_uri(url)\n     else:\n         return add_http_if_no_scheme(url)\n+\n+\n+def strip_url_credentials(url, origin_only=False, keep_fragments=False):\n+\n+    if url is None:\n+        return None\n+\n+    if not isinstance(url, ParseResult):\n+        parsed_url = urlparse(url)\n+    else:\n+        parsed_url = url\n+\n+    netloc = parsed_url.netloc\n+    # strip username and password if present\n+    if parsed_url.username or parsed_url.password:\n+        netloc = netloc.split('@')[-1]\n+\n+    # strip standard protocol numbers\n+    # Note: strictly speaking, standard port numbers should only be\n+    # stripped when comparing origins\n+    if parsed_url.port:\n+        if (parsed_url.scheme, parsed_url.port) in (('http', 80), ('https', 443)):\n+            netloc = netloc.replace(':{p.port}'.format(p=parsed_url), '')\n+\n+    return urlunparse((\n+        parsed_url.scheme,\n+        netloc,\n+        '/' if origin_only else parsed_url.path,\n+        '' if origin_only else parsed_url.params,\n+        '' if origin_only else parsed_url.query,\n+        '' if not keep_fragments else parsed_url.fragment\n+    ))\n\n@@ -6,7 +6,8 @@ from six.moves.urllib.parse import urlparse\n \n from scrapy.spiders import Spider\n from scrapy.utils.url import (url_is_from_any_domain, url_is_from_spider,\n-                              add_http_if_no_scheme, guess_scheme, parse_url)\n+                              add_http_if_no_scheme, guess_scheme,\n+                              parse_url, strip_url_credentials)\n \n __doctests__ = ['scrapy.utils.url']\n \n@@ -241,5 +242,94 @@ for k, args in enumerate ([\n     setattr (GuessSchemeTest, t_method.__name__, t_method)\n \n \n+class StripUrlCredentials(unittest.TestCase):\n+\n+    def test_noop(self):\n+        self.assertEqual(strip_url_credentials(\n+            'http://www.example.com/index.html'),\n+            'http://www.example.com/index.html')\n+\n+    def test_noop_query_string(self):\n+        self.assertEqual(strip_url_credentials(\n+            'http://www.example.com/index.html?somekey=somevalue'),\n+            'http://www.example.com/index.html?somekey=somevalue')\n+\n+    def test_fragments(self):\n+        self.assertEqual(strip_url_credentials(\n+            'http://www.example.com/index.html?somekey=somevalue#section', keep_fragments=True),\n+            'http://www.example.com/index.html?somekey=somevalue#section')\n+\n+    def test_noop_trailing_path(self):\n+        self.assertEqual(strip_url_credentials(\n+            'http://www.example.com/'),\n+            'http://www.example.com/')\n+\n+    def test_noop_trailing_path2(self):\n+        self.assertEqual(strip_url_credentials(\n+            'http://www.example.com'),\n+            'http://www.example.com')\n+\n+    def test_trailing_path_origin(self):\n+        self.assertEqual(strip_url_credentials(\n+            'http://www.example.com', origin_only=True),\n+            'http://www.example.com/')\n+\n+    def test_username(self):\n+        # username is stripped (and fragment too)\n+        self.assertEqual(strip_url_credentials(\n+            'http://username@www.example.com/index.html?somekey=somevalue#section'),\n+            'http://www.example.com/index.html?somekey=somevalue')\n+\n+    def test_username_empty_pass(self):\n+        # same as above\n+        self.assertEqual(strip_url_credentials(\n+            'https://username:@www.example.com/index.html?somekey=somevalue#section'),\n+            'https://www.example.com/index.html?somekey=somevalue')\n+\n+    def test_username_password(self):\n+        self.assertEqual(strip_url_credentials(\n+            'ftp://username:password@www.example.com/index.html?somekey=somevalue#section'),\n+            'ftp://www.example.com/index.html?somekey=somevalue')\n+\n+    def test_default_http_port(self):\n+        self.assertEqual(strip_url_credentials(\n+            'http://username:password@www.example.com:80/index.html'),\n+            'http://www.example.com/index.html')\n+\n+    def test_non_default_http_port(self):\n+        self.assertEqual(strip_url_credentials(\n+            'http://username:password@www.example.com:8080/index.html'),\n+            'http://www.example.com:8080/index.html')\n+\n+    def test_default_https_port(self):\n+        self.assertEqual(strip_url_credentials(\n+            'https://username:password@www.example.com:443/index.html'),\n+            'https://www.example.com/index.html')\n+\n+    def test_non_default_https_port(self):\n+        self.assertEqual(strip_url_credentials(\n+            'https://username:password@www.example.com:442/index.html'),\n+            'https://www.example.com:442/index.html')\n+\n+    def test_origin_only(self):\n+        self.assertEqual(strip_url_credentials(\n+            'http://username:password@www.example.com/index.html', origin_only=True),\n+            'http://www.example.com/')\n+\n+    def test_default_http_port_origin_only(self):\n+        self.assertEqual(strip_url_credentials(\n+            'http://username:password@www.example.com:80/index.html', origin_only=True),\n+            'http://www.example.com/')\n+\n+    def test_non_default_http_port_origin_only(self):\n+        self.assertEqual(strip_url_credentials(\n+            'http://username:password@www.example.com:8008/index.html', origin_only=True),\n+            'http://www.example.com:8008/')\n+\n+    def test_default_https_port_origin_only(self):\n+        self.assertEqual(strip_url_credentials(\n+            'https://username:password@www.example.com:443/index.html', origin_only=True),\n+            'https://www.example.com/')\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8864d0e8c19d0ddb52175be1952a4013b13cd86f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 146 | Lines Deleted: 74 | Files Changed: 3 | Hunks: 32 | Methods Changed: 25 | Complexity Δ (Sum/Max): 1/3 | Churn Δ: 220 | Churn Cumulative: 2515 | Contributors (this commit): 17 | Commits (past 90d): 22 | Contributors (cumulative): 24 | DMM Complexity: 0.9607843137254902\n\nDIFF:\n@@ -8,7 +8,7 @@ from scrapy import signals\n from scrapy.utils.python import to_native_str\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.misc import load_object\n-from scrapy.utils.url import strip_url_credentials\n+from scrapy.utils.url import strip_url\n \n \n LOCAL_SCHEMES = ('about', 'blob', 'data', 'filesystem',)\n@@ -53,7 +53,11 @@ class ReferrerPolicy(object):\n             return None\n         parsed_url = urlparse_cached(req_or_resp)\n         if parsed_url.scheme not in self.NOREFERRER_SCHEMES:\n-            return strip_url_credentials(parsed_url, origin_only=origin_only)\n+            return strip_url(parsed_url,\n+                             strip_credentials=True,\n+                             strip_fragment=True,\n+                             strip_default_port=True,\n+                             origin_only=origin_only)\n \n     def origin(self, req_or_resp):\n         \"\"\"Return serialized origin (scheme, host, path) for a request or response URL.\"\"\"\n\n@@ -105,33 +105,37 @@ def guess_scheme(url):\n         return add_http_if_no_scheme(url)\n \n \n-def strip_url_credentials(url, origin_only=False, keep_fragments=False):\n+def strip_url(url, strip_credentials=True, strip_default_port=True, origin_only=False, strip_fragment=True):\n+\n+    \"\"\"Strip URL string from some of its components:\n+\n+    - `strip_credentials` removes \"user:password@\"\n+    - `strip_default_port` removes \":80\" (resp. \":443\", \":21\")\n+      from http:// (resp. https://, ftp://) URLs\n+    - `origin_only` replaces path component with \"/\", also dropping\n+      query and fragment components ; it also strips credentials\n+    - `strip_fragment` drops any #fragment component\n+    \"\"\"\n \n     if url is None:\n         return None\n-\n     if not isinstance(url, ParseResult):\n         parsed_url = urlparse(url)\n     else:\n         parsed_url = url\n-\n     netloc = parsed_url.netloc\n-    # strip username and password if present\n-    if parsed_url.username or parsed_url.password:\n+    if (strip_credentials or origin_only) and (parsed_url.username or parsed_url.password):\n         netloc = netloc.split('@')[-1]\n-\n-    # strip standard protocol numbers\n-    # Note: strictly speaking, standard port numbers should only be\n-    # stripped when comparing origins\n-    if parsed_url.port:\n-        if (parsed_url.scheme, parsed_url.port) in (('http', 80), ('https', 443)):\n+    if strip_default_port and parsed_url.port:\n+        if (parsed_url.scheme, parsed_url.port) in (('http', 80),\n+                                                    ('https', 443),\n+                                                    ('ftp', 21)):\n             netloc = netloc.replace(':{p.port}'.format(p=parsed_url), '')\n-\n     return urlunparse((\n         parsed_url.scheme,\n         netloc,\n         '/' if origin_only else parsed_url.path,\n         '' if origin_only else parsed_url.params,\n         '' if origin_only else parsed_url.query,\n-        '' if not keep_fragments else parsed_url.fragment\n+        '' if strip_fragment else parsed_url.fragment\n     ))\n\n@@ -7,7 +7,7 @@ from six.moves.urllib.parse import urlparse\n from scrapy.spiders import Spider\n from scrapy.utils.url import (url_is_from_any_domain, url_is_from_spider,\n                               add_http_if_no_scheme, guess_scheme,\n-                              parse_url, strip_url_credentials)\n+                              parse_url, strip_url)\n \n __doctests__ = ['scrapy.utils.url']\n \n@@ -242,94 +242,158 @@ for k, args in enumerate ([\n     setattr (GuessSchemeTest, t_method.__name__, t_method)\n \n \n-class StripUrlCredentials(unittest.TestCase):\n+class StripUrl(unittest.TestCase):\n \n     def test_noop(self):\n-        self.assertEqual(strip_url_credentials(\n+        self.assertEqual(strip_url(\n             'http://www.example.com/index.html'),\n             'http://www.example.com/index.html')\n \n     def test_noop_query_string(self):\n-        self.assertEqual(strip_url_credentials(\n+        self.assertEqual(strip_url(\n             'http://www.example.com/index.html?somekey=somevalue'),\n             'http://www.example.com/index.html?somekey=somevalue')\n \n     def test_fragments(self):\n-        self.assertEqual(strip_url_credentials(\n-            'http://www.example.com/index.html?somekey=somevalue#section', keep_fragments=True),\n+        self.assertEqual(strip_url(\n+            'http://www.example.com/index.html?somekey=somevalue#section', strip_fragment=False),\n             'http://www.example.com/index.html?somekey=somevalue#section')\n \n-    def test_noop_trailing_path(self):\n-        self.assertEqual(strip_url_credentials(\n+    def test_path(self):\n+        for input_url, origin, output_url in [\n+            ('http://www.example.com/',\n+             False,\n              'http://www.example.com/'),\n-            'http://www.example.com/')\n \n-    def test_noop_trailing_path2(self):\n-        self.assertEqual(strip_url_credentials(\n+            ('http://www.example.com',\n+             False,\n              'http://www.example.com'),\n-            'http://www.example.com')\n \n-    def test_trailing_path_origin(self):\n-        self.assertEqual(strip_url_credentials(\n-            'http://www.example.com', origin_only=True),\n-            'http://www.example.com/')\n+            ('http://www.example.com',\n+             True,\n+             'http://www.example.com/'),\n+            ]:\n+            self.assertEqual(strip_url(input_url, origin_only=origin), output_url)\n+            self.assertEqual(strip_url(urlparse(input_url), origin_only=origin), output_url)\n \n-    def test_username(self):\n-        # username is stripped (and fragment too)\n-        self.assertEqual(strip_url_credentials(\n-            'http://username@www.example.com/index.html?somekey=somevalue#section'),\n-            'http://www.example.com/index.html?somekey=somevalue')\n+    def test_credentials(self):\n+        for i, o in [\n+            ('http://username@www.example.com/index.html?somekey=somevalue#section',\n+             'http://www.example.com/index.html?somekey=somevalue'),\n \n-    def test_username_empty_pass(self):\n-        # same as above\n-        self.assertEqual(strip_url_credentials(\n-            'https://username:@www.example.com/index.html?somekey=somevalue#section'),\n-            'https://www.example.com/index.html?somekey=somevalue')\n+            ('https://username:@www.example.com/index.html?somekey=somevalue#section',\n+             'https://www.example.com/index.html?somekey=somevalue'),\n \n-    def test_username_password(self):\n-        self.assertEqual(strip_url_credentials(\n-            'ftp://username:password@www.example.com/index.html?somekey=somevalue#section'),\n-            'ftp://www.example.com/index.html?somekey=somevalue')\n+            ('ftp://username:password@www.example.com/index.html?somekey=somevalue#section',\n+             'ftp://www.example.com/index.html?somekey=somevalue'),\n+            ]:\n+            self.assertEqual(strip_url(i, strip_credentials=True), o)\n+            self.assertEqual(strip_url(urlparse(i), strip_credentials=True), o)\n \n-    def test_default_http_port(self):\n-        self.assertEqual(strip_url_credentials(\n-            'http://username:password@www.example.com:80/index.html'),\n-            'http://www.example.com/index.html')\n+    def test_default_ports_creds_off(self):\n+        for i, o in [\n+            ('http://username:password@www.example.com:80/index.html?somekey=somevalue#section',\n+             'http://www.example.com/index.html?somekey=somevalue'),\n \n-    def test_non_default_http_port(self):\n-        self.assertEqual(strip_url_credentials(\n+            ('http://username:password@www.example.com:8080/index.html#section',\n+             'http://www.example.com:8080/index.html'),\n+\n+            ('http://username:password@www.example.com:443/index.html?somekey=somevalue&someotherkey=sov#section',\n+             'http://www.example.com:443/index.html?somekey=somevalue&someotherkey=sov'),\n+\n+            ('https://username:password@www.example.com:443/index.html',\n+             'https://www.example.com/index.html'),\n+\n+            ('https://username:password@www.example.com:442/index.html',\n+             'https://www.example.com:442/index.html'),\n+\n+            ('https://username:password@www.example.com:80/index.html',\n+             'https://www.example.com:80/index.html'),\n+\n+            ('ftp://username:password@www.example.com:21/file.txt',\n+             'ftp://www.example.com/file.txt'),\n+\n+            ('ftp://username:password@www.example.com:221/file.txt',\n+             'ftp://www.example.com:221/file.txt'),\n+            ]:\n+            self.assertEqual(strip_url(i), o)\n+            self.assertEqual(strip_url(urlparse(i)), o)\n+\n+    def test_default_ports(self):\n+        for i, o in [\n+            ('http://username:password@www.example.com:80/index.html',\n+             'http://username:password@www.example.com/index.html'),\n+\n+            ('http://username:password@www.example.com:8080/index.html',\n              'http://username:password@www.example.com:8080/index.html'),\n-            'http://www.example.com:8080/index.html')\n \n-    def test_default_https_port(self):\n-        self.assertEqual(strip_url_credentials(\n-            'https://username:password@www.example.com:443/index.html'),\n-            'https://www.example.com/index.html')\n+            ('http://username:password@www.example.com:443/index.html',\n+             'http://username:password@www.example.com:443/index.html'),\n \n-    def test_non_default_https_port(self):\n-        self.assertEqual(strip_url_credentials(\n+            ('https://username:password@www.example.com:443/index.html',\n+             'https://username:password@www.example.com/index.html'),\n+\n+            ('https://username:password@www.example.com:442/index.html',\n              'https://username:password@www.example.com:442/index.html'),\n-            'https://www.example.com:442/index.html')\n+\n+            ('https://username:password@www.example.com:80/index.html',\n+             'https://username:password@www.example.com:80/index.html'),\n+\n+            ('ftp://username:password@www.example.com:21/file.txt',\n+             'ftp://username:password@www.example.com/file.txt'),\n+\n+            ('ftp://username:password@www.example.com:221/file.txt',\n+             'ftp://username:password@www.example.com:221/file.txt'),\n+            ]:\n+            self.assertEqual(strip_url(i, strip_default_port=True, strip_credentials=False), o)\n+            self.assertEqual(strip_url(urlparse(i), strip_default_port=True, strip_credentials=False), o)\n+\n+    def test_default_ports_keep(self):\n+        for i, o in [\n+            ('http://username:password@www.example.com:80/index.html?somekey=somevalue&someotherkey=sov#section',\n+             'http://username:password@www.example.com:80/index.html?somekey=somevalue&someotherkey=sov'),\n+\n+            ('http://username:password@www.example.com:8080/index.html?somekey=somevalue&someotherkey=sov#section',\n+             'http://username:password@www.example.com:8080/index.html?somekey=somevalue&someotherkey=sov'),\n+\n+            ('http://username:password@www.example.com:443/index.html',\n+             'http://username:password@www.example.com:443/index.html'),\n+\n+            ('https://username:password@www.example.com:443/index.html',\n+             'https://username:password@www.example.com:443/index.html'),\n+\n+            ('https://username:password@www.example.com:442/index.html',\n+             'https://username:password@www.example.com:442/index.html'),\n+\n+            ('https://username:password@www.example.com:80/index.html',\n+             'https://username:password@www.example.com:80/index.html'),\n+\n+            ('ftp://username:password@www.example.com:21/file.txt',\n+             'ftp://username:password@www.example.com:21/file.txt'),\n+\n+            ('ftp://username:password@www.example.com:221/file.txt',\n+             'ftp://username:password@www.example.com:221/file.txt'),\n+            ]:\n+            self.assertEqual(strip_url(i, strip_default_port=False, strip_credentials=False), o)\n+            self.assertEqual(strip_url(urlparse(i), strip_default_port=False, strip_credentials=False), o)\n \n     def test_origin_only(self):\n-        self.assertEqual(strip_url_credentials(\n-            'http://username:password@www.example.com/index.html', origin_only=True),\n-            'http://www.example.com/')\n+        for i, o in [\n+            ('http://username:password@www.example.com/index.html',\n+             'http://www.example.com/'),\n \n-    def test_default_http_port_origin_only(self):\n-        self.assertEqual(strip_url_credentials(\n-            'http://username:password@www.example.com:80/index.html', origin_only=True),\n-            'http://www.example.com/')\n+            ('http://username:password@www.example.com:80/foo/bar?query=value#somefrag',\n+             'http://www.example.com/'),\n \n-    def test_non_default_http_port_origin_only(self):\n-        self.assertEqual(strip_url_credentials(\n-            'http://username:password@www.example.com:8008/index.html', origin_only=True),\n-            'http://www.example.com:8008/')\n+            ('http://username:password@www.example.com:8008/foo/bar?query=value#somefrag',\n+             'http://www.example.com:8008/'),\n+\n+            ('https://username:password@www.example.com:443/index.html',\n+             'https://www.example.com/'),\n+            ]:\n+            self.assertEqual(strip_url(i, origin_only=True), o)\n+            self.assertEqual(strip_url(urlparse(i), origin_only=True), o)\n \n-    def test_default_https_port_origin_only(self):\n-        self.assertEqual(strip_url_credentials(\n-            'https://username:password@www.example.com:443/index.html', origin_only=True),\n-            'https://www.example.com/')\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
