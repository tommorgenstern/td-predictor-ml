{"custom_id": "keras#6e302074f141ae12e3d4b58928f60f53f091564a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 515 | Lines Deleted: 9 | Files Changed: 4 | Hunks: 6 | Methods Changed: 23 | Complexity Δ (Sum/Max): 23/24 | Churn Δ: 524 | Churn Cumulative: 3663 | Contributors (this commit): 10 | Commits (past 90d): 34 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,508 @@\n+\"\"\"\n+Title: Low-light image enhancement using MIRNet\n+Author: [Soumik Rakshit](http://github.com/soumik12345)\n+Converted to Keras Core by: [Soumik Rakshit](http://github.com/soumik12345)\n+Date created: 2021/09/11\n+Last modified: 2023/07/15\n+Description: Implementing the MIRNet architecture for low-light image enhancement.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+With the goal of recovering high-quality image content from its degraded version, image\n+restoration enjoys numerous applications, such as in\n+photography, security, medical imaging, and remote sensing. In this example, we implement the\n+**MIRNet** model for low-light image enhancement, a fully-convolutional architecture that\n+learns an enriched set of\n+features that combines contextual information from multiple scales, while\n+simultaneously preserving the high-resolution spatial details.\n+\n+### References:\n+\n+- [Learning Enriched Features for Real Image Restoration and Enhancement](https://arxiv.org/abs/2003.06792)\n+- [The Retinex Theory of Color Vision](http://www.cnbc.cmu.edu/~tai/cp_papers/E.Land_Retinex_Theory_ScientifcAmerican.pdf)\n+- [Two deterministic half-quadratic regularization algorithms for computed imaging](https://ieeexplore.ieee.org/document/413553)\n+\"\"\"\n+\n+\"\"\"\n+## Downloading LOLDataset\n+\n+The **LoL Dataset** has been created for low-light image enhancement.\n+It provides 485 images for training and 15 for testing. Each image pair in the dataset\n+consists of a low-light input image and its corresponding well-exposed reference image.\n+\"\"\"\n+\n+\"\"\"shell\n+pip install -q git+https://github.com/keras-team/keras-core\n+\"\"\"\n+\n+import os\n+\n+os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n+\n+import random\n+import numpy as np\n+from glob import glob\n+from PIL import Image, ImageOps\n+import matplotlib.pyplot as plt\n+\n+import keras_core as keras\n+from keras_core import layers\n+\n+import tensorflow as tf\n+\n+\"\"\"shell\n+wget https://huggingface.co/datasets/geekyrakshit/LoL-Dataset/resolve/main/lol_dataset.zip\n+unzip -q lol_dataset.zip && rm lol_dataset.zip\n+\"\"\"\n+\n+\"\"\"\n+## Creating a TensorFlow Dataset\n+\n+We use 300 image pairs from the LoL Dataset's training set for training,\n+and we use the remaining 185 image pairs for validation.\n+We generate random crops of size `128 x 128` from the image pairs to be\n+used for both training and validation.\n+\"\"\"\n+\n+random.seed(10)\n+\n+IMAGE_SIZE = 128\n+BATCH_SIZE = 4\n+MAX_TRAIN_IMAGES = 300\n+\n+\n+def read_image(image_path):\n+    image = tf.io.read_file(image_path)\n+    image = tf.image.decode_png(image, channels=3)\n+    image.set_shape([None, None, 3])\n+    image = tf.cast(image, dtype=tf.float32) / 255.0\n+    return image\n+\n+\n+def random_crop(low_image, enhanced_image):\n+    low_image_shape = tf.shape(low_image)[:2]\n+    low_w = tf.random.uniform(\n+        shape=(), maxval=low_image_shape[1] - IMAGE_SIZE + 1, dtype=tf.int32\n+    )\n+    low_h = tf.random.uniform(\n+        shape=(), maxval=low_image_shape[0] - IMAGE_SIZE + 1, dtype=tf.int32\n+    )\n+    low_image_cropped = low_image[\n+        low_h : low_h + IMAGE_SIZE, low_w : low_w + IMAGE_SIZE\n+    ]\n+    enhanced_image_cropped = enhanced_image[\n+        low_h : low_h + IMAGE_SIZE, low_w : low_w + IMAGE_SIZE\n+    ]\n+    # in order to avoid `NONE` during shape inference\n+    low_image_cropped.set_shape([IMAGE_SIZE, IMAGE_SIZE, 3])\n+    enhanced_image_cropped.set_shape([IMAGE_SIZE, IMAGE_SIZE, 3])\n+    return low_image_cropped, enhanced_image_cropped\n+\n+\n+def load_data(low_light_image_path, enhanced_image_path):\n+    low_light_image = read_image(low_light_image_path)\n+    enhanced_image = read_image(enhanced_image_path)\n+    low_light_image, enhanced_image = random_crop(low_light_image, enhanced_image)\n+    return low_light_image, enhanced_image\n+\n+\n+def get_dataset(low_light_images, enhanced_images):\n+    dataset = tf.data.Dataset.from_tensor_slices((low_light_images, enhanced_images))\n+    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n+    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n+    return dataset\n+\n+\n+train_low_light_images = sorted(glob(\"./lol_dataset/our485/low/*\"))[:MAX_TRAIN_IMAGES]\n+train_enhanced_images = sorted(glob(\"./lol_dataset/our485/high/*\"))[:MAX_TRAIN_IMAGES]\n+\n+val_low_light_images = sorted(glob(\"./lol_dataset/our485/low/*\"))[MAX_TRAIN_IMAGES:]\n+val_enhanced_images = sorted(glob(\"./lol_dataset/our485/high/*\"))[MAX_TRAIN_IMAGES:]\n+\n+test_low_light_images = sorted(glob(\"./lol_dataset/eval15/low/*\"))\n+test_enhanced_images = sorted(glob(\"./lol_dataset/eval15/high/*\"))\n+\n+\n+train_dataset = get_dataset(train_low_light_images, train_enhanced_images)\n+val_dataset = get_dataset(val_low_light_images, val_enhanced_images)\n+\n+\n+print(\"Train Dataset:\", train_dataset.element_spec)\n+print(\"Val Dataset:\", val_dataset.element_spec)\n+\n+\"\"\"\n+## MIRNet Model\n+\n+Here are the main features of the MIRNet model:\n+\n+- A feature extraction model that computes a complementary set of features across multiple\n+spatial scales, while maintaining the original high-resolution features to preserve\n+precise spatial details.\n+- A regularly repeated mechanism for information exchange, where the features across\n+multi-resolution branches are progressively fused together for improved representation\n+learning.\n+- A new approach to fuse multi-scale features using a selective kernel network\n+that dynamically combines variable receptive fields and faithfully preserves\n+the original feature information at each spatial resolution.\n+- A recursive residual design that progressively breaks down the input signal\n+in order to simplify the overall learning process, and allows the construction\n+of very deep networks.\n+\n+\n+![](https://raw.githubusercontent.com/soumik12345/MIRNet/master/assets/mirnet_architecture.png)\n+\"\"\"\n+\n+\"\"\"\n+### Selective Kernel Feature Fusion\n+\n+The Selective Kernel Feature Fusion or SKFF module performs dynamic adjustment of\n+receptive fields via two operations: **Fuse** and **Select**. The Fuse operator generates\n+global feature descriptors by combining the information from multi-resolution streams.\n+The Select operator uses these descriptors to recalibrate the feature maps (of different\n+streams) followed by their aggregation.\n+\n+**Fuse**: The SKFF receives inputs from three parallel convolution streams carrying\n+different scales of information. We first combine these multi-scale features using an\n+element-wise sum, on which we apply Global Average Pooling (GAP) across the spatial\n+dimension. Next, we apply a channel- downscaling convolution layer to generate a compact\n+feature representation which passes through three parallel channel-upscaling convolution\n+layers (one for each resolution stream) and provides us with three feature descriptors.\n+\n+**Select**: This operator applies the softmax function to the feature descriptors to\n+obtain the corresponding activations that are used to adaptively recalibrate multi-scale\n+feature maps. The aggregated features are defined as the sum of product of the corresponding\n+multi-scale feature and the feature descriptor.\n+\n+![](https://i.imgur.com/7U6ixF6.png)\n+\"\"\"\n+\n+\n+def selective_kernel_feature_fusion(\n+    multi_scale_feature_1, multi_scale_feature_2, multi_scale_feature_3\n+):\n+    channels = list(multi_scale_feature_1.shape)[-1]\n+    combined_feature = layers.Add()(\n+        [multi_scale_feature_1, multi_scale_feature_2, multi_scale_feature_3]\n+    )\n+    gap = layers.GlobalAveragePooling2D()(combined_feature)\n+    channel_wise_statistics = layers.Reshape((1, 1, channels))(gap)\n+    compact_feature_representation = layers.Conv2D(\n+        filters=channels // 8, kernel_size=(1, 1), activation=\"relu\"\n+    )(channel_wise_statistics)\n+    feature_descriptor_1 = layers.Conv2D(\n+        channels, kernel_size=(1, 1), activation=\"softmax\"\n+    )(compact_feature_representation)\n+    feature_descriptor_2 = layers.Conv2D(\n+        channels, kernel_size=(1, 1), activation=\"softmax\"\n+    )(compact_feature_representation)\n+    feature_descriptor_3 = layers.Conv2D(\n+        channels, kernel_size=(1, 1), activation=\"softmax\"\n+    )(compact_feature_representation)\n+    feature_1 = multi_scale_feature_1 * feature_descriptor_1\n+    feature_2 = multi_scale_feature_2 * feature_descriptor_2\n+    feature_3 = multi_scale_feature_3 * feature_descriptor_3\n+    aggregated_feature = layers.Add()([feature_1, feature_2, feature_3])\n+    return aggregated_feature\n+\n+\n+\"\"\"\n+### Dual Attention Unit\n+\n+The Dual Attention Unit or DAU is used to extract features in the convolutional streams.\n+While the SKFF block fuses information across multi-resolution branches, we also need a\n+mechanism to share information within a feature tensor, both along the spatial and the\n+channel dimensions which is done by the DAU block. The DAU suppresses less useful\n+features and only allows more informative ones to pass further. This feature\n+recalibration is achieved by using **Channel Attention** and **Spatial Attention**\n+mechanisms.\n+\n+The **Channel Attention** branch exploits the inter-channel relationships of the\n+convolutional feature maps by applying squeeze and excitation operations. Given a feature\n+map, the squeeze operation applies Global Average Pooling across spatial dimensions to\n+encode global context, thus yielding a feature descriptor. The excitation operator passes\n+this feature descriptor through two convolutional layers followed by the sigmoid gating\n+and generates activations. Finally, the output of Channel Attention branch is obtained by\n+rescaling the input feature map with the output activations.\n+\n+The **Spatial Attention** branch is designed to exploit the inter-spatial dependencies of\n+convolutional features. The goal of Spatial Attention is to generate a spatial attention\n+map and use it to recalibrate the incoming features. To generate the spatial attention\n+map, the Spatial Attention branch first independently applies Global Average Pooling and\n+Max Pooling operations on input features along the channel dimensions and concatenates\n+the outputs to form a resultant feature map which is then passed through a convolution\n+and sigmoid activation to obtain the spatial attention map. This spatial attention map is\n+then used to rescale the input feature map.\n+\n+![](https://i.imgur.com/Dl0IwQs.png)\n+\"\"\"\n+\n+class ChannelPooling(layers.Layer):\n+    def __init__(self, axis=-1, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.axis = axis\n+        self.concat = layers.Concatenate(axis=self.axis)\n+    \n+    def call(self, inputs):\n+        average_pooling = tf.expand_dims(\n+            tf.reduce_mean(inputs, axis=-1), axis=-1\n+        )\n+        max_pooling = tf.expand_dims(\n+            tf.reduce_max(inputs, axis=-1), axis=-1\n+        )\n+        return self.concat([average_pooling, max_pooling])\n+    \n+    def get_config(self):\n+        config = super().get_config()\n+        config.update({\"axis\": self.axis})\n+\n+\n+def spatial_attention_block(input_tensor):\n+    compressed_feature_map = ChannelPooling(axis=-1)(input_tensor)\n+    feature_map = layers.Conv2D(1, kernel_size=(1, 1))(compressed_feature_map)\n+    feature_map = keras.activations.sigmoid(feature_map)\n+    return input_tensor * feature_map\n+\n+\n+def channel_attention_block(input_tensor):\n+    channels = list(input_tensor.shape)[-1]\n+    average_pooling = layers.GlobalAveragePooling2D()(input_tensor)\n+    feature_descriptor = layers.Reshape((1, 1, channels))(average_pooling)\n+    feature_activations = layers.Conv2D(\n+        filters=channels // 8, kernel_size=(1, 1), activation=\"relu\"\n+    )(feature_descriptor)\n+    feature_activations = layers.Conv2D(\n+        filters=channels, kernel_size=(1, 1), activation=\"sigmoid\"\n+    )(feature_activations)\n+    return input_tensor * feature_activations\n+\n+\n+def dual_attention_unit_block(input_tensor):\n+    channels = list(input_tensor.shape)[-1]\n+    feature_map = layers.Conv2D(\n+        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n+    )(input_tensor)\n+    feature_map = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(\n+        feature_map\n+    )\n+    channel_attention = channel_attention_block(feature_map)\n+    spatial_attention = spatial_attention_block(feature_map)\n+    concatenation = layers.Concatenate(axis=-1)([channel_attention, spatial_attention])\n+    concatenation = layers.Conv2D(channels, kernel_size=(1, 1))(concatenation)\n+    return layers.Add()([input_tensor, concatenation])\n+\n+\n+\"\"\"\n+### Multi-Scale Residual Block\n+\n+The Multi-Scale Residual Block is capable of generating a spatially-precise output by\n+maintaining high-resolution representations, while receiving rich contextual information\n+from low-resolutions. The MRB consists of multiple (three in this paper)\n+fully-convolutional streams connected in parallel. It allows information exchange across\n+parallel streams in order to consolidate the high-resolution features with the help of\n+low-resolution features, and vice versa. The MIRNet employs a recursive residual design\n+(with skip connections) to ease the flow of information during the learning process. In\n+order to maintain the residual nature of our architecture, residual resizing modules are\n+used to perform downsampling and upsampling operations that are used in the Multi-scale\n+Residual Block.\n+\n+![](https://i.imgur.com/wzZKV57.png)\n+\"\"\"\n+\n+# Recursive Residual Modules\n+\n+\n+def down_sampling_module(input_tensor):\n+    channels = list(input_tensor.shape)[-1]\n+    main_branch = layers.Conv2D(channels, kernel_size=(1, 1), activation=\"relu\")(\n+        input_tensor\n+    )\n+    main_branch = layers.Conv2D(\n+        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n+    )(main_branch)\n+    main_branch = layers.MaxPooling2D()(main_branch)\n+    main_branch = layers.Conv2D(channels * 2, kernel_size=(1, 1))(main_branch)\n+    skip_branch = layers.MaxPooling2D()(input_tensor)\n+    skip_branch = layers.Conv2D(channels * 2, kernel_size=(1, 1))(skip_branch)\n+    return layers.Add()([skip_branch, main_branch])\n+\n+\n+def up_sampling_module(input_tensor):\n+    channels = list(input_tensor.shape)[-1]\n+    main_branch = layers.Conv2D(channels, kernel_size=(1, 1), activation=\"relu\")(\n+        input_tensor\n+    )\n+    main_branch = layers.Conv2D(\n+        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n+    )(main_branch)\n+    main_branch = layers.UpSampling2D()(main_branch)\n+    main_branch = layers.Conv2D(channels // 2, kernel_size=(1, 1))(main_branch)\n+    skip_branch = layers.UpSampling2D()(input_tensor)\n+    skip_branch = layers.Conv2D(channels // 2, kernel_size=(1, 1))(skip_branch)\n+    return layers.Add()([skip_branch, main_branch])\n+\n+\n+# MRB Block\n+def multi_scale_residual_block(input_tensor, channels):\n+    # features\n+    level1 = input_tensor\n+    level2 = down_sampling_module(input_tensor)\n+    level3 = down_sampling_module(level2)\n+    # DAU\n+    level1_dau = dual_attention_unit_block(level1)\n+    level2_dau = dual_attention_unit_block(level2)\n+    level3_dau = dual_attention_unit_block(level3)\n+    # SKFF\n+    level1_skff = selective_kernel_feature_fusion(\n+        level1_dau,\n+        up_sampling_module(level2_dau),\n+        up_sampling_module(up_sampling_module(level3_dau)),\n+    )\n+    level2_skff = selective_kernel_feature_fusion(\n+        down_sampling_module(level1_dau), level2_dau, up_sampling_module(level3_dau)\n+    )\n+    level3_skff = selective_kernel_feature_fusion(\n+        down_sampling_module(down_sampling_module(level1_dau)),\n+        down_sampling_module(level2_dau),\n+        level3_dau,\n+    )\n+    # DAU 2\n+    level1_dau_2 = dual_attention_unit_block(level1_skff)\n+    level2_dau_2 = up_sampling_module((dual_attention_unit_block(level2_skff)))\n+    level3_dau_2 = up_sampling_module(\n+        up_sampling_module(dual_attention_unit_block(level3_skff))\n+    )\n+    # SKFF 2\n+    skff_ = selective_kernel_feature_fusion(level1_dau_2, level2_dau_2, level3_dau_2)\n+    conv = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(skff_)\n+    return layers.Add()([input_tensor, conv])\n+\n+\n+\"\"\"\n+### MIRNet Model\n+\"\"\"\n+\n+\n+def recursive_residual_group(input_tensor, num_mrb, channels):\n+    conv1 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(input_tensor)\n+    for _ in range(num_mrb):\n+        conv1 = multi_scale_residual_block(conv1, channels)\n+    conv2 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(conv1)\n+    return layers.Add()([conv2, input_tensor])\n+\n+\n+def mirnet_model(num_rrg, num_mrb, channels):\n+    input_tensor = keras.Input(shape=[None, None, 3])\n+    x1 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(input_tensor)\n+    for _ in range(num_rrg):\n+        x1 = recursive_residual_group(x1, num_mrb, channels)\n+    conv = layers.Conv2D(3, kernel_size=(3, 3), padding=\"same\")(x1)\n+    output_tensor = layers.Add()([input_tensor, conv])\n+    return keras.Model(input_tensor, output_tensor)\n+\n+\n+model = mirnet_model(num_rrg=3, num_mrb=2, channels=64)\n+\n+\"\"\"\n+## Training\n+\n+- We train MIRNet using **Charbonnier Loss** as the loss function and **Adam\n+Optimizer** with a learning rate of `1e-4`.\n+- We use **Peak Signal Noise Ratio** or PSNR as a metric which is an expression for the\n+ratio between the maximum possible value (power) of a signal and the power of distorting\n+noise that affects the quality of its representation.\n+\"\"\"\n+\n+\n+def charbonnier_loss(y_true, y_pred):\n+    return tf.reduce_mean(tf.sqrt(tf.square(y_true - y_pred) + tf.square(1e-3)))\n+\n+\n+def peak_signal_noise_ratio(y_true, y_pred):\n+    return tf.image.psnr(y_pred, y_true, max_val=255.0)\n+\n+\n+optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n+model.compile(\n+    optimizer=optimizer, loss=charbonnier_loss, metrics=[peak_signal_noise_ratio]\n+)\n+\n+history = model.fit(\n+    train_dataset,\n+    validation_data=val_dataset,\n+    epochs=50,\n+    callbacks=[\n+        keras.callbacks.ReduceLROnPlateau(\n+            monitor=\"val_peak_signal_noise_ratio\",\n+            factor=0.5,\n+            patience=5,\n+            verbose=1,\n+            min_delta=1e-7,\n+            mode=\"max\",\n+        )\n+    ],\n+)\n+\n+def plot_history(value):\n+    plt.plot(history.history[value], label=f\"train_{value}\")\n+    plt.plot(history.history[f\"val_{value}\"], label=f\"val_{value}\")\n+    plt.xlabel(\"Epochs\")\n+    plt.ylabel(value)\n+    plt.title(f\"Train and Validation {value} Over Epochs\", fontsize=14)\n+    plt.legend()\n+    plt.grid()\n+    plt.show()\n+\n+\n+plot_history(\"loss\")\n+plot_history(\"peak_signal_noise_ratio\")\n+\n+\"\"\"\n+## Inference\n+\"\"\"\n+\n+\n+def plot_results(images, titles, figure_size=(12, 12)):\n+    fig = plt.figure(figsize=figure_size)\n+    for i in range(len(images)):\n+        fig.add_subplot(1, len(images), i + 1).set_title(titles[i])\n+        _ = plt.imshow(images[i])\n+        plt.axis(\"off\")\n+    plt.show()\n+\n+\n+def infer(original_image):\n+    image = keras.utils.img_to_array(original_image)\n+    image = image.astype(\"float32\") / 255.0\n+    image = np.expand_dims(image, axis=0)\n+    output = model.predict(image, verbose=0)\n+    output_image = output[0] * 255.0\n+    output_image = output_image.clip(0, 255)\n+    output_image = output_image.reshape(\n+        (np.shape(output_image)[0], np.shape(output_image)[1], 3)\n+    )\n+    output_image = Image.fromarray(np.uint8(output_image))\n+    original_image = Image.fromarray(np.uint8(original_image))\n+    return output_image\n+\n+\n+\"\"\"\n+### Inference on Test Images\n+\n+We compare the test images from LOLDataset enhanced by MIRNet with images\n+enhanced via the `PIL.ImageOps.autocontrast()` function.\n+\n+You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/lowlight-enhance-mirnet)\n+and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/Enhance_Low_Light_Image).\n+\"\"\"\n+\n+\n+for low_light_image in random.sample(test_low_light_images, 6):\n+    original_image = Image.open(low_light_image)\n+    enhanced_image = infer(original_image)\n+    plot_results(\n+        [original_image, ImageOps.autocontrast(original_image), enhanced_image],\n+        [\"Original\", \"PIL Autocontrast\", \"MIRNet Enhanced\"],\n+        (20, 12),\n+    )\n\n\n@@ -467,15 +467,13 @@ def conv_transpose(\n     if isinstance(dilation_rate, int):\n         dilation_rate = [dilation_rate] * len(kernel_spatial_shape)\n     for i, value in enumerate(padding_values):\n-        total_padding = value[0] + value[1]\n+        both_side_padding = value[0]\n+        longer_side_padding = value[1] - value[0]\n         padding_arg.append(\n             dilation_rate[i] * (kernel_spatial_shape[i] - 1)\n-            - total_padding // 2\n+            - both_side_padding,\n         )\n-        if total_padding % 2 == 0:\n-            output_padding_arg.append(0)\n-        else:\n-            output_padding_arg.append(1)\n+        output_padding_arg.append(longer_side_padding)\n \n     if num_spatial_dims == 1:\n         outputs = tnn.conv_transpose1d(\n\n@@ -315,8 +315,8 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         },\n         {\n             \"filters\": 6,\n-            \"kernel_size\": 2,\n-            \"strides\": 3,\n+            \"kernel_size\": 7,\n+            \"strides\": 16,\n             \"padding\": \"same\",\n             \"output_padding\": 2,\n             \"data_format\": \"channels_last\",\n@@ -361,7 +361,7 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             dilation_rate=dilation_rate,\n         )\n \n-        inputs = np.random.normal(size=[2, 8, 8, 4])\n+        inputs = np.random.normal(size=[2, 14, 14, 4])\n         layer.build(input_shape=inputs.shape)\n         tf_keras_layer.build(input_shape=inputs.shape)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#34cea8c95915b78ab2275d0f37892fef172fc204", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 1351 | Contributors (this commit): 4 | Commits (past 90d): 13 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -102,10 +102,21 @@ class ConvTransposeBasicTest(testing.TestCase, parameterized.TestCase):\n             \"strides\": (2, 1),\n             \"padding\": \"valid\",\n             \"output_padding\": None,\n+            \"data_format\": \"channels_first\",\n+            \"dilation_rate\": (1, 1),\n+            \"input_shape\": (2, 4, 8, 8),\n+            \"output_shape\": (2, 6, 16, 10),\n+        },\n+        {\n+            \"filters\": 2,\n+            \"kernel_size\": (7, 7),\n+            \"strides\": (16, 16),\n+            \"padding\": \"valid\",\n+            \"output_padding\": None,\n             \"data_format\": \"channels_last\",\n             \"dilation_rate\": (1, 1),\n-            \"input_shape\": (2, 8, 8, 4),\n-            \"output_shape\": (2, 16, 10, 6),\n+            \"input_shape\": (1, 14, 14, 2),\n+            \"output_shape\": (1, 224, 224, 2),\n         },\n     )\n     def test_conv2d_transpose_basic(\n@@ -331,6 +342,15 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             \"data_format\": \"channels_last\",\n             \"dilation_rate\": (1, 1),\n         },\n+        {\n+            \"filters\": 2,\n+            \"kernel_size\": (7, 7),\n+            \"strides\": (16, 16),\n+            \"padding\": \"valid\",\n+            \"output_padding\": None,\n+            \"data_format\": \"channels_last\",\n+            \"dilation_rate\": (1, 1),\n+        },\n     )\n     def test_conv2d_transpose(\n         self,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#455c15314cb911444d7fbfc8a396b1f7989fb933", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 40 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 41 | Churn Cumulative: 45 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -55,7 +55,46 @@ class Resize(Operation):\n def resize(\n     image, size, method=\"bilinear\", antialias=False, data_format=\"channels_last\"\n ):\n-    # TODO: add docstring\n+    \"\"\"Resize images to size using the specified method.\n+\n+    Args:\n+        image: Input image or batch of images. Must be 3D or 4D.\n+        size: Size of output image in `(height, width)` format.\n+        method: Interpolation method. Available methods are `\"nearest\"`,\n+            `\"bilinear\"`, and `\"bicubic\"`. Defaults to `\"bilinear\"`.\n+        antialias: Whether to use an antialiasing filter when downsampling an\n+            image. Defaults to `False`.\n+        data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n+            The ordering of the dimensions in the inputs. `\"channels_last\"`\n+            corresponds to inputs with shape `(batch, height, width, channels)`\n+            while `\"channels_first\"` corresponds to inputs with shape\n+            `(batch, channels, height, weight)`. It defaults to the\n+            `image_data_format` value found in your Keras config file at\n+            `~/.keras/keras.json`. If you never set it, then it will be\n+            `\"channels_last\"`.\n+\n+    Returns:\n+        Resized image or batch of images.\n+\n+    Examples:\n+\n+    >>> x = np.random.random((2, 4, 4, 3)) # batch of 2 RGB images\n+    >>> y = keras_core.ops.image.resize(x, (2, 2))\n+    >>> y.shape\n+    (2, 2, 2, 3)\n+\n+    >>> x = np.random.random((4, 4, 3)) # single RGB image\n+    >>> y = keras_core.ops.image.resize(x, (2, 2))\n+    >>> y.shape\n+    (2, 2, 3)\n+\n+    >>> x = np.random.random((2, 3, 4, 4)) # batch of 2 RGB images\n+    >>> y = keras_core.ops.image.resize(x, (2, 2),\n+    ...     data_format=\"channels_first\")\n+    >>> y.shape\n+    (2, 3, 2, 2)\n+    \"\"\"\n+\n     if any_symbolic_tensors((image,)):\n         return Resize(\n             size, method=method, antialias=antialias, data_format=data_format\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7c9bda9d2a4dab07d48d8d0f665605da1240da23", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 8 | Churn Cumulative: 1359 | Contributors (this commit): 4 | Commits (past 90d): 14 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,7 +1,9 @@\n import numpy as np\n+import pytest\n import tensorflow as tf\n from absl.testing import parameterized\n \n+from keras_core import backend\n from keras_core import layers\n from keras_core import testing\n \n@@ -131,6 +133,12 @@ class ConvTransposeBasicTest(testing.TestCase, parameterized.TestCase):\n         input_shape,\n         output_shape,\n     ):\n+        if (\n+            data_format == \"channels_first\"\n+            and backend.backend() == \"tensorflow\"\n+        ):\n+            pytest.skip(\"channels_first unsupported on CPU with TF\")\n+\n         self.run_layer_test(\n             layers.Conv2DTranspose,\n             init_kwargs={\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4853a0a6f11278cd28635d0e88f39b3ba5ce93f6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 877 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 18 | Complexity Δ (Sum/Max): 48/48 | Churn Δ: 877 | Churn Cumulative: 877 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.9248366013071896\n\nDIFF:\n@@ -0,0 +1,877 @@\n+\"\"\"\n+Title: A Vision Transformer without Attention\n+Author: [Aritra Roy Gosthipaty](https://twitter.com/ariG23498), [Ritwik Raha](https://twitter.com/ritwik_raha)\n+Converted to Keras Core: [Muhammad Anas Raza](https://anasrz.com)\n+Date created: 2022/02/24\n+Last modified: 2023/07/15\n+Description: A minimal implementation of ShiftViT.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+[Vision Transformers](https://arxiv.org/abs/2010.11929) (ViTs) have sparked a wave of\n+research at the intersection of Transformers and Computer Vision (CV).\n+\n+ViTs can simultaneously model long- and short-range dependencies, thanks to\n+the Multi-Head Self-Attention mechanism in the Transformer block. Many researchers believe\n+that the success of ViTs are purely due to the attention layer, and they seldom\n+think about other parts of the ViT model.\n+\n+In the academic paper\n+[When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism](https://arxiv.org/abs/2201.10801)\n+the authors propose to demystify the success of ViTs with the introduction of a **NO\n+PARAMETER** operation in place of the attention operation. They swap the attention\n+operation with a shifting operation.\n+\n+In this example, we minimally implement the paper with close alignement to the author's\n+[official implementation](https://github.com/microsoft/SPACH/blob/main/models/shiftvit.py).\n+\"\"\"\n+\n+\"\"\"\n+## Setup and imports\n+\"\"\"\n+\n+import os\n+os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n+\n+import numpy as np\n+import matplotlib.pyplot as plt\n+\n+import tensorflow as tf\n+import keras_core as keras\n+from keras_core import layers\n+\n+\n+\n+# Setting seed for reproducibiltiy\n+SEED = 42\n+keras.utils.set_random_seed(SEED)\n+\n+\"\"\"\n+## Hyperparameters\n+\n+These are the hyperparameters that we have chosen for the experiment.\n+Please feel free to tune them.\n+\"\"\"\n+\n+\n+class Config(object):\n+    # DATA\n+    batch_size = 256\n+    buffer_size = batch_size * 2\n+    input_shape = (32, 32, 3)\n+    num_classes = 10\n+\n+    # AUGMENTATION\n+    image_size = 48\n+\n+    # ARCHITECTURE\n+    patch_size = 4\n+    projected_dim = 96\n+    num_shift_blocks_per_stages = [2, 4, 8, 2]\n+    epsilon = 1e-5\n+    stochastic_depth_rate = 0.2\n+    mlp_dropout_rate = 0.2\n+    num_div = 12\n+    shift_pixel = 1\n+    mlp_expand_ratio = 2\n+\n+    # OPTIMIZER\n+    lr_start = 1e-5\n+    lr_max = 1e-3\n+    weight_decay = 1e-4\n+\n+    # TRAINING\n+    epochs = 100\n+\n+\n+config = Config()\n+\n+\"\"\"\n+## Load the CIFAR-10 dataset\n+\n+We use the CIFAR-10 dataset for our experiments.\n+\"\"\"\n+\n+(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n+(x_train, y_train), (x_val, y_val) = (\n+    (x_train[:40000], y_train[:40000]),\n+    (x_train[40000:], y_train[40000:]),\n+)\n+print(f\"Training samples: {len(x_train)}\")\n+print(f\"Validation samples: {len(x_val)}\")\n+print(f\"Testing samples: {len(x_test)}\")\n+\n+AUTO = tf.data.AUTOTUNE\n+train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n+train_ds = train_ds.shuffle(config.buffer_size).batch(config.batch_size).prefetch(AUTO)\n+\n+val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n+val_ds = val_ds.batch(config.batch_size).prefetch(AUTO)\n+\n+test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n+test_ds = test_ds.batch(config.batch_size).prefetch(AUTO)\n+\n+\"\"\"\n+## Data Augmentation\n+\n+The augmentation pipeline consists of:\n+\n+- Rescaling\n+- Resizing\n+- Random cropping\n+- Random horizontal flipping\n+\n+_Note_: The image data augmentation layers do not apply\n+data transformations at inference time. This means that\n+when these layers are called with `training=False` they\n+behave differently. Refer to the\n+[documentation](https://keras.io/api/layers/preprocessing_layers/image_augmentation/)\n+for more details.\n+\"\"\"\n+\n+\n+def get_augmentation_model():\n+    \"\"\"Build the data augmentation model.\"\"\"\n+    data_augmentation = keras.Sequential(\n+        [\n+            layers.Resizing(config.input_shape[0] + 20, config.input_shape[0] + 20),\n+            layers.RandomCrop(config.image_size, config.image_size),\n+            layers.RandomFlip(\"horizontal\"),\n+            layers.Rescaling(1 / 255.0),\n+        ]\n+    )\n+    return data_augmentation\n+\n+\n+\"\"\"\n+## The ShiftViT architecture\n+\n+In this section, we build the architecture proposed in\n+[the ShiftViT paper](https://arxiv.org/abs/2201.10801).\n+\n+| ![ShiftViT Architecture](https://i.imgur.com/CHU40HX.png) |\n+| :--: |\n+| Figure 1: The entire architecutre of ShiftViT.\n+[Source](https://arxiv.org/abs/2201.10801) |\n+\n+The architecture as shown in Fig. 1, is inspired by\n+[Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030).\n+Here the authors propose a modular architecture with 4 stages. Each stage works on its\n+own spatial size, creating a hierarchical architecture.\n+\n+An input image of size `HxWx3` is split into non-overlapping patches of size `4x4`.\n+This is done via the patchify layer which results in individual tokens of feature size `48`\n+(`4x4x3`). Each stage comprises two parts:\n+\n+1. Embedding Generation\n+2. Stacked Shift Blocks\n+\n+We discuss the stages and the modules in detail in what follows.\n+\n+_Note_: Compared to the [official implementation](https://github.com/microsoft/SPACH/blob/main/models/shiftvit.py)\n+we restructure some key components to better fit the Keras API.\n+\"\"\"\n+\n+\"\"\"\n+### The ShiftViT Block\n+\n+| ![ShiftViT block](https://i.imgur.com/IDe35vo.gif) |\n+| :--: |\n+| Figure 2: From the Model to a Shift Block. |\n+\n+Each stage in the ShiftViT architecture comprises of a Shift Block as shown in Fig 2.\n+\n+| ![Shift Vit Block](https://i.imgur.com/0q13pLu.png) |\n+| :--: |\n+| Figure 3: The Shift ViT Block. [Source](https://arxiv.org/abs/2201.10801) |\n+\n+The Shift Block as shown in Fig. 3, comprises of the following:\n+\n+1. Shift Operation\n+2. Linear Normalization\n+3. MLP Layer\n+\"\"\"\n+\n+\"\"\"\n+#### The MLP block\n+\n+The MLP block is intended to be a stack of densely-connected layers.s\n+\"\"\"\n+\n+\n+class MLP(layers.Layer):\n+    \"\"\"Get the MLP layer for each shift block.\n+\n+    Args:\n+        mlp_expand_ratio (int): The ratio with which the first feature map is expanded.\n+        mlp_dropout_rate (float): The rate for dropout.\n+    \"\"\"\n+\n+    def __init__(self, mlp_expand_ratio, mlp_dropout_rate, **kwargs):\n+        super().__init__(**kwargs)\n+        self.mlp_expand_ratio = mlp_expand_ratio\n+        self.mlp_dropout_rate = mlp_dropout_rate\n+\n+    def build(self, input_shape):\n+        input_channels = input_shape[-1]\n+        initial_filters = int(self.mlp_expand_ratio * input_channels)\n+\n+        self.mlp = keras.Sequential(\n+            [\n+                layers.Dense(\n+                    units=initial_filters,\n+                    activation=tf.nn.gelu,\n+                ),\n+                layers.Dropout(rate=self.mlp_dropout_rate),\n+                layers.Dense(units=input_channels),\n+                layers.Dropout(rate=self.mlp_dropout_rate),\n+            ]\n+        )\n+\n+    def call(self, x):\n+        x = self.mlp(x)\n+        return x\n+\n+\n+\"\"\"\n+#### The DropPath layer\n+\n+Stochastic depth is a regularization technique that randomly drops a set of\n+layers. During inference, the layers are kept as they are. It is very\n+similar to Dropout, but it operates on a block of layers rather\n+than on individual nodes present inside a layer.\n+\"\"\"\n+\n+\n+class DropPath(layers.Layer):\n+    \"\"\"Drop Path also known as the Stochastic Depth layer.\n+\n+    Refernece:\n+        - https://keras.io/examples/vision/cct/#stochastic-depth-for-regularization\n+        - github.com:rwightman/pytorch-image-models\n+    \"\"\"\n+\n+    def __init__(self, drop_path_prob, **kwargs):\n+        super().__init__(**kwargs)\n+        self.drop_path_prob = drop_path_prob\n+\n+    def call(self, x, training=False):\n+        if training:\n+            keep_prob = 1 - self.drop_path_prob\n+            shape = (tf.shape(x)[0],) + (1,) * (len(x.shape) - 1)\n+            random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n+            random_tensor = tf.floor(random_tensor)\n+            return (x / keep_prob) * random_tensor\n+        return x\n+\n+\n+\"\"\"\n+#### Block\n+\n+The most important operation in this paper is the **shift opperation**. In this section,\n+we describe the shift operation and compare it with its original implementation provided\n+by the authors.\n+\n+A generic feature map is assumed to have the shape `[N, H, W, C]`. Here we choose a\n+`num_div` parameter that decides the division size of the channels. The first 4 divisions\n+are shifted (1 pixel) in the left, right, up, and down direction. The remaining splits\n+are kept as is. After partial shifting the shifted channels are padded and the overflown\n+pixels are chopped off. This completes the partial shifting operation.\n+\n+In the original implementation, the code is approximately:\n+\n+```python\n+out[:, g * 0:g * 1, :, :-1] = x[:, g * 0:g * 1, :, 1:]  # shift left\n+out[:, g * 1:g * 2, :, 1:] = x[:, g * 1:g * 2, :, :-1]  # shift right\n+out[:, g * 2:g * 3, :-1, :] = x[:, g * 2:g * 3, 1:, :]  # shift up\n+out[:, g * 3:g * 4, 1:, :] = x[:, g * 3:g * 4, :-1, :]  # shift down\n+\n+out[:, g * 4:, :, :] = x[:, g * 4:, :, :]  # no shift\n+```\n+\n+In TensorFlow it would be infeasible for us to assign shifted channels to a tensor in the\n+middle of the training process. This is why we have resorted to the following procedure:\n+\n+1. Split the channels with the `num_div` parameter.\n+2. Select each of the first four spilts and shift and pad them in the respective\n+directions.\n+3. After shifting and padding, we concatenate the channel back.\n+\n+| ![Manim rendered animation for shift operation](https://i.imgur.com/PReeULP.gif) |\n+| :--: |\n+| Figure 4: The TensorFlow style shifting |\n+\n+The entire procedure is explained in the Fig. 4.\n+\"\"\"\n+\n+\n+class ShiftViTBlock(layers.Layer):\n+    \"\"\"A unit ShiftViT Block\n+\n+    Args:\n+        shift_pixel (int): The number of pixels to shift. Default to 1.\n+        mlp_expand_ratio (int): The ratio with which MLP features are\n+            expanded. Default to 2.\n+        mlp_dropout_rate (float): The dropout rate used in MLP.\n+        num_div (int): The number of divisions of the feature map's channel.\n+            Totally, 4/num_div of channels will be shifted. Defaults to 12.\n+        epsilon (float): Epsilon constant.\n+        drop_path_prob (float): The drop probability for drop path.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        epsilon,\n+        drop_path_prob,\n+        mlp_dropout_rate,\n+        num_div=12,\n+        shift_pixel=1,\n+        mlp_expand_ratio=2,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.shift_pixel = shift_pixel\n+        self.mlp_expand_ratio = mlp_expand_ratio\n+        self.mlp_dropout_rate = mlp_dropout_rate\n+        self.num_div = num_div\n+        self.epsilon = epsilon\n+        self.drop_path_prob = drop_path_prob\n+\n+    def build(self, input_shape):\n+        self.H = input_shape[1]\n+        self.W = input_shape[2]\n+        self.C = input_shape[3]\n+        self.layer_norm = layers.LayerNormalization(epsilon=self.epsilon)\n+        self.drop_path = (\n+            DropPath(drop_path_prob=self.drop_path_prob)\n+            if self.drop_path_prob > 0.0\n+            else layers.Activation(\"linear\")\n+        )\n+        self.mlp = MLP(\n+            mlp_expand_ratio=self.mlp_expand_ratio,\n+            mlp_dropout_rate=self.mlp_dropout_rate,\n+        )\n+\n+    def get_shift_pad(self, x, mode):\n+        \"\"\"Shifts the channels according to the mode chosen.\"\"\"\n+        if mode == \"left\":\n+            offset_height = 0\n+            offset_width = 0\n+            target_height = 0\n+            target_width = self.shift_pixel\n+        elif mode == \"right\":\n+            offset_height = 0\n+            offset_width = self.shift_pixel\n+            target_height = 0\n+            target_width = self.shift_pixel\n+        elif mode == \"up\":\n+            offset_height = 0\n+            offset_width = 0\n+            target_height = self.shift_pixel\n+            target_width = 0\n+        else:\n+            offset_height = self.shift_pixel\n+            offset_width = 0\n+            target_height = self.shift_pixel\n+            target_width = 0\n+        crop = tf.image.crop_to_bounding_box(\n+            x,\n+            offset_height=offset_height,\n+            offset_width=offset_width,\n+            target_height=self.H - target_height,\n+            target_width=self.W - target_width,\n+        )\n+        shift_pad = tf.image.pad_to_bounding_box(\n+            crop,\n+            offset_height=offset_height,\n+            offset_width=offset_width,\n+            target_height=self.H,\n+            target_width=self.W,\n+        )\n+        return shift_pad\n+\n+    def call(self, x, training=False):\n+        # Split the feature maps\n+        x_splits = tf.split(x, num_or_size_splits=self.C // self.num_div, axis=-1)\n+\n+        # Shift the feature maps\n+        x_splits[0] = self.get_shift_pad(x_splits[0], mode=\"left\")\n+        x_splits[1] = self.get_shift_pad(x_splits[1], mode=\"right\")\n+        x_splits[2] = self.get_shift_pad(x_splits[2], mode=\"up\")\n+        x_splits[3] = self.get_shift_pad(x_splits[3], mode=\"down\")\n+\n+        # Concatenate the shifted and unshifted feature maps\n+        x = tf.concat(x_splits, axis=-1)\n+\n+        # Add the residual connection\n+        shortcut = x\n+        x = shortcut + self.drop_path(self.mlp(self.layer_norm(x)), training=training)\n+        return x\n+\n+\n+\"\"\"\n+### The ShiftViT blocks\n+\n+| ![Shift Blokcs](https://i.imgur.com/FKy5NnD.png) |\n+| :--: |\n+| Figure 5: Shift Blocks in the architecture. [Source](https://arxiv.org/abs/2201.10801) |\n+\n+Each stage of the architecture has shift blocks as shown in Fig.5. Each of these blocks\n+contain a variable number of stacked ShiftViT block (as built in the earlier section).\n+\n+Shift blocks are followed by a PatchMerging layer that scales down feature inputs. The\n+PatchMerging layer helps in the pyramidal structure of the model.\n+\"\"\"\n+\n+\"\"\"\n+#### The PatchMerging layer\n+\n+This layer merges the two adjacent tokens. This layer helps in scaling the features down\n+spatially and increasing the features up channel wise. We use a Conv2D layer to merge the\n+patches.\n+\"\"\"\n+\n+\n+class PatchMerging(layers.Layer):\n+    \"\"\"The Patch Merging layer.\n+\n+    Args:\n+        epsilon (float): The epsilon constant.\n+    \"\"\"\n+\n+    def __init__(self, epsilon, **kwargs):\n+        super().__init__(**kwargs)\n+        self.epsilon = epsilon\n+\n+    def build(self, input_shape):\n+        filters = 2 * input_shape[-1]\n+        self.reduction = layers.Conv2D(\n+            filters=filters, kernel_size=2, strides=2, padding=\"same\", use_bias=False\n+        )\n+        self.layer_norm = layers.LayerNormalization(epsilon=self.epsilon)\n+\n+    def call(self, x):\n+        # Apply the patch merging algorithm on the feature maps\n+        x = self.layer_norm(x)\n+        x = self.reduction(x)\n+        return x\n+\n+\n+\"\"\"\n+#### Stacked Shift Blocks\n+\n+Each stage will have a variable number of stacked ShiftViT Blocks, as suggested in\n+the paper. This is a generic layer that will contain the stacked shift vit blocks\n+with the patch merging layer as well. Combining the two operations (shift ViT\n+block and patch merging) is a design choice we picked for better code reusability.\n+\"\"\"\n+\n+\n+# Note: This layer will have a different depth of stacking\n+# for different stages on the model.\n+class StackedShiftBlocks(layers.Layer):\n+    \"\"\"The layer containing stacked ShiftViTBlocks.\n+\n+    Args:\n+        epsilon (float): The epsilon constant.\n+        mlp_dropout_rate (float): The dropout rate used in the MLP block.\n+        num_shift_blocks (int): The number of shift vit blocks for this stage.\n+        stochastic_depth_rate (float): The maximum drop path rate chosen.\n+        is_merge (boolean): A flag that determines the use of the Patch Merge\n+            layer after the shift vit blocks.\n+        num_div (int): The division of channels of the feature map. Defaults to 12.\n+        shift_pixel (int): The number of pixels to shift. Defaults to 1.\n+        mlp_expand_ratio (int): The ratio with which the initial dense layer of\n+            the MLP is expanded Defaults to 2.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        epsilon,\n+        mlp_dropout_rate,\n+        num_shift_blocks,\n+        stochastic_depth_rate,\n+        is_merge,\n+        num_div=12,\n+        shift_pixel=1,\n+        mlp_expand_ratio=2,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.epsilon = epsilon\n+        self.mlp_dropout_rate = mlp_dropout_rate\n+        self.num_shift_blocks = num_shift_blocks\n+        self.stochastic_depth_rate = stochastic_depth_rate\n+        self.is_merge = is_merge\n+        self.num_div = num_div\n+        self.shift_pixel = shift_pixel\n+        self.mlp_expand_ratio = mlp_expand_ratio\n+\n+    def build(self, input_shapes):\n+        # Calculate stochastic depth probabilities.\n+        # Reference: https://keras.io/examples/vision/cct/#the-final-cct-model\n+        dpr = [\n+            x\n+            for x in np.linspace(\n+                start=0, stop=self.stochastic_depth_rate, num=self.num_shift_blocks\n+            )\n+        ]\n+\n+        # Build the shift blocks as a list of ShiftViT Blocks\n+        self.shift_blocks = list()\n+        for num in range(self.num_shift_blocks):\n+            self.shift_blocks.append(\n+                ShiftViTBlock(\n+                    num_div=self.num_div,\n+                    epsilon=self.epsilon,\n+                    drop_path_prob=dpr[num],\n+                    mlp_dropout_rate=self.mlp_dropout_rate,\n+                    shift_pixel=self.shift_pixel,\n+                    mlp_expand_ratio=self.mlp_expand_ratio,\n+                )\n+            )\n+        if self.is_merge:\n+            self.patch_merge = PatchMerging(epsilon=self.epsilon)\n+\n+    def call(self, x, training=False):\n+        for shift_block in self.shift_blocks:\n+            x = shift_block(x, training=training)\n+        if self.is_merge:\n+            x = self.patch_merge(x)\n+        return x\n+\n+\n+\"\"\"\n+## The ShiftViT model\n+\n+Build the ShiftViT custom model.\n+\"\"\"\n+\n+\n+class ShiftViTModel(keras.Model):\n+    \"\"\"The ShiftViT Model.\n+\n+    Args:\n+        data_augmentation (keras.Model): A data augmentation model.\n+        projected_dim (int): The dimension to which the patches of the image are\n+            projected.\n+        patch_size (int): The patch size of the images.\n+        num_shift_blocks_per_stages (list[int]): A list of all the number of shit\n+            blocks per stage.\n+        epsilon (float): The epsilon constant.\n+        mlp_dropout_rate (float): The dropout rate used in the MLP block.\n+        stochastic_depth_rate (float): The maximum drop rate probability.\n+        num_div (int): The number of divisions of the channesl of the feature\n+            map. Defaults to 12.\n+        shift_pixel (int): The number of pixel to shift. Default to 1.\n+        mlp_expand_ratio (int): The ratio with which the initial mlp dense layer\n+            is expanded to. Defaults to 2.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        data_augmentation,\n+        projected_dim,\n+        patch_size,\n+        num_shift_blocks_per_stages,\n+        epsilon,\n+        mlp_dropout_rate,\n+        stochastic_depth_rate,\n+        num_div=12,\n+        shift_pixel=1,\n+        mlp_expand_ratio=2,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.data_augmentation = data_augmentation\n+        self.patch_projection = layers.Conv2D(\n+            filters=projected_dim,\n+            kernel_size=patch_size,\n+            strides=patch_size,\n+            padding=\"same\",\n+        )\n+        self.stages = list()\n+        for index, num_shift_blocks in enumerate(num_shift_blocks_per_stages):\n+            if index == len(num_shift_blocks_per_stages) - 1:\n+                # This is the last stage, do not use the patch merge here.\n+                is_merge = False\n+            else:\n+                is_merge = True\n+            # Build the stages.\n+            self.stages.append(\n+                StackedShiftBlocks(\n+                    epsilon=epsilon,\n+                    mlp_dropout_rate=mlp_dropout_rate,\n+                    num_shift_blocks=num_shift_blocks,\n+                    stochastic_depth_rate=stochastic_depth_rate,\n+                    is_merge=is_merge,\n+                    num_div=num_div,\n+                    shift_pixel=shift_pixel,\n+                    mlp_expand_ratio=mlp_expand_ratio,\n+                )\n+            )\n+        self.global_avg_pool = layers.GlobalAveragePooling2D()\n+\n+    def get_config(self):\n+        config = super().get_config()\n+        config.update(\n+            {\n+                \"data_augmentation\": self.data_augmentation,\n+                \"patch_projection\": self.patch_projection,\n+                \"stages\": self.stages,\n+                \"global_avg_pool\": self.global_avg_pool,\n+            }\n+        )\n+        return config\n+\n+    def _calculate_loss(self, data, training=False):\n+        (images, labels) = data\n+\n+        # Augment the images\n+        augmented_images = self.data_augmentation(images, training=training)\n+\n+        # Create patches and project the pathces.\n+        projected_patches = self.patch_projection(augmented_images)\n+\n+        # Pass through the stages\n+        x = projected_patches\n+        for stage in self.stages:\n+            x = stage(x, training=training)\n+\n+        # Get the logits.\n+        logits = self.global_avg_pool(x)\n+\n+        # Calculate the loss and return it.\n+        total_loss = self.compute_loss(data, labels, logits)\n+        return total_loss, labels, logits\n+\n+    def train_step(self, inputs):\n+        with tf.GradientTape() as tape:\n+            total_loss, labels, logits = self._calculate_loss(\n+                data=inputs, training=True\n+            )\n+\n+        # Apply gradients.\n+        train_vars = [\n+            self.data_augmentation.trainable_variables,\n+            self.patch_projection.trainable_variables,\n+            self.global_avg_pool.trainable_variables,\n+        ]\n+        train_vars = train_vars + [stage.trainable_variables for stage in self.stages]\n+\n+        # Optimize the gradients.\n+        grads = tape.gradient(total_loss, train_vars)\n+        trainable_variable_list = []\n+        for grad, var in zip(grads, train_vars):\n+            for g, v in zip(grad, var):\n+                trainable_variable_list.append((g, v))\n+        self.optimizer.apply_gradients(trainable_variable_list)\n+\n+        # Update the metrics\n+        for metric in self.metrics:\n+            if metric.name == \"loss\":\n+                metric.update_state(total_loss)\n+            else:\n+                metric.update_state(labels, logits)\n+\n+        # Return a dict mapping metric names to current value\n+        return {m.name: m.result() for m in self.metrics}\n+\n+    def test_step(self, data):\n+        loss, labels, logits = self._calculate_loss(data=data, training=False)\n+\n+        # Update the metrics\n+        for metric in self.metrics:\n+            if metric.name == \"loss\":\n+                metric.update_state(loss)\n+            else:\n+                metric.update_state(labels, logits)\n+\n+        # Return a dict mapping metric names to current value\n+        return {m.name: m.result() for m in self.metrics}\n+\n+\n+\"\"\"\n+## Instantiate the model\n+\"\"\"\n+\n+model = ShiftViTModel(\n+    data_augmentation=get_augmentation_model(),\n+    projected_dim=config.projected_dim,\n+    patch_size=config.patch_size,\n+    num_shift_blocks_per_stages=config.num_shift_blocks_per_stages,\n+    epsilon=config.epsilon,\n+    mlp_dropout_rate=config.mlp_dropout_rate,\n+    stochastic_depth_rate=config.stochastic_depth_rate,\n+    num_div=config.num_div,\n+    shift_pixel=config.shift_pixel,\n+    mlp_expand_ratio=config.mlp_expand_ratio,\n+)\n+\n+\"\"\"\n+## Learning rate schedule\n+\n+In many experiments, we want to warm up the model with a slowly increasing learning rate\n+and then cool down the model with a slowly decaying learning rate. In the warmup cosine\n+decay, the learning rate linearly increases for the warmup steps and then decays with a\n+cosine decay.\n+\"\"\"\n+\n+\n+# Some code is taken from:\n+# https://www.kaggle.com/ashusma/training-rfcx-tensorflow-tpu-effnet-b2.\n+class WarmUpCosine(keras.optimizers.schedules.LearningRateSchedule):\n+    \"\"\"A LearningRateSchedule that uses a warmup cosine decay schedule.\"\"\"\n+\n+    def __init__(self, lr_start, lr_max, warmup_steps, total_steps):\n+        \"\"\"\n+        Args:\n+            lr_start: The initial learning rate\n+            lr_max: The maximum learning rate to which lr should increase to in\n+                the warmup steps\n+            warmup_steps: The number of steps for which the model warms up\n+            total_steps: The total number of steps for the model training\n+        \"\"\"\n+        super().__init__()\n+        self.lr_start = lr_start\n+        self.lr_max = lr_max\n+        self.warmup_steps = warmup_steps\n+        self.total_steps = total_steps\n+        self.pi = tf.constant(np.pi)\n+\n+    def __call__(self, step):\n+        # Check whether the total number of steps is larger than the warmup\n+        # steps. If not, then throw a value error.\n+        if self.total_steps < self.warmup_steps:\n+            raise ValueError(\n+                f\"Total number of steps {self.total_steps} must be\"\n+                + f\"larger or equal to warmup steps {self.warmup_steps}.\"\n+            )\n+\n+        # `cos_annealed_lr` is a graph that increases to 1 from the initial\n+        # step to the warmup step. After that this graph decays to -1 at the\n+        # final step mark.\n+        cos_annealed_lr = tf.cos(\n+            self.pi\n+            * (tf.cast(step, tf.float32) - self.warmup_steps)\n+            / tf.cast(self.total_steps - self.warmup_steps, tf.float32)\n+        )\n+\n+        # Shift the mean of the `cos_annealed_lr` graph to 1. Now the grpah goes\n+        # from 0 to 2. Normalize the graph with 0.5 so that now it goes from 0\n+        # to 1. With the normalized graph we scale it with `lr_max` such that\n+        # it goes from 0 to `lr_max`\n+        learning_rate = 0.5 * self.lr_max * (1 + cos_annealed_lr)\n+\n+        # Check whether warmup_steps is more than 0.\n+        if self.warmup_steps > 0:\n+            # Check whether lr_max is larger that lr_start. If not, throw a value\n+            # error.\n+            if self.lr_max < self.lr_start:\n+                raise ValueError(\n+                    f\"lr_start {self.lr_start} must be smaller or\"\n+                    + f\"equal to lr_max {self.lr_max}.\"\n+                )\n+\n+            # Calculate the slope with which the learning rate should increase\n+            # in the warumup schedule. The formula for slope is m = ((b-a)/steps)\n+            slope = (self.lr_max - self.lr_start) / self.warmup_steps\n+\n+            # With the formula for a straight line (y = mx+c) build the warmup\n+            # schedule\n+            warmup_rate = slope * tf.cast(step, tf.float32) + self.lr_start\n+\n+            # When the current step is lesser that warmup steps, get the line\n+            # graph. When the current step is greater than the warmup steps, get\n+            # the scaled cos graph.\n+            learning_rate = tf.where(\n+                step < self.warmup_steps, warmup_rate, learning_rate\n+            )\n+\n+        # When the current step is more that the total steps, return 0 else return\n+        # the calculated graph.\n+        return tf.where(\n+            step > self.total_steps, 0.0, learning_rate, name=\"learning_rate\"\n+        )\n+\n+\n+\"\"\"\n+## Compile and train the model\n+\"\"\"\n+\n+# Get the total number of steps for training.\n+total_steps = int((len(x_train) / config.batch_size) * config.epochs)\n+\n+# Calculate the number of steps for warmup.\n+warmup_epoch_percentage = 0.15\n+warmup_steps = int(total_steps * warmup_epoch_percentage)\n+\n+# Initialize the warmupcosine schedule.\n+scheduled_lrs = WarmUpCosine(\n+    lr_start=1e-5,\n+    lr_max=1e-3,\n+    warmup_steps=warmup_steps,\n+    total_steps=total_steps,\n+)\n+\n+# Get the optimizer.\n+optimizer = keras.optimizers.AdamW(\n+    learning_rate=scheduled_lrs, weight_decay=config.weight_decay\n+)\n+\n+# Compile and pretrain the model.\n+model.compile(\n+    optimizer=optimizer,\n+    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n+    metrics=[\n+        keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n+        keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n+    ],\n+)\n+\n+# Train the model\n+history = model.fit(\n+    train_ds,\n+    epochs=config.epochs,\n+    validation_data=val_ds,\n+    callbacks=[\n+        keras.callbacks.EarlyStopping(\n+            monitor=\"val_accuracy\",\n+            patience=5,\n+            mode=\"auto\",\n+        )\n+    ],\n+)\n+\n+# Evaluate the model with the test dataset.\n+print(\"TESTING\")\n+loss, acc_top1, acc_top5 = model.evaluate(test_ds)\n+print(f\"Loss: {loss:0.2f}\")\n+print(f\"Top 1 test accuracy: {acc_top1*100:0.2f}%\")\n+print(f\"Top 5 test accuracy: {acc_top5*100:0.2f}%\")\n+\n+\"\"\"\n+## Conclusion\n+\n+The most impactful contribution of the paper is not the novel architecture, but\n+the idea that hierarchical ViTs trained with no attention can perform quite well. This\n+opens up the question of how essential attention is to the performance of ViTs.\n+\n+For curious minds, we would suggest reading the\n+[ConvNexT](https://arxiv.org/abs/2201.03545) paper which attends more to the training\n+paradigms and architectural details of ViTs rather than providing a novel architecture\n+based on attention.\n+\n+Acknowledgements:\n+\n+- We would like to thank [PyImageSearch](https://pyimagesearch.com) for providing us with\n+resources that helped in the completion of this project.\n+- We would like to thank [JarvisLabs.ai](https://jarvislabs.ai/) for providing with the\n+GPU credits.\n+- We would like to thank [Manim Community](https://www.manim.community/) for the manim\n+library.\n+- A personal note of thanks to [Puja Roychowdhury](https://twitter.com/pleb_talks) for\n+helping us with the Learning Rate Schedule.\n+\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
