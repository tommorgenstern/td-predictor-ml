{"custom_id": "scrapy#76e19baa2931f5c3f06900c4fd7b6d39c728509a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 376 | Contributors (this commit): 5 | Commits (past 90d): 12 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -118,7 +118,7 @@ EXTENSIONS_BASE = {\n }\n \n FEED_URI = None\n-FEED_URI_ARGS = None # a function to extend uri arguments\n+FEED_URI_PARAMS = None # a function to extend uri arguments\n FEED_FORMAT = 'jsonlines'\n FEED_STORE_EMPTY = False\n FEED_STORAGES = {}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a71521bfba933b3d093600144881a2c3e82fc108", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 21 | Files Changed: 15 | Hunks: 16 | Methods Changed: 2 | Complexity Δ (Sum/Max): 38/22 | Churn Δ: 30 | Churn Cumulative: 572 | Contributors (this commit): 5 | Commits (past 90d): 36 | Contributors (cumulative): 23 | DMM Complexity: 0.08333333333333333\n\nDIFF:\n@@ -68,18 +68,6 @@ def _print_usage(inside_project):\n             print \"  %s\" % cmdclass.short_desc()\n     print\n \n-def _update_default_settings(module, cmdname):\n-    if not module:\n-        return\n-    try:\n-        mod = __import__('%s.%s' % (module, cmdname), {}, {}, [''])\n-    except ImportError:\n-        return\n-    settingsdict = vars(mod)\n-    for k, v in settingsdict.iteritems():\n-        if not k.startswith(\"_\"):\n-            settings.defaults[k] = v\n-\n def execute(argv=None):\n     if argv is None:\n         argv = sys.argv\n@@ -87,8 +75,6 @@ def execute(argv=None):\n     cmds = _get_commands_dict()\n \n     cmdname = _get_command_name(argv)\n-    _update_default_settings('scrapy.conf.commands', cmdname)\n-    _update_default_settings(settings['COMMANDS_SETTINGS_MODULE'], cmdname)\n \n     parser = optparse.OptionParser(formatter=optparse.TitledHelpFormatter(), \\\n         conflict_handler='resolve', add_help_option=False)\n@@ -120,6 +106,7 @@ def execute(argv=None):\n         print 'Use \"scrapy-ctl.py -h\" for help' \n         sys.exit(2)\n \n+    settings.defaults.update(cmd.default_settings)\n     del args[0]  # remove command name from args\n     send_catch_log(signal=command_executed, cmdname=cmdname, cmdobj=cmd, \\\n         args=args, opts=opts)\n\n@@ -15,6 +15,9 @@ class ScrapyCommand(object):\n \n     requires_project = False\n \n+    # default settings to be used for this command instead of global defaults\n+    default_settings = {}\n+\n     def syntax(self):\n         \"\"\"\n         Command syntax (preferably one-line). Do not include command name.\n\n@@ -27,6 +27,7 @@ def sanitize_module_name(module_name):\n class Command(ScrapyCommand):\n \n     requires_project = True\n+    default_settings = {'LOG_ENABLED': False}\n \n     def syntax(self):\n         return \"[options] <name> <domain>\"\n\n@@ -4,6 +4,7 @@ from scrapy.spider import spiders\n class Command(ScrapyCommand):\n \n     requires_project = True\n+    default_settings = {'LOG_ENABLED': False}\n \n     def short_desc(self):\n         return \"List available spiders\"\n\n@@ -4,6 +4,7 @@ from scrapy.conf import settings as settings_\n class Command(ScrapyCommand):\n \n     requires_project = False\n+    default_settings = {'LOG_ENABLED': False}\n \n     def syntax(self):\n         return \"[options]\"\n\n@@ -10,6 +10,7 @@ from scrapy.shell import Shell\n class Command(ScrapyCommand):\n \n     requires_project = False\n+    default_settings = {'LOG_LEVEL': 'WARNING'}\n \n     def syntax(self):\n         return \"[url|file]\"\n\n@@ -24,6 +24,7 @@ IGNORE = ignore_patterns('*.pyc', '.svn')\n class Command(ScrapyCommand):\n \n     requires_project = False\n+    default_settings = {'LOG_ENABLED': False}\n \n     def syntax(self):\n         return \"<project_name>\"\n\n\n@@ -1 +0,0 @@\n-LOG_ENABLED = False\n\n@@ -1 +0,0 @@\n-LOG_ENABLED = False\n\n@@ -1 +0,0 @@\n-LOG_ENABLED = False\n\n@@ -1 +0,0 @@\n-LOG_ENABLED = False\n\n@@ -1 +0,0 @@\n-LOG_LEVEL='WARNING'\n\n@@ -1 +0,0 @@\n-LOG_ENABLED = False\n\n@@ -22,7 +22,6 @@ CLOSESPIDER_TIMEOUT = 0\n CLOSESPIDER_ITEMPASSED = 0\n \n COMMANDS_MODULE = ''\n-COMMANDS_SETTINGS_MODULE = ''\n \n CONCURRENT_ITEMS = 100\n CONCURRENT_REQUESTS_PER_SPIDER = 8\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#50f2236d042dbac616ad733dd0f1a4827839e8ae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 179 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,11 +10,6 @@ from scrapy import log\n from scrapy.xlib import lsprofcalltree\n from scrapy.conf import settings\n from scrapy.command import ScrapyCommand\n-from scrapy.utils.signal import send_catch_log\n-\n-# Signal that carries information about the command which was executed\n-# args: cmdname, cmdobj, args, opts\n-command_executed = object()\n \n def _find_commands(dir):\n     try:\n@@ -108,8 +103,6 @@ def execute(argv=None):\n \n     settings.defaults.update(cmd.default_settings)\n     del args[0]  # remove command name from args\n-    send_catch_log(signal=command_executed, cmdname=cmdname, cmdobj=cmd, \\\n-        args=args, opts=opts)\n     from scrapy.core.manager import scrapymanager\n     scrapymanager.configure(control_reactor=True)\n     ret = _run_command(cmd, args, opts)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5522afca8db97115f7c0fe12496a51db0d626ef1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 161 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,6 +6,7 @@ See documentation in docs/item-pipeline.rst\n \n from scrapy import log\n from scrapy.middleware import MiddlewareManager\n+from scrapy.utils.python import get_func_args\n \n class ItemPipelineManager(MiddlewareManager):\n \n@@ -23,14 +24,14 @@ class ItemPipelineManager(MiddlewareManager):\n \n     def _add_middleware(self, pipe):\n         super(ItemPipelineManager, self)._add_middleware(pipe)\n-        if hasattr(pipe, 'process_item'):\n+        func = getattr(pipe, 'process_item', None)\n+        if func:\n             # FIXME: remove in Scrapy 0.11\n-            from scrapy.utils.python import get_func_args\n-            if get_func_args(pipe.process_item.im_func)[1] == 'spider':\n+            fargs = get_func_args(func.im_func)\n+            if fargs and fargs[1] == 'spider':\n                 log.msg(\"Update %s.process_item() method to receive (item, spider) instead of (spider, item) or they will stop working on Scrapy 0.11\" % pipe.__class__.__name__, log.WARNING)\n-                pipe.process_item = self._wrap_old_process_item(pipe.process_item)\n-\n-            self.methods['process_item'].append(pipe.process_item)\n+                func = self._wrap_old_process_item(func)\n+            self.methods['process_item'].append(func)\n \n     def process_item(self, item, spider):\n         return self._process_chain('process_item', item, spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#34554da201e070695d2b569f9cff27f0c3206d34", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 22 | Files Changed: 9 | Hunks: 15 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 39 | Churn Cumulative: 540 | Contributors (this commit): 5 | Commits (past 90d): 25 | Contributors (cumulative): 18 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,4 +0,0 @@\n-#!/usr/bin/env python\n-\n-from scrapy.cmdline import execute\n-execute()\n\n@@ -2,7 +2,7 @@\n #\n # To create the first spider for your project use this command:\n #\n-#   scrapy-ctl.py genspider myspider myspider-domain.com\n+#   scrapy genspider myspider myspider-domain.com\n #\n # For more info see:\n # http://doc.scrapy.org/topics/spiders.html\n\n@@ -2,7 +2,7 @@\n #\n # To create the first spider for your project use this command:\n #\n-#   scrapy-ctl.py genspider myspider myspider-domain.com\n+#   scrapy genspider myspider myspider-domain.com\n #\n # For more info see:\n # http://doc.scrapy.org/topics/spiders.html\n\n@@ -51,9 +51,9 @@ def _print_usage(inside_project):\n     print \"Usage\"\n     print \"=====\\n\"\n     print \"To run a command:\"\n-    print \"  scrapy-ctl.py <command> [options] [args]\\n\"\n+    print \"  scrapy <command> [options] [args]\\n\"\n     print \"To get help:\"\n-    print \"  scrapy-ctl.py <command> -h\\n\"\n+    print \"  scrapy <command> -h\\n\"\n     print \"Available commands\"\n     print \"==================\\n\"\n     cmds = _get_commands_dict()\n@@ -66,6 +66,10 @@ def _print_usage(inside_project):\n def execute(argv=None):\n     if argv is None:\n         argv = sys.argv\n+    if any('scrapy-ctl' in x for x in argv):\n+        import warnings\n+        warnings.warn(\"`scrapy-ctl.py` command-line tool is deprecated and will be removed in Scrapy 0.11, use `scrapy` instead\",\n+            DeprecationWarning, stacklevel=2)\n \n     cmds = _get_commands_dict()\n \n@@ -82,7 +86,7 @@ def execute(argv=None):\n         parser.usage = \"%%prog %s %s\" % (cmdname, cmd.syntax())\n         parser.description = cmd.long_desc()\n         if cmd.requires_project and not settings.settings_module:\n-            print \"Error running: scrapy-ctl.py %s\\n\" % cmdname\n+            print \"Error running: scrapy %s\\n\" % cmdname\n             print \"Cannot find project settings module in python path: %s\" % \\\n                 settings.settings_module_path\n             sys.exit(1)\n@@ -98,7 +102,7 @@ def execute(argv=None):\n         sys.exit(2)\n     else:\n         print \"Unknown command: %s\\n\" % cmdname\n-        print 'Use \"scrapy-ctl.py -h\" for help' \n+        print 'Use \"scrapy -h\" for help' \n         sys.exit(2)\n \n     settings.defaults.update(cmd.default_settings)\n\n@@ -13,7 +13,6 @@ TEMPLATES_PATH = join(scrapy.__path__[0], 'templates', 'project')\n \n TEMPLATES_TO_RENDER = (\n     ('scrapy.cfg',),\n-    ('scrapy-ctl.py',),\n     ('${project_name}', 'settings.py.tmpl'),\n     ('${project_name}', 'items.py.tmpl'),\n     ('${project_name}', 'pipelines.py.tmpl'),\n@@ -47,7 +46,6 @@ class Command(ScrapyCommand):\n         moduletpl = join(TEMPLATES_PATH, 'module')\n         copytree(moduletpl, join(project_name, project_name), ignore=IGNORE)\n         shutil.copy(join(TEMPLATES_PATH, 'scrapy.cfg'), project_name)\n-        shutil.copy(join(TEMPLATES_PATH, 'scrapy-ctl.py'), project_name)\n         for paths in TEMPLATES_TO_RENDER:\n             path = join(*paths)\n             tplfile = join(project_name,\n\n@@ -2,7 +2,7 @@\n #\n # To create the first spider for your project use this command:\n #\n-#   scrapy-ctl.py genspider myspider myspider-domain.com\n+#   scrapy genspider myspider myspider-domain.com\n #\n # For more info see:\n # http://doc.scrapy.org/topics/spiders.html\n\n@@ -1,7 +0,0 @@\n-#!/usr/bin/env python\n-\n-import os\n-os.environ.setdefault('SCRAPY_SETTINGS_MODULE', '${project_name}.settings')\n-\n-from scrapy.cmdline import execute\n-execute()\n\n@@ -35,7 +35,7 @@ class StartprojectTest(ProjectTest):\n     def test_startproject(self):\n         self.assertEqual(0, self.call('startproject', self.project_name))\n \n-        assert exists(join(self.proj_path, 'scrapy-ctl.py'))\n+        assert exists(join(self.proj_path, 'scrapy.cfg'))\n         assert exists(join(self.proj_path, 'testproject'))\n         assert exists(join(self.proj_mod_path, '__init__.py'))\n         assert exists(join(self.proj_mod_path, 'items.py'))\n\n@@ -74,6 +74,10 @@ if len(sys.argv) > 1 and sys.argv[1] == 'bdist_wininst':\n     for file_info in data_files:\n         file_info[0] = '\\\\PURELIB\\\\%s' % file_info[0]\n \n+scripts = ['bin/scrapy', 'bin/scrapy-ws.py', 'bin/scrapy-sqs.py']\n+if os.name == 'nt':\n+    scripts.append('extras/scrapy.bat')\n+\n # Dynamically calculate the version based on scrapy.__version__\n version = \".\".join(map(str, __import__('scrapy').version_info[:2]))\n \n@@ -90,7 +94,7 @@ setup_args = {\n     'packages': packages,\n     'cmdclass': cmdclasses,\n     'data_files': data_files,\n-    'scripts': ['bin/scrapy', 'bin/scrapy-ctl.py', 'bin/scrapy-ws.py', 'bin/scrapy-sqs.py'],\n+    'scripts': scripts,\n     'classifiers': [\n         'Programming Language :: Python',\n         'Programming Language :: Python :: 2.5',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#94ead94bf6eb064f62f7b546f0800c5a829d2995", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 15 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,3 +9,8 @@ def setup(app):\n         rolename      = \"signal\",\n         indextemplate = \"pair: %s; signal\",\n     )\n+    app.add_crossref_type(\n+        directivename = \"command\",\n+        rolename      = \"command\",\n+        indextemplate = \"pair: %s; command\",\n+    )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#85f3a4a603a6b42a000b442c53307660950767c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 379 | Contributors (this commit): 5 | Commits (past 90d): 14 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -228,8 +228,6 @@ SPIDER_MIDDLEWARES_BASE = {\n \n SPIDER_MODULES = []\n \n-SPIDERPROFILER_ENABLED = False\n-\n SQS_QUEUE = 'scrapy'\n SQS_VISIBILITY_TIMEOUT = 7200\n SQS_POLLING_DELAY = 30\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#abd7a5e221a3d4acadb1f9a50df0e8641aeaa090", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 55 | Lines Deleted: 33 | Files Changed: 4 | Hunks: 16 | Methods Changed: 7 | Complexity Δ (Sum/Max): 4/6 | Churn Δ: 88 | Churn Cumulative: 490 | Contributors (this commit): 4 | Commits (past 90d): 10 | Contributors (cumulative): 9 | DMM Complexity: 0.1111111111111111\n\nDIFF:\n@@ -1,18 +1,17 @@\n import sys\n import os\n \n-from scrapy.xlib.pydispatch import dispatcher\n-from scrapy.contrib.exporter import XmlItemExporter\n+from scrapy import log\n+from scrapy.utils.spider import iter_spider_classes\n from scrapy.command import ScrapyCommand\n from scrapy.core.manager import scrapymanager\n-from scrapy import signals\n \n def _import_file(filepath):\n     abspath = os.path.abspath(filepath)\n     dirname, file = os.path.split(abspath)\n     fname, fext = os.path.splitext(file)\n     if fext != '.py':\n-        raise ValueError(\"Only Python files supported: %s\" % abspath)\n+        raise ValueError(\"Not a Python source file: %s\" % abspath)\n     if dirname:\n         sys.path = [dirname] + sys.path\n     try:\n@@ -22,7 +21,6 @@ def _import_file(filepath):\n             sys.path.pop(0)\n     return module\n \n-\n class Command(ScrapyCommand):\n \n     requires_project = False\n@@ -34,28 +32,25 @@ class Command(ScrapyCommand):\n         return \"Run a spider\"\n \n     def long_desc(self):\n-        return \"Run the spider defined in the given file. The file must be a \" \\\n-            \"Python module which defines a SPIDER variable with a instance of \" \\\n-            \"the spider to run.\"\n-\n-    def add_options(self, parser):\n-        super(Command, self).add_options(parser)\n-        parser.add_option(\"--output\", dest=\"output\", metavar=\"FILE\",\n-            help=\"store scraped items to FILE in XML format\")\n+        return \"Run the spider defined in the given file\"\n \n     def run(self, args, opts):\n         if len(args) != 1:\n             return False\n-        if opts.output:\n-            file = open(opts.output, 'w+b')\n-            exporter = XmlItemExporter(file)\n-            dispatcher.connect(exporter.export_item, signal=signals.item_passed)\n-            exporter.start_exporting()\n-        module = _import_file(args[0])\n-\n+        filename = args[0]\n+        if not os.path.exists(filename):\n+            log.msg(\"File not found: %s\\n\" % filename, log.ERROR)\n+            return\n+        try:\n+            module = _import_file(filename)\n+        except (ImportError, ValueError), e:\n+            log.msg(\"Unable to load %r: %s\\n\" % (filename, e), log.ERROR)\n+            return\n+        spclasses = list(iter_spider_classes(module))\n+        if not spclasses:\n+            log.msg(\"No spider found in file: %s\\n\" % filename, log.ERROR)\n+            return\n+        spider = spclasses.pop()()\n         # schedule spider and start engine\n-        scrapymanager.queue.append_spider(module.SPIDER)\n+        scrapymanager.queue.append_spider(spider)\n         scrapymanager.start()\n-\n-        if opts.output:\n-            exporter.finish_exporting()\n\n@@ -3,12 +3,10 @@ SpiderManager is the class which locates and manages all website-specific\n spiders\n \"\"\"\n \n-import inspect\n-\n from scrapy import log\n from scrapy.conf import settings\n from scrapy.utils.misc import walk_modules\n-from scrapy.spider import BaseSpider\n+from scrapy.utils.spider import iter_spider_classes\n \n \n class SpiderManager(object):\n@@ -68,11 +66,8 @@ class SpiderManager(object):\n         self.loaded = True\n \n     def _load_spiders(self, module):\n-        for obj in vars(module).itervalues():\n-            if inspect.isclass(obj) and issubclass(obj, BaseSpider):\n-                name = getattr(obj, 'name', None)\n-                if name is not None:\n-                    self._spiders[name] = obj\n+        for spcls in iter_spider_classes(module):\n+            self._spiders[spcls.name] = spcls\n \n     def close_spider(self, spider):\n         pass\n\n@@ -1,8 +1,19 @@\n import unittest\n from scrapy.http import Request\n from scrapy.item import BaseItem\n-from scrapy.utils.spider import iterate_spider_output\n+from scrapy.utils.spider import iterate_spider_output, iter_spider_classes\n \n+from scrapy.contrib.spiders import CrawlSpider\n+\n+\n+class MyBaseSpider(CrawlSpider):\n+    pass # abstract spider\n+\n+class MySpider1(MyBaseSpider):\n+    name = 'myspider1'\n+\n+class MySpider2(MyBaseSpider):\n+    name = 'myspider2'\n \n class UtilsSpidersTestCase(unittest.TestCase):\n \n@@ -16,6 +27,10 @@ class UtilsSpidersTestCase(unittest.TestCase):\n         self.assertEqual(list(iterate_spider_output(o)), [o])\n         self.assertEqual(list(iterate_spider_output([r, i, o])), [r, i, o])\n \n+    def test_iter_spider_classes(self):\n+        import scrapy.tests.test_utils_spider\n+        it = iter_spider_classes(scrapy.tests.test_utils_spider)\n+        self.assertEqual(set(it), set([MySpider1, MySpider2]))\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -1,6 +1,23 @@\n+import inspect\n+\n from scrapy.item import BaseItem\n from scrapy.utils.misc import  arg_to_iter\n \n \n def iterate_spider_output(result):\n     return [result] if isinstance(result, BaseItem) else arg_to_iter(result)\n+\n+def iter_spider_classes(module):\n+    \"\"\"Return an iterator over all spider classes defined in the given module\n+    that can be instantiated (ie. which have name)\n+    \"\"\"\n+    # this needs to be imported here until get rid of the spider manager\n+    # singleton in scrapy.spider.spiders\n+    from scrapy.spider import BaseSpider\n+\n+    for obj in vars(module).itervalues():\n+        if inspect.isclass(obj) and \\\n+           issubclass(obj, BaseSpider) and \\\n+           obj.__module__ == module.__name__ and \\\n+           getattr(obj, 'name', None):\n+            yield obj\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9740ad62a6c6f418532105f76315354252cb8831", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 63 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 67 | Churn Cumulative: 244 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,10 +1,13 @@\n+from __future__ import with_statement\n+\n import sys\n import os\n import subprocess\n-from os.path import exists, join, dirname\n+from os.path import exists, join, dirname, abspath\n from shutil import rmtree\n from tempfile import mkdtemp\n-import unittest\n+\n+from twisted.trial import unittest\n \n import scrapy\n \n@@ -29,6 +32,12 @@ class ProjectTest(unittest.TestCase):\n         return subprocess.call(args, stdout=out, stderr=out, cwd=self.cwd, \\\n             env=self.env, **kwargs)\n \n+    def call_get_proc(self, *new_args, **kwargs):\n+        out = os.tmpfile()\n+        args = (sys.executable, '-m', 'scrapy.cmdline') + new_args\n+        return subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, \\\n+            cwd=self.cwd, env=self.env, **kwargs)\n+\n \n class StartprojectTest(ProjectTest):\n \n@@ -100,6 +109,56 @@ class MiscCommandsTest(CommandTest):\n     def test_list(self):\n         self.assertEqual(0, self.call('list'))\n \n+class RunSpiderCommandTest(CommandTest):\n+\n+    def test_runspider(self):\n+        tmpdir = self.mktemp()\n+        os.mkdir(tmpdir)\n+        fname = abspath(join(tmpdir, 'myspider.py'))\n+        with open(fname, 'w') as f:\n+            f.write(\"\"\"\n+from scrapy import log\n+from scrapy.spider import BaseSpider\n+\n+class MySpider(BaseSpider):\n+    name = 'myspider'\n+\n+    def start_requests(self):\n+        self.log(\"It Works!\")\n+        return []\n+\"\"\")\n+        p = self.call_get_proc('runspider', fname)\n+        log = p.stderr.read()\n+        self.assert_(\"[myspider] DEBUG: It Works!\" in log)\n+        self.assert_(\"[myspider] INFO: Spider opened\" in log)\n+        self.assert_(\"[myspider] INFO: Closing spider (finished)\" in log)\n+        self.assert_(\"[myspider] INFO: Spider closed (finished)\" in log)\n+\n+    def test_runspider_no_spider_found(self):\n+        tmpdir = self.mktemp()\n+        os.mkdir(tmpdir)\n+        fname = abspath(join(tmpdir, 'myspider.py'))\n+        with open(fname, 'w') as f:\n+            f.write(\"\"\"\n+from scrapy import log\n+from scrapy.spider import BaseSpider\n+\"\"\")\n+        p = self.call_get_proc('runspider', fname)\n+        log = p.stderr.read()\n+        self.assert_(\"ERROR: No spider found in file\" in log)\n+\n+    def test_runspider_file_not_found(self):\n+        p = self.call_get_proc('runspider', 'some_non_existent_file')\n+        log = p.stderr.read()\n+        self.assert_(\"ERROR: File not found: some_non_existent_file\" in log)\n+\n+    def test_runspider_unable_to_load(self):\n+        tmpdir = self.mktemp()\n+        os.mkdir(tmpdir)\n+        fname = abspath(join(tmpdir, 'myspider.txt'))\n+        with open(fname, 'w') as f:\n+            f.write(\"\")\n+        p = self.call_get_proc('runspider', fname)\n+        log = p.stderr.read()\n+        self.assert_(\"ERROR: Unable to load\" in log)\n \n-if __name__ == '__main__':\n-    unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2ff5a83b7a16c74b55d8765001ce8832d8d5c634", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 364 | Lines Deleted: 1 | Files Changed: 5 | Hunks: 5 | Methods Changed: 34 | Complexity Δ (Sum/Max): 62/34 | Churn Δ: 365 | Churn Cumulative: 744 | Contributors (this commit): 5 | Commits (past 90d): 20 | Contributors (cumulative): 9 | DMM Complexity: 0.8981481481481481\n\nDIFF:\n@@ -0,0 +1,46 @@\n+from scrapy.command import ScrapyCommand\n+from scrapy.utils.misc import load_object\n+from scrapy.conf import settings\n+\n+class Command(ScrapyCommand):\n+\n+    requires_project = True\n+    default_settings = {'LOG_ENABLED': False}\n+\n+    def syntax(self):\n+        return \"[options] <list|clear|add spider1 ..>\"\n+\n+    def short_desc(self):\n+        return \"Control execution queue\"\n+\n+    def add_options(self, parser):\n+        ScrapyCommand.add_options(self, parser)\n+        parser.add_option(\"--priority\", dest=\"priority\", type=\"float\", default=0.0, \\\n+            help=\"priority to use for adding spiders\")\n+        parser.add_option(\"-a\", \"--arg\", dest=\"spargs\", action=\"append\", default=[], \\\n+            help=\"spider arguments to use for adding spiders\")\n+\n+    def run(self, args, opts):\n+        if len(args) < 1:\n+            return False\n+        cmd = args[0]\n+\n+        botname = settings['BOT_NAME']\n+        queue = load_object(settings['SERVICE_QUEUE'])().queue\n+\n+        if cmd == 'add':\n+            if len(args) < 2:\n+                return False\n+            msg = dict(x for x in [x.split('=', 1) for x in opts.spargs])\n+            for x in args[1:]:\n+                msg.update(name=x)\n+                queue.put(msg)\n+                print \"Added (priority=%s): %s\" % (opts.priority, msg)\n+        elif cmd == 'list':\n+            for x, y in queue:\n+                print \"(priority=%s) %s\" % (y, x)\n+        elif cmd == 'clear':\n+            queue.clear()\n+            print \"Cleared %s queue\" % botname\n+        else:\n+            return False\n\n@@ -209,7 +209,8 @@ SCHEDULER_MIDDLEWARES_BASE = {\n \n SCHEDULER_ORDER = 'DFO'\n \n-SERVICE_QUEUE = 'scrapy.core.queue.KeepAliveExecutionQueue'\n+SERVICE_QUEUE = 'scrapy.contrib.queue.SqliteExecutionQueue'\n+SERVICE_QUEUE_FILE = 'scrapy.db'\n \n SPIDER_MANAGER_CLASS = 'scrapy.contrib.spidermanager.SpiderManager'\n \n\n@@ -0,0 +1,20 @@\n+from scrapy.core.queue import ExecutionQueue\n+from scrapy.utils.sqlite import JsonSqlitePriorityQueue\n+from scrapy.conf import settings\n+\n+class SqliteExecutionQueue(ExecutionQueue):\n+\n+    queue_class = JsonSqlitePriorityQueue\n+\n+    def __init__(self, *a, **kw):\n+        super(SqliteExecutionQueue, self).__init__(*a, **kw)\n+        self.queue = JsonSqlitePriorityQueue(settings['SERVICE_QUEUE_FILE'])\n+\n+    def _append_next(self):\n+        msg = self.queue.pop()\n+        if msg:\n+            name = msg.pop('name')\n+            self.append_spider_name(name, **msg)\n+\n+    def is_finished(self):\n+        return False\n\n@@ -0,0 +1,141 @@\n+import unittest\n+from datetime import datetime\n+from decimal import Decimal\n+\n+from scrapy.http import Request\n+from scrapy.utils.sqlite import SqlitePriorityQueue, JsonSqlitePriorityQueue, \\\n+    PickleSqlitePriorityQueue, SqliteDict, JsonSqliteDict, PickleSqliteDict\n+\n+\n+class SqliteDictTest(unittest.TestCase):\n+\n+    dict_class = SqliteDict\n+    test_dict = {'hello': 'world', 'int': 1, 'float': 1.5}\n+\n+    def test_basic_types(self):\n+        test = self.test_dict\n+        d = self.dict_class()\n+        d.update(test)\n+        self.failUnlessEqual(d.items(), test.items())\n+        d.clear()\n+        self.failIf(d.items())\n+\n+\n+class JsonSqliteDictTest(SqliteDictTest):\n+\n+    dict_class = JsonSqliteDict\n+    test_dict = SqliteDictTest.test_dict.copy()\n+    test_dict.update({'list': ['a', 'world'], 'dict': {'some': 'dict'}})\n+\n+\n+class PickleSqliteDictTest(JsonSqliteDictTest):\n+\n+    dict_class = PickleSqliteDict\n+    test_dict = JsonSqliteDictTest.test_dict.copy()\n+    test_dict.update({'decimal': Decimal(\"10\"), 'datetime': datetime.now()})\n+\n+    def test_request_persistance(self):\n+        r1 = Request(\"http://www.example.com\", body=\"some\")\n+        d = self.dict_class()\n+        d['request'] = r1\n+        r2 = d['request']\n+        self.failUnless(isinstance(r2, Request))\n+        self.failUnlessEqual(r1.url, r2.url)\n+        self.failUnlessEqual(r1.body, r2.body)\n+\n+\n+class SqlitePriorityQueueTest(unittest.TestCase):\n+\n+    queue_class = SqlitePriorityQueue\n+\n+    supported_values = [\"bytes\", u\"\\xa3\", 123, 1.2, True]\n+\n+    def setUp(self):\n+        self.q = self.queue_class()\n+\n+    def test_empty(self):\n+        self.failUnless(self.q.pop() is None)\n+\n+    def test_one(self):\n+        msg = \"a message\"\n+        self.q.put(msg)\n+        self.failIf(\"_id\" in msg)\n+        self.failUnlessEqual(self.q.pop(), msg)\n+        self.failUnless(self.q.pop() is None)\n+\n+    def test_multiple(self):\n+        msg1 = \"first message\"\n+        msg2 = \"second message\"\n+        self.q.put(msg1)\n+        self.q.put(msg2)\n+        out = []\n+        out.append(self.q.pop())\n+        out.append(self.q.pop())\n+        self.failUnless(msg1 in out)\n+        self.failUnless(msg2 in out)\n+        self.failUnless(self.q.pop() is None)\n+\n+    def test_priority(self):\n+        msg1 = \"message 1\"\n+        msg2 = \"message 2\"\n+        msg3 = \"message 3\"\n+        msg4 = \"message 4\"\n+        self.q.put(msg1, priority=1.0)\n+        self.q.put(msg2, priority=5.0)\n+        self.q.put(msg3, priority=3.0)\n+        self.q.put(msg4, priority=2.0)\n+        self.failUnlessEqual(self.q.pop(), msg2)\n+        self.failUnlessEqual(self.q.pop(), msg3)\n+        self.failUnlessEqual(self.q.pop(), msg4)\n+        self.failUnlessEqual(self.q.pop(), msg1)\n+\n+    def test_iter_len_clear(self):\n+        self.failUnlessEqual(len(self.q), 0)\n+        self.failUnlessEqual(list(self.q), [])\n+        msg1 = \"message 1\"\n+        msg2 = \"message 2\"\n+        msg3 = \"message 3\"\n+        msg4 = \"message 4\"\n+        self.q.put(msg1, priority=1.0)\n+        self.q.put(msg2, priority=5.0)\n+        self.q.put(msg3, priority=3.0)\n+        self.q.put(msg4, priority=2.0)\n+        self.failUnlessEqual(len(self.q), 4)\n+        self.failUnlessEqual(list(self.q), \\\n+            [(msg2, 5.0), (msg3, 3.0), (msg4, 2.0), (msg1, 1.0)])\n+        self.q.clear()\n+        self.failUnlessEqual(len(self.q), 0)\n+        self.failUnlessEqual(list(self.q), [])\n+\n+    def test_types(self):\n+        for x in self.supported_values:\n+            self.q.put(x)\n+            self.failUnlessEqual(self.q.pop(), x)\n+\n+\n+class JsonSqlitePriorityQueueTest(SqlitePriorityQueueTest):\n+\n+    queue_class = JsonSqlitePriorityQueue\n+\n+    supported_values = SqlitePriorityQueueTest.supported_values + [\n+        [\"a\", \"list\", 1],\n+        {\"a\": \"dict\"},\n+    ]\n+\n+\n+class PickleSqlitePriorityQueueTest(JsonSqlitePriorityQueueTest):\n+\n+    queue_class = PickleSqlitePriorityQueue\n+\n+    supported_values = JsonSqlitePriorityQueueTest.supported_values + [\n+        Decimal(\"10\"),\n+        datetime.now(),\n+    ]\n+\n+    def test_request_persistance(self):\n+        r1 = Request(\"http://www.example.com\", body=\"some\")\n+        self.q.put(r1)\n+        r2 = self.q.pop()\n+        self.failUnless(isinstance(r2, Request))\n+        self.failUnlessEqual(r1.url, r2.url)\n+        self.failUnlessEqual(r1.body, r2.body)\n\n@@ -0,0 +1,155 @@\n+import sqlite3\n+import cPickle\n+from UserDict import DictMixin\n+\n+from scrapy.utils.py26 import json\n+\n+\n+class SqliteDict(DictMixin):\n+    \"\"\"SQLite-backed dictionary\"\"\"\n+\n+    def __init__(self, database=':memory:', table=\"dict\"):\n+        self.database = database\n+        self.table = table\n+        self.conn = sqlite3.connect(database)\n+        q = \"create table if not exists %s (key text primary key, value blob)\" \\\n+            % table\n+        self.conn.execute(q)\n+\n+    def __getitem__(self, key):\n+        key = self.encode(key)\n+        q = \"select value from %s where key=?\" % self.table\n+        value = self.conn.execute(q, (key,)).fetchone()[0]\n+        return self.decode(value)\n+\n+    def __setitem__(self, key, value):\n+        key, value = self.encode(key), self.encode(value)\n+        q = \"insert into %s (key, value) values (?,?)\" % self.table\n+        self.conn.execute(q, (key, value))\n+        self.conn.commit()\n+\n+    def __delitem__(self, key):\n+        key = self.encode(key)\n+        q = \"delete from %s where key=?\" % self.table\n+        self.conn.execute(q, (key,))\n+        self.conn.commit()\n+\n+    def iterkeys(self):\n+        q = \"select key from %s\" % self.table\n+        return (self.decode(x[0]) for x in self.conn.execute(q))\n+\n+    def keys(self):\n+        return list(self.iterkeys())\n+\n+    def itervalues(self):\n+        q = \"select value from %s\" % self.table\n+        return (self.decode(x[0]) for x in self.conn.execute(q))\n+\n+    def values(self):\n+        return list(self.itervalues())\n+\n+    def iteritems(self):\n+        q = \"select key, value from %s\" % self.table\n+        return ((self.decode(x[0]), self.decode(x[1])) for x in self.conn.execute(q))\n+\n+    def items(self):\n+        return list(self.iteritems())\n+\n+    def encode(self, obj):\n+        return obj\n+\n+    def decode(self, text):\n+        return text\n+\n+\n+class PickleSqliteDict(SqliteDict):\n+\n+    def encode(self, obj):\n+        return buffer(cPickle.dumps(obj, protocol=2))\n+\n+    def decode(self, text):\n+        return cPickle.loads(str(text))\n+\n+\n+class JsonSqliteDict(SqliteDict):\n+\n+    def encode(self, obj):\n+        return json.dumps(obj)\n+\n+    def decode(self, text):\n+        return json.loads(text)\n+\n+\n+\n+class SqlitePriorityQueue(object):\n+    \"\"\"SQLite priority queue. It relies on SQLite concurrency support for\n+    providing atomic inter-process operations.\n+    \"\"\"\n+\n+    def __init__(self, database=':memory:', table=\"queue\"):\n+        self.database = database\n+        self.table = table\n+        self.conn = sqlite3.connect(database)\n+        q = \"create table if not exists %s (id integer primary key, \" \\\n+            \"priority real key, message blob)\" % table\n+        self.conn.execute(q)\n+\n+    def put(self, message, priority=0.0):\n+        args = (priority, self.encode(message))\n+        q = \"insert into %s (priority, message) values (?,?)\" % self.table\n+        self.conn.execute(q, args)\n+        self.conn.commit()\n+\n+    def pop(self):\n+        q = \"select id, message from %s order by priority desc limit 1\" \\\n+            % self.table\n+        idmsg = self.conn.execute(q).fetchone()\n+        if idmsg is None:\n+            return\n+        id, msg = idmsg\n+        q = \"delete from %s where id=?\" % self.table\n+        c = self.conn.execute(q, (id,))\n+        if not c.rowcount: # record vanished, so let's try again\n+            self.conn.rollback()\n+            return self.pop()\n+        self.conn.commit()\n+        return self.decode(msg)\n+\n+    def clear(self):\n+        self.conn.execute(\"delete from %s\" % self.table)\n+        self.conn.commit()\n+\n+    def __len__(self):\n+        q = \"select count(*) from %s\" % self.table\n+        return self.conn.execute(q).fetchone()[0]\n+\n+    def __iter__(self):\n+        q = \"select message, priority from %s order by priority desc\" % \\\n+            self.table\n+        return ((self.decode(x), y) for x, y in self.conn.execute(q))\n+\n+    def encode(self, obj):\n+        return obj\n+\n+    def decode(self, text):\n+        return text\n+\n+\n+class PickleSqlitePriorityQueue(SqlitePriorityQueue):\n+\n+    def encode(self, obj):\n+        return buffer(cPickle.dumps(obj, protocol=2))\n+\n+    def decode(self, text):\n+        return cPickle.loads(str(text))\n+\n+\n+class JsonSqlitePriorityQueue(SqlitePriorityQueue):\n+\n+    def encode(self, obj):\n+        return json.dumps(obj)\n+\n+    def decode(self, text):\n+        return json.loads(text)\n+\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#66525e77c78bd86be008798ab43ea5984f043e24", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 27 | Churn Cumulative: 218 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -63,21 +63,32 @@ def _print_usage(inside_project):\n             print \"  %s\" % cmdclass.short_desc()\n     print\n \n+def check_deprecated_scrapy_ctl(argv):\n+    \"\"\"Check if Scrapy was called using the deprecated scrapy-ctl command and\n+    warn in that case, also creating a scrapy.cfg if it doesn't exist.\n+    \"\"\"\n+    if not any('scrapy-ctl' in x for x in argv):\n+        return\n+    import warnings\n+    warnings.warn(\"`scrapy-ctl.py` command-line tool is deprecated and will be removed in Scrapy 0.11, use `scrapy` instead\",\n+        DeprecationWarning, stacklevel=3)\n+    if settings.settings_module:\n+        projpath = os.path.abspath(os.path.dirname(os.path.dirname(settings.settings_module.__file__)))\n+        cfg_path = os.path.join(projpath, 'scrapy.cfg')\n+        if not os.path.exists(cfg_path):\n+            with open(cfg_path, 'w') as f:\n+                f.write(\"# generated automatically - feel free to edit\" + os.linesep)\n+                f.write(\"[default]\" + os.linesep)\n+                f.write(\"settings = %s\" % settings.settings_module_path + os.linesep)\n+\n def execute(argv=None):\n     if argv is None:\n         argv = sys.argv\n-    if any('scrapy-ctl' in x for x in argv):\n-        import warnings\n-        warnings.warn(\"`scrapy-ctl.py` command-line tool is deprecated and will be removed in Scrapy 0.11, use `scrapy` instead\",\n-            DeprecationWarning, stacklevel=2)\n-\n+    check_deprecated_scrapy_ctl(argv) # TODO: remove for Scrapy 0.11\n     cmds = _get_commands_dict()\n-\n     cmdname = _get_command_name(argv)\n-\n     parser = optparse.OptionParser(formatter=optparse.TitledHelpFormatter(), \\\n         conflict_handler='resolve', add_help_option=False)\n-\n     if cmdname in cmds:\n         cmd = cmds[cmdname]\n         cmd.add_options(parser)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6dd76ab54bad01a5d310065b7c2293c671d98232", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 18 | Files Changed: 2 | Hunks: 11 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 34 | Churn Cumulative: 262 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -10,7 +10,6 @@ from scrapy.shell import Shell\n class Command(ScrapyCommand):\n \n     requires_project = False\n-    default_settings = {'LOG_LEVEL': 'WARNING'}\n \n     def syntax(self):\n         return \"[url|file]\"\n\n@@ -10,7 +10,9 @@ import urlparse\n import signal\n \n from twisted.internet import reactor, threads\n+from twisted.python.failure import Failure\n \n+from scrapy import log\n from scrapy.spider import BaseSpider, spiders\n from scrapy.selector import XmlXPathSelector, HtmlXPathSelector\n from scrapy.utils.misc import load_object\n@@ -50,16 +52,19 @@ class Shell(object):\n             url = request.url\n         else:\n             url = parse_url(request_or_url)\n-            request = Request(url)\n+            request = Request(url, dont_filter=True)\n \n         spider = spiders.create_for_request(request, BaseSpider('default'), \\\n             log_multiple=True)\n \n         print \"Fetching %s...\" % request\n         scrapymanager.engine.open_spider(spider)\n-        response = threads.blockingCallFromThread(reactor, scrapymanager.engine.schedule, \\\n-            request, spider)\n-        if response:\n+        response = None\n+        try:\n+            response = threads.blockingCallFromThread(reactor, \\\n+                scrapymanager.engine.schedule, request, spider)\n+        except:\n+            log.err(Failure(), \"Error fetching response\", spider=spider)\n         self.populate_vars(url, response, request, spider)\n         if print_help:\n             self.print_help()\n@@ -86,20 +91,17 @@ class Shell(object):\n             self.update_vars(self.vars)\n \n     def print_help(self):\n-        print \"Available objects\"\n-        print \"=================\"\n         print\n+        print \"Available objects:\"\n         for k, v in self.vars.iteritems():\n             if relevant_var(k):\n-                print \"  %-10s: %s\" % (k, v)\n+                print \"  %-10s %s\" % (k, v)\n         print\n-        print \"Available shortcuts\"\n-        print \"===================\"\n-        print\n-        print \"  shelp()           : Prints this help.\"\n+        print \"Convenient shortcuts:\"\n+        print \"  shelp()           Print this help\"\n         if not self.nofetch:\n-            print \"  fetch(req_or_url) : Fetch a new request or URL and update objects\"\n-        print \"  view(response)    : View response in a browser\"\n+            print \"  fetch(req_or_url) Fetch a new request or URL and update shell objects\"\n+        print \"  view(response)    View response in a browser\"\n         print\n \n     def start(self, url):\n@@ -112,10 +114,7 @@ class Shell(object):\n \n     def inspect_response(self, response):\n         print\n-        print \"Scrapy Shell\"\n-        print \"============\"\n-        print\n-        print \"Inspecting: %s\" % response\n+        print \"Scrapy Shell - inspecting response: %s\" % response\n         print \"Use shelp() to see available objects\"\n         print\n         request = response.request\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#136b0e74507e48145257144d7ff836be0d189f97", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1461 | Contributors (this commit): 4 | Commits (past 90d): 12 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -194,7 +194,7 @@ class ExecutionEngine(object):\n                 errmsg = str(_failure)\n                 level = log.ERROR\n             if errmsg:\n-                log.msg(\"Crawling <%s>: %s\" % (request.url, errmsg), \\\n+                log.msg(\"Error downloading <%s>: %s\" % (request.url, errmsg), \\\n                     level=level, spider=spider)\n             return Failure(IgnoreRequest(str(exc)))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7858244dca17513cf9995519997697a01a51694b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 23 | Files Changed: 2 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 2/6 | Churn Δ: 54 | Churn Cumulative: 314 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -17,6 +17,7 @@ from scrapy.spider import BaseSpider, spiders\n from scrapy.selector import XmlXPathSelector, HtmlXPathSelector\n from scrapy.utils.misc import load_object\n from scrapy.utils.response import open_in_browser\n+from scrapy.utils.console import start_python_console\n from scrapy.conf import settings\n from scrapy.core.manager import scrapymanager\n from scrapy.core.queue import KeepAliveExecutionQueue\n@@ -24,7 +25,7 @@ from scrapy.http import Request, TextResponse\n \n def relevant_var(varname):\n     return varname not in ['shelp', 'fetch', 'view', '__builtins__', 'In', \\\n-        'Out', 'help'] and not varname.startswith('_')\n+        'Out', 'help', 'namespace'] and not varname.startswith('_')\n \n def parse_url(url):\n     \"\"\"Parse url which can be a direct path to a direct file\"\"\"\n@@ -120,27 +121,7 @@ class Shell(object):\n         request = response.request\n         url = request.url\n         self.populate_vars(url, response, request)\n-        self._run_console()\n-\n-    def _run_console(self):\n-        try:\n-            try: # use IPython if available\n-                import IPython\n-                shell = IPython.Shell.IPShell(argv=[], user_ns=self.vars)\n-                ip = shell.IP.getapi()\n-                shell.mainloop()\n-            except ImportError:\n-                import code\n-                try: # readline module is only available on unix systems\n-                    import readline\n-                except ImportError:\n-                    pass\n-                else:\n-                    import rlcompleter\n-                    readline.parse_and_bind(\"tab:complete\")\n-                code.interact(local=self.vars)\n-        except SystemExit: # raised when using exit() in python code.interact\n-            pass\n+        start_python_console(self.vars)\n \n     def _console_thread(self, url=None):\n         self.populate_vars()\n@@ -148,7 +129,7 @@ class Shell(object):\n             result = self.fetch(url, print_help=True)\n         else:\n             self.print_help()\n-        self._run_console()\n+        start_python_console(self.vars)\n         reactor.callFromThread(scrapymanager.stop)\n \n def inspect_response(response):\n\n@@ -0,0 +1,27 @@\n+\n+def start_python_console(namespace=None, noipython=False):\n+    \"\"\"Start Python console binded to the given namespace. If IPython is\n+    available, an IPython console will be started instead, unless `noipython`\n+    is True. Also, tab completion will be used on Unix systems.\n+    \"\"\"\n+    if namespace is None:\n+        namespace = {}\n+    try:\n+        try: # use IPython if available\n+            if noipython:\n+                raise ImportError\n+            import IPython\n+            shell = IPython.Shell.IPShellEmbed(argv=[], user_ns=namespace)\n+            shell()\n+        except ImportError:\n+            import code\n+            try: # readline module is only available on unix systems\n+                import readline\n+            except ImportError:\n+                pass\n+            else:\n+                import rlcompleter\n+                readline.parse_and_bind(\"tab:complete\")\n+            code.interact(banner='', local=namespace)\n+    except SystemExit: # raised when using exit() in python code.interact\n+        pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1d3b9e2ca8bd4adc73692317e17ad6c569ab0705", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 127 | Lines Deleted: 84 | Files Changed: 4 | Hunks: 29 | Methods Changed: 24 | Complexity Δ (Sum/Max): 9/6 | Churn Δ: 211 | Churn Cumulative: 751 | Contributors (this commit): 4 | Commits (past 90d): 16 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,6 +4,8 @@ Scrapy Shell\n See documentation in docs/topics/shell.rst\n \"\"\"\n \n+from scrapy.core.manager import scrapymanager\n+from scrapy.core.queue import KeepAliveExecutionQueue\n from scrapy.command import ScrapyCommand\n from scrapy.shell import Shell\n \n@@ -28,5 +30,7 @@ class Command(ScrapyCommand):\n \n     def run(self, args, opts):\n         url = args[0] if args else None\n-        shell = Shell(self.update_vars)\n-        shell.start(url)\n+        shell = Shell(scrapymanager, update_vars=self.update_vars, inthread=True)\n+        shell.start(url=url).addBoth(lambda _: scrapymanager.stop())\n+        scrapymanager.queue = KeepAliveExecutionQueue()\n+        scrapymanager.start()\n\n@@ -4,73 +4,69 @@ Scrapy Shell\n See documentation in docs/topics/shell.rst\n \"\"\"\n \n-import os\n-import urllib\n-import urlparse\n import signal\n \n from twisted.internet import reactor, threads\n from twisted.python.failure import Failure\n \n from scrapy import log\n+from scrapy.item import BaseItem\n from scrapy.spider import BaseSpider, spiders\n-from scrapy.selector import XmlXPathSelector, HtmlXPathSelector\n+from scrapy.selector import XPathSelector, XmlXPathSelector, HtmlXPathSelector\n from scrapy.utils.misc import load_object\n from scrapy.utils.response import open_in_browser\n+from scrapy.utils.url import any_to_uri\n from scrapy.utils.console import start_python_console\n from scrapy.conf import settings\n-from scrapy.core.manager import scrapymanager\n-from scrapy.core.queue import KeepAliveExecutionQueue\n-from scrapy.http import Request, TextResponse\n-\n-def relevant_var(varname):\n-    return varname not in ['shelp', 'fetch', 'view', '__builtins__', 'In', \\\n-        'Out', 'help', 'namespace'] and not varname.startswith('_')\n-\n-def parse_url(url):\n-    \"\"\"Parse url which can be a direct path to a direct file\"\"\"\n-    url = url.strip()\n-    if url:\n-        u = urlparse.urlparse(url)\n-        if not u.scheme:\n-            path = os.path.abspath(url).replace(os.sep, '/')\n-            url = 'file://' + urllib.pathname2url(path)\n-            u = urlparse.urlparse(url)\n-    return url\n-\n+from scrapy.http import Request, Response, TextResponse\n \n class Shell(object):\n \n-    def __init__(self, update_vars=None, nofetch=False):\n-        self.vars = {}\n-        self.update_vars = update_vars\n-        self.item_class = load_object(settings['DEFAULT_ITEM_CLASS'])\n-        self.nofetch = nofetch\n+    relevant_classes = (BaseSpider, Request, Response, BaseItem, XPathSelector)\n \n-    def fetch(self, request_or_url, print_help=False):\n+    def __init__(self, crawler, update_vars=None, inthread=False):\n+        self.crawler = crawler\n+        self.vars = {}\n+        self.update_vars = update_vars or (lambda x: None)\n+        self.item_class = load_object(settings['DEFAULT_ITEM_CLASS'])\n+        self.inthread = inthread\n+\n+    def start(self, *a, **kw):\n+        # disable accidental Ctrl-C key press from shutting down the engine\n+        signal.signal(signal.SIGINT, signal.SIG_IGN)\n+        if self.inthread:\n+            return threads.deferToThread(self._start, *a, **kw)\n+        else:\n+            self._start(*a, **kw)\n+\n+    def _start(self, url=None, request=None, response=None, spider=None):\n+        if url:\n+            self.fetch(url, spider)\n+        elif request:\n+            self.fetch(request, spider)\n+        elif response:\n+            request = response.request\n+            self.populate_vars(request.url, response, request, spider)\n+        start_python_console(self.vars)\n+\n+    def fetch(self, request_or_url, spider=None):\n         if isinstance(request_or_url, Request):\n             request = request_or_url\n             url = request.url\n         else:\n-            url = parse_url(request_or_url)\n+            url = any_to_uri(request_or_url)\n             request = Request(url, dont_filter=True)\n-\n+        if spider is None:\n             spider = spiders.create_for_request(request, BaseSpider('default'), \\\n                 log_multiple=True)\n-\n-        print \"Fetching %s...\" % request\n-        scrapymanager.engine.open_spider(spider)\n+        self.crawler.engine.open_spider(spider)\n         response = None\n         try:\n             response = threads.blockingCallFromThread(reactor, \\\n-                scrapymanager.engine.schedule, request, spider)\n+                self.crawler.engine.schedule, request, spider)\n         except:\n             log.err(Failure(), \"Error fetching response\", spider=spider)\n         self.populate_vars(url, response, request, spider)\n-        if print_help:\n-            self.print_help()\n-        else:\n-            print \"Done - use shelp() to see available objects\"\n \n     def populate_vars(self, url=None, response=None, request=None, spider=None):\n         item = self.item_class()\n@@ -79,59 +75,35 @@ class Shell(object):\n             if isinstance(response, TextResponse):\n                 self.vars['xxs'] = XmlXPathSelector(response)\n                 self.vars['hxs'] = HtmlXPathSelector(response)\n-            self.vars['url'] = url\n             self.vars['response'] = response\n             self.vars['request'] = request\n             self.vars['spider'] = spider\n-        if not self.nofetch:\n+        if self.inthread:\n             self.vars['fetch'] = self.fetch\n         self.vars['view'] = open_in_browser\n         self.vars['shelp'] = self.print_help\n-\n-        if self.update_vars:\n         self.update_vars(self.vars)\n+        self.print_help()\n \n     def print_help(self):\n-        print\n-        print \"Available objects:\"\n-        for k, v in self.vars.iteritems():\n-            if relevant_var(k):\n-                print \"  %-10s %s\" % (k, v)\n-        print\n-        print \"Convenient shortcuts:\"\n-        print \"  shelp()           Print this help\"\n-        if not self.nofetch:\n-            print \"  fetch(req_or_url) Fetch a new request or URL and update shell objects\"\n-        print \"  view(response)    View response in a browser\"\n-        print\n+        self.p(\"Available Scrapy objects:\")\n+        for k, v in sorted(self.vars.iteritems()):\n+            if self._is_relevant(v):\n+                self.p(\"  %-10s %s\" % (k, v))\n+        self.p(\"Useful shortcuts:\")\n+        self.p(\"  shelp()           Shell help (print this help)\")\n+        if self.inthread:\n+            self.p(\"  fetch(req_or_url) Fetch request (or URL) and update local objects\")\n+        self.p(\"  view(response)    View response in a browser\")\n \n-    def start(self, url):\n-        # disable accidental Ctrl-C key press from shutting down the engine\n-        signal.signal(signal.SIGINT, signal.SIG_IGN)\n+    def p(self, line=''):\n+        print \"[s] %s\" % line\n \n-        reactor.callInThread(self._console_thread, url)\n-        scrapymanager.queue = KeepAliveExecutionQueue()\n-        scrapymanager.start()\n+    def _is_relevant(self, value):\n+        return isinstance(value, self.relevant_classes)\n \n-    def inspect_response(self, response):\n-        print\n-        print \"Scrapy Shell - inspecting response: %s\" % response\n-        print \"Use shelp() to see available objects\"\n-        print\n-        request = response.request\n-        url = request.url\n-        self.populate_vars(url, response, request)\n-        start_python_console(self.vars)\n \n-    def _console_thread(self, url=None):\n-        self.populate_vars()\n-        if url:\n-            result = self.fetch(url, print_help=True)\n-        else:\n-            self.print_help()\n-        start_python_console(self.vars)\n-        reactor.callFromThread(scrapymanager.stop)\n-\n-def inspect_response(response):\n+def inspect_response(response, spider=None):\n     \"\"\"Open a shell to inspect the given response\"\"\"\n-    Shell(nofetch=True).inspect_response(response)\n+    from scrapy.core.manager import scrapymanager\n+    Shell(scrapymanager).start(response=response, spider=spider)\n\n@@ -1,8 +1,9 @@\n+import os\n import unittest\n from scrapy.spider import BaseSpider\n from scrapy.utils.url import url_is_from_any_domain, safe_url_string, safe_download_url, \\\n     url_query_parameter, add_or_replace_parameter, url_query_cleaner, canonicalize_url, \\\n-    urljoin_rfc, url_is_from_spider\n+    urljoin_rfc, url_is_from_spider, file_uri_to_path, path_to_file_uri, any_to_uri\n \n class UrlUtilsTest(unittest.TestCase):\n \n@@ -273,6 +274,51 @@ class UrlUtilsTest(unittest.TestCase):\n         self.assertEqual(canonicalize_url(u'http://www.example.com/caf%E9-con-leche.htm'),\n                                            'http://www.example.com/caf%E9-con-leche.htm')\n \n+    def test_path_to_file_uri(self):\n+        if os.name == 'nt':\n+            self.assertEqual(path_to_file_uri(\"C:\\\\windows\\clock.avi\"),\n+                             \"file:///C|/windows/clock.avi\")\n+        else:\n+            self.assertEqual(path_to_file_uri(\"/some/path.txt\"),\n+                             \"file:///some/path.txt\")\n+\n+        fn = \"test.txt\"\n+        x = path_to_file_uri(fn)\n+        self.assert_(x.startswith('file:///'))\n+        self.assertEqual(file_uri_to_path(x), os.path.abspath(fn))\n+\n+    def test_file_uri_to_path(self):\n+        if os.name == 'nt':\n+            self.assertEqual(file_uri_to_path(\"file:///C|/windows/clock.avi\"),\n+                             \"C:\\\\windows\\clock.avi\")\n+            uri = \"file:///C|/windows/clock.avi\"\n+            uri2 = path_to_file_uri(file_uri_to_path(uri))\n+            self.assertEqual(uri, uri2)\n+        else:\n+            self.assertEqual(file_uri_to_path(\"file:///path/to/test.txt\"),\n+                             \"/path/to/test.txt\")\n+            self.assertEqual(file_uri_to_path(\"/path/to/test.txt\"),\n+                             \"/path/to/test.txt\")\n+            uri = \"file:///path/to/test.txt\"\n+            uri2 = path_to_file_uri(file_uri_to_path(uri))\n+            self.assertEqual(uri, uri2)\n+\n+        self.assertEqual(file_uri_to_path(\"test.txt\"),\n+                         \"test.txt\")\n+\n+    def test_any_to_uri(self):\n+        if os.name == 'nt':\n+            self.assertEqual(any_to_uri(\"C:\\\\windows\\clock.avi\"),\n+                             \"file:///C|/windows/clock.avi\")\n+        else:\n+            self.assertEqual(any_to_uri(\"/some/path.txt\"),\n+                             \"file:///some/path.txt\")\n+        self.assertEqual(any_to_uri(\"file:///some/path.txt\"),\n+                         \"file:///some/path.txt\")\n+        self.assertEqual(any_to_uri(\"http://www.example.com/some/path.txt\"),\n+                         \"http://www.example.com/some/path.txt\")\n+\n+\n if __name__ == \"__main__\":\n     unittest.main()\n \n\n@@ -3,6 +3,7 @@ This module contains general purpose URL functions not found in the standard\n library.\n \"\"\"\n \n+import os\n import re\n import urlparse\n import urllib\n@@ -154,3 +155,23 @@ def canonicalize_url(url, keep_blank_values=True, keep_fragments=False, \\\n     path = urllib.quote(urllib.unquote(path))\n     fragment = '' if not keep_fragments else fragment\n     return urlparse.urlunparse((scheme, netloc, path, params, query, fragment))\n+\n+def path_to_file_uri(path):\n+    \"\"\"Convert local filesystem path to legal File URIs as described in:\n+    http://en.wikipedia.org/wiki/File_URI_scheme\n+    \"\"\"\n+    x = urllib.pathname2url(os.path.abspath(path))\n+    return 'file:///%s' % x.lstrip('/')\n+\n+def file_uri_to_path(uri):\n+    \"\"\"Convert File URI to local filesystem path according to:\n+    http://en.wikipedia.org/wiki/File_URI_scheme\n+    \"\"\"\n+    return urllib.url2pathname(urlparse.urlparse(uri).path)\n+\n+def any_to_uri(uri_or_path):\n+    \"\"\"If given a path name, return its File URI, otherwise return it\n+    unmodified\n+    \"\"\"\n+    u = urlparse.urlparse(uri_or_path)\n+    return uri_or_path if u.scheme else path_to_file_uri(uri_or_path)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
