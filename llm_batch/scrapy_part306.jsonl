{"custom_id": "scrapy#3600582f56071dd9f40f31ed44e09294a78dc13f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 122 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -39,7 +39,8 @@ def curl_to_request_kwargs(curl_command, ignore_unknown_options=True):\n \n     :param str curl_command: string containing the curl command\n     :param bool ignore_unknown_options: If true, only a warning is emitted when\n-    cURL options are unknown. Otherwise raises an error. (default: True)\n+                                        cURL options are unknown. Otherwise\n+                                        raises an error. (default: True)\n     :return: dictionary of Request kwargs\n     \"\"\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#336f19f5cc6edd0392c77f38b857d3e40bf565da", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 117 | Lines Deleted: 118 | Files Changed: 56 | Hunks: 117 | Methods Changed: 113 | Complexity Δ (Sum/Max): -2/1 | Churn Δ: 235 | Churn Cumulative: 27300 | Contributors (this commit): 130 | Commits (past 90d): 158 | Contributors (cumulative): 610 | DMM Complexity: 0.0\n\nDIFF:\n@@ -27,7 +27,7 @@ class QPSSpider(Spider):\n     slots = 1\n \n     def __init__(self, *a, **kw):\n-        super(QPSSpider, self).__init__(*a, **kw)\n+        super().__init__(*a, **kw)\n         if self.qps is not None:\n             self.qps = float(self.qps)\n             self.download_delay = 1 / self.qps\n\n@@ -11,7 +11,7 @@ class Command(fetch.Command):\n         return \"Fetch a URL using the Scrapy downloader and show its contents in a browser\"\n \n     def add_options(self, parser):\n-        super(Command, self).add_options(parser)\n+        super().add_options(parser)\n         parser.remove_option(\"--headers\")\n \n     def _print_response(self, response, opts):\n\n@@ -56,7 +56,7 @@ class ReturnsContract(Contract):\n     }\n \n     def __init__(self, *args, **kwargs):\n-        super(ReturnsContract, self).__init__(*args, **kwargs)\n+        super().__init__(*args, **kwargs)\n \n         if len(self.args) not in [1, 2, 3]:\n             raise ValueError(\n\n@@ -20,7 +20,7 @@ class ScrapyClientContextFactory(BrowserLikePolicyForHTTPS):\n     \"\"\"\n \n     def __init__(self, method=SSL.SSLv23_METHOD, tls_verbose_logging=False, tls_ciphers=None, *args, **kwargs):\n-        super(ScrapyClientContextFactory, self).__init__(*args, **kwargs)\n+        super().__init__(*args, **kwargs)\n         self._ssl_method = method\n         self.tls_verbose_logging = tls_verbose_logging\n         if tls_ciphers:\n@@ -45,7 +45,7 @@ class ScrapyClientContextFactory(BrowserLikePolicyForHTTPS):\n         #   (https://github.com/scrapy/scrapy/issues/1429#issuecomment-131782133)\n         #\n         # * getattr() for `_ssl_method` attribute for context factories\n-        #   not calling super(..., self).__init__\n+        #   not calling super().__init__\n         return CertificateOptions(\n             verify=False,\n             method=getattr(self, 'method', getattr(self, '_ssl_method', None)),\n\n@@ -126,7 +126,7 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n \n     def __init__(self, reactor, host, port, proxyConf, contextFactory, timeout=30, bindAddress=None):\n         proxyHost, proxyPort, self._proxyAuthHeader = proxyConf\n-        super(TunnelingTCP4ClientEndpoint, self).__init__(reactor, proxyHost, proxyPort, timeout, bindAddress)\n+        super().__init__(reactor, proxyHost, proxyPort, timeout, bindAddress)\n         self._tunnelReadyDeferred = defer.Deferred()\n         self._tunneledHost = host\n         self._tunneledPort = port\n@@ -178,7 +178,7 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n \n     def connect(self, protocolFactory):\n         self._protocolFactory = protocolFactory\n-        connectDeferred = super(TunnelingTCP4ClientEndpoint, self).connect(protocolFactory)\n+        connectDeferred = super().connect(protocolFactory)\n         connectDeferred.addCallback(self.requestTunnel)\n         connectDeferred.addErrback(self.connectFailed)\n         return self._tunnelReadyDeferred\n@@ -215,7 +215,7 @@ class TunnelingAgent(Agent):\n \n     def __init__(self, reactor, proxyConf, contextFactory=None,\n                  connectTimeout=None, bindAddress=None, pool=None):\n-        super(TunnelingAgent, self).__init__(reactor, contextFactory, connectTimeout, bindAddress, pool)\n+        super().__init__(reactor, contextFactory, connectTimeout, bindAddress, pool)\n         self._proxyConf = proxyConf\n         self._contextFactory = contextFactory\n \n@@ -235,7 +235,7 @@ class TunnelingAgent(Agent):\n         # otherwise, same remote host connection request could reuse\n         # a cached tunneled connection to a different proxy\n         key = key + self._proxyConf\n-        return super(TunnelingAgent, self)._requestWithEndpoint(\n+        return super()._requestWithEndpoint(\n             key=key,\n             endpoint=endpoint,\n             method=method,\n@@ -249,7 +249,7 @@ class TunnelingAgent(Agent):\n class ScrapyProxyAgent(Agent):\n \n     def __init__(self, reactor, proxyURI, connectTimeout=None, bindAddress=None, pool=None):\n-        super(ScrapyProxyAgent, self).__init__(\n+        super().__init__(\n             reactor=reactor,\n             connectTimeout=connectTimeout,\n             bindAddress=bindAddress,\n\n@@ -47,7 +47,7 @@ class ScrapyClientTLSOptions(ClientTLSOptions):\n     \"\"\"\n \n     def __init__(self, hostname, ctx, verbose_logging=False):\n-        super(ScrapyClientTLSOptions, self).__init__(hostname, ctx)\n+        super().__init__(hostname, ctx)\n         self.verbose_logging = verbose_logging\n \n     def _identityVerifyingInfoCallback(self, connection, where, ret):\n\n@@ -34,7 +34,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         return build_component_list(settings.getwithbase('SPIDER_MIDDLEWARES'))\n \n     def _add_middleware(self, mw):\n-        super(SpiderMiddlewareManager, self)._add_middleware(mw)\n+        super()._add_middleware(mw)\n         if hasattr(mw, 'process_spider_input'):\n             self.methods['process_spider_input'].append(mw.process_spider_input)\n         if hasattr(mw, 'process_start_requests'):\n\n@@ -277,7 +277,7 @@ class CrawlerProcess(CrawlerRunner):\n     \"\"\"\n \n     def __init__(self, settings=None, install_root_handler=True):\n-        super(CrawlerProcess, self).__init__(settings)\n+        super().__init__(settings)\n         install_shutdown_handlers(self._signal_shutdown)\n         configure_logging(self.settings, install_root_handler)\n         log_scrapy_info(self.settings)\n\n@@ -92,7 +92,7 @@ class MetaRefreshMiddleware(BaseRedirectMiddleware):\n     enabled_setting = 'METAREFRESH_ENABLED'\n \n     def __init__(self, settings):\n-        super(MetaRefreshMiddleware, self).__init__(settings)\n+        super().__init__(settings)\n         self._ignore_tags = settings.getlist('METAREFRESH_IGNORE_TAGS')\n         self._maxdelay = settings.getint('METAREFRESH_MAXDELAY')\n \n\n@@ -37,7 +37,7 @@ class CloseSpider(Exception):\n     \"\"\"Raise this from callbacks to request the spider to be closed\"\"\"\n \n     def __init__(self, reason='cancelled'):\n-        super(CloseSpider, self).__init__()\n+        super().__init__()\n         self.reason = reason\n \n \n@@ -74,7 +74,7 @@ class UsageError(Exception):\n \n     def __init__(self, *a, **kw):\n         self.print_help = kw.pop('print_help', True)\n-        super(UsageError, self).__init__(*a, **kw)\n+        super().__init__(*a, **kw)\n \n \n class ScrapyDeprecationWarning(Warning):\n\n@@ -301,7 +301,7 @@ class PythonItemExporter(BaseItemExporter):\n \n     def _configure(self, options, dont_fail=False):\n         self.binary = options.pop('binary', True)\n-        super(PythonItemExporter, self)._configure(options, dont_fail)\n+        super()._configure(options, dont_fail)\n         if self.binary:\n             warnings.warn(\n                 \"PythonItemExporter will drop support for binary export in the future\",\n\n@@ -8,7 +8,7 @@ class Headers(CaselessDict):\n \n     def __init__(self, seq=None, encoding='utf-8'):\n         self.encoding = encoding\n-        super(Headers, self).__init__(seq)\n+        super().__init__(seq)\n \n     def normkey(self, key):\n         \"\"\"Normalize key to bytes\"\"\"\n@@ -37,19 +37,19 @@ class Headers(CaselessDict):\n \n     def __getitem__(self, key):\n         try:\n-            return super(Headers, self).__getitem__(key)[-1]\n+            return super().__getitem__(key)[-1]\n         except IndexError:\n             return None\n \n     def get(self, key, def_val=None):\n         try:\n-            return super(Headers, self).get(key, def_val)[-1]\n+            return super().get(key, def_val)[-1]\n         except IndexError:\n             return None\n \n     def getlist(self, key, def_val=None):\n         try:\n-            return super(Headers, self).__getitem__(key)\n+            return super().__getitem__(key)\n         except KeyError:\n             if def_val is not None:\n                 return self.normvalue(def_val)\n\n@@ -24,7 +24,7 @@ class FormRequest(Request):\n         if formdata and kwargs.get('method') is None:\n             kwargs['method'] = 'POST'\n \n-        super(FormRequest, self).__init__(*args, **kwargs)\n+        super().__init__(*args, **kwargs)\n \n         if formdata:\n             items = formdata.items() if isinstance(formdata, dict) else formdata\n\n@@ -32,7 +32,7 @@ class JsonRequest(Request):\n             if 'method' not in kwargs:\n                 kwargs['method'] = 'POST'\n \n-        super(JsonRequest, self).__init__(*args, **kwargs)\n+        super().__init__(*args, **kwargs)\n         self.headers.setdefault('Content-Type', 'application/json')\n         self.headers.setdefault('Accept', 'application/json, text/javascript, */*; q=0.01')\n \n@@ -47,7 +47,7 @@ class JsonRequest(Request):\n         elif not body_passed and data_passed:\n             kwargs['body'] = self._dumps(data)\n \n-        return super(JsonRequest, self).replace(*args, **kwargs)\n+        return super().replace(*args, **kwargs)\n \n     def _dumps(self, data):\n         \"\"\"Convert to JSON \"\"\"\n\n@@ -31,5 +31,5 @@ class XmlRpcRequest(Request):\n         if encoding is not None:\n             kwargs['encoding'] = encoding\n \n-        super(XmlRpcRequest, self).__init__(*args, **kwargs)\n+        super().__init__(*args, **kwargs)\n         self.headers.setdefault('Content-Type', 'text/xml')\n\n@@ -35,13 +35,13 @@ class TextResponse(Response):\n         self._cached_benc = None\n         self._cached_ubody = None\n         self._cached_selector = None\n-        super(TextResponse, self).__init__(*args, **kwargs)\n+        super().__init__(*args, **kwargs)\n \n     def _set_url(self, url):\n         if isinstance(url, str):\n             self._url = to_unicode(url, self.encoding)\n         else:\n-            super(TextResponse, self)._set_url(url)\n+            super()._set_url(url)\n \n     def _set_body(self, body):\n         self._body = b''  # used by encoding detection\n@@ -51,7 +51,7 @@ class TextResponse(Response):\n                                 type(self).__name__)\n             self._body = body.encode(self._encoding)\n         else:\n-            super(TextResponse, self)._set_body(body)\n+            super()._set_body(body)\n \n     def replace(self, *args, **kwargs):\n         kwargs.setdefault('encoding', self.encoding)\n@@ -166,7 +166,7 @@ class TextResponse(Response):\n         elif isinstance(url, parsel.SelectorList):\n             raise ValueError(\"SelectorList is not supported\")\n         encoding = self.encoding if encoding is None else encoding\n-        return super(TextResponse, self).follow(\n+        return super().follow(\n             url=url,\n             callback=callback,\n             method=method,\n@@ -226,7 +226,7 @@ class TextResponse(Response):\n             for sel in selectors:\n                 with suppress(_InvalidSelector):\n                     urls.append(_url_from_selector(sel))\n-        return super(TextResponse, self).follow_all(\n+        return super().follow_all(\n             urls=urls,\n             callback=callback,\n             method=method,\n\n@@ -39,7 +39,7 @@ class BaseItem(_BaseItem, metaclass=_BaseItemMeta):\n         if issubclass(cls, BaseItem) and not issubclass(cls, (Item, DictItem)):\n             warn('scrapy.item.BaseItem is deprecated, please use scrapy.item.Item instead',\n                  ScrapyDeprecationWarning, stacklevel=2)\n-        return super(BaseItem, cls).__new__(cls, *args, **kwargs)\n+        return super().__new__(cls, *args, **kwargs)\n \n \n class Field(dict):\n@@ -55,7 +55,7 @@ class ItemMeta(_BaseItemMeta):\n     def __new__(mcs, class_name, bases, attrs):\n         classcell = attrs.pop('__classcell__', None)\n         new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))\n-        _class = super(ItemMeta, mcs).__new__(mcs, 'x_' + class_name, new_bases, attrs)\n+        _class = super().__new__(mcs, 'x_' + class_name, new_bases, attrs)\n \n         fields = getattr(_class, 'fields', {})\n         new_attrs = {}\n@@ -70,7 +70,7 @@ class ItemMeta(_BaseItemMeta):\n         new_attrs['_class'] = _class\n         if classcell is not None:\n             new_attrs['__classcell__'] = classcell\n-        return super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs)\n+        return super().__new__(mcs, class_name, bases, new_attrs)\n \n \n class DictItem(MutableMapping, BaseItem):\n@@ -81,7 +81,7 @@ class DictItem(MutableMapping, BaseItem):\n         if issubclass(cls, DictItem) and not issubclass(cls, Item):\n             warn('scrapy.item.DictItem is deprecated, please use scrapy.item.Item instead',\n                  ScrapyDeprecationWarning, stacklevel=2)\n-        return super(DictItem, cls).__new__(cls, *args, **kwargs)\n+        return super().__new__(cls, *args, **kwargs)\n \n     def __init__(self, *args, **kwargs):\n         self._values = {}\n@@ -109,7 +109,7 @@ class DictItem(MutableMapping, BaseItem):\n     def __setattr__(self, name, value):\n         if not name.startswith('_'):\n             raise AttributeError(\"Use item[%r] = %r to set field value\" % (name, value))\n-        super(DictItem, self).__setattr__(name, value)\n+        super().__setattr__(name, value)\n \n     def __len__(self):\n         return len(self._values)\n\n@@ -65,7 +65,7 @@ class FilteringLinkExtractor:\n             warn('scrapy.linkextractors.FilteringLinkExtractor is deprecated, '\n                  'please use scrapy.linkextractors.LinkExtractor instead',\n                  ScrapyDeprecationWarning, stacklevel=2)\n-        return super(FilteringLinkExtractor, cls).__new__(cls)\n+        return super().__new__(cls)\n \n     def __init__(self, link_extractor, allow, deny, allow_domains, deny_domains,\n                  restrict_xpaths, canonicalize, deny_extensions, restrict_css, restrict_text):\n\n@@ -126,7 +126,7 @@ class LxmlLinkExtractor(FilteringLinkExtractor):\n             strip=strip,\n             canonicalized=canonicalize\n         )\n-        super(LxmlLinkExtractor, self).__init__(\n+        super().__init__(\n             link_extractor=lx,\n             allow=allow,\n             deny=deny,\n\n@@ -376,7 +376,7 @@ class FilesPipeline(MediaPipeline):\n             resolve('FILES_RESULT_FIELD'), self.FILES_RESULT_FIELD\n         )\n \n-        super(FilesPipeline, self).__init__(download_func=download_func, settings=settings)\n+        super().__init__(download_func=download_func, settings=settings)\n \n     @classmethod\n     def from_settings(cls, settings):\n\n@@ -45,8 +45,7 @@ class ImagesPipeline(FilesPipeline):\n     DEFAULT_IMAGES_RESULT_FIELD = 'images'\n \n     def __init__(self, store_uri, download_func=None, settings=None):\n-        super(ImagesPipeline, self).__init__(store_uri, settings=settings,\n-                                             download_func=download_func)\n+        super().__init__(store_uri, settings=settings, download_func=download_func)\n \n         if isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n\n@@ -17,7 +17,7 @@ class CachingThreadedResolver(ThreadedResolver):\n     \"\"\"\n \n     def __init__(self, reactor, cache_size, timeout):\n-        super(CachingThreadedResolver, self).__init__(reactor)\n+        super().__init__(reactor)\n         dnscache.limit = cache_size\n         self.timeout = timeout\n \n@@ -40,7 +40,7 @@ class CachingThreadedResolver(ThreadedResolver):\n         # so the input argument above is simply overridden\n         # to enforce Scrapy's DNS_TIMEOUT setting's value\n         timeout = (self.timeout,)\n-        d = super(CachingThreadedResolver, self).getHostByName(name, timeout)\n+        d = super().getHostByName(name, timeout)\n         if dnscache.limit:\n             d.addCallback(self._cache_result, name)\n         return d\n@@ -80,16 +80,16 @@ class CachingHostnameResolver:\n         class CachingResolutionReceiver(resolutionReceiver):\n \n             def resolutionBegan(self, resolution):\n-                super(CachingResolutionReceiver, self).resolutionBegan(resolution)\n+                super().resolutionBegan(resolution)\n                 self.resolution = resolution\n                 self.resolved = False\n \n             def addressResolved(self, address):\n-                super(CachingResolutionReceiver, self).addressResolved(address)\n+                super().addressResolved(address)\n                 self.resolved = True\n \n             def resolutionComplete(self):\n-                super(CachingResolutionReceiver, self).resolutionComplete()\n+                super().resolutionComplete()\n                 if self.resolved:\n                     dnscache[hostName] = self.resolution\n \n\n@@ -79,4 +79,4 @@ class Selector(_ParselSelector, object_ref):\n             kwargs.setdefault('base_url', response.url)\n \n         self.response = response\n-        super(Selector, self).__init__(text=text, type=st, root=root, **kwargs)\n+        super().__init__(text=text, type=st, root=root, **kwargs)\n\n@@ -439,7 +439,7 @@ class Settings(BaseSettings):\n         # Do not pass kwarg values here. We don't want to promote user-defined\n         # dicts, and we want to update, not replace, default dicts with the\n         # values given by the user\n-        super(Settings, self).__init__()\n+        super().__init__()\n         self.setmodule(default_settings, 'default')\n         # Promote default dictionaries to BaseSettings instances for per-key\n         # priorities\n\n@@ -15,7 +15,7 @@ class HttpError(IgnoreRequest):\n \n     def __init__(self, response, *args, **kwargs):\n         self.response = response\n-        super(HttpError, self).__init__(*args, **kwargs)\n+        super().__init__(*args, **kwargs)\n \n \n class HttpErrorMiddleware:\n\n@@ -75,7 +75,7 @@ class CrawlSpider(Spider):\n     rules = ()\n \n     def __init__(self, *a, **kw):\n-        super(CrawlSpider, self).__init__(*a, **kw)\n+        super().__init__(*a, **kw)\n         self._compile_rules()\n \n     def _parse(self, response, **kwargs):\n@@ -145,6 +145,6 @@ class CrawlSpider(Spider):\n \n     @classmethod\n     def from_crawler(cls, crawler, *args, **kwargs):\n-        spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)\n+        spider = super().from_crawler(crawler, *args, **kwargs)\n         spider._follow_links = crawler.settings.getbool('CRAWLSPIDER_FOLLOW_LINKS', True)\n         return spider\n\n@@ -6,7 +6,7 @@ class InitSpider(Spider):\n     \"\"\"Base Spider with initialization facilities\"\"\"\n \n     def start_requests(self):\n-        self._postinit_reqs = super(InitSpider, self).start_requests()\n+        self._postinit_reqs = super().start_requests()\n         return iterate_spider_output(self.init_request())\n \n     def initialized(self, response=None):\n\n@@ -18,7 +18,7 @@ class SitemapSpider(Spider):\n     sitemap_alternate_links = False\n \n     def __init__(self, *a, **kw):\n-        super(SitemapSpider, self).__init__(*a, **kw)\n+        super().__init__(*a, **kw)\n         self._cbs = []\n         for r, c in self.sitemap_rules:\n             if isinstance(c, str):\n\n@@ -20,7 +20,7 @@ def _with_mkdir(queue_class):\n             if not os.path.exists(dirname):\n                 os.makedirs(dirname, exist_ok=True)\n \n-            super(DirectoriesCreated, self).__init__(path, *args, **kwargs)\n+            super().__init__(path, *args, **kwargs)\n \n     return DirectoriesCreated\n \n@@ -31,10 +31,10 @@ def _serializable_queue(queue_class, serialize, deserialize):\n \n         def push(self, obj):\n             s = serialize(obj)\n-            super(SerializableQueue, self).push(s)\n+            super().push(s)\n \n         def pop(self):\n-            s = super(SerializableQueue, self).pop()\n+            s = super().pop()\n             if s:\n                 return deserialize(s)\n \n@@ -47,7 +47,7 @@ def _scrapy_serialization_queue(queue_class):\n \n         def __init__(self, crawler, key):\n             self.spider = crawler.spider\n-            super(ScrapyRequestQueue, self).__init__(key)\n+            super().__init__(key)\n \n         @classmethod\n         def from_crawler(cls, crawler, key, *args, **kwargs):\n@@ -55,10 +55,10 @@ def _scrapy_serialization_queue(queue_class):\n \n         def push(self, request):\n             request = request_to_dict(request, self.spider)\n-            return super(ScrapyRequestQueue, self).push(request)\n+            return super().push(request)\n \n         def pop(self):\n-            request = super(ScrapyRequestQueue, self).pop()\n+            request = super().pop()\n \n             if not request:\n                 return None\n\n@@ -54,7 +54,7 @@ class StatsCollector:\n class MemoryStatsCollector(StatsCollector):\n \n     def __init__(self, crawler):\n-        super(MemoryStatsCollector, self).__init__(crawler)\n+        super().__init__(crawler)\n         self.spider_stats = {}\n \n     def _persist_stats(self, stats, spider):\n\n@@ -15,7 +15,7 @@ class CaselessDict(dict):\n     __slots__ = ()\n \n     def __init__(self, seq=None):\n-        super(CaselessDict, self).__init__()\n+        super().__init__()\n         if seq:\n             self.update(seq)\n \n@@ -53,7 +53,7 @@ class CaselessDict(dict):\n     def update(self, seq):\n         seq = seq.items() if isinstance(seq, Mapping) else seq\n         iseq = ((self.normkey(k), self.normvalue(v)) for k, v in seq)\n-        super(CaselessDict, self).update(iseq)\n+        super().update(iseq)\n \n     @classmethod\n     def fromkeys(cls, keys, value=None):\n@@ -70,14 +70,14 @@ class LocalCache(collections.OrderedDict):\n     \"\"\"\n \n     def __init__(self, limit=None):\n-        super(LocalCache, self).__init__()\n+        super().__init__()\n         self.limit = limit\n \n     def __setitem__(self, key, value):\n         if self.limit:\n             while len(self) >= self.limit:\n                 self.popitem(last=False)\n-        super(LocalCache, self).__setitem__(key, value)\n+        super().__setitem__(key, value)\n \n \n class LocalWeakReferencedCache(weakref.WeakKeyDictionary):\n@@ -93,18 +93,18 @@ class LocalWeakReferencedCache(weakref.WeakKeyDictionary):\n     \"\"\"\n \n     def __init__(self, limit=None):\n-        super(LocalWeakReferencedCache, self).__init__()\n+        super().__init__()\n         self.data = LocalCache(limit=limit)\n \n     def __setitem__(self, key, value):\n         try:\n-            super(LocalWeakReferencedCache, self).__setitem__(key, value)\n+            super().__setitem__(key, value)\n         except TypeError:\n             pass  # key is not weak-referenceable, skip caching\n \n     def __getitem__(self, key):\n         try:\n-            return super(LocalWeakReferencedCache, self).__getitem__(key)\n+            return super().__getitem__(key)\n         except (TypeError, KeyError):\n             return None  # key is either not weak-referenceable or not cached\n \n\n@@ -57,7 +57,7 @@ def create_deprecated_class(\n         warned_on_subclass = False\n \n         def __new__(metacls, name, bases, clsdict_):\n-            cls = super(DeprecatedClass, metacls).__new__(metacls, name, bases, clsdict_)\n+            cls = super().__new__(metacls, name, bases, clsdict_)\n             if metacls.deprecated_class is None:\n                 metacls.deprecated_class = cls\n             return cls\n@@ -73,7 +73,7 @@ def create_deprecated_class(\n                 if warn_once:\n                     msg += ' (warning only on first subclass, there may be others)'\n                 warnings.warn(msg, warn_category, stacklevel=2)\n-            super(DeprecatedClass, cls).__init__(name, bases, clsdict_)\n+            super().__init__(name, bases, clsdict_)\n \n         # see https://www.python.org/dev/peps/pep-3119/#overloading-isinstance-and-issubclass\n         # and https://docs.python.org/reference/datamodel.html#customizing-instance-and-subclass-checks\n@@ -88,7 +88,7 @@ def create_deprecated_class(\n                 # is the deprecated class itself - subclasses of the\n                 # deprecated class should not use custom `__subclasscheck__`\n                 # method.\n-                return super(DeprecatedClass, cls).__subclasscheck__(sub)\n+                return super().__subclasscheck__(sub)\n \n             if not inspect.isclass(sub):\n                 raise TypeError(\"issubclass() arg 1 must be a class\")\n@@ -102,7 +102,7 @@ def create_deprecated_class(\n                 msg = instance_warn_message.format(cls=_clspath(cls, old_class_path),\n                                                    new=_clspath(new_class, new_class_path))\n                 warnings.warn(msg, warn_category, stacklevel=2)\n-            return super(DeprecatedClass, cls).__call__(*args, **kwargs)\n+            return super().__call__(*args, **kwargs)\n \n     deprecated_cls = DeprecatedClass(name, (new_class,), clsdict or {})\n \n\n@@ -176,7 +176,7 @@ class LogCounterHandler(logging.Handler):\n     \"\"\"Record log levels count into a crawler stats\"\"\"\n \n     def __init__(self, crawler, *args, **kwargs):\n-        super(LogCounterHandler, self).__init__(*args, **kwargs)\n+        super().__init__(*args, **kwargs)\n         self.crawler = crawler\n \n     def emit(self, record):\n\n@@ -33,7 +33,7 @@ class ScrapyJSONEncoder(json.JSONEncoder):\n         elif isinstance(o, Response):\n             return \"<%s %s %s>\" % (type(o).__name__, o.status, o.url)\n         else:\n-            return super(ScrapyJSONEncoder, self).default(o)\n+            return super().default(o)\n \n \n class ScrapyJSONDecoder(json.JSONDecoder):\n\n@@ -7,12 +7,12 @@ class SiteTest:\n \n     def setUp(self):\n         from twisted.internet import reactor\n-        super(SiteTest, self).setUp()\n+        super().setUp()\n         self.site = reactor.listenTCP(0, test_site(), interface=\"127.0.0.1\")\n         self.baseurl = \"http://localhost:%d/\" % self.site.getHost().port\n \n     def tearDown(self):\n-        super(SiteTest, self).tearDown()\n+        super().tearDown()\n         self.site.stopListening()\n \n     def url(self, path):\n\n@@ -19,7 +19,7 @@ from scrapy.utils.test import get_from_asyncio_queue\n \n class MockServerSpider(Spider):\n     def __init__(self, mockserver=None, *args, **kwargs):\n-        super(MockServerSpider, self).__init__(*args, **kwargs)\n+        super().__init__(*args, **kwargs)\n         self.mockserver = mockserver\n \n \n@@ -28,7 +28,7 @@ class MetaSpider(MockServerSpider):\n     name = 'meta'\n \n     def __init__(self, *args, **kwargs):\n-        super(MetaSpider, self).__init__(*args, **kwargs)\n+        super().__init__(*args, **kwargs)\n         self.meta = {}\n \n     def closed(self, reason):\n@@ -41,7 +41,7 @@ class FollowAllSpider(MetaSpider):\n     link_extractor = LinkExtractor()\n \n     def __init__(self, total=10, show=20, order=\"rand\", maxlatency=0.0, *args, **kwargs):\n-        super(FollowAllSpider, self).__init__(*args, **kwargs)\n+        super().__init__(*args, **kwargs)\n         self.urls_visited = []\n         self.times = []\n         qargs = {'total': total, 'show': show, 'order': order, 'maxlatency': maxlatency}\n@@ -60,7 +60,7 @@ class DelaySpider(MetaSpider):\n     name = 'delay'\n \n     def __init__(self, n=1, b=0, *args, **kwargs):\n-        super(DelaySpider, self).__init__(*args, **kwargs)\n+        super().__init__(*args, **kwargs)\n         self.n = n\n         self.b = b\n         self.t1 = self.t2 = self.t2_err = 0\n@@ -82,7 +82,7 @@ class SimpleSpider(MetaSpider):\n     name = 'simple'\n \n     def __init__(self, url=\"http://localhost:8998\", *args, **kwargs):\n-        super(SimpleSpider, self).__init__(*args, **kwargs)\n+        super().__init__(*args, **kwargs)\n         self.start_urls = [url]\n \n     def parse(self, response):\n@@ -153,7 +153,7 @@ class ItemSpider(FollowAllSpider):\n     name = 'item'\n \n     def parse(self, response):\n-        for request in super(ItemSpider, self).parse(response):\n+        for request in super().parse(response):\n             yield request\n             yield Item()\n             yield {}\n@@ -172,7 +172,7 @@ class ErrorSpider(FollowAllSpider):\n         raise self.exception_cls('Expected exception')\n \n     def parse(self, response):\n-        for request in super(ErrorSpider, self).parse(response):\n+        for request in super().parse(response):\n             yield request\n             self.raise_exception()\n \n@@ -183,7 +183,7 @@ class BrokenStartRequestsSpider(FollowAllSpider):\n     fail_yielding = False\n \n     def __init__(self, *a, **kw):\n-        super(BrokenStartRequestsSpider, self).__init__(*a, **kw)\n+        super().__init__(*a, **kw)\n         self.seedsseen = []\n \n     def start_requests(self):\n@@ -201,7 +201,7 @@ class BrokenStartRequestsSpider(FollowAllSpider):\n \n     def parse(self, response):\n         self.seedsseen.append(response.meta.get('seed'))\n-        for req in super(BrokenStartRequestsSpider, self).parse(response):\n+        for req in super().parse(response):\n             yield req\n \n \n@@ -243,7 +243,7 @@ class DuplicateStartRequestsSpider(MockServerSpider):\n                 yield Request(url, dont_filter=self.dont_filter)\n \n     def __init__(self, url=\"http://localhost:8998\", *args, **kwargs):\n-        super(DuplicateStartRequestsSpider, self).__init__(*args, **kwargs)\n+        super().__init__(*args, **kwargs)\n         self.visited = 0\n \n     def parse(self, response):\n\n@@ -17,7 +17,7 @@ class ParseCommandTest(ProcessTest, SiteTest, CommandTest):\n     command = 'parse'\n \n     def setUp(self):\n-        super(ParseCommandTest, self).setUp()\n+        super().setUp()\n         self.spider_name = 'parse_spider'\n         fname = abspath(join(self.proj_mod_path, 'spiders', 'myspider.py'))\n         with open(fname, 'w') as f:\n\n@@ -151,7 +151,7 @@ def get_permissions_dict(path, renamings=None, ignore=None):\n class StartprojectTemplatesTest(ProjectTest):\n \n     def setUp(self):\n-        super(StartprojectTemplatesTest, self).setUp()\n+        super().setUp()\n         self.tmpl = join(self.temp_path, 'templates')\n         self.tmpl_proj = join(self.tmpl, 'project')\n \n@@ -315,7 +315,7 @@ class StartprojectTemplatesTest(ProjectTest):\n class CommandTest(ProjectTest):\n \n     def setUp(self):\n-        super(CommandTest, self).setUp()\n+        super().setUp()\n         self.call('startproject', self.project_name)\n         self.cwd = join(self.temp_path, self.project_name)\n         self.env['SCRAPY_SETTINGS_MODULE'] = '%s.settings' % self.project_name\n\n@@ -378,7 +378,7 @@ class ContractsManagerTest(unittest.TestCase):\n             name = 'test_same_url'\n \n             def __init__(self, *args, **kwargs):\n-                super(TestSameUrlSpider, self).__init__(*args, **kwargs)\n+                super().__init__(*args, **kwargs)\n                 self.visited = 0\n \n             def start_requests(s):\n\n@@ -530,7 +530,7 @@ class Https11InvalidDNSId(Https11TestCase):\n     \"\"\"Connect to HTTPS hosts with IP while certificate uses domain names IDs.\"\"\"\n \n     def setUp(self):\n-        super(Https11InvalidDNSId, self).setUp()\n+        super().setUp()\n         self.host = '127.0.0.1'\n \n \n@@ -549,7 +549,7 @@ class Https11InvalidDNSPattern(Https11TestCase):\n             'SSL connection certificate: issuer \"/C=IE/O=Scrapy/CN=127.0.0.1\", '\n             'subject \"/C=IE/O=Scrapy/CN=127.0.0.1\"'\n         )\n-        super(Https11InvalidDNSPattern, self).setUp()\n+        super().setUp()\n \n \n class Https11CustomCiphers(unittest.TestCase):\n\n@@ -134,7 +134,7 @@ class DbmStorageWithCustomDbmModuleTest(DbmStorageTest):\n \n     def _get_settings(self, **new_settings):\n         new_settings.setdefault('HTTPCACHE_DBM_MODULE', self.dbm_module)\n-        return super(DbmStorageWithCustomDbmModuleTest, self)._get_settings(**new_settings)\n+        return super()._get_settings(**new_settings)\n \n     def test_custom_dbm_module_loaded(self):\n         # make sure our dbm module has been loaded\n@@ -151,7 +151,7 @@ class FilesystemStorageGzipTest(FilesystemStorageTest):\n \n     def _get_settings(self, **new_settings):\n         new_settings.setdefault('HTTPCACHE_GZIP', True)\n-        return super(FilesystemStorageTest, self)._get_settings(**new_settings)\n+        return super()._get_settings(**new_settings)\n \n \n class DummyPolicyTest(_BaseTest):\n\n@@ -189,7 +189,7 @@ class RobotsTxtMiddlewareWithRerpTest(RobotsTxtMiddlewareTest):\n         skip = \"Rerp parser is not installed\"\n \n     def setUp(self):\n-        super(RobotsTxtMiddlewareWithRerpTest, self).setUp()\n+        super().setUp()\n         self.crawler.settings.set('ROBOTSTXT_PARSER', 'scrapy.robotstxt.RerpRobotParser')\n \n \n@@ -198,5 +198,5 @@ class RobotsTxtMiddlewareWithReppyTest(RobotsTxtMiddlewareTest):\n         skip = \"Reppy parser is not installed\"\n \n     def setUp(self):\n-        super(RobotsTxtMiddlewareWithReppyTest, self).setUp()\n+        super().setUp()\n         self.crawler.settings.set('ROBOTSTXT_PARSER', 'scrapy.robotstxt.ReppyRobotParser')\n\n@@ -593,7 +593,7 @@ class CustomExporterItemTest(unittest.TestCase):\n                 if name == 'age':\n                     return str(int(value) + 1)\n                 else:\n-                    return super(CustomItemExporter, self).serialize_field(field, name, value)\n+                    return super().serialize_field(field, name, value)\n \n         i = self.item_class(name='John', age='22')\n         a = ItemAdapter(i)\n\n@@ -1265,7 +1265,7 @@ class JsonRequestTest(RequestTest):\n \n     def setUp(self):\n         warnings.simplefilter(\"always\")\n-        super(JsonRequestTest, self).setUp()\n+        super().setUp()\n \n     def test_data(self):\n         r1 = self.request_class(url=\"http://www.example.com/\")\n@@ -1419,7 +1419,7 @@ class JsonRequestTest(RequestTest):\n \n     def tearDown(self):\n         warnings.resetwarnings()\n-        super(JsonRequestTest, self).tearDown()\n+        super().tearDown()\n \n \n if __name__ == \"__main__\":\n\n@@ -305,7 +305,7 @@ class TextResponseTest(BaseResponseTest):\n     response_class = TextResponse\n \n     def test_replace(self):\n-        super(TextResponseTest, self).test_replace()\n+        super().test_replace()\n         r1 = self.response_class(\"http://www.example.com\", body=\"hello\", encoding=\"cp852\")\n         r2 = r1.replace(url=\"http://www.example.com/other\")\n         r3 = r1.replace(url=\"http://www.example.com/other\", encoding=\"latin1\")\n\n@@ -312,7 +312,7 @@ class ItemMetaClassCellRegression(unittest.TestCase):\n                 # requirement. When not done properly raises an error:\n                 # TypeError: __class__ set to <class '__main__.MyItem'>\n                 # defining 'MyItem' as <class '__main__.MyItem'>\n-                super(MyItem, self).__init__(*args, **kwargs)\n+                super().__init__(*args, **kwargs)\n \n \n class DictItemTest(unittest.TestCase):\n\n@@ -516,7 +516,7 @@ class LxmlLinkExtractorTestCase(Base.LinkExtractorTestCase):\n         ])\n \n     def test_restrict_xpaths_with_html_entities(self):\n-        super(LxmlLinkExtractorTestCase, self).test_restrict_xpaths_with_html_entities()\n+        super().test_restrict_xpaths_with_html_entities()\n \n     def test_filteringlinkextractor_deprecation_warning(self):\n         \"\"\"Make sure the FilteringLinkExtractor deprecation warning is not\n\n@@ -250,7 +250,7 @@ class TestOutputProcessorItem(unittest.TestCase):\n             temp = Field()\n \n             def __init__(self, *args, **kwargs):\n-                super(TempItem, self).__init__(self, *args, **kwargs)\n+                super().__init__(self, *args, **kwargs)\n                 self.setdefault('temp', 0.3)\n \n         class TempLoader(ItemLoader):\n\n@@ -579,7 +579,7 @@ class TestOutputProcessorDict(unittest.TestCase):\n \n         class TempDict(dict):\n             def __init__(self, *args, **kwargs):\n-                super(TempDict, self).__init__(self, *args, **kwargs)\n+                super().__init__(self, *args, **kwargs)\n                 self.setdefault('temp', 0.3)\n \n         class TempLoader(ItemLoader):\n\n@@ -118,7 +118,7 @@ class LogFormatterTestCase(unittest.TestCase):\n \n class LogFormatterSubclass(LogFormatter):\n     def crawled(self, request, response, spider):\n-        kwargs = super(LogFormatterSubclass, self).crawled(request, response, spider)\n+        kwargs = super().crawled(request, response, spider)\n         CRAWLEDMSG = (\n             \"Crawled (%(status)s) %(request)s (referer: %(referer)s) %(flags)s\"\n         )\n\n@@ -53,7 +53,7 @@ class TestMiddlewareManager(MiddlewareManager):\n         return ['tests.test_middleware.%s' % x for x in ['M1', 'MOff', 'M3']]\n \n     def _add_middleware(self, mw):\n-        super(TestMiddlewareManager, self)._add_middleware(mw)\n+        super()._add_middleware(mw)\n         if hasattr(mw, 'process'):\n             self.methods['process'].append(mw.process)\n \n\n@@ -162,18 +162,18 @@ class BaseMediaPipelineTestCase(unittest.TestCase):\n class MockedMediaPipeline(MediaPipeline):\n \n     def __init__(self, *args, **kwargs):\n-        super(MockedMediaPipeline, self).__init__(*args, **kwargs)\n+        super().__init__(*args, **kwargs)\n         self._mockcalled = []\n \n     def download(self, request, info):\n         self._mockcalled.append('download')\n-        return super(MockedMediaPipeline, self).download(request, info)\n+        return super().download(request, info)\n \n     def media_to_download(self, request, info):\n         self._mockcalled.append('media_to_download')\n         if 'result' in request.meta:\n             return request.meta.get('result')\n-        return super(MockedMediaPipeline, self).media_to_download(request, info)\n+        return super().media_to_download(request, info)\n \n     def get_media_requests(self, item, info):\n         self._mockcalled.append('get_media_requests')\n@@ -181,15 +181,15 @@ class MockedMediaPipeline(MediaPipeline):\n \n     def media_downloaded(self, response, request, info):\n         self._mockcalled.append('media_downloaded')\n-        return super(MockedMediaPipeline, self).media_downloaded(response, request, info)\n+        return super().media_downloaded(response, request, info)\n \n     def media_failed(self, failure, request, info):\n         self._mockcalled.append('media_failed')\n-        return super(MockedMediaPipeline, self).media_failed(failure, request, info)\n+        return super().media_failed(failure, request, info)\n \n     def item_completed(self, results, item, info):\n         self._mockcalled.append('item_completed')\n-        item = super(MockedMediaPipeline, self).item_completed(results, item, info)\n+        item = super().item_completed(results, item, info)\n         item['results'] = results\n         return item\n \n\n@@ -10,7 +10,7 @@ class SignalCatcherSpider(Spider):\n     name = 'signal_catcher'\n \n     def __init__(self, crawler, url, *args, **kwargs):\n-        super(SignalCatcherSpider, self).__init__(*args, **kwargs)\n+        super().__init__(*args, **kwargs)\n         crawler.signals.connect(self.on_request_left,\n                                 signal=request_left_downloader)\n         self.caught_times = 0\n\n@@ -117,7 +117,7 @@ class BaseRobotParserTest:\n class PythonRobotParserTest(BaseRobotParserTest, unittest.TestCase):\n     def setUp(self):\n         from scrapy.robotstxt import PythonRobotParser\n-        super(PythonRobotParserTest, self)._setUp(PythonRobotParser)\n+        super()._setUp(PythonRobotParser)\n \n     def test_length_based_precedence(self):\n         raise unittest.SkipTest(\"RobotFileParser does not support length based directives precedence.\")\n@@ -132,7 +132,7 @@ class ReppyRobotParserTest(BaseRobotParserTest, unittest.TestCase):\n \n     def setUp(self):\n         from scrapy.robotstxt import ReppyRobotParser\n-        super(ReppyRobotParserTest, self)._setUp(ReppyRobotParser)\n+        super()._setUp(ReppyRobotParser)\n \n     def test_order_based_precedence(self):\n         raise unittest.SkipTest(\"Reppy does not support order based directives precedence.\")\n@@ -144,7 +144,7 @@ class RerpRobotParserTest(BaseRobotParserTest, unittest.TestCase):\n \n     def setUp(self):\n         from scrapy.robotstxt import RerpRobotParser\n-        super(RerpRobotParserTest, self)._setUp(RerpRobotParser)\n+        super()._setUp(RerpRobotParser)\n \n     def test_length_based_precedence(self):\n         raise unittest.SkipTest(\"Rerp does not support length based directives precedence.\")\n@@ -156,7 +156,7 @@ class ProtegoRobotParserTest(BaseRobotParserTest, unittest.TestCase):\n \n     def setUp(self):\n         from scrapy.robotstxt import ProtegoRobotParser\n-        super(ProtegoRobotParserTest, self)._setUp(ProtegoRobotParser)\n+        super()._setUp(ProtegoRobotParser)\n \n     def test_order_based_precedence(self):\n         raise unittest.SkipTest(\"Protego does not support order based directives precedence.\")\n\n@@ -53,7 +53,7 @@ class MockCrawler(Crawler):\n             JOBDIR=jobdir,\n             DUPEFILTER_CLASS='scrapy.dupefilters.BaseDupeFilter',\n         )\n-        super(MockCrawler, self).__init__(Spider, settings)\n+        super().__init__(Spider, settings)\n         self.engine = MockEngine(downloader=MockDownloader())\n \n \n@@ -296,7 +296,7 @@ class StartUrlsSpider(Spider):\n \n     def __init__(self, start_urls):\n         self.start_urls = start_urls\n-        super(StartUrlsSpider, self).__init__(name='StartUrlsSpider')\n+        super().__init__(name='StartUrlsSpider')\n \n     def parse(self, response):\n         pass\n\n@@ -19,7 +19,7 @@ class _HttpErrorSpider(MockServerSpider):\n     bypass_status_codes = set()\n \n     def __init__(self, *args, **kwargs):\n-        super(_HttpErrorSpider, self).__init__(*args, **kwargs)\n+        super().__init__(*args, **kwargs)\n         self.start_urls = [\n             self.mockserver.url(\"/status?n=200\"),\n             self.mockserver.url(\"/status?n=404\"),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9d84289109b2368d5929d8b60ce583529c19fe4c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 10 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 29 | Churn Cumulative: 1290 | Contributors (this commit): 26 | Commits (past 90d): 12 | Contributors (cumulative): 39 | DMM Complexity: 1.0\n\nDIFF:\n@@ -7,10 +7,12 @@ import inspect\n import re\n import sys\n import weakref\n+import warnings\n from functools import partial, wraps\n from itertools import chain\n \n from scrapy.utils.decorators import deprecated\n+from scrapy.exceptions import ScrapyDeprecationWarning\n \n \n def flatten(x):\n@@ -275,10 +277,10 @@ def equal_attributes(obj1, obj2, attributes):\n     return True\n \n \n-@deprecated\n class WeakKeyCache:\n \n     def __init__(self, default_factory):\n+        warnings.warn(\"Call to deprecated Class WeakKeyCache\", category=ScrapyDeprecationWarning, stacklevel=2)\n         self.default_factory = default_factory\n         self._weakdict = weakref.WeakKeyDictionary()\n \n\n@@ -0,0 +1,3 @@\n+from scrapy.utils.decorators import deprecated\n+\n+\n\n@@ -1,15 +1,18 @@\n import functools\n+import gc\n import operator\n import platform\n import unittest\n+from itertools import count\n from sys import version_info\n from warnings import catch_warnings\n \n from scrapy.utils.python import (\n     memoizemethod_noargs, binary_is_text, equal_attributes,\n-    get_func_args, to_bytes, to_unicode,\n+    WeakKeyCache, get_func_args, to_bytes, to_unicode,\n     without_none_values, MutableChain)\n \n+\n __doctests__ = ['scrapy.utils.python']\n \n \n@@ -152,6 +155,23 @@ class UtilsPythonTestCase(unittest.TestCase):\n         a.meta['z'] = 2\n         self.assertFalse(equal_attributes(a, b, [compare_z, 'x']))\n \n+    def test_weakkeycache(self):\n+        class _Weakme:\n+            pass\n+\n+        _values = count()\n+        wk = WeakKeyCache(lambda k: next(_values))\n+        k = _Weakme()\n+        v = wk[k]\n+        self.assertEqual(v, wk[k])\n+        self.assertNotEqual(v, wk[_Weakme()])\n+        self.assertEqual(v, wk[k])\n+        del k\n+        for _ in range(100):\n+            if wk._weakdict:\n+                gc.collect()\n+        self.assertFalse(len(wk._weakdict))\n+\n     def test_get_func_args(self):\n         def f1(a, b, c):\n             pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b35d1f2b2c430f4d12cb8f9d408dfa0c0051746d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 6 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,3 +0,0 @@\n-from scrapy.utils.decorators import deprecated\n-\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#983b7ddf2e39c480efc6d104054f92f570714ac8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 852 | Contributors (this commit): 25 | Commits (past 90d): 7 | Contributors (cumulative): 25 | DMM Complexity: None\n\nDIFF:\n@@ -6,13 +6,13 @@ import gc\n import inspect\n import re\n import sys\n-import weakref\n import warnings\n+import weakref\n from functools import partial, wraps\n from itertools import chain\n \n-from scrapy.utils.decorators import deprecated\n from scrapy.exceptions import ScrapyDeprecationWarning\n+from scrapy.utils.decorators import deprecated\n \n \n def flatten(x):\n@@ -280,7 +280,7 @@ def equal_attributes(obj1, obj2, attributes):\n class WeakKeyCache:\n \n     def __init__(self, default_factory):\n-        warnings.warn(\"Call to deprecated Class WeakKeyCache\", category=ScrapyDeprecationWarning, stacklevel=2)\n+        warnings.warn(\"The WeakKeyCache class is deprecated\", category=ScrapyDeprecationWarning, stacklevel=2)\n         self.default_factory = default_factory\n         self._weakdict = weakref.WeakKeyDictionary()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4dc09f09aa9698b02f2cbf2e3001202388eba043", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 580 | Contributors (this commit): 26 | Commits (past 90d): 11 | Contributors (cumulative): 26 | DMM Complexity: None\n\nDIFF:\n@@ -23,7 +23,6 @@ install_requires = [\n     'cryptography>=2.0',\n     'cssselect>=0.9.1',\n     'itemloaders>=1.0.1',\n-    'lxml>=3.5.0',\n     'parsel>=1.5.0',\n     'PyDispatcher>=2.0.5',\n     'pyOpenSSL>=16.2.0',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1cc8d5829fc1b1b10fd852db693ac44dc9be0ef1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 3 | Churn Cumulative: 330 | Contributors (this commit): 14 | Commits (past 90d): 8 | Contributors (cumulative): 14 | DMM Complexity: 0.0\n\nDIFF:\n@@ -42,10 +42,7 @@ class Command(ScrapyCommand):\n \n     def _is_valid_name(self, project_name):\n         def _module_exists(module_name):\n-            try:\n             spec = find_spec(module_name)\n-            except ModuleNotFoundError:\n-                return False\n             return spec is not None and spec.loader is not None\n \n         if not re.search(r'^[_a-zA-Z]\\w*$', project_name):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
