{"custom_id": "scrapy#d06bb12e351f61ee20c28626b003f4b59fd689f7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 65 | Files Changed: 2 | Hunks: 21 | Methods Changed: 15 | Complexity Δ (Sum/Max): -7/0 | Churn Δ: 88 | Churn Cumulative: 721 | Contributors (this commit): 1 | Commits (past 90d): 9 | Contributors (cumulative): 2 | DMM Complexity: 0.14285714285714285\n\nDIFF:\n@@ -4,8 +4,7 @@ from typing import Dict, List\n from h2.config import H2Configuration\n from h2.connection import H2Connection\n from h2.events import (\n-    ConnectionTerminated, DataReceived, ResponseReceived, RemoteSettingsChanged,\n-    StreamEnded, StreamReset, TrailersReceived, WindowUpdated\n+    ConnectionTerminated, DataReceived, ResponseReceived, StreamEnded, StreamReset, WindowUpdated\n )\n from twisted.internet.protocol import connectionDone, Protocol\n \n@@ -13,7 +12,6 @@ from scrapy.core.http2.stream import Stream\n from scrapy.http import Request\n \n LOGGER = logging.getLogger(__name__)\n-LOGGER.debug = print\n \n \n class H2ClientProtocol(Protocol):\n@@ -44,18 +42,26 @@ class H2ClientProtocol(Protocol):\n     def _new_stream(self, request: Request):\n         \"\"\"Instantiates a new Stream object\n         \"\"\"\n-        stream = Stream(self.next_stream_id, request, self)\n+        stream = Stream(self.next_stream_id, request, self.conn)\n         self.next_stream_id += 2\n \n         self.streams[stream.stream_id] = stream\n         return stream\n \n+    def _send_pending_requests(self):\n+        # TODO: handle MAX_CONCURRENT_STREAMS\n+        # Initiate all pending requests\n+        for stream in self._pending_request_stream_pool:\n+            stream.initiate_request()\n+            self._write_to_transport()\n+\n+        self._pending_request_stream_pool.clear()\n+\n     def _write_to_transport(self):\n         \"\"\" Write data to the underlying transport connection\n         from the HTTP2 connection instance if any\n         \"\"\"\n         data = self.conn.data_to_send()\n-        if data:\n         self.transport.write(data)\n \n     def request(self, _request: Request):\n@@ -75,10 +81,11 @@ class H2ClientProtocol(Protocol):\n         \"\"\"Called by Twisted when the connection is established. We can start\n         sending some data now: we should open with the connection preamble.\n         \"\"\"\n-        LOGGER.debug(\"Connection made to {}\".format(self.transport))\n         self.conn.initiate_connection()\n         self._write_to_transport()\n \n+        self._send_pending_requests()\n+\n         self.is_connection_made = True\n \n     def dataReceived(self, data):\n@@ -89,7 +96,6 @@ class H2ClientProtocol(Protocol):\n     def connectionLost(self, reason=connectionDone):\n         \"\"\"Called by Twisted when the transport connection is lost.\n         \"\"\"\n-        LOGGER.debug(f\"connectionLost {reason}\")\n         stream_ids = list(self.streams.keys())\n \n         for stream in self._pending_request_stream_pool:\n@@ -119,47 +125,10 @@ class H2ClientProtocol(Protocol):\n                 self.stream_ended(event)\n             elif isinstance(event, StreamReset):\n                 self.stream_reset(event)\n-            elif isinstance(event, TrailersReceived):\n-                self.trailers_received(event)\n             elif isinstance(event, WindowUpdated):\n                 self.window_updated(event)\n-            elif isinstance(event, RemoteSettingsChanged):\n-                self.remote_settings_changed(event)\n-\n-    def send_headers(self, stream_id, headers):\n-        \"\"\"Send the headers for a given stream to the resource\n-        Initiates a new connection hence.\n-        This function is wrapper for :func:`~h2.connection.H2Connection.send_headers`\n-\n-        Arguments:\n-            stream_id {int} -- Valid stream id\n-            headers {List[Tuple[str, str]]} -- Headers of the request\n-        \"\"\"\n-        LOGGER.debug(f'Send Headers: stream_id={stream_id} headers={headers}')\n-        self.conn.send_headers(stream_id, headers, end_stream=False)\n-\n-    def send_data(self, stream_id, data):\n-        \"\"\"Send the data for a given stream to the resource.\n-        Requires request headers to be sent at least once before this\n-        function is called.\n-        This function is wrapper for :func:`~h2.connection.H2Connection.send_data`\n-\n-        Arguments:\n-            stream_id {int} -- Valid stream id\n-            data {bytes} -- The data to send on the stream.\n-        \"\"\"\n-        LOGGER.debug(f\"Send Data: stream_id={stream_id} data={data}\")\n-        self.conn.send_data(stream_id, data, end_stream=False)\n-\n-    def end_stream(self, stream_id):\n-        \"\"\"End the given stream.\n-        This function is wrapper for :func:`~h2.connection.H2Connection.end_stream`\n-\n-        Arguments:\n-             stream_id {int} - Valid stream id\n-        \"\"\"\n-        LOGGER.debug(f\"End Stream: stream_id={stream_id}\")\n-        self.conn.end_stream(stream_id)\n+            else:\n+                LOGGER.info(\"Received unhandled event {}\".format(event))\n \n     # Event handler functions starts here\n     def connection_terminated(self, event: ConnectionTerminated):\n@@ -173,14 +142,6 @@ class H2ClientProtocol(Protocol):\n         stream_id = event.stream_id\n         self.streams[stream_id].receive_headers(event.headers)\n \n-    def remote_settings_changed(self, event: RemoteSettingsChanged):\n-        # TODO: handle MAX_CONCURRENT_STREAMS\n-        # Initiate all pending requests\n-        for stream in self._pending_request_stream_pool:\n-            stream.initiate_request()\n-\n-        self._pending_request_stream_pool.clear()\n-\n     def stream_ended(self, event: StreamEnded):\n         stream_id = event.stream_id\n         self.streams[stream_id].end_stream()\n@@ -188,9 +149,6 @@ class H2ClientProtocol(Protocol):\n     def stream_reset(self, event: StreamReset):\n         pass\n \n-    def trailers_received(self, event: TrailersReceived):\n-        pass\n-\n     def window_updated(self, event: WindowUpdated):\n         stream_id = event.stream_id\n         if stream_id != 0:\n\n@@ -1,5 +1,6 @@\n from urllib.parse import urlparse\n \n+from h2.connection import H2Connection\n from twisted.internet.defer import Deferred\n \n from scrapy.http import Request, Response\n@@ -17,7 +18,7 @@ class Stream:\n     1. Combine all the data frames\n     \"\"\"\n \n-    def __init__(self, stream_id: int, request: Request, connection):\n+    def __init__(self, stream_id: int, request: Request, connection: H2Connection):\n         \"\"\"\n         Arguments:\n             stream_id {int} -- For one HTTP/2 connection each stream is\n@@ -27,7 +28,7 @@ class Stream:\n         \"\"\"\n         self.stream_id = stream_id\n         self._request = request\n-        self._client_protocol = connection\n+        self._conn = connection\n \n         self._request_body = self._request.body\n         self.content_length = 0 if self._request_body is None else len(self._request_body)\n@@ -70,13 +71,12 @@ class Stream:\n             # TODO: Check if scheme can be \"http\" for HTTP/2 ?\n             (\":scheme\", \"https\"),\n             (\":path\", url.path),\n-            # (\"Content-Length\", str(self.content_length))\n \n             # TODO: Make sure 'Content-Type' and 'Content-Encoding' headers\n             #  are sent for request having body\n         ]\n \n-        self._client_protocol.send_headers(self.stream_id, http2_request_headers)\n+        self._conn.send_headers(self.stream_id, http2_request_headers, end_stream=False)\n         self.send_data()\n \n     def send_data(self):\n@@ -95,10 +95,10 @@ class Stream:\n         #    3.2 Small number of requests\n \n         # Firstly, check what the flow control window is for current stream.\n-        window_size = self._client_protocol.conn.local_flow_control_window(stream_id=self.stream_id)\n+        window_size = self._conn.local_flow_control_window(stream_id=self.stream_id)\n \n         # Next, check what the maximum frame size is.\n-        max_frame_size = self._client_protocol.conn.max_outbound_frame_size\n+        max_frame_size = self._conn.max_outbound_frame_size\n \n         # We will send no more than the window size or the remaining file size\n         # of data in this call, whichever is smaller.\n@@ -111,14 +111,14 @@ class Stream:\n             data_chunk_start = self.content_length - self.remaining_content_length\n             data_chunk = self._request_body[data_chunk_start:data_chunk_start + chunk_size]\n \n-            self._client_protocol.send_data(self.stream_id, data_chunk, end_stream=False)\n+            self._conn.send_data(self.stream_id, data_chunk, end_stream=False)\n \n             bytes_to_send = max(0, bytes_to_send - chunk_size)\n             self.remaining_content_length = max(0, self.remaining_content_length - chunk_size)\n \n         # End the stream if no more data has to be send\n         if self.remaining_content_length == 0:\n-            self._client_protocol.end_stream(self.stream_id)\n+            self._conn.end_stream(self.stream_id)\n         else:\n             # TODO: Continue from here :)\n             pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ec98dabfab60283303a9208ccd8177d9f995ba72", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 386 | Lines Deleted: 153 | Files Changed: 18 | Hunks: 124 | Methods Changed: 43 | Complexity Δ (Sum/Max): -1/2 | Churn Δ: 539 | Churn Cumulative: 8829 | Contributors (this commit): 86 | Commits (past 90d): 71 | Contributors (cumulative): 250 | DMM Complexity: 0.9821428571428571\n\nDIFF:\n@@ -281,6 +281,7 @@ coverage_ignore_pyobjects = [\n # -------------------------------------\n \n intersphinx_mapping = {\n+    'attrs': ('https://www.attrs.org/en/stable/', None),\n     'coverage': ('https://coverage.readthedocs.io/en/stable', None),\n     'cssselect': ('https://cssselect.readthedocs.io/en/latest', None),\n     'pytest': ('https://docs.pytest.org/en/latest', None),\n\n@@ -1,11 +1,11 @@\n import json\n import logging\n \n+from itemadapter import is_item, ItemAdapter\n from w3lib.url import is_url\n \n from scrapy.commands import ScrapyCommand\n from scrapy.http import Request\n-from scrapy.item import _BaseItem\n from scrapy.utils import display\n from scrapy.utils.conf import arglist_to_dict\n from scrapy.utils.spider import iterate_spider_output, spidercls_for_request\n@@ -81,7 +81,7 @@ class Command(ScrapyCommand):\n             items = self.items.get(lvl, [])\n \n         print(\"# Scraped Items \", \"-\" * 60)\n-        display.pprint([dict(x) for x in items], colorize=colour)\n+        display.pprint([ItemAdapter(x).asdict() for x in items], colorize=colour)\n \n     def print_requests(self, lvl=None, colour=True):\n         if lvl is None:\n@@ -117,7 +117,7 @@ class Command(ScrapyCommand):\n         items, requests = [], []\n \n         for x in iterate_spider_output(callback(response, **cb_kwargs)):\n-            if isinstance(x, (_BaseItem, dict)):\n+            if is_item(x):\n                 items.append(x)\n             elif isinstance(x, Request):\n                 requests.append(x)\n\n@@ -1,10 +1,10 @@\n import json\n \n-from scrapy.item import _BaseItem\n-from scrapy.http import Request\n-from scrapy.exceptions import ContractFail\n+from itemadapter import is_item, ItemAdapter\n \n from scrapy.contracts import Contract\n+from scrapy.exceptions import ContractFail\n+from scrapy.http import Request\n \n \n # contracts\n@@ -48,11 +48,11 @@ class ReturnsContract(Contract):\n     \"\"\"\n \n     name = 'returns'\n-    objects = {\n-        'request': Request,\n-        'requests': Request,\n-        'item': (_BaseItem, dict),\n-        'items': (_BaseItem, dict),\n+    object_type_verifiers = {\n+        'request': lambda x: isinstance(x, Request),\n+        'requests': lambda x: isinstance(x, Request),\n+        'item': is_item,\n+        'items': is_item,\n     }\n \n     def __init__(self, *args, **kwargs):\n@@ -64,7 +64,7 @@ class ReturnsContract(Contract):\n                 % len(self.args)\n             )\n         self.obj_name = self.args[0] or None\n-        self.obj_type = self.objects[self.obj_name]\n+        self.obj_type_verifier = self.object_type_verifiers[self.obj_name]\n \n         try:\n             self.min_bound = int(self.args[1])\n@@ -79,7 +79,7 @@ class ReturnsContract(Contract):\n     def post_process(self, output):\n         occurrences = 0\n         for x in output:\n-            if isinstance(x, self.obj_type):\n+            if self.obj_type_verifier(x):\n                 occurrences += 1\n \n         assertion = (self.min_bound <= occurrences <= self.max_bound)\n@@ -103,8 +103,8 @@ class ScrapesContract(Contract):\n \n     def post_process(self, output):\n         for x in output:\n-            if isinstance(x, (_BaseItem, dict)):\n-                missing = [arg for arg in self.args if arg not in x]\n+            if is_item(x):\n+                missing = [arg for arg in self.args if arg not in ItemAdapter(x)]\n                 if missing:\n-                    raise ContractFail(\n-                        \"Missing fields: %s\" % \", \".join(missing))\n+                    missing_str = \", \".join(missing)\n+                    raise ContractFail(\"Missing fields: %s\" % missing_str)\n\n@@ -4,18 +4,18 @@ extracts information from them\"\"\"\n import logging\n from collections import deque\n \n-from twisted.python.failure import Failure\n+from itemadapter import is_item\n from twisted.internet import defer\n+from twisted.python.failure import Failure\n \n-from scrapy.utils.defer import defer_result, defer_succeed, parallel, iter_errback\n-from scrapy.utils.spider import iterate_spider_output\n-from scrapy.utils.misc import load_object, warn_on_generator_with_return_value\n-from scrapy.utils.log import logformatter_adapter, failure_to_exc_info\n-from scrapy.exceptions import CloseSpider, DropItem, IgnoreRequest\n from scrapy import signals\n-from scrapy.http import Request, Response\n-from scrapy.item import _BaseItem\n from scrapy.core.spidermw import SpiderMiddlewareManager\n+from scrapy.exceptions import CloseSpider, DropItem, IgnoreRequest\n+from scrapy.http import Request, Response\n+from scrapy.utils.defer import defer_result, defer_succeed, iter_errback, parallel\n+from scrapy.utils.log import failure_to_exc_info, logformatter_adapter\n+from scrapy.utils.misc import load_object, warn_on_generator_with_return_value\n+from scrapy.utils.spider import iterate_spider_output\n \n \n logger = logging.getLogger(__name__)\n@@ -191,7 +191,7 @@ class Scraper:\n         \"\"\"\n         if isinstance(output, Request):\n             self.crawler.engine.crawl(request=output, spider=spider)\n-        elif isinstance(output, (_BaseItem, dict)):\n+        elif is_item(output):\n             self.slot.itemproc_size += 1\n             dfd = self.itemproc.process_item(output, spider)\n             dfd.addBoth(self._itemproc_finished, output, response, spider)\n@@ -200,10 +200,11 @@ class Scraper:\n             pass\n         else:\n             typename = type(output).__name__\n-            logger.error('Spider must return Request, BaseItem, dict or None, '\n-                         'got %(typename)r in %(request)s',\n+            logger.error(\n+                'Spider must return request, item, or None, got %(typename)r in %(request)s',\n                 {'request': request, 'typename': typename},\n-                         extra={'spider': spider})\n+                extra={'spider': spider},\n+            )\n \n     def _log_download_errors(self, spider_failure, download_failure, request, spider):\n         \"\"\"Log and silence errors that come from the engine (typically download\n\n@@ -4,16 +4,18 @@ Item Exporters are used to export/serialize items into different formats.\n \n import csv\n import io\n-import pprint\n import marshal\n-import warnings\n import pickle\n+import pprint\n+import warnings\n from xml.sax.saxutils import XMLGenerator\n \n-from scrapy.utils.serialize import ScrapyJSONEncoder\n-from scrapy.utils.python import to_bytes, to_unicode, is_listlike\n-from scrapy.item import _BaseItem\n+from itemadapter import is_item, ItemAdapter\n+\n from scrapy.exceptions import ScrapyDeprecationWarning\n+from scrapy.item import _BaseItem\n+from scrapy.utils.python import is_listlike, to_bytes, to_unicode\n+from scrapy.utils.serialize import ScrapyJSONEncoder\n \n \n __all__ = ['BaseItemExporter', 'PprintItemExporter', 'PickleItemExporter',\n@@ -56,11 +58,14 @@ class BaseItemExporter:\n         \"\"\"Return the fields to export as an iterable of tuples\n         (name, serialized_value)\n         \"\"\"\n+        item = ItemAdapter(item)\n+\n         if include_empty is None:\n             include_empty = self.export_empty_fields\n+\n         if self.fields_to_export is None:\n-            if include_empty and not isinstance(item, dict):\n-                field_iter = item.fields.keys()\n+            if include_empty:\n+                field_iter = item.field_names()\n             else:\n                 field_iter = item.keys()\n         else:\n@@ -71,8 +76,8 @@ class BaseItemExporter:\n \n         for field_name in field_iter:\n             if field_name in item:\n-                field = {} if isinstance(item, dict) else item.fields[field_name]\n-                value = self.serialize_field(field, field_name, item[field_name])\n+                field_meta = item.get_field_meta(field_name)\n+                value = self.serialize_field(field_meta, field_name, item[field_name])\n             else:\n                 value = default_value\n \n@@ -297,6 +302,7 @@ class PythonItemExporter(BaseItemExporter):\n \n     .. _msgpack: https://pypi.org/project/msgpack/\n     \"\"\"\n+\n     def _configure(self, options, dont_fail=False):\n         self.binary = options.pop('binary', True)\n         super(PythonItemExporter, self)._configure(options, dont_fail)\n@@ -314,22 +320,22 @@ class PythonItemExporter(BaseItemExporter):\n     def _serialize_value(self, value):\n         if isinstance(value, _BaseItem):\n             return self.export_item(value)\n-        if isinstance(value, dict):\n-            return dict(self._serialize_dict(value))\n-        if is_listlike(value):\n+        elif is_item(value):\n+            return dict(self._serialize_item(value))\n+        elif is_listlike(value):\n             return [self._serialize_value(v) for v in value]\n         encode_func = to_bytes if self.binary else to_unicode\n         if isinstance(value, (str, bytes)):\n             return encode_func(value, encoding=self.encoding)\n         return value\n \n-    def _serialize_dict(self, value):\n-        for key, val in value.items():\n+    def _serialize_item(self, item):\n+        for key, value in ItemAdapter(item).items():\n             key = to_bytes(key) if self.binary else key\n-            yield key, self._serialize_value(val)\n+            yield key, self._serialize_value(value)\n \n     def export_item(self, item):\n         result = dict(self._get_serialized_fields(item))\n         if self.binary:\n-            result = dict(self._serialize_dict(result))\n+            result = dict(self._serialize_item(result))\n         return result\n\n@@ -36,7 +36,7 @@ class BaseItem(_BaseItem, metaclass=_BaseItemMeta):\n     \"\"\"\n \n     def __new__(cls, *args, **kwargs):\n-        if issubclass(cls, BaseItem) and not (issubclass(cls, Item) or issubclass(cls, DictItem)):\n+        if issubclass(cls, BaseItem) and not issubclass(cls, (Item, DictItem)):\n             warn('scrapy.item.BaseItem is deprecated, please use scrapy.item.Item instead',\n                  ScrapyDeprecationWarning, stacklevel=2)\n         return super(BaseItem, cls).__new__(cls, *args, **kwargs)\n\n@@ -6,6 +6,8 @@ See documentation in docs/topics/loaders.rst\n from collections import defaultdict\n from contextlib import suppress\n \n+from itemadapter import ItemAdapter\n+\n from scrapy.item import Item\n from scrapy.loader.common import wrap_loader_context\n from scrapy.loader.processors import Identity\n@@ -44,7 +46,7 @@ class ItemLoader:\n         self._local_item = context['item'] = item\n         self._local_values = defaultdict(list)\n         # values from initial item\n-        for field_name, value in item.items():\n+        for field_name, value in ItemAdapter(item).items():\n             self._values[field_name] += arg_to_iter(value)\n \n     @property\n@@ -127,13 +129,12 @@ class ItemLoader:\n         return value\n \n     def load_item(self):\n-        item = self.item\n+        adapter = ItemAdapter(self.item)\n         for field_name in tuple(self._values):\n             value = self.get_output_value(field_name)\n             if value is not None:\n-                item[field_name] = value\n-\n-        return item\n+                adapter[field_name] = value\n+        return adapter.item\n \n     def get_output_value(self, field_name):\n         proc = self.get_output_processor(field_name)\n@@ -174,11 +175,8 @@ class ItemLoader:\n                                     value, type(e).__name__, str(e)))\n \n     def _get_item_field_attr(self, field_name, key, default=None):\n-        if isinstance(self.item, Item):\n-            value = self.item.fields[field_name].get(key, default)\n-        else:\n-            value = default\n-        return value\n+        field_meta = ItemAdapter(self.item).get_field_meta(field_name)\n+        return field_meta.get(key, default)\n \n     def _check_selector_method(self):\n         if self.selector is None:\n\n@@ -10,24 +10,26 @@ import mimetypes\n import os\n import time\n from collections import defaultdict\n-from email.utils import parsedate_tz, mktime_tz\n+from contextlib import suppress\n+from email.utils import mktime_tz, parsedate_tz\n from ftplib import FTP\n from io import BytesIO\n from urllib.parse import urlparse\n \n+from itemadapter import ItemAdapter\n from twisted.internet import defer, threads\n \n+from scrapy.exceptions import IgnoreRequest, NotConfigured\n+from scrapy.http import Request\n from scrapy.pipelines.media import MediaPipeline\n from scrapy.settings import Settings\n-from scrapy.exceptions import NotConfigured, IgnoreRequest\n-from scrapy.http import Request\n-from scrapy.utils.misc import md5sum\n-from scrapy.utils.log import failure_to_exc_info\n-from scrapy.utils.python import to_bytes\n-from scrapy.utils.request import referer_str\n from scrapy.utils.boto import is_botocore\n from scrapy.utils.datatypes import CaselessDict\n from scrapy.utils.ftp import ftp_store_file\n+from scrapy.utils.log import failure_to_exc_info\n+from scrapy.utils.misc import md5sum\n+from scrapy.utils.python import to_bytes\n+from scrapy.utils.request import referer_str\n \n \n logger = logging.getLogger(__name__)\n@@ -517,7 +519,8 @@ class FilesPipeline(MediaPipeline):\n \n     # Overridable Interface\n     def get_media_requests(self, item, info):\n-        return [Request(x) for x in item.get(self.files_urls_field, [])]\n+        urls = ItemAdapter(item).get(self.files_urls_field, [])\n+        return [Request(u) for u in urls]\n \n     def file_downloaded(self, response, request, info):\n         path = self.file_path(request, response=response, info=info)\n@@ -528,8 +531,8 @@ class FilesPipeline(MediaPipeline):\n         return checksum\n \n     def item_completed(self, results, item, info):\n-        if isinstance(item, dict) or self.files_result_field in item.fields:\n-            item[self.files_result_field] = [x for ok, x in results if ok]\n+        with suppress(KeyError):\n+            ItemAdapter(item)[self.files_result_field] = [x for ok, x in results if ok]\n         return item\n \n     def file_path(self, request, response=None, info=None):\n\n@@ -5,17 +5,19 @@ See documentation in topics/media-pipeline.rst\n \"\"\"\n import functools\n import hashlib\n+from contextlib import suppress\n from io import BytesIO\n \n+from itemadapter import ItemAdapter\n from PIL import Image\n \n+from scrapy.exceptions import DropItem\n+from scrapy.http import Request\n+from scrapy.pipelines.files import FileException, FilesPipeline\n+# TODO: from scrapy.pipelines.media import MediaPipeline\n+from scrapy.settings import Settings\n from scrapy.utils.misc import md5sum\n from scrapy.utils.python import to_bytes\n-from scrapy.http import Request\n-from scrapy.settings import Settings\n-from scrapy.exceptions import DropItem\n-# TODO: from scrapy.pipelines.media import MediaPipeline\n-from scrapy.pipelines.files import FileException, FilesPipeline\n \n \n class NoimagesDrop(DropItem):\n@@ -157,11 +159,12 @@ class ImagesPipeline(FilesPipeline):\n         return image, buf\n \n     def get_media_requests(self, item, info):\n-        return [Request(x) for x in item.get(self.images_urls_field, [])]\n+        urls = ItemAdapter(item).get(self.images_urls_field, [])\n+        return [Request(u) for u in urls]\n \n     def item_completed(self, results, item, info):\n-        if isinstance(item, dict) or self.images_result_field in item.fields:\n-            item[self.images_result_field] = [x for ok, x in results if ok]\n+        with suppress(KeyError):\n+            ItemAdapter(item)[self.images_result_field] = [x for ok, x in results if ok]\n         return item\n \n     def file_path(self, request, response=None, info=None):\n\n@@ -6,6 +6,7 @@ See documentation in docs/topics/shell.rst\n import os\n import signal\n \n+from itemadapter import is_item\n from twisted.internet import threads, defer\n from twisted.python import threadable\n from w3lib.url import any_to_uri\n@@ -13,20 +14,18 @@ from w3lib.url import any_to_uri\n from scrapy.crawler import Crawler\n from scrapy.exceptions import IgnoreRequest\n from scrapy.http import Request, Response\n-from scrapy.item import _BaseItem\n from scrapy.settings import Settings\n from scrapy.spiders import Spider\n-from scrapy.utils.console import start_python_console\n+from scrapy.utils.conf import get_config\n+from scrapy.utils.console import DEFAULT_PYTHON_SHELLS, start_python_console\n from scrapy.utils.datatypes import SequenceExclude\n from scrapy.utils.misc import load_object\n from scrapy.utils.response import open_in_browser\n-from scrapy.utils.conf import get_config\n-from scrapy.utils.console import DEFAULT_PYTHON_SHELLS\n \n \n class Shell:\n \n-    relevant_classes = (Crawler, Spider, Request, Response, _BaseItem, Settings)\n+    relevant_classes = (Crawler, Spider, Request, Response, Settings)\n \n     def __init__(self, crawler, update_vars=None, code=None):\n         self.crawler = crawler\n@@ -154,7 +153,7 @@ class Shell:\n         return \"\\n\".join(\"[s] %s\" % line for line in b)\n \n     def _is_relevant(self, value):\n-        return isinstance(value, self.relevant_classes)\n+        return isinstance(value, self.relevant_classes) or is_item(value)\n \n \n def inspect_response(response, spider):\n\n@@ -31,7 +31,7 @@ class XMLFeedSpider(Spider):\n         processing required before returning the results to the framework core,\n         for example setting the item GUIDs. It receives a list of results and\n         the response which originated that results. It must return a list of\n-        results (Items or Requests).\n+        results (items or requests).\n         \"\"\"\n         return results\n \n\n@@ -2,10 +2,10 @@ import json\n import datetime\n import decimal\n \n+from itemadapter import is_item, ItemAdapter\n from twisted.internet import defer\n \n from scrapy.http import Request, Response\n-from scrapy.item import _BaseItem\n \n \n class ScrapyJSONEncoder(json.JSONEncoder):\n@@ -26,8 +26,8 @@ class ScrapyJSONEncoder(json.JSONEncoder):\n             return str(o)\n         elif isinstance(o, defer.Deferred):\n             return str(o)\n-        elif isinstance(o, _BaseItem):\n-            return dict(o)\n+        elif is_item(o):\n+            return ItemAdapter(o).asdict()\n         elif isinstance(o, Request):\n             return \"<%s %s %s>\" % (type(o).__name__, o.method, o.url)\n         elif isinstance(o, Response):\n\n@@ -80,6 +80,7 @@ setup(\n         'w3lib>=1.17.0',\n         'zope.interface>=4.1.3',\n         'protego>=0.1.15',\n+        'itemadapter>=0.1.0',\n     ],\n     extras_require=extras_require,\n )\n\n@@ -16,9 +16,11 @@ import sys\n from collections import defaultdict\n from urllib.parse import urlparse\n \n+import attr\n+from itemadapter import ItemAdapter\n from pydispatch import dispatcher\n from testfixtures import LogCapture\n-from twisted.internet import reactor, defer\n+from twisted.internet import defer, reactor\n from twisted.trial import unittest\n from twisted.web import server, static, util\n \n@@ -32,7 +34,7 @@ from scrapy.spiders import Spider\n from scrapy.utils.signal import disconnect_all\n from scrapy.utils.test import get_crawler\n \n-from tests import tests_datadir, get_testdata\n+from tests import get_testdata, tests_datadir\n \n \n class TestItem(Item):\n@@ -41,6 +43,13 @@ class TestItem(Item):\n     price = Field()\n \n \n+@attr.s\n+class AttrsItem:\n+    name = attr.ib(default=\"\")\n+    url = attr.ib(default=\"\")\n+    price = attr.ib(default=0)\n+\n+\n class TestSpider(Spider):\n     name = \"scrapytest.org\"\n     allowed_domains = [\"scrapytest.org\", \"localhost\"]\n@@ -79,6 +88,27 @@ class DictItemsSpider(TestSpider):\n     item_cls = dict\n \n \n+class AttrsItemsSpider(TestSpider):\n+    item_class = AttrsItem\n+\n+\n+try:\n+    from dataclasses import make_dataclass\n+except ImportError:\n+    DataClassItemsSpider = None\n+else:\n+    TestDataClass = make_dataclass(\"TestDataClass\", [(\"name\", str), (\"url\", str), (\"price\", int)])\n+\n+    class DataClassItemsSpider(DictItemsSpider):\n+        def parse_item(self, response):\n+            item = super().parse_item(response)\n+            return TestDataClass(\n+                name=item.get('name'),\n+                url=item.get('url'),\n+                price=item.get('price'),\n+            )\n+\n+\n class ItemZeroDivisionErrorSpider(TestSpider):\n     custom_settings = {\n         \"ITEM_PIPELINES\": {\n@@ -204,7 +234,10 @@ class EngineTest(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_crawler(self):\n-        for spider in TestSpider, DictItemsSpider:\n+\n+        for spider in (TestSpider, DictItemsSpider, AttrsItemsSpider, DataClassItemsSpider):\n+            if spider is None:\n+                continue\n             self.run = CrawlerRun(spider)\n             yield self.run.run()\n             self._assert_visited_urls()\n@@ -281,6 +314,7 @@ class EngineTest(unittest.TestCase):\n     def _assert_scraped_items(self):\n         self.assertEqual(2, len(self.run.itemresp))\n         for item, response in self.run.itemresp:\n+            item = ItemAdapter(item)\n             self.assertEqual(item['url'], response.url)\n             if 'item1.html' in item['url']:\n                 self.assertEqual('Item 1 name', item['name'])\n\n@@ -1,6 +1,9 @@\n from functools import partial\n import unittest\n \n+import attr\n+from itemadapter import ItemAdapter\n+\n from scrapy.http import HtmlResponse\n from scrapy.item import Item, Field\n from scrapy.loader import ItemLoader\n@@ -9,6 +12,13 @@ from scrapy.loader.processors import (Compose, Identity, Join,\n from scrapy.selector import Selector\n \n \n+try:\n+    from dataclasses import make_dataclass, field as dataclass_field\n+except ImportError:\n+    make_dataclass = None\n+    dataclass_field = None\n+\n+\n # test items\n class NameItem(Item):\n     name = Field()\n@@ -28,6 +38,11 @@ class TestNestedItem(Item):\n     image = Field()\n \n \n+@attr.s\n+class AttrsNameItem:\n+    name = attr.ib(default=\"\")\n+\n+\n # test item loaders\n class NameItemLoader(ItemLoader):\n     default_item_class = TestItem\n@@ -466,7 +481,7 @@ class InitializationTestMixin:\n         il = ItemLoader(item=input_item)\n         loaded_item = il.load_item()\n         self.assertIsInstance(loaded_item, self.item_class)\n-        self.assertEqual(dict(loaded_item), {'name': ['foo']})\n+        self.assertEqual(ItemAdapter(loaded_item).asdict(), {'name': ['foo']})\n \n     def test_keep_list(self):\n         \"\"\"Loaded item should contain values from the initial item\"\"\"\n@@ -474,7 +489,7 @@ class InitializationTestMixin:\n         il = ItemLoader(item=input_item)\n         loaded_item = il.load_item()\n         self.assertIsInstance(loaded_item, self.item_class)\n-        self.assertEqual(dict(loaded_item), {'name': ['foo', 'bar']})\n+        self.assertEqual(ItemAdapter(loaded_item).asdict(), {'name': ['foo', 'bar']})\n \n     def test_add_value_singlevalue_singlevalue(self):\n         \"\"\"Values added after initialization should be appended\"\"\"\n@@ -483,7 +498,7 @@ class InitializationTestMixin:\n         il.add_value('name', 'bar')\n         loaded_item = il.load_item()\n         self.assertIsInstance(loaded_item, self.item_class)\n-        self.assertEqual(dict(loaded_item), {'name': ['foo', 'bar']})\n+        self.assertEqual(ItemAdapter(loaded_item).asdict(), {'name': ['foo', 'bar']})\n \n     def test_add_value_singlevalue_list(self):\n         \"\"\"Values added after initialization should be appended\"\"\"\n@@ -492,7 +507,7 @@ class InitializationTestMixin:\n         il.add_value('name', ['item', 'loader'])\n         loaded_item = il.load_item()\n         self.assertIsInstance(loaded_item, self.item_class)\n-        self.assertEqual(dict(loaded_item), {'name': ['foo', 'item', 'loader']})\n+        self.assertEqual(ItemAdapter(loaded_item).asdict(), {'name': ['foo', 'item', 'loader']})\n \n     def test_add_value_list_singlevalue(self):\n         \"\"\"Values added after initialization should be appended\"\"\"\n@@ -501,7 +516,7 @@ class InitializationTestMixin:\n         il.add_value('name', 'qwerty')\n         loaded_item = il.load_item()\n         self.assertIsInstance(loaded_item, self.item_class)\n-        self.assertEqual(dict(loaded_item), {'name': ['foo', 'bar', 'qwerty']})\n+        self.assertEqual(ItemAdapter(loaded_item).asdict(), {'name': ['foo', 'bar', 'qwerty']})\n \n     def test_add_value_list_list(self):\n         \"\"\"Values added after initialization should be appended\"\"\"\n@@ -510,7 +525,7 @@ class InitializationTestMixin:\n         il.add_value('name', ['item', 'loader'])\n         loaded_item = il.load_item()\n         self.assertIsInstance(loaded_item, self.item_class)\n-        self.assertEqual(dict(loaded_item), {'name': ['foo', 'bar', 'item', 'loader']})\n+        self.assertEqual(ItemAdapter(loaded_item).asdict(), {'name': ['foo', 'bar', 'item', 'loader']})\n \n     def test_get_output_value_singlevalue(self):\n         \"\"\"Getting output value must not remove value from item\"\"\"\n@@ -519,7 +534,7 @@ class InitializationTestMixin:\n         self.assertEqual(il.get_output_value('name'), ['foo'])\n         loaded_item = il.load_item()\n         self.assertIsInstance(loaded_item, self.item_class)\n-        self.assertEqual(loaded_item, dict({'name': ['foo']}))\n+        self.assertEqual(ItemAdapter(loaded_item).asdict(), dict({'name': ['foo']}))\n \n     def test_get_output_value_list(self):\n         \"\"\"Getting output value must not remove value from item\"\"\"\n@@ -528,7 +543,7 @@ class InitializationTestMixin:\n         self.assertEqual(il.get_output_value('name'), ['foo', 'bar'])\n         loaded_item = il.load_item()\n         self.assertIsInstance(loaded_item, self.item_class)\n-        self.assertEqual(loaded_item, dict({'name': ['foo', 'bar']}))\n+        self.assertEqual(ItemAdapter(loaded_item).asdict(), dict({'name': ['foo', 'bar']}))\n \n     def test_values_single(self):\n         \"\"\"Values from initial item must be added to loader._values\"\"\"\n@@ -551,6 +566,22 @@ class InitializationFromItemTest(InitializationTestMixin, unittest.TestCase):\n     item_class = NameItem\n \n \n+class InitializationFromAttrsItemTest(InitializationTestMixin, unittest.TestCase):\n+    item_class = AttrsNameItem\n+\n+\n+@unittest.skipIf(not make_dataclass, \"dataclasses module is not available\")\n+class InitializationFromDataClassTest(InitializationTestMixin, unittest.TestCase):\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        if make_dataclass:\n+            self.item_class = make_dataclass(\n+                \"TestDataClass\",\n+                [(\"name\", list, dataclass_field(default_factory=list))],\n+            )\n+\n+\n class BaseNoInputReprocessingLoader(ItemLoader):\n     title_in = MapCompose(str.upper)\n     title_out = TakeFirst()\n\n@@ -2,22 +2,41 @@ import os\n import random\n import time\n from io import BytesIO\n-from tempfile import mkdtemp\n from shutil import rmtree\n-from unittest import mock\n+from tempfile import mkdtemp\n+from unittest import mock, skipIf\n from urllib.parse import urlparse\n \n-from twisted.trial import unittest\n+import attr\n+from itemadapter import ItemAdapter\n from twisted.internet import defer\n+from twisted.trial import unittest\n \n-from scrapy.pipelines.files import FilesPipeline, FSFilesStore, S3FilesStore, GCSFilesStore, FTPFilesStore\n-from scrapy.item import Item, Field\n from scrapy.http import Request, Response\n+from scrapy.item import Field, Item\n+from scrapy.pipelines.files import (\n+    FilesPipeline,\n+    FSFilesStore,\n+    FTPFilesStore,\n+    GCSFilesStore,\n+    S3FilesStore,\n+)\n from scrapy.settings import Settings\n-from scrapy.utils.test import assert_aws_environ, get_s3_content_and_delete\n-from scrapy.utils.test import assert_gcs_environ, get_gcs_content_and_delete\n-from scrapy.utils.test import get_ftp_content_and_delete\n from scrapy.utils.boto import is_botocore\n+from scrapy.utils.test import (\n+    assert_aws_environ,\n+    assert_gcs_environ,\n+    get_ftp_content_and_delete,\n+    get_gcs_content_and_delete,\n+    get_s3_content_and_delete,\n+)\n+\n+\n+try:\n+    from dataclasses import make_dataclass, field as dataclass_field\n+except ImportError:\n+    make_dataclass = None\n+    dataclass_field = None\n \n \n def _mocked_download_func(request, info):\n@@ -143,43 +162,88 @@ class FilesPipelineTestCase(unittest.TestCase):\n             p.stop()\n \n \n-class FilesPipelineTestCaseFields(unittest.TestCase):\n+class FilesPipelineTestCaseFieldsMixin:\n \n     def test_item_fields_default(self):\n-        class TestItem(Item):\n-            name = Field()\n-            file_urls = Field()\n-            files = Field()\n-\n-        for cls in TestItem, dict:\n         url = 'http://www.example.com/files/1.txt'\n-            item = cls({'name': 'item1', 'file_urls': [url]})\n+        item = self.item_class(name='item1', file_urls=[url])\n         pipeline = FilesPipeline.from_settings(Settings({'FILES_STORE': 's3://example/files/'}))\n         requests = list(pipeline.get_media_requests(item, None))\n         self.assertEqual(requests[0].url, url)\n         results = [(True, {'url': url})]\n-            pipeline.item_completed(results, item, None)\n-            self.assertEqual(item['files'], [results[0][1]])\n+        item = pipeline.item_completed(results, item, None)\n+        files = ItemAdapter(item).get(\"files\")\n+        self.assertEqual(files, [results[0][1]])\n+        self.assertIsInstance(item, self.item_class)\n \n     def test_item_fields_override_settings(self):\n-        class TestItem(Item):\n-            name = Field()\n-            files = Field()\n-            stored_file = Field()\n-\n-        for cls in TestItem, dict:\n         url = 'http://www.example.com/files/1.txt'\n-            item = cls({'name': 'item1', 'files': [url]})\n+        item = self.item_class(name='item1', custom_file_urls=[url])\n         pipeline = FilesPipeline.from_settings(Settings({\n             'FILES_STORE': 's3://example/files/',\n-                'FILES_URLS_FIELD': 'files',\n-                'FILES_RESULT_FIELD': 'stored_file'\n+            'FILES_URLS_FIELD': 'custom_file_urls',\n+            'FILES_RESULT_FIELD': 'custom_files'\n         }))\n         requests = list(pipeline.get_media_requests(item, None))\n         self.assertEqual(requests[0].url, url)\n         results = [(True, {'url': url})]\n-            pipeline.item_completed(results, item, None)\n-            self.assertEqual(item['stored_file'], [results[0][1]])\n+        item = pipeline.item_completed(results, item, None)\n+        custom_files = ItemAdapter(item).get(\"custom_files\")\n+        self.assertEqual(custom_files, [results[0][1]])\n+        self.assertIsInstance(item, self.item_class)\n+\n+\n+class FilesPipelineTestCaseFieldsDict(FilesPipelineTestCaseFieldsMixin, unittest.TestCase):\n+    item_class = dict\n+\n+\n+class FilesPipelineTestItem(Item):\n+    name = Field()\n+    # default fields\n+    file_urls = Field()\n+    files = Field()\n+    # overridden fields\n+    custom_file_urls = Field()\n+    custom_files = Field()\n+\n+\n+class FilesPipelineTestCaseFieldsItem(FilesPipelineTestCaseFieldsMixin, unittest.TestCase):\n+    item_class = FilesPipelineTestItem\n+\n+\n+@skipIf(not make_dataclass, \"dataclasses module is not available\")\n+class FilesPipelineTestCaseFieldsDataClass(FilesPipelineTestCaseFieldsMixin, unittest.TestCase):\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        if make_dataclass:\n+            self.item_class = make_dataclass(\n+                \"FilesPipelineTestDataClass\",\n+                [\n+                    (\"name\", str),\n+                    # default fields\n+                    (\"file_urls\", list, dataclass_field(default_factory=list)),\n+                    (\"files\", list, dataclass_field(default_factory=list)),\n+                    # overridden fields\n+                    (\"custom_file_urls\", list, dataclass_field(default_factory=list)),\n+                    (\"custom_files\", list, dataclass_field(default_factory=list)),\n+                ],\n+            )\n+\n+\n+@attr.s\n+class FilesPipelineTestAttrsItem:\n+    name = attr.ib(default=\"\")\n+    # default fields\n+    file_urls = attr.ib(default=lambda: [])\n+    files = attr.ib(default=lambda: [])\n+    # overridden fields\n+    custom_file_urls = attr.ib(default=lambda: [])\n+    custom_files = attr.ib(default=lambda: [])\n+\n+\n+class FilesPipelineTestCaseFieldsAttrsItem(FilesPipelineTestCaseFieldsMixin, unittest.TestCase):\n+    item_class = FilesPipelineTestAttrsItem\n \n \n class FilesPipelineTestCaseCustomSettings(unittest.TestCase):\n\n@@ -1,17 +1,28 @@\n-import io\n import hashlib\n+import io\n import random\n-from tempfile import mkdtemp\n from shutil import rmtree\n+from tempfile import mkdtemp\n+from unittest import skipIf\n \n+import attr\n+from itemadapter import ItemAdapter\n from twisted.trial import unittest\n \n-from scrapy.item import Item, Field\n from scrapy.http import Request, Response\n-from scrapy.settings import Settings\n+from scrapy.item import Field, Item\n from scrapy.pipelines.images import ImagesPipeline\n+from scrapy.settings import Settings\n from scrapy.utils.python import to_bytes\n \n+\n+try:\n+    from dataclasses import make_dataclass, field as dataclass_field\n+except ImportError:\n+    make_dataclass = None\n+    dataclass_field = None\n+\n+\n skip = False\n try:\n     from PIL import Image\n@@ -124,43 +135,89 @@ class DeprecatedImagesPipeline(ImagesPipeline):\n         return 'thumbsup/%s/%s.jpg' % (thumb_id, thumb_guid)\n \n \n-class ImagesPipelineTestCaseFields(unittest.TestCase):\n+class ImagesPipelineTestCaseFieldsMixin:\n \n     def test_item_fields_default(self):\n-        class TestItem(Item):\n-            name = Field()\n-            image_urls = Field()\n-            images = Field()\n-\n-        for cls in TestItem, dict:\n         url = 'http://www.example.com/images/1.jpg'\n-            item = cls({'name': 'item1', 'image_urls': [url]})\n+        item = self.item_class(name='item1', image_urls=[url])\n         pipeline = ImagesPipeline.from_settings(Settings({'IMAGES_STORE': 's3://example/images/'}))\n         requests = list(pipeline.get_media_requests(item, None))\n         self.assertEqual(requests[0].url, url)\n         results = [(True, {'url': url})]\n-            pipeline.item_completed(results, item, None)\n-            self.assertEqual(item['images'], [results[0][1]])\n+        item = pipeline.item_completed(results, item, None)\n+        images = ItemAdapter(item).get(\"images\")\n+        self.assertEqual(images, [results[0][1]])\n+        self.assertIsInstance(item, self.item_class)\n \n     def test_item_fields_override_settings(self):\n-        class TestItem(Item):\n-            name = Field()\n-            image = Field()\n-            stored_image = Field()\n-\n-        for cls in TestItem, dict:\n         url = 'http://www.example.com/images/1.jpg'\n-            item = cls({'name': 'item1', 'image': [url]})\n+        item = self.item_class(name='item1', custom_image_urls=[url])\n         pipeline = ImagesPipeline.from_settings(Settings({\n             'IMAGES_STORE': 's3://example/images/',\n-                'IMAGES_URLS_FIELD': 'image',\n-                'IMAGES_RESULT_FIELD': 'stored_image'\n+            'IMAGES_URLS_FIELD': 'custom_image_urls',\n+            'IMAGES_RESULT_FIELD': 'custom_images'\n         }))\n         requests = list(pipeline.get_media_requests(item, None))\n         self.assertEqual(requests[0].url, url)\n         results = [(True, {'url': url})]\n-            pipeline.item_completed(results, item, None)\n-            self.assertEqual(item['stored_image'], [results[0][1]])\n+        item = pipeline.item_completed(results, item, None)\n+        custom_images = ItemAdapter(item).get(\"custom_images\")\n+        self.assertEqual(custom_images, [results[0][1]])\n+        self.assertIsInstance(item, self.item_class)\n+\n+\n+class ImagesPipelineTestCaseFieldsDict(ImagesPipelineTestCaseFieldsMixin, unittest.TestCase):\n+    item_class = dict\n+\n+\n+class ImagesPipelineTestItem(Item):\n+    name = Field()\n+    # default fields\n+    image_urls = Field()\n+    images = Field()\n+    # overridden fields\n+    custom_image_urls = Field()\n+    custom_images = Field()\n+\n+\n+class ImagesPipelineTestCaseFieldsItem(ImagesPipelineTestCaseFieldsMixin, unittest.TestCase):\n+    item_class = ImagesPipelineTestItem\n+\n+\n+@skipIf(not make_dataclass, \"dataclasses module is not available\")\n+class ImagesPipelineTestCaseFieldsDataClass(ImagesPipelineTestCaseFieldsMixin, unittest.TestCase):\n+    item_class = None\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        if make_dataclass:\n+            self.item_class = make_dataclass(\n+                \"FilesPipelineTestDataClass\",\n+                [\n+                    (\"name\", str),\n+                    # default fields\n+                    (\"image_urls\", list, dataclass_field(default_factory=list)),\n+                    (\"images\", list, dataclass_field(default_factory=list)),\n+                    # overridden fields\n+                    (\"custom_image_urls\", list, dataclass_field(default_factory=list)),\n+                    (\"custom_images\", list, dataclass_field(default_factory=list)),\n+                ],\n+            )\n+\n+\n+@attr.s\n+class ImagesPipelineTestAttrsItem:\n+    name = attr.ib(default=\"\")\n+    # default fields\n+    image_urls = attr.ib(default=lambda: [])\n+    images = attr.ib(default=lambda: [])\n+    # overridden fields\n+    custom_image_urls = attr.ib(default=lambda: [])\n+    custom_images = attr.ib(default=lambda: [])\n+\n+\n+class ImagesPipelineTestCaseFieldsAttrsItem(ImagesPipelineTestCaseFieldsMixin, unittest.TestCase):\n+    item_class = ImagesPipelineTestAttrsItem\n \n \n class ImagesPipelineTestCaseCustomSettings(unittest.TestCase):\n\n@@ -1,18 +1,25 @@\n+import datetime\n import json\n import unittest\n-import datetime\n from decimal import Decimal\n \n+import attr\n from twisted.internet import defer\n \n-from scrapy.utils.serialize import ScrapyJSONEncoder\n from scrapy.http import Request, Response\n+from scrapy.utils.serialize import ScrapyJSONEncoder\n+\n+\n+try:\n+    from dataclasses import make_dataclass\n+except ImportError:\n+    make_dataclass = None\n \n \n class JsonEncoderTestCase(unittest.TestCase):\n \n     def setUp(self):\n-        self.encoder = ScrapyJSONEncoder()\n+        self.encoder = ScrapyJSONEncoder(sort_keys=True)\n \n     def test_encode_decode(self):\n         dt = datetime.datetime(2010, 1, 2, 10, 11, 12)\n@@ -31,7 +38,8 @@ class JsonEncoderTestCase(unittest.TestCase):\n         for input, output in [('foo', 'foo'), (d, ds), (t, ts), (dt, dts),\n                               (dec, decs), (['foo', d], ['foo', ds]), (s, ss),\n                               (dt_set, dt_sets)]:\n-            self.assertEqual(self.encoder.encode(input), json.dumps(output))\n+            self.assertEqual(self.encoder.encode(input),\n+                             json.dumps(output, sort_keys=True))\n \n     def test_encode_deferred(self):\n         self.assertIn('Deferred', self.encoder.encode(defer.Deferred()))\n@@ -47,3 +55,30 @@ class JsonEncoderTestCase(unittest.TestCase):\n         rs = self.encoder.encode(r)\n         self.assertIn(r.url, rs)\n         self.assertIn(str(r.status), rs)\n+\n+    @unittest.skipIf(not make_dataclass, \"No dataclass support\")\n+    def test_encode_dataclass_item(self):\n+        TestDataClass = make_dataclass(\n+            \"TestDataClass\",\n+            [(\"name\", str), (\"url\", str), (\"price\", int)],\n+        )\n+        item = TestDataClass(name=\"Product\", url=\"http://product.org\", price=1)\n+        encoded = self.encoder.encode(item)\n+        self.assertEqual(\n+            encoded,\n+            '{\"name\": \"Product\", \"price\": 1, \"url\": \"http://product.org\"}'\n+        )\n+\n+    def test_encode_attrs_item(self):\n+        @attr.s\n+        class AttrsItem:\n+            name = attr.ib(type=str)\n+            url = attr.ib(type=str)\n+            price = attr.ib(type=int)\n+\n+        item = AttrsItem(name=\"Product\", url=\"http://product.org\", price=1)\n+        encoded = self.encoder.encode(item)\n+        self.assertEqual(\n+            encoded,\n+            '{\"name\": \"Product\", \"price\": 1, \"url\": \"http://product.org\"}'\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
