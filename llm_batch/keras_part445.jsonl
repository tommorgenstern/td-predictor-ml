{"custom_id": "keras#a95a2af089473d4fea9fdb26b0d29d6dde8f7466", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 73 | Lines Deleted: 58 | Files Changed: 2 | Hunks: 33 | Methods Changed: 10 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 131 | Churn Cumulative: 1655 | Contributors (this commit): 3 | Commits (past 90d): 23 | Contributors (cumulative): 4 | DMM Complexity: 0.5714285714285714\n\nDIFF:\n@@ -66,7 +66,8 @@ def split_dataset(dataset,\n     raise ValueError('you must specify either `left_size` or `right_size`'\n                     ' Received: `left_size`= None, and `right_size`=None')\n \n-  dataset_as_list = _convert_dataset_to_list(dataset)\n+  dataset_type_spec = _get_type_spec(dataset)\n+  dataset_as_list = _convert_dataset_to_list(dataset,dataset_type_spec)\n \n   if seed is None:\n     seed = np.random.randint(1e6)\n@@ -76,32 +77,20 @@ def split_dataset(dataset,\n \n   total_length = len(dataset_as_list)\n \n-  left_size,right_size = _rescale_dataset_split_sizes(left_size,\n-                                                      right_size,\n+  left_size,right_size = _rescale_dataset_split_sizes(left_size,right_size,\n                                                       total_length)\n-\n   left_split = list(dataset_as_list[:left_size])\n   right_split = list(dataset_as_list[-right_size:])\n \n-  if _get_type_spec(dataset) is tuple:\n+  if dataset_type_spec  is tuple:\n+    left_split = tuple(np.array(sample) for sample in zip(*left_split))\n+    right_split = tuple(np.array(sample) for sample in zip(*right_split))\n+  elif dataset_type_spec  is list:\n     left_split = tuple(np.array(sample) for sample in zip(*left_split))\n     right_split = tuple(np.array(sample) for sample in zip(*right_split))\n   \n-  try:\n   left_split = tf.data.Dataset.from_tensor_slices(left_split)\n-  except ValueError as e:\n-    raise ValueError(f'With left_size={left_size} and right_size={right_size}.'\n-                     f' unable to create the dataset from left_split of shape '\n-                     f'{np.array(left_split).shape}. \\nReceived error: {e}')\n-\n-  try:\n   right_split = tf.data.Dataset.from_tensor_slices(right_split)\n-  except Exception as e:\n-    raise ValueError(f' with `left_size`={left_size} and '\n-                     f' `right_size`={right_size}'\n-                     f' unable to create the dataset from '\n-                     f' right_split of shape {np.array(right_split).shape}. '\n-                     f' Received error: {e}')\n \n   left_split = left_split.prefetch(tf.data.AUTOTUNE)\n   right_split = right_split.prefetch(tf.data.AUTOTUNE)\n@@ -109,13 +98,14 @@ def split_dataset(dataset,\n   return left_split, right_split\n \n def _convert_dataset_to_list(dataset,\n+                             dataset_type_spec,\n                              data_size_warning_flag= True,\n                              ensure_shape_similarity = True):\n   \"\"\"Helper function to convert a tf.data.Dataset  object or a list/tuple of numpy.ndarrays to a list\n   \"\"\"\n   # TODO (prakashsellathurai): add support for Batched  and unbatched dict tf datasets\n-  dataset_type_spec = _get_type_spec(dataset)\n-  dataset_iterator = _get_data_iterator_from_dataset(dataset)\n+  \n+  dataset_iterator = _get_data_iterator_from_dataset(dataset,dataset_type_spec)\n   dataset_as_list = []\n \n   start_time = time.time()\n@@ -126,35 +116,37 @@ def _convert_dataset_to_list(dataset,\n \n     if dataset_type_spec is tuple:\n       dataset_as_list.append(np.array(sample) )\n+    elif dataset_type_spec is list:\n+      dataset_as_list.append(np.array(sample) )\n     else:\n       dataset_as_list.append(sample)\n \n-\n   return dataset_as_list\n \n-def _get_data_iterator_from_dataset(dataset) :\n+def _get_data_iterator_from_dataset(dataset,dataset_type_spec) :\n   \"\"\"Helper function to get the data iterator from a tf.data.Dataset object\n   \"\"\"\n-  if isinstance(dataset,(list)):\n+  if dataset_type_spec == list:\n     if len(dataset) == 0:\n-      raise ValueError('`dataset` must be a non-empty list of'\n-                       ' numpy.ndarrays or tf.data.Dataset objects.')\n+      raise ValueError('Received an empty list dataset '\n+                       'Please provide a non-empty list dataset')\n \n     if isinstance(dataset[0],np.ndarray):\n       expected_shape = dataset[0].shape\n       for i,element in enumerate(dataset):\n         if not np.array(element).shape[0] == expected_shape[0]:\n-          raise ValueError(f' Expected  numpy array of {type(dataset)} '\n-                           f'in `dataset` to have the same length. '\n-                           f'\\n Received: dataset[{i}] with length = '\n-                           f'{np.array(element).shape},'\n-                           f' while dataset[0] has length {dataset[0].shape}')\n+          raise ValueError('Received a list of numpy arrays with different '\n+                           f'lengths. Mismatch found at index {i}, '\n+                           f'Expected shape={expected_shape} '\n+                           f'Received shape={np.array(element).shape}.'\n+                           f'Please provide a list of numpy arrays with '\n+                           f'equal length.')\n     else:\n       raise ValueError('Expected a list of numpy.ndarrays,'\n                        'Received: {}'.format(type(dataset[0])))\n \n     return iter(zip(*dataset))\n-  elif  isinstance(dataset,tuple):\n+  elif  dataset_type_spec == tuple:\n     if len(dataset) == 0:\n       raise ValueError('`dataset` must be a non-empty list of'\n                        ' numpy.ndarrays or tf.data.Dataset objects.')\n@@ -163,26 +155,36 @@ def _get_data_iterator_from_dataset(dataset) :\n       expected_shape = dataset[0].shape\n       for i,element in enumerate(dataset):\n         if not np.array(element).shape[0] == expected_shape[0]:\n-          raise ValueError(f' Expected  numpy array of {type(dataset)} '\n-                           f'in `dataset` to have the same length. '\n-                           f'\\n Received: dataset[{i}] with length = '\n-                           f'{np.array(element).shape},'\n-                           f' while dataset[0] has length {dataset[0].shape}')\n+          raise ValueError('Received a tuple of numpy arrays with different '\n+                           f'lengths. Mismatch found at index {i}, '\n+                           f'Expected shape={expected_shape} '\n+                           f'Received shape={np.array(element).shape}.'\n+                           f'Please provide a tuple of numpy arrays with '\n+                           f'equal length.')\n     else:\n       raise ValueError('Expected a tuple of numpy.ndarrays,'\n                        'Received: {}'.format(type(dataset[0])))\n \n     return  iter(zip(*dataset))\n-  elif isinstance(dataset,tf.data.Dataset):\n+  elif dataset_type_spec == tf.data.Dataset:\n     if is_batched(dataset):\n       dataset = dataset.unbatch()\n     return iter(dataset)\n-  elif isinstance(dataset,np.ndarray):\n+  elif dataset_type_spec == np.ndarray:\n     return iter(dataset)\n \n def _get_type_spec(dataset):\n   if isinstance(dataset,(tuple)):\n     return tuple\n+  elif isinstance(dataset,(list)):\n+    return list\n+  elif isinstance(dataset,(np.ndarray)):\n+    return np.ndarray\n+  elif isinstance(dataset,dict):\n+    return dict\n+  elif isinstance(dataset,(tf.data.Dataset)):\n+    return tf.data.Dataset\n+  else:\n     return None\n \n def _get_next_sample(dataset_iterator,\n@@ -326,8 +328,7 @@ def _rescale_dataset_split_sizes(left_size,right_size,total_length):\n \n def is_batched(tf_dataset):\n   \"\"\"returns true if given tf dataset  is batched or false if not\n-\n-  refer: https://stackoverflow.com/a/66101853/8336491\n+  reference: https://stackoverflow.com/a/66101853/8336491\n   \"\"\"\n   try:\n     return tf_dataset.__class__.__name__ == 'BatchDataset'\n\n@@ -24,6 +24,7 @@ class SplitDatasetTest(tf.test.TestCase):\n     self.assertAllEqual(dataset[-40:] ,list(right_split))\n \n   def test_list_of_numpy_arrays(self):\n+    # test with list of np arrays with same shapes\n     dataset=[np.ones(shape=(200, 32)), np.zeros(shape=(200, 32))]\n     res = dataset_utils.split_dataset(dataset, left_size=4)\n \n@@ -33,18 +34,23 @@ class SplitDatasetTest(tf.test.TestCase):\n     self.assertIsInstance(left_split, tf.data.Dataset)\n     self.assertIsInstance(right_split, tf.data.Dataset)\n \n-    self.assertLen(left_split, 4)\n-    self.assertLen(right_split, 196)\n+    self.assertEqual(np.array(list(left_split)).shape,(4,2,32))\n+    self.assertEqual(np.array(list(right_split)).shape,(196,2,32))\n \n-    self.assertAllEqual(list(zip(*dataset))[:4] ,list(left_split))\n-    self.assertAllEqual(list(zip(*dataset))[4:] ,list(right_split))\n-    self.assertAllEqual(list(left_split)+list(right_split),list(zip(*dataset)))\n+    # test with different shapes\n+    dataset = [np.ones(shape=(5, 3)), np.ones(shape=(5, ))]\n+    left_split,right_split = dataset_utils.split_dataset(dataset,left_size=0.3)\n     \n-    dataset=[np.ones(shape=(200, 32))]\n-    left_split,right_split = dataset_utils.split_dataset(dataset,\n-                                                             left_size=4)\n-    self.assertAllEqual(list(zip(*dataset)),\n-                        list(left_split)+list(right_split))\n+    self.assertEqual(np.array(list(left_split)).shape,(2,2))\n+    self.assertEqual(np.array(list(right_split)).shape,(3,2))\n+\n+    self.assertEqual(np.array(list(left_split)[0]).shape,(2,))\n+    self.assertEqual(np.array(list(left_split)[0][0]).shape,(3,))\n+    self.assertEqual(np.array(list(left_split)[0][1]).shape,())\n+\n+    self.assertEqual(np.array(list(right_split)[0]).shape,(2,))\n+    self.assertEqual(np.array(list(right_split)[0][0]).shape,(3,))\n+    self.assertEqual(np.array(list(right_split)[0][1]).shape,())\n     \n   def test_dataset_with_invalid_shape(self):\n     with self.assertRaises(ValueError):\n@@ -268,32 +274,40 @@ class SplitDatasetTest(tf.test.TestCase):\n       dataset_utils.split_dataset(dataset=np.array([1,2,3]), left_size=None)\n \n     with self.assertRaises(ValueError):\n-      dataset_utils.split_dataset(np.array([1,2,3]),left_size=None,right_size=None)\n+      dataset_utils.split_dataset(np.array([1,2,3]),left_size=None,\n+                                  right_size=None)\n \n     with self.assertRaises(ValueError):\n-      dataset_utils.split_dataset(np.array([1,2,3]),left_size=3,right_size=None)\n+      dataset_utils.split_dataset(np.array([1,2,3]),left_size=3,\n+                                  right_size=None)\n \n     with self.assertRaises(ValueError):\n-      dataset_utils.split_dataset(np.array([1,2,3]),left_size=3,right_size=None)\n+      dataset_utils.split_dataset(np.array([1,2,3]),left_size=3,\n+                                  right_size=None)\n \n     with self.assertRaises(ValueError):\n-      dataset_utils.split_dataset(np.array([1,2,3]), left_size=0,right_size=0)\n+      dataset_utils.split_dataset(np.array([1,2,3]), left_size=0,\n+                                  right_size=0)\n \n   def test_invalid_left_and_right_size_types(self):\n     with self.assertRaises(TypeError):\n-      dataset_utils.split_dataset(np.array([1,2,3]),left_size='1',right_size='1')\n+      dataset_utils.split_dataset(np.array([1,2,3]),left_size='1',\n+                                  right_size='1')\n \n     with self.assertRaises(TypeError):\n-      dataset_utils.split_dataset(np.array([1,2,3]),left_size=0,right_size='1')\n+      dataset_utils.split_dataset(np.array([1,2,3]),left_size=0,\n+                                  right_size='1')\n       \n     with self.assertRaises(TypeError):\n-      dataset_utils.split_dataset(np.array([1,2,3]),left_size='100',right_size=None)\n+      dataset_utils.split_dataset(np.array([1,2,3]),left_size='100',\n+                                  right_size=None)\n \n     with self.assertRaises(TypeError):\n       dataset_utils.split_dataset(np.array([1,2,3]),right_size='1')\n       \n     with self.assertRaises(TypeError):\n-      dataset_utils.split_dataset(np.array([1,2,3]),left_size=0.5,right_size='1')\n+      dataset_utils.split_dataset(np.array([1,2,3]),left_size=0.5,\n+                                  right_size='1')\n \n if __name__ == \"__main__\":\n   tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4e939e1451457c03554d64c316098c3281052fa6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 139 | Contributors (this commit): 4 | Commits (past 90d): 10 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -110,6 +110,8 @@ def image_dataset_from_directory(directory,\n     subset: Subset of the data to return.\n         One of \"training\", \"validation\" or \"both\".\n         Only used if `validation_split` is set.\n+        When `subset=\"both\"`, the utility returns a tuple of two datasets\n+        (the training and validation datasets respectively).\n     interpolation: String, the interpolation method used when resizing images.\n       Defaults to `bilinear`. Supports `bilinear`, `nearest`, `bicubic`,\n       `area`, `lanczos3`, `lanczos5`, `gaussian`, `mitchellcubic`.\n\n@@ -95,6 +95,8 @@ def text_dataset_from_directory(directory,\n     subset: Subset of the data to return.\n         One of \"training\", \"validation\" or \"both\".\n         Only used if `validation_split` is set.\n+        When `subset=\"both\"`, the utility returns a tuple of two datasets\n+        (the training and validation datasets respectively).\n     follow_links: Whether to visits subdirectories pointed to by symlinks.\n         Defaults to False.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2d84d278b523fd8a3c6f9029fb346f0895016687", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 504 | Contributors (this commit): 1 | Commits (past 90d): 12 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -367,7 +367,7 @@ class AudioDatasetFromDirectoryTest(test_combinations.TestCase):\n             )\n \n         with self.assertRaisesRegex(\n-            ValueError, '`subset` must be either \"training\" or'\n+            ValueError, '`subset` must be either \"training\",'\n         ):\n             _ = audio_dataset.audio_dataset_from_directory(\n                 directory, validation_split=0.2, subset=\"other\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#275a0707c066bd90501e3fb3a0c26b1b676c06d3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 40 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 9 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 47 | Churn Cumulative: 1013 | Contributors (this commit): 2 | Commits (past 90d): 18 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -46,18 +46,41 @@ def get_current_layout_map():\n \n @keras_export('keras.dtensor.experimental.LayoutMap', v1=[])\n class LayoutMap(collections.abc.MutableMapping):\n+  \"\"\"A dict-like object that maps string to `Layout` instances.\n \n-  def __init__(self, mesh=None):\n-    \"\"\"A dict like object that maps between string name and dtensor.Layout.\n+  `LayoutMap` uses a string as key and a `Layout` as value. There is a behavior\n+  difference between a normal Python dict and this class. The string key will be\n+  treated as a regex when retrieving the value. See the docstring of\n+  `get` for more details.\n \n-    Note that this class might behave differently than a normal dict, eg, it\n-    will treat the all the existing keys as a regex to map against input key.\n+  See below for a usage example. You can define the naming schema\n+  of the `Layout`, and then retrieve the corresponding `Layout` instance.\n+\n+  To use the `LayoutMap` with a `Model`, please see the docstring of\n+  `tf.keras.dtensor.experimental.layout_map_scope`.\n+\n+  ```python\n+  map = LayoutMap(mesh=None)\n+  map['.*dense.*kernel'] = layout_2d\n+  map['.*dense.*bias'] = layout_1d\n+  map['.*conv2d.*kernel'] = layout_4d\n+  map['.*conv2d.*bias'] = layout_1d\n+\n+  layout_1 = map['dense_1.kernel']    #   layout_1 == layout_2d\n+  layout_2 = map['dense_1.bias']      #   layout_2 == layout_1d\n+  layout_3 = map['dense_2.kernel']    #   layout_3 == layout_2d\n+  layout_4 = map['dense_2.bias']      #   layout_4 == layout_1d\n+  layout_5 = map['my_model/conv2d_123/kernel']    #   layout_5 == layout_4d\n+  layout_6 = map['my_model/conv2d_123/bias']      #   layout_6 == layout_1d\n+  ```\n \n   Args:\n-      mesh: An optional dtensor.Mesh that is used to provide all replicated\n-        layout as default when there isn't a layout is found based on the\n-        mapping.\n+    mesh: An optional `Mesh` that can be used to create all replicated\n+      layout as default when there isn't a layout found based on the input\n+      string query.\n   \"\"\"\n+\n+  def __init__(self, mesh=None):\n     self._layout_map = collections.OrderedDict()\n     self._default_mesh = mesh\n \n@@ -105,9 +128,17 @@ class LayoutMap(collections.abc.MutableMapping):\n     return iter(self._layout_map)\n \n   def get_default_mesh(self):\n+    \"\"\"Return the default `Mesh` set at instance creation.\n+\n+    The `Mesh` can be used to create default replicated `Layout` when there\n+    isn't a match of the input string query.\n+    \"\"\"\n     return self._default_mesh\n \n \n+LayoutMap.get.__doc__ = LayoutMap.__getitem__.__doc__\n+\n+\n @keras_export('keras.dtensor.experimental.layout_map_scope', v1=[])\n @contextlib.contextmanager\n def layout_map_scope(layout_map):\n\n@@ -26,6 +26,7 @@ from keras.optimizers.schedules import learning_rate_schedule\n import tensorflow.compat.v2 as tf\n \n from tensorflow.python.util.tf_export import keras_export  # pylint: disable=g-direct-tensorflow-import\n+from tensorflow.tools.docs import doc_controls\n \n \n # pylint: disable=protected-access,missing-class-docstring\n@@ -103,6 +104,7 @@ class Optimizer(optimizer_lib._BaseOptimizer):\n         dtype=model_variable.dtype,\n         trainable=False)\n \n+  @doc_controls.do_not_generate_docs\n   def aggregate_gradients(self, grads_and_vars):\n     # Hide the aggregate_gradients from Optimizer.aggregate_gradients\n     raise NotImplementedError(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dfe6a6bc0315ac84c27001531a773cc6f9cbbe50", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 31 | Files Changed: 1 | Hunks: 20 | Methods Changed: 9 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 63 | Churn Cumulative: 2409 | Contributors (this commit): 1 | Commits (past 90d): 8 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -28,6 +28,7 @@ from keras import backend\n from keras import layers\n from keras import utils\n from keras import Model\n+from keras import Sequential\n from keras.applications import imagenet_utils\n from keras.engine import training\n \n@@ -159,19 +160,20 @@ class StochasticDepth(layers.Layer):\n     - https://github.com.rwightman/pytorch-image-models\n \n   Args:\n-    drop_path (float): Probability of dropping paths. Should be within [0, 1].\n+    drop_path_rate (float): Probability of dropping paths. Should be within\n+      [0, 1].\n   \n   Returns:\n     Tensor either with the residual path dropped or kept.\n \n   \"\"\"\n-  def __init__(self, drop_path, **kwargs):\n+  def __init__(self, drop_path_rate, **kwargs):\n     super().__init__(**kwargs)\n-    self.drop_path = drop_path\n+    self.drop_path_rate = drop_path_rate\n \n   def call(self, x, training=False):\n     if training:\n-      keep_prob = 1 - self.drop_path\n+      keep_prob = 1 - self.drop_path_rate\n       shape = (tf.shape(x)[0],) + (1,) * (len(tf.shape(x)) - 1)\n       random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n       random_tensor = tf.floor(random_tensor)\n@@ -180,7 +182,7 @@ class StochasticDepth(layers.Layer):\n \n   def get_config(self):\n     config = super().get_config()\n-    config.update({\"drop_path\": self.drop_path})\n+    config.update({\"drop_path_rate\": self.drop_path_rate})\n     return config\n \n \n@@ -206,10 +208,12 @@ class ConvNeXtBlock(Model):\n   Returns:\n     A keras.Model instance.\n   \"\"\"\n-  def __init__(self, projection_dim, drop_path=0.0, \n+  def __init__(self, projection_dim, drop_path_rate=0.0, \n     layer_scale_init_value=1e-6, **kwargs):\n     super().__init__(**kwargs)\n     self.projection_dim = projection_dim\n+    self.drop_path_rate = drop_path_rate\n+    self.layer_scale_init_value = layer_scale_init_value\n     name = kwargs[\"name\"]\n     \n     if layer_scale_init_value > 0.0:\n@@ -230,8 +234,8 @@ class ConvNeXtBlock(Model):\n     self.pw_conv_2 = layers.Dense(projection_dim, \n       name=name + \"_pointwise_conv_2\")\n     self.drop_path = (\n-      StochasticDepth(drop_path, name=name + \"_stochastic_depth\")\n-      if drop_path > 0.0\n+      StochasticDepth(drop_path_rate, name=name + \"_stochastic_depth\")\n+      if drop_path_rate > 0.0\n       else layers.Activation(\"linear\", name=name + \"_identity\")\n     )\n \n@@ -249,6 +253,14 @@ class ConvNeXtBlock(Model):\n \n     return inputs + self.drop_path(x)\n \n+  def get_config(self):\n+    config = {\n+      \"projection_dim\": self.projection_dim,\n+      \"drop_path_rate\": self.drop_path_rate,\n+      \"layer_scale_init_value\": self.layer_scale_init_value,\n+    }\n+    return config\n+\n \n def PreStem(name=None):\n   \"\"\"Normalizes inputs with ImageNet-1k mean and std.\n@@ -290,7 +302,7 @@ def Head(num_classes=1000, name=None):\n     x = layers.GlobalAveragePooling2D(name=name + \"_head_gap\")(x)\n     x = layers.LayerNormalization(\n       epsilon=1e-6, name=name + \"_head_layernorm\")(x)\n-    x = layers.Dense(num_classes, name=name + \"head_dense\")(x)\n+    x = layers.Dense(num_classes, name=name + \"_head_dense\")(x)\n     return x\n \n   return apply\n@@ -393,7 +405,7 @@ def ConvNeXt(depths,\n     x = PreStem(name=model_name)(x)\n   \n   # Stem block.\n-  stem = tf.keras.Sequential(\n+  stem = Sequential(\n     [\n       layers.Conv2D(projection_dims[0], kernel_size=4, strides=4,\n         name=model_name + \"_stem_conv\"),\n@@ -407,7 +419,7 @@ def ConvNeXt(depths,\n   downsample_layers = []\n   downsample_layers.append(stem)\n   for i in range(3):\n-    downsample_layer = tf.keras.Sequential(\n+    downsample_layer = Sequential(\n       [\n         layers.LayerNormalization(epsilon=1e-6, \n           name=model_name + \"_downsampling_layernorm_\" + str(i)),\n@@ -419,36 +431,25 @@ def ConvNeXt(depths,\n     downsample_layers.append(downsample_layer)\n   \n   # Stochastic depth schedule.\n+  # This is referred from the original ConvNeXt codebase:\n+  # https://github.com/facebookresearch/ConvNeXt/blob/main/models/convnext.py#L86\n   dp_rates = [x for x in tf.linspace(0.0, drop_path_rate, sum(depths))]\n \n-  # ConvNeXt stages.\n-  stages = []\n+  # First apply downsampling blocks and then apply ConvNeXt stages.\n   cur = 0\n   for i in range(4):\n-    stage = tf.keras.Sequential(\n-      [\n-        *[\n-            ConvNeXtBlock(\n+    x = downsample_layers[i](x)\n+    for j in range(depths[i]):\n+      x = ConvNeXtBlock(\n         projection_dim=projection_dims[i],\n-              drop_path=dp_rates[cur + j],\n+        drop_path_rate=dp_rates[cur + j],\n         layer_scale_init_value=layer_scale_init_value,\n         name=model_name + f\"_stage_{i}_block_{j}\",\n-            )\n-            for j in range(depths[i])\n-        ]\n-      ],\n-      name=model_name + f\"_stage_{i}\",\n-    )\n-    stages.append(stage)\n+      )(x)\n     cur += depths[i]\n \n-  # Apply the stages.\n-  for i in range(len(stages)):\n-    x = downsample_layers[i](x)\n-    x = stages[i](x)\n-\n   if include_top:\n-    x = Head(num_classes=classes)(x)\n+    x = Head(num_classes=classes, name=model_name)(x)\n     imagenet_utils.validate_activation(classifier_activation, weights)\n \n   else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#607e3f0447a8d129c226b7b750a6db6273d4fbba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 217 | Lines Deleted: 168 | Files Changed: 1 | Hunks: 19 | Methods Changed: 8 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 385 | Churn Cumulative: 1197 | Contributors (this commit): 3 | Commits (past 90d): 15 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,10 +14,6 @@\n # ==============================================================================\n \"\"\"Keras image dataset loading utilities.\"\"\"\n \n-\n-\n-\n-\n import tensorflow.compat.v2 as tf\n # pylint: disable=g-classes-have-attributes\n \n@@ -57,16 +53,17 @@ def split_dataset(dataset,\n   Returns:\n       A tuple of two `tf.data.Dataset` objects: the left and right splits.\n   \"\"\"\n+  dataset_type_spec = _get_type_spec(dataset)\n   \n-  if not isinstance(dataset,(tf.data.Dataset,list,tuple,np.ndarray)):\n+  if dataset_type_spec not in [tf.data.Dataset,list,tuple,np.ndarray]:\n     raise TypeError('`dataset` must be either a tf.data.Dataset object '\n                    f'or a list/tuple of arrays. Received : {type(dataset)}')\n \n   if right_size is None and left_size is None:\n-    raise ValueError('you must specify either `left_size` or `right_size`'\n-                    ' Received: `left_size`= None, and `right_size`=None')\n+    raise ValueError('At least one of the `left_size` or `right_size` '\n+                     'must be specified .Received: left_size=None and'\n+                     'right_size=None')\n \n-  dataset_type_spec = _get_type_spec(dataset)\n   dataset_as_list = _convert_dataset_to_list(dataset,dataset_type_spec)\n \n   if seed is None:\n@@ -82,12 +79,8 @@ def split_dataset(dataset,\n   left_split = list(dataset_as_list[:left_size])\n   right_split = list(dataset_as_list[-right_size:])\n \n-  if dataset_type_spec  is tuple:\n-    left_split = tuple(np.array(sample) for sample in zip(*left_split))\n-    right_split = tuple(np.array(sample) for sample in zip(*right_split))\n-  elif dataset_type_spec  is list:\n-    left_split = tuple(np.array(sample) for sample in zip(*left_split))\n-    right_split = tuple(np.array(sample) for sample in zip(*right_split))\n+  left_split = _restore_dataset_from_list(left_split,dataset_type_spec)\n+  right_split = _restore_dataset_from_list(right_split,dataset_type_spec)\n \n   left_split = tf.data.Dataset.from_tensor_slices(left_split)\n   right_split = tf.data.Dataset.from_tensor_slices(right_split)\n@@ -101,7 +94,23 @@ def _convert_dataset_to_list(dataset,\n                              dataset_type_spec,\n                              data_size_warning_flag= True,\n                              ensure_shape_similarity = True):\n-  \"\"\"Helper function to convert a tf.data.Dataset  object or a list/tuple of numpy.ndarrays to a list\n+  \"\"\"Convert a tf.data.Dataset  object or a list/tuple of numpy.ndarrays to a list\n+\n+  Args:\n+      dataset : A `tf.data.Dataset` object or a list/tuple of arrays.\n+      dataset_type_spec : the type of the dataset\n+      data_size_warning_flag (bool, optional): If set to True ,a warning will \n+                                               be issued if the dataset takes \n+                                               longer than 10 seconds to \n+                                               iterate. Defaults to True.\n+      ensure_shape_similarity (bool, optional): If set to True , the shape of \n+                                                the first sample will be used \n+                                                to validate the shape of rest \n+                                                of the samples.\n+                                                Defaults to True.\n+\n+  Returns:\n+      List: A list of tuples/numpy arrays.\n   \"\"\"\n   # TODO (prakashsellathurai): add support for Batched  and unbatched dict tf datasets\n   \n@@ -113,10 +122,7 @@ def _convert_dataset_to_list(dataset,\n                                  ensure_shape_similarity,\n                                  data_size_warning_flag,\n                                  start_time):\n-\n-    if dataset_type_spec is tuple:\n-      dataset_as_list.append(np.array(sample) )\n-    elif dataset_type_spec is list:\n+    if dataset_type_spec in [tuple ,list]:\n       dataset_as_list.append(np.array(sample))\n     else:\n       dataset_as_list.append(sample)\n@@ -124,14 +130,29 @@ def _convert_dataset_to_list(dataset,\n   return dataset_as_list\n \n def _get_data_iterator_from_dataset(dataset,dataset_type_spec) :\n-  \"\"\"Helper function to get the data iterator from a tf.data.Dataset object\n+  \"\"\"Get the iterator from the dataset\n+\n+  Args:\n+      dataset :  A `tf.data.Dataset` object or a list/tuple of arrays.\n+      dataset_type_spec : the type of the dataset\n+\n+  Raises:\n+      ValueError: \n+                - If the dataset is empty.\n+                - If the dataset is not a `tf.data.Dataset` object \n+                  or a list/tuple of arrays.\n+                - If the dataset is a list/tuple of arrays and the\n+                  length of the list/tuple is not equal to the number\n+\n+  Returns:\n+      iterator: An `iterator` object.\n   \"\"\"\n   if dataset_type_spec == list:\n     if len(dataset) == 0:\n-      raise ValueError('Received an empty list dataset '\n-                       'Please provide a non-empty list dataset')\n+      raise ValueError('Received an empty list dataset. '\n+                       'Please provide a non-empty list of arrays.')\n \n-    if isinstance(dataset[0],np.ndarray):\n+    if _get_type_spec(dataset[0]) is np.ndarray:\n       expected_shape = dataset[0].shape\n       for i,element in enumerate(dataset):\n         if not np.array(element).shape[0] == expected_shape[0]:\n@@ -140,7 +161,7 @@ def _get_data_iterator_from_dataset(dataset,dataset_type_spec) :\n                           f'Expected shape={expected_shape} '\n                           f'Received shape={np.array(element).shape}.'\n                           f'Please provide a list of numpy arrays with '\n-                           f'equal length.')\n+                          f'same length.')\n     else:\n       raise ValueError('Expected a list of numpy.ndarrays,'\n                        'Received: {}'.format(type(dataset[0])))\n@@ -148,10 +169,10 @@ def _get_data_iterator_from_dataset(dataset,dataset_type_spec) :\n     return iter(zip(*dataset))\n   elif  dataset_type_spec == tuple:\n     if len(dataset) == 0:\n-      raise ValueError('`dataset` must be a non-empty list of'\n-                       ' numpy.ndarrays or tf.data.Dataset objects.')\n+      raise ValueError('Received an empty list dataset.'\n+                       'Please provide a non-empty tuple of arrays.')\n \n-    if isinstance(dataset[0],np.ndarray):\n+    if _get_type_spec(dataset[0]) is np.ndarray:\n       expected_shape = dataset[0].shape\n       for i,element in enumerate(dataset):\n         if not np.array(element).shape[0] == expected_shape[0]:\n@@ -160,7 +181,7 @@ def _get_data_iterator_from_dataset(dataset,dataset_type_spec) :\n                           f'Expected shape={expected_shape} '\n                           f'Received shape={np.array(element).shape}.'\n                           f'Please provide a tuple of numpy arrays with '\n-                           f'equal length.')\n+                           'same length.')\n     else:\n       raise ValueError('Expected a tuple of numpy.ndarrays,'\n                        'Received: {}'.format(type(dataset[0])))\n@@ -173,7 +194,174 @@ def _get_data_iterator_from_dataset(dataset,dataset_type_spec) :\n   elif dataset_type_spec == np.ndarray:\n     return iter(dataset)\n \n+def _get_next_sample(dataset_iterator,\n+                    ensure_shape_similarity,\n+                    data_size_warning_flag,\n+                    start_time):\n+  \"\"\"\"Yield data samples from the `dataset_iterator`\n+  \n+  Args:\n+      dataset_iterator : An `iterator` object.\n+      data_size_warning_flag (bool, optional): If set to True ,a warning will \n+                                               be issued if the dataset takes \n+                                               longer than 10 seconds to \n+                                               iterate. Defaults to True.\n+      ensure_shape_similarity (bool, optional): If set to True , the shape of \n+                                                the first sample will be used \n+                                                to validate the shape of rest \n+                                                of the samples.\n+                                                Defaults to True.\n+      start_time (float): the start time of the dataset iteration. this is\n+                          used only if `data_size_warning_flag` is set to true.\n+\n+  Raises:\n+      ValueError: - If the dataset is empty.\n+                  - If `ensure_shape_similarity` is set to True and the\n+                    shape of the first sample is not equal to the shape of\n+                    atleast one of the rest of the samples.\n+\n+  yields:\n+      data_sample: A tuple/list of numpy arrays.\n+  \"\"\"\n+  try:\n+    dataset_iterator = iter(dataset_iterator)\n+    first_sample = next(dataset_iterator)\n+    if isinstance(first_sample,(tf.Tensor,np.ndarray)):\n+      first_sample_shape = np.array(first_sample).shape\n+    else:\n+      first_sample_shape = None\n+      ensure_shape_similarity  = False\n+    yield first_sample\n+  except StopIteration:\n+    raise ValueError('Received an empty Dataset. `dataset` must '\n+                     'be a non-empty list/tuple of numpy.ndarrays '\n+                     'or tf.data.Dataset objects.')\n+\n+  for i,sample in enumerate(dataset_iterator):\n+    if ensure_shape_similarity:\n+      if first_sample_shape != np.array(sample).shape:\n+        raise ValueError('All `dataset` samples must have same shape, '\n+                        f'Expected shape: {np.array(first_sample).shape} '\n+                        f'Received shape: {np.array(sample).shape} at index '\n+                        f'{i}.')\n+    if data_size_warning_flag:\n+      if i % 10 == 0:\n+        cur_time = time.time()\n+        # warns user if the dataset is too large to iterate within 10s\n+        if int(cur_time - start_time) > 10 and data_size_warning_flag:\n+          warnings.warn('The dataset is taking longer than 10 seconds to '\n+                        'iterate. This may be due to the size of the dataset. '\n+                        'Please consider using a smaller dataset'\n+                        '(e.g. < 10,000 samples). \\nTo hide this '\n+                        'warning message, set `data_size_warning_flag=False`.',\n+                        category=ResourceWarning,\n+                        source='split_dataset')\n+          data_size_warning_flag = False\n+    yield sample\n+\n+def _restore_dataset_from_list(dataset,dataset_type_spec):\n+  \"\"\"Restore the dataset from the list of arrays.\n+  \"\"\"\n+  if dataset_type_spec  in [tuple,list]:\n+    dataset = tuple(np.array(sample) for sample in zip(*dataset))\n+  return dataset\n+\n+def _rescale_dataset_split_sizes(left_size,right_size,total_length):\n+  \"\"\"Rescale the dataset split sizes to ensure that the sum of \n+  the split sizes is equal to the total length of the dataset.\n+  \n+  Args:\n+      left_size : The size of the left dataset split.\n+      right_size : The size of the right dataset split.\n+      total_length : The total length of the dataset.\n+\n+  Raises:\n+      TypeError: - If `left_size` or `right_size` is not an integer or float.\n+      ValueError: - If `left_size` or `right_size` is negative or greater\n+                    than 1 or greater than `total_length`.\n+\n+  Returns:\n+      tuple: A tuple of rescaled left_size and right_size \n+  \"\"\"\n+  left_size_type = type(left_size)\n+  right_size_type = type(right_size)\n+\n+  # check both left_size and right_size are integers or floats\n+  if ((left_size is not None and left_size_type not in [int,float]) and\n+      (right_size is not None and right_size_type not in [int,float])):\n+    raise TypeError('Invalid `left_size` and `right_size` Types. Expected: '\n+                    'integer or float or None, Received: type(left_size)='\n+                   f'{left_size_type} and type(right_size)={right_size_type}')\n+  \n+  # check left_size is a integer or float\n+  if left_size is not None and left_size_type not in [int,float]:\n+    raise TypeError('Invalid `left_size` Type.Expected: int or float or None, '\n+                   f'Received: type(left_size)={left_size_type}.  ')\n+\n+  # check right_size is a integer or float\n+  if right_size is not None and right_size_type not in [int,float]:\n+    raise TypeError(f'Invalid `right_size` Type.Expected: int or float or None,'\n+                    f'Received: type(right_size)={right_size_type}.  ')\n+\n+  # check left_size and right_size are non-zero\n+  if left_size == 0 and right_size == 0:\n+    raise ValueError('Both `left_size` and `right_size` are zero. '\n+                     'Atleast one of the split sizes must be non-zero.')\n+\n+  # check left_size is non-negative and less than 1 and less than total_length\n+  if (left_size_type == int and (left_size <= 0 or left_size>= total_length)\n+      or left_size_type == float and (left_size <= 0 or left_size>= 1) ):\n+    raise ValueError('`left_size` should be either a positive integer '\n+                    f'and smaller than {total_length} or a float '\n+                     'within the range `[0, 1]`. Received: left_size='\n+                    f'{left_size}')\n+\n+  # check right_size is non-negative and less than 1 and less than total_length\n+  if (right_size_type == int and (right_size <= 0 or right_size>= total_length)\n+      or right_size_type == float and (right_size <= 0 or right_size>= 1)):\n+    raise ValueError('`right_size` should be either a positive integer '\n+                    f'and smaller than {total_length} or a float '\n+                     'within the range `[0, 1]`. Received: right_size='\n+                    f'{right_size}')\n+\n+  # check sum of left_size and right_size is less than or equal to total_length\n+  if right_size_type == left_size_type == float and right_size + left_size > 1:\n+    raise ValueError('The sum of `left_size` and `right_size` is greater '\n+                     'than 1. It must be less than or equal to 1.')\n+\n+  if left_size_type == float:\n+    left_size = round(left_size*total_length)\n+  elif left_size_type == int:\n+    left_size = float(left_size)\n+\n+  if right_size_type == float:\n+    right_size = round(right_size*total_length)\n+  elif right_size_type == int:\n+    right_size = float(right_size)\n+\n+  if left_size is None:\n+    left_size = total_length - right_size\n+  elif right_size is None:\n+    right_size = total_length - left_size\n+\n+  if left_size + right_size > total_length:\n+    raise ValueError('The sum of `left_size` and `right_size` should '\n+                     'be smaller than the {total_length}. '\n+                    f'Received: left_size + right_size = {left_size+right_size}'\n+                    f'and total_length = {total_length}')\n+\n+  for split,side in [(left_size,'left'),(right_size,'right')]:\n+    if split == 0:\n+      raise ValueError(f'With `dataset` of length={total_length}, `left_size`='\n+                        '{left_size} and `right_size`={right_size}.'\n+                       f'Resulting {side} side dataset split will be empty. '\n+                        'Adjust any of the aforementioned parameters')\n+\n+  left_size,right_size = int(left_size) ,int(right_size)\n+  return left_size,right_size\n+\n def _get_type_spec(dataset):\n+  \"\"\"Get the type spec of the dataset.\"\"\"\n   if isinstance(dataset,(tuple)):\n     return tuple\n   elif isinstance(dataset,(list)):\n@@ -187,147 +375,8 @@ def _get_type_spec(dataset):\n   else:\n     return None\n \n-def _get_next_sample(dataset_iterator,\n-                    ensure_shape_similarity,\n-                    data_size_warning_flag,\n-                    start_time):\n-  \"\"\"Helper function to yield samples from a `dataset_iterator`\n-     if `ensure_shape_similarity` is set to True raises error if the\n-     shapes of the samples are not the same .\n-     if `data_size_warning_flag` is set to True, raises warning if the\n-     dataset iteration takes more than 10 seconds.\n-  \"\"\"\n-\n-  try:\n-    dataset_iterator = iter(dataset_iterator)\n-    first_sample = next(dataset_iterator)\n-    if isinstance(first_sample,(tf.Tensor,np.ndarray)):\n-      first_sample_shape = np.array(first_sample).shape\n-    else:\n-      first_sample_shape = None\n-      ensure_shape_similarity  = False\n-    yield first_sample\n-  except StopIteration:\n-    raise ValueError(' Received  an empty Dataset i.e dataset with no elements.'\n-                     ' `dataset` must be a non-empty list/tuple of'\n-                     ' numpy.ndarrays or tf.data.Dataset objects.')\n-\n-  if isinstance(first_sample,dict):\n-    raise TypeError('`dataset` must be either a tf.data.Dataset object'\n-                    ' or a list/tuple of arrays. '\n-                    ' Received : tf.data.Dataset with dict elements')\n-\n-  for i,sample in enumerate(dataset_iterator):\n-    if ensure_shape_similarity:\n-      if first_sample_shape != np.array(sample).shape:\n-        raise ValueError(' All elements of `dataset` must have the same shape,'\n-                        f' Expected shape: {np.array(first_sample).shape}'\n-                        f' Received shape: {np.array(sample).shape} at'\n-                        f' index {i}')\n-\n-    if data_size_warning_flag:\n-      if i % 10 == 0:\n-        cur_time = time.time()\n-        # warns user if the dataset is too large to iterate within 10s\n-        if int(cur_time - start_time) > 10 and data_size_warning_flag:\n-          warnings.warn(' Takes too long time to process the `dataset`,'\n-                        ' this function is only  for small datasets '\n-                        ' (e.g. < 10,000 samples).',category=ResourceWarning,\n-                        source='split_dataset',stacklevel=2)\n-          data_size_warning_flag = False\n-\n-    yield sample\n-\n-\n-def _rescale_dataset_split_sizes(left_size,right_size,total_length):\n-  \"\"\"Helper function to rescale  left_size/right_size args relative\n-  to dataset's size\n-  \"\"\"\n-\n-  left_size_type = type(left_size)\n-  right_size_type = type(right_size)\n-\n-  if ((left_size is not None and left_size_type not in [int,float]) and\n-      (right_size is not None and right_size_type not in [int,float])):\n-    raise TypeError('Invalid `left_size` and `right_size` Types. '\n-                     'Expected: integer or float or None. '\n-                     f' Received: {left_size_type} and {right_size_type}')\n-\n-  if left_size is not None and left_size_type not in [int,float]:\n-    raise TypeError(f'Invalid `left_size` Type. Received: {left_size_type}.  '\n-                     ' Expected: int or float or None')\n-\n-  if right_size is not None and right_size_type not in [int,float]:\n-    raise TypeError(f'Invalid `right_size` Type. Received: {right_size_type}.'\n-                    ' Expected: int or float or None')\n-\n-  if left_size == 0 and right_size == 0:\n-    raise ValueError('Invalid `left_size` and `right_size` values. '\n-                     'You must specify either `left_size` or `right_size` with '\n-                     f'value greater than 0 and less than {total_length} '\n-                      'or a float within range [0,1] to split the dataset'\n-                      f'Received: `left_size`={left_size}, '\n-                      f'`right_size`={right_size}')\n-\n-  if (left_size_type == int\n-      and (left_size <= 0 or left_size>= total_length)\n-      or left_size_type == float\n-      and (left_size <= 0 or left_size>= 1) ):\n-    raise ValueError('`left_size` should be either a positive integer '\n-                     f'and smaller than {total_length} or a float '\n-                     'within the range `[0, 1]`. Received: left_size='\n-                     f'{left_size}')\n-\n-  if (right_size_type == int\n-      and (right_size <= 0 or right_size>= total_length)\n-      or right_size_type == float\n-      and (right_size <= 0 or right_size>= 1)):\n-    raise ValueError('`right_size` should be either a positive integer '\n-                     f'and smaller than {total_length} or '\n-                     'a float within the range `[0, 1]`. Received: right_size='\n-                     f'{right_size}')\n-\n-  if right_size_type == left_size_type == float and right_size + left_size > 1:\n-    raise ValueError('sum of `left_size` and `right_size`'\n-                     ' should be within `[0,1]`.'\n-                    f'Received: {right_size + left_size} ,'\n-                    'reduce the `left_size` or `right_size`')\n-\n-  if left_size_type == float:\n-    left_size = round(left_size*total_length)\n-  elif left_size_type == int:\n-    left_size = float(left_size)\n-\n-  if right_size_type == float:\n-    right_size = round(right_size*total_length)\n-  elif right_size_type == int:\n-    right_size = float(right_size)\n-\n-\n-  if left_size is None:\n-    left_size = total_length - right_size\n-  elif right_size is None:\n-    right_size = total_length - left_size\n-\n-  if left_size + right_size > total_length:\n-    raise ValueError('The sum of `left_size` and `right_size`'\n-                     f' should be smaller than the samples {total_length} '\n-                     ' reduce `left_size` or `right_size` ' )\n-\n-\n-  for split,side in [(left_size,'left'),(right_size,'right')]:\n-    if split == 0:\n-      raise ValueError(f'with dataset of length={total_length} '\n-                      '`left_size`={left_size} and `right_size`={right_size}, '\n-                      f'resulting {side} dataset split will be empty. '\n-                      'Adjust any of the aforementioned parameters')\n-\n-  left_size,right_size = int(left_size) ,int(right_size)\n-  return left_size,right_size\n-\n-\n def is_batched(tf_dataset):\n-  \"\"\"returns true if given tf dataset  is batched or false if not\n+  \"\"\"\"Check if the tf.data.Dataset is batched.\n   reference: https://stackoverflow.com/a/66101853/8336491\n   \"\"\"\n   try:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#46130091764f20cccd46c94cd746e37ac4b9ce75", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 60 | Lines Deleted: 40 | Files Changed: 1 | Hunks: 20 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 100 | Churn Cumulative: 943 | Contributors (this commit): 1 | Commits (past 90d): 10 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -53,24 +53,18 @@ class SplitDatasetTest(tf.test.TestCase):\n     self.assertEqual(np.array(list(right_split)[0][1]).shape,())\n     \n   def test_dataset_with_invalid_shape(self):\n-    with self.assertRaises(ValueError):\n+    with self.assertRaisesRegex(ValueError,\n+                                'Received a list of numpy arrays '\n+                                'with different length'):\n       dataset=[np.ones(shape=(200, 32)), np.zeros(shape=(100, 32))]\n       dataset_utils.split_dataset(dataset, left_size=4)\n \n-    with self.assertRaises(ValueError):\n+    with self.assertRaisesRegex(ValueError,\n+                                'Received a tuple of numpy arrays '\n+                                'with different length'):\n       dataset=(np.ones(shape=(200, 32)), np.zeros(shape=(201, 32)))\n       dataset_utils.split_dataset(dataset, left_size=4)\n \n-    with self.assertRaises(ValueError):\n-      dataset=tf.data.Dataset.from_tensor_slices(\n-        np.ones(shape=(200, 32,32,1)), np.zeros(shape=(32)))\n-      dataset_utils.split_dataset(dataset, left_size=4)\n-\n-    with self.assertRaises(ValueError):\n-      dataset=tf.data.Dataset.from_tensor_slices(\n-        (np.ones(shape=(200, 32,32,1)), np.zeros(shape=(32))))\n-      dataset_utils.split_dataset(dataset, left_size=4)\n-\n   def test_tuple_of_numpy_arrays(self):\n     dataset=(np.random.rand(4, 3), np.random.rand(4, 3))\n     left_split,right_split = dataset_utils.split_dataset(dataset, left_size=2)\n@@ -180,22 +174,35 @@ class SplitDatasetTest(tf.test.TestCase):\n     self.assertEqual(len(right_split), 4)\n \n   def test_invalid_dataset(self):\n-    with self.assertRaises(TypeError):\n+    with self.assertRaisesRegex(TypeError,\n+                                '`dataset` must be either a tf.data.Dataset '\n+                               f'object or a list/tuple of arrays. Received '\n+                                ': <class \\'NoneType\\'>'):\n       dataset_utils.split_dataset(dataset=None, left_size=5)\n-    with self.assertRaises(TypeError):\n+    with self.assertRaisesRegex(TypeError,\n+                                '`dataset` must be either a tf.data.Dataset '\n+                               f'object or a list/tuple of arrays. Received '\n+                                ': <class \\'int\\'>'):\n       dataset_utils.split_dataset(dataset=1, left_size=5)\n-    with self.assertRaises(TypeError):\n+    with self.assertRaisesRegex(TypeError,\n+                                '`dataset` must be either a tf.data.Dataset '\n+                               f'object or a list/tuple of arrays. Received '\n+                                ': <class \\'float\\'>'):\n       dataset_utils.split_dataset(dataset=float(1.2), left_size=5)\n-    with self.assertRaises(TypeError):\n+    with self.assertRaisesRegex(TypeError,\n+                                '`dataset` must be either a tf.data.Dataset '\n+                               f'object or a list/tuple of arrays. Received '\n+                                ': <class \\'dict\\'>'):\n       dataset_utils.split_dataset(dataset=dict({}), left_size=5)\n-    with self.assertRaises(TypeError):\n+    with self.assertRaisesRegex(TypeError,\n+                                '`dataset` must be either a tf.data.Dataset '\n+                               f'object or a list/tuple of arrays. Received '\n+                                ': <class \\'float\\'>'):\n       dataset_utils.split_dataset(dataset=float('INF'), left_size=5)\n \n   def test_valid_left_and_right_sizes(self):\n     dataset = np.array([1,2,3])\n-    splitted_dataset = dataset_utils.split_dataset(dataset,\n-                                                   left_size=1,\n-                                                   right_size=2)\n+    splitted_dataset = dataset_utils.split_dataset(dataset,1,2)\n     assert(len(splitted_dataset) == 2)\n     left_split,right_split = splitted_dataset\n     self.assertEqual(len(left_split), 1)\n@@ -262,50 +269,63 @@ class SplitDatasetTest(tf.test.TestCase):\n     self.assertEqual(len(right_split), 1)\n \n   def test_invalid_float_left_and_right_sizes(self):\n-    with self.assertRaises(ValueError):\n+    expected_regex = (r'^(.*?(\\bleft_size\\b).*?(\\bshould be\\b)'\n+                      r'.*?(\\bwithin the range\\b).*?(\\b0\\b).*?(\\b1\\b))')\n+    with self.assertRaisesRegexp(ValueError,expected_regex):\n       dataset = [np.ones(shape=(200, 32,32)), np.zeros(shape=(200, 32,32))]\n       dataset_utils.split_dataset(dataset, left_size=1.5,right_size=0.2)\n-    with self.assertRaises(ValueError):\n-      dataset = [1]\n-      dataset_utils.split_dataset(dataset, left_size=0.8,right_size=0.2)\n+\n+    expected_regex = (r'^(.*?(\\bright_size\\b).*?(\\bshould be\\b)'\n+                      r'.*?(\\bwithin the range\\b).*?(\\b0\\b).*?(\\b1\\b))')\n+    with self.assertRaisesRegex(ValueError,expected_regex):\n+      dataset = [np.ones(shape=(200, 32)), np.zeros(shape=(200, 32))]\n+      dataset_utils.split_dataset(dataset, left_size=0.8,right_size=-0.8)\n \n   def test_None_and_zero_left_and_right_size(self):\n-    with self.assertRaises(ValueError):\n-      dataset_utils.split_dataset(dataset=np.array([1,2,3]), left_size=None)\n+    expected_regex = (r'^.*?(\\bleft_size\\b).*?(\\bright_size\\b).*?(\\bmust '\n+                      r'be specified\\b).*?(\\bReceived: left_size=None and'\n+                      r' right_size=None\\b)')\n \n-    with self.assertRaises(ValueError):\n+    with self.assertRaisesRegex(ValueError,expected_regex):\n+      dataset_utils.split_dataset(dataset=np.array([1,2,3]), left_size=None)\n+    with self.assertRaisesRegex(ValueError, expected_regex):\n       dataset_utils.split_dataset(np.array([1,2,3]),left_size=None,\n                                   right_size=None)\n \n-    with self.assertRaises(ValueError):\n-      dataset_utils.split_dataset(np.array([1,2,3]),left_size=3,\n-                                  right_size=None)\n+    expected_regex = (r'^.*?(\\bleft_size\\b).*?(\\bshould be\\b)'\n+                      r'.*?(\\bpositive\\b).*?(\\bsmaller than 3\\b)')\n+    with self.assertRaisesRegex(ValueError,expected_regex):\n+      dataset_utils.split_dataset(np.array([1,2,3]),left_size=3)\n \n-    with self.assertRaises(ValueError):\n-      dataset_utils.split_dataset(np.array([1,2,3]),left_size=3,\n-                                  right_size=None)\n-\n-    with self.assertRaises(ValueError):\n+    expected_regex = ('Both `left_size` and `right_size` are zero. '\n+                     'Atleast one of the split sizes must be non-zero.')\n+    with self.assertRaisesRegex(ValueError,expected_regex):\n       dataset_utils.split_dataset(np.array([1,2,3]), left_size=0,\n                                   right_size=0)\n \n   def test_invalid_left_and_right_size_types(self):\n-    with self.assertRaises(TypeError):\n+    expected_regex = (r'^.*?(\\bInvalid `left_size` and `right_size` Types'\n+                      r'\\b).*?(\\bExpected: integer or float or None\\b)')\n+    with self.assertRaisesRegex(TypeError,expected_regex):\n       dataset_utils.split_dataset(np.array([1,2,3]), left_size='1',\n                                   right_size='1')\n \n-    with self.assertRaises(TypeError):\n+    expected_regex = (r'^.*?(\\bInvalid `right_size` Type\\b)')   \n+    with self.assertRaisesRegex(TypeError,expected_regex):\n       dataset_utils.split_dataset(np.array([1,2,3]),left_size=0,\n                                   right_size='1')\n  \n-    with self.assertRaises(TypeError):\n+    expected_regex = (r'^.*?(\\bInvalid `left_size` Type\\b)')   \n+    with self.assertRaisesRegex(TypeError,expected_regex):     \n       dataset_utils.split_dataset(np.array([1,2,3]),left_size='100',\n                                   right_size=None)\n \n-    with self.assertRaises(TypeError):\n+    expected_regex = (r'^.*?(\\bInvalid `right_size` Type\\b)')   \n+    with self.assertRaisesRegex(TypeError,expected_regex): \n       dataset_utils.split_dataset(np.array([1,2,3]),right_size='1')\n       \n-    with self.assertRaises(TypeError):\n+    expected_regex = (r'^.*?(\\bInvalid `right_size` Type\\b)')   \n+    with self.assertRaisesRegex(TypeError,expected_regex): \n       dataset_utils.split_dataset(np.array([1,2,3]),left_size=0.5,\n                                   right_size='1')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
