{"custom_id": "keras#b525f5f4d733d464657f158000457c0b9f45c1d2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 108 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,5 @@\n from __future__ import absolute_import\n from __future__ import print_function\n-import h5py\n import numpy as np\n import sys\n from collections import defaultdict\n@@ -10,6 +9,8 @@ class HDF5Matrix():\n     refs = defaultdict(int)\n \n     def __init__(self, datapath, dataset, start, end, normalizer=None):\n+        import h5py\n+\n         if datapath not in list(self.refs.keys()):\n             f = h5py.File(datapath)\n             self.refs[datapath] = f\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dad54ec2114f486a68ea3b1ad5c462fcf7af8097", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 23 | Churn Cumulative: 243 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -10,6 +10,29 @@ import sys\n def load_data(path='reuters.pkl', nb_words=None, skip_top=0,\n               maxlen=None, test_split=0.2, seed=113,\n               start_char=1, oov_char=2, index_from=3):\n+    '''\n+    # Arguments\n+        path: where to store the data (in `/.keras/dataset`)\n+        nb_words: max number of words to include. Words are ranked\n+            by how often they occur (in the training set) and only\n+            the most frequent words are kept\n+        skip_top: skip the top N most frequently occuring words\n+            (which may not be informative).\n+        maxlen: truncate sequences after this length.\n+        test_split: Fraction of the dataset to be used as test data.\n+        seed: random seed for sample shuffling.\n+        start_char: The start of a sequence will be marked with this character.\n+            Set to 1 because 0 is usually the padding character.\n+        oov_char: words that were cut out because of the `nb_words`\n+            or `skip_top` limit will be replaced with this character.\n+        index_from: index actual words with this index and higher.\n+\n+    Note that the 'out of vocabulary' character is only used for\n+    words that were present in the training set but are not included\n+    because they're not making the `nb_words` cut here.\n+    Words that were not seen in the trining set but are in the test set\n+    have simply been skipped.\n+    '''\n \n     path = get_file(path, origin='https://s3.amazonaws.com/text-datasets/reuters.pkl')\n     f = open(path, 'rb')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a4d191d4f97a8100a561b87802fdd6ab5db0ed26", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 105 | Contributors (this commit): 6 | Commits (past 90d): 5 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -42,6 +42,7 @@ if os.path.exists(_config_path):\n     _BACKEND = _backend\n \n # save config file\n+if not os.path.exists(_config_path):\n     _config = {'floatx': floatx(),\n                'epsilon': epsilon(),\n                'backend': _BACKEND,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3d5bf9753f6a770bf02b3eecd8b85fb2195cbc85", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 9948 | Contributors (this commit): 60 | Commits (past 90d): 34 | Contributors (cumulative): 67 | DMM Complexity: None\n\nDIFF:\n@@ -1313,7 +1313,7 @@ class Model(Container):\n             max_q_size: maximum size for the generator queue\n             nb_worker: maximum number of processes to spin up when using process based threading\n             pickle_safe: if True, use process based threading. Note that because\n-                this implementation relies on multiprocessing, you should not pass non\n+                this implementation relies on multiprocessing, you should not pass\n                 non picklable arguments to the generator as they can't be passed\n                 easily to children processes.\n \n@@ -1507,7 +1507,7 @@ class Model(Container):\n             max_q_size: maximum size for the generator queue\n             nb_worker: maximum number of processes to spin up when using process based threading\n             pickle_safe: if True, use process based threading. Note that because\n-                this implementation relies on multiprocessing, you should not pass non\n+                this implementation relies on multiprocessing, you should not pass\n                 non picklable arguments to the generator as they can't be passed\n                 easily to children processes.\n \n@@ -1592,7 +1592,7 @@ class Model(Container):\n             max_q_size: maximum size for the generator queue\n             nb_worker: maximum number of processes to spin up when using process based threading\n             pickle_safe: if True, use process based threading. Note that because\n-                this implementation relies on multiprocessing, you should not pass non\n+                this implementation relies on multiprocessing, you should not pass\n                 non picklable arguments to the generator as they can't be passed\n                 easily to children processes.\n \n\n@@ -811,7 +811,7 @@ class Sequential(Model):\n             max_q_size: maximum size for the generator queue\n             nb_worker: maximum number of processes to spin up\n             pickle_safe: if True, use process based threading. Note that because\n-                this implementation relies on multiprocessing, you should not pass non\n+                this implementation relies on multiprocessing, you should not pass\n                 non picklable arguments to the generator as they can't be passed\n                 easily to children processes.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1941eaabe0e71d850ea7fb2fdcca4c987b8e363d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 7 | Churn Cumulative: 2083 | Contributors (this commit): 20 | Commits (past 90d): 21 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -413,19 +413,20 @@ def generator_queue(generator, max_q_size=10,\n     if pickle_safe:\n         q = multiprocessing.Queue(maxsize=max_q_size)\n         _stop = multiprocessing.Event()\n+        lock = multiprocessing.Lock()\n     else:\n         q = queue.Queue()\n         _stop = threading.Event()\n+        lock = threading.Lock()\n \n     try:\n         def data_generator_task():\n             while not _stop.is_set():\n                 try:\n                     if pickle_safe or q.qsize() < max_q_size:\n-                        try:\n+                        lock.acquire()\n                         generator_output = next(generator)\n-                        except ValueError:\n-                            continue\n+                        lock.release()\n                         q.put(generator_output)\n                     else:\n                         time.sleep(wait_time)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b2392413fabc193727b1e2c5b12e694068c93405", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 14 | Churn Cumulative: 3208 | Contributors (this commit): 21 | Commits (past 90d): 28 | Contributors (cumulative): 21 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1143,8 +1143,6 @@ class Merge(Layer):\n         self.mode = mode\n         self.concat_axis = concat_axis\n         self.dot_axes = dot_axes\n-        if type(self.dot_axes) == int:\n-            self.dot_axes = [self.dot_axes, ] * 2\n         self._output_shape = output_shape\n         self.node_indices = node_indices\n         self._output_mask = output_mask\n@@ -1220,16 +1218,16 @@ class Merge(Layer):\n             n2 = len(shape2)\n             if type(dot_axes) == int:\n                 if dot_axes < 0:\n-                    dot_axes = [dot_axes % n1, dot_axes % n2]\n+                    self.dot_axes = [dot_axes % n1, dot_axes % n2]\n                 else:\n-                    dot_axes = [n1 - dot_axes, n2 - dot_axes]\n-            if type(dot_axes) not in [list, tuple]:\n+                    self.dot_axes = [dot_axes, ] * 2\n+            if type(self.dot_axes) not in [list, tuple]:\n                 raise Exception('Invalid type for dot_axes - should be a list.')\n-            if len(dot_axes) != 2:\n+            if len(self.dot_axes) != 2:\n                 raise Exception('Invalid format for dot_axes - should contain two elements.')\n-            if type(dot_axes[0]) is not int or type(dot_axes[1]) is not int:\n+            if type(self.dot_axes[0]) is not int or type(self.dot_axes[1]) is not int:\n                 raise Exception('Invalid format for dot_axes - list elements should be \"int\".')\n-            if shape1[dot_axes[0]] != shape2[dot_axes[1]]:\n+            if shape1[self.dot_axes[0]] != shape2[self.dot_axes[1]]:\n                 raise Exception('Dimension incompatibility using dot mode: ' +\n                                 '%s != %s. ' % (shape1[dot_axes[0]], shape2[dot_axes[1]]) +\n                                 'Layer shapes: %s, %s' % (shape1, shape2))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#52c1a7456fe8fc8ecb91ed7a5122d566aa2cb4af", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 1424 | Contributors (this commit): 16 | Commits (past 90d): 7 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -229,6 +229,9 @@ class Adagrad(Optimizer):\n     # Arguments\n         lr: float >= 0. Learning rate.\n         epsilon: float >= 0.\n+    \n+    # References\n+        - [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n     '''\n     def __init__(self, lr=0.01, epsilon=1e-8, **kwargs):\n         super(Adagrad, self).__init__(**kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f25e894558d9ef11d89645049e01142ef971dcc7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 188 | Lines Deleted: 21 | Files Changed: 7 | Hunks: 13 | Methods Changed: 19 | Complexity Δ (Sum/Max): 47/31 | Churn Δ: 209 | Churn Cumulative: 9633 | Contributors (this commit): 55 | Commits (past 90d): 112 | Contributors (cumulative): 105 | DMM Complexity: 0.848\n\nDIFF:\n@@ -9,8 +9,8 @@ import numpy as np\n np.random.seed(1337)  # for reproducibility\n \n from keras.preprocessing import sequence\n-from keras.models import Model\n-from keras.layers import Dense, Dropout, Embedding, LSTM, Input, merge\n+from keras.models import Sequential\n+from keras.layers import Dense, Dropout, Embedding, LSTM, Input, Bidirectional\n from keras.datasets import imdb\n \n \n@@ -31,24 +31,11 @@ print('X_test shape:', X_test.shape)\n y_train = np.array(y_train)\n y_test = np.array(y_test)\n \n-\n-# this is the placeholder tensor for the input sequences\n-sequence = Input(shape=(maxlen,), dtype='int32')\n-# this embedding layer will transform the sequences of integers\n-# into vectors of size 128\n-embedded = Embedding(max_features, 128, input_length=maxlen)(sequence)\n-\n-# apply forwards LSTM\n-forwards = LSTM(64)(embedded)\n-# apply backwards LSTM\n-backwards = LSTM(64, go_backwards=True)(embedded)\n-\n-# concatenate the outputs of the 2 LSTMs\n-merged = merge([forwards, backwards], mode='concat', concat_axis=-1)\n-after_dp = Dropout(0.5)(merged)\n-output = Dense(1, activation='sigmoid')(after_dp)\n-\n-model = Model(input=sequence, output=output)\n+model = Sequential()\n+model.add(Embedding(max_features, 128, input_length=maxlen))\n+model.add(Bidirectional(LSTM(64)))\n+model.add(Dropout(0.5))\n+model.add(Dense(1, activation='sigmoid'))\n \n # try using different optimizers and different optimizer configs\n model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n\n@@ -859,6 +859,15 @@ def one_hot(indices, nb_classes):\n     return tf.one_hot(indices, depth=nb_classes, axis=-1)\n \n \n+def reverse(x, axes):\n+    '''Reverse a tensor along the the specified axes\n+    '''\n+    if type(axes) == int:\n+        axes = [axes]\n+    dims = [True if i in axes else False for i in range(len(x.get_shape()._dims))]\n+    return tf.reverse(x, dims)\n+\n+\n # VALUE MANIPULATION\n \n \n\n@@ -606,6 +606,15 @@ def one_hot(indices, nb_classes):\n     return oh\n \n \n+def reverse(x, axes):\n+    '''Reverse a tensor along the the specified axes\n+    '''\n+    if type(axes) == int:\n+        axes = [axes]\n+    slices = [slice(None, None, -1) if i in axes else slice(None, None, None) for i in range(x.ndim)]\n+    return x[slices]\n+\n+\n # VALUE MANIPULATION\n \n \n\n@@ -284,9 +284,13 @@ class Layer(object):\n \n         # these properties will be set upon call of self.build(),\n         # which itself will be called upon self.add_inbound_node if necessary.\n+        if not hasattr(self, 'trainable_weights'):\n             self.trainable_weights = []\n+        if not hasattr(self, 'non_trainable_weights'):\n             self.non_trainable_weights = []\n+        if not hasattr(self, 'regularizers'):\n             self.regularizers = []\n+        if not hasattr(self, 'constraints'):\n             self.constraints = {}  # dict {tensor: constraint instance}\n         self.built = False\n \n\n@@ -133,3 +133,127 @@ class TimeDistributed(Wrapper):\n             output_shape = self.get_output_shape_for(input_shape)\n             y = K.reshape(y, (-1, input_length) + output_shape[2:])\n         return y\n+\n+\n+class Bidirectional(Wrapper):\n+    ''' Bidirectional wrapper for RNNs\n+\n+    # Arguments:\n+        layer: `Recurrent` instance.\n+        merge_mode: Mode by which outputs of the forward and backward RNNs will be combined. One of {'sum', 'mul', 'concat', 'ave', None}. If None, the outputs will not be combined, they will be returned as a list.\n+\n+    # Examples:\n+    ```python\n+    model = Sequential()\n+    model.add(Bidirectional(LSTM(10, return_sequences=True), input_shape=(5, 10)))\n+    model.add(Bidirectional(LSTM(10)))\n+    model.add(Dense(5))\n+    model.add(Activation('softmax'))\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+    ```\n+    '''\n+    def __init__(self, layer, merge_mode='concat', weights=None, **kwargs):\n+        assert merge_mode in ['sum', 'mul', 'ave', 'concat', None], \"Invalid merge mode. Merge mode should be one of {'sum', 'mul', 'ave', 'concat', None}\"\n+        self.forward_layer = layer\n+        config = layer.get_config()\n+        config['go_backwards'] = not config['go_backwards']\n+        self.backward_layer = layer.__class__.from_config(config)\n+        self.forward_layer.name = 'forward_' + self.forward_layer.name\n+        self.backward_layer.name = 'backward_' + self.backward_layer.name\n+        self.merge_mode = merge_mode\n+        if weights:\n+            nw = len(weights)\n+            self.forward_layer.initial_weights = weights[:nw//2]\n+            self.backward_layer.initial_weights = weights[nw//2:]\n+        self.stateful = layer.stateful\n+        self.return_sequences = layer.return_sequences\n+        self.supports_masking = True\n+        super(Bidirectional, self).__init__(layer, **kwargs)\n+\n+    def get_weights(self):\n+        return self.forward_layer.get_weights() + self.backward_layer.get_weights()\n+\n+    def set_weights(self, weights):\n+        nw = len(weights)\n+        self.forward_layer.set_weights(weights[:nw//2])\n+        self.backward_layer.set_weights(weights[nw//2:])\n+\n+    def get_output_shape_for(self, input_shape):\n+        if self.merge_mode in ['sum', 'ave', 'mul']:\n+            return self.forward_layer.get_output_shape_for(input_shape)\n+        elif self.merge_mode == 'concat':\n+            shape = list(self.forward_layer.get_output_shape_for(input_shape))\n+            shape[-1] *= 2\n+            return tuple(shape)\n+        elif self.merge_mode is None:\n+            return [self.forward_layer.get_output_shape_for(input_shape)] * 2\n+\n+    def call(self, X, mask=None):\n+        Y = self.forward_layer.call(X, mask)\n+        Y_rev = self.backward_layer.call(X, mask)\n+        if self.return_sequences:\n+            Y_rev = K.reverse(Y_rev, 1)\n+        if self.merge_mode == 'concat':\n+            return K.concatenate([Y, Y_rev])\n+        elif self.merge_mode == 'sum':\n+            return Y + Y_rev\n+        elif self.merge_mode == 'ave':\n+            return (Y + Y_rev) / 2\n+        elif self.merge_mode == 'mul':\n+            return Y * Y_rev\n+        elif self.merge_mode is None:\n+            return [Y, Y_rev]\n+\n+    def reset_states(self):\n+        self.forward_layer.reset_states()\n+        self.backward_layer.reset_states()\n+\n+    def build(self, input_shape):\n+        self.forward_layer.build(input_shape)\n+        self.backward_layer.build(input_shape)\n+\n+    def compute_mask(self, input, mask):\n+        if self.return_sequences:\n+            if not self.merge_mode:\n+                return [mask, mask]\n+            else:\n+                return mask\n+        else:\n+            return None\n+\n+    @property\n+    def trainable_weights(self):\n+        if hasattr(self.forward_layer, 'trainable_weights'):\n+            return self.forward_layer.trainable_weights + self.backward_layer.trainable_weights\n+        return []\n+\n+    @property\n+    def non_trainable_weights(self):\n+        if hasattr(self.forward_layer, 'non_trainable_weights'):\n+            return self.forward_layer.non_trainable_weights + self.backward_layer.non_trainable_weights\n+        return []\n+\n+    @property\n+    def updates(self):\n+        if hasattr(self.forward_layer, 'updates'):\n+            return self.forward_layer.updates + self.backward_layer.updates\n+        return []\n+\n+    @property\n+    def regularizers(self):\n+        if hasattr(self.forward_layer, 'regularizers'):\n+            return self.forward_layer.regularizers + self.backward_layer.regularizers\n+        return []\n+\n+    @property\n+    def constraints(self):\n+        _constraints = {}\n+        if hasattr(self.forward_layer, 'constraints'):\n+            _constraints.update(self.forward_layer.constraints)\n+            _constraints.update(self.backward_layer.constraints)\n+        return _constraints\n+\n+    def get_config(self):\n+        config = {\"merge_mode\": self.merge_mode}\n+        base_config = super(Bidirectional, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n\n@@ -70,6 +70,8 @@ class TestBackend(object):\n         check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 5, 3),\n                                    axes=(2, 2))\n         check_single_tensor_operation('transpose', (4, 2))\n+        check_single_tensor_operation('reverse', (4, 3, 2), axes=1)\n+        check_single_tensor_operation('reverse', (4, 3, 2), axes=(1, 2))\n \n     def test_shape_operations(self):\n         # concatenate\n@@ -633,5 +635,6 @@ class TestBackend(object):\n             koh = K.eval(K.one_hot(K.variable(indices, dtype='int32'), nb_classes))\n             assert np.all(koh == oh)\n \n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n@@ -3,7 +3,7 @@ import numpy as np\n from numpy.testing import assert_allclose\n from keras.utils.test_utils import keras_test\n from keras.layers import wrappers, Input\n-from keras.layers import core, convolutional\n+from keras.layers import core, convolutional, recurrent\n from keras.models import Sequential, Model, model_from_json\n \n \n@@ -76,5 +76,36 @@ def test_TimeDistributed():\n     outer_model.fit(np.random.random((10, 3, 2)), np.random.random((10, 3, 3)), nb_epoch=1, batch_size=10)\n \n \n+@keras_test\n+def test_Bidirectional():\n+    for rnn in [recurrent.SimpleRNN, recurrent.LSTM]:\n+        for mode in ['sum', 'concat']:\n+            x = np.random.random((5, 3, 2))\n+            output_dim = 6 if mode == 'concat' else 3\n+            y = np.random.random((5, output_dim))\n+\n+            # test with Sequential model\n+            model = Sequential()\n+            model.add(wrappers.Bidirectional(rnn(3), merge_mode=mode, input_shape=(3, 2)))\n+            model.add(core.Activation('sigmoid'))\n+            model.compile(loss='mse', optimizer='sgd')\n+            model.fit(x, y, nb_epoch=1, batch_size=5)\n+\n+            # test stacked bidirectional layers\n+            model = Sequential()\n+            model.add(wrappers.Bidirectional(rnn(2, return_sequences=True), merge_mode=mode, input_shape=(3, 2)))\n+            model.add(wrappers.Bidirectional(rnn(3), merge_mode=mode))\n+            model.add(core.Activation('sigmoid'))\n+            model.compile(loss='mse', optimizer='sgd')\n+            model.fit(x, y, nb_epoch=1, batch_size=5)\n+\n+            # test with functional API\n+            input = Input((3, 2))\n+            output = wrappers.Bidirectional(rnn(3), merge_mode=mode)(input)\n+            model = Model(input, output)\n+            model.compile(loss='mse', optimizer='sgd')\n+            model.fit(x, y, nb_epoch=1, batch_size=5)\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#33ff9dbce230068e65dedb2f41a00c39c70bef93", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 14 | Files Changed: 1 | Hunks: 8 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 36 | Churn Cumulative: 161 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -78,33 +78,41 @@ def test_TimeDistributed():\n \n @keras_test\n def test_Bidirectional():\n-    for rnn in [recurrent.SimpleRNN, recurrent.LSTM]:\n+    rnn = recurrent.SimpleRNN\n+    nb_sample = 1\n+    dim = 2\n+    timesteps = 2\n     for mode in ['sum', 'concat']:\n-            x = np.random.random((5, 3, 2))\n-            output_dim = 6 if mode == 'concat' else 3\n-            y = np.random.random((5, output_dim))\n+        x = np.random.random((nb_sample, timesteps, dim))\n+        output_dim = 2 if mode == 'concat' else 1\n+        y = np.random.random((nb_sample, output_dim))\n \n         # test with Sequential model\n         model = Sequential()\n-            model.add(wrappers.Bidirectional(rnn(3), merge_mode=mode, input_shape=(3, 2)))\n-            model.add(core.Activation('sigmoid'))\n+        model.add(wrappers.Bidirectional(rnn(1),\n+                                         merge_mode=mode, input_shape=(timesteps, dim)))\n         model.compile(loss='mse', optimizer='sgd')\n-            model.fit(x, y, nb_epoch=1, batch_size=5)\n+        model.fit(x, y, nb_epoch=1, batch_size=1)\n+\n+        # test config\n+        model.get_config()\n+        model = model_from_json(model.to_json())\n+        model.summary()\n \n         # test stacked bidirectional layers\n         model = Sequential()\n-            model.add(wrappers.Bidirectional(rnn(2, return_sequences=True), merge_mode=mode, input_shape=(3, 2)))\n-            model.add(wrappers.Bidirectional(rnn(3), merge_mode=mode))\n-            model.add(core.Activation('sigmoid'))\n+        model.add(wrappers.Bidirectional(rnn(1, return_sequences=True),\n+                                         merge_mode=mode, input_shape=(timesteps, dim)))\n+        model.add(wrappers.Bidirectional(rnn(1), merge_mode=mode))\n         model.compile(loss='mse', optimizer='sgd')\n-            model.fit(x, y, nb_epoch=1, batch_size=5)\n+        model.fit(x, y, nb_epoch=1, batch_size=1)\n \n         # test with functional API\n-            input = Input((3, 2))\n-            output = wrappers.Bidirectional(rnn(3), merge_mode=mode)(input)\n+        input = Input((timesteps, dim))\n+        output = wrappers.Bidirectional(rnn(1), merge_mode=mode)(input)\n         model = Model(input, output)\n         model.compile(loss='mse', optimizer='sgd')\n-            model.fit(x, y, nb_epoch=1, batch_size=5)\n+        model.fit(x, y, nb_epoch=1, batch_size=1)\n \n \n if __name__ == '__main__':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3bf76379868f963b37fa2c015ba26b8f99f738a8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 5 | Churn Cumulative: 430 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -153,7 +153,10 @@ class Bidirectional(Wrapper):\n     ```\n     '''\n     def __init__(self, layer, merge_mode='concat', weights=None, **kwargs):\n-        assert merge_mode in ['sum', 'mul', 'ave', 'concat', None], \"Invalid merge mode. Merge mode should be one of {'sum', 'mul', 'ave', 'concat', None}\"\n+        if merge_mode not in ['sum', 'mul', 'ave', 'concat', None]:\n+            raise ValueError('Invalid merge mode. '\n+                             'Merge mode should be one of '\n+                             '{\"sum\", \"mul\", \"ave\", \"concat\", None}')\n         self.forward_layer = layer\n         config = layer.get_config()\n         config['go_backwards'] = not config['go_backwards']\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#007d2c2e25791b600f4a5e4ccee17e1a5929d8e9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 7 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 176 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -79,17 +79,18 @@ def test_TimeDistributed():\n @keras_test\n def test_Bidirectional():\n     rnn = recurrent.SimpleRNN\n-    nb_sample = 1\n+    nb_sample = 2\n     dim = 2\n     timesteps = 2\n+    output_dim = 2\n     for mode in ['sum', 'concat']:\n         x = np.random.random((nb_sample, timesteps, dim))\n-        output_dim = 2 if mode == 'concat' else 1\n-        y = np.random.random((nb_sample, output_dim))\n+        target_dim = 2 * output_dim if mode == 'concat' else output_dim\n+        y = np.random.random((nb_sample, target_dim))\n \n         # test with Sequential model\n         model = Sequential()\n-        model.add(wrappers.Bidirectional(rnn(1),\n+        model.add(wrappers.Bidirectional(rnn(output_dim),\n                                          merge_mode=mode, input_shape=(timesteps, dim)))\n         model.compile(loss='mse', optimizer='sgd')\n         model.fit(x, y, nb_epoch=1, batch_size=1)\n@@ -101,15 +102,15 @@ def test_Bidirectional():\n \n         # test stacked bidirectional layers\n         model = Sequential()\n-        model.add(wrappers.Bidirectional(rnn(1, return_sequences=True),\n+        model.add(wrappers.Bidirectional(rnn(output_dim, return_sequences=True),\n                                          merge_mode=mode, input_shape=(timesteps, dim)))\n-        model.add(wrappers.Bidirectional(rnn(1), merge_mode=mode))\n+        model.add(wrappers.Bidirectional(rnn(output_dim), merge_mode=mode))\n         model.compile(loss='mse', optimizer='sgd')\n         model.fit(x, y, nb_epoch=1, batch_size=1)\n \n         # test with functional API\n         input = Input((timesteps, dim))\n-        output = wrappers.Bidirectional(rnn(1), merge_mode=mode)(input)\n+        output = wrappers.Bidirectional(rnn(output_dim), merge_mode=mode)(input)\n         model = Model(input, output)\n         model.compile(loss='mse', optimizer='sgd')\n         model.fit(x, y, nb_epoch=1, batch_size=1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#02ff1d4462197a4d419ed407b0b6fa45fba14632", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 174 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -26,7 +26,7 @@ nb_filters = 32\n # size of pooling area for max pooling\n nb_pool = 2\n # convolution kernel size\n-nb_conv = 3\n+kernel_size = (3, 3)\n \n # the data, shuffled and split between train and test sets\n (X_train, y_train), (X_test, y_test) = mnist.load_data()\n@@ -47,11 +47,11 @@ Y_test = np_utils.to_categorical(y_test, nb_classes)\n \n model = Sequential()\n \n-model.add(Convolution2D(nb_filters, nb_conv, nb_conv,\n+model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1],\n                         border_mode='valid',\n                         input_shape=(1, img_rows, img_cols)))\n model.add(Activation('relu'))\n-model.add(Convolution2D(nb_filters, nb_conv, nb_conv))\n+model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1]))\n model.add(Activation('relu'))\n model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n model.add(Dropout(0.25))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8d9cb782fb73e5d08dd75f29a1158e68df0e17fc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 384 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 11 | Complexity Δ (Sum/Max): 25/25 | Churn Δ: 384 | Churn Cumulative: 384 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,384 @@\n+'''This is an implementation of Net2Net experiment with MNIST in\n+'Net2Net: Accelerating Learning via Knowledge Transfer'\n+by Tianqi Chen, Ian Goodfellow, and Jonathon Shlens\n+\n+arXiv:1511.05641v4 [cs.LG] 23 Apr 2016\n+http://arxiv.org/abs/1511.05641\n+\n+Notes\n+- What:\n+  + Net2Net is a group of methods to transfer knowledge from a teacher neural\n+    net to a student net,so that the student net can be trained faster than\n+    from scratch.\n+  + The paper discussed two specific methods of Net2Net, i.e. Net2WiderNet\n+    and Net2DeeperNet.\n+  + Net2WiderNet replaces a model with an equivalent wider model that has\n+    more units in each hidden layer.\n+  + Net2DeeperNet replaces a model with an equivalent deeper model.\n+  + Both are based on the idea of 'function-preserving transformations of\n+    neural nets'.\n+- Why:\n+  + Enable fast exploration of multiple neural nets in experimentation and\n+    design process,by creating a series of wider and deeper models with\n+    transferable knowledge.\n+  + Enable 'lifelong learning system' by gradually adjusting model complexity\n+    to data availability,and reusing transferable knowledge.\n+\n+Experiments\n+- Teacher model: a basic CNN model trained on MNIST for 3 epochs.\n+- Net2WiderNet exepriment:\n+  + Student model has a wider Conv2D layer and a wider FC layer.\n+  + Comparison of 'random-padding' vs 'net2wider' weight initialization.\n+  + With both methods, student model should immediately perform as well as\n+    teacher model, but 'net2wider' is slightly better.\n+- Net2DeeperNet experiment:\n+  + Student model has an extra Conv2D layer and an extra FC layer.\n+  + Comparison of 'random-init' vs 'net2deeper' weight initialization.\n+  + Starting performance of 'net2deeper' is better than 'random-init'.\n+- Hyper-parameters:\n+  + SGD with momentum=0.9 is used for training teacher and student models.\n+  + Learning rate adjustment: it's suggested to reduce learning rate\n+    to 1/10 for student model.\n+  + Addition of noise in 'net2wider' is used to break weight symmetry\n+    and thus enable full capacity of student models. It is optional\n+    when a Dropout layer is used.\n+\n+Results\n+- Tested with 'Theano' backend and 'th' image_dim_ordering.\n+- Running on GPU GeForce GTX 980M\n+- Performance Comparisons - validation loss values during first 3 epochs:\n+(1) teacher_model:             0.075    0.041    0.041\n+(2) wider_random_pad:          0.036    0.034    0.032\n+(3) wider_net2wider:           0.032    0.030    0.030\n+(4) deeper_random_init:        0.061    0.043    0.041\n+(5) deeper_net2deeper:         0.032    0.031    0.029\n+'''\n+\n+from __future__ import print_function\n+import numpy as np\n+np.random.seed(1337)\n+\n+from keras.models import Sequential\n+from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n+from keras.optimizers import SGD\n+from keras.utils import np_utils\n+from keras.datasets import mnist\n+\n+input_shape = (1, 28, 28)  # image shape\n+nb_class = 10  # number of class\n+\n+\n+# load and pre-process data\n+def preprocess_input(x):\n+    return x.reshape((-1, ) + input_shape) / 255.\n+\n+\n+def preprocess_output(y):\n+    return np_utils.to_categorical(y)\n+\n+(train_x, train_y), (validation_x, validation_y) = mnist.load_data()\n+train_x, validation_x = map(preprocess_input, [train_x, validation_x])\n+train_y, validation_y = map(preprocess_output, [train_y, validation_y])\n+print('Loading MNIST data...')\n+print('train_x shape:', train_x.shape, 'train_y shape:', train_y.shape)\n+print('validation_x shape:', validation_x.shape,\n+      'validation_y shape', validation_y.shape)\n+\n+\n+# knowledge transfer algorithms\n+def wider2net_conv2d(teacher_w1, teacher_b1, teacher_w2, new_width, init):\n+    '''Get initial weights for a wider conv2d layer with a bigger nb_filter,\n+    by 'random-padding' or 'net2wider'.\n+\n+    # Arguments\n+        teacher_w1: `weight` of conv2d layer to become wider,\n+          of shape (nb_filter1, nb_channel1, kh1, kw1)\n+        teacher_b1: `bias` of conv2d layer to become wider,\n+          of shape (nb_filter1, )\n+        teacher_w2: `weight` of next connected conv2d layer,\n+          of shape (nb_filter2, nb_channel2, kh2, kw2)\n+        new_width: new `nb_filter` for the wider conv2d layer\n+        init: initialization algorithm for new weights,\n+          either 'random-pad' or 'net2wider'\n+    '''\n+    assert teacher_w1.shape[0] == teacher_w2.shape[1], (\n+        'successive layers from teacher model should have compatible shapes')\n+    assert teacher_w1.shape[0] == teacher_b1.shape[0], (\n+        'weight and bias from same layer should have compatible shapes')\n+    assert new_width > teacher_w1.shape[0], (\n+        'new width (nb_filter) should be bigger than the existing one')\n+\n+    n = new_width - teacher_w1.shape[0]\n+    if init == 'random-pad':\n+        new_w1 = np.random.normal(0, 0.1, size=(n, ) + teacher_w1.shape[1:])\n+        new_b1 = np.ones(n) * 0.1\n+        new_w2 = np.random.normal(0, 0.1, size=(\n+            teacher_w2.shape[0], n) + teacher_w2.shape[2:])\n+    elif init == 'net2wider':\n+        index = np.random.randint(teacher_w1.shape[0], size=n)\n+        factors = np.bincount(index)[index] + 1.\n+        new_w1 = teacher_w1[index, :, :, :]\n+        new_b1 = teacher_b1[index]\n+        new_w2 = teacher_w2[:, index, :, :] / factors.reshape((1, -1, 1, 1))\n+    else:\n+        raise ValueError('Unsupported weight initializer: %s' % init)\n+\n+    student_w1 = np.concatenate((teacher_w1, new_w1), axis=0)\n+    if init == 'random-pad':\n+        student_w2 = np.concatenate((teacher_w2, new_w2), axis=1)\n+    elif init == 'net2wider':\n+        # add small noise to break symmetry, so that student model will have\n+        # full capacity later\n+        noise = np.random.normal(0, 5e-2 * new_w2.std(), size=new_w2.shape)\n+        student_w2 = np.concatenate((teacher_w2, new_w2 + noise), axis=1)\n+        student_w2[:, index, :, :] = new_w2\n+    student_b1 = np.concatenate((teacher_b1, new_b1), axis=0)\n+\n+    return student_w1, student_b1, student_w2\n+\n+\n+def wider2net_fc(teacher_w1, teacher_b1, teacher_w2, new_width, init):\n+    '''Get initial weights for a wider fully connected (dense) layer\n+       with a bigger nout, by 'random-padding' or 'net2wider'.\n+\n+    # Arguments\n+        teacher_w1: `weight` of fc layer to become wider,\n+          of shape (nin1, nout1)\n+        teacher_b1: `bias` of fc layer to become wider,\n+          of shape (nout1, )\n+        teacher_w2: `weight` of next connected fc layer,\n+          of shape (nin2, nout2)\n+        new_width: new `nout` for the wider fc layer\n+        init: initialization algorithm for new weights,\n+          either 'random-pad' or 'net2wider'\n+    '''\n+    assert teacher_w1.shape[1] == teacher_w2.shape[0], (\n+        'successive layers from teacher model should have compatible shapes')\n+    assert teacher_w1.shape[1] == teacher_b1.shape[0], (\n+        'weight and bias from same layer should have compatible shapes')\n+    assert new_width > teacher_w1.shape[1], (\n+        'new width (nout) should be bigger than the existing one')\n+\n+    n = new_width - teacher_w1.shape[1]\n+    if init == 'random-pad':\n+        new_w1 = np.random.normal(0, 0.1, size=(teacher_w1.shape[0], n))\n+        new_b1 = np.ones(n) * 0.1\n+        new_w2 = np.random.normal(0, 0.1, size=(n, teacher_w2.shape[1]))\n+    elif init == 'net2wider':\n+        index = np.random.randint(teacher_w1.shape[1], size=n)\n+        factors = np.bincount(index)[index] + 1.\n+        new_w1 = teacher_w1[:, index]\n+        new_b1 = teacher_b1[index]\n+        new_w2 = teacher_w2[index, :] / factors[:, np.newaxis]\n+    else:\n+        raise ValueError('Unsupported weight initializer: %s' % init)\n+\n+    student_w1 = np.concatenate((teacher_w1, new_w1), axis=1)\n+    if init == 'random-pad':\n+        student_w2 = np.concatenate((teacher_w2, new_w2), axis=0)\n+    elif init == 'net2wider':\n+        # add small noise to break symmetry, so that student model will have\n+        # full capacity later\n+        noise = np.random.normal(0, 5e-2 * new_w2.std(), size=new_w2.shape)\n+        student_w2 = np.concatenate((teacher_w2, new_w2 + noise), axis=0)\n+        student_w2[index, :] = new_w2\n+    student_b1 = np.concatenate((teacher_b1, new_b1), axis=0)\n+\n+    return student_w1, student_b1, student_w2\n+\n+\n+def deeper2net_conv2d(teacher_w):\n+    '''Get initial weights for a deeper conv2d layer by net2deeper'.\n+\n+    # Arguments\n+        teacher_w: `weight` of previous conv2d layer,\n+          of shape (nb_filter, nb_channel, kh, kw)\n+    '''\n+    nb_filter, nb_channel, kh, kw = teacher_w.shape\n+    student_w = np.zeros((nb_filter, nb_filter, kh, kw))\n+    for i in xrange(nb_filter):\n+        student_w[i, i, (kh - 1) / 2, (kw - 1) / 2] = 1.\n+    student_b = np.zeros(nb_filter)\n+    return student_w, student_b\n+\n+\n+def copy_weights(teacher_model, student_model, layer_names):\n+    '''Copy weights from teacher_model to student_model,\n+     for layers with names listed in layer_names\n+    '''\n+    for name in layer_names:\n+        weights = teacher_model.get_layer(name=name).get_weights()\n+        student_model.get_layer(name=name).set_weights(weights)\n+\n+\n+# methods to construct teacher_model and student_models\n+def make_teacher_model(train_data, validation_data, nb_epoch=3):\n+    '''Train a simple CNN as teacher model.\n+    '''\n+    model = Sequential()\n+    model.add(Conv2D(64, 3, 3, input_shape=input_shape,\n+                     border_mode='same', name='conv1'))\n+    model.add(MaxPooling2D(name='pool1'))\n+    model.add(Conv2D(64, 3, 3, border_mode='same', name='conv2'))\n+    model.add(MaxPooling2D(name='pool2'))\n+    model.add(Flatten(name='flatten'))\n+    model.add(Dense(64, activation='relu', name='fc1'))\n+    model.add(Dense(nb_class, activation='softmax', name='fc2'))\n+    model.compile(loss='categorical_crossentropy',\n+                  optimizer=SGD(lr=0.01, momentum=0.9),\n+                  metrics=['accuracy'])\n+\n+    train_x, train_y = train_data\n+    history = model.fit(train_x, train_y, nb_epoch=nb_epoch,\n+                        validation_data=validation_data)\n+    return model, history\n+\n+\n+def make_wider_student_model(teacher_model, train_data,\n+                             validation_data, init, nb_epoch=3):\n+    '''Train a wider student model based on teacher_model,\n+       with either 'random-pad' (baseline) or 'net2wider'\n+    '''\n+    new_conv1_width = 128\n+    new_fc1_width = 128\n+\n+    model = Sequential()\n+    # a wider conv1 compared to teacher_model\n+    model.add(Conv2D(new_conv1_width, 3, 3, input_shape=input_shape,\n+                     border_mode='same', name='conv1'))\n+    model.add(MaxPooling2D(name='pool1'))\n+    model.add(Conv2D(64, 3, 3, border_mode='same', name='conv2'))\n+    model.add(MaxPooling2D(name='pool2'))\n+    model.add(Flatten(name='flatten'))\n+    # a wider fc1 compared to teacher model\n+    model.add(Dense(new_fc1_width, activation='relu', name='fc1'))\n+    model.add(Dense(nb_class, activation='softmax', name='fc2'))\n+\n+    # The weights for other layers need to be copied from teacher_model\n+    # to student_model, except for widened layers\n+    # and their immediate downstreams, which will be initialized separately.\n+    # For this example there are no other layers that need to be copied.\n+\n+    w_conv1, b_conv1 = teacher_model.get_layer('conv1').get_weights()\n+    w_conv2, b_conv2 = teacher_model.get_layer('conv2').get_weights()\n+    new_w_conv1, new_b_conv1, new_w_conv2 = wider2net_conv2d(\n+        w_conv1, b_conv1, w_conv2, new_conv1_width, init)\n+    model.get_layer('conv1').set_weights([new_w_conv1, new_b_conv1])\n+    model.get_layer('conv2').set_weights([new_w_conv2, b_conv2])\n+\n+    w_fc1, b_fc1 = teacher_model.get_layer('fc1').get_weights()\n+    w_fc2, b_fc2 = teacher_model.get_layer('fc2').get_weights()\n+    new_w_fc1, new_b_fc1, new_w_fc2 = wider2net_fc(\n+        w_fc1, b_fc1, w_fc2, new_fc1_width, init)\n+    model.get_layer('fc1').set_weights([new_w_fc1, new_b_fc1])\n+    model.get_layer('fc2').set_weights([new_w_fc2, b_fc2])\n+\n+    model.compile(loss='categorical_crossentropy',\n+                  optimizer=SGD(lr=0.001, momentum=0.9),\n+                  metrics=['accuracy'])\n+\n+    train_x, train_y = train_data\n+    history = model.fit(train_x, train_y, nb_epoch=nb_epoch,\n+                        validation_data=validation_data)\n+    return model, history\n+\n+\n+def make_deeper_student_model(teacher_model, train_data,\n+                              validation_data, init, nb_epoch=3):\n+    '''Train a deeper student model based on teacher_model,\n+       with either 'random-init' (baseline) or 'net2deeper'\n+    '''\n+    model = Sequential()\n+    model.add(Conv2D(64, 3, 3, input_shape=input_shape,\n+                     border_mode='same', name='conv1'))\n+    model.add(MaxPooling2D(name='pool1'))\n+    model.add(Conv2D(64, 3, 3, border_mode='same', name='conv2'))\n+    # add another conv2d layer to make original conv2 deeper\n+    if init == 'net2deeper':\n+        prev_w, _ = model.get_layer('conv2').get_weights()\n+        new_weights = deeper2net_conv2d(prev_w)\n+        model.add(Conv2D(64, 3, 3, border_mode='same',\n+                         name='conv2-deeper', weights=new_weights))\n+    elif init == 'random-init':\n+        model.add(Conv2D(64, 3, 3, border_mode='same', name='conv2-deeper'))\n+    else:\n+        raise ValueError('Unsupported weight initializer: %s' % init)\n+    model.add(MaxPooling2D(name='pool2'))\n+    model.add(Flatten(name='flatten'))\n+    model.add(Dense(64, activation='relu', name='fc1'))\n+    # add another fc layer to make original fc1 deeper\n+    if init == 'net2deeper':\n+        # net2deeper for fc layer with relu, is just an identity initializer\n+        model.add(Dense(64, init='identity',\n+                        activation='relu', name='fc1-deeper'))\n+    elif init == 'random-init':\n+        model.add(Dense(64, activation='relu', name='fc1-deeper'))\n+    else:\n+        raise ValueError('Unsupported weight initializer: %s' % init)\n+    model.add(Dense(nb_class, activation='softmax', name='fc2'))\n+\n+    # copy weights for other layers\n+    copy_weights(teacher_model, model, layer_names=[\n+                 'conv1', 'conv2', 'fc1', 'fc2'])\n+\n+    model.compile(loss='categorical_crossentropy',\n+                  optimizer=SGD(lr=0.001, momentum=0.9),\n+                  metrics=['accuracy'])\n+\n+    train_x, train_y = train_data\n+    history = model.fit(train_x, train_y, nb_epoch=nb_epoch,\n+                        validation_data=validation_data)\n+    return model, history\n+\n+\n+# experiments setup\n+def net2wider_experiment():\n+    '''Benchmark performances of\n+    (1) a teacher model,\n+    (2) a wider student model with `random_pad` initializer\n+    (3) a wider student model with `Net2WiderNet` initializer\n+    '''\n+    train_data = (train_x, train_y)\n+    validation_data = (validation_x, validation_y)\n+    print('\\nExperiment of Net2WiderNet ...')\n+    print('\\nbuilding teacher model ...')\n+    teacher_model, _ = make_teacher_model(train_data,\n+                                          validation_data,\n+                                          nb_epoch=3)\n+\n+    print('\\nbuilding wider student model by random padding ...')\n+    make_wider_student_model(teacher_model, train_data,\n+                             validation_data, 'random-pad',\n+                             nb_epoch=3)\n+    print('\\nbuilding wider student model by net2wider ...')\n+    make_wider_student_model(teacher_model, train_data,\n+                             validation_data, 'net2wider',\n+                             nb_epoch=3)\n+\n+\n+def net2deeper_experiment():\n+    '''Benchmark performances of\n+    (1) a teacher model,\n+    (2) a deeper student model with `random_init` initializer\n+    (3) a deeper student model with `Net2DeeperNet` initializer\n+    '''\n+    train_data = (train_x, train_y)\n+    validation_data = (validation_x, validation_y)\n+    print('\\nExperiment of Net2DeeperNet ...')\n+    print('\\nbuilding teacher model ...')\n+    teacher_model, _ = make_teacher_model(train_data,\n+                                          validation_data,\n+                                          nb_epoch=3)\n+\n+    print('\\nbuilding deeper student model by random init ...')\n+    make_deeper_student_model(teacher_model, train_data,\n+                              validation_data, 'random-init',\n+                              nb_epoch=3)\n+    print('\\nbuilding deeper student model by net2deeper ...')\n+    make_deeper_student_model(teacher_model, train_data,\n+                              validation_data, 'net2deeper',\n+                              nb_epoch=3)\n+\n+# run the experiments\n+net2wider_experiment()\n+net2deeper_experiment()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d4f5dff8eeaa3f13e5ab3cba3e8babaea66fc90e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 2551 | Contributors (this commit): 28 | Commits (past 90d): 43 | Contributors (cumulative): 28 | DMM Complexity: None\n\nDIFF:\n@@ -1658,6 +1658,7 @@ def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n                                                   labels=sparse_labels,\n                                                   sequence_length=input_length), 1)\n \n+\n def ctc_decode(y_pred, input_length, greedy=True, beam_width=None,\n                dict_seq_lens=None, dict_values=None):\n     '''Decodes the output of a softmax using either\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
