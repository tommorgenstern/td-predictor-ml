{"custom_id": "scrapy#ce5a132f12341a4118edb7c8ae3b7c2a27306057", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 19 | Files Changed: 9 | Hunks: 11 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 30 | Churn Cumulative: 9909 | Contributors (this commit): 87 | Commits (past 90d): 26 | Contributors (cumulative): 193 | DMM Complexity: 0.5\n\nDIFF:\n@@ -231,6 +231,7 @@ linkcheck_ignore = [\n     r\"http://localhost:\\d+\",\n     \"http://hg.scrapy.org\",\n     \"http://directory.google.com/\",\n+    r\"https://github.com/scrapy/scrapy/issues/\\d+\",\n ]\n \n \n\n@@ -24,7 +24,6 @@ logger = logging.getLogger(__name__)\n class AjaxCrawlMiddleware:\n     \"\"\"\n     Handle 'AJAX crawlable' pages marked as crawlable via meta tag.\n-    For more info see https://developers.google.com/webmasters/ajax-crawling/docs/getting-started.\n     \"\"\"\n \n     def __init__(self, settings: BaseSettings):\n@@ -70,8 +69,7 @@ class AjaxCrawlMiddleware:\n \n     def _has_ajax_crawlable_variant(self, response: Response) -> bool:\n         \"\"\"\n-        Return True if a page without hash fragment could be \"AJAX crawlable\"\n-        according to https://developers.google.com/webmasters/ajax-crawling/docs/getting-started.\n+        Return True if a page without hash fragment could be \"AJAX crawlable\".\n         \"\"\"\n         body = response.text[: self.lookup_bytes]\n         return _has_ajaxcrawlable_meta(body)\n\n@@ -222,7 +222,7 @@ class Request(object_ref):\n         **kwargs: Any,\n     ) -> Self:\n         \"\"\"Create a Request object from a string containing a `cURL\n-        <https://curl.haxx.se/>`_ command. It populates the HTTP method, the\n+        <https://curl.se/>`_ command. It populates the HTTP method, the\n         URL, the headers, the cookies and the body. It accepts the same\n         arguments as the :class:`Request` class, taking preference and\n         overriding the values of the same arguments contained in the cURL\n\n@@ -46,17 +46,15 @@ def fingerprint(\n \n     The request fingerprint is a hash that uniquely identifies the resource the\n     request points to. For example, take the following two urls:\n-\n-    http://www.example.com/query?id=111&cat=222\n-    http://www.example.com/query?cat=222&id=111\n+    ``http://www.example.com/query?id=111&cat=222``,\n+    ``http://www.example.com/query?cat=222&id=111``.\n \n     Even though those are two different URLs both point to the same resource\n     and are equivalent (i.e. they should return the same response).\n \n     Another example are cookies used to store session ids. Suppose the\n     following page is only accessible to authenticated users:\n-\n-    http://www.example.com/members/offers.html\n+    ``http://www.example.com/members/offers.html``.\n \n     Lots of sites use a cookie to store the session id, which adds a random\n     component to the HTTP Request and thus should be ignored when calculating\n\n@@ -61,8 +61,7 @@ def parse_url(url: UrlT, encoding: str | None = None) -> ParseResult:\n \n def escape_ajax(url: str) -> str:\n     \"\"\"\n-    Return the crawlable url according to:\n-    https://developers.google.com/webmasters/ajax-crawling/docs/getting-started\n+    Return the crawlable url\n \n     >>> escape_ajax(\"www.example.com/ajax.html#!key=value\")\n     'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue'\n\n@@ -143,7 +143,7 @@ class RequestTest(unittest.TestCase):\n         # percent-escaping sequences that do not match valid UTF-8 sequences\n         # should be kept untouched (just upper-cased perhaps)\n         #\n-        # See https://tools.ietf.org/html/rfc3987#section-3.2\n+        # See https://datatracker.ietf.org/doc/html/rfc3987#section-3.2\n         #\n         # \"Conversions from URIs to IRIs MUST NOT use any character encoding\n         # other than UTF-8 in steps 3 and 4, even if it might be possible to\n\n@@ -220,9 +220,7 @@ skip_pillow: str | None\n try:\n     from PIL import Image  # noqa: imported just to check for the import error\n except ImportError:\n-    skip_pillow = (\n-        \"Missing Python Imaging Library, install https://pypi.python.org/pypi/Pillow\"\n-    )\n+    skip_pillow = \"Missing Python Imaging Library, install https://pypi.org/pypi/Pillow\"\n else:\n     skip_pillow = None\n \n\n@@ -19,9 +19,7 @@ skip_pillow: str | None\n try:\n     from PIL import Image\n except ImportError:\n-    skip_pillow = (\n-        \"Missing Python Imaging Library, install https://pypi.python.org/pypi/Pillow\"\n-    )\n+    skip_pillow = \"Missing Python Imaging Library, install https://pypi.org/pypi/Pillow\"\n else:\n     encoders = {\"jpeg_encoder\", \"jpeg_decoder\"}\n     if not encoders.issubset(set(Image.core.__dict__)):  # type: ignore[attr-defined]\n\n@@ -21,7 +21,7 @@ try:\n     from PIL import Image  # noqa: imported just to check for the import error\n except ImportError:\n     skip_pillow: str | None = (\n-        \"Missing Python Imaging Library, install https://pypi.python.org/pypi/Pillow\"\n+        \"Missing Python Imaging Library, install https://pypi.org/pypi/Pillow\"\n     )\n else:\n     skip_pillow = None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e7f5ae0b34ef87503884967f8b6c031d3f213c3e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 21 | Churn Cumulative: 630 | Contributors (this commit): 16 | Commits (past 90d): 2 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -55,16 +55,13 @@ class ItemMeta(ABCMeta):\n \n \n class Item(MutableMapping[str, Any], object_ref, metaclass=ItemMeta):\n-    \"\"\"\n-    Base class for scraped items.\n+    \"\"\"Base class for scraped items.\n \n-    In Scrapy, an object is considered an ``item`` if it is an instance of either\n-    :class:`Item` or :class:`dict`, or any subclass. For example, when the output of a\n-    spider callback is evaluated, only instances of :class:`Item` or\n-    :class:`dict` are passed to :ref:`item pipelines <topics-item-pipeline>`.\n-\n-    If you need instances of a custom class to be considered items by Scrapy,\n-    you must inherit from either :class:`Item` or :class:`dict`.\n+    In Scrapy, an object is considered an ``item`` if it's supported by the\n+    `itemadapter`_ library. For example, when the output of a spider callback\n+    is evaluated, only such objects are passed to :ref:`item pipelines\n+    <topics-item-pipeline>`. :class:`Item` is one of the classes supported by\n+    `itemadapter`_ by default.\n \n     Items must declare :class:`Field` attributes, which are processed and stored\n     in the ``fields`` attribute. This restricts the set of allowed field names\n@@ -75,8 +72,14 @@ class Item(MutableMapping[str, Any], object_ref, metaclass=ItemMeta):\n \n     Unlike instances of :class:`dict`, instances of :class:`Item` may be\n     :ref:`tracked <topics-leaks-trackrefs>` to debug memory leaks.\n+\n+    .. _itemadapter: https://github.com/scrapy/itemadapter\n     \"\"\"\n \n+    #: A dictionary containing *all declared fields* for this Item, not only\n+    #: those populated. The keys are the field names and the values are the\n+    #: :class:`Field` objects used in the :ref:`Item declaration\n+    #: <topics-items-declaring>`.\n     fields: dict[str, Field]\n \n     def __init__(self, *args: Any, **kwargs: Any):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d2156696c45e023479ae1bdee8623bb6212e975c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 3 | Files Changed: 7 | Hunks: 18 | Methods Changed: 8 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 48 | Churn Cumulative: 6568 | Contributors (this commit): 70 | Commits (past 90d): 21 | Contributors (cumulative): 166 | DMM Complexity: 0.84375\n\nDIFF:\n@@ -111,7 +111,7 @@ def md5sum(file: IO[bytes]) -> str:\n     \"\"\"\n     warnings.warn(\n         (\n-            \"The scrapy.utils.misc.md5sum function is deprecated, and will be \"\n+            \"The scrapy.utils.misc.md5sum function is deprecated and will be \"\n             \"removed in a future version of Scrapy.\"\n         ),\n         ScrapyDeprecationWarning,\n\n@@ -8,12 +8,14 @@ import gc\n import inspect\n import re\n import sys\n+import warnings\n import weakref\n from collections.abc import AsyncIterable, Iterable, Mapping\n from functools import partial, wraps\n from itertools import chain\n from typing import TYPE_CHECKING, Any, TypeVar, overload\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.asyncgen import as_async_generator\n \n if TYPE_CHECKING:\n@@ -47,6 +49,11 @@ def flatten(x: Iterable[Any]) -> list[Any]:\n     >>> flatten([\"foo\", [\"baz\", 42], \"bar\"])\n     ['foo', 'baz', 42, 'bar']\n     \"\"\"\n+    warnings.warn(\n+        \"The flatten function is deprecated and will be removed in a future version of Scrapy.\",\n+        category=ScrapyDeprecationWarning,\n+        stacklevel=2,\n+    )\n     return list(iflatten(x))\n \n \n@@ -54,6 +61,11 @@ def iflatten(x: Iterable[Any]) -> Iterable[Any]:\n     \"\"\"iflatten(sequence) -> iterator\n \n     Similar to ``.flatten()``, but returns iterator instead\"\"\"\n+    warnings.warn(\n+        \"The iflatten function is deprecated and will be removed in a future version of Scrapy.\",\n+        category=ScrapyDeprecationWarning,\n+        stacklevel=2,\n+    )\n     for el in x:\n         if is_listlike(el):\n             yield from iflatten(el)\n@@ -272,6 +284,11 @@ def equal_attributes(\n     obj1: Any, obj2: Any, attributes: list[str | Callable[[Any], Any]] | None\n ) -> bool:\n     \"\"\"Compare two objects attributes\"\"\"\n+    warnings.warn(\n+        \"The equal_attributes function is deprecated and will be removed in a future version of Scrapy.\",\n+        category=ScrapyDeprecationWarning,\n+        stacklevel=2,\n+    )\n     # not attributes given return False by default\n     if not attributes:\n         return False\n\n@@ -130,7 +130,7 @@ class RequestFingerprinter:\n         if implementation != \"SENTINEL\":\n             message = (\n                 \"'REQUEST_FINGERPRINTER_IMPLEMENTATION' is a deprecated setting.\\n\"\n-                \"And it will be removed in future version of Scrapy.\"\n+                \"It will be removed in a future version of Scrapy.\"\n             )\n             warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)\n         self._fingerprint = fingerprint\n@@ -147,6 +147,11 @@ def request_authenticate(\n     \"\"\"Authenticate the given request (in place) using the HTTP basic access\n     authentication mechanism (RFC 2617) and the given username and password\n     \"\"\"\n+    warnings.warn(\n+        \"The request_authenticate function is deprecated and will be removed in a future version of Scrapy.\",\n+        category=ScrapyDeprecationWarning,\n+        stacklevel=2,\n+    )\n     request.headers[\"Authorization\"] = basic_auth_header(username, password)\n \n \n\n@@ -1,11 +1,13 @@\n import datetime\n import decimal\n import json\n+import warnings\n from typing import Any\n \n from itemadapter import ItemAdapter, is_item\n from twisted.internet import defer\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Request, Response\n \n \n@@ -36,4 +38,10 @@ class ScrapyJSONEncoder(json.JSONEncoder):\n \n \n class ScrapyJSONDecoder(json.JSONDecoder):\n-    pass\n+    def __init__(self, *args, **kwargs):\n+        warnings.warn(\n+            \"The ScrapyJSONDecoder class is deprecated and will be removed in a future version of Scrapy.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        super().__init__(*args, **kwargs)\n\n@@ -6,6 +6,7 @@ from __future__ import annotations\n \n import asyncio\n import os\n+import warnings\n from importlib import import_module\n from pathlib import Path\n from posixpath import split\n@@ -16,6 +17,7 @@ from twisted.trial.unittest import SkipTest\n \n from scrapy import Spider\n from scrapy.crawler import Crawler\n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.boto import is_botocore_available\n \n if TYPE_CHECKING:\n@@ -125,6 +127,11 @@ def assert_samelines(\n     \"\"\"Asserts text1 and text2 have the same lines, ignoring differences in\n     line endings between platforms\n     \"\"\"\n+    warnings.warn(\n+        \"The assert_samelines function is deprecated and will be removed in a future version of Scrapy.\",\n+        category=ScrapyDeprecationWarning,\n+        stacklevel=2,\n+    )\n     testcase.assertEqual(text1.splitlines(), text2.splitlines(), msg)\n \n \n\n@@ -3,6 +3,7 @@ import operator\n import platform\n import sys\n \n+import pytest\n from twisted.trial import unittest\n \n from scrapy.utils.asyncgen import as_async_generator, collect_asyncgen\n@@ -151,6 +152,7 @@ class BinaryIsTextTest(unittest.TestCase):\n \n \n class UtilsPythonTestCase(unittest.TestCase):\n+    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n     def test_equal_attributes(self):\n         class Obj:\n             pass\n\n@@ -6,6 +6,8 @@ import warnings\n from hashlib import sha1\n from weakref import WeakKeyDictionary\n \n+import pytest\n+\n from scrapy.http import Request\n from scrapy.utils.python import to_bytes\n from scrapy.utils.request import (\n@@ -19,6 +21,7 @@ from scrapy.utils.test import get_crawler\n \n \n class UtilsRequestTest(unittest.TestCase):\n+    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n     def test_request_authenticate(self):\n         r = Request(\"http://www.example.com\")\n         request_authenticate(r, \"someuser\", \"somepass\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f57fc454beb4d7746002bb69457cf8add6cc3bcb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 22 | Files Changed: 5 | Hunks: 12 | Methods Changed: 9 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 42 | Churn Cumulative: 1915 | Contributors (this commit): 21 | Commits (past 90d): 9 | Contributors (cumulative): 35 | DMM Complexity: 0.0\n\nDIFF:\n@@ -36,13 +36,10 @@ class Slot:\n         concurrency: int,\n         delay: float,\n         randomize_delay: bool,\n-        *,\n-        throttle: bool | None = None,\n     ):\n         self.concurrency: int = concurrency\n         self.delay: float = delay\n         self.randomize_delay: bool = randomize_delay\n-        self.throttle = throttle\n \n         self.active: set[Request] = set()\n         self.queue: deque[tuple[Request, Deferred[Response]]] = deque()\n@@ -67,15 +64,13 @@ class Slot:\n         return (\n             f\"{cls_name}(concurrency={self.concurrency!r}, \"\n             f\"delay={self.delay:.2f}, \"\n-            f\"randomize_delay={self.randomize_delay!r}, \"\n-            f\"throttle={self.throttle!r})\"\n+            f\"randomize_delay={self.randomize_delay!r})\"\n         )\n \n     def __str__(self) -> str:\n         return (\n             f\"<downloader.Slot concurrency={self.concurrency!r} \"\n             f\"delay={self.delay:.2f} randomize_delay={self.randomize_delay!r} \"\n-            f\"throttle={self.throttle!r} \"\n             f\"len(active)={len(self.active)} len(queue)={len(self.queue)} \"\n             f\"len(transferring)={len(self.transferring)} \"\n             f\"lastseen={datetime.fromtimestamp(self.lastseen).isoformat()}>\"\n@@ -146,8 +141,7 @@ class Downloader:\n                 slot_settings.get(\"delay\", delay),\n             )\n             randomize_delay = slot_settings.get(\"randomize_delay\", self.randomize_delay)\n-            throttle = slot_settings.get(\"throttle\", None)\n-            new_slot = Slot(conc, delay, randomize_delay, throttle=throttle)\n+            new_slot = Slot(conc, delay, randomize_delay)\n             self.slots[key] = new_slot\n \n         return key, self.slots[key]\n\n@@ -64,7 +64,11 @@ class AutoThrottle:\n     ) -> None:\n         key, slot = self._get_slot(request, spider)\n         latency = request.meta.get(\"download_latency\")\n-        if latency is None or slot is None or slot.throttle is False:\n+        if (\n+            latency is None\n+            or slot is None\n+            or request.meta.get(\"dont_throttle\", False) is True\n+        ):\n             return\n \n         olddelay = slot.delay\n\n@@ -8,5 +8,5 @@ class SlotTest(unittest.TestCase):\n         slot = Slot(concurrency=8, delay=0.1, randomize_delay=True)\n         self.assertEqual(\n             repr(slot),\n-            \"Slot(concurrency=8, delay=0.10, randomize_delay=True, throttle=None)\",\n+            \"Slot(concurrency=8, delay=0.10, randomize_delay=True)\",\n         )\n\n@@ -80,7 +80,6 @@ def test_params():\n         \"concurrency\": 1,\n         \"delay\": 2,\n         \"randomize_delay\": False,\n-        \"throttle\": False,\n     }\n     settings = {\n         \"DOWNLOAD_SLOTS\": {\n\n@@ -157,17 +157,20 @@ def test_startdelay_definition(min_spider, min_setting, start_setting, expected)\n \n \n @pytest.mark.parametrize(\n-    (\"meta\", \"slot\", \"throttle\"),\n+    (\"meta\", \"slot\"),\n     (\n-        ({}, None, None),\n-        ({\"download_latency\": 1.0}, None, None),\n-        ({\"download_slot\": \"foo\"}, None, None),\n-        ({\"download_slot\": \"foo\"}, \"foo\", None),\n-        ({\"download_latency\": 1.0, \"download_slot\": \"foo\"}, None, None),\n-        ({\"download_latency\": 1.0, \"download_slot\": \"foo\"}, \"foo\", False),\n+        ({}, None),\n+        ({\"download_latency\": 1.0}, None),\n+        ({\"download_slot\": \"foo\"}, None),\n+        ({\"download_slot\": \"foo\"}, \"foo\"),\n+        ({\"download_latency\": 1.0, \"download_slot\": \"foo\"}, None),\n+        (\n+            {\"download_latency\": 1.0, \"download_slot\": \"foo\", \"dont_throttle\": True},\n+            \"foo\",\n+        ),\n     ),\n )\n-def test_skipped(meta, slot, throttle):\n+def test_skipped(meta, slot):\n     crawler = get_crawler()\n     at = build_from_crawler(AutoThrottle, crawler)\n     spider = TestSpider()\n@@ -178,9 +181,7 @@ def test_skipped(meta, slot, throttle):\n     crawler.engine.downloader = Mock()\n     crawler.engine.downloader.slots = {}\n     if slot is not None:\n-        _slot = Mock()\n-        _slot.throttle = throttle\n-        crawler.engine.downloader.slots[slot] = _slot\n+        crawler.engine.downloader.slots[slot] = object()\n     at._adjust_delay = None  # Raise exception if called.\n \n     at._response_downloaded(None, request, spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2a4b7fe0f8b2e1ce8c43998aad503f2b0b68495b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 549 | Contributors (this commit): 9 | Commits (past 90d): 6 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -67,7 +67,7 @@ class AutoThrottle:\n         if (\n             latency is None\n             or slot is None\n-            or request.meta.get(\"dont_throttle\", False) is True\n+            or request.meta.get(\"autothrottle_dont_adjust_delay\", False) is True\n         ):\n             return\n \n\n@@ -165,7 +165,11 @@ def test_startdelay_definition(min_spider, min_setting, start_setting, expected)\n         ({\"download_slot\": \"foo\"}, \"foo\"),\n         ({\"download_latency\": 1.0, \"download_slot\": \"foo\"}, None),\n         (\n-            {\"download_latency\": 1.0, \"download_slot\": \"foo\", \"dont_throttle\": True},\n+            {\n+                \"download_latency\": 1.0,\n+                \"download_slot\": \"foo\",\n+                \"autothrottle_dont_adjust_delay\": True,\n+            },\n             \"foo\",\n         ),\n     ),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b042ad255db139adc740cd97047b6607889f9f1c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 47 | Lines Deleted: 46 | Files Changed: 39 | Hunks: 51 | Methods Changed: 63 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 93 | Churn Cumulative: 39651 | Contributors (this commit): 202 | Commits (past 90d): 121 | Contributors (cumulative): 765 | DMM Complexity: 1.0\n\nDIFF:\n@@ -57,7 +57,7 @@ def pytest_addoption(parser):\n def reactor_pytest(request):\n     if not request.cls:\n         # doctests\n-        return\n+        return None\n     request.cls.reactor_pytest = request.config.getoption(\"--reactor\")\n     return request.cls.reactor_pytest\n \n\n@@ -23,7 +23,7 @@ class Command(BaseRunSpiderCommand):\n     def run(self, args: list[str], opts: argparse.Namespace) -> None:\n         if len(args) < 1:\n             raise UsageError()\n-        elif len(args) > 1:\n+        if len(args) > 1:\n             raise UsageError(\n                 \"running 'scrapy crawl' with more than one spider is not supported\"\n             )\n\n@@ -35,7 +35,8 @@ class Command(ScrapyCommand):\n         try:\n             spidercls = self.crawler_process.spider_loader.load(args[0])\n         except KeyError:\n-            return self._err(f\"Spider not found: {args[0]}\")\n+            self._err(f\"Spider not found: {args[0]}\")\n+            return\n \n         sfile = sys.modules[spidercls.__module__].__file__\n         assert sfile\n\n@@ -399,7 +399,6 @@ class Command(BaseRunSpiderCommand):\n         # parse arguments\n         if not len(args) == 1 or not is_url(args[0]):\n             raise UsageError()\n-        else:\n         url = args[0]\n \n         # prepare spidercls\n\n@@ -38,7 +38,9 @@ class Contract:\n             assert cb is not None\n \n             @wraps(cb)\n-            def wrapper(response: Response, **cb_kwargs: Any) -> list[Any]:\n+            def wrapper(  # pylint: disable=inconsistent-return-statements\n+                response: Response, **cb_kwargs: Any\n+            ) -> list[Any]:\n                 try:\n                     results.startTest(self.testcase_pre)\n                     self.pre_process(response)\n@@ -67,7 +69,9 @@ class Contract:\n             assert cb is not None\n \n             @wraps(cb)\n-            def wrapper(response: Response, **cb_kwargs: Any) -> list[Any]:\n+            def wrapper(  # pylint: disable=inconsistent-return-statements\n+                response: Response, **cb_kwargs: Any\n+            ) -> list[Any]:\n                 cb_result = cb(response, **cb_kwargs)\n                 if isinstance(cb_result, (AsyncGenerator, CoroutineType)):\n                     raise TypeError(\"Contracts don't support async callbacks\")\n\n@@ -92,7 +92,6 @@ class DownloadHandlers:\n             )\n             self._notconfigured[scheme] = str(ex)\n             return None\n-        else:\n         self._handlers[scheme] = dh\n         return dh\n \n\n@@ -70,7 +70,10 @@ class ReceivedDataProtocol(Protocol):\n         return self.__filename\n \n     def close(self) -> None:\n-        self.body.close() if self.filename else self.body.seek(0)\n+        if self.filename:\n+            self.body.close()\n+        else:\n+            self.body.seek(0)\n \n \n _CODE_RE = re.compile(r\"\\d+\")\n\n@@ -73,7 +73,7 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n         ) -> Generator[Deferred[Any], Any, Response | Request]:\n             if response is None:\n                 raise TypeError(\"Received None in process_response\")\n-            elif isinstance(response, Request):\n+            if isinstance(response, Request):\n                 return response\n \n             for method in self.methods[\"process_response\"]:\n\n@@ -172,7 +172,7 @@ class ExecutionEngine:\n         assert self.spider is not None  # typing\n \n         if self.paused:\n-            return None\n+            return\n \n         while (\n             not self._needs_backout()\n@@ -418,7 +418,7 @@ class ExecutionEngine:\n             if isinstance(x, Failure) and isinstance(x.value, ex)\n         }\n         if DontCloseSpider in detected_ex:\n-            return None\n+            return\n         if self.spider_is_idle():\n             ex = detected_ex.get(CloseSpider, CloseSpider(reason=\"finished\"))\n             assert isinstance(ex, CloseSpider)  # typing\n\n@@ -312,7 +312,6 @@ class Scheduler(BaseScheduler):\n             assert self.stats is not None\n             self.stats.inc_value(\"scheduler/unserializable\", spider=self.spider)\n             return False\n-        else:\n         return True\n \n     def _mqpush(self, request: Request) -> None:\n\n@@ -174,9 +174,8 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                 # _process_spider_exception too, which complicates the architecture\n                 msg = f\"Async iterable returned from {method.__qualname__} cannot be downgraded\"\n                 raise _InvalidOutput(msg)\n-            elif result is None:\n+            if result is None:\n                 continue\n-            else:\n             msg = (\n                 f\"{method.__qualname__} must return None \"\n                 f\"or an iterable, got {type(result)}\"\n\n@@ -105,7 +105,7 @@ class ItemFilter:\n class IFeedStorage(Interface):\n     \"\"\"Interface that all Feed Storages must implement\"\"\"\n \n-    def __init__(uri, *, feed_options=None):\n+    def __init__(uri, *, feed_options=None):  # pylint: disable=super-init-not-called\n         \"\"\"Initialize the storage with the parameters given in the URI and the\n         feed-specific options (see :setting:`FEEDS`)\"\"\"\n \n\n@@ -152,7 +152,6 @@ def _get_form(\n         form = forms[formnumber]\n     except IndexError:\n         raise IndexError(f\"Form number {formnumber} not found in {response}\")\n-    else:\n     return cast(FormElement, form)\n \n \n@@ -264,5 +263,4 @@ def _get_clickable(\n             f\"Multiple elements found ({el!r}) matching the \"\n             f\"criteria in clickdata: {clickdata!r}\"\n         )\n-    else:\n     raise ValueError(f\"No clickable element matching clickdata: {clickdata!r}\")\n\n@@ -117,8 +117,8 @@ class MailSender:\n             if charset:\n                 msg.set_charset(charset)\n             msg.attach(MIMEText(body, \"plain\", charset or \"us-ascii\"))\n-            for attach_name, mimetype, f in attachs:\n-                part = MIMEBase(*mimetype.split(\"/\"))\n+            for attach_name, attach_mimetype, f in attachs:\n+                part = MIMEBase(*attach_mimetype.split(\"/\"))\n                 part.set_payload(f.read())\n                 Encoders.encode_base64(part)\n                 part.add_header(\n\n@@ -265,7 +265,6 @@ class S3FilesStore:\n                 kwarg = mapping[key]\n             except KeyError:\n                 raise TypeError(f'Header \"{key}\" is not supported by botocore')\n-            else:\n             extra[kwarg] = value\n         return extra\n \n\n@@ -141,7 +141,6 @@ class CachingHostnameResolver:\n                 addressTypes,\n                 transportSemantics,\n             )\n-        else:\n         resolutionReceiver.resolutionBegan(HostResolution(hostName))\n         for addr in addresses:\n             resolutionReceiver.addressResolved(addr)\n\n@@ -501,9 +501,7 @@ class BaseSettings(MutableMapping[_SettingsKeyT, Any]):\n         except KeyError:\n             if default is self.__default:\n                 raise\n-\n             return default\n-        else:\n         self.__delitem__(name)\n         return value\n \n\n@@ -325,7 +325,6 @@ def _load_policy_class(\n         msg = f\"Could not load referrer policy {policy!r}\"\n         if not warning_only:\n             raise RuntimeError(msg)\n-        else:\n         warnings.warn(msg, RuntimeWarning)\n         return None\n \n\n@@ -44,7 +44,6 @@ def build_component_list(\n                         \"convert to the same \"\n                         \"object, please update your settings\"\n                     )\n-                else:\n                 compbs.set(convert(k), v, priority=prio)\n             return compbs\n         _check_components(compdict)\n\n@@ -101,7 +101,7 @@ DEFAULT_PYTHON_SHELLS: KnownShellsT = {\n \n def get_shell_embed_func(\n     shells: Iterable[str] | None = None, known_shells: KnownShellsT | None = None\n-) -> Any:\n+) -> EmbedFuncT | None:\n     \"\"\"Return the first acceptable shell-embed function\n     from a given list of shell names.\n     \"\"\"\n@@ -117,6 +117,7 @@ def get_shell_embed_func(\n                 return known_shells[shell]()\n             except ImportError:\n                 continue\n+    return None\n \n \n def start_python_console(\n\n@@ -109,7 +109,6 @@ def mustbe_deferred(\n         return defer_fail(failure.Failure(e))\n     except Exception:\n         return defer_fail(failure.Failure())\n-    else:\n     return defer_result(result)\n \n \n\n@@ -36,7 +36,6 @@ def _colorize(text: str, colorize: bool = True) -> str:\n         from pygments import highlight\n     except ImportError:\n         return text\n-    else:\n     from pygments.formatters import TerminalFormatter\n     from pygments.lexers import PythonLexer\n \n\n@@ -51,6 +51,7 @@ class TopLevelFormatter(logging.Filter):\n     \"\"\"\n \n     def __init__(self, loggers: list[str] | None = None):\n+        super().__init__()\n         self.loggers: list[str] = loggers or []\n \n     def filter(self, record: logging.LogRecord) -> bool:\n\n@@ -323,7 +323,6 @@ def without_none_values(\n     \"\"\"\n     if isinstance(iterable, Mapping):\n         return {k: v for k, v in iterable.items() if v is not None}\n-    else:\n     # the iterable __init__ must take another iterable\n     return type(iterable)(v for v in iterable if v is not None)  # type: ignore[call-arg]\n \n\n@@ -26,7 +26,7 @@ if TYPE_CHECKING:\n _T = TypeVar(\"_T\")\n \n \n-def listen_tcp(portrange: list[int], host: str, factory: ServerFactory) -> Port:  # type: ignore[return]\n+def listen_tcp(portrange: list[int], host: str, factory: ServerFactory) -> Port:  # type: ignore[return]  # pylint: disable=inconsistent-return-statements\n     \"\"\"Like reactor.listenTCP but tries different ports in a range.\"\"\"\n     from twisted.internet import reactor\n \n\n@@ -66,7 +66,6 @@ def _remove_html_comments(body: bytes) -> bytes:\n         end = body.find(b\"-->\", start + 1)\n         if end == -1:\n             return body[:start]\n-        else:\n         body = body[:start] + body[end + 3 :]\n         start = body.find(b\"<!--\")\n     return body\n\n@@ -66,6 +66,7 @@ def get_oldest(class_name: str) -> Any:\n             if not wdict:\n                 break\n             return min(wdict.items(), key=itemgetter(1))[0]\n+    return None\n \n \n def iter_all(class_name: str) -> Iterable[Any]:\n\n@@ -14,7 +14,7 @@ from urllib.parse import ParseResult, urldefrag, urlparse, urlunparse\n \n # scrapy.utils.url was moved to w3lib.url and import * ensures this\n # move doesn't break old code\n-from w3lib.url import *\n+from w3lib.url import *  # pylint: disable=unused-wildcard-import\n from w3lib.url import _safe_chars, _unquotepath  # noqa: F401\n \n from scrapy.utils.python import to_unicode\n\n@@ -377,11 +377,13 @@ class SingleRequestSpider(MetaSpider):\n             return self.callback_func(response)\n         if \"next\" in response.meta:\n             return response.meta[\"next\"]\n+        return None\n \n     def on_error(self, failure):\n         self.meta[\"failure\"] = failure\n         if callable(self.errback_func):\n             return self.errback_func(failure)\n+        return None\n \n \n class DuplicateStartRequestsSpider(MockServerSpider):\n\n@@ -143,7 +143,8 @@ class CrawlerTestCase(BaseCrawlerTest):\n             def from_crawler(cls, crawler):\n                 return cls(crawler=crawler)\n \n-            def __init__(self, crawler):\n+            def __init__(self, crawler, **kwargs: Any):\n+                super().__init__(**kwargs)\n                 self.crawler = crawler\n \n             def start_requests(self):\n@@ -223,7 +224,8 @@ class CrawlerTestCase(BaseCrawlerTest):\n             def from_crawler(cls, crawler):\n                 return cls(crawler=crawler)\n \n-            def __init__(self, crawler):\n+            def __init__(self, crawler, **kwargs: Any):\n+                super().__init__(**kwargs)\n                 self.crawler = crawler\n \n             def start_requests(self):\n@@ -301,7 +303,8 @@ class CrawlerTestCase(BaseCrawlerTest):\n             def from_crawler(cls, crawler):\n                 return cls(crawler=crawler)\n \n-            def __init__(self, crawler):\n+            def __init__(self, crawler, **kwargs: Any):\n+                super().__init__(**kwargs)\n                 self.crawler = crawler\n \n             def start_requests(self):\n@@ -379,7 +382,8 @@ class CrawlerTestCase(BaseCrawlerTest):\n             def from_crawler(cls, crawler):\n                 return cls(crawler=crawler)\n \n-            def __init__(self, crawler):\n+            def __init__(self, crawler, **kwargs: Any):\n+                super().__init__(**kwargs)\n                 self.crawler = crawler\n \n             def start_requests(self):\n\n@@ -25,7 +25,7 @@ def _cookie_to_set_cookie_value(cookie):\n     for key in (\"name\", \"value\", \"path\", \"domain\"):\n         if cookie.get(key) is None:\n             if key in (\"name\", \"value\"):\n-                return\n+                return None\n             continue\n         if isinstance(cookie[key], (bool, float, int, str)):\n             decoded[key] = str(cookie[key])\n\n@@ -436,8 +436,7 @@ class Base:\n \n             def process_value(value):\n                 m = re.search(r\"javascript:goToPage\\('(.*?)'\", value)\n-                if m:\n-                    return m.group(1)\n+                return m.group(1) if m else None\n \n             lx = self.extractor_cls(process_value=process_value)\n             self.assertEqual(\n\n@@ -69,8 +69,7 @@ class BasicItemLoaderTest(unittest.TestCase):\n     def test_load_item_ignore_none_field_values(self):\n         def validate_sku(value):\n             # Let's assume a SKU is only digits.\n-            if value.isdigit():\n-                return value\n+            return value if value.isdigit() else None\n \n         class MyLoader(ItemLoader):\n             name_out = Compose(lambda vs: vs[0])  # take first which allows empty values\n\n@@ -198,7 +198,6 @@ class DropSomeItemsPipeline:\n         if self.drop:\n             self.drop = False\n             raise DropItem(\"Ignoring item\")\n-        else:\n         self.drop = True\n \n \n\n@@ -627,7 +627,6 @@ class TestGCSFilesStore(unittest.TestCase):\n             import google.cloud.storage  # noqa\n         except ModuleNotFoundError:\n             raise unittest.SkipTest(\"google-cloud-storage is not installed\")\n-        else:\n         with mock.patch(\"google.cloud.storage\") as _:\n             with mock.patch(\"scrapy.pipelines.files.time\") as _:\n                 uri = \"gs://my_bucket/my_prefix/\"\n\n@@ -159,7 +159,7 @@ class RequestSerializationTest(unittest.TestCase):\n \n \n class TestSpiderMixin:\n-    def __mixin_callback(self, response):\n+    def __mixin_callback(self, response):  # pylint: disable=unused-private-member\n         pass\n \n \n@@ -191,7 +191,8 @@ class TestSpider(Spider, TestSpiderMixin):\n     __parse_item_reference = private_parse_item\n     __handle_error_reference = private_handle_error\n \n-    def __init__(self):\n+    def __init__(self, **kwargs):\n+        super().__init__(**kwargs)\n         self.delegated_callback = TestSpiderDelegation().delegated_callback\n \n     def parse_item(self, response):\n@@ -200,5 +201,5 @@ class TestSpider(Spider, TestSpiderMixin):\n     def handle_error(self, failure):\n         pass\n \n-    def __parse_item_private(self, response):\n+    def __parse_item_private(self, response):  # pylint: disable=unused-private-member\n         pass\n\n@@ -686,6 +686,7 @@ class CustomPythonOrgPolicy(ReferrerPolicy):\n             return b\"https://python.org/\"\n         if scheme == \"http\":\n             return b\"http://python.org/\"\n+        return None\n \n \n class TestSettingsCustomPolicy(TestRefererMiddleware):\n\n@@ -158,6 +158,7 @@ class CaseInsensitiveDictMixin:\n             def _normvalue(self, value):\n                 if value is not None:\n                     return value + 1\n+                return None\n \n             normvalue = _normvalue  # deprecated CaselessDict class\n \n\n@@ -182,6 +182,7 @@ class AsyncCooperatorTest(unittest.TestCase):\n             return dfd\n         # simulate trivial sync processing\n         results.append(o)\n+        return None\n \n     @staticmethod\n     def get_async_iterable(length):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eda3a89b3fe3e88ed8b90d032a2f25b92a1b79ca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 86 | Files Changed: 5 | Hunks: 24 | Methods Changed: 10 | Complexity Δ (Sum/Max): -3/4 | Churn Δ: 128 | Churn Cumulative: 3062 | Contributors (this commit): 49 | Commits (past 90d): 13 | Contributors (cumulative): 102 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2,12 +2,13 @@ from __future__ import annotations\n \n import logging\n import pprint\n+import warnings\n from collections import defaultdict, deque\n from typing import TYPE_CHECKING, Any, TypeVar, cast\n \n-from scrapy.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.utils.defer import process_chain, process_parallel\n-from scrapy.utils.misc import build_from_crawler, build_from_settings, load_object\n+from scrapy.utils.misc import build_from_crawler, load_object\n \n if TYPE_CHECKING:\n     from collections.abc import Callable, Iterable\n@@ -20,7 +21,7 @@ if TYPE_CHECKING:\n \n     from scrapy import Spider\n     from scrapy.crawler import Crawler\n-    from scrapy.settings import Settings\n+    from scrapy.settings import BaseSettings, Settings\n \n     _P = ParamSpec(\"_P\")\n \n@@ -50,8 +51,27 @@ class MiddlewareManager:\n     def _get_mwlist_from_settings(cls, settings: Settings) -> list[Any]:\n         raise NotImplementedError\n \n+    @staticmethod\n+    def _build_from_settings(objcls: type[_T], settings: BaseSettings) -> _T:\n+        if hasattr(objcls, \"from_settings\"):\n+            instance = objcls.from_settings(settings)  # type: ignore[attr-defined]\n+            method_name = \"from_settings\"\n+        else:\n+            instance = objcls()\n+            method_name = \"__new__\"\n+        if instance is None:\n+            raise TypeError(f\"{objcls.__qualname__}.{method_name} returned None\")\n+        return cast(_T, instance)\n+\n     @classmethod\n     def from_settings(cls, settings: Settings, crawler: Crawler | None = None) -> Self:\n+        if crawler is None:\n+            warnings.warn(\n+                \"Calling MiddlewareManager.from_settings() without a Crawler instance is deprecated.\"\n+                \" As this method will be deprecated in the future, please switch to from_crawler().\",\n+                category=ScrapyDeprecationWarning,\n+                stacklevel=2,\n+            )\n         mwlist = cls._get_mwlist_from_settings(settings)\n         middlewares = []\n         enabled = []\n@@ -61,7 +81,7 @@ class MiddlewareManager:\n                 if crawler is not None:\n                     mw = build_from_crawler(mwcls, crawler)\n                 else:\n-                    mw = build_from_settings(mwcls, settings)\n+                    mw = MiddlewareManager._build_from_settings(mwcls, settings)\n                 middlewares.append(mw)\n                 enabled.append(clspath)\n             except NotConfigured as e:\n\n@@ -26,7 +26,6 @@ if TYPE_CHECKING:\n \n     from scrapy import Spider\n     from scrapy.crawler import Crawler\n-    from scrapy.settings import BaseSettings\n \n \n _ITERABLE_SINGLE_VALUES = dict, Item, str, bytes\n@@ -150,7 +149,7 @@ def create_instance(objcls, settings, crawler, *args, **kwargs):\n     \"\"\"\n     warnings.warn(\n         \"The create_instance() function is deprecated. \"\n-        \"Please use build_from_crawler() or build_from_settings() instead.\",\n+        \"Please use build_from_crawler() instead.\",\n         category=ScrapyDeprecationWarning,\n         stacklevel=2,\n     )\n@@ -176,7 +175,7 @@ def create_instance(objcls, settings, crawler, *args, **kwargs):\n def build_from_crawler(\n     objcls: type[T], crawler: Crawler, /, *args: Any, **kwargs: Any\n ) -> T:\n-    \"\"\"Construct a class instance using its ``from_crawler`` constructor.\n+    \"\"\"Construct a class instance using its ``from_crawler`` or ``from_settings`` constructor.\n \n     ``*args`` and ``**kwargs`` are forwarded to the constructor.\n \n@@ -196,26 +195,6 @@ def build_from_crawler(\n     return cast(T, instance)\n \n \n-def build_from_settings(\n-    objcls: type[T], settings: BaseSettings, /, *args: Any, **kwargs: Any\n-) -> T:\n-    \"\"\"Construct a class instance using its ``from_settings`` constructor.\n-\n-    ``*args`` and ``**kwargs`` are forwarded to the constructor.\n-\n-    Raises ``TypeError`` if the resulting instance is ``None``.\n-    \"\"\"\n-    if hasattr(objcls, \"from_settings\"):\n-        instance = objcls.from_settings(settings, *args, **kwargs)  # type: ignore[attr-defined]\n-        method_name = \"from_settings\"\n-    else:\n-        instance = objcls(*args, **kwargs)\n-        method_name = \"__new__\"\n-    if instance is None:\n-        raise TypeError(f\"{objcls.__qualname__}.{method_name} returned None\")\n-    return cast(T, instance)\n-\n-\n @contextmanager\n def set_environ(**kwargs: str) -> Iterator[None]:\n     \"\"\"Temporarily set environment variables inside the context manager and\n\n@@ -2,7 +2,7 @@ from twisted.trial import unittest\n \n from scrapy.exceptions import NotConfigured\n from scrapy.middleware import MiddlewareManager\n-from scrapy.settings import Settings\n+from scrapy.utils.test import get_crawler\n \n \n class M1:\n@@ -23,8 +23,6 @@ class M2:\n     def close_spider(self, spider):\n         pass\n \n-    pass\n-\n \n class M3:\n     def process(self, response, request, spider):\n@@ -83,7 +81,7 @@ class MiddlewareManagerTest(unittest.TestCase):\n         self.assertEqual(mwman.middlewares, (m1, m2, m3))\n \n     def test_enabled_from_settings(self):\n-        settings = Settings()\n-        mwman = TestMiddlewareManager.from_settings(settings)\n+        crawler = get_crawler()\n+        mwman = TestMiddlewareManager.from_crawler(crawler)\n         classes = [x.__class__ for x in mwman.middlewares]\n         self.assertEqual(classes, [M1, M3])\n\n@@ -10,7 +10,6 @@ from scrapy.item import Field, Item\n from scrapy.utils.misc import (\n     arg_to_iter,\n     build_from_crawler,\n-    build_from_settings,\n     create_instance,\n     load_object,\n     rel_has_nofollow,\n@@ -197,39 +196,6 @@ class UtilsMiscTestCase(unittest.TestCase):\n         with self.assertRaises(TypeError):\n             build_from_crawler(m, crawler, *args, **kwargs)\n \n-    def test_build_from_settings(self):\n-        settings = mock.MagicMock()\n-        args = (True, 100.0)\n-        kwargs = {\"key\": \"val\"}\n-\n-        def _test_with_settings(mock, settings):\n-            build_from_settings(mock, settings, *args, **kwargs)\n-            if hasattr(mock, \"from_settings\"):\n-                mock.from_settings.assert_called_once_with(settings, *args, **kwargs)\n-                self.assertEqual(mock.call_count, 0)\n-            else:\n-                mock.assert_called_once_with(*args, **kwargs)\n-\n-        # Check usage of correct constructor using three mocks:\n-        #   1. with no alternative constructors\n-        #   2. with from_settings() constructor\n-        #   3. with from_settings() and from_crawler() constructor\n-        spec_sets = (\n-            [\"__qualname__\"],\n-            [\"__qualname__\", \"from_settings\"],\n-            [\"__qualname__\", \"from_settings\", \"from_crawler\"],\n-        )\n-        for specs in spec_sets:\n-            m = mock.MagicMock(spec_set=specs)\n-            _test_with_settings(m, settings)\n-            m.reset_mock()\n-\n-        # Check adoption of crawler settings\n-        m = mock.MagicMock(spec_set=[\"__qualname__\", \"from_settings\"])\n-        m.from_settings.return_value = None\n-        with self.assertRaises(TypeError):\n-            build_from_settings(m, settings, *args, **kwargs)\n-\n     def test_set_environ(self):\n         assert os.environ.get(\"some_test_environ\") is None\n         with set_environ(some_test_environ=\"test_value\"):\n\n@@ -9,25 +9,18 @@ from tempfile import mkdtemp\n \n import OpenSSL.SSL\n from twisted.internet import defer, reactor\n+from twisted.internet.defer import inlineCallbacks\n+from twisted.internet.testing import StringTransport\n+from twisted.protocols.policies import WrappingFactory\n from twisted.trial import unittest\n from twisted.web import resource, server, static, util\n \n-try:\n-    from twisted.internet.testing import StringTransport\n-except ImportError:\n-    # deprecated in Twisted 19.7.0\n-    # (remove once we bump our requirement past that version)\n-    from twisted.test.proto_helpers import StringTransport\n-\n-from twisted.internet.defer import inlineCallbacks\n-from twisted.protocols.policies import WrappingFactory\n-\n from scrapy.core.downloader import webclient as client\n from scrapy.core.downloader.contextfactory import ScrapyClientContextFactory\n from scrapy.http import Headers, Request\n-from scrapy.settings import Settings\n-from scrapy.utils.misc import build_from_settings\n+from scrapy.utils.misc import build_from_crawler\n from scrapy.utils.python import to_bytes, to_unicode\n+from scrapy.utils.test import get_crawler\n from tests.mockserver import (\n     BrokenDownloadResource,\n     ErrorResource,\n@@ -469,22 +462,22 @@ class WebClientCustomCiphersSSLTestCase(WebClientSSLTestCase):\n \n     def testPayload(self):\n         s = \"0123456789\" * 10\n-        settings = Settings({\"DOWNLOADER_CLIENT_TLS_CIPHERS\": self.custom_ciphers})\n-        client_context_factory = build_from_settings(\n-            ScrapyClientContextFactory, settings\n+        crawler = get_crawler(\n+            settings_dict={\"DOWNLOADER_CLIENT_TLS_CIPHERS\": self.custom_ciphers}\n         )\n+        client_context_factory = build_from_crawler(ScrapyClientContextFactory, crawler)\n         return getPage(\n             self.getURL(\"payload\"), body=s, contextFactory=client_context_factory\n         ).addCallback(self.assertEqual, to_bytes(s))\n \n     def testPayloadDisabledCipher(self):\n         s = \"0123456789\" * 10\n-        settings = Settings(\n-            {\"DOWNLOADER_CLIENT_TLS_CIPHERS\": \"ECDHE-RSA-AES256-GCM-SHA384\"}\n-        )\n-        client_context_factory = build_from_settings(\n-            ScrapyClientContextFactory, settings\n+        crawler = get_crawler(\n+            settings_dict={\n+                \"DOWNLOADER_CLIENT_TLS_CIPHERS\": \"ECDHE-RSA-AES256-GCM-SHA384\"\n+            }\n         )\n+        client_context_factory = build_from_crawler(ScrapyClientContextFactory, crawler)\n         d = getPage(\n             self.getURL(\"payload\"), body=s, contextFactory=client_context_factory\n         )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#83d4939d41ab8790587f721755a74f883cc04e31", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 38 | Files Changed: 1 | Hunks: 8 | Methods Changed: 6 | Complexity Δ (Sum/Max): -6/0 | Churn Δ: 50 | Churn Cumulative: 2480 | Contributors (this commit): 45 | Commits (past 90d): 6 | Contributors (cumulative): 45 | DMM Complexity: 0.0\n\nDIFF:\n@@ -62,6 +62,11 @@ def build_storage(\n     preargs: Iterable[Any] = (),\n     **kwargs: Any,\n ) -> _StorageT:\n+    warnings.warn(\n+        \"scrapy.extensions.feedexport.build_storage() is deprecated, call the builder directly.\",\n+        category=ScrapyDeprecationWarning,\n+        stacklevel=2,\n+    )\n     kwargs[\"feed_options\"] = feed_options\n     return builder(*preargs, uri, *args, **kwargs)\n \n@@ -248,8 +253,7 @@ class S3FeedStorage(BlockingFeedStorage):\n         *,\n         feed_options: dict[str, Any] | None = None,\n     ) -> Self:\n-        return build_storage(\n-            cls,\n+        return cls(\n             uri,\n             access_key=crawler.settings[\"AWS_ACCESS_KEY_ID\"],\n             secret_key=crawler.settings[\"AWS_SECRET_ACCESS_KEY\"],\n@@ -323,10 +327,9 @@ class FTPFeedStorage(BlockingFeedStorage):\n         *,\n         feed_options: dict[str, Any] | None = None,\n     ) -> Self:\n-        return build_storage(\n-            cls,\n+        return cls(\n             uri,\n-            crawler.settings.getbool(\"FEED_STORAGE_FTP_ACTIVE\"),\n+            use_active_mode=crawler.settings.getbool(\"FEED_STORAGE_FTP_ACTIVE\"),\n             feed_options=feed_options,\n         )\n \n@@ -407,15 +410,12 @@ class FeedSlot:\n             self.exporter.start_exporting()\n             self._exporting = True\n \n-    def _get_instance(\n-        self, objcls: type[BaseItemExporter], *args: Any, **kwargs: Any\n-    ) -> BaseItemExporter:\n-        return build_from_crawler(objcls, self.crawler, *args, **kwargs)\n-\n     def _get_exporter(\n         self, file: IO[bytes], format: str, *args: Any, **kwargs: Any\n     ) -> BaseItemExporter:\n-        return self._get_instance(self.exporters[format], file, *args, **kwargs)\n+        return build_from_crawler(\n+            self.exporters[format], self.crawler, file, *args, **kwargs\n+        )\n \n     def finish_exporting(self) -> None:\n         if self._exporting:\n@@ -692,34 +692,8 @@ class FeedExporter:\n     def _get_storage(\n         self, uri: str, feed_options: dict[str, Any]\n     ) -> FeedStorageProtocol:\n-        \"\"\"Fork of create_instance specific to feed storage classes\n-\n-        It supports not passing the *feed_options* parameters to classes that\n-        do not support it, and issuing a deprecation warning instead.\n-        \"\"\"\n         feedcls = self.storages.get(urlparse(uri).scheme, self.storages[\"file\"])\n-        crawler = getattr(self, \"crawler\", None)\n-\n-        def build_instance(\n-            builder: type[FeedStorageProtocol], *preargs: Any\n-        ) -> FeedStorageProtocol:\n-            return build_storage(\n-                builder, uri, feed_options=feed_options, preargs=preargs\n-            )\n-\n-        instance: FeedStorageProtocol\n-        if crawler and hasattr(feedcls, \"from_crawler\"):\n-            instance = build_instance(feedcls.from_crawler, crawler)\n-            method_name = \"from_crawler\"\n-        elif hasattr(feedcls, \"from_settings\"):\n-            instance = build_instance(feedcls.from_settings, self.settings)\n-            method_name = \"from_settings\"\n-        else:\n-            instance = build_instance(feedcls)\n-            method_name = \"__new__\"\n-        if instance is None:\n-            raise TypeError(f\"{feedcls.__qualname__}.{method_name} returned None\")\n-        return instance\n+        return build_from_crawler(feedcls, self.crawler, uri, feed_options=feed_options)\n \n     def _get_uri_params(\n         self,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
