{"custom_id": "keras#3b92b678f7e1bd86eac29e8c9f0cbe782d147c68", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 196 | Contributors (this commit): 7 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -583,7 +583,7 @@ def batch_normalization(\n         offset = jnp.reshape(offset, shape)\n         res = res + offset\n \n-    return x * inv + res\n+    return jnp.add(x * inv, res)\n \n \n def ctc_loss(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4c67dcf3755ebd33c472199089255701b6f699e1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1549 | Lines Deleted: 248 | Files Changed: 21 | Hunks: 145 | Methods Changed: 133 | Complexity Δ (Sum/Max): 140/26 | Churn Δ: 1797 | Churn Cumulative: 12742 | Contributors (this commit): 34 | Commits (past 90d): 154 | Contributors (cumulative): 119 | DMM Complexity: 0.7534246575342466\n\nDIFF:\n@@ -115,6 +115,10 @@ class KerasVariable:\n         self._trainable = trainable\n         self._autocast = autocast\n         self._aggregation = aggregation\n+        # `self._overwrite_with_gradient` is an internal property to determine\n+        # whether this variable should be overwritten by the computed gradient.\n+        # Ref: https://github.com/google/flax/blob/main/flax/linen/fp8_ops.py\n+        self._overwrite_with_gradient = False\n         if isinstance(initializer, str):\n             from keras import initializers\n \n@@ -266,6 +270,28 @@ class KerasVariable:\n     def trainable(self, value):\n         self._trainable = value\n \n+    @property\n+    def overwrite_with_gradient(self):\n+        \"\"\"Whether this variable should be overwritten by the gradient.\n+\n+        This property is designed for a special case where we want to overwrite\n+        the variable directly with its computed gradient. For example, in float8\n+        training, new `scale` and `amax_history` are computed as gradients, and\n+        we want to overwrite them directly instead of following the typical\n+        procedure such as gradient descent with a learning rate, gradient\n+        clipping and weight decaying.\n+        \"\"\"\n+        return self._overwrite_with_gradient\n+\n+    @overwrite_with_gradient.setter\n+    def overwrite_with_gradient(self, value):\n+        if not isinstance(value, bool):\n+            raise TypeError(\n+                \"`overwrite_with_gradient` must be a boolean. \"\n+                f\"Received: {value}\"\n+            )\n+        self._overwrite_with_gradient = value\n+\n     @property\n     def regularizer(self):\n         return self._regularizer\n\n@@ -239,6 +239,18 @@ class VariablePropertiesTest(test_case.TestCase, parameterized.TestCase):\n         v = backend.Variable(initializer=np.ones((2, 2)), name=\"test_var\")\n         self.assertEqual(v.path, \"test_var\")\n \n+    def test_overwrite_with_gradient_setter(self):\n+        v = backend.Variable(\n+            initializer=initializers.RandomNormal(),\n+            shape=(2, 2),\n+        )\n+        self.assertFalse(v.overwrite_with_gradient)\n+        v.overwrite_with_gradient = True\n+        self.assertTrue(v.overwrite_with_gradient)\n+\n+        with self.assertRaisesRegex(TypeError, \"must be a boolean.\"):\n+            v.overwrite_with_gradient = \"true\"\n+\n \n class VariableNumpyValueAndAssignmentTest(test_case.TestCase):\n     \"\"\"tests for KerasVariable.numpy(), KerasVariable.value()\n\n@@ -1,10 +1,15 @@\n from keras import backend\n from keras.dtype_policies import dtype_policy\n+from keras.dtype_policies.dtype_policy import QUANTIZATION_MODES\n from keras.dtype_policies.dtype_policy import FloatDTypePolicy\n from keras.dtype_policies.dtype_policy import QuantizedDTypePolicy\n+from keras.dtype_policies.dtype_policy import QuantizedFloat8DTypePolicy\n \n \n def get(identifier):\n+    from keras.dtype_policies.dtype_policy import (\n+        _get_quantized_dtype_policy_by_str,\n+    )\n     from keras.saving import serialization_lib\n \n     if identifier is None:\n@@ -14,8 +19,8 @@ def get(identifier):\n     if isinstance(identifier, dict):\n         return serialization_lib.deserialize_keras_object(identifier)\n     if isinstance(identifier, str):\n-        if identifier.startswith(\"int8\"):\n-            return QuantizedDTypePolicy(identifier)\n+        if identifier.startswith(QUANTIZATION_MODES):\n+            return _get_quantized_dtype_policy_by_str(identifier)\n         else:\n             return FloatDTypePolicy(identifier)\n     try:\n\n@@ -3,6 +3,8 @@ from keras import ops\n from keras.api_export import keras_export\n from keras.backend.common import global_state\n \n+QUANTIZATION_MODES = (\"int8\", \"float8\")\n+\n \n @keras_export(\n     [\n@@ -55,7 +57,7 @@ class DTypePolicy:\n     to explicitly construct a `DTypePolicy` object.\n     \"\"\"\n \n-    def __new__(cls, name):\n+    def __new__(cls, name, *args, **kwargs):\n         if not isinstance(name, str):\n             raise TypeError(\n                 \"'name' must be a string, such as 'mixed_float16'. \"\n@@ -64,8 +66,8 @@ class DTypePolicy:\n         # For backwards compatibility\n         # TODO: We should consider deprecating this behavior\n         if cls is __class__:\n-            if name.startswith(\"int8\"):\n-                return QuantizedDTypePolicy(name)\n+            if name.startswith(QUANTIZATION_MODES):\n+                return _get_quantized_dtype_policy_by_str(name)\n             return FloatDTypePolicy(name)\n         return super().__new__(cls)\n \n@@ -204,23 +206,23 @@ class QuantizedDTypePolicy(DTypePolicy):\n \n     def _parse_name(self, name):\n         error_msg = (\n-            f\"Cannot convert '{name}' to a QuantizedDTypePolicy. \"\n-            \"Valid policies include \"\n-            \"'int8_from_float32', 'int8_from_float16', 'int8_from_bfloat16', \"\n-            \"'int8_from_mixed_float16', 'int8_from_mixed_bfloat16'.\"\n+            f\"Cannot convert '{name}' to a {self.__class__.__name__}. \"\n+            f\"Valid policies are: {self._get_all_valid_policies()}.\"\n         )\n         split_name = name.split(\"_from_\")\n         if len(split_name) != 2:\n             raise ValueError(error_msg)\n         mode, from_name = split_name\n-        if mode not in (\"int8\",):\n+        if mode not in QUANTIZATION_MODES:\n             raise ValueError(error_msg)\n-        if from_name == \"mixed_float16\":\n+        if from_name == \"mixed_float16\" and mode != \"int8\":\n             return mode, \"float16\", \"float32\"\n         elif from_name == \"mixed_bfloat16\":\n             return mode, \"bfloat16\", \"float32\"\n         try:\n             dtype = backend.standardize_dtype(from_name)\n+            if dtype == \"float16\" and mode == \"int8\":\n+                raise ValueError\n             return mode, dtype, dtype\n         except ValueError:\n             raise ValueError(error_msg)\n@@ -237,6 +239,67 @@ class QuantizedDTypePolicy(DTypePolicy):\n     def __repr__(self):\n         return f'<QuantizedDTypePolicy \"{self._name}\">'\n \n+    def _get_all_valid_policies(self):\n+        valid_float_policies = [\n+            \"float32\",\n+            \"float16\",\n+            \"bfloat16\",\n+            \"mixed_float16\",\n+            \"mixed_bfloat16\",\n+        ]\n+        valid_policies = [\n+            f\"{mode}_from_{policy}\"\n+            for mode in (\"int8\",)\n+            for policy in valid_float_policies\n+        ]\n+        # Remove invalid policies\n+        valid_policies.remove(\"int8_from_float16\")\n+        valid_policies.remove(\"int8_from_mixed_float16\")\n+        return valid_policies\n+\n+\n+@keras_export(\n+    [\n+        \"keras.QuantizedFloat8DTypePolicy\",\n+        \"keras.dtype_policies.QuantizedFloat8DTypePolicy\",\n+    ]\n+)\n+class QuantizedFloat8DTypePolicy(QuantizedDTypePolicy):\n+    def __init__(self, name, amax_history_length=1024):\n+        super().__init__(name)\n+        if not isinstance(amax_history_length, int):\n+            raise TypeError(\n+                \"`amax_history_length` must be an integer. \"\n+                f\"Received: amax_history_length={amax_history_length}\"\n+            )\n+        self._amax_history_length = amax_history_length\n+\n+    @property\n+    def amax_history_length(self):\n+        \"\"\"The length of the amax history window.\n+\n+        This property is used for scaling factor computation in float8 training.\n+        \"\"\"\n+        return self._amax_history_length\n+\n+    def __repr__(self):\n+        return f'<QuantizedFloat8DTypePolicy \"{self._name}\">'\n+\n+    def _get_all_valid_policies(self):\n+        valid_float_policies = [\n+            \"float32\",\n+            \"float16\",\n+            \"bfloat16\",\n+            \"mixed_float16\",\n+            \"mixed_bfloat16\",\n+        ]\n+        valid_policies = [\n+            f\"{mode}_from_{policy}\"\n+            for mode in (\"float8\")\n+            for policy in valid_float_policies\n+        ]\n+        return valid_policies\n+\n \n @keras_export(\n     [\n@@ -254,8 +317,8 @@ def set_dtype_policy(policy):\n     \"\"\"\n     if not isinstance(policy, DTypePolicy):\n         if isinstance(policy, str):\n-            if policy.startswith(\"int8\"):\n-                policy = QuantizedDTypePolicy(policy)\n+            if policy.startswith(QUANTIZATION_MODES):\n+                policy = _get_quantized_dtype_policy_by_str(policy)\n             else:\n                 policy = FloatDTypePolicy(policy)\n         else:\n@@ -283,3 +346,18 @@ def dtype_policy():\n         policy = FloatDTypePolicy(backend.floatx())\n         set_dtype_policy(policy)\n     return policy\n+\n+\n+def _get_quantized_dtype_policy_by_str(policy):\n+    if not isinstance(policy, str):\n+        raise TypeError(f\"`policy` must be a string. Received: policy={policy}\")\n+    if not policy.startswith(QUANTIZATION_MODES):\n+        raise ValueError(\n+            \"`policy` is incompatible with the current supported quantization.\"\n+        )\n+    if policy.startswith(\"int8\"):\n+        return QuantizedDTypePolicy(policy)\n+    elif policy.startswith(\"float8\"):\n+        return QuantizedFloat8DTypePolicy(policy)\n+    else:\n+        raise NotImplementedError\n\n@@ -1,6 +1,9 @@\n+from absl.testing import parameterized\n+\n from keras.dtype_policies.dtype_policy import DTypePolicy\n from keras.dtype_policies.dtype_policy import FloatDTypePolicy\n from keras.dtype_policies.dtype_policy import QuantizedDTypePolicy\n+from keras.dtype_policies.dtype_policy import QuantizedFloat8DTypePolicy\n from keras.dtype_policies.dtype_policy import dtype_policy\n from keras.dtype_policies.dtype_policy import set_dtype_policy\n from keras.testing import test_case\n@@ -61,13 +64,14 @@ class DTypePolicyTest(test_case.TestCase):\n         new_policy = DTypePolicy.from_config(config)\n         self.assertEqual(new_policy.name, \"mixed_float16\")\n \n-    def test_deepcopy(self):\n+    def test_serialization(self):\n         \"\"\"Test builtin serialization methods.\"\"\"\n         import copy\n         import pickle\n \n-        # copy.deepcopy\n         policy = DTypePolicy(\"mixed_float16\")\n+\n+        # copy.deepcopy\n         copied_policy = copy.deepcopy(policy)\n         self.assertEqual(\n             repr(copied_policy), '<FloatDTypePolicy \"mixed_float16\">'\n@@ -151,62 +155,163 @@ class FloatDTypePolicyTest(test_case.TestCase):\n         self.assertEqual(new_policy.name, \"mixed_float16\")\n \n \n-class QuantizedDTypePolicyTest(test_case.TestCase):\n-    def test_initialization_valid_name(self):\n-        \"\"\"Test initialization with a valid name.\"\"\"\n-        policy = QuantizedDTypePolicy(\"int8_from_mixed_float16\")\n-        self.assertEqual(policy.compute_dtype, \"float16\")\n-        self.assertEqual(policy.variable_dtype, \"float32\")\n+class QuantizedDTypePolicyTest(test_case.TestCase, parameterized.TestCase):\n+    @parameterized.named_parameters(\n+        (\"float32\", \"float32\", \"float32\", \"float32\"),\n+        (\"bfloat16\", \"bfloat16\", \"bfloat16\", \"bfloat16\"),\n+        (\"mixed_bfloat16\", \"mixed_bfloat16\", \"bfloat16\", \"float32\"),\n+    )\n+    def test_initialization_for_int8(\n+        self, from_name, expected_compute_dtype, expected_variable_dtype\n+    ):\n+        name = f\"int8_from_{from_name}\"\n+        policy = QuantizedDTypePolicy(name)\n+        self.assertEqual(policy.name, name)\n+        self.assertEqual(policy.compute_dtype, expected_compute_dtype)\n+        self.assertEqual(policy.variable_dtype, expected_variable_dtype)\n+        self.assertEqual(repr(policy), f'<QuantizedDTypePolicy \"{name}\">')\n \n-    def test_initialization_invalid_name(self):\n-        \"\"\"Test initialization with an invalid name.\"\"\"\n+    @parameterized.named_parameters(\n+        (\"float32\", \"float32\", \"float32\", \"float32\"),\n+        (\"float16\", \"float16\", \"float16\", \"float16\"),\n+        (\"bfloat16\", \"bfloat16\", \"bfloat16\", \"bfloat16\"),\n+        (\"mixed_float16\", \"mixed_float16\", \"float16\", \"float32\"),\n+        (\"mixed_bfloat16\", \"mixed_bfloat16\", \"bfloat16\", \"float32\"),\n+    )\n+    def test_initialization_for_float8(\n+        self, from_name, expected_compute_dtype, expected_variable_dtype\n+    ):\n+        name = f\"float8_from_{from_name}\"\n+        policy = QuantizedFloat8DTypePolicy(name)\n+        self.assertEqual(policy.name, name)\n+        self.assertEqual(policy.compute_dtype, expected_compute_dtype)\n+        self.assertEqual(policy.variable_dtype, expected_variable_dtype)\n+        self.assertEqual(repr(policy), f'<QuantizedFloat8DTypePolicy \"{name}\">')\n+\n+    @parameterized.named_parameters(\n+        (\"abc\", \"abc\"),\n+        (\"abc_from_def\", \"abc_from_def\"),\n+        (\"int8_from_float16\", \"int8_from_float16\"),\n+        (\"int8_from_mixed_float16\", \"int8_from_mixed_float16\"),\n+    )\n+    def test_initialization_with_invalid_name(self, invalid_name):\n         with self.assertRaisesRegex(ValueError, \"Cannot convert\"):\n-            QuantizedDTypePolicy(\"invalid_name\")\n+            QuantizedDTypePolicy(invalid_name)\n \n     def test_initialization_non_string_name(self):\n         \"\"\"Test initialization with a non-string name.\"\"\"\n         with self.assertRaisesRegex(TypeError, \"'name' must be a string\"):\n             QuantizedDTypePolicy(123)\n \n-    def test_properties_mixed_float16(self):\n-        \"\"\"Test properties for 'mixed_float16'.\"\"\"\n-        policy = QuantizedDTypePolicy(\"int8_from_mixed_float16\")\n-        self.assertEqual(policy.compute_dtype, \"float16\")\n-        self.assertEqual(policy.variable_dtype, \"float32\")\n-\n-    def test_properties_mixed_bfloat16(self):\n-        \"\"\"Test properties for 'mixed_bfloat16'.\"\"\"\n-        policy = QuantizedDTypePolicy(\"int8_from_mixed_bfloat16\")\n-        self.assertEqual(policy.compute_dtype, \"bfloat16\")\n-        self.assertEqual(policy.variable_dtype, \"float32\")\n-\n-    def test_initialization_with_invalid_name_behaviour(self):\n-        \"\"\"Test initialization behavior with an invalid name.\"\"\"\n-        with self.assertRaisesRegex(ValueError, \"Cannot convert\"):\n-            QuantizedDTypePolicy(\"invalid_name\")\n-\n-    def test_properties(self):\n-        \"\"\"Test variable_dtype, compute_dtype, and name properties.\"\"\"\n-        policy = QuantizedDTypePolicy(\"int8_from_mixed_float16\")\n-        self.assertEqual(policy.variable_dtype, \"float32\")\n-        self.assertEqual(policy.compute_dtype, \"float16\")\n-        self.assertEqual(policy.name, \"int8_from_mixed_float16\")\n-\n-    def test_repr(self):\n-        \"\"\"Test __repr__ method.\"\"\"\n-        policy = QuantizedDTypePolicy(\"int8_from_mixed_float16\")\n-        self.assertEqual(\n-            repr(policy), '<QuantizedDTypePolicy \"int8_from_mixed_float16\">'\n-        )\n-\n     def test_get_config_from_config(self):\n         \"\"\"Test get_config and from_config methods.\"\"\"\n-        policy = QuantizedDTypePolicy(\"int8_from_mixed_float16\")\n+        policy = QuantizedDTypePolicy(\"int8_from_mixed_bfloat16\")\n         config = policy.get_config()\n-        self.assertEqual(config, {\"name\": \"int8_from_mixed_float16\"})\n+        self.assertEqual(config, {\"name\": \"int8_from_mixed_bfloat16\"})\n \n         new_policy = QuantizedDTypePolicy.from_config(config)\n-        self.assertEqual(new_policy.name, \"int8_from_mixed_float16\")\n+        self.assertEqual(new_policy.name, \"int8_from_mixed_bfloat16\")\n+\n+    @parameterized.named_parameters(\n+        (\n+            \"int8_from_mixed_bfloat16\",\n+            \"int8_from_mixed_bfloat16\",\n+            '<QuantizedDTypePolicy \"int8_from_mixed_bfloat16\">',\n+        ),\n+        (\n+            \"float8_from_mixed_bfloat16\",\n+            \"float8_from_mixed_bfloat16\",\n+            '<QuantizedFloat8DTypePolicy \"float8_from_mixed_bfloat16\">',\n+        ),\n+    )\n+    def test_serialization(self, name, repr_str):\n+        import copy\n+        import pickle\n+\n+        policy = DTypePolicy(name)\n+\n+        # copy.deepcopy\n+        copied_policy = copy.deepcopy(policy)\n+        self.assertEqual(repr(copied_policy), repr_str)\n+        # copy.copy\n+        copied_policy = copy.copy(policy)\n+        self.assertEqual(repr(copied_policy), repr_str)\n+        # pickle\n+        temp_dir = self.get_temp_dir()\n+        with open(f\"{temp_dir}/policy.pickle\", \"wb\") as f:\n+            pickle.dump(policy, f)\n+        with open(f\"{temp_dir}/policy.pickle\", \"rb\") as f:\n+            copied_policy = pickle.load(f)\n+        self.assertEqual(repr(copied_policy), repr_str)\n+\n+    def test_properties_for_float8(self):\n+        policy = QuantizedFloat8DTypePolicy(\"float8_from_mixed_bfloat16\")\n+        self.assertEqual(policy.amax_history_length, 1024)\n+        policy = QuantizedFloat8DTypePolicy(\"float8_from_mixed_bfloat16\", 512)\n+        self.assertEqual(policy.amax_history_length, 512)\n+\n+    def test_invalid_properties_for_float8(self):\n+        with self.assertRaisesRegex(TypeError, \"must be an integer.\"):\n+            QuantizedFloat8DTypePolicy(\"float8_from_float32\", \"512\")\n+        with self.assertRaisesRegex(TypeError, \"must be an integer.\"):\n+            QuantizedFloat8DTypePolicy(\"float8_from_float32\", 512.0)\n+\n+    def test_serialization_for_float8(self):\n+        import copy\n+        import pickle\n+\n+        policy = QuantizedFloat8DTypePolicy(\"float8_from_mixed_bfloat16\", 123)\n+\n+        # copy.deepcopy\n+        copied_policy = copy.deepcopy(policy)\n+        self.assertEqual(\n+            repr(copied_policy),\n+            '<QuantizedFloat8DTypePolicy \"float8_from_mixed_bfloat16\">',\n+        )\n+        self.assertEqual(copied_policy.amax_history_length, 123)\n+        # copy.copy\n+        copied_policy = copy.copy(policy)\n+        self.assertEqual(\n+            repr(copied_policy),\n+            '<QuantizedFloat8DTypePolicy \"float8_from_mixed_bfloat16\">',\n+        )\n+        self.assertEqual(copied_policy.amax_history_length, 123)\n+        # pickle\n+        temp_dir = self.get_temp_dir()\n+        with open(f\"{temp_dir}/policy.pickle\", \"wb\") as f:\n+            pickle.dump(policy, f)\n+        with open(f\"{temp_dir}/policy.pickle\", \"rb\") as f:\n+            copied_policy = pickle.load(f)\n+        self.assertEqual(\n+            repr(copied_policy),\n+            '<QuantizedFloat8DTypePolicy \"float8_from_mixed_bfloat16\">',\n+        )\n+        self.assertEqual(copied_policy.amax_history_length, 123)\n+\n+    @parameterized.named_parameters(\n+        (\"int8_from_mixed_bfloat16\", \"int8_from_mixed_bfloat16\"),\n+        (\"float8_from_mixed_bfloat16\", \"float8_from_mixed_bfloat16\"),\n+    )\n+    def test_get_quantized_dtype_policy_by_str(self, name):\n+        from keras.dtype_policies.dtype_policy import (\n+            _get_quantized_dtype_policy_by_str,\n+        )\n+\n+        policy = _get_quantized_dtype_policy_by_str(name)\n+        self.assertEqual(policy.name, name)\n+\n+    def test_invalid_get_quantized_dtype_policy_by_str(self):\n+        from keras.dtype_policies.dtype_policy import (\n+            _get_quantized_dtype_policy_by_str,\n+        )\n+\n+        with self.assertRaisesRegex(TypeError, \"must be a string.\"):\n+            _get_quantized_dtype_policy_by_str(123)\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"is incompatible with the current supported quantization.\",\n+        ):\n+            _get_quantized_dtype_policy_by_str(\"float7\")\n \n \n class DTypePolicyGlobalFunctionsTest(test_case.TestCase):\n@@ -222,9 +327,9 @@ class DTypePolicyGlobalFunctionsTest(test_case.TestCase):\n \n     def test_set_dtype_policy_valid_string_quantized(self):\n         \"\"\"Test set_dtype_policy with a valid string.\"\"\"\n-        set_dtype_policy(\"int8_from_mixed_float16\")\n+        set_dtype_policy(\"int8_from_mixed_bfloat16\")\n         policy = dtype_policy()\n-        self.assertEqual(policy.name, \"int8_from_mixed_float16\")\n+        self.assertEqual(policy.name, \"int8_from_mixed_bfloat16\")\n \n     def test_set_dtype_policy_valid_policy(self):\n         \"\"\"Test set_dtype_policy with a valid FloatDTypePolicy object.\"\"\"\n@@ -235,10 +340,10 @@ class DTypePolicyGlobalFunctionsTest(test_case.TestCase):\n \n     def test_set_dtype_policy_valid_policy_quantized(self):\n         \"\"\"Test set_dtype_policy with a valid FloatDTypePolicy object.\"\"\"\n-        policy_obj = QuantizedDTypePolicy(\"int8_from_mixed_float16\")\n+        policy_obj = QuantizedDTypePolicy(\"int8_from_mixed_bfloat16\")\n         set_dtype_policy(policy_obj)\n         policy = dtype_policy()\n-        self.assertEqual(policy.name, \"int8_from_mixed_float16\")\n+        self.assertEqual(policy.name, \"int8_from_mixed_bfloat16\")\n \n     def test_set_dtype_policy_invalid(self):\n         \"\"\"Test set_dtype_policy with an invalid input.\"\"\"\n@@ -282,17 +387,17 @@ class QuantizedDTypePolicyEdgeCasesTest(test_case.TestCase):\n     def test_special_character_name(self):\n         \"\"\"Test initialization with special characters in the name.\"\"\"\n         with self.assertRaisesRegex(ValueError, \"Cannot convert\"):\n-            QuantizedDTypePolicy(\"@int8_from_mixed_float16!\")\n+            QuantizedDTypePolicy(\"@int8_from_mixed_bfloat16!\")\n \n     def test_very_long_name(self):\n         \"\"\"Test initialization with a very long name.\"\"\"\n         with self.assertRaisesRegex(ValueError, \"Cannot convert\"):\n-            QuantizedDTypePolicy(\"int8_from_mixed_float16\" * 100)\n+            QuantizedDTypePolicy(\"int8_from_mixed_bfloat16\" * 100)\n \n     def test_almost_valid_name(self):\n         \"\"\"Test initialization with a name close to a valid one.\"\"\"\n         with self.assertRaisesRegex(ValueError, \"Cannot convert\"):\n-            QuantizedDTypePolicy(\"int7_from_mixed_float16\")\n+            QuantizedDTypePolicy(\"int7_from_mixed_bfloat16\")\n \n \n class DTypePolicyGlobalFunctionsEdgeCasesTest(test_case.TestCase):\n\n@@ -1,3 +1,5 @@\n+import ml_dtypes\n+\n from keras import activations\n from keras import backend\n from keras import constraints\n@@ -100,11 +102,17 @@ class Dense(Layer):\n \n     def build(self, input_shape):\n         input_dim = input_shape[-1]\n-        if isinstance(self.dtype_policy, dtype_policies.QuantizedDTypePolicy):\n+        # We use `self._dtype_policy` to check to avoid issues in torch dynamo\n+        is_quantized = isinstance(\n+            self._dtype_policy, dtype_policies.QuantizedDTypePolicy\n+        )\n+        if is_quantized:\n             self.quantized_build(\n                 input_shape, mode=self.dtype_policy.quantization_mode\n             )\n-        else:\n+        if not is_quantized or self.dtype_policy.quantization_mode != \"int8\":\n+            # If the layer is quantized to int8, `self._kernel` will be added\n+            # in `self._int8_build`. Therefore, we skip it here.\n             self._kernel = self.add_weight(\n                 name=\"kernel\",\n                 shape=(input_dim, self.units),\n@@ -199,7 +207,20 @@ class Dense(Layer):\n         if self.use_bias:\n             store[\"1\"] = self.bias\n         if isinstance(self.dtype_policy, dtype_policies.QuantizedDTypePolicy):\n+            mode = self.dtype_policy.quantization_mode\n+            if mode == \"int8\":\n                 store[\"2\"] = kernel_scale\n+            elif mode == \"float8\":\n+                store[\"2\"] = self.inputs_scale\n+                store[\"3\"] = self.inputs_amax_history\n+                store[\"4\"] = self.kernel_scale\n+                store[\"5\"] = self.kernel_amax_history\n+                store[\"6\"] = self.outputs_grad_scale\n+                store[\"7\"] = self.outputs_grad_amax_history\n+            else:\n+                raise NotImplementedError(\n+                    self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode)\n+                )\n \n     def load_own_variables(self, store):\n         if not self.lora_enabled:\n@@ -213,7 +234,20 @@ class Dense(Layer):\n         if self.use_bias:\n             self.bias.assign(store[\"1\"])\n         if isinstance(self.dtype_policy, dtype_policies.QuantizedDTypePolicy):\n+            mode = self.dtype_policy.quantization_mode\n+            if mode == \"int8\":\n                 self.kernel_scale.assign(store[\"2\"])\n+            elif mode == \"float8\":\n+                self.inputs_scale.assign(store[\"2\"])\n+                self.inputs_amax_history.assign(store[\"3\"])\n+                self.kernel_scale.assign(store[\"4\"])\n+                self.kernel_amax_history.assign(store[\"5\"])\n+                self.outputs_grad_scale.assign(store[\"6\"])\n+                self.outputs_grad_amax_history.assign(store[\"7\"])\n+            else:\n+                raise NotImplementedError(\n+                    self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode)\n+                )\n         if self.lora_enabled:\n             self.lora_kernel_a.assign(ops.zeros(self.lora_kernel_a.shape))\n             self.lora_kernel_b.assign(ops.zeros(self.lora_kernel_b.shape))\n@@ -273,27 +307,108 @@ class Dense(Layer):\n                 f\"Expected: {[v.name for v in all_vars]}\"\n             )\n \n-    \"\"\"Quantization-related methods\"\"\"\n+    \"\"\"Quantization-related (int8 and float8) methods\"\"\"\n+\n+    QUANTIZATION_MODE_ERROR_TEMPLATE = (\n+        f\"Invalid quantization mode. Expected one of \"\n+        f\"{dtype_policies.QUANTIZATION_MODES}. \"\n+        \"Received: quantization_mode={mode}\"\n+    )\n \n     def quantized_build(self, input_shape, mode):\n-        input_dim = input_shape[-1]\n         if mode == \"int8\":\n+            input_dim = input_shape[-1]\n+            kernel_shape = (input_dim, self.units)\n+            self._int8_build(kernel_shape)\n+        elif mode == \"float8\":\n+            self._float8_build()\n+        else:\n+            raise NotImplementedError(\n+                self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode)\n+            )\n+\n+    def _int8_build(\n+        self,\n+        kernel_shape,\n+        kernel_initializer=\"zeros\",\n+        kernel_scale_initializer=\"ones\",\n+    ):\n         self.inputs_quantizer = quantizers.AbsMaxQuantizer(axis=-1)\n         self._kernel = self.add_weight(\n             name=\"kernel\",\n-                shape=(input_dim, self.units),\n-                initializer=\"zeros\",\n+            shape=kernel_shape,\n+            initializer=kernel_initializer,\n             dtype=\"int8\",\n             trainable=False,\n         )\n         self.kernel_scale = self.add_weight(\n             name=\"kernel_scale\",\n             shape=(self.units,),\n-                initializer=\"ones\",\n+            initializer=kernel_scale_initializer,\n             trainable=False,\n         )\n \n+    def _float8_build(self):\n+        if not isinstance(\n+            self.dtype_policy, dtype_policies.QuantizedFloat8DTypePolicy\n+        ):\n+            raise TypeError(\n+                \"`self.dtype_policy` must be the type of \"\n+                f\"QuantizedFloat8DTypePolicy. Received {self.dtype_policy}\"\n+            )\n+        amax_history_length = self.dtype_policy.amax_history_length\n+        # We set `trainable=True` because we will use the gradients to overwrite\n+        # these variables\n+        scale_kwargs = {\n+            \"shape\": (),\n+            \"initializer\": \"ones\",\n+            \"dtype\": \"float32\",  # Always be float32\n+            \"trainable\": True,\n+            \"autocast\": False,\n+        }\n+        amax_history_kwargs = {\n+            \"shape\": (amax_history_length,),\n+            \"initializer\": \"zeros\",\n+            \"dtype\": \"float32\",  # Always be float32\n+            \"trainable\": True,\n+            \"autocast\": False,\n+        }\n+        self.inputs_scale = self.add_weight(name=\"inputs_scale\", **scale_kwargs)\n+        self.inputs_amax_history = self.add_weight(\n+            name=\"inputs_amax_history\", **amax_history_kwargs\n+        )\n+        self.kernel_scale = self.add_weight(name=\"kernel_scale\", **scale_kwargs)\n+        self.kernel_amax_history = self.add_weight(\n+            name=\"kernel_amax_history\", **amax_history_kwargs\n+        )\n+        self.outputs_grad_scale = self.add_weight(\n+            name=\"outputs_grad_scale\", **scale_kwargs\n+        )\n+        self.outputs_grad_amax_history = self.add_weight(\n+            name=\"outputs_grad_amax_history\", **amax_history_kwargs\n+        )\n+        # We need to set `overwrite_with_gradient=True` to instruct the\n+        # optimizer to directly overwrite these variables with their computed\n+        # gradients during training\n+        self.inputs_scale.overwrite_with_gradient = True\n+        self.inputs_amax_history.overwrite_with_gradient = True\n+        self.kernel_scale.overwrite_with_gradient = True\n+        self.kernel_amax_history.overwrite_with_gradient = True\n+        self.outputs_grad_scale.overwrite_with_gradient = True\n+        self.outputs_grad_amax_history.overwrite_with_gradient = True\n+\n     def quantized_call(self, inputs):\n+        if self.dtype_policy.quantization_mode == \"int8\":\n+            return self._int8_call(inputs)\n+        elif self.dtype_policy.quantization_mode == \"float8\":\n+            return self._float8_call(inputs)\n+        else:\n+            mode = self.dtype_policy.quantization_mode\n+            raise NotImplementedError(\n+                self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode)\n+            )\n+\n+    def _int8_call(self, inputs):\n         @ops.custom_gradient\n         def matmul_with_inputs_gradient(inputs, kernel, kernel_scale):\n             def grad_fn(*args, upstream=None):\n@@ -328,6 +443,93 @@ class Dense(Layer):\n             x = self.activation(x)\n         return x\n \n+    def _float8_call(self, inputs):\n+        if self.lora_enabled:\n+            raise NotImplementedError(\n+                \"Currently, `_float8_call` doesn't support LoRA\"\n+            )\n+\n+        @ops.custom_gradient\n+        def quantized_dequantize_inputs(inputs, scale, amax_history):\n+            new_scale = quantizers.compute_float8_scale(\n+                ops.max(amax_history, axis=0),\n+                scale,\n+                ops.cast(\n+                    float(ml_dtypes.finfo(\"float8_e4m3fn\").max), \"float32\"\n+                ),\n+            )\n+            qdq_inputs = quantizers.quantize_and_dequantize(\n+                inputs, scale, \"float8_e4m3fn\", self.compute_dtype\n+            )\n+            new_amax_history = quantizers.compute_float8_amax_history(\n+                inputs, amax_history\n+            )\n+\n+            def grad(*args, upstream=None, variables=None):\n+                if upstream is None:\n+                    (upstream,) = args\n+                return upstream, new_scale, new_amax_history\n+\n+            return qdq_inputs, grad\n+\n+        @ops.custom_gradient\n+        def quantized_dequantize_outputs(outputs, scale, amax_history):\n+            \"\"\"Quantize-dequantize the output gradient but not the output.\"\"\"\n+\n+            def grad(*args, upstream=None, variables=None):\n+                if upstream is None:\n+                    (upstream,) = args\n+                new_scale = quantizers.compute_float8_scale(\n+                    ops.max(amax_history, axis=0),\n+                    scale,\n+                    ops.cast(\n+                        float(ml_dtypes.finfo(\"float8_e5m2\").max), \"float32\"\n+                    ),\n+                )\n+                qdq_upstream = quantizers.quantize_and_dequantize(\n+                    upstream, scale, \"float8_e5m2\", self.compute_dtype\n+                )\n+                new_amax_history = quantizers.compute_float8_amax_history(\n+                    upstream, amax_history\n+                )\n+                return qdq_upstream, new_scale, new_amax_history\n+\n+            return outputs, grad\n+\n+        x = ops.matmul(\n+            quantized_dequantize_inputs(\n+                inputs,\n+                ops.convert_to_tensor(self.inputs_scale),\n+                ops.convert_to_tensor(self.inputs_amax_history),\n+            ),\n+            quantized_dequantize_inputs(\n+                ops.convert_to_tensor(self._kernel),\n+                ops.convert_to_tensor(self.kernel_scale),\n+                ops.convert_to_tensor(self.kernel_amax_history),\n+            ),\n+        )\n+        # `quantized_dequantize_outputs` is placed immediately after\n+        # `ops.matmul` for the sake of pattern matching in gemm_rewrite. That\n+        # way, the qdq will be adjacent to the corresponding matmul_bprop in the\n+        # bprop.\n+        x = quantized_dequantize_outputs(\n+            x,\n+            ops.convert_to_tensor(self.outputs_grad_scale),\n+            ops.convert_to_tensor(self.outputs_grad_amax_history),\n+        )\n+        if self.bias is not None:\n+            # Under non-mixed precision cases, F32 bias has to be converted to\n+            # BF16 first to get the biasAdd fusion support. ref. PR\n+            # https://github.com/tensorflow/tensorflow/pull/60306\n+            bias = self.bias\n+            if self.dtype_policy.compute_dtype == \"float32\":\n+                bias_bf16 = ops.cast(bias, \"bfloat16\")\n+                bias = ops.cast(bias_bf16, bias.dtype)\n+            x = ops.add(x, bias)\n+        if self.activation is not None:\n+            x = self.activation(x)\n+        return x\n+\n     def quantize(self, mode):\n         import gc\n \n@@ -338,6 +540,17 @@ class Dense(Layer):\n                 \"method implemented.\"\n             )\n         self._check_quantize_args(mode, self.compute_dtype)\n+\n+        # Set new dtype policy\n+        if not isinstance(\n+            self.dtype_policy, dtype_policies.QuantizedDTypePolicy\n+        ):\n+            quantized_dtype = f\"{mode}_from_{self.dtype_policy.name}\"\n+            # We set the internal `self._dtype_policy` instead of using the\n+            # setter to avoid double `quantize` call\n+            self._dtype_policy = dtype_policies.get(quantized_dtype)\n+\n+        self._tracker.unlock()\n         if mode == \"int8\":\n             if backend.standardize_dtype(self._kernel.dtype) == \"int8\":\n                 raise ValueError(\"`quantize` can only be done once per layer.\")\n@@ -348,38 +561,25 @@ class Dense(Layer):\n                 self._kernel, axis=0\n             )\n             kernel_scale = ops.squeeze(kernel_scale, axis=0)\n-            self._tracker.unlock()\n             self._untrack_variable(self._kernel)\n             kernel_shape = self._kernel.shape\n             del self._kernel\n-            self._kernel = self.add_weight(\n-                name=\"kernel\",\n-                shape=kernel_shape,\n-                # Prevent adding a large constant to the computation graph\n-                initializer=lambda shape, dtype: kernel_value,\n-                dtype=\"int8\",\n-                trainable=False,\n+            # Utilize a lambda expression as an initializer to prevent adding a\n+            # large constant to the computation graph.\n+            self._int8_build(\n+                kernel_shape,\n+                lambda shape, dtype: kernel_value,\n+                lambda shape, dtype: kernel_scale,\n             )\n-            self.kernel_scale = self.add_weight(\n-                name=\"kernel_scale\",\n-                shape=(self.units,),\n-                # Prevent adding a large constant to the computation graph\n-                initializer=lambda shape, dtype: kernel_scale,\n-                trainable=False,\n+        elif mode == \"float8\":\n+            if hasattr(self, \"inputs_amax_history\"):\n+                raise ValueError(\"`quantize` can only be done once per layer.\")\n+            self._float8_build()\n+        else:\n+            raise NotImplementedError(\n+                self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode)\n             )\n         self._tracker.lock()\n-        else:\n-            NotImplementedError(\n-                \"Invalid quantization mode. Expected 'int8'. \"\n-                f\"Received: mode={mode}\"\n-            )\n-\n-        # Set new dtype policy\n-        if not isinstance(\n-            self.dtype_policy, dtype_policies.QuantizedDTypePolicy\n-        ):\n-            quantized_dtype = f\"{mode}_from_{self.dtype_policy.name}\"\n-            self.dtype_policy = dtype_policies.get(quantized_dtype)\n \n         # Release memory manually because sometimes the backend doesn't\n         gc.collect()\n\n@@ -2,19 +2,22 @@ import os\n \n import numpy as np\n import pytest\n+from absl.testing import parameterized\n \n from keras import backend\n from keras import constraints\n from keras import layers\n from keras import models\n from keras import ops\n+from keras import optimizers\n+from keras import random\n from keras import saving\n from keras import testing\n from keras.backend.common import keras_tensor\n from keras.export import export_lib\n \n \n-class DenseTest(testing.TestCase):\n+class DenseTest(testing.TestCase, parameterized.TestCase):\n     @pytest.mark.requires_trainable_backend\n     def test_dense_basics(self):\n         # 2D case, no bias.\n@@ -329,6 +332,8 @@ class DenseTest(testing.TestCase):\n         with self.assertRaisesRegex(ValueError, \"lora is already enabled\"):\n             layer.enable_lora(rank=2)\n \n+    \"\"\"Test quantization-related (int8 and float8) methods\"\"\"\n+\n     @pytest.mark.skipif(\n         backend.backend() == \"numpy\",\n         reason=f\"{backend.backend()} does not support ops.custom_gradient.\",\n@@ -383,8 +388,57 @@ class DenseTest(testing.TestCase):\n             backend.standardize_dtype(layer.kernel_scale.dtype), \"float32\"\n         )\n \n+    @parameterized.named_parameters(\n+        (\"int8\", \"int8\"),\n+        (\"float8\", \"float8\"),\n+    )\n+    def test_quantize_on_unbuilt_layer(self, mode):\n+        layer = layers.Dense(units=2)\n+        with self.assertRaisesRegex(\n+            ValueError, \"Cannot quantize a layer that isn't yet built.\"\n+        ):\n+            layer.quantize(mode)\n+\n+    @parameterized.named_parameters(\n+        (\"int8\", \"int8\"),\n+        (\"float8\", \"float8\"),\n+    )\n+    def test_quantize_on_subclass(self, mode):\n+        class MyDense(layers.Dense):\n+            pass\n+\n+        layer = MyDense(units=16)\n+        layer.build((None, 8))\n+        with self.assertRaises(NotImplementedError):\n+            layer.quantize(mode)\n+\n+    @parameterized.named_parameters(\n+        (\"int8\", \"int8\"),\n+        (\"float8\", \"float8\"),\n+    )\n+    def test_quantize_when_already_quantized(self, mode):\n+        layer = layers.Dense(units=2)\n+        layer.build((None, 2))\n+        layer.quantize(mode)\n+        with self.assertRaisesRegex(\n+            ValueError, \"`quantize` can only be done once per layer.\"\n+        ):\n+            layer.quantize(mode)\n+\n+    @parameterized.named_parameters(\n+        (\"int8\", \"int8_from_float32\", 3),\n+        (\"float8\", \"float8_from_float32\", 8),\n+    )\n+    def test_quantize_by_setting_dtype_policy(\n+        self, policy, expected_num_variables\n+    ):\n+        layer = layers.Dense(units=2)\n+        layer.build((None, 2))\n+        layer.dtype_policy = policy\n+        self.assertLen(layer.variables, expected_num_variables)\n+\n     @pytest.mark.requires_trainable_backend\n-    def test_quantize_dtype_argument(self):\n+    def test_quantize_int8_dtype_argument(self):\n         self.run_layer_test(\n             layers.Dense,\n             init_kwargs={\n@@ -400,33 +454,8 @@ class DenseTest(testing.TestCase):\n             supports_masking=True,\n         )\n \n-    def test_quantize_on_unbuilt_layer(self):\n-        layer = layers.Dense(units=2)\n-        with self.assertRaisesRegex(\n-            ValueError, \"Cannot quantize a layer that isn't yet built.\"\n-        ):\n-            layer.quantize(\"int8\")\n-\n-    def test_quantize_on_subclass(self):\n-        class MyDense(layers.Dense):\n-            pass\n-\n-        layer = MyDense(units=16)\n-        layer.build((None, 8))\n-        with self.assertRaises(NotImplementedError):\n-            layer.quantize(\"int8\")\n-\n-    def test_quantize_when_already_quantized(self):\n-        layer = layers.Dense(units=2)\n-        layer.build((None, 2))\n-        layer.quantize(\"int8\")\n-        with self.assertRaisesRegex(\n-            ValueError, \"`quantize` can only be done once per layer.\"\n-        ):\n-            layer.quantize(\"int8\")\n-\n     @pytest.mark.requires_trainable_backend\n-    def test_quantize_when_lora_enabled(self):\n+    def test_quantize_int8_when_lora_enabled(self):\n         # Note that saving and loading with lora_enabled and quantized are\n         # lossy, so we use a weak correctness test for model outputs (atol=0.5).\n         config = dict(units=16)\n@@ -490,7 +519,7 @@ class DenseTest(testing.TestCase):\n             import tensorflow as tf\n \n             temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n-            ref_input = tf.random.normal((32, 8))\n+            ref_input = tf.random.normal((2, 8))\n             ref_output = model(ref_input)\n             export_lib.export_model(model, temp_filepath)\n             reloaded_layer = export_lib.TFSMLayer(temp_filepath)\n@@ -505,3 +534,187 @@ class DenseTest(testing.TestCase):\n                 reloaded_layer.non_trainable_weights,\n                 len(model.non_trainable_weights),\n             )\n+\n+    @pytest.mark.requires_trainable_backend\n+    def test_quantize_float8_dtype_argument(self):\n+        self.run_layer_test(\n+            layers.Dense,\n+            init_kwargs={\n+                \"units\": 5,\n+                \"dtype\": \"float8_from_mixed_bfloat16\",\n+            },\n+            input_shape=(2, 3, 4),\n+            expected_output_shape=(2, 3, 5),\n+            expected_num_trainable_weights=8,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=True,\n+        )\n+\n+    @pytest.mark.requires_trainable_backend\n+    def test_quantize_float8(self):\n+        import ml_dtypes\n+\n+        from keras import quantizers\n+\n+        layer = layers.Dense(units=32)\n+        layer.build((None, 16))\n+        layer.quantize(\"float8\")\n+        optimizer = optimizers.AdamW(learning_rate=0.1)\n+        optimizer.build(layer.trainable_variables)\n+\n+        def loss_fn(x, dy):\n+            y = layer(x, training=True)\n+            loss = y * ops.cast(dy, y.dtype)\n+            return ops.sum(loss)\n+\n+        if backend.backend() == \"tensorflow\":\n+            import tensorflow as tf\n+\n+            @tf.function(jit_compile=True)\n+            def train_one_step(x, dy):\n+                with tf.GradientTape() as tape:\n+                    loss = loss_fn(x, dy)\n+                grads = tape.gradient(loss, layer.trainable_variables)\n+                optimizer.apply(grads, layer.trainable_variables)\n+\n+        elif backend.backend() == \"jax\":\n+            import jax\n+\n+            def stateless_loss_fn(trainable_variables, x, dy):\n+                y = layer.stateless_call(trainable_variables, [], x)[0]\n+                loss = y * ops.cast(dy, y.dtype)\n+                return ops.sum(loss)\n+\n+            grad_fn = jax.jit(jax.grad(stateless_loss_fn))\n+\n+            def train_one_step(x, dy):\n+                trainable_variables = [\n+                    v.value for v in layer.trainable_variables\n+                ]\n+                optimizer_variables = [v.value for v in optimizer.variables]\n+                grads = grad_fn(trainable_variables, x, dy)\n+                trainable_variables, optimizer_variables = (\n+                    optimizer.stateless_apply(\n+                        optimizer_variables, grads, trainable_variables\n+                    )\n+                )\n+                for variable, value in zip(\n+                    layer.trainable_variables, trainable_variables\n+                ):\n+                    variable.assign(value)\n+                for variable, value in zip(\n+                    optimizer.variables, optimizer_variables\n+                ):\n+                    variable.assign(value)\n+\n+        elif backend.backend() == \"torch\":\n+\n+            def train_one_step(x, dy):\n+                layer.zero_grad()\n+                loss = loss_fn(x, dy)\n+                loss.backward()\n+                grads = [v.value.grad for v in layer.trainable_variables]\n+                optimizer.apply(grads, layer.trainable_variables)\n+\n+        scale_x, amax_history_x = ops.ones(()), ops.zeros((1024,))\n+        scale_k, amax_history_k = ops.ones(()), ops.zeros((1024,))\n+        scale_g, amax_history_g = ops.ones(()), ops.zeros((1024,))\n+        e4m3_max = ops.cast(\n+            float(ml_dtypes.finfo(\"float8_e4m3fn\").max), \"float32\"\n+        )\n+        e5m2_max = ops.cast(\n+            float(ml_dtypes.finfo(\"float8_e5m2\").max), \"float32\"\n+        )\n+\n+        for _ in range(3):\n+            x = random.normal((16, 16), dtype=\"float32\")\n+            g = random.normal((16, 32), dtype=\"float32\")\n+            k = ops.convert_to_tensor(layer._kernel)\n+\n+            # Manually compute the expected amax history and scaling factors.\n+            amax_from_history_x = ops.max(amax_history_x)\n+            amax_from_history_k = ops.max(amax_history_k)\n+            amax_from_history_g = ops.max(amax_history_g)\n+            scale_x = quantizers.compute_float8_scale(\n+                amax_from_history_x, scale_x, e4m3_max\n+            )\n+            scale_k = quantizers.compute_float8_scale(\n+                amax_from_history_k, scale_k, e4m3_max\n+            )\n+            scale_g = quantizers.compute_float8_scale(\n+                amax_from_history_g, scale_g, e5m2_max\n+            )\n+            amax_history_x = quantizers.compute_float8_amax_history(\n+                x, amax_history_x\n+            )\n+            amax_history_k = quantizers.compute_float8_amax_history(\n+                k, amax_history_k\n+            )\n+            amax_history_g = quantizers.compute_float8_amax_history(\n+                g, amax_history_g\n+            )\n+\n+            train_one_step(x, g)\n+\n+            self.assertAllClose(layer.inputs_amax_history, amax_history_x)\n+            self.assertAllClose(layer.kernel_amax_history, amax_history_k)\n+            self.assertAllClose(layer.outputs_grad_amax_history, amax_history_g)\n+            self.assertAllClose(layer.inputs_scale, scale_x)\n+            self.assertAllClose(layer.kernel_scale, scale_k)\n+            self.assertAllClose(layer.outputs_grad_scale, scale_g)\n+\n+    @pytest.mark.requires_trainable_backend\n+    def test_quantize_float8_fitting(self):\n+        config = dict(units=16)\n+        layer = layers.Dense(**config)\n+        layer.build((None, 8))\n+        layer.quantize(\"float8\")\n+        self.assertLen(layer.trainable_weights, 8)\n+        self.assertLen(layer.non_trainable_weights, 0)\n+\n+        # Try calling fit()\n+        x = np.random.random((64, 8))\n+        y = np.random.random((64, 16))\n+        model = models.Sequential([layer])\n+        model.compile(optimizer=\"sgd\", loss=\"mse\")\n+        model.fit(x, y, epochs=2)\n+\n+        # Try saving and reloading the model\n+        temp_filepath = os.path.join(\n+            self.get_temp_dir(), \"quantized_float8_model.keras\"\n+        )\n+        model.save(temp_filepath)\n+        new_model = saving.load_model(temp_filepath)\n+        self.assertAllClose(model.predict(x), new_model.predict(x))\n+\n+        # Try saving and reloading the model's weights only\n+        temp_filepath = os.path.join(\n+            self.get_temp_dir(), \"quantized_float8_model.weights.h5\"\n+        )\n+        model.save_weights(temp_filepath)\n+        new_model = models.Sequential([layers.Dense(**config)])\n+        new_model.build((None, 8))\n+        new_model.quantize(\"float8\")\n+        new_model.load_weights(temp_filepath)\n+        self.assertAllClose(model.predict(x), new_model.predict(x))\n+\n+        # Test export and TFSMLayer reloading when using tensorflow backend\n+        if backend.backend() == \"tensorflow\":\n+            import tensorflow as tf\n+\n+            temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n+            ref_input = tf.random.normal((2, 8))\n+            ref_output = model(ref_input)\n+            export_lib.export_model(model, temp_filepath)\n+            reloaded_layer = export_lib.TFSMLayer(temp_filepath)\n+            self.assertAllClose(reloaded_layer(ref_input), ref_output)\n+            self.assertLen(reloaded_layer.weights, len(model.weights))\n+            self.assertLen(\n+                reloaded_layer.trainable_weights, len(model.trainable_weights)\n+            )\n+            self.assertLen(\n+                reloaded_layer.non_trainable_weights,\n+                len(model.non_trainable_weights),\n+            )\n\n@@ -1,6 +1,7 @@\n import re\n import string\n \n+import ml_dtypes\n import numpy as np\n \n from keras import activations\n@@ -153,13 +154,19 @@ class EinsumDense(Layer):\n         )\n         kernel_shape, bias_shape, full_output_shape = shape_data\n         self.full_output_shape = tuple(full_output_shape)\n-        # `quantized_build` needs `self.input_spec`\n+        # `self._int8_build` needs `self.input_spec`\n         self.input_spec = InputSpec(ndim=len(input_shape))\n-        if isinstance(self.dtype_policy, dtype_policies.QuantizedDTypePolicy):\n+        # We use `self._dtype_policy` to check to avoid issues in torch dynamo\n+        is_quantized = isinstance(\n+            self._dtype_policy, dtype_policies.QuantizedDTypePolicy\n+        )\n+        if is_quantized:\n             self.quantized_build(\n                 input_shape, mode=self.dtype_policy.quantization_mode\n             )\n-        else:\n+        if not is_quantized or self.dtype_policy.quantization_mode != \"int8\":\n+            # If the layer is quantized to int8, `self._kernel` will be added\n+            # in `self._int8_build`. Therefore, we skip it here.\n             self._kernel = self.add_weight(\n                 name=\"kernel\",\n                 shape=tuple(kernel_shape),\n@@ -255,7 +262,20 @@ class EinsumDense(Layer):\n         if self.bias is not None:\n             store[\"1\"] = self.bias\n         if isinstance(self.dtype_policy, dtype_policies.QuantizedDTypePolicy):\n+            mode = self.dtype_policy.quantization_mode\n+            if mode == \"int8\":\n                 store[\"2\"] = kernel_scale\n+            elif mode == \"float8\":\n+                store[\"2\"] = self.inputs_scale\n+                store[\"3\"] = self.inputs_amax_history\n+                store[\"4\"] = self.kernel_scale\n+                store[\"5\"] = self.kernel_amax_history\n+                store[\"6\"] = self.outputs_grad_scale\n+                store[\"7\"] = self.outputs_grad_amax_history\n+            else:\n+                raise NotImplementedError(\n+                    self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode)\n+                )\n \n     def load_own_variables(self, store):\n         if not self.lora_enabled:\n@@ -269,7 +289,20 @@ class EinsumDense(Layer):\n         if self.bias is not None:\n             self.bias.assign(store[\"1\"])\n         if isinstance(self.dtype_policy, dtype_policies.QuantizedDTypePolicy):\n+            mode = self.dtype_policy.quantization_mode\n+            if mode == \"int8\":\n                 self.kernel_scale.assign(store[\"2\"])\n+            elif mode == \"float8\":\n+                self.inputs_scale.assign(store[\"2\"])\n+                self.inputs_amax_history.assign(store[\"3\"])\n+                self.kernel_scale.assign(store[\"4\"])\n+                self.kernel_amax_history.assign(store[\"5\"])\n+                self.outputs_grad_scale.assign(store[\"6\"])\n+                self.outputs_grad_amax_history.assign(store[\"7\"])\n+            else:\n+                raise NotImplementedError(\n+                    self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode)\n+                )\n         if self.lora_enabled:\n             self.lora_kernel_a.assign(ops.zeros(self.lora_kernel_a.shape))\n             self.lora_kernel_b.assign(ops.zeros(self.lora_kernel_b.shape))\n@@ -333,9 +366,16 @@ class EinsumDense(Layer):\n                 f\"Expected: {[v.name for v in all_vars]}\"\n             )\n \n-    \"\"\"Quantization-related methods\"\"\"\n+    \"\"\"Quantization-related (int8 and float8) methods\"\"\"\n+\n+    QUANTIZATION_MODE_ERROR_TEMPLATE = (\n+        f\"Invalid quantization mode. Expected one of \"\n+        f\"{dtype_policies.QUANTIZATION_MODES}. \"\n+        \"Received: quantization_mode={mode}\"\n+    )\n \n     def quantized_build(self, input_shape, mode):\n+        if mode == \"int8\":\n             shape_data = _analyze_einsum_string(\n                 self.equation,\n                 self.bias_axes,\n@@ -343,7 +383,20 @@ class EinsumDense(Layer):\n                 self.partial_output_shape,\n             )\n             kernel_shape, _, _ = shape_data\n-        if mode == \"int8\":\n+            self._int8_build(kernel_shape)\n+        elif mode == \"float8\":\n+            self._float8_build()\n+        else:\n+            raise NotImplementedError(\n+                self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode)\n+            )\n+\n+    def _int8_build(\n+        self,\n+        kernel_shape,\n+        kernel_initializer=\"zeros\",\n+        kernel_scale_initializer=\"ones\",\n+    ):\n         (\n             self._input_reduced_axes,\n             self._kernel_reduced_axes,\n@@ -360,7 +413,7 @@ class EinsumDense(Layer):\n         self._kernel = self.add_weight(\n             name=\"kernel\",\n             shape=kernel_shape,\n-                initializer=\"zeros\",\n+            initializer=kernel_initializer,\n             dtype=\"int8\",\n             trainable=False,\n         )\n@@ -375,11 +428,71 @@ class EinsumDense(Layer):\n         self.kernel_scale = self.add_weight(\n             name=\"kernel_scale\",\n             shape=kernel_scale_shape,\n-                initializer=\"ones\",\n+            initializer=kernel_scale_initializer,\n             trainable=False,\n         )\n \n+    def _float8_build(self):\n+        if not isinstance(\n+            self.dtype_policy, dtype_policies.QuantizedFloat8DTypePolicy\n+        ):\n+            raise TypeError(\n+                \"`self.dtype_policy` must be the type of \"\n+                f\"QuantizedFloat8DTypePolicy. Received {self.dtype_policy}\"\n+            )\n+        amax_history_length = self.dtype_policy.amax_history_length\n+        # We set `trainable=True` because we will use the gradients to overwrite\n+        # these variables\n+        scale_kwargs = {\n+            \"shape\": (),\n+            \"initializer\": \"ones\",\n+            \"dtype\": \"float32\",  # Always be float32\n+            \"trainable\": True,\n+            \"autocast\": False,\n+        }\n+        amax_history_kwargs = {\n+            \"shape\": (amax_history_length,),\n+            \"initializer\": \"zeros\",\n+            \"dtype\": \"float32\",  # Always be float32\n+            \"trainable\": True,\n+            \"autocast\": False,\n+        }\n+        self.inputs_scale = self.add_weight(name=\"inputs_scale\", **scale_kwargs)\n+        self.inputs_amax_history = self.add_weight(\n+            name=\"inputs_amax_history\", **amax_history_kwargs\n+        )\n+        self.kernel_scale = self.add_weight(name=\"kernel_scale\", **scale_kwargs)\n+        self.kernel_amax_history = self.add_weight(\n+            name=\"kernel_amax_history\", **amax_history_kwargs\n+        )\n+        self.outputs_grad_scale = self.add_weight(\n+            name=\"outputs_grad_scale\", **scale_kwargs\n+        )\n+        self.outputs_grad_amax_history = self.add_weight(\n+            name=\"outputs_grad_amax_history\", **amax_history_kwargs\n+        )\n+        # We need to set `overwrite_with_gradient=True` to instruct the\n+        # optimizer to directly overwrite these variables with their computed\n+        # gradients during training\n+        self.inputs_scale.overwrite_with_gradient = True\n+        self.inputs_amax_history.overwrite_with_gradient = True\n+        self.kernel_scale.overwrite_with_gradient = True\n+        self.kernel_amax_history.overwrite_with_gradient = True\n+        self.outputs_grad_scale.overwrite_with_gradient = True\n+        self.outputs_grad_amax_history.overwrite_with_gradient = True\n+\n     def quantized_call(self, inputs):\n+        if self.dtype_policy.quantization_mode == \"int8\":\n+            return self._int8_call(inputs)\n+        elif self.dtype_policy.quantization_mode == \"float8\":\n+            return self._float8_call(inputs)\n+        else:\n+            mode = self.dtype_policy.quantization_mode\n+            raise NotImplementedError(\n+                self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode)\n+            )\n+\n+    def _int8_call(self, inputs):\n         @ops.custom_gradient\n         def einsum_with_inputs_gradient(inputs, kernel, kernel_scale):\n             def grad_fn(*args, upstream=None):\n@@ -442,6 +555,94 @@ class EinsumDense(Layer):\n             x = self.activation(x)\n         return x\n \n+    def _float8_call(self, inputs):\n+        if self.lora_enabled:\n+            raise NotImplementedError(\n+                \"Currently, `_float8_call` doesn't support LoRA\"\n+            )\n+\n+        @ops.custom_gradient\n+        def quantized_dequantize_inputs(inputs, scale, amax_history):\n+            new_scale = quantizers.compute_float8_scale(\n+                ops.max(amax_history, axis=0),\n+                scale,\n+                ops.cast(\n+                    float(ml_dtypes.finfo(\"float8_e4m3fn\").max), \"float32\"\n+                ),\n+            )\n+            qdq_inputs = quantizers.quantize_and_dequantize(\n+                inputs, scale, \"float8_e4m3fn\", self.compute_dtype\n+            )\n+            new_amax_history = quantizers.compute_float8_amax_history(\n+                inputs, amax_history\n+            )\n+\n+            def grad(*args, upstream=None, variables=None):\n+                if upstream is None:\n+                    (upstream,) = args\n+                return upstream, new_scale, new_amax_history\n+\n+            return qdq_inputs, grad\n+\n+        @ops.custom_gradient\n+        def quantized_dequantize_outputs(outputs, scale, amax_history):\n+            \"\"\"Quantize-dequantize the output gradient but not the output.\"\"\"\n+\n+            def grad(*args, upstream=None, variables=None):\n+                if upstream is None:\n+                    (upstream,) = args\n+                new_scale = quantizers.compute_float8_scale(\n+                    ops.max(amax_history, axis=0),\n+                    scale,\n+                    ops.cast(\n+                        float(ml_dtypes.finfo(\"float8_e5m2\").max), \"float32\"\n+                    ),\n+                )\n+                qdq_upstream = quantizers.quantize_and_dequantize(\n+                    upstream, scale, \"float8_e5m2\", self.compute_dtype\n+                )\n+                new_amax_history = quantizers.compute_float8_amax_history(\n+                    upstream, amax_history\n+                )\n+                return qdq_upstream, new_scale, new_amax_history\n+\n+            return outputs, grad\n+\n+        x = ops.einsum(\n+            self.equation,\n+            quantized_dequantize_inputs(\n+                inputs,\n+                ops.convert_to_tensor(self.inputs_scale),\n+                ops.convert_to_tensor(self.inputs_amax_history),\n+            ),\n+            quantized_dequantize_inputs(\n+                ops.convert_to_tensor(self._kernel),\n+                ops.convert_to_tensor(self.kernel_scale),\n+                ops.convert_to_tensor(self.kernel_amax_history),\n+            ),\n+        )\n+        # `quantized_dequantize_outputs` is placed immediately after\n+        # `ops.einsum` for the sake of pattern matching in gemm_rewrite. That\n+        # way, the qdq will be adjacent to the corresponding einsum_bprop in the\n+        # bprop.\n+        x = quantized_dequantize_outputs(\n+            x,\n+            ops.convert_to_tensor(self.outputs_grad_scale),\n+            ops.convert_to_tensor(self.outputs_grad_amax_history),\n+        )\n+        if self.bias is not None:\n+            # Under non-mixed precision cases, F32 bias has to be converted to\n+            # BF16 first to get the biasAdd fusion support. ref. PR\n+            # https://github.com/tensorflow/tensorflow/pull/60306\n+            bias = self.bias\n+            if self.dtype_policy.compute_dtype == \"float32\":\n+                bias_bf16 = ops.cast(bias, \"bfloat16\")\n+                bias = ops.cast(bias_bf16, bias.dtype)\n+            x = ops.add(x, bias)\n+        if self.activation is not None:\n+            x = self.activation(x)\n+        return x\n+\n     def quantize(self, mode):\n         import gc\n \n@@ -452,6 +653,17 @@ class EinsumDense(Layer):\n                 \"method implemented.\"\n             )\n         self._check_quantize_args(mode, self.compute_dtype)\n+\n+        # Set new dtype policy\n+        if not isinstance(\n+            self.dtype_policy, dtype_policies.QuantizedDTypePolicy\n+        ):\n+            quantized_dtype = f\"{mode}_from_{self.dtype_policy.name}\"\n+            # We set the internal `self._dtype_policy` instead of using the\n+            # setter to avoid double `quantize` call\n+            self._dtype_policy = dtype_policies.get(quantized_dtype)\n+\n+        self._tracker.unlock()\n         if mode == \"int8\":\n             if backend.standardize_dtype(self._kernel.dtype) == \"int8\":\n                 raise ValueError(\"`quantize` can only be done once per layer.\")\n@@ -486,35 +698,25 @@ class EinsumDense(Layer):\n                 kernel_scale = ops.squeeze(\n                     kernel_scale, axis=self._kernel_squeeze_axes\n                 )\n-            self._tracker.unlock()\n             self._untrack_variable(self._kernel)\n             kernel_shape = self._kernel.shape\n             del self._kernel\n-            self._kernel = self.add_weight(\n-                name=\"kernel\",\n-                shape=kernel_shape,\n-                # Prevent adding a large constant to the computation graph\n-                initializer=lambda shape, dtype: kernel_value,\n-                dtype=\"int8\",\n-                trainable=False,\n+            # Utilize a lambda expression as an initializer to prevent adding a\n+            # large constant to the computation graph.\n+            self._int8_build(\n+                kernel_shape,\n+                lambda shape, dtype: kernel_value,\n+                lambda shape, dtype: kernel_scale,\n             )\n-            self.kernel_scale = self.add_weight(\n-                name=\"kernel_scale\",\n-                shape=kernel_scale.shape,\n-                # Prevent adding a large constant to the computation graph\n-                initializer=lambda shape, dtype: kernel_scale,\n-                trainable=False,\n+        elif mode == \"float8\":\n+            if hasattr(self, \"inputs_amax_history\"):\n+                raise ValueError(\"`quantize` can only be done once per layer.\")\n+            self._float8_build()\n+        else:\n+            raise NotImplementedError(\n+                self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode)\n             )\n         self._tracker.lock()\n-        else:\n-            NotImplementedError()\n-\n-        # Set new dtype policy\n-        if not isinstance(\n-            self.dtype_policy, dtype_policies.QuantizedDTypePolicy\n-        ):\n-            quantized_dtype = f\"{mode}_from_{self.dtype_policy.name}\"\n-            self.dtype_policy = dtype_policies.get(quantized_dtype)\n \n         # Release memory manually because sometimes the backend doesn't\n         gc.collect()\n\n@@ -9,6 +9,8 @@ from keras import constraints\n from keras import layers\n from keras import models\n from keras import ops\n+from keras import optimizers\n+from keras import random\n from keras import saving\n from keras import testing\n from keras.export import export_lib\n@@ -378,6 +380,8 @@ class EinsumDenseTest(testing.TestCase, parameterized.TestCase):\n             supports_masking=False,\n         )\n \n+    \"\"\"Test quantization-related (int8 and float8) methods\"\"\"\n+\n     @pytest.mark.skipif(\n         backend.backend() == \"numpy\",\n         reason=f\"{backend.backend()} does not support ops.custom_gradient.\",\n@@ -465,8 +469,73 @@ class EinsumDenseTest(testing.TestCase, parameterized.TestCase):\n             backend.standardize_dtype(layer.kernel_scale.dtype), \"float32\"\n         )\n \n+    @parameterized.named_parameters(\n+        (\"int8\", \"int8\"),\n+        (\"float8\", \"float8\"),\n+    )\n+    def test_quantize_on_unbuilt_layer(self, mode):\n+        layer = layers.EinsumDense(\n+            equation=\"ab,bcd->acd\",\n+            output_shape=(8, 32),\n+            bias_axes=\"d\",\n+        )\n+        with self.assertRaisesRegex(\n+            ValueError, \"Cannot quantize a layer that isn't yet built.\"\n+        ):\n+            layer.quantize(mode)\n+\n+    @parameterized.named_parameters(\n+        (\"int8\", \"int8\"),\n+        (\"float8\", \"float8\"),\n+    )\n+    def test_quantize_on_subclass(self, mode):\n+        class MyEinsumDense(layers.EinsumDense):\n+            pass\n+\n+        layer = MyEinsumDense(\n+            equation=\"ab,bcd->acd\",\n+            output_shape=(8, 32),\n+            bias_axes=\"d\",\n+        )\n+        layer.build((None, 3))\n+        with self.assertRaises(NotImplementedError):\n+            layer.quantize(mode)\n+\n+    @parameterized.named_parameters(\n+        (\"int8\", \"int8\"),\n+        (\"float8\", \"float8\"),\n+    )\n+    def test_quantize_when_already_quantized(self, mode):\n+        layer = layers.EinsumDense(\n+            equation=\"ab,bcd->acd\",\n+            output_shape=(8, 32),\n+            bias_axes=\"d\",\n+        )\n+        layer.build((None, 3))\n+        layer.quantize(mode)\n+        with self.assertRaisesRegex(\n+            ValueError, \"`quantize` can only be done once per layer.\"\n+        ):\n+            layer.quantize(mode)\n+\n+    @parameterized.named_parameters(\n+        (\"int8\", \"int8_from_float32\", 3),\n+        (\"float8\", \"float8_from_float32\", 8),\n+    )\n+    def test_quantize_by_setting_dtype_policy(\n+        self, policy, expected_num_variables\n+    ):\n+        layer = layers.EinsumDense(\n+            equation=\"ab,bcd->acd\",\n+            output_shape=(8, 32),\n+            bias_axes=\"d\",\n+        )\n+        layer.build((None, 3))\n+        layer.dtype_policy = policy\n+        self.assertLen(layer.variables, expected_num_variables)\n+\n     @pytest.mark.requires_trainable_backend\n-    def test_quantize_dtype_argument(self):\n+    def test_quantize_int8_dtype_argument(self):\n         self.run_layer_test(\n             layers.EinsumDense,\n             init_kwargs={\n@@ -484,45 +553,8 @@ class EinsumDenseTest(testing.TestCase, parameterized.TestCase):\n             supports_masking=False,\n         )\n \n-    def test_quantize_on_unbuilt_layer(self):\n-        layer = layers.EinsumDense(\n-            equation=\"ab,bcd->acd\",\n-            output_shape=(8, 32),\n-            bias_axes=\"d\",\n-        )\n-        with self.assertRaisesRegex(\n-            ValueError, \"Cannot quantize a layer that isn't yet built.\"\n-        ):\n-            layer.quantize(\"int8\")\n-\n-    def test_quantize_on_subclass(self):\n-        class MyEinsumDense(layers.EinsumDense):\n-            pass\n-\n-        layer = MyEinsumDense(\n-            equation=\"ab,bcd->acd\",\n-            output_shape=(8, 32),\n-            bias_axes=\"d\",\n-        )\n-        layer.build((None, 3))\n-        with self.assertRaises(NotImplementedError):\n-            layer.quantize(\"int8\")\n-\n-    def test_quantize_when_already_quantized(self):\n-        layer = layers.EinsumDense(\n-            equation=\"ab,bcd->acd\",\n-            output_shape=(8, 32),\n-            bias_axes=\"d\",\n-        )\n-        layer.build((None, 3))\n-        layer.quantize(\"int8\")\n-        with self.assertRaisesRegex(\n-            ValueError, \"`quantize` can only be done once per layer.\"\n-        ):\n-            layer.quantize(\"int8\")\n-\n     @pytest.mark.requires_trainable_backend\n-    def test_quantize_when_lora_enabled(self):\n+    def test_quantize_int8_when_lora_enabled(self):\n         config = dict(\n             equation=\"ab,bcd->acd\",\n             output_shape=(8, 32),\n@@ -603,3 +635,197 @@ class EinsumDenseTest(testing.TestCase, parameterized.TestCase):\n                 reloaded_layer.non_trainable_weights,\n                 len(model.non_trainable_weights),\n             )\n+\n+    @pytest.mark.requires_trainable_backend\n+    def test_quantize_float8_dtype_argument(self):\n+        self.run_layer_test(\n+            layers.EinsumDense,\n+            init_kwargs={\n+                \"equation\": \"ab,bcd->acd\",\n+                \"output_shape\": (8, 32),\n+                \"bias_axes\": \"d\",\n+                \"dtype\": \"float8_from_mixed_bfloat16\",\n+            },\n+            input_shape=(2, 3),\n+            expected_output_shape=(2, 8, 32),\n+            expected_num_trainable_weights=8,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=False,\n+        )\n+\n+    @pytest.mark.requires_trainable_backend\n+    def test_quantize_float8(self):\n+        import ml_dtypes\n+\n+        from keras import quantizers\n+\n+        layer = layers.EinsumDense(\n+            \"ab,bc->ac\",\n+            output_shape=[32],\n+            bias_axes=\"c\",\n+        )\n+        layer.build((None, 16))\n+        layer.quantize(\"float8\")\n+        optimizer = optimizers.AdamW(learning_rate=0.1)\n+        optimizer.build(layer.trainable_variables)\n+\n+        def loss_fn(x, dy):\n+            y = layer(x, training=True)\n+            loss = y * ops.cast(dy, y.dtype)\n+            return ops.sum(loss)\n+\n+        if backend.backend() == \"tensorflow\":\n+            import tensorflow as tf\n+\n+            @tf.function(jit_compile=True)\n+            def train_one_step(x, dy):\n+                with tf.GradientTape() as tape:\n+                    loss = loss_fn(x, dy)\n+                grads = tape.gradient(loss, layer.trainable_variables)\n+                optimizer.apply(grads, layer.trainable_variables)\n+\n+        elif backend.backend() == \"jax\":\n+            import jax\n+\n+            def stateless_loss_fn(trainable_variables, x, dy):\n+                y = layer.stateless_call(trainable_variables, [], x)[0]\n+                loss = y * ops.cast(dy, y.dtype)\n+                return ops.sum(loss)\n+\n+            grad_fn = jax.jit(jax.grad(stateless_loss_fn))\n+\n+            def train_one_step(x, dy):\n+                trainable_variables = [\n+                    v.value for v in layer.trainable_variables\n+                ]\n+                optimizer_variables = [v.value for v in optimizer.variables]\n+                grads = grad_fn(trainable_variables, x, dy)\n+                trainable_variables, optimizer_variables = (\n+                    optimizer.stateless_apply(\n+                        optimizer_variables, grads, trainable_variables\n+                    )\n+                )\n+                for variable, value in zip(\n+                    layer.trainable_variables, trainable_variables\n+                ):\n+                    variable.assign(value)\n+                for variable, value in zip(\n+                    optimizer.variables, optimizer_variables\n+                ):\n+                    variable.assign(value)\n+\n+        elif backend.backend() == \"torch\":\n+\n+            def train_one_step(x, dy):\n+                layer.zero_grad()\n+                loss = loss_fn(x, dy)\n+                loss.backward()\n+                grads = [v.value.grad for v in layer.trainable_variables]\n+                optimizer.apply(grads, layer.trainable_variables)\n+\n+        scale_x, amax_history_x = ops.ones(()), ops.zeros((1024,))\n+        scale_k, amax_history_k = ops.ones(()), ops.zeros((1024,))\n+        scale_g, amax_history_g = ops.ones(()), ops.zeros((1024,))\n+        e4m3_max = ops.cast(\n+            float(ml_dtypes.finfo(\"float8_e4m3fn\").max), \"float32\"\n+        )\n+        e5m2_max = ops.cast(\n+            float(ml_dtypes.finfo(\"float8_e5m2\").max), \"float32\"\n+        )\n+\n+        for _ in range(3):\n+            x = random.normal((16, 16), dtype=\"float32\")\n+            g = random.normal((16, 32), dtype=\"float32\")\n+            k = ops.convert_to_tensor(layer._kernel)\n+\n+            # Manually compute the expected amax history and scaling factors.\n+            amax_from_history_x = ops.max(amax_history_x)\n+            amax_from_history_k = ops.max(amax_history_k)\n+            amax_from_history_g = ops.max(amax_history_g)\n+            scale_x = quantizers.compute_float8_scale(\n+                amax_from_history_x, scale_x, e4m3_max\n+            )\n+            scale_k = quantizers.compute_float8_scale(\n+                amax_from_history_k, scale_k, e4m3_max\n+            )\n+            scale_g = quantizers.compute_float8_scale(\n+                amax_from_history_g, scale_g, e5m2_max\n+            )\n+            amax_history_x = quantizers.compute_float8_amax_history(\n+                x, amax_history_x\n+            )\n+            amax_history_k = quantizers.compute_float8_amax_history(\n+                k, amax_history_k\n+            )\n+            amax_history_g = quantizers.compute_float8_amax_history(\n+                g, amax_history_g\n+            )\n+\n+            train_one_step(x, g)\n+\n+            self.assertAllClose(layer.inputs_amax_history, amax_history_x)\n+            self.assertAllClose(layer.kernel_amax_history, amax_history_k)\n+            self.assertAllClose(layer.outputs_grad_amax_history, amax_history_g)\n+            self.assertAllClose(layer.inputs_scale, scale_x)\n+            self.assertAllClose(layer.kernel_scale, scale_k)\n+            self.assertAllClose(layer.outputs_grad_scale, scale_g)\n+\n+    @pytest.mark.requires_trainable_backend\n+    def test_quantize_float8_fitting(self):\n+        config = dict(\n+            equation=\"ab,bcd->acd\",\n+            output_shape=(8, 32),\n+            bias_axes=\"d\",\n+        )\n+        layer = layers.EinsumDense(**config)\n+        layer.build((None, 3))\n+        layer.quantize(\"float8\")\n+        self.assertLen(layer.trainable_weights, 8)\n+        self.assertLen(layer.non_trainable_weights, 0)\n+\n+        # Try calling fit()\n+        x = np.random.random((64, 3))\n+        y = np.random.random((64, 8, 32))\n+        model = models.Sequential([layer])\n+        model.compile(optimizer=\"sgd\", loss=\"mse\")\n+        model.fit(x, y, epochs=2)\n+\n+        # Try saving and reloading the model\n+        temp_filepath = os.path.join(\n+            self.get_temp_dir(), \"quantized_lora_model.keras\"\n+        )\n+        model.save(temp_filepath)\n+        new_model = saving.load_model(temp_filepath)\n+        self.assertAllClose(model.predict(x), new_model.predict(x))\n+\n+        # Try saving and reloading the model's weights only\n+        temp_filepath = os.path.join(\n+            self.get_temp_dir(), \"quantized_lora_model.weights.h5\"\n+        )\n+        model.save_weights(temp_filepath)\n+        new_model = models.Sequential([layers.EinsumDense(**config)])\n+        new_model.build((None, 3))\n+        new_model.quantize(\"float8\")\n+        new_model.load_weights(temp_filepath)\n+        self.assertAllClose(model.predict(x), new_model.predict(x))\n+\n+        # Test export and TFSMLayer reloading when using tensorflow backend\n+        if backend.backend() == \"tensorflow\":\n+            import tensorflow as tf\n+\n+            temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n+            ref_input = tf.random.normal((2, 3))\n+            ref_output = model(ref_input)\n+            export_lib.export_model(model, temp_filepath)\n+            reloaded_layer = export_lib.TFSMLayer(temp_filepath)\n+            self.assertAllClose(reloaded_layer(ref_input), ref_output)\n+            self.assertLen(reloaded_layer.weights, len(model.weights))\n+            self.assertLen(\n+                reloaded_layer.trainable_weights, len(model.trainable_weights)\n+            )\n+            self.assertLen(\n+                reloaded_layer.non_trainable_weights,\n+                len(model.non_trainable_weights),\n+            )\n\n@@ -111,11 +111,15 @@ class Embedding(Layer):\n     def build(self, input_shape=None):\n         if self.built:\n             return\n-        if isinstance(self.dtype_policy, dtype_policies.QuantizedDTypePolicy):\n+        # We use `self._dtype_policy` to check to avoid issues in torch dynamo\n+        is_quantized = isinstance(\n+            self._dtype_policy, dtype_policies.QuantizedDTypePolicy\n+        )\n+        if is_quantized:\n             self.quantized_build(\n                 input_shape, mode=self.dtype_policy.quantization_mode\n             )\n-        else:\n+        if not is_quantized or self.dtype_policy.quantization_mode != \"int8\":\n             self._embeddings = self.add_weight(\n                 shape=(self.input_dim, self.output_dim),\n                 initializer=self.embeddings_initializer,\n@@ -197,7 +201,13 @@ class Embedding(Layer):\n         )\n         store[\"0\"] = embeddings_value\n         if isinstance(self.dtype_policy, dtype_policies.QuantizedDTypePolicy):\n+            mode = self.dtype_policy.quantization_mode\n+            if mode == \"int8\":\n                 store[\"1\"] = embeddings_scale\n+            else:\n+                raise NotImplementedError(\n+                    self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode)\n+                )\n \n     def load_own_variables(self, store):\n         if not self.lora_enabled:\n@@ -209,7 +219,13 @@ class Embedding(Layer):\n         # default ordering will change after quantization\n         self._embeddings.assign(store[\"0\"])\n         if isinstance(self.dtype_policy, dtype_policies.QuantizedDTypePolicy):\n+            mode = self.dtype_policy.quantization_mode\n+            if mode == \"int8\":\n                 self.embeddings_scale.assign(store[\"1\"])\n+            else:\n+                raise NotImplementedError(\n+                    self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode)\n+                )\n         if self.lora_enabled:\n             self.lora_embeddings_a.assign(\n                 ops.zeros(self.lora_embeddings_a.shape)\n@@ -275,26 +291,51 @@ class Embedding(Layer):\n                 f\"Expected: {[v.name for v in all_vars]}\"\n             )\n \n-    \"\"\"Quantization-related methods\"\"\"\n+    \"\"\"Quantization-related (int8) methods\"\"\"\n+\n+    QUANTIZATION_MODE_ERROR_TEMPLATE = (\n+        \"Invalid quantization mode. Expected 'int8'. \"\n+        \"Received: quantization_mode={mode}\"\n+    )\n \n     def quantized_build(self, input_shape, mode):\n         if mode == \"int8\":\n+            self._int8_build()\n+        else:\n+            raise NotImplementedError(\n+                self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode)\n+            )\n+\n+    def _int8_build(\n+        self,\n+        embeddings_initializer=\"zeros\",\n+        embeddings_scale_initializer=\"ones\",\n+    ):\n         self.inputs_quantizer = quantizers.AbsMaxQuantizer(axis=-1)\n         self._embeddings = self.add_weight(\n             name=\"embeddings\",\n             shape=(self.input_dim, self.output_dim),\n-                initializer=\"zeros\",\n+            initializer=embeddings_initializer,\n             dtype=\"int8\",\n             trainable=False,\n         )\n         self.embeddings_scale = self.add_weight(\n             name=\"embeddings_scale\",\n             shape=(self.output_dim,),\n-                initializer=\"ones\",\n+            initializer=embeddings_scale_initializer,\n             trainable=False,\n         )\n \n     def quantized_call(self, inputs):\n+        if self.dtype_policy.quantization_mode == \"int8\":\n+            return self._int8_call(inputs)\n+        else:\n+            mode = self.dtype_policy.quantization_mode\n+            raise NotImplementedError(\n+                self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode)\n+            )\n+\n+    def _int8_call(self, inputs):\n         # We cannot update quantized self._embeddings, so the custom gradient is\n         # not needed\n         if backend.standardize_dtype(inputs.dtype) not in (\"int32\", \"int64\"):\n@@ -321,6 +362,17 @@ class Embedding(Layer):\n                 \"method implemented.\"\n             )\n         self._check_quantize_args(mode, self.compute_dtype)\n+\n+        # Set new dtype policy\n+        if not isinstance(\n+            self.dtype_policy, dtype_policies.QuantizedDTypePolicy\n+        ):\n+            quantized_dtype = f\"{mode}_from_{self.dtype_policy.name}\"\n+            # We set the internal `self._dtype_policy` instead of using the\n+            # setter to avoid double `quantize` call\n+            self._dtype_policy = dtype_policies.get(quantized_dtype)\n+\n+        self._tracker.unlock()\n         if mode == \"int8\":\n             if backend.standardize_dtype(self._embeddings.dtype) == \"int8\":\n                 raise ValueError(\"`quantize` can only be done once per layer.\")\n@@ -332,37 +384,19 @@ class Embedding(Layer):\n                 self._embeddings, axis=0\n             )\n             embeddings_scale = ops.squeeze(embeddings_scale, axis=0)\n-            self._tracker.unlock()\n             self._untrack_variable(self._embeddings)\n             del self._embeddings\n-            self._embeddings = self.add_weight(\n-                name=\"embeddings\",\n-                shape=(self.input_dim, self.output_dim),\n-                # Prevent adding a large constant to the computation graph\n-                initializer=lambda shape, dtype: embeddings_value,\n-                dtype=\"int8\",\n-                trainable=False,\n+            # Utilize a lambda expression as an initializer to prevent adding a\n+            # large constant to the computation graph.\n+            self._int8_build(\n+                lambda shape, dtype: embeddings_value,\n+                lambda shape, dtype: embeddings_scale,\n             )\n-            self.embeddings_scale = self.add_weight(\n-                name=\"embeddings_scale\",\n-                shape=(self.output_dim,),\n-                # Prevent adding a large constant to the computation graph\n-                initializer=lambda shape, dtype: embeddings_scale,\n-                trainable=False,\n+        else:\n+            raise NotImplementedError(\n+                self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode)\n             )\n         self._tracker.lock()\n-        else:\n-            NotImplementedError(\n-                \"Invalid quantization mode. Expected 'int8'. \"\n-                f\"Received: mode={mode}\"\n-            )\n-\n-        # Set new dtype policy\n-        if not isinstance(\n-            self.dtype_policy, dtype_policies.QuantizedDTypePolicy\n-        ):\n-            quantized_dtype = f\"{mode}_from_{self.dtype_policy.name}\"\n-            self.dtype_policy = dtype_policies.get(quantized_dtype)\n \n         # Release memory manually because sometimes the backend doesn't\n         gc.collect()\n\n@@ -2,6 +2,7 @@ import os\n \n import numpy as np\n import pytest\n+from absl.testing import parameterized\n \n from keras import backend\n from keras import constraints\n@@ -13,7 +14,7 @@ from keras.export import export_lib\n from keras.testing import test_case\n \n \n-class EmbeddingTest(test_case.TestCase):\n+class EmbeddingTest(test_case.TestCase, parameterized.TestCase):\n     @pytest.mark.requires_trainable_backend\n     def test_embedding_basics(self):\n         self.run_layer_test(\n@@ -317,6 +318,17 @@ class EmbeddingTest(test_case.TestCase):\n         ):\n             layer.quantize(\"int8\")\n \n+    @parameterized.named_parameters(\n+        (\"int8\", \"int8_from_float32\", 2),\n+    )\n+    def test_quantize_by_setting_dtype_policy(\n+        self, policy, expected_num_variables\n+    ):\n+        layer = layers.Embedding(10, 16)\n+        layer.build()\n+        layer.dtype_policy = policy\n+        self.assertLen(layer.variables, expected_num_variables)\n+\n     @pytest.mark.requires_trainable_backend\n     def test_quantize_when_lora_enabled(self):\n         layer = layers.Embedding(10, 16)\n\n@@ -668,6 +668,17 @@ class Layer(BackendLayer, Operation):\n                 )\n             variable.assign(value)\n \n+    @property\n+    def dtype_policy(self):\n+        return self._dtype_policy\n+\n+    @dtype_policy.setter\n+    def dtype_policy(self, value):\n+        self._dtype_policy = dtype_policies.get(value)\n+        if isinstance(self._dtype_policy, dtype_policies.QuantizedDTypePolicy):\n+            if self.built:\n+                self.quantize(self._dtype_policy.quantization_mode)\n+\n     @property\n     def dtype(self):\n         \"\"\"Alias of `layer.variable_dtype`.\"\"\"\n@@ -1127,9 +1138,11 @@ class Layer(BackendLayer, Operation):\n                 f\"Layer '{self.name}' (of type '{self.__class__.__name__}') \"\n                 \"is not built yet.\"\n             )\n-        if mode not in (\"int8\",):\n+        if mode not in dtype_policies.QUANTIZATION_MODES:\n             raise ValueError(\n-                f\"`quantize` must be one of ('int8'). Received: mode={mode}\"\n+                \"Invalid quantization mode. \"\n+                f\"Expected one of {dtype_policies.QUANTIZATION_MODES}. \"\n+                f\"Received: mode={mode}\"\n             )\n         if mode == \"int8\" and compute_dtype == \"float16\":\n             raise ValueError(\n\n@@ -2,6 +2,7 @@ import numpy as np\n import pytest\n \n from keras import backend\n+from keras import dtype_policies\n from keras import layers\n from keras import metrics\n from keras import models\n@@ -925,3 +926,16 @@ class LayerTest(testing.TestCase):\n         self.assertLen(layer.trainable_weights, 2)\n         self.assertLen(model.trainable_weights, 2)\n         self.assertLen(model.non_trainable_weights, 0)\n+\n+    def test_dtype_policy_setter(self):\n+        layer = layers.Dense(2)\n+        # Set by string\n+        layer.dtype_policy = \"mixed_bfloat16\"\n+        self.assertEqual(layer.dtype_policy.name, \"mixed_bfloat16\")\n+        self.assertEqual(layer.dtype_policy.compute_dtype, \"bfloat16\")\n+        self.assertEqual(layer.dtype_policy.variable_dtype, \"float32\")\n+        # Set by FloatDTypePolicy\n+        layer.dtype_policy = dtype_policies.FloatDTypePolicy(\"mixed_float16\")\n+        self.assertEqual(layer.dtype_policy.name, \"mixed_float16\")\n+        self.assertEqual(layer.dtype_policy.compute_dtype, \"float16\")\n+        self.assertEqual(layer.dtype_policy.variable_dtype, \"float32\")\n\n@@ -357,14 +357,16 @@ class Model(Trainer, base_trainer.Trainer, Layer):\n             mode: The mode of the quantization. Only 'int8' is supported at this\n                 time.\n         \"\"\"\n+        from keras.dtype_policies import QUANTIZATION_MODES\n+\n         if not self.built:\n             raise ValueError(\n                 \"The model must be built first before calling `quantize()`.\"\n             )\n-        if mode not in (\"int8\",):\n+        if mode not in QUANTIZATION_MODES:\n             raise ValueError(\n-                \"Invalid quantization mode. Expected 'int8'. \"\n-                f\"Received: mode={mode}\"\n+                \"Invalid quantization mode. \"\n+                f\"Expected one of {QUANTIZATION_MODES}. Received: mode={mode}\"\n             )\n         mode_changed = False\n         for layer in self._flatten_layers():\n\n@@ -563,19 +563,29 @@ class ModelTest(testing.TestCase, parameterized.TestCase):\n         ):\n             model.fit(x, (y1, y2), batch_size=2, epochs=1, verbose=0)\n \n-    def test_quantize(self):\n+    @parameterized.named_parameters(\n+        (\"int8\", \"int8\"),\n+        (\"float8\", \"float8\"),\n+    )\n+    def test_quantize(self, mode):\n         model = _get_model()\n         x1 = np.random.rand(2, 3)\n         x2 = np.random.rand(2, 3)\n-        model.quantize(\"int8\")\n+        model.quantize(mode)\n         _ = model((x1, x2))\n \n         for layer in model._flatten_layers():\n             if isinstance(layer, (layers.Dense, layers.EinsumDense)):\n-                self.assertEqual(layer.dtype_policy.name, \"int8_from_float32\")\n-                self.assertEqual(layer.dtype_policy.quantization_mode, \"int8\")\n+                self.assertEqual(\n+                    layer.dtype_policy.name, f\"{mode}_from_float32\"\n+                )\n+                self.assertEqual(layer.dtype_policy.quantization_mode, mode)\n \n-    def test_quantize_unbuilt(self):\n+    @parameterized.named_parameters(\n+        (\"int8\", \"int8\"),\n+        (\"float8\", \"float8\"),\n+    )\n+    def test_quantize_unbuilt(self, mode):\n         class MyModel(Model):\n             def __init__(self):\n                 super().__init__()\n@@ -592,20 +602,24 @@ class ModelTest(testing.TestCase, parameterized.TestCase):\n         with self.assertRaisesRegex(\n             ValueError, \"The model must be built first before calling\"\n         ):\n-            model.quantize(\"int8\")\n+            model.quantize(mode)\n \n         x = np.random.rand(2, 3)\n         _ = model(x)\n-        model.quantize(\"int8\")\n+        model.quantize(mode)\n \n     def test_quantize_invalid_args(self):\n         model = _get_model()\n         with self.assertRaisesRegex(\n-            ValueError, \"Invalid quantization mode. Expected 'int8'.\"\n+            ValueError, \"Invalid quantization mode. Expected one of\"\n         ):\n             model.quantize(\"abc\")\n \n-    def test_quantize_nested_model(self):\n+    @parameterized.named_parameters(\n+        (\"int8\", \"int8\"),\n+        (\"float8\", \"float8\"),\n+    )\n+    def test_quantize_nested_model(self, mode):\n         class NestedLayer(layers.Layer):\n             def __init__(self, units):\n                 super().__init__()\n@@ -631,8 +645,9 @@ class ModelTest(testing.TestCase, parameterized.TestCase):\n         inputs = layers.Input([3])\n         outputs = DoubleNestedLayer(8)(inputs)\n         model = Model(inputs, outputs)\n-        model.quantize(\"int8\")\n+        model.quantize(mode)\n \n+        if mode == \"int8\":\n             kernel_count = 0\n             for weight in model.weights:\n                 if weight.name == \"kernel\":\n@@ -641,3 +656,6 @@ class ModelTest(testing.TestCase, parameterized.TestCase):\n                         backend.standardize_dtype(weight.dtype), \"int8\"\n                     )\n             self.assertEqual(kernel_count, 3)\n+        if mode == \"float8\":\n+            # kernel + bias + scale * 3 + amax_history * 3 == 8\n+            self.assertEqual(len(model.weights), 3 * 8)\n\n@@ -23,7 +23,7 @@ class Operation:\n                 \"cannot contain character `/`. \"\n                 f\"Received: name={name} (of type {type(name)})\"\n             )\n-        self.dtype_policy = dtype_policies.get(dtype)\n+        self._dtype_policy = dtype_policies.get(dtype)\n         self.name = name\n         self._inbound_nodes = []\n         self._outbound_nodes = []\n@@ -36,7 +36,7 @@ class Operation:\n                 call_fn = self.symbolic_call\n             else:\n                 if isinstance(\n-                    self.dtype_policy, dtype_policies.QuantizedDTypePolicy\n+                    self._dtype_policy, dtype_policies.QuantizedDTypePolicy\n                 ):\n                     call_fn = self.quantized_call\n                 else:\n@@ -50,7 +50,7 @@ class Operation:\n         # Plain flow.\n         if any_symbolic_tensors(args, kwargs):\n             return self.symbolic_call(*args, **kwargs)\n-        if isinstance(self.dtype_policy, dtype_policies.QuantizedDTypePolicy):\n+        if isinstance(self._dtype_policy, dtype_policies.QuantizedDTypePolicy):\n             return self.quantized_call(*args, **kwargs)\n         else:\n             return self.call(*args, **kwargs)\n\n@@ -324,6 +324,14 @@ class BaseOptimizer:\n             self._check_variables_are_known(trainable_variables)\n \n         with backend.name_scope(self.name, caller=self):\n+            # Overwrite targeted variables directly with their gradients if\n+            # their `overwrite_with_gradient` is set.\n+            grads, trainable_variables = (\n+                self._overwrite_variables_directly_with_gradients(\n+                    grads, trainable_variables\n+                )\n+            )\n+\n             # Filter empty gradients.\n             grads, trainable_variables = self._filter_empty_gradients(\n                 grads, trainable_variables\n@@ -583,6 +591,36 @@ class BaseOptimizer:\n             return self._learning_rate()\n         return self._learning_rate\n \n+    def _overwrite_variables_directly_with_gradients(self, grads, vars):\n+        \"\"\"Overwrite the variables directly by their gradients.\n+\n+        This method is designed for a special case where we want to overwrite\n+        the variable directly with its computed gradient. For example, in float8\n+        training, new `scale` and `amax_history` are computed as gradients, and\n+        we want to overwrite them directly instead of following the typical\n+        procedure such as gradient descent with a learning rate, gradient\n+        clipping and weight decaying.\n+\n+        After the update, the processed pairs will be filtered out.\n+        \"\"\"\n+        # Shortcut for `tf.Variable` because it doesn't have a\n+        # `overwrite_with_gradient` attr\n+        if not hasattr(vars[0], \"overwrite_with_gradient\"):\n+            return grads, vars\n+\n+        # Shallow copies\n+        filtered_grads = list(grads)\n+        filtered_vars = list(vars)\n+\n+        # Iterate from right to left for safe popping\n+        for i in range(len(filtered_grads) - 1, -1, -1):\n+            g, v = filtered_grads[i], filtered_vars[i]\n+            if v.overwrite_with_gradient:\n+                v.assign(g)\n+                filtered_grads.pop(i)\n+                filtered_vars.pop(i)\n+        return filtered_grads, filtered_vars\n+\n     def _filter_empty_gradients(self, grads, vars):\n         filtered_grads = list(grads)\n         filtered_vars = list(vars)\n\n@@ -247,12 +247,26 @@ class OptimizerTest(testing.TestCase):\n     def test_callable_learning_rate(self):\n         v = backend.Variable([[1.0, 2.0], [3.0, 4.0]])\n         grads = backend.convert_to_tensor([[1.0, 1.0], [1.0, 1.0]])\n-        optimizer = optimizers.AdamW(learning_rate=lambda: 0.0001)\n+        optimizer = optimizers.SGD(learning_rate=lambda: 0.1)\n         self.assertAllClose(optimizer.iterations, 0)\n         optimizer.apply_gradients([(grads, v)])\n-        self.assertAllClose(v, [[1.0, 2.0], [3.0, 4.0]], atol=1e-4)\n+        self.assertAllClose(v, [[0.9, 1.9], [2.9, 3.9]])\n         self.assertAllClose(optimizer.iterations, 1)\n \n+    def test_overwrite_with_gradient(self):\n+        v = backend.Variable([[1.0, 2.0], [3.0, 4.0]])\n+        v.overwrite_with_gradient = True\n+        v2 = backend.Variable([[1.0, 2.0], [3.0, 4.0]])\n+        grads = backend.convert_to_tensor([[1.0, 1.0], [1.0, 1.0]])\n+        grads2 = backend.convert_to_tensor([[1.0, 1.0], [1.0, 1.0]])\n+\n+        optimizer = optimizers.SGD(learning_rate=1.0)\n+        optimizer.apply_gradients([(grads, v), (grads2, v2)])\n+\n+        # `v` is overwritten by its gradient but `v2` is updated normally\n+        self.assertAllClose(v, [[1.0, 1.0], [1.0, 1.0]])\n+        self.assertAllClose(v2, [[0.0, 1.0], [2.0, 3.0]])\n+\n     def test_setting_lr_to_callable_untracks_lr_var(self):\n         adam = optimizers.Adam(learning_rate=0.001)\n         self.assertLen(adam.variables, 2)\n\n@@ -4,6 +4,9 @@ from keras.api_export import keras_export\n from keras.quantizers.quantizers import AbsMaxQuantizer\n from keras.quantizers.quantizers import Quantizer\n from keras.quantizers.quantizers import abs_max_quantize\n+from keras.quantizers.quantizers import compute_float8_amax_history\n+from keras.quantizers.quantizers import compute_float8_scale\n+from keras.quantizers.quantizers import quantize_and_dequantize\n from keras.saving import serialization_lib\n from keras.utils.naming import to_snake_case\n \n\n@@ -1,7 +1,11 @@\n+import ml_dtypes\n+\n from keras import backend\n from keras import ops\n from keras.api_export import keras_export\n \n+\"\"\"Int8-related classes and methods\"\"\"\n+\n \n @keras_export([\"keras.Quantizer\", \"keras.quantizers.Quantizer\"])\n class Quantizer:\n@@ -100,3 +104,45 @@ class AbsMaxQuantizer(Quantizer):\n             \"epsilon\": self.epsilon,\n             \"output_dtype\": self.output_dtype,\n         }\n+\n+\n+\"\"\"Float8-related methods\"\"\"\n+\n+\n+@keras_export([\"keras.quantizers.compute_float8_scale\"])\n+def compute_float8_scale(amax, scale, dtype_max, margin=0):\n+    # The algorithm for computing the new scale is sourced from\n+    # https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/api/jax.html#transformer_engine.jax.update_fp8_metas\n+    # wherein the `original_scale` corresponds to the reciprocal of the\n+    # `scale` passed in this function.\n+    scale = ops.reciprocal(scale)\n+    sf = ops.divide(ops.divide(dtype_max, amax), 2**margin)\n+    sf = ops.where(amax > 0.0, sf, scale)\n+    sf = ops.where(ops.isfinite(amax), sf, scale)\n+    return ops.reciprocal(sf)\n+\n+\n+@keras_export([\"keras.quantizers.compute_float8_amax_history\"])\n+def compute_float8_amax_history(x, amax_history):\n+    amax_update = ops.cast(ops.max(ops.abs(x)), amax_history.dtype)\n+    new_amax_history = ops.scatter_update(\n+        ops.roll(amax_history, shift=-1),\n+        [[0]],\n+        ops.reshape(amax_update, [1]),\n+    )\n+    return new_amax_history\n+\n+\n+@keras_export([\"keras.quantizers.quantize_and_dequantize\"])\n+def quantize_and_dequantize(inputs, scale, quantized_dtype, compute_dtype):\n+    # Quantize\n+    quantized_dtype_max = ops.cast(\n+        float(ml_dtypes.finfo(quantized_dtype).max), compute_dtype\n+    )\n+    x = ops.divide(inputs, ops.cast(scale, compute_dtype))\n+    x = ops.clip(x, -quantized_dtype_max, quantized_dtype_max)\n+    x = ops.cast(x, quantized_dtype)\n+\n+    # Dequantize\n+    x = ops.multiply(ops.cast(x, compute_dtype), ops.cast(scale, compute_dtype))\n+    return x\n\n@@ -35,3 +35,43 @@ class QuantizersTest(testing.TestCase):\n \n         # Test serialization\n         self.run_class_serialization_test(quantizer)\n+\n+    def test_compute_float8_scale(self):\n+        amax = 3.0\n+        scale = 4.0\n+        dtype_max = 448.0  # float8_e4m3fn\n+        # The algorithm for computing the new scale is sourced from\n+        # https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/api/jax.html#transformer_engine.jax.update_fp8_metas\n+        expected_scale = 1.0 / (dtype_max / amax) / (2**0)\n+\n+        computed_scale = quantizers.compute_float8_scale(amax, scale, dtype_max)\n+        self.assertAllClose(computed_scale, expected_scale)\n+\n+    def test_compute_float8_amax_history(self):\n+        values = random.uniform([3, 4, 5], minval=-1, maxval=1)\n+        amax_history = random.uniform([123])\n+        amax_from_values = ops.max(ops.abs(values))\n+\n+        computed_amax_history = quantizers.compute_float8_amax_history(\n+            values, amax_history\n+        )\n+        self.assertAllClose(computed_amax_history[0], amax_from_values)\n+        # Shift to left with 1 step\n+        self.assertAllClose(\n+            computed_amax_history[1:], ops.roll(amax_history, -1)[1:]\n+        )\n+\n+    def test_quantize_and_dequantize(self):\n+        scale = 1.0 / 100.0\n+        values = random.uniform([3, 4, 5], minval=-1, maxval=1)\n+        qdq_values = quantizers.quantize_and_dequantize(\n+            values, scale, \"float8_e4m3fn\", \"float32\"\n+        )\n+        # A loose assertion due to an expected quantization error\n+        self.assertAllClose(qdq_values, values, atol=1e-1)\n+\n+        qdq_values = quantizers.quantize_and_dequantize(\n+            values, scale, \"float8_e5m2\", \"float32\"\n+        )\n+        # A loose assertion due to an expected quantization error\n+        self.assertAllClose(qdq_values, values, atol=5e-1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
