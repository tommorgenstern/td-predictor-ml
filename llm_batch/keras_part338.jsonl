{"custom_id": "keras#b21347fb7c47d58a1fcefc748ea9a39bcd2b39e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 11 | Files Changed: 4 | Hunks: 6 | Methods Changed: 8 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 56 | Churn Cumulative: 18198 | Contributors (this commit): 102 | Commits (past 90d): 44 | Contributors (cumulative): 106 | DMM Complexity: 0.8928571428571429\n\nDIFF:\n@@ -44,6 +44,31 @@ class DatasetCreatorModelFitTest(test_base.DatasetCreatorModelFitTestBase):\n     model = self._model_fit(strategy)\n     self.assertEqual(model.optimizer.iterations, 100)\n \n+  def testModelFitwithStepsPerEpochNegativeOne(self, strategy):\n+    def dataset_fn(input_context):\n+      del input_context\n+      x = tf.random.uniform((10, 10))\n+      y = tf.random.uniform((10,))\n+      return tf.data.Dataset.from_tensor_slices(\n+          (x, y)).shuffle(10).batch(2)\n+\n+    if strategy._should_use_with_coordinator:\n+      with self.assertRaises((tf.errors.OutOfRangeError,\n+                              tf.errors.CancelledError)):\n+        self._model_fit(\n+            strategy,\n+            steps_per_epoch=-1,\n+            x=dataset_creator.DatasetCreator(dataset_fn),\n+            validation_data=dataset_creator.DatasetCreator(dataset_fn),\n+        )\n+    else:\n+      self._model_fit(\n+          strategy,\n+          steps_per_epoch=-1,\n+          x=dataset_creator.DatasetCreator(dataset_fn),\n+          validation_data=dataset_creator.DatasetCreator(dataset_fn),\n+      )\n+\n   def testModelFitWithNumpyData(self, strategy):\n     x = np.random.rand(100, 10)\n     y = np.random.rand(100, 1)\n\n@@ -89,11 +89,6 @@ class DatasetCreatorModelFitTestBase(tf.test.TestCase, parameterized.TestCase):\n         if not is_loss_float:\n           raise RuntimeError(\"loss is supposed to be in the logs and float.\")\n \n-      def on_train_end(self, logs=None):\n-        if self._prev_epoch != 9:\n-          raise RuntimeError(\"Unexpected last epoch: {}\".format(\n-              self._prev_epoch))\n-\n     with strategy.scope():\n       model = sequential.Sequential([core_layers.Dense(10)])\n       if with_normalization_layer:\n\n@@ -1281,8 +1281,18 @@ class DataHandler(object):\n     # TODO(b/150292341): Allow multiple async steps here.\n     return self._inferred_steps is None\n \n+  def _log_indefinite_training_warning(self):\n+    logging.warning(\"The training loop will run indefinitely since you have \"\n+                    \"set `steps_per_epoch=-1`. Please use batch-level \"\n+                    \"callbacks to save checkpoints or log training progress, \"\n+                    \"etc\")\n+\n   def _infer_steps(self, steps, dataset):\n     \"\"\"Infers steps_per_epoch needed to loop through a dataset.\"\"\"\n+    if steps == -1:\n+      self._log_indefinite_training_warning()\n+      return None\n+\n     if steps is not None:\n       return steps\n \n@@ -1356,9 +1366,11 @@ class _ClusterCoordinatorDataHandler(DataHandler):\n \n     self._dataset = self._model._cluster_coordinator.create_per_worker_dataset(  # pylint: disable=protected-access\n         per_worker_dataset_fn)\n-    if steps_per_epoch is None:\n-      raise ValueError(\n-          \"`steps_per_epoch` must be specified with `ParameterServerStrategy`.\")\n+\n+    if steps_per_epoch == -1:\n+      self._inferred_steps = None\n+      self._log_indefinite_training_warning()\n+    else:\n       self._inferred_steps = steps_per_epoch\n \n   def sync(self):\n\n@@ -1019,9 +1019,11 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             `tf.data` dataset, and 'steps_per_epoch'\n             is None, the epoch will run until the input dataset is exhausted.\n             When passing an infinitely repeating dataset, you must specify the\n-            `steps_per_epoch` argument. This argument is not supported with\n-            array inputs. `steps_per_epoch=None` is not supported when using\n-            `tf.distribute.experimental.ParameterServerStrategy`.\n+            `steps_per_epoch` argument. If `steps_per_epoch=-1` the training\n+            will run indefinitely with an infinitely repeating dataset.\n+            This argument is not supported with array inputs.\n+            When using `tf.distribute.experimental.ParameterServerStrategy`:\n+              * `steps_per_epoch=None` is not supported.\n         validation_steps: Only relevant if `validation_data` is provided and\n             is a `tf.data` dataset. Total number of steps (batches of\n             samples) to draw before stopping when performing validation\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#14336659f21a1dbaa81d3ae23d972049cc0a4f7a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 161 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 161 | Churn Cumulative: 524 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -83,6 +83,34 @@ class Dense(keras_layers.Dense, base.Layer):\n     bias_constraint: Constraint function for the bias.\n     kernel: Weight matrix (TensorFlow variable or tensor).\n     bias: Bias vector, if applicable (TensorFlow variable or tensor).\n+\n+\n+  @compatibility(TF2)\n+  This API is not compatible with eager execution or `tf.function`.\n+\n+  Please refer to [migration guide]\n+  (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)\n+  for more details on migrating a TF1 model to Keras. In TF2 the corresponding\n+  layer is `tf.keras.layers.Dense`.\n+\n+\n+  #### Structural Mapping to Native TF2\n+\n+  None of the supported arguments have changed name.\n+\n+  Before:\n+\n+  ```python\n+   dense = tf.compat.v1.layers.Dense(units=3)\n+  ```\n+\n+  After:\n+\n+  ```python\n+   dense = tf.keras.layers.Dense(units=3)\n+  ```\n+\n+  @end_compatibility\n   \"\"\"\n \n   def __init__(self, units,\n@@ -171,6 +199,34 @@ def dense(\n \n   Raises:\n     ValueError: if eager execution is enabled.\n+\n+\n+  @compatibility(TF2)\n+  This API is not compatible with eager execution or `tf.function`.\n+\n+  Please refer to [migration guide]\n+  (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)\n+  for more details on migrating a TF1 model to Keras. In TF2 the corresponding\n+  layer is `tf.keras.layers.Dense`.\n+\n+\n+  #### Structural Mapping to Native TF2\n+\n+  None of the supported arguments have changed name.\n+\n+  Before:\n+\n+  ```python\n+   y = tf.compat.v1.layers.dense(x, units=3)\n+  ```\n+\n+  After:\n+\n+  ```python\n+   y = tf.keras.layers.Dense(units=3)(x)\n+  ```\n+  @end_compatibility\n+\n   \"\"\"\n   warnings.warn('`tf.layers.dense` is deprecated and '\n                 'will be removed in a future version. '\n@@ -215,6 +271,33 @@ class Dropout(keras_layers.Dropout, base.Layer):\n       `tf.compat.v1.set_random_seed`.\n       for behavior.\n     name: The name of the layer (string).\n+\n+\n+  @compatibility(TF2)\n+  This API is not compatible with eager execution or `tf.function`.\n+\n+  Please refer to [migration guide]\n+  (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)\n+  for more details on migrating a TF1 model to Keras. In TF2 the corresponding\n+  layer is `tf.keras.layers.Dropout`.\n+\n+\n+  #### Structural Mapping to Native TF2\n+\n+  None of the supported arguments have changed name.\n+\n+  Before:\n+\n+  ```python\n+   dropout = tf.compat.v1.layers.Dropout()\n+  ```\n+\n+  After:\n+\n+  ```python\n+   dropout = tf.keras.layers.Dropout()\n+  ```\n+  @end_compatibility\n   \"\"\"\n \n   def __init__(self, rate=0.5,\n@@ -270,6 +353,32 @@ def dropout(inputs,\n \n   Raises:\n     ValueError: if eager execution is enabled.\n+\n+  @compatibility(TF2)\n+  This API is not compatible with eager execution or `tf.function`.\n+\n+  Please refer to [migration guide]\n+  (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)\n+  for more details on migrating a TF1 model to Keras. In TF2 the corresponding\n+  layer is `tf.keras.layers.Dropout`.\n+\n+\n+  #### Structural Mapping to Native TF2\n+\n+  None of the supported arguments have changed name.\n+\n+  Before:\n+\n+  ```python\n+   y = tf.compat.v1.layers.dropout(x)\n+  ```\n+\n+  After:\n+\n+  ```python\n+   y = tf.keras.layers.Dropout()(x)\n+  ```\n+  @end_compatibility\n   \"\"\"\n   warnings.warn('`tf.layers.dropout` is deprecated and '\n                 'will be removed in a future version. '\n@@ -301,6 +410,32 @@ class Flatten(keras_layers.Flatten, base.Layer):\n     y = Flatten()(x)\n     # now `y` has shape `(None, None)`\n   ```\n+\n+  @compatibility(TF2)\n+  This API is not compatible with eager execution or `tf.function`.\n+\n+  Please refer to [migration guide]\n+  (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)\n+  for more details on migrating a TF1 model to Keras. In TF2 the corresponding\n+  layer is `tf.keras.layers.Flatten`.\n+\n+\n+  #### Structural Mapping to Native TF2\n+\n+  None of the supported arguments have changed name.\n+\n+  Before:\n+\n+  ```python\n+   flatten = tf.compat.v1.layers.Flatten()\n+  ```\n+\n+  After:\n+\n+  ```python\n+   flatten = tf.keras.layers.Flatten()\n+  ```\n+  @end_compatibility\n   \"\"\"\n   pass\n \n@@ -333,6 +468,32 @@ def flatten(inputs, name=None, data_format='channels_last'):\n     y = flatten(x)\n     # now `y` has shape `(None, None)`\n   ```\n+\n+  @compatibility(TF2)\n+  This API is not compatible with eager execution or`tf.function`.\n+\n+  Please refer to [migration guide]\n+  (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)\n+  for more details on migrating a TF1 model to Keras. In TF2 the corresponding\n+  layer is `tf.keras.layers.Flatten`.\n+\n+\n+  #### Structural Mapping to Native TF2\n+\n+  None of the supported arguments have changed name.\n+\n+  Before:\n+\n+  ```python\n+   y = tf.compat.v1.layers.flatten(x)\n+  ```\n+\n+  After:\n+\n+  ```python\n+   y = tf.keras.layers.Flatten()(x)\n+  ```\n+  @end_compatibility\n   \"\"\"\n   warnings.warn('`tf.layers.flatten` is deprecated and '\n                 'will be removed in a future version. '\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#66f630352da406d28a6eaf82aa0fd1f92a1d5fe6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 124 | Files Changed: 5 | Hunks: 5 | Methods Changed: 7 | Complexity Δ (Sum/Max): -17/0 | Churn Δ: 124 | Churn Cumulative: 4360 | Contributors (this commit): 4 | Commits (past 90d): 34 | Contributors (cumulative): 17 | DMM Complexity: 0.30851063829787234\n\nDIFF:\n@@ -91,17 +91,6 @@ class PreprocessingLayer(Layer, metaclass=abc.ABCMeta):\n     \"\"\"Resets the statistics of the preprocessing layer.\"\"\"\n     raise NotImplementedError\n \n-  def merge_state(self, layers):\n-    \"\"\"Merge the statistics of multiple preprocessing layers.\n-\n-    This layer will contain the merged state.\n-\n-    Arguments:\n-      layers: Layers whose statistics should be merge with the statistics of\n-        this layer.\n-    \"\"\"\n-    raise NotImplementedError\n-\n   def finalize_state(self):\n     \"\"\"Finalize the statistics for the preprocessing layer.\n \n\n@@ -228,24 +228,6 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n     summary = summarize(data, self.epsilon)\n     self.summary.assign(merge_summaries(summary, self.summary, self.epsilon))\n \n-  def merge_state(self, layers):\n-    for l in layers + [self]:\n-      if l.input_bin_boundaries is not None:\n-        raise ValueError(\n-            \"Cannot merge Discretization layer {} that has been initialized \"\n-            \"with `bin_boundaries`, use `num_bins` instead. You passed \"\n-            \"`bin_boundaries={}`.\".format(l.name, l.input_bin_boundaries))\n-      if not l.built:\n-        raise ValueError(\n-            \"Cannot merge Discretization layer {}, it has no state. You need \"\n-            \"to call `adapt` on this layer before merging.\".format(l.name))\n-\n-    summary = self.summary\n-    for l in layers:\n-      summary = merge_summaries(summary, l.summary, self.epsilon)\n-    self.summary.assign(summary)\n-    self.finalize_state()\n-\n   def finalize_state(self):\n     if self.input_bin_boundaries is not None or not self.built:\n       return\n\n@@ -218,44 +218,6 @@ class DiscretizationAdaptTest(keras_parameterized.TestCase,\n     output_data = model.predict(test_data)\n     self.assertAllClose(expected, output_data)\n \n-  def test_merge_state(self):\n-    data = np.arange(300)\n-    partial_ds_1 = tf.data.Dataset.from_tensor_slices(data[:100])\n-    partial_ds_2 = tf.data.Dataset.from_tensor_slices(data[100:200])\n-    partial_ds_3 = tf.data.Dataset.from_tensor_slices(data[200:])\n-    full_ds = partial_ds_1.concatenate(partial_ds_2).concatenate(partial_ds_3)\n-\n-    # Use a higher epsilon to avoid any discrepencies from the quantile\n-    # approximation.\n-    full_layer = discretization.Discretization(num_bins=3, epsilon=0.001)\n-    full_layer.adapt(full_ds.batch(2))\n-\n-    partial_layer_1 = discretization.Discretization(num_bins=3, epsilon=0.001)\n-    partial_layer_1.adapt(partial_ds_1.batch(2))\n-    partial_layer_2 = discretization.Discretization(num_bins=3, epsilon=0.001)\n-    partial_layer_2.adapt(partial_ds_2.batch(2))\n-    partial_layer_3 = discretization.Discretization(num_bins=3, epsilon=0.001)\n-    partial_layer_3.adapt(partial_ds_3.batch(2))\n-    partial_layer_1.merge_state([partial_layer_2, partial_layer_3])\n-    merged_layer = partial_layer_1\n-\n-    data = np.arange(300)\n-    self.assertAllClose(full_layer(data), merged_layer(data))\n-\n-  def test_merge_with_stateless_layers_fails(self):\n-    layer1 = discretization.Discretization(num_bins=2, name=\"layer1\")\n-    layer1.adapt([1, 2, 3])\n-    layer2 = discretization.Discretization(bin_boundaries=[0, 1], name=\"layer2\")\n-    with self.assertRaisesRegex(ValueError, \"Cannot merge.*layer2\"):\n-      layer1.merge_state([layer2])\n-\n-  def test_merge_with_unadapted_layers_fails(self):\n-    layer1 = discretization.Discretization(num_bins=2, name=\"layer1\")\n-    layer1.adapt([1, 2, 3])\n-    layer2 = discretization.Discretization(num_bins=2, name=\"layer2\")\n-    with self.assertRaisesRegex(ValueError, \"Cannot merge.*layer2\"):\n-      layer1.merge_state([layer2])\n-\n   def test_multiple_adapts(self):\n     first_adapt = [[1], [2], [3]]\n     second_adapt = [[4], [5], [6]]\n\n@@ -225,40 +225,6 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n     self.adapt_variance.assign(total_variance)\n     self.count.assign(total_count)\n \n-  def merge_state(self, layers):\n-    layers = layers + [self]\n-    for l in layers:\n-      if l.input_mean is not None:\n-        raise ValueError(\n-            'Cannot merge Normalization layer {} that has initialized with '\n-            '`mean` and `variance`, you passed `mean={}` and `variance={}`.'\n-            .format(l.name, l.input_mean, l.input_variance))\n-      if not l.built:\n-        raise ValueError(\n-            'Cannot merge Normalization layer {}, it has no state. You need to '\n-            'call `adapt` on this layer before merging.'.format(l.name))\n-\n-    layer_counts = [l.count for l in layers]\n-    layer_means = [l.adapt_mean for l in layers]\n-    layer_variances = [l.adapt_variance for l in layers]\n-\n-    total_count = tf.reduce_sum(layer_counts)\n-    layer_weightings = (\n-        tf.cast(layer_counts, self.dtype) / tf.cast(total_count, self.dtype))\n-    layer_weightings = tf.reshape(\n-        layer_weightings,\n-        shape=[len(layers)] + [1] * self.adapt_mean.shape.rank)\n-\n-    total_mean = tf.reduce_sum(layer_means * layer_weightings, axis=0)\n-    inter_layer_variances = (layer_means - total_mean)**2\n-    total_variance = tf.reduce_sum(\n-        ((layer_variances + inter_layer_variances) * layer_weightings), axis=0)\n-\n-    self.adapt_mean.assign(total_mean)\n-    self.adapt_variance.assign(total_variance)\n-    self.count.assign(total_count)\n-    self.finalize_state()\n-\n   def reset_state(self):  # pylint: disable=method-hidden\n     if self.input_mean is not None or not self.built:\n       return\n\n@@ -273,29 +273,6 @@ class NormalizationAdaptTest(keras_parameterized.TestCase,\n          keras.layers.Dense(1)])\n     model.summary()\n \n-  def test_merge_state(self):\n-    data = np.random.rand(30, 10, 2)\n-    ds = tf.data.Dataset.from_tensor_slices(data).batch(2)\n-    norm = normalization.Normalization(axis=(1, 2))\n-    norm.adapt(ds)\n-\n-    partial_ds_1 = ds.shard(3, 0)\n-    partial_ds_2 = ds.shard(3, 1)\n-    partial_ds_3 = ds.shard(3, 2)\n-\n-    norm_1 = normalization.Normalization(axis=(1, 2))\n-    norm_2 = normalization.Normalization(axis=(1, 2))\n-    norm_3 = normalization.Normalization(axis=(1, 2))\n-\n-    norm_1.adapt(partial_ds_1)\n-    norm_2.adapt(partial_ds_2)\n-    norm_3.adapt(partial_ds_3)\n-\n-    norm_1.merge_state([norm_2, norm_3])\n-    merged_norm = norm_1\n-\n-    self.assertAllClose(norm(data), merged_norm(data))\n-\n   def test_multiple_adapts(self):\n     first_adapt = [[0], [2], [0], [2]]\n     second_adapt = [[2], [4], [2], [4]]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#be8468411ab57083a83214ea3593be7917165404", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 770 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -640,10 +640,9 @@ def map_to_output_names(y_pred, output_names, struct):\n   outputs_are_flat_list = (not single_output and\n                            isinstance(y_pred, (list, tuple)) and\n                            not any(tf.nest.is_nested(y_p) for y_p in y_pred))\n-\n+  struct = copy.copy(struct)\n   if (single_output or outputs_are_flat_list) and isinstance(struct, dict):\n     output_names = output_names or create_pseudo_output_names(y_pred)\n-    struct = copy.copy(struct)\n     new_struct = [struct.pop(name, None) for name in output_names]\n     if struct:\n       raise ValueError('Found unexpected keys that do not correspond '\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ae36af8adfe774b8d47e6b3d70a4a631fec06bfb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 74 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 74 | Churn Cumulative: 74 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,74 @@\n+# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Saving utilities to support Python's Pickle protocol.\n+\"\"\"\n+\n+import os\n+import tarfile\n+from io import BytesIO\n+from uuid import uuid4\n+\n+from numpy import asarray\n+import tensorflow.compat.v2 as tf\n+\n+from keras.saving.save import load_model\n+\n+\n+def unpack_model(packed_keras_model):\n+  \"\"\"Reconstruct a Model from the result of pack_model.\n+\n+  Args:\n+      packed_keras_model (np.array): return value from pack_model.\n+\n+  Returns:\n+      keras.Model: a Keras Model instance.\n+  \"\"\"\n+  temp_dir = f\"ram://{uuid4()}\"\n+  b = BytesIO(packed_keras_model)\n+  with tarfile.open(fileobj=b, mode=\"r\") as archive:\n+    for fname in archive.getnames():\n+      dest_path = os.path.join(temp_dir, fname)\n+      tf.io.gfile.makedirs(os.path.dirname(dest_path))\n+      with tf.io.gfile.GFile(dest_path, \"wb\") as f:\n+        f.write(archive.extractfile(fname).read())\n+  model = load_model(temp_dir)\n+  tf.io.gfile.rmtree(temp_dir)\n+  return model\n+\n+\n+def pack_model(model):\n+  \"\"\"Pack a Keras Model into a numpy array for pickling.\n+\n+  Args:\n+      model (tf.keras.Model): a Keras Model instance.\n+\n+  Returns:\n+      tuple: an unpacking function (unpack_model) and it's arguments,\n+      as per Python's pickle protocol.\n+  \"\"\"\n+  temp_dir = f\"ram://{uuid4()}\"\n+  model.save(temp_dir)\n+  b = BytesIO()\n+  with tarfile.open(fileobj=b, mode=\"w\") as archive:\n+    for root, _, filenames in tf.io.gfile.walk(temp_dir):\n+      for filename in filenames:\n+        dest_path = os.path.join(root, filename)\n+        with tf.io.gfile.GFile(dest_path, \"rb\") as f:\n+          info = tarfile.TarInfo(name=os.path.relpath(dest_path, temp_dir))\n+          info.size = f.size()\n+          archive.addfile(tarinfo=info, fileobj=f)\n+  tf.io.gfile.rmtree(temp_dir)\n+  b.seek(0)\n+  return unpack_model, (asarray(memoryview(b.read())), )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c58332245117ee79409069e5f32f9ced33bb1be8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 24 | Churn Cumulative: 15250 | Contributors (this commit): 102 | Commits (past 90d): 18 | Contributors (cumulative): 103 | DMM Complexity: 1.0\n\nDIFF:\n@@ -37,6 +37,7 @@ from keras.mixed_precision import policy\n from keras.saving import hdf5_format\n from keras.saving import save\n from keras.saving import saving_utils\n+from keras.saving.pickle_utils import pack_model\n from keras.saving.saved_model import json_utils\n from keras.saving.saved_model import model_serialization\n from keras.utils import generic_utils\n@@ -321,6 +322,26 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n     super(Model, self).__setattr__(name, value)\n \n+  def __reduce__(self):\n+    return pack_model(self)\n+\n+  def __deepcopy__(self, memo):\n+    if self.built:\n+      deserializer, serialized = pack_model(self)\n+      new = deserializer(*serialized)\n+      memo[id(self)] = new\n+    else:\n+      deserializer, serialized, *rest = super(Model, self).__reduce__()\n+      new = deserializer(*serialized)\n+      memo[id(self)] = new\n+      if rest:\n+        state = copy.deepcopy(rest[0], memo=memo)\n+        new.__setstate__(state)\n+    return new\n+\n+  def __copy__(self):\n+    return self.__deepcopy__({})\n+\n   @generic_utils.default\n   def build(self, input_shape):\n     \"\"\"Builds the model based on input shapes received.\n\n@@ -15,13 +15,14 @@\n \"\"\"Saving utilities to support Python's Pickle protocol.\n \"\"\"\n \n+import tensorflow.compat.v2 as tf\n+\n import os\n import tarfile\n from io import BytesIO\n from uuid import uuid4\n \n from numpy import asarray\n-import tensorflow.compat.v2 as tf\n \n from keras.saving.save import load_model\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1e849d6e34638716a985c6a011bbea109ce57171", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 72 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 72 | Churn Cumulative: 72 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,72 @@\n+# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for pickling / deepcopying of Keras Models.\"\"\"\n+\n+import tensorflow.compat.v2 as tf\n+\n+import copy\n+import pickle\n+\n+import numpy as np\n+\n+from keras import keras_parameterized\n+from keras import layers as layers_module\n+from keras.engine import input_layer\n+from keras.engine import training as training_module\n+\n+\n+class TestPickleProtocol(keras_parameterized.TestCase):\n+  \"\"\"Tests pickle protoocol support.\n+  \"\"\"\n+\n+  @keras_parameterized.run_all_keras_modes\n+  def test_pickle_model(self):\n+    \"\"\"Test copy.copy, copy.deepcopy and pickle on Functional Model.\"\"\"\n+\n+    def roundtrip(model):\n+      model = copy.copy(model)\n+      model = copy.deepcopy(model)\n+      for protocol in range(5):  # support up to protocol version 5\n+        model = pickle.loads(pickle.dumps(model, protocol=protocol))\n+      return model\n+\n+    # create model\n+    x = input_layer.Input((3,))\n+    y = layers_module.Dense(2)(x)\n+    original_model = training_module.Model(x, y)\n+    model = original_model\n+    original_weights = model.get_weights()\n+    # roundtrip without compiling\n+    model = roundtrip(model)\n+    # compile\n+    model.compile(optimizer='sgd', loss='mse')\n+    # roundtrip compiled but not trained\n+    model = roundtrip(model)\n+    # train\n+    x = np.random.random((1000, 3))\n+    y = np.random.random((1000, 2))\n+    model.fit(x, y)\n+    y1 = model.predict(x)\n+    # roundtrip with training\n+    model = roundtrip(model)\n+    y2 = model.predict(x)\n+    self.assertAllClose(y1, y2)\n+    # check that the original model has not been changed\n+    final_weights = original_model.get_weights()\n+    self.assertAllClose(original_weights, final_weights)\n+    self.assertNotAllClose(original_weights, model.get_weights())\n+\n+if __name__ == '__main__':\n+  tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0c9462af6d8c456579c09bd705fd60594ee33611", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 69 | Lines Deleted: 11 | Files Changed: 3 | Hunks: 13 | Methods Changed: 8 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 80 | Churn Cumulative: 26060 | Contributors (this commit): 108 | Commits (past 90d): 33 | Contributors (cumulative): 119 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1228,6 +1228,16 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n \n   @trainable.setter\n   def trainable(self, value):\n+    \"\"\"Sets trainable attribute for the layer and its sublayers.\n+\n+    When this value is changed during training (e.g. with a\n+    `tf.keras.callbacks.Callback`) you need to call the parent\n+    `tf.keras.Model.make_train_function` with `force=True` in order to recompile\n+    the training graph.\n+\n+    Args:\n+      value: Boolean with the desired state for the layer's trainable attribute.\n+    \"\"\"\n     for layer in self._flatten_layers():\n       layer._trainable = value\n \n\n@@ -804,7 +804,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         return_metrics[metric.name] = result\n     return return_metrics\n \n-  def make_train_function(self):\n+  def make_train_function(self, force=False):\n     \"\"\"Creates a function that executes one step of training.\n \n     This method can be overridden to support custom training logic.\n@@ -816,7 +816,12 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n     This function is cached the first time `Model.fit` or\n     `Model.train_on_batch` is called. The cache is cleared whenever\n-    `Model.compile` is called.\n+    `Model.compile` is called. You can skip the cache and generate again the\n+    function with `force=True`.\n+\n+    Args:\n+      force: Whether to regenerate the train function and skip the cached\n+        function if available.\n \n     Returns:\n       Function. The function created by this method should accept a\n@@ -824,7 +829,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       be passed to `tf.keras.Callbacks.on_train_batch_end`, such as\n       `{'loss': 0.2, 'accuracy': 0.7}`.\n     \"\"\"\n-    if self.train_function is not None:\n+    if self.train_function is not None and not force:\n       return self.train_function\n \n     def step_function(model, iterator):\n@@ -844,7 +849,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       write_scalar_summaries(outputs, step=model._train_counter)  # pylint: disable=protected-access\n       return outputs\n \n-    if self._steps_per_execution.numpy().item() == 1:\n+    if (self._steps_per_execution is None or\n+        self._steps_per_execution.numpy().item() == 1):\n \n       def train_function(iterator):\n         \"\"\"Runs a training execution with one step.\"\"\"\n@@ -1277,7 +1283,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         return_metrics[metric.name] = result\n     return return_metrics\n \n-  def make_test_function(self):\n+  def make_test_function(self, force=False):\n     \"\"\"Creates a function that executes one step of evaluation.\n \n     This method can be overridden to support custom evaluation logic.\n@@ -1289,14 +1295,19 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n     This function is cached the first time `Model.evaluate` or\n     `Model.test_on_batch` is called. The cache is cleared whenever\n-    `Model.compile` is called.\n+    `Model.compile` is called. You can skip the cache and generate again the\n+    function with `force=True`.\n+\n+    Args:\n+      force: Whether to regenerate the test function and skip the cached\n+        function if available.\n \n     Returns:\n       Function. The function created by this method should accept a\n       `tf.data.Iterator`, and return a `dict` containing values that will\n       be passed to `tf.keras.Callbacks.on_test_batch_end`.\n     \"\"\"\n-    if self.test_function is not None:\n+    if self.test_function is not None and not force:\n       return self.test_function\n \n     def step_function(model, iterator):\n@@ -1315,7 +1326,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n           outputs, self.distribute_strategy, reduction='first')\n       return outputs\n \n-    if self._steps_per_execution.numpy().item() == 1:\n+    if (self._steps_per_execution is None or\n+        self._steps_per_execution.numpy().item() == 1):\n \n       def test_function(iterator):\n         \"\"\"Runs an evaluation execution with one step.\"\"\"\n@@ -1528,7 +1540,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     x, _, _ = data_adapter.unpack_x_y_sample_weight(data)\n     return self(x, training=False)\n \n-  def make_predict_function(self):\n+  def make_predict_function(self, force=False):\n     \"\"\"Creates a function that executes one step of inference.\n \n     This method can be overridden to support custom inference logic.\n@@ -1540,13 +1552,18 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n     This function is cached the first time `Model.predict` or\n     `Model.predict_on_batch` is called. The cache is cleared whenever\n-    `Model.compile` is called.\n+    `Model.compile` is called. You can skip the cache and generate again the\n+    function with `force=True`.\n+\n+    Args:\n+      force: Whether to regenerate the predict function and skip the cached\n+        function if available.\n \n     Returns:\n       Function. The function created by this method should accept a\n       `tf.data.Iterator`, and return the outputs of the `Model`.\n     \"\"\"\n-    if self.predict_function is not None:\n+    if self.predict_function is not None and not force:\n       return self.predict_function\n \n     def step_function(model, iterator):\n\n@@ -1670,6 +1670,37 @@ class TrainingTest(keras_parameterized.TestCase):\n       self.assertEqual(results_list,\n                        [results_dict['mean'], results_dict['sum']])\n \n+  @keras_parameterized.run_all_keras_modes\n+  @keras_parameterized.run_with_all_model_types\n+  def test_model_make_function(self):\n+    layers = [\n+        layers_module.Dense(10, dtype=np.float64),\n+        layers_module.Dense(10, dtype=np.float64)\n+    ]\n+    model = testing_utils.get_model_from_layers(layers, input_shape=(1,))\n+    model.compile('sgd', 'mse', run_eagerly=testing_utils.should_run_eagerly())\n+\n+    original_train_function = model.make_train_function()\n+    self.assertIsNotNone(original_train_function)\n+    self.assertEqual(model.make_train_function(), original_train_function)\n+    # Check that we regenerate it without reusing the cached version.\n+    self.assertNotEqual(\n+        model.make_train_function(force=True), original_train_function)\n+\n+    original_test_function = model.make_test_function()\n+    self.assertIsNotNone(original_test_function)\n+    self.assertEqual(model.make_test_function(), original_test_function)\n+    # Check that we regenerate it without reusing the cached version.\n+    self.assertNotEqual(\n+        model.make_test_function(force=True), original_test_function)\n+\n+    original_predict_function = model.make_predict_function()\n+    self.assertIsNotNone(original_predict_function)\n+    self.assertEqual(model.make_predict_function(), original_predict_function)\n+    # Check that we regenerate it without reusing the cached version.\n+    self.assertNotEqual(\n+        model.make_predict_function(force=True), original_predict_function)\n+\n \n class TestExceptionsAndWarnings(keras_parameterized.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a7f3c6f26fb4a18d91abc52cbaccd47e6930786b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 773 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -640,9 +640,10 @@ def map_to_output_names(y_pred, output_names, struct):\n   outputs_are_flat_list = (not single_output and\n                            isinstance(y_pred, (list, tuple)) and\n                            not any(tf.nest.is_nested(y_p) for y_p in y_pred))\n-  struct = copy.copy(struct)\n+\n   if (single_output or outputs_are_flat_list) and isinstance(struct, dict):\n     output_names = output_names or create_pseudo_output_names(y_pred)\n+    struct = copy.copy(struct)\n     new_struct = [struct.pop(name, None) for name in output_names]\n     if struct:\n       raise ValueError('Found unexpected keys that do not correspond '\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#52954ef7fea24294ee0bbe8774d805375bfc313f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 168 | Lines Deleted: 153 | Files Changed: 8 | Hunks: 90 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 321 | Churn Cumulative: 8298 | Contributors (this commit): 5 | Commits (past 90d): 60 | Contributors (cumulative): 32 | DMM Complexity: None\n\nDIFF:\n@@ -38,10 +38,9 @@ class CategoryEncoding(base_layer.Layer):\n \n   This layer provides options for condensing data into a categorical encoding\n   when the total number of tokens are known in advance. It accepts integer\n-  values as inputs and outputs a dense representation (one sample = 1-index\n-  tensor of float values representing data about the sample's tokens) of those\n-  inputs. For integer inputs where the total number of tokens is not known, see\n-  `tf.keras.layers.experimental.preprocessing.IntegerLookup`.\n+  values as inputs, and it outputs a dense representation of those\n+  inputs. For integer inputs where the total number of tokens is not known,\n+  use instead `tf.keras.layers.experimental.preprocessing.IntegerLookup`.\n \n   Examples:\n \n@@ -81,7 +80,7 @@ class CategoryEncoding(base_layer.Layer):\n \n   Args:\n     num_tokens: The total number of tokens the layer should support. All inputs\n-      to the layer must integers in the range 0 <= value < num_tokens or an\n+      to the layer must integers in the range `0 <= value < num_tokens`, or an\n       error will be thrown.\n     output_mode: Specification for the output of the layer.\n       Defaults to `\"multi_hot\"`. Values can be `\"one_hot\"`, `\"multi_hot\"` or\n@@ -94,10 +93,10 @@ class CategoryEncoding(base_layer.Layer):\n         - `\"multi_hot\"`: Encodes each sample in the input into a single array\n           of `num_tokens` size, containing a 1 for each vocabulary term present\n           in the sample. Treats the last dimension as the sample dimension, if\n-          input shape is (..., sample_length), output shape will be\n-          (..., num_tokens).\n-        - `\"count\"`: As `\"multi_hot\"`, but the int array contains a count of the\n-          number of times the token at that index appeared in the sample.\n+          input shape is `(..., sample_length)`, output shape will be\n+          `(..., num_tokens)`.\n+        - `\"count\"`: Like `\"multi_hot\"`, but the int array contains a count of\n+          the number of times the token at that index appeared in the sample.\n     sparse: Boolean. If true, returns a `SparseTensor` instead of a dense\n       `Tensor`. Defaults to `False`.\n \n\n@@ -36,11 +36,11 @@ def summarize(values, epsilon):\n   returned (with weights of 1).\n \n   Args:\n-      values: 1-D `np.ndarray` to be summarized.\n-      epsilon: A `'float32'` that determines the approxmiate desired precision.\n+      values: 1D `np.ndarray` to be summarized.\n+      epsilon: A `'float32'` that determines the approximate desired precision.\n \n   Returns:\n-      A 2-D `np.ndarray` that is a summary of the inputs. First column is the\n+      A 2D `np.ndarray` that is a summary of the inputs. First column is the\n       interpolated partition values, the second is the weights (counts).\n   \"\"\"\n \n@@ -67,11 +67,11 @@ def compress(summary, epsilon):\n   cumulative weight will give the new weight for that bin.\n \n   Args:\n-      summary: 2-D `np.ndarray` summary to be compressed.\n+      summary: 2D `np.ndarray` summary to be compressed.\n       epsilon: A `'float32'` that determines the approxmiate desired precision.\n \n   Returns:\n-      A 2-D `np.ndarray` that is a compressed summary. First column is the\n+      A 2D `np.ndarray` that is a compressed summary. First column is the\n       interpolated partition values, the second is the weights (counts).\n   \"\"\"\n   # TODO(b/184863356): remove the numpy escape hatch here.\n@@ -101,8 +101,8 @@ def merge_summaries(prev_summary, next_summary, epsilon):\n   them to stay within `epsilon` error tolerance.\n \n   Args:\n-      prev_summary: 2-D `np.ndarray` summary to be merged with `next_summary`.\n-      next_summary: 2-D `np.ndarray` summary to be merged with `prev_summary`.\n+      prev_summary: 2D `np.ndarray` summary to be merged with `next_summary`.\n+      next_summary: 2D `np.ndarray` summary to be merged with `prev_summary`.\n       epsilon: A float that determines the approxmiate desired precision.\n \n   Returns:\n\n@@ -33,8 +33,8 @@ class Hashing(base_layer.Layer):\n \n   This layer transforms single or multiple categorical inputs to hashed output.\n   It converts a sequence of int or string to a sequence of int. The stable hash\n-  function uses tensorflow::ops::Fingerprint to produce universal output that\n-  is consistent across platforms.\n+  function uses `tensorflow::ops::Fingerprint` to produce the same output\n+  consistently across all platforms.\n \n   This layer uses [FarmHash64](https://github.com/google/farmhash) by default,\n   which provides a consistent hashed output across different platforms and is\n@@ -46,7 +46,7 @@ class Hashing(base_layer.Layer):\n   [SipHash64](https://github.com/google/highwayhash) hash function, with\n   the `salt` value serving as additional input to the hash function.\n \n-  Example (FarmHash64):\n+  **Example (FarmHash64)**\n \n   >>> layer = tf.keras.layers.experimental.preprocessing.Hashing(num_bins=3)\n   >>> inp = [['A'], ['B'], ['C'], ['D'], ['E']]\n@@ -58,7 +58,7 @@ class Hashing(base_layer.Layer):\n            [1],\n            [2]])>\n \n-  Example (FarmHash64) with a mask value:\n+  **Example (FarmHash64) with a mask value**\n \n   >>> layer = tf.keras.layers.experimental.preprocessing.Hashing(num_bins=3,\n   ...    mask_value='')\n@@ -71,7 +71,7 @@ class Hashing(base_layer.Layer):\n            [2],\n            [2]])>\n \n-  Example (SipHash64):\n+  **Example (SipHash64)**\n \n   >>> layer = tf.keras.layers.experimental.preprocessing.Hashing(num_bins=3,\n   ...    salt=[133, 137])\n@@ -84,7 +84,7 @@ class Hashing(base_layer.Layer):\n            [0],\n            [2]])>\n \n-  Example (Siphash64 with a single integer, same as `salt=[133, 133]`\n+  **Example (Siphash64 with a single integer, same as `salt=[133, 133]`)**\n \n   >>> layer = tf.keras.layers.experimental.preprocessing.Hashing(num_bins=3,\n   ...    salt=133)\n@@ -97,8 +97,6 @@ class Hashing(base_layer.Layer):\n            [1],\n            [0]])>\n \n-  Reference: [SipHash with salt](https://www.131002.net/siphash/siphash.pdf)\n-\n   Args:\n     num_bins: Number of hash bins. Note that this includes the `mask_value` bin,\n       so the effective number of bins is `(num_bins - 1)` if `mask_value` is\n@@ -114,14 +112,19 @@ class Hashing(base_layer.Layer):\n       tuple/list of 2 unsigned integer numbers, see reference paper for details.\n     **kwargs: Keyword arguments to construct a layer.\n \n-  Input shape: A single or list of string, int32 or int64 `Tensor`,\n-    `SparseTensor` or `RaggedTensor` of shape `[batch_size, ...,]`\n+  Input shape:\n+    A single or list of string, int32 or int64 `Tensor`,\n+    `SparseTensor` or `RaggedTensor` of shape `(batch_size, ...,)`\n \n-  Output shape: An int64 `Tensor`, `SparseTensor` or `RaggedTensor` of shape\n-    `[batch_size, ...]`. If any input is `RaggedTensor` then output is\n+  Output shape:\n+    An int64 `Tensor`, `SparseTensor` or `RaggedTensor` of shape\n+    `(batch_size, ...)`. If any input is `RaggedTensor` then output is\n     `RaggedTensor`, otherwise if any input is `SparseTensor` then output is\n     `SparseTensor`, otherwise the output is `Tensor`.\n \n+  Reference:\n+    - [SipHash with salt](https://www.131002.net/siphash/siphash.pdf)\n+\n   \"\"\"\n \n   def __init__(self, num_bins, mask_value=None, salt=None, **kwargs):\n\n@@ -59,14 +59,14 @@ class Resizing(base_layer.Layer):\n   \"\"\"Image resizing layer.\n \n   Resize the batched image input to target height and width. The input should\n-  be a 4-D tensor in the format of NHWC.\n+  be a 4D tensor in `\"channels_last\"` format.\n \n   Args:\n     height: Integer, the height of the output shape.\n     width: Integer, the width of the output shape.\n-    interpolation: String, the interpolation method. Defaults to `bilinear`.\n-      Supports `bilinear`, `nearest`, `bicubic`, `area`, `lanczos3`, `lanczos5`,\n-      `gaussian`, `mitchellcubic`\n+    interpolation: String, the interpolation method. Defaults to `\"bilinear\"`.\n+      Supports `\"bilinear\"`, `\"nearest\"`, `\"bicubic\"`, `\"area\"`, `\"lanczos3\"`,\n+      `\"lanczos5\"`, `\"gaussian\"`, `\"mitchellcubic\"`.\n     crop_to_aspect_ratio: If True, resize the images without aspect\n       ratio distortion. When the original aspect ratio differs from the target\n       aspect ratio, the output image will be cropped so as to return the largest\n@@ -125,7 +125,7 @@ class CenterCrop(base_layer.Layer):\n \n   Input shape:\n     4D tensor with shape:\n-    `(samples, height, width, channels)`, data_format='channels_last'.\n+    `(samples, height, width, channels)`, in `\"channels_last\"` format.\n \n   Output shape:\n     4D tensor with shape:\n@@ -199,7 +199,7 @@ class RandomCrop(base_layer.Layer):\n \n   Input shape:\n     4D tensor with shape:\n-    `(samples, height, width, channels)`, data_format='channels_last'.\n+    `(samples, height, width, channels)`, in `\"channels_last\"` format.\n \n   Output shape:\n     4D tensor with shape:\n@@ -357,17 +357,17 @@ class RandomFlip(base_layer.Layer):\n \n   Input shape:\n     4D tensor with shape:\n-    `(samples, height, width, channels)`, data_format='channels_last'.\n+    `(samples, height, width, channels)`, in `\"channels_last\"` format.\n \n   Output shape:\n     4D tensor with shape:\n-    `(samples, height, width, channels)`, data_format='channels_last'.\n+    `(samples, height, width, channels)`, in `\"channels_last\"` format.\n \n   Attributes:\n-    mode: String indicating which flip mode to use. Can be \"horizontal\",\n-      \"vertical\", or \"horizontal_and_vertical\". Defaults to\n-      \"horizontal_and_vertical\". \"horizontal\" is a left-right flip and\n-      \"vertical\" is a top-bottom flip.\n+    mode: String indicating which flip mode to use. Can be `\"horizontal\"`,\n+      `\"vertical\"`, or `\"horizontal_and_vertical\"`. Defaults to\n+      `\"horizontal_and_vertical\"`. `\"horizontal\"` is a left-right flip and\n+      `\"vertical\"` is a top-bottom flip.\n     seed: Integer. Used to create a random seed.\n   \"\"\"\n \n@@ -438,9 +438,10 @@ class RandomTranslation(base_layer.Layer):\n       value means shifting image up, while a positive value means shifting image\n       down. When represented as a single positive float, this value is used for\n       both the upper and lower bound. For instance, `height_factor=(-0.2, 0.3)`\n-      results in an output shifted by a random amount in the range [-20%, +30%].\n+      results in an output shifted by a random amount in the range\n+      `[-20%, +30%]`.\n       `height_factor=0.2` results in an output height shifted by a random amount\n-      in the range [-20%, +20%].\n+      in the range `[-20%, +20%]`.\n     width_factor: a float represented as fraction of value, or a tuple of size 2\n       representing lower and upper bound for shifting horizontally. A negative\n       value means shifting image left, while a positive value means shifting\n@@ -450,7 +451,7 @@ class RandomTranslation(base_layer.Layer):\n       shifted right by 30%. `width_factor=0.2` results in an output height\n       shifted left or right by 20%.\n     fill_mode: Points outside the boundaries of the input are filled according\n-      to the given mode (one of `{'constant', 'reflect', 'wrap', 'nearest'}`).\n+      to the given mode (one of `{\"constant\", \"reflect\", \"wrap\", \"nearest\"}`).\n       - *reflect*: `(d c b a | a b c d | d c b a)` The input is extended by\n         reflecting about the edge of the last pixel.\n       - *constant*: `(k k k k | a b c d | k k k k)` The input is extended by\n@@ -459,19 +460,19 @@ class RandomTranslation(base_layer.Layer):\n         wrapping around to the opposite edge.\n       - *nearest*: `(a a a a | a b c d | d d d d)` The input is extended by the\n         nearest pixel.\n-    interpolation: Interpolation mode. Supported values: \"nearest\", \"bilinear\".\n+    interpolation: Interpolation mode. Supported values: `\"nearest\"`,\n+      `\"bilinear\"`.\n     seed: Integer. Used to create a random seed.\n     fill_value: a float represents the value to be filled outside the boundaries\n-      when `fill_mode` is \"constant\".\n+      when `fill_mode=\"constant\"`.\n+\n   Input shape:\n     4D tensor with shape: `(samples, height, width, channels)`,\n-      data_format='channels_last'.\n+      in `\"channels_last\"` format.\n+\n   Output shape:\n     4D tensor with shape: `(samples, height, width, channels)`,\n-      data_format='channels_last'.\n-  Raise:\n-    ValueError: if either bound is not between [0, 1], or upper bound is less\n-      than lower bound.\n+      in `\"channels_last\"` format.\n   \"\"\"\n \n   def __init__(self,\n@@ -580,12 +581,12 @@ def get_translation_matrix(translations, name=None):\n   \"\"\"Returns projective transform(s) for the given translation(s).\n \n   Args:\n-    translations: A matrix of 2-element lists representing [dx, dy] to translate\n-      for each image (for a batch of images).\n+    translations: A matrix of 2-element lists representing `[dx, dy]`\n+      to translate for each image (for a batch of images).\n     name: The name of the op.\n \n   Returns:\n-    A tensor of shape (num_images, 8) projective transforms which can be given\n+    A tensor of shape `(num_images, 8)` projective transforms which can be given\n       to `transform`.\n   \"\"\"\n   with backend.name_scope(name or 'translation_matrix'):\n@@ -619,9 +620,10 @@ def transform(images,\n   \"\"\"Applies the given transform(s) to the image(s).\n \n   Args:\n-    images: A tensor of shape (num_images, num_rows, num_columns, num_channels)\n-      (NHWC), (num_rows, num_columns, num_channels) (HWC), or (num_rows,\n-      num_columns) (HW). The rank must be statically known (the shape is not\n+    images: A tensor of shape\n+      `(num_images, num_rows, num_columns, num_channels)`\n+      (NHWC), `(num_rows, num_columns, num_channels)` (HWC), or `(num_rows,\n+      num_columns)` (HW). The rank must be statically known (the shape is not\n       `TensorShape(None)`.\n     transforms: Projective transform matrix/matrices. A vector of length 8 or\n       tensor of size N x 8. If one row of transforms is [a0, a1, a2, b0, b1, b2,\n@@ -631,26 +633,37 @@ def transform(images,\n       transform mapping input points to output points. Note that gradients are\n       not backpropagated into transformation parameters.\n     fill_mode: Points outside the boundaries of the input are filled according\n-      to the given mode (one of `{'constant', 'reflect', 'wrap', 'nearest'}`).\n+      to the given mode (one of `{\"constant\", \"reflect\", \"wrap\", \"nearest\"}`).\n     fill_value: a float represents the value to be filled outside the boundaries\n-      when `fill_mode` is \"constant\".\n-    interpolation: Interpolation mode. Supported values: \"nearest\", \"bilinear\".\n-    output_shape: Output dimesion after the transform, [height, width]. If None,\n-      output is the same size as input image.\n-    name: The name of the op.  ## Fill mode.\n-  Behavior for each valid value is as follows:  reflect (d c b a | a b c d | d c\n-    b a) The input is extended by reflecting about the edge of the last pixel.\n-    constant (k k k k | a b c d | k k k k) The input is extended by filling all\n-    values beyond the edge with the same constant value k = 0.  wrap (a b c d |\n-    a b c d | a b c d) The input is extended by wrapping around to the opposite\n-    edge.  nearest (a a a a | a b c d | d d d d) The input is extended by the\n-    nearest pixel.\n+      when `fill_mode=\"constant\"`.\n+    interpolation: Interpolation mode. Supported values: `\"nearest\"`,\n+      `\"bilinear\"`.\n+    output_shape: Output dimension after the transform, `[height, width]`.\n+      If `None`, output is the same size as input image.\n+    name: The name of the op.\n+\n+  Fill mode behavior for each valid value is as follows:\n+\n+  - reflect (d c b a | a b c d | d c b a)\n+  The input is extended by reflecting about the edge of the last pixel.\n+\n+  - constant (k k k k | a b c d | k k k k)\n+  The input is extended by filling all\n+  values beyond the edge with the same constant value k = 0.\n+\n+  - wrap (a b c d | a b c d | a b c d)\n+  The input is extended by wrapping around to the opposite edge.\n+\n+  - nearest (a a a a | a b c d | d d d d)\n+  The input is extended by the nearest pixel.\n+\n   Input shape:\n     4D tensor with shape: `(samples, height, width, channels)`,\n-      data_format='channels_last'.\n+      in `\"channels_last\"` format.\n+\n   Output shape:\n     4D tensor with shape: `(samples, height, width, channels)`,\n-      data_format='channels_last'.\n+      in `\"channels_last\"` format.\n \n   Returns:\n     Image(s) with the same type and shape as `images`, with the given\n@@ -739,14 +752,14 @@ class RandomRotation(base_layer.Layer):\n \n   Input shape:\n     4D tensor with shape:\n-    `(samples, height, width, channels)`, data_format='channels_last'.\n+    `(samples, height, width, channels)`, in `\"channels_last\"` format.\n \n   Output shape:\n     4D tensor with shape:\n-    `(samples, height, width, channels)`, data_format='channels_last'.\n+    `(samples, height, width, channels)`, in `\"channels_last\"` format.\n \n   Attributes:\n-    factor: a float represented as fraction of 2pi, or a tuple of size 2\n+    factor: a float represented as fraction of 2 Pi, or a tuple of size 2\n       representing lower and upper bound for rotating clockwise and\n       counter-clockwise. A positive values means rotating counter clock-wise,\n       while a negative value means clock-wise. When represented as a single\n@@ -755,7 +768,7 @@ class RandomRotation(base_layer.Layer):\n       amount in the range `[-20% * 2pi, 30% * 2pi]`. `factor=0.2` results in an\n       output rotating by a random amount in the range `[-20% * 2pi, 20% * 2pi]`.\n     fill_mode: Points outside the boundaries of the input are filled according\n-      to the given mode (one of `{'constant', 'reflect', 'wrap', 'nearest'}`).\n+      to the given mode (one of `{\"constant\", \"reflect\", \"wrap\", \"nearest\"}`).\n       - *reflect*: `(d c b a | a b c d | d c b a)` The input is extended by\n         reflecting about the edge of the last pixel.\n       - *constant*: `(k k k k | a b c d | k k k k)` The input is extended by\n@@ -764,13 +777,11 @@ class RandomRotation(base_layer.Layer):\n         wrapping around to the opposite edge.\n       - *nearest*: `(a a a a | a b c d | d d d d)` The input is extended by the\n         nearest pixel.\n-    interpolation: Interpolation mode. Supported values: \"nearest\", \"bilinear\".\n+    interpolation: Interpolation mode. Supported values: `\"nearest\"`,\n+      `\"bilinear\"`.\n     seed: Integer. Used to create a random seed.\n     fill_value: a float represents the value to be filled outside the boundaries\n-      when `fill_mode` is \"constant\".\n-  Raise:\n-    ValueError: if either bound is not between [0, 1], or upper bound is less\n-      than lower bound.\n+      when `fill_mode=\"constant\"`.\n   \"\"\"\n \n   def __init__(self,\n@@ -852,9 +863,9 @@ class RandomZoom(base_layer.Layer):\n       represented as a single float, this value is used for both the upper and\n       lower bound. A positive value means zooming out, while a negative value\n       means zooming in. For instance, `height_factor=(0.2, 0.3)` result in an\n-      output zoomed out by a random amount in the range [+20%, +30%].\n+      output zoomed out by a random amount in the range `[+20%, +30%]`.\n       `height_factor=(-0.3, -0.2)` result in an output zoomed in by a random\n-      amount in the range [+20%, +30%].\n+      amount in the range `[+20%, +30%]`.\n     width_factor: a float represented as fraction of value, or a tuple of size 2\n       representing lower and upper bound for zooming horizontally. When\n       represented as a single float, this value is used for both the upper and\n@@ -863,7 +874,7 @@ class RandomZoom(base_layer.Layer):\n       output zooming in between 20% to 30%. Defaults to `None`, i.e., zooming\n       vertical and horizontal directions by preserving the aspect ratio.\n     fill_mode: Points outside the boundaries of the input are filled according\n-      to the given mode (one of `{'constant', 'reflect', 'wrap', 'nearest'}`).\n+      to the given mode (one of `{\"constant\", \"reflect\", \"wrap\", \"nearest\"}`).\n       - *reflect*: `(d c b a | a b c d | d c b a)` The input is extended by\n         reflecting about the edge of the last pixel.\n       - *constant*: `(k k k k | a b c d | k k k k)` The input is extended by\n@@ -872,10 +883,11 @@ class RandomZoom(base_layer.Layer):\n         wrapping around to the opposite edge.\n       - *nearest*: `(a a a a | a b c d | d d d d)` The input is extended by the\n         nearest pixel.\n-    interpolation: Interpolation mode. Supported values: \"nearest\", \"bilinear\".\n+    interpolation: Interpolation mode. Supported values: `\"nearest\"`,\n+      `\"bilinear\"`.\n     seed: Integer. Used to create a random seed.\n     fill_value: a float represents the value to be filled outside the boundaries\n-      when `fill_mode` is \"constant\".\n+      when `fill_mode=\"constant\"`.\n \n   Example:\n \n@@ -887,13 +899,11 @@ class RandomZoom(base_layer.Layer):\n \n   Input shape:\n     4D tensor with shape: `(samples, height, width, channels)`,\n-      data_format='channels_last'.\n+      in `\"channels_last\"` format.\n+\n   Output shape:\n     4D tensor with shape: `(samples, height, width, channels)`,\n-      data_format='channels_last'.\n-  Raise:\n-    ValueError: if lower bound is not between [0, 1], or upper bound is\n-      negative.\n+      in `\"channels_last\"` format.\n   \"\"\"\n \n   def __init__(self,\n@@ -996,16 +1006,17 @@ def get_zoom_matrix(zooms, image_height, image_width, name=None):\n   \"\"\"Returns projective transform(s) for the given zoom(s).\n \n   Args:\n-    zooms: A matrix of 2-element lists representing [zx, zy] to zoom for each\n+    zooms: A matrix of 2-element lists representing `[zx, zy]` to zoom for each\n       image (for a batch of images).\n     image_height: Height of the image(s) to be transformed.\n     image_width: Width of the image(s) to be transformed.\n     name: The name of the op.\n \n   Returns:\n-    A tensor of shape (num_images, 8). Projective transforms which can be given\n-      to operation `image_projective_transform_v2`. If one row of transforms is\n-       [a0, a1, a2, b0, b1, b2, c0, c1], then it maps the *output* point\n+    A tensor of shape `(num_images, 8)`. Projective transforms which can be\n+      given to operation `image_projective_transform_v2`.\n+      If one row of transforms is\n+       `[a0, a1, a2, b0, b1, b2, c0, c1]`, then it maps the *output* point\n        `(x, y)` to a transformed *input* point\n        `(x', y') = ((a0 x + a1 y + a2) / k, (b0 x + b1 y + b2) / k)`,\n        where `k = c0 x + c1 y + 1`.\n@@ -1046,21 +1057,18 @@ class RandomContrast(base_layer.Layer):\n \n   Input shape:\n     4D tensor with shape:\n-    `(samples, height, width, channels)`, data_format='channels_last'.\n+    `(samples, height, width, channels)`, in `\"channels_last\"` format.\n \n   Output shape:\n     4D tensor with shape:\n-    `(samples, height, width, channels)`, data_format='channels_last'.\n+    `(samples, height, width, channels)`, in `\"channels_last\"` format.\n \n   Attributes:\n     factor: a positive float represented as fraction of value, or a tuple of\n       size 2 representing lower and upper bound. When represented as a single\n       float, lower = upper. The contrast factor will be randomly picked between\n-      [1.0 - lower, 1.0 + upper].\n+      `[1.0 - lower, 1.0 + upper]`.\n     seed: Integer. Used to create a random seed.\n-  Raise:\n-    ValueError: if lower bound is not between [0, 1], or upper bound is\n-      negative.\n   \"\"\"\n \n   def __init__(self, factor, seed=None, **kwargs):\n@@ -1111,7 +1119,7 @@ class RandomHeight(base_layer.Layer):\n   \"\"\"Randomly vary the height of a batch of images during training.\n \n   Adjusts the height of a batch of images by a random factor. The input\n-  should be a 4-D tensor in the \"channels_last\" image data format.\n+  should be a 4D tensor in the `\"channels_last\"` image data format.\n \n   By default, this layer is inactive during inference.\n \n@@ -1124,13 +1132,15 @@ class RandomHeight(base_layer.Layer):\n       `factor=(-0.2, 0.3)` results in an output with height changed by a random\n       amount in the range `[-20%, +30%]. `factor=0.2` results in an output with\n       height changed by a random amount in the range `[-20%, +20%]`.\n-    interpolation: String, the interpolation method. Defaults to `bilinear`.\n-      Supports `bilinear`, `nearest`, `bicubic`, `area`, `lanczos3`, `lanczos5`,\n-      `gaussian`, `mitchellcubic`\n+    interpolation: String, the interpolation method. Defaults to `\"bilinear\"`.\n+      Supports `\"bilinear\"`, `\"nearest\"`, `\"bicubic\"`, `\"area\"`,\n+      `\"lanczos3\"`, `\"lanczos5\"`, `\"gaussian\"`, `\"mitchellcubic\"`.\n     seed: Integer. Used to create a random seed.\n+\n   Input shape:\n     4D tensor with shape: `(samples, height, width, channels)`\n-      (data_format='channels_last').\n+      (in `\"channels_last\"` format.).\n+\n   Output shape:\n     4D tensor with shape: `(samples, random_height, width, channels)`.\n   \"\"\"\n@@ -1207,7 +1217,7 @@ class RandomWidth(base_layer.Layer):\n   \"\"\"Randomly vary the width of a batch of images during training.\n \n   Adjusts the width of a batch of images by a random factor. The input\n-  should be a 4-D tensor in the \"channels_last\" image data format.\n+  should be a 4D tensor in the `\"channels_last\"` image data format.\n \n   By default, this layer is inactive during inference.\n \n@@ -1221,12 +1231,14 @@ class RandomWidth(base_layer.Layer):\n       range `[-20%, +30%]`. `factor=0.2` results in an output with width changed\n       by a random amount in the range `[-20%, +20%]`.\n     interpolation: String, the interpolation method. Defaults to `bilinear`.\n-      Supports `bilinear`, `nearest`, `bicubic`, `area`, `lanczos3`, `lanczos5`,\n-      `gaussian`, `mitchellcubic`\n+      Supports `\"bilinear\"`, `\"nearest\"`, `\"bicubic\"`, `\"area\"`, `\"lanczos3\"`,\n+      `\"lanczos5\"`, `\"gaussian\"`, `\"mitchellcubic\"`.\n     seed: Integer. Used to create a random seed.\n+\n   Input shape:\n     4D tensor with shape: `(samples, height, width, channels)`\n-      (data_format='channels_last').\n+      (in `\"channels_last\"` format.).\n+\n   Output shape:\n     4D tensor with shape: `(samples, height, random_width, channels)`.\n   \"\"\"\n\n@@ -244,7 +244,7 @@ class IntegerLookup(index_lookup.IndexLookup):\n   **Inverse lookup**\n \n   This example demonstrates how to map indices to tokens using this layer. (You\n-  can also use adapt() with inverse=True, but for simplicity we'll pass the\n+  can also use `adapt()` with `inverse=True`, but for simplicity we'll pass the\n   vocab in this example.)\n \n   >>> vocab = [12, 36, 1138, 42]\n\n@@ -30,16 +30,16 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n \n   This layer will coerce its inputs into a distribution centered around\n   0 with standard deviation 1. It accomplishes this by precomputing the mean and\n-  variance of the data, and calling (input-mean)/sqrt(var) at runtime.\n+  variance of the data, and calling `(input - mean) / sqrt(var)` at runtime.\n \n-  What happens in `adapt`: Compute mean and variance of the data and store them\n-    as the layer's weights. `adapt` should be called before `fit`, `evaluate`,\n-    or `predict`.\n+  What happens in `adapt()`: Compute mean and variance of the data and store\n+  them as the layer's weights. `adapt()` should be called before `fit()`,\n+  `evaluate()`, or `predict()`.\n \n   Args:\n       axis: Integer, tuple of integers, or None. The axis or axes that should\n         have a separate mean and variance for each index in the shape. For\n-        example, if shape is (None, 5) and axis=1, the layer will track 5\n+        example, if shape is `(None, 5)` and `axis=1`, the layer will track 5\n         separate mean and variance values for the last axis. If `axis` is set to\n         `None`, the layer will normalize all elements in the input by a scalar\n         mean and variance. Defaults to -1, where the last axis of the input is\n@@ -49,16 +49,16 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n         separately. In this case, consider passing `axis=None`.\n       mean: The mean value(s) to use during normalization. The passed value(s)\n         will be broadcast to the shape of the kept axes above; if the value(s)\n-        cannot be broadcast, an error will be raised when this layer's build()\n+        cannot be broadcast, an error will be raised when this layer's `build()`\n         method is called.\n       variance: The variance value(s) to use during normalization. The passed\n         value(s) will be broadcast to the shape of the kept axes above; if the\n         value(s) cannot be broadcast, an error will be raised when this layer's\n-        build() method is called.\n+        `build()` method is called.\n \n   Examples:\n \n-  Calculate a global mean and variance by analyzing the dataset in `adapt`.\n+  Calculate a global mean and variance by analyzing the dataset in `adapt()`.\n \n   >>> adapt_data = np.array([1., 2., 3., 4., 5.], dtype='float32')\n   >>> input_data = np.array([1., 2., 3.], dtype='float32')\n@@ -81,7 +81,6 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n   <tf.Tensor: shape=(1, 3), dtype=float32, numpy=\n   array([0., 0., 0.], dtype=float32)>\n \n-\n   Pass the mean and variance directly.\n \n   >>> input_data = np.array([[1.], [2.], [3.]], dtype='float32')\n\n@@ -128,9 +128,9 @@ class StringLookup(index_lookup.IndexLookup):\n   >>> layer.get_vocabulary()\n   ['[UNK]', 'd', 'z', 'c', 'b', 'a']\n \n-  Note that the OOV token [UNK] has been added to the vocabulary. The remaining\n-  tokens are sorted by frequency ('d', which has 2 occurrences, is first) then\n-  by inverse sort order.\n+  Note that the OOV token `\"[UNK]\"` has been added to the vocabulary.\n+  The remaining tokens are sorted by frequency\n+  (`\"d\"`, which has 2 occurrences, is first) then by inverse sort order.\n \n   >>> data = tf.constant([[\"a\", \"c\", \"d\"], [\"d\", \"z\", \"b\"]])\n   >>> layer = StringLookup()\n@@ -204,7 +204,7 @@ class StringLookup(index_lookup.IndexLookup):\n \n   **TF-IDF output**\n \n-  Configure the layer with `output_mode='tf_idf'`. As with multi_hot output, the\n+  Configure the layer with `output_mode=\"tf_idf\"`. As with multi_hot output, the\n   first `num_oov_indices` dimensions in the output represent OOV values.\n \n   Each token bin will output `token_count * idf_weight`, where the idf weights\n@@ -215,7 +215,7 @@ class StringLookup(index_lookup.IndexLookup):\n   >>> vocab = [\"a\", \"b\", \"c\", \"d\"]\n   >>> idf_weights = [0.25, 0.75, 0.6, 0.4]\n   >>> data = tf.constant([[\"a\", \"c\", \"d\", \"d\"], [\"d\", \"z\", \"b\", \"z\"]])\n-  >>> layer = StringLookup(output_mode='tf_idf')\n+  >>> layer = StringLookup(output_mode=\"tf_idf\")\n   >>> layer.set_vocabulary(vocab, idf_weights=idf_weights)\n   >>> layer(data)\n   <tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n@@ -228,21 +228,21 @@ class StringLookup(index_lookup.IndexLookup):\n   >>> vocab = [\"[UNK]\", \"a\", \"b\", \"c\", \"d\"]\n   >>> idf_weights = [0.9, 0.25, 0.75, 0.6, 0.4]\n   >>> data = tf.constant([[\"a\", \"c\", \"d\", \"d\"], [\"d\", \"z\", \"b\", \"z\"]])\n-  >>> layer = StringLookup(output_mode='tf_idf')\n+  >>> layer = StringLookup(output_mode=\"tf_idf\")\n   >>> layer.set_vocabulary(vocab, idf_weights=idf_weights)\n   >>> layer(data)\n   <tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n     array([[0.  , 0.25, 0.  , 0.6 , 0.8 ],\n            [1.8 , 0.  , 0.75, 0.  , 0.4 ]], dtype=float32)>\n \n-  When adapting the layer in tf_idf mode, each input sample will be considered a\n-  document, and idf weight per token will be calculated as\n+  When adapting the layer in `\"tf_idf\"` mode, each input sample will be\n+  considered a document, and IDF weight per token will be calculated as\n   `log(1 + num_documents / (1 + token_document_count))`.\n \n   **Inverse lookup**\n \n   This example demonstrates how to map indices to strings using this layer. (You\n-  can also use adapt() with inverse=True, but for simplicity we'll pass the\n+  can also use `adapt()` with `inverse=True`, but for simplicity we'll pass the\n   vocab in this example.)\n \n   >>> vocab = [\"a\", \"b\", \"c\", \"d\"]\n@@ -271,11 +271,11 @@ class StringLookup(index_lookup.IndexLookup):\n   array([[b'a', b'c', b'd'],\n          [b'd', b'[UNK]', b'b']], dtype=object)>\n \n-  In this example, the input value 'z' resulted in an output of '[UNK]', since\n-  1000 was not in the vocabulary - it got represented as an OOV, and all OOV\n-  values are returned as '[OOV}' in the inverse layer. Also, note that for the\n-  inverse to work, you must have already set the forward layer vocabulary\n-  either directly or via adapt() before calling get_vocabulary().\n+  In this example, the input value `\"z\"` resulted in an output of `\"[UNK]\"`,\n+  since 1000 was not in the vocabulary - it got represented as an OOV, and all\n+  OOV values are returned as `\"[UNK]\"` in the inverse layer. Also, note that\n+  for the inverse to work, you must have already set the forward layer\n+  vocabulary either directly or via `adapt()` before calling `get_vocabulary()`.\n   \"\"\"\n \n   def __init__(self,\n\n@@ -68,7 +68,7 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n   representation (one example = 1D tensor of float values representing data\n   about the example's tokens).\n \n-  If desired, the user can call this layer's adapt() method on a dataset.\n+  If desired, the user can call this layer's `adapt()` method on a dataset.\n   When this layer is adapted, it will analyze the dataset, determine the\n   frequency of individual string values, and create a 'vocabulary' from them.\n   This vocabulary can have unlimited size or be capped, depending on the\n@@ -78,14 +78,14 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n \n   The processing of each example contains the following steps:\n \n-    1. standardize each example (usually lowercasing + punctuation stripping)\n-    2. split each example into substrings (usually words)\n-    3. recombine substrings into tokens (usually ngrams)\n-    4. index tokens (associate a unique int value with each token)\n-    5. transform each example using this index, either into a vector of ints or\n+  1. Standardize each example (usually lowercasing + punctuation stripping)\n+  2. Split each example into substrings (usually words)\n+  3. Recombine substrings into tokens (usually ngrams)\n+  4. Index tokens (associate a unique int value with each token)\n+  5. Transform each example using this index, either into a vector of ints or\n      a dense float vector.\n \n-  Some notes on passing Callables to customize splitting and normalization for\n+  Some notes on passing callables to customize splitting and normalization for\n   this layer:\n \n   1. Any callable can be passed to this Layer, but if you want to serialize\n@@ -108,7 +108,7 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n     max_tokens: The maximum size of the vocabulary for this layer. If None,\n       there is no cap on the size of the vocabulary. Note that this vocabulary\n       contains 1 OOV token, so the effective number of tokens is `(max_tokens -\n-      1 - (1 if output == `\"int\"` else 0))`.\n+      1 - (1 if output_mode == \"int\" else 0))`.\n     standardize: Optional specification for standardization to apply to the\n       input text. Values can be None (no standardization),\n       `\"lower_and_strip_punctuation\"` (lowercase and remove punctuation) or a\n@@ -125,24 +125,26 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n       be `\"int\"`, `\"multi_hot\"`, `\"count\"` or `\"tf_idf\"`, configuring the layer\n       as follows:\n         - `\"int\"`: Outputs integer indices, one integer index per split string\n-          token. When output == `\"int\"`, 0 is reserved for masked locations;\n-          this reduces the vocab size to max_tokens-2 instead of max_tokens-1\n+          token. When `output_mode == \"int\"`, 0 is reserved for masked\n+          locations; this reduces the vocab size to\n+          `max_tokens - 2` instead of `max_tokens - 1`.\n         - `\"multi_hot\"`: Outputs a single int array per batch, of either\n           vocab_size or max_tokens size, containing 1s in all elements where the\n           token mapped to that index exists at least once in the batch item.\n-        - `\"count\"`: As `\"multi_hot\"`, but the int array contains a count of the\n-          number of times the token at that index appeared in the batch item.\n-        - `\"tf_idf\"`: As `\"multi_hot\"`, but the TF-IDF algorithm is applied to\n+        - `\"count\"`: Like `\"multi_hot\"`, but the int array contains a count of\n+          the number of times the token at that index appeared in the\n+          batch item.\n+        - `\"tf_idf\"`: Like `\"multi_hot\"`, but the TF-IDF algorithm is applied to\n           find the value in each token slot.\n     output_sequence_length: Only valid in INT mode. If set, the output will have\n       its time dimension padded or truncated to exactly `output_sequence_length`\n-      values, resulting in a tensor of shape [batch_size,\n-      output_sequence_length] regardless of how many tokens resulted from the\n-      splitting step. Defaults to None.\n+      values, resulting in a tensor of shape\n+      `(batch_size, output_sequence_length)` regardless of how many tokens\n+      resulted from the splitting step. Defaults to None.\n     pad_to_max_tokens: Only valid in  `\"multi_hot\"`, `\"count\"`, and `\"tf_idf\"`\n       modes. If True, the output will have its feature axis padded to\n       `max_tokens` even if the number of unique tokens in the vocabulary is less\n-      than max_tokens, resulting in a tensor of shape [batch_size, max_tokens]\n+      than max_tokens, resulting in a tensor of shape `(batch_size, max_tokens)`\n       regardless of vocabulary size. Defaults to False.\n     vocabulary: An optional list of vocabulary terms, or a path to a text file\n       containing a vocabulary to load into this layer. The file should contain\n@@ -151,7 +153,7 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n \n   Example:\n \n-  This example instantiates a TextVectorization layer that lowercases text,\n+  This example instantiates a `TextVectorization` layer that lowercases text,\n   splits on whitespace, strips punctuation, and outputs integer vocab indices.\n \n   >>> text_dataset = tf.data.Dataset.from_tensor_slices([\"foo\", \"bar\", \"baz\"])\n@@ -191,8 +193,8 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n \n   Example:\n \n-  This example instantiates a TextVectorization layer by passing a list\n-  of vocabulary terms to the layer's __init__ method.\n+  This example instantiates a `TextVectorization` layer by passing a list\n+  of vocabulary terms to the layer's `__init__()` method.\n \n   >>> vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n   >>> max_len = 4  # Sequence length to pad the outputs to.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
