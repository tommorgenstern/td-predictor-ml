{"custom_id": "keras#a9740c65508d41e60f18f06705be5bbb73ffed41", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 51 | Churn Cumulative: 5342 | Contributors (this commit): 6 | Commits (past 90d): 7 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -23,6 +23,7 @@ from keras.engine import base_preprocessing_layer\n from keras.layers.preprocessing import preprocessing_utils as utils\n from keras.preprocessing.image import smart_resize\n from keras.utils import control_flow_util\n+from keras.utils import tf_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n from tensorflow.python.util.tf_export import keras_export\n@@ -64,6 +65,9 @@ class Resizing(base_layer.Layer):\n   Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and of\n   interger or floating point dtype. By default, the layer will output floats.\n \n+  This layer can be called on tf.RaggedTensor batches of input images of\n+  distinct sizes, and will resize the outputs to dense tensors of uniform size.\n+\n   For an overview and full list of preprocessing layers, see the preprocessing\n   [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n \n@@ -104,18 +108,29 @@ class Resizing(base_layer.Layer):\n     else:\n       input_dtype = tf.float32\n     inputs = utils.ensure_tensor(inputs, dtype=input_dtype)\n+    size = [self.height, self.width]\n     if self.crop_to_aspect_ratio:\n-      outputs = smart_resize(\n-          inputs,\n-          size=[self.height, self.width],\n+      def resize_to_aspect(x):\n+        if tf_utils.is_ragged(inputs):\n+          x = x.to_tensor()\n+        return smart_resize(\n+            x,\n+            size=size,\n             interpolation=self._interpolation_method)\n+\n+      if tf_utils.is_ragged(inputs):\n+        size_as_shape = tf.TensorShape(size)\n+        shape = size_as_shape + inputs.shape[-1:]\n+        spec = tf.TensorSpec(shape, input_dtype)\n+        outputs = tf.map_fn(resize_to_aspect, inputs, fn_output_signature=spec)\n+      else:\n+        outputs = resize_to_aspect(inputs)\n     else:\n       outputs = tf.image.resize(\n           inputs,\n-          size=[self.height, self.width],\n+          size=size,\n           method=self._interpolation_method)\n-    outputs = tf.cast(outputs, self.compute_dtype)\n-    return outputs\n+    return tf.cast(outputs, self.compute_dtype)\n \n   def compute_output_shape(self, input_shape):\n     input_shape = tf.TensorShape(input_shape).as_list()\n\n@@ -160,6 +160,30 @@ class ResizingTest(keras_parameterized.TestCase):\n       expected_output = np.reshape(expected_output, (2, 2, 1))\n       self.assertAllEqual(expected_output, output_image)\n \n+  @parameterized.named_parameters(('crop_to_aspect_ratio_false', False),\n+                                  ('crop_to_aspect_ratio_true', True))\n+  def test_ragged_image(self, crop_to_aspect_ratio):\n+    with testing_utils.use_gpu():\n+      inputs = tf.ragged.constant([\n+          np.ones((8, 8, 1)),\n+          np.ones((8, 4, 1)),\n+          np.ones((4, 8, 1)),\n+          np.ones((2, 2, 1)),\n+      ], dtype='float32')\n+      layer = image_preprocessing.Resizing(\n+          2,\n+          2,\n+          interpolation='nearest',\n+          crop_to_aspect_ratio=crop_to_aspect_ratio)\n+      outputs = layer(inputs)\n+      expected_output = [[[[1.], [1.]], [[1.], [1.]]],\n+                         [[[1.], [1.]], [[1.], [1.]]],\n+                         [[[1.], [1.]], [[1.], [1.]]],\n+                         [[[1.], [1.]], [[1.], [1.]]]]\n+      self.assertIsInstance(outputs, tf.Tensor)\n+      self.assertNotIsInstance(outputs, tf.RaggedTensor)\n+      self.assertAllEqual(expected_output, outputs)\n+\n   @testing_utils.run_v2_only\n   def test_output_dtypes(self):\n     inputs = np.array([[[1], [2]], [[3], [4]]], dtype='float64')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ccc9357778bf3ee6d1ae7c7b77e744586a4401d8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 1033 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -3309,10 +3309,8 @@ def top_k_categorical_accuracy(y_true, y_pred, k=5):\n   Returns:\n     Top K categorical accuracy value.\n   \"\"\"\n-  return tf.cast(\n-      tf.math.in_top_k(\n-          predictions=y_pred, targets=tf.math.argmax(y_true, axis=-1), k=k),\n-      dtype=backend.floatx())\n+  y_true = tf.math.argmax(y_true, axis=-1)\n+  return sparse_top_k_categorical_accuracy(y_true, y_pred, k=k)\n \n \n @keras_export('keras.metrics.sparse_top_k_categorical_accuracy')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a8385c3835a417dc85f8bc862ef87d9c35dc9017", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1039 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -3240,10 +3240,8 @@ def categorical_accuracy(y_true, y_pred):\n   Returns:\n     Categorical accuracy values.\n   \"\"\"\n-  return tf.cast(\n-      tf.equal(\n-          tf.math.argmax(y_true, axis=-1), tf.math.argmax(y_pred, axis=-1)),\n-      backend.floatx())\n+  y_true = tf.math.argmax(y_true, axis=-1)\n+  return sparse_categorical_accuracy(y_true, y_pred)\n \n \n @keras_export('keras.metrics.sparse_categorical_accuracy')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#56c8e70854f85df4fa8bf4b91bbb264174450e85", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 257 | Lines Deleted: 230 | Files Changed: 106 | Hunks: 153 | Methods Changed: 31 | Complexity Δ (Sum/Max): 1220/197 | Churn Δ: 487 | Churn Cumulative: 81716 | Contributors (this commit): 267 | Commits (past 90d): 202 | Contributors (cumulative): 632 | DMM Complexity: 1.0\n\nDIFF:\n@@ -17,7 +17,7 @@\n import tensorflow.compat.v2 as tf\n \n from keras.benchmarks import benchmark_util\n-from keras.optimizer_v2 import adam\n+from keras.optimizers.optimizer_v2 import adam\n from tensorflow.python.platform.benchmark import ParameterizedBenchmark\n \n \n\n@@ -29,7 +29,7 @@ import time\n from keras import backend\n from keras.distribute import distributed_file_utils\n from keras.distribute import worker_training_state\n-from keras.optimizer_v2 import learning_rate_schedule\n+from keras.optimizers import learning_rate_schedule\n from keras.utils import generic_utils\n from keras.utils import io_utils\n from keras.utils import tf_utils\n\n@@ -36,8 +36,8 @@ from keras.callbacks import BackupAndRestoreExperimental\n from keras.engine import sequential\n from keras.layers import Activation\n from keras.layers import Dense\n-from keras.optimizer_v2 import gradient_descent\n-from keras.optimizer_v2 import learning_rate_schedule\n+from keras.optimizers import learning_rate_schedule\n+from keras.optimizers.optimizer_v2 import gradient_descent\n from keras.utils import io_utils\n from keras.utils import np_utils\n import numpy as np\n\n@@ -18,7 +18,7 @@ import os\n import tensorflow.compat.v2 as tf\n \n from absl.testing import parameterized\n-from keras.optimizer_v2 import adam\n+from keras.optimizers.optimizer_v2 import adam\n \n \n class TrainingCheckpointTests(tf.test.TestCase, parameterized.TestCase):\n\n@@ -20,7 +20,7 @@ from absl.testing import parameterized\n from keras import layers\n from keras import testing_utils\n from keras.engine import training\n-from keras.optimizer_v2 import gradient_descent as gradient_descent_keras\n+from keras.optimizers.optimizer_v2 import gradient_descent as gradient_descent_keras\n \n \n @testing_utils.run_v2_only\n\n@@ -24,7 +24,7 @@ import numpy as np\n import keras\n from keras.distribute import strategy_combinations\n from keras.layers import core\n-from keras.optimizer_v2 import gradient_descent\n+from keras.optimizers.optimizer_v2 import gradient_descent\n \n \n class CustomModel(tf.Module):\n@@ -82,7 +82,7 @@ class KerasModelsTest(tf.test.TestCase, parameterized.TestCase):\n \n     with distribution.scope():\n       model = _get_model()\n-      optimizer = keras.optimizer_v2.rmsprop.RMSprop()\n+      optimizer = keras.optimizers.optimizer_v2.rmsprop.RMSprop()\n \n     @tf.function\n     def train_step(replicated_inputs):\n@@ -120,7 +120,7 @@ class KerasModelsTest(tf.test.TestCase, parameterized.TestCase):\n \n     with distribution.scope():\n       model = get_subclass_model()\n-      optimizer = keras.optimizer_v2.rmsprop.RMSprop()\n+      optimizer = keras.optimizers.optimizer_v2.rmsprop.RMSprop()\n \n     @tf.function\n     def train_step(iterator):\n@@ -145,7 +145,7 @@ class KerasModelsTest(tf.test.TestCase, parameterized.TestCase):\n \n     with distribution.scope():\n       model = _get_model()\n-      optimizer = keras.optimizer_v2.rmsprop.RMSprop()\n+      optimizer = keras.optimizers.optimizer_v2.rmsprop.RMSprop()\n \n     @tf.function\n     def train_step(iterator):\n@@ -177,7 +177,7 @@ class KerasModelsTest(tf.test.TestCase, parameterized.TestCase):\n       y = keras.layers.Flatten()(y)\n       y = keras.layers.Dense(4, name=\"dense\")(y)\n       model = keras.Model(x, y)\n-      optimizer = keras.optimizer_v2.rmsprop.RMSprop()\n+      optimizer = keras.optimizers.optimizer_v2.rmsprop.RMSprop()\n \n     @tf.function\n     def train_step(iterator):\n@@ -220,7 +220,7 @@ class KerasModelsTest(tf.test.TestCase, parameterized.TestCase):\n \n     with distribution.scope():\n       model = create_lstm_model()\n-      optimizer = keras.optimizer_v2.gradient_descent.SGD()\n+      optimizer = keras.optimizers.optimizer_v2.gradient_descent.SGD()\n \n     @tf.function\n     def train_step(input_iterator):\n@@ -262,7 +262,8 @@ class KerasModelsTest(tf.test.TestCase, parameterized.TestCase):\n \n     with distribution.scope():\n       model = get_model()\n-      optimizer = keras.optimizer_v2.gradient_descent.SGD(0.1, momentum=0.01)\n+      optimizer = keras.optimizers.optimizer_v2.gradient_descent.SGD(\n+          0.1, momentum=0.01)\n       weights_file = os.path.join(self.get_temp_dir(), \".h5\")\n       model.save_weights(weights_file)\n       model2 = get_model()\n@@ -329,7 +330,8 @@ class KerasModelsTest(tf.test.TestCase, parameterized.TestCase):\n \n     with distribution.scope():\n       model = get_model()\n-      optimizer = keras.optimizer_v2.gradient_descent.SGD(0.1, momentum=0.01)\n+      optimizer = keras.optimizers.optimizer_v2.gradient_descent.SGD(\n+          0.1, momentum=0.01)\n \n     @tf.function\n     def train_step(iterator):\n@@ -369,7 +371,8 @@ class KerasModelsTest(tf.test.TestCase, parameterized.TestCase):\n \n     with distribution.scope():\n       model = get_model()\n-      optimizer = keras.optimizer_v2.gradient_descent.SGD(0.1, momentum=0.01)\n+      optimizer = keras.optimizers.optimizer_v2.gradient_descent.SGD(\n+          0.1, momentum=0.01)\n \n     @tf.function\n     def compute_loss(images, targets):\n\n@@ -19,7 +19,7 @@ import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n from tensorflow.python.distribute import values\n from keras.distribute import strategy_combinations as keras_strategy_combinations\n-from keras.optimizer_v2 import gradient_descent\n+from keras.optimizers.optimizer_v2 import gradient_descent\n \n \n class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n\n@@ -27,7 +27,7 @@ from keras import callbacks as callbacks_lib\n from keras.engine import sequential\n from keras.layers import core as core_layers\n from keras.layers.preprocessing import string_lookup\n-from keras.optimizer_v2 import gradient_descent\n+from keras.optimizers.optimizer_v2 import gradient_descent\n from keras.utils import dataset_creator\n from tensorflow.python.platform import tf_logging as logging\n \n\n@@ -36,7 +36,7 @@ from keras.distribute.strategy_combinations import strategies_minus_tpu\n from keras.distribute.strategy_combinations import tpu_strategies\n from keras.engine import base_layer_utils\n from keras.mixed_precision import policy\n-from keras.optimizer_v2 import gradient_descent as gradient_descent_keras\n+from keras.optimizers.optimizer_v2 import gradient_descent as gradient_descent_keras\n from keras.utils import losses_utils\n from keras.utils import np_utils\n \n\n@@ -18,7 +18,7 @@ import tensorflow.compat.v2 as tf\n \n from keras import callbacks\n from keras.distribute import distributed_training_utils_v1\n-from keras.optimizer_v2 import adam\n+from keras.optimizers.optimizer_v2 import adam\n \n \n class DistributedTrainingUtilsTest(tf.test.TestCase):\n\n@@ -27,7 +27,7 @@ from keras import optimizers\n from keras.distribute import distribute_coordinator_utils as dc\n from keras.distribute import distributed_training_utils as dist_utils\n from keras.engine import training_utils_v1\n-from keras.optimizer_v2 import optimizer_v2\n+from keras.optimizers.optimizer_v2 import optimizer_v2\n from keras.utils import tf_contextlib\n from keras.utils.mode_keys import ModeKeys\n from tensorflow.python.platform import tf_logging as logging\n\n@@ -23,7 +23,7 @@ from keras import backend\n from keras import testing_utils\n from keras.distribute import keras_correctness_test_base\n from keras.distribute import strategy_combinations\n-from keras.optimizer_v2 import gradient_descent as gradient_descent_keras\n+from keras.optimizers.optimizer_v2 import gradient_descent as gradient_descent_keras\n \n \n def all_strategy_combinations_with_eager_and_graph_modes():\n\n@@ -20,7 +20,7 @@ import numpy as np\n \n import keras\n from keras.distribute import keras_correctness_test_base\n-from keras.optimizer_v2 import gradient_descent as gradient_descent_keras\n+from keras.optimizers.optimizer_v2 import gradient_descent as gradient_descent_keras\n \n \n class DistributionStrategyEmbeddingModelCorrectnessTest(\n\n@@ -20,7 +20,7 @@ import numpy as np\n import keras\n from keras import testing_utils\n from keras.distribute import keras_correctness_test_base\n-from keras.optimizer_v2 import gradient_descent\n+from keras.optimizers.optimizer_v2 import gradient_descent\n \n \n @testing_utils.run_all_without_tensor_float_32(\n\n@@ -20,8 +20,8 @@ from absl.testing import parameterized\n import numpy as np\n \n import keras\n-from keras.optimizer_v2 import adam\n-from keras.optimizer_v2 import gradient_descent\n+from keras.optimizers.optimizer_v2 import adam\n+from keras.optimizers.optimizer_v2 import gradient_descent\n \n \n def get_model():\n\n@@ -18,8 +18,8 @@ from absl.testing import parameterized\n \n from keras.engine import sequential\n from keras.layers import core\n-from keras.optimizer_v2 import adagrad\n-from keras.optimizer_v2 import gradient_descent\n+from keras.optimizers.optimizer_v2 import adagrad\n+from keras.optimizers.optimizer_v2 import gradient_descent\n from keras.premade_models import linear\n from keras.premade_models import wide_deep\n from keras.utils import dataset_creator\n\n@@ -24,7 +24,7 @@ from keras.distribute import keras_correctness_test_base\n from keras.layers import recurrent as rnn_v1\n from keras.layers import recurrent_v2 as rnn_v2\n from keras.mixed_precision import policy\n-from keras.optimizer_v2 import gradient_descent as gradient_descent_keras\n+from keras.optimizers.optimizer_v2 import gradient_descent as gradient_descent_keras\n \n \n class _DistributionStrategyRnnModelCorrectnessTest(\n\n@@ -20,7 +20,7 @@ import numpy as np\n \n import keras\n from keras.distribute import keras_correctness_test_base\n-from keras.optimizer_v2 import gradient_descent as gradient_descent_keras\n+from keras.optimizers.optimizer_v2 import gradient_descent as gradient_descent_keras\n \n \n def strategies_for_stateful_embedding_model():\n\n@@ -20,7 +20,7 @@ from keras.distribute import optimizer_combinations\n from keras.distribute.test_example import batchnorm_example\n from keras.distribute.test_example import minimize_loss_example\n from keras.layers import core\n-from keras.optimizer_v2 import optimizer_v2\n+from keras.optimizers.optimizer_v2 import optimizer_v2\n import numpy\n import tensorflow.compat.v2 as tf\n \n\n@@ -23,7 +23,7 @@ import keras\n from tensorflow.python.eager import backprop\n from keras.engine import training as keras_training\n from keras.layers import core as keras_core\n-from keras.optimizer_v2 import rmsprop\n+from keras.optimizers.optimizer_v2 import rmsprop\n from keras.utils import kpl_test_utils\n from tensorflow.python.training import optimizer as optimizer_lib\n \n\n@@ -32,9 +32,9 @@ from keras import backend\n from keras import callbacks\n from keras import metrics as metrics_module\n from keras import models\n-from keras import optimizer_v1\n+from keras.optimizers import optimizer_v1\n from keras.distribute import multi_worker_testing_utils\n-from keras.optimizer_v2 import rmsprop\n+from keras.optimizers.optimizer_v2 import rmsprop\n from keras.utils import kpl_test_utils\n \n # pylint: disable=g-direct-tensorflow-import\n\n@@ -20,7 +20,7 @@ import threading\n import unittest\n import keras\n from tensorflow.python.distribute.cluster_resolver import SimpleClusterResolver\n-from keras.optimizer_v2 import gradient_descent\n+from keras.optimizers.optimizer_v2 import gradient_descent\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.training.server_lib import ClusterSpec\n \n\n@@ -14,15 +14,15 @@\n # ==============================================================================\n \"\"\"Strategy and optimizer combinations for combinations.combine().\"\"\"\n \n-from keras.optimizer_experimental import adam as adam_experimental\n-from keras.optimizer_v2 import adadelta as adadelta_keras_v2\n-from keras.optimizer_v2 import adagrad as adagrad_keras_v2\n-from keras.optimizer_v2 import adam as adam_keras_v2\n-from keras.optimizer_v2 import adamax as adamax_keras_v2\n-from keras.optimizer_v2 import ftrl as ftrl_keras_v2\n-from keras.optimizer_v2 import gradient_descent as gradient_descent_keras_v2\n-from keras.optimizer_v2 import nadam as nadam_keras_v2\n-from keras.optimizer_v2 import rmsprop as rmsprop_keras_v2\n+from keras.optimizers.optimizer_experimental import adam as adam_experimental\n+from keras.optimizers.optimizer_v2 import adadelta as adadelta_keras_v2\n+from keras.optimizers.optimizer_v2 import adagrad as adagrad_keras_v2\n+from keras.optimizers.optimizer_v2 import adam as adam_keras_v2\n+from keras.optimizers.optimizer_v2 import adamax as adamax_keras_v2\n+from keras.optimizers.optimizer_v2 import ftrl as ftrl_keras_v2\n+from keras.optimizers.optimizer_v2 import gradient_descent as gradient_descent_keras_v2\n+from keras.optimizers.optimizer_v2 import nadam as nadam_keras_v2\n+from keras.optimizers.optimizer_v2 import rmsprop as rmsprop_keras_v2\n import tensorflow.compat.v2 as tf\n \n \n\n@@ -24,7 +24,7 @@ from absl.testing import parameterized\n import keras\n from keras import testing_utils\n from keras.distribute import sidecar_evaluator as sidecar_evaluator_lib\n-from keras.optimizer_v2 import gradient_descent\n+from keras.optimizers.optimizer_v2 import gradient_descent\n import numpy as np\n import tensorflow.compat.v2 as tf\n from tensorflow.python.platform import tf_logging as logging  # pylint: disable=g-direct-tensorflow-import\n\n@@ -20,7 +20,7 @@ import numpy as np\n \n import keras\n from keras.distribute import model_collection_base\n-from keras.optimizer_v2 import gradient_descent\n+from keras.optimizers.optimizer_v2 import gradient_descent\n \n _BATCH_SIZE = 10\n \n\n@@ -16,7 +16,7 @@\n \n from keras.legacy_tf_layers import core\n from keras.legacy_tf_layers import normalization\n-from keras.optimizer_v2 import optimizer_v2\n+from keras.optimizers.optimizer_v2 import optimizer_v2\n \n import tensorflow.compat.v2 as tf\n \n\n@@ -31,7 +31,7 @@ from keras.engine import input_layer\n from keras.engine import sequential\n from keras.engine import training as training_lib\n from keras.legacy_tf_layers import core as legacy_core\n-from keras.optimizer_v2 import rmsprop\n+from keras.optimizers.optimizer_v2 import rmsprop\n from keras.utils import control_flow_util\n \n \n\n@@ -23,7 +23,7 @@ import keras\n from keras import keras_parameterized\n from keras import testing_utils\n from keras.engine import base_layer\n-from keras.optimizer_v2 import rmsprop\n+from keras.optimizers.optimizer_v2 import rmsprop\n \n \n class ControlFlowLayer1(base_layer.Layer):\n\n@@ -55,7 +55,7 @@ class SimpleBiasTest(keras_parameterized.TestCase):\n     model = testing_utils.get_model_from_layers([testing_utils.Bias()],\n                                                 input_shape=(1,))\n     model.compile(\n-        keras.optimizer_v2.gradient_descent.SGD(0.1),\n+        keras.optimizers.optimizer_v2.gradient_descent.SGD(0.1),\n         'mae',\n         run_eagerly=testing_utils.should_run_eagerly())\n     return model\n@@ -93,7 +93,7 @@ class MultipleInputTest(keras_parameterized.TestCase):\n     else:\n       model = multi_input_functional()\n     model.compile(\n-        keras.optimizer_v2.gradient_descent.SGD(0.1),\n+        keras.optimizers.optimizer_v2.gradient_descent.SGD(0.1),\n         'mae',\n         run_eagerly=testing_utils.should_run_eagerly())\n     return model\n\n@@ -23,7 +23,6 @@ import weakref\n \n from keras import backend\n from keras import callbacks as callbacks_module\n-from keras import optimizer_v1\n from keras import optimizers\n from keras.engine import base_layer\n from keras.engine import base_layer_utils\n@@ -32,7 +31,8 @@ from keras.engine import data_adapter\n from keras.engine import training_utils\n from keras.mixed_precision import loss_scale_optimizer as lso\n from keras.mixed_precision import policy\n-from keras.optimizer_experimental import optimizer as optimizer_experimental\n+from keras.optimizers import optimizer_v1\n+from keras.optimizers.optimizer_experimental import optimizer as optimizer_experimental\n from keras.saving import hdf5_format\n from keras.saving import pickle_utils\n from keras.saving import save\n\n@@ -23,7 +23,7 @@ import keras\n from keras import keras_parameterized\n from keras import metrics as metrics_module\n from keras import testing_utils\n-from keras.optimizer_v2 import rmsprop\n+from keras.optimizers.optimizer_v2 import rmsprop\n \n \n class TrainingTest(keras_parameterized.TestCase):\n\n@@ -29,7 +29,7 @@ from keras import testing_utils\n from keras.engine import input_layer\n from keras.engine import training\n from keras.engine import training_generator_v1\n-from keras.optimizer_v2 import rmsprop\n+from keras.optimizers.optimizer_v2 import rmsprop\n from keras.utils import data_utils\n \n \n\n@@ -28,8 +28,6 @@ from keras import keras_parameterized\n from keras import layers as layers_module\n from keras import losses\n from keras import metrics as metrics_module\n-from keras import optimizer_experimental\n-from keras import optimizer_v2\n from keras import testing_utils\n from keras.callbacks import Callback\n from keras.engine import input_layer\n@@ -37,6 +35,8 @@ from keras.engine import sequential\n from keras.engine import training as training_module\n from keras.engine import training_utils_v1\n from keras.layers.preprocessing import string_lookup\n+from keras.optimizers import optimizer_v2\n+from keras.optimizers.optimizer_experimental import sgd as sgd_experimental\n from keras.utils import data_utils\n from keras.utils import io_utils\n from keras.utils import np_utils\n@@ -1938,7 +1938,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     tensors = tf.random.uniform((4, 4)), tf.random.uniform((4,))\n     dataset = tf.data.Dataset.from_tensor_slices(tensors).repeat().batch(1)\n \n-    optimizer = optimizer_experimental.sgd.SGD(use_ema=True, ema_momentum=1)\n+    optimizer = sgd_experimental.SGD(use_ema=True, ema_momentum=1)\n     model.compile(optimizer, loss='mse', steps_per_execution=10)\n     initial_value = tf.Variable(model.trainable_variables[0])\n     history = model.fit(dataset, epochs=2, steps_per_epoch=10)\n\n@@ -23,7 +23,7 @@ import numpy as np\n from keras import backend\n from keras import losses\n from keras import metrics as metrics_module\n-from keras import optimizer_v1\n+from keras.optimizers import optimizer_v1\n from keras import optimizers\n from keras.distribute import distributed_training_utils\n from keras.distribute import distributed_training_utils_v1\n@@ -37,7 +37,7 @@ from keras.engine import training_utils\n from keras.engine import training_utils_v1\n from keras.mixed_precision import loss_scale_optimizer\n from keras.mixed_precision import policy\n-from keras.optimizer_v2 import optimizer_v2\n+from keras.optimizers.optimizer_v2 import optimizer_v2\n from keras.saving import saving_utils\n from keras.saving.saved_model import model_serialization\n from keras.utils import data_utils\n\n@@ -333,7 +333,7 @@ class TestStatefulLambda(keras_parameterized.TestCase):\n \n     model = testing_utils.get_model_from_layers([layer], input_shape=(10,))\n     model.compile(\n-        keras.optimizer_v2.gradient_descent.SGD(0.1),\n+        keras.optimizers.optimizer_v2.gradient_descent.SGD(0.1),\n         'mae',\n         run_eagerly=testing_utils.should_run_eagerly())\n     x, y = np.ones((10, 10), 'float32'), 2 * np.ones((10, 10), 'float32')\n@@ -427,7 +427,7 @@ class TestStatefulLambda(keras_parameterized.TestCase):\n \n     model = testing_utils.get_model_from_layers([layer], input_shape=(10,))\n     model.compile(\n-        keras.optimizer_v2.gradient_descent.SGD(0.1),\n+        keras.optimizers.optimizer_v2.gradient_descent.SGD(0.1),\n         'mae',\n         run_eagerly=testing_utils.should_run_eagerly())\n     x, y = np.ones((10, 10), 'float32'), 2 * np.ones((10, 10), 'float32')\n\n@@ -27,7 +27,7 @@ from tensorflow.python.framework import test_util\n from keras import combinations\n from keras import keras_parameterized\n from keras import testing_utils\n-from keras.optimizer_v2.rmsprop import RMSprop\n+from keras.optimizers.optimizer_v2.rmsprop import RMSprop\n \n \n @keras_parameterized.run_all_keras_modes\n\n@@ -22,7 +22,7 @@ import keras\n from keras import combinations\n from keras import testing_utils\n from keras.layers.locally_connected import locally_connected_utils\n-from keras.optimizer_v2 import rmsprop\n+from keras.optimizers.optimizer_v2 import rmsprop\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n\n@@ -25,7 +25,7 @@ import keras\n from keras import keras_parameterized\n from keras import testing_utils\n from keras.engine import keras_tensor\n-from keras.optimizer_v2 import adam\n+from keras.optimizers.optimizer_v2 import adam\n from keras.saving import model_config\n \n \n\n@@ -22,14 +22,14 @@ import threading\n from absl.testing import parameterized\n import numpy as np\n from keras.mixed_precision import autocast_variable\n-from keras.optimizer_v2 import adadelta\n-from keras.optimizer_v2 import adagrad\n-from keras.optimizer_v2 import adam\n-from keras.optimizer_v2 import adamax\n-from keras.optimizer_v2 import ftrl\n-from keras.optimizer_v2 import gradient_descent as gradient_descent_v2\n-from keras.optimizer_v2 import nadam\n-from keras.optimizer_v2 import rmsprop\n+from keras.optimizers.optimizer_v2 import adadelta\n+from keras.optimizers.optimizer_v2 import adagrad\n+from keras.optimizers.optimizer_v2 import adam\n+from keras.optimizers.optimizer_v2 import adamax\n+from keras.optimizers.optimizer_v2 import ftrl\n+from keras.optimizers.optimizer_v2 import gradient_descent as gradient_descent_v2\n+from keras.optimizers.optimizer_v2 import nadam\n+from keras.optimizers.optimizer_v2 import rmsprop\n \n maybe_distribute = tf.__internal__.test.combinations.combine(distribution=[\n     tf.__internal__.distribute.combinations.default_strategy,\n\n@@ -30,7 +30,7 @@ from keras.engine import input_spec\n from keras.mixed_precision import get_layer_policy\n from keras.mixed_precision import policy\n from keras.mixed_precision import test_util as mp_test_util\n-from keras.optimizer_v2 import gradient_descent\n+from keras.optimizers.optimizer_v2 import gradient_descent\n \n \n class MultiplyLayerWithFunction(mp_test_util.MultiplyLayer):\n\n@@ -18,7 +18,7 @@ import tensorflow.compat.v2 as tf\n \n import time\n from keras.mixed_precision import loss_scale_optimizer\n-from keras.optimizer_v2 import adam\n+from keras.optimizers.optimizer_v2 import adam\n \n \n def _get_strategy(num_gpus):\n\n@@ -17,9 +17,9 @@\n from keras import backend\n from keras import optimizers\n from keras.mixed_precision import loss_scale as keras_loss_scale_module\n-from keras.optimizer_experimental import optimizer as optimizer_experimental\n-from keras.optimizer_v2 import optimizer_v2\n-from keras.optimizer_v2 import utils as optimizer_utils\n+from keras.optimizers.optimizer_experimental import optimizer as optimizer_experimental\n+from keras.optimizers.optimizer_v2 import optimizer_v2\n+from keras.optimizers.optimizer_v2 import utils as optimizer_utils\n \n import tensorflow.compat.v2 as tf\n # pylint: disable=g-direct-tensorflow-import\n\n@@ -23,11 +23,11 @@ from keras import combinations\n from keras import optimizers\n from keras.mixed_precision import loss_scale_optimizer\n from keras.mixed_precision import test_util as mp_test_util\n-from keras.optimizer_experimental import optimizer as optimizer_experimental\n-from keras.optimizer_experimental import sgd as sgd_experimental\n-from keras.optimizer_v2 import adam\n-from keras.optimizer_v2 import gradient_descent\n-from keras.optimizer_v2 import optimizer_v2\n+from keras.optimizers.optimizer_experimental import optimizer as optimizer_experimental\n+from keras.optimizers.optimizer_experimental import sgd as sgd_experimental\n+from keras.optimizers.optimizer_v2 import adam\n+from keras.optimizers.optimizer_v2 import gradient_descent\n+from keras.optimizers.optimizer_v2 import optimizer_v2\n \n import numpy as np\n import tensorflow.compat.v2 as tf\n\n@@ -22,7 +22,7 @@ from keras import keras_parameterized\n from keras import testing_utils\n from keras.mixed_precision import loss_scale_optimizer as loss_scale_optimizer_v2\n from keras.mixed_precision import policy\n-from keras.optimizer_v2 import gradient_descent as gradient_descent_v2\n+from keras.optimizers.optimizer_v2 import gradient_descent as gradient_descent_v2\n \n \n class MixedPrecisionTest(keras_parameterized.TestCase):\n\n@@ -26,7 +26,7 @@ from keras import combinations\n from keras import keras_parameterized\n from keras import layers\n from keras import models\n-from keras import optimizer_v1\n+from keras.optimizers import optimizer_v1\n from keras import testing_utils\n from keras.applications import densenet\n from keras.applications import efficientnet\n@@ -45,7 +45,7 @@ from keras.mixed_precision import get_layer_policy\n from keras.mixed_precision import loss_scale_optimizer\n from keras.mixed_precision import policy\n from keras.mixed_precision import test_util as mp_test_util\n-from keras.optimizer_v2 import gradient_descent\n+from keras.optimizers.optimizer_v2 import gradient_descent\n from keras.saving import save\n from keras.utils import generic_utils\n \n\n@@ -22,7 +22,7 @@ from keras import testing_utils\n from keras.engine import base_layer_utils\n from keras.mixed_precision import device_compatibility_check\n from keras.mixed_precision import policy as mp_policy\n-from keras.optimizer_v2 import gradient_descent\n+from keras.optimizers.optimizer_v2 import gradient_descent\n from tensorflow.python.platform import tf_logging\n \n \n\n@@ -18,7 +18,7 @@\n import tensorflow.compat.v2 as tf\n from keras import backend\n from keras import metrics as metrics_module\n-from keras import optimizer_v1\n+from keras.optimizers import optimizer_v1\n from keras.engine import functional\n from keras.engine import sequential\n from keras.engine import training\n\n@@ -27,7 +27,7 @@ from keras import backend\n from keras import keras_parameterized\n from keras import metrics\n from keras import models\n-from keras import optimizer_v1\n+from keras.optimizers import optimizer_v1\n from keras import testing_utils\n \n \n@@ -261,7 +261,7 @@ class TestModelCloning(keras_parameterized.TestCase):\n     model = keras.Model(inputs=inputs, outputs=outputs)\n     model.compile(\n         loss=keras.losses.CategoricalCrossentropy(),\n-        optimizer=keras.optimizer_v2.rmsprop.RMSprop(lr=0.01),\n+        optimizer=keras.optimizers.optimizer_v2.rmsprop.RMSprop(lr=0.01),\n         metrics=['accuracy'])\n     keras.models.clone_model(model)\n \n@@ -452,7 +452,7 @@ class TestCloneAndBuildModel(keras_parameterized.TestCase):\n     self.assertEqual('mse', model.loss)\n     self.assertIsInstance(\n         model.optimizer,\n-        (optimizer_v1.RMSprop, keras.optimizer_v2.rmsprop.RMSprop))\n+        (optimizer_v1.RMSprop, keras.optimizers.optimizer_v2.rmsprop.RMSprop))\n \n   def _clone_and_build_test_helper(self, model, model_type):\n     inp = np.random.random((10, 4))\n@@ -549,7 +549,7 @@ class TestCloneAndBuildModel(keras_parameterized.TestCase):\n     with tf.Graph().as_default():\n       with self.session():\n         model = testing_utils.get_small_sequential_mlp(3, 4)\n-        optimizer = keras.optimizer_v2.adam.Adam()\n+        optimizer = keras.optimizers.optimizer_v2.adam.Adam()\n         model.compile(\n             optimizer, 'mse', metrics=['acc', metrics.categorical_accuracy],\n             )\n\n@@ -21,22 +21,22 @@ For more examples see the base class `tf.keras.optimizers.Optimizer`.\n import tensorflow.compat.v2 as tf\n \n from keras import backend\n-from keras.optimizer_experimental import optimizer as optimizer_experimental\n-from keras.optimizer_experimental import adadelta as adadelta_experimental\n-from keras.optimizer_experimental import adagrad as adagrad_experimental\n-from keras.optimizer_experimental import adam as adam_experimental\n-from keras.optimizer_experimental import sgd as sgd_experimental\n-from keras.optimizer_v1 import Optimizer\n-from keras.optimizer_v1 import TFOptimizer\n-from keras.optimizer_v2 import adadelta as adadelta_v2\n-from keras.optimizer_v2 import adagrad as adagrad_v2\n-from keras.optimizer_v2 import adam as adam_v2\n-from keras.optimizer_v2 import adamax as adamax_v2\n-from keras.optimizer_v2 import ftrl\n-from keras.optimizer_v2 import gradient_descent as gradient_descent_v2\n-from keras.optimizer_v2 import nadam as nadam_v2\n-from keras.optimizer_v2 import optimizer_v2\n-from keras.optimizer_v2 import rmsprop as rmsprop_v2\n+from keras.optimizers.optimizer_experimental import optimizer as optimizer_experimental\n+from keras.optimizers.optimizer_experimental import adadelta as adadelta_experimental\n+from keras.optimizers.optimizer_experimental import adagrad as adagrad_experimental\n+from keras.optimizers.optimizer_experimental import adam as adam_experimental\n+from keras.optimizers.optimizer_experimental import sgd as sgd_experimental\n+from keras.optimizers.optimizer_v1 import Optimizer\n+from keras.optimizers.optimizer_v1 import TFOptimizer\n+from keras.optimizers.optimizer_v2 import adadelta as adadelta_v2\n+from keras.optimizers.optimizer_v2 import adagrad as adagrad_v2\n+from keras.optimizers.optimizer_v2 import adam as adam_v2\n+from keras.optimizers.optimizer_v2 import adamax as adamax_v2\n+from keras.optimizers.optimizer_v2 import ftrl\n+from keras.optimizers.optimizer_v2 import gradient_descent as gradient_descent_v2\n+from keras.optimizers.optimizer_v2 import nadam as nadam_v2\n+from keras.optimizers.optimizer_v2 import optimizer_v2 as base_optimizer_v2\n+from keras.optimizers.optimizer_v2 import rmsprop as rmsprop_v2\n from keras.utils.generic_utils import deserialize_keras_object\n from keras.utils.generic_utils import serialize_keras_object\n from tensorflow.python.util.tf_export import keras_export\n@@ -128,7 +128,8 @@ def get(identifier):\n   \"\"\"\n   if isinstance(\n       identifier,\n-      (Optimizer, optimizer_v2.OptimizerV2, optimizer_experimental.Optimizer)):\n+      (Optimizer, base_optimizer_v2.OptimizerV2,\n+       optimizer_experimental.Optimizer)):\n     return identifier\n   # Wrap legacy TF optimizer instances\n   elif isinstance(identifier, tf.compat.v1.train.Optimizer):\n\n\n@@ -19,8 +19,8 @@ import math\n from absl.testing import parameterized\n \n from keras import combinations\n-from keras.optimizer_v2 import gradient_descent\n-from keras.optimizer_v2 import learning_rate_schedule\n+from keras.optimizers import learning_rate_schedule\n+from keras.optimizers.optimizer_v2 import gradient_descent\n import numpy as np\n \n import tensorflow.compat.v2 as tf\n\n@@ -17,7 +17,7 @@\n import tensorflow.compat.v2 as tf\n \n import functools\n-from keras.optimizer_v2 import learning_rate_schedule\n+from keras.optimizers import learning_rate_schedule\n from tensorflow.python.util.tf_export import tf_export\n \n \n\n@@ -19,7 +19,6 @@ import tensorflow.compat.v2 as tf\n import math\n from keras import combinations\n from keras import keras_parameterized\n-from keras.optimizer_v2 import legacy_learning_rate_decay as learning_rate_decay\n \n \n @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n\n@@ -0,0 +1,14 @@\n+# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n\n@@ -14,14 +14,13 @@\n # ==============================================================================\n \"\"\"Adadelta optimizer implementation.\"\"\"\n \n-from keras.optimizer_experimental import optimizer\n+from keras.optimizers.optimizer_experimental import optimizer\n from keras.utils import generic_utils\n import tensorflow.compat.v2 as tf\n # pylint: disable=g-direct-tensorflow-import\n from tensorflow.python.util.tf_export import keras_export\n \n \n-# pylint: disable=g-classes-have-attributes\n @generic_utils.register_keras_serializable()\n @keras_export('keras.optimizers.experimental.Adadelta', v1=[])\n class Adadelta(optimizer.Optimizer):\n@@ -40,7 +39,7 @@ class Adadelta(optimizer.Optimizer):\n   don't have to set an initial learning rate. In this version, the initial\n   learning rate can be set, as in most other Keras optimizers.\n \n-  Args:\n+  Attributes:\n     learning_rate: Initial value for the learning rate:\n       either a floating point value,\n       or a `tf.keras.optimizers.schedules.LearningRateSchedule` instance.\n\n@@ -15,14 +15,13 @@\n \"\"\"Adagrad optimizer implementation.\"\"\"\n \n from keras import initializers\n-from keras.optimizer_experimental import optimizer\n+from keras.optimizers.optimizer_experimental import optimizer\n from keras.utils import generic_utils\n import tensorflow.compat.v2 as tf\n # pylint: disable=g-direct-tensorflow-import\n from tensorflow.python.util.tf_export import keras_export\n \n \n-# pylint: disable=g-classes-have-attributes\n @generic_utils.register_keras_serializable()\n @keras_export('keras.optimizers.experimental.Adagrad', v1=[])\n class Adagrad(optimizer.Optimizer):\n@@ -33,7 +32,7 @@ class Adagrad(optimizer.Optimizer):\n   updated during training. The more updates a parameter receives,\n   the smaller the updates.\n \n-  Args:\n+  Attributes:\n     learning_rate: Initial value for the learning rate:\n       either a floating point value,\n       or a `tf.keras.optimizers.schedules.LearningRateSchedule` instance.\n\n@@ -14,14 +14,13 @@\n # ==============================================================================\n \"\"\"Adam optimizer implementation.\"\"\"\n \n-from keras.optimizer_experimental import optimizer\n+from keras.optimizers.optimizer_experimental import optimizer\n from keras.utils import generic_utils\n import tensorflow.compat.v2 as tf\n # pylint: disable=g-direct-tensorflow-import\n from tensorflow.python.util.tf_export import keras_export\n \n \n-# pylint: disable=g-classes-have-attributes\n @generic_utils.register_keras_serializable()\n @keras_export('keras.optimizers.experimental.Adam', v1=[])\n class Adam(optimizer.Optimizer):\n@@ -37,7 +36,7 @@ class Adam(optimizer.Optimizer):\n   gradients, and is well suited for problems that are large in terms of\n   data/parameters*\".\n \n-  Args:\n+  Attributes:\n     learning_rate: A `tf.Tensor`, floating point value, a schedule that is a\n       `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable\n       that takes no arguments and returns the actual value to use. The\n\n@@ -22,8 +22,8 @@ from absl import logging\n \n from keras import backend\n from keras import initializers\n-from keras.optimizer_v2 import learning_rate_schedule\n-from keras.optimizer_v2 import utils as optimizer_utils\n+from keras.optimizers import learning_rate_schedule\n+from keras.optimizers.optimizer_v2 import utils as optimizer_utils\n import tensorflow.compat.v2 as tf\n # pylint: disable=g-direct-tensorflow-import\n from tensorflow.python.util.tf_export import keras_export\n@@ -518,7 +518,6 @@ class _BaseOptimizer(tf.Module):\n     return cls(**config)\n \n \n-# pylint: disable=g-classes-have-attributes\n @keras_export(\"keras.optimizers.experimental.Optimizer\", v1=[])\n class Optimizer(_BaseOptimizer):\n   \"\"\"Abstract optimizer base class.\n@@ -526,7 +525,7 @@ class Optimizer(_BaseOptimizer):\n   This class supports distributed training. If you want to implement your own\n   optimizer, please subclass this class instead of _BaseOptimizer.\n \n-  Args:\n+  Attributes:\n     name: string. The name to use for momentum accumulator weights created by\n       the optimizer.\n     clipnorm: float. If set, the gradient of each weight is individually\n\n@@ -9,17 +9,17 @@ import re\n from absl import logging\n from absl.testing import parameterized\n import keras\n-from keras.optimizer_experimental import adadelta as adadelta_new\n-from keras.optimizer_experimental import adagrad as adagrad_new\n-from keras.optimizer_experimental import adam as adam_new\n-from keras.optimizer_experimental import rmsprop as rmsprop_new\n-from keras.optimizer_experimental import sgd as sgd_new\n-from keras.optimizer_v2 import adadelta as adadelta_old\n-from keras.optimizer_v2 import adagrad as adagrad_old\n-from keras.optimizer_v2 import adam as adam_old\n-from keras.optimizer_v2 import gradient_descent as sgd_old\n-from keras.optimizer_v2 import learning_rate_schedule\n-from keras.optimizer_v2 import rmsprop as rmsprop_old\n+from keras.optimizers import learning_rate_schedule\n+from keras.optimizers.optimizer_experimental import adadelta as adadelta_new\n+from keras.optimizers.optimizer_experimental import adagrad as adagrad_new\n+from keras.optimizers.optimizer_experimental import adam as adam_new\n+from keras.optimizers.optimizer_experimental import rmsprop as rmsprop_new\n+from keras.optimizers.optimizer_experimental import sgd as sgd_new\n+from keras.optimizers.optimizer_v2 import adadelta as adadelta_old\n+from keras.optimizers.optimizer_v2 import adagrad as adagrad_old\n+from keras.optimizers.optimizer_v2 import adam as adam_old\n+from keras.optimizers.optimizer_v2 import gradient_descent as sgd_old\n+from keras.optimizers.optimizer_v2 import rmsprop as rmsprop_old\n from keras.utils import losses_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n\n@@ -14,12 +14,11 @@\n # ==============================================================================\n \"\"\"RMSprop optimizer implementation.\"\"\"\n \n-from keras.optimizer_experimental import optimizer\n+from keras.optimizers.optimizer_experimental import optimizer\n from keras.utils import generic_utils\n import tensorflow.compat.v2 as tf\n \n \n-# pylint: disable=g-classes-have-attributes\n @generic_utils.register_keras_serializable()\n class RMSprop(optimizer.Optimizer):\n   r\"\"\"Optimizer that implements the RMSprop algorithm.\n@@ -34,7 +33,7 @@ class RMSprop(optimizer.Optimizer):\n   The centered version additionally maintains a moving average of the\n   gradients, and uses that average to estimate the variance.\n \n-  Args:\n+  Attributes:\n     learning_rate: Initial value for the learning rate:\n       either a floating point value,\n       or a `tf.keras.optimizers.schedules.LearningRateSchedule` instance.\n\n@@ -14,14 +14,13 @@\n # ==============================================================================\n \"\"\"SGD optimizer implementation.\"\"\"\n \n-from keras.optimizer_experimental import optimizer\n+from keras.optimizers.optimizer_experimental import optimizer\n from keras.utils import generic_utils\n import tensorflow.compat.v2 as tf\n # pylint: disable=g-direct-tensorflow-import\n from tensorflow.python.util.tf_export import keras_export\n \n \n-# pylint: disable=g-classes-have-attributes\n @generic_utils.register_keras_serializable()\n @keras_export('keras.optimizers.experimental.SGD', v1=[])\n class SGD(optimizer.Optimizer):\n@@ -47,7 +46,7 @@ class SGD(optimizer.Optimizer):\n   w = w + momentum * velocity - learning_rate * g\n   ```\n \n-  Args:\n+  Attributes:\n     learning_rate: A `Tensor`, floating point value, or a schedule that is a\n       `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable\n       that takes no arguments and returns the actual value to use. The\n\n\n@@ -0,0 +1,14 @@\n+# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n\n@@ -19,7 +19,7 @@ import tensorflow.compat.v2 as tf\n \n import numpy as np\n from keras import backend_config\n-from keras.optimizer_v2 import optimizer_v2\n+from keras.optimizers.optimizer_v2 import optimizer_v2\n from tensorflow.python.util.tf_export import keras_export\n \n \n\n@@ -19,7 +19,7 @@ import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n import numpy as np\n from keras import combinations\n-from keras.optimizer_v2 import adadelta\n+from keras.optimizers.optimizer_v2 import adadelta\n \n _DATA_TYPES = [\n     tf.half, tf.float32, tf.float64, tf.complex64,\n\n@@ -19,7 +19,7 @@ import tensorflow.compat.v2 as tf\n \n import numpy as np\n from keras import backend_config\n-from keras.optimizer_v2 import optimizer_v2\n+from keras.optimizers.optimizer_v2 import optimizer_v2\n from tensorflow.python.util.tf_export import keras_export\n \n \n\n@@ -21,8 +21,8 @@ import copy\n from absl.testing import parameterized\n import numpy as np\n from keras import combinations\n-from keras.optimizer_v2 import adagrad\n-from keras.optimizer_v2 import learning_rate_schedule\n+from keras.optimizers.optimizer_v2 import adagrad\n+from keras.optimizers import learning_rate_schedule\n \n _DATA_TYPES = [\n     tf.half, tf.float32, tf.float64, tf.complex64,\n\n@@ -16,7 +16,7 @@\n \n import tensorflow.compat.v2 as tf\n from keras import backend_config\n-from keras.optimizer_v2 import optimizer_v2\n+from keras.optimizers.optimizer_v2 import optimizer_v2\n from tensorflow.python.util.tf_export import keras_export\n \n \n\n@@ -19,9 +19,9 @@ import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n import numpy as np\n from keras import combinations\n-from keras import optimizer_v1\n-from keras.optimizer_v2 import adam\n-from keras.optimizer_v2 import learning_rate_schedule\n+from keras.optimizers import optimizer_v1\n+from keras.optimizers.optimizer_v2 import adam\n+from keras.optimizers import learning_rate_schedule\n \n \n def adam_update_numpy(param,\n\n@@ -16,7 +16,7 @@\n \n import tensorflow.compat.v2 as tf\n from keras import backend_config\n-from keras.optimizer_v2 import optimizer_v2\n+from keras.optimizers.optimizer_v2 import optimizer_v2\n from tensorflow.python.util.tf_export import keras_export\n \n \n\n@@ -19,7 +19,7 @@ import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n import numpy as np\n from keras import combinations\n-from keras.optimizer_v2 import adamax\n+from keras.optimizers.optimizer_v2 import adamax\n \n \n def adamax_update_numpy(param,\n\n@@ -13,12 +13,12 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Ftrl-proximal optimizer implementation.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n+# pylint: disable=g-bad-import-order\n # pylint: disable=g-classes-have-attributes\n \n-from keras.optimizer_v2 import optimizer_v2\n-from tensorflow.python.util.tf_export import keras_export\n+import tensorflow.compat.v2 as tf\n+from keras.optimizers.optimizer_v2 import optimizer_v2\n+from tensorflow.python.util.tf_export import keras_export  # pylint: disable=g-direct-tensorflow-import\n \n \n @keras_export('keras.optimizers.Ftrl')\n\n@@ -17,7 +17,7 @@\n import tensorflow.compat.v2 as tf\n \n import numpy as np\n-from keras.optimizer_v2 import ftrl\n+from keras.optimizers.optimizer_v2 import ftrl\n \n \n class FtrlOptimizerTest(tf.test.TestCase):\n\n@@ -13,10 +13,11 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"SGD optimizer implementation.\"\"\"\n-\n+# pylint: disable=g-bad-import-order\n+# pylint: disable=g-classes-have-attributes\n import tensorflow.compat.v2 as tf\n-from keras.optimizer_v2 import optimizer_v2\n-from tensorflow.python.util.tf_export import keras_export\n+from keras.optimizers.optimizer_v2 import optimizer_v2\n+from tensorflow.python.util.tf_export import keras_export  # pylint: disable=g-direct-tensorflow-import\n \n \n @keras_export(\"keras.optimizers.SGD\")\n\n@@ -19,8 +19,8 @@ import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n import numpy as np\n from keras import combinations\n-from keras.optimizer_v2 import gradient_descent\n-from keras.optimizer_v2 import learning_rate_schedule\n+from keras.optimizers.optimizer_v2 import gradient_descent\n+from keras.optimizers import learning_rate_schedule\n \n \n class GradientDescentOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n\n@@ -16,8 +16,8 @@\n \n import tensorflow.compat.v2 as tf\n from keras import backend_config\n-from keras.optimizer_v2 import learning_rate_schedule\n-from keras.optimizer_v2 import optimizer_v2\n+from keras.optimizers import learning_rate_schedule\n+from keras.optimizers.optimizer_v2 import optimizer_v2\n from tensorflow.python.util.tf_export import keras_export\n \n \n\n@@ -17,7 +17,7 @@\n import tensorflow.compat.v2 as tf\n \n import numpy as np\n-from keras.optimizer_v2 import nadam\n+from keras.optimizers.optimizer_v2 import nadam\n \n \n def get_beta_accumulators(opt, dtype):\n\n@@ -23,8 +23,8 @@ import warnings\n from keras import backend\n from keras import initializers\n from keras.engine import base_layer_utils\n-from keras.optimizer_v2 import learning_rate_schedule\n-from keras.optimizer_v2 import utils as optimizer_utils\n+from keras.optimizers import learning_rate_schedule\n+from keras.optimizers.optimizer_v2 import utils as optimizer_utils\n from keras.utils import generic_utils\n from keras.utils import layer_utils\n from keras.utils import tf_inspect\n\n@@ -28,23 +28,23 @@ from keras import callbacks\n from keras import combinations\n from keras import keras_parameterized\n from keras import losses\n-from keras import optimizer_v1\n+from keras.optimizers import optimizer_v1\n from keras import testing_utils\n from keras.engine import input_layer\n from keras.engine import sequential\n from keras.engine import training\n from keras.layers import core\n from keras.layers import regularization\n-from keras.optimizer_v2 import adadelta\n-from keras.optimizer_v2 import adagrad\n-from keras.optimizer_v2 import adam\n-from keras.optimizer_v2 import adamax\n-from keras.optimizer_v2 import ftrl\n-from keras.optimizer_v2 import gradient_descent\n-from keras.optimizer_v2 import learning_rate_schedule\n-from keras.optimizer_v2 import nadam\n-from keras.optimizer_v2 import optimizer_v2\n-from keras.optimizer_v2 import rmsprop\n+from keras.optimizers.optimizer_v2 import adadelta\n+from keras.optimizers.optimizer_v2 import adagrad\n+from keras.optimizers.optimizer_v2 import adam\n+from keras.optimizers.optimizer_v2 import adamax\n+from keras.optimizers.optimizer_v2 import ftrl\n+from keras.optimizers.optimizer_v2 import gradient_descent\n+from keras.optimizers import learning_rate_schedule\n+from keras.optimizers.optimizer_v2 import nadam\n+from keras.optimizers.optimizer_v2 import optimizer_v2\n+from keras.optimizers.optimizer_v2 import rmsprop\n from keras.utils import np_utils\n \n \n\n@@ -19,7 +19,7 @@ import tensorflow.compat.v2 as tf\n \n import numpy as np\n from keras import backend_config\n-from keras.optimizer_v2 import optimizer_v2\n+from keras.optimizers.optimizer_v2 import optimizer_v2\n from tensorflow.python.util.tf_export import keras_export\n \n \n\n@@ -25,8 +25,8 @@ import numpy as np\n from tensorflow.python.framework import test_util\n from keras import combinations\n from keras import testing_utils\n-from keras.optimizer_v2 import learning_rate_schedule\n-from keras.optimizer_v2 import rmsprop\n+from keras.optimizers import learning_rate_schedule\n+from keras.optimizers.optimizer_v2 import rmsprop\n \n _DATA_TYPES = [\n     tf.half, tf.float32, tf.float64, tf.complex64,\n\n\n@@ -23,8 +23,8 @@ import numpy as np\n \n import keras\n from keras import keras_parameterized\n-from keras import optimizer_v1\n from keras import testing_utils\n+from keras.optimizers import optimizer_v1\n from keras.utils import np_utils\n from tensorflow.python.training.adam import AdamOptimizer\n from tensorflow.python.training.experimental.loss_scale_optimizer import MixedPrecisionLossScaleOptimizer\n\n@@ -25,7 +25,7 @@ from keras.engine import sequential\n from keras.engine import training\n from keras.feature_column import dense_features_v2\n from keras.layers import core\n-from keras.optimizer_v2 import gradient_descent\n+from keras.optimizers.optimizer_v2 import gradient_descent\n from keras.premade_models import linear\n \n \n\n@@ -24,7 +24,7 @@ from keras.engine import sequential\n from keras.engine import training\n from keras.feature_column import dense_features_v2\n from keras.layers import core\n-from keras.optimizer_v2 import gradient_descent\n+from keras.optimizers.optimizer_v2 import gradient_descent\n from keras.premade_models import linear\n from keras.premade_models import wide_deep\n \n\n@@ -23,8 +23,8 @@ import os\n import numpy as np\n \n from keras import backend\n-from keras import optimizer_v1\n-from keras.optimizer_experimental import optimizer as optimizer_experimental\n+from keras.optimizers import optimizer_v1\n+from keras.optimizers.optimizer_experimental import optimizer as optimizer_experimental\n from keras.saving import model_config as model_config_lib\n from keras.saving import saving_utils\n from keras.saving.saved_model import json_utils\n\n@@ -26,7 +26,7 @@ import keras\n from keras import keras_parameterized\n from keras import layers\n from keras import losses\n-from keras import optimizer_v2\n+from keras.optimizers import optimizer_v2\n from keras import testing_utils\n from keras.utils import generic_utils\n from keras.utils import losses_utils\n\n@@ -26,7 +26,7 @@ import keras\n from keras import keras_parameterized\n from keras import layers\n from keras import metrics\n-from keras import optimizer_v2\n+from keras.optimizers import optimizer_v2\n from keras import testing_utils\n from keras.utils import generic_utils\n \n\n@@ -30,7 +30,7 @@ import keras\n from keras import combinations\n from keras import keras_parameterized\n from keras import losses\n-from keras import optimizer_v1\n+from keras.optimizers import optimizer_v1\n from keras import optimizers\n from keras import testing_utils\n from keras.engine import functional\n@@ -458,7 +458,7 @@ class TestWholeModelSaving(keras_parameterized.TestCase):\n           input_shape=(3,))\n       model.compile(\n           loss=keras.losses.MSE,\n-          optimizer=keras.optimizer_v2.rmsprop.RMSprop(lr=0.0001),\n+          optimizer=keras.optimizers.optimizer_v2.rmsprop.RMSprop(lr=0.0001),\n           metrics=[\n               keras.metrics.categorical_accuracy,\n               keras.metrics.CategoricalCrossentropy(\n@@ -902,15 +902,16 @@ class TestWholeModelSaving(keras_parameterized.TestCase):\n       model = keras.models.Sequential([keras.layers.Dense(1, input_shape=(3,))])\n       # Set the model's optimizer but don't compile. This can happen if the\n       # model is trained with a custom training loop.\n-      model.optimizer = keras.optimizer_v2.rmsprop.RMSprop(lr=0.0001)\n+      model.optimizer = keras.optimizers.optimizer_v2.rmsprop.RMSprop(lr=0.0001)\n       if not tf.executing_eagerly():\n         session.run([v.initializer for v in model.variables])\n       model.save(saved_model_dir, save_format=save_format)\n \n       if save_format in ['tf', 'tensorflow']:\n         loaded = keras.models.load_model(saved_model_dir)\n-        self.assertIsInstance(loaded.optimizer,\n-                              keras.optimizer_v2.optimizer_v2.OptimizerV2)\n+        self.assertIsInstance(\n+            loaded.optimizer,\n+            keras.optimizers.optimizer_v2.optimizer_v2.OptimizerV2)\n \n   @combinations.generate(combinations.combine(mode=['eager']))\n   def test_functional_model_with_getitem_op_layer(self):\n\n@@ -26,7 +26,7 @@ import numpy as np\n import keras\n from keras import combinations\n from keras import keras_parameterized\n-from keras import optimizer_v1\n+from keras.optimizers import optimizer_v1\n from keras import testing_utils\n from keras.engine import training\n from keras.saving import hdf5_format\n\n@@ -20,7 +20,7 @@ import types\n from keras import backend\n from keras import regularizers\n from keras.engine import input_spec\n-from keras.optimizer_v2 import optimizer_v2\n+from keras.optimizers.optimizer_v2 import optimizer_v2\n from keras.protobuf import saved_metadata_pb2\n from keras.protobuf import versions_pb2\n from keras.saving import saving_utils\n\n@@ -17,8 +17,8 @@\n import warnings\n \n from keras import backend\n-from keras import optimizer_v1\n-from keras.optimizer_v2 import optimizer_v2\n+from keras.optimizers import optimizer_v1\n+from keras.optimizers.optimizer_v2 import optimizer_v2\n from keras.saving import model_config\n from keras.saving import saving_utils\n from keras.saving import utils_v1 as model_utils\n\n@@ -24,10 +24,10 @@ from absl.testing import parameterized\n import numpy as np\n \n import keras\n-from keras import optimizer_v1\n+from keras.optimizers import optimizer_v1\n from keras.engine import training as model_lib\n-from keras.optimizer_v2 import adadelta\n-from keras.optimizer_v2 import rmsprop\n+from keras.optimizers.optimizer_v2 import adadelta\n+from keras.optimizers.optimizer_v2 import rmsprop\n from keras.saving import saved_model_experimental as keras_saved_model\n from keras.saving import utils_v1 as model_utils\n from keras.utils import control_flow_util\n\n@@ -22,7 +22,7 @@ import copy\n import os\n from keras import backend\n from keras import losses\n-from keras import optimizer_v1\n+from keras.optimizers import optimizer_v1\n from keras import optimizers\n from keras.engine import base_layer_utils\n from keras.utils import generic_utils\n@@ -151,7 +151,7 @@ def trace_model_call(model, input_signature=None):\n def model_metadata(model, include_optimizer=True, require_config=True):\n   \"\"\"Returns a dictionary containing the model metadata.\"\"\"\n   from keras import __version__ as keras_version  # pylint: disable=g-import-not-at-top\n-  from keras.optimizer_v2 import optimizer_v2  # pylint: disable=g-import-not-at-top\n+  from keras.optimizers.optimizer_v2 import optimizer_v2  # pylint: disable=g-import-not-at-top\n \n   model_config = {'class_name': model.__class__.__name__}\n   try:\n\n@@ -27,7 +27,7 @@ from keras import keras_parameterized\n from keras import testing_utils\n from keras.engine import sequential\n from keras.feature_column import dense_features\n-from keras.optimizer_v2 import gradient_descent\n+from keras.optimizers.optimizer_v2 import gradient_descent\n from keras.saving import saving_utils\n \n \n\n@@ -25,13 +25,13 @@ from keras import backend\n from keras import layers\n from keras import models\n from keras.engine import base_layer_utils\n-from keras.optimizer_v2 import adadelta as adadelta_v2\n-from keras.optimizer_v2 import adagrad as adagrad_v2\n-from keras.optimizer_v2 import adam as adam_v2\n-from keras.optimizer_v2 import adamax as adamax_v2\n-from keras.optimizer_v2 import gradient_descent as gradient_descent_v2\n-from keras.optimizer_v2 import nadam as nadam_v2\n-from keras.optimizer_v2 import rmsprop as rmsprop_v2\n+from keras.optimizers.optimizer_v2 import adadelta as adadelta_v2\n+from keras.optimizers.optimizer_v2 import adagrad as adagrad_v2\n+from keras.optimizers.optimizer_v2 import adam as adam_v2\n+from keras.optimizers.optimizer_v2 import adamax as adamax_v2\n+from keras.optimizers.optimizer_v2 import gradient_descent as gradient_descent_v2\n+from keras.optimizers.optimizer_v2 import nadam as nadam_v2\n+from keras.optimizers.optimizer_v2 import rmsprop as rmsprop_v2\n from keras.utils import tf_contextlib\n from keras.utils import tf_inspect\n import numpy as np\n\n@@ -22,7 +22,7 @@ from keras import keras_parameterized\n from keras import layers\n from keras import losses\n from keras import Model\n-from keras import optimizer_v2\n+from keras.optimizers import optimizer_v2\n from keras import Sequential\n from keras import testing_utils\n from tensorflow.python.platform import tf_logging as logging\n\n@@ -61,7 +61,7 @@ class LayerWithTrainingArg(keras.layers.Layer):\n \n \n def add_loss_step(defun):\n-  optimizer = keras.optimizer_v2.adam.Adam()\n+  optimizer = keras.optimizers.optimizer_v2.adam.Adam()\n   model = testing_utils.get_model_from_layers([LayerWithLosses()],\n                                               input_shape=(10,))\n \n@@ -82,7 +82,7 @@ def add_loss_step(defun):\n \n \n def batch_norm_step(defun):\n-  optimizer = keras.optimizer_v2.adadelta.Adadelta()\n+  optimizer = keras.optimizers.optimizer_v2.adadelta.Adadelta()\n   model = testing_utils.get_model_from_layers([\n       keras.layers.BatchNormalization(momentum=0.9),\n       keras.layers.Dense(1, kernel_initializer='zeros', activation='softmax')\n@@ -105,7 +105,7 @@ def batch_norm_step(defun):\n \n \n def add_metric_step(defun):\n-  optimizer = keras.optimizer_v2.rmsprop.RMSprop()\n+  optimizer = keras.optimizers.optimizer_v2.rmsprop.RMSprop()\n   model = testing_utils.get_model_from_layers([\n       LayerWithMetrics(),\n       keras.layers.Dense(1, kernel_initializer='zeros', activation='softmax')\n\n@@ -67,7 +67,7 @@ class VectorClassificationIntegrationTest(keras_parameterized.TestCase):\n         input_shape=x_train.shape[1:])\n     model.compile(\n         loss='categorical_crossentropy',\n-        optimizer=keras.optimizer_v2.adam.Adam(0.005),\n+        optimizer=keras.optimizers.optimizer_v2.adam.Adam(0.005),\n         metrics=['acc'],\n         run_eagerly=testing_utils.should_run_eagerly())\n     history = model.fit(x_train, y_train, epochs=10, batch_size=10,\n@@ -103,7 +103,7 @@ class VectorClassificationIntegrationTest(keras_parameterized.TestCase):\n     model = keras.models.Model(x, y)\n     model.compile(\n         loss='categorical_crossentropy',\n-        optimizer=keras.optimizer_v2.adam.Adam(0.005),\n+        optimizer=keras.optimizers.optimizer_v2.adam.Adam(0.005),\n         metrics=['acc'],\n         run_eagerly=testing_utils.should_run_eagerly())\n     self.assertLen(model.losses, 2)\n@@ -143,7 +143,7 @@ class SequentialIntegrationTest(KerasIntegrationTest):\n     ])\n     model.compile(\n         loss='categorical_crossentropy',\n-        optimizer=keras.optimizer_v2.adam.Adam(0.005),\n+        optimizer=keras.optimizers.optimizer_v2.adam.Adam(0.005),\n         metrics=['acc'],\n         run_eagerly=testing_utils.should_run_eagerly())\n     model.fit(x_train, y_train, epochs=1, batch_size=10,\n@@ -156,7 +156,7 @@ class SequentialIntegrationTest(KerasIntegrationTest):\n \n     model.compile(\n         loss='categorical_crossentropy',\n-        optimizer=keras.optimizer_v2.adam.Adam(0.005),\n+        optimizer=keras.optimizers.optimizer_v2.adam.Adam(0.005),\n         metrics=['acc'],\n         run_eagerly=testing_utils.should_run_eagerly())\n     history = model.fit(x_train, y_train, epochs=10, batch_size=10,\n@@ -192,7 +192,7 @@ class TimeseriesClassificationIntegrationTest(keras_parameterized.TestCase):\n         layers, input_shape=x_train.shape[1:])\n     model.compile(\n         loss='categorical_crossentropy',\n-        optimizer=keras.optimizer_v2.adam.Adam(0.005),\n+        optimizer=keras.optimizers.optimizer_v2.adam.Adam(0.005),\n         metrics=['acc'],\n         run_eagerly=testing_utils.should_run_eagerly())\n     history = model.fit(x_train, y_train, epochs=15, batch_size=10,\n@@ -222,7 +222,7 @@ class TimeseriesClassificationIntegrationTest(keras_parameterized.TestCase):\n                                                   dtype=tf.float32)))\n       model.compile(\n           loss='categorical_crossentropy',\n-          optimizer=keras.optimizer_v2.adam.Adam(0.005),\n+          optimizer=keras.optimizers.optimizer_v2.adam.Adam(0.005),\n           metrics=['acc'],\n           run_eagerly=testing_utils.should_run_eagerly())\n \n@@ -261,7 +261,7 @@ class ImageClassificationIntegrationTest(keras_parameterized.TestCase):\n         layers, input_shape=x_train.shape[1:])\n     model.compile(\n         loss='categorical_crossentropy',\n-        optimizer=keras.optimizer_v2.adam.Adam(0.005),\n+        optimizer=keras.optimizers.optimizer_v2.adam.Adam(0.005),\n         metrics=['acc'],\n         run_eagerly=testing_utils.should_run_eagerly())\n     history = model.fit(x_train, y_train, epochs=10, batch_size=10,\n@@ -305,7 +305,7 @@ class ActivationV2IntegrationTest(keras_parameterized.TestCase):\n \n     model.compile(\n         loss='categorical_crossentropy',\n-        optimizer=keras.optimizer_v2.adam.Adam(0.005),\n+        optimizer=keras.optimizers.optimizer_v2.adam.Adam(0.005),\n         metrics=['accuracy'],\n         run_eagerly=testing_utils.should_run_eagerly())\n     model.fit(x_train, y_train, epochs=2, batch_size=10,\n\n@@ -57,7 +57,7 @@ class MemoryCheckerTest(tf.test.TestCase):\n       ])\n \n       model.compile(\n-          optimizer=keras.optimizer_v2.gradient_descent.SGD(lr=0.01),\n+          optimizer=keras.optimizers.optimizer_v2.gradient_descent.SGD(lr=0.01),\n           loss='mean_squared_error',\n           metrics=['accuracy'])\n       states = [[1] * rows * columns for _ in range(20)]\n\n@@ -25,7 +25,7 @@ import numpy as np\n \n import keras\n from keras import keras_parameterized\n-from keras import optimizer_v1\n+from keras.optimizers import optimizer_v1\n from keras import testing_utils\n from keras.tests import model_architectures\n \n\n@@ -19,7 +19,7 @@ import tensorflow.compat.v2 as tf\n import os\n from tensorflow.python.framework import test_util\n from keras.layers import core\n-from keras.optimizer_v2 import adam\n+from keras.optimizers.optimizer_v2 import adam\n \n \n class _ModelWithOptimizerUsingDefun(tf.train.Checkpoint):\n\n@@ -21,7 +21,7 @@ import numpy as np\n from keras import keras_parameterized\n from keras import layers\n from keras import metrics\n-from keras import optimizer_v2\n+from keras.optimizers import optimizer_v2\n from keras import testing_utils\n \n \n\n@@ -28,7 +28,7 @@ from keras.engine import sequential\n from keras.engine import training\n from keras.layers import core\n from keras.layers import reshaping\n-from keras.optimizer_v2 import adam\n+from keras.optimizers.optimizer_v2 import adam\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.training.tracking import util as trackable_utils\n \n\n@@ -18,7 +18,7 @@ from tensorflow.compiler.tests import xla_test\n import tensorflow.compat.v2 as tf\n from keras.engine import training\n from keras.layers import core\n-from keras.optimizer_v2 import adam\n+from keras.optimizers.optimizer_v2 import adam\n from tensorflow.python.training.tracking import util as trackable_utils\n \n \n\n@@ -24,7 +24,7 @@ from keras.distribute import multi_worker_testing_utils\n from keras.engine import data_adapter\n from keras.engine import sequential\n from keras.layers import core as core_layers\n-from keras.optimizer_v2 import gradient_descent\n+from keras.optimizers.optimizer_v2 import gradient_descent\n from keras.utils import dataset_creator\n from tensorflow.python.training.server_lib import ClusterSpec\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
