{"custom_id": "keras#00135a70697dc9b4611971e5833e2d84501c3da7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 839 | Lines Deleted: 348 | Files Changed: 46 | Hunks: 190 | Methods Changed: 85 | Complexity Δ (Sum/Max): 33/8 | Churn Δ: 1187 | Churn Cumulative: 92273 | Contributors (this commit): 274 | Commits (past 90d): 193 | Contributors (cumulative): 509 | DMM Complexity: 0.6509433962264151\n\nDIFF:\n@@ -44,7 +44,7 @@ _TF_ACTIVATIONS_V2 = {\n @keras_export('keras.activations.softmax')\n @tf.__internal__.dispatch.add_dispatch_support\n def softmax(x, axis=-1):\n-  \"\"\"Softmax converts a real vector to a vector of categorical probabilities.\n+  \"\"\"Softmax converts a vector of values to a probability distribution.\n \n   The elements of the output vector are in range (0, 1) and sum to 1.\n \n@@ -68,13 +68,24 @@ def softmax(x, axis=-1):\n     Tensor, output of softmax transformation (all values are non-negative\n       and sum to 1).\n \n-  Raises:\n-      ValueError: In case `dim(x) == 1`.\n+  Examples:\n+\n+  **Example 1: standalone usage**\n+\n+  >>> inputs = tf.random.normal(shape=(32, 10))\n+  >>> outputs = tf.keras.activations.softmax(inputs)\n+  >>> tf.reduce_sum(outputs[0, :])  # Each sample in the batch now sums to 1\n+  <tf.Tensor: shape=(), dtype=float32, numpy=1.0000001>\n+\n+  **Example 2: usage in a `Dense` layer**\n+\n+  >>> layer = tf.keras.layers.Dense(32, activation=tf.keras.activations.softmax)\n   \"\"\"\n-  rank = x.shape.rank\n-  if rank == 2:\n-    output = tf.compat.v1.math.softmax(x)\n-  elif rank > 2:\n+  if x.shape.rank > 1:\n+    if isinstance(axis, int):\n+      output = tf.compat.v1.math.softmax(x, axis=axis)\n+    else:\n+      # nn.softmax does not support tuple axis.\n       e = tf.exp(x - tf.reduce_max(x, axis=axis, keepdims=True))\n       s = tf.reduce_sum(e, axis=axis, keepdims=True)\n       output = e / s\n\n@@ -92,6 +92,26 @@ class KerasActivationsTest(tf.test.TestCase, parameterized.TestCase):\n     with self.assertRaises(ValueError):\n       activations.softmax(x)\n \n+  def test_softmax_2d_axis_0(self):\n+    x = backend.placeholder(ndim=2)\n+    f = backend.function([x], [activations.softmax(x, axis=0)])\n+    test_values = np.random.random((2, 5))\n+    result = f([test_values])[0]\n+    expected = np.zeros((2, 5))\n+    for i in range(5):\n+      expected[:, i] = _ref_softmax(test_values[:, i])\n+    self.assertAllClose(result, expected, rtol=1e-05)\n+\n+  def test_softmax_3d_axis_tuple(self):\n+    x = backend.placeholder(ndim=3)\n+    f = backend.function([x], [activations.softmax(x, axis=(1, 2))])\n+    test_values = np.random.random((2, 3, 5))\n+    result = f([test_values])[0]\n+    expected = np.zeros((2, 3, 5))\n+    for i in range(2):\n+      expected[i, :, :] = _ref_softmax(test_values[i, :, :])\n+    self.assertAllClose(result, expected, rtol=1e-05)\n+\n   def test_temporal_softmax(self):\n     x = backend.placeholder(shape=(2, 2, 3))\n     f = backend.function([x], [activations.softmax(x)])\n\n@@ -142,13 +142,22 @@ def DenseNet(\n   - [Densely Connected Convolutional Networks](\n       https://arxiv.org/abs/1608.06993) (CVPR 2017)\n \n-  Optionally loads weights pre-trained on ImageNet.\n-  Note that the data format convention used by the model is\n-  the one specified in your Keras config at `~/.keras/keras.json`.\n+  This function returns a Keras image classification model,\n+  optionally loaded with weights pre-trained on ImageNet.\n+\n+  For image classification use cases, see\n+  [this page for detailed examples](\n+    https://keras.io/api/applications/#usage-examples-for-image-classification-models).\n+\n+  For transfer learning use cases, make sure to read the\n+  [guide to transfer learning & fine-tuning](\n+    https://keras.io/guides/transfer_learning/).\n \n   Note: each Keras Application expects a specific kind of input preprocessing.\n   For DenseNet, call `tf.keras.applications.densenet.preprocess_input` on your\n   inputs before passing them to the model.\n+  `densenet.preprocess_input` will scale pixels between 0 and 1 and then\n+  will normalize each channel with respect to the ImageNet dataset statistics.\n \n   Args:\n     blocks: numbers of building blocks for the four dense layers.\n@@ -184,15 +193,11 @@ def DenseNet(\n     classifier_activation: A `str` or callable. The activation function to use\n       on the \"top\" layer. Ignored unless `include_top=True`. Set\n       `classifier_activation=None` to return the logits of the \"top\" layer.\n+      When loading pretrained weights, `classifier_activation` can only\n+      be `None` or `\"softmax\"`.\n \n   Returns:\n     A `keras.Model` instance.\n-\n-  Raises:\n-    ValueError: in case of invalid argument for `weights`,\n-      or invalid input shape.\n-    ValueError: if `classifier_activation` is not `softmax` or `None` when\n-      using a pretrained top layer.\n   \"\"\"\n   if not (weights in {'imagenet', None} or tf.io.gfile.exists(weights)):\n     raise ValueError('The `weights` argument should be either '\n\n@@ -150,10 +150,23 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n   - [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](\n       https://arxiv.org/abs/1905.11946) (ICML 2019)\n \n-  Optionally loads weights pre-trained on ImageNet.\n-  Note that the data format convention used by the model is\n-  the one specified in your Keras config at `~/.keras/keras.json`.\n-  If you have never configured it, it defaults to `\"channels_last\"`.\n+  This function returns a Keras image classification model,\n+  optionally loaded with weights pre-trained on ImageNet.\n+\n+  For image classification use cases, see\n+  [this page for detailed examples](\n+    https://keras.io/api/applications/#usage-examples-for-image-classification-models).\n+\n+  For transfer learning use cases, make sure to read the\n+  [guide to transfer learning & fine-tuning](\n+    https://keras.io/guides/transfer_learning/).\n+\n+  Note: each Keras Application expects a specific kind of input preprocessing.\n+  For EfficientNet, input preprocessing is included as part of the model\n+  (as a `Rescaling` layer), and thus\n+  `tf.keras.applications.efficientnet.preprocess_input` is actually a\n+  pass-through function. EfficientNet models expect their inputs to be float\n+  tensors of pixels with values in the [0-255] range.\n \n   Args:\n     include_top: Whether to include the fully-connected\n@@ -186,6 +199,8 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n         on the \"top\" layer. Ignored unless `include_top=True`. Set\n         `classifier_activation=None` to return the logits of the \"top\" layer.\n         Defaults to 'softmax'.\n+        When loading pretrained weights, `classifier_activation` can only\n+        be `None` or `\"softmax\"`.\n \n   Returns:\n     A `keras.Model` instance.\n@@ -211,14 +226,6 @@ def EfficientNet(\n     classifier_activation='softmax'):\n   \"\"\"Instantiates the EfficientNet architecture using given scaling coefficients.\n \n-  Reference:\n-  - [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](\n-      https://arxiv.org/abs/1905.11946) (ICML 2019)\n-\n-  Optionally loads weights pre-trained on ImageNet.\n-  Note that the data format convention used by the model is\n-  the one specified in your Keras config at `~/.keras/keras.json`.\n-\n   Args:\n     width_coefficient: float, scaling coefficient for network width.\n     depth_coefficient: float, scaling coefficient for network depth.\n\n@@ -58,14 +58,23 @@ def InceptionResNetV2(include_top=True,\n      Residual Connections on Learning](https://arxiv.org/abs/1602.07261)\n     (AAAI 2017)\n \n-  Optionally loads weights pre-trained on ImageNet.\n-  Note that the data format convention used by the model is\n-  the one specified in your Keras config at `~/.keras/keras.json`.\n+  This function returns a Keras image classification model,\n+  optionally loaded with weights pre-trained on ImageNet.\n+\n+  For image classification use cases, see\n+  [this page for detailed examples](\n+    https://keras.io/api/applications/#usage-examples-for-image-classification-models).\n+\n+  For transfer learning use cases, make sure to read the\n+  [guide to transfer learning & fine-tuning](\n+    https://keras.io/guides/transfer_learning/).\n \n   Note: each Keras Application expects a specific kind of input preprocessing.\n   For InceptionResNetV2, call\n   `tf.keras.applications.inception_resnet_v2.preprocess_input`\n   on your inputs before passing them to the model.\n+  `inception_resnet_v2.preprocess_input`\n+  will scale input pixels between -1 and 1.\n \n   Args:\n     include_top: whether to include the fully-connected\n@@ -97,16 +106,12 @@ def InceptionResNetV2(include_top=True,\n     classifier_activation: A `str` or callable. The activation function to use\n       on the \"top\" layer. Ignored unless `include_top=True`. Set\n       `classifier_activation=None` to return the logits of the \"top\" layer.\n+      When loading pretrained weights, `classifier_activation` can only\n+      be `None` or `\"softmax\"`.\n     **kwargs: For backwards compatibility only.\n \n   Returns:\n     A `keras.Model` instance.\n-\n-  Raises:\n-    ValueError: in case of invalid argument for `weights`,\n-      or invalid input shape.\n-    ValueError: if `classifier_activation` is not `softmax` or `None` when\n-      using a pretrained top layer.\n   \"\"\"\n   global layers\n   if 'layers' in kwargs:\n\n@@ -60,13 +60,21 @@ def InceptionV3(\n   - [Rethinking the Inception Architecture for Computer Vision](\n       http://arxiv.org/abs/1512.00567) (CVPR 2016)\n \n-  Optionally loads weights pre-trained on ImageNet.\n-  Note that the data format convention used by the model is\n-  the one specified in the `tf.keras.backend.image_data_format()`.\n+  This function returns a Keras image classification model,\n+  optionally loaded with weights pre-trained on ImageNet.\n+\n+  For image classification use cases, see\n+  [this page for detailed examples](\n+    https://keras.io/api/applications/#usage-examples-for-image-classification-models).\n+\n+  For transfer learning use cases, make sure to read the\n+  [guide to transfer learning & fine-tuning](\n+    https://keras.io/guides/transfer_learning/).\n \n   Note: each Keras Application expects a specific kind of input preprocessing.\n-  For InceptionV3, call `tf.keras.applications.inception_v3.preprocess_input`\n+  For `InceptionV3`, call `tf.keras.applications.inception_v3.preprocess_input`\n   on your inputs before passing them to the model.\n+  `inception_v3.preprocess_input` will scale input pixels between -1 and 1.\n \n   Args:\n     include_top: Boolean, whether to include the fully-connected\n@@ -100,15 +108,11 @@ def InceptionV3(\n     classifier_activation: A `str` or callable. The activation function to use\n       on the \"top\" layer. Ignored unless `include_top=True`. Set\n       `classifier_activation=None` to return the logits of the \"top\" layer.\n+      When loading pretrained weights, `classifier_activation` can only\n+      be `None` or `\"softmax\"`.\n \n   Returns:\n     A `keras.Model` instance.\n-\n-  Raises:\n-    ValueError: in case of invalid argument for `weights`,\n-      or invalid input shape.\n-    ValueError: if `classifier_activation` is not `softmax` or `None` when\n-      using a pretrained top layer.\n   \"\"\"\n   if not (weights in {'imagenet', None} or tf.io.gfile.exists(weights)):\n     raise ValueError('The `weights` argument should be either '\n\n@@ -101,13 +101,21 @@ def MobileNet(input_shape=None,\n      for Mobile Vision Applications](\n       https://arxiv.org/abs/1704.04861)\n \n-  Optionally loads weights pre-trained on ImageNet.\n-  Note that the data format convention used by the model is\n-  the one specified in the `tf.keras.backend.image_data_format()`.\n+  This function returns a Keras image classification model,\n+  optionally loaded with weights pre-trained on ImageNet.\n+\n+  For image classification use cases, see\n+  [this page for detailed examples](\n+    https://keras.io/api/applications/#usage-examples-for-image-classification-models).\n+\n+  For transfer learning use cases, make sure to read the\n+  [guide to transfer learning & fine-tuning](\n+    https://keras.io/guides/transfer_learning/).\n \n   Note: each Keras Application expects a specific kind of input preprocessing.\n   For MobileNet, call `tf.keras.applications.mobilenet.preprocess_input`\n   on your inputs before passing them to the model.\n+  `mobilenet.preprocess_input` will scale input pixels between -1 and 1.\n \n   Args:\n     input_shape: Optional shape tuple, only to be specified if `include_top`\n@@ -149,15 +157,11 @@ def MobileNet(input_shape=None,\n     classifier_activation: A `str` or callable. The activation function to use\n       on the \"top\" layer. Ignored unless `include_top=True`. Set\n       `classifier_activation=None` to return the logits of the \"top\" layer.\n+      When loading pretrained weights, `classifier_activation` can only\n+      be `None` or `\"softmax\"`.\n     **kwargs: For backwards compatibility only.\n   Returns:\n     A `keras.Model` instance.\n-\n-  Raises:\n-    ValueError: in case of invalid argument for `weights`,\n-      or invalid input shape.\n-    ValueError: if `classifier_activation` is not `softmax` or `None` when\n-      using a pretrained top layer.\n   \"\"\"\n   global layers\n   if 'layers' in kwargs:\n\n@@ -106,15 +106,33 @@ def MobileNetV2(input_shape=None,\n                 **kwargs):\n   \"\"\"Instantiates the MobileNetV2 architecture.\n \n+  MobileNetV2 is very similar to the original MobileNet,\n+  except that it uses inverted residual blocks with\n+  bottlenecking features. It has a drastically lower\n+  parameter count than the original MobileNet.\n+  MobileNets support any input size greater\n+  than 32 x 32, with larger image sizes\n+  offering better performance.\n+\n   Reference:\n   - [MobileNetV2: Inverted Residuals and Linear Bottlenecks](\n       https://arxiv.org/abs/1801.04381) (CVPR 2018)\n \n-  Optionally loads weights pre-trained on ImageNet.\n+  This function returns a Keras image classification model,\n+  optionally loaded with weights pre-trained on ImageNet.\n+\n+  For image classification use cases, see\n+  [this page for detailed examples](\n+    https://keras.io/api/applications/#usage-examples-for-image-classification-models).\n+\n+  For transfer learning use cases, make sure to read the\n+  [guide to transfer learning & fine-tuning](\n+    https://keras.io/guides/transfer_learning/).\n \n   Note: each Keras Application expects a specific kind of input preprocessing.\n   For MobileNetV2, call `tf.keras.applications.mobilenet_v2.preprocess_input`\n   on your inputs before passing them to the model.\n+  `mobilenet_v2.preprocess_input` will scale input pixels between -1 and 1.\n \n   Args:\n     input_shape: Optional shape tuple, to be specified if you would\n@@ -163,17 +181,12 @@ def MobileNetV2(input_shape=None,\n     classifier_activation: A `str` or callable. The activation function to use\n       on the \"top\" layer. Ignored unless `include_top=True`. Set\n       `classifier_activation=None` to return the logits of the \"top\" layer.\n+      When loading pretrained weights, `classifier_activation` can only\n+      be `None` or `\"softmax\"`.\n     **kwargs: For backwards compatibility only.\n \n   Returns:\n     A `keras.Model` instance.\n-\n-  Raises:\n-    ValueError: in case of invalid argument for `weights`,\n-      or invalid input shape or invalid alpha, rows when\n-      weights='imagenet'\n-    ValueError: if `classifier_activation` is not `softmax` or `None` when\n-      using a pretrained top layer.\n   \"\"\"\n   global layers\n   if 'layers' in kwargs:\n\n@@ -59,7 +59,7 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n   - [Searching for MobileNetV3](\n       https://arxiv.org/pdf/1905.02244.pdf) (ICCV 2019)\n \n-  The following table describes the performance of MobileNets:\n+  The following table describes the performance of MobileNets v3:\n   ------------------------------------------------------------------------\n   MACs stands for Multiply Adds\n \n@@ -72,11 +72,20 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n   | mobilenet_v3_small_0.75_224             | 44  | 2.4 |   65.4   |   12.8  |\n   | mobilenet_v3_small_minimalistic_1.0_224 | 65  | 2.0 |   61.9   |   12.2  |\n \n-  The weights for all 6 models are obtained and translated from the Tensorflow\n-  checkpoints from TensorFlow checkpoints found [here]\n-  (https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet/README.md).\n+  For image classification use cases, see\n+  [this page for detailed examples](\n+    https://keras.io/api/applications/#usage-examples-for-image-classification-models).\n \n-  Optionally loads weights pre-trained on ImageNet.\n+  For transfer learning use cases, make sure to read the\n+  [guide to transfer learning & fine-tuning](\n+    https://keras.io/guides/transfer_learning/).\n+\n+  Note: each Keras Application expects a specific kind of input preprocessing.\n+  For ModelNetV3, input preprocessing is included as part of the model\n+  (as a `Rescaling` layer), and thus\n+  `tf.keras.applications.mobilenet_v3.preprocess_input` is actually a\n+  pass-through function. ModelNetV3 models expect their inputs to be float\n+  tensors of pixels with values in the [0-255] range.\n \n   Args:\n     input_shape: Optional shape tuple, to be specified if you would\n@@ -131,6 +140,8 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n     classifier_activation: A `str` or callable. The activation function to use\n       on the \"top\" layer. Ignored unless `include_top=True`. Set\n       `classifier_activation=None` to return the logits of the \"top\" layer.\n+      When loading pretrained weights, `classifier_activation` can only\n+      be `None` or `\"softmax\"`.\n \n   Call arguments:\n     inputs: A floating point `numpy.array` or a `tf.Tensor`, 4D with 3 color\n@@ -138,13 +149,6 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n \n   Returns:\n     A `keras.Model` instance.\n-\n-  Raises:\n-    ValueError: in case of invalid argument for `weights`,\n-      or invalid input shape or invalid alpha, rows when\n-      weights='imagenet'\n-    ValueError: if `classifier_activation` is not `softmax` or `None` when\n-      using a pretrained top layer.\n \"\"\"\n \n \n@@ -572,7 +576,6 @@ def preprocess_input(x, data_format=None):  # pylint: disable=unused-argument\n   Returns:\n     Unchanged `numpy.array` or `tf.Tensor`.\n   \"\"\"\n-\n   return x\n \n \n\n@@ -82,9 +82,18 @@ def NASNet(input_shape=None,\n   - [Learning Transferable Architectures for Scalable Image Recognition](\n       https://arxiv.org/abs/1707.07012) (CVPR 2018)\n \n-  Optionally loads weights pre-trained on ImageNet.\n-  Note that the data format convention used by the model is\n-  the one specified in your Keras config at `~/.keras/keras.json`.\n+  For image classification use cases, see\n+  [this page for detailed examples](\n+    https://keras.io/api/applications/#usage-examples-for-image-classification-models).\n+\n+  For transfer learning use cases, make sure to read the\n+  [guide to transfer learning & fine-tuning](\n+    https://keras.io/guides/transfer_learning/).\n+\n+  Note: each Keras Application expects a specific kind of input preprocessing.\n+  For NasNet, call `tf.keras.applications.nasnet.preprocess_input`\n+  on your inputs before passing them to the model.\n+  `nasnet.preprocess_input` will scale input pixels between -1 and 1.\n \n   Args:\n     input_shape: Optional shape tuple, the input shape\n@@ -137,15 +146,11 @@ def NASNet(input_shape=None,\n     classifier_activation: A `str` or callable. The activation function to use\n       on the \"top\" layer. Ignored unless `include_top=True`. Set\n       `classifier_activation=None` to return the logits of the \"top\" layer.\n+      When loading pretrained weights, `classifier_activation` can only\n+      be `None` or `\"softmax\"`.\n \n   Returns:\n     A `keras.Model` instance.\n-\n-  Raises:\n-    ValueError: In case of invalid argument for `weights`,\n-      invalid input shape or invalid `penultimate_filters` value.\n-    ValueError: if `classifier_activation` is not `softmax` or `None` when\n-      using a pretrained top layer.\n   \"\"\"\n   if not (weights in {'imagenet', None} or tf.io.gfile.exists(weights)):\n     raise ValueError('The `weights` argument should be either '\n\n@@ -72,14 +72,6 @@ def ResNet(stack_fn,\n            **kwargs):\n   \"\"\"Instantiates the ResNet, ResNetV2, and ResNeXt architecture.\n \n-  Reference:\n-  - [Deep Residual Learning for Image Recognition](\n-      https://arxiv.org/abs/1512.03385) (CVPR 2015)\n-\n-  Optionally loads weights pre-trained on ImageNet.\n-  Note that the data format convention used by the model is\n-  the one specified in your Keras config at `~/.keras/keras.json`.\n-\n   Args:\n     stack_fn: a function that returns output tensor for the\n       stacked residual blocks.\n@@ -118,15 +110,12 @@ def ResNet(stack_fn,\n     classifier_activation: A `str` or callable. The activation function to use\n       on the \"top\" layer. Ignored unless `include_top=True`. Set\n       `classifier_activation=None` to return the logits of the \"top\" layer.\n+      When loading pretrained weights, `classifier_activation` can only\n+      be `None` or `\"softmax\"`.\n     **kwargs: For backwards compatibility only.\n+\n   Returns:\n     A `keras.Model` instance.\n-\n-  Raises:\n-    ValueError: in case of invalid argument for `weights`,\n-      or invalid input shape.\n-    ValueError: if `classifier_activation` is not `softmax` or `None` when\n-      using a pretrained top layer.\n   \"\"\"\n   global layers\n   if 'layers' in kwargs:\n@@ -540,13 +529,20 @@ DOC = \"\"\"\n   - [Deep Residual Learning for Image Recognition](\n       https://arxiv.org/abs/1512.03385) (CVPR 2015)\n \n-  Optionally loads weights pre-trained on ImageNet.\n-  Note that the data format convention used by the model is\n-  the one specified in your Keras config at `~/.keras/keras.json`.\n+  For image classification use cases, see\n+  [this page for detailed examples](\n+    https://keras.io/api/applications/#usage-examples-for-image-classification-models).\n+\n+  For transfer learning use cases, make sure to read the\n+  [guide to transfer learning & fine-tuning](\n+    https://keras.io/guides/transfer_learning/).\n \n   Note: each Keras Application expects a specific kind of input preprocessing.\n   For ResNet, call `tf.keras.applications.resnet.preprocess_input` on your\n   inputs before passing them to the model.\n+  `resnet.preprocess_input` will convert the input images from RGB to BGR,\n+  then will zero-center each color channel with respect to the ImageNet dataset,\n+  without scaling.\n \n   Args:\n     include_top: whether to include the fully-connected\n@@ -577,6 +573,11 @@ DOC = \"\"\"\n     classes: optional number of classes to classify images\n       into, only to be specified if `include_top` is True, and\n       if no `weights` argument is specified.\n+    classifier_activation: A `str` or callable. The activation function to use\n+      on the \"top\" layer. Ignored unless `include_top=True`. Set\n+      `classifier_activation=None` to return the logits of the \"top\" layer.\n+      When loading pretrained weights, `classifier_activation` can only\n+      be `None` or `\"softmax\"`.\n \n   Returns:\n     A Keras model instance.\n\n@@ -144,13 +144,18 @@ DOC = \"\"\"\n   - [Identity Mappings in Deep Residual Networks]\n     (https://arxiv.org/abs/1603.05027) (CVPR 2016)\n \n-  Optionally loads weights pre-trained on ImageNet.\n-  Note that the data format convention used by the model is\n-  the one specified in your Keras config at `~/.keras/keras.json`.\n+  For image classification use cases, see\n+  [this page for detailed examples](\n+    https://keras.io/api/applications/#usage-examples-for-image-classification-models).\n+\n+  For transfer learning use cases, make sure to read the\n+  [guide to transfer learning & fine-tuning](\n+    https://keras.io/guides/transfer_learning/).\n \n   Note: each Keras Application expects a specific kind of input preprocessing.\n   For ResNetV2, call `tf.keras.applications.resnet_v2.preprocess_input` on your\n   inputs before passing them to the model.\n+  `resnet_v2.preprocess_input` will scale input pixels between -1 and 1.\n \n   Args:\n     include_top: whether to include the fully-connected\n@@ -184,6 +189,8 @@ DOC = \"\"\"\n     classifier_activation: A `str` or callable. The activation function to use\n       on the \"top\" layer. Ignored unless `include_top=True`. Set\n       `classifier_activation=None` to return the logits of the \"top\" layer.\n+      When loading pretrained weights, `classifier_activation` can only\n+      be `None` or `\"softmax\"`.\n \n   Returns:\n     A `keras.Model` instance.\n\n@@ -58,18 +58,22 @@ def VGG16(\n   - [Very Deep Convolutional Networks for Large-Scale Image Recognition](\n   https://arxiv.org/abs/1409.1556) (ICLR 2015)\n \n-  By default, it loads weights pre-trained on ImageNet. Check 'weights' for\n-  other options.\n+  For image classification use cases, see\n+  [this page for detailed examples](\n+    https://keras.io/api/applications/#usage-examples-for-image-classification-models).\n \n-  This model can be built both with 'channels_first' data format\n-  (channels, height, width) or 'channels_last' data format\n-  (height, width, channels).\n+  For transfer learning use cases, make sure to read the\n+  [guide to transfer learning & fine-tuning](\n+    https://keras.io/guides/transfer_learning/).\n \n   The default input size for this model is 224x224.\n \n   Note: each Keras Application expects a specific kind of input preprocessing.\n   For VGG16, call `tf.keras.applications.vgg16.preprocess_input` on your\n   inputs before passing them to the model.\n+  `vgg16.preprocess_input` will convert the input images from RGB to BGR,\n+  then will zero-center each color channel with respect to the ImageNet dataset,\n+  without scaling.\n \n   Args:\n       include_top: whether to include the 3 fully-connected\n@@ -105,15 +109,11 @@ def VGG16(\n       classifier_activation: A `str` or callable. The activation function to use\n           on the \"top\" layer. Ignored unless `include_top=True`. Set\n           `classifier_activation=None` to return the logits of the \"top\" layer.\n+          When loading pretrained weights, `classifier_activation` can only\n+          be `None` or `\"softmax\"`.\n \n   Returns:\n     A `keras.Model` instance.\n-\n-  Raises:\n-    ValueError: in case of invalid argument for `weights`,\n-      or invalid input shape.\n-    ValueError: if `classifier_activation` is not `softmax` or `None` when\n-      using a pretrained top layer.\n   \"\"\"\n   if not (weights in {'imagenet', None} or tf.io.gfile.exists(weights)):\n     raise ValueError('The `weights` argument should be either '\n\n@@ -58,18 +58,22 @@ def VGG19(\n   - [Very Deep Convolutional Networks for Large-Scale Image Recognition](\n       https://arxiv.org/abs/1409.1556) (ICLR 2015)\n \n-  By default, it loads weights pre-trained on ImageNet. Check 'weights' for\n-  other options.\n+  For image classification use cases, see\n+  [this page for detailed examples](\n+    https://keras.io/api/applications/#usage-examples-for-image-classification-models).\n \n-  This model can be built both with 'channels_first' data format\n-  (channels, height, width) or 'channels_last' data format\n-  (height, width, channels).\n+  For transfer learning use cases, make sure to read the\n+  [guide to transfer learning & fine-tuning](\n+    https://keras.io/guides/transfer_learning/).\n \n   The default input size for this model is 224x224.\n \n   Note: each Keras Application expects a specific kind of input preprocessing.\n   For VGG19, call `tf.keras.applications.vgg19.preprocess_input` on your\n   inputs before passing them to the model.\n+  `vgg19.preprocess_input` will convert the input images from RGB to BGR,\n+  then will zero-center each color channel with respect to the ImageNet dataset,\n+  without scaling.\n \n   Args:\n     include_top: whether to include the 3 fully-connected\n@@ -105,15 +109,11 @@ def VGG19(\n     classifier_activation: A `str` or callable. The activation function to use\n       on the \"top\" layer. Ignored unless `include_top=True`. Set\n       `classifier_activation=None` to return the logits of the \"top\" layer.\n+      When loading pretrained weights, `classifier_activation` can only\n+      be `None` or `\"softmax\"`.\n \n   Returns:\n     A `keras.Model` instance.\n-\n-  Raises:\n-    ValueError: in case of invalid argument for `weights`,\n-      or invalid input shape.\n-    ValueError: if `classifier_activation` is not `softmax` or `None` when\n-      using a pretrained top layer.\n   \"\"\"\n   if not (weights in {'imagenet', None} or tf.io.gfile.exists(weights)):\n     raise ValueError('The `weights` argument should be either '\n\n@@ -64,14 +64,20 @@ def Xception(\n   - [Xception: Deep Learning with Depthwise Separable Convolutions](\n       https://arxiv.org/abs/1610.02357) (CVPR 2017)\n \n-  Optionally loads weights pre-trained on ImageNet.\n-  Note that the data format convention used by the model is\n-  the one specified in your Keras config at `~/.keras/keras.json`.\n-  Note that the default input image size for this model is 299x299.\n+  For image classification use cases, see\n+  [this page for detailed examples](\n+    https://keras.io/api/applications/#usage-examples-for-image-classification-models).\n+\n+  For transfer learning use cases, make sure to read the\n+  [guide to transfer learning & fine-tuning](\n+    https://keras.io/guides/transfer_learning/).\n+\n+  The default input image size for this model is 299x299.\n \n   Note: each Keras Application expects a specific kind of input preprocessing.\n   For Xception, call `tf.keras.applications.xception.preprocess_input` on your\n   inputs before passing them to the model.\n+  `xception.preprocess_input` will scale input pixels between -1 and 1.\n \n   Args:\n     include_top: whether to include the fully-connected\n@@ -105,15 +111,11 @@ def Xception(\n     classifier_activation: A `str` or callable. The activation function to use\n       on the \"top\" layer. Ignored unless `include_top=True`. Set\n       `classifier_activation=None` to return the logits of the \"top\" layer.\n+      When loading pretrained weights, `classifier_activation` can only\n+      be `None` or `\"softmax\"`.\n \n   Returns:\n     A `keras.Model` instance.\n-\n-  Raises:\n-    ValueError: in case of invalid argument for `weights`,\n-      or invalid input shape.\n-    ValueError: if `classifier_activation` is not `softmax` or `None` when\n-      using a pretrained top layer.\n   \"\"\"\n   if not (weights in {'imagenet', None} or tf.io.gfile.exists(weights)):\n     raise ValueError('The `weights` argument should be either '\n\n@@ -1107,33 +1107,22 @@ class ProgbarLogger(Callback):\n class History(Callback):\n   \"\"\"Callback that records events into a `History` object.\n \n-  This callback gets added by default to\n-  every call of `model.fit()`. That's how Keras keeps track of model training\n-  history. You should never have to add this callback manually.\n-\n-  The `History` callback instance that is automatically added to `fit()`\n-  gets returned at the end of `fit()`.\n-  This `History` instance contains a `.history` attribute, which is a dict\n-  that stores the logs of the model, such as the loss values for each\n-  training epoch.\n+  This callback is automatically applied to\n+  every Keras model. The `History` object\n+  gets returned by the `fit` method of models.\n \n   Example:\n \n   >>> model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])\n-  >>> model.compile(optimizer='sgd', loss='mse')\n-  >>> history = model.fit(np.zeros((32, 5)), np.zeros((32, 10)),\n-  ...                     epochs=5)\n-  >>> # Inspect training parameters\n+  >>> model.compile(tf.keras.optimizers.SGD(), loss='mse')\n+  >>> history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),\n+  ...                     epochs=10)\n   >>> print(history.params)\n-  {'verbose': 1, 'epochs': 5, 'steps': 1}\n-  >>> # Inspect the keys of the `history` dict\n+  {'verbose': 1, 'epochs': 10, 'steps': 1}\n+  >>> # check the keys of history object\n   >>> print(history.history.keys())\n   dict_keys(['loss'])\n-  >>> print(history.history.keys())\n-  dict_keys(['loss'])\n-  >>> # This is the loss history over the 5 training epochs\n-  >>> history.history['loss']\n-  [0.0, 0.0, 0.0, 0.0, 0.0]\n+\n   \"\"\"\n \n   def __init__(self):\n\n@@ -30,22 +30,53 @@ from tensorflow.python.util.tf_export import keras_export\n \n @keras_export('keras.datasets.cifar10.load_data')\n def load_data():\n-  \"\"\"Loads [CIFAR10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).\n+  \"\"\"Loads the CIFAR10 dataset.\n \n   This is a dataset of 50,000 32x32 color training images and 10,000 test\n   images, labeled over 10 categories. See more info at the\n   [CIFAR homepage](https://www.cs.toronto.edu/~kriz/cifar.html).\n \n+  The classes are:\n+\n+  | Label | Description |\n+  |:-----:|-------------|\n+  |   0   | airplane    |\n+  |   1   | automobile  |\n+  |   2   | bird        |\n+  |   3   | cat         |\n+  |   4   | deer        |\n+  |   5   | dog         |\n+  |   6   | frog        |\n+  |   7   | horse       |\n+  |   8   | ship        |\n+  |   9   | truck       |\n+\n   Returns:\n-      Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n+    Tuple of NumPy arrays: `(x_train, y_train), (x_test, y_test)`.\n \n-      **x_train, x_test**: uint8 arrays of RGB image data with shape\n-        `(num_samples, 3, 32, 32)` if `tf.keras.backend.image_data_format()` is\n-        `'channels_first'`, or `(num_samples, 32, 32, 3)` if the data format\n-        is `'channels_last'`.\n+  **x_train**: uint8 NumPy array of grayscale image data with shapes\n+    `(50000, 32, 32, 3)`, containing the training data. Pixel values range\n+    from 0 to 255.\n \n-      **y_train, y_test**: uint8 arrays of category labels\n-        (integers in range 0-9) each with shape (num_samples, 1).\n+  **y_train**: uint8 NumPy array of labels (integers in range 0-9)\n+    with shape `(50000, 1)` for the training data.\n+\n+  **x_test**: uint8 NumPy array of grayscale image data with shapes\n+    (10000, 32, 32, 3), containing the test data. Pixel values range\n+    from 0 to 255.\n+\n+  **y_test**: uint8 NumPy array of labels (integers in range 0-9)\n+    with shape `(10000, 1)` for the test data.\n+\n+  Example:\n+\n+  ```python\n+  (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n+  assert x_train.shape == (50000, 32, 32, 3)\n+  assert x_test.shape == (10000, 32, 32, 3)\n+  assert y_train.shape == (50000, 1)\n+  assert y_test.shape == (10000, 1)\n+  ```\n   \"\"\"\n   dirname = 'cifar-10-batches-py'\n   origin = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n\n@@ -30,7 +30,7 @@ from tensorflow.python.util.tf_export import keras_export\n \n @keras_export('keras.datasets.cifar100.load_data')\n def load_data(label_mode='fine'):\n-  \"\"\"Loads [CIFAR100 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).\n+  \"\"\"Loads the CIFAR100 dataset.\n \n   This is a dataset of 50,000 32x32 color training images and\n   10,000 test images, labeled over 100 fine-grained classes that are\n@@ -43,18 +43,31 @@ def load_data(label_mode='fine'):\n       coarse-grained superclasses.\n \n   Returns:\n-      Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n+    Tuple of NumPy arrays: `(x_train, y_train), (x_test, y_test)`.\n \n-      **x_train, x_test**: uint8 arrays of RGB image data with shape\n-        `(num_samples, 3, 32, 32)` if `tf.keras.backend.image_data_format()` is\n-        `'channels_first'`, or `(num_samples, 32, 32, 3)` if the data format\n-        is `'channels_last'`.\n+  **x_train**: uint8 NumPy array of grayscale image data with shapes\n+    `(50000, 32, 32, 3)`, containing the training data. Pixel values range\n+    from 0 to 255.\n \n-      **y_train, y_test**: uint8 arrays of category labels with shape\n-        (num_samples, 1).\n+  **y_train**: uint8 NumPy array of labels (integers in range 0-99)\n+    with shape `(50000, 1)` for the training data.\n \n-  Raises:\n-      ValueError: in case of invalid `label_mode`.\n+  **x_test**: uint8 NumPy array of grayscale image data with shapes\n+    (10000, 32, 32, 3), containing the test data. Pixel values range\n+    from 0 to 255.\n+\n+  **y_test**: uint8 NumPy array of labels (integers in range 0-99)\n+    with shape `(10000, 1)` for the test data.\n+\n+  Example:\n+\n+  ```python\n+  (x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n+  assert x_train.shape == (50000, 32, 32, 3)\n+  assert x_test.shape == (10000, 32, 32, 3)\n+  assert y_train.shape == (50000, 1)\n+  assert y_test.shape == (10000, 1)\n+  ```\n   \"\"\"\n   if label_mode not in ['fine', 'coarse']:\n     raise ValueError('`label_mode` must be one of `\"fine\"`, `\"coarse\"`.')\n\n@@ -33,7 +33,9 @@ def load_data():\n \n   This is a dataset of 60,000 28x28 grayscale images of 10 fashion categories,\n   along with a test set of 10,000 images. This dataset can be used as\n-  a drop-in replacement for MNIST. The class labels are:\n+  a drop-in replacement for MNIST.\n+\n+  The classes are:\n \n   | Label | Description |\n   |:-----:|-------------|\n@@ -49,13 +51,29 @@ def load_data():\n   |   9   | Ankle boot  |\n \n   Returns:\n-      Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n+    Tuple of NumPy arrays: `(x_train, y_train), (x_test, y_test)`.\n \n-      **x_train, x_test**: uint8 arrays of grayscale image data with shape\n-        (num_samples, 28, 28).\n+  **x_train**: uint8 NumPy array of grayscale image data with shapes\n+    `(60000, 28, 28)`, containing the training data.\n \n-      **y_train, y_test**: uint8 arrays of labels (integers in range 0-9)\n-        with shape (num_samples,).\n+  **y_train**: uint8 NumPy array of labels (integers in range 0-9)\n+    with shape `(60000,)` for the training data.\n+\n+  **x_test**: uint8 NumPy array of grayscale image data with shapes\n+    (10000, 28, 28), containing the test data.\n+\n+  **y_test**: uint8 NumPy array of labels (integers in range 0-9)\n+    with shape `(10000,)` for the test data.\n+\n+  Example:\n+\n+  ```python\n+  (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n+  assert x_train.shape == (60000, 28, 28)\n+  assert x_test.shape == (10000, 28, 28)\n+  assert y_train.shape == (60000,)\n+  assert y_test.shape == (10000,)\n+  ```\n \n   License:\n     The copyright for Fashion-MNIST is held by Zalando SE.\n\n@@ -171,6 +171,19 @@ def get_word_index(path='imdb_word_index.json'):\n \n   Returns:\n       The word index dictionary. Keys are word strings, values are their index.\n+\n+  Example:\n+\n+  ```python\n+  # Retrieve the training sequences.\n+  (x_train, _), _ = keras.datasets.imdb.load_data()\n+  # Retrieve the word index file mapping words to indices\n+  word_index = keras.datasets.imdb.get_word_index()\n+  # Reverse the word index to obtain a dict mapping indices to words\n+  inverted_word_index = dict((i, word) for (word, i) in word_index.items())\n+  # Decode the first sequence in the dataset\n+  decoded_sequence = \" \".join(inverted_word_index[i] for i in x_train[0])\n+  ```\n   \"\"\"\n   origin_folder = 'https://storage.googleapis.com/tensorflow/tf-keras-datasets/'\n   path = get_file(\n\n@@ -26,26 +26,43 @@ from tensorflow.python.util.tf_export import keras_export\n \n @keras_export('keras.datasets.mnist.load_data')\n def load_data(path='mnist.npz'):\n-  \"\"\"Loads the [MNIST dataset](http://yann.lecun.com/exdb/mnist/).\n+  \"\"\"Loads the MNIST dataset.\n \n   This is a dataset of 60,000 28x28 grayscale images of the 10 digits,\n   along with a test set of 10,000 images.\n   More info can be found at the\n   [MNIST homepage](http://yann.lecun.com/exdb/mnist/).\n \n-\n   Args:\n     path: path where to cache the dataset locally\n       (relative to `~/.keras/datasets`).\n \n   Returns:\n-      Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n+    Tuple of NumPy arrays: `(x_train, y_train), (x_test, y_test)`.\n \n-      **x_train, x_test**: uint8 arrays of grayscale image data with shapes\n-        (num_samples, 28, 28).\n+  **x_train**: uint8 NumPy array of grayscale image data with shapes\n+    `(60000, 28, 28)`, containing the training data. Pixel values range\n+    from 0 to 255.\n \n-      **y_train, y_test**: uint8 arrays of digit labels (integers in range 0-9)\n-        with shapes (num_samples,).\n+  **y_train**: uint8 NumPy array of digit labels (integers in range 0-9)\n+    with shape `(60000,)` for the training data.\n+\n+  **x_test**: uint8 NumPy array of grayscale image data with shapes\n+    (10000, 28, 28), containing the test data. Pixel values range\n+    from 0 to 255.\n+\n+  **y_test**: uint8 NumPy array of digit labels (integers in range 0-9)\n+    with shape `(10000,)` for the test data.\n+\n+  Example:\n+\n+  ```python\n+  (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n+  assert x_train.shape == (60000, 28, 28)\n+  assert x_test.shape == (10000, 28, 28)\n+  assert y_train.shape == (60000,)\n+  assert y_test.shape == (10000,)\n+  ```\n \n   License:\n     Yann LeCun and Corinna Cortes hold the copyright of MNIST dataset,\n\n@@ -59,7 +59,6 @@ def load_data(path='reuters.npz',\n   As a convention, \"0\" does not stand for a specific word, but instead is used\n   to encode any unknown word.\n \n-\n   Args:\n     path: where to cache the data (relative to `~/.keras/dataset`).\n     num_words: integer or None. Words are\n\n@@ -50,11 +50,11 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n \n     @tf.function\n     def optimize():\n-      grads = values.PerReplica([\n-          tf.convert_to_tensor([1., 1.]),\n-          tf.convert_to_tensor([2., 2.]),\n-      ])\n-\n+      with tf.compat.v1.device(distribution.extended.worker_devices[0]):\n+        v1 = tf.convert_to_tensor([1., 1.])\n+      with tf.compat.v1.device(distribution.extended.worker_devices[1]):\n+        v2 = tf.convert_to_tensor([2., 2.])\n+      grads = values.PerReplica([v1, v2])\n       def step_fn(grads):\n         optimizer.apply_gradients(\n             [(grads, v)],\n\n@@ -26,6 +26,7 @@ import numpy as np\n \n import keras\n from tensorflow.python.distribute import multi_worker_test_base\n+from tensorflow.python.distribute import strategy_combinations\n from tensorflow.python.distribute.cluster_resolver import SimpleClusterResolver\n from keras import backend\n from keras import testing_utils\n@@ -235,6 +236,7 @@ def all_strategy_minus_default_and_tpu_combinations():\n           tf.__internal__.distribute.combinations.one_device_strategy_gpu,\n           tf.__internal__.distribute.combinations.mirrored_strategy_with_gpu_and_cpu,\n           tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus,\n+          strategy_combinations.mirrored_strategy_with_two_gpus_no_merge_call,\n       ],\n       mode=['graph', 'eager'])\n \n@@ -1289,7 +1291,9 @@ class TestDistributionStrategyWithDatasets(tf.test.TestCase,\n       tf.__internal__.test.combinations.combine(\n           distribution=[\n               tf.__internal__.distribute.combinations.mirrored_strategy_with_gpu_and_cpu,\n-              tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus\n+              tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus,\n+              strategy_combinations\n+              .mirrored_strategy_with_two_gpus_no_merge_call,\n           ],\n           mode=['graph', 'eager']))\n   def test_learning_phase_value(self, distribution):\n@@ -2025,7 +2029,9 @@ class TestDistributionStrategyWithKerasModels(tf.test.TestCase,\n       tf.__internal__.test.combinations.combine(\n           distribution=[\n               tf.__internal__.distribute.combinations.mirrored_strategy_with_gpu_and_cpu,\n-              tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus\n+              tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus,\n+              strategy_combinations\n+              .mirrored_strategy_with_two_gpus_no_merge_call,\n           ],\n           mode=['graph', 'eager'],\n           reduction=[\n@@ -2182,6 +2188,8 @@ class TestDistributionStrategyWithKerasModels(tf.test.TestCase,\n               tf.__internal__.distribute.combinations.one_device_strategy_gpu,\n               tf.__internal__.distribute.combinations.mirrored_strategy_with_gpu_and_cpu,\n               tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus,\n+              strategy_combinations\n+              .mirrored_strategy_with_two_gpus_no_merge_call,\n           ],\n           mode=['eager']))\n   def test_distribution_strategy_with_add_metric_object(\n\n@@ -20,6 +20,7 @@ from __future__ import print_function\n import tensorflow.compat.v2 as tf\n \n from absl.testing import parameterized\n+from tensorflow.python.distribute import strategy_combinations\n from keras import metrics\n \n \n@@ -73,6 +74,7 @@ def all_combinations():\n           tf.__internal__.distribute.combinations.one_device_strategy,\n           tf.__internal__.distribute.combinations.mirrored_strategy_with_gpu_and_cpu,\n           tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus,\n+          strategy_combinations.mirrored_strategy_with_two_gpus_no_merge_call,\n       ],\n       mode=[\"graph\"])\n \n\n@@ -21,6 +21,7 @@ import tensorflow.compat.v2 as tf\n \n from absl.testing import parameterized\n import numpy as np\n+from tensorflow.python.distribute import strategy_combinations\n from keras.engine import sequential\n from keras.layers import core\n from keras.optimizer_v2 import adagrad\n@@ -37,6 +38,7 @@ def strategy_combinations_eager_data_fn():\n           tf.__internal__.distribute.combinations.one_device_strategy_gpu,\n           tf.__internal__.distribute.combinations.mirrored_strategy_with_gpu_and_cpu,\n           tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus,\n+          strategy_combinations.mirrored_strategy_with_two_gpus_no_merge_call,\n           tf.__internal__.distribute.combinations.multi_worker_mirrored_2x1_cpu,\n           tf.__internal__.distribute.combinations.multi_worker_mirrored_2x1_gpu,\n           tf.__internal__.distribute.combinations.multi_worker_mirrored_2x2_gpu,\n\n@@ -22,6 +22,7 @@ import tensorflow.compat.v2 as tf\n \n from absl.testing import parameterized\n import numpy\n+from tensorflow.python.distribute import strategy_combinations\n from keras.distribute import optimizer_combinations\n from keras.distribute.test_example import batchnorm_example\n from keras.distribute.test_example import minimize_loss_example\n@@ -291,7 +292,9 @@ class MinimizeLossStepTest(tf.test.TestCase, parameterized.TestCase):\n               tf.__internal__.test.combinations.combine(distribution=[\n                   tf.__internal__.distribute.combinations.one_device_strategy,\n                   tf.__internal__.distribute.combinations.mirrored_strategy_with_gpu_and_cpu,\n-                  tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus\n+                  tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus,\n+                  strategy_combinations\n+                  .mirrored_strategy_with_two_gpus_no_merge_call,\n               ]),\n               tf.__internal__.test.combinations.times(\n                   tf.__internal__.test.combinations.combine(optimizer_fn=optimizer_combinations\n\n@@ -19,6 +19,7 @@ from __future__ import division\n from __future__ import print_function\n \n import tensorflow.compat.v2 as tf\n+from tensorflow.python.distribute import strategy_combinations as strategy_combinations_base\n from keras.optimizer_v2 import adadelta as adadelta_keras_v2\n from keras.optimizer_v2 import adagrad as adagrad_keras_v2\n from keras.optimizer_v2 import adam as adam_keras_v2\n@@ -79,6 +80,8 @@ def distributions_and_v1_optimizers():\n           tf.__internal__.distribute.combinations.one_device_strategy,\n           tf.__internal__.distribute.combinations.mirrored_strategy_with_gpu_and_cpu,\n           tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus,\n+          strategy_combinations_base\n+          .mirrored_strategy_with_two_gpus_no_merge_call,\n       ],\n       optimizer_fn=optimizers_v1)\n \n@@ -90,6 +93,8 @@ def distributions_and_v2_optimizers():\n           tf.__internal__.distribute.combinations.one_device_strategy,\n           tf.__internal__.distribute.combinations.mirrored_strategy_with_gpu_and_cpu,\n           tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus,\n+          strategy_combinations_base\n+          .mirrored_strategy_with_two_gpus_no_merge_call,\n       ],\n       optimizer_fn=optimizers_v2)\n \n@@ -101,5 +106,7 @@ def distributions_and_v1_and_v2_optimizers():\n           tf.__internal__.distribute.combinations.one_device_strategy,\n           tf.__internal__.distribute.combinations.mirrored_strategy_with_gpu_and_cpu,\n           tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus,\n+          strategy_combinations_base\n+          .mirrored_strategy_with_two_gpus_no_merge_call,\n       ],\n       optimizer_fn=optimizers_v1_and_v2)\n\n@@ -223,6 +223,22 @@ class ShardedVariableTest(tf.test.TestCase):\n         multi_worker_testing_utils.make_parameter_server_cluster(3, 2),\n         variable_partitioner=tf.distribute.experimental.partitioners.FixedShardsPartitioner(2))\n \n+  def assert_list_all_equal(self, list1, list2):\n+    \"\"\"Used in lieu of `assertAllEqual`.\n+\n+    This is used to replace standard `assertAllEqual` for the cases where\n+    `list1` and `list2` contain `AggregatingVariable`. Lists with\n+    `AggregatingVariable` are not convertible to numpy array via `np.array`\n+    calls as numpy would raise `ValueError: setting an array element with a\n+    sequence.`\n+\n+    Args:\n+      list1: The first list to compare equality.\n+      list2: The second list to compare equality.\n+    \"\"\"\n+    for lhs, rhs in zip(list1, list2):\n+      self.assertEqual(lhs, rhs)\n+\n   def test_keras_layer_setattr(self):\n \n     class Layer(base_layer.Layer):\n@@ -241,10 +257,11 @@ class ShardedVariableTest(tf.test.TestCase):\n     self.assertLen(layer.non_trainable_weights, 2)\n     self.assertEqual(layer.non_trainable_weights[0], [2])\n     self.assertEqual(layer.non_trainable_weights[1], [3])\n-    self.assertAllEqual(layer.weights,\n-                        layer.trainable_weights + layer.non_trainable_weights)\n-    self.assertAllEqual(layer.trainable_weights, layer.trainable_variables)\n-    self.assertAllEqual(layer.weights, layer.variables)\n+    self.assert_list_all_equal(\n+        layer.weights, layer.trainable_weights + layer.non_trainable_weights)\n+    self.assert_list_all_equal(layer.trainable_weights,\n+                               layer.trainable_variables)\n+    self.assert_list_all_equal(layer.weights, layer.variables)\n \n     checkpoint_deps = set(dep.ref for dep in layer._checkpoint_dependencies)\n     self.assertEqual(checkpoint_deps, set([layer.w, layer.b]))\n@@ -273,10 +290,11 @@ class ShardedVariableTest(tf.test.TestCase):\n     self.assertLen(layer.non_trainable_weights, 2)\n     self.assertEqual(layer.non_trainable_weights[0], [2.])\n     self.assertEqual(layer.non_trainable_weights[1], [3.])\n-    self.assertAllEqual(layer.weights,\n-                        layer.trainable_weights + layer.non_trainable_weights)\n-    self.assertAllEqual(layer.trainable_weights, layer.trainable_variables)\n-    self.assertAllEqual(layer.weights, layer.variables)\n+    self.assert_list_all_equal(\n+        layer.weights, layer.trainable_weights + layer.non_trainable_weights)\n+    self.assert_list_all_equal(layer.trainable_weights,\n+                               layer.trainable_variables)\n+    self.assert_list_all_equal(layer.weights, layer.variables)\n \n     checkpoint_deps = set(dep.ref for dep in layer._checkpoint_dependencies)\n     self.assertEqual(checkpoint_deps, set([layer.w, layer.b]))\n\n@@ -51,14 +51,28 @@ def list_checkpoint_attributes(ckpt_dir_or_file):\n class SidecarEvaluator(object):\n   \"\"\"A class designed for a dedicated evaluator task.\n \n-  `SidecarEvaluator` is expected to be run on a process in\n-  a separate machine from the training cluster. It continuously loads\n-  checkpoints saved periodically by that training counterpart, and performs\n-  evaluation using the model (with compiled metrics) provided at `__init__`. It\n-  is expected to be used for the purpose of a dedicated evaluator, evaluating\n-  the metric results of a training cluster which has one or more workers\n-  performing the training and saving checkpoints. `SidecarEvaluator` uses\n-  `model.evaluate` for evaluation.\n+  `SidecarEvaluator` is expected to be run in a process on a separate machine\n+  from the training cluster. It is meant for the purpose of a dedicated\n+  evaluator, evaluating the metric results of a training cluster which has one\n+  or more workers performing the training, and saving checkpoints.\n+\n+  The `SidecarEvaluator` API is compatible with both Custom Training Loop (CTL),\n+  and Keras `Model.fit` to be used in the training cluster. Using the model\n+  (with compiled metrics) provided at `__init__`, `SidecarEvaluator` repeatedly\n+  performs evaluation \"epochs\" when it finds a checkpoint that has not yet been\n+  used. Depending on the `steps` argument, an eval epoch is evaluation over all\n+  eval data, or up to certain number of steps (batches). See examples below for\n+  how the training program should save the checkpoints in order to be recognized\n+  by `SidecarEvaluator`.\n+\n+  Since under the hood, `SidecarEvaluator` uses `model.evaluate` for evaluation,\n+  it also supports arbitrary Keras callbacks. That is, if one or more callbacks\n+  are provided, their `on_test_batch_begin` and `on_test_batch_end` methods are\n+  called at the start and end of a batch, and their `on_test_begin` and\n+  `on_test_end` are called at the start and end of an evaluation epoch. Note\n+  that `SidecarEvaluator` may skip some checkpoints because it always picks up\n+  the latest checkpoint available, and during an evaluation epoch, multiple\n+  checkpoints can be produced from the training side.\n \n   Example:\n   ```python\n@@ -86,17 +100,31 @@ class SidecarEvaluator(object):\n   TensorBoard 2.4.0a0 at http://host:port (Press CTRL+C to quit)\n   ```\n \n-  The `checkpoint_dir` should contain checkpoints that track `model` and\n-  `optimizer` to fulfill `SidecarEvaluator`'s\n-  expectation:\n+  If the training cluster uses a CTL, the `checkpoint_dir` should contain\n+  checkpoints that track both `model` and `optimizer`, to fulfill\n+  `SidecarEvaluator`'s expectation. This can be done by a\n+  `tf.train.Checkpoint` and a `tf.train.CheckpointManager`:\n \n   ```python\n+  checkpoint_dir = ...  # Same `checkpoint_dir` supplied to `SidecarEvaluator`.\n   checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n   checkpoint_manager = tf.train.CheckpointManager(\n       checkpoint, checkpoint_dir=..., max_to_keep=...)\n   checkpoint_manager.save()\n   ```\n \n+  If the training cluster uses Keras `Model.fit` API, a\n+  `tf.keras.callbacks.ModelCheckpoint` should be used, with\n+  `save_weights_only=True`, and the `filepath` should have 'ckpt-{epoch}'\n+  appended:\n+\n+  ```python\n+  checkpoint_dir = ...  # Same `checkpoint_dir` supplied to `SidecarEvaluator`.\n+  model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n+      filepath=os.path.join(checkpoint_dir, 'ckpt-{epoch}'),\n+      save_weights_only=True)\n+  model.fit(dataset, epochs, callbacks=[model_checkpoint])\n+  ```\n   \"\"\"\n \n   def __init__(self,\n@@ -176,7 +204,6 @@ class SidecarEvaluator(object):\n             if re.match(r'^layer_with_weights-[\\d+]', attribute) is not None:\n               self.model.load_weights(latest_checkpoint)\n               break\n-        else:\n         # The model checkpoint might not include optimizer in cases, e.g.\n         # using a custom training loop. Directly assign the iterations\n         # property to be used in callbacks.\n@@ -205,7 +232,8 @@ class SidecarEvaluator(object):\n           'Evaluation starts: Model weights loaded from latest '\n           'checkpoint file: %s.', latest_checkpoint)\n \n-      self.model.evaluate(self.data, steps=self.steps, callbacks=self.callbacks)\n+      self.model.evaluate(\n+          self.data, steps=self.steps, callbacks=self.callbacks, verbose=2)\n \n       logging.info(\n           'End of evaluation. Metrics: %s', ' '.join([\n\n@@ -16,17 +16,21 @@\n import tensorflow.compat.v2 as tf\n \"\"\"Strategy combinations for combinations.combine().\"\"\"\n \n+from tensorflow.python.distribute import strategy_combinations\n+\n \n multidevice_strategies = [\n     tf.__internal__.distribute.combinations.mirrored_strategy_with_gpu_and_cpu,\n     tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus,\n+    strategy_combinations.mirrored_strategy_with_two_gpus_no_merge_call,\n     tf.__internal__.distribute.combinations.tpu_strategy,\n ]\n \n multiworker_strategies = [\n     tf.__internal__.distribute.combinations.multi_worker_mirrored_2x1_cpu,\n     tf.__internal__.distribute.combinations.multi_worker_mirrored_2x1_gpu,\n-    tf.__internal__.distribute.combinations.multi_worker_mirrored_2x2_gpu\n+    tf.__internal__.distribute.combinations.multi_worker_mirrored_2x2_gpu,\n+    strategy_combinations.multi_worker_mirrored_2x2_gpu_no_merge_call\n ]\n \n strategies_minus_default_minus_tpu = [\n@@ -34,6 +38,7 @@ strategies_minus_default_minus_tpu = [\n     tf.__internal__.distribute.combinations.one_device_strategy_gpu,\n     tf.__internal__.distribute.combinations.mirrored_strategy_with_gpu_and_cpu,\n     tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus,\n+    strategy_combinations.mirrored_strategy_with_two_gpus_no_merge_call,\n     tf.__internal__.distribute.combinations.central_storage_strategy_with_gpu_and_cpu\n ]\n \n@@ -43,6 +48,7 @@ strategies_minus_tpu = [\n     tf.__internal__.distribute.combinations.one_device_strategy_gpu,\n     tf.__internal__.distribute.combinations.mirrored_strategy_with_gpu_and_cpu,\n     tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus,\n+    strategy_combinations.mirrored_strategy_with_two_gpus_no_merge_call,\n     tf.__internal__.distribute.combinations.central_storage_strategy_with_gpu_and_cpu\n ]\n \n@@ -50,6 +56,7 @@ multi_worker_mirrored_strategies = [\n     tf.__internal__.distribute.combinations.multi_worker_mirrored_2x1_cpu,\n     tf.__internal__.distribute.combinations.multi_worker_mirrored_2x1_gpu,\n     tf.__internal__.distribute.combinations.multi_worker_mirrored_2x2_gpu,\n+    strategy_combinations.multi_worker_mirrored_2x2_gpu_no_merge_call\n ]\n \n tpu_strategies = [\n\n@@ -590,7 +590,10 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n       elif dtype.is_integer or dtype.is_unsigned or dtype.is_bool:\n         initializer = initializers.get('zeros')\n       # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX here?\n-      else:\n+      elif 'getter' not in kwargs:\n+        # When `getter` is specified, it's possibly fine for `initializer` to be\n+        # None since it's up to the custom `getter` to raise error in case it\n+        # indeed needs `initializer`.\n         raise ValueError('An initializer for variable %s of type %s is required'\n                          ' for layer %s' % (name, dtype.base_dtype, self.name))\n \n\n@@ -299,6 +299,14 @@ class BaseLayerTest(keras_parameterized.TestCase):\n       # Cannot access tensor.name in eager execution.\n       self.assertIn('Variable_2/Regularizer', layer.losses[0].name)\n \n+  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  def test_add_weight_by_getter(self):\n+    layer = base_layer.Layer()\n+    variable = tf.Variable('abc')\n+    added = layer.add_weight(\n+        dtype=tf.string, getter=lambda *_, **__: variable)\n+    self.assertIs(variable, added)\n+\n   @combinations.generate(combinations.keras_mode_combinations(mode=['eager']))\n   def test_learning_phase_freezing_for_layers(self):\n \n\n@@ -357,6 +357,7 @@ class Layer(base_layer.Layer):\n       if kwarg not in ['getter', 'collections', 'experimental_autocast',\n                        'caching_device']:\n         raise TypeError('Unknown keyword argument:', kwarg)\n+    has_custom_getter = 'getter' in kwargs\n     getter = kwargs.pop('getter', base_layer_utils.make_variable)\n     collections_arg = kwargs.pop('collections', None)\n     # 'experimental_autocast' can be set to False by the caller to indicate an\n@@ -398,7 +399,10 @@ class Layer(base_layer.Layer):\n       elif dtype.is_integer or dtype.is_unsigned or dtype.is_bool:\n         initializer = tf.compat.v1.zeros_initializer()\n       # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX here?\n-      else:\n+      elif not has_custom_getter:\n+        # When `getter` is specified, it's possibly fine for `initializer` to be\n+        # None since it's up to the custom `getter` to raise error in case it\n+        # indeed needs `initializer`.\n         raise ValueError('An initializer for variable %s of type %s is required'\n                          ' for layer %s' % (name, dtype.base_dtype, self.name))\n \n\n@@ -42,15 +42,6 @@ from tensorflow.python.util.tf_export import keras_export\n keras_data_adapter_gauge = tf.__internal__.monitoring.BoolGauge(\n     \"/tensorflow/api/oss-keras/data_adapters\", \"keras data adapter usage\", \"method\")\n \n-try:\n-  from scipy import sparse as scipy_sparse  # pylint: disable=g-import-not-at-top\n-except ImportError:\n-  scipy_sparse = None\n-try:\n-  import pandas as pd  # pylint: disable=g-import-not-at-top\n-except ImportError:\n-  pd = None\n-\n \n @six.add_metaclass(abc.ABCMeta)\n class DataAdapter(object):\n@@ -223,9 +214,7 @@ class TensorLikeDataAdapter(DataAdapter):\n     if y is not None:\n       flat_inputs += tf.nest.flatten(y)\n \n-    tensor_types = (tf.Tensor, np.ndarray)\n-    if pd:\n-      tensor_types = (tf.Tensor, np.ndarray, pd.Series, pd.DataFrame)\n+    tensor_types = _get_tensor_types()\n \n     def _is_tensor(v):\n       if isinstance(v, tensor_types):\n@@ -558,9 +547,7 @@ class CompositeTensorDataAdapter(DataAdapter):\n           not _is_distributed_dataset(v)):\n         return True\n       # Support Scipy sparse tensors if scipy is installed\n-      if scipy_sparse is not None and scipy_sparse.issparse(v):\n-        return True\n-      return False\n+      return _is_scipy_sparse(v)\n \n     def _is_tensor_or_composite(v):\n       if isinstance(v, (tf.Tensor, np.ndarray)):\n@@ -1036,7 +1023,7 @@ def _process_tensorlike(inputs):\n       if issubclass(x.dtype.type, np.floating):\n         dtype = backend.floatx()\n       return tf.convert_to_tensor(x, dtype=dtype)\n-    elif scipy_sparse and scipy_sparse.issparse(x):\n+    elif _is_scipy_sparse(x):\n       return _scipy_sparse_to_sparse_tensor(x)\n     return x\n \n@@ -1452,9 +1439,7 @@ def train_validation_split(arrays, validation_split):\n   \"\"\"\n \n   def _can_split(t):\n-    tensor_types = (tf.Tensor, np.ndarray)\n-    if pd:\n-      tensor_types = (tf.Tensor, np.ndarray, pd.Series, pd.DataFrame)\n+    tensor_types = _get_tensor_types()\n     return isinstance(t, tensor_types) or t is None\n \n   flat_arrays = tf.nest.flatten(arrays)\n@@ -1635,6 +1620,24 @@ def _check_data_cardinality(data):\n     raise ValueError(msg)\n \n \n+def _get_tensor_types():\n+  try:\n+    import pandas as pd  # pylint: disable=g-import-not-at-top\n+\n+    return (tf.Tensor, np.ndarray, pd.Series, pd.DataFrame)\n+  except ImportError:\n+    return (tf.Tensor, np.ndarray)\n+\n+\n+def _is_scipy_sparse(x):\n+  try:\n+    from scipy.sparse import issparse  # pylint: disable=g-import-not-at-top\n+\n+    return issparse(x)\n+  except ImportError:\n+    return False\n+\n+\n def _scipy_sparse_to_sparse_tensor(t):\n   \"\"\"Converts a SciPy sparse matrix to a SparseTensor.\"\"\"\n   sparse_coo = t.tocoo()\n\n@@ -47,7 +47,6 @@ from keras.saving.saved_model import json_utils\n from keras.saving.saved_model import model_serialization\n from keras.utils import generic_utils\n from keras.utils import layer_utils\n-from keras.utils import tf_inspect\n from keras.utils import tf_utils\n from keras.utils import version_utils\n from keras.utils.io_utils import ask_to_proceed_with_overwrite\n@@ -772,14 +771,23 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     # data when a `tf.data.Dataset` is provided.\n     data = data_adapter.expand_1d(data)\n     x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)\n-\n+    # Run forward pass.\n     with tf.GradientTape() as tape:\n       y_pred = self(x, training=True)\n       loss = self.compiled_loss(\n           y, y_pred, sample_weight, regularization_losses=self.losses)\n+    # Run backwards pass.\n     self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n     self.compiled_metrics.update_state(y, y_pred, sample_weight)\n-    return {m.name: m.result() for m in self.metrics}\n+    # Collect metrics to return\n+    return_metrics = {}\n+    for metric in self.metrics:\n+      result = metric.result()\n+      if isinstance(result, dict):\n+        return_metrics.update(result)\n+      else:\n+        return_metrics[metric.name] = result\n+    return return_metrics\n \n   def make_train_function(self):\n     \"\"\"Creates a function that executes one step of training.\n@@ -1145,7 +1153,6 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         if validation_data and self._should_eval(epoch, validation_freq):\n           # Create data_handler for evaluation and cache it.\n           if getattr(self, '_eval_data_handler', None) is None:\n-            self._fit_frame = tf_inspect.currentframe()\n             self._eval_data_handler = data_adapter.get_data_handler(\n                 x=val_x,\n                 y=val_y,\n@@ -1169,7 +1176,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n               max_queue_size=max_queue_size,\n               workers=workers,\n               use_multiprocessing=use_multiprocessing,\n-              return_dict=True)\n+              return_dict=True,\n+              _use_cached_eval_dataset=True)\n           val_logs = {'val_' + name: val for name, val in val_logs.items()}\n           epoch_logs.update(val_logs)\n \n@@ -1181,7 +1189,6 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       # If eval data_hanlder exists, delete it after all epochs are done.\n       if getattr(self, '_eval_data_handler', None) is not None:\n         del self._eval_data_handler\n-        del self._fit_frame\n       callbacks.on_train_end(logs=training_logs)\n       return self.history\n \n@@ -1215,9 +1222,16 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     # Updates stateful loss metrics.\n     self.compiled_loss(\n         y, y_pred, sample_weight, regularization_losses=self.losses)\n-\n     self.compiled_metrics.update_state(y, y_pred, sample_weight)\n-    return {m.name: m.result() for m in self.metrics}\n+    # Collect metrics to return\n+    return_metrics = {}\n+    for metric in self.metrics:\n+      result = metric.result()\n+      if isinstance(result, dict):\n+        return_metrics.update(result)\n+      else:\n+        return_metrics[metric.name] = result\n+    return return_metrics\n \n   def make_test_function(self):\n     \"\"\"Creates a function that executes one step of evaluation.\n@@ -1289,7 +1303,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                max_queue_size=10,\n                workers=1,\n                use_multiprocessing=False,\n-               return_dict=False):\n+               return_dict=False,\n+               **kwargs):\n     \"\"\"Returns the loss value & metrics values for the model in test mode.\n \n     Computation is done in batches (see the `batch_size` arg.)\n@@ -1353,6 +1368,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         return_dict: If `True`, loss and metric results are returned as a dict,\n           with each key being the name of the metric. If `False`, they are\n           returned as a list.\n+        **kwargs: Unused at this time.\n \n     See the discussion of `Unpacking behavior for iterator-like inputs` for\n     `Model.fit`.\n@@ -1372,6 +1388,9 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     self._assert_compile_was_called()\n     self._check_call_args('evaluate')\n     _disallow_inside_tf_function('evaluate')\n+    use_cached_eval_dataset = kwargs.pop('_use_cached_eval_dataset', False)\n+    if kwargs:\n+      raise TypeError('Invalid keyword arguments: %s' % (kwargs,))\n \n     if self.distribute_strategy._should_use_with_coordinator:  # pylint: disable=protected-access\n       raise NotImplementedError('`model.evaluate` is not yet supported with '\n@@ -1379,8 +1398,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n     with self.distribute_strategy.scope():\n       # Use cached evaluation data only when it's called in `Model.fit`\n-      if (getattr(self, '_fit_frame', None) is not None\n-          and tf_inspect.currentframe().f_back is self._fit_frame\n+      if (use_cached_eval_dataset\n           and getattr(self, '_eval_data_handler', None) is not None):\n         data_handler = self._eval_data_handler\n       else:\n@@ -1432,16 +1450,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       if return_dict:\n         return logs\n       else:\n-        results = []\n-        for name in self.metrics_names:\n-          if name in logs:\n-            results.append(logs[name])\n-        for key in sorted(logs.keys()):\n-          if key not in self.metrics_names:\n-            results.append(logs[key])\n-        if len(results) == 1:\n-          return results[0]\n-        return results\n+        return flatten_metrics_in_order(logs, self.metrics_names)\n \n   def predict_step(self, data):\n     \"\"\"The logic for one inference step.\n@@ -1770,10 +1779,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     if return_dict:\n       return logs\n     else:\n-      results = [logs.get(name, None) for name in self.metrics_names]\n-      if len(results) == 1:\n-        return results[0]\n-      return results\n+      return flatten_metrics_in_order(logs, self.metrics_names)\n \n   def test_on_batch(self,\n                     x,\n@@ -1829,10 +1835,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     if return_dict:\n       return logs\n     else:\n-      results = [logs.get(name, None) for name in self.metrics_names]\n-      if len(results) == 1:\n-        return results[0]\n-      return results\n+      return flatten_metrics_in_order(logs, self.metrics_names)\n \n   def predict_on_batch(self, x):\n     \"\"\"Returns predictions for a single batch of samples.\n@@ -2883,3 +2886,17 @@ def _is_readable_tf_checkpoint(filepath):\n   except tf.errors.DataLossError:\n     # The checkpoint is not readable in TensorFlow format.\n     return False\n+\n+\n+def flatten_metrics_in_order(logs, metrics_names):\n+  \"\"\"Turns the `logs` dict into a list as per key order of `metrics_names`.\"\"\"\n+  results = []\n+  for name in metrics_names:\n+    if name in logs:\n+      results.append(logs[name])\n+  for key in sorted(logs.keys()):\n+    if key not in metrics_names:\n+      results.append(logs[key])\n+  if len(results) == 1:\n+    return results[0]\n+  return results\n\n@@ -1762,7 +1762,6 @@ class LossWeightingTest(keras_parameterized.TestCase):\n         input_shape=(input_dim,),\n         num_classes=num_classes)\n     int_y_test = y_test.copy()\n-    int_y_train = y_train.copy()\n     # convert class vectors to binary class matrices\n     y_train = np_utils.to_categorical(y_train, num_classes)\n     y_test = np_utils.to_categorical(y_test, num_classes)\n@@ -3420,6 +3419,41 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n       model.add_metric(\n           tf.reduce_sum(y), name='metric_1', aggregation=None)\n \n+  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  def test_calling_evaluate_in_callback_during_fit(self):\n+    # Check fix for a bug that caused `evaluate` to hit a cached dataset\n+    # when run from inside a fit callback.\n+    x = layers_module.Input(shape=(2,))\n+    y = layers_module.Dense(2, kernel_initializer='ones', use_bias=False)(x)\n+    model = training_module.Model(x, y)\n+\n+    ones = np.ones((10, 2), dtype=np.float32)\n+    zeros = np.zeros((10, 2), dtype=np.float32)\n+    train_ds = tf.data.Dataset.from_tensor_slices(\n+        (ones, ones)).batch(5)\n+    val_ds_1 = tf.data.Dataset.from_tensor_slices(\n+        (ones, ones)).batch(5)\n+    val_ds_2 = tf.data.Dataset.from_tensor_slices(\n+        (zeros, zeros)).batch(5)\n+    model.compile('sgd', 'mse', run_eagerly=testing_utils.should_run_eagerly())\n+\n+    class MyCallback(Callback):\n+\n+      def on_epoch_end(self, *args, **kwargs):\n+        eval_result = self.model.evaluate(val_ds_2)\n+        if abs(eval_result) > 1e-7:\n+          raise AssertionError(\n+              'Expected to hit the zeros dataset but got high loss value of %s'\n+              % eval_result)\n+\n+    history = model.fit(\n+        train_ds, validation_data=val_ds_1, callbacks=[MyCallback()])\n+    # Evaluate at the end of fit should hit the ones dataset (cached)\n+    self.assertGreater(abs(history.history['val_loss'][-1]), 0.1)\n+    # Standalone call to evaluate should not hit the cached dataset\n+    eval_result = model.evaluate(val_ds_2)\n+    self.assertLess(abs(eval_result), 1e-7)\n+\n   @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n   def test_model_with_nested_compiled_model(self):\n \n@@ -3467,6 +3501,64 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n     self.assertEqual([m.name for m in outer_model.metrics],\n                      ['loss', 'acc2', 'mean', 'mean1', 'mean2'])\n \n+  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  def test_model_with_metric_class_that_returns_dict(self):\n+    x = layers_module.Input(shape=(2,))\n+    y = layers_module.Dense(3)(x)\n+    model = training_module.Model(x, y)\n+\n+    class DictMetric(metrics_module.Metric):\n+\n+      def __init__(self):\n+        super(DictMetric, self).__init__()\n+        self.sample_count = tf.Variable(0)\n+        self.l2_sum = tf.Variable(0.)\n+\n+      def update_state(self, y_true, y_pred, sample_weight=None):\n+        self.l2_sum.assign_add(\n+            tf.reduce_sum(tf.square(y_true - y_pred)))\n+        self.sample_count.assign_add(tf.compat.v1.shape(y_true)[0])\n+\n+      def reset_state(self):\n+        self.sample_count.assign(0)\n+        self.l2_sum.assign(0.)\n+\n+      def result(self):\n+        mse = self.l2_sum / tf.cast(self.sample_count, 'float32')\n+        rmse = tf.sqrt(mse)\n+        return {'my_mse': mse,\n+                'my_rmse': rmse}\n+\n+    model.compile('sgd',\n+                  'mse',\n+                  metrics=['mae', DictMetric()],\n+                  run_eagerly=testing_utils.should_run_eagerly())\n+\n+    history = model.fit(np.ones((10, 2)), np.ones((10, 3)))\n+    self.assertEqual(list(history.history.keys()),\n+                     ['loss', 'mae', 'my_mse', 'my_rmse'])\n+    list_evaluate_res = model.evaluate(\n+        np.ones((10, 2)), np.ones((10, 3)))\n+    self.assertEqual(len(list_evaluate_res), 4)\n+    dict_evaluate_res = model.evaluate(\n+        np.ones((10, 2)), np.ones((10, 3)), return_dict=True)\n+    self.assertEqual(list(dict_evaluate_res.keys()),\n+                     ['loss', 'mae', 'my_mse', 'my_rmse'])\n+    list_train_on_batch_res = model.train_on_batch(\n+        np.ones((10, 2)), np.ones((10, 3)))\n+    self.assertEqual(len(list_train_on_batch_res), 4)\n+    dict_train_on_batch_res = model.train_on_batch(\n+        np.ones((10, 2)), np.ones((10, 3)), return_dict=True)\n+    self.assertEqual(list(dict_train_on_batch_res.keys()),\n+                     ['loss', 'mae', 'my_mse', 'my_rmse'])\n+    list_test_on_batch_res = model.test_on_batch(\n+        np.ones((10, 2)), np.ones((10, 3)))\n+    self.assertEqual(len(list_test_on_batch_res), 4)\n+    dict_test_on_batch_res = model.test_on_batch(\n+        np.ones((10, 2)), np.ones((10, 3)), return_dict=True)\n+    self.assertEqual(list(dict_test_on_batch_res.keys()),\n+                     ['loss', 'mae', 'my_mse', 'my_rmse'])\n+\n \n class BareUpdateLayer(layers_module.Layer):\n \n\n@@ -109,15 +109,15 @@ class Pooling1D(Layer):\n class MaxPooling1D(Pooling1D):\n   \"\"\"Max pooling operation for 1D temporal data.\n \n-  Downsamples the input representation by taking the maximum value over the\n-  window defined by `pool_size`. The window is shifted by `strides`.  The\n-  resulting output when using \"valid\" padding option has a shape of:\n+  Downsamples the input representation by taking the maximum value over a\n+  spatial window of size `pool_size`. The window is shifted by `strides`.  The\n+  resulting output, when using the `\"valid\"` padding option, has a shape of:\n   `output_shape = (input_shape - pool_size + 1) / strides)`\n \n-  The resulting output shape when using the \"same\" padding option is:\n+  The resulting output shape when using the `\"same\"` padding option is:\n   `output_shape = input_shape / strides`\n \n-  For example, for strides=1 and padding=\"valid\":\n+  For example, for `strides=1` and `padding=\"valid\"`:\n \n   >>> x = tf.constant([1., 2., 3., 4., 5.])\n   >>> x = tf.reshape(x, [1, 5, 1])\n@@ -130,7 +130,7 @@ class MaxPooling1D(Pooling1D):\n           [4.],\n           [5.]]], dtype=float32)>\n \n-  For example, for strides=2 and padding=\"valid\":\n+  For example, for `strides=2` and `padding=\"valid\"`:\n \n   >>> x = tf.constant([1., 2., 3., 4., 5.])\n   >>> x = tf.reshape(x, [1, 5, 1])\n@@ -141,7 +141,7 @@ class MaxPooling1D(Pooling1D):\n   array([[[2.],\n           [4.]]], dtype=float32)>\n \n-  For example, for strides=1 and padding=\"same\":\n+  For example, for `strides=1` and `padding=\"same\"`:\n \n   >>> x = tf.constant([1., 2., 3., 4., 5.])\n   >>> x = tf.reshape(x, [1, 5, 1])\n@@ -398,17 +398,21 @@ class Pooling2D(Layer):\n class MaxPooling2D(Pooling2D):\n   \"\"\"Max pooling operation for 2D spatial data.\n \n-  Downsamples the input representation by taking the maximum value over the\n-  window defined by `pool_size` for each dimension along the features axis.\n-  The window is shifted by `strides` in each dimension.  The resulting output\n-  when using \"valid\" padding option has a shape(number of rows or columns) of:\n-  `output_shape = math.floor((input_shape - pool_size) / strides) + 1`\n-  (when input_shape >= pool_size)\n+  Downsamples the input along its spatial dimensions (height and width)\n+  by taking the maximum value over an input window\n+  (of size defined by `pool_size`) for each channel of the input.\n+  The window is shifted by `strides` along each dimension.\n \n-  The resulting output shape when using the \"same\" padding option is:\n+  The resulting output,\n+  when using the `\"valid\"` padding option, has a spatial shape\n+  (number of rows or columns) of:\n+  `output_shape = math.floor((input_shape - pool_size) / strides) + 1`\n+  (when `input_shape >= pool_size`)\n+\n+  The resulting output shape when using the `\"same\"` padding option is:\n   `output_shape = math.floor((input_shape - 1) / strides) + 1`\n \n-  For example, for stride=(1,1) and padding=\"valid\":\n+  For example, for `strides=(1, 1)` and `padding=\"valid\"`:\n \n   >>> x = tf.constant([[1., 2., 3.],\n   ...                  [4., 5., 6.],\n@@ -423,7 +427,7 @@ class MaxPooling2D(Pooling2D):\n             [[8.],\n              [9.]]]], dtype=float32)>\n \n-  For example, for stride=(2,2) and padding=\"valid\":\n+  For example, for `strides=(2, 2)` and `padding=\"valid\"`:\n \n   >>> x = tf.constant([[1., 2., 3., 4.],\n   ...                  [5., 6., 7., 8.],\n@@ -531,17 +535,20 @@ class MaxPooling2D(Pooling2D):\n class AveragePooling2D(Pooling2D):\n   \"\"\"Average pooling operation for spatial data.\n \n-  Downsamples the input representation by taking the average value over the\n-  window defined by `pool_size` for each dimension along the features axis.\n-  The window is shifted by `strides` in each dimension.  The resulting output\n-  when using \"valid\" padding option has a shape(number of rows or columns) of:\n-  `output_shape = math.floor((input_shape - pool_size) / strides) + 1`\n-  (when input_shape >= pool_size)\n+  Downsamples the input along its spatial dimensions (height and width)\n+  by taking the average value over an input window\n+  (of size defined by `pool_size`) for each channel of the input.\n+  The window is shifted by `strides` along each dimension.\n \n-  The resulting output shape when using the \"same\" padding option is:\n+  The resulting output when using `\"valid\"` padding option has a shape\n+  (number of rows or columns) of:\n+  `output_shape = math.floor((input_shape - pool_size) / strides) + 1`\n+  (when `input_shape >= pool_size`)\n+\n+  The resulting output shape when using the `\"same\"` padding option is:\n   `output_shape = math.floor((input_shape - 1) / strides) + 1`\n \n-  For example, for stride=(1,1) and padding=\"valid\":\n+  For example, for `strides=(1, 1)` and `padding=\"valid\"`:\n \n   >>> x = tf.constant([[1., 2., 3.],\n   ...                  [4., 5., 6.],\n@@ -556,7 +563,7 @@ class AveragePooling2D(Pooling2D):\n             [[6.],\n              [7.]]]], dtype=float32)>\n \n-  For example, for stride=(2,2) and padding=\"valid\":\n+  For example, for `stride=(2, 2)` and `padding=\"valid\"`:\n \n   >>> x = tf.constant([[1., 2., 3., 4.],\n   ...                  [5., 6., 7., 8.],\n@@ -569,7 +576,7 @@ class AveragePooling2D(Pooling2D):\n     array([[[[3.5],\n              [5.5]]]], dtype=float32)>\n \n-  For example, for stride=(1,1) and padding=\"same\":\n+  For example, for `strides=(1, 1)` and `padding=\"same\"`:\n \n   >>> x = tf.constant([[1., 2., 3.],\n   ...                  [4., 5., 6.],\n@@ -738,6 +745,11 @@ class Pooling3D(Layer):\n class MaxPooling3D(Pooling3D):\n   \"\"\"Max pooling operation for 3D data (spatial or spatio-temporal).\n \n+  Downsamples the input along its spatial dimensions (depth, height, and width)\n+  by taking the maximum value over an input window\n+  (of size defined by `pool_size`) for each channel of the input.\n+  The window is shifted by `strides` along each dimension.\n+\n   Args:\n     pool_size: Tuple of 3 integers,\n       factors by which to downscale (dim1, dim2, dim3).\n@@ -773,6 +785,19 @@ class MaxPooling3D(Pooling3D):\n     - If `data_format='channels_first'`:\n       5D tensor with shape:\n       `(batch_size, channels, pooled_dim1, pooled_dim2, pooled_dim3)`\n+\n+  Example:\n+\n+  ```python\n+  depth = 30\n+  height = 30\n+  width = 30\n+  input_channels = 3\n+\n+  inputs = tf.keras.Input(shape=(depth, height, width, input_channels))\n+  layer = tf.keras.layers.MaxPooling3D(pool_size=3)\n+  outputs = layer(inputs)  # Shape: (batch_size, 10, 10, 10, 3)\n+  ```\n   \"\"\"\n \n   def __init__(self,\n@@ -791,6 +816,11 @@ class MaxPooling3D(Pooling3D):\n class AveragePooling3D(Pooling3D):\n   \"\"\"Average pooling operation for 3D data (spatial or spatio-temporal).\n \n+  Downsamples the input along its spatial dimensions (depth, height, and width)\n+  by taking the average value over an input window\n+  (of size defined by `pool_size`) for each channel of the input.\n+  The window is shifted by `strides` along each dimension.\n+\n   Args:\n     pool_size: tuple of 3 integers,\n       factors by which to downscale (dim1, dim2, dim3).\n@@ -826,6 +856,19 @@ class AveragePooling3D(Pooling3D):\n     - If `data_format='channels_first'`:\n       5D tensor with shape:\n       `(batch_size, channels, pooled_dim1, pooled_dim2, pooled_dim3)`\n+\n+  Example:\n+\n+  ```python\n+  depth = 30\n+  height = 30\n+  width = 30\n+  input_channels = 3\n+\n+  inputs = tf.keras.Input(shape=(depth, height, width, input_channels))\n+  layer = tf.keras.layers.AveragePooling3D(pool_size=3)\n+  outputs = layer(inputs)  # Shape: (batch_size, 10, 10, 10, 3)\n+  ```\n   \"\"\"\n \n   def __init__(self,\n\n@@ -273,7 +273,8 @@ class Metric(base_layer.Layer):\n \n   ### For use by subclasses ###\n   @doc_controls.for_subclass_implementers\n-  def add_weight(self,\n+  def add_weight(\n+      self,\n       name,\n       shape=(),\n       aggregation=tf.compat.v1.VariableAggregation.SUM,\n@@ -372,7 +373,15 @@ class Reduce(Metric):\n     [values], sample_weight = \\\n         metrics_utils.ragged_assert_compatible_and_get_flat_values(\n             [values], sample_weight)\n+    try:\n       values = tf.cast(values, self._dtype)\n+    except (ValueError, TypeError):\n+      msg = ('The output of a metric function can only be a single Tensor. '\n+             'Got: %s' % (values,))\n+      if isinstance(values, dict):\n+        msg += ('. To return a dict of values, implement a custom Metric '\n+                'subclass.')\n+      raise RuntimeError(msg)\n     if sample_weight is not None:\n       sample_weight = tf.cast(sample_weight, self._dtype)\n       # Update dimensions of weights to match with values if possible.\n\n@@ -34,7 +34,7 @@ from keras import metrics\n from keras import Model\n from keras import testing_utils\n from keras.engine import base_layer\n-from keras.engine import training as training_mod\n+from keras.engine import training as training_module\n \n \n @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n@@ -2078,7 +2078,7 @@ class CustomMetricsTest(tf.test.TestCase):\n \n   def test_metric_not_tracked_as_sublayer_in_model(self):\n \n-    class MyModel(training_mod.Model):\n+    class MyModel(training_module.Model):\n \n       def __init__(self, **kwargs):\n         super(MyModel, self).__init__(**kwargs)\n@@ -2097,6 +2097,50 @@ class CustomMetricsTest(tf.test.TestCase):\n     self.assertLen(model.layers, 0)\n     self.assertLen(model.metrics, 2)\n \n+  def test_invalid_custom_metric_class_error_msg(self):\n+    x = layers.Input(shape=(2,))\n+    y = layers.Dense(3)(x)\n+    model = training_module.Model(x, y)\n+\n+    class BadMetric(metrics.Metric):\n+\n+      def update_state(self, y_true, y_pred, sample_weight=None):\n+        return\n+\n+      def result(self):\n+        return\n+\n+    with self.assertRaisesRegex(RuntimeError,\n+                                'can only be a single'):\n+      model.compile('sgd',\n+                    'mse',\n+                    metrics=[BadMetric()])\n+      model.fit(np.ones((10, 2)), np.ones((10, 3)))\n+\n+  def test_invalid_custom_metric_fn_error_msg(self):\n+    x = layers.Input(shape=(2,))\n+    y = layers.Dense(3)(x)\n+    model = training_module.Model(x, y)\n+\n+    def bad_metric(y_true, y_pred, sample_weight=None):  # pylint: disable=unused-argument\n+      return None\n+\n+    def dict_metric(y_true, y_pred, sample_weight=None):  # pylint: disable=unused-argument\n+      return {'value': 0.}\n+\n+    with self.assertRaisesRegex(RuntimeError,\n+                                'The output of a metric function can only be'):\n+      model.compile('sgd',\n+                    'mse',\n+                    metrics=[bad_metric])\n+      model.fit(np.ones((10, 2)), np.ones((10, 3)))\n+    with self.assertRaisesRegex(RuntimeError,\n+                                'To return a dict of values, implement'):\n+      model.compile('sgd',\n+                    'mse',\n+                    metrics=[dict_metric])\n+      model.fit(np.ones((10, 2)), np.ones((10, 3)))\n+\n \n def _get_model(compile_metrics):\n   model_layers = [\n\n@@ -326,7 +326,8 @@ class _DynamicLossScaleState(tf.__internal__.tracking.Trackable):\n         step.\n     \"\"\"\n     grads = tf.nest.flatten(grads)\n-    if tf.distribute.has_strategy():\n+    if tf.distribute.has_strategy(\n+    ) and tf.distribute.in_cross_replica_context():\n       distribution = tf.distribute.get_strategy()\n       is_finite_per_replica = distribution.extended.call_for_each_replica(\n           _is_all_finite, args=(grads,))\n@@ -700,28 +701,13 @@ class LossScaleOptimizer(_DelegatingTrackableMixin, optimizer_v2.OptimizerV2):\n       # pylint: enable=protected-access\n \n     grads_and_vars = tuple(grads_and_vars)\n-    return tf.distribute.get_replica_context().merge_call(\n-        self._apply_gradients_cross_replica,\n-        args=(grads_and_vars, name))\n-\n-  def _apply_gradients_cross_replica(self, distribution, grads_and_vars, name):\n     grads = [g for g, _ in grads_and_vars]\n-    if isinstance(self._loss_scale, _DynamicLossScaleState):\n-      loss_scale_update_op, should_apply_grads = self._loss_scale.update(grads)\n-    else:\n-      loss_scale_update_op = tf.no_op()\n-      should_apply_grads = True\n-\n-    def apply_fn():\n     # We do not want DistributionStrategy to unwrap any MirroredVariables in\n-      # grads_and_vars, because even in a replica context, the wrapped optimizer\n-      # expects mirrored variables. So we wrap the variables with an\n+    # grads_and_vars, because even in a replica context, the wrapped\n+    # optimizer expects mirrored variables. So we wrap the variables with an\n     # _UnwrapPreventer, preventing DistributionStrategy from unwrapping the\n     # MirroredVariables.\n     wrapped_vars = _UnwrapPreventer([v for _, v in grads_and_vars])\n-      return distribution.extended.call_for_each_replica(\n-          self._apply_gradients,\n-          args=(grads, wrapped_vars, name))\n \n     def do_not_apply_fn():\n       # Normally self._optimizer.iterations is incremented in\n@@ -729,14 +715,43 @@ class LossScaleOptimizer(_DelegatingTrackableMixin, optimizer_v2.OptimizerV2):\n       # branch, we increment it here instead.\n       return self._optimizer.iterations.assign_add(1, read_value=False)\n \n-    # Note: We must call this cond() in a cross-replica context.\n-    # DistributionStrategy does not support having a cond in a replica context\n-    # with a branch that calls `merge_call`, and self._optimizer.apply_gradients\n-    # calls `merge_call`.\n+    def _if_should_apply_grads(grads):\n+      if isinstance(self._loss_scale, _DynamicLossScaleState):\n+        return self._loss_scale.update(grads)\n+      else:\n+        return (tf.no_op(), True)\n+\n+    if optimizer_utils.strategy_supports_no_merge_call():\n+      loss_scale_update_op, should_apply_grads = _if_should_apply_grads(grads)\n+      def apply_fn():\n+        return self._apply_gradients(grads, wrapped_vars, name)\n+\n       maybe_apply_op = smart_cond.smart_cond(should_apply_grads, apply_fn,\n                                              do_not_apply_fn)\n       return tf.group(maybe_apply_op, loss_scale_update_op)\n \n+    else:\n+\n+      def _apply_gradients_cross_replica(distribution, grads, wrapped_vars,\n+                                         name):\n+        loss_scale_update_op, should_apply_grads = _if_should_apply_grads(grads)\n+\n+        def apply_fn():\n+          return distribution.extended.call_for_each_replica(\n+              self._apply_gradients,\n+              args=(grads, wrapped_vars, name))\n+\n+        # Note: We must call this cond() in a cross-replica context.\n+        # DistributionStrategy does not support having a cond in a replica\n+        # context with a branch that calls `merge_call`, and\n+        # self._optimizer.apply_gradients calls `merge_call`.\n+        maybe_apply_op = smart_cond.smart_cond(should_apply_grads, apply_fn,\n+                                               do_not_apply_fn)\n+        return tf.group(maybe_apply_op, loss_scale_update_op)\n+      return tf.distribute.get_replica_context().merge_call(\n+          _apply_gradients_cross_replica,\n+          args=(grads, wrapped_vars, name))\n+\n   def _apply_gradients(self, grads, wrapped_vars, name):\n     # Pass experimental_aggregate_gradients=False since LossScaleOptimizer\n     # already aggregated the gradients.\n\n@@ -27,7 +27,6 @@ import functools\n import warnings\n \n import six\n-from tensorflow.python.distribute import values as ds_values\n from keras import backend\n from keras import initializers\n from keras.engine import base_layer_utils\n@@ -654,12 +653,8 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n         grads_and_vars = self._aggregate_gradients(grads_and_vars)\n       grads_and_vars = self._transform_gradients(grads_and_vars)\n \n-      return tf.distribute.get_replica_context().merge_call(\n-          functools.partial(self._distributed_apply, apply_state=apply_state),\n-          args=(grads_and_vars,),\n-          kwargs={\n-              \"name\": name,\n-          })\n+      return self._distributed_apply(strategy, grads_and_vars, name,\n+                                     apply_state)\n \n   def _distributed_apply(self, distribution, grads_and_vars, name, apply_state):\n     \"\"\"`apply_gradients` using a `DistributionStrategy`.\"\"\"\n@@ -692,21 +687,13 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n     update_ops = []\n     with name_scope_only_in_function_or_graph(name or self._name):\n       for grad, var in grads_and_vars:\n-        # TODO(crccw): It's not allowed to assign PerReplica value to\n-        # MirroredVariable.  Remove this after we relax this restriction.\n-        def _assume_mirrored(grad):\n-          if isinstance(grad, ds_values.PerReplica):\n-            return ds_values.Mirrored(grad.values)\n-          return grad\n-\n-        grad = tf.nest.map_structure(_assume_mirrored, grad)\n         # Colocate the update with variables to avoid unnecessary communication\n         # delays. See b/136304694.\n         with distribution.extended.colocate_vars_with(var):\n           with name_scope_only_in_function_or_graph(\n               \"update\" if eagerly_outside_functions else \"update_\" +\n               var.op.name):\n-            update_ops.extend(distribution.extended.update(\n+            update_ops.append(tf.distribute.get_replica_context()._update(  # pylint: disable=protected-access\n                 var, apply_grad_to_update_var, args=(grad,), group=False))\n \n       any_symbolic = any(isinstance(i, tf.Operation) or\n\n@@ -33,12 +33,13 @@ def all_reduce_sum_gradients(grads_and_vars):\n   \"\"\"\n   grads_and_vars = list(grads_and_vars)\n   filtered_grads_and_vars = filter_empty_gradients(grads_and_vars)\n-  # We switch to a cross-replica context since there is a bug which causes\n-  # IndexedSlices to be converted to dense tensors when all-reduced in a\n-  # replica context.\n-  # TODO(b/150507409): Do not switch to a cross-replica context once the bug\n-  # is fixed.\n   if filtered_grads_and_vars:\n+    if strategy_supports_no_merge_call():\n+      grads = [pair[0] for pair in filtered_grads_and_vars]\n+      reduced = tf.distribute.get_strategy().extended._replica_ctx_all_reduce(  # pylint: disable=protected-access\n+          tf.distribute.ReduceOp.SUM, grads)\n+    else:\n+      # TODO(b/183257003): Remove this branch\n       reduced = tf.distribute.get_replica_context().merge_call(\n           _all_reduce_sum_fn, args=(filtered_grads_and_vars,))\n   else:\n@@ -147,3 +148,11 @@ def make_gradient_clipvalue_fn(clipvalue):\n def _all_reduce_sum_fn(distribution, grads_and_vars):\n   return distribution.extended.batch_reduce_to(tf.distribute.ReduceOp.SUM,\n                                                grads_and_vars)\n+\n+\n+def strategy_supports_no_merge_call():\n+  \"\"\"Returns if the current Strategy can operate in pure replica context.\"\"\"\n+  if not tf.distribute.has_strategy():\n+    return True\n+  strategy = tf.distribute.get_strategy()\n+  return not getattr(strategy.extended, \"_use_merge_call\", True)\n\n@@ -159,13 +159,13 @@ def array_to_img(x, data_format=None, scale=True, dtype=None):\n \n \n   Args:\n-      x: Input Numpy array.\n+      x: Input data, in any form that can be converted to a Numpy array.\n       data_format: Image data format, can be either \"channels_first\" or\n         \"channels_last\". Defaults to `None`, in which case the global setting\n         `tf.keras.backend.image_data_format()` is used (unless you changed it,\n         it defaults to \"channels_last\").\n-      scale: Whether to rescale image values to be within `[0, 255]`. Defaults\n-        to `True`.\n+      scale: Whether to rescale the image such that minimum and maximum values\n+        are 0 and 255 respectively. Defaults to `True`.\n       dtype: Dtype to use. Default to `None`, in which case the global setting\n       `tf.keras.backend.floatx()` is used (unless you changed it, it defaults\n       to \"float32\")\n\n@@ -24,7 +24,6 @@ import re\n import types\n \n from google.protobuf import message\n-\n from keras import backend\n from keras import regularizers\n from keras.engine import input_spec\n\n@@ -107,7 +107,23 @@ def result_wrapper(result_fn):\n     has_strategy = tf.distribute.has_strategy()\n     replica_context = tf.distribute.get_replica_context()\n     if not has_strategy or replica_context is None:\n-      result_t = tf.identity(result_fn(*args))\n+      raw_result = result_fn(*args)\n+      # Results need to be wrapped in a `tf.identity` op to ensure\n+      # correct execution order.\n+      if isinstance(raw_result,\n+                    (tf.Tensor, tf.Variable, float, int)):\n+        result_t = tf.identity(raw_result)\n+      elif isinstance(raw_result, dict):\n+        result_t = {key: tf.identity(value)\n+                    for key, value in raw_result.items()}\n+      else:\n+        try:\n+          result_t = tf.identity(raw_result)\n+        except (ValueError, TypeError):\n+          raise RuntimeError(\n+              'The output of `metric.result()` can only be a single '\n+              'Tensor/Variable, or a dict of Tensors/Variables. '\n+              'For metric %s, got result %s.' % (metric_obj.name, raw_result))\n     else:\n       # TODO(psv): Test distribution of metrics using different distribution\n       # strategies.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
