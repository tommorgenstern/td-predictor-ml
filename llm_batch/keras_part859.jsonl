{"custom_id": "keras#8409e18dd73dcffaa4c5772326ce33866d8c4df5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 199 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 5 | Methods Changed: 7 | Complexity Δ (Sum/Max): 11/7 | Churn Δ: 201 | Churn Cumulative: 334 | Contributors (this commit): 5 | Commits (past 90d): 11 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,7 +2,14 @@ from keras.src.api_export import keras_export\n from keras.src.layers.preprocessing.image_preprocessing.base_image_preprocessing_layer import (  # noqa: E501\n     BaseImagePreprocessingLayer,\n )\n+from keras.src.layers.preprocessing.image_preprocessing.bounding_boxes.converters import (  # noqa: E501\n+    clip_to_image_size,\n+)\n+from keras.src.layers.preprocessing.image_preprocessing.bounding_boxes.converters import (  # noqa: E501\n+    convert_format,\n+)\n from keras.src.random.seed_generator import SeedGenerator\n+from keras.src.utils import backend_utils\n \n HORIZONTAL = \"horizontal\"\n VERTICAL = \"vertical\"\n@@ -77,7 +84,7 @@ class RandomFlip(BaseImagePreprocessingLayer):\n         flips = self.backend.numpy.less_equal(\n             self.backend.random.uniform(shape=flips_shape, seed=seed), 0.5\n         )\n-        return {\"flips\": flips}\n+        return {\"flips\": flips, \"input_shape\": shape}\n \n     def transform_images(self, images, transformation, training=True):\n         images = self.backend.cast(images, self.compute_dtype)\n@@ -94,7 +101,82 @@ class RandomFlip(BaseImagePreprocessingLayer):\n         transformation,\n         training=True,\n     ):\n-        raise NotImplementedError\n+        def _flip_boxes_horizontal(boxes):\n+            x1, x2, x3, x4 = self.backend.numpy.split(boxes, 4, axis=-1)\n+            outputs = self.backend.numpy.concatenate(\n+                [1 - x3, x2, 1 - x1, x4], axis=-1\n+            )\n+            return outputs\n+\n+        def _flip_boxes_vertical(boxes):\n+            x1, x2, x3, x4 = self.backend.numpy.split(boxes, 4, axis=-1)\n+            outputs = self.backend.numpy.concatenate(\n+                [x1, 1 - x4, x3, 1 - x2], axis=-1\n+            )\n+            return outputs\n+\n+        def _transform_xyxy(boxes, box_flips):\n+            if backend_utils.in_tf_graph():\n+                self.backend.set_backend(\"tensorflow\")\n+\n+            bboxes = boxes[\"boxes\"]\n+            if self.mode in {HORIZONTAL, HORIZONTAL_AND_VERTICAL}:\n+                bboxes = self.backend.numpy.where(\n+                    box_flips,\n+                    _flip_boxes_horizontal(bboxes),\n+                    bboxes,\n+                )\n+            if self.mode in {VERTICAL, HORIZONTAL_AND_VERTICAL}:\n+                bboxes = self.backend.numpy.where(\n+                    box_flips,\n+                    _flip_boxes_vertical(bboxes),\n+                    bboxes,\n+                )\n+\n+            self.backend.reset()\n+\n+            return bboxes\n+\n+        flips = self.backend.numpy.squeeze(transformation[\"flips\"], axis=-1)\n+\n+        if self.data_format == \"channels_first\":\n+            height_axis = -2\n+            width_axis = -1\n+        else:\n+            height_axis = -3\n+            width_axis = -2\n+\n+        input_height, input_width = (\n+            transformation[\"input_shape\"][height_axis],\n+            transformation[\"input_shape\"][width_axis],\n+        )\n+\n+        bounding_boxes = convert_format(\n+            bounding_boxes,\n+            source=self.bounding_box_format,\n+            target=\"rel_xyxy\",\n+            height=input_height,\n+            width=input_width,\n+        )\n+\n+        bounding_boxes[\"boxes\"] = _transform_xyxy(bounding_boxes, flips)\n+\n+        bounding_boxes = clip_to_image_size(\n+            bounding_boxes=bounding_boxes,\n+            height=input_height,\n+            width=input_width,\n+            format=\"xyxy\",\n+        )\n+\n+        bounding_boxes = convert_format(\n+            bounding_boxes,\n+            source=\"rel_xyxy\",\n+            target=self.bounding_box_format,\n+            height=input_height,\n+            width=input_width,\n+        )\n+\n+        return bounding_boxes\n \n     def transform_segmentation_masks(\n         self, segmentation_masks, transformation, training=True\n\n@@ -168,3 +168,118 @@ class RandomFlipTest(testing.TestCase):\n         ds = tf_data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n         output = next(iter(ds)).numpy()\n         self.assertAllClose(output, expected_output)\n+\n+    @parameterized.named_parameters(\n+        (\n+            \"with_horizontal\",\n+            \"horizontal\",\n+            [[4, 1, 6, 3], [0, 4, 2, 6]],\n+        ),\n+        (\n+            \"with_vertical\",\n+            \"vertical\",\n+            [[2, 7, 4, 9], [6, 4, 8, 6]],\n+        ),\n+        (\n+            \"with_horizontal_and_vertical\",\n+            \"horizontal_and_vertical\",\n+            [[4, 7, 6, 9], [0, 4, 2, 6]],\n+        ),\n+    )\n+    def test_random_flip_bounding_boxes(self, mode, expected_boxes):\n+        data_format = backend.config.image_data_format()\n+        if data_format == \"channels_last\":\n+            image_shape = (10, 8, 3)\n+        else:\n+            image_shape = (3, 10, 8)\n+        input_image = np.random.random(image_shape)\n+        bounding_boxes = {\n+            \"boxes\": np.array(\n+                [\n+                    [2, 1, 4, 3],\n+                    [6, 4, 8, 6],\n+                ]\n+            ),\n+            \"labels\": np.array([[1, 2]]),\n+        }\n+        input_data = {\"images\": input_image, \"bounding_boxes\": bounding_boxes}\n+        random_flip_layer = layers.RandomFlip(\n+            mode,\n+            data_format=data_format,\n+            seed=42,\n+            bounding_box_format=\"xyxy\",\n+        )\n+\n+        transformation = {\n+            \"flips\": np.asarray([[True]]),\n+            \"input_shape\": input_image.shape,\n+        }\n+        output = random_flip_layer.transform_bounding_boxes(\n+            input_data[\"bounding_boxes\"],\n+            transformation=transformation,\n+            training=True,\n+        )\n+\n+        self.assertAllClose(output[\"boxes\"], expected_boxes)\n+\n+    @parameterized.named_parameters(\n+        (\n+            \"with_horizontal\",\n+            \"horizontal\",\n+            [[4, 1, 6, 3], [0, 4, 2, 6]],\n+        ),\n+        (\n+            \"with_vertical\",\n+            \"vertical\",\n+            [[2, 7, 4, 9], [6, 4, 8, 6]],\n+        ),\n+        (\n+            \"with_horizontal_and_vertical\",\n+            \"horizontal_and_vertical\",\n+            [[4, 7, 6, 9], [0, 4, 2, 6]],\n+        ),\n+    )\n+    def test_random_flip_tf_data_bounding_boxes(self, mode, expected_boxes):\n+        data_format = backend.config.image_data_format()\n+        if data_format == \"channels_last\":\n+            image_shape = (1, 10, 8, 3)\n+        else:\n+            image_shape = (1, 3, 10, 8)\n+        input_image = np.random.random(image_shape)\n+        bounding_boxes = {\n+            \"boxes\": np.array(\n+                [\n+                    [\n+                        [2, 1, 4, 3],\n+                        [6, 4, 8, 6],\n+                    ]\n+                ]\n+            ),\n+            \"labels\": np.array([[1, 2]]),\n+        }\n+\n+        input_data = {\"images\": input_image, \"bounding_boxes\": bounding_boxes}\n+\n+        ds = tf_data.Dataset.from_tensor_slices(input_data)\n+        random_flip_layer = layers.RandomFlip(\n+            mode,\n+            data_format=data_format,\n+            seed=42,\n+            bounding_box_format=\"xyxy\",\n+        )\n+\n+        transformation = {\n+            \"flips\": np.asarray([[True]]),\n+            \"input_shape\": input_image.shape,\n+        }\n+        ds = ds.map(\n+            lambda x: random_flip_layer.transform_bounding_boxes(\n+                x[\"bounding_boxes\"],\n+                transformation=transformation,\n+                training=True,\n+            )\n+        )\n+\n+        output = next(iter(ds))\n+        expected_boxes = np.array(expected_boxes)\n+        self.assertAllClose(output[\"boxes\"], expected_boxes)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#400dfd4b0b03b3c25bac726bb13a89ea64cab135", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 73 | Lines Deleted: 12 | Files Changed: 3 | Hunks: 13 | Methods Changed: 8 | Complexity Δ (Sum/Max): 11/9 | Churn Δ: 85 | Churn Cumulative: 2598 | Contributors (this commit): 10 | Commits (past 90d): 28 | Contributors (cumulative): 19 | DMM Complexity: 0.5740740740740741\n\nDIFF:\n@@ -447,6 +447,7 @@ class CompileLoss(losses_module.Loss):\n         )\n         self._flat_losses = None\n         self._y_pred_build_structure = None\n+        self._y_true_build_structure = None\n \n     @property\n     def metrics(self):\n@@ -512,10 +513,6 @@ class CompileLoss(losses_module.Loss):\n                 y_true, y_pred, loss, output_names, current_path\n             )\n \n-        # At this point loss should match the structure of y_pred and y_true\n-        # We assume y_pred and y_true have the same structure and this\n-        # have been already asserted.\n-\n         if not isinstance(loss, type(y_pred)):\n             raise KeyError(\n                 f\"The path: {current_path} in \"\n@@ -528,17 +525,20 @@ class CompileLoss(losses_module.Loss):\n             iterator = loss.items()\n \n             def key_check_fn(key, objs):\n-                return all([key in obj for obj in objs])\n+                return all(\n+                    [isinstance(obj, dict) and key in obj for obj in objs]\n+                )\n \n         elif issubclass(type(loss), (list, tuple)):\n             iterator = enumerate(loss)\n \n             def key_check_fn(key, objs):\n-                try:\n-                    [obj[key] for obj in objs]\n-                except:\n-                    return False\n-                return True\n+                return all(\n+                    [\n+                        issubclass(type(obj), (list, tuple)) and key < len(obj)\n+                        for obj in objs\n+                    ]\n+                )\n \n         else:\n             raise TypeError(\n@@ -547,6 +547,8 @@ class CompileLoss(losses_module.Loss):\n             )\n \n         for key, _loss in iterator:\n+            if _loss is None:\n+                continue\n             if not key_check_fn(key, (y_true, y_pred)):\n                 raise KeyError(\n                     f\"The path: {current_path + (key,)} in \"\n@@ -570,6 +572,11 @@ class CompileLoss(losses_module.Loss):\n \n         # Pytree leaf container\n         class WeightedLoss:\n+            def __new__(cls, loss, weight):\n+                if loss is None:\n+                    return None\n+                return object.__new__(cls)\n+\n             def __init__(self, loss, weight):\n                 self.loss = loss\n                 self.weight = weight\n@@ -646,6 +653,9 @@ class CompileLoss(losses_module.Loss):\n         self._y_pred_build_structure = tree.map_structure(\n             lambda x: None, y_pred\n         )\n+        self._y_true_build_structure = tree.map_structure(\n+            lambda x: None, y_true\n+        )\n         self.built = True\n \n     def _get_y_pred_output_names(self, y_pred):\n@@ -683,8 +693,22 @@ class CompileLoss(losses_module.Loss):\n                     # Check case where y_true has the same structure but uses\n                     # different (but reconcilable) container types,\n                     # e.g `list` vs `tuple`.\n+                    try:\n                         tree.assert_same_paths(y_true, y_pred)\n-                    y_true = tree.pack_sequence_as(y_pred, tree.flatten(y_true))\n+                        y_true = tree.pack_sequence_as(\n+                            y_pred, tree.flatten(y_true)\n+                        )\n+                    except:\n+                        # Check case where loss is partially defined over y_pred\n+                        flat_y_true = tree.flatten(y_true)\n+                        flat_loss = tree.flatten(self._user_loss)\n+                        flat_loss_non_nones = [\n+                            loss for loss in flat_loss if loss is not None\n+                        ]\n+                        assert len(flat_y_true) == len(flat_loss_non_nones)\n+                        if not tree.is_nested(y_true):\n+                            y_true = flat_y_true\n+\n             except:\n                 y_true_struct = tree.map_structure(lambda _: \"*\", y_true)\n                 y_pred_struct = tree.map_structure(lambda _: \"*\", y_pred)\n@@ -705,8 +729,13 @@ class CompileLoss(losses_module.Loss):\n             y_pred = tree.pack_sequence_as(\n                 self._y_pred_build_structure, tree.flatten(y_pred)\n             )\n+        try:\n+            tree.assert_same_structure(\n+                self._y_true_build_structure, y_true, check_types=False\n+            )\n+        except ValueError:\n             y_true = tree.pack_sequence_as(\n-                self._y_pred_build_structure, tree.flatten(y_true)\n+                self._y_true_build_structure, tree.flatten(y_true)\n             )\n \n         # We need to add a dummy `None` if the model has only a single output.\n\n@@ -520,3 +520,18 @@ class TestCompileLoss(testing.TestCase):\n         ):\n             wrong_struc_y_true = [np.array([[1]])]\n             compile_loss(wrong_struc_y_true, y_pred)\n+\n+    def test_y_true_partial_y_pred_span(self):\n+        ones = np.ones((320, 3))\n+        zeros = np.zeros((320, 3))\n+        y_pred = [ones, zeros, zeros]\n+        y_true = ones\n+\n+        compile_loss = CompileLoss(\n+            loss=[\"mse\", None, None], output_names=[\"a\", \"b\", \"c\"]\n+        )\n+        # build call\n+        compile_loss(y_true, y_pred)\n+        # built call\n+        loss = compile_loss(y_true, y_pred)\n+        self.assertEqual(loss, 0.0)\n\n@@ -2544,6 +2544,23 @@ class TestTrainer(testing.TestCase):\n             atol=1e-3,\n         )\n \n+    @pytest.mark.requires_trainable_backend\n+    def test_partial_loss_partial_label(self):\n+        inputs = keras.Input((2,))\n+        x = keras.layers.Dense(3, kernel_initializer=\"ones\")(inputs)\n+        partial_model = keras.Model(inputs, [x, x, x])\n+        partial_model.compile(loss=[\"mse\", None, None])\n+        full_model = keras.Model(inputs, [x, x, x])\n+        full_model.compile(loss=[\"mse\", \"mse\", \"mse\"])\n+\n+        eval_x = np.ones((32, 2))\n+        eval_y = np.ones((32, 3))\n+\n+        partial_logs = partial_model.evaluate(eval_x, eval_y, return_dict=True)\n+        logs = full_model.evaluate(eval_x, [eval_y] * 3, return_dict=True)\n+\n+        self.assertAlmostEqual(partial_logs[\"loss\"] * 3, logs[\"loss\"])\n+\n     def test_symbolic_build(self):\n         class ExampleModelWithTrainingArgs(Trainer, layers.Layer):\n             def __init__(self, units):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#270029bf827493efab6f457e6348be05f7656168", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 35 | Churn Cumulative: 1151 | Contributors (this commit): 8 | Commits (past 90d): 18 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -760,14 +760,14 @@ class CompileLoss(losses_module.Loss):\n             value = ops.cast(\n                 loss_fn(y_t, y_p, _sample_weight), dtype=self.dtype\n             )\n-            if loss_weight is not None:\n-                value = ops.multiply(value, loss_weight)\n-            loss_values.append(value)\n-            # Record individual losses.\n+            # Record *unweighted* individual losses.\n             if metric:\n                 metric.update_state(\n                     value, sample_weight=tree.flatten(y_p)[0].shape[0]\n                 )\n+            if loss_weight is not None:\n+                value = ops.multiply(value, loss_weight)\n+            loss_values.append(value)\n \n         if loss_values:\n             total_loss = sum(loss_values)\n\n@@ -375,6 +375,33 @@ class TestCompileLoss(testing.TestCase):\n         value = compile_loss(y_true, y_pred)\n         self.assertAllClose(value, 1.07666, atol=1e-5)\n \n+    def test_struct_loss_valid_weights(self):\n+        y_true = {\n+            \"a\": np.array([1, 2]),\n+            \"b\": np.array([1, 2]),\n+        }\n+        y_pred = {\n+            \"a\": np.array([3, 4]),\n+            \"b\": np.array([3, 4]),\n+        }\n+        loss = {\"a\": \"mse\", \"b\": \"mse\"}\n+        compile_loss = CompileLoss(\n+            loss=loss,\n+            output_names=[\"a\", \"b\"],\n+            loss_weights={\n+                \"a\": np.ones((2,)),\n+                \"b\": np.zeros((2,)),\n+            },\n+        )\n+        compile_loss.build(y_true, y_pred)\n+        value = compile_loss(y_true, y_pred)\n+        self.assertAllClose(value, 4)\n+\n+        # Metrics still report unweighted loss.\n+        a_loss_mean, b_loss_mean = compile_loss.metrics\n+        self.assertEqual(a_loss_mean.result(), 4)\n+        self.assertEqual(b_loss_mean.result(), 4)\n+\n     def test_struct_loss_invalid_weights(self):\n         y_true = {\n             \"a\": {\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b2af6d55455044e4ca3c1ce1ec5b98b29f2d5693", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 6 | Files Changed: 4 | Hunks: 7 | Methods Changed: 6 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 24 | Churn Cumulative: 2019 | Contributors (this commit): 16 | Commits (past 90d): 29 | Contributors (cumulative): 27 | DMM Complexity: 0.0\n\nDIFF:\n@@ -67,7 +67,7 @@ class JsonUtilsTestTF(testing.TestCase):\n             \"serialized\": None,\n         }\n         string = json_utils.Encoder().encode(invalid_type_spec)\n-        with self.assertRaisesRegexp(\n+        with self.assertRaisesRegex(\n             ValueError, \"No TypeSpec has been registered\"\n         ):\n             loaded = json_utils.decode(string)\n\n@@ -2975,7 +2975,7 @@ class NumpyTwoInputOpsCorrectnessTest(testing.TestCase):\n         self.assertAllClose(knp.where(x > 1), np.where(x > 1))\n         self.assertAllClose(knp.Where()(x > 1), np.where(x > 1))\n \n-        with self.assertRaisesRegexp(\n+        with self.assertRaisesRegex(\n             ValueError, \"`x1` and `x2` either both should be `None`\"\n         ):\n             knp.where(x > 1, x, None)\n@@ -4898,10 +4898,10 @@ class SparseTest(testing.TestCase):\n     ]\n \n     def assertSameSparseness(self, x, y):\n-        self.assertEquals(sparseness(x), sparseness(y))\n+        self.assertEqual(sparseness(x), sparseness(y))\n \n     def assertSparseness(self, x, expected_sparseness):\n-        self.assertEquals(sparseness(x), expected_sparseness)\n+        self.assertEqual(sparseness(x), expected_sparseness)\n \n     @parameterized.named_parameters(ELEMENTWISE_UNARY_OPS_TESTS)\n     def test_elementwise_unary_symbolic_static_shape(\n\n@@ -29,7 +29,7 @@ class OptimizerTest(testing.TestCase):\n         v = backend.Variable([[3.0, 4.0], [5.0, 6.0]])\n         grads = None\n         optimizer = optimizers.SGD(learning_rate=1.0)\n-        with self.assertRaisesRegexp(\n+        with self.assertRaisesRegex(\n             ValueError, \"No gradients provided for any variable.\"\n         ):\n             optimizer.apply_gradients([(grads, v)])\n\n@@ -676,6 +676,18 @@ class CompileLoss(losses_module.Loss):\n             return self.call(y_true, y_pred, sample_weight)\n \n     def call(self, y_true, y_pred, sample_weight=None):\n+        if not tree.is_nested(y_true) and not tree.is_nested(y_pred):\n+            # Fast path: single output case / no loss-tracking metric.\n+            if not self.built:\n+                self.build(y_true, y_pred)\n+            _, loss_fn, loss_weight, _ = self._flat_losses[0]\n+            loss_value = ops.cast(\n+                loss_fn(y_true, y_pred, sample_weight), dtype=self.dtype\n+            )\n+            if loss_weight is not None:\n+                loss_value = ops.multiply(loss_value, loss_weight)\n+            return loss_value\n+\n         try:\n             tree.assert_same_structure(y_pred, y_true, check_types=False)\n         except ValueError:\n@@ -749,7 +761,7 @@ class CompileLoss(losses_module.Loss):\n                 object = object[_path]\n             return object\n \n-        for (path, loss_fn, loss_weight, name), metric in zip(\n+        for (path, loss_fn, loss_weight, _), metric in zip(\n             self._flat_losses, metrics\n         ):\n             y_t, y_p = resolve_path(path, y_true), resolve_path(path, y_pred)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8deee17166a2f36685a9b51d8ddc6dfee2db1298", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 210 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -101,6 +101,9 @@ class RandomFlip(BaseImagePreprocessingLayer):\n         transformation,\n         training=True,\n     ):\n+        if backend_utils.in_tf_graph():\n+            self.backend.set_backend(\"tensorflow\")\n+\n         def _flip_boxes_horizontal(boxes):\n             x1, x2, x3, x4 = self.backend.numpy.split(boxes, 4, axis=-1)\n             outputs = self.backend.numpy.concatenate(\n@@ -116,9 +119,6 @@ class RandomFlip(BaseImagePreprocessingLayer):\n             return outputs\n \n         def _transform_xyxy(boxes, box_flips):\n-            if backend_utils.in_tf_graph():\n-                self.backend.set_backend(\"tensorflow\")\n-\n             bboxes = boxes[\"boxes\"]\n             if self.mode in {HORIZONTAL, HORIZONTAL_AND_VERTICAL}:\n                 bboxes = self.backend.numpy.where(\n@@ -132,9 +132,6 @@ class RandomFlip(BaseImagePreprocessingLayer):\n                     _flip_boxes_vertical(bboxes),\n                     bboxes,\n                 )\n-\n-            self.backend.reset()\n-\n             return bboxes\n \n         flips = self.backend.numpy.squeeze(transformation[\"flips\"], axis=-1)\n@@ -176,6 +173,8 @@ class RandomFlip(BaseImagePreprocessingLayer):\n             width=input_width,\n         )\n \n+        self.backend.reset()\n+\n         return bounding_boxes\n \n     def transform_segmentation_masks(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#63751d20bcc2627fc88f912c04fa84a68c3eb519", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 129 | Lines Deleted: 0 | Files Changed: 15 | Hunks: 19 | Methods Changed: 13 | Complexity Δ (Sum/Max): 16/5 | Churn Δ: 129 | Churn Cumulative: 4458 | Contributors (this commit): 23 | Commits (past 90d): 109 | Contributors (cumulative): 105 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,6 +12,7 @@ from keras.src.activations.activations import elu\n from keras.src.activations.activations import exponential\n from keras.src.activations.activations import gelu\n from keras.src.activations.activations import glu\n+from keras.src.activations.activations import hard_shrink\n from keras.src.activations.activations import hard_sigmoid\n from keras.src.activations.activations import hard_silu\n from keras.src.activations.activations import hard_silu as hard_swish\n\n@@ -73,6 +73,7 @@ from keras.src.ops.nn import dot_product_attention\n from keras.src.ops.nn import elu\n from keras.src.ops.nn import gelu\n from keras.src.ops.nn import glu\n+from keras.src.ops.nn import hard_shrink\n from keras.src.ops.nn import hard_sigmoid\n from keras.src.ops.nn import hard_silu\n from keras.src.ops.nn import hard_silu as hard_swish\n\n@@ -18,6 +18,7 @@ from keras.src.ops.nn import dot_product_attention\n from keras.src.ops.nn import elu\n from keras.src.ops.nn import gelu\n from keras.src.ops.nn import glu\n+from keras.src.ops.nn import hard_shrink\n from keras.src.ops.nn import hard_sigmoid\n from keras.src.ops.nn import hard_silu\n from keras.src.ops.nn import hard_silu as hard_swish\n\n@@ -12,6 +12,7 @@ from keras.src.activations.activations import elu\n from keras.src.activations.activations import exponential\n from keras.src.activations.activations import gelu\n from keras.src.activations.activations import glu\n+from keras.src.activations.activations import hard_shrink\n from keras.src.activations.activations import hard_sigmoid\n from keras.src.activations.activations import hard_silu\n from keras.src.activations.activations import hard_silu as hard_swish\n\n@@ -73,6 +73,7 @@ from keras.src.ops.nn import dot_product_attention\n from keras.src.ops.nn import elu\n from keras.src.ops.nn import gelu\n from keras.src.ops.nn import glu\n+from keras.src.ops.nn import hard_shrink\n from keras.src.ops.nn import hard_sigmoid\n from keras.src.ops.nn import hard_silu\n from keras.src.ops.nn import hard_silu as hard_swish\n\n@@ -18,6 +18,7 @@ from keras.src.ops.nn import dot_product_attention\n from keras.src.ops.nn import elu\n from keras.src.ops.nn import gelu\n from keras.src.ops.nn import glu\n+from keras.src.ops.nn import hard_shrink\n from keras.src.ops.nn import hard_sigmoid\n from keras.src.ops.nn import hard_silu\n from keras.src.ops.nn import hard_silu as hard_swish\n\n@@ -5,6 +5,7 @@ from keras.src.activations.activations import elu\n from keras.src.activations.activations import exponential\n from keras.src.activations.activations import gelu\n from keras.src.activations.activations import glu\n+from keras.src.activations.activations import hard_shrink\n from keras.src.activations.activations import hard_sigmoid\n from keras.src.activations.activations import hard_silu\n from keras.src.activations.activations import hard_tanh\n@@ -45,6 +46,7 @@ ALL_OBJECTS = {\n     hard_sigmoid,\n     hard_silu,\n     hard_tanh,\n+    hard_shrink,\n     linear,\n     mish,\n     log_softmax,\n\n@@ -373,6 +373,22 @@ def hard_tanh(x):\n     return ops.hard_tanh(x)\n \n \n+@keras_export(\"keras.activations.hard_shrink\")\n+def hard_shrink(x, threshold=0.5):\n+    \"\"\"Hard Shrink activation function.\n+\n+    It is defined as:\n+\n+    `hard_shrink(x) = x if |x| > threshold`, `hard_shrink(x) = 0 otherwise`.\n+\n+    Args:\n+        x: Input tensor.\n+        threshold: Threshold value. Defaults to `0.5`.\n+\n+    \"\"\"\n+    return ops.hard_shrink(x, threshold=threshold)\n+\n+\n @keras_export(\"keras.activations.sigmoid\")\n def sigmoid(x):\n     \"\"\"Sigmoid activation function.\n\n@@ -665,6 +665,15 @@ class ActivationsTest(testing.TestCase):\n         expected = hard_tanh(x)\n         self.assertAllClose(result, expected, rtol=1e-05)\n \n+    def test_hard_shrink(self):\n+        def hard_shrink(x):\n+            return np.where(np.abs(x) > 0.5, x, 0.0)\n+\n+        x = np.random.random((2, 5))\n+        result = activations.hard_shrink(x[np.newaxis, :])[0]\n+        expected = hard_shrink(x)\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n     def test_elu(self):\n         x = np.random.random((2, 5))\n         result = activations.elu(x[np.newaxis, :])[0]\n\n@@ -103,6 +103,11 @@ def hard_tanh(x):\n     return jnn.hard_tanh(x)\n \n \n+def hard_shrink(x, threshold=0.5):\n+    x = convert_to_tensor(x)\n+    return jnp.where(jnp.abs(x) > threshold, x, 0.0)\n+\n+\n def softmax(x, axis=-1):\n     x = convert_to_tensor(x)\n     return jnn.softmax(x, axis=axis)\n\n@@ -139,6 +139,15 @@ def hard_tanh(x):\n     return np.array(np.clip(x, min_val, max_val), dtype=x.dtype)\n \n \n+def hard_shrink(x, threshold=0.5):\n+    x = convert_to_tensor(x)\n+    threshold = np.asarray(threshold, x.dtype)\n+    return np.array(\n+        np.where(np.abs(x) > threshold, x, np.array(0.0, dtype=x.dtype)),\n+        dtype=x.dtype,\n+    )\n+\n+\n def softmax(x, axis=None):\n     exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n     return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n\n@@ -96,6 +96,10 @@ def hard_tanh(x):\n     return tf.clip_by_value(x, clip_value_min=-1.0, clip_value_max=1.0)\n \n \n+def hard_shrink(x, threshold=0.5):\n+    return tf.where(tf.abs(x) > threshold, x, tf.zeros_like(x))\n+\n+\n def softmax(x, axis=-1):\n     logits = x\n     if axis is None:\n\n@@ -103,6 +103,11 @@ def hard_tanh(x):\n     return tnn.hardtanh(x, min_val=-1.0, max_val=1.0)\n \n \n+def hard_shrink(x, threshold=0.5):\n+    x = convert_to_tensor(x)\n+    return tnn.hardshrink(x, lambd=threshold)\n+\n+\n def softmax(x, axis=-1):\n     x = convert_to_tensor(x)\n     dtype = backend.standardize_dtype(x.dtype)\n\n@@ -618,6 +618,46 @@ def hard_tanh(x):\n     return backend.nn.hard_tanh(x)\n \n \n+class HardShrink(Operation):\n+    def __init__(self, threshold=0.5):\n+        super().__init__()\n+        self.threshold = threshold\n+\n+    def call(self, x):\n+        return backend.nn.hard_shrink(x, self.threshold)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape, dtype=x.dtype)\n+\n+\n+@keras_export([\"keras.ops.hard_shrink\", \"keras.ops.nn.hard_shrink\"])\n+def hard_shrink(x, threshold=0.5):\n+    \"\"\"Hard Shrink activation function.\n+\n+    The Hard Shrink function is a thresholding operation defined as:\n+\n+    `f(x) = x if |x| > threshold`, `f(x) = 0 otherwise`.\n+\n+    Args:\n+        x: Input tensor.\n+        threshold: Threshold value. Defaults to `0.5`.\n+\n+    Returns:\n+        A tensor with the same shape as `x`.\n+\n+    Example:\n+\n+    >>> x = np.array([-0.5, 0., 1.])\n+    >>> x_hard_shrink = keras.ops.hard_shrink(x)\n+    >>> print(x_hard_shrink)\n+    array([0. 0. 1.], shape=(3,), dtype=float64)\n+\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return HardShrink(threshold).symbolic_call(x)\n+    return backend.nn.hard_shrink(x, threshold)\n+\n+\n class Softmax(Operation):\n     def __init__(self, axis=-1):\n         super().__init__()\n\n@@ -153,6 +153,10 @@ class NNOpsDynamicShapeTest(testing.TestCase):\n         x = KerasTensor([None, 2, 3])\n         self.assertEqual(knn.hard_tanh(x).shape, (None, 2, 3))\n \n+    def test_hard_shrink(self):\n+        x = KerasTensor([None, 2, 3])\n+        self.assertEqual(knn.hard_shrink(x).shape, (None, 2, 3))\n+\n     def test_softmax(self):\n         x = KerasTensor([None, 2, 3])\n         self.assertEqual(knn.softmax(x).shape, (None, 2, 3))\n@@ -810,6 +814,10 @@ class NNOpsStaticShapeTest(testing.TestCase):\n         x = KerasTensor([1, 2, 3])\n         self.assertEqual(knn.hard_tanh(x).shape, (1, 2, 3))\n \n+    def test_hard_shrink(self):\n+        x = KerasTensor([1, 2, 3])\n+        self.assertEqual(knn.hard_shrink(x).shape, (1, 2, 3))\n+\n     def test_softmax(self):\n         x = KerasTensor([1, 2, 3])\n         self.assertEqual(knn.softmax(x).shape, (1, 2, 3))\n@@ -1337,6 +1345,13 @@ class NNOpsCorrectnessTest(testing.TestCase):\n             [-1.0, 0.0, 1.0, 1.0, 1.0],\n         )\n \n+    def test_hard_shrink(self):\n+        x = np.array([-0.5, 0, 1, 2, 3], dtype=np.float32)\n+        self.assertAllClose(\n+            knn.hard_shrink(x),\n+            [0.0, 0.0, 1.0, 2.0, 3.0],\n+        )\n+\n     def test_softmax(self):\n         x = np.array([[1, 2, 3], [1, 2, 3]], dtype=np.float32)\n         self.assertAllClose(\n@@ -2444,6 +2459,24 @@ class NNOpsDtypeTest(testing.TestCase):\n             expected_dtype,\n         )\n \n+    @parameterized.named_parameters(named_product(dtype=FLOAT_DTYPES))\n+    def test_hard_shrink(self, dtype):\n+        import torch\n+        import torch.nn.functional as tnn\n+\n+        x = knp.ones((1), dtype=dtype)\n+        x_torch = torch.ones(1, dtype=getattr(torch, dtype))\n+        expected_dtype = standardize_dtype(tnn.hardshrink(x_torch).dtype)\n+\n+        self.assertEqual(\n+            standardize_dtype(knn.hard_shrink(x).dtype),\n+            expected_dtype,\n+        )\n+        self.assertEqual(\n+            standardize_dtype(knn.HardShrink().symbolic_call(x).dtype),\n+            expected_dtype,\n+        )\n+\n     @parameterized.named_parameters(named_product(dtype=FLOAT_DTYPES))\n     def test_glu(self, dtype):\n         import jax.nn as jnn\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9e1cddd5c86c014724d22b0763eed8ed8a98e662", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 762 | Contributors (this commit): 10 | Commits (past 90d): 18 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -379,11 +379,12 @@ def hard_shrink(x, threshold=0.5):\n \n     It is defined as:\n \n-    `hard_shrink(x) = x if |x| > threshold`, `hard_shrink(x) = 0 otherwise`.\n+    `hard_shrink(x) = x` if `|x| > threshold`,\n+    `hard_shrink(x) = 0` otherwise.\n \n     Args:\n         x: Input tensor.\n-        threshold: Threshold value. Defaults to `0.5`.\n+        threshold: Threshold value. Defaults to 0.5.\n \n     \"\"\"\n     return ops.hard_shrink(x, threshold=threshold)\n\n@@ -636,11 +636,12 @@ def hard_shrink(x, threshold=0.5):\n \n     The Hard Shrink function is a thresholding operation defined as:\n \n-    `f(x) = x if |x| > threshold`, `f(x) = 0 otherwise`.\n+    `f(x) = x` if `|x| > threshold`,\n+    `f(x) = 0` otherwise.\n \n     Args:\n         x: Input tensor.\n-        threshold: Threshold value. Defaults to `0.5`.\n+        threshold: Threshold value. Defaults to 0.5.\n \n     Returns:\n         A tensor with the same shape as `x`.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#04ced2dc7b7470145ee3d19a44d3c0851c37dfc9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 11 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 34 | Churn Cumulative: 1199 | Contributors (this commit): 8 | Commits (past 90d): 21 | Contributors (cumulative): 13 | DMM Complexity: 0.14285714285714285\n\nDIFF:\n@@ -715,12 +715,17 @@ class CompileLoss(losses_module.Loss):\n                         flat_y_true = tree.flatten(y_true)\n                         flat_loss = tree.flatten(self._user_loss)\n                         flat_loss_non_nones = [\n-                            loss for loss in flat_loss if loss is not None\n+                            (i, loss)\n+                            for i, loss in enumerate(flat_loss)\n+                            if loss is not None\n                         ]\n                         assert len(flat_y_true) == len(flat_loss_non_nones)\n-                        if not tree.is_nested(y_true):\n-                            y_true = flat_y_true\n-\n+                        y_true = [None] * len(flat_loss)\n+                        for y_t, (i, loss) in zip(\n+                            flat_y_true, flat_loss_non_nones\n+                        ):\n+                            y_true[i] = y_t\n+                        y_true = tree.pack_sequence_as(self._user_loss, y_true)\n             except:\n                 y_true_struct = tree.map_structure(lambda _: \"*\", y_true)\n                 y_pred_struct = tree.map_structure(lambda _: \"*\", y_pred)\n\n@@ -548,15 +548,22 @@ class TestCompileLoss(testing.TestCase):\n             wrong_struc_y_true = [np.array([[1]])]\n             compile_loss(wrong_struc_y_true, y_pred)\n \n-    def test_y_true_partial_y_pred_span(self):\n+    @parameterized.parameters(\n+        [\"mse\", None, None],\n+        [None, \"mse\", None],\n+        [None, None, \"mse\"],\n+        [None, \"mse\", \"mse\"],\n+        [\"mse\", None, \"mse\"],\n+    )\n+    def test_y_true_partial_y_pred_span(self, *loss_conf):\n+        loss_conf = list(loss_conf)\n         ones = np.ones((320, 3))\n         zeros = np.zeros((320, 3))\n-        y_pred = [ones, zeros, zeros]\n-        y_true = ones\n-\n-        compile_loss = CompileLoss(\n-            loss=[\"mse\", None, None], output_names=[\"a\", \"b\", \"c\"]\n-        )\n+        twos = np.ones((320, 3)) * 2\n+        y_pred = [zeros, ones, twos]\n+        y_true = [y for y, loss in zip(y_pred, loss_conf) if loss is not None]\n+        y_true = y_true[0] if len(y_true) == 1 else y_true\n+        compile_loss = CompileLoss(loss=loss_conf, output_names=[\"a\", \"b\", \"c\"])\n         # build call\n         compile_loss(y_true, y_pred)\n         # built call\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e0b3b79ed96df33e5aa23e13dc3df08cb8f399fa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 95 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 8 | Methods Changed: 6 | Complexity Δ (Sum/Max): 14/9 | Churn Δ: 95 | Churn Cumulative: 220 | Contributors (this commit): 6 | Commits (past 90d): 6 | Contributors (cumulative): 11 | DMM Complexity: 0.76\n\nDIFF:\n@@ -63,6 +63,12 @@ class BackupAndRestore(Callback):\n           When set to an integer, the callback saves the checkpoint every\n           `save_freq` batches. Set `save_freq=False` only if using\n           preemption checkpointing (i.e. with `save_before_preemption=True`).\n+        double_checkpoint: Boolean. If enabled, `BackupAndRestore` callback\n+          will save 2 last training states (current and previous). After\n+          interruption if current state can't be loaded due to IO error\n+          (e.g. file corrupted) it will try to restore previous one. Such\n+          behaviour will consume twice more space on disk, but increase fault\n+          tolerance. Defaults to `False`.\n         delete_checkpoint: Boolean. This `BackupAndRestore`\n           callback works by saving a checkpoint to back up the training state.\n           If `delete_checkpoint=True`, the checkpoint will be deleted after\n@@ -74,10 +80,12 @@ class BackupAndRestore(Callback):\n         self,\n         backup_dir,\n         save_freq=\"epoch\",\n+        double_checkpoint=False,\n         delete_checkpoint=True,\n     ):\n         super().__init__()\n         self.save_freq = save_freq\n+        self.double_checkpoint = double_checkpoint\n         self.delete_checkpoint = delete_checkpoint\n         self._batches_seen_since_last_saving = 0\n         self._last_batch_seen = 0\n@@ -90,6 +98,10 @@ class BackupAndRestore(Callback):\n         self._training_metadata_path = file_utils.join(\n             backup_dir, \"training_metadata.json\"\n         )\n+        self._prev_weights_path = self._weights_path + \".bkp\"\n+        self._prev_training_metadata_path = (\n+            self._training_metadata_path + \".bkp\"\n+        )\n         if save_freq != \"epoch\" and not isinstance(save_freq, int):\n             raise ValueError(\n                 \"Invalid value for argument `save_freq`. \"\n@@ -98,6 +110,23 @@ class BackupAndRestore(Callback):\n             )\n \n     def on_train_begin(self, logs=None):\n+        try:\n+            self._load_model()\n+        except OSError as e:\n+            # Weights may be corrupted. Trying to load previous one.\n+            if not file_utils.exists(self._prev_weights_path):\n+                raise e\n+            file_utils.copy(self._prev_weights_path, self._weights_path)\n+            if file_utils.exists(self._prev_training_metadata_path):\n+                file_utils.copy(\n+                    self._prev_training_metadata_path,\n+                    self._training_metadata_path,\n+                )\n+            elif file_utils.exists(self._training_metadata_path):\n+                file_utils.remove(self._training_metadata_path)\n+            self._load_model()\n+\n+    def _load_model(self):\n         \"\"\"Get training state from temporary file and restore it.\"\"\"\n         if not self.model.built:\n             raise ValueError(\n@@ -143,6 +172,14 @@ class BackupAndRestore(Callback):\n         # Create host directory if it doesn't exist.\n         if not file_utils.exists(self.backup_dir):\n             file_utils.makedirs(self.backup_dir)\n+        if self.double_checkpoint and file_utils.exists(self._weights_path):\n+            file_utils.copy(self._weights_path, self._prev_weights_path)\n+        if self.double_checkpoint and file_utils.exists(\n+            self._training_metadata_path\n+        ):\n+            file_utils.copy(\n+                self._training_metadata_path, self._prev_training_metadata_path\n+            )\n         self.model.save_weights(filepath=self._weights_path, overwrite=True)\n         with file_utils.File(self._training_metadata_path, \"w\") as f:\n             training_metadata = {\n\n@@ -147,6 +147,55 @@ class BackupAndRestoreCallbackTest(testing.TestCase):\n             self.assertEqual(hist.epoch[-1], 4)\n             self.assertEqual(int(model.layers[0].counter.value), 5 * 3)\n \n+    # Checking if after interruption and weights corruption, previous model\n+    # params and weights are loaded\n+    @pytest.mark.requires_trainable_backend\n+    def test_backup_corrupted(self):\n+        temp_dir = self.get_temp_dir()\n+        backup_dir = file_utils.join(temp_dir, \"subdir\")\n+        self.assertFalse(file_utils.exists(backup_dir))\n+\n+        model = self.make_model()\n+        self.assertEqual(int(model.layers[0].counter.value), 0)\n+        cbk = callbacks.BackupAndRestore(\n+            backup_dir=backup_dir, save_freq=\"epoch\", double_checkpoint=True\n+        )\n+\n+        x_train = np.random.random((10, 3))\n+        y_train = np.random.random((10, 1))\n+\n+        try:\n+            model.fit(\n+                x_train,\n+                y_train,\n+                batch_size=4,\n+                callbacks=[\n+                    cbk,\n+                    InterruptingCallback(steps_int=None, epoch_int=2),\n+                ],\n+                epochs=6,\n+                verbose=0,\n+            )\n+        except RuntimeError:\n+            self.assertEqual(cbk._current_epoch, 2)\n+            self.assertTrue(file_utils.exists(backup_dir))\n+            self.assertTrue(file_utils.exists(cbk._weights_path))\n+            self.assertTrue(file_utils.exists(cbk._training_metadata_path))\n+            self.assertTrue(file_utils.exists(cbk._prev_weights_path))\n+            self.assertTrue(file_utils.exists(cbk._prev_training_metadata_path))\n+            self.assertEqual(int(model.layers[0].counter.value), 6)\n+\n+            # Corruption weights\n+            with file_utils.File(cbk._weights_path, \"w\") as f:\n+                f.write(\"0\")\n+\n+            hist = model.fit(\n+                x_train, y_train, batch_size=4, callbacks=[cbk], epochs=5\n+            )\n+            self.assertEqual(cbk._current_epoch, 5)\n+            self.assertEqual(hist.epoch[-1], 4)\n+            self.assertEqual(int(model.layers[0].counter.value), 5 * 3)\n+\n     # Checking if after interruption, when model is deleted\n     @pytest.mark.requires_trainable_backend\n     def test_model_deleted_case_epoch(self):\n\n@@ -473,6 +473,15 @@ def isdir(path):\n     return os.path.isdir(path)\n \n \n+def remove(path):\n+    if is_remote_path(path):\n+        if gfile.available:\n+            return gfile.remove(path)\n+        else:\n+            _raise_if_no_gfile(path)\n+    return os.remove(path)\n+\n+\n def rmtree(path):\n     if is_remote_path(path):\n         if gfile.available:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b0b9d041833dce623871c1e233ea24316d4471be", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 10 | Churn Cumulative: 62 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -6,6 +6,7 @@ import numpy as np\n from keras.src import backend\n from keras.src import tree\n from keras.src.trainers.data_adapters import data_adapter_utils\n+from keras.src.utils.module_utils import tensorflow as tf\n \n try:\n     import pandas\n@@ -369,6 +370,15 @@ def convert_to_sliceable(arrays, target_backend=None):\n         if x is None:\n             return x\n \n+        # Special case: handle np \"object\" arrays containing strings\n+        if (\n+            isinstance(x, np.ndarray)\n+            and str(x.dtype) == \"object\"\n+            and backend.backend() == \"tensorflow\"\n+            and all(isinstance(e, str) for e in x)\n+        ):\n+            x = tf.convert_to_tensor(x, dtype=\"string\")\n+\n         # Step 1. Determine which Sliceable class to use.\n         if isinstance(x, np.ndarray):\n             sliceable_class = NumpySliceable\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#38570047a3b547197b56403727b0ffe533477e37", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 122 | Lines Deleted: 0 | Files Changed: 15 | Hunks: 19 | Methods Changed: 13 | Complexity Δ (Sum/Max): 16/5 | Churn Δ: 122 | Churn Cumulative: 4590 | Contributors (this commit): 23 | Commits (past 90d): 126 | Contributors (cumulative): 106 | DMM Complexity: 1.0\n\nDIFF:\n@@ -32,3 +32,4 @@ from keras.src.activations.activations import softmax\n from keras.src.activations.activations import softplus\n from keras.src.activations.activations import softsign\n from keras.src.activations.activations import tanh\n+from keras.src.activations.activations import tanh_shrink\n\n@@ -98,6 +98,7 @@ from keras.src.ops.nn import softmax\n from keras.src.ops.nn import softplus\n from keras.src.ops.nn import softsign\n from keras.src.ops.nn import sparse_categorical_crossentropy\n+from keras.src.ops.nn import tanh_shrink\n from keras.src.ops.numpy import abs\n from keras.src.ops.numpy import absolute\n from keras.src.ops.numpy import add\n\n@@ -43,3 +43,4 @@ from keras.src.ops.nn import softmax\n from keras.src.ops.nn import softplus\n from keras.src.ops.nn import softsign\n from keras.src.ops.nn import sparse_categorical_crossentropy\n+from keras.src.ops.nn import tanh_shrink\n\n@@ -32,3 +32,4 @@ from keras.src.activations.activations import softmax\n from keras.src.activations.activations import softplus\n from keras.src.activations.activations import softsign\n from keras.src.activations.activations import tanh\n+from keras.src.activations.activations import tanh_shrink\n\n@@ -98,6 +98,7 @@ from keras.src.ops.nn import softmax\n from keras.src.ops.nn import softplus\n from keras.src.ops.nn import softsign\n from keras.src.ops.nn import sparse_categorical_crossentropy\n+from keras.src.ops.nn import tanh_shrink\n from keras.src.ops.numpy import abs\n from keras.src.ops.numpy import absolute\n from keras.src.ops.numpy import add\n\n@@ -43,3 +43,4 @@ from keras.src.ops.nn import softmax\n from keras.src.ops.nn import softplus\n from keras.src.ops.nn import softsign\n from keras.src.ops.nn import sparse_categorical_crossentropy\n+from keras.src.ops.nn import tanh_shrink\n\n@@ -23,6 +23,7 @@ from keras.src.activations.activations import softmax\n from keras.src.activations.activations import softplus\n from keras.src.activations.activations import softsign\n from keras.src.activations.activations import tanh\n+from keras.src.activations.activations import tanh_shrink\n from keras.src.api_export import keras_export\n from keras.src.saving import object_registration\n from keras.src.saving import serialization_lib\n@@ -41,6 +42,7 @@ ALL_OBJECTS = {\n     gelu,\n     glu,\n     tanh,\n+    tanh_shrink,\n     sigmoid,\n     exponential,\n     hard_sigmoid,\n\n@@ -358,6 +358,20 @@ def tanh(x):\n     return ops.tanh(x)\n \n \n+@keras_export(\"keras.activations.tanh_shrink\")\n+def tanh_shrink(x):\n+    \"\"\"Tanh shrink activation function.\n+\n+    It is defined as:\n+\n+    `f(x) = x - tanh(x)`.\n+\n+    Args:\n+        x: Input tensor.\n+    \"\"\"\n+    return ops.tanh_shrink(x)\n+\n+\n @keras_export(\"keras.activations.hard_tanh\")\n def hard_tanh(x):\n     \"\"\"HardTanh activation function.\n\n@@ -656,6 +656,15 @@ class ActivationsTest(testing.TestCase):\n         expected = glu(x, axis=-2)\n         self.assertAllClose(result, expected, rtol=1e-05)\n \n+    def test_tanh_shrink(self):\n+        def tanh_shrink(x):\n+            return x - np.tanh(x)\n+\n+        x = np.random.random((2, 5))\n+        result = activations.tanh_shrink(x[np.newaxis, :])[0]\n+        expected = tanh_shrink(x)\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n     def test_hard_tanh(self):\n         def hard_tanh(x):\n             return np.clip(x, -1.0, 1.0)\n\n@@ -38,6 +38,11 @@ def tanh(x):\n     return jnn.tanh(x)\n \n \n+def tanh_shrink(x):\n+    x = convert_to_tensor(x)\n+    return x - jnp.tanh(x)\n+\n+\n def softplus(x):\n     x = convert_to_tensor(x)\n     return jnn.softplus(x)\n\n@@ -35,6 +35,11 @@ def tanh(x):\n     return np.tanh(x)\n \n \n+def tanh_shrink(x):\n+    x = convert_to_tensor(x)\n+    return x - np.tanh(x)\n+\n+\n def softplus(x):\n     x = convert_to_tensor(x)\n     return np.logaddexp(x, np.array(0.0, x.dtype))\n\n@@ -30,6 +30,10 @@ def tanh(x):\n     return tf.nn.tanh(x)\n \n \n+def tanh_shrink(x):\n+    return x - tf.math.tanh(x)\n+\n+\n def softplus(x):\n     return tf.math.softplus(x)\n \n\n@@ -35,6 +35,11 @@ def tanh(x):\n     return tnn.tanh(x)\n \n \n+def tanh_shrink(x):\n+    x = convert_to_tensor(x)\n+    return tnn.tanhshrink(x)\n+\n+\n def softplus(x):\n     x = convert_to_tensor(x)\n     return tnn.softplus(x)\n\n@@ -579,6 +579,45 @@ def glu(x, axis=-1):\n     return backend.nn.glu(x, axis=axis)\n \n \n+class TanhShrink(Operation):\n+    def __init__(self):\n+        super().__init__()\n+\n+    def call(self, x):\n+        return backend.nn.tanh_shrink(x)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape, dtype=x.dtype)\n+\n+\n+@keras_export([\"keras.ops.tanh_shrink\", \"keras.ops.nn.tanh_shrink\"])\n+def tanh_shrink(x):\n+    \"\"\"Applies the tanh shrink function element-wise.\n+\n+    It is defined as:\n+\n+    `f(x) = x - tanh(x)`.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        Output tensor of the same shape as `x`, where each element is\n+        transformed according to the tanh shrink operation.\n+\n+    Example:\n+\n+    >>> x = np.array([ -1., 0., 1.])\n+    >>> x_tanh_shrink = keras.ops.tanh_shrink(x)\n+    >>> print(x_tanh_shrink)\n+    array([-0.23840584  0.  0.23840584], shape=(3,), dtype=float64)\n+\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return TanhShrink().symbolic_call(x)\n+    return backend.nn.tanh_shrink(x)\n+\n+\n class HardTanh(Operation):\n     def __init__(self):\n         super().__init__()\n\n@@ -149,6 +149,10 @@ class NNOpsDynamicShapeTest(testing.TestCase):\n         x = KerasTensor([None, 2, 3])\n         self.assertEqual(knn.glu(x).shape, (None, 2, 3))\n \n+    def test_tanh_shrink(self):\n+        x = KerasTensor([None, 2, 3])\n+        self.assertEqual(knn.tanh_shrink(x).shape, (None, 2, 3))\n+\n     def test_hard_tanh(self):\n         x = KerasTensor([None, 2, 3])\n         self.assertEqual(knn.hard_tanh(x).shape, (None, 2, 3))\n@@ -810,6 +814,10 @@ class NNOpsStaticShapeTest(testing.TestCase):\n         x = KerasTensor([1, 2, 3])\n         self.assertEqual(knn.glu(x).shape, (1, 2, 3))\n \n+    def test_tanh_shrink(self):\n+        x = KerasTensor([1, 2, 3])\n+        self.assertEqual(knn.tanh_shrink(x).shape, (1, 2, 3))\n+\n     def test_hard_tanh(self):\n         x = KerasTensor([1, 2, 3])\n         self.assertEqual(knn.hard_tanh(x).shape, (1, 2, 3))\n@@ -1338,6 +1346,13 @@ class NNOpsCorrectnessTest(testing.TestCase):\n             [-0.8807971, 0.0, 0.98201376],\n         )\n \n+    def test_tanh_shrink(self):\n+        x = np.array([-1, 0, 1, 2, 3], dtype=np.float32)\n+        self.assertAllClose(\n+            knn.tanh_shrink(x),\n+            [-0.238406, 0.0, 0.238406, 1.035972, 2.004945],\n+        )\n+\n     def test_hard_tanh(self):\n         x = np.array([-1, 0, 1, 2, 3], dtype=np.float32)\n         self.assertAllClose(\n@@ -2441,6 +2456,24 @@ class NNOpsDtypeTest(testing.TestCase):\n             expected_dtype,\n         )\n \n+    @parameterized.named_parameters(named_product(dtype=FLOAT_DTYPES))\n+    def test_tanh_shrink(self, dtype):\n+        import torch\n+        import torch.nn.functional as tnn\n+\n+        x = knp.ones((1), dtype=dtype)\n+        x_torch = torch.ones(1, dtype=getattr(torch, dtype))\n+        expected_dtype = standardize_dtype(tnn.tanhshrink(x_torch).dtype)\n+\n+        self.assertEqual(\n+            standardize_dtype(knn.tanh_shrink(x).dtype),\n+            expected_dtype,\n+        )\n+        self.assertEqual(\n+            standardize_dtype(knn.TanhShrink().symbolic_call(x).dtype),\n+            expected_dtype,\n+        )\n+\n     @parameterized.named_parameters(named_product(dtype=FLOAT_DTYPES))\n     def test_hard_tanh(self, dtype):\n         import jax.nn as jnn\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
