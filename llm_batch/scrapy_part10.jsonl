{"custom_id": "scrapy#30803b9e89427f70fae5e5d809ddc8940b33984a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 91 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 92 | Churn Cumulative: 381 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,5 @@\n import unittest\n-from scrapy.utils.url import url_is_from_any_domain, safe_url_string, safe_download_url, url_query_parameter, add_or_replace_parameter, url_query_cleaner\n+from scrapy.utils.url import url_is_from_any_domain, safe_url_string, safe_download_url, url_query_parameter, add_or_replace_parameter, url_query_cleaner, canonicalize_url\n \n class UrlUtilsTest(unittest.TestCase):\n \n@@ -74,6 +74,73 @@ class UrlUtilsTest(unittest.TestCase):\n         self.assertEqual(url_query_cleaner(\"product.html?id=200&foo=bar&name=wired\", ['id', 'name']),\n                          'product.html?id=200&name=wired')\n \n+    def test_canonicalize_url(self):\n+        # no query arguments\n+        self.assertEqual(canonicalize_url(\"http://www.example.com\"),\n+                         \"http://www.example.com\")\n+\n+        # typical usage\n+        self.assertEqual(canonicalize_url(\"http://www.example.com/do?a=1&b=2&c=3\"),\n+                                          \"http://www.example.com/do?a=1&b=2&c=3\")\n+        self.assertEqual(canonicalize_url(\"http://www.example.com/do?c=1&b=2&a=3\"),\n+                                          \"http://www.example.com/do?a=3&b=2&c=1\")\n+        self.assertEqual(canonicalize_url(\"http://www.example.com/do?&a=1\"),\n+                                          \"http://www.example.com/do?a=1\")\n+        \n+        # sorting by argument values\n+        self.assertEqual(canonicalize_url(\"http://www.example.com/do?c=3&b=5&b=2&a=50\"),\n+                         \"http://www.example.com/do?a=50&b=2&b=5&c=3\")\n+\n+        # using keep_blank_values\n+        self.assertEqual(canonicalize_url(\"http://www.example.com/do?b=&a=2\"),\n+                                          \"http://www.example.com/do?a=2\")\n+        self.assertEqual(canonicalize_url(\"http://www.example.com/do?b=&a=2\", keep_blank_values=True),\n+                                          \"http://www.example.com/do?a=2&b=\")\n+        self.assertEqual(canonicalize_url(\"http://www.example.com/do?b=&c&a=2\"),\n+                                          \"http://www.example.com/do?a=2\")\n+        self.assertEqual(canonicalize_url(\"http://www.example.com/do?b=&c&a=2\", keep_blank_values=True),\n+                                          \"http://www.example.com/do?a=2&b=&c=\")\n+                        \n+        # spaces\n+        self.assertEqual(canonicalize_url(\"http://www.example.com/do?q=a space&a=1\"),\n+                                          \"http://www.example.com/do?a=1&q=a+space\")\n+        self.assertEqual(canonicalize_url(\"http://www.example.com/do?q=a+space&a=1\"),\n+                                          \"http://www.example.com/do?a=1&q=a+space\")\n+        self.assertEqual(canonicalize_url(\"http://www.example.com/do?q=a%20space&a=1\"),\n+                                          \"http://www.example.com/do?a=1&q=a+space\")\n+\n+        # normalize percent-encoding case (in paths)\n+        self.assertEqual(canonicalize_url(\"http://www.example.com/a%a3do\"),\n+                                          \"http://www.example.com/a%A3do\"),\n+        # normalize percent-encoding case (in query arguments)\n+        self.assertEqual(canonicalize_url(\"http://www.example.com/do?k=b%a3\"),\n+                                          \"http://www.example.com/do?k=b%A3\")\n+\n+        # non-ASCII percent-encoding in paths\n+        self.assertEqual(canonicalize_url(\"http://www.example.com/a do?a=1\"),\n+                                          \"http://www.example.com/a%20do?a=1\"),\n+        self.assertEqual(canonicalize_url(\"http://www.example.com/a %20do?a=1\"),\n+                                          \"http://www.example.com/a%20%20do?a=1\"),\n+        self.assertEqual(canonicalize_url(\"http://www.example.com/a do\\xc2\\xa3.html?a=1\"),\n+                                          \"http://www.example.com/a%20do%C2%A3.html?a=1\")\n+        # non-ASCII percent-encoding in query arguments\n+        self.assertEqual(canonicalize_url(u\"http://www.example.com/do?price=\\xa3500&a=5&z=3\"),\n+                                          u\"http://www.example.com/do?a=5&price=%C2%A3500&z=3\")\n+        self.assertEqual(canonicalize_url(\"http://www.example.com/do?price=\\xc2\\xa3500&a=5&z=3\"),\n+                                          \"http://www.example.com/do?a=5&price=%C2%A3500&z=3\")\n+        self.assertEqual(canonicalize_url(\"http://www.example.com/do?price(\\xc2\\xa3)=500&a=1\"),\n+                                          \"http://www.example.com/do?a=1&price%28%C2%A3%29=500\")\n+\n+        # urls containing auth and ports\n+        self.assertEqual(canonicalize_url(u\"http://user:pass@www.example.com:81/do?now=1\"),\n+                                          u\"http://user:pass@www.example.com:81/do?now=1\")\n+\n+        # remove fragments\n+        self.assertEqual(canonicalize_url(u\"http://user:pass@www.example.com/do?a=1#frag\"),\n+                                          u\"http://user:pass@www.example.com/do?a=1\")\n+        self.assertEqual(canonicalize_url(u\"http://user:pass@www.example.com/do?a=1#frag\", remove_fragments=False),\n+                                          u\"http://user:pass@www.example.com/do?a=1#frag\")\n+\n if __name__ == \"__main__\":\n     unittest.main()\n \n\n@@ -124,3 +124,26 @@ def add_or_replace_parameter(url, name, new_value, sep='&'):\n         next_url = url.replace(name+'='+parameter,\n                                name+'='+new_value)\n     return next_url\n+\n+def canonicalize_url(url, keep_blank_values=False, remove_fragments=True):\n+    \"\"\"Canonicalize url by applying the following procedures:\n+\n+    - sort query arguments, first by key, then by value\n+    - percent encode paths and query arguments. non-ASCII characters are\n+      percent-encoded using UTF-8 (RFC-3986)\n+    - normalize all spaces (in query arguments) '+' (plus symbol)\n+    - normalize percent encodings case (%2f -> %2F)\n+    - remove query arguments with blank values (unless keep_blank_values is True)\n+    - remove fragments (if remove_fragments is True)\n+\n+    For examples see the tests in scrapy.tests.test_utils_url\n+    \"\"\"\n+\n+    parts = list(urlparse.urlparse(url))\n+    keyvals = cgi.parse_qsl(parts[4], keep_blank_values)\n+    keyvals.sort()\n+    parts[2] = urllib.quote(urllib.unquote(parts[2]))\n+    parts[4] = urllib.urlencode(keyvals)\n+    if remove_fragments:\n+        parts[5] = ''\n+    return urlparse.urlunparse(parts)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#29c3715c5a11d93d9019d6792b92692e685e4f4f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 389 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -138,7 +138,7 @@ class UrlUtilsTest(unittest.TestCase):\n         # remove fragments\n         self.assertEqual(canonicalize_url(u\"http://user:pass@www.example.com/do?a=1#frag\"),\n                                           u\"http://user:pass@www.example.com/do?a=1\")\n-        self.assertEqual(canonicalize_url(u\"http://user:pass@www.example.com/do?a=1#frag\", remove_fragments=False),\n+        self.assertEqual(canonicalize_url(u\"http://user:pass@www.example.com/do?a=1#frag\", keep_fragments=True),\n                                           u\"http://user:pass@www.example.com/do?a=1#frag\")\n \n if __name__ == \"__main__\":\n\n@@ -125,7 +125,7 @@ def add_or_replace_parameter(url, name, new_value, sep='&'):\n                                name+'='+new_value)\n     return next_url\n \n-def canonicalize_url(url, keep_blank_values=False, remove_fragments=True):\n+def canonicalize_url(url, keep_blank_values=False, keep_fragments=False):\n     \"\"\"Canonicalize url by applying the following procedures:\n \n     - sort query arguments, first by key, then by value\n@@ -134,7 +134,7 @@ def canonicalize_url(url, keep_blank_values=False, remove_fragments=True):\n     - normalize all spaces (in query arguments) '+' (plus symbol)\n     - normalize percent encodings case (%2f -> %2F)\n     - remove query arguments with blank values (unless keep_blank_values is True)\n-    - remove fragments (if remove_fragments is True)\n+    - remove fragments (unless keep_fragments is True)\n \n     For examples see the tests in scrapy.tests.test_utils_url\n     \"\"\"\n@@ -144,6 +144,6 @@ def canonicalize_url(url, keep_blank_values=False, remove_fragments=True):\n     keyvals.sort()\n     parts[2] = urllib.quote(urllib.unquote(parts[2]))\n     parts[4] = urllib.urlencode(keyvals)\n-    if remove_fragments:\n+    if not keep_fragments:\n         parts[5] = ''\n     return urlparse.urlunparse(parts)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#96d24c7640c5fbef8a32c6f3f5872b1e8476474e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 242 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -42,7 +42,7 @@ _safe_chars = urllib.always_safe + '%' + _reserved\n def safe_url_string(url, use_encoding='utf8'):\n     \"\"\"Convert a unicode (or utf8 string) object into a legal URL.\n \n-    Illegal characters are escaped.  See rfc3968.\n+    Illegal characters are escaped (RFC-3986)\n \n     It is safe to call this function multiple times. Do not pass this\n     function strings in encodings other than utf8.\n@@ -50,9 +50,6 @@ def safe_url_string(url, use_encoding='utf8'):\n     The use_encoding argument is the encoding to use to determine the numerical\n     values in the escaping. For urls on html pages, you should use the original\n     encoding of that page.\n-\n-    html pages you should escape urls in the original encoding\n-    of the page and not using utf8.\n     \"\"\"\n     s = url.encode(use_encoding)\n     return urllib.quote(s,  _safe_chars)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d7d94482a95ccf895c9069b44f1403fdd36344b1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 202 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -69,9 +69,10 @@ class Request(object):\n         return self.deferred\n \n     def set_url(self, url):\n-        assert isinstance(url, basestring), 'Request url argument must be str or unicode, got %s:' % (type(url), url)\n+        assert isinstance(url, basestring), 'Request url argument must be str or unicode, got %s:' % type(url).__name__\n         decoded_url = url if isinstance(url, unicode) else url.decode(self.encoding)\n         self._url = Url(safe_url_string(decoded_url, self.encoding))\n+        self._fingerprint = None # invalidate cached fingerprint\n     url = property(lambda x: x._url, set_url)\n \n     def httpauth(self, http_user, http_pass):\n@@ -123,10 +124,15 @@ class Request(object):\n         return new\n \n     def fingerprint(self):\n-        \"\"\"Returns unique resource fingerprint\"\"\"\n-        if self._fingerprint and not self.fingerprint_params:\n+        \"\"\"Returns unique resource fingerprint with caching support\"\"\"\n+        if not self._fingerprint or self.fingerprint_params:\n+            self.update_fingerprint()\n         return self._fingerprint\n \n+    def update_fingerprint(self):\n+        \"\"\"Update request fingerprint, based on its current data. A request\n+        fingerprint is a hash which uniquely identifies the HTTP resource\"\"\"\n+\n         headers = {}\n         if self.fingerprint_params:\n             if 'tamperfunc' in self.fingerprint_params:\n@@ -162,7 +168,6 @@ class Request(object):\n                 fp.update(v)\n \n         self._fingerprint = fp.hexdigest()\n-        return self._fingerprint\n \n     def to_string(self):\n         \"\"\" Return raw HTTP request representation (as string). This is\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c9cafd5c43a0446ba122772ab5986971a1600132", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 17 | Churn Cumulative: 17 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,17 @@\n+\"\"\"\n+UrlFilterMiddleware: canonicalizes URLs to filter out duplicated ones\n+\"\"\"\n+\n+from scrapy.http import Request\n+from scrapy.utils.url import canonicalize_url\n+\n+class UrlFilterMiddleware(object):\n+    def process_result(self, response, result, spider):\n+        disabled = getattr(spider, 'urlfilter_disabled', False)\n+        for r in result:\n+            if isinstance(r, Request) and not disabled:\n+                curl = canonicalize_url(r.url)\n+                # only assign if different to avoid re-calculating fingerprint\n+                if curl != r.url: \n+                    r.url = curl\n+            yield r\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#037e6c2125936c0cd8d0759e82850bbb69ed0f1d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 8 | Files Changed: 5 | Hunks: 8 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 29 | Churn Cumulative: 160 | Contributors (this commit): 3 | Commits (past 90d): 18 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -1,12 +1,14 @@\n \"\"\"\n-Limits the scheduler request queue from the point of view of the spider.\n-That is, if the scheduler queue contains an equal or greater ammount of\n-requests than the specified limit, the further requests generated by the\n-spider will be ignored.\n+RequestLimitMiddleware: Limits the scheduler request queue from the point of\n+view of the spider. That is, if the scheduler queue contains an equal or\n+greater ammount of requests than the specified limit, the new requests\n+(generated by the spider) will be ignored.\n+\n The limit is setted from the spider attribute \"requests_queue_size\". If not\n found, from the scrapy setting \"REQUESTS_QUEUE_SIZE\". If not found, no limit\n will be applied. If given a value of 0, no limit will be applied.\n \"\"\"\n+\n from scrapy.core.engine import scrapyengine\n from scrapy.conf import settings\n from scrapy.http import Request\n\n@@ -1,3 +1,8 @@\n+\"\"\"\n+OffsiteMiddleware: Filters out Requests for URLs outside the domains covered by\n+the spider.\n+\"\"\"\n+\n from scrapy.core import log\n from scrapy.http import Request\n from scrapy.utils.url import url_is_from_spider\n\n@@ -1,7 +1,11 @@\n+\"\"\"\n+RefererMiddleware: populates Request referer field, based on the Response which\n+originated it.\n+\"\"\"\n+\n from scrapy.core import log\n from scrapy.http import Request\n \n-\n class CrawlMiddleware(object):\n     def process_result(self, response, result, spider):\n         def _set_referer(r):\n\n@@ -1,5 +1,5 @@\n \"\"\"\n-Scrapemiddlware to restrict crawling to only some particular URLs\n+RestrictMiddleware: restricts crawling to fixed set of particular URLs\n \"\"\"\n \n from scrapy.http import Request\n\n@@ -1,11 +1,13 @@\n+\"\"\"\n+UrlLengthMiddleware: Filters out requests with URLs longer than URLLENGTH_LIMIT\n+\"\"\"\n+\n from scrapy.core import log\n from scrapy.http import Request\n from scrapy.core.exceptions import NotConfigured\n from scrapy.conf import settings\n \n class UrlLengthMiddleware(object):\n-    \"\"\"This middleware discard requests with URLs longer than URLLENGTH_LIMIT\"\"\"\n-\n     def __init__(self):\n         self.maxlength = settings.getint('URLLENGTH_LIMIT')\n         if not self.maxlength:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a88fb416c7888a1383f6508d9ddd29ecca8b9f34", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 22 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -6,7 +6,7 @@ originated it.\n from scrapy.core import log\n from scrapy.http import Request\n \n-class CrawlMiddleware(object):\n+class RefererMiddleware(object):\n     def process_result(self, response, result, spider):\n         def _set_referer(r):\n             if isinstance(r, Request):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#41fa98801c01d13b0617922534bd00b42bdca10c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 8 | Churn Cumulative: 81 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,10 +1,6 @@\n import re\n \n-class DuplicatedAdaptorName(Exception):\n-    def __init__(self, name):\n-        self.__name = name\n-    def __str__(self):\n-        return name\n+class DuplicatedAdaptorName(Exception): pass\n \n class Adaptor(object):\n     \"\"\"\n@@ -21,6 +17,7 @@ class Adaptor(object):\n         return self.basefunction(value, **pipeargs)\n         \n class AdaptorPipe:\n+\n     def __init__(self, define_from=None, adaptorclass=None):\n         \"\"\"\n         If \"define_from\" is given, constructs pipeline from this.\n@@ -35,6 +32,7 @@ class AdaptorPipe:\n         \"\"\"\n         self.__adaptorspipe = []\n         self.__adaptorclass = adaptorclass or Adaptor\n+\n     @property\n     def adaptors_names(self):\n         _adaptors = []\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#472a0de139bddf9d6b61fb8f6832a9f321a56e24", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 21 | Files Changed: 2 | Hunks: 11 | Methods Changed: 8 | Complexity Δ (Sum/Max): 2/5 | Churn Δ: 44 | Churn Cumulative: 210 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -7,12 +7,12 @@ class Adaptor(object):\n     Adaptors instances should be instantiated and used only\n     inside the AdaptorPipe.\n     \"\"\"\n-    def __init__(self, function, name, attribute_re=None):\n+    def __init__(self, function, name, attribute_re=None, attribute_list=None):\n         self.name = name\n         self.basefunction = function\n-        if not attribute_re:\n-            attribute_re = \".*\"\n+        attribute_re = attribute_re or \".*\"\n         self.attribute_re = re.compile(attribute_re)\n+        self.attribute_list = attribute_list or []\n     def function(self, value, **pipeargs):\n         return self.basefunction(value, **pipeargs)\n         \n@@ -40,17 +40,24 @@ class AdaptorPipe:\n             _adaptors.append(a.name)\n         return _adaptors\n     \n-    def insertadaptor(self, function, name, attrs_re=None, after=None, before=None):\n+    def insertadaptor(self, function, name, attrs_re=None, attrs_list=None, after=None, before=None):\n         \"\"\"\n         Inserts a \"function\" as an adaptor that will apply for attribute names\n-        which matches \"attrs_re\" regex, after adaptor of name \"after\", or before\n-        adaptor of name \"before\". The \"function\" must always have a **keyword\n+        which matches regex given in \"attrs_re\" (None matches all), or are included in \"attrs_list\" list.\n+        The match logic is as follows:\n+        The attribute name will match if:\n+        1. The attrs_re matches and attrs_list is empty\n+        2. The attrs_re matches and attribute name is in attrs_list\n+\n+        If \"after\" is given, inserts the adaptor after the already inserted adaptor\n+        of the name given in this parameter, If \"before\" is given, inserts it before\n+        the adaptor of the given name. The \"function\" must always have a **keyword\n         argument to ignore unused keywords. \"name\" is the name of the adaptor.\n         \"\"\"\n         if name in self.adaptors_names:\n             raise DuplicatedAdaptorName(name)\n         else:\n-            adaptor = self.__adaptorclass(function, name, attrs_re)\n+            adaptor = self.__adaptorclass(function, name, attrs_re, attrs_list)\n             #by default append adaptor at end of pipe\n             pos = len(self.adaptors_names)\n             if after:\n@@ -66,6 +73,12 @@ class AdaptorPipe:\n         Pass the given pipeargs to each adaptor function in the pipe.\n         \"\"\"\n         for adaptor in self.__adaptorspipe:\n-            if adaptor.attribute_re.search(attrname):\n+            adapt = False\n+            if adaptor.attribute_re.search(attrname) and not adaptor.attribute_list:\n+                adapt = True\n+            elif adaptor.attribute_re.search(attrname) and attrname in adaptor.attribute_list:\n+                adapt = True\n+            if adapt:\n                 value = adaptor.function(value, **pipeargs)\n+\n         return value\n\n@@ -1,13 +1,5 @@\n from scrapy.item.adaptors import AdaptorPipe\n \n-def extract(value):\n-    if hasattr(value, 'extract'):\n-        value = value.extract()\n-    return value\n-    \n-standardpipe = AdaptorPipe()\n-standardpipe.insertadaptor(extract, \"extract\")\n-\n class ScrapedItem(object):\n     \"\"\"\n     This is the base class for all scraped items.\n@@ -16,12 +8,9 @@ class ScrapedItem(object):\n     * guid (unique global indentifier)\n     * url (URL where that item was scraped from)\n     \"\"\"\n-    adaptors_pipe = standardpipe\n-    \n-    def set_adaptors_pipe(adaptors_pipes):\n-        ScrapedItem.adaptors_pipes = adaptors_pipes\n+    adaptors_pipe = AdaptorPipe()\n \n     def attribute(self, attrname, value, **pipeargs):\n-        value =ScrapedItem.adaptors_pipe.execute(attrname, value, **pipeargs)\n+        value = self.adaptors_pipe.execute(attrname, value, **pipeargs)\n         if not hasattr(self, attrname):\n             setattr(self, attrname, value)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2a30073ece92333943c8301215566aa9bd69fe98", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 32 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -8,6 +8,10 @@ from scrapy.utils.misc import load_class\n from scrapy.extension import extensions\n from scrapy.conf import settings\n from scrapy.core.manager import scrapymanager\n+from scrapy.http import Request, Response\n+from scrapy.core.downloader.handlers import download_any\n+from scrapy.fetcher import get_or_create_spider\n+from scrapy.utils.url import canonicalize_url\n \n #This code comes from twisted 8. We define here while\n #using old twisted version.\n@@ -68,15 +72,13 @@ class Command(ScrapyCommand):\n             #print _failure\n \n         print \"Downloading URL...           \",\n-        from scrapy.http import Request, Response\n-        from scrapy.core.downloader.handlers import download_any\n-        from scrapy.fetcher import get_or_create_spider\n+        url = canonicalize_url(url.strip())\n         r = Request(url)\n         spider = get_or_create_spider(url)\n         try:\n             result = blockingCallFromThread(reactor, download_any, r, spider)\n             if isinstance(result, Response):\n-                print \"OK\"\n+                print \"Done.\"\n                 self.generate_vars(url, result)\n                 return True\n         except Exception, e:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#972896cd87f4773362daee8d39ffe4504d9ddcb2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 34 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -72,7 +72,7 @@ class Command(ScrapyCommand):\n             #print _failure\n \n         print \"Downloading URL...           \",\n-        url = canonicalize_url(url.strip())\n+        url = url.strip()\n         r = Request(url)\n         spider = get_or_create_spider(url)\n         try:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f54ba9f7e90027ba43d7446ac0b30263de14d5ee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 35 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -11,7 +11,6 @@ from scrapy.core.manager import scrapymanager\n from scrapy.http import Request, Response\n from scrapy.core.downloader.handlers import download_any\n from scrapy.fetcher import get_or_create_spider\n-from scrapy.utils.url import canonicalize_url\n \n #This code comes from twisted 8. We define here while\n #using old twisted version.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6b16ee5e674afc6a534cceefd79c9f4831384bf3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 227 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -112,7 +112,7 @@ def memoize(cache, hash):\n \n \n def deferred_degenerate(generator, container=None, next_delay=0):\n-    generator = iter(generator)\n+    generator = iter(generator or [])\n     deferred = defer.Deferred()\n     container = container or []\n     def _next():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#82a9fa9ffc538243f6b165c69c557ac39312a71c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 19 | Churn Cumulative: 129 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -10,8 +10,7 @@ class Adaptor(object):\n     def __init__(self, function, name, attribute_re=None, attribute_list=None):\n         self.name = name\n         self.basefunction = function\n-        attribute_re = attribute_re or \".*\"\n-        self.attribute_re = re.compile(attribute_re)\n+        self.attribute_re = re.compile(attribute_re) if attribute_re else None\n         self.attribute_list = attribute_list or []\n     def function(self, value, **pipeargs):\n         return self.basefunction(value, **pipeargs)\n@@ -44,10 +43,8 @@ class AdaptorPipe:\n         \"\"\"\n         Inserts a \"function\" as an adaptor that will apply for attribute names\n         which matches regex given in \"attrs_re\" (None matches all), or are included in \"attrs_list\" list.\n-        The match logic is as follows:\n-        The attribute name will match if:\n-        1. The attrs_re matches and attrs_list is empty\n-        2. The attrs_re matches and attribute name is in attrs_list\n+        If both, attrs_re and attrs_list are given, apply both. Else if only one is given, apply those.\n+        Else, all attributes will match.\n         \n         If \"after\" is given, inserts the adaptor after the already inserted adaptor\n         of the name given in this parameter, If \"before\" is given, inserts it before\n@@ -74,9 +71,15 @@ class AdaptorPipe:\n         \"\"\"\n         for adaptor in self.__adaptorspipe:\n             adapt = False\n-            if adaptor.attribute_re.search(attrname) and not adaptor.attribute_list:\n+            if adaptor.attribute_re:\n+                if adaptor.attribute_re.search(attrname):\n+                    if adaptor.attribute_list:\n+                        adapt = attrname in adaptor.attribute_list\n+                    else:\n                         adapt = True\n-            elif adaptor.attribute_re.search(attrname) and attrname in adaptor.attribute_list:\n+            elif adaptor.attribute_list:\n+                adapt = attrname in adaptor.attribute_list\n+            else:\n                 adapt = True\n             if adapt:\n                 value = adaptor.function(value, **pipeargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1fa947dd678e02e449c033df6b8c2d125fa7311b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 17 | Churn Cumulative: 146 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -17,20 +17,23 @@ class Adaptor(object):\n         \n class AdaptorPipe:\n \n-    def __init__(self, define_from=None, adaptorclass=None):\n+    def __init__(self, adaptors=None, adaptorclass=None):\n         \"\"\"\n-        If \"define_from\" is given, constructs pipeline from this.\n-        \"define_from\" is an ordered tuple of triplets, each of which\n-        has the attribute name regex, the adaptor name, and the module\n-        path to the adaptor function. Example:\n+        If \"adaptors\" is given, constructs pipeline from this.\n+        \"define_from\" is an ordered tuple of 4-elements tuples, each of which\n+        has the same parameters you give to the insertadaptor method, except \n+        'after' and 'before', because you define the adaptors order in the tuple.\n+        . Example:\n         (\n-          (\"url\", \"remove_entities\", \"scrapy.utils.markup.remove_entities\")\n-          (\".*\", \"replace_tags\", \"scrapy.utils.markup.replace_tags\")\n+          (\"my_function\", \"my_function\", None, \"name\")\n           ...\n         )\n         \"\"\"\n         self.__adaptorspipe = []\n         self.__adaptorclass = adaptorclass or Adaptor\n+        if adaptors:\n+            for entry in adaptors:\n+                self.insertadaptor(*entry)\n \n     @property\n     def adaptors_names(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3cb1ab87945bf6b9f9e5f76b40c8cbb8c8479c27", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 196 | Lines Deleted: 0 | Files Changed: 4 | Hunks: 4 | Methods Changed: 18 | Complexity Δ (Sum/Max): 35/16 | Churn Δ: 196 | Churn Cumulative: 196 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,74 @@\n+\"\"\"\n+Here put the rules that the rules engine / pipline will take to process it.\n+\"\"\"\n+from scrapy.core.exceptions import NotConfigured\n+from scrapy.utils.misc import load_class\n+from scrapy.core import log \n+from scrapy.conf import settings\n+\n+class Rule(object):\n+    \"\"\"\n+    Interface of the Rules. \n+    Implement this class to create a new rule.\n+    \"\"\"\n+    def __init__(self, wresponse=None):\n+        self.__responsewrapper = wresponse\n+    \n+    def __getresponsewrapper(self):\n+        return self.__responsewrapper\n+    def __setresponsewrapper(self, wresponse):\n+        self.__responsewrapper = wresponse\n+\n+    responsewrapper = property(__getresponsewrapper, __setresponsewrapper)\n+    \n+    def check(self):\n+        result = 0.0\n+        if self.responsewrapper:\n+            result = self.holds()\n+            if result < 0 or result > 1:\n+                raise ValueError, \"Value must be between 0 and 1.\"\n+        return result\n+\n+    def holds(self):\n+        \"\"\"\n+        User of this class must override this method.\n+        Put here the conditions that must be satisfied by the rule.\n+        The return value must be a number between 0.0 and 1.0.\n+        \"\"\"\n+        return 0.0\n+\n+class RulesManager(object):\n+    \"\"\"\n+    This class contains the RulesManager  which takes care of loading and\n+    keeping track of all enabled rules. It also contains an instantiated\n+    RulesManager (rules) to be used as singleton.\n+    The RulesManager contains the rules classes, not instances of the rules \n+    classes, this approach give us more flexiblility in our Rules Engine.\n+    \"\"\"\n+\n+    def __init__(self):\n+        self.loaded = False\n+        self.enabled = {}\n+\n+    def load(self):\n+        \"\"\"\n+        Load enabled extensions in settings module\n+        \"\"\"\n+        \n+        self.loaded = False\n+        self.enabled.clear()\n+        \n+        for extension_path in settings.getlist('SIMPAGES_RULES'):\n+            cls = load_class(extension_path)\n+            self.enabled[cls.__name__] = cls\n+                \n+        self.loaded = True\n+    \n+    def reload(self):\n+        self.load()\n+\n+rules = RulesManager()\n+\n+\n+\n+\n\n@@ -0,0 +1,9 @@\n+\"\"\"\n+Contains exceptions of the simpages stuffs\n+\"\"\"\n+\n+class RulesNotLoaded(Exception):\n+    \"\"\"\n+    Indicate that the rules was not loaded.\n+    \"\"\"\n+    pass\n\n@@ -0,0 +1,54 @@\n+\"\"\"\n+Represent the pipepline of execution to the rules engine.\n+\"\"\"\n+from scrapy.core import log\n+from scrapy.contrib.rulengine.exceptions import RulesNotLoaded\n+\n+from scrapy.contrib.rulengine import rules\n+\n+class RulesPipeline(object):\n+\n+    rulesLoaded = []\n+    loaded = False\n+    \n+    def __init__(self, wresponse):\n+        \"\"\"\n+        wresponse: is a response wrapper object that contain the response.\n+        \"\"\"\n+        self.__rules = []\n+        self._responsewrapper = wresponse\n+    \n+    @staticmethod\n+    def loadRules():\n+        RulesPipeline.loaded = True\n+        rules.load()\n+        try:\n+            for rulename in rules.enabled.keys():\n+                ldr_msg = 'Loading ... %s' % rulename\n+                ruleClass = rules.enabled[rulename]\n+                RulesPipeline.rulesLoaded.append(ruleClass())\n+                log.msg(ldr_msg)\n+                print ldr_msg\n+        except Exception, e:\n+            RulesPipeline.loaded = False\n+            RulesPipeline.rulesLoaded = []\n+            log.msg(e)\n+            print e\n+\n+    def execute(self):\n+        \"\"\"\n+        Return a dictionary that conatins all the rules executed.\n+        \"\"\"\n+        if RulesPipeline.loaded:\n+            rules_loaded = RulesPipeline.rulesLoaded\n+            info_dict = {}\n+            info_dict['rules_executed'] = {}\n+            total = 0.0\n+            for rule in rules_loaded:\n+                rule.responsewrapper = self._responsewrapper\n+                rule_result = rule.check()\n+                total += rule_result\n+                info_dict['rules_executed'][rule.__class__.__name__] = rule_result\n+            return info_dict\n+        else:\n+            raise RulesNotLoaded, 'Problems loading the rules.'\n\n@@ -0,0 +1,59 @@\n+\"\"\"\n+This class serve as wrapper of the response object. \n+The object created by this class are not response, but are objects that contains a response a a lot of useful information about the response.\n+\"\"\"\n+\n+import re\n+\n+from BeautifulSoup import Tag\n+\n+def extractText(soup):\n+    text_in_tags = soup.findAll(text=True)\n+    res = []\n+    for tag in text_in_tags:\n+        if isinstance(tag.parent, Tag) and tag.parent.name not in('script'):\n+            res.append(tag)\n+    return res\n+\n+class ResponseWrapper(object):\n+    def __init__(self, response):\n+        self._response = response        \n+        self.__soup_bodytext = extractText(response.soup.body) if response else None\n+        self.__soup_headtext = extractText(response.soup.head) if response else None        \n+        self._soup = None\n+        self._bodytext = None\n+        self._headtext = None\n+        self._cleanbodytext = None\n+    \n+    def __cleantext(self, souptext):\n+        return filter(lambda x:re.search('\\w+', x), souptext)\n+\n+    @property\n+    def soup(self):        \n+        if (self._soup):\n+            self._soup = self._response.soup\n+        return self._response.soup\n+\n+    @property\n+    def bodytext(self):\n+        if not self._bodytext:\n+            self._bodytext = self.__cleantext(self.__soup_bodytext)\n+        return self._bodytext\n+\n+    @property\n+    def headtext(self):\n+        if not self._headtext:\n+            self._headtext = self.__cleantext(self.__soup_headtext)\n+        return self._headtext\n+\n+    @property\n+    def cleanbodytext(self):\n+        if not self._cleanbodytext:\n+            text = ' '.join(self.bodytext)\n+            text = text.lower()\n+            text = text.replace('\\n', '')\n+            text = text.replace('\\t', '')\n+            text = re.sub('&.*?;', '', text)\n+            self._cleanbodytext = text\n+        return self._cleanbodytext\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f4f9626c3f9578adc09bfcb64933aa62813aa985", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 56 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -43,11 +43,9 @@ class RulesPipeline(object):\n             rules_loaded = RulesPipeline.rulesLoaded\n             info_dict = {}\n             info_dict['rules_executed'] = {}\n-            total = 0.0\n             for rule in rules_loaded:\n                 rule.responsewrapper = self._responsewrapper\n                 rule_result = rule.check()\n-                total += rule_result\n                 info_dict['rules_executed'][rule.__class__.__name__] = rule_result\n             return info_dict\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#daf51203e3a6d89b039d5270abc522848611cc96", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 372 | Contributors (this commit): 2 | Commits (past 90d): 11 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -61,7 +61,7 @@ class ClusterMasterWeb(ClusterMaster):\n             else:\n                 sep = \"\\r\"\n                 domains = args[\"schedule\"]\n-            priority = eval(args.get(\"priority\", [\"PRIORITY_NORMAL\"])[0])\n+            priority = int(args.get(\"priority\", [\"PRIORITY_NORMAL\"])[0])\n             \n             #spider settings\n             slist = args.get(\"settings\", [\"\"])[0].split(sep)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#51bdd6944b610aaa6431cb5cae1d988748ecac87", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 68 | Files Changed: 2 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): -10/3 | Churn Δ: 82 | Churn Cumulative: 481 | Contributors (this commit): 2 | Commits (past 90d): 16 | Contributors (cumulative): 3 | DMM Complexity: 0.4782608695652174\n\nDIFF:\n@@ -1,67 +0,0 @@\n-\"\"\"\n-Generic Adaptors for item adaptation pipe. Performs tasks\n-usually needed by most cases\n-\"\"\"\n-\n-import re\n-\n-from scrapy.item.models import BaseAdaptor\n-from scrapy.utils.python import flatten, unique\n-from scrapy.xpath import XPathSelector\n-from scrapy.utils.misc import unquote_html, location_str\n-from scrapy.conf import settings\n-\n-class ExtendedAdaptor(BaseAdaptor):\n-\n-    def do(self, function, a, *args, **kwargs):\n-        \"\"\"Executes an adaptor printing a lot of useful debugging info.\"\"\"\n-    \n-        debug = kwargs.get('debug')\n-        fname = function.__name__\n-    \n-        if debug:\n-            print repr(kwargs)\n-            print \"  %07s | input >\" % fname, location_str(a)\n-        a = function(a, *args, **kwargs)\n-        if debug:\n-            print \"  %07s | output>\" % fname, a\n-    \n-        return a\n-        \n-class ExtractAdaptor(ExtendedAdaptor):\n-\n-    def function(self, location, **pipeargs):\n-        return self.do(self.extract, location, **pipeargs)\n-    \n-    def extract(self, location, **kwargs):\n-        \"\"\"Extract a list of strings from the location passed.\n-        Receives a list of XPathSelectors or an XPathSelector,\n-        or a list of strings or a string.\n-        \n-        Return a list of strings extracted.\n-\n-        This function *always* returns a list.\n-        \"\"\"\n-        if not location:\n-            return []\n-        if isinstance(location, (list, tuple)):\n-            strings = flatten([self.extract(o, **kwargs) for o in location])\n-            if kwargs.get('remove_dupes', False):\n-                strings = unique(strings)\n-            if kwargs.get('first', False):\n-                return strings[:1]\n-            else:\n-                return strings\n-        # XPathSelector\n-        elif isinstance(location, XPathSelector):\n-            strings = [location.extract()]\n-        # Strings\n-        elif isinstance(location, unicode):\n-            strings = [unquote_html(location, keep_reserved=True)]\n-        elif isinstance(location, str):\n-            encoding = kwargs.get(\"encoding\", settings.get('DEFAULT_DATA_ENCODING'))\n-            strings = [unquote_html(unicode(location, encoding), keep_reserved=True)]\n-        else:\n-            raise TypeError, \"unsupported location type: %s\" % type(location)\n-\n-        return strings\n\\ No newline at end of file\n\n@@ -1,4 +1,6 @@\n import re\n+from traceback import format_exc\n+from scrapy.core import log\n \n class DuplicatedAdaptorName(Exception): pass\n \n@@ -67,7 +69,7 @@ class AdaptorPipe:\n             self.__adaptorspipe.insert(pos, adaptor)\n             return pos\n \n-    def execute(self, attrname, value, **pipeargs):\n+    def execute(self, attrname, value, debug=False, **pipeargs):\n         \"\"\"\n         Execute pipeline for attribute name \"attrname\" and value \"value\".\n         Pass the given pipeargs to each adaptor function in the pipe.\n@@ -85,6 +87,17 @@ class AdaptorPipe:\n             else:\n                 adapt = True\n             if adapt:\n+                try:\n+                    if debug:\n+                        print \"pipeargs: %s\" % repr(pipeargs)\n+                        print \"  %07s | input >\" % adaptor.name, repr(value)\n                     value = adaptor.function(value, **pipeargs)\n+                    if debug:\n+                        print \"  %07s | output>\" % adaptor.name, repr(value)\n+\n+                except Exception, e:\n+                    print \"Error in '%s' adaptor. Traceback text:\" % adaptor.name\n+                    print format_exc()\n+                    return\n                     \n         return value\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4e61f057619493c8d1021e70b954531fa08c93d9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 719 | Contributors (this commit): 2 | Commits (past 90d): 26 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -203,7 +203,7 @@ class ClusterMaster:\n             d.addCallbacks(callback=lambda obj: self.add_node(obj, _name), errback=_errback)\n \n         server, port = url.split(\":\")\n-        port = eval(port)\n+        port = int(port)\n         log.msg(\"Connecting to cluster worker %s...\" % name)\n         log.msg(\"Server: %s, Port: %s\" % (server, port))\n         factory = ScrapyPBClientFactory(self, name)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c2af3124bac3f3f92e98e42cd83f28a2b4c54331", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 10 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 21 | Churn Cumulative: 182 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: 0.5\n\nDIFF:\n@@ -9,11 +9,12 @@ class Adaptor(object):\n     Adaptors instances should be instantiated and used only\n     inside the AdaptorPipe.\n     \"\"\"\n-    def __init__(self, function, name, attribute_re=None, attribute_list=None):\n+    def __init__(self, function, name, attribute_re=None, attribute_list=None, negative_match=False):\n         self.name = name\n         self.basefunction = function\n         self.attribute_re = re.compile(attribute_re) if attribute_re else None\n         self.attribute_list = attribute_list or []\n+        self.negative_match = negative_match\n     def function(self, value, **pipeargs):\n         return self.basefunction(value, **pipeargs)\n         \n@@ -44,13 +45,14 @@ class AdaptorPipe:\n             _adaptors.append(a.name)\n         return _adaptors\n     \n-    def insertadaptor(self, function, name, attrs_re=None, attrs_list=None, after=None, before=None):\n+    def insertadaptor(self, function, name, attrs_re=None, attrs_list=None, negative_match=False, after=None, before=None):\n         \"\"\"\n         Inserts a \"function\" as an adaptor that will apply for attribute names\n         which matches regex given in \"attrs_re\" (None matches all), or are included in \"attrs_list\" list.\n         If both, attrs_re and attrs_list are given, apply both. Else if only one is given, apply those.\n         Else, all attributes will match.\n-        \n+        If negative_match is True, the adaptor will be applied only if there is no match. By default,\n+        negative_match is False (i.e. match -> apply)\n         If \"after\" is given, inserts the adaptor after the already inserted adaptor\n         of the name given in this parameter, If \"before\" is given, inserts it before\n         the adaptor of the given name. The \"function\" must always have a **keyword\n@@ -59,7 +61,7 @@ class AdaptorPipe:\n         if name in self.adaptors_names:\n             raise DuplicatedAdaptorName(name)\n         else:\n-            adaptor = self.__adaptorclass(function, name, attrs_re, attrs_list)\n+            adaptor = self.__adaptorclass(function, name, attrs_re, attrs_list, negative_match)\n             #by default append adaptor at end of pipe\n             pos = len(self.adaptors_names)\n             if after:\n@@ -75,17 +77,18 @@ class AdaptorPipe:\n         Pass the given pipeargs to each adaptor function in the pipe.\n         \"\"\"\n         for adaptor in self.__adaptorspipe:\n-            adapt = False\n+            match = False\n             if adaptor.attribute_re:\n                 if adaptor.attribute_re.search(attrname):\n                     if adaptor.attribute_list:\n-                        adapt = attrname in adaptor.attribute_list\n+                        match = attrname in adaptor.attribute_list\n                     else:\n-                        adapt = True\n+                        match = True\n             elif adaptor.attribute_list:\n-                adapt = attrname in adaptor.attribute_list\n+                match = attrname in adaptor.attribute_list\n             else:\n-                adapt = True\n+                match = True\n+            adapt = match ^ adaptor.negative_match\n             if adapt:\n                 try:\n                     if debug:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
