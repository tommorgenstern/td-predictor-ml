{"custom_id": "scrapy#18d303b5f1815ce8d0a41ba04b56cfbe25c9616e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 27 | Files Changed: 22 | Hunks: 44 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 63 | Churn Cumulative: 3510 | Contributors (this commit): 7 | Commits (past 90d): 26 | Contributors (cumulative): 50 | DMM Complexity: None\n\nDIFF:\n@@ -1,10 +1,9 @@\n-import sys\n+from w3lib.url import is_url\n \n from scrapy import log\n from scrapy.command import ScrapyCommand\n from scrapy.conf import settings\n from scrapy.http import Request\n-from scrapy.utils.url import is_url\n from scrapy.utils.conf import arglist_to_dict\n from scrapy.exceptions import UsageError\n \n\n@@ -11,10 +11,11 @@ import netrc\n from urlparse import urlparse, urljoin\n from subprocess import Popen, PIPE, check_call\n \n+from w3lib.form import encode_multipart\n+\n from scrapy.command import ScrapyCommand\n from scrapy.exceptions import UsageError\n from scrapy.utils.py26 import json\n-from scrapy.utils.multipart import encode_multipart\n from scrapy.utils.http import basic_auth_header\n from scrapy.utils.conf import get_config, closest_scrapy_cfg\n \n\n@@ -1,10 +1,11 @@\n import pprint\n \n+from w3lib.url import is_url\n+\n from scrapy import log\n from scrapy.command import ScrapyCommand\n from scrapy.http import Request\n from scrapy.spider import BaseSpider\n-from scrapy.utils.url import is_url\n from scrapy.exceptions import UsageError\n \n class Command(ScrapyCommand):\n\n@@ -1,9 +1,9 @@\n+from w3lib.url import is_url\n from scrapy.command import ScrapyCommand\n from scrapy.http import Request\n from scrapy.item import BaseItem\n from scrapy.utils import display\n from scrapy.utils.spider import iterate_spider_output, create_spider_for_request\n-from scrapy.utils.url import is_url\n from scrapy.exceptions import UsageError\n from scrapy import log\n \n\n@@ -4,7 +4,7 @@ HTTP basic auth downloader middleware\n See documentation in docs/topics/downloader-middleware.rst\n \"\"\"\n \n-from scrapy.utils.http import basic_auth_header\n+from w3lib.http import basic_auth_header\n from scrapy.utils.python import WeakKeyCache\n \n \n\n@@ -5,13 +5,14 @@ from os.path import join, exists\n from time import time\n import cPickle as pickle\n \n+from w3lib.http import headers_dict_to_raw, headers_raw_to_dict\n+\n from scrapy.xlib.pydispatch import dispatcher\n from scrapy import signals\n from scrapy.http import Headers\n from scrapy.exceptions import NotConfigured, IgnoreRequest\n from scrapy.core.downloader.responsetypes import responsetypes\n from scrapy.utils.request import request_fingerprint\n-from scrapy.utils.http import headers_dict_to_raw, headers_raw_to_dict\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.misc import load_object\n from scrapy.utils.project import data_path\n\n@@ -1,6 +1,7 @@\n+from w3lib.url import urljoin_rfc\n+\n from scrapy import log\n from scrapy.http import HtmlResponse\n-from scrapy.utils.url import urljoin_rfc\n from scrapy.utils.response import get_meta_refresh\n from scrapy.exceptions import IgnoreRequest\n from scrapy.conf import settings\n\n@@ -12,14 +12,14 @@ from ftplib import FTP\n from shutil import copyfileobj\n \n from zope.interface import Interface, implements\n-\n from twisted.internet import defer, threads\n+from w3lib.url import file_uri_to_path\n+\n from scrapy import log, signals\n from scrapy.xlib.pydispatch import dispatcher\n from scrapy.utils.ftp import ftp_makedirs_cwd\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.misc import load_object\n-from scrapy.utils.url import file_uri_to_path\n from scrapy.conf import settings\n \n \n\n@@ -4,9 +4,10 @@ HTMLParser-based link extractor\n \n from HTMLParser import HTMLParser\n \n+from w3lib.url import safe_url_string, urljoin_rfc\n+\n from scrapy.link import Link\n from scrapy.utils.python import unique as unique_list\n-from scrapy.utils.url import safe_url_string, urljoin_rfc\n \n class HtmlParserLinkExtractor(HTMLParser):\n \n\n@@ -3,9 +3,9 @@ This module implements the HtmlImageLinkExtractor for extracting\n image links only.\n \"\"\"\n \n-\n+from w3lib.url import urljoin_rfc\n from scrapy.link import Link\n-from scrapy.utils.url import canonicalize_url, urljoin_rfc\n+from scrapy.utils.url import canonicalize_url\n from scrapy.utils.python import unicode_to_str, flatten\n from scrapy.selector.libxml2sel import XPathSelectorList, HtmlXPathSelector\n \n\n@@ -7,10 +7,10 @@ because it collides with the lxml library module.\n \n from lxml import etree\n import lxml.html\n+from w3lib.url import safe_url_string, urljoin_rfc\n \n from scrapy.link import Link\n from scrapy.utils.python import unique as unique_list, str_to_unicode\n-from scrapy.utils.url import safe_url_string, urljoin_rfc\n \n class LxmlLinkExtractor(object):\n     def __init__(self, tag=\"a\", attr=\"href\", process=None, unique=False):\n\n@@ -1,7 +1,7 @@\n import re\n \n-from scrapy.utils.url import urljoin_rfc\n-from scrapy.utils.markup import remove_tags, remove_entities, replace_escape_chars\n+from w3lib.url import urljoin_rfc\n+from w3lib.html import remove_tags, remove_entities, replace_escape_chars\n \n from scrapy.link import Link\n from .sgml import SgmlLinkExtractor\n\n@@ -4,11 +4,13 @@ SGMLParser-based Link extractors\n \n import re\n \n+from w3lib.url import safe_url_string, urljoin_rfc\n+\n from scrapy.selector import HtmlXPathSelector\n from scrapy.link import Link\n from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.python import FixedSGMLParser, unique as unique_list, str_to_unicode\n-from scrapy.utils.url import safe_url_string, urljoin_rfc, canonicalize_url, url_is_from_any_domain\n+from scrapy.utils.url import canonicalize_url, url_is_from_any_domain\n from scrapy.utils.response import get_base_url\n \n class BaseSgmlLinkExtractor(FixedSGMLParser):\n\n@@ -1,9 +1,10 @@\n \"\"\"Request Extractors\"\"\"\n+from w3lib.url import safe_url_string, urljoin_rfc\n+\n from scrapy.http import Request\n from scrapy.selector import HtmlXPathSelector\n from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.python import FixedSGMLParser, str_to_unicode\n-from scrapy.utils.url import safe_url_string, urljoin_rfc\n \n from itertools import ifilter\n \n\n@@ -1,5 +1,5 @@\n+from w3lib.url import file_uri_to_path\n from scrapy.core.downloader.responsetypes import responsetypes\n-from scrapy.utils.url import file_uri_to_path\n from scrapy.utils.decorator import defers\n \n class FileDownloadHandler(object):\n\n@@ -1,5 +1,5 @@\n+from w3lib.http import headers_dict_to_raw\n from scrapy.utils.datatypes import CaselessDict\n-from scrapy.utils.http import headers_dict_to_raw\n \n \n class Headers(CaselessDict):\n\n@@ -7,8 +7,9 @@ See documentation in docs/topics/request-response.rst\n \n import copy\n \n+from w3lib.url import safe_url_string\n+\n from scrapy.http.headers import Headers\n-from scrapy.utils.url import safe_url_string\n from scrapy.utils.trackref import object_ref\n from scrapy.utils.decorator import deprecated\n from scrapy.http.common import deprecated_setter\n\n@@ -7,6 +7,7 @@ See documentation in docs/topics/shell.rst\n import signal\n \n from twisted.internet import reactor, threads\n+from w3lib.url import any_to_uri\n \n from scrapy.item import BaseItem\n from scrapy.spider import BaseSpider\n@@ -14,7 +15,6 @@ from scrapy.selector import XPathSelector, XmlXPathSelector, HtmlXPathSelector\n from scrapy.utils.spider import create_spider_for_request\n from scrapy.utils.misc import load_object\n from scrapy.utils.response import open_in_browser\n-from scrapy.utils.url import any_to_uri\n from scrapy.utils.console import start_python_console\n from scrapy.settings import Settings\n from scrapy.http import Request, Response, HtmlResponse, XmlResponse\n\n@@ -1,13 +1,13 @@\n import os, urlparse\n+from cStringIO import StringIO\n \n from zope.interface.verify import verifyObject\n from twisted.trial import unittest\n from twisted.internet import defer\n-from cStringIO import StringIO\n+from w3lib.url import path_to_file_uri\n \n from scrapy.spider import BaseSpider\n from scrapy.contrib.feedexport import IFeedStorage, FileFeedStorage, FTPFeedStorage, S3FeedStorage, StdoutFeedStorage\n-from scrapy.utils.url import path_to_file_uri\n from scrapy.utils.test import assert_aws_environ\n \n class FeedStorageTest(unittest.TestCase):\n\n@@ -8,6 +8,7 @@ from twisted.web import server, static, util, resource\n from twisted.web.test.test_webclient import ForeverTakingResource, \\\n         NoLengthResource, HostHeaderResource, \\\n         PayloadResource, BrokenDownloadResource\n+from w3lib.url import path_to_file_uri\n \n from scrapy.core.downloader.webclient import PartialDownloadError\n from scrapy.core.downloader.handlers.file import FileDownloadHandler\n@@ -15,7 +16,6 @@ from scrapy.core.downloader.handlers.http import HttpDownloadHandler\n from scrapy.core.downloader.handlers.s3 import S3DownloadHandler\n from scrapy.spider import BaseSpider\n from scrapy.http import Request\n-from scrapy.utils.url import path_to_file_uri\n from scrapy import optional_features\n \n \n\n@@ -4,8 +4,8 @@ import re\n import hashlib\n from pkgutil import iter_modules\n \n+from w3lib.html import remove_entities\n from scrapy.utils.python import flatten\n-from scrapy.utils.markup import remove_entities\n \n def arg_to_iter(arg):\n     \"\"\"Convert an argument to an iterable. The argument can be a None, single\n\n@@ -8,9 +8,10 @@ import weakref\n from base64 import urlsafe_b64encode\n from urlparse import urlunparse\n \n+from w3lib.http import basic_auth_header\n+\n from scrapy.utils.url import canonicalize_url\n from scrapy.utils.httpobj import urlparse_cached\n-from scrapy.utils.http import basic_auth_header\n \n \n _fingerprint_cache = weakref.WeakKeyDictionary()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bf73002428408778ee71e511e9282925c7180b02", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 80 | Files Changed: 6 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): -6/0 | Churn Δ: 80 | Churn Cumulative: 197 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 14 | DMM Complexity: 0.0\n\nDIFF:\n\n@@ -1,10 +0,0 @@\n-from scrapy.item import Item, Field\n-\n-class GoogledirItem(Item):\n-\n-    name = Field()\n-    url = Field()\n-    description = Field()\n-\n-    def __str__(self):\n-        return \"Google Category: name=%s url=%s\" % (self['name'], self['url'])\n\n@@ -1,15 +0,0 @@\n-from scrapy.exceptions import DropItem\n-\n-class FilterWordsPipeline(object):\n-    \"\"\"A pipeline for filtering out items which contain certain words in their\n-    description\"\"\"\n-\n-    # put all words in lowercase\n-    words_to_filter = ['politics', 'religion']\n-\n-    def process_item(self, item, spider):\n-        for word in self.words_to_filter:\n-            if word in unicode(item['description']).lower():\n-                raise DropItem(\"Contains forbidden word: %s\" % word)\n-        else:\n-            return item\n\n@@ -1,14 +0,0 @@\n-# - Scrapy settings for googledir project\n-\n-import googledir\n-\n-BOT_NAME = 'googledir'\n-BOT_VERSION = '1.0'\n-\n-SPIDER_MODULES = ['googledir.spiders']\n-NEWSPIDER_MODULE = 'googledir.spiders'\n-DEFAULT_ITEM_CLASS = 'scrapy.item.Item'\n-USER_AGENT = '%s/%s' % (BOT_NAME, BOT_VERSION)\n-\n-ITEM_PIPELINES = ['googledir.pipelines.FilterWordsPipeline']\n-\n\n@@ -1 +0,0 @@\n-# Place here all your scrapy spiders\n\n@@ -1,40 +0,0 @@\n-from scrapy.selector import HtmlXPathSelector\n-from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\n-from scrapy.contrib.spiders import CrawlSpider, Rule\n-from scrapy.contrib.loader import XPathItemLoader\n-from googledir.items import GoogledirItem\n-\n-class GoogleDirectorySpider(CrawlSpider):\n-\n-    name = 'directory.google.com'\n-    allowed_domains = ['directory.google.com']\n-    start_urls = ['http://directory.google.com/']\n-\n-    rules = (\n-        Rule(SgmlLinkExtractor(allow='directory.google.com/[A-Z][a-zA-Z_/]+$'),\n-            'parse_category',\n-            follow=True,\n-        ),\n-    )\n-    \n-    def parse_category(self, response):\n-        # The main selector we're using to extract data from the page\n-        main_selector = HtmlXPathSelector(response)\n-\n-        # The XPath to website links in the directory page\n-        xpath = '//td[descendant::a[contains(@href, \"#pagerank\")]]/following-sibling::td/font'\n-\n-        # Get a list of (sub) selectors to each website node pointed by the XPath\n-        sub_selectors = main_selector.select(xpath)\n-\n-        # Iterate over the sub-selectors to extract data for each website\n-        for selector in sub_selectors:\n-            item = GoogledirItem()\n-\n-            l = XPathItemLoader(item=item, selector=selector)\n-            l.add_xpath('name', 'a/text()')\n-            l.add_xpath('url', 'a/@href')\n-            l.add_xpath('description', 'font[2]/text()')\n-\n-            # Here we populate the item and yield it\n-            yield l.load_item()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cf572bb6423691d9976b1113df1f174fc93ae5de", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 284 | Files Changed: 12 | Hunks: 12 | Methods Changed: 8 | Complexity Δ (Sum/Max): -14/0 | Churn Δ: 284 | Churn Cumulative: 783 | Contributors (this commit): 4 | Commits (past 90d): 12 | Contributors (cumulative): 32 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1 +0,0 @@\n-# googledir project\n\n@@ -1,16 +0,0 @@\n-# Define here the models for your scraped items\n-#\n-# See documentation in:\n-# http://doc.scrapy.org/topics/items.html\n-\n-from scrapy.item import Item, Field\n-\n-class GoogledirItem(Item):\n-\n-    name = Field(default='')\n-    url = Field(default='')\n-    description = Field(default='')\n-\n-    def __str__(self):\n-        return \"Google Category: name=%s url=%s\" \\\n-                    % (self['name'], self['url'])\n\n@@ -1,22 +0,0 @@\n-# Define your item pipelines here\n-#\n-# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n-# See: http://doc.scrapy.org/topics/item-pipeline.html\n-\n-from scrapy.exceptions import DropItem\n-\n-class FilterWordsPipeline(object):\n-    \"\"\"\n-    A pipeline for filtering out items which contain certain \n-    words in their description\n-    \"\"\" \n-\n-    # put all words in lowercase\n-    words_to_filter = ['politics', 'religion']\n-\n-    def process_item(self, item, spider):\n-        for word in self.words_to_filter:\n-            if word in unicode(item['description']).lower():\n-                raise DropItem(\"Contains forbidden word: %s\" % word)\n-        else:\n-            return item\n\n@@ -1,16 +0,0 @@\n-# Scrapy settings for googledir project\n-#\n-# For simplicity, this file contains only the most important settings by\n-# default. All the other settings are documented here:\n-#\n-#     http://doc.scrapy.org/topics/settings.html\n-\n-BOT_NAME = 'googledir'\n-BOT_VERSION = '1.0'\n-\n-SPIDER_MODULES = ['googledir.spiders']\n-NEWSPIDER_MODULE = 'googledir.spiders'\n-DEFAULT_ITEM_CLASS = 'googledir.items.GoogledirItem'\n-USER_AGENT = '%s/%s' % (BOT_NAME, BOT_VERSION)\n-\n-ITEM_PIPELINES = ['googledir.pipelines.FilterWordsPipeline']\n\n@@ -1,8 +0,0 @@\n-# This package will contain the spiders of your Scrapy project\n-#\n-# To create the first spider for your project use this command:\n-#\n-#   scrapy genspider myspider myspider-domain.com\n-#\n-# For more info see:\n-# http://doc.scrapy.org/topics/spiders.html\n\n@@ -1,38 +0,0 @@\n-from scrapy.selector import HtmlXPathSelector\n-from scrapy.contrib.loader import XPathItemLoader\n-from scrapy.contrib_exp.crawlspider import CrawlSpider, Rule\n-\n-from googledir.items import GoogledirItem\n-\n-class GoogleDirectorySpider(CrawlSpider):\n-\n-    name = 'google_directory'\n-    allowed_domains = ['directory.google.com']\n-    start_urls = ['http://directory.google.com/']\n-\n-    rules = (\n-        # search for categories pattern and follow links\n-        Rule(r'/[A-Z][a-zA-Z_/]+$', 'parse_category', follow=True),\n-        )\n-\n-    def parse_category(self, response):\n-        # The main selector we're using to extract data from the page\n-        main_selector = HtmlXPathSelector(response)\n-\n-        # The XPath to website links in the directory page\n-        xpath = '//td[descendant::a[contains(@href, \"#pagerank\")]]/following-sibling::td/font'\n-\n-        # Get a list of (sub) selectors to each website node pointed by the XPath\n-        sub_selectors = main_selector.select(xpath)\n-\n-        # Iterate over the sub-selectors to extract data for each website\n-        for selector in sub_selectors:\n-            item = GoogledirItem()\n-\n-            l = XPathItemLoader(item=item, selector=selector)\n-            l.add_xpath('name', 'a/text()')\n-            l.add_xpath('url', 'a/@href')\n-            l.add_xpath('description', 'font[2]/text()')\n-\n-            # Here we populate the item and yield it\n-            yield l.load_item()\n\n@@ -1 +0,0 @@\n-# package\n\n@@ -1,12 +0,0 @@\n-# Define here the models for your scraped items\n-#\n-# See documentation in:\n-# http://doc.scrapy.org/topics/items.html\n-\n-from scrapy.item import Item, Field\n-\n-class ImdbItem(Item):\n-    # define the fields for your item here like:\n-    # name = Field()\n-    title = Field()\n-    url = Field()\n\n@@ -1,8 +0,0 @@\n-# Define your item pipelines here\n-#\n-# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n-# See: http://doc.scrapy.org/topics/item-pipeline.html\n-\n-class ImdbPipeline(object):\n-    def process_item(self, item, spider):\n-        return item\n\n@@ -1,15 +0,0 @@\n-# Scrapy settings for imdb project\n-#\n-# For simplicity, this file contains only the most important settings by\n-# default. All the other settings are documented here:\n-#\n-#     http://doc.scrapy.org/topics/settings.html\n-\n-BOT_NAME = 'imdb'\n-BOT_VERSION = '1.0'\n-\n-SPIDER_MODULES = ['imdb.spiders']\n-NEWSPIDER_MODULE = 'imdb.spiders'\n-DEFAULT_ITEM_CLASS = 'imdb.items.ImdbItem'\n-USER_AGENT = '%s/%s' % (BOT_NAME, BOT_VERSION)\n-\n\n@@ -1,8 +0,0 @@\n-# This package will contain the spiders of your Scrapy project\n-#\n-# To create the first spider for your project use this command:\n-#\n-#   scrapy genspider myspider myspider-domain.com\n-#\n-# For more info see:\n-# http://doc.scrapy.org/topics/spiders.html\n\n@@ -1,139 +0,0 @@\n-from scrapy.http import Request\n-from scrapy.selector import HtmlXPathSelector\n-from scrapy.contrib.loader import XPathItemLoader\n-from scrapy.contrib_exp.crawlspider import CrawlSpider, Rule\n-from scrapy.contrib_exp.crawlspider.reqext import SgmlRequestExtractor\n-from scrapy.contrib_exp.crawlspider.reqproc import Canonicalize, \\\n-        FilterDupes, FilterUrl\n-from scrapy.utils.url import urljoin_rfc\n-\n-from imdb.items import ImdbItem, Field\n-\n-from itertools import chain, imap, izip\n-\n-class UsaOpeningWeekMovie(ImdbItem):\n-    pass\n-\n-class UsaTopWeekMovie(ImdbItem):\n-    pass\n-\n-class Top250Movie(ImdbItem):\n-    rank = Field()\n-    rating = Field()\n-    year = Field()\n-    votes = Field()\n-\n-class MovieItem(ImdbItem):\n-    release_date = Field()\n-    tagline = Field()\n-\n-\n-class ImdbSiteSpider(CrawlSpider):\n-    name = 'imdb.com'\n-    allowed_domains = ['imdb.com']\n-    start_urls = ['http://www.imdb.com/']\n-\n-    # extract requests using this classes from urls matching 'follow' flag\n-    request_extractors = [\n-        SgmlRequestExtractor(tags=['a'], attrs=['href']),\n-        ]\n-\n-    # process requests using this classes from urls matching 'follow' flag\n-    request_processors = [\n-        Canonicalize(),\n-        FilterDupes(),\n-        FilterUrl(deny=r'/tt\\d+/$'), # deny movie url as we will dispatch\n-                                     # manually the movie requests\n-        ]\n-\n-    # include domain bit for demo purposes\n-    rules = (\n-        # these two rules expects requests from start url\n-        Rule(r'imdb.com/nowplaying/$', 'parse_now_playing'),\n-        Rule(r'imdb.com/chart/top$', 'parse_top_250'),\n-        # this rule will parse requests manually dispatched\n-        Rule(r'imdb.com/title/tt\\d+/$', 'parse_movie_info'),\n-    )\n-\n-    def parse_now_playing(self, response):\n-        \"\"\"Scrapes USA openings this week and top 10 in week\"\"\"\n-        self.log(\"Parsing USA Top Week\")\n-        hxs = HtmlXPathSelector(response)\n-\n-        _urljoin = lambda url: self._urljoin(response, url)\n-\n-        #\n-        # openings this week\n-        #\n-        openings = hxs.select('//table[@class=\"movies\"]//a[@class=\"title\"]')\n-        boxoffice = hxs.select('//table[@class=\"boxoffice movies\"]//a[@class=\"title\"]')\n-\n-        opening_titles = openings.select('text()').extract()\n-        opening_urls = imap(_urljoin, openings.select('@href').extract())\n-\n-        box_titles = boxoffice.select('text()').extract()\n-        box_urls = imap(_urljoin, boxoffice.select('@href').extract())\n-\n-        # items \n-        opening_items = (UsaOpeningWeekMovie(title=title, url=url)\n-                            for (title, url)\n-                            in izip(opening_titles, opening_urls))\n-\n-        box_items = (UsaTopWeekMovie(title=title, url=url) \n-                        for (title, url)\n-                        in izip(box_titles, box_urls))\n-\n-        # movie requests\n-        requests = imap(self.make_requests_from_url,\n-                        chain(opening_urls, box_urls))\n-\n-        return chain(opening_items, box_items, requests)\n-\n-    def parse_top_250(self, response):\n-        \"\"\"Scrapes movies from top 250 list\"\"\"\n-        self.log(\"Parsing Top 250\")\n-        hxs = HtmlXPathSelector(response)\n-\n-        # scrap each row in the table\n-        rows = hxs.select('//div[@id=\"main\"]/table/tr//a/ancestor::tr')\n-        for row in rows:\n-            fields = row.select('td//text()').extract()\n-            url, = row.select('td//a/@href').extract()\n-            url = self._urljoin(response, url)\n-\n-            item = Top250Movie()\n-            item['title'] = fields[2]\n-            item['url'] = url\n-            item['rank'] = fields[0]\n-            item['rating'] = fields[1]\n-            item['year'] = fields[3]\n-            item['votes'] = fields[4]\n-\n-            # scrapped top250 item\n-            yield item\n-            # fetch movie\n-            yield self.make_requests_from_url(url)\n-\n-    def parse_movie_info(self, response):\n-        \"\"\"Scrapes movie information\"\"\"\n-        self.log(\"Parsing Movie Info\")\n-        hxs = HtmlXPathSelector(response)\n-        selector = hxs.select('//div[@class=\"maindetails\"]')\n-\n-        item = MovieItem()\n-        # set url\n-        item['url'] = response.url\n-\n-        # use item loader for other attributes\n-        l = XPathItemLoader(item=item, selector=selector)\n-        l.add_xpath('title', './/h1/text()')\n-        l.add_xpath('release_date', './/h5[text()=\"Release Date:\"]'\n-                                    '/following-sibling::div/text()')\n-        l.add_xpath('tagline', './/h5[text()=\"Tagline:\"]'\n-                               '/following-sibling::div/text()')\n-\n-        yield l.load_item()\n-\n-    def _urljoin(self, response, url):\n-        \"\"\"Helper to convert relative urls to absolute\"\"\"\n-        return urljoin_rfc(response.url, url, response.encoding)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7f97259ba7536946185c9d79bd4c0a9557e15eab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 152 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -120,7 +120,7 @@ setup_args = {\n \n try:\n     from setuptools import setup\n-    setup_args['install_requires'] = ['Twisted>=2.5', 'lxml', 'w3lib']\n+    setup_args['install_requires'] = ['Twisted>=2.5', 'lxml', 'w3lib', 'pyOpenSSL']\n     if sys.version_info < (2, 6):\n         setup_args['install_requires'] += ['simplejson']\n except ImportError:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#afa23688c6d625d21cf008eed778f485a5e3e838", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 28 | Churn Cumulative: 38 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -19,12 +19,10 @@ class Headers(CaselessDict):\n \n     def normvalue(self, value):\n         \"\"\"Headers must not be unicode\"\"\"\n-        if isinstance(value, unicode):\n-            value = value.encode(self.encoding)\n-\n-        if isinstance(value, (list, tuple)):\n-            return list(value)\n-        return [value]\n+        if not hasattr(value, '__iter__'):\n+            value = [value]\n+        return [x.encode(self.encoding) if isinstance(x, unicode) else x \\\n+            for x in value]\n \n     def __getitem__(self, key):\n         try:\n\n@@ -1,5 +1,4 @@\n import unittest\n-import weakref\n import copy\n \n from scrapy.http import Headers\n@@ -34,6 +33,23 @@ class HeadersTest(unittest.TestCase):\n         self.assertEqual(h.getlist('X-Forwarded-For'), hlist)\n         assert h.getlist('X-Forwarded-For') is not hlist\n \n+    def test_encode_utf8(self):\n+        h = Headers({u'key': u'\\xa3'}, encoding='utf-8')\n+        key, val = dict(h).items()[0]\n+        assert isinstance(key, str), key\n+        assert isinstance(val[0], str), val[0]\n+        self.assertEqual(val[0], '\\xc2\\xa3')\n+\n+    def test_encode_latin1(self):\n+        h = Headers({u'key': u'\\xa3'}, encoding='latin1')\n+        key, val = dict(h).items()[0]\n+        self.assertEqual(val[0], '\\xa3')\n+\n+    def test_encode_multiple(self):\n+        h = Headers({u'key': [u'\\xa3']}, encoding='utf-8')\n+        key, val = dict(h).items()[0]\n+        self.assertEqual(val[0], '\\xc2\\xa3')\n+\n     def test_delete_and_contains(self):\n         h = Headers()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bac46ba438a1dac5cabcaba8ed085a475ff406b6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 523 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -25,7 +25,7 @@ class Request(object_ref):\n                  dont_filter=False, errback=None):\n \n         self._encoding = encoding  # this one has to be set first\n-        self.method = method.upper()\n+        self.method = str(method).upper()\n         self._set_url(url)\n         self._set_body(body)\n         self.priority = priority\n\n@@ -162,6 +162,10 @@ class RequestTest(unittest.TestCase):\n         self.assertEqual(r4.meta, {})\n         assert r4.dont_filter is False\n \n+    def test_method_always_str(self):\n+        r = self.request_class(\"http://www.example.com\", method=u\"POST\")\n+        assert isinstance(r.method, str)\n+\n     def test_weakref_slots(self):\n         \"\"\"Check that classes are using slots and are weak-referenceable\"\"\"\n         x = self.request_class('http://www.example.com')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7e62a0a1a1eb745460eadf57fdea13a2d392b10b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 29 | Files Changed: 2 | Hunks: 9 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/2 | Churn Δ: 52 | Churn Cumulative: 474 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,39 +10,26 @@ from twisted.python.failure import Failure\n \n from scrapy.exceptions import IgnoreRequest\n from scrapy.conf import settings\n+from scrapy.utils.python import setattr_default\n from scrapy.utils.defer import mustbe_deferred\n from scrapy.utils.signal import send_catch_log\n-from scrapy.utils import deprecate\n from scrapy import signals\n from scrapy import log\n from .middleware import DownloaderMiddlewareManager\n from .handlers import DownloadHandlers\n \n-\n class SpiderInfo(object):\n     \"\"\"Simple class to keep information and state for each open spider\"\"\"\n \n     def __init__(self, spider):\n-        if hasattr(spider, 'download_delay'):\n-            deprecate.attribute(spider, 'download_delay', 'DOWNLOAD_DELAY')\n-            self._download_delay = spider.download_delay\n-        else:\n-            self._download_delay = spider.settings.getfloat('DOWNLOAD_DELAY')\n-        if self._download_delay:\n-            self.max_concurrent_requests = 1\n-        else:\n-            if hasattr(spider, 'max_concurrent_requests'):\n-                deprecate.attribute(spider, 'max_concurrent_requests', 'CONCURRENT_REQUESTS_PER_SPIDER')\n-                self.max_concurrent_requests = spider.max_concurrent_requests\n-            else:\n-                self.max_concurrent_requests = spider.settings.getint('CONCURRENT_REQUESTS_PER_SPIDER')\n-        if self._download_delay and spider.settings.getbool('RANDOMIZE_DOWNLOAD_DELAY'):\n-            # same policy as wget --random-wait\n-            self.random_delay_interval = (0.5*self._download_delay, \\\n-                1.5*self._download_delay)\n-        else:\n-            self.random_delay_interval = None\n-\n+        setattr_default(spider, 'download_delay', spider.settings.getfloat('DOWNLOAD_DELAY'))\n+        setattr_default(spider, 'randomize_download_delay', spider.settings.getbool('RANDOMIZE_DOWNLOAD_DELAY'))\n+        setattr_default(spider, 'max_concurrent_requests', spider.settings.getint('CONCURRENT_REQUESTS_PER_SPIDER'))\n+        if spider.download_delay > 0 and spider.max_concurrent_requests > 1:\n+            spider.max_concurrent_requests = 1\n+            msg = \"Setting max_concurrent_requests=1 because of download_delay=%s\" % spider.download_delay\n+            log.msg(msg, spider=spider)\n+        self.spider = spider\n         self.active = set()\n         self.queue = []\n         self.transferring = set()\n@@ -51,17 +38,17 @@ class SpiderInfo(object):\n         self.next_request_calls = set()\n \n     def free_transfer_slots(self):\n-        return self.max_concurrent_requests - len(self.transferring)\n+        return self.spider.max_concurrent_requests - len(self.transferring)\n \n     def needs_backout(self):\n         # use self.active to include requests in the downloader middleware\n-        return len(self.active) > 2 * self.max_concurrent_requests\n+        return len(self.active) > 2 * self.spider.max_concurrent_requests\n \n     def download_delay(self):\n-        if self.random_delay_interval:\n-            return random.uniform(*self.random_delay_interval)\n-        else:\n-            return self._download_delay\n+        delay = self.spider.download_delay\n+        if self.spider.randomize_download_delay:\n+            delay = random.uniform(0.5*delay, 1.5*delay)\n+        return delay\n \n     def cancel_request_calls(self):\n         for call in self.next_request_calls:\n@@ -130,7 +117,7 @@ class Downloader(object):\n         delay = site.download_delay()\n         if delay:\n             penalty = delay - now + site.lastseen\n-            if penalty > 0:\n+            if penalty > 0 and site.free_transfer_slots():\n                 d = defer.Deferred()\n                 d.addCallback(self._process_queue)\n                 call = reactor.callLater(penalty, d.callback, spider)\n\n@@ -215,3 +215,10 @@ def is_writable(path):\n         return os.access(path, os.W_OK)\n     else:\n         return os.access(os.path.dirname(path), os.W_OK)\n+\n+def setattr_default(obj, name, value):\n+    \"\"\"Set attribute value, but only if it's not already set. Similar to\n+    setdefault() for dicts.\n+    \"\"\"\n+    if not hasattr(obj, name):\n+        setattr(obj, name, value)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#079de6771904edabbd75209a61d5277bbe6460bd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 273 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -4,6 +4,7 @@ Download web pages using asynchronous IO\n \n import random\n from time import time\n+from collections import deque\n \n from twisted.internet import reactor, defer\n from twisted.python.failure import Failure\n@@ -31,7 +32,7 @@ class SpiderInfo(object):\n             log.msg(msg, spider=spider)\n         self.spider = spider\n         self.active = set()\n-        self.queue = []\n+        self.queue = deque()\n         self.transferring = set()\n         self.closing = False\n         self.lastseen = 0\n@@ -128,7 +129,7 @@ class Downloader(object):\n \n         # Process enqueued requests if there are free slots to transfer for this site\n         while site.queue and site.free_transfer_slots() > 0:\n-            request, deferred = site.queue.pop(0)\n+            request, deferred = site.queue.popleft()\n             if site.closing:\n                 dfd = defer.fail(Failure(IgnoreRequest()))\n             else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f9aa819b06870d77704244036ab950eb753c59c8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 603 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -1,6 +1,8 @@\n \"\"\"This module implements the Scraper component which parses responses and\n extracts information from them\"\"\"\n \n+from collections import deque\n+\n from twisted.python.failure import Failure\n from twisted.internet import defer\n \n@@ -24,7 +26,7 @@ class SpiderInfo(object):\n \n     def __init__(self, max_active_size=5000000):\n         self.max_active_size = max_active_size\n-        self.queue = []\n+        self.queue = deque()\n         self.active = set()\n         self.active_size = 0\n         self.itemproc_size = 0\n@@ -40,7 +42,7 @@ class SpiderInfo(object):\n         return deferred\n \n     def next_response_request_deferred(self):\n-        response, request, deferred = self.queue.pop(0)\n+        response, request, deferred = self.queue.popleft()\n         self.active.add(response)\n         return response, request, deferred\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#668dfcabf368722fe6088f36481c265bd860062c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1796 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -82,8 +82,6 @@ class Downloader(object):\n \n         site.active.add(request)\n         def _deactivate(response):\n-            send_catch_log(signal=signals.response_received, \\\n-                response=response, request=request, spider=spider)\n             site.active.remove(request)\n             self._close_if_idle(spider)\n             return response\n\n@@ -157,6 +157,8 @@ class ExecutionEngine(object):\n                 response.request = request # tie request to response received\n                 log.msg(log.formatter.crawled(request, response, spider), \\\n                     level=log.DEBUG, spider=spider)\n+                send_catch_log(signal=signals.response_received, \\\n+                    response=response, request=request, spider=spider)\n                 return response\n             elif isinstance(response, Request):\n                 return mustbe_deferred(self.schedule, response, spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bd8d7f5cf438364de5ab22a3e032b13cb706e5fe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 149 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,4 @@\n+from time import time\n from urlparse import urlparse, urlunparse, urldefrag\n \n from twisted.python import failure\n@@ -97,7 +98,8 @@ class ScrapyHTTPClientFactory(HTTPClientFactory):\n         self.headers = Headers(request.headers)\n         self.response_headers = None\n         self.timeout = request.meta.get('download_timeout') or timeout\n-        self.deferred = defer.Deferred().addCallback(self._build_response)\n+        self.start_time = time()\n+        self.deferred = defer.Deferred().addCallback(self._build_response, request)\n \n         self._set_connection_attributes(request)\n \n@@ -110,7 +112,8 @@ class ScrapyHTTPClientFactory(HTTPClientFactory):\n             # just in case a broken http/1.1 decides to keep connection alive\n             self.headers.setdefault(\"Connection\", \"close\")\n \n-    def _build_response(self, body):\n+    def _build_response(self, body, request):\n+        request.meta['download_latency'] = self.headers_time-self.start_time\n         status = int(self.status)\n         headers = Headers(self.response_headers)\n         respcls = responsetypes.from_args(headers=headers, url=self.url)\n@@ -125,4 +128,5 @@ class ScrapyHTTPClientFactory(HTTPClientFactory):\n             self.path = self.url\n \n     def gotHeaders(self, headers):\n+        self.headers_time = time()\n         self.response_headers = headers\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#403dc536e22be4b5a3d80a8f12a203c572a612eb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 29 | Churn Cumulative: 121 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -3,8 +3,15 @@ from scrapy import signals\n \n class AutoThrottle(object):\n     \"\"\"\n+    ============\n+    AutoThrottle\n+    ============\n+\n     This is an extension for automatically throttling crawling speed based on\n-    load. It has the following design goals:\n+    load.\n+\n+    Design goals\n+    ============\n \n     1. be nicer to sites instead of using default download delay of zero\n \n@@ -13,16 +20,28 @@ class AutoThrottle(object):\n     the optimum one. the user only needs to specify the maximum concurrent\n     requests it allows, and the extension does the rest.\n \n+    Download latencies\n+    ==================\n \n-    It adjusts download delays and concurrency based on the following rules:\n+    In Scrapy, the download latency is the (real) time elapsed between\n+    establishing the TCP connection and receiving the HTTP headers.\n+\n+    Note that these latencies are very hard to measure accurately in a\n+    cooperative multitasking environment because Scrapy may be busy processing\n+    a spider callback, for example, and unable to attend downloads. However,\n+    the latencies should give a reasonable estimate of how busy Scrapy (and\n+    ultimately, the server) is. This extension builds on that premise.\n+\n+    Throttling rules\n+    ================\n+\n+    This adjusts download delays and concurrency based on the following rules:\n \n     1. spiders always start with one concurrent request and a download delay of\n     START_DELAY\n \n     2. when a response is received, the download delay is adjusted to the\n-    average of previous download delay and the latency of the response. The\n-    latency is the time elapsed since establishing the connection to receiving\n-    the HTTP headers.\n+    average of previous download delay and the latency of the response.\n \n     3. after CONCURRENCY_CHECK_PERIOD responses have passed, the average\n     latency of this period is checked against the previous one and:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d29eccba5678074f70b54509211beddf517e6d2f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 122 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -64,6 +64,7 @@ class AutoThrottle(object):\n \n     def __init__(self):\n         dispatcher.connect(self.spider_opened, signal=signals.spider_opened)\n+        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n         dispatcher.connect(self.response_received, signal=signals.response_received)\n         self.last_latencies = {}\n         self.last_lat = {}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ab6a4d053f476cf2c382f174296f06c1737d0e4a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 124 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -97,7 +97,7 @@ class AutoThrottle(object):\n             curavg, curdev = avg_stdev(latencies)\n             preavg, predev = self.last_lat[spider]\n             self.last_lat[spider] = curavg, curdev\n-            latencies[:] = []\n+            del latencies[:]\n             if curavg > preavg + predev:\n                 if spider.max_concurrent_requests > 1:\n                     spider.max_concurrent_requests -= 1\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#315457c2ef672ed6df9c4bdd0029769859e1f926", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 12 | Churn Cumulative: 86 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,6 +4,7 @@ import os\n from scrapy.utils.spider import iter_spider_classes\n from scrapy.command import ScrapyCommand\n from scrapy.exceptions import UsageError\n+from scrapy.utils.conf import arglist_to_dict\n \n def _import_file(filepath):\n     abspath = os.path.abspath(filepath)\n@@ -33,8 +34,17 @@ class Command(ScrapyCommand):\n     def long_desc(self):\n         return \"Run the spider defined in the given file\"\n \n+    def add_options(self, parser):\n+        ScrapyCommand.add_options(self, parser)\n+        parser.add_option(\"-a\", dest=\"spargs\", action=\"append\", default=[], metavar=\"NAME=VALUE\", \\\n+            help=\"set spider argument (may be repeated)\")\n+\n     def process_options(self, args, opts):\n         ScrapyCommand.process_options(self, args, opts)\n+        try:\n+            opts.spargs = arglist_to_dict(opts.spargs)\n+        except ValueError:\n+            raise UsageError(\"Invalid -a value, use -a NAME=VALUE\", print_help=False)\n \n     def run(self, args, opts):\n         if len(args) != 1:\n@@ -49,7 +59,7 @@ class Command(ScrapyCommand):\n         spclasses = list(iter_spider_classes(module))\n         if not spclasses:\n             raise UsageError(\"No spider found in file: %s\\n\" % filename)\n-        spider = spclasses.pop()()\n+        spider = spclasses.pop()(**opts.spargs)\n         # schedule spider and start engine\n         self.crawler.queue.append_spider(spider)\n         self.crawler.start()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#accb6ed8307054a6703a37bea20990119c1e166c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 14 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -246,7 +246,7 @@ SQS_REGION = 'us-east-1'\n \n STATS_CLASS = 'scrapy.statscol.MemoryStatsCollector'\n STATS_ENABLED = True\n-STATS_DUMP = False\n+STATS_DUMP = True\n \n STATS_SDB_DOMAIN = 'scrapy_stats'\n STATS_SDB_ASYNC = False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#495152bd505bccddfe9de7a4fd8585d7815657bb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 8 | Methods Changed: 6 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 175 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 0.6666666666666666\n\nDIFF:\n@@ -9,9 +9,10 @@ from scrapy.http import Request\n \n class DepthMiddleware(object):\n \n-    def __init__(self, maxdepth, stats=None):\n+    def __init__(self, maxdepth, stats=None, verbose_stats=False):\n         self.maxdepth = maxdepth\n         self.stats = stats\n+        self.verbose_stats = verbose_stats\n         if self.stats and self.maxdepth:\n             stats.set_value('envinfo/request_depth_limit', maxdepth)\n \n@@ -19,11 +20,12 @@ class DepthMiddleware(object):\n     def from_settings(cls, settings):\n         maxdepth = settings.getint('DEPTH_LIMIT')\n         usestats = settings.getbool('DEPTH_STATS')\n+        verbose = settings.getbool('DEPTH_STATS_VERBOSE')\n         if usestats:\n             from scrapy.stats import stats\n         else:\n             stats = None\n-        return cls(maxdepth, stats)\n+        return cls(maxdepth, stats, verbose)\n \n     def process_spider_output(self, response, result, spider):\n         def _filter(request):\n@@ -35,14 +37,15 @@ class DepthMiddleware(object):\n                         level=log.DEBUG, spider=spider)\n                     return False\n                 elif self.stats:\n+                    if self.verbose_stats:\n                         self.stats.inc_value('request_depth_count/%s' % depth, spider=spider)\n-                    if depth > self.stats.get_value('request_depth_max', 0, spider=spider):\n-                        self.stats.set_value('request_depth_max', depth, spider=spider)\n+                    self.stats.max_value('request_depth_max', depth, spider=spider)\n             return True\n \n         # base case (depth=0)\n         if self.stats and 'depth' not in response.request.meta: \n             response.request.meta['depth'] = 0\n+            if self.verbose_stats:\n                 self.stats.inc_value('request_depth_count/0', spider=spider)\n \n         return (r for r in result or () if _filter(r))\n\n@@ -14,7 +14,7 @@ class TestDepthMiddleware(TestCase):\n         self.stats = StatsCollector()\n         self.stats.open_spider(self.spider)\n \n-        self.mw = DepthMiddleware(1, self.stats)\n+        self.mw = DepthMiddleware(1, self.stats, True)\n         self.assertEquals(self.stats.get_value('envinfo/request_depth_limit'), 1)\n \n     def test_process_spider_output(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
