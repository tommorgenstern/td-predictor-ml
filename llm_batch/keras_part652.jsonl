{"custom_id": "keras#d2cb9bf8ca0a07c603794e8b14420f316fadf10f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 11/11 | Churn Δ: 1 | Churn Cumulative: 1577 | Contributors (this commit): 27 | Commits (past 90d): 2 | Contributors (cumulative): 27 | DMM Complexity: None\n\nDIFF:\n@@ -70,6 +70,7 @@ def is_interactive_logging_enabled():\n     )\n \n \n+@logging.skip_log_prefix\n def print_msg(message, line_break=True):\n     \"\"\"Print the message to absl logging or stdout.\"\"\"\n     if is_interactive_logging_enabled():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1c36702b54ff9c04a0b7211283470dae0422bbb7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 56/56 | Churn Δ: 2 | Churn Cumulative: 1662 | Contributors (this commit): 11 | Commits (past 90d): 3 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -198,7 +198,7 @@ class Policy:\n             raise TypeError(f\"'name' must be a string, but got: {name}\")\n         self._name = name\n         self._compute_dtype, self._variable_dtype = self._parse_name(name)\n-        if name in (\"mixed_float16\", \"mixed_bloat16\"):\n+        if name in (\"mixed_float16\", \"mixed_bfloat16\"):\n             device_compatibility_check.log_device_compatibility_check(name)\n \n     def _parse_name(self, name):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a5f1b796ee37d973a73ccd6a80dc76d9d32698f3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 714 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -60,7 +60,7 @@ class Poisson(base_metric.MeanMetricWrapper):\n \n     ```python\n     model.compile(optimizer='sgd',\n-                  loss='mse',\n+                  loss='categorical_crossentropy',\n                   metrics=[tf.keras.metrics.Poisson()])\n     ```\n     \"\"\"\n@@ -98,7 +98,7 @@ class KLDivergence(base_metric.MeanMetricWrapper):\n \n     ```python\n     model.compile(optimizer='sgd',\n-                  loss='mse',\n+                  loss='categorical_crossentropy',\n                   metrics=[tf.keras.metrics.KLDivergence()])\n     ```\n     \"\"\"\n@@ -143,7 +143,7 @@ class BinaryCrossentropy(base_metric.MeanMetricWrapper):\n     ```python\n     model.compile(\n         optimizer='sgd',\n-        loss='mse',\n+        loss='binary_crossentropy',\n         metrics=[tf.keras.metrics.BinaryCrossentropy()])\n     ```\n     \"\"\"\n@@ -213,7 +213,7 @@ class CategoricalCrossentropy(base_metric.MeanMetricWrapper):\n     ```python\n     model.compile(\n       optimizer='sgd',\n-      loss='mse',\n+      loss='categorical_crossentropy',\n       metrics=[tf.keras.metrics.CategoricalCrossentropy()])\n     ```\n     \"\"\"\n@@ -294,7 +294,7 @@ class SparseCategoricalCrossentropy(base_metric.MeanMetricWrapper):\n     ```python\n     model.compile(\n       optimizer='sgd',\n-      loss='mse',\n+      loss='sparse_categorical_crossentropy',\n       metrics=[tf.keras.metrics.SparseCategoricalCrossentropy()])\n     ```\n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6376740aca209be59e6a1fda79635d45cb8a32e7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 131 | Lines Deleted: 82 | Files Changed: 32 | Hunks: 67 | Methods Changed: 52 | Complexity Δ (Sum/Max): 10/4 | Churn Δ: 213 | Churn Cumulative: 13893 | Contributors (this commit): 15 | Commits (past 90d): 366 | Contributors (cumulative): 116 | DMM Complexity: 0.6785714285714286\n\nDIFF:\n@@ -42,10 +42,12 @@ def _get_concrete_noise_shape(inputs, noise_shape):\n         return inputs.shape\n \n     concrete_inputs_shape = inputs.shape\n-    noise_shape = []\n+    concrete_noise_shape = []\n     for i, value in enumerate(noise_shape):\n-        noise_shape.append(concrete_inputs_shape[i] if value is None else value)\n-    return noise_shape\n+        concrete_noise_shape.append(\n+            concrete_inputs_shape[i] if value is None else value\n+        )\n+    return concrete_noise_shape\n \n \n def dropout(inputs, rate, noise_shape=None, seed=None):\n\n@@ -59,7 +59,7 @@ def to_torch_dtype(dtype):\n class Variable(KerasVariable):\n     def _initialize(self, value):\n         self._value = torch.nn.Parameter(\n-            convert_to_tensor(value, dtype=self._dtype).to(get_device()),\n+            convert_to_tensor(value, dtype=self._dtype),\n             requires_grad=self.trainable,\n         ).to(get_device())\n \n@@ -87,18 +87,6 @@ class Variable(KerasVariable):\n     def __array__(self, dtype=None):\n         return _prepare_for_numpy(self.value).__array__(dtype)\n \n-    @property\n-    def value(self):\n-        value = super().value\n-        # Create and use a symbolic tensor stub in symbolic calls.\n-        if get_device() == \"meta\" and value.device != \"meta\":\n-            return torch.empty(\n-                size=value.shape,\n-                dtype=value.dtype,\n-                device=\"meta\",\n-            )\n-        return value\n-\n \n def convert_to_tensor(x, dtype=None):\n     dtype = to_torch_dtype(dtype or getattr(x, \"dtype\", None))\n@@ -116,7 +104,9 @@ def convert_to_tensor(x, dtype=None):\n     elif len(x) > 0 and isinstance(x[0], torch.Tensor):\n         # Handle list or tuple of torch tensors\n         return torch.stack(x)\n-\n+    if isinstance(x, np.ndarray) and x.dtype == np.uint32:\n+        # Torch backend does not support uint32.\n+        x = x.astype(np.int64)\n     return torch.as_tensor(x, dtype=dtype, device=get_device())\n \n \n@@ -180,7 +170,6 @@ def compute_output_spec(fn, *args, **kwargs):\n                 )\n             return x\n \n-        with device_scope(\"meta\"):\n         args_1, kwargs_1 = nest.map_structure(\n             lambda x: convert_keras_tensor_to_torch(x, fill_value=83),\n             (args, kwargs),\n@@ -190,7 +179,6 @@ def compute_output_spec(fn, *args, **kwargs):\n         outputs = outputs_1\n \n         if none_in_shape:\n-            with device_scope(\"meta\"):\n             args_2, kwargs_2 = nest.map_structure(\n                 lambda x: convert_keras_tensor_to_torch(x, fill_value=89),\n                 (args, kwargs),\n\n@@ -3,6 +3,7 @@ import torch\n \n from keras_core.backend.torch.core import cast\n from keras_core.backend.torch.core import convert_to_tensor\n+from keras_core.backend.torch.core import get_device\n from keras_core.backend.torch.core import to_torch_dtype\n \n TORCH_INT_TYPES = (\n@@ -49,13 +50,7 @@ def mean(x, axis=None, keepdims=False):\n \n def max(x, axis=None, keepdims=False, initial=None):\n     x = convert_to_tensor(x)\n-    if axis is None:\n-        result = torch.max(x)\n-    else:\n-        if isinstance(axis, list):\n-            axis = axis[-1]\n-        result = torch.max(x, dim=axis, keepdim=keepdims)\n-\n+    result = amax(x, axis=axis, keepdims=keepdims)\n     if isinstance(getattr(result, \"values\", None), torch.Tensor):\n         result = result.values\n \n@@ -68,14 +63,14 @@ def ones(shape, dtype=\"float32\"):\n     dtype = to_torch_dtype(dtype)\n     if isinstance(shape, int):\n         shape = (shape,)\n-    return torch.ones(size=shape, dtype=dtype)\n+    return torch.ones(size=shape, dtype=dtype).to(get_device())\n \n \n def zeros(shape, dtype=\"float32\"):\n     dtype = to_torch_dtype(dtype)\n     if isinstance(shape, int):\n         shape = (shape,)\n-    return torch.zeros(size=shape, dtype=dtype)\n+    return torch.zeros(size=shape, dtype=dtype).to(get_device())\n \n \n def zeros_like(x, dtype=None):\n@@ -605,7 +600,7 @@ def outer(x1, x2):\n \n def pad(x, pad_width, mode=\"constant\"):\n     x = convert_to_tensor(x)\n-    pad_sum = ()\n+    pad_sum = []\n     pad_width = list(pad_width)[::-1]  # torch uses reverse order\n     for pad in pad_width:\n         pad_sum += pad\n@@ -722,6 +717,9 @@ def swapaxes(x, axis1, axis2):\n def take(x, indices, axis=None):\n     x = convert_to_tensor(x)\n     indices = convert_to_tensor(indices).long()\n+    if x.ndim == 2 and (axis is None or axis == 0):\n+        # This case is equivalent to embedding lookup.\n+        return torch.nn.functional.embedding(indices, x)\n     if axis is not None:\n         return torch.index_select(x, dim=axis, index=indices).squeeze(axis)\n     return torch.take(x, index=indices)\n@@ -860,7 +858,7 @@ def eye(N, M=None, k=None, dtype=\"float32\"):\n     M = N if M is None else M\n     k = 0 if k is None else k\n     if k == 0:\n-        return torch.eye(N, M, dtype=dtype)\n+        return torch.eye(N, M, dtype=dtype).to(get_device())\n     diag_length = np.maximum(N, M)\n     diag = torch.ones(diag_length, dtype=dtype)\n-    return torch.diag(diag, diagonal=k)[:N, :M]\n+    return torch.diag(diag, diagonal=k)[:N, :M].to(get_device())\n\n@@ -2,6 +2,7 @@ import torch\n import torch.nn.functional as tnn\n \n from keras_core.backend.config import floatx\n+from keras_core.backend.torch.core import get_device\n from keras_core.backend.torch.core import to_torch_dtype\n from keras_core.random.seed_generator import SeedGenerator\n from keras_core.random.seed_generator import draw_seed\n@@ -21,16 +22,19 @@ def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n     generator = torch_seed_generator(seed)\n     return torch.normal(\n         mean, stddev, size=shape, generator=generator, dtype=dtype\n-    )\n+    ).to(get_device())\n \n \n def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n     dtype = dtype or floatx()\n     dtype = to_torch_dtype(dtype)\n     generator = torch_seed_generator(seed)\n-    return (maxval - minval) * torch.rand(\n+    if len(shape) == 0:\n+        shape = (1,)\n+    output = (maxval - minval) * torch.rand(\n         *shape, generator=generator, dtype=dtype\n     ) + minval\n+    return output.to(get_device())\n \n \n def randint(shape, minval, maxval, dtype=\"int32\", seed=None):\n@@ -50,12 +54,33 @@ def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n     trunc_x = torch.empty(shape)\n     trunc_x.data.copy_(x.gather(-1, indexes).squeeze(-1))\n     trunc_x.data.mul_(stddev).add_(mean)\n-    return trunc_x\n+    return trunc_x.to(get_device())\n+\n+\n+def _get_concrete_noise_shape(inputs, noise_shape):\n+    if noise_shape is None:\n+        return inputs.shape\n+\n+    concrete_inputs_shape = inputs.shape\n+    concrete_noise_shape = []\n+    for i, value in enumerate(noise_shape):\n+        concrete_noise_shape.append(\n+            concrete_inputs_shape[i] if value is None else value\n+        )\n+    return concrete_noise_shape\n \n \n def dropout(inputs, rate, noise_shape=None, seed=None):\n-    # TODO: setting seed globally via `manual_seed` might create side effects.\n-    if seed is not None:\n-        seed_val, _ = draw_seed(seed)\n-        torch.manual_seed(int(seed_val))\n-    return tnn.dropout(inputs, p=rate)\n+    seed, _ = draw_seed(seed)\n+    generator = torch.Generator()\n+    generator.manual_seed(int(seed))\n+\n+    keep_prob = 1.0 - rate\n+    noise_shape = _get_concrete_noise_shape(inputs, noise_shape)\n+    keep_prob_matrix = torch.full(noise_shape, keep_prob)\n+    mask = torch.bernoulli(keep_prob_matrix, generator=generator).bool()\n+    mask = torch.broadcast_to(mask, inputs.shape)\n+    mask = mask.to(get_device())\n+    return torch.where(\n+        mask, inputs / keep_prob, torch.zeros_like(inputs, dtype=inputs.dtype)\n+    )\n\n@@ -407,7 +407,7 @@ class ConvCorrectnessTest(testing.TestCase, parameterized.TestCase):\n \n         outputs = layer(inputs)\n         expected = tf_keras_layer(inputs)\n-        self.assertAllClose(outputs, expected)\n+        self.assertAllClose(outputs, expected, rtol=5e-4)\n \n     @parameterized.parameters(\n         {\n@@ -482,4 +482,4 @@ class ConvCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         outputs = layer(inputs)\n \n         expected = tf_keras_layer(inputs)\n-        self.assertAllClose(outputs, expected, rtol=1e-5)\n+        self.assertAllClose(outputs, expected, rtol=5e-4)\n\n@@ -31,6 +31,8 @@ class MaskingTest(testing.TestCase):\n         layer = layers.Masking(mask_value=0.0)\n         self.assertAllClose(layer.compute_mask(x), expected_mask)\n \n+        test_obj = self\n+\n         class TestLayer(layers.Layer):\n             def __init__(self, **kwargs):\n                 super().__init__(**kwargs)\n@@ -41,7 +43,7 @@ class MaskingTest(testing.TestCase):\n \n             def call(self, inputs, mask=None):\n                 assert mask is not None\n-                np.testing.assert_allclose(mask, expected_mask)\n+                test_obj.assertAllClose(mask, expected_mask)\n                 return inputs\n \n         model = models.Sequential(\n\n@@ -528,12 +528,15 @@ class Layer(BackendLayer, Operation):\n         # 1. Convert any array arguments to tensors of correct dtype.\n         def maybe_convert(x):\n             if backend.is_tensor(x):\n+                # Handle Torch device placement.\n+                if backend.backend() == \"torch\":\n+                    x = backend.convert_to_tensor(x)\n                 if (\n                     self.autocast\n                     and backend.is_float_dtype(x.dtype)\n                     and x.dtype != self.compute_dtype\n                 ):\n-                    return backend.cast(x, dtype=self.compute_dtype)\n+                    x = backend.cast(x, dtype=self.compute_dtype)\n                 return x\n             elif isinstance(x, backend.KerasTensor):\n                 if (\n@@ -948,6 +951,7 @@ class Layer(BackendLayer, Operation):\n             shapes_dict = get_shapes_dict(self.build, call_spec, self.__class__)\n             self._build_shapes_dict = shapes_dict\n             failure = False\n+\n             if len(shapes_dict) == 1:\n                 # Single arg: pass it positionally\n                 input_shape = tuple(shapes_dict.values())[0]\n\n@@ -363,13 +363,13 @@ class LayerTest(testing.TestCase):\n         y = layer(x)\n         self.assertEqual(layer.compute_dtype, \"float16\")\n         self.assertEqual(layer.variable_dtype, \"float16\")\n-        self.assertEqual(y.dtype.name, \"float16\")\n+        self.assertEqual(backend.standardize_dtype(y.dtype), \"float16\")\n \n         layer = layers.Dense(2, dtype=\"mixed_float16\")\n         y = layer(x)\n         self.assertEqual(layer.compute_dtype, \"float16\")\n         self.assertEqual(layer.variable_dtype, \"float32\")\n-        self.assertEqual(y.dtype.name, \"float16\")\n+        self.assertEqual(backend.standardize_dtype(y.dtype), \"float16\")\n         self.assertEqual(layer.kernel.dtype, \"float32\")\n \n     def test_masking(self):\n\n@@ -88,8 +88,10 @@ class BatchNormalizationTest(testing.TestCase, parameterized.TestCase):\n         broadcast_shape = [1] * len(input_shape)\n         broadcast_shape[axis] = input_shape[axis]\n         out = backend.convert_to_numpy(out)\n-        out -= np.reshape(np.array(layer.beta), broadcast_shape)\n-        out /= np.reshape(np.array(layer.gamma), broadcast_shape)\n+        out -= np.reshape(backend.convert_to_numpy(layer.beta), broadcast_shape)\n+        out /= np.reshape(\n+            backend.convert_to_numpy(layer.gamma), broadcast_shape\n+        )\n \n         reduction_axes = list(range(len(input_shape)))\n         del reduction_axes[axis]\n@@ -132,8 +134,8 @@ class BatchNormalizationTest(testing.TestCase, parameterized.TestCase):\n             out = layer(x, training=True)\n \n         out = backend.convert_to_numpy(out)\n-        out -= np.reshape(np.array(layer.beta), (1, 1, 1, 3))\n-        out /= np.reshape(np.array(layer.gamma), (1, 1, 1, 3))\n+        out -= np.reshape(backend.convert_to_numpy(layer.beta), (1, 1, 1, 3))\n+        out /= np.reshape(backend.convert_to_numpy(layer.gamma), (1, 1, 1, 3))\n \n         self.assertAllClose(np.mean(out, axis=(0, 1, 2)), 0.0, atol=1e-3)\n         self.assertAllClose(np.std(out, axis=(0, 1, 2)), 1.0, atol=1e-3)\n\n@@ -205,7 +205,7 @@ class LayerNormalization(Layer):\n         # Compute the batch normalization.\n         inv = 1 / ops.sqrt(variance + self.epsilon)\n         if scale is not None:\n-            inv *= scale\n+            inv = inv * scale\n \n         x = offset - mean * inv if offset is not None else -mean * inv\n         outputs = inputs * ops.cast(inv, inputs.dtype) + ops.cast(\n\n@@ -1,10 +1,12 @@\n import numpy as np\n \n+from keras_core import backend\n from keras_core import layers\n from keras_core import testing\n \n \n def squared_l2_norm(x):\n+    x = backend.convert_to_numpy(x)\n     return np.sum(x**2)\n \n \n\n@@ -136,8 +136,7 @@ class AveragePoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         ((2, 3), (2, 2), \"same\", \"channels_last\"),\n     )\n     def test_average_pooling2d(self, pool_size, strides, padding, data_format):\n-        inputs = np.arange(300, dtype=\"float32\").reshape((3, 5, 5, 4))\n-\n+        inputs = np.arange(16, dtype=\"float32\").reshape((1, 4, 4, 1))\n         layer = layers.AveragePooling2D(\n             pool_size=pool_size,\n             strides=strides,\n\n@@ -187,7 +187,7 @@ class Discretization(Layer):\n \n     def __call__(self, inputs):\n         if not isinstance(inputs, (tf.Tensor, np.ndarray, backend.KerasTensor)):\n-            inputs = tf.convert_to_tensor(np.array(inputs))\n+            inputs = tf.convert_to_tensor(backend.convert_to_numpy(inputs))\n         if not self.built:\n             self.build(inputs.shape)\n         return super().__call__(inputs)\n\n@@ -265,7 +265,6 @@ class Normalization(Layer):\n                 total_count += batch_count\n                 batch_weight = float(batch_count) / total_count\n                 existing_weight = 1.0 - batch_weight\n-\n                 new_total_mean = (\n                     total_mean * existing_weight + batch_mean * batch_weight\n                 )\n\n@@ -35,7 +35,7 @@ class RandomBrightnessTest(testing.TestCase):\n         layer = layers.RandomBrightness([0, 1.0])\n         np.random.seed(seed)\n         inputs = np.random.randint(0, 255, size=(224, 224, 3))\n-        output = layer(inputs)\n+        output = backend.convert_to_numpy(layer(inputs))\n         diff = output - inputs\n         diff = backend.convert_to_numpy(diff)\n         self.assertTrue(np.amin(diff) >= 0)\n@@ -45,9 +45,8 @@ class RandomBrightnessTest(testing.TestCase):\n         layer = layers.RandomBrightness([-1.0, 0.0])\n         np.random.seed(seed)\n         inputs = np.random.randint(0, 255, size=(224, 224, 3))\n-        output = layer(inputs)\n+        output = backend.convert_to_numpy(layer(inputs))\n         diff = output - inputs\n-        diff = backend.convert_to_numpy(diff)\n         self.assertTrue(np.amax(diff) <= 0)\n         self.assertTrue(np.mean(diff) < 0)\n \n\n@@ -1,6 +1,7 @@\n import numpy as np\n import tensorflow as tf\n \n+from keras_core import backend\n from keras_core import layers\n from keras_core import testing\n \n@@ -31,6 +32,7 @@ class RandomContrastTest(testing.TestCase):\n         inp_mean = np.mean(inputs, axis=-3, keepdims=True)\n         inp_mean = np.mean(inp_mean, axis=-2, keepdims=True)\n         actual_outputs = (inputs - inp_mean) * factor + inp_mean\n+        outputs = backend.convert_to_numpy(outputs)\n         actual_outputs = np.clip(outputs, 0, 255)\n \n         self.assertAllClose(outputs, actual_outputs)\n\n@@ -68,7 +68,7 @@ class RandomCrop(Layer):\n \n     def call(self, inputs, training=True):\n         if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n-            inputs = tf.convert_to_tensor(np.array(inputs))\n+            inputs = tf.convert_to_tensor(backend.convert_to_numpy(inputs))\n         outputs = self.layer.call(inputs, training=training)\n         if (\n             backend.backend() != \"tensorflow\"\n\n@@ -60,7 +60,7 @@ class RandomFlip(Layer):\n \n     def call(self, inputs, training=True):\n         if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n-            inputs = tf.convert_to_tensor(np.array(inputs))\n+            inputs = tf.convert_to_tensor(backend.convert_to_numpy(inputs))\n         outputs = self.layer.call(inputs, training=training)\n         if (\n             backend.backend() != \"tensorflow\"\n\n@@ -103,7 +103,7 @@ class RandomRotation(Layer):\n \n     def call(self, inputs, training=True):\n         if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n-            inputs = tf.convert_to_tensor(np.array(inputs))\n+            inputs = tf.convert_to_tensor(backend.convert_to_numpy(inputs))\n         outputs = self.layer.call(inputs, training=training)\n         if (\n             backend.backend() != \"tensorflow\"\n\n@@ -93,7 +93,7 @@ class RandomTranslation(Layer):\n \n     def call(self, inputs, training=True):\n         if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n-            inputs = tf.convert_to_tensor(np.array(inputs))\n+            inputs = tf.convert_to_tensor(backend.convert_to_numpy(inputs))\n         outputs = self.layer.call(inputs, training=training)\n         if (\n             backend.backend() != \"tensorflow\"\n\n@@ -116,7 +116,7 @@ class RandomZoom(Layer):\n \n     def call(self, inputs, training=True):\n         if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n-            inputs = tf.convert_to_tensor(np.array(inputs))\n+            inputs = tf.convert_to_tensor(backend.convert_to_numpy(inputs))\n         outputs = self.layer.call(inputs, training=training)\n         if (\n             backend.backend() != \"tensorflow\"\n\n@@ -65,6 +65,11 @@ class ResizingTest(testing.TestCase, parameterized.TestCase):\n             supports_masking=False,\n             run_training_check=False,\n         )\n+\n+    @pytest.mark.skipif(\n+        backend.backend() == \"torch\", reason=\"Torch does not support lanczos.\"\n+    )\n+    def test_resizing_basics_lanczos5(self):\n         self.run_layer_test(\n             layers.Resizing,\n             init_kwargs={\n\n@@ -26,6 +26,7 @@ class DropoutTest(testing.TestCase):\n         inputs = np.ones((20, 500))\n         layer = layers.Dropout(0.5, seed=1337)\n         outputs = layer(inputs, training=True)\n+        outputs = backend.convert_to_numpy(outputs)\n         self.assertAllClose(np.mean(outputs), 1.0, atol=0.02)\n         self.assertAllClose(np.max(outputs), 2.0)\n \n\n@@ -1,5 +1,6 @@\n import numpy as np\n \n+from keras_core import backend\n from keras_core import layers\n from keras_core import testing\n \n@@ -25,5 +26,7 @@ class GaussianDropoutTest(testing.TestCase):\n         layer = layers.GaussianDropout(0.3, seed=1337)\n         outputs = layer(inputs, training=True)\n         self.assertAllClose(\n-            np.std(outputs), np.sqrt(0.3 / (1 - 0.3)), atol=0.02\n+            np.std(backend.convert_to_numpy(outputs)),\n+            np.sqrt(0.3 / (1 - 0.3)),\n+            atol=0.02,\n         )\n\n@@ -1,5 +1,6 @@\n import numpy as np\n \n+from keras_core import backend\n from keras_core import layers\n from keras_core import testing\n \n@@ -24,4 +25,6 @@ class GaussianNoiseTest(testing.TestCase):\n         inputs = np.ones((20, 500))\n         layer = layers.GaussianNoise(0.3, seed=1337)\n         outputs = layer(inputs, training=True)\n-        self.assertAllClose(np.std(outputs), 0.3, atol=0.02)\n+        self.assertAllClose(\n+            np.std(backend.convert_to_numpy(outputs)), 0.3, atol=0.02\n+        )\n\n@@ -23,7 +23,7 @@ class UpSamplingTest(testing.TestCase):\n         )\n \n     def test_upsampling_1d_correctness(self):\n-        np.testing.assert_array_equal(\n+        self.assertAllClose(\n             layers.UpSampling1D(size=2)(np.arange(12).reshape((2, 2, 3))),\n             np.array(\n                 [\n@@ -44,7 +44,7 @@ class UpSamplingTest(testing.TestCase):\n         )\n \n     def test_upsampling_1d_correctness_with_ones(self):\n-        np.testing.assert_array_equal(\n+        self.assertAllClose(\n             layers.UpSampling1D(size=3)(np.ones((2, 1, 5))), np.ones((2, 3, 5))\n         )\n \n\n@@ -1,5 +1,6 @@\n # flake8: noqa\n import numpy as np\n+import pytest\n from absl.testing import parameterized\n \n from keras_core import backend\n@@ -103,7 +104,7 @@ class UpSampling2dTest(testing.TestCase, parameterized.TestCase):\n     def test_upsampling_2d_correctness(self):\n         input_shape = (2, 2, 1, 3)\n         x = np.arange(np.prod(input_shape)).reshape(input_shape)\n-        np.testing.assert_array_equal(\n+        self.assertAllClose(\n             layers.UpSampling2D(size=(1, 2))(x),\n             # fmt: off\n             np.array(\n@@ -122,11 +123,14 @@ class UpSampling2dTest(testing.TestCase, parameterized.TestCase):\n     def test_upsampling_2d_various_interpolation_methods(self):\n         input_shape = (2, 2, 1, 3)\n         x = np.arange(np.prod(input_shape)).reshape(input_shape)\n-        for interpolation in [\n-            \"bicubic\",\n-            \"bilinear\",\n-            \"lanczos3\",\n-            \"lanczos5\",\n-            \"nearest\",\n-        ]:\n+        for interpolation in [\"nearest\", \"bilinear\", \"bicubic\"]:\n+            layers.UpSampling2D(size=(1, 2), interpolation=interpolation)(x)\n+\n+    @pytest.mark.skipif(\n+        backend.backend() == \"torch\", reason=\"Torch does not support lanczos.\"\n+    )\n+    def test_upsampling_2d_lanczos_interpolation_methods(self):\n+        input_shape = (2, 2, 1, 3)\n+        x = np.arange(np.prod(input_shape)).reshape(input_shape)\n+        for interpolation in [\"lanczos3\", \"lanczos5\"]:\n             layers.UpSampling2D(size=(1, 2), interpolation=interpolation)(x)\n\n@@ -88,7 +88,7 @@ class UpSampling3dTest(testing.TestCase, parameterized.TestCase):\n     def test_upsampling_3d_correctness(self):\n         input_shape = (2, 1, 2, 1, 3)\n         x = np.arange(np.prod(input_shape)).reshape(input_shape)\n-        np.testing.assert_array_equal(\n+        self.assertAllClose(\n             layers.UpSampling3D(size=(2, 2, 2))(x),\n             np.array(\n                 [\n\n@@ -383,7 +383,8 @@ class RNN(Layer):\n         # Note that states may be deeply nested\n         # (e.g. in the stacked cells case).\n         initial_state = nest.map_structure(\n-            lambda x: ops.cast(x, dtype=self.compute_dtype), initial_state\n+            lambda x: backend.convert_to_tensor(x, dtype=self.compute_dtype),\n+            initial_state,\n         )\n \n         last_output, outputs, states = self.inner_loop(\n\n@@ -1,5 +1,8 @@\n import random as python_random\n \n+import numpy as np\n+\n+from keras_core import backend\n from keras_core.api_export import keras_core_export\n \n \n@@ -38,11 +41,12 @@ def draw_seed(seed):\n     from keras_core.backend import convert_to_tensor\n \n     if isinstance(seed, SeedGenerator):\n+        seed_state = seed.state\n         # Use * 1 to create a copy\n-        new_seed_value = seed.state.value * 1\n-        seed.state.assign(\n-            seed.state + convert_to_tensor([0, 1], dtype=\"uint32\")\n-        )\n+        new_seed_value = seed_state.value * 1\n+        seed.state.assign(seed_state + np.array([0, 1], dtype=\"uint32\"))\n+        if backend.backend() == \"torch\":\n+            return backend.convert_to_numpy(new_seed_value)\n         return new_seed_value\n     elif isinstance(seed, int):\n         return convert_to_tensor([seed, 0], dtype=\"uint32\")\n\n@@ -162,11 +162,15 @@ def serialize_keras_object(obj):\n         }\n     if isinstance(obj, tf.TensorShape):\n         return obj.as_list() if obj._dims is not None else None\n-    if isinstance(obj, (tf.Tensor, jax.numpy.ndarray)):\n+    if isinstance(obj, (tf.Tensor, jax.numpy.ndarray)) or hasattr(\n+        obj, \"device\"\n+    ):\n+        # Import torch creates circular dependency, so we use\n+        # `hasattr(obj, \"device\")` to check if obj is a torch tensor.\n         return {\n             \"class_name\": \"__tensor__\",\n             \"config\": {\n-                \"value\": np.array(obj).tolist(),\n+                \"value\": backend.convert_to_numpy(obj).tolist(),\n                 \"dtype\": backend.standardize_dtype(obj.dtype),\n             },\n         }\n\n@@ -399,6 +399,8 @@ def smart_resize(\n     # smaller.\n     crop_height = backend_module.numpy.minimum(height, crop_height)\n     crop_width = backend_module.numpy.minimum(width, crop_width)\n+    crop_height = backend_module.cast(crop_height, \"int32\")\n+    crop_width = backend_module.cast(crop_width, \"int32\")\n \n     crop_box_hstart = backend_module.cast(\n         backend_module.cast(height - crop_height, \"float32\") / 2, \"int32\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#92cb349c14e389718cba0e94c0c87daacbe821bb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 2939 | Contributors (this commit): 8 | Commits (past 90d): 33 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -67,6 +67,7 @@ def logsumexp(x, axis=None, keepdims=False):\n \n \n def qr(x, mode=\"reduced\"):\n+    x = convert_to_tensor(x)\n     if mode not in {\"reduced\", \"complete\"}:\n         raise ValueError(\n             \"`mode` argument value not supported. \"\n\n@@ -3,6 +3,7 @@ import pytest\n import tensorflow as tf\n from absl.testing import parameterized\n \n+from keras_core import backend\n from keras_core import testing\n from keras_core.backend.common.keras_tensor import KerasTensor\n from keras_core.operations import nn as knn\n@@ -286,7 +287,7 @@ class NNOpsDynamicShapeTest(testing.TestCase, parameterized.TestCase):\n         # dtype tests\n         x = np.arange(5)\n         out = knn.one_hot(x, 5, axis=0, dtype=dtype)\n-        self.assertEqual(out.dtype.name, dtype)\n+        self.assertEqual(backend.standardize_dtype(out.dtype), dtype)\n \n \n class NNOpsStaticShapeTest(testing.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#46a83e473469861357dee4354b9f73efc940ddf9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 691 | Contributors (this commit): 5 | Commits (past 90d): 9 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -543,7 +543,6 @@ def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n \n \n def binary_crossentropy(target, output, from_logits=False):\n-    # TODO: `torch.as_tensor` has device arg. Need to think how to pass it.\n     target = convert_to_tensor(target)\n     output = convert_to_tensor(output)\n \n@@ -560,4 +559,5 @@ def binary_crossentropy(target, output, from_logits=False):\n             output, target, reduction=\"none\"\n         )\n     else:\n+        output = torch.clip(output, epsilon(), 1.0 - epsilon())\n         return tnn.binary_cross_entropy(output, target, reduction=\"none\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#445c6fed9d261d26b09fa4c619b0652441253f5e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 642 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 25 | Complexity Δ (Sum/Max): 42/42 | Churn Δ: 642 | Churn Cumulative: 642 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.9106529209621993\n\nDIFF:\n@@ -0,0 +1,642 @@\n+\"\"\"\n+Title: Image Captioning\n+Author: [A_K_Nain](https://twitter.com/A_K_Nain)\n+Date created: 2021/05/29\n+Last modified: 2021/10/31\n+Description: Implement an image captioning model using a CNN and a Transformer.\n+Accelerator: GPU\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import os\n+import re\n+import numpy as np\n+import matplotlib.pyplot as plt\n+\n+import tensorflow as tf\n+import keras_core as keras\n+from keras_core import layers\n+from keras_core.applications import efficientnet\n+from keras_core.layers import TextVectorization\n+\n+\n+seed = 111\n+np.random.seed(seed)\n+tf.random.set_seed(seed)\n+\n+\"\"\"\n+## Download the dataset\n+\n+We will be using the Flickr8K dataset for this tutorial. This dataset comprises over\n+8,000 images, that are each paired with five different captions.\n+\"\"\"\n+\n+\n+\"\"\"shell\n+wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n+wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\n+unzip -qq Flickr8k_Dataset.zip\n+unzip -qq Flickr8k_text.zip\n+rm Flickr8k_Dataset.zip Flickr8k_text.zip\n+\"\"\"\n+\n+\n+# Path to the images\n+IMAGES_PATH = \"Flicker8k_Dataset\"\n+\n+# Desired image dimensions\n+IMAGE_SIZE = (299, 299)\n+\n+# Vocabulary size\n+VOCAB_SIZE = 10000\n+\n+# Fixed length allowed for any sequence\n+SEQ_LENGTH = 25\n+\n+# Dimension for the image embeddings and token embeddings\n+EMBED_DIM = 512\n+\n+# Per-layer units in the feed-forward network\n+FF_DIM = 512\n+\n+# Other training parameters\n+BATCH_SIZE = 64\n+EPOCHS = 30\n+AUTOTUNE = tf.data.AUTOTUNE\n+\n+\"\"\"\n+## Preparing the dataset\n+\"\"\"\n+\n+\n+def load_captions_data(filename):\n+    \"\"\"Loads captions (text) data and maps them to corresponding images.\n+\n+    Args:\n+        filename: Path to the text file containing caption data.\n+\n+    Returns:\n+        caption_mapping: Dictionary mapping image names and the corresponding captions\n+        text_data: List containing all the available captions\n+    \"\"\"\n+\n+    with open(filename) as caption_file:\n+        caption_data = caption_file.readlines()\n+        caption_mapping = {}\n+        text_data = []\n+        images_to_skip = set()\n+\n+        for line in caption_data:\n+            line = line.rstrip(\"\\n\")\n+            # Image name and captions are separated using a tab\n+            img_name, caption = line.split(\"\\t\")\n+\n+            # Each image is repeated five times for the five different captions.\n+            # Each image name has a suffix `#(caption_number)`\n+            img_name = img_name.split(\"#\")[0]\n+            img_name = os.path.join(IMAGES_PATH, img_name.strip())\n+\n+            # We will remove caption that are either too short to too long\n+            tokens = caption.strip().split()\n+\n+            if len(tokens) < 5 or len(tokens) > SEQ_LENGTH:\n+                images_to_skip.add(img_name)\n+                continue\n+\n+            if img_name.endswith(\"jpg\") and img_name not in images_to_skip:\n+                # We will add a start and an end token to each caption\n+                caption = \"<start> \" + caption.strip() + \" <end>\"\n+                text_data.append(caption)\n+\n+                if img_name in caption_mapping:\n+                    caption_mapping[img_name].append(caption)\n+                else:\n+                    caption_mapping[img_name] = [caption]\n+\n+        for img_name in images_to_skip:\n+            if img_name in caption_mapping:\n+                del caption_mapping[img_name]\n+\n+        return caption_mapping, text_data\n+\n+\n+def train_val_split(caption_data, train_size=0.8, shuffle=True):\n+    \"\"\"Split the captioning dataset into train and validation sets.\n+\n+    Args:\n+        caption_data (dict): Dictionary containing the mapped caption data\n+        train_size (float): Fraction of all the full dataset to use as training data\n+        shuffle (bool): Whether to shuffle the dataset before splitting\n+\n+    Returns:\n+        Traning and validation datasets as two separated dicts\n+    \"\"\"\n+\n+    # 1. Get the list of all image names\n+    all_images = list(caption_data.keys())\n+\n+    # 2. Shuffle if necessary\n+    if shuffle:\n+        np.random.shuffle(all_images)\n+\n+    # 3. Split into training and validation sets\n+    train_size = int(len(caption_data) * train_size)\n+\n+    training_data = {\n+        img_name: caption_data[img_name] for img_name in all_images[:train_size]\n+    }\n+    validation_data = {\n+        img_name: caption_data[img_name] for img_name in all_images[train_size:]\n+    }\n+\n+    # 4. Return the splits\n+    return training_data, validation_data\n+\n+\n+# Load the dataset\n+captions_mapping, text_data = load_captions_data(\"Flickr8k.token.txt\")\n+\n+# Split the dataset into training and validation sets\n+train_data, valid_data = train_val_split(captions_mapping)\n+print(\"Number of training samples: \", len(train_data))\n+print(\"Number of validation samples: \", len(valid_data))\n+\n+\"\"\"\n+## Vectorizing the text data\n+\n+We'll use the `TextVectorization` layer to vectorize the text data,\n+that is to say, to turn the\n+original strings into integer sequences where each integer represents the index of\n+a word in a vocabulary. We will use a custom string standardization scheme\n+(strip punctuation characters except `<` and `>`) and the default\n+splitting scheme (split on whitespace).\n+\"\"\"\n+\n+\n+def custom_standardization(input_string):\n+    lowercase = tf.strings.lower(input_string)\n+    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n+\n+\n+strip_chars = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n+strip_chars = strip_chars.replace(\"<\", \"\")\n+strip_chars = strip_chars.replace(\">\", \"\")\n+\n+vectorization = TextVectorization(\n+    max_tokens=VOCAB_SIZE,\n+    output_mode=\"int\",\n+    output_sequence_length=SEQ_LENGTH,\n+    standardize=custom_standardization,\n+)\n+vectorization.adapt(text_data)\n+\n+# Data augmentation for image data\n+image_augmentation = keras.Sequential(\n+    [\n+        layers.RandomFlip(\"horizontal\"),\n+        layers.RandomRotation(0.2),\n+        layers.RandomContrast(0.3),\n+    ]\n+)\n+\n+\n+\"\"\"\n+## Building a `tf.data.Dataset` pipeline for training\n+\n+We will generate pairs of images and corresponding captions using a `tf.data.Dataset` object.\n+The pipeline consists of two steps:\n+\n+1. Read the image from the disk\n+2. Tokenize all the five captions corresponding to the image\n+\"\"\"\n+\n+\n+def decode_and_resize(img_path):\n+    img = tf.io.read_file(img_path)\n+    img = tf.image.decode_jpeg(img, channels=3)\n+    img = tf.image.resize(img, IMAGE_SIZE)\n+    img = tf.image.convert_image_dtype(img, tf.float32)\n+    return img\n+\n+\n+def process_input(img_path, captions):\n+    return decode_and_resize(img_path), vectorization(captions)\n+\n+\n+def make_dataset(images, captions):\n+    dataset = tf.data.Dataset.from_tensor_slices((images, captions))\n+    dataset = dataset.shuffle(BATCH_SIZE * 8)\n+    dataset = dataset.map(process_input, num_parallel_calls=AUTOTUNE)\n+    dataset = dataset.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n+\n+    return dataset\n+\n+\n+# Pass the list of images and the list of corresponding captions\n+train_dataset = make_dataset(list(train_data.keys()), list(train_data.values()))\n+\n+valid_dataset = make_dataset(list(valid_data.keys()), list(valid_data.values()))\n+\n+\n+\"\"\"\n+## Building the model\n+\n+Our image captioning architecture consists of three models:\n+\n+1. A CNN: used to extract the image features\n+2. A TransformerEncoder: The extracted image features are then passed to a Transformer\n+                    based encoder that generates a new representation of the inputs\n+3. A TransformerDecoder: This model takes the encoder output and the text data\n+                    (sequences) as inputs and tries to learn to generate the caption.\n+\"\"\"\n+\n+\n+def get_cnn_model():\n+    base_model = efficientnet.EfficientNetB0(\n+        input_shape=(*IMAGE_SIZE, 3),\n+        include_top=False,\n+        weights=\"imagenet\",\n+    )\n+    # We freeze our feature extractor\n+    base_model.trainable = False\n+    base_model_out = base_model.output\n+    base_model_out = layers.Reshape((-1, base_model_out.shape[-1]))(base_model_out)\n+    cnn_model = keras.models.Model(base_model.input, base_model_out)\n+    return cnn_model\n+\n+\n+class TransformerEncoderBlock(layers.Layer):\n+    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n+        super().__init__(**kwargs)\n+        self.embed_dim = embed_dim\n+        self.dense_dim = dense_dim\n+        self.num_heads = num_heads\n+        self.attention_1 = layers.MultiHeadAttention(\n+            num_heads=num_heads, key_dim=embed_dim, dropout=0.0\n+        )\n+        self.layernorm_1 = layers.LayerNormalization()\n+        self.layernorm_2 = layers.LayerNormalization()\n+        self.dense_1 = layers.Dense(embed_dim, activation=\"relu\")\n+\n+    def call(self, inputs, training, mask=None):\n+        inputs = self.layernorm_1(inputs)\n+        inputs = self.dense_1(inputs)\n+\n+        attention_output_1 = self.attention_1(\n+            query=inputs,\n+            value=inputs,\n+            key=inputs,\n+            attention_mask=None,\n+            training=training,\n+        )\n+        out_1 = self.layernorm_2(inputs + attention_output_1)\n+        return out_1\n+\n+\n+class PositionalEmbedding(layers.Layer):\n+    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n+        super().__init__(**kwargs)\n+        self.token_embeddings = layers.Embedding(\n+            input_dim=vocab_size, output_dim=embed_dim\n+        )\n+        self.position_embeddings = layers.Embedding(\n+            input_dim=sequence_length, output_dim=embed_dim\n+        )\n+        self.sequence_length = sequence_length\n+        self.vocab_size = vocab_size\n+        self.embed_dim = embed_dim\n+        self.embed_scale = tf.math.sqrt(tf.cast(embed_dim, tf.float32))\n+\n+    def call(self, inputs):\n+        length = tf.shape(inputs)[-1]\n+        positions = tf.range(start=0, limit=length, delta=1)\n+        embedded_tokens = self.token_embeddings(inputs)\n+        embedded_tokens = embedded_tokens * self.embed_scale\n+        embedded_positions = self.position_embeddings(positions)\n+        return embedded_tokens + embedded_positions\n+\n+    def compute_mask(self, inputs, mask=None):\n+        return tf.math.not_equal(inputs, 0)\n+\n+\n+class TransformerDecoderBlock(layers.Layer):\n+    def __init__(self, embed_dim, ff_dim, num_heads, **kwargs):\n+        super().__init__(**kwargs)\n+        self.embed_dim = embed_dim\n+        self.ff_dim = ff_dim\n+        self.num_heads = num_heads\n+        self.attention_1 = layers.MultiHeadAttention(\n+            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n+        )\n+        self.attention_2 = layers.MultiHeadAttention(\n+            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n+        )\n+        self.ffn_layer_1 = layers.Dense(ff_dim, activation=\"relu\")\n+        self.ffn_layer_2 = layers.Dense(embed_dim)\n+\n+        self.layernorm_1 = layers.LayerNormalization()\n+        self.layernorm_2 = layers.LayerNormalization()\n+        self.layernorm_3 = layers.LayerNormalization()\n+\n+        self.embedding = PositionalEmbedding(\n+            embed_dim=EMBED_DIM, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE\n+        )\n+        self.out = layers.Dense(VOCAB_SIZE, activation=\"softmax\")\n+\n+        self.dropout_1 = layers.Dropout(0.3)\n+        self.dropout_2 = layers.Dropout(0.5)\n+        self.supports_masking = True\n+\n+    def call(self, inputs, encoder_outputs, training, mask=None):\n+        inputs = self.embedding(inputs)\n+        causal_mask = self.get_causal_attention_mask(inputs)\n+\n+        if mask is not None:\n+            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n+            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n+            combined_mask = tf.minimum(combined_mask, causal_mask)\n+\n+        attention_output_1 = self.attention_1(\n+            query=inputs,\n+            value=inputs,\n+            key=inputs,\n+            attention_mask=combined_mask,\n+            training=training,\n+        )\n+        out_1 = self.layernorm_1(inputs + attention_output_1)\n+\n+        attention_output_2 = self.attention_2(\n+            query=out_1,\n+            value=encoder_outputs,\n+            key=encoder_outputs,\n+            attention_mask=padding_mask,\n+            training=training,\n+        )\n+        out_2 = self.layernorm_2(out_1 + attention_output_2)\n+\n+        ffn_out = self.ffn_layer_1(out_2)\n+        ffn_out = self.dropout_1(ffn_out, training=training)\n+        ffn_out = self.ffn_layer_2(ffn_out)\n+\n+        ffn_out = self.layernorm_3(ffn_out + out_2, training=training)\n+        ffn_out = self.dropout_2(ffn_out, training=training)\n+        preds = self.out(ffn_out)\n+        return preds\n+\n+    def get_causal_attention_mask(self, inputs):\n+        input_shape = tf.shape(inputs)\n+        batch_size, sequence_length = input_shape[0], input_shape[1]\n+        i = tf.range(sequence_length)[:, tf.newaxis]\n+        j = tf.range(sequence_length)\n+        mask = tf.cast(i >= j, dtype=\"int32\")\n+        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n+        mult = tf.concat(\n+            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n+            axis=0,\n+        )\n+        return tf.tile(mask, mult)\n+\n+\n+class ImageCaptioningModel(keras.Model):\n+    def __init__(\n+        self,\n+        cnn_model,\n+        encoder,\n+        decoder,\n+        num_captions_per_image=5,\n+        image_aug=None,\n+    ):\n+        super().__init__()\n+        self.cnn_model = cnn_model\n+        self.encoder = encoder\n+        self.decoder = decoder\n+        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n+        self.acc_tracker = keras.metrics.Mean(name=\"accuracy\")\n+        self.num_captions_per_image = num_captions_per_image\n+        self.image_aug = image_aug\n+\n+    def calculate_loss(self, y_true, y_pred, mask):\n+        loss = self.loss(y_true, y_pred)\n+        mask = tf.cast(mask, dtype=loss.dtype)\n+        loss *= mask\n+        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n+\n+    def calculate_accuracy(self, y_true, y_pred, mask):\n+        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n+        accuracy = tf.math.logical_and(mask, accuracy)\n+        accuracy = tf.cast(accuracy, dtype=tf.float32)\n+        mask = tf.cast(mask, dtype=tf.float32)\n+        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n+\n+    def _compute_caption_loss_and_acc(self, img_embed, batch_seq, training=True):\n+        encoder_out = self.encoder(img_embed, training=training)\n+        batch_seq_inp = batch_seq[:, :-1]\n+        batch_seq_true = batch_seq[:, 1:]\n+        mask = tf.math.not_equal(batch_seq_true, 0)\n+        batch_seq_pred = self.decoder(\n+            batch_seq_inp, encoder_out, training=training, mask=mask\n+        )\n+        loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)\n+        acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)\n+        return loss, acc\n+\n+    def train_step(self, batch_data):\n+        batch_img, batch_seq = batch_data\n+        batch_loss = 0\n+        batch_acc = 0\n+\n+        if self.image_aug:\n+            batch_img = self.image_aug(batch_img)\n+\n+        # 1. Get image embeddings\n+        img_embed = self.cnn_model(batch_img)\n+\n+        # 2. Pass each of the five captions one by one to the decoder\n+        # along with the encoder outputs and compute the loss as well as accuracy\n+        # for each caption.\n+        for i in range(self.num_captions_per_image):\n+            with tf.GradientTape() as tape:\n+                loss, acc = self._compute_caption_loss_and_acc(\n+                    img_embed, batch_seq[:, i, :], training=True\n+                )\n+\n+                # 3. Update loss and accuracy\n+                batch_loss += loss\n+                batch_acc += acc\n+\n+            # 4. Get the list of all the trainable weights\n+            train_vars = (\n+                self.encoder.trainable_variables + self.decoder.trainable_variables\n+            )\n+\n+            # 5. Get the gradients\n+            grads = tape.gradient(loss, train_vars)\n+\n+            # 6. Update the trainable weights\n+            self.optimizer.apply_gradients(zip(grads, train_vars))\n+\n+        # 7. Update the trackers\n+        batch_acc /= float(self.num_captions_per_image)\n+        self.loss_tracker.update_state(batch_loss)\n+        self.acc_tracker.update_state(batch_acc)\n+\n+        # 8. Return the loss and accuracy values\n+        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n+\n+    def test_step(self, batch_data):\n+        batch_img, batch_seq = batch_data\n+        batch_loss = 0\n+        batch_acc = 0\n+\n+        # 1. Get image embeddings\n+        img_embed = self.cnn_model(batch_img)\n+\n+        # 2. Pass each of the five captions one by one to the decoder\n+        # along with the encoder outputs and compute the loss as well as accuracy\n+        # for each caption.\n+        for i in range(self.num_captions_per_image):\n+            loss, acc = self._compute_caption_loss_and_acc(\n+                img_embed, batch_seq[:, i, :], training=False\n+            )\n+\n+            # 3. Update batch loss and batch accuracy\n+            batch_loss += loss\n+            batch_acc += acc\n+\n+        batch_acc /= float(self.num_captions_per_image)\n+\n+        # 4. Update the trackers\n+        self.loss_tracker.update_state(batch_loss)\n+        self.acc_tracker.update_state(batch_acc)\n+\n+        # 5. Return the loss and accuracy values\n+        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n+\n+    @property\n+    def metrics(self):\n+        # We need to list our metrics here so the `reset_states()` can be\n+        # called automatically.\n+        return [self.loss_tracker, self.acc_tracker]\n+\n+\n+cnn_model = get_cnn_model()\n+encoder = TransformerEncoderBlock(embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_heads=1)\n+decoder = TransformerDecoderBlock(embed_dim=EMBED_DIM, ff_dim=FF_DIM, num_heads=2)\n+caption_model = ImageCaptioningModel(\n+    cnn_model=cnn_model,\n+    encoder=encoder,\n+    decoder=decoder,\n+    image_aug=image_augmentation,\n+)\n+\n+\"\"\"\n+## Model training\n+\"\"\"\n+\n+\n+# Define the loss function\n+cross_entropy = keras.losses.SparseCategoricalCrossentropy(\n+    from_logits=False, reduction=None,\n+)\n+\n+# EarlyStopping criteria\n+early_stopping = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n+\n+\n+# Learning Rate Scheduler for the optimizer\n+class LRSchedule(keras.optimizers.schedules.learning_rate_schedule.LearningRateSchedule):\n+    def __init__(self, post_warmup_learning_rate, warmup_steps):\n+        super().__init__()\n+        self.post_warmup_learning_rate = post_warmup_learning_rate\n+        self.warmup_steps = warmup_steps\n+\n+    def __call__(self, step):\n+        global_step = tf.cast(step, tf.float32)\n+        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n+        warmup_progress = global_step / warmup_steps\n+        warmup_learning_rate = self.post_warmup_learning_rate * warmup_progress\n+        return tf.cond(\n+            global_step < warmup_steps,\n+            lambda: warmup_learning_rate,\n+            lambda: self.post_warmup_learning_rate,\n+        )\n+\n+\n+# Create a learning rate schedule\n+num_train_steps = len(train_dataset) * EPOCHS\n+num_warmup_steps = num_train_steps // 15\n+lr_schedule = LRSchedule(post_warmup_learning_rate=1e-4, warmup_steps=num_warmup_steps)\n+\n+# Compile the model\n+caption_model.compile(optimizer=keras.optimizers.Adam(lr_schedule), loss=cross_entropy)\n+\n+# Fit the model\n+caption_model.fit(\n+    train_dataset,\n+    epochs=EPOCHS,\n+    validation_data=valid_dataset,\n+    callbacks=[early_stopping],\n+)\n+\n+\"\"\"\n+## Check sample predictions\n+\"\"\"\n+\n+vocab = vectorization.get_vocabulary()\n+index_lookup = dict(zip(range(len(vocab)), vocab))\n+max_decoded_sentence_length = SEQ_LENGTH - 1\n+valid_images = list(valid_data.keys())\n+\n+\n+def generate_caption():\n+    # Select a random image from the validation dataset\n+    sample_img = np.random.choice(valid_images)\n+\n+    # Read the image from the disk\n+    sample_img = decode_and_resize(sample_img)\n+    img = sample_img.numpy().clip(0, 255).astype(np.uint8)\n+    plt.imshow(img)\n+    plt.show()\n+\n+    # Pass the image to the CNN\n+    img = tf.expand_dims(sample_img, 0)\n+    img = caption_model.cnn_model(img)\n+\n+    # Pass the image features to the Transformer encoder\n+    encoded_img = caption_model.encoder(img, training=False)\n+\n+    # Generate the caption using the Transformer decoder\n+    decoded_caption = \"<start> \"\n+    for i in range(max_decoded_sentence_length):\n+        tokenized_caption = vectorization([decoded_caption])[:, :-1]\n+        mask = tf.math.not_equal(tokenized_caption, 0)\n+        predictions = caption_model.decoder(\n+            tokenized_caption, encoded_img, training=False, mask=mask\n+        )\n+        sampled_token_index = np.argmax(predictions[0, i, :])\n+        sampled_token = index_lookup[sampled_token_index]\n+        if sampled_token == \"<end>\":\n+            break\n+        decoded_caption += \" \" + sampled_token\n+\n+    decoded_caption = decoded_caption.replace(\"<start> \", \"\")\n+    decoded_caption = decoded_caption.replace(\" <end>\", \"\").strip()\n+    print(\"Predicted Caption: \", decoded_caption)\n+\n+\n+# Check predictions for a few samples\n+generate_caption()\n+generate_caption()\n+generate_caption()\n+\n+\"\"\"\n+## End Notes\n+\n+We saw that the model starts to generate reasonable captions after a few epochs. To keep\n+this example easily runnable, we have trained it with a few constraints, like a minimal\n+number of attention heads. To improve the predictions, you can try changing these training\n+settings and find a good model for your use case.\n+\"\"\"\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
