{"custom_id": "keras#1665377cb2551bd05bff215346cce2ef100ffd66", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 223 | Lines Deleted: 106 | Files Changed: 9 | Hunks: 87 | Methods Changed: 61 | Complexity Δ (Sum/Max): -29/9 | Churn Δ: 329 | Churn Cumulative: 12453 | Contributors (this commit): 12 | Commits (past 90d): 74 | Contributors (cumulative): 50 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,4 @@\n+import operator\n import warnings\n \n \n@@ -255,3 +256,35 @@ def compute_conv_transpose_output_shape(\n     else:\n         output_shape = [input_shape[0], filters] + output_shape\n     return output_shape\n+\n+\n+def canonicalize_axis(axis, num_dims):\n+    \"\"\"Canonicalize an axis in [-num_dims, num_dims) to [0, num_dims).\"\"\"\n+    axis = operator.index(axis)\n+    if not -num_dims <= axis < num_dims:\n+        raise ValueError(\n+            f\"axis {axis} is out of bounds for an array with dimension \"\n+            f\"{num_dims}.\"\n+        )\n+    if axis < 0:\n+        axis = axis + num_dims\n+    return axis\n+\n+\n+def standardize_axis_for_numpy(axis):\n+    \"\"\"Standardize an axis to a tuple if it is a list in the numpy backend.\"\"\"\n+    return tuple(axis) if isinstance(axis, list) else axis\n+\n+\n+def to_tuple_or_list(value):\n+    \"\"\"Convert the non-`None` value to either a tuple or a list.\"\"\"\n+    if value is None:\n+        return value\n+    if not isinstance(value, (int, tuple, list)):\n+        raise ValueError(\n+            \"`value` must be an integer, tuple or list. \"\n+            f\"Received: value={value}\"\n+        )\n+    if isinstance(value, int):\n+        return (value,)\n+    return value\n\n@@ -6,6 +6,8 @@ import jax.numpy as jnp\n \n from keras.backend import config\n from keras.backend.common import dtypes\n+from keras.backend.common.backend_utils import canonicalize_axis\n+from keras.backend.common.backend_utils import to_tuple_or_list\n from keras.backend.common.variables import standardize_dtype\n from keras.backend.jax import sparse\n from keras.backend.jax.core import cast\n@@ -367,14 +369,7 @@ def concatenate(xs, axis=0):\n     bcoo_count = builtins.sum(isinstance(x, jax_sparse.BCOO) for x in xs)\n     if bcoo_count:\n         if bcoo_count == len(xs):\n-            ndim = len(xs[0].shape)\n-            if not -ndim <= axis < ndim:\n-                raise ValueError(\n-                    f\"In `axis`, axis {axis} is out of bounds for array \"\n-                    f\"of dimension {ndim}\"\n-                )\n-            if axis < 0:\n-                axis = axis + ndim\n+            axis = canonicalize_axis(axis, len(xs[0].shape))\n             return jax_sparse.bcoo_concatenate(xs, dimension=axis)\n         else:\n             xs = [\n@@ -1040,8 +1035,7 @@ def squeeze(x, axis=None):\n     if isinstance(x, jax_sparse.BCOO):\n         if axis is None:\n             axis = tuple(i for i, d in enumerate(x.shape) if d == 1)\n-        elif isinstance(axis, int):\n-            axis = (axis,)\n+        axis = to_tuple_or_list(axis)\n         return jax_sparse.bcoo_squeeze(x, dimensions=axis)\n     return jnp.squeeze(x, axis=axis)\n \n@@ -1055,11 +1049,8 @@ def transpose(x, axes=None):\n         else:\n             permutation = []\n             for a in axes:\n-                if not -num_dims <= a < num_dims:\n-                    raise ValueError(\n-                        f\"axis {a} out of bounds for tensor of rank {num_dims}\"\n-                    )\n-                permutation.append(a if a >= 0 else a + num_dims)\n+                a = canonicalize_axis(a, num_dims)\n+                permutation.append(a)\n         return jax_sparse.bcoo_transpose(x, permutation=permutation)\n     return jnp.transpose(x, axes=axes)\n \n\n@@ -4,6 +4,7 @@ import tree\n from keras.backend import config\n from keras.backend import standardize_dtype\n from keras.backend.common import dtypes\n+from keras.backend.common.backend_utils import standardize_axis_for_numpy\n from keras.backend.numpy.core import convert_to_tensor\n \n \n@@ -82,7 +83,7 @@ def multiply(x1, x2):\n \n \n def mean(x, axis=None, keepdims=False):\n-    axis = tuple(axis) if isinstance(axis, list) else axis\n+    axis = standardize_axis_for_numpy(axis)\n     x = convert_to_tensor(x)\n     ori_dtype = standardize_dtype(x.dtype)\n     if \"int\" in ori_dtype or ori_dtype == \"bool\":\n@@ -93,7 +94,7 @@ def mean(x, axis=None, keepdims=False):\n \n \n def max(x, axis=None, keepdims=False, initial=None):\n-    axis = tuple(axis) if isinstance(axis, list) else axis\n+    axis = standardize_axis_for_numpy(axis)\n     return np.max(x, axis=axis, keepdims=keepdims, initial=initial)\n \n \n@@ -116,27 +117,27 @@ def abs(x):\n \n \n def all(x, axis=None, keepdims=False):\n-    axis = tuple(axis) if isinstance(axis, list) else axis\n+    axis = standardize_axis_for_numpy(axis)\n     return np.all(x, axis=axis, keepdims=keepdims)\n \n \n def any(x, axis=None, keepdims=False):\n-    axis = tuple(axis) if isinstance(axis, list) else axis\n+    axis = standardize_axis_for_numpy(axis)\n     return np.any(x, axis=axis, keepdims=keepdims)\n \n \n def amax(x, axis=None, keepdims=False):\n-    axis = tuple(axis) if isinstance(axis, list) else axis\n+    axis = standardize_axis_for_numpy(axis)\n     return np.amax(x, axis=axis, keepdims=keepdims)\n \n \n def amin(x, axis=None, keepdims=False):\n-    axis = tuple(axis) if isinstance(axis, list) else axis\n+    axis = standardize_axis_for_numpy(axis)\n     return np.amin(x, axis=axis, keepdims=keepdims)\n \n \n def append(x1, x2, axis=None):\n-    axis = tuple(axis) if isinstance(axis, list) else axis\n+    axis = standardize_axis_for_numpy(axis)\n     x1 = convert_to_tensor(x1)\n     x2 = convert_to_tensor(x2)\n     dtype = dtypes.result_type(x1.dtype, x2.dtype)\n@@ -227,17 +228,17 @@ def arctanh(x):\n \n \n def argmax(x, axis=None):\n-    axis = tuple(axis) if isinstance(axis, list) else axis\n+    axis = standardize_axis_for_numpy(axis)\n     return np.argmax(x, axis=axis).astype(\"int32\")\n \n \n def argmin(x, axis=None):\n-    axis = tuple(axis) if isinstance(axis, list) else axis\n+    axis = standardize_axis_for_numpy(axis)\n     return np.argmin(x, axis=axis).astype(\"int32\")\n \n \n def argsort(x, axis=-1):\n-    axis = tuple(axis) if isinstance(axis, list) else axis\n+    axis = standardize_axis_for_numpy(axis)\n     return np.argsort(x, axis=axis).astype(\"int32\")\n \n \n@@ -246,7 +247,7 @@ def array(x, dtype=None):\n \n \n def average(x, axis=None, weights=None):\n-    axis = tuple(axis) if isinstance(axis, list) else axis\n+    axis = standardize_axis_for_numpy(axis)\n     x = convert_to_tensor(x)\n     dtypes_to_resolve = [x.dtype, float]\n     if weights is not None:\n@@ -311,7 +312,7 @@ def clip(x, x_min, x_max):\n \n \n def concatenate(xs, axis=0):\n-    axis = tuple(axis) if isinstance(axis, list) else axis\n+    axis = standardize_axis_for_numpy(axis)\n     dtype_set = set([getattr(x, \"dtype\", type(x)) for x in xs])\n     if len(dtype_set) > 1:\n         dtype = dtypes.result_type(*dtype_set)\n@@ -354,14 +355,14 @@ def cosh(x):\n \n \n def count_nonzero(x, axis=None):\n-    axis = tuple(axis) if isinstance(axis, list) else axis\n+    axis = standardize_axis_for_numpy(axis)\n     # np.count_nonzero will return python int when axis=None, so we need\n     # to convert_to_tensor\n     return convert_to_tensor(np.count_nonzero(x, axis=axis)).astype(\"int32\")\n \n \n def cross(x1, x2, axisa=-1, axisb=-1, axisc=-1, axis=None):\n-    axis = tuple(axis) if isinstance(axis, list) else axis\n+    axis = standardize_axis_for_numpy(axis)\n     x1 = convert_to_tensor(x1)\n     x2 = convert_to_tensor(x2)\n     dtype = dtypes.result_type(x1.dtype, x2.dtype)\n@@ -378,7 +379,7 @@ def cross(x1, x2, axisa=-1, axisb=-1, axisc=-1, axis=None):\n \n \n def cumprod(x, axis=None, dtype=None):\n-    axis = tuple(axis) if isinstance(axis, list) else axis\n+    axis = standardize_axis_for_numpy(axis)\n     dtype = dtypes.result_type(dtype or x.dtype)\n     if dtype == \"bool\":\n         dtype = \"int32\"\n@@ -386,7 +387,7 @@ def cumprod(x, axis=None, dtype=None):\n \n \n def cumsum(x, axis=None, dtype=None):\n-    axis = tuple(axis) if isinstance(axis, list) else axis\n+    axis = standardize_axis_for_numpy(axis)\n     dtype = dtypes.result_type(dtype or x.dtype)\n     if dtype == \"bool\":\n         dtype = \"int32\"\n@@ -398,14 +399,9 @@ def diag(x, k=0):\n \n \n def diagonal(x, offset=0, axis1=0, axis2=1):\n-    axis1 = tuple(axis1) if isinstance(axis1, list) else axis1\n-    axis2 = tuple(axis2) if isinstance(axis2, list) else axis2\n-    return np.diagonal(\n-        x,\n-        offset=offset,\n-        axis1=axis1,\n-        axis2=axis2,\n-    )\n+    axis1 = standardize_axis_for_numpy(axis1)\n+    axis2 = standardize_axis_for_numpy(axis2)\n+    return np.diagonal(x, offset=offset, axis1=axis1, axis2=axis2)\n \n \n def diff(a, n=1, axis=-1):\n@@ -443,7 +439,7 @@ def exp(x):\n \n \n def expand_dims(x, axis):\n-    axis = tuple(axis) if isinstance(axis, list) else axis\n+    axis = standardize_axis_for_numpy(axis)\n     return np.expand_dims(x, axis)\n \n \n@@ -456,7 +452,7 @@ def expm1(x):\n \n \n def flip(x, axis=None):\n-    axis = tuple(axis) if isinstance(axis, list) else axis\n+    axis = standardize_axis_for_numpy(axis)\n     return np.flip(x, axis=axis)\n \n \n@@ -534,7 +530,7 @@ def less_equal(x1, x2):\n def linspace(\n     start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0\n ):\n-    axis = tuple(axis) if isinstance(axis, list) else axis\n+    axis = standardize_axis_for_numpy(axis)\n     if dtype is None:\n         dtypes_to_resolve = [\n             getattr(start, \"dtype\", type(start)),\n@@ -657,7 +653,7 @@ def meshgrid(*x, indexing=\"xy\"):\n \n \n def min(x, axis=None, keepdims=False, initial=None):\n-    axis = tuple(axis) if isinstance(axis, list) else axis\n+    axis = standardize_axis_for_numpy(axis)\n     return np.min(x, axis=axis, keepdims=keepdims, initial=initial)\n \n \n@@ -737,7 +733,7 @@ def pad(x, pad_width, mode=\"constant\", constant_values=None):\n \n \n def prod(x, axis=None, keepdims=False, dtype=None):\n-    axis = tuple(axis) if isinstance(axis, list) else axis\n+    axis = standardize_axis_for_numpy(axis)\n     x = convert_to_tensor(x)\n     if dtype is None:\n         dtype = dtypes.result_type(x.dtype)\n@@ -749,7 +745,7 @@ def prod(x, axis=None, keepdims=False, dtype=None):\n \n \n def quantile(x, q, axis=None, method=\"linear\", keepdims=False):\n-    axis = tuple(axis) if isinstance(axis, list) else axis\n+    axis = standardize_axis_for_numpy(axis)\n     x = convert_to_tensor(x)\n \n     ori_dtype = standardize_dtype(x.dtype)\n@@ -818,17 +814,17 @@ def size(x):\n \n \n def sort(x, axis=-1):\n-    axis = tuple(axis) if isinstance(axis, list) else axis\n+    axis = standardize_axis_for_numpy(axis)\n     return np.sort(x, axis=axis)\n \n \n def split(x, indices_or_sections, axis=0):\n-    axis = tuple(axis) if isinstance(axis, list) else axis\n+    axis = standardize_axis_for_numpy(axis)\n     return np.split(x, indices_or_sections, axis=axis)\n \n \n def stack(x, axis=0):\n-    axis = tuple(axis) if isinstance(axis, list) else axis\n+    axis = standardize_axis_for_numpy(axis)\n     dtype_set = set([getattr(a, \"dtype\", type(a)) for a in x])\n     if len(dtype_set) > 1:\n         dtype = dtypes.result_type(*dtype_set)\n@@ -837,7 +833,7 @@ def stack(x, axis=0):\n \n \n def std(x, axis=None, keepdims=False):\n-    axis = tuple(axis) if isinstance(axis, list) else axis\n+    axis = standardize_axis_for_numpy(axis)\n     x = convert_to_tensor(x)\n     ori_dtype = standardize_dtype(x.dtype)\n     if \"int\" in ori_dtype or ori_dtype == \"bool\":\n@@ -850,12 +846,12 @@ def swapaxes(x, axis1, axis2):\n \n \n def take(x, indices, axis=None):\n-    axis = tuple(axis) if isinstance(axis, list) else axis\n+    axis = standardize_axis_for_numpy(axis)\n     return np.take(x, indices, axis=axis)\n \n \n def take_along_axis(x, indices, axis=None):\n-    axis = tuple(axis) if isinstance(axis, list) else axis\n+    axis = standardize_axis_for_numpy(axis)\n     return np.take_along_axis(x, indices, axis=axis)\n \n \n@@ -898,8 +894,8 @@ def tile(x, repeats):\n \n \n def trace(x, offset=0, axis1=0, axis2=1):\n-    axis1 = tuple(axis1) if isinstance(axis1, list) else axis1\n-    axis2 = tuple(axis2) if isinstance(axis2, list) else axis2\n+    axis1 = standardize_axis_for_numpy(axis1)\n+    axis2 = standardize_axis_for_numpy(axis2)\n     x = convert_to_tensor(x)\n     dtype = standardize_dtype(x.dtype)\n     if dtype not in (\"int64\", \"uint32\", \"uint64\"):\n@@ -1027,7 +1023,7 @@ def sqrt(x):\n \n \n def squeeze(x, axis=None):\n-    axis = tuple(axis) if isinstance(axis, list) else axis\n+    axis = standardize_axis_for_numpy(axis)\n     return np.squeeze(x, axis=axis)\n \n \n@@ -1037,7 +1033,7 @@ def transpose(x, axes=None):\n \n \n def var(x, axis=None, keepdims=False):\n-    axis = tuple(axis) if isinstance(axis, list) else axis\n+    axis = standardize_axis_for_numpy(axis)\n     x = convert_to_tensor(x)\n     compute_dtype = dtypes.result_type(x.dtype, \"float32\")\n     result_dtype = dtypes.result_type(x.dtype, float)\n@@ -1047,7 +1043,7 @@ def var(x, axis=None, keepdims=False):\n \n \n def sum(x, axis=None, keepdims=False):\n-    axis = tuple(axis) if isinstance(axis, list) else axis\n+    axis = standardize_axis_for_numpy(axis)\n     dtype = standardize_dtype(x.dtype)\n     # follow jax's rule\n     if dtype in (\"bool\", \"int8\", \"int16\"):\n\n@@ -13,6 +13,8 @@ from tensorflow.python.ops.linalg.sparse import sparse_csr_matrix_ops\n from keras.backend import config\n from keras.backend import standardize_dtype\n from keras.backend.common import dtypes\n+from keras.backend.common.backend_utils import canonicalize_axis\n+from keras.backend.common.backend_utils import to_tuple_or_list\n from keras.backend.tensorflow import sparse\n from keras.backend.tensorflow.core import convert_to_tensor\n \n@@ -426,16 +428,15 @@ def mean(x, axis=None, keepdims=False):\n             sum = tf.reduce_sum(x.values, keepdims=keepdims)\n             return sum / tf.cast(tf.reduce_prod(x.dense_shape), dtype=sum.dtype)\n \n-        if isinstance(axis, int):\n-            axis = [axis]\n-        elif not axis:\n+        axis = to_tuple_or_list(axis)\n+        if not axis:\n             # Empty axis tuple, this is a no-op\n             return x\n \n         dense_shape = tf.convert_to_tensor(x.dense_shape)\n         rank = tf.shape(dense_shape)[0]\n         # Normalize axis: convert negative values and sort\n-        axis = [rank + a if a < 0 else a for a in axis]\n+        axis = [canonicalize_axis(a, rank) for a in axis]\n         axis.sort()\n \n         if axis == [0]:\n@@ -700,8 +701,7 @@ def array(x, dtype=None):\n \n def average(x, axis=None, weights=None):\n     x = convert_to_tensor(x)\n-    if not isinstance(axis, (list, tuple)):\n-        axis = (axis,)\n+    axis = to_tuple_or_list(axis)\n     dtypes_to_resolve = [x.dtype, float]\n     if weights is not None:\n         weights = convert_to_tensor(weights)\n@@ -715,6 +715,9 @@ def average(x, axis=None, weights=None):\n     x = tf.cast(x, compute_dtype)\n     if weights is not None:\n         weights = tf.cast(weights, compute_dtype)\n+    if axis is None:\n+        x = tfnp.average(x, weights=weights, axis=None)\n+        return tf.cast(x, result_dtype)\n     for a in axis:\n         # `tfnp.average` does not handle multiple axes.\n         x = tfnp.average(x, weights=weights, axis=a)\n@@ -937,13 +940,21 @@ def exp(x):\n \n \n def expand_dims(x, axis):\n+    x = convert_to_tensor(x)\n+    axis = to_tuple_or_list(axis)\n+    out_ndim = len(x.shape) + len(axis)\n+    axis = sorted([canonicalize_axis(a, out_ndim) for a in axis])\n     if isinstance(x, tf.SparseTensor):\n         from keras.ops.operation_utils import compute_expand_dims_output_shape\n \n-        output = tf.sparse.expand_dims(x, axis)\n-        output.set_shape(compute_expand_dims_output_shape(x.shape, axis))\n-        return output\n-    return tf.expand_dims(x, axis)\n+        output_shape = compute_expand_dims_output_shape(x.shape, axis)\n+        for a in axis:\n+            x = tf.sparse.expand_dims(x, a)\n+        x.set_shape(output_shape)\n+        return x\n+    for a in axis:\n+        x = tf.expand_dims(x, a)\n+    return x\n \n \n @sparse.elementwise_unary\n@@ -1385,9 +1396,8 @@ def _quantile(x, q, axis=None, method=\"linear\", keepdims=False):\n         y = tf.reshape(x, [-1])\n     else:\n         x_ndims = len(x.shape)\n-\n         # _make_static_axis_non_negative_list\n-        axis = list(map(lambda x: x if x >= 0 else x + x_ndims, axis))\n+        axis = [canonicalize_axis(a, x_ndims) for a in axis]\n \n         # _move_dims_to_flat_end\n         other_dims = sorted(set(range(x_ndims)).difference(axis))\n@@ -1483,11 +1493,9 @@ def _quantile(x, q, axis=None, method=\"linear\", keepdims=False):\n \n \n def quantile(x, q, axis=None, method=\"linear\", keepdims=False):\n-    if isinstance(axis, int):\n-        axis = [axis]\n-\n     x = convert_to_tensor(x)\n     q = convert_to_tensor(q)\n+    axis = to_tuple_or_list(axis)\n     compute_dtype = dtypes.result_type(x.dtype, float)\n     x = tf.cast(x, compute_dtype)\n     return _quantile(x, q, axis=axis, method=method, keepdims=keepdims)\n@@ -1902,21 +1910,23 @@ def sqrt(x):\n \n \n def squeeze(x, axis=None):\n-    if isinstance(x, tf.SparseTensor):\n+    x = convert_to_tensor(x)\n+    axis = to_tuple_or_list(axis)\n     static_shape = x.shape.as_list()\n     if axis is not None:\n-            if static_shape[axis] != 1:\n+        for a in axis:\n+            if static_shape[a] != 1:\n                 raise ValueError(\n-                    f\"Cannot squeeze axis {axis}, because the \"\n+                    f\"Cannot squeeze axis={a}, because the \"\n                     \"dimension is not 1.\"\n                 )\n-            if axis < 0:\n-                axis += len(static_shape)\n+        axis = sorted([canonicalize_axis(a, len(static_shape)) for a in axis])\n+    if isinstance(x, tf.SparseTensor):\n         dynamic_shape = tf.shape(x)\n         new_shape = []\n         gather_indices = []\n         for i, dim in enumerate(static_shape):\n-            if not (dim == 1 if axis is None else i == axis):\n+            if not (dim == 1 if axis is None else i in axis):\n                 new_shape.append(dim if dim is not None else dynamic_shape[i])\n                 gather_indices.append(i)\n         new_indices = tf.gather(x.indices, gather_indices, axis=1)\n\n@@ -6,6 +6,8 @@ import torch\n from keras.backend import KerasTensor\n from keras.backend import config\n from keras.backend.common import dtypes\n+from keras.backend.common.backend_utils import canonicalize_axis\n+from keras.backend.common.backend_utils import to_tuple_or_list\n from keras.backend.common.variables import standardize_dtype\n from keras.backend.torch.core import cast\n from keras.backend.torch.core import convert_to_tensor\n@@ -119,8 +121,7 @@ def mean(x, axis=None, keepdims=False):\n     if axis == () or axis == []:\n         # Torch handles the empty axis case differently from numpy.\n         return x\n-    elif isinstance(axis, int):\n-        axis = (axis,)  # see [NB] below\n+    axis = to_tuple_or_list(axis)  # see [NB] below\n \n     ori_dtype = standardize_dtype(x.dtype)\n     # torch.mean only supports floating point inputs\n@@ -208,8 +209,7 @@ def all(x, axis=None, keepdims=False):\n     x = convert_to_tensor(x)\n     if axis is None:\n         return cast(torch.all(x), \"bool\")\n-    if not isinstance(axis, (list, tuple)):\n-        axis = (axis,)\n+    axis = to_tuple_or_list(axis)\n     for a in axis:\n         # `torch.all` does not handle multiple axes.\n         x = torch.all(x, dim=a, keepdim=keepdims)\n@@ -220,8 +220,7 @@ def any(x, axis=None, keepdims=False):\n     x = convert_to_tensor(x)\n     if axis is None:\n         return cast(torch.any(x), \"bool\")\n-    if not isinstance(axis, (list, tuple)):\n-        axis = (axis,)\n+    axis = to_tuple_or_list(axis)\n     for a in axis:\n         # `torch.any` does not handle multiple axes.\n         x = torch.any(x, dim=a, keepdim=keepdims)\n@@ -602,7 +601,12 @@ def exp(x):\n \n def expand_dims(x, axis):\n     x = convert_to_tensor(x)\n-    return torch.unsqueeze(x, dim=axis)\n+    axis = to_tuple_or_list(axis)\n+    out_ndim = len(x.shape) + len(axis)\n+    axis = sorted([canonicalize_axis(a, out_ndim) for a in axis])\n+    for a in axis:\n+        x = torch.unsqueeze(x, dim=a)\n+    return x\n \n \n def expm1(x):\n@@ -617,8 +621,7 @@ def flip(x, axis=None):\n     x = convert_to_tensor(x)\n     if axis is None:\n         axis = tuple(range(x.ndim))\n-    if isinstance(axis, int):\n-        axis = (axis,)\n+    axis = to_tuple_or_list(axis)\n     return torch.flip(x, dims=axis)\n \n \n@@ -892,7 +895,7 @@ def median(x, axis=None, keepdims=False):\n         y = reshape(x, [-1])\n     else:\n         # transpose\n-        axis = list(map(lambda a: a if a >= 0 else a + x.ndim, axis))\n+        axis = [canonicalize_axis(a, x.ndim) for a in axis]\n         other_dims = sorted(set(range(x.ndim)).difference(axis))\n         perm = other_dims + list(axis)\n         x_permed = torch.permute(x, dims=perm)\n@@ -1072,8 +1075,7 @@ def prod(x, axis=None, keepdims=False, dtype=None):\n         compute_dtype = \"float32\"\n     if axis is None:\n         return cast(torch.prod(x, dtype=to_torch_dtype(compute_dtype)), dtype)\n-    if not isinstance(axis, (list, tuple)):\n-        axis = (axis,)\n+    axis = to_tuple_or_list(axis)\n     for a in axis:\n         # `torch.prod` does not handle multiple axes.\n         x = cast(\n@@ -1086,11 +1088,9 @@ def prod(x, axis=None, keepdims=False, dtype=None):\n \n \n def quantile(x, q, axis=None, method=\"linear\", keepdims=False):\n-    if isinstance(axis, int):\n-        axis = [axis]\n-\n     x = convert_to_tensor(x)\n     q = convert_to_tensor(q)\n+    axis = to_tuple_or_list(axis)\n \n     compute_dtype = dtypes.result_type(x.dtype, \"float32\")\n     result_dtype = dtypes.result_type(x.dtype, float)\n@@ -1105,7 +1105,7 @@ def quantile(x, q, axis=None, method=\"linear\", keepdims=False):\n         y = reshape(x, [-1])\n     else:\n         # transpose\n-        axis = list(map(lambda a: a if a >= 0 else a + x.ndim, axis))\n+        axis = [canonicalize_axis(a, x.ndim) for a in axis]\n         other_dims = sorted(set(range(x.ndim)).difference(axis))\n         perm = other_dims + list(axis)\n         x_permed = torch.permute(x, dims=perm)\n@@ -1266,8 +1266,7 @@ def take(x, indices, axis=None):\n         x = torch.reshape(x, (-1,))\n         axis = 0\n     if axis is not None:\n-        # make sure axis is non-negative\n-        axis = len(x.shape) + axis if axis < 0 else axis\n+        axis = canonicalize_axis(axis, x.ndim)\n         shape = x.shape[:axis] + indices.shape + x.shape[axis + 1 :]\n         # ravel the `indices` since `index_select` expects `indices`\n         # to be a vector (1-D tensor).\n\n@@ -151,6 +151,8 @@ from keras.api_export import keras_export\n from keras.backend import KerasTensor\n from keras.backend import any_symbolic_tensors\n from keras.backend.common import dtypes\n+from keras.backend.common.backend_utils import canonicalize_axis\n+from keras.backend.common.backend_utils import to_tuple_or_list\n from keras.ops import operation_utils\n from keras.ops.operation import Operation\n from keras.ops.operation_utils import broadcast_shapes\n@@ -2499,10 +2501,10 @@ def exp(x):\n class ExpandDims(Operation):\n     def __init__(self, axis):\n         super().__init__()\n-        if isinstance(axis, list):\n+        if not isinstance(axis, (int, tuple, list)):\n             raise ValueError(\n                 \"The `axis` argument to `expand_dims` should be an integer, \"\n-                f\"but received a list: {axis}.\"\n+                f\"tuple or list. Received axis={axis}\"\n             )\n         self.axis = axis\n \n@@ -5740,16 +5742,20 @@ class Squeeze(Operation):\n     def compute_output_spec(self, x):\n         input_shape = list(x.shape)\n         sparse = getattr(x, \"sparse\", False)\n-        if self.axis is None:\n+        axis = to_tuple_or_list(self.axis)\n+        if axis is None:\n             output_shape = list(filter((1).__ne__, input_shape))\n             return KerasTensor(output_shape, dtype=x.dtype, sparse=sparse)\n         else:\n-            if input_shape[self.axis] != 1:\n+            for a in axis:\n+                if input_shape[a] != 1:\n                     raise ValueError(\n-                    f\"Cannot squeeze axis {self.axis}, because the dimension \"\n+                        f\"Cannot squeeze axis {a}, because the dimension \"\n                         \"is not 1.\"\n                     )\n-            del input_shape[self.axis]\n+            axis = [canonicalize_axis(a, len(input_shape)) for a in axis]\n+            for a in sorted(axis, reverse=True):\n+                del input_shape[a]\n             return KerasTensor(input_shape, dtype=x.dtype, sparse=sparse)\n \n \n\n@@ -2,6 +2,7 @@ import contextlib\n import functools\n import itertools\n import math\n+import warnings\n \n import numpy as np\n import pytest\n@@ -929,6 +930,13 @@ class NumpyOneInputOpsDynamicShapeTest(testing.TestCase):\n             x = KerasTensor((None, 1))\n             knp.squeeze(x, axis=0)\n \n+        # Multiple axes\n+        x = KerasTensor((None, 1, 1, 1))\n+        self.assertEqual(knp.squeeze(x, (1, 2)).shape, (None, 1))\n+        self.assertEqual(knp.squeeze(x, (-1, -2)).shape, (None, 1))\n+        self.assertEqual(knp.squeeze(x, (1, 2, 3)).shape, (None,))\n+        self.assertEqual(knp.squeeze(x, (-1, 1)).shape, (None, 1))\n+\n     def test_transpose(self):\n         x = KerasTensor((None, 3))\n         self.assertEqual(knp.transpose(x).shape, (3, None))\n@@ -1143,6 +1151,11 @@ class NumpyOneInputOpsDynamicShapeTest(testing.TestCase):\n         self.assertEqual(knp.expand_dims(x, 1).shape, (None, 1, 3))\n         self.assertEqual(knp.expand_dims(x, -2).shape, (None, 1, 3))\n \n+        # Multiple axes\n+        self.assertEqual(knp.expand_dims(x, (1, 2)).shape, (None, 1, 1, 3))\n+        self.assertEqual(knp.expand_dims(x, (-1, -2)).shape, (None, 3, 1, 1))\n+        self.assertEqual(knp.expand_dims(x, (-1, 1)).shape, (None, 1, 3, 1))\n+\n     def test_expm1(self):\n         x = KerasTensor((None, 3))\n         self.assertEqual(knp.expm1(x).shape, (None, 3))\n@@ -1479,6 +1492,13 @@ class NumpyOneInputOpsStaticShapeTest(testing.TestCase):\n         with self.assertRaises(ValueError):\n             knp.squeeze(x, axis=0)\n \n+        # Multiple axes\n+        x = KerasTensor((2, 1, 1, 1))\n+        self.assertEqual(knp.squeeze(x, (1, 2)).shape, (2, 1))\n+        self.assertEqual(knp.squeeze(x, (-1, -2)).shape, (2, 1))\n+        self.assertEqual(knp.squeeze(x, (1, 2, 3)).shape, (2,))\n+        self.assertEqual(knp.squeeze(x, (-1, 1)).shape, (2, 1))\n+\n     def test_transpose(self):\n         x = KerasTensor((2, 3))\n         self.assertEqual(knp.transpose(x).shape, (3, 2))\n@@ -1650,6 +1670,11 @@ class NumpyOneInputOpsStaticShapeTest(testing.TestCase):\n         self.assertEqual(knp.expand_dims(x, 1).shape, (2, 1, 3, 4))\n         self.assertEqual(knp.expand_dims(x, -2).shape, (2, 3, 1, 4))\n \n+        # Multiple axes\n+        self.assertEqual(knp.expand_dims(x, (1, 2)).shape, (2, 1, 1, 3, 4))\n+        self.assertEqual(knp.expand_dims(x, (-1, -2)).shape, (2, 3, 4, 1, 1))\n+        self.assertEqual(knp.expand_dims(x, (-1, 1)).shape, (2, 1, 3, 4, 1))\n+\n     def test_expm1(self):\n         x = KerasTensor((2, 3))\n         self.assertEqual(knp.expm1(x).shape, (2, 3))\n@@ -2852,6 +2877,18 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(knp.Squeeze()(x), np.squeeze(x))\n         self.assertAllClose(knp.Squeeze(axis=0)(x), np.squeeze(x, axis=0))\n \n+        # Multiple axes\n+        x = np.ones([2, 1, 1, 1])\n+        self.assertAllClose(knp.squeeze(x, (1, 2)), np.squeeze(x, (1, 2)))\n+        self.assertAllClose(knp.squeeze(x, (-1, -2)), np.squeeze(x, (-1, -2)))\n+        self.assertAllClose(knp.squeeze(x, (1, 2, 3)), np.squeeze(x, (1, 2, 3)))\n+        self.assertAllClose(knp.squeeze(x, (-1, 1)), np.squeeze(x, (-1, 1)))\n+\n+        self.assertAllClose(knp.Squeeze((1, 2))(x), np.squeeze(x, (1, 2)))\n+        self.assertAllClose(knp.Squeeze((-1, -2))(x), np.squeeze(x, (-1, -2)))\n+        self.assertAllClose(knp.Squeeze((1, 2, 3))(x), np.squeeze(x, (1, 2, 3)))\n+        self.assertAllClose(knp.Squeeze((-1, 1))(x), np.squeeze(x, (-1, 1)))\n+\n     def test_transpose(self):\n         x = np.ones([1, 2, 3, 4, 5])\n         self.assertAllClose(knp.transpose(x), np.transpose(x))\n@@ -3291,6 +3328,27 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(knp.ExpandDims(1)(x), np.expand_dims(x, 1))\n         self.assertAllClose(knp.ExpandDims(-2)(x), np.expand_dims(x, -2))\n \n+        # Multiple axes\n+        self.assertAllClose(\n+            knp.expand_dims(x, (1, 2)), np.expand_dims(x, (1, 2))\n+        )\n+        self.assertAllClose(\n+            knp.expand_dims(x, (-1, -2)), np.expand_dims(x, (-1, -2))\n+        )\n+        self.assertAllClose(\n+            knp.expand_dims(x, (-1, 1)), np.expand_dims(x, (-1, 1))\n+        )\n+\n+        self.assertAllClose(\n+            knp.ExpandDims((1, 2))(x), np.expand_dims(x, (1, 2))\n+        )\n+        self.assertAllClose(\n+            knp.ExpandDims((-1, -2))(x), np.expand_dims(x, (-1, -2))\n+        )\n+        self.assertAllClose(\n+            knp.ExpandDims((-1, 1))(x), np.expand_dims(x, (-1, 1))\n+        )\n+\n     def test_expm1(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n         self.assertAllClose(knp.expm1(x), np.expm1(x))\n@@ -3955,7 +4013,7 @@ class NumpyArrayCreateOpsCorrectnessTest(testing.TestCase):\n         self.assertAllClose(knp.Arange()(3, 7, 2), np.arange(3, 7, 2))\n \n         self.assertEqual(standardize_dtype(knp.arange(3).dtype), \"int32\")\n-        with pytest.warns(None) as record:\n+        with warnings.catch_warnings(record=True) as record:\n             knp.arange(3, dtype=\"int\")\n         self.assertEqual(len(record), 0)\n \n\n@@ -4,6 +4,8 @@ import numpy as np\n import tree\n \n from keras.api_export import keras_export\n+from keras.backend.common.backend_utils import canonicalize_axis\n+from keras.backend.common.backend_utils import to_tuple_or_list\n \n \n def broadcast_shapes(shape1, shape2):\n@@ -55,7 +57,7 @@ def compute_expand_dims_output_shape(input_shape, axis):\n \n     Args:\n         input_shape: Input shape.\n-        axis: int for the axis to expand.\n+        axis: int or sequence of ints for the axis to expand.\n \n     Returns:\n         Tuple of ints: The output shape after the `expand_dims` operation.\n@@ -63,9 +65,14 @@ def compute_expand_dims_output_shape(input_shape, axis):\n     input_shape = list(input_shape)\n     if axis is None:\n         axis = len(input_shape)\n-    elif axis < 0:\n-        axis = len(input_shape) + 1 + axis\n-    return tuple(input_shape[:axis] + [1] + input_shape[axis:])\n+    axis = to_tuple_or_list(axis)\n+    out_ndim = len(axis) + len(input_shape)\n+    axis = [canonicalize_axis(a, out_ndim) for a in axis]\n+    shape_iter = iter(input_shape)\n+    new_shape = [\n+        1 if ax in axis else next(shape_iter) for ax in range(out_ndim)\n+    ]\n+    return tuple(new_shape)\n \n \n def compute_pooling_output_shape(\n\n@@ -18,6 +18,23 @@ class OperationUtilsTest(testing.TestCase):\n         inputs = input_layer.Input(shape=(10,))\n         self.assertIs(operation_utils.get_source_inputs(inputs)[0], inputs)\n \n+    def test_compute_expand_dims_output_shape(self):\n+        input_shape = (2, 3, 4)\n+        axis = -1\n+        output_shape = operation_utils.compute_expand_dims_output_shape(\n+            input_shape, axis\n+        )\n+        expected_output_shape = (2, 3, 4, 1)\n+        self.assertEqual(output_shape, expected_output_shape)\n+\n+        input_shape = (2, 3, 4)\n+        axis = (1, -1)\n+        output_shape = operation_utils.compute_expand_dims_output_shape(\n+            input_shape, axis\n+        )\n+        expected_output_shape = (2, 1, 3, 4, 1)\n+        self.assertEqual(output_shape, expected_output_shape)\n+\n     def test_compute_pooling_output_shape(self):\n         input_shape = (1, 4, 4, 1)\n         pool_size = (2, 2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ec8c4e0e1ea08263d6c2d2c1a386c819214afa62", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 12 | Methods Changed: 4 | Complexity Δ (Sum/Max): 6/4 | Churn Δ: 44 | Churn Cumulative: 212 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -30,6 +30,7 @@ def image_dataset_from_directory(\n     interpolation=\"bilinear\",\n     follow_links=False,\n     crop_to_aspect_ratio=False,\n+    pad_to_aspect_ratio=False,\n     data_format=None,\n     verbose=True,\n ):\n@@ -114,6 +115,13 @@ def image_dataset_from_directory(\n             (of size `image_size`) that matches the target aspect ratio. By\n             default (`crop_to_aspect_ratio=False`), aspect ratio may not be\n             preserved.\n+        pad_to_aspect_ratio: If `True`, resize the images without aspect\n+            ratio distortion. When the original aspect ratio differs from the\n+            target aspect ratio, the output image will be padded so as to\n+            return the largest possible window in the image\n+            (of size `image_size`) that matches the target aspect ratio. By\n+            default (`pad_to_aspect_ratio=False`), aspect ratio may not be\n+            preserved.\n         data_format: If None uses keras.config.image_data_format()\n             otherwise either 'channel_last' or 'channel_first'.\n         verbose: Whether to display number information on classes and\n@@ -263,6 +271,7 @@ def image_dataset_from_directory(\n             num_classes=len(class_names) if class_names else 0,\n             interpolation=interpolation,\n             crop_to_aspect_ratio=crop_to_aspect_ratio,\n+            pad_to_aspect_ratio=pad_to_aspect_ratio,\n             data_format=data_format,\n         )\n \n@@ -275,6 +284,7 @@ def image_dataset_from_directory(\n             num_classes=len(class_names) if class_names else 0,\n             interpolation=interpolation,\n             crop_to_aspect_ratio=crop_to_aspect_ratio,\n+            pad_to_aspect_ratio=pad_to_aspect_ratio,\n             data_format=data_format,\n         )\n \n@@ -323,6 +333,7 @@ def image_dataset_from_directory(\n             num_classes=len(class_names) if class_names else 0,\n             interpolation=interpolation,\n             crop_to_aspect_ratio=crop_to_aspect_ratio,\n+            pad_to_aspect_ratio=pad_to_aspect_ratio,\n             data_format=data_format,\n         )\n \n@@ -355,6 +366,7 @@ def paths_and_labels_to_dataset(\n     interpolation,\n     data_format,\n     crop_to_aspect_ratio=False,\n+    pad_to_aspect_ratio=False,\n ):\n     \"\"\"Constructs a dataset of images and labels.\"\"\"\n     # TODO(fchollet): consider making num_parallel_calls settable\n@@ -365,6 +377,7 @@ def paths_and_labels_to_dataset(\n         interpolation,\n         data_format,\n         crop_to_aspect_ratio,\n+        pad_to_aspect_ratio,\n     )\n     img_ds = path_ds.map(\n         lambda x: load_image(x, *args), num_parallel_calls=tf.data.AUTOTUNE\n@@ -384,12 +397,20 @@ def load_image(\n     interpolation,\n     data_format,\n     crop_to_aspect_ratio=False,\n+    pad_to_aspect_ratio=False,\n ):\n     \"\"\"Load an image from a path and resize it.\"\"\"\n     img = tf.io.read_file(path)\n     img = tf.image.decode_image(\n         img, channels=num_channels, expand_animations=False\n     )\n+\n+    if pad_to_aspect_ratio and crop_to_aspect_ratio:\n+        raise ValueError(\n+            \"Only one of pad_to_aspect_ratio, crop_to_aspect_ratio\"\n+            \" can be set to True\"\n+        )\n+\n     if crop_to_aspect_ratio:\n         from keras.backend import tensorflow as tf_backend\n \n@@ -402,10 +423,17 @@ def load_image(\n             data_format=data_format,\n             backend_module=tf_backend,\n         )\n+    elif pad_to_aspect_ratio:\n+        img = tf.image.resize_with_pad(\n+            img, image_size[0], image_size[1], method=interpolation\n+        )\n+        if data_format == \"channels_first\":\n+            img = tf.transpose(img, (2, 0, 1))\n     else:\n         img = tf.image.resize(img, image_size, method=interpolation)\n         if data_format == \"channels_first\":\n             img = tf.transpose(img, (2, 0, 1))\n+\n     if data_format == \"channels_last\":\n         img.set_shape((image_size[0], image_size[1], num_channels))\n     else:\n\n@@ -370,6 +370,22 @@ class ImageDatasetFromDirectoryTest(testing.TestCase):\n         self.assertLen(batch, 2)\n         self.assertEqual(batch[0].shape, output_shape)\n \n+    def test_image_dataset_from_directory_pad_to_aspect_ratio(self):\n+        directory = self._prepare_directory(num_classes=2, count=5)\n+        dataset = image_dataset_utils.image_dataset_from_directory(\n+            directory,\n+            batch_size=5,\n+            image_size=(18, 18),\n+            pad_to_aspect_ratio=True,\n+        )\n+        if backend.config.image_data_format() == \"channels_last\":\n+            output_shape = [5, 18, 18, 3]\n+        else:\n+            output_shape = [5, 3, 18, 18]\n+        batch = next(iter(dataset))\n+        self.assertLen(batch, 2)\n+        self.assertEqual(batch[0].shape, output_shape)\n+\n     def test_image_dataset_from_directory_errors(self):\n         directory = self._prepare_directory(num_classes=3, count=5)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4f63678a72a2e7c16c32506189e5428d93d7b945", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 2 | Churn Cumulative: 64 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -100,7 +100,7 @@ def pack_x_y_sample_weight(x, y=None, sample_weight=None):\n         # there is no ambiguity. This also makes NumPy and Dataset\n         # consistent in that the user does not have to wrap their Dataset\n         # data in an unnecessary tuple.\n-        if not isinstance(x, tuple or list):\n+        if not isinstance(x, (tuple, list)):\n             return x\n         else:\n             return (x,)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#04eca3a23f6c46891bd71e89a4a990121958b965", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 40 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -83,7 +83,7 @@ def dropout(inputs, rate, noise_shape=None, seed=None):\n \n def shuffle(x, axis=0, seed=None):\n     seed = jax_draw_seed(seed)\n-    return jax.random.shuffle(seed, x, axis)\n+    return jax.random.permutation(seed, x, axis, independent=True)\n \n \n def gamma(shape, alpha, dtype=None, seed=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#404e8f3d57b1bc23588789408383acd7ec2c5911", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 455 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -42,7 +42,7 @@ def list_devices(device_type=None):\n \n \n @keras_export(\"keras.distribution.initialize\")\n-def initialize(job_addresses=None, num_processes=None, proceed_id=None):\n+def initialize(job_addresses=None, num_processes=None, process_id=None):\n     \"\"\"Initialize the distribution system for multi-host/process setting.\n \n     Calling `initialize` will prepare the backend for execution on multi-host\n@@ -126,9 +126,9 @@ def initialize(job_addresses=None, num_processes=None, proceed_id=None):\n         and \"KERAS_DISTRIBUTION_NUM_PROCESSES\" in os.environ\n     ):\n         num_processes = int(os.environ[\"KERAS_DISTRIBUTION_NUM_PROCESSES\"])\n-    if proceed_id is None and \"KERAS_DISTRIBUTION_PROCESS_ID\" in os.environ:\n-        proceed_id = int(os.environ[\"KERAS_DISTRIBUTION_PROCESS_ID\"])\n-    distribution_lib.initialize(job_addresses, num_processes, proceed_id)\n+    if process_id is None and \"KERAS_DISTRIBUTION_PROCESS_ID\" in os.environ:\n+        process_id = int(os.environ[\"KERAS_DISTRIBUTION_PROCESS_ID\"])\n+    distribution_lib.initialize(job_addresses, num_processes, process_id)\n \n \n @keras_export(\"keras.distribution.DeviceMesh\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0d0be6aadb7d9bfafaf8bea895789f4aa3b0abf7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 42 | Churn Cumulative: 66 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,6 +1,10 @@\n+import copy\n+import importlib\n+import os\n import sys\n \n from keras import backend as backend_module\n+from keras.api_export import keras_export\n from keras.backend.common import global_state\n \n \n@@ -82,3 +86,41 @@ class DynamicBackend:\n             from keras import backend as numpy_backend\n \n             return getattr(numpy_backend, name)\n+\n+\n+@keras_export(\"keras.config.set_backend\")\n+def set_backend(backend):\n+    \"\"\"Reload the backend (and the Keras package).\n+\n+    Example:\n+\n+    ```python\n+    keras.config.set_backend(\"jax\")\n+    ```\n+\n+    Note that this will **NOT** convert the type of any already\n+    instantiated objects, except for the `keras` module itself.\n+    Thus, any layers / tensors / etc. already created will no\n+    longer be usable without errors. It is strongly recommended **not**\n+    to keep around **any** Keras-originated objects instances created\n+    before calling `set_backend()`.\n+    \"\"\"\n+    os.environ[\"KERAS_BACKEND\"] = backend\n+    # Clear module cache.\n+    loaded_modules = [\n+        key for key in sys.modules.keys() if key.startswith(\"keras\")\n+    ]\n+    for key in loaded_modules:\n+        del sys.modules[key]\n+    # Reimport Keras with the new backend (set via KERAS_BACKEND).\n+    import keras\n+\n+    # Finally: refresh all imported Keras submodules.\n+    globs = copy.copy(globals())\n+    for key, value in globs.items():\n+        if value.__class__ == keras.__class__:\n+            if str(value).startswith(\"<module 'keras.\"):\n+                module_name = str(value)\n+                module_name = module_name[module_name.find(\"'\") + 1 :]\n+                module_name = module_name[: module_name.find(\"'\")]\n+                globals()[key] = importlib.import_module(module_name)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ffa9d52fa0539f39d09a3f80b4727ecd8948b6e0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 10 | Churn Cumulative: 170 | Contributors (this commit): 7 | Commits (past 90d): 9 | Contributors (cumulative): 9 | DMM Complexity: 0.0\n\nDIFF:\n@@ -53,6 +53,9 @@ class StatelessScope:\n                     \"all keys in argument `mapping` must be KerasVariable \"\n                     f\"instances. Received instead: {k}\"\n                 )\n+            if isinstance(v, KerasVariable):\n+                v = backend.cast(v.value, dtype=k.dtype)\n+            else:\n                 v = backend.convert_to_tensor(v, dtype=k.dtype)\n             if k.shape != v.shape:\n                 raise ValueError(\n\n@@ -111,7 +111,12 @@ def convert_to_tensor(x, dtype=None, sparse=None):\n             x = tf.convert_to_tensor(x)\n             return tf.cast(x, dtype)\n         return tf.convert_to_tensor(x, dtype=dtype)\n-    elif dtype is not None:\n+    elif dtype is not None and not x.dtype == dtype:\n+        if isinstance(x, tf.SparseTensor):\n+            x_shape = x.shape\n+            x = tf.cast(x, dtype)\n+            x.set_shape(x_shape)\n+            return x\n         return tf.cast(x, dtype=dtype)\n     else:\n         return x\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f189dece3cc1efd16100dd1462f650a6b7c66f9f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 6 | Churn Cumulative: 88 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -197,6 +197,8 @@ def gamma(shape, alpha, dtype=None, seed=None):\n     alpha = torch.broadcast_to(convert_to_tensor(alpha), shape)\n     beta = torch.ones(shape, device=get_device())\n     prev_rng_state = torch.random.get_rng_state()\n+    # Do not draw seed during symbolic execution\n+    if not get_device() == \"meta\":\n         first_seed, second_seed = draw_seed(seed)\n         torch.manual_seed(first_seed + second_seed)\n     gamma_distribution = torch.distributions.gamma.Gamma(alpha, beta)\n@@ -211,6 +213,8 @@ def binomial(shape, counts, probabilities, dtype=None, seed=None):\n     counts = torch.broadcast_to(convert_to_tensor(counts), shape)\n     probabilities = torch.broadcast_to(convert_to_tensor(probabilities), shape)\n     prev_rng_state = torch.random.get_rng_state()\n+    # Do not draw seed during symbolic execution\n+    if not get_device() == \"meta\":\n         first_seed, second_seed = draw_seed(seed)\n         torch.manual_seed(first_seed + second_seed)\n     binomial_distribution = torch.distributions.binomial.Binomial(\n@@ -227,6 +231,8 @@ def beta(shape, alpha, beta, dtype=None, seed=None):\n     alpha = torch.broadcast_to(convert_to_tensor(alpha), shape)\n     beta = torch.broadcast_to(convert_to_tensor(beta), shape)\n     prev_rng_state = torch.random.get_rng_state()\n+    # Do not draw seed during symbolic execution\n+    if not get_device() == \"meta\":\n         first_seed, second_seed = draw_seed(seed)\n         torch.manual_seed(first_seed + second_seed)\n     beta_distribution = torch.distributions.beta.Beta(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2c96829982db73f871285822f399642b028347a2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 149 | Lines Deleted: 28 | Files Changed: 1 | Hunks: 21 | Methods Changed: 8 | Complexity Δ (Sum/Max): 31/31 | Churn Δ: 177 | Churn Cumulative: 335 | Contributors (this commit): 7 | Commits (past 90d): 4 | Contributors (cumulative): 7 | DMM Complexity: 0.008547008547008548\n\nDIFF:\n@@ -15,7 +15,7 @@ from keras.backend.common.global_state import clear_session\n from keras.backend.common.keras_tensor import KerasTensor\n from keras.models import Model\n from keras.utils import traceback_utils\n-from keras.utils.shape_utils import map_shape_structure\n+from keras.utils.shape_utils import is_shape_tuple\n \n \n class TestCase(unittest.TestCase):\n@@ -119,7 +119,7 @@ class TestCase(unittest.TestCase):\n         layer_cls,\n         init_kwargs,\n         input_shape=None,\n-        input_dtype=\"float32\",\n+        input_dtype=None,\n         input_sparse=False,\n         input_data=None,\n         call_kwargs=None,\n@@ -196,13 +196,51 @@ class TestCase(unittest.TestCase):\n             )\n         if expected_mask_shape is not None and supports_masking is not True:\n             raise ValueError(\n-                \"\"\"In order to use expected_mask_shape, supports_masking\n-                must be True.\"\"\"\n+                \"In order to use expected_mask_shape, supports_masking \"\n+                \"must be True.\"\n             )\n \n         init_kwargs = init_kwargs or {}\n         call_kwargs = call_kwargs or {}\n \n+        if input_shape is not None and input_dtype is not None:\n+            if isinstance(input_shape, tuple) and is_shape_tuple(\n+                input_shape[0]\n+            ):\n+                self.assertIsInstance(input_dtype, tuple)\n+                self.assertEqual(\n+                    len(input_shape),\n+                    len(input_dtype),\n+                    msg=\"The number of input shapes and dtypes does not match\",\n+                )\n+            elif isinstance(input_shape, dict):\n+                self.assertIsInstance(input_dtype, dict)\n+                self.assertEqual(\n+                    set(input_shape.keys()),\n+                    set(input_dtype.keys()),\n+                    msg=\"The number of input shapes and dtypes does not match\",\n+                )\n+            elif isinstance(input_shape, list):\n+                self.assertIsInstance(input_dtype, list)\n+                self.assertEqual(\n+                    len(input_shape),\n+                    len(input_dtype),\n+                    msg=\"The number of input shapes and dtypes does not match\",\n+                )\n+            elif not isinstance(input_shape, tuple):\n+                raise ValueError(\"The type of input_shape is not supported\")\n+        if input_shape is not None and input_dtype is None:\n+            if isinstance(input_shape, tuple) and is_shape_tuple(\n+                input_shape[0]\n+            ):\n+                input_dtype = [\"float32\"] * len(input_shape)\n+            elif isinstance(input_shape, dict):\n+                input_dtype = {k: \"float32\" for k in input_shape.keys()}\n+            elif isinstance(input_shape, list):\n+                input_dtype = [\"float32\"] * len(input_shape)\n+            else:\n+                input_dtype = \"float32\"\n+\n         # Serialization test.\n         layer = layer_cls(**init_kwargs)\n         self.run_class_serialization_test(layer, custom_objects)\n@@ -244,7 +282,22 @@ class TestCase(unittest.TestCase):\n \n         def run_output_asserts(layer, output, eager=False):\n             if expected_output_shape is not None:\n-                if isinstance(expected_output_shape, tuple):\n+                if isinstance(expected_output_shape, tuple) and is_shape_tuple(\n+                    expected_output_shape[0]\n+                ):\n+                    self.assertIsInstance(output, tuple)\n+                    self.assertEqual(\n+                        len(output),\n+                        len(expected_output_shape),\n+                        msg=\"Unexpected number of outputs\",\n+                    )\n+                    output_shape = tuple(v.shape for v in output)\n+                    self.assertEqual(\n+                        expected_output_shape,\n+                        output_shape,\n+                        msg=\"Unexpected output shape\",\n+                    )\n+                elif isinstance(expected_output_shape, tuple):\n                     self.assertEqual(\n                         expected_output_shape,\n                         output.shape,\n@@ -257,9 +310,7 @@ class TestCase(unittest.TestCase):\n                         set(expected_output_shape.keys()),\n                         msg=\"Unexpected output dict keys\",\n                     )\n-                    output_shape = {\n-                        k: v.shape for k, v in expected_output_shape.items()\n-                    }\n+                    output_shape = {k: v.shape for k, v in output.items()}\n                     self.assertEqual(\n                         expected_output_shape,\n                         output_shape,\n@@ -272,13 +323,64 @@ class TestCase(unittest.TestCase):\n                         len(expected_output_shape),\n                         msg=\"Unexpected number of outputs\",\n                     )\n-                    output_shape = [v.shape for v in expected_output_shape]\n+                    output_shape = [v.shape for v in output]\n                     self.assertEqual(\n                         expected_output_shape,\n                         output_shape,\n                         msg=\"Unexpected output shape\",\n                     )\n+                else:\n+                    raise ValueError(\n+                        \"The type of expected_output_shape is not supported\"\n+                    )\n             if expected_output_dtype is not None:\n+                if isinstance(expected_output_dtype, tuple):\n+                    self.assertIsInstance(output, tuple)\n+                    self.assertEqual(\n+                        len(output),\n+                        len(expected_output_dtype),\n+                        msg=\"Unexpected number of outputs\",\n+                    )\n+                    output_dtype = tuple(\n+                        backend.standardize_dtype(v.dtype) for v in output\n+                    )\n+                    self.assertEqual(\n+                        expected_output_dtype,\n+                        output_dtype,\n+                        msg=\"Unexpected output dtype\",\n+                    )\n+                elif isinstance(expected_output_dtype, dict):\n+                    self.assertIsInstance(output, dict)\n+                    self.assertEqual(\n+                        set(output.keys()),\n+                        set(expected_output_dtype.keys()),\n+                        msg=\"Unexpected output dict keys\",\n+                    )\n+                    output_dtype = {\n+                        k: backend.standardize_dtype(v.dtype)\n+                        for k, v in output.items()\n+                    }\n+                    self.assertEqual(\n+                        expected_output_dtype,\n+                        output_dtype,\n+                        msg=\"Unexpected output dtype\",\n+                    )\n+                elif isinstance(expected_output_dtype, list):\n+                    self.assertIsInstance(output, list)\n+                    self.assertEqual(\n+                        len(output),\n+                        len(expected_output_dtype),\n+                        msg=\"Unexpected number of outputs\",\n+                    )\n+                    output_dtype = [\n+                        backend.standardize_dtype(v.dtype) for v in output\n+                    ]\n+                    self.assertEqual(\n+                        expected_output_dtype,\n+                        output_dtype,\n+                        msg=\"Unexpected output dtype\",\n+                    )\n+                else:\n                     output_dtype = tree.flatten(output)[0].dtype\n                     self.assertEqual(\n                         expected_output_dtype,\n@@ -435,27 +537,23 @@ def create_keras_tensors(input_shape, dtype, sparse):\n     if isinstance(input_shape, dict):\n         return {\n             utils.removesuffix(k, \"_shape\"): KerasTensor(\n-                v, dtype=dtype, sparse=sparse\n+                v, dtype=dtype[k], sparse=sparse\n             )\n             for k, v in input_shape.items()\n         }\n-    return map_shape_structure(\n-        lambda shape: KerasTensor(shape, dtype=dtype, sparse=sparse),\n+    return map_shape_dtype_structure(\n+        lambda shape, dt: KerasTensor(shape, dtype=dt, sparse=sparse),\n         input_shape,\n+        dtype,\n     )\n \n \n def create_eager_tensors(input_shape, dtype, sparse):\n     from keras.backend import random\n \n-    if dtype not in [\n-        \"float16\",\n-        \"float32\",\n-        \"float64\",\n-        \"int16\",\n-        \"int32\",\n-        \"int64\",\n-    ]:\n+    if set(tree.flatten(dtype)).difference(\n+        [\"float16\", \"float32\", \"float64\", \"int16\", \"int32\", \"int64\"]\n+    ):\n         raise ValueError(\n             \"dtype must be a standard float or int dtype. \"\n             f\"Received: dtype={dtype}\"\n@@ -465,18 +563,18 @@ def create_eager_tensors(input_shape, dtype, sparse):\n         if backend.backend() == \"tensorflow\":\n             import tensorflow as tf\n \n-            def create_fn(shape):\n+            def create_fn(shape, dt):\n                 rng = np.random.default_rng(0)\n-                x = (4 * rng.standard_normal(shape)).astype(dtype)\n+                x = (4 * rng.standard_normal(shape)).astype(dt)\n                 x = np.multiply(x, rng.random(shape) < 0.7)\n                 return tf.sparse.from_dense(x)\n \n         elif backend.backend() == \"jax\":\n             import jax.experimental.sparse as jax_sparse\n \n-            def create_fn(shape):\n+            def create_fn(shape, dt):\n                 rng = np.random.default_rng(0)\n-                x = (4 * rng.standard_normal(shape)).astype(dtype)\n+                x = (4 * rng.standard_normal(shape)).astype(dt)\n                 x = np.multiply(x, rng.random(shape) < 0.7)\n                 return jax_sparse.BCOO.fromdense(x, n_batch=1)\n \n@@ -487,14 +585,37 @@ def create_eager_tensors(input_shape, dtype, sparse):\n \n     else:\n \n-        def create_fn(shape):\n+        def create_fn(shape, dt):\n             return ops.cast(\n-                random.uniform(shape, dtype=\"float32\") * 3, dtype=dtype\n+                random.uniform(shape, dtype=\"float32\") * 3, dtype=dt\n             )\n \n     if isinstance(input_shape, dict):\n         return {\n-            utils.removesuffix(k, \"_shape\"): create_fn(v)\n+            utils.removesuffix(k, \"_shape\"): create_fn(v, dtype[k])\n             for k, v in input_shape.items()\n         }\n-    return map_shape_structure(create_fn, input_shape)\n+    return map_shape_dtype_structure(create_fn, input_shape, dtype)\n+\n+\n+def map_shape_dtype_structure(fn, shape, dtype):\n+    \"\"\"Variant of tree.map_structure that operates on shape tuples.\"\"\"\n+    if is_shape_tuple(shape):\n+        return fn(tuple(shape), dtype)\n+    if isinstance(shape, list):\n+        return [\n+            map_shape_dtype_structure(fn, s, d) for s, d in zip(shape, dtype)\n+        ]\n+    if isinstance(shape, tuple):\n+        return tuple(\n+            map_shape_dtype_structure(fn, s, d) for s, d in zip(shape, dtype)\n+        )\n+    if isinstance(shape, dict):\n+        return {\n+            k: map_shape_dtype_structure(fn, v, dtype[k])\n+            for k, v in shape.items()\n+        }\n+    else:\n+        raise ValueError(\n+            f\"Cannot map function to unknown objects {shape} and {dtype}\"\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
