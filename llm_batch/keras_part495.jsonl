{"custom_id": "keras#9a3d0c96fcb23d50c95ea78d118faaf4bb0064b4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 1 | Files Changed: 3 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 22 | Churn Cumulative: 16014 | Contributors (this commit): 121 | Commits (past 90d): 32 | Contributors (cumulative): 143 | DMM Complexity: 0.8125\n\nDIFF:\n@@ -2733,6 +2733,13 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n                 1.0 / batch_run_time,\n                 step=self._train_step,\n             )\n+\n+        # `logs` is a `RemoteValue` when using asynchronous strategies, for now\n+        # we just disable `update_freq` entirely in those cases.\n+        if isinstance(logs, dict):\n+            for name, value in logs.items():\n+                tf.summary.scalar(\"batch_\" + name, value, step=self._train_step)\n+\n         if not self._should_trace:\n             return\n \n\n@@ -3038,6 +3038,7 @@ class TestTensorBoardV2(test_combinations.TestCase):\n         self.assertEqual(\n             summary_file.scalars,\n             {\n+                _ObservedSummary(logdir=self.train_dir, tag=\"batch_loss\"),\n                 _ObservedSummary(logdir=self.train_dir, tag=\"epoch_loss\"),\n                 _ObservedSummary(logdir=self.validation_dir, tag=\"epoch_loss\"),\n                 _ObservedSummary(\n@@ -3100,6 +3101,7 @@ class TestTensorBoardV2(test_combinations.TestCase):\n         self.assertEqual(\n             summary_file.scalars,\n             {\n+                _ObservedSummary(logdir=self.train_dir, tag=\"batch_loss\"),\n                 _ObservedSummary(logdir=self.train_dir, tag=\"epoch_loss\"),\n                 _ObservedSummary(\n                     logdir=self.train_dir, tag=\"epoch_learning_rate\"\n@@ -3285,6 +3287,7 @@ class TestTensorBoardV2(test_combinations.TestCase):\n         self.assertEqual(\n             summary_file.scalars,\n             {\n+                _ObservedSummary(logdir=self.train_dir, tag=\"batch_loss\"),\n                 _ObservedSummary(logdir=self.train_dir, tag=\"epoch_loss\"),\n                 _ObservedSummary(logdir=self.validation_dir, tag=\"epoch_loss\"),\n                 _ObservedSummary(\n\n@@ -77,7 +77,17 @@ class DistributedTrainingTest(tf.test.TestCase):\n \n         x = tf.keras.utils.experimental.DatasetCreator(dataset_fn)\n \n-        model.fit(x, epochs=2, steps_per_epoch=10)\n+        model.fit(\n+            x,\n+            epochs=2,\n+            steps_per_epoch=10,\n+            callbacks=[\n+                tf.keras.callbacks.TensorBoard(\n+                    update_freq=5,\n+                    write_steps_per_second=True,\n+                )\n+            ],\n+        )\n \n \n if __name__ == \"__main__\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#86e9ed51317872e1dfc75b03c1e257638bd7b083", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 48 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 55 | Churn Cumulative: 9477 | Contributors (this commit): 115 | Commits (past 90d): 12 | Contributors (cumulative): 115 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,3 @@\n-# flake8: noqa\n # Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -2350,12 +2349,12 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n         write_steps_per_second: whether to log the training steps per second\n           into Tensorboard. This supports both epoch and batch frequency\n           logging.\n-        update_freq: **disabled**\n-\n-          Warning: Batch-level summary writing using `update_freq` is\n-          currently unsupported. A suggested workaround is shown in the\n-          [TensorBoard Scalars tutorial](https://www.tensorflow.org/tensorboard/scalars_and_keras#batch-level_logging). # pylint: disable=protected-access\n-\n+        update_freq: `'batch'` or `'epoch'` or integer. When using `'batch'`,\n+          writes the losses and metrics to TensorBoard after each batch. The\n+          same applies for `'epoch'`. If using an integer, let's say `1000`, the\n+          callback will write the metrics and losses to TensorBoard every 1000\n+          batches. Note that writing too frequently to TensorBoard can slow down\n+          your training.\n         profile_batch: Profile the batch(es) to sample compute characteristics.\n           profile_batch must be a non-negative integer or a tuple of integers.\n           A pair of positive integers signify a range of batches to profile.\n@@ -2377,6 +2376,48 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n     # Then run the tensorboard command to view the visualizations.\n     ```\n \n+    Custom batch-level summaries in a subclassed Model:\n+\n+    ```python\n+    class MyModel(tf.keras.Model):\n+\n+      def build(self, _):\n+        self.dense = tf.keras.layers.Dense(10)\n+\n+      def call(self, x):\n+        outputs = self.dense(x)\n+        tf.summary.histogram('outputs', outputs)\n+        return outputs\n+\n+    model = MyModel()\n+    model.compile('sgd', 'mse')\n+\n+    # Make sure to set `update_freq=N` to log a batch-level summary every N\n+    # batches.  In addition to any `tf.summary` contained in `Model.call`,\n+    # metrics added in `Model.compile` will be logged every N batches.\n+    tb_callback = tf.keras.callbacks.TensorBoard('./logs', update_freq=1)\n+    model.fit(x_train, y_train, callbacks=[tb_callback])\n+    ```\n+\n+    Custom batch-level summaries in a Functional API Model:\n+\n+    ```python\n+    def my_summary(x):\n+      tf.summary.histogram('x', x)\n+      return x\n+\n+    inputs = tf.keras.Input(10)\n+    x = tf.keras.layers.Dense(10)(inputs)\n+    outputs = tf.keras.layers.Lambda(my_summary)(x)\n+    model = tf.keras.Model(inputs, outputs)\n+    model.compile('sgd', 'mse')\n+\n+    # Make sure to set `update_freq=N` to log a batch-level summary every N\n+    # batches. In addition to any `tf.summary` contained in `Model.call`,\n+    # metrics added in `Model.compile` will be logged every N batches.\n+    tb_callback = tf.keras.callbacks.TensorBoard('./logs', update_freq=1)\n+    model.fit(x_train, y_train, callbacks=[tb_callback])\n+    ```\n \n     Profiling:\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6fed9116cb32d5cd9f10cfa38062cae4a27e4743", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 13 | Files Changed: 5 | Hunks: 10 | Methods Changed: 7 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 35 | Churn Cumulative: 7360 | Contributors (this commit): 12 | Commits (past 90d): 17 | Contributors (cumulative): 40 | DMM Complexity: 0.75\n\nDIFF:\n@@ -1214,6 +1214,7 @@ def gru_with_backend_selection(\n         return_sequences,\n     ):\n         \"\"\"Use cuDNN kernel when mask is none or strictly right padded.\"\"\"\n+\n         def cudnn_gru_fn():\n             return gpu_gru(\n                 inputs=inputs,\n@@ -1243,8 +1244,10 @@ def gru_with_backend_selection(\n                 return_sequences=return_sequences,\n             )\n \n-        return tf.cond(\n-            gru_lstm_utils.is_cudnn_supported_inputs(mask, time_major, sequence_lengths),\n+        return tf.__internal__.smart_cond.smart_cond(\n+            gru_lstm_utils.is_cudnn_supported_inputs(\n+                mask, time_major, sequence_lengths\n+            ),\n             true_fn=cudnn_gru_fn,\n             false_fn=standard_gru_fn,\n         )\n\n@@ -169,18 +169,19 @@ def has_fully_masked_sequence(mask):\n \n \n def is_cudnn_supported_inputs(mask, time_major, sequence_lengths):\n-    if tf.sysconfig.get_build_info()['is_rocm_build']:\n+    if tf.sysconfig.get_build_info()[\"is_rocm_build\"]:\n         if not time_major:\n-          return tf.constant(False)\n-       if mask!=None:\n+            return False\n+        if mask is not None:\n             return tf.reduce_all(mask)\n-       elif sequence_lengths!=None:\n-          return tf.math.equal(tf.reduce_min(sequence_lengths), tf.reduce_max(sequence_lengths))\n+        elif sequence_lengths is not None:\n+            return tf.math.equal(\n+                tf.reduce_min(sequence_lengths), tf.reduce_max(sequence_lengths)\n+            )\n         else:\n-          return tf.constant(True)\n-\n-    if mask==None:\n-        return tf.constant(True)\n+            return True\n+    if mask is None:\n+        return True\n     if time_major:\n         mask = tf.transpose(mask)\n \n\n@@ -610,6 +610,7 @@ class GRUGraphRewriteTest(test_combinations.TestCase):\n             existing_loss = loss_value\n \n         _, runtime_value = model.predict(x_train)\n+        if not tf.sysconfig.get_build_info()[\"is_rocm_build\"]:\n             if tf.test.is_gpu_available():\n                 self.assertEqual(runtime_value[0], gru_lstm_utils.RUNTIME_GPU)\n             else:\n\n@@ -1253,6 +1253,7 @@ def lstm_with_backend_selection(\n         return_sequences,\n     ):\n         \"\"\"Use cuDNN kernel when mask is none or strictly right padded.\"\"\"\n+\n         def cudnn_lstm_fn():\n             return gpu_lstm(\n                 inputs=inputs,\n@@ -1284,8 +1285,10 @@ def lstm_with_backend_selection(\n                 return_sequences=return_sequences,\n             )\n \n-        return tf.cond(\n-            gru_lstm_utils.is_cudnn_supported_inputs(mask, time_major, sequence_lengths),\n+        return tf.__internal__.smart_cond.smart_cond(\n+            gru_lstm_utils.is_cudnn_supported_inputs(\n+                mask, time_major, sequence_lengths\n+            ),\n             true_fn=cudnn_lstm_fn,\n             false_fn=stardard_lstm_fn,\n         )\n\n@@ -814,6 +814,7 @@ class LSTMGraphRewriteTest(test_combinations.TestCase):\n             existing_loss = loss_value\n \n         _, runtime_value = model.predict(x_train)\n+        if not tf.sysconfig.get_build_info()[\"is_rocm_build\"]:\n             if tf.test.is_gpu_available():\n                 self.assertEqual(runtime_value[0], gru_lstm_utils.RUNTIME_GPU)\n             else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d7b316c30ce4cf3559b1808d4939768340b443ca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 80 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 6 | Methods Changed: 11 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 84 | Churn Cumulative: 29052 | Contributors (this commit): 143 | Commits (past 90d): 40 | Contributors (cumulative): 160 | DMM Complexity: 1.0\n\nDIFF:\n@@ -691,7 +691,6 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n               [XLA](https://www.tensorflow.org/xla) is an optimizing compiler\n               for machine learning.\n               `jit_compile` is not enabled for by default.\n-              This option cannot be enabled with `run_eagerly=True`.\n               Note that `jit_compile=True`\n               may not necessarily work for all models.\n               For more information on supported operations please refer to the\n@@ -942,6 +941,33 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     def run_eagerly(self, value):\n         self._run_eagerly = value\n \n+    @property\n+    def jit_compile(self):\n+        \"\"\"Specify whether to compile the model with XLA.\n+\n+        [XLA](https://www.tensorflow.org/xla) is an optimizing compiler\n+        for machine learning. `jit_compile` is not enabled by default.\n+        Note that `jit_compile=True` may not necessarily work for all models.\n+\n+        For more information on supported operations please refer to the\n+        [XLA documentation](https://www.tensorflow.org/xla). Also refer to\n+        [known XLA issues](https://www.tensorflow.org/xla/known_issues)\n+        for more details.\n+        \"\"\"\n+        return self._jit_compile\n+\n+    @jit_compile.setter\n+    def jit_compile(self, value):\n+        # Function remains cached with previous jit_compile settings\n+        if self._jit_compile == value:\n+            # Avoid reseting compiler cache if possible if the value is the same\n+            return\n+\n+        self._jit_compile = value\n+\n+        # Setting `jit_compile` should invalidate previously cached functions.\n+        self._reset_compile_cache()\n+\n     @property\n     def distribute_reduction_method(self):\n         \"\"\"The method employed to reduce per-replica values during training.\n@@ -1225,7 +1251,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                     model._train_counter.assign_add(1)\n                 return outputs\n \n-            if self._jit_compile:\n+            if self.jit_compile:\n                 run_step = tf.function(\n                     run_step, jit_compile=True, reduce_retracing=True\n                 )\n@@ -1795,7 +1821,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                     model._test_counter.assign_add(1)\n                 return outputs\n \n-            if self._jit_compile:\n+            if self.jit_compile:\n                 run_step = tf.function(\n                     run_step, jit_compile=True, reduce_retracing=True\n                 )\n@@ -2114,7 +2140,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                     model._predict_counter.assign_add(1)\n                 return outputs\n \n-            if self._jit_compile:\n+            if self.jit_compile:\n                 run_step = tf.function(\n                     run_step, jit_compile=True, reduce_retracing=True\n                 )\n\n@@ -265,6 +265,56 @@ class TrainingTest(test_combinations.TestCase):\n         model.evaluate(x, y)\n         model.predict(x)\n \n+    @test_combinations.run_all_keras_modes(always_skip_v1=True)\n+    def test_jit_compile_true_for_evaluate_predict_but_false_for_compile(self):\n+        # Test with jit_compile = True for model.compile(), model.evaluate(),\n+        # model.predict()\n+        model = sequential.Sequential([layers_module.Dense(1)])\n+        self.assertIsNone(model._jit_compile)\n+        self.assertIsNone(model.jit_compile)\n+        model.compile(\"sgd\", loss=\"mse\")\n+        model.jit_compile = True\n+        self.assertTrue(model._jit_compile)\n+        self.assertTrue(model.jit_compile)\n+        x, y = np.ones((10, 1)), np.ones((10, 1))\n+        model.fit(x, y, epochs=2)\n+        model.evaluate(x, y)\n+        model.predict(x)\n+        self.assertTrue(model._jit_compile)\n+        self.assertTrue(model.jit_compile)\n+        model.compile(\"sgd\", loss=\"mse\", jit_compile=False)\n+        self.assertFalse(model._jit_compile)\n+        self.assertFalse(model.jit_compile)\n+        model.compile(\"sgd\", loss=\"mse\", jit_compile=True)\n+        self.assertTrue(model._jit_compile)\n+        self.assertTrue(model.jit_compile)\n+\n+    @test_combinations.run_all_keras_modes(always_skip_v1=True)\n+    def test_predict_xla_compile_with_jit_compile_setter_false_then_true(self):\n+        vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n+        input_array = np.array(\n+            [\n+                [\"earth\", \"wind\", \"and\", \"fire\"],\n+                [\"fire\", \"and\", \"earth\", \"michigan\"],\n+            ]\n+        )\n+        strategy = tf.distribute.MirroredStrategy()\n+        with strategy.scope():\n+            input_data = keras.Input(shape=(None,), dtype=tf.string)\n+            # Added a string op unsupported by XLA compiler to make sure that an\n+            # error is thrown, This ensures that the graph is indeed being\n+            # compiled using XLA\n+            layer = string_lookup.StringLookup(vocabulary=vocab_data)\n+            int_data = layer(input_data)\n+            model = keras.Model(inputs=input_data, outputs=int_data)\n+            # Compiled without jit_compile\n+            model.predict(input_array)\n+            model.jit_compile = True\n+            with self.assertRaisesRegex(\n+                tf.errors.InvalidArgumentError, \"Graph execution error\"\n+            ):\n+                model.predict(input_array)\n+\n     @test_combinations.run_all_keras_modes(always_skip_v1=True)\n     def test_fit_without_loss_at_compile(self):\n         model = sequential.Sequential([layers_module.Dense(1)])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fd2951d40ea933030cd650d656faf5311dd6b40d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 60 | Lines Deleted: 59 | Files Changed: 39 | Hunks: 58 | Methods Changed: 47 | Complexity Δ (Sum/Max): 34/38 | Churn Δ: 119 | Churn Cumulative: 99534 | Contributors (this commit): 201 | Commits (past 90d): 136 | Contributors (cumulative): 544 | DMM Complexity: None\n\nDIFF:\n@@ -250,7 +250,7 @@ def EfficientNet(\n     classes=1000,\n     classifier_activation=\"softmax\",\n ):\n-    \"\"\"Instantiates the EfficientNet architecture using given scaling coefficients.\n+    \"\"\"Instantiates the EfficientNet architecture.\n \n     Args:\n       width_coefficient: float, scaling coefficient for network width.\n\n@@ -433,7 +433,7 @@ def Stem(name=None):\n \n \n def SqueezeAndExciteBlock(filters_in, se_filters, name=None):\n-    \"\"\"Implements the Squeeze and excite block (https://arxiv.org/abs/1709.01507).\n+    \"\"\"Implements the Squeeze & Excite block (https://arxiv.org/abs/1709.01507).\n \n     Args:\n       filters_in: input filters to the block\n\n@@ -361,7 +361,7 @@ def global_learning_phase_is_set():\n \n \n def _mark_func_graph_as_unsaveable(graph, learning_phase):\n-    \"\"\"Mark func graph as unsaveable due to use of symbolic keras learning phase.\n+    \"\"\"Mark graph as unsaveable due to use of symbolic keras learning phase.\n \n     Functions that capture the symbolic learning phase cannot be exported to\n     SavedModel. Mark the funcgraph as unsaveable, so that an error will be\n@@ -914,7 +914,7 @@ def _get_current_tf_device():\n \n \n def _is_current_explicit_device(device_type):\n-    \"\"\"Check if the current device is explicitly set on the device type specified.\n+    \"\"\"Check if the current device is explicitly set to `device_type`.\n \n     Args:\n         device_type: A string containing `GPU` or `CPU` (case-insensitive).\n@@ -1178,7 +1178,7 @@ def unique_object_name(\n     zero_based=False,\n     avoid_observed_names=False,\n ):\n-    \"\"\"Makes a object name (or arbitrary string) unique within a TensorFlow graph.\n+    \"\"\"Makes a object name (or any string) unique within a Keras session.\n \n     Args:\n       name: String name to make unique.\n@@ -1510,7 +1510,7 @@ def shape(x):\n @keras_export(\"keras.backend.int_shape\")\n @doc_controls.do_not_generate_docs\n def int_shape(x):\n-    \"\"\"Returns the shape of tensor or variable as a tuple of int or None entries.\n+    \"\"\"Returns shape of tensor/variable as a tuple of int/None entries.\n \n     Args:\n         x: Tensor or variable.\n@@ -2832,7 +2832,7 @@ def cumsum(x, axis=0):\n @tf.__internal__.dispatch.add_dispatch_support\n @doc_controls.do_not_generate_docs\n def cumprod(x, axis=0):\n-    \"\"\"Cumulative product of the values in a tensor, alongside the specified axis.\n+    \"\"\"Cumulative product of the values in a tensor alongside `axis`.\n \n     Args:\n         x: A tensor or variable.\n\n@@ -3486,7 +3486,7 @@ class TestTensorBoardV2NonParameterizedTest(test_combinations.TestCase):\n         self.assertEqual(1, self._count_xplane_file(logdir=self.logdir))\n \n     def test_TensorBoard_autoTrace_outerProfiler(self):\n-        \"\"\"Runs a profiler session that interferes with the one from the callback.\n+        \"\"\"Runs a profiler session that interferes with the callback's one.\n \n         The callback will not generate a profile but execution will proceed\n         without crashing due to unhandled exceptions.\n\n@@ -359,7 +359,7 @@ class TestDistributionStrategyDnnCorrectness(\n         )\n     )\n     def test_fused_batch_norm_uneven_batch(self, distribution):\n-        \"\"\"Test that fused batch norm works when the last device may get empty data.\n+        \"\"\"Test that fused BN works when the last device gets empty data.\n \n         Adapted from\n         https://www.tensorflow.org/tutorials/distribute/custom_training\n\n@@ -185,7 +185,7 @@ class WorkerTrainingState:\n     def maybe_load_initial_counters_from_ckpt(\n         self, steps_per_epoch, initial_epoch, mode\n     ):\n-        \"\"\"Maybe load initial epoch from ckpt considering possible worker recovery.\n+        \"\"\"Maybe load 1st epoch from checkpoint, considering worker recovery.\n \n         When `_ckpt_saved_epoch` attribute exists and is not\n         `CKPT_SAVED_EPOCH_UNUSED_VALUE`, this is under multi-worker training\n\n@@ -95,7 +95,7 @@ _name_scope_unnester_stack = threading.local()\n \n @contextlib.contextmanager\n def _name_scope_unnester(full_name_scope):\n-    \"\"\"Helper to get relative name scope from fully specified nested name scopes.\n+    \"\"\"Helper to get relative name scope from fully-speced nested name scopes.\n \n     Args:\n       full_name_scope: full(absolute) name scope path.\n@@ -481,7 +481,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     @tf.__internal__.tracking.no_automatic_dependency_tracking\n     @generic_utils.default\n     def build(self, input_shape):\n-        \"\"\"Creates the variables of the layer (optional, for subclass implementers).\n+        \"\"\"Creates the variables of the layer (for subclass implementers).\n \n         This is a method that implementers of subclasses of `Layer` or `Model`\n         can override if they need a state-creation step in-between\n@@ -3025,7 +3025,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n \n     @tf.__internal__.tracking.no_automatic_dependency_tracking\n     def _maybe_create_attribute(self, name, default_value):\n-        \"\"\"Create the attribute with the default value if it hasn't been created.\n+        \"\"\"Create attribute (with the default value) if it hasn't been created.\n \n         This is useful for fields that is used for tracking purpose,\n         _trainable_weights, or _layers. Note that user could create a layer\n@@ -3331,7 +3331,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n \n     @tf.__internal__.tracking.no_automatic_dependency_tracking\n     def _set_save_spec(self, inputs, args=None, kwargs=None):\n-        \"\"\"Defines the save spec so that serialization is able to trace layer call.\n+        \"\"\"Defines the save spec so that serialization can trace layer calls.\n \n         The TensorSpecs of the call function `inputs`, `args`, and `kwargs` are\n         saved into a tuple of `([inputs] + args, kwargs)`.\n\n@@ -57,7 +57,7 @@ def make_variable(\n     partitioner=None,\n     layout=None,\n ):\n-    \"\"\"Temporary util to create a variable (relies on `variable_scope.variable`).\n+    \"\"\"Util to create a variable (relies on `variable_scope.variable`).\n \n     Some reuse-related technicalities prevent us from using\n     `variable_scope.get_variable()` directly, so we use a subcomponent\n@@ -823,7 +823,7 @@ def v2_dtype_behavior_enabled():\n \n \n class TrackableWeightHandler:\n-    \"\"\"Keras wrapper for handling tracking.Trackable object saving and restoring.\n+    \"\"\"Keras wrapper for handling Trackable object saving and restoring.\n \n     This class handles Trackables in both V1 and V2 modes, ensuring that they\n     can be saved and restored with the correct data and without adding\n\n@@ -254,7 +254,7 @@ class Layer(base_layer.Layer):\n     @tf.__internal__.tracking.no_automatic_dependency_tracking\n     @generic_utils.default\n     def build(self, input_shape):\n-        \"\"\"Creates the variables of the layer (optional, for subclass implementers).\n+        \"\"\"Creates the variables of the layer (for subclass implementers).\n \n         This is a method that implementers of subclasses of `Layer` or `Model`\n         can override if they need a state-creation step in-between\n@@ -2233,7 +2233,7 @@ class Layer(base_layer.Layer):\n \n     @tf.__internal__.tracking.no_automatic_dependency_tracking\n     def _maybe_create_attribute(self, name, default_value):\n-        \"\"\"Create the attribute with the default value if it hasn't been created.\n+        \"\"\"Create attribute (with the default value) if it hasn't been created.\n \n         This is useful for fields that is used for tracking purpose,\n         _trainable_weights, or _layers. Note that user could create a layer\n\n@@ -401,7 +401,7 @@ class MetricsContainer(Container):\n         self._from_serialized = from_serialized\n \n     def _check_duplicated_metrics(self, metrics, weighted_metrics):\n-        \"\"\"Check and raise error when user provided metrics has any duplications.\n+        \"\"\"Raise error when user provided metrics have any duplications.\n \n         Note that metrics are stateful container, a shared metric instance\n         between model.metric and model.weighted_metric will make the same\n\n@@ -1522,7 +1522,7 @@ def reconstruct_from_config(config, custom_objects=None, created_layers=None):\n \n \n def get_network_config(network, serialize_layer_fn=None, config=None):\n-    \"\"\"Builds the config, which consists of the node graph and serialized layers.\n+    \"\"\"Build the config, which consists of the node graph and serialized layers.\n \n     Args:\n       network: A Network object.\n\n@@ -429,7 +429,7 @@ class KerasTensor:\n \n     @classmethod\n     def _overload_operator(cls, tensor_class, operator):\n-        \"\"\"Overload an operator with the same implementation as a base Tensor class.\n+        \"\"\"Overload operator with the same implementation as the Tensor class.\n \n         We pull the operator out of the class dynamically to avoid ordering\n         issues.\n\n@@ -1830,7 +1830,7 @@ def initialize_iterator(iterator):\n \n \n def extract_tensors_from_dataset(dataset):\n-    \"\"\"Extract a tuple of tensors `inputs, targets, sample_weight` from a dataset.\n+    \"\"\"Extract tuple of tensors `inputs, targets, sample_weight` from a dataset.\n \n     Args:\n       dataset: Dataset instance.\n\n@@ -55,7 +55,7 @@ except ImportError:\n \n \n class Model(training_lib.Model):\n-    \"\"\"`Model` groups layers into an object with training and inference features.\n+    \"\"\"A model groups layers into an object with training & inference features.\n \n     There are two ways to instantiate a `Model`:\n \n@@ -165,7 +165,7 @@ class Model(training_lib.Model):\n         return base_layer.Layer.get_weights(self)\n \n     def load_weights(self, filepath, by_name=False, skip_mismatch=False):\n-        \"\"\"Loads all layer weights, either from a TensorFlow or an HDF5 weight file.\n+        \"\"\"Loads all layer weights, either from a TensorFlow or an HDF5 file.\n \n         If `by_name` is False weights are loaded based on the network's\n         topology. This means the architecture should be the same as when the\n@@ -688,7 +688,7 @@ class Model(training_lib.Model):\n         use_multiprocessing=False,\n         **kwargs,\n     ):\n-        \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset).\n+        \"\"\"Trains the model for a fixed number of epochs (dataset iterations).\n \n         Args:\n             x: Input data. It could be:\n@@ -1695,7 +1695,7 @@ class Model(training_lib.Model):\n             self.total_loss = self._prepare_total_loss(masks)\n \n     def _prepare_skip_target_masks(self):\n-        \"\"\"Boolean mask for whether the target in the output list should be skipped.\n+        \"\"\"Boolean mask for whether target in output list should be skipped.\n \n         If the loss function corresponding to a model output is None, then this\n         output will be skipped during total loss calculation and feed targets\n@@ -1859,7 +1859,7 @@ class Model(training_lib.Model):\n         self._replicated_model.set_original_model(self)\n \n     def _validate_or_infer_batch_size(self, batch_size, steps, x):\n-        \"\"\"Validates that the `batch_size` provided is consistent with InputLayer.\n+        \"\"\"Validates that `batch_size` provided is consistent with InputLayer.\n \n         It's possible that the user specified a static batch size in their\n         InputLayer. If so, this method checks the provided `batch_size` and `x`\n@@ -3127,7 +3127,7 @@ class Model(training_lib.Model):\n         ]\n \n     def _maybe_load_initial_epoch_from_ckpt(self, initial_epoch, mode):\n-        \"\"\"Maybe load initial epoch from ckpt considering possible worker recovery.\n+        \"\"\"Maybe load 1st epoch from checkpoint, considering worker recovery.\n \n         Refer to tensorflow/python/keras/distribute/worker_training_state.py\n         for more information.\n\n@@ -86,7 +86,7 @@ class _BaseFeaturesLayer(Layer):\n         super().build(None)\n \n     def _output_shape(self, input_shape, num_elements):\n-        \"\"\"Computes expected output shape of the layer or a column's dense tensor.\n+        \"\"\"Computes expected output shape of the dense tensor of the layer.\n \n         Args:\n           input_shape: Tensor or array with batch shape.\n\n@@ -86,7 +86,7 @@ class Initializer:\n         )\n \n     def get_config(self):\n-        \"\"\"Returns the configuration of the initializer as a JSON-serializable dict.\n+        \"\"\"Returns the initializer's configuration as a JSON-serializable dict.\n \n         Returns:\n           A JSON-serializable Python dict.\n@@ -474,7 +474,7 @@ class TruncatedNormal(Initializer):\n         )\n \n     def __call__(self, shape, dtype=None, **kwargs):\n-        \"\"\"Returns a tensor object initialized to random normal values (truncated).\n+        \"\"\"Returns a tensor initialized to random normal values (truncated).\n \n         Args:\n           shape: Shape of the tensor.\n@@ -525,7 +525,7 @@ class TruncatedNormal(Initializer):\n     v1=[],\n )\n class VarianceScaling(Initializer):\n-    \"\"\"Initializer capable of adapting its scale to the shape of weights tensors.\n+    \"\"\"Initializer that adapts its scale to the shape of its input tensors.\n \n     Also available via the shortcut function\n     `tf.keras.initializers.variance_scaling`.\n\n@@ -149,7 +149,7 @@ class MultiWorkerTutorialTest(parameterized.TestCase, tf.test.TestCase):\n         )\n     )\n     def testMwmsWithModelFit(self, mode):\n-        \"\"\"Test multi-worker training flow demo'ed in go/multi-worker-with-keras.\n+        \"\"\"Test multi-worker training flow demoed in go/multi-worker-with-keras.\n \n         This test should be kept in sync with the code samples in\n         go/multi-worker-with-keras.\n\n@@ -139,7 +139,7 @@ def local_conv_matmul(inputs, kernel, kernel_mask, output_shape):\n def local_conv_sparse_matmul(\n     inputs, kernel, kernel_idxs, kernel_shape, output_shape\n ):\n-    \"\"\"Apply N-D convolution with un-shared weights using a single sparse matmul.\n+    \"\"\"Apply N-D convolution with unshared weights using a single sparse matmul.\n \n     This method outputs `inputs . tf.sparse.SparseTensor(indices=kernel_idxs,\n     values=kernel, dense_shape=kernel_shape)`, with `.` standing for\n\n@@ -558,7 +558,7 @@ class NormalizationLayersGraphModeOnlyTest(\n             self.assertAllClose(x1, x2, atol=1e-7)\n \n     def test_batchnorm_trainable(self, layer):\n-        \"\"\"Tests that batchnorm layer is trainable when learning phase is enabled.\n+        \"\"\"Tests that BN layer is trainable when learning phase is enabled.\n \n         Computes mean and std for current inputs then\n         applies batch normalization using them.\n\n@@ -24,7 +24,7 @@ from keras.utils import conv_utils\n \n \n class Pooling2D(Layer):\n-    \"\"\"Pooling layer for arbitrary pooling functions, for 2D inputs (e.g. images).\n+    \"\"\"Pooling layer for arbitrary pooling functions, for 2D data (e.g. images).\n \n     This class only exists for code reuse. It will never be an exposed API.\n \n\n@@ -437,7 +437,7 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n         return dict(list(base_config.items()) + list(config.items()))\n \n     def set_vocabulary(self, vocabulary, idf_weights=None):\n-        \"\"\"Sets vocabulary (and optionally document frequency) data for this layer.\n+        \"\"\"Sets vocabulary (and optionally document frequency) for this layer.\n \n         This method sets the vocabulary and idf weights for this layer directly,\n         instead of analyzing a dataset through `adapt`. It should be used\n\n@@ -72,7 +72,7 @@ class PreprocessingStage(\n                 continue\n \n             def map_fn(x):\n-                \"\"\"Maps `PreprocessingStage` inputs to inputs at `current_layer_index`.\n+                \"\"\"Maps this object's inputs to those at current_layer_index.\n \n                 Args:\n                   x: Batch of inputs seen in entry of the `PreprocessingStage`\n\n@@ -521,7 +521,7 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n         return dict(list(base_config.items()) + list(config.items()))\n \n     def set_vocabulary(self, vocabulary, idf_weights=None):\n-        \"\"\"Sets vocabulary (and optionally document frequency) data for this layer.\n+        \"\"\"Sets vocabulary (and optionally document frequency) for this layer.\n \n         This method sets the vocabulary and idf weights for this layer directly,\n         instead of analyzing a dataset through 'adapt'. It should be used\n\n@@ -511,7 +511,7 @@ class ResidualWrapper(_RNNCellWrapper):\n         self._residual_fn = residual_fn\n \n     def _call_wrapped_cell(self, inputs, state, cell_call_fn, **kwargs):\n-        \"\"\"Run the cell and then apply the residual_fn on its inputs to its outputs.\n+        \"\"\"Run the cell and apply the residual_fn.\n \n         Args:\n           inputs: cell inputs.\n\n@@ -551,7 +551,7 @@ class ResidualWrapper(_RNNCellWrapperV1):\n         self._residual_fn = residual_fn\n \n     def _call_wrapped_cell(self, inputs, state, cell_call_fn, **kwargs):\n-        \"\"\"Run the cell and then apply the residual_fn on its inputs to its outputs.\n+        \"\"\"Run the cell and apply the residual_fn.\n \n         Args:\n           inputs: cell inputs.\n\n@@ -648,7 +648,7 @@ _LSTMStateTuple = collections.namedtuple(\"LSTMStateTuple\", (\"c\", \"h\"))\n @keras_export(v1=[\"keras.__internal__.legacy.rnn_cell.LSTMStateTuple\"])\n @tf_export(v1=[\"nn.rnn_cell.LSTMStateTuple\"])\n class LSTMStateTuple(_LSTMStateTuple):\n-    \"\"\"Tuple used by LSTM Cells for `state_size`, `zero_state`, and output state.\n+    \"\"\"Tuple used by LSTM Cells for `state_size`, `zero_state`, & output state.\n \n     Stores two elements: `(c, h)`, in that order. Where `c` is the hidden state\n     and `h` is the output.\n\n@@ -180,7 +180,7 @@ def conv1d(\n     name=None,\n     reuse=None,\n ):\n-    \"\"\"Functional interface for 1D convolution layer (e.g. temporal convolution).\n+    \"\"\"Functional interface for 1D convolution (e.g. temporal convolution).\n \n     This layer creates a convolution kernel that is convolved\n     (actually cross-correlated) with the layer input to produce a tensor of\n\n@@ -136,7 +136,7 @@ def validate_synchronization_aggregation_trainable(\n \n \n class _EagerVariableStore(tf.Module):\n-    \"\"\"TF2-compatible VariableStore that avoids collections & tracks regularizers.\n+    \"\"\"TF2-safe VariableStore that avoids collections & tracks regularizers.\n \n     New variable names and new variables can be created; all stored\n     variables are initialized with the initializer passed to __init__.\n\n@@ -670,7 +670,7 @@ class BinaryCrossentropy(LossFunctionWrapper):\n \n @keras_export(\"keras.losses.BinaryFocalCrossentropy\")\n class BinaryFocalCrossentropy(LossFunctionWrapper):\n-    \"\"\"Computes the focal cross-entropy loss between true labels and predictions.\n+    \"\"\"Computes focal cross-entropy loss between true labels and predictions.\n \n     Binary cross-entropy loss is often used for binary (0 or 1) classification\n     tasks. The loss function requires the following inputs:\n\n@@ -39,7 +39,7 @@ def numpy_text(tensor, is_repr=False):\n \n \n class AutoCastVariable(tf.Variable, tf.__internal__.types.Tensor):\n-    \"\"\"Variable that will cast itself to a different dtype in applicable contexts.\n+    \"\"\"Variable that casts itself to a different dtype in applicable contexts.\n \n     This class wraps a floating-point `tf.Variable`. It emulates the variable\n     interface and delegates to the wrapped variable, but it additionally will\n\n@@ -1091,7 +1091,7 @@ class NoisyLinearCosineDecay(LearningRateSchedule):\n \n @keras_export(\"keras.optimizers.schedules.serialize\")\n def serialize(learning_rate_schedule):\n-    \"\"\"Serializes a `LearningRateSchedule` into a JSON-compatible representation.\n+    \"\"\"Serializes a `LearningRateSchedule` into a JSON-compatible dict.\n \n     Args:\n       learning_rate_schedule: The `LearningRateSchedule` object to serialize.\n\n@@ -841,7 +841,7 @@ def validate_filename(filename, white_list_formats):\n \n \n class DataFrameIterator(BatchFromFilesMixin, Iterator):\n-    \"\"\"Iterator capable of reading images from a directory on disk as a dataframe.\n+    \"\"\"Iterator capable of reading images from a directory as a dataframe.\n \n     Args:\n         dataframe: Pandas dataframe containing the filepaths relative to\n\n@@ -228,11 +228,12 @@ class TimeseriesGenerator(data_utils.Sequence):\n         }\n \n     def to_json(self, **kwargs):\n-        \"\"\"Returns a JSON string containing the timeseries generator configuration.\n+        \"\"\"Returns a JSON string containing the generator's configuration.\n \n         Args:\n-            **kwargs: Additional keyword arguments\n-                to be passed to `json.dumps()`.\n+            **kwargs: Additional keyword arguments to be passed\n+                to `json.dumps()`.\n+\n         Returns:\n             A JSON string containing the tokenizer configuration.\n         \"\"\"\n\n@@ -329,7 +329,7 @@ class L2(Regularizer):\n     v1=[],\n )\n class OrthogonalRegularizer(Regularizer):\n-    \"\"\"A regularizer that encourages input vectors to be orthogonal to each other.\n+    \"\"\"Regularizer that encourages input vectors to be orthogonal to each other.\n \n     It can be applied to either the rows of a matrix (`mode=\"rows\"`) or its\n     columns (`mode=\"columns\"`). When applied to a `Dense` kernel of shape\n\n@@ -468,7 +468,7 @@ class TestWholeModelSaving(test_combinations.TestCase):\n         return os.path.join(temp_dir, dirname)\n \n     def _assert_same_weights_and_metrics(self, model, loaded_model):\n-        \"\"\"Checks that the loaded weights and metrics are the same as the original.\n+        \"\"\"Checks that loaded weights & metrics are the same as the original.\n \n         Args:\n           model: original model\n\n@@ -310,7 +310,7 @@ class LayerAttributes(\n         copy_from=[CommonEndpoints],\n     )\n ):\n-    \"\"\"Layer checkpointable objects + functions that are saved to the SavedModel.\n+    \"\"\"Layer checkpointable objects + functions saved to the SavedModel.\n \n     List of all attributes:\n       All attributes from CommonEndpoints\n@@ -335,7 +335,7 @@ class ModelAttributes(\n         \"ModelAttributes\", copy_from=[LayerAttributes]\n     )\n ):\n-    \"\"\"Model checkpointable objects + functions that are saved to the SavedModel.\n+    \"\"\"Model checkpointable objects + functions saved to the SavedModel.\n \n     List of all attributes:\n       All attributes from LayerAttributes (including CommonEndpoints)\n\n@@ -35,7 +35,7 @@ training_lib = LazyLoader(\"training_lib\", globals(), \"keras.engine.training\")\n def use_wrapped_call(\n     layer, call_fn, call_spec, default_training_value=None, return_method=False\n ):\n-    \"\"\"Creates fn that adds the losses returned by call_fn & returns the outputs.\n+    \"\"\"Creates fn that adds losses returned by call_fn & returns the outputs.\n \n     Args:\n       layer: A Keras layer object\n@@ -251,7 +251,7 @@ def should_save_traces():\n \n @tf_contextlib.contextmanager\n def no_automatic_dependency_tracking_scope(obj):\n-    \"\"\"A context that disables automatic dependency tracking when assigning attrs.\n+    \"\"\"Context that disables automatic dependency tracking when assigning attrs.\n \n     Objects that inherit from Autotrackable automatically creates dependencies\n     to trackable objects through attribute assignments, and wraps data\n\n@@ -84,7 +84,7 @@ def exact_gaussian_kernel(x, y, stddev):\n \n \n def exact_laplacian_kernel(x, y, stddev):\n-    r\"\"\"Computes exact Laplacian kernel value(s) for tensors x and y using stddev.\n+    r\"\"\"Computes exact Laplacian kernel value(s) for tensors x & y using stddev.\n \n     The Laplacian kernel for vectors u, v is defined as follows:\n          K(u, v) = exp(-||u-v|| / stddev)\n\n@@ -352,7 +352,7 @@ def print_summary(\n         print_row(fields, positions, nested_level)\n \n     def print_layer_summary_with_connections(layer, nested_level=0):\n-        \"\"\"Prints a summary for a single layer (including topological connections).\n+        \"\"\"Prints a summary for a single layer (including its connections).\n \n         Args:\n             layer: target layer.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f87571534263eaeeade012d572ee246ba8933352", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 137 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 10/6 | Churn Δ: 137 | Churn Cumulative: 386 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: 0.5206611570247934\n\nDIFF:\n@@ -80,6 +80,10 @@ class Embedding(Layer):\n         This argument is required if you are going to connect\n         `Flatten` then `Dense` layers upstream\n         (without it, the shape of the dense outputs cannot be computed).\n+      sparse: If True, calling this layer returns a `tf.SparseTensor`. If False,\n+        the layer returns a dense `tf.Tensor`. For an entry with no features in\n+        a sparse tensor (entry with value 0), the embedding vector of index 0 is\n+        returned by default.\n \n     Input shape:\n       2D tensor with shape: `(batch_size, input_length)`.\n@@ -121,6 +125,7 @@ class Embedding(Layer):\n         embeddings_constraint=None,\n         mask_zero=False,\n         input_length=None,\n+        sparse=False,\n         **kwargs,\n     ):\n         if \"input_shape\" not in kwargs:\n@@ -157,6 +162,13 @@ class Embedding(Layer):\n         self.mask_zero = mask_zero\n         self.supports_masking = mask_zero\n         self.input_length = input_length\n+        self.sparse = sparse\n+        if self.sparse and self.mask_zero:\n+            raise ValueError(\n+                \"`mask_zero` cannot be enabled when \"\n+                \"`tf.keras.layers.Embedding` is used with `tf.SparseTensor` \"\n+                \"input.\"\n+            )\n \n     @tf_utils.shape_type_conversion\n     def build(self, input_shape=None):\n@@ -205,7 +217,50 @@ class Embedding(Layer):\n         dtype = backend.dtype(inputs)\n         if dtype != \"int32\" and dtype != \"int64\":\n             inputs = tf.cast(inputs, \"int32\")\n+        if isinstance(inputs, tf.sparse.SparseTensor):\n+            if self.sparse:\n+                # get sparse embedding values\n+                embedding_values = tf.nn.embedding_lookup(\n+                    params=self.embeddings, ids=inputs.values\n+                )\n+                embedding_values = tf.reshape(embedding_values, [-1])\n+                # get sparse embedding indices\n+                indices_values_embed_axis = tf.range(self.output_dim)\n+                repeat_times = [inputs.indices.shape[0]]\n+                indices_values_embed_axis = tf.expand_dims(\n+                    tf.tile(indices_values_embed_axis, repeat_times), -1\n+                )\n+                indices_values_embed_axis = tf.cast(\n+                    indices_values_embed_axis, dtype=tf.int64\n+                )\n+                current_indices = tf.repeat(\n+                    inputs.indices, [self.output_dim], axis=0\n+                )\n+                new_indices = tf.concat(\n+                    [current_indices, indices_values_embed_axis], 1\n+                )\n+                new_shape = tf.concat(\n+                    [tf.cast(inputs.shape, dtype=tf.int64), [self.output_dim]],\n+                    axis=-1,\n+                )\n+                out = tf.SparseTensor(\n+                    indices=new_indices,\n+                    values=embedding_values,\n+                    dense_shape=new_shape,\n+                )\n+            else:\n+                sparse_inputs_expanded = tf.sparse.expand_dims(inputs, axis=-1)\n+                out = tf.nn.safe_embedding_lookup_sparse(\n+                    embedding_weights=self.embeddings,\n+                    sparse_ids=sparse_inputs_expanded,\n+                    default_id=0,\n+                )\n+        else:\n             out = tf.nn.embedding_lookup(self.embeddings, inputs)\n+\n+        if self.sparse and not isinstance(out, tf.SparseTensor):\n+            out = tf.sparse.from_dense(out)\n+\n         if (\n             self._dtype_policy.compute_dtype\n             != self._dtype_policy.variable_dtype\n\n@@ -144,6 +144,88 @@ class EmbeddingTest(test_combinations.TestCase):\n         finally:\n             policy.set_global_policy(\"float32\")\n \n+    @test_combinations.run_all_keras_modes\n+    def test_embedding_with_sparse_input_sparse_output(self):\n+        layer = keras.layers.Embedding(\n+            input_dim=3,\n+            output_dim=2,\n+            weights=[np.array([[0.0, 0.0], [1.0, 1.0], [2.0, 2.0]])],\n+            sparse=True,\n+        )\n+        input = tf.SparseTensor(\n+            indices=[[0, 1], [1, 2]], values=[1, 2], dense_shape=[3, 3]\n+        )\n+        output = layer(input)\n+        expected_output = tf.SparseTensor(\n+            indices=[[0, 1, 0], [0, 1, 1], [1, 2, 0], [1, 2, 1]],\n+            values=[1.0, 1.0, 2.0, 2.0],\n+            dense_shape=[3, 3, 2],\n+        )\n+        self.assertAllClose(output.indices, expected_output.indices)\n+        self.assertAllClose(output.values, expected_output.values)\n+        self.assertAllClose(output.dense_shape, expected_output.dense_shape)\n+\n+    @test_combinations.run_all_keras_modes\n+    def test_embedding_with_sparse_input_dense_output(self):\n+        layer = keras.layers.Embedding(\n+            input_dim=3,\n+            output_dim=2,\n+            weights=[np.array([[0.1, 0.1], [1.0, 1.0], [2.0, 2.0]])],\n+            sparse=False,\n+        )\n+        input = tf.SparseTensor(\n+            indices=[[0, 1], [1, 2]], values=[1, 2], dense_shape=[3, 3]\n+        )\n+        output = layer(input)\n+        expected_output = tf.constant(\n+            [\n+                [[0.1, 0.1], [1.0, 1.0], [0.1, 0.1]],\n+                [[0.1, 0.1], [0.1, 0.1], [2.0, 2.0]],\n+                [[0.1, 0.1], [0.1, 0.1], [0.1, 0.1]],\n+            ]\n+        )\n+        self.assertAllClose(output, expected_output)\n+\n+    @test_combinations.run_all_keras_modes\n+    def test_error_message_for_mask_zero_enabled_with_sparse_tensor(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"`mask_zero` cannot be enabled when \"\n+            \"`tf.keras.layers.Embedding` is used with `tf.SparseTensor` \"\n+            \"input.\",\n+        ):\n+            layer = keras.layers.Embedding(\n+                input_dim=3,\n+                output_dim=2,\n+                weights=[np.array([[0.1, 0.1], [1.0, 1.0], [2.0, 2.0]])],\n+                sparse=True,\n+                mask_zero=True,\n+            )\n+            inputs = tf.SparseTensor(\n+                indices=[[0, 1], [1, 2]], values=[1, 2], dense_shape=[3, 3]\n+            )\n+            layer(inputs)\n+\n+    @test_combinations.run_all_keras_modes\n+    def test_embedding_with_dense_input_sprase_output(self):\n+        layer = keras.layers.Embedding(\n+            input_dim=3,\n+            output_dim=2,\n+            weights=[np.array([[0, 0], [1.0, 1.0], [2.0, 2.0]])],\n+            sparse=True,\n+            mask_zero=False,\n+        )\n+        inputs = tf.constant([0, 0, 0, 2, 1])\n+        output = layer(inputs)\n+        expected_output = tf.SparseTensor(\n+            indices=[[3, 0], [3, 1], [4, 0], [4, 1]],\n+            values=[2.0, 2.0, 1.0, 1.0],\n+            dense_shape=[5, 2],\n+        )\n+        self.assertAllClose(output.indices, expected_output.indices)\n+        self.assertAllClose(output.values, expected_output.values)\n+        self.assertAllClose(output.dense_shape, expected_output.dense_shape)\n+\n \n if __name__ == \"__main__\":\n     tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
