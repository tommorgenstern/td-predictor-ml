{"custom_id": "scrapy#eb1e62a28c7254a4130c4ff253af269a23ac45e7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 180 | Lines Deleted: 0 | Files Changed: 7 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 180 | Churn Cumulative: 180 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n\n@@ -0,0 +1,13 @@\n+# Define here the models for your scraped items\n+\n+from scrapy.contrib.item import RobustScrapedItem\n+\n+class GoogledirItem(RobustScrapedItem):\n+    \"\"\"Directory website link\"\"\"\n+\n+    ATTRIBUTES = {\n+        'guid': basestring,\n+        'name': basestring,\n+        'url': basestring,\n+        'description': basestring,\n+        }\n\n@@ -0,0 +1,5 @@\n+# Define yours item pipelines here\n+\n+class GoogledirPipeline(object):\n+    def process_item(self, domain, item):\n+        return item\n\n@@ -0,0 +1,106 @@\n+import googledir\n+\n+# ---------------------------------------------------------------------------\n+# - Scrapy settings for googledir                                    -\n+# ---------------------------------------------------------------------------\n+PROJECT_NAME = 'googledir'\n+\n+BOT_NAME = PROJECT_NAME\n+BOT_VERSION = '1.0'\n+\n+SPIDER_MODULES = ['googledir.spiders']\n+NEWSPIDER_MODULE = 'googledir.spiders'\n+TEMPLATES_DIR = '%s/templates' % googledir.__path__[0]\n+ENABLED_SPIDERS_FILE = '%s/conf/enabled_spiders.list' % googledir.__path__[0]\n+DEFAULT_ITEM_CLASS = 'scrapy.item.ScrapedItem'\n+USER_AGENT = '%s/%s' % (BOT_NAME, BOT_VERSION)\n+\n+# The amount of time (in secs) that the downloader should wait before\n+# downloading consecutive pages from the same spider. This can be used\n+# to throttle the crawling speed to avoid hitting servers too\n+# hard. Decimal numbers are supported. Example:\n+# DOWNLOAD_DELAY = 2.5\n+DOWNLOAD_TIMEOUT = 600\n+\n+# use this spider class as default when no spider was found for a given url\n+#DEFAULT_SPIDER = 'scrapy.contrib.spiders.generic.GenericSpider'\n+\n+# uncomment if you want to add your own custom scrapy commands\n+#COMMANDS_MODULE = 'googledir.commands'\n+#COMMANDS_SETTINGS_MODULE = 'googledir.conf.commands'\n+\n+#Global timeout between sucessive downloads (can be overrided by spider\n+#attribute download_timeout\n+#DOWNLOAD_TIMEOUT = 0\n+\n+MYSQL_CONNECTION_SETTINGS = {\"charset\": \"utf8\" }\n+MYSQL_CONNECTION_PING_PERIOD = 600\n+\n+SCHEDULER = 'scrapy.core.scheduler.Scheduler'\n+SCHEDULER_ORDER = 'BFO'   # available orders: BFO (default), DFO\n+\n+#CACHE2_DIR = '/tmp/cache2'  # if set, enables HTTP cache\n+#CACHE2_IGNORE_MISSING = 0   # ignore requests not in cache\n+#CACHE2_SECTORIZE = 1         # sectorize domains to distribute storage among servers\n+\n+#STATS_ENABLED = 1   # enable stats\n+#STATS_CLEANUP = 0   # cleanup domain stats when a domain is closed (saves memory)\n+#STATS_DEBUG = 0     # log stats on domain closed\n+\n+EXTENSIONS = (\n+    'scrapy.management.web.WebConsole',\n+    'scrapy.management.telnet.TelnetConsole',\n+)\n+\n+DOWNLOADER_MIDDLEWARES = (\n+    # Engine side\n+    'scrapy.contrib.downloadermiddleware.errorpages.ErrorPagesMiddleware',\n+    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware',\n+    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware',\n+    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware',\n+    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware',\n+    'scrapy.contrib.downloadermiddleware.common.CommonMiddleware',\n+    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware',\n+    'scrapy.contrib.downloadermiddleware.compression.CompressionMiddleware',\n+    'scrapy.contrib.downloadermiddleware.debug.CrawlDebug',\n+    'scrapy.contrib.downloadermiddleware.cache.CacheMiddleware',\n+    # Downloader side\n+)\n+\n+SPIDER_MIDDLEWARES = (\n+    # Engine side\n+    'scrapy.contrib.spidermiddleware.limit.RequestLimitMiddleware',\n+    'scrapy.contrib.spidermiddleware.restrict.RestrictMiddleware',\n+    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware',\n+    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware',\n+    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware',\n+    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware',\n+    'scrapy.contrib.spidermiddleware.urlfilter.UrlFilterMiddleware',\n+    'scrapy.contrib.spidermiddleware.duplicatesfilter.DuplicatesFilterMiddleware',\n+    # Spider side\n+)\n+\n+ITEM_PIPELINES = (\n+    'googledir.pipelines.GoogledirPipeline',\n+)\n+\n+#DEPTH_LIMIT = 10  # limit the maximum link depth to follow\n+#DEPTH_STATS = 1    # enable depth stats\n+\n+# Limit URL length. See: http://www.boutell.com/newfaq/misc/urllength.html\n+URLLENGTH_LIMIT = 2083\n+\n+#WEBCONSOLE_ENABLED = 1\n+#WEBCONSOLE_PORT = 8060  # if not set uses a dynamic port\n+\n+#TELNETCONSOLE_ENABLED = 1\n+#TELNETCONSOLE_PORT = 2020  # if not set uses a dynamic port\n+\n+# global mail sending settings\n+#MAIL_HOST = 'localhost'\n+#MAIL_FROM = 'scrapybot@localhost'\n+\n+# scrapy webservice\n+WS_ENABLED = 0\n+\n+SPIDERPROFILER_ENABLED = 0\n\n@@ -0,0 +1 @@\n+# Place here all your scrapy spiders\n\n@@ -0,0 +1,48 @@\n+# -*- coding: utf8 -*-\n+import re\n+\n+from scrapy.xpath import HtmlXPathSelector\n+from scrapy.link.extractors import RegexLinkExtractor\n+from scrapy.contrib.spiders import CrawlSpider, Rule\n+from scrapy.contrib_exp import adaptors\n+from scrapy.utils.misc import items_to_csv\n+from googledir.items import GoogledirItem\n+\n+class GoogleDirectorySpider(CrawlSpider):\n+    domain_name = 'google.com'\n+    start_urls = ['http://www.google.com/dirhp']\n+\n+    rules = (\n+        Rule(RegexLinkExtractor(allow=('google.com/[A-Z][a-zA-Z_/]+$',),),\n+            'parse_category',\n+            follow=True,\n+        ),\n+    )\n+    csv_file = open('scraped_items.csv', 'ab+')\n+    \n+    def parse_category(self, response):\n+        # The selector we're going to use in order to extract data from the page\n+        hxs = HtmlXPathSelector(response)\n+\n+        # The path to website links in directory page\n+        links = hxs.x('//td[descendant::a[contains(@href, \"#pagerank\")]]/following-sibling::td/font')\n+\n+        # The list of functions to apply to an attribute before assigning its value\n+        adaptor_pipe = [adaptors.extract, adaptors.delist(''), adaptors.strip]\n+        adaptor_map = {\n+            'name': adaptor_pipe,\n+            'url': adaptor_pipe,\n+            'description': adaptor_pipe,\n+            }\n+\n+        for link in links:\n+            item = GoogledirItem()\n+            item.set_adaptors(adaptor_map)\n+\n+            item.attribute('name', link.x('a/text()'))\n+            item.attribute('url', link.x('a/@href'))\n+            item.attribute('description', link.x('font[2]/text()'))\n+            items_to_csv(self.csv_file, [item])\n+            yield item\n+\n+SPIDER = GoogleDirectorySpider()\n\n@@ -0,0 +1,7 @@\n+#!/usr/bin/env python\n+\n+import os\n+os.environ.setdefault('SCRAPYSETTINGS_MODULE', 'googledir.settings')\n+\n+from scrapy.command.cmdline import execute\n+execute()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#522fe78c2b19cdcc42fc04f60efba2e9b228a6d9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 151 | Files Changed: 6 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 151 | Churn Cumulative: 302 | Contributors (this commit): 2 | Commits (past 90d): 12 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n\n@@ -1,6 +0,0 @@\n-# Define here the models for your scraped items\n-\n-from scrapy.item import ScrapedItem\n-\n-class MyItem(ScrapedItem):\n-    pass\n\n@@ -1,4 +0,0 @@\n-#!/usr/bin/env python\n-\n-from scrapy.command.cmdline import execute\n-execute()\n\n@@ -1,96 +0,0 @@\n-import google_bot\n-\n-# ---------------------------------------------------------------------------\n-# - Scrapy settings for google_bot                                    -\n-# ---------------------------------------------------------------------------\n-\n-BOT_NAME = 'scrapybot'\n-BOT_VERSION = '1.0'\n-\n-SPIDER_MODULES = ['google_bot.spiders']\n-NEWSPIDER_MODULE = 'google_bot.spiders'\n-TEMPLATES_DIR = '%s/templates' % google_bot.__path__[0]\n-ENABLED_SPIDERS_FILE = '%s/conf/enabled_spiders.list' % google_bot.__path__[0]\n-DEFAULT_ITEM_CLASS = 'scrapy.item.ScrapedItem'\n-USER_AGENT = '%s/%s' % (BOT_NAME, BOT_VERSION)\n-DOWNLOAD_TIMEOUT = 600\n-\n-# uncomment if you want to add your own custom scrapy commands\n-#COMMANDS_MODULE = 'google_bot.commands'\n-#COMMANDS_SETTINGS_MODULE = 'google_bot.conf.commands'\n-\n-#Global timeout between sucessive downloads (can be overrided by spider\n-#attribute download_timeout\n-#DOWNLOAD_TIMEOUT = 0\n-\n-MYSQL_CONNECTION_SETTINGS = {\"charset\": \"utf8\" }\n-MYSQL_CONNECTION_PING_PERIOD = 600\n-\n-SCHEDULER = 'scrapy.core.scheduler.Scheduler'\n-SCHEDULER_ORDER = 'BFO'   # available orders: BFO (default), DFO\n-\n-#CACHE2_DIR = '/tmp/cache2'  # if set, enables HTTP cache\n-#CACHE2_IGNORE_MISSING = 0   # ignore requests not in cache\n-#CACHE2_SECTORIZE = 1         # sectorize domains to distribute storage among servers\n-\n-#STATS_ENABLED = 1   # enable stats\n-#STATS_CLEANUP = 0   # cleanup domain stats when a domain is closed (saves memory)\n-#STATS_DEBUG = 0     # log stats on domain closed\n-\n-EXTENSIONS = (\n-    'scrapy.management.web.WebConsole',\n-    'scrapy.management.telnet.TelnetConsole',\n-)\n-\n-DOWNLOADER_MIDDLEWARES = (\n-    # Engine side\n-    'scrapy.contrib.downloadermiddleware.errorpages.ErrorPagesMiddleware',\n-    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware',\n-    'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware',\n-    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware',\n-    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware',\n-    'scrapy.contrib.downloadermiddleware.common.CommonMiddleware',\n-    'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware',\n-    'scrapy.contrib.downloadermiddleware.compression.CompressionMiddleware',\n-    'scrapy.contrib.downloadermiddleware.debug.CrawlDebug',\n-    'scrapy.contrib.downloadermiddleware.cache.CacheMiddleware',\n-    # Downloader side\n-)\n-\n-SPIDER_MIDDLEWARES = (\n-    # Engine side\n-    'scrapy.contrib.spidermiddleware.limit.RequestLimitMiddleware',\n-    'scrapy.contrib.spidermiddleware.restrict.RestrictMiddleware',\n-    'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware',\n-    'scrapy.contrib.spidermiddleware.referer.RefererMiddleware',\n-    'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware',\n-    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware',\n-    'scrapy.contrib.spidermiddleware.urlfilter.UrlFilterMiddleware',\n-    # Spider side\n-)\n-\n-# Item pipelines are usually configured by commands (see conf/commands)\n-ITEM_PIPELINES = (\n-    'scrapy.contrib.pipeline.show.ShowItemPipeline',\n-)\n-\n-#DEPTH_LIMIT = 10  # limit the maximum link depth to follow\n-#DEPTH_STATS = 1    # enable depth stats\n-\n-# Limit URL length. See: http://www.boutell.com/newfaq/misc/urllength.html\n-URLLENGTH_LIMIT = 2083\n-\n-#WEBCONSOLE_ENABLED = 1\n-#WEBCONSOLE_PORT = 8060  # if not set uses a dynamic port\n-\n-#TELNETCONSOLE_ENABLED = 1\n-#TELNETCONSOLE_PORT = 2020  # if not set uses a dynamic port\n-\n-# global mail sending settings\n-#MAIL_HOST = 'localhost'\n-#MAIL_FROM = 'scrapybot@localhost'\n-\n-# scrapy webservice\n-WS_ENABLED = 0\n-\n-SPIDERPROFILER_ENABLED = 0\n\n@@ -1 +0,0 @@\n-# Place here all your scrapy spiders\n\n@@ -1,44 +0,0 @@\n-# -*- coding: utf8 -*-\n-from scrapy.xpath import HtmlXPathSelector\n-from scrapy.item import ScrapedItem\n-from scrapy.contrib import adaptors\n-from scrapy.contrib.spiders import CrawlSpider, Rule\n-from scrapy.link.extractors import RegexLinkExtractor\n-from scrapy.utils.misc import items_to_csv\n-\n-class GoogleDirectorySpider(CrawlSpider):\n-    domain_name = 'google.com'\n-    start_urls = ['http://www.google.com/dirhp']\n-\n-    rules = (\n-        Rule(RegexLinkExtractor(allow=('google.com/[A-Z][a-zA-Z_/]+$', ), ),\n-            'parse_category',\n-            follow=True,\n-        ),\n-    )\n-\n-    adaptor_pipe = [adaptors.extract, adaptors.Delist(), adaptors.strip]\n-    csv_file = open('scraped_items.csv', 'w')\n-\n-    def parse_category(self, response):\n-        items = [] # The item (links to websites) list we're going to return\n-        hxs = HtmlXPathSelector(response) # The selector we're going to use in order to extract data from the page\n-        links = hxs.x('//td[descendant::a[contains(@href, \"#pagerank\")]]/following-sibling::td/font')\n-\n-        for link in links:\n-            item = ScrapedItem()\n-            item.set_adaptors({\n-                'name': self.adaptor_pipe,\n-                'url': self.adaptor_pipe,\n-                'description': self.adaptor_pipe,\n-            })\n-\n-            item.attribute('name', link.x('a/text()'))\n-            item.attribute('url', link.x('a/@href'))\n-            item.attribute('description', link.x('font[2]/text()'))\n-            items.append(item)\n-\n-        items_to_csv(self.csv_file, items)\n-        return items\n-\n-SPIDER = GoogleDirectorySpider()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#730575a53f64df672d3b3c90be017451bc9323c6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 563 | Contributors (this commit): 4 | Commits (past 90d): 9 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -179,7 +179,13 @@ def string_camelcase(string):\n # shutil.copytree from Python 2.6 (for backwards compatibility)\n import fnmatch\n import os\n-from shutil import copy2, copystat, WindowsError\n+from shutil import copy2, copystat\n+\n+\n+try:\n+    WindowsError\n+except NameError:\n+    WindowsError = None\n \n \n def ignore_patterns(*patterns):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#489e86bd16ab8072d9c99ef3651cb4121073fefb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 70 | Lines Deleted: 64 | Files Changed: 3 | Hunks: 6 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/16 | Churn Δ: 134 | Churn Cumulative: 1151 | Contributors (this commit): 4 | Commits (past 90d): 21 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -7,7 +7,7 @@ from optparse import OptionParser\n \n import scrapy\n from scrapy.utils.misc import render_templatefile, string_camelcase\n-from scrapy.utils.misc import ignore_patterns, copytree\n+from scrapy.utils.python import ignore_patterns, copytree\n \n usage = \"\"\"\n scrapy-admin.py [options] [command]\n@@ -44,7 +44,6 @@ def main():\n         if len(args) >= 2:\n             project_name = args[1]\n             project_root_path = project_name\n-            project_module_path = '%s/%s' % (project_name, project_name)\n \n             roottpl = os.path.join(PROJECT_TEMPLATES_PATH, 'root')\n             copytree(roottpl, project_name, ignore=IGNORE)\n\n@@ -175,65 +175,3 @@ def string_camelcase(string):\n     \"\"\"\n     return CAMELCASE_INVALID_CHARS.sub('', string.title())\n \n-\n-# shutil.copytree from Python 2.6 (for backwards compatibility)\n-import fnmatch\n-import os\n-from shutil import copy2, copystat\n-\n-\n-try:\n-    WindowsError\n-except NameError:\n-    WindowsError = None\n-\n-\n-def ignore_patterns(*patterns):\n-    def _ignore_patterns(path, names):\n-        ignored_names = []\n-        for pattern in patterns:\n-            ignored_names.extend(fnmatch.filter(names, pattern))\n-        return set(ignored_names)\n-    return _ignore_patterns\n-\n-\n-def copytree(src, dst, symlinks=False, ignore=None):\n-    names = os.listdir(src)\n-    if ignore is not None:\n-        ignored_names = ignore(src, names)\n-    else:\n-        ignored_names = set()\n-\n-    os.makedirs(dst)\n-    errors = []\n-    for name in names:\n-        if name in ignored_names:\n-            continue\n-        srcname = os.path.join(src, name)\n-        dstname = os.path.join(dst, name)\n-        try:\n-            if symlinks and os.path.islink(srcname):\n-                linkto = os.readlink(srcname)\n-                os.symlink(linkto, dstname)\n-            elif os.path.isdir(srcname):\n-                copytree(srcname, dstname, symlinks, ignore)\n-            else:\n-                copy2(srcname, dstname)\n-            # XXX What about devices, sockets etc.?\n-        except (IOError, os.error), why:\n-            errors.append((srcname, dstname, str(why)))\n-        # catch the Error from the recursive copytree so that we can\n-        # continue with other files\n-        except Error, err:\n-            errors.extend(err.args[0])\n-    try:\n-        copystat(src, dst)\n-    except OSError, why:\n-        if WindowsError is not None and isinstance(why, WindowsError):\n-            # Copying file access times may fail on Windows\n-            pass\n-        else:\n-            errors.extend((src, dst, str(why)))\n-    if errors:\n-        raise Error, errors\n-\n\n@@ -1,7 +1,14 @@\n \"\"\"\n This module contains essential stuff that should've come with Python itself ;)\n+\n+It also contains functions (or functionality) which is in Python versions\n+higher than 2.5 which is the lower version support by Scrapy.\n+\n \"\"\"\n import re\n+import fnmatch\n+import os\n+from shutil import copy2, copystat\n from sgmllib import SGMLParser\n \n class FixedSGMLParser(SGMLParser):\n@@ -165,3 +172,65 @@ def isbinarytext(text):\n     \"\"\"\n     assert isinstance(text, str), \"text must be str, got '%s'\" % type(text).__name__\n     return any(c in _BINARYCHARS for c in text)\n+\n+\n+# ----- shutil.copytree function from Python 2.6 adds ignore argument ---- #\n+\n+try:\n+    WindowsError\n+except NameError:\n+    WindowsError = None\n+\n+class Error(EnvironmentError):\n+    pass\n+\n+def ignore_patterns(*patterns):\n+    def _ignore_patterns(path, names):\n+        ignored_names = []\n+        for pattern in patterns:\n+            ignored_names.extend(fnmatch.filter(names, pattern))\n+        return set(ignored_names)\n+    return _ignore_patterns\n+\n+def copytree(src, dst, symlinks=False, ignore=None):\n+    names = os.listdir(src)\n+    if ignore is not None:\n+        ignored_names = ignore(src, names)\n+    else:\n+        ignored_names = set()\n+\n+    os.makedirs(dst)\n+    errors = []\n+    for name in names:\n+        if name in ignored_names:\n+            continue\n+        srcname = os.path.join(src, name)\n+        dstname = os.path.join(dst, name)\n+        try:\n+            if symlinks and os.path.islink(srcname):\n+                linkto = os.readlink(srcname)\n+                os.symlink(linkto, dstname)\n+            elif os.path.isdir(srcname):\n+                copytree(srcname, dstname, symlinks, ignore)\n+            else:\n+                copy2(srcname, dstname)\n+            # XXX What about devices, sockets etc.?\n+        except (IOError, os.error), why:\n+            errors.append((srcname, dstname, str(why)))\n+        # catch the Error from the recursive copytree so that we can\n+        # continue with other files\n+        except Error, err:\n+            errors.extend(err.args[0])\n+    try:\n+        copystat(src, dst)\n+    except OSError, why:\n+        if WindowsError is not None and isinstance(why, WindowsError):\n+            # Copying file access times may fail on Windows\n+            pass\n+        else:\n+            errors.extend((src, dst, str(why)))\n+    if errors:\n+        raise Error, errors\n+\n+# ----- end of shutil.copytree function from Python 2.6 ---- #\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#714c3e20e404a53634d4f88506ba1afd50be0006", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 260 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -2,7 +2,7 @@\n This module contains essential stuff that should've come with Python itself ;)\n \n It also contains functions (or functionality) which is in Python versions\n-higher than 2.5 which is the lower version support by Scrapy.\n+higher than 2.5 which is the lowest version supported by Scrapy.\n \n \"\"\"\n import re\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#84f63b146ba57046c457c3c46c6251d8b15578e3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 28 | Files Changed: 1 | Hunks: 21 | Methods Changed: 12 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 81 | Churn Cumulative: 83 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -95,25 +95,31 @@ class ClusterNodeBroker(pb.Referenceable):\n         \"\"\"\n \n         domain = domain_info['domain']\n-        dsettings  = domain_info['settings']\n         priority = domain_info['priority']\n+        spider_settings  = domain_info['settings']\n+        dsettings = self.master.compute_final_spider_settings(domain, spider_settings)\n \n         def _run_errback(failure):\n             self._logfailure(\"Error while running domain=%s\" % domain, failure)\n             self.master.loading.remove(domain)\n             newprio = priority - 1 # increase priority for reschedule\n-            self.master.reschedule([domain], dsettings, newprio, reason=\"error while try to run it\")\n+            self.master.reschedule([domain], spider_settings, newprio,\n+                    reason=\"error while try to run it\")\n \n         def _run_callback(status):\n             if status['callresponse'][0] == ResponseCode.NO_FREE_SLOT:\n-                log.msg(\"ClusterMaster: No available slots at worker=%s when trying to run domain=%s\" % (self.name, domain), log.WARNING)\n+                log.msg(\"ClusterMaster: No available slots at worker=%s when trying to run domain=%s\"\n+                        % (self.name, domain), log.WARNING)\n                 self.master.loading.remove(domain)\n                 newprio = priority - 1 # increase priority for rerunning asap\n-                self.master.reschedule([domain], dsettings, newprio, reason=\"no available slots at worker=%s\" % self.name)\n+                self.master.reschedule([domain], spider_settings, newprio,\n+                        reason=\"no available slots at worker=%s\" % self.name)\n             elif status['callresponse'][0] == ResponseCode.DOMAIN_ALREADY_RUNNING:\n-                log.msg(\"ClusterMaster: Already running domain=%s at worker=%s\" % (domain, self.name), log.WARNING)\n+                log.msg(\"ClusterMaster: Already running domain=%s at worker=%s\" %\n+                        (domain, self.name), log.WARNING)\n                 self.master.loading.remove(domain)\n-                self.master.reschedule([domain], dsettings, priority, reason=\"domain already running at worker=%s\" % self.name)\n+                self.master.reschedule([domain], spider_settings, priority,\n+                        reason=\"domain already running at worker=%s\" % self.name)\n \n         try:\n             log.msg(\"ClusterMaster: Running domain=%s at worker=%s\" % (domain, self.name), log.DEBUG)\n@@ -127,16 +133,21 @@ class ClusterNodeBroker(pb.Referenceable):\n     def remote_update(self, worker_status, domain, domain_status):\n         \"\"\"Called remotely form worker when domains finish to update status\"\"\"\n         self._set_status(worker_status)\n+        stats = self.master.statistics\n+        dstats = stats[\"domains\"]\n+\n         if domain in self.master.loading and domain_status == \"running\":\n             self.master.loading.remove(domain)\n-            self.master.statistics[\"domains\"][\"running\"].add(domain)\n+            dstats[\"running\"].add(domain)\n         elif domain_status in (\"done\", \"terminated\"):\n-            self.master.statistics[\"domains\"][\"running\"].remove(domain)\n-            self.master.statistics[\"domains\"][\"scraped\"][domain] = self.master.statistics[\"domains\"][\"scraped\"].get(domain, 0) + 1\n-            self.master.statistics[\"scraped_count\"] = self.master.statistics.get(\"scraped_count\", 0) + 1\n-            if domain in self.master.statistics[\"domains\"][\"lost\"]:\n-                self.master.statistics[\"domains\"][\"lost\"].remove(domain)\n-        log.msg(\"ClusterMaster: Changed status to <%s> for domain=%s at worker=%s\" % (domain_status, domain, self.name))\n+            dstats[\"running\"].remove(domain)\n+            dstats[\"scraped\"][domain] = dstats[\"scraped\"].get(domain, 0) + 1\n+            stats[\"scraped_count\"] = stats.get(\"scraped_count\", 0) + 1\n+            if domain in dstats[\"lost\"]:\n+                dstats[\"lost\"].remove(domain)\n+\n+        log.msg(\"ClusterMaster: Changed status to <%s> for domain=%s at worker=%s\" %\n+                (domain_status, domain, self.name))\n \n     def _logfailure(self, msg, failure):\n         log.msg(\"ClusterMaster: %s (worker=%s)\\n%s\" % (msg, self.name, failure), log.ERROR)\n@@ -154,13 +165,17 @@ class ClusterNodeBroker(pb.Referenceable):\n             self.logdir = status['logdir']\n             free_slots = self.maxproc - len(self.running)\n \n-            # load domains by one, so to mix up better the domain loading between nodes. The next one in the same node will be loaded\n-            # when there is no loading domain or in the next status update. This way also we load the nodes softly\n+            # load domains by one, so to mix up better the domain loading between nodes.\n+            # The next one in the same node will be loaded\n+            # when there is no loading domain or in the next status update.\n+            # This way also we load the nodes softly\n             if self.available and free_slots > 0 and self.master.pending:\n                 pending = self.master.pending.pop(0)\n-                # if domain already running in some node, reschedule with same priority (so it will be run later)\n+                # If domain already running in some node, reschedule with same\n+                # priority (so it will be run later)\n                 if pending['domain'] in self.master.running or pending['domain'] in self.master.loading:\n-                    self.master.reschedule([pending['domain']], pending['settings'], pending['priority'], reason=\"domain already running in other worker\")\n+                    self.master.reschedule([pending['domain']], pending['settings'],\n+                            pending['priority'], reason=\"domain already running in other worker\")\n                 else:\n                     self.run(pending)\n                     self.master.loading.append(pending['domain'])\n@@ -233,10 +248,12 @@ class ClusterMaster(object):\n         try:\n             reactor.connectTCP(server, port, factory)\n         except Exception, err:\n-            log.msg(\"ClusterMaster: Could not connect to worker=%s (%s): %s\" % (name, hostport, err), log.ERROR)\n+            log.msg(\"ClusterMaster: Could not connect to worker=%s (%s): %s\" %\n+                    (name, hostport, err), log.ERROR)\n         else:\n             def _eb(failure):\n-                log.msg(\"ClusterMaster: Could not connect to worker=%s (%s): %s\" % (name, hostport, failure.value), log.ERROR)\n+                log.msg(\"ClusterMaster: Could not connect to worker=%s (%s): %s\" %\n+                        (name, hostport, failure.value), log.ERROR)\n \n             d = factory.getRootObject()\n             d.addCallbacks(callback=lambda obj: self.add_node(obj, name), errback=_eb)\n@@ -249,11 +266,12 @@ class ClusterMaster(object):\n             else:\n                 self.load_node(name, hostport)\n \n+        dstats = self.statistics[\"domains\"]\n         real_running = set(self.running.keys())\n-        lost = self.statistics[\"domains\"][\"running\"].difference(real_running)\n+        lost = dstats[\"running\"].difference(real_running)\n         for domain in lost:\n-            self.statistics[\"domains\"][\"lost_count\"][domain] = self.statistics[\"domains\"][\"lost_count\"].get(domain, 0) + 1\n-        self.statistics[\"domains\"][\"lost\"] = self.statistics[\"domains\"][\"lost\"].union(lost)\n+            dstats[\"lost_count\"][domain] = dstats[\"lost_count\"].get(domain, 0) + 1\n+        dstats[\"lost\"] = dstats[\"lost\"].union(lost)\n \n     def add_node(self, cworker, name):\n         \"\"\"Add node given its node\"\"\"\n@@ -282,20 +300,27 @@ class ClusterMaster(object):\n                     pd['priority'] = priority\n                     self.pending.insert(insert_pos, pd)\n             else:\n-                final_spider_settings = self.get_spider_groupsettings(domain)\n-                final_spider_settings.update(self.global_settings)\n-                final_spider_settings.update(spider_settings or {})\n-                self.pending.insert(insert_pos, {'domain': domain, 'settings': final_spider_settings, 'priority': priority})\n+                self.pending.insert(insert_pos,\n+                        {'domain': domain, 'settings': spider_settings, 'priority': priority})\n+\n+    def compute_final_spider_settings(self, domain, spider_settings=None):\n+        \"\"\"Return merged dictionary with final settings to run spider\"\"\"\n+        final = dict(self.get_spider_groupsettings(domain) or {})\n+        final.update(self.global_settings)\n+        final.update(spider_settings or {})\n+        return final\n \n     def schedule(self, domains, spider_settings=None, priority=20):\n         \"\"\"Schedule the given domains, with the given priority\"\"\"\n         self._schedule(domains, spider_settings, priority)\n-        log.msg(\"clustermaster: Scheduled domains=%s with priority=%s\" % (','.join(domains), priority), log.DEBUG)\n+        log.msg(\"clustermaster: Scheduled domains=%s with priority=%s\" %\n+                (','.join(domains), priority), log.DEBUG)\n \n     def reschedule(self, domains, spider_settings=None, priority=20, reason=None):\n         \"\"\"Reschedule the given domains, with the given priority\"\"\"\n         self._schedule(domains, spider_settings, priority)\n-        log.msg(\"clustermaster: Rescheduled domains=%s with priority=%s reason='%s'\" % (','.join(domains), priority, reason), log.DEBUG)\n+        log.msg(\"clustermaster: Rescheduled domains=%s with priority=%s reason='%s'\" %\n+                (','.join(domains), priority, reason), log.DEBUG)\n \n \n     def stop(self, domains):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b20ac057b33ea18d192e7911ea323e48884ef34f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 11 | Churn Cumulative: 66 | Contributors (this commit): 3 | Commits (past 90d): 14 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,2 +1,2 @@\n-from scrapy.contrib.spiders.crawl import CrawlSpider, Rule\n+from scrapy.contrib.spiders.crawl import CrawlSpider, Rule, rule\n from scrapy.contrib.spiders.feed import XMLFeedSpider, CSVFeedSpider\n\n@@ -10,6 +10,7 @@ import copy\n from scrapy.http import Request\n from scrapy.spider import BaseSpider\n from scrapy.conf import settings\n+from scrapy.link.extractors import RegexLinkExtractor\n \n class Rule(object):\n     \"\"\"\n@@ -45,6 +46,14 @@ class Rule(object):\n         else:\n             self.follow = follow\n \n+\n+def rule(regex, callback=None, exclude=None, follow=None):\n+    \"\"\"Shorthand for creating CrawlSpider rules\"\"\"\n+    deny = [exclude] if exclude else []\n+    link_extractor = RegexLinkExtractor(allow=[regex], deny=deny) \n+    return Rule(link_extractor, callback=callback, follow=follow)\n+\n+\n class CrawlSpider(BaseSpider):\n     \"\"\"\n     Class for spiders that crawl over web pages and extract/parse their links\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f8affd0c3d4de330bdcc8c87cb2235beed70fd73", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 83 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 24/24 | Churn Δ: 83 | Churn Cumulative: 83 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.7058823529411765\n\nDIFF:\n@@ -0,0 +1,83 @@\n+import re\n+\n+class ItemFieldValueError(Exception):\n+    pass\n+\n+\n+class ItemField(object):\n+    def __init__(self, required=False):\n+        self.required = required\n+\n+    def to_python(self, value):\n+        pass\n+\n+\n+class BooleanItemField(ItemField):\n+    def to_python(self, value):\n+        if value in (True, False): return value\n+        if value in ('t', 'True', '1'): return True\n+        if value in ('f', 'False', '0'): return False\n+        raise ItemFieldValueError(\"This value must be either True or False.\")\n+\n+\n+ansi_date_re = re.compile(r'^\\d{4}-\\d{1,2}-\\d{1,2}$')\n+\n+\n+class DateItemField(ItemField):\n+    def to_python(self, value):\n+        if value is None:\n+            return value\n+        if isinstance(value, datetime.datetime):\n+            return value.date()\n+        if isinstance(value, datetime.date):\n+            return value\n+\n+        if not ansi_date_re.search(value):\n+            raise ItemFieldValueError(\n+                \"Enter a valid date in YYYY-MM-DD format.\")\n+\n+        year, month, day = map(int, value.split('-'))\n+        try:\n+            return datetime.date(year, month, day)\n+        except ValueError, e:\n+            raise ItemFieldValueError(\"Invalid date: %s\" % str(e))\n+\n+\n+class DecimalItemField(ItemField):\n+    def to_python(self, value):\n+        if value is None:\n+            return value\n+        try:\n+            return decimal.Decimal(value)\n+        except decimal.InvalidOperation:\n+            raise ItemFieldValueError(\"This value must be a decimal number.\")\n+\n+\n+class FloatItemField(ItemField):\n+    def to_python(self, value):\n+        if value is None:\n+            return value\n+        try:\n+            return float(value)\n+        except (TypeError, ValueError):\n+            raise ItemFieldValueError(\"This value must be a float.\")\n+\n+\n+class IntegerItemField(ItemField):\n+    def to_python(self, value):\n+        if value is None:\n+            return value\n+        try:\n+            return int(value)\n+        except (TypeError, ValueError):\n+            raise ItemFieldValueError(\"This value must be an integer.\")\n+\n+\n+class StringItemField(ItemField):\n+    def to_python(self, value):\n+        if isinstance(value, basestring):\n+            return value\n+        if value is None:\n+            return value\n+        raise ItemFieldValueError(\"This field must be a string.\")\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3039453148fab51c1b6bceba4016d5472a637005", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 62 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 7 | Methods Changed: 10 | Complexity Δ (Sum/Max): 40/27 | Churn Δ: 64 | Churn Cumulative: 64 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,2 @@\n+from scrapy.contrib.newitem.models import Item\n+from scrapy.contrib.newitem.fields import *\n\n@@ -1,15 +1,31 @@\n+import datetime\n+import decimal\n import re\n \n+\n class ItemFieldValueError(Exception):\n     pass\n \n \n class ItemField(object):\n-    def __init__(self, required=False):\n+    def __init__(self, required=False, default=None):\n         self.required = required\n+        self.default = default\n \n     def to_python(self, value):\n-        pass\n+        \"\"\"\n+        Converts the input value into the expected Python data type.\n+        Subclasses should override this.\n+        \"\"\"\n+        return value\n+\n+    def from_list(self, list):\n+        \"Converts the input list into the expected Python data type.\"\n+        return self.to_python(list.join(''))\n+\n+    def default_value(self):\n+        \"Returns the default value for this field\"\n+        return self.default or self.to_python(None)\n \n \n class BooleanItemField(ItemField):\n\n@@ -0,0 +1,42 @@\n+from scrapy.contrib.newitem.fields import ItemField\n+\n+\n+class Item(object):\n+    \"\"\"\n+    This is the base class for all scraped items.\n+    \"\"\"\n+\n+    def __init__(self):\n+        self._values = {}\n+        self._fields = self._get_fields()\n+\n+    def _get_fields(self):\n+        return dict(i for i in self.__class__.__dict__.iteritems() if\n+                    isinstance(i[1], ItemField))\n+\n+    def __setattr__(self, name, value):\n+        if not name.startswith('_') and name in self._fields.keys():\n+            self._values[name] = self._fields[name].to_python(value)\n+        else:\n+            # for now setting values that are not fields is permitted\n+            object.__setattr__(self, name, value)\n+\n+    def __getattribute__(self, name):\n+        if not name.startswith('_') and name in self._fields.keys():\n+            try:\n+                return self._values[name]\n+            except KeyError:\n+                # what to to in this case?\n+                # maybe return a default value:\n+                # return self._fields[name].default_value()\n+                return None\n+        else:\n+            return object.__getattribute__(self, name)\n+\n+    def __repr__(self):\n+        \"\"\"\n+        Generate the following format so that items can be deserialized\n+        easily: ClassName({'attrib': value, ...})\n+        \"\"\"\n+        reprdict = dict((field, getattr(self, field)) for field in self._fields)\n+        return \"%s(%s)\" % (self.__class__.__name__, repr(reprdict))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9739f9320edd57ebc71f7ee85d87004bbe0229e2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n\n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#de13bbe7f507f47d17d07d368765013612c89505", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 93 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -40,6 +40,7 @@ class DuplicatesFilterMiddleware(object):\n \n     def process_spider_output(self, response, result, spider):\n         domain = spider.domain_name\n+        self.filter.add(domain, response.request)\n \n         for req in result:\n             if isinstance(req, Request):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#14900a74520f866603b507aea2c191f0622774ae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 340 | Contributors (this commit): 2 | Commits (past 90d): 17 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -41,7 +41,7 @@ class RegexLinkExtractor(LinkExtractor):\n     \"\"\"\n \n     def __init__(self, allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(), \n-                 tags=('a', 'area'), attrs=('href'), canonicalize=True, unique=True):\n+                 tags=('a', 'area'), attrs=('href',), canonicalize=True, unique=True):\n         self.allow_res = [x if isinstance(x, _re_type) else re.compile(x) for x in allow]\n         self.deny_res = [x if isinstance(x, _re_type) else re.compile(x) for x in deny]\n         self.allow_domains = set(allow_domains)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c546585af0cc32ff2440351c5a8bfa1db58ba5ac", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 12 | Files Changed: 3 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 26 | Churn Cumulative: 512 | Contributors (this commit): 3 | Commits (past 90d): 27 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -40,8 +40,7 @@ class DuplicatesFilterMiddleware(object):\n \n     def process_spider_output(self, response, result, spider):\n         domain = spider.domain_name\n-        # FIXME there's a conflict between test_spidermiddleware_duplicatesfilter.py and test_engine.py\n-        #self.filter.add(domain, response.request)\n+        self.filter.add(domain, response.request)\n \n         for req in result:\n             if isinstance(req, Request):\n\n@@ -41,7 +41,7 @@ class RegexLinkExtractor(LinkExtractor):\n     \"\"\"\n \n     def __init__(self, allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(), \n-                 tags=('a', 'area'), attrs=('href',), canonicalize=True, unique=True):\n+                 tags=('a', 'area'), attrs=('href'), canonicalize=True, unique=True):\n         self.allow_res = [x if isinstance(x, _re_type) else re.compile(x) for x in allow]\n         self.deny_res = [x if isinstance(x, _re_type) else re.compile(x) for x in deny]\n         self.allow_domains = set(allow_domains)\n\n@@ -15,16 +15,19 @@ class DuplicatesFilterMiddlewareTest(unittest.TestCase):\n         mw = DuplicatesFilterMiddleware()\n         mw.filter.open('scrapytest.org')\n \n-        response = Response('')\n+        rq = Request('http://scrapytest.org/')\n+        response = Response('http://scrapytest.org/')\n+        response.request = rq\n         r1 = Request('http://scrapytest.org/1')\n         r2 = Request('http://scrapytest.org/2')\n         r3 = Request('http://scrapytest.org/2')\n \n         filtered = list(mw.process_spider_output(response, [r1, r2, r3], self.spider))\n \n-        assert r1 in filtered\n-        assert r2 in filtered\n-        assert r3 not in filtered\n+        self.assertFalse(rq in filtered)\n+        self.assertTrue(r1 in filtered)\n+        self.assertTrue(r2 in filtered)\n+        self.assertFalse(r3 in filtered)\n \n         mw.filter.close('scrapytest.org')\n \n@@ -35,16 +38,16 @@ class SimplePerDomainFilterTest(unittest.TestCase):\n         domain = 'scrapytest.org'\n         filter = SimplePerDomainFilter()\n         filter.open(domain)\n-        assert domain in filter\n+        self.assertTrue(domain in filter)\n \n         r1 = Request('http://scrapytest.org/1')\n         r2 = Request('http://scrapytest.org/2')\n         r3 = Request('http://scrapytest.org/2')\n \n-        assert filter.add(domain, r1)\n-        assert filter.add(domain, r2)\n-        assert not filter.add(domain, r3)\n+        self.assertTrue(filter.add(domain, r1))\n+        self.assertTrue(filter.add(domain, r2))\n+        self.assertFalse(filter.add(domain, r3))\n \n         filter.close(domain)\n-        assert domain not in filter\n+        self.assertFalse(domain in filter)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d95878c0f7764889b0242d9cb6d79d288c36c0bd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 13/13 | Churn Δ: 6 | Churn Cumulative: 6 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,2 +1,2 @@\n-from scrapy.contrib.newitem.models import Item\n-from scrapy.contrib.newitem.fields import *\n+from scrapy.contrib_exp.newitem.models import Item\n+from scrapy.contrib_exp.newitem.fields import *\n\n@@ -1,4 +1,4 @@\n-from scrapy.contrib.newitem.fields import ItemField\n+from scrapy.contrib_exp.newitem.fields import ItemField\n \n \n class Item(object):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1017f12163410205b954c974a8db26fe560d43e8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 17 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,14 +11,16 @@ class Item(object):\n         self._fields = self._get_fields()\n \n     def _get_fields(self):\n-        return dict(i for i in self.__class__.__dict__.iteritems() if\n-                    isinstance(i[1], ItemField))\n+        return dict(i for i in self.__class__.__dict__.iteritems() \\\n+                    if isinstance(i[1], ItemField))\n \n     def __setattr__(self, name, value):\n-        if not name.startswith('_') and name in self._fields.keys():\n+        if not name.startswith('_'):\n+            if name in self._fields.keys():\n                 self._values[name] = self._fields[name].to_python(value)\n             else:\n-            # for now setting values that are not fields is permitted\n+                raise AttributeError(name)\n+        else:\n             object.__setattr__(self, name, value)\n \n     def __getattribute__(self, name):\n@@ -26,10 +28,7 @@ class Item(object):\n             try:\n                 return self._values[name]\n             except KeyError:\n-                # what to to in this case?\n-                # maybe return a default value:\n-                # return self._fields[name].default_value()\n-                return None\n+                return self._fields[name].default_value()\n         else:\n             return object.__getattribute__(self, name)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#22549161a50b26c21343b9bec8a9e075dd46b223", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 7 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 25 | Churn Cumulative: 96 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -15,19 +15,20 @@ class DuplicatesFilterMiddlewareTest(unittest.TestCase):\n         mw = DuplicatesFilterMiddleware()\n         mw.filter.open('scrapytest.org')\n \n-        rq = Request('http://scrapytest.org/')\n         response = Response('http://scrapytest.org/')\n-        response.request = rq\n+        response.request = Request('http://scrapytest.org/')\n+\n+        r0 = Request('http://scrapytest.org/')\n         r1 = Request('http://scrapytest.org/1')\n         r2 = Request('http://scrapytest.org/2')\n         r3 = Request('http://scrapytest.org/2')\n \n-        filtered = list(mw.process_spider_output(response, [r1, r2, r3], self.spider))\n+        filtered = list(mw.process_spider_output(response, [r0, r1, r2, r3], self.spider))\n \n-        self.assertFalse(rq in filtered)\n-        self.assertTrue(r1 in filtered)\n-        self.assertTrue(r2 in filtered)\n-        self.assertFalse(r3 in filtered)\n+        assert r0 not in filtered\n+        assert r1 in filtered\n+        assert r2 in filtered\n+        assert r3 not in filtered\n \n         mw.filter.close('scrapytest.org')\n \n@@ -38,16 +39,16 @@ class SimplePerDomainFilterTest(unittest.TestCase):\n         domain = 'scrapytest.org'\n         filter = SimplePerDomainFilter()\n         filter.open(domain)\n-        self.assertTrue(domain in filter)\n+        assert domain in filter\n \n         r1 = Request('http://scrapytest.org/1')\n         r2 = Request('http://scrapytest.org/2')\n         r3 = Request('http://scrapytest.org/2')\n \n-        self.assertTrue(filter.add(domain, r1))\n-        self.assertTrue(filter.add(domain, r2))\n-        self.assertFalse(filter.add(domain, r3))\n+        assert filter.add(domain, r1)\n+        assert filter.add(domain, r2)\n+        assert not filter.add(domain, r3)\n \n         filter.close(domain)\n-        self.assertFalse(domain in filter)\n+        assert domain not in filter\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
