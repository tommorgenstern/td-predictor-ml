{"custom_id": "keras#335989b42ed14150821861b2996afc66d2af583e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 67 | Files Changed: 3 | Hunks: 19 | Methods Changed: 12 | Complexity Δ (Sum/Max): -7/0 | Churn Δ: 103 | Churn Cumulative: 1692 | Contributors (this commit): 10 | Commits (past 90d): 21 | Contributors (cumulative): 14 | DMM Complexity: 0.0\n\nDIFF:\n@@ -19,24 +19,9 @@ class TFOptimizer(base_optimizer.BaseOptimizer):\n         super().__init__(*args, **kwargs)\n         self._distribution_strategy = tf.distribute.get_strategy()\n \n-    def build(self, variables):\n-        super().build(variables)\n-        if hasattr(self, \"_ema_vars_initialized\"):\n-            # We must use `backend.Variable` in\n-            # `self._update_model_variables_moving_average` to correctly\n-            # initialize moving averages with the same value as\n-            # `trainable_variables` during the first `self.apply`.\n-            with backend.name_scope(self.name, caller=self):\n-                _ema_vars_initialized = backend.Variable(\n-                    False,\n-                    name=\"_ema_vars_initialized\",\n-                    dtype=\"bool\",\n-                    trainable=False,\n-                )\n-                self._track_variable(_ema_vars_initialized)\n-                self._ema_vars_initialized = _ema_vars_initialized\n-\n-    def add_variable_from_reference(self, reference_variable, name=None):\n+    def add_variable_from_reference(\n+        self, reference_variable, name=None, initializer=\"zeros\"\n+    ):\n         if isinstance(reference_variable, backend.Variable):\n             colocate_var = reference_variable.value\n         else:\n@@ -46,7 +31,7 @@ class TFOptimizer(base_optimizer.BaseOptimizer):\n             colocate_var\n         ):\n             return super().add_variable_from_reference(\n-                reference_variable, name=name\n+                reference_variable, name=name, initializer=initializer\n             )\n \n     def stateless_apply(self, optimizer_variables, grads, trainable_variables):\n@@ -178,26 +163,3 @@ class TFOptimizer(base_optimizer.BaseOptimizer):\n             grads,\n             accumulators,\n         )\n-\n-    def _update_model_variables_moving_average(self, trainable_variables):\n-        \"\"\"Update the stored moving average using the latest value.\"\"\"\n-        if self.use_ema:\n-            for var, average in zip(\n-                trainable_variables, self._model_variables_moving_average\n-            ):\n-\n-                def _update_fn(var, average):\n-                    average.assign(\n-                        self.ema_momentum * average\n-                        + (1 - self.ema_momentum) * var\n-                    )\n-\n-                def _assign_fn(var, average):\n-                    average.assign(var)\n-\n-                tf.cond(\n-                    self._ema_vars_initialized,\n-                    lambda: _update_fn(var, average),\n-                    lambda: _assign_fn(var, average),\n-                )\n-            self._ema_vars_initialized.assign(True)\n\n@@ -139,7 +139,6 @@ class BaseOptimizer:\n     def build(self, variables):\n         if self.use_ema:\n             self._model_variables_moving_average = []\n-            self._ema_vars_initialized = False\n         if self.gradient_accumulation_steps:\n             self._accumulated_gradients = []\n         for i, variable in enumerate(variables):\n@@ -148,14 +147,16 @@ class BaseOptimizer:\n                 self._model_variables_moving_average.append(\n                     self.add_variable_from_reference(\n                         variable,\n-                        \"average\",\n+                        name=\"average\",\n+                        initializer=None,  # Initialize with var value\n                     )\n                 )\n             if self.gradient_accumulation_steps:\n                 self._accumulated_gradients.append(\n                     self.add_variable_from_reference(\n                         variable,\n-                        \"gradient_accumulator\",\n+                        name=\"gradient_accumulator\",\n+                        initializer=\"zeros\",\n                     )\n                 )\n         self._trainable_variables = variables[:]\n@@ -192,11 +193,16 @@ class BaseOptimizer:\n         self._track_variable(variable)\n         return variable\n \n-    def add_variable_from_reference(self, reference_variable, name=None):\n+    def add_variable_from_reference(\n+        self, reference_variable, name=None, initializer=\"zeros\"\n+    ):\n         \"\"\"Add an all-zeros variable with the shape and dtype of a reference\n         variable.\n         \"\"\"\n-        initializer = initializers.Zeros()\n+        if initializer is not None:\n+            initializer = initializers.get(initializer)\n+        else:\n+            initializer = lambda _: reference_variable.value  # noqa: E731\n         name = name or \"var\"\n         if hasattr(reference_variable, \"path\"):\n             name = reference_variable.path.replace(\"/\", \"_\") + \"_\" + name\n@@ -676,14 +682,9 @@ class BaseOptimizer:\n             for var, average in zip(\n                 trainable_variables, self._model_variables_moving_average\n             ):\n-                if self._ema_vars_initialized:\n                 average.assign(\n-                        self.ema_momentum * average\n-                        + (1 - self.ema_momentum) * var\n+                    self.ema_momentum * average + (1 - self.ema_momentum) * var\n                 )\n-                else:\n-                    average.assign(var)\n-            self._ema_vars_initialized = True\n \n     def _overwrite_model_variables_with_average_value(\n         self, trainable_variables\n\n@@ -7,7 +7,6 @@ from keras import layers\n from keras import models\n from keras import optimizers\n from keras import testing\n-from keras.utils import numerical_utils\n \n \n class OptimizerTest(testing.TestCase):\n@@ -34,35 +33,42 @@ class OptimizerTest(testing.TestCase):\n         self.assertAllClose(v, [[2.0, 3.0], [4.0, 5.0]])\n         self.assertAllClose(\n             optimizer._model_variables_moving_average[0],\n-            [[2.0, 3.0], [4.0, 5.0]],\n+            [[2.9, 3.9], [4.9, 5.9]],  # avg of initial v + current v\n         )\n         optimizer.apply_gradients([(grads, v)])\n         self.assertAllClose(v, [[1.0, 2.0], [3.0, 4.0]])\n         self.assertAllClose(\n             optimizer._model_variables_moving_average[0],\n-            [[1.9, 2.9], [3.9, 4.9]],\n+            [[2.71, 3.71], [4.71, 5.71]],\n         )\n         optimizer.apply_gradients([(grads, v)])\n-        self.assertAllClose(v, [[1.71, 2.71], [3.71, 4.71]])\n+        # Variables were overwritten with EMA\n+        self.assertAllClose(v, [[2.439, 3.439], [4.439, 5.439]])\n         self.assertAllClose(\n             optimizer._model_variables_moving_average[0],\n-            [[1.71, 2.71], [3.71, 4.71]],\n+            [[2.439, 3.439], [4.439, 5.439]],\n         )\n \n     @pytest.mark.requires_trainable_backend\n     def test_ema_with_model_fit(self):\n-        x_train = np.ones((10, 3)).astype(\"float32\")\n-        y_train = np.zeros((10,)).astype(\"float32\")\n-        y_train = numerical_utils.to_categorical(y_train)\n-        optimizer = optimizers.SGD(use_ema=True, ema_momentum=0.9999)\n-        model = models.Sequential(\n-            [layers.Dense(2, use_bias=False, kernel_initializer=\"ones\")]\n+        x_train = np.ones((1, 1)).astype(\"float32\")\n+        y_train = np.zeros((1, 1)).astype(\"float32\")\n+        optimizer = optimizers.SGD(\n+            learning_rate=1.0, use_ema=True, ema_momentum=0.9\n+        )\n+        model = models.Sequential(\n+            [layers.Dense(2, kernel_initializer=\"ones\", use_bias=False)]\n+        )\n+        model.compile(loss=\"mse\", optimizer=optimizer, run_eagerly=True)\n+        model.fit(x_train, y_train, batch_size=1, epochs=2)\n+        self.assertAllClose(\n+            optimizer._model_variables_moving_average[0].numpy(),\n+            [[0.81, 0.81]],\n+            atol=1e-5,\n         )\n-        model.compile(loss=\"mse\", optimizer=optimizer)\n-        model.fit(x_train, y_train, batch_size=10, epochs=1)\n         self.assertAllClose(\n             model.trainable_variables[0].numpy(),\n-            [[0.98, 0.98], [0.98, 0.98], [0.98, 0.98]],\n+            [[0.81, 0.81]],\n             atol=1e-5,\n         )\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#815353e1be1734fc0624eda2ac9d57d4b64d3e48", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 359 | Contributors (this commit): 4 | Commits (past 90d): 13 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -153,12 +153,21 @@ class OptimizerDistributeTest(testing.TestCase):\n             self.assertAllClose(v, [[2.0, 3.0], [4.0, 5.0]])\n             self.assertAllClose(\n                 optimizer._model_variables_moving_average[0],\n-                [[2.0, 3.0], [4.0, 5.0]],\n+                [[2.9, 3.9], [4.9, 5.9]],  # avg of initial v + current v\n             )\n             self.strategy.run(lambda: optimizer.apply_gradients([(grads, v)]))\n             self.assertAllClose(v, [[1.0, 2.0], [3.0, 4.0]])\n+            self.assertAllClose(\n+                optimizer._model_variables_moving_average[0],\n+                [[2.71, 3.71], [4.71, 5.71]],\n+            )\n             self.strategy.run(lambda: optimizer.apply_gradients([(grads, v)]))\n-            self.assertAllClose(v, [[1.71, 2.71], [3.71, 4.71]])\n+            # Variables were overwritten with EMA\n+            self.assertAllClose(v, [[2.439, 3.439], [4.439, 5.439]])\n+            self.assertAllClose(\n+                optimizer._model_variables_moving_average[0],\n+                [[2.439, 3.439], [4.439, 5.439]],\n+            )\n \n     def test_gradient_accumulation(self):\n         with self.strategy.scope():\n\n@@ -202,7 +202,9 @@ class BaseOptimizer:\n         if initializer is not None:\n             initializer = initializers.get(initializer)\n         else:\n-            initializer = lambda _: reference_variable.value  # noqa: E731\n+            initializer = (\n+                lambda shape, dtype: reference_variable.value\n+            )  # noqa: E731\n         name = name or \"var\"\n         if hasattr(reference_variable, \"path\"):\n             name = reference_variable.path.replace(\"/\", \"_\") + \"_\" + name\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#598d50567ac6b74fc05843d9ad24bfb2f89075da", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 1492 | Contributors (this commit): 9 | Commits (past 90d): 14 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -202,9 +202,9 @@ class BaseOptimizer:\n         if initializer is not None:\n             initializer = initializers.get(initializer)\n         else:\n-            initializer = (\n-                lambda shape, dtype: reference_variable.value\n-            )  # noqa: E731\n+            initializer = lambda shape, dtype: ops.copy(  # noqa: E731\n+                reference_variable.value\n+            )\n         name = name or \"var\"\n         if hasattr(reference_variable, \"path\"):\n             name = reference_variable.path.replace(\"/\", \"_\") + \"_\" + name\n\n@@ -59,7 +59,7 @@ class OptimizerTest(testing.TestCase):\n         model = models.Sequential(\n             [layers.Dense(2, kernel_initializer=\"ones\", use_bias=False)]\n         )\n-        model.compile(loss=\"mse\", optimizer=optimizer, run_eagerly=True)\n+        model.compile(loss=\"mse\", optimizer=optimizer)\n         model.fit(x_train, y_train, batch_size=1, epochs=2)\n         self.assertAllClose(\n             optimizer._model_variables_moving_average[0].numpy(),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#68f2cbda46d7d7ebdb72618f0383c8415131175f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 289 | Contributors (this commit): 3 | Commits (past 90d): 10 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -202,9 +202,10 @@ class BaseOptimizer:\n         if initializer is not None:\n             initializer = initializers.get(initializer)\n         else:\n-            initializer = lambda shape, dtype: ops.copy(  # noqa: E731\n-                reference_variable.value\n-            )\n+\n+            def initializer(shape, dtype):\n+                return ops.copy(reference_variable.value)\n+\n         name = name or \"var\"\n         if hasattr(reference_variable, \"path\"):\n             name = reference_variable.path.replace(\"/\", \"_\") + \"_\" + name\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#efbd02e848bba11eddb096449b31e22e93cca2e6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 71 | Lines Deleted: 138 | Files Changed: 3 | Hunks: 14 | Methods Changed: 14 | Complexity Δ (Sum/Max): -25/16 | Churn Δ: 209 | Churn Cumulative: 497 | Contributors (this commit): 6 | Commits (past 90d): 20 | Contributors (cumulative): 11 | DMM Complexity: 0.9649122807017544\n\nDIFF:\n@@ -62,42 +62,6 @@ class JAXTrainer(base_trainer.Trainer):\n                 loss = self.optimizer.scale_loss(loss)\n         return loss, (unscaled_loss, y_pred, non_trainable_variables)\n \n-    def _eager_build(self, data_batch):\n-        model_unbuilt = not all(layer.built for layer in self._flatten_layers())\n-        compile_metrics_unbuilt = (\n-            self._compile_metrics is not None\n-            and not self._compile_metrics.built\n-        )\n-        if model_unbuilt or compile_metrics_unbuilt:\n-\n-            def _convert_data_to_spec(d):\n-                if d is None:\n-                    return None\n-                return backend.KerasTensor(d.shape, d.dtype)\n-\n-            data_spec = tree.map_structure(_convert_data_to_spec, data_batch)\n-            (\n-                x_spec,\n-                y_spec,\n-                sample_weight_spec,\n-            ) = data_adapter_utils.unpack_x_y_sample_weight(data_spec)\n-            # Note that this __call__ run the forward path and trigger variable\n-            # creation.\n-            y_pred_spec = backend.compute_output_spec(self.__call__, x_spec)\n-            if compile_metrics_unbuilt:\n-                # This will trigger the metric variable creation.\n-                backend.compute_output_spec(\n-                    self.compute_metrics,\n-                    x_spec,\n-                    y_spec,\n-                    y_pred_spec,\n-                    sample_weight=sample_weight_spec,\n-                )\n-\n-        if self.optimizer is not None and not self.optimizer.built:\n-            # Build optimizer\n-            self.optimizer.build(self.trainable_variables)\n-\n     def train_step(self, state, data):\n         (\n             trainable_variables,\n@@ -383,20 +347,7 @@ class JAXTrainer(base_trainer.Trainer):\n             steps_per_execution=self.steps_per_execution,\n         )\n \n-        needs_building = (\n-            not all(layer.built for layer in self._flatten_layers())\n-            or not self.optimizer.built\n-            or (\n-                self._compile_metrics is not None\n-                and not self._compile_metrics.built\n-            )\n-        )\n-        if needs_building:\n-            # Build the model on one batch of data.\n-            for _, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n-                data_batch = data[0]\n-                self._eager_build(data_batch)\n-                break\n+        self._symbolic_build(iterator=epoch_iterator)\n \n         # Container that configures and calls callbacks.\n         if not isinstance(callbacks, callbacks_module.CallbackList):\n@@ -552,18 +503,7 @@ class JAXTrainer(base_trainer.Trainer):\n                 steps_per_execution=self.steps_per_execution,\n             )\n \n-        needs_building = not all(\n-            layer.built for layer in self._flatten_layers()\n-        ) or (\n-            self._compile_metrics is not None\n-            and not self._compile_metrics.built\n-        )\n-        if needs_building:\n-            # Build the model on one batch of data.\n-            for _, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n-                data_batch = data[0]\n-                self._eager_build(data_batch)\n-                break\n+        self._symbolic_build(iterator=epoch_iterator)\n \n         # Container that configures and calls callbacks.\n         if not isinstance(callbacks, callbacks_module.CallbackList):\n@@ -729,7 +669,7 @@ class JAXTrainer(base_trainer.Trainer):\n         data = self._distribute_data(data)\n \n         # Maybe build model\n-        self._eager_build(data)\n+        self._symbolic_build(data_batch=data)\n         self._record_training_state_sharding_spec()\n         self.make_train_function()\n \n@@ -781,7 +721,7 @@ class JAXTrainer(base_trainer.Trainer):\n         data = (x, y, sample_weight)\n         data = self._distribute_data(data)\n         # Maybe build model\n-        self._eager_build(data)\n+        self._symbolic_build(data_batch=data)\n         self._record_training_state_sharding_spec()\n         self.make_test_function()\n \n\n@@ -8,9 +8,6 @@ from packaging.version import parse\n from keras import backend\n from keras import callbacks as callbacks_module\n from keras import optimizers as optimizers_module\n-from keras.backend.common import standardize_dtype\n-from keras.backend.common.keras_tensor import KerasTensor\n-from keras.backend.torch.core import is_tensor\n from keras.trainers import data_adapters\n from keras.trainers import epoch_iterator\n from keras.trainers import trainer as base_trainer\n@@ -163,55 +160,6 @@ class TorchTrainer(base_trainer.Trainer):\n         else:\n             self.predict_function = one_step_on_data\n \n-    def _symbolic_build(self, data_batch):\n-        model_unbuilt = not all(layer.built for layer in self._flatten_layers())\n-        compile_metrics_unbuilt = (\n-            self._compile_metrics is not None\n-            and not self._compile_metrics.built\n-        )\n-        if model_unbuilt or compile_metrics_unbuilt:\n-            # Create symbolic tensors matching an input batch.\n-\n-            def to_symbolic_input(v):\n-                if is_tensor(v):\n-                    return KerasTensor(v.shape, standardize_dtype(v.dtype))\n-                return v\n-\n-            data_batch = tree.map_structure(to_symbolic_input, data_batch)\n-            (\n-                x,\n-                y,\n-                sample_weight,\n-            ) = data_adapter_utils.unpack_x_y_sample_weight(data_batch)\n-            # Build all model state with `backend.compute_output_spec`.\n-            try:\n-                y_pred = backend.compute_output_spec(self, x)\n-            except Exception as e:\n-                raise RuntimeError(\n-                    \"Unable to automatically build the model. \"\n-                    \"Please build it yourself before calling \"\n-                    \"fit/evaluate/predict. \"\n-                    \"A model is 'built' when its variables have \"\n-                    \"been created and its `self.built` attribute \"\n-                    \"is True. Usually, calling the model on a batch \"\n-                    \"of data is the right way to build it.\\n\"\n-                    \"Exception encountered:\\n\"\n-                    f\"'{e}'\"\n-                )\n-            if compile_metrics_unbuilt:\n-                # Build all metric state with `backend.compute_output_spec`.\n-                backend.compute_output_spec(\n-                    self.compute_metrics,\n-                    x,\n-                    y,\n-                    y_pred,\n-                    sample_weight=sample_weight,\n-                )\n-        if self.optimizer is not None and not self.optimizer.built:\n-            # Build optimizer\n-            self.optimizer.build(self.trainable_variables)\n-        self._post_build()\n-\n     @traceback_utils.filter_traceback\n     def fit(\n         self,\n@@ -270,20 +218,7 @@ class TorchTrainer(base_trainer.Trainer):\n             steps_per_execution=self.steps_per_execution,\n         )\n \n-        needs_building = (\n-            not all(layer.built for layer in self._flatten_layers())\n-            or not self.optimizer.built\n-            or (\n-                self._compile_metrics is not None\n-                and not self._compile_metrics.built\n-            )\n-        )\n-        if needs_building:\n-            # Build the model on one batch of data.\n-            for _, data in epoch_iterator.enumerate_epoch():\n-                data_batch = data[0]\n-                self._symbolic_build(data_batch)\n-                break\n+        self._symbolic_build(iterator=epoch_iterator)\n \n         # Container that configures and calls callbacks.\n         if not isinstance(callbacks, callbacks_module.CallbackList):\n@@ -403,12 +338,7 @@ class TorchTrainer(base_trainer.Trainer):\n                 steps_per_execution=self.steps_per_execution,\n             )\n \n-        if not all(layer.built for layer in self._flatten_layers()):\n-            # Build the model on one batch of data.\n-            for _, data in epoch_iterator.enumerate_epoch():\n-                data_batch = data[0]\n-                self._symbolic_build(data_batch)\n-                break\n+        self._symbolic_build(iterator=epoch_iterator)\n \n         # Container that configures and calls callbacks.\n         if not isinstance(callbacks, callbacks_module.CallbackList):\n@@ -525,7 +455,7 @@ class TorchTrainer(base_trainer.Trainer):\n         data = (x, y, sample_weight)\n \n         # Maybe build model\n-        self._symbolic_build(data)\n+        self._symbolic_build(data_batch=data)\n         self.make_train_function()\n \n         logs = self.train_function([data])\n@@ -546,7 +476,7 @@ class TorchTrainer(base_trainer.Trainer):\n         data = (x, y, sample_weight)\n \n         # Maybe build model\n-        self._symbolic_build(data)\n+        self._symbolic_build(data_batch=data)\n         self.make_test_function()\n \n         logs = self.test_function([data])\n\n@@ -1,6 +1,8 @@\n import platform\n import warnings\n \n+import tree\n+\n from keras import backend\n from keras import metrics as metrics_module\n from keras import ops\n@@ -9,6 +11,7 @@ from keras.optimizers.loss_scale_optimizer import LossScaleOptimizer\n from keras.saving import serialization_lib\n from keras.trainers.compile_utils import CompileLoss\n from keras.trainers.compile_utils import CompileMetrics\n+from keras.trainers.data_adapters import data_adapter_utils\n from keras.utils import traceback_utils\n from keras.utils import tracking\n \n@@ -878,6 +881,66 @@ class Trainer:\n                 msg += f\"calling `{method_name}()`.\"\n             raise ValueError(msg)\n \n+    def _symbolic_build(self, iterator=None, data_batch=None):\n+        model_unbuilt = not all(layer.built for layer in self._flatten_layers())\n+        compile_metrics_unbuilt = (\n+            self._compile_metrics is not None\n+            and not self._compile_metrics.built\n+        )\n+        optimizer_unbuilt = (\n+            self.optimizer is not None and not self.optimizer.built\n+        )\n+        if model_unbuilt or compile_metrics_unbuilt or optimizer_unbuilt:\n+            if data_batch is None:\n+                for _, data in iterator.enumerate_epoch():\n+                    data_batch = data[0]\n+                    break\n+\n+        if model_unbuilt or compile_metrics_unbuilt:\n+            # Create symbolic tensors matching an input batch.\n+\n+            def to_symbolic_input(v):\n+                if v is None:\n+                    return None\n+                return backend.KerasTensor(\n+                    v.shape, backend.standardize_dtype(v.dtype)\n+                )\n+\n+            data_batch = tree.map_structure(to_symbolic_input, data_batch)\n+            (\n+                x,\n+                y,\n+                sample_weight,\n+            ) = data_adapter_utils.unpack_x_y_sample_weight(data_batch)\n+            # Build all model state with `backend.compute_output_spec`.\n+            try:\n+                y_pred = backend.compute_output_spec(self, x)\n+            except Exception as e:\n+                raise RuntimeError(\n+                    \"Unable to automatically build the model. \"\n+                    \"Please build it yourself before calling \"\n+                    \"fit/evaluate/predict. \"\n+                    \"A model is 'built' when its variables have \"\n+                    \"been created and its `self.built` attribute \"\n+                    \"is True. Usually, calling the model on a batch \"\n+                    \"of data is the right way to build it.\\n\"\n+                    \"Exception encountered:\\n\"\n+                    f\"'{e}'\"\n+                )\n+            if compile_metrics_unbuilt:\n+                # Build all metric state with `backend.compute_output_spec`.\n+                backend.compute_output_spec(\n+                    self.compute_metrics,\n+                    x,\n+                    y,\n+                    y_pred,\n+                    sample_weight=sample_weight,\n+                )\n+        if optimizer_unbuilt:\n+            # Build optimizer\n+            self.optimizer.build(self.trainable_variables)\n+        self._post_build()\n+\n \n def model_supports_jit(model):\n     # XLA not supported with TF on MacOS GPU\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e2fc5fa8b86be1ebbacf3172791895fe39b948f3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 19 | Files Changed: 2 | Hunks: 13 | Methods Changed: 6 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 31 | Churn Cumulative: 1530 | Contributors (this commit): 9 | Commits (past 90d): 17 | Contributors (cumulative): 10 | DMM Complexity: 0.3333333333333333\n\nDIFF:\n@@ -148,7 +148,6 @@ class BaseOptimizer:\n                     self.add_variable_from_reference(\n                         variable,\n                         name=\"average\",\n-                        initializer=None,  # Initialize with var value\n                     )\n                 )\n             if self.gradient_accumulation_steps:\n@@ -156,7 +155,6 @@ class BaseOptimizer:\n                     self.add_variable_from_reference(\n                         variable,\n                         name=\"gradient_accumulator\",\n-                        initializer=\"zeros\",\n                     )\n                 )\n         self._trainable_variables = variables[:]\n@@ -199,13 +197,6 @@ class BaseOptimizer:\n         \"\"\"Add an all-zeros variable with the shape and dtype of a reference\n         variable.\n         \"\"\"\n-        if initializer is not None:\n-            initializer = initializers.get(initializer)\n-        else:\n-\n-            def initializer(shape, dtype):\n-                return ops.copy(reference_variable.value)\n-\n         name = name or \"var\"\n         if hasattr(reference_variable, \"path\"):\n             name = reference_variable.path.replace(\"/\", \"_\") + \"_\" + name\n@@ -685,9 +676,11 @@ class BaseOptimizer:\n             for var, average in zip(\n                 trainable_variables, self._model_variables_moving_average\n             ):\n-                average.assign(\n-                    self.ema_momentum * average + (1 - self.ema_momentum) * var\n+                not_first_step = ops.not_equal(self.iterations, 0)\n+                momentum = (\n+                    ops.cast(not_first_step, var.dtype) * self.ema_momentum\n                 )\n+                average.assign(momentum * average + (1 - momentum) * var)\n \n     def _overwrite_model_variables_with_average_value(\n         self, trainable_variables\n\n@@ -33,20 +33,20 @@ class OptimizerTest(testing.TestCase):\n         self.assertAllClose(v, [[2.0, 3.0], [4.0, 5.0]])\n         self.assertAllClose(\n             optimizer._model_variables_moving_average[0],\n-            [[2.9, 3.9], [4.9, 5.9]],  # avg of initial v + current v\n+            [[2.0, 3.0], [4.0, 5.0]],  # initialized after first step\n         )\n         optimizer.apply_gradients([(grads, v)])\n         self.assertAllClose(v, [[1.0, 2.0], [3.0, 4.0]])\n         self.assertAllClose(\n             optimizer._model_variables_moving_average[0],\n-            [[2.71, 3.71], [4.71, 5.71]],\n+            [[1.9, 2.9], [3.9, 4.9]],\n         )\n         optimizer.apply_gradients([(grads, v)])\n         # Variables were overwritten with EMA\n-        self.assertAllClose(v, [[2.439, 3.439], [4.439, 5.439]])\n+        self.assertAllClose(v, [[1.71, 2.71], [3.71, 4.71]])\n         self.assertAllClose(\n             optimizer._model_variables_moving_average[0],\n-            [[2.439, 3.439], [4.439, 5.439]],\n+            [[1.71, 2.71], [3.71, 4.71]],\n         )\n \n     @pytest.mark.requires_trainable_backend\n@@ -54,21 +54,21 @@ class OptimizerTest(testing.TestCase):\n         x_train = np.ones((1, 1)).astype(\"float32\")\n         y_train = np.zeros((1, 1)).astype(\"float32\")\n         optimizer = optimizers.SGD(\n-            learning_rate=1.0, use_ema=True, ema_momentum=0.9\n+            learning_rate=0.1, use_ema=True, ema_momentum=0.9\n         )\n         model = models.Sequential(\n             [layers.Dense(2, kernel_initializer=\"ones\", use_bias=False)]\n         )\n-        model.compile(loss=\"mse\", optimizer=optimizer)\n+        model.compile(loss=\"mse\", optimizer=optimizer, run_eagerly=True)\n         model.fit(x_train, y_train, batch_size=1, epochs=2)\n         self.assertAllClose(\n             optimizer._model_variables_moving_average[0].numpy(),\n-            [[0.81, 0.81]],\n+            [[0.891, 0.891]],\n             atol=1e-5,\n         )\n         self.assertAllClose(\n             model.trainable_variables[0].numpy(),\n-            [[0.81, 0.81]],\n+            [[0.891, 0.891]],\n             atol=1e-5,\n         )\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e51ecdd624518912128b070690ee94719b687332", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 91 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -153,20 +153,20 @@ class OptimizerDistributeTest(testing.TestCase):\n             self.assertAllClose(v, [[2.0, 3.0], [4.0, 5.0]])\n             self.assertAllClose(\n                 optimizer._model_variables_moving_average[0],\n-                [[2.9, 3.9], [4.9, 5.9]],  # avg of initial v + current v\n+                [[2.0, 3.0], [4.0, 5.0]],  # initialized after first step\n             )\n             self.strategy.run(lambda: optimizer.apply_gradients([(grads, v)]))\n             self.assertAllClose(v, [[1.0, 2.0], [3.0, 4.0]])\n             self.assertAllClose(\n                 optimizer._model_variables_moving_average[0],\n-                [[2.71, 3.71], [4.71, 5.71]],\n+                [[1.9, 2.9], [3.9, 4.9]],\n             )\n             self.strategy.run(lambda: optimizer.apply_gradients([(grads, v)]))\n             # Variables were overwritten with EMA\n-            self.assertAllClose(v, [[2.439, 3.439], [4.439, 5.439]])\n+            self.assertAllClose(v, [[1.71, 2.71], [3.71, 4.71]])\n             self.assertAllClose(\n                 optimizer._model_variables_moving_average[0],\n-                [[2.439, 3.439], [4.439, 5.439]],\n+                [[1.71, 2.71], [3.71, 4.71]],\n             )\n \n     def test_gradient_accumulation(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fe2f54aa5bc42fb23a96449cf90434ab9bb6a2cd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 12 | Files Changed: 6 | Hunks: 12 | Methods Changed: 10 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 1796 | Contributors (this commit): 9 | Commits (past 90d): 19 | Contributors (cumulative): 30 | DMM Complexity: None\n\nDIFF:\n@@ -54,7 +54,7 @@ class Cropping1D(Layer):\n             if length <= 0:\n                 raise ValueError(\n                     \"`cropping` parameter of `Cropping1D` layer must be \"\n-                    \"greater than the input length. Received: input_shape=\"\n+                    \"smaller than the input length. Received: input_shape=\"\n                     f\"{input_shape}, cropping={self.cropping}\"\n                 )\n         else:\n@@ -68,7 +68,7 @@ class Cropping1D(Layer):\n         ):\n             raise ValueError(\n                 \"`cropping` parameter of `Cropping1D` layer must be \"\n-                \"greater than the input length. Received: inputs.shape=\"\n+                \"smaller than the input length. Received: inputs.shape=\"\n                 f\"{inputs.shape}, cropping={self.cropping}\"\n             )\n         if self.cropping[1] == 0:\n\n@@ -64,7 +64,7 @@ class Cropping1DTest(testing.TestCase):\n     def test_cropping_1d_errors_if_cropping_more_than_available(self):\n         with self.assertRaisesRegex(\n             ValueError,\n-            \"`cropping` parameter of `Cropping1D` layer must be greater than\",\n+            \"`cropping` parameter of `Cropping1D` layer must be smaller than\",\n         ):\n             input_layer = layers.Input(batch_shape=(3, 5, 7))\n             layers.Cropping1D(cropping=(2, 3))(input_layer)\n@@ -74,7 +74,7 @@ class Cropping1DTest(testing.TestCase):\n \n         with self.assertRaisesRegex(\n             ValueError,\n-            \"`cropping` parameter of `Cropping1D` layer must be greater than\",\n+            \"`cropping` parameter of `Cropping1D` layer must be smaller than\",\n         ):\n             layer = layers.Cropping1D(cropping=(3, 3))\n             _ = layer(inputs)\n\n@@ -96,7 +96,7 @@ class Cropping2D(Layer):\n                 and sum(self.cropping[1]) >= input_shape[3]\n             ):\n                 raise ValueError(\n-                    \"Values in `cropping` argument should be greater than the \"\n+                    \"Values in `cropping` argument should be smaller than the \"\n                     \"corresponding spatial dimension of the input. Received: \"\n                     f\"input_shape={input_shape}, cropping={self.cropping}\"\n                 )\n@@ -119,7 +119,7 @@ class Cropping2D(Layer):\n                 and sum(self.cropping[1]) >= input_shape[2]\n             ):\n                 raise ValueError(\n-                    \"Values in `cropping` argument should be greater than the \"\n+                    \"Values in `cropping` argument should be smaller than the \"\n                     \"corresponding spatial dimension of the input. Received: \"\n                     f\"input_shape={input_shape}, cropping={self.cropping}\"\n                 )\n@@ -144,7 +144,7 @@ class Cropping2D(Layer):\n                 and sum(self.cropping[1]) >= inputs.shape[3]\n             ):\n                 raise ValueError(\n-                    \"Values in `cropping` argument should be greater than the \"\n+                    \"Values in `cropping` argument should be smaller than the \"\n                     \"corresponding spatial dimension of the input. Received: \"\n                     f\"inputs.shape={inputs.shape}, cropping={self.cropping}\"\n                 )\n@@ -181,7 +181,7 @@ class Cropping2D(Layer):\n                 and sum(self.cropping[1]) >= inputs.shape[2]\n             ):\n                 raise ValueError(\n-                    \"Values in `cropping` argument should be greater than the \"\n+                    \"Values in `cropping` argument should be smaller than the \"\n                     \"corresponding spatial dimension of the input. Received: \"\n                     f\"inputs.shape={inputs.shape}, cropping={self.cropping}\"\n                 )\n\n@@ -126,7 +126,7 @@ class Cropping2DTest(testing.TestCase, parameterized.TestCase):\n \n         with self.assertRaisesRegex(\n             ValueError,\n-            \"Values in `cropping` argument should be greater than the \"\n+            \"Values in `cropping` argument should be smaller than the \"\n             \"corresponding spatial dimension of the input.\",\n         ):\n             layer = layers.Cropping2D(\n\n@@ -112,7 +112,7 @@ class Cropping3D(Layer):\n             spatial_dims[index] -= sum(self.cropping[index])\n             if spatial_dims[index] <= 0:\n                 raise ValueError(\n-                    \"Values in `cropping` argument should be greater than the \"\n+                    \"Values in `cropping` argument should be smaller than the \"\n                     \"corresponding spatial dimension of the input. Received: \"\n                     f\"input_shape={input_shape}, cropping={self.cropping}\"\n                 )\n@@ -134,7 +134,7 @@ class Cropping3D(Layer):\n             spatial_dims[index] -= sum(self.cropping[index])\n             if spatial_dims[index] <= 0:\n                 raise ValueError(\n-                    \"Values in `cropping` argument should be greater than the \"\n+                    \"Values in `cropping` argument should be smaller than the \"\n                     \"corresponding spatial dimension of the input. Received: \"\n                     f\"inputs.shape={inputs.shape}, cropping={self.cropping}\"\n                 )\n\n@@ -190,7 +190,7 @@ class Cropping3DTest(testing.TestCase, parameterized.TestCase):\n             input_layer = layers.Input(batch_shape=shape)\n \n         expected_error_msg = (\n-            \"Values in `cropping` argument should be greater than the\"\n+            \"Values in `cropping` argument should be smaller than the\"\n         )\n \n         with self.assertRaisesRegex(ValueError, expected_error_msg):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#aaf4289c831cf69359c89c1b45117557cb2f500b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 71 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 6 | Methods Changed: 9 | Complexity Δ (Sum/Max): 10/9 | Churn Δ: 71 | Churn Cumulative: 2482 | Contributors (this commit): 5 | Commits (past 90d): 10 | Contributors (cumulative): 10 | DMM Complexity: 0.9555555555555556\n\nDIFF:\n@@ -314,6 +314,12 @@ def _walk_trackable(trackable):\n         raise ValueError(f\"Invalid obj_type: {obj_type}\")\n     attr_skiplist = get_attr_skiplist(obj_type)\n \n+    # Save all layers directly tracked by Sequential and Functional first.\n+    # This helps avoid ordering concerns for subclassed Sequential or Functional\n+    # models with extra attributes--the internal Keras state take precedence.\n+    if obj_type in (\"Sequential\", \"Functional\"):\n+        yield \"layers\", trackable.layers\n+\n     for child_attr in sorted(dir(trackable), key=lambda x: _name_key(x)):\n         if child_attr.startswith(\"__\") or child_attr in attr_skiplist:\n             continue\n\n@@ -125,6 +125,32 @@ class CompileOverridingSequential(keras.Sequential):\n         super().compile(*args, **kwargs)\n \n \n+@keras.saving.register_keras_serializable(package=\"my_custom_package\")\n+class SubclassFunctional(keras.Model):\n+    \"\"\"Subclassed functional identical to `_get_basic_functional_model`.\"\"\"\n+\n+    def __init__(self, **kwargs):\n+        inputs = keras.Input(shape=(4,), batch_size=2)\n+        dense = keras.layers.Dense(1, name=\"first_dense\")\n+        x = dense(inputs)\n+        outputs = keras.layers.Dense(1, name=\"second_dense\")(x)\n+        super().__init__(inputs=inputs, outputs=outputs, **kwargs)\n+        # Attrs for layers in the functional graph should not affect saving\n+        self.layer_attr = dense\n+\n+    @property\n+    def layer_property(self):\n+        # Properties for layers in the functional graph should not affect saving\n+        return self.layer_attr\n+\n+    def get_config(self):\n+        return {}\n+\n+    @classmethod\n+    def from_config(cls, config):\n+        return cls(**config)\n+\n+\n @keras.saving.register_keras_serializable(package=\"my_custom_package\")\n def my_mean_squared_error(y_true, y_pred):\n     \"\"\"Identical to built-in `mean_squared_error`, but as a custom fn.\"\"\"\n@@ -200,6 +226,17 @@ def _get_basic_functional_model(compile=True):\n     return functional_model\n \n \n+def _get_subclassed_functional_model(compile=True):\n+    functional_model = SubclassFunctional()\n+    if compile:\n+        functional_model.compile(\n+            optimizer=\"adam\",\n+            loss=my_mean_squared_error,\n+            metrics=[keras.metrics.Hinge(), \"mse\"],\n+        )\n+    return functional_model\n+\n+\n @pytest.mark.requires_trainable_backend\n class SavingTest(testing.TestCase):\n     def _test_inference_after_instantiation(self, model):\n@@ -234,6 +271,10 @@ class SavingTest(testing.TestCase):\n         model = _get_custom_functional_model(compile=False)\n         self._test_inference_after_instantiation(model)\n \n+    def test_inference_after_instantiation_subclassed_functional(self):\n+        model = _get_subclassed_functional_model(compile=False)\n+        self._test_inference_after_instantiation(model)\n+\n     def _test_compile_preserved(self, model):\n         x_ref = np.random.random((2, 4))\n         y_ref = np.random.random((2, 1))\n@@ -286,6 +327,10 @@ class SavingTest(testing.TestCase):\n         model = _get_custom_functional_model(compile=True)\n         self._test_compile_preserved(model)\n \n+    def test_compile_preserved_subclassed_functional(self):\n+        model = _get_subclassed_functional_model(compile=True)\n+        self._test_compile_preserved(model)\n+\n     def test_saving_preserve_unbuilt_state(self):\n         temp_filepath = os.path.join(self.get_temp_dir(), \"my_model.keras\")\n         subclassed_model = CustomModelX()\n@@ -432,6 +477,26 @@ class SavingTest(testing.TestCase):\n         model.load_weights(temp_filepath)\n         self.assertAllClose(model.predict(ref_input), ref_output, atol=1e-6)\n \n+    def test_save_weights_subclassed_functional(self):\n+        # The subclassed and basic functional model should have the same\n+        # weights structure.\n+        temp_filepath = Path(\n+            os.path.join(self.get_temp_dir(), \"mymodel.weights.h5\")\n+        )\n+        model = _get_basic_functional_model()\n+        ref_input = np.random.random((2, 4))\n+        ref_output = model.predict(ref_input)\n+        # Test saving basic, loading subclassed.\n+        saving_lib.save_weights_only(model, temp_filepath)\n+        model = _get_subclassed_functional_model()\n+        saving_lib.load_weights_only(model, temp_filepath)\n+        self.assertAllClose(model.predict(ref_input), ref_output, atol=1e-6)\n+        # Test saving subclassed, loading basic.\n+        saving_lib.save_weights_only(model, temp_filepath)\n+        model = _get_basic_functional_model()\n+        saving_lib.load_weights_only(model, temp_filepath)\n+        self.assertAllClose(model.predict(ref_input), ref_output, atol=1e-6)\n+\n     def test_compile_arg(self):\n         temp_filepath = os.path.join(self.get_temp_dir(), \"mymodel.keras\")\n         model = _get_basic_functional_model()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#791bda8ed798f2d3232e5d19bce57439526dda19", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 266 | Contributors (this commit): 4 | Commits (past 90d): 13 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -49,9 +49,6 @@ def test_model_fit():\n             ),\n             loss=losses.MeanSquaredError(),\n             metrics=[metrics.MeanSquaredError()],\n-            # TODO(scottzhu): Find out where is the variable\n-            #  that is not created eagerly and break the usage of XLA.\n-            jit_compile=False,\n         )\n         history = model.fit(\n             x,\n\n@@ -222,6 +222,10 @@ class Trainer:\n                 # Disable XLA on CPU-only machines.\n                 return False\n \n+            if self._distribute_strategy:\n+                # Disable XLA with tf.distribute\n+                return False\n+\n         if model_supports_jit(self):\n             return True\n         return False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d1df2ae4d851000e5e268e533a83f8b8b5ea6587", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 21 | Files Changed: 2 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 44 | Churn Cumulative: 403 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -6,19 +6,26 @@ from keras.callbacks.callback import Callback\n \n @keras_export(\"keras.callbacks.SwapEMAWeights\")\n class SwapEMAWeights(Callback):\n-    \"\"\"Swaps EMA weights before and after the evaluation.\n+    \"\"\"Swaps model weights and EMA weights before and after evaluation.\n \n-    `SwapEMAWeights` callback is used in conjunction with the optimizer using\n-    `use_ema=True`.\n+    This callbacks replaces the model's weight values with the values of\n+    the optimizer's EMA weights (the exponential moving average of the past\n+    model weights values, implementing \"Polyak averaging\") before model\n+    evaluation, and restores the previous weights after evaluation.\n \n-    Note that: we use swapping to save memory. The behavior is undefined if you\n-    modify the EMA weights or model weights in other callbacks.\n+    The `SwapEMAWeights` callback is to be used in conjunction with\n+    an optimizer that sets `use_ema=True`.\n+\n+    Note that the weights are swapped in-place in order to save memory.\n+    The behavior is undefined if you modify the EMA weights\n+    or model weights in other callbacks.\n \n     Example:\n \n     ```python\n-    # Remember to set `use_ema=True`\n-    model.compile(optimizer=SGD(use_ema=True), loss=..., metrics=...)\n+    # Remember to set `use_ema=True` in the optimizer\n+    optimizer = SGD(use_ema=True)\n+    model.compile(optimizer=optimizer, loss=..., metrics=...)\n \n     # Metrics will be computed with EMA weights\n     model.fit(X_train, Y_train, callbacks=[SwapEMAWeights()])\n@@ -33,10 +40,10 @@ class SwapEMAWeights(Callback):\n     ```\n \n     Args:\n-        swap_on_epoch: whether to perform swapping `on_epoch_begin` and\n-            `on_epoch_end`. This is useful if you want to use EMA weights for\n-            other callbacks such as `ModelCheckpoint`. Defaults to `False`.\n-\n+        swap_on_epoch: whether to perform swapping at `on_epoch_begin()`\n+            and `on_epoch_end()`. This is useful if you want to use\n+            EMA weights for other callbacks such as `ModelCheckpoint`.\n+            Defaults to `False`.\n     \"\"\"\n \n     def __init__(self, swap_on_epoch=False):\n@@ -111,9 +118,8 @@ class SwapEMAWeights(Callback):\n         if not hasattr(optimizer, \"_model_variables_moving_average\"):\n             raise ValueError(\n                 \"SwapEMAWeights must be used when \"\n-                \"`_model_variables_moving_average` exists in the optimizer. \"\n-                \"Please verify if you have set `use_ema=True` in your \"\n-                f\"optimizer. Received: use_ema={optimizer.use_ema}\"\n+                \"`use_ema=True` is set on the optimizer. \"\n+                f\"Received: use_ema={optimizer.use_ema}\"\n             )\n         if backend.backend() == \"tensorflow\":\n             self._tf_swap_variables(optimizer)\n@@ -129,9 +135,8 @@ class SwapEMAWeights(Callback):\n         if not hasattr(optimizer, \"_model_variables_moving_average\"):\n             raise ValueError(\n                 \"SwapEMAWeights must be used when \"\n-                \"`_model_variables_moving_average` exists in the optimizer. \"\n-                \"Please verify if you have set `use_ema=True` in the \"\n-                f\"optimizer. Received: use_ema={optimizer.use_ema}\"\n+                \"`use_ema=True` is set on the optimizer. \"\n+                f\"Received: use_ema={optimizer.use_ema}\"\n             )\n         if backend.backend() == \"tensorflow\":\n             self._tf_finalize_ema_values(optimizer)\n\n@@ -53,10 +53,7 @@ class SwapEMAWeightsTest(testing.TestCase):\n         model = self._get_compiled_model(use_ema=False)\n         with self.assertRaisesRegex(\n             ValueError,\n-            (\n-                \"SwapEMAWeights must be used when \"\n-                \"`_model_variables_moving_average` exists in the optimizer. \"\n-            ),\n+            (\"SwapEMAWeights must be used when \" \"`use_ema=True` is set\"),\n         ):\n             model.fit(\n                 self.x_train,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
