{"custom_id": "scrapy#8c23da943c5e892515f4fa2eb57229839802010a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 79 | Files Changed: 3 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 84 | Churn Cumulative: 3667 | Contributors (this commit): 49 | Commits (past 90d): 11 | Contributors (cumulative): 79 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,75 +0,0 @@\n-from pathlib import Path\n-\n-from setuptools import find_packages, setup\n-\n-version = (Path(__file__).parent / \"scrapy/VERSION\").read_text(\"ascii\").strip()\n-\n-\n-install_requires = [\n-    \"Twisted>=21.7.0\",\n-    \"cryptography>=37.0.0\",\n-    \"cssselect>=0.9.1\",\n-    \"itemloaders>=1.0.1\",\n-    \"parsel>=1.5.0\",\n-    \"pyOpenSSL>=22.0.0\",\n-    \"queuelib>=1.4.2\",\n-    \"service_identity>=18.1.0\",\n-    \"w3lib>=1.17.0\",\n-    \"zope.interface>=5.1.0\",\n-    \"protego>=0.1.15\",\n-    \"itemadapter>=0.1.0\",\n-    \"packaging\",\n-    \"tldextract\",\n-    \"lxml>=4.6.0\",\n-    \"defusedxml>=0.7.1\",\n-]\n-extras_require = {\n-    ':platform_python_implementation == \"CPython\"': [\"PyDispatcher>=2.0.5\"],\n-    ':platform_python_implementation == \"PyPy\"': [\"PyPyDispatcher>=2.1.0\"],\n-}\n-\n-\n-setup(\n-    name=\"Scrapy\",\n-    version=version,\n-    url=\"https://scrapy.org\",\n-    project_urls={\n-        \"Documentation\": \"https://docs.scrapy.org/\",\n-        \"Source\": \"https://github.com/scrapy/scrapy\",\n-        \"Tracker\": \"https://github.com/scrapy/scrapy/issues\",\n-    },\n-    description=\"A high-level Web Crawling and Web Scraping framework\",\n-    long_description=open(\"README.rst\", encoding=\"utf-8\").read(),\n-    author=\"Scrapy developers\",\n-    author_email=\"pablo@pablohoffman.com\",\n-    maintainer=\"Pablo Hoffman\",\n-    maintainer_email=\"pablo@pablohoffman.com\",\n-    license=\"BSD\",\n-    packages=find_packages(exclude=(\"tests\", \"tests.*\")),\n-    include_package_data=True,\n-    zip_safe=False,\n-    entry_points={\"console_scripts\": [\"scrapy = scrapy.cmdline:execute\"]},\n-    classifiers=[\n-        \"Framework :: Scrapy\",\n-        \"Development Status :: 5 - Production/Stable\",\n-        \"Environment :: Console\",\n-        \"Intended Audience :: Developers\",\n-        \"License :: OSI Approved :: BSD License\",\n-        \"Operating System :: OS Independent\",\n-        \"Programming Language :: Python\",\n-        \"Programming Language :: Python :: 3\",\n-        \"Programming Language :: Python :: 3.9\",\n-        \"Programming Language :: Python :: 3.10\",\n-        \"Programming Language :: Python :: 3.11\",\n-        \"Programming Language :: Python :: 3.12\",\n-        \"Programming Language :: Python :: 3.13\",\n-        \"Programming Language :: Python :: Implementation :: CPython\",\n-        \"Programming Language :: Python :: Implementation :: PyPy\",\n-        \"Topic :: Internet :: WWW/HTTP\",\n-        \"Topic :: Software Development :: Libraries :: Application Frameworks\",\n-        \"Topic :: Software Development :: Libraries :: Python Modules\",\n-    ],\n-    python_requires=\">=3.9\",\n-    install_requires=install_requires,\n-    extras_require=extras_require,\n-)\n\n@@ -899,7 +899,7 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         p.expect_exact(\"shutting down gracefully\")\n         # sending the second signal too fast often causes problems\n         d = Deferred()\n-        reactor.callLater(0.1, d.callback, None)\n+        reactor.callLater(0.01, d.callback, None)\n         yield d\n         p.kill(sig)\n         p.expect_exact(\"forcing unclean shutdown\")\n\n@@ -144,9 +144,10 @@ class SpiderLoaderTest(unittest.TestCase):\n             self.assertRaises(SyntaxError, SpiderLoader.from_settings, settings)\n \n     def test_syntax_error_warning(self):\n-        with warnings.catch_warnings(record=True) as w, mock.patch.object(\n-            SpiderLoader, \"_load_spiders\"\n-        ) as m:\n+        with (\n+            warnings.catch_warnings(record=True) as w,\n+            mock.patch.object(SpiderLoader, \"_load_spiders\") as m,\n+        ):\n             m.side_effect = SyntaxError\n             module = \"tests.test_spiderloader.test_spiders.spider1\"\n             settings = Settings(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4dcc04be48b5c9ed096e91bf2928f5e421ce2153", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 59 | Lines Deleted: 12 | Files Changed: 2 | Hunks: 9 | Methods Changed: 8 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 71 | Churn Cumulative: 1596 | Contributors (this commit): 24 | Commits (past 90d): 6 | Contributors (cumulative): 33 | DMM Complexity: 1.0\n\nDIFF:\n@@ -111,18 +111,9 @@ class ScrapyClientContextFactory(BrowserLikePolicyForHTTPS):\n     def getCertificateOptions(self) -> CertificateOptions:\n         # setting verify=True will require you to provide CAs\n         # to verify against; in other words: it's not that simple\n-\n-        # backward-compatible SSL/TLS method:\n-        #\n-        # * this will respect `method` attribute in often recommended\n-        #   `ScrapyClientContextFactory` subclass\n-        #   (https://github.com/scrapy/scrapy/issues/1429#issuecomment-131782133)\n-        #\n-        # * getattr() for `_ssl_method` attribute for context factories\n-        #   not calling super().__init__\n         return CertificateOptions(\n             verify=False,\n-            method=getattr(self, \"method\", getattr(self, \"_ssl_method\", None)),\n+            method=self._ssl_method,\n             fixBrokenPeers=True,\n             acceptableCiphers=self.tls_ciphers,\n         )\n\n@@ -3,21 +3,29 @@ from twisted.internet import defer\n Tests borrowed from the twisted.web.client tests.\n \"\"\"\n \n+from __future__ import annotations\n+\n import shutil\n from pathlib import Path\n from tempfile import mkdtemp\n+from typing import Any\n \n import OpenSSL.SSL\n+from pytest import raises\n from twisted.internet import defer, reactor\n-from twisted.internet.defer import inlineCallbacks\n+from twisted.internet.defer import Deferred, inlineCallbacks\n from twisted.internet.testing import StringTransport\n from twisted.protocols.policies import WrappingFactory\n from twisted.trial import unittest\n from twisted.web import resource, server, static, util\n \n from scrapy.core.downloader import webclient as client\n-from scrapy.core.downloader.contextfactory import ScrapyClientContextFactory\n+from scrapy.core.downloader.contextfactory import (\n+    ScrapyClientContextFactory,\n+    load_context_factory_from_settings,\n+)\n from scrapy.http import Headers, Request\n+from scrapy.settings import Settings\n from scrapy.utils.misc import build_from_crawler\n from scrapy.utils.python import to_bytes, to_unicode\n from scrapy.utils.test import get_crawler\n@@ -482,3 +490,51 @@ class WebClientCustomCiphersSSLTestCase(WebClientSSLTestCase):\n             self.getURL(\"payload\"), body=s, contextFactory=client_context_factory\n         )\n         return self.assertFailure(d, OpenSSL.SSL.Error)\n+\n+\n+class WebClientTLSMethodTestCase(WebClientSSLTestCase):\n+    def _assert_factory_works(\n+        self, client_context_factory: ScrapyClientContextFactory\n+    ) -> Deferred[Any]:\n+        s = \"0123456789\" * 10\n+        return getPage(\n+            self.getURL(\"payload\"), body=s, contextFactory=client_context_factory\n+        ).addCallback(self.assertEqual, to_bytes(s))\n+\n+    def test_setting_default(self):\n+        crawler = get_crawler()\n+        settings = Settings()\n+        client_context_factory = load_context_factory_from_settings(settings, crawler)\n+        assert client_context_factory._ssl_method == OpenSSL.SSL.SSLv23_METHOD\n+        return self._assert_factory_works(client_context_factory)\n+\n+    def test_setting_none(self):\n+        crawler = get_crawler()\n+        settings = Settings({\"DOWNLOADER_CLIENT_TLS_METHOD\": None})\n+        with raises(KeyError):\n+            load_context_factory_from_settings(settings, crawler)\n+\n+    def test_setting_bad(self):\n+        crawler = get_crawler()\n+        settings = Settings({\"DOWNLOADER_CLIENT_TLS_METHOD\": \"bad\"})\n+        with raises(KeyError):\n+            load_context_factory_from_settings(settings, crawler)\n+\n+    def test_setting_explicit(self):\n+        crawler = get_crawler()\n+        settings = Settings({\"DOWNLOADER_CLIENT_TLS_METHOD\": \"TLSv1.2\"})\n+        client_context_factory = load_context_factory_from_settings(settings, crawler)\n+        assert client_context_factory._ssl_method == OpenSSL.SSL.TLSv1_2_METHOD\n+        return self._assert_factory_works(client_context_factory)\n+\n+    def test_direct_from_crawler(self):\n+        # the setting is ignored\n+        crawler = get_crawler(settings_dict={\"DOWNLOADER_CLIENT_TLS_METHOD\": \"bad\"})\n+        client_context_factory = build_from_crawler(ScrapyClientContextFactory, crawler)\n+        assert client_context_factory._ssl_method == OpenSSL.SSL.SSLv23_METHOD\n+        return self._assert_factory_works(client_context_factory)\n+\n+    def test_direct_init(self):\n+        client_context_factory = ScrapyClientContextFactory(OpenSSL.SSL.TLSv1_2_METHOD)\n+        assert client_context_factory._ssl_method == OpenSSL.SSL.TLSv1_2_METHOD\n+        return self._assert_factory_works(client_context_factory)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cc146b9df7c6039ab0e0654b5844f5978e5bc565", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 3332 | Contributors (this commit): 51 | Commits (past 90d): 17 | Contributors (cumulative): 71 | DMM Complexity: None\n\nDIFF:\n@@ -186,7 +186,7 @@ def _get_inputs(\n \n     if not dont_click:\n         clickable = _get_clickable(clickdata, form)\n-        if clickable and clickable[0] not in formdata and not clickable[0] is None:\n+        if clickable and clickable[0] not in formdata and clickable[0] is not None:\n             values.append(clickable)\n \n     formdata_items = formdata.items() if isinstance(formdata, dict) else formdata\n\n@@ -218,7 +218,7 @@ class FileDownloadCrawlTestCase(TestCase):\n \n skip_pillow: str | None\n try:\n-    from PIL import Image  # noqa: imported just to check for the import error\n+    from PIL import Image  # noqa: F401\n except ImportError:\n     skip_pillow = \"Missing Python Imaging Library, install https://pypi.org/pypi/Pillow\"\n else:\n\n@@ -19,7 +19,7 @@ from scrapy.utils.signal import disconnect_all\n from scrapy.utils.test import get_crawler\n \n try:\n-    from PIL import Image  # noqa: imported just to check for the import error\n+    from PIL import Image  # noqa: F401\n except ImportError:\n     skip_pillow: str | None = (\n         \"Missing Python Imaging Library, install https://pypi.org/pypi/Pillow\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5680bee968d91239da27fff00aa33a9d891f4109", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 330 | Contributors (this commit): 19 | Commits (past 90d): 3 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -24,7 +24,9 @@ collect_ignore = [\n     *_py_files(\"tests/CrawlerRunner\"),\n ]\n \n-with Path(\"tests/ignores.txt\").open(encoding=\"utf-8\") as reader:\n+base_dir = Path(__file__).parent\n+ignore_file_path = base_dir / \"tests\" / \"ignores.txt\"\n+with ignore_file_path.open(encoding=\"utf-8\") as reader:\n     for line in reader:\n         file_path = line.strip()\n         if file_path and file_path[0] != \"#\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d47f142d0ffff0e8f87eee67c4c0e7edbc71613e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 37 | Files Changed: 22 | Hunks: 55 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 81 | Churn Cumulative: 21655 | Contributors (this commit): 140 | Commits (past 90d): 87 | Contributors (cumulative): 467 | DMM Complexity: None\n\nDIFF:\n@@ -1,6 +1,5 @@\n from __future__ import annotations\n \n-import argparse\n import subprocess  # nosec\n import sys\n import time\n@@ -13,6 +12,7 @@ from scrapy.http import Response, TextResponse\n from scrapy.linkextractors import LinkExtractor\n \n if TYPE_CHECKING:\n+    import argparse\n     from collections.abc import Iterable\n \n     from scrapy import Request\n\n@@ -5,7 +5,6 @@ from typing import TYPE_CHECKING\n \n from w3lib.url import is_url\n \n-from scrapy import Spider\n from scrapy.commands import ScrapyCommand\n from scrapy.exceptions import UsageError\n from scrapy.http import Request, Response\n@@ -15,6 +14,8 @@ from scrapy.utils.spider import DefaultSpider, spidercls_for_request\n if TYPE_CHECKING:\n     from argparse import ArgumentParser, Namespace\n \n+    from scrapy import Spider\n+\n \n class Command(ScrapyCommand):\n     requires_project = False\n\n@@ -1,12 +1,11 @@\n from __future__ import annotations\n \n-import argparse\n import os\n import shutil\n import string\n from importlib import import_module\n from pathlib import Path\n-from typing import Any, cast\n+from typing import TYPE_CHECKING, Any, cast\n from urllib.parse import urlparse\n \n import scrapy\n@@ -14,6 +13,9 @@ from scrapy.commands import ScrapyCommand\n from scrapy.exceptions import UsageError\n from scrapy.utils.template import render_templatefile, string_camelcase\n \n+if TYPE_CHECKING:\n+    import argparse\n+\n \n def sanitize_module_name(module_name: str) -> str:\n     \"\"\"Sanitize the given module name, by replacing dashes and points\n\n@@ -1,6 +1,5 @@\n from __future__ import annotations\n \n-import argparse\n import functools\n import inspect\n import json\n@@ -22,6 +21,7 @@ from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.spider import spidercls_for_request\n \n if TYPE_CHECKING:\n+    import argparse\n     from collections.abc import AsyncGenerator, Coroutine, Iterable\n \n     from twisted.python.failure import Failure\n\n@@ -1,6 +1,5 @@\n from __future__ import annotations\n \n-import argparse\n import sys\n from importlib import import_module\n from pathlib import Path\n@@ -11,6 +10,7 @@ from scrapy.exceptions import UsageError\n from scrapy.utils.spider import iter_spider_classes\n \n if TYPE_CHECKING:\n+    import argparse\n     from os import PathLike\n     from types import ModuleType\n \n\n@@ -9,7 +9,6 @@ from __future__ import annotations\n from threading import Thread\n from typing import TYPE_CHECKING, Any\n \n-from scrapy import Spider\n from scrapy.commands import ScrapyCommand\n from scrapy.http import Request\n from scrapy.shell import Shell\n@@ -19,6 +18,8 @@ from scrapy.utils.url import guess_scheme\n if TYPE_CHECKING:\n     from argparse import ArgumentParser, Namespace\n \n+    from scrapy import Spider\n+\n \n class Command(ScrapyCommand):\n     requires_project = False\n\n@@ -1,6 +1,5 @@\n from __future__ import annotations\n \n-import argparse\n import os\n import re\n import string\n@@ -8,12 +7,16 @@ from importlib.util import find_spec\n from pathlib import Path\n from shutil import copy2, copystat, ignore_patterns, move\n from stat import S_IWUSR as OWNER_WRITE_PERMISSION\n+from typing import TYPE_CHECKING\n \n import scrapy\n from scrapy.commands import ScrapyCommand\n from scrapy.exceptions import UsageError\n from scrapy.utils.template import render_templatefile, string_camelcase\n \n+if TYPE_CHECKING:\n+    import argparse\n+\n TEMPLATES_TO_RENDER: tuple[tuple[str, ...], ...] = (\n     (\"scrapy.cfg\",),\n     (\"${project_name}\", \"settings.py.tmpl\"),\n\n@@ -15,7 +15,6 @@ from scrapy.core.downloader.handlers import DownloadHandlers\n from scrapy.core.downloader.middleware import DownloaderMiddlewareManager\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.resolver import dnscache\n-from scrapy.signalmanager import SignalManager\n from scrapy.utils.defer import mustbe_deferred\n from scrapy.utils.httpobj import urlparse_cached\n \n@@ -23,6 +22,7 @@ if TYPE_CHECKING:\n     from scrapy.crawler import Crawler\n     from scrapy.http import Response\n     from scrapy.settings import BaseSettings\n+    from scrapy.signalmanager import SignalManager\n \n \n _T = TypeVar(\"_T\")\n\n@@ -3,7 +3,6 @@\n from __future__ import annotations\n \n import logging\n-from collections.abc import Callable\n from typing import TYPE_CHECKING, Any, Protocol, cast\n \n from twisted.internet import defer\n@@ -15,7 +14,7 @@ from scrapy.utils.misc import build_from_crawler, load_object\n from scrapy.utils.python import without_none_values\n \n if TYPE_CHECKING:\n-    from collections.abc import Generator\n+    from collections.abc import Callable, Generator\n \n     from twisted.internet.defer import Deferred\n \n\n@@ -17,13 +17,9 @@ from twisted.internet.task import LoopingCall\n from twisted.python.failure import Failure\n \n from scrapy import signals\n-from scrapy.core.downloader import Downloader\n from scrapy.core.scraper import Scraper, _HandleOutputDeferred\n from scrapy.exceptions import CloseSpider, DontCloseSpider, IgnoreRequest\n from scrapy.http import Request, Response\n-from scrapy.logformatter import LogFormatter\n-from scrapy.settings import Settings\n-from scrapy.signalmanager import SignalManager\n from scrapy.utils.log import failure_to_exc_info, logformatter_adapter\n from scrapy.utils.misc import build_from_crawler, load_object\n from scrapy.utils.reactor import CallLaterOnce\n@@ -31,9 +27,12 @@ from scrapy.utils.reactor import CallLaterOnce\n if TYPE_CHECKING:\n     from collections.abc import Callable, Generator, Iterable, Iterator\n \n+    from scrapy.core.downloader import Downloader\n     from scrapy.core.scheduler import BaseScheduler\n     from scrapy.crawler import Crawler\n-    from scrapy.settings import BaseSettings\n+    from scrapy.logformatter import LogFormatter\n+    from scrapy.settings import BaseSettings, Settings\n+    from scrapy.signalmanager import SignalManager\n     from scrapy.spiders import Spider\n \n \n\n@@ -16,9 +16,6 @@ from scrapy import Spider, signals\n from scrapy.core.spidermw import SpiderMiddlewareManager\n from scrapy.exceptions import CloseSpider, DropItem, IgnoreRequest\n from scrapy.http import Request, Response\n-from scrapy.logformatter import LogFormatter\n-from scrapy.pipelines import ItemPipelineManager\n-from scrapy.signalmanager import SignalManager\n from scrapy.utils.defer import (\n     aiter_errback,\n     defer_fail,\n@@ -35,6 +32,9 @@ if TYPE_CHECKING:\n     from collections.abc import Generator, Iterable\n \n     from scrapy.crawler import Crawler\n+    from scrapy.logformatter import LogFormatter\n+    from scrapy.pipelines import ItemPipelineManager\n+    from scrapy.signalmanager import SignalManager\n \n \n logger = logging.getLogger(__name__)\n\n@@ -18,10 +18,8 @@ from scrapy.addons import AddonManager\n from scrapy.core.engine import ExecutionEngine\n from scrapy.extension import ExtensionManager\n from scrapy.interfaces import ISpiderLoader\n-from scrapy.logformatter import LogFormatter\n from scrapy.settings import BaseSettings, Settings, overridden_settings\n from scrapy.signalmanager import SignalManager\n-from scrapy.statscollectors import StatsCollector\n from scrapy.utils.log import (\n     LogCounterHandler,\n     configure_logging,\n@@ -42,7 +40,9 @@ from scrapy.utils.reactor import (\n if TYPE_CHECKING:\n     from collections.abc import Generator, Iterable\n \n+    from scrapy.logformatter import LogFormatter\n     from scrapy.spiderloader import SpiderLoader\n+    from scrapy.statscollectors import StatsCollector\n     from scrapy.utils.request import RequestFingerprinter\n \n \n\n@@ -10,8 +10,7 @@ import pickle  # nosec\n import pprint\n from collections.abc import Callable, Iterable, Mapping\n from io import BytesIO, TextIOWrapper\n-from json import JSONEncoder\n-from typing import Any\n+from typing import TYPE_CHECKING, Any\n from xml.sax.saxutils import XMLGenerator  # nosec\n from xml.sax.xmlreader import AttributesImpl  # nosec\n \n@@ -21,6 +20,9 @@ from scrapy.item import Field, Item\n from scrapy.utils.python import is_listlike, to_bytes, to_unicode\n from scrapy.utils.serialize import ScrapyJSONEncoder\n \n+if TYPE_CHECKING:\n+    from json import JSONEncoder\n+\n __all__ = [\n     \"BaseItemExporter\",\n     \"PprintItemExporter\",\n\n@@ -25,7 +25,6 @@ from zope.interface import Interface, implementer\n from scrapy import Spider, signals\n from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.extensions.postprocessing import PostProcessingManager\n-from scrapy.settings import Settings\n from scrapy.utils.conf import feed_complete_default_values_from_settings\n from scrapy.utils.defer import maybe_deferred_to_future\n from scrapy.utils.ftp import ftp_store_file\n@@ -44,7 +43,7 @@ if TYPE_CHECKING:\n \n     from scrapy.crawler import Crawler\n     from scrapy.exporters import BaseItemExporter\n-    from scrapy.settings import BaseSettings\n+    from scrapy.settings import BaseSettings, Settings\n \n \n logger = logging.getLogger(__name__)\n\n@@ -2,13 +2,11 @@ from __future__ import annotations\n \n import gzip\n import logging\n-import os\n import pickle  # nosec\n from email.utils import mktime_tz, parsedate_tz\n from importlib import import_module\n from pathlib import Path\n from time import time\n-from types import ModuleType\n from typing import IO, TYPE_CHECKING, Any, cast\n from weakref import WeakKeyDictionary\n \n@@ -19,10 +17,11 @@ from scrapy.responsetypes import responsetypes\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.project import data_path\n from scrapy.utils.python import to_bytes, to_unicode\n-from scrapy.utils.request import RequestFingerprinter\n \n if TYPE_CHECKING:\n+    import os\n     from collections.abc import Callable\n+    from types import ModuleType\n \n     # typing.Concatenate requires Python 3.10\n     from typing_extensions import Concatenate\n@@ -30,6 +29,7 @@ if TYPE_CHECKING:\n     from scrapy.http.request import Request\n     from scrapy.settings import BaseSettings\n     from scrapy.spiders import Spider\n+    from scrapy.utils.request import RequestFingerprinter\n \n \n logger = logging.getLogger(__name__)\n\n@@ -2,7 +2,6 @@ from __future__ import annotations\n \n import logging\n from datetime import datetime, timezone\n-from json import JSONEncoder\n from typing import TYPE_CHECKING, Any\n \n from twisted.internet import task\n@@ -13,6 +12,8 @@ from scrapy.utils.serialize import ScrapyJSONEncoder\n \n if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n+    from json import JSONEncoder\n+\n     from typing_extensions import Self\n \n     from scrapy.crawler import Crawler\n\n@@ -13,7 +13,6 @@ import pprint\n from typing import TYPE_CHECKING, Any\n \n from twisted.internet import protocol\n-from twisted.internet.tcp import Port\n \n from scrapy import signals\n from scrapy.exceptions import NotConfigured\n@@ -24,6 +23,7 @@ from scrapy.utils.trackref import print_live_refs\n \n if TYPE_CHECKING:\n     from twisted.conch import telnet\n+    from twisted.internet.tcp import Port\n \n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n\n@@ -21,7 +21,6 @@ from typing import (\n \n from w3lib.url import safe_url_string\n \n-import scrapy\n from scrapy.http.headers import Headers\n from scrapy.utils.curl import curl_to_request_kwargs\n from scrapy.utils.python import to_bytes\n@@ -37,6 +36,7 @@ if TYPE_CHECKING:\n     # typing.NotRequired and typing.Self require Python 3.11\n     from typing_extensions import Concatenate, NotRequired, Self\n \n+    from scrapy import Spider\n     from scrapy.http import Response\n \n     CallbackT = Callable[Concatenate[Response, ...], Any]\n@@ -252,7 +252,7 @@ class Request(object_ref):\n         request_kwargs.update(kwargs)\n         return cls(**request_kwargs)\n \n-    def to_dict(self, *, spider: scrapy.Spider | None = None) -> dict[str, Any]:\n+    def to_dict(self, *, spider: Spider | None = None) -> dict[str, Any]:\n         \"\"\"Return a dictionary containing the Request's data.\n \n         Use :func:`~scrapy.utils.request.request_from_dict` to convert back into a :class:`~scrapy.Request` object.\n\n@@ -5,7 +5,6 @@ import logging\n from typing import TYPE_CHECKING, Protocol, cast\n \n from scrapy import Request\n-from scrapy.core.downloader import Downloader\n from scrapy.utils.misc import build_from_crawler\n \n if TYPE_CHECKING:\n@@ -14,6 +13,7 @@ if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy.core.downloader import Downloader\n     from scrapy.crawler import Crawler\n \n logger = logging.getLogger(__name__)\n\n@@ -3,7 +3,6 @@ from __future__ import annotations\n import numbers\n import os\n import sys\n-from collections.abc import Iterable\n from configparser import ConfigParser\n from operator import itemgetter\n from pathlib import Path\n@@ -15,7 +14,7 @@ from scrapy.utils.deprecate import update_classpath\n from scrapy.utils.python import without_none_values\n \n if TYPE_CHECKING:\n-    from collections.abc import Collection, Mapping, MutableMapping\n+    from collections.abc import Collection, Iterable, Mapping, MutableMapping\n \n \n def build_component_list(\n\n@@ -7,7 +7,6 @@ from typing import TYPE_CHECKING, Any, Generic, TypeVar\n from warnings import catch_warnings, filterwarnings\n \n from twisted.internet import asyncioreactor, error\n-from twisted.internet.base import DelayedCall\n \n from scrapy.utils.misc import load_object\n \n@@ -15,6 +14,7 @@ if TYPE_CHECKING:\n     from asyncio import AbstractEventLoop, AbstractEventLoopPolicy\n     from collections.abc import Callable\n \n+    from twisted.internet.base import DelayedCall\n     from twisted.internet.protocol import ServerFactory\n     from twisted.internet.tcp import Port\n \n\n@@ -16,7 +16,6 @@ from unittest import TestCase, mock\n from twisted.trial.unittest import SkipTest\n \n from scrapy import Spider\n-from scrapy.crawler import Crawler\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.boto import is_botocore_available\n \n@@ -26,6 +25,8 @@ if TYPE_CHECKING:\n     from twisted.internet.defer import Deferred\n     from twisted.web.client import Response as TxResponse\n \n+    from scrapy.crawler import Crawler\n+\n \n _T = TypeVar(\"_T\")\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f4d8d6d8acf8ed26230c3c2b2b51425659de7105", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 8 | Files Changed: 3 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 8444 | Contributors (this commit): 50 | Commits (past 90d): 5 | Contributors (cumulative): 59 | DMM Complexity: 0.0\n\nDIFF:\n@@ -17,7 +17,7 @@ class NoRequestsSpider(Spider):\n configure_logging({\"LOG_FORMAT\": \"%(levelname)s: %(message)s\", \"LOG_LEVEL\": \"DEBUG\"})\n \n \n-from scrapy.utils.reactor import install_reactor\n+from scrapy.utils.reactor import install_reactor  # noqa: E402\n \n install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n \n@@ -25,7 +25,7 @@ runner = CrawlerRunner()\n \n d = runner.crawl(NoRequestsSpider)\n \n-from twisted.internet import reactor\n+from twisted.internet import reactor  # noqa: E402\n \n d.addBoth(callback=lambda _: reactor.stop())\n reactor.run()\n\n@@ -473,7 +473,7 @@ class S3FeedStorageTest(unittest.TestCase):\n class GCSFeedStorageTest(unittest.TestCase):\n     def test_parse_settings(self):\n         try:\n-            from google.cloud.storage import Client  # noqa\n+            from google.cloud.storage import Client  # noqa: F401\n         except ImportError:\n             raise unittest.SkipTest(\"GCSFeedStorage requires google-cloud-storage\")\n \n@@ -487,7 +487,7 @@ class GCSFeedStorageTest(unittest.TestCase):\n \n     def test_parse_empty_acl(self):\n         try:\n-            from google.cloud.storage import Client  # noqa\n+            from google.cloud.storage import Client  # noqa: F401\n         except ImportError:\n             raise unittest.SkipTest(\"GCSFeedStorage requires google-cloud-storage\")\n \n@@ -504,7 +504,7 @@ class GCSFeedStorageTest(unittest.TestCase):\n     @defer.inlineCallbacks\n     def test_store(self):\n         try:\n-            from google.cloud.storage import Client  # noqa\n+            from google.cloud.storage import Client  # noqa: F401\n         except ImportError:\n             raise unittest.SkipTest(\"GCSFeedStorage requires google-cloud-storage\")\n \n\n@@ -273,9 +273,7 @@ class ItemMetaTest(unittest.TestCase):\n                 def f(self):\n                     # For rationale of this see:\n                     # https://github.com/python/cpython/blob/ee1a81b77444c6715cbe610e951c655b6adab88b/Lib/test/test_super.py#L222\n-                    return (\n-                        __class__  # noqa  https://github.com/scrapy/scrapy/issues/2836\n-                    )\n+                    return __class__\n \n             MyItem()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cde0845ab2ac390ad7671f50f52e7b6033d4bbc1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 74 | Lines Deleted: 68 | Files Changed: 35 | Hunks: 55 | Methods Changed: 37 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 142 | Churn Cumulative: 31442 | Contributors (this commit): 216 | Commits (past 90d): 139 | Contributors (cumulative): 775 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,6 +1,6 @@\n from __future__ import annotations\n \n-import subprocess  # nosec\n+import subprocess\n import sys\n import time\n from typing import TYPE_CHECKING, Any\n@@ -40,9 +40,9 @@ class _BenchServer:\n         from scrapy.utils.test import get_testenv\n \n         pargs = [sys.executable, \"-u\", \"-m\", \"scrapy.utils.benchserver\"]\n-        self.proc = subprocess.Popen(\n+        self.proc = subprocess.Popen(  # noqa: S603\n             pargs, stdout=subprocess.PIPE, env=get_testenv()\n-        )  # nosec\n+        )\n         assert self.proc.stdout\n         self.proc.stdout.readline()\n \n\n@@ -41,4 +41,4 @@ class Command(ScrapyCommand):\n         sfile = sys.modules[spidercls.__module__].__file__\n         assert sfile\n         sfile = sfile.replace(\".pyc\", \".py\")\n-        self.exitcode = os.system(f'{editor} \"{sfile}\"')  # nosec\n+        self.exitcode = os.system(f'{editor} \"{sfile}\"')  # noqa: S605\n\n@@ -118,7 +118,7 @@ class Command(ScrapyCommand):\n         if template_file:\n             self._genspider(module, name, url, opts.template, template_file)\n             if opts.edit:\n-                self.exitcode = os.system(f'scrapy edit \"{name}\"')  # nosec\n+                self.exitcode = os.system(f'scrapy edit \"{name}\"')  # noqa: S605\n \n     def _generate_template_variables(\n         self,\n\n@@ -52,7 +52,7 @@ class Slot:\n \n     def download_delay(self) -> float:\n         if self.randomize_delay:\n-            return random.uniform(0.5 * self.delay, 1.5 * self.delay)  # nosec\n+            return random.uniform(0.5 * self.delay, 1.5 * self.delay)  # noqa: S311\n         return self.delay\n \n     def close(self) -> None:\n\n@@ -115,7 +115,7 @@ def get_retry_request(\n         return new_request\n     stats.inc_value(f\"{stats_base_key}/max_reached\")\n     logger.error(\n-        \"Gave up retrying %(request)s (failed %(retry_times)d times): \" \"%(reason)s\",\n+        \"Gave up retrying %(request)s (failed %(retry_times)d times): %(reason)s\",\n         {\"request\": request, \"retry_times\": retry_times, \"reason\": reason},\n         extra={\"spider\": spider},\n     )\n\n@@ -6,13 +6,13 @@ from __future__ import annotations\n \n import csv\n import marshal\n-import pickle  # nosec\n+import pickle\n import pprint\n from collections.abc import Callable, Iterable, Mapping\n from io import BytesIO, TextIOWrapper\n from typing import TYPE_CHECKING, Any\n-from xml.sax.saxutils import XMLGenerator  # nosec\n-from xml.sax.xmlreader import AttributesImpl  # nosec\n+from xml.sax.saxutils import XMLGenerator\n+from xml.sax.xmlreader import AttributesImpl\n \n from itemadapter import ItemAdapter, is_item\n \n\n@@ -681,7 +681,7 @@ class FeedExporter:\n                 return True\n             except NotConfigured as e:\n                 logger.error(\n-                    \"Disabled feed storage scheme: %(scheme)s. \" \"Reason: %(reason)s\",\n+                    \"Disabled feed storage scheme: %(scheme)s. Reason: %(reason)s\",\n                     {\"scheme\": scheme, \"reason\": str(e)},\n                 )\n         else:\n\n@@ -2,7 +2,7 @@ from __future__ import annotations\n \n import gzip\n import logging\n-import pickle  # nosec\n+import pickle\n from email.utils import mktime_tz, parsedate_tz\n from importlib import import_module\n from pathlib import Path\n@@ -309,7 +309,7 @@ class DbmCacheStorage:\n         if 0 < self.expiration_secs < time() - float(ts):\n             return None  # expired\n \n-        return cast(dict[str, Any], pickle.loads(db[f\"{key}_data\"]))  # nosec\n+        return cast(dict[str, Any], pickle.loads(db[f\"{key}_data\"]))  # noqa: S301\n \n \n class FilesystemCacheStorage:\n@@ -392,7 +392,7 @@ class FilesystemCacheStorage:\n         if 0 < self.expiration_secs < time() - mtime:\n             return None  # expired\n         with self._open(metapath, \"rb\") as f:\n-            return cast(dict[str, Any], pickle.load(f))  # nosec\n+            return cast(dict[str, Any], pickle.load(f))  # noqa: S301\n \n \n def parse_cachecontrol(header: bytes) -> dict[bytes, bytes | None]:\n\n@@ -1,6 +1,6 @@\n from __future__ import annotations\n \n-import pickle  # nosec\n+import pickle\n from pathlib import Path\n from typing import TYPE_CHECKING\n \n@@ -41,7 +41,7 @@ class SpiderState:\n     def spider_opened(self, spider: Spider) -> None:\n         if self.jobdir and Path(self.statefn).exists():\n             with Path(self.statefn).open(\"rb\") as f:\n-                spider.state = pickle.load(f)  # type: ignore[attr-defined]  # nosec\n+                spider.state = pickle.load(f)  # type: ignore[attr-defined]  # noqa: S301\n         else:\n             spider.state = {}  # type: ignore[attr-defined]\n \n\n@@ -11,11 +11,13 @@ from collections.abc import Iterable\n from typing import TYPE_CHECKING, Any, Optional, Union, cast\n from urllib.parse import urlencode, urljoin, urlsplit, urlunsplit\n \n-from lxml.html import FormElement  # nosec\n-from lxml.html import InputElement  # nosec\n-from lxml.html import MultipleSelectOptions  # nosec\n-from lxml.html import SelectElement  # nosec\n-from lxml.html import TextareaElement  # nosec\n+from lxml.html import (\n+    FormElement,\n+    InputElement,\n+    MultipleSelectOptions,\n+    SelectElement,\n+    TextareaElement,\n+)\n from w3lib.html import strip_html5_whitespace\n \n from scrapy.http.request import Request\n\n@@ -107,7 +107,7 @@ class Response(object_ref):\n             self._url: str = url\n         else:\n             raise TypeError(\n-                f\"{type(self).__name__} url must be str, \" f\"got {type(url).__name__}\"\n+                f\"{type(self).__name__} url must be str, got {type(url).__name__}\"\n             )\n \n     @property\n\n@@ -308,7 +308,7 @@ def _url_from_selector(sel: parsel.Selector) -> str:\n         raise _InvalidSelector(f\"Unsupported selector: {sel}\")\n     if sel.root.tag not in (\"a\", \"link\"):\n         raise _InvalidSelector(\n-            \"Only <a> and <link> elements are supported; \" f\"got <{sel.root.tag}>\"\n+            f\"Only <a> and <link> elements are supported; got <{sel.root.tag}>\"\n         )\n     href = sel.root.get(\"href\")\n     if href is None:\n\n@@ -12,7 +12,7 @@ from functools import partial\n from typing import TYPE_CHECKING, Any, Union, cast\n from urllib.parse import urljoin, urlparse\n \n-from lxml import etree  # nosec\n+from lxml import etree\n from parsel.csstranslator import HTMLTranslator\n from w3lib.html import strip_html5_whitespace\n from w3lib.url import canonicalize_url, safe_url_string\n@@ -26,7 +26,7 @@ from scrapy.utils.url import url_has_any_extension, url_is_from_any_domain\n \n if TYPE_CHECKING:\n \n-    from lxml.html import HtmlElement  # nosec\n+    from lxml.html import HtmlElement\n \n     from scrapy import Selector\n     from scrapy.http import TextResponse\n\n@@ -66,7 +66,7 @@ def _md5sum(file: IO[bytes]) -> str:\n     >>> _md5sum(BytesIO(b'file content to hash'))\n     '784406af91dd5a54fbb9c84c2236595a'\n     \"\"\"\n-    m = hashlib.md5()  # nosec\n+    m = hashlib.md5()  # noqa: S324\n     while True:\n         d = file.read(8096)\n         if not d:\n@@ -399,7 +399,7 @@ class FTPFilesStore:\n                     ftp.set_pasv(False)\n                 file_path = f\"{self.basedir}/{path}\"\n                 last_modified = float(ftp.voidcmd(f\"MDTM {file_path}\")[4:].strip())\n-                m = hashlib.md5()  # nosec\n+                m = hashlib.md5()  # noqa: S324\n                 ftp.retrbinary(f\"RETR {file_path}\", m.update)\n                 return {\"last_modified\": last_modified, \"checksum\": m.hexdigest()}\n             # The file doesn't exist\n@@ -734,7 +734,7 @@ class FilesPipeline(MediaPipeline):\n         *,\n         item: Any = None,\n     ) -> str:\n-        media_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()  # nosec\n+        media_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()  # noqa: S324\n         media_ext = Path(request.url).suffix\n         # Handles empty and wild extensions by trying to guess the\n         # mime type then extension or default to empty string otherwise\n\n@@ -258,7 +258,7 @@ class ImagesPipeline(FilesPipeline):\n         *,\n         item: Any = None,\n     ) -> str:\n-        image_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()  # nosec\n+        image_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()  # noqa: S324\n         return f\"full/{image_guid}.jpg\"\n \n     def thumb_path(\n@@ -270,5 +270,5 @@ class ImagesPipeline(FilesPipeline):\n         *,\n         item: Any = None,\n     ) -> str:\n-        thumb_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()  # nosec\n+        thumb_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()  # noqa: S324\n         return f\"thumbs/{thumb_id}/{thumb_guid}.jpg\"\n\n@@ -33,7 +33,7 @@ def _path_safe(text: str) -> str:\n     pathable_slot = \"\".join([c if c.isalnum() or c in \"-._\" else \"_\" for c in text])\n     # as we replace some letters we can get collision for different slots\n     # add we add unique part\n-    unique_slot = hashlib.md5(text.encode(\"utf8\")).hexdigest()  # nosec\n+    unique_slot = hashlib.md5(text.encode(\"utf8\")).hexdigest()  # noqa: S324\n     return \"-\".join([pathable_slot, unique_slot])\n \n \n\n@@ -178,7 +178,7 @@ FILES_STORE_S3_ACL = \"private\"\n FILES_STORE_GCS_ACL = \"\"\n \n FTP_USER = \"anonymous\"\n-FTP_PASSWORD = \"guest\"  # nosec\n+FTP_PASSWORD = \"guest\"  # noqa: S105\n FTP_PASSIVE_MODE = True\n \n GCS_PROJECT_ID = None\n\n@@ -70,7 +70,7 @@ class Shell:\n         else:\n             self.populate_vars()\n         if self.code:\n-            print(eval(self.code, globals(), self.vars))  # nosec\n+            print(eval(self.code, globals(), self.vars))  # noqa: S307\n         else:\n             \"\"\"\n             Detect interactive shell setting in scrapy.cfg\n\n@@ -5,7 +5,7 @@ Scheduler queues\n from __future__ import annotations\n \n import marshal\n-import pickle  # nosec\n+import pickle\n from pathlib import Path\n from typing import TYPE_CHECKING, Any\n \n\n@@ -15,7 +15,7 @@ class Root(Resource):\n     def render(self, request: Request) -> bytes:\n         total = _getarg(request, b\"total\", 100, int)\n         show = _getarg(request, b\"show\", 10, int)\n-        nlist = [random.randint(1, total) for _ in range(show)]  # nosec\n+        nlist = [random.randint(1, total) for _ in range(show)]  # noqa: S311\n         request.write(b\"<html><head></head><body>\")\n         assert request.args is not None\n         args = request.args.copy()\n\n@@ -32,7 +32,7 @@ def get_engine_status(engine: ExecutionEngine) -> list[tuple[str, Any]]:\n     checks: list[tuple[str, Any]] = []\n     for test in tests:\n         try:\n-            checks += [(test, eval(test))]  # nosec\n+            checks += [(test, eval(test))]  # noqa: S307\n         except Exception as e:\n             checks += [(test, f\"{type(e).__name__} (exception)\")]\n \n\n@@ -7,7 +7,7 @@ from io import StringIO\n from typing import TYPE_CHECKING, Any, Literal, cast, overload\n from warnings import warn\n \n-from lxml import etree  # nosec\n+from lxml import etree\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Response, TextResponse\n@@ -41,10 +41,10 @@ def xmliter(obj: Response | str | bytes, nodename: str) -> Iterator[Selector]:\n \n     nodename_patt = re.escape(nodename)\n \n-    DOCUMENT_HEADER_RE = re.compile(r\"<\\?xml[^>]+>\\s*\", re.S)\n-    HEADER_END_RE = re.compile(rf\"<\\s*/{nodename_patt}\\s*>\", re.S)\n-    END_TAG_RE = re.compile(r\"<\\s*/([^\\s>]+)\\s*>\", re.S)\n-    NAMESPACE_RE = re.compile(r\"((xmlns[:A-Za-z]*)=[^>\\s]+)\", re.S)\n+    DOCUMENT_HEADER_RE = re.compile(r\"<\\?xml[^>]+>\\s*\", re.DOTALL)\n+    HEADER_END_RE = re.compile(rf\"<\\s*/{nodename_patt}\\s*>\", re.DOTALL)\n+    END_TAG_RE = re.compile(r\"<\\s*/([^\\s>]+)\\s*>\", re.DOTALL)\n+    NAMESPACE_RE = re.compile(r\"((xmlns[:A-Za-z]*)=[^>\\s]+)\", re.DOTALL)\n     text = _body_or_str(obj)\n \n     document_header_match = re.search(DOCUMENT_HEADER_RE, text)\n@@ -58,7 +58,9 @@ def xmliter(obj: Response | str | bytes, nodename: str) -> Iterator[Selector]:\n         for tagname in reversed(re.findall(END_TAG_RE, header_end)):\n             assert header_end_idx\n             tag = re.search(\n-                rf\"<\\s*{tagname}.*?xmlns[:=][^>]*>\", text[: header_end_idx[1]], re.S\n+                rf\"<\\s*{tagname}.*?xmlns[:=][^>]*>\",\n+                text[: header_end_idx[1]],\n+                re.DOTALL,\n             )\n             if tag:\n                 for x in re.findall(NAMESPACE_RE, tag.group()):\n\n@@ -116,7 +116,7 @@ def md5sum(file: IO[bytes]) -> str:\n         ScrapyDeprecationWarning,\n         stacklevel=2,\n     )\n-    m = hashlib.md5()  # nosec\n+    m = hashlib.md5()  # noqa: S324\n     while True:\n         d = file.read(8096)\n         if not d:\n\n@@ -136,7 +136,7 @@ def to_bytes(\n         return text\n     if not isinstance(text, str):\n         raise TypeError(\n-            \"to_bytes must receive a str or bytes \" f\"object, got {type(text).__name__}\"\n+            f\"to_bytes must receive a str or bytes object, got {type(text).__name__}\"\n         )\n     if encoding is None:\n         encoding = \"utf-8\"\n\n@@ -94,7 +94,9 @@ def fingerprint(\n             \"headers\": headers,\n         }\n         fingerprint_json = json.dumps(fingerprint_data, sort_keys=True)\n-        cache[cache_key] = hashlib.sha1(fingerprint_json.encode()).digest()  # nosec\n+        cache[cache_key] = hashlib.sha1(  # noqa: S324\n+            fingerprint_json.encode()\n+        ).digest()\n     return cache[cache_key]\n \n \n\n@@ -104,7 +104,7 @@ def open_in_browser(\n     elif isinstance(response, TextResponse):\n         ext = \".txt\"\n     else:\n-        raise TypeError(\"Unsupported response type: \" f\"{response.__class__.__name__}\")\n+        raise TypeError(f\"Unsupported response type: {response.__class__.__name__}\")\n     fd, fname = tempfile.mkstemp(ext)\n     os.write(fd, body)\n     os.close(fd)\n\n@@ -10,7 +10,7 @@ from __future__ import annotations\n from typing import TYPE_CHECKING, Any\n from urllib.parse import urljoin\n \n-import lxml.etree  # nosec\n+import lxml.etree\n \n if TYPE_CHECKING:\n     from collections.abc import Iterable, Iterator\n@@ -24,7 +24,7 @@ class Sitemap:\n         xmlp = lxml.etree.XMLParser(\n             recover=True, remove_comments=True, resolve_entities=False\n         )\n-        self._root = lxml.etree.fromstring(xmltext, parser=xmlp)  # nosec\n+        self._root = lxml.etree.fromstring(xmltext, parser=xmlp)  # noqa: S320\n         rt = self._root.tag\n         self.type = self._root.tag.split(\"}\", 1)[1] if \"}\" in rt else rt\n \n\n@@ -89,7 +89,7 @@ def escape_ajax(url: str) -> str:\n \n def add_http_if_no_scheme(url: str) -> str:\n     \"\"\"Add http as the default scheme if it is missing from the url.\"\"\"\n-    match = re.match(r\"^\\w+://\", url, flags=re.I)\n+    match = re.match(r\"^\\w+://\", url, flags=re.IGNORECASE)\n     if not match:\n         parts = urlparse(url)\n         scheme = \"http:\" if parts.netloc else \"http://\"\n\n@@ -3,7 +3,7 @@ import sys\n \n import cryptography\n import cssselect\n-import lxml.etree  # nosec\n+import lxml.etree\n import parsel\n import twisted\n import w3lib\n\n@@ -238,7 +238,7 @@ class StartprojectTemplatesTest(ProjectTest):\n         args = [\"--set\", f\"TEMPLATES_DIR={self.tmpl}\"]\n         p, out, err = self.proc(\"startproject\", self.project_name, *args)\n         self.assertIn(\n-            f\"New Scrapy project '{self.project_name}', \" \"using template directory\",\n+            f\"New Scrapy project '{self.project_name}', using template directory\",\n             out,\n         )\n         self.assertIn(self.tmpl_proj, out)\n\n@@ -67,8 +67,8 @@ class TestSpider(Spider):\n     allowed_domains = [\"scrapytest.org\", \"localhost\"]\n \n     itemurl_re = re.compile(r\"item\\d+.html\")\n-    name_re = re.compile(r\"<h1>(.*?)</h1>\", re.M)\n-    price_re = re.compile(r\">Price: \\$(.*?)<\", re.M)\n+    name_re = re.compile(r\"<h1>(.*?)</h1>\", re.MULTILINE)\n+    price_re = re.compile(r\">Price: \\$(.*?)<\", re.MULTILINE)\n \n     item_cls: type = TestItem\n \n\n@@ -631,7 +631,7 @@ class TestGCSFilesStore(unittest.TestCase):\n         \"\"\"\n         assert_gcs_environ()\n         try:\n-            import google.cloud.storage  # noqa\n+            import google.cloud.storage  # noqa: F401\n         except ModuleNotFoundError:\n             raise unittest.SkipTest(\"google-cloud-storage is not installed\")\n         with mock.patch(\"google.cloud.storage\") as _:\n\n@@ -66,14 +66,14 @@ class BaseRobotParserTest:\n         self.assertTrue(rp.allowed(\"https://www.site.local/is_allowed_too\", \"second\"))\n \n     def test_length_based_precedence(self):\n-        robotstxt_robotstxt_body = b\"User-agent: * \\n\" b\"Disallow: / \\n\" b\"Allow: /page\"\n+        robotstxt_robotstxt_body = b\"User-agent: * \\nDisallow: / \\nAllow: /page\"\n         rp = self.parser_cls.from_crawler(\n             crawler=None, robotstxt_body=robotstxt_robotstxt_body\n         )\n         self.assertTrue(rp.allowed(\"https://www.site.local/page\", \"*\"))\n \n     def test_order_based_precedence(self):\n-        robotstxt_robotstxt_body = b\"User-agent: * \\n\" b\"Disallow: / \\n\" b\"Allow: /page\"\n+        robotstxt_robotstxt_body = b\"User-agent: * \\nDisallow: / \\nAllow: /page\"\n         rp = self.parser_cls.from_crawler(\n             crawler=None, robotstxt_body=robotstxt_robotstxt_body\n         )\n\n@@ -21,9 +21,9 @@ class TrackrefTestCase(unittest.TestCase):\n         trackref.live_refs.clear()\n \n     def test_format_live_refs(self):\n-        o1 = Foo()  # NOQA\n-        o2 = Bar()  # NOQA\n-        o3 = Foo()  # NOQA\n+        o1 = Foo()  # noqa: F841\n+        o2 = Bar()  # noqa: F841\n+        o3 = Foo()  # noqa: F841\n         self.assertEqual(\n             trackref.format_live_refs(),\n             \"\"\"\\\n@@ -50,7 +50,7 @@ Bar                                 1   oldest: 0s ago\n \n     @mock.patch(\"sys.stdout\", new_callable=StringIO)\n     def test_print_live_refs_with_objects(self, stdout):\n-        o1 = Foo()  # NOQA\n+        o1 = Foo()  # noqa: F841\n         trackref.print_live_refs()\n         self.assertEqual(\n             stdout.getvalue(),\n@@ -61,11 +61,11 @@ Foo                                 1   oldest: 0s ago\\n\\n\"\"\",\n         )\n \n     def test_get_oldest(self):\n-        o1 = Foo()  # NOQA\n+        o1 = Foo()  # noqa: F841\n \n         o1_time = time()\n \n-        o2 = Bar()  # NOQA\n+        o2 = Bar()  # noqa: F841\n \n         o3_time = time()\n         if o3_time <= o1_time:\n@@ -74,15 +74,15 @@ Foo                                 1   oldest: 0s ago\\n\\n\"\"\",\n         if o3_time <= o1_time:\n             raise SkipTest(\"time.time is not precise enough\")\n \n-        o3 = Foo()  # NOQA\n+        o3 = Foo()  # noqa: F841\n         self.assertIs(trackref.get_oldest(\"Foo\"), o1)\n         self.assertIs(trackref.get_oldest(\"Bar\"), o2)\n         self.assertIsNone(trackref.get_oldest(\"XXX\"))\n \n     def test_iter_all(self):\n-        o1 = Foo()  # NOQA\n-        o2 = Bar()  # NOQA\n-        o3 = Foo()  # NOQA\n+        o1 = Foo()  # noqa: F841\n+        o2 = Bar()  # noqa: F841\n+        o3 = Foo()  # noqa: F841\n         self.assertEqual(\n             set(trackref.iter_all(\"Foo\")),\n             {o1, o3},\n\n@@ -161,7 +161,7 @@ class ScrapyHTTPPageGetterTests(unittest.TestCase):\n \n         # test minimal sent headers\n         factory = client.ScrapyHTTPClientFactory(Request(\"http://foo/bar\"))\n-        self._test(factory, b\"GET /bar HTTP/1.0\\r\\n\" b\"Host: foo\\r\\n\" b\"\\r\\n\")\n+        self._test(factory, b\"GET /bar HTTP/1.0\\r\\nHost: foo\\r\\n\\r\\n\")\n \n         # test a simple POST with body and content-type\n         factory = client.ScrapyHTTPClientFactory(\n@@ -191,7 +191,7 @@ class ScrapyHTTPPageGetterTests(unittest.TestCase):\n \n         self._test(\n             factory,\n-            b\"POST /bar HTTP/1.0\\r\\n\" b\"Host: foo\\r\\n\" b\"Content-Length: 0\\r\\n\" b\"\\r\\n\",\n+            b\"POST /bar HTTP/1.0\\r\\nHost: foo\\r\\nContent-Length: 0\\r\\n\\r\\n\",\n         )\n \n         # test with single and multivalued headers\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
