{"custom_id": "keras#b577dd9c5d4eb2277883fd0bf475d411b76bb106", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 2369 | Contributors (this commit): 32 | Commits (past 90d): 9 | Contributors (cumulative): 32 | DMM Complexity: None\n\nDIFF:\n@@ -6,6 +6,7 @@ import re\n import inspect\n import os\n import shutil\n+import six\n \n try:\n     import pathlib\n@@ -325,7 +326,7 @@ def get_module_docstring(filepath):\n     Also finds the line at which the docstring ends.\n     \"\"\"\n     co = compile(open(filepath).read(), filepath, 'exec')\n-    if co.co_consts and isinstance(co.co_consts[0], str):\n+    if co.co_consts and isinstance(co.co_consts[0], six.string_types):\n         docstring = co.co_consts[0]\n     else:\n         print('Could not get the docstring from ' + filepath)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#aac5391c984feda699f270c3757fcbed47749ad4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2371 | Contributors (this commit): 33 | Commits (past 90d): 10 | Contributors (cumulative): 33 | DMM Complexity: None\n\nDIFF:\n@@ -106,7 +106,7 @@ def class_to_source_link(cls):\n \n def code_snippet(snippet):\n     result = '```python\\n'\n-    result += snippet + '\\n'\n+    result += snippet.encode('unicode_escape').decode('utf8') + '\\n'\n     result += '```\\n'\n     return result\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e59570ae26670f788d6c649191031e4a8824f955", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 670 | Contributors (this commit): 10 | Commits (past 90d): 3 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -124,7 +124,7 @@ class TruncatedNormal(Initializer):\n \n     These values are similar to values from a `RandomNormal`\n     except that values more than two standard deviations from the mean\n-    are discarded and re-drawn. This is the recommended initializer for\n+    are discarded and redrawn. This is the recommended initializer for\n     neural network weights and filters.\n \n     # Arguments\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a1397169ddf8595736c01fcea084c8e34e1a3884", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 43 | Lines Deleted: 13 | Files Changed: 11 | Hunks: 20 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 56 | Churn Cumulative: 27574 | Contributors (this commit): 200 | Commits (past 90d): 22 | Contributors (cumulative): 306 | DMM Complexity: 0.8571428571428571\n\nDIFF:\n@@ -648,6 +648,9 @@ class ModelCheckpoint(Callback):\n         save_best_only: if `save_best_only=True`,\n             the latest best model according to\n             the quantity monitored will not be overwritten.\n+        save_weights_only: if True, then only the model's weights will be\n+            saved (`model.save_weights(filepath)`), else the full model\n+            is saved (`model.save(filepath)`).\n         mode: one of {auto, min, max}.\n             If `save_best_only=True`, the decision\n             to overwrite the current save file is made\n@@ -656,9 +659,6 @@ class ModelCheckpoint(Callback):\n             this should be `max`, for `val_loss` this should\n             be `min`, etc. In `auto` mode, the direction is\n             automatically inferred from the name of the monitored quantity.\n-        save_weights_only: if True, then only the model's weights will be\n-            saved (`model.save_weights(filepath)`), else the full model\n-            is saved (`model.save(filepath)`).\n         period: Interval (number of epochs) between checkpoints.\n     \"\"\"\n \n@@ -967,13 +967,13 @@ class TensorBoard(Callback):\n             and weight histograms for the layers of the model. If set to 0,\n             histograms won't be computed. Validation data (or split) must be\n             specified for histogram visualizations.\n+        batch_size: size of batch of inputs to feed to the network\n+            for histograms computation.\n         write_graph: whether to visualize the graph in TensorBoard.\n             The log file can become quite large when\n             write_graph is set to True.\n         write_grads: whether to visualize gradient histograms in TensorBoard.\n             `histogram_freq` must be greater than 0.\n-        batch_size: size of batch of inputs to feed to the network\n-            for histograms computation.\n         write_images: whether to write model weights to visualize as\n             image in TensorBoard.\n         embeddings_freq: frequency (in epochs) at which selected embedding\n\n@@ -26,6 +26,7 @@ class Sequential(Model):\n \n     # Arguments\n         layers: list of layers to add to the model.\n+        name: Name given to the model\n \n     # Example\n \n\n@@ -50,6 +50,9 @@ class Masking(Layer):\n         model.add(Masking(mask_value=0., input_shape=(timesteps, features)))\n         model.add(LSTM(32))\n     ```\n+\n+    # Arguments\n+        mask_value: Either None or mask value to skip\n     \"\"\"\n \n     def __init__(self, mask_value=0., **kwargs):\n@@ -625,6 +628,8 @@ class Lambda(Layer):\n                  `output_shape = (None, ) + output_shape`\n             If a function, it specifies the entire shape as a function of the\n             input shape: `output_shape = f(input_shape)`\n+        mask: Either None (indicating no masking) or a Tensor indicating the\n+          input mask for Embedding.\n         arguments: optional dictionary of keyword arguments to be passed\n             to the function.\n \n\n@@ -335,17 +335,17 @@ class CuDNNLSTM(_CuDNNRNN):\n         kernel_initializer: Initializer for the `kernel` weights matrix,\n             used for the linear transformation of the inputs.\n             (see [initializers](../initializers.md)).\n-        unit_forget_bias: Boolean.\n-            If True, add 1 to the bias of the forget gate at initialization.\n-            Setting it to true will also force `bias_initializer=\"zeros\"`.\n-            This is recommended in [Jozefowicz et al. (2015)](\n-            http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf).\n         recurrent_initializer: Initializer for the `recurrent_kernel`\n             weights matrix,\n             used for the linear transformation of the recurrent state.\n             (see [initializers](../initializers.md)).\n         bias_initializer: Initializer for the bias vector\n             (see [initializers](../initializers.md)).\n+        unit_forget_bias: Boolean.\n+            If True, add 1 to the bias of the forget gate at initialization.\n+            Setting it to true will also force `bias_initializer=\"zeros\"`.\n+            This is recommended in [Jozefowicz et al. (2015)](\n+            http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf).\n         kernel_regularizer: Regularizer function applied to\n             the `kernel` weights matrix\n             (see [regularizer](../regularizers.md)).\n\n@@ -45,6 +45,9 @@ class Embedding(Layer):\n         embeddings_regularizer: Regularizer function applied to\n             the `embeddings` matrix\n             (see [regularizer](../regularizers.md)).\n+        activity_regularizer: Regularizer function applied to\n+            the output of the layer (its \"activation\").\n+            (see [regularizer](../regularizers.md)).\n         embeddings_constraint: Constraint function applied to\n             the `embeddings` matrix\n             (see [constraints](../constraints.md)).\n\n@@ -47,6 +47,7 @@ class LocallyConnected1D(Layer):\n             any `dilation_rate` value != 1.\n         padding: Currently only supports `\"valid\"` (case-insensitive).\n             `\"same\"` may be supported in the future.\n+        data_format: String, one of `channels_first`, `channels_last`.\n         activation: Activation function to use\n             (see [activations](../activations.md)).\n             If you don't specify anything, no activation is applied\n\n@@ -116,6 +116,8 @@ class AlphaDropout(Layer):\n         rate: float, drop probability (as with `Dropout`).\n             The multiplicative noise will have\n             standard deviation `sqrt(rate / (1 - rate))`.\n+        noise_shape: A 1-D `Tensor` of type `int32`, representing the\n+            shape for randomly generated keep/drop flags.\n         seed: A Python integer to use as random seed.\n \n     # Input shape\n\n@@ -340,6 +340,7 @@ class Bidirectional(Wrapper):\n             One of {'sum', 'mul', 'concat', 'ave', None}.\n             If None, the outputs will not be combined,\n             they will be returned as a list.\n+        weights: Initial weights to load in the Bidirectional model\n \n     # Raises\n         ValueError: In case of invalid `merge_mode` argument.\n\n@@ -532,7 +532,8 @@ class Adamax(Optimizer):\n \n     # Arguments\n         lr: float >= 0. Learning rate.\n-        beta_1/beta_2: floats, 0 < beta < 1. Generally close to 1.\n+        beta_1: floats, 0 < beta < 1. Generally close to 1.\n+        beta_2: floats, 0 < beta < 1. Generally close to 1.\n         epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n         decay: float >= 0. Learning rate decay over each update.\n \n@@ -614,8 +615,10 @@ class Nadam(Optimizer):\n \n     # Arguments\n         lr: float >= 0. Learning rate.\n-        beta_1/beta_2: floats, 0 < beta < 1. Generally close to 1.\n+        beta_1: floats, 0 < beta < 1. Generally close to 1.\n+        beta_2: floats, 0 < beta < 1. Generally close to 1.\n         epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n+        schedule_decay: floats, 0 < schedule_decay < 1.\n \n     # References\n         - [Nadam report](http://cs229.stanford.edu/proj2015/054_report.pdf)\n@@ -694,6 +697,9 @@ class Nadam(Optimizer):\n \n class TFOptimizer(Optimizer):\n     \"\"\"Wrapper class for native TensorFlow optimizers.\n+\n+    # Arguments\n+        optimizer: Selected optimizer\n     \"\"\"\n \n     def __init__(self, optimizer):\n\n@@ -650,7 +650,7 @@ class GeneratorEnqueuer(SequenceEnqueuer):\n     Used in `fit_generator`, `evaluate_generator`, `predict_generator`.\n \n     # Arguments\n-        generator: a generator function which yields data\n+        sequence: a sequence function which yields data\n         use_multiprocessing: use multiprocessing if True, otherwise threading\n         wait_time: time to sleep in-between calls to `put()`\n         random_seed: Initial seed for workers,\n\n@@ -18,6 +18,14 @@ accepted_module = ['keras.legacy.layers', 'keras.utils.generic_utils']\n MIN_CODE_SIZE = 10\n \n \n+def handle_class_init(name, member):\n+    init_args = [\n+        arg for arg in list(inspect.signature(member.__init__).parameters.keys())\n+        if arg not in ['self', 'args', 'kwargs']\n+    ]\n+    assert_args_presence(init_args, member.__doc__, member, name)\n+\n+\n def handle_class(name, member):\n     if is_accepted(name, member):\n         return\n@@ -25,6 +33,9 @@ def handle_class(name, member):\n     if member.__doc__ is None and not member_too_small(member):\n         raise ValueError(\"{} class doesn't have any documentation\".format(name),\n                          member.__module__, inspect.getmodule(member).__file__)\n+\n+    handle_class_init(name, member)\n+\n     for n, met in inspect.getmembers(member):\n         if inspect.ismethod(met):\n             handle_method(n, met)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6368af9a5685266a094415eeb7c73e6d23f7d950", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 8 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 22 | Churn Cumulative: 2393 | Contributors (this commit): 33 | Commits (past 90d): 10 | Contributors (cumulative): 33 | DMM Complexity: 0.0\n\nDIFF:\n@@ -27,6 +27,8 @@ if sys.version[0] == '2':\n     reload(sys)\n     sys.setdefaultencoding('utf8')\n \n+keras_dir = pathlib.Path(__file__).resolve().parents[1]\n+\n \n def get_function_signature(function, method=True):\n     wrapped = getattr(function, '_original_function', None)\n@@ -367,22 +369,25 @@ def copy_examples():\n \n \n def generate():\n+    sources_dir = os.path.join(keras_dir, 'docs', 'sources')\n+    template_dir = os.path.join(keras_dir, 'docs', 'templates')\n+\n     if K.backend() != 'tensorflow':\n         raise RuntimeError('The documentation must be built '\n                            'with the TensorFlow backend because this '\n                            'is the only backend with docstrings.')\n \n     print('Cleaning up existing sources directory.')\n-    if os.path.exists('sources'):\n-        shutil.rmtree('sources')\n+    if os.path.exists(sources_dir):\n+        shutil.rmtree(sources_dir)\n \n     print('Populating sources directory with templates.')\n-    shutil.copytree('templates', 'sources')\n+    shutil.copytree(template_dir, sources_dir)\n \n-    readme = read_file('../README.md')\n-    index = read_file('templates/index.md')\n+    readme = read_file(os.path.join(keras_dir, 'README.md'))\n+    index = read_file(os.path.join(template_dir, 'index.md'))\n     index = index.replace('{{autogenerated}}', readme[readme.find('##'):])\n-    with open('sources/index.md', 'w') as f:\n+    with open(os.path.join(sources_dir, 'index.md'), 'w') as f:\n         f.write(index)\n \n     print('Generating docs for Keras %s.' % keras.__version__)\n@@ -434,7 +439,7 @@ def generate():\n         # Either insert content into existing page,\n         # or create page otherwise.\n         page_name = page_data['page']\n-        path = os.path.join('sources', page_name)\n+        path = os.path.join(sources_dir, page_name)\n         if os.path.exists(path):\n             template = read_file(path)\n             if '{{autogenerated}}' not in template:\n@@ -451,7 +456,8 @@ def generate():\n         with open(path, 'w') as f:\n             f.write(mkdown)\n \n-    shutil.copyfile('../CONTRIBUTING.md', 'sources/contributing.md')\n+    shutil.copyfile(os.path.join(keras_dir, 'CONTRIBUTING.md'),\n+                    os.path.join(sources_dir, 'contributing.md'))\n     copy_examples()\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fe35050a8f18dc52304aa8da4e463eececa25240", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 161 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -58,7 +58,8 @@ def _test_application_basic(app, last_dim=1000):\n def _test_application_notop(app, last_dim):\n     output_shape = _get_output_shape(\n         lambda: app(weights=None, include_top=False))\n-    assert output_shape == (None, None, None, last_dim)\n+    assert len(output_shape) == 4\n+    assert output_shape[-1] == last_dim\n \n \n def test_mobilenet_v2_legacy_import():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7a4c2fcdadf8ddaab1cd7fb461fae58bfa36a6ea", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 33 | Churn Cumulative: 10330 | Contributors (this commit): 51 | Commits (past 90d): 13 | Contributors (cumulative): 67 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1071,6 +1071,11 @@ def update_add(x, increment):\n     return C.assign(x, result)\n \n \n+def update_sub(x, decrement):\n+    result = x - decrement\n+    return C.assign(x, result)\n+\n+\n def gradients(loss, variables):\n     # cntk does not support gradients as symbolic op,\n     # to hook up with keras model\n@@ -2737,10 +2742,6 @@ def to_dense(tensor):\n     raise NotImplementedError\n \n \n-def update_sub(x, decrement):\n-    raise NotImplementedError\n-\n-\n def cumsum(x, axis=0):\n     raise NotImplementedError\n \n\n@@ -569,6 +569,30 @@ class TestBackend(object):\n     def test_log(self):\n         check_single_tensor_operation('log', (4, 2), WITH_NP)\n \n+    @pytest.mark.skipif(K.backend() == 'theano',\n+                        reason='theano returns tuples for update ops')\n+    def test_update_add(self):\n+        x = np.random.randn(3, 4)\n+        x_var = K.variable(x)\n+        increment = np.random.randn(3, 4)\n+\n+        x += increment\n+        K.eval(K.update_add(x_var, increment))\n+\n+        assert_allclose(x, K.eval(x_var), atol=1e-05)\n+\n+    @pytest.mark.skipif(K.backend() == 'theano',\n+                        reason='theano returns tuples for update ops')\n+    def test_update_sub(self):\n+        x = np.random.randn(3, 4)\n+        x_var = K.variable(x)\n+        decrement = np.random.randn(3, 4)\n+\n+        x -= decrement\n+        K.eval(K.update_sub(x_var, decrement))\n+\n+        assert_allclose(x, K.eval(x_var), atol=1e-05)\n+\n     @pytest.mark.skipif(K.backend() == 'cntk',\n                         reason='cntk doesn\\'t support gradient in this way.')\n     def test_gradient(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3fb9df96e03e4916e9db1c2594070bc6afa47f35", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 2409 | Contributors (this commit): 33 | Commits (past 90d): 11 | Contributors (cumulative): 33 | DMM Complexity: 0.0\n\nDIFF:\n@@ -336,19 +336,20 @@ def get_module_docstring(filepath):\n     return docstring, co.co_firstlineno\n \n \n-def copy_examples():\n+def copy_examples(examples_dir, destination_dir):\n     \"\"\"Copy the examples directory in the documentation.\n \n     Prettify files by extracting the docstrings written in Markdown.\n     \"\"\"\n-    pathlib.Path('sources/examples').mkdir(exist_ok=True)\n-    for file in os.listdir('../examples'):\n+    pathlib.Path(destination_dir).mkdir(exist_ok=True)\n+    for file in os.listdir(examples_dir):\n         if not file.endswith('.py'):\n             continue\n-        docstring, starting_line = get_module_docstring('../examples/' + file)\n-        destination_file = 'sources/examples/' + file[:-2] + 'md'\n+        module_path = os.path.join(examples_dir, file)\n+        docstring, starting_line = get_module_docstring(module_path)\n+        destination_file = os.path.join(destination_dir, file[:-2] + 'md')\n         with open(destination_file, 'w+') as f_out, \\\n-                open('../examples/' + file, 'r+') as f_in:\n+                open(os.path.join(examples_dir, file), 'r+') as f_in:\n \n             f_out.write(docstring + '\\n\\n')\n \n@@ -458,7 +459,8 @@ def generate():\n \n     shutil.copyfile(os.path.join(keras_dir, 'CONTRIBUTING.md'),\n                     os.path.join(sources_dir, 'contributing.md'))\n-    copy_examples()\n+    copy_examples(os.path.join(keras_dir, 'examples'),\n+                  os.path.join(sources_dir, 'examples'))\n \n \n if __name__ == '__main__':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c55ce21fdd4f55709269140e068cc840a7c7ec48", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 675 | Contributors (this commit): 12 | Commits (past 90d): 2 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -1,14 +1,14 @@\n-'''Trains a memory network on the bAbI dataset.\n+'''\n+#Trains a memory network on the bAbI dataset.\n \n References:\n \n - Jason Weston, Antoine Bordes, Sumit Chopra, Tomas Mikolov, Alexander M. Rush,\n-  \"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\",\n-  http://arxiv.org/abs/1502.05698\n+  [\"Towards AI-Complete Question Answering:\n+  A Set of Prerequisite Toy Tasks\"](http://arxiv.org/abs/1502.05698)\n \n - Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus,\n-  \"End-To-End Memory Networks\",\n-  http://arxiv.org/abs/1503.08895\n+  [\"End-To-End Memory Networks\"](http://arxiv.org/abs/1503.08895)\n \n Reaches 98.6% accuracy on task 'single_supporting_fact_10k' after 120 epochs.\n Time per epoch: 3s on CPU (core i7).\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#74ccfc4df6002bcbb44dc4ea65400b8b2dbef994", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 19 | Churn Cumulative: 2826 | Contributors (this commit): 33 | Commits (past 90d): 15 | Contributors (cumulative): 36 | DMM Complexity: 1.0\n\nDIFF:\n@@ -369,8 +369,12 @@ def copy_examples(examples_dir, destination_dir):\n             f_out.write('```')\n \n \n-def generate():\n-    sources_dir = os.path.join(keras_dir, 'docs', 'sources')\n+def generate(sources_dir):\n+    \"\"\"Generates the markdown files for the documentation.\n+\n+    # Arguments\n+        sources_dir: Where to put the markdown files.\n+    \"\"\"\n     template_dir = os.path.join(keras_dir, 'docs', 'templates')\n \n     if K.backend() != 'tensorflow':\n@@ -464,4 +468,4 @@ def generate():\n \n \n if __name__ == '__main__':\n-    generate()\n+    generate(os.path.join(keras_dir, 'docs', 'sources'))\n\n@@ -1,3 +1,4 @@\n+import os\n from markdown import markdown\n from docs import autogen\n import pytest\n@@ -386,5 +387,13 @@ def test_doc_multiple_sections_code():\n     assert 'def dot(x, y):' in generated\n \n \n+def test_docs_in_custom_destination_dir(tmpdir):\n+    autogen.generate(tmpdir)\n+    assert os.path.isdir(os.path.join(tmpdir, 'layers'))\n+    assert os.path.isdir(os.path.join(tmpdir, 'models'))\n+    assert os.path.isdir(os.path.join(tmpdir, 'examples'))\n+    assert os.listdir(os.path.join(tmpdir, 'examples'))\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8bc53bef4fd373d0f4276d00793b9a35fb1a4ef9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 657 | Contributors (this commit): 17 | Commits (past 90d): 1 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,5 @@\n-'''Train a simple deep CNN on the CIFAR10 small images dataset.\n+'''\n+#Train a simple deep CNN on the CIFAR10 small images dataset.\n \n It gets to 75% validation accuracy in 25 epochs, and 79% after 50 epochs.\n (it's still underfitting at that point, though).\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e2aced27b622642995d1941aa5daa087bf85b710", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 10 | Churn Cumulative: 181 | Contributors (this commit): 11 | Commits (past 90d): 1 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -168,6 +168,10 @@ def plot_model(model,\n             'LR' creates a horizontal plot.\n         expand_nested: whether to expand nested models into clusters.\n         dpi: dot DPI.\n+\n+    # Returns:\n+        A Jupyter notebook Image object if Jupyter is installed.\n+        This enables in-line display of the model plots in notebooks.\n     \"\"\"\n     dot = model_to_dot(model, show_shapes, show_layer_names, rankdir,\n                        expand_nested, dpi)\n@@ -177,3 +181,9 @@ def plot_model(model,\n     else:\n         extension = extension[1:]\n     dot.write(to_file, format=extension)\n+    # Return the image as a Jupyter Image object, to be displayed in-line.\n+    try:\n+        from IPython import display\n+        return display.Image(filename=to_file)\n+    except ImportError:\n+        pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#539aff8b37023caa177c1dcea6470af6e8b2d88f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 268 | Contributors (this commit): 8 | Commits (past 90d): 1 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,5 @@\n-\"\"\"Train a simple CNN-Capsule Network on the CIFAR10 small images dataset.\n+\"\"\"\n+#Train a simple CNN-Capsule Network on the CIFAR10 small images dataset.\n \n Without Data Augmentation:\n It gets to 75% validation accuracy in 10 epochs,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#79d9d3c669f5dded8beb38eec0a0c9519885df77", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 179 | Lines Deleted: 61 | Files Changed: 1 | Hunks: 32 | Methods Changed: 6 | Complexity Δ (Sum/Max): 15/15 | Churn Δ: 240 | Churn Cumulative: 573 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: 0.4411764705882353\n\nDIFF:\n@@ -1,33 +1,45 @@\n-'''Visualization of the filters of VGG16, via gradient ascent in input space.\n+\"\"\"Visualization of the filters of VGG16, via gradient ascent in input space.\n \n This script can run on CPU in a few minutes.\n \n Results example: http://i.imgur.com/4nj4KjN.jpg\n-'''\n+\"\"\"\n from __future__ import print_function\n \n-import numpy as np\n import time\n+import numpy as np\n+from PIL import Image as pil_image\n from keras.preprocessing.image import save_img\n+from keras import layers\n from keras.applications import vgg16\n from keras import backend as K\n \n-# dimensions of the generated pictures for each filter.\n-img_width = 128\n-img_height = 128\n \n-# the name of the layer we want to visualize\n-# (see model definition at keras/applications/vgg16.py)\n-layer_name = 'block5_conv1'\n+def normalize(x):\n+    \"\"\"utility function to normalize a tensor.\n \n-# util function to convert a tensor into a valid image\n+    # Arguments\n+        x: An input tensor.\n+\n+    # Returns\n+        The normalized input tensor.\n+    \"\"\"\n+    return x / (K.sqrt(K.mean(K.square(x))) + K.epsilon())\n \n \n def deprocess_image(x):\n-    # normalize tensor: center on 0., ensure std is 0.1\n+    \"\"\"utility function to convert a float array into a valid uint8 image.\n+\n+    # Arguments\n+        x: A numpy-array representing the generated image.\n+\n+    # Returns\n+        A processed numpy-array, which could be used in e.g. imshow.\n+    \"\"\"\n+    # normalize tensor: center on 0., ensure std is 0.25\n     x -= x.mean()\n     x /= (x.std() + K.epsilon())\n-    x *= 0.1\n+    x *= 0.25\n \n     # clip to [0, 1]\n     x += 0.5\n@@ -41,34 +53,69 @@ def deprocess_image(x):\n     return x\n \n \n-# build the VGG16 network with ImageNet weights\n-model = vgg16.VGG16(weights='imagenet', include_top=False)\n-print('Model loaded.')\n+def process_image(x, former):\n+    \"\"\"utility function to convert a valid uint8 image back into a float array.\n+       Reverses `deprocess_image`.\n \n-model.summary()\n+    # Arguments\n+        x: A numpy-array, which could be used in e.g. imshow.\n+        former: The former numpy-array.\n+                Need to determine the former mean and variance.\n \n-# this is the placeholder for the input images\n-input_img = model.input\n-\n-# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n-layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n+    # Returns\n+        A processed numpy-array representing the generated image.\n+    \"\"\"\n+    if K.image_data_format() == 'channels_first':\n+        x = x.transpose((2, 0, 1))\n+    return (x / 255 - 0.5) * 4 * former.std() + former.mean()\n \n \n-def normalize(x):\n-    # utility function to normalize a tensor by its L2 norm\n-    return x / (K.sqrt(K.mean(K.square(x))) + K.epsilon())\n+def visualize_layer(model,\n+                    layer_name,\n+                    step=1.,\n+                    epochs=15,\n+                    upscaling_steps=9,\n+                    upscaling_factor=1.2,\n+                    output_dim=(412, 412),\n+                    filter_range=(0, None)):\n+    \"\"\"Visualizes the most relevant filters of one conv-layer in a certain model.\n \n+    # Arguments\n+        model: The model containing layer_name.\n+        layer_name: The name of the layer to be visualized.\n+                    Has to be a part of model.\n+        step: step size for gradient ascent.\n+        epochs: Number of iterations for gradient ascent.\n+        upscaling_steps: Number of upscaling steps.\n+                         Starting image is in this case (80, 80).\n+        upscaling_factor: Factor to which to slowly upgrade\n+                          the image towards output_dim.\n+        output_dim: [img_width, img_height] The output image dimensions.\n+        filter_range: Tupel[lower, upper]\n+                      Determines the to be computed filter numbers.\n+                      If the second value is `None`,\n+                      the last filter will be inferred as the upper boundary.\n+    \"\"\"\n \n-kept_filters = []\n-for filter_index in range(200):\n-    # we only scan through the first 200 filters,\n-    # but there are actually 512 of them\n-    print('Processing filter %d' % filter_index)\n-    start_time = time.time()\n+    def _generate_filter_image(input_img,\n+                               layer_output,\n+                               filter_index):\n+        \"\"\"Generates image for one particular filter.\n+\n+        # Arguments\n+            input_img: The input-image Tensor.\n+            layer_output: The output-image Tensor.\n+            filter_index: The to be processed filter number.\n+                          Assumed to be valid.\n+\n+        #Returns\n+            Either None if no image could be generated.\n+            or a tuple of the image (array) itself and the last loss.\n+        \"\"\"\n+        s_time = time.time()\n \n         # we build a loss function that maximizes the activation\n         # of the nth filter of the layer considered\n-    layer_output = layer_dict[layer_name].output\n         if K.image_data_format() == 'channels_first':\n             loss = K.mean(layer_output[:, filter_index, :, :])\n         else:\n@@ -83,57 +130,128 @@ for filter_index in range(200):\n         # this function returns the loss and grads given the input picture\n         iterate = K.function([input_img], [loss, grads])\n \n-    # step size for gradient ascent\n-    step = 1.\n-\n         # we start from a gray image with some random noise\n+        intermediate_dim = tuple(\n+            int(x / (upscaling_factor ** upscaling_steps)) for x in output_dim)\n         if K.image_data_format() == 'channels_first':\n-        input_img_data = np.random.random((1, 3, img_width, img_height))\n+            input_img_data = np.random.random(\n+                (1, 3, intermediate_dim[0], intermediate_dim[1]))\n         else:\n-        input_img_data = np.random.random((1, img_width, img_height, 3))\n+            input_img_data = np.random.random(\n+                (1, intermediate_dim[0], intermediate_dim[1], 3))\n         input_img_data = (input_img_data - 0.5) * 20 + 128\n \n-    # we run gradient ascent for 20 steps\n-    for i in range(20):\n+        # Slowly upscaling towards the original size prevents\n+        # a dominating high-frequency of the to visualized structure\n+        # as it would occur if we directly compute the 412d-image.\n+        # Behaves as a better starting point for each following dimension\n+        # and therefore avoids poor local minima\n+        for up in reversed(range(upscaling_steps)):\n+            # we run gradient ascent for e.g. 20 steps\n+            for _ in range(epochs):\n                 loss_value, grads_value = iterate([input_img_data])\n                 input_img_data += grads_value * step\n \n-        print('Current loss value:', loss_value)\n-        if loss_value <= 0.:\n                 # some filters get stuck to 0, we can skip them\n-            break\n+                if loss_value <= K.epsilon():\n+                    return None\n+\n+            # Calulate upscaled dimension\n+            intermediate_dim = tuple(\n+                int(x / (upscaling_factor ** up)) for x in output_dim)\n+            # Upscale\n+            img = deprocess_image(input_img_data[0])\n+            img = np.array(pil_image.fromarray(img).resize(intermediate_dim,\n+                                                           pil_image.BICUBIC))\n+            input_img_data = [process_image(img, input_img_data[0])]\n \n         # decode the resulting input image\n-    if loss_value > 0:\n         img = deprocess_image(input_img_data[0])\n-        kept_filters.append((img, loss_value))\n-    end_time = time.time()\n-    print('Filter %d processed in %ds' % (filter_index, end_time - start_time))\n+        e_time = time.time()\n+        print('Costs of filter {:3}: {:5.0f} ( {:4.2f}s )'.format(filter_index,\n+                                                                  loss_value,\n+                                                                  e_time - s_time))\n+        return img, loss_value\n \n-# we will stich the best 64 filters on a 8 x 8 grid.\n-n = 8\n+    def _draw_filters(filters, n=None):\n+        \"\"\"Draw the best filters in a nxn grid.\n+\n+        # Arguments\n+            filters: A List of generated images and their corresponding losses\n+                     for each processed filter.\n+            n: dimension of the grid.\n+               If none, the largest possible square will be used\n+        \"\"\"\n+        if n is None:\n+            n = int(np.floor(np.sqrt(len(filters))))\n \n         # the filters that have the highest loss are assumed to be better-looking.\n-# we will only keep the top 64 filters.\n-kept_filters.sort(key=lambda x: x[1], reverse=True)\n-kept_filters = kept_filters[:n * n]\n+        # we will only keep the top n*n filters.\n+        filters.sort(key=lambda x: x[1], reverse=True)\n+        filters = filters[:n * n]\n \n         # build a black picture with enough space for\n-# our 8 x 8 filters of size 128 x 128, with a 5px margin in between\n-margin = 5\n-width = n * img_width + (n - 1) * margin\n-height = n * img_height + (n - 1) * margin\n-stitched_filters = np.zeros((width, height, 3))\n+        # e.g. our 8 x 8 filters of size 412 x 412, with a 5px margin in between\n+        MARGIN = 5\n+        width = n * output_dim[0] + (n - 1) * MARGIN\n+        height = n * output_dim[1] + (n - 1) * MARGIN\n+        stitched_filters = np.zeros((width, height, 3), dtype='uint8')\n \n         # fill the picture with our saved filters\n         for i in range(n):\n             for j in range(n):\n-        img, loss = kept_filters[i * n + j]\n-        width_margin = (img_width + margin) * i\n-        height_margin = (img_height + margin) * j\n+                img, _ = filters[i * n + j]\n+                width_margin = (output_dim[0] + MARGIN) * i\n+                height_margin = (output_dim[1] + MARGIN) * j\n                 stitched_filters[\n-            width_margin: width_margin + img_width,\n-            height_margin: height_margin + img_height, :] = img\n+                    width_margin: width_margin + output_dim[0],\n+                    height_margin: height_margin + output_dim[1], :] = img\n \n         # save the result to disk\n-save_img('stitched_filters_%dx%d.png' % (n, n), stitched_filters)\n+        save_img('vgg_{0:}_{1:}x{1:}.png'.format(layer_name, n), stitched_filters)\n+\n+    # this is the placeholder for the input images\n+    assert len(model.inputs) == 1\n+    input_img = model.inputs[0]\n+\n+    # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n+    layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n+\n+    output_layer = layer_dict[layer_name]\n+    assert isinstance(output_layer, layers.Conv2D)\n+\n+    # Compute to be processed filter range\n+    filter_lower = filter_range[0]\n+    filter_upper = (filter_range[1]\n+                    if filter_range[1] is not None\n+                    else len(output_layer.get_weights()[1]))\n+    assert(filter_lower >= 0\n+           and filter_upper <= len(output_layer.get_weights()[1])\n+           and filter_upper > filter_lower)\n+    print('Compute filters {:} to {:}'.format(filter_lower, filter_upper))\n+\n+    # iterate through each filter and generate its corresponding image\n+    processed_filters = []\n+    for f in range(filter_lower, filter_upper):\n+        img_loss = _generate_filter_image(input_img, output_layer.output, f)\n+\n+        if img_loss is not None:\n+            processed_filters.append(img_loss)\n+\n+    print('{} filter processed.'.format(len(processed_filters)))\n+    # Finally draw and store the best filters to disk\n+    _draw_filters(processed_filters)\n+\n+\n+if __name__ == '__main__':\n+    # the name of the layer we want to visualize\n+    # (see model definition at keras/applications/vgg16.py)\n+    LAYER_NAME = 'block5_conv1'\n+\n+    # build the VGG16 network with ImageNet weights\n+    vgg = vgg16.VGG16(weights='imagenet', include_top=False)\n+    print('Model loaded.')\n+    vgg.summary()\n+\n+    # example function call\n+    visualize_layer(vgg, LAYER_NAME)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c170b8e0a1b1f761d89fc9edcd603336b65f07c3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 7 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 26 | Churn Cumulative: 211 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -1,34 +1,36 @@\n-'''Train a simple deep CNN on the CIFAR10 small images dataset.\n+'''\n+#Train a simple deep CNN on the CIFAR10 small images dataset using augmentation.\n \n-Using Tensorflow internal augmentation APIs by replacing ImageGenerator with\n+Using TensorFlow internal augmentation APIs by replacing ImageGenerator with\n an embedded AugmentLayer using LambdaLayer, which is faster on GPU.\n \n-# Benchmark of `ImageGenerator` vs `AugmentLayer` both using augmentation 2D:\n+** Benchmark of `ImageGenerator`(IG) vs `AugmentLayer`(AL) both using augmentation\n+2D:**\n+\n (backend = Tensorflow-GPU, Nvidia Tesla P100-SXM2)\n \n-Settings: horizontal_flip = True\n-----------------------------------------------------------------------------\n-Epoch     | ImageGenerator | ImageGenerator | AugmentLayer  | Augment Layer\n-Number    | %Accuracy      | Performance    | %Accuracy     | Performance\n-----------------------------------------------------------------------------\n+Epoch no. | IG %Accuracy   | IG Performance | AL %Accuracy  | AL Performance\n+---------:|---------------:|---------------:|--------------:|--------------:\n 1         | 44.84          | 15 ms/step     | 45.54         | 358 us/step\n 2         | 52.34          |  8 ms/step     | 50.55         | 285 us/step\n 8         | 65.45          |  8 ms/step     | 65.59         | 281 us/step\n 25        | 76.74          |  8 ms/step     | 76.17         | 280 us/step\n 100       | 78.81          |  8 ms/step     | 78.70         | 285 us/step\n----------------------------------------------------------------------------\n \n-Settings: rotation = 30.0\n-----------------------------------------------------------------------------\n-Epoch     | ImageGenerator | ImageGenerator | AugmentLayer  | Augment Layer\n-Number    | %Accuracy      | Performance    | %Accuracy     | Performance\n-----------------------------------------------------------------------------\n+Settings: horizontal_flip = True\n+\n+\n+Epoch no. | IG %Accuracy   | IG Performance | AL %Accuracy  | AL Performance\n+---------:|---------------:|---------------:|--------------:|--------------:\n 1         | 43.46          | 15 ms/step     | 42.21         | 334 us/step\n 2         | 48.95          | 11 ms/step     | 48.06         | 282 us/step\n 8         | 63.59          | 11 ms/step     | 61.35         | 290 us/step\n 25        | 72.25          | 12 ms/step     | 71.08         | 287 us/step\n 100       | 76.35          | 11 ms/step     | 74.62         | 286 us/step\n----------------------------------------------------------------------------\n+\n+Settings: rotation = 30.0\n+\n+\n (Corner process and rotation precision by `ImageGenerator` and `AugmentLayer`\n are slightly different.)\n '''\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#58399c111a4639526f8d13d4bfa62fc3d0695b02", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 38 | Churn Cumulative: 905 | Contributors (this commit): 7 | Commits (past 90d): 1 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -1,12 +1,36 @@\n-\"\"\"Trains a ResNet on the CIFAR10 dataset.\n+\"\"\"\n+#Trains a ResNet on the CIFAR10 dataset.\n \n-ResNet v1\n-[a] Deep Residual Learning for Image Recognition\n-https://arxiv.org/pdf/1512.03385.pdf\n+ResNet v1:\n+[Deep Residual Learning for Image Recognition\n+](https://arxiv.org/pdf/1512.03385.pdf)\n \n-ResNet v2\n-[b] Identity Mappings in Deep Residual Networks\n-https://arxiv.org/pdf/1603.05027.pdf\n+ResNet v2:\n+[Identity Mappings in Deep Residual Networks\n+](https://arxiv.org/pdf/1603.05027.pdf)\n+\n+\n+Model|n|200-epoch accuracy|Original paper accuracy |sec/epoch GTX1080Ti\n+:------------|--:|-------:|-----------------------:|---:\n+ResNet20   v1|  3| 92.16 %|                 91.25 %|35\n+ResNet32   v1|  5| 92.46 %|                 92.49 %|50\n+ResNet44   v1|  7| 92.50 %|                 92.83 %|70\n+ResNet56   v1|  9| 92.71 %|                 93.03 %|90\n+ResNet110  v1| 18| 92.65 %|            93.39+-.16 %|165\n+ResNet164  v1| 27|     - %|                 94.07 %|  -\n+ResNet1001 v1|N/A|     - %|                 92.39 %|  -\n+\n+&nbsp;\n+\n+Model|n|200-epoch accuracy|Original paper accuracy |sec/epoch GTX1080Ti\n+:------------|--:|-------:|-----------------------:|---:\n+ResNet20   v2|  2|     - %|                     - %|---\n+ResNet32   v2|N/A| NA    %|            NA         %| NA\n+ResNet44   v2|N/A| NA    %|            NA         %| NA\n+ResNet56   v2|  6| 93.01 %|            NA         %|100\n+ResNet110  v2| 12| 93.15 %|            93.63      %|180\n+ResNet164  v2| 18|     - %|            94.54      %|  -\n+ResNet1001 v2|111|     - %|            95.08+-.14 %|  -\n \"\"\"\n \n from __future__ import print_function\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b38b1b9d61e383e486634cf34b1bd41fd5fb9f50", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 55 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -2,7 +2,7 @@\n \n It follows Hadsell-et-al.'06 [1] by computing the Euclidean distance on the\n output of the shared network and by optimizing the contrastive loss (see paper\n-for mode details).\n+for more details).\n \n # References\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a546ad4ad63c4410284387e55a8cd63a4244bf92", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 578 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -1,8 +1,9 @@\n-\"\"\"Visualization of the filters of VGG16, via gradient ascent in input space.\n+\"\"\"\n+#Visualization of the filters of VGG16, via gradient ascent in input space.\n \n This script can run on CPU in a few minutes.\n \n-Results example: http://i.imgur.com/4nj4KjN.jpg\n+Results example: ![Visualization](http://i.imgur.com/4nj4KjN.jpg)\n \"\"\"\n from __future__ import print_function\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#05d713504852b490afcf2607aea1ce923e93ecfe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 2433 | Contributors (this commit): 33 | Commits (past 90d): 13 | Contributors (cumulative): 33 | DMM Complexity: None\n\nDIFF:\n@@ -375,7 +375,7 @@ def generate(sources_dir):\n     # Arguments\n         sources_dir: Where to put the markdown files.\n     \"\"\"\n-    template_dir = os.path.join(keras_dir, 'docs', 'templates')\n+    template_dir = os.path.join(str(keras_dir), 'docs', 'templates')\n \n     if K.backend() != 'tensorflow':\n         raise RuntimeError('The documentation must be built '\n@@ -389,7 +389,7 @@ def generate(sources_dir):\n     print('Populating sources directory with templates.')\n     shutil.copytree(template_dir, sources_dir)\n \n-    readme = read_file(os.path.join(keras_dir, 'README.md'))\n+    readme = read_file(os.path.join(str(keras_dir), 'README.md'))\n     index = read_file(os.path.join(template_dir, 'index.md'))\n     index = index.replace('{{autogenerated}}', readme[readme.find('##'):])\n     with open(os.path.join(sources_dir, 'index.md'), 'w') as f:\n@@ -461,11 +461,11 @@ def generate(sources_dir):\n         with open(path, 'w') as f:\n             f.write(mkdown)\n \n-    shutil.copyfile(os.path.join(keras_dir, 'CONTRIBUTING.md'),\n-                    os.path.join(sources_dir, 'contributing.md'))\n-    copy_examples(os.path.join(keras_dir, 'examples'),\n-                  os.path.join(sources_dir, 'examples'))\n+    shutil.copyfile(os.path.join(str(keras_dir), 'CONTRIBUTING.md'),\n+                    os.path.join(str(sources_dir), 'contributing.md'))\n+    copy_examples(os.path.join(str(keras_dir), 'examples'),\n+                  os.path.join(str(sources_dir), 'examples'))\n \n \n if __name__ == '__main__':\n-    generate(os.path.join(keras_dir, 'docs', 'sources'))\n+    generate(os.path.join(str(keras_dir), 'docs', 'sources'))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
