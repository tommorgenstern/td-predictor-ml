{"custom_id": "scrapy#9411cf4e708ea60c7a6972a6804334f2a799e5c6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 54 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -51,7 +51,7 @@ class DecompressionMiddleware:\n         archive = BytesIO(response.body)\n         try:\n             zip_file = zipfile.ZipFile(archive)\n-        except zipfile.BadZipfile:\n+        except zipfile.BadZipFile:\n             return\n \n         namelist = zip_file.namelist()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#101a0c32d71090989be8d37f7b4a464b433705ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 70 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 10 | Methods Changed: 13 | Complexity Δ (Sum/Max): 6/3 | Churn Δ: 79 | Churn Cumulative: 2909 | Contributors (this commit): 35 | Commits (past 90d): 9 | Contributors (cumulative): 48 | DMM Complexity: 1.0\n\nDIFF:\n@@ -31,6 +31,14 @@ def extract_domain(url):\n     return o.netloc\n \n \n+def verify_url_scheme(url):\n+    \"\"\"Check url for scheme and insert https if none found.\"\"\"\n+    parsed = urlparse(url)\n+    if parsed.scheme == \"\" and parsed.netloc == \"\":\n+        parsed = urlparse(\"//\" + url)._replace(scheme=\"https\")\n+    return parsed.geturl()\n+\n+\n class Command(ScrapyCommand):\n     requires_project = False\n     default_settings = {\"LOG_ENABLED\": False}\n@@ -91,7 +99,7 @@ class Command(ScrapyCommand):\n             raise UsageError()\n \n         name, url = args[0:2]\n-        domain = extract_domain(url)\n+        url = verify_url_scheme(url)\n         module = sanitize_module_name(name)\n \n         if self.settings.get(\"BOT_NAME\") == module:\n@@ -103,18 +111,20 @@ class Command(ScrapyCommand):\n \n         template_file = self._find_template(opts.template)\n         if template_file:\n-            self._genspider(module, name, domain, opts.template, template_file)\n+            self._genspider(module, name, url, opts.template, template_file)\n             if opts.edit:\n                 self.exitcode = os.system(f'scrapy edit \"{name}\"')\n \n-    def _genspider(self, module, name, domain, template_name, template_file):\n+    def _genspider(self, module, name, url, template_name, template_file):\n         \"\"\"Generate the spider module, based on the given template\"\"\"\n         capitalized_module = \"\".join(s.capitalize() for s in module.split(\"_\"))\n+        domain = extract_domain(url)\n         tvars = {\n             \"project_name\": self.settings.get(\"BOT_NAME\"),\n             \"ProjectName\": string_camelcase(self.settings.get(\"BOT_NAME\")),\n             \"module\": module,\n             \"name\": name,\n+            \"url\": url,\n             \"domain\": domain,\n             \"classname\": f\"{capitalized_module}Spider\",\n         }\n\n@@ -541,7 +541,7 @@ class GenspiderCommandTest(CommandTest):\n             ).group(1),\n         )\n         self.assertEqual(\n-            f\"http://{domain}/\",\n+            f\"https://{domain}\",\n             self.find_in_file(\n                 Path(self.proj_mod_path, \"spiders\", \"test_name.py\"),\n                 r\"start_urls\\s*=\\s*\\[['\\\"](.+)['\\\"]\\]\",\n@@ -549,13 +549,64 @@ class GenspiderCommandTest(CommandTest):\n         )\n \n     def test_url_schema(self):\n-        self.test_url(\"http://test.com\", \"test.com\")\n+        self.test_url(\"https://test.com\", \"test.com\")\n \n-    def test_url_path(self):\n-        self.test_url(\"test.com/some/other/page\", \"test.com\")\n+    def test_template_start_urls(\n+        self, url=\"test.com\", expected=\"https://test.com\", template=\"basic\"\n+    ):\n+        self.assertEqual(\n+            0, self.call(\"genspider\", \"-t\", template, \"--force\", \"test_name\", url)\n+        )\n+        self.assertEqual(\n+            expected,\n+            self.find_in_file(\n+                Path(self.proj_mod_path, \"spiders\", \"test_name.py\"),\n+                r\"start_urls\\s*=\\s*\\[['\\\"](.+)['\\\"]\\]\",\n+            ).group(1),\n+        )\n \n-    def test_url_schema_path(self):\n-        self.test_url(\"https://test.com/some/other/page\", \"test.com\")\n+    def test_genspider_basic_start_urls(self):\n+        self.test_template_start_urls(\"https://test.com\", \"https://test.com\", \"basic\")\n+        self.test_template_start_urls(\"http://test.com\", \"http://test.com\", \"basic\")\n+        self.test_template_start_urls(\n+            \"http://test.com/other/path\", \"http://test.com/other/path\", \"basic\"\n+        )\n+        self.test_template_start_urls(\n+            \"test.com/other/path\", \"https://test.com/other/path\", \"basic\"\n+        )\n+\n+    def test_genspider_crawl_start_urls(self):\n+        self.test_template_start_urls(\"https://test.com\", \"https://test.com\", \"crawl\")\n+        self.test_template_start_urls(\"http://test.com\", \"http://test.com\", \"crawl\")\n+        self.test_template_start_urls(\n+            \"http://test.com/other/path\", \"http://test.com/other/path\", \"crawl\"\n+        )\n+        self.test_template_start_urls(\n+            \"test.com/other/path\", \"https://test.com/other/path\", \"crawl\"\n+        )\n+        self.test_template_start_urls(\"test.com\", \"https://test.com\", \"crawl\")\n+\n+    def test_genspider_xmlfeed_start_urls(self):\n+        self.test_template_start_urls(\n+            \"https://test.com/feed.xml\", \"https://test.com/feed.xml\", \"xmlfeed\"\n+        )\n+        self.test_template_start_urls(\n+            \"http://test.com/feed.xml\", \"http://test.com/feed.xml\", \"xmlfeed\"\n+        )\n+        self.test_template_start_urls(\n+            \"test.com/feed.xml\", \"https://test.com/feed.xml\", \"xmlfeed\"\n+        )\n+\n+    def test_genspider_csvfeed_start_urls(self):\n+        self.test_template_start_urls(\n+            \"https://test.com/feed.csv\", \"https://test.com/feed.csv\", \"csvfeed\"\n+        )\n+        self.test_template_start_urls(\n+            \"http://test.com/feed.xml\", \"http://test.com/feed.xml\", \"csvfeed\"\n+        )\n+        self.test_template_start_urls(\n+            \"test.com/feed.csv\", \"https://test.com/feed.csv\", \"csvfeed\"\n+        )\n \n \n class GenspiderStandaloneCommandTest(ProjectTest):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d60b4edd11436e61284615ec7ce89f8ac7e46d9a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 13 | Methods Changed: 6 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 47 | Churn Cumulative: 382 | Contributors (this commit): 16 | Commits (past 90d): 7 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,10 +1,11 @@\n import asyncio\n import sys\n from contextlib import suppress\n-from warnings import catch_warnings, filterwarnings\n+from warnings import catch_warnings, filterwarnings, warn\n \n from twisted.internet import asyncioreactor, error\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.misc import load_object\n \n \n@@ -54,7 +55,31 @@ class CallLaterOnce:\n         return self._func(*self._a, **self._kw)\n \n \n+def set_asyncio_event_loop_policy():\n+    \"\"\"The policy functions from asyncio often behave unexpectedly,\n+    so we restrict their use to the absolutely essential case.\n+    This should only be used to install the reactor.\n+    \"\"\"\n+    _get_asyncio_event_loop_policy()\n+\n+\n def get_asyncio_event_loop_policy():\n+    warn(\n+        \"Call to deprecated function \"\n+        \"scrapy.utils.reactor.get_asyncio_event_loop_policy().\\n\"\n+        \"\\n\"\n+        \"Please use get_event_loop, new_event_loop and set_event_loop\"\n+        \" from asyncio instead, as the corresponding policy methods may lead\"\n+        \" to unexpected behaviour.\\n\"\n+        \"This function is replaced by set_asyncio_event_loop_policy and\"\n+        \" is meant to be used only when the reactor is being installed.\",\n+        category=ScrapyDeprecationWarning,\n+        stacklevel=2,\n+    )\n+    return _get_asyncio_event_loop_policy()\n+\n+\n+def _get_asyncio_event_loop_policy():\n     policy = asyncio.get_event_loop_policy()\n     if (\n         sys.version_info >= (3, 8)\n@@ -63,7 +88,6 @@ def get_asyncio_event_loop_policy():\n     ):\n         policy = asyncio.WindowsSelectorEventLoopPolicy()\n         asyncio.set_event_loop_policy(policy)\n-\n     return policy\n \n \n@@ -73,6 +97,7 @@ def install_reactor(reactor_path, event_loop_path=None):\n     path if the asyncio reactor is enabled\"\"\"\n     reactor_class = load_object(reactor_path)\n     if reactor_class is asyncioreactor.AsyncioSelectorReactor:\n+        set_asyncio_event_loop_policy()\n         with suppress(error.ReactorAlreadyInstalledError):\n             event_loop = set_asyncio_event_loop(event_loop_path)\n             asyncioreactor.install(eventloop=event_loop)\n@@ -90,7 +115,6 @@ def _get_asyncio_event_loop():\n \n def set_asyncio_event_loop(event_loop_path):\n     \"\"\"Sets and returns the event loop with specified import path.\"\"\"\n-    policy = get_asyncio_event_loop_policy()\n     if event_loop_path is not None:\n         event_loop_class = load_object(event_loop_path)\n         event_loop = event_loop_class()\n@@ -109,15 +133,13 @@ def set_asyncio_event_loop(event_loop_path):\n                     message=\"There is no current event loop\",\n                     category=DeprecationWarning,\n                 )\n-                event_loop = policy.get_event_loop()\n+                event_loop = asyncio.get_event_loop()\n         except RuntimeError:\n             # `get_event_loop` raises RuntimeError when called with no asyncio\n             # event loop yet installed in the following scenarios:\n-            # - From a thread other than the main thread. For example, when\n-            #   using ``scrapy shell``.\n             # - Previsibly on Python 3.14 and later.\n             #   https://github.com/python/cpython/issues/100160#issuecomment-1345581902\n-            event_loop = policy.new_event_loop()\n+            event_loop = asyncio.new_event_loop()\n             asyncio.set_event_loop(event_loop)\n     return event_loop\n \n\n@@ -1,9 +1,14 @@\n+import asyncio\n import warnings\n from unittest import TestCase\n \n from pytest import mark\n \n-from scrapy.utils.reactor import install_reactor, is_asyncio_reactor_installed\n+from scrapy.utils.reactor import (\n+    install_reactor,\n+    is_asyncio_reactor_installed,\n+    set_asyncio_event_loop,\n+)\n \n \n @mark.usefixtures(\"reactor_pytest\")\n@@ -23,3 +28,7 @@ class AsyncioTest(TestCase):\n         from twisted.internet import reactor\n \n         assert original_reactor == reactor\n+\n+    async def test_set_asyncio_event_loop(self):\n+        install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n+        assert set_asyncio_event_loop() is asyncio.get_running_loop()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#39dbfa1d8276e1c8abb6aedc51f644ee27264f90", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 21 | Files Changed: 2 | Hunks: 14 | Methods Changed: 11 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 35 | Churn Cumulative: 8232 | Contributors (this commit): 49 | Commits (past 90d): 12 | Contributors (cumulative): 62 | DMM Complexity: 0.0\n\nDIFF:\n@@ -235,8 +235,10 @@ class BlockingFeedStorageTest(unittest.TestCase):\n \n \n class S3FeedStorageTest(unittest.TestCase):\n-    def test_parse_credentials(self):\n+    def setUp(self):\n         skip_if_no_boto()\n+\n+    def test_parse_credentials(self):\n         aws_credentials = {\n             \"AWS_ACCESS_KEY_ID\": \"settings_key\",\n             \"AWS_SECRET_ACCESS_KEY\": \"settings_secret\",\n@@ -272,8 +274,6 @@ class S3FeedStorageTest(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_store(self):\n-        skip_if_no_boto()\n-\n         settings = {\n             \"AWS_ACCESS_KEY_ID\": \"access_key\",\n             \"AWS_SECRET_ACCESS_KEY\": \"secret_key\",\n@@ -392,7 +392,6 @@ class S3FeedStorageTest(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_store_botocore_without_acl(self):\n-        skip_if_no_boto()\n         storage = S3FeedStorage(\n             \"s3://mybucket/export.csv\",\n             \"access_key\",\n@@ -408,7 +407,6 @@ class S3FeedStorageTest(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_store_botocore_with_acl(self):\n-        skip_if_no_boto()\n         storage = S3FeedStorage(\n             \"s3://mybucket/export.csv\", \"access_key\", \"secret_key\", \"custom-acl\"\n         )\n@@ -888,15 +886,10 @@ class FeedExportTest(FeedExportTestBase):\n     @defer.inlineCallbacks\n     def test_stats_multiple_file(self):\n         settings = {\n-            \"AWS_ACCESS_KEY_ID\": \"access_key\",\n-            \"AWS_SECRET_ACCESS_KEY\": \"secret_key\",\n             \"FEEDS\": {\n                 printf_escape(path_to_url(str(self._random_temp_filename()))): {\n                     \"format\": \"json\",\n                 },\n-                \"s3://bucket/key/foo.csv\": {\n-                    \"format\": \"csv\",\n-                },\n                 \"stdout:\": {\n                     \"format\": \"xml\",\n                 },\n@@ -908,18 +901,12 @@ class FeedExportTest(FeedExportTestBase):\n         self.assertIn(\n             \"feedexport/success_count/FileFeedStorage\", crawler.stats.get_stats()\n         )\n-        self.assertIn(\n-            \"feedexport/success_count/S3FeedStorage\", crawler.stats.get_stats()\n-        )\n         self.assertIn(\n             \"feedexport/success_count/StdoutFeedStorage\", crawler.stats.get_stats()\n         )\n         self.assertEqual(\n             crawler.stats.get_value(\"feedexport/success_count/FileFeedStorage\"), 1\n         )\n-        self.assertEqual(\n-            crawler.stats.get_value(\"feedexport/success_count/S3FeedStorage\"), 1\n-        )\n         self.assertEqual(\n             crawler.stats.get_value(\"feedexport/success_count/StdoutFeedStorage\"), 1\n         )\n@@ -2535,7 +2522,6 @@ class BatchDeliveriesTest(FeedExportTestBase):\n     @defer.inlineCallbacks\n     def test_s3_export(self):\n         skip_if_no_boto()\n-\n         bucket = \"mybucket\"\n         items = [\n             self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n@@ -2707,6 +2693,9 @@ class S3FeedStoragePreFeedOptionsTest(unittest.TestCase):\n \n     maxDiff = None\n \n+    def setUp(self):\n+        skip_if_no_boto()\n+\n     def test_init(self):\n         settings_dict = {\n             \"FEED_URI\": \"file:///tmp/foobar\",\n\n@@ -225,12 +225,16 @@ class FilesPipelineTestCase(unittest.TestCase):\n \n \n class FilesPipelineTestCaseFieldsMixin:\n+    def setUp(self):\n+        self.tempdir = mkdtemp()\n+\n+    def tearDown(self):\n+        rmtree(self.tempdir)\n+\n     def test_item_fields_default(self):\n         url = \"http://www.example.com/files/1.txt\"\n         item = self.item_class(name=\"item1\", file_urls=[url])\n-        pipeline = FilesPipeline.from_settings(\n-            Settings({\"FILES_STORE\": \"s3://example/files/\"})\n-        )\n+        pipeline = FilesPipeline.from_settings(Settings({\"FILES_STORE\": self.tempdir}))\n         requests = list(pipeline.get_media_requests(item, None))\n         self.assertEqual(requests[0].url, url)\n         results = [(True, {\"url\": url})]\n@@ -245,7 +249,7 @@ class FilesPipelineTestCaseFieldsMixin:\n         pipeline = FilesPipeline.from_settings(\n             Settings(\n                 {\n-                    \"FILES_STORE\": \"s3://example/files/\",\n+                    \"FILES_STORE\": self.tempdir,\n                     \"FILES_URLS_FIELD\": \"custom_file_urls\",\n                     \"FILES_RESULT_FIELD\": \"custom_files\",\n                 }\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7e7b41c6b32a639395cf3183e98fa8a8a78afd98", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 9 | Methods Changed: 6 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 35 | Churn Cumulative: 8730 | Contributors (this commit): 47 | Commits (past 90d): 18 | Contributors (cumulative): 77 | DMM Complexity: 1.0\n\nDIFF:\n@@ -228,12 +228,13 @@ class S3FeedStorage(BlockingFeedStorage):\n \n     def _store_in_thread(self, file):\n         file.seek(0)\n-        kwargs = {\"ACL\": self.acl} if self.acl else {}\n         if self._using_boto3:\n+            kwargs = {\"ExtraArgs\": {\"ACL\": self.acl}} if self.acl else {}\n             self.s3_client.upload_fileobj(\n                 Bucket=self.bucketname, Key=self.keyname, Fileobj=file, **kwargs\n             )\n         else:\n+            kwargs = {\"ACL\": self.acl} if self.acl else {}\n             self.s3_client.put_object(\n                 Bucket=self.bucketname, Key=self.keyname, Body=file, **kwargs\n             )\n\n@@ -44,6 +44,7 @@ from scrapy.extensions.feedexport import (\n     S3FeedStorage,\n     StdoutFeedStorage,\n     _FeedSlot,\n+    is_boto3_available,\n )\n from scrapy.settings import Settings\n from scrapy.utils.python import to_unicode\n@@ -285,6 +286,15 @@ class S3FeedStorageTest(unittest.TestCase):\n         verifyObject(IFeedStorage, storage)\n \n         file = mock.MagicMock()\n+\n+        if is_boto3_available():\n+            storage.s3_client = mock.MagicMock()\n+            yield storage.store(file)\n+            self.assertEqual(\n+                storage.s3_client.upload_fileobj.call_args,\n+                mock.call(Bucket=bucket, Key=key, Fileobj=file),\n+            )\n+        else:\n             from botocore.stub import Stubber\n \n             with Stubber(storage.s3_client) as stub:\n@@ -391,7 +401,7 @@ class S3FeedStorageTest(unittest.TestCase):\n         self.assertEqual(storage.endpoint_url, \"https://example.com\")\n \n     @defer.inlineCallbacks\n-    def test_store_botocore_without_acl(self):\n+    def test_store_without_acl(self):\n         storage = S3FeedStorage(\n             \"s3://mybucket/export.csv\",\n             \"access_key\",\n@@ -403,10 +413,18 @@ class S3FeedStorageTest(unittest.TestCase):\n \n         storage.s3_client = mock.MagicMock()\n         yield storage.store(BytesIO(b\"test file\"))\n-        self.assertNotIn(\"ACL\", storage.s3_client.put_object.call_args[1])\n+        if is_boto3_available():\n+            acl = (\n+                storage.s3_client.upload_fileobj.call_args[1]\n+                .get(\"ExtraArgs\", {})\n+                .get(\"ACL\")\n+            )\n+        else:\n+            acl = storage.s3_client.put_object.call_args[1].get(\"ACL\")\n+        self.assertIsNone(acl)\n \n     @defer.inlineCallbacks\n-    def test_store_botocore_with_acl(self):\n+    def test_store_with_acl(self):\n         storage = S3FeedStorage(\n             \"s3://mybucket/export.csv\", \"access_key\", \"secret_key\", \"custom-acl\"\n         )\n@@ -416,9 +434,11 @@ class S3FeedStorageTest(unittest.TestCase):\n \n         storage.s3_client = mock.MagicMock()\n         yield storage.store(BytesIO(b\"test file\"))\n-        self.assertEqual(\n-            storage.s3_client.put_object.call_args[1].get(\"ACL\"), \"custom-acl\"\n-        )\n+        if is_boto3_available():\n+            acl = storage.s3_client.upload_fileobj.call_args[1][\"ExtraArgs\"][\"ACL\"]\n+        else:\n+            acl = storage.s3_client.put_object.call_args[1][\"ACL\"]\n+        self.assertEqual(acl, \"custom-acl\")\n \n     def test_overwrite_default(self):\n         with LogCapture() as log:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c8ed793257d952a425ea1e55def4e1c3b3ca8b68", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -6/0 | Churn Δ: 4 | Churn Cumulative: 7067 | Contributors (this commit): 38 | Commits (past 90d): 9 | Contributors (cumulative): 38 | DMM Complexity: None\n\nDIFF:\n@@ -1057,13 +1057,13 @@ class FeedExportTest(FeedExportTestBase):\n                 self._random_temp_filename(): {\"format\": \"csv\"},\n             },\n             \"FEED_STORAGES\": {\"file\": LogOnStoreFileStorage},\n+            \"FEED_STORE_EMPTY\": False,\n         }\n \n         with LogCapture() as log:\n             yield self.exported_no_data(settings)\n \n-        print(log)\n-        self.assertEqual(str(log).count(\"Storage.store is called\"), 3)\n+        self.assertEqual(str(log).count(\"Storage.store is called\"), 0)\n \n     @defer.inlineCallbacks\n     def test_export_multiple_item_classes(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4ebc08ef1042cafd16c6a7eb20e3a4dcf43e3c97", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 17 | Files Changed: 3 | Hunks: 11 | Methods Changed: 6 | Complexity Δ (Sum/Max): 4/6 | Churn Δ: 31 | Churn Cumulative: 8862 | Contributors (this commit): 48 | Commits (past 90d): 23 | Contributors (cumulative): 86 | DMM Complexity: 0.14285714285714285\n\nDIFF:\n@@ -21,7 +21,7 @@ from zope.interface import Interface, implementer\n from scrapy import Spider, signals\n from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.extensions.postprocessing import PostProcessingManager\n-from scrapy.utils.boto import is_boto3_available, is_botocore_available\n+from scrapy.utils.boto import is_botocore_available\n from scrapy.utils.conf import feed_complete_default_values_from_settings\n from scrapy.utils.ftp import ftp_store_file\n from scrapy.utils.log import failure_to_exc_info\n@@ -30,6 +30,13 @@ from scrapy.utils.python import get_func_args, without_none_values\n \n logger = logging.getLogger(__name__)\n \n+try:\n+    import boto3  # noqa: F401\n+\n+    IS_BOTO3_AVAILABLE = True\n+except ImportError:\n+    IS_BOTO3_AVAILABLE = False\n+\n \n def build_storage(builder, uri, *args, feed_options=None, preargs=(), **kwargs):\n     argument_names = get_func_args(builder)\n@@ -173,9 +180,8 @@ class S3FeedStorage(BlockingFeedStorage):\n         self.keyname = u.path[1:]  # remove first \"/\"\n         self.acl = acl\n         self.endpoint_url = endpoint_url\n-        self._using_boto3 = is_boto3_available()\n \n-        if self._using_boto3:\n+        if IS_BOTO3_AVAILABLE:\n             import boto3.session\n \n             session = boto3.session.Session()\n@@ -228,7 +234,7 @@ class S3FeedStorage(BlockingFeedStorage):\n \n     def _store_in_thread(self, file):\n         file.seek(0)\n-        if self._using_boto3:\n+        if IS_BOTO3_AVAILABLE:\n             kwargs = {\"ExtraArgs\": {\"ACL\": self.acl}} if self.acl else {}\n             self.s3_client.upload_fileobj(\n                 Bucket=self.bucketname, Key=self.keyname, Fileobj=file, **kwargs\n\n@@ -8,12 +8,3 @@ def is_botocore_available():\n         return True\n     except ImportError:\n         return False\n-\n-\n-def is_boto3_available():\n-    try:\n-        import boto3  # noqa: F401\n-\n-        return True\n-    except ImportError:\n-        return False\n\n@@ -35,6 +35,7 @@ import scrapy\n from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.exporters import CsvItemExporter, JsonItemExporter\n from scrapy.extensions.feedexport import (\n+    IS_BOTO3_AVAILABLE,\n     BlockingFeedStorage,\n     FeedExporter,\n     FileFeedStorage,\n@@ -44,7 +45,6 @@ from scrapy.extensions.feedexport import (\n     S3FeedStorage,\n     StdoutFeedStorage,\n     _FeedSlot,\n-    is_boto3_available,\n )\n from scrapy.settings import Settings\n from scrapy.utils.python import to_unicode\n@@ -287,7 +287,7 @@ class S3FeedStorageTest(unittest.TestCase):\n \n         file = mock.MagicMock()\n \n-        if is_boto3_available():\n+        if IS_BOTO3_AVAILABLE:\n             storage.s3_client = mock.MagicMock()\n             yield storage.store(file)\n             self.assertEqual(\n@@ -413,7 +413,7 @@ class S3FeedStorageTest(unittest.TestCase):\n \n         storage.s3_client = mock.MagicMock()\n         yield storage.store(BytesIO(b\"test file\"))\n-        if is_boto3_available():\n+        if IS_BOTO3_AVAILABLE:\n             acl = (\n                 storage.s3_client.upload_fileobj.call_args[1]\n                 .get(\"ExtraArgs\", {})\n@@ -434,7 +434,7 @@ class S3FeedStorageTest(unittest.TestCase):\n \n         storage.s3_client = mock.MagicMock()\n         yield storage.store(BytesIO(b\"test file\"))\n-        if is_boto3_available():\n+        if IS_BOTO3_AVAILABLE:\n             acl = storage.s3_client.upload_fileobj.call_args[1][\"ExtraArgs\"][\"ACL\"]\n         else:\n             acl = storage.s3_client.put_object.call_args[1][\"ACL\"]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ada917307844950a81226f020b596d5932187f6e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 154 | Contributors (this commit): 6 | Commits (past 90d): 5 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -20,6 +20,9 @@ def x509name_to_string(x509name):\n \n def get_temp_key_info(ssl_object):\n     # adapted from OpenSSL apps/s_cb.c::ssl_print_tmp_key()\n+    if not hasattr(pyOpenSSLutil.lib, \"SSL_get_server_tmp_key\"):\n+        # removed in cryptography 40.0.0\n+        return None\n     temp_key_p = pyOpenSSLutil.ffi.new(\"EVP_PKEY **\")\n     if not pyOpenSSLutil.lib.SSL_get_server_tmp_key(ssl_object, temp_key_p):\n         return None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b83fa60a0a76364a64d4036e826a713a9a47f96d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 517 | Contributors (this commit): 20 | Commits (past 90d): 3 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -164,6 +164,7 @@ class MailSender:\n                 \"mailerr\": errstr,\n             },\n         )\n+        return failure\n \n     def _sendmail(self, to_addrs, msg):\n         from twisted.internet import reactor\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#90ce6589eee58e8aca9c368a71907b30250df68d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 404 | Contributors (this commit): 19 | Commits (past 90d): 5 | Contributors (cumulative): 19 | DMM Complexity: 0.0\n\nDIFF:\n@@ -88,7 +88,11 @@ class LxmlParserLinkExtractor:\n                 url = self.process_attr(attr_val)\n                 if url is None:\n                     continue\n+            try:\n                 url = safe_url_string(url, encoding=response_encoding)\n+            except ValueError:\n+                continue  # Disregard badly formatted urls\n+\n             # to fix relative links after process_value\n             url = urljoin(response_url, url)\n             link = Link(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9ef00c5c0bc16c9b44c878952a2b54310dc25638", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 408 | Contributors (this commit): 19 | Commits (past 90d): 6 | Contributors (cumulative): 19 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2,6 +2,7 @@\n Link extractor based on lxml.html\n \"\"\"\n import operator\n+import logging\n from functools import partial\n from urllib.parse import urljoin, urlparse\n \n@@ -23,6 +24,8 @@ from scrapy.utils.python import unique as unique_list\n from scrapy.utils.response import get_base_url\n from scrapy.utils.url import url_has_any_extension, url_is_from_any_domain\n \n+logger = logging.getLogger(__name__)\n+\n # from lxml/src/lxml/html/__init__.py\n XHTML_NAMESPACE = \"http://www.w3.org/1999/xhtml\"\n \n@@ -91,6 +94,7 @@ class LxmlParserLinkExtractor:\n             try:\n                 url = safe_url_string(url, encoding=response_encoding)\n             except ValueError:\n+                logger.error(f\"Skipping extraction of bad link {url}\")\n                 continue  # Disregard badly formatted urls\n \n             # to fix relative links after process_value\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9cbcf7724df7de9a449659fbf68bc5d532c33499", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 23 | Churn Cumulative: 1613 | Contributors (this commit): 13 | Commits (past 90d): 3 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -815,3 +815,26 @@ class LxmlLinkExtractorTestCase(Base.LinkExtractorTestCase):\n \n     def test_restrict_xpaths_with_html_entities(self):\n         super().test_restrict_xpaths_with_html_entities()\n+\n+    def test_skip_bad_links(self):\n+        html = b\"\"\"\n+        <a href=\"http://example.org/ignore_links : http://example.com/like_this\">Why would you do this?</a>\n+        <a href=\"http://example.org/item2.html\">Good Link</a>\n+        <a href=\"http://example.org/item3.html\">Good Link 2</a>\n+        \"\"\"\n+        response = HtmlResponse(\"http://example.org/index.html\", body=html)\n+        self.assertEqual(\n+            [link for link in lx.extract_links(response)],\n+            [\n+                Link(\n+                    url=\"http://example.org/item2.html\",\n+                    text=\"Good Link\",\n+                    nofollow=False,\n+                ),\n+                Link(\n+                    url=\"http://example.org/item3.html\",\n+                    text=\"Good Link 2\",\n+                    nofollow=False,\n+                ),\n+            ],\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7cb7cf1ad1aa3d75b494a5b069e5b76b60328daa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 1614 | Contributors (this commit): 13 | Commits (past 90d): 4 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -823,6 +823,7 @@ class LxmlLinkExtractorTestCase(Base.LinkExtractorTestCase):\n         <a href=\"http://example.org/item3.html\">Good Link 2</a>\n         \"\"\"\n         response = HtmlResponse(\"http://example.org/index.html\", body=html)\n+        lx = self.extractor_cls()\n         self.assertEqual(\n             [link for link in lx.extract_links(response)],\n             [\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#00d93026c8b078d75e4fe43c344f6524f9b45f28", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1618 | Contributors (this commit): 13 | Commits (past 90d): 5 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -818,11 +818,11 @@ class LxmlLinkExtractorTestCase(Base.LinkExtractorTestCase):\n \n     def test_skip_bad_links(self):\n         html = b\"\"\"\n-        <a href=\"http://example.org/ignore_links : http://example.com/like_this\">Why would you do this?</a>\n+        <a href=\"http://Some wierd html : http://example.com/like_this\">Why would you do this?</a>\n         <a href=\"http://example.org/item2.html\">Good Link</a>\n         <a href=\"http://example.org/item3.html\">Good Link 2</a>\n         \"\"\"\n-        response = HtmlResponse(\"http://example.org/index.html\", body=html)\n+        response = HtmlResponse(\"http://example.org/index.html\", body=html, encoding='utf-8')\n         lx = self.extractor_cls()\n         self.assertEqual(\n             [link for link in lx.extract_links(response)],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4043560547faac0ee4cfadba4d0f02b4be1f72de", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1620 | Contributors (this commit): 13 | Commits (past 90d): 6 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -822,7 +822,7 @@ class LxmlLinkExtractorTestCase(Base.LinkExtractorTestCase):\n         <a href=\"http://example.org/item2.html\">Good Link</a>\n         <a href=\"http://example.org/item3.html\">Good Link 2</a>\n         \"\"\"\n-        response = HtmlResponse(\"http://example.org/index.html\", body=html, encoding='utf-8')\n+        response = HtmlResponse(\"http://example.org/index.html\", body=html)\n         lx = self.extractor_cls()\n         self.assertEqual(\n             [link for link in lx.extract_links(response)],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c9a5934494cbb3fe0aa572b536ee222a3e212487", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 410 | Contributors (this commit): 19 | Commits (past 90d): 7 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -94,7 +94,7 @@ class LxmlParserLinkExtractor:\n             try:\n                 url = safe_url_string(url, encoding=response_encoding)\n             except ValueError:\n-                logger.error(f\"Skipping extraction of bad link {url}\")\n+                logger.debug(f\"Skipping extraction of bad link {url}\")\n                 continue  # Disregard badly formatted urls\n \n             # to fix relative links after process_value\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#608b7de582af891a37a3fab60423c847af648db8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 1625 | Contributors (this commit): 13 | Commits (past 90d): 7 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -1,6 +1,7 @@\n import pickle\n import re\n import unittest\n+import sys \n \n from scrapy.http import HtmlResponse, XmlResponse\n from scrapy.link import Link\n@@ -816,6 +817,10 @@ class LxmlLinkExtractorTestCase(Base.LinkExtractorTestCase):\n     def test_restrict_xpaths_with_html_entities(self):\n         super().test_restrict_xpaths_with_html_entities()\n \n+    @unittest.skipIf(\n+        sys.version_info < (3, 8),\n+        reason=\"Urllib3 is less strict in versions for python 3.7 so does not cause spider to crash\",\n+    )\n     def test_skip_bad_links(self):\n         html = b\"\"\"\n         <a href=\"http://Some wierd html : http://example.com/like_this\">Why would you do this?</a>\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#618e82dbe104c4b97cc4f7b37bf9130a76093734", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1627 | Contributors (this commit): 13 | Commits (past 90d): 8 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -819,7 +819,7 @@ class LxmlLinkExtractorTestCase(Base.LinkExtractorTestCase):\n \n     @unittest.skipIf(\n         sys.version_info < (3, 8),\n-        reason=\"Urllib3 is less strict in versions for python 3.7 so does not cause spider to crash\",\n+        reason=\"some library for python 3.7 so is less strict so bad links like htis don't crash scrapy\",\n     )\n     def test_skip_bad_links(self):\n         html = b\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a0e2e36b52743c9bf1cc03408bf4eb66e68a1edc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 15 | Files Changed: 2 | Hunks: 12 | Methods Changed: 5 | Complexity Δ (Sum/Max): 2/3 | Churn Δ: 24 | Churn Cumulative: 8780 | Contributors (this commit): 47 | Commits (past 90d): 22 | Contributors (cumulative): 79 | DMM Complexity: None\n\nDIFF:\n@@ -115,7 +115,6 @@ class BlockingFeedStorage:\n \n @implementer(IFeedStorage)\n class StdoutFeedStorage:\n-\n     def __init__(self, uri, _stdout=None, *, feed_options=None):\n         if not _stdout:\n             _stdout = sys.stdout.buffer\n@@ -384,7 +383,6 @@ class FeedExporter:\n         return defer.DeferredList(deferred_list) if deferred_list else None\n \n     def _close_slot(self, slot, spider):\n-\n         def get_file(slot_):\n             if isinstance(slot_.file, PostProcessingManager):\n                 return slot_.file.file\n\n@@ -1640,15 +1640,13 @@ class FeedExportTest(FeedExportTestBase):\n \n     @defer.inlineCallbacks\n     def test_storage_file_no_postprocessing(self):\n-\n         @implementer(IFeedStorage)\n         class Storage:\n-\n             def __init__(self, uri, *, feed_options=None):\n                 pass\n \n             def open(self, spider):\n-                Storage.open_file = tempfile.NamedTemporaryFile(prefix='feed-')\n+                Storage.open_file = tempfile.NamedTemporaryFile(prefix=\"feed-\")\n                 return Storage.open_file\n \n             def store(self, file):\n@@ -1656,23 +1654,21 @@ class FeedExportTest(FeedExportTestBase):\n                 file.close()\n \n         settings = {\n-            'FEEDS': {self._random_temp_filename(): {'format': 'jsonlines'}},\n-            'FEED_STORAGES': {'file': Storage},\n+            \"FEEDS\": {self._random_temp_filename(): {\"format\": \"jsonlines\"}},\n+            \"FEED_STORAGES\": {\"file\": Storage},\n         }\n         yield self.exported_no_data(settings)\n         self.assertIs(Storage.open_file, Storage.store_file)\n \n     @defer.inlineCallbacks\n     def test_storage_file_postprocessing(self):\n-\n         @implementer(IFeedStorage)\n         class Storage:\n-\n             def __init__(self, uri, *, feed_options=None):\n                 pass\n \n             def open(self, spider):\n-                Storage.open_file = tempfile.NamedTemporaryFile(prefix='feed-')\n+                Storage.open_file = tempfile.NamedTemporaryFile(prefix=\"feed-\")\n                 return Storage.open_file\n \n             def store(self, file):\n@@ -1680,15 +1676,15 @@ class FeedExportTest(FeedExportTestBase):\n                 file.close()\n \n         settings = {\n-            'FEEDS': {\n+            \"FEEDS\": {\n                 self._random_temp_filename(): {\n-                    'format': 'jsonlines',\n-                    'postprocessing': [\n-                        'scrapy.extensions.postprocessing.GzipPlugin',\n+                    \"format\": \"jsonlines\",\n+                    \"postprocessing\": [\n+                        \"scrapy.extensions.postprocessing.GzipPlugin\",\n                     ],\n                 },\n             },\n-            'FEED_STORAGES': {'file': Storage},\n+            \"FEED_STORAGES\": {\"file\": Storage},\n         }\n         yield self.exported_no_data(settings)\n         self.assertIs(Storage.open_file, Storage.store_file)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#23017e6e926116162cc8fbdaf38e02670ad5d5cb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 1684 | Contributors (this commit): 39 | Commits (past 90d): 13 | Contributors (cumulative): 39 | DMM Complexity: 1.0\n\nDIFF:\n@@ -385,6 +385,7 @@ class FeedExporter:\n     def _close_slot(self, slot, spider):\n         def get_file(slot_):\n             if isinstance(slot_.file, PostProcessingManager):\n+                slot_.file.close()\n                 return slot_.file.file\n             return slot_.file\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c7730627a0f99afccca11437b0775757c50ca9e8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 239 | Lines Deleted: 141 | Files Changed: 16 | Hunks: 142 | Methods Changed: 134 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 380 | Churn Cumulative: 12860 | Contributors (this commit): 86 | Commits (past 90d): 42 | Contributors (cumulative): 277 | DMM Complexity: 0.9126984126984127\n\nDIFF:\n@@ -4,8 +4,9 @@ from datetime import datetime\n from time import time\n \n from twisted.internet import defer, task\n+from twisted.internet.defer import Deferred\n \n-from scrapy import signals\n+from scrapy import Request, Spider, signals\n from scrapy.core.downloader.handlers import DownloadHandlers\n from scrapy.core.downloader.middleware import DownloaderMiddlewareManager\n from scrapy.resolver import dnscache\n@@ -86,7 +87,7 @@ class Downloader:\n         self._slot_gc_loop.start(60)\n         self.per_slot_settings = self.settings.getdict(\"DOWNLOAD_SLOTS\", {})\n \n-    def fetch(self, request, spider):\n+    def fetch(self, request: Request, spider: Spider) -> Deferred:\n         def _deactivate(response):\n             self.active.remove(request)\n             return response\n@@ -206,12 +207,12 @@ class Downloader:\n \n         return dfd.addBoth(finish_transferring)\n \n-    def close(self):\n+    def close(self) -> None:\n         self._slot_gc_loop.stop()\n         for slot in self.slots.values():\n             slot.close()\n \n-    def _slot_gc(self, age=60):\n+    def _slot_gc(self, age: float = 60) -> None:\n         mintime = time() - age\n         for key, slot in list(self.slots.items()):\n             if not slot.active and slot.lastseen + slot.delay < mintime:\n\n@@ -8,13 +8,16 @@ import logging\n import warnings\n from time import time\n from typing import (\n+    TYPE_CHECKING,\n     Any,\n     Callable,\n     Generator,\n     Iterable,\n     Iterator,\n+    List,\n     Optional,\n     Set,\n+    Type,\n     Union,\n     cast,\n )\n@@ -28,30 +31,36 @@ from scrapy.core.downloader import Downloader\n from scrapy.core.scraper import Scraper\n from scrapy.exceptions import CloseSpider, DontCloseSpider, ScrapyDeprecationWarning\n from scrapy.http import Request, Response\n-from scrapy.settings import BaseSettings\n+from scrapy.logformatter import LogFormatter\n+from scrapy.settings import BaseSettings, Settings\n+from scrapy.signalmanager import SignalManager\n from scrapy.spiders import Spider\n from scrapy.utils.log import failure_to_exc_info, logformatter_adapter\n from scrapy.utils.misc import create_instance, load_object\n from scrapy.utils.reactor import CallLaterOnce\n \n+if TYPE_CHECKING:\n+    from scrapy.core.scheduler import BaseScheduler\n+    from scrapy.crawler import Crawler\n+\n logger = logging.getLogger(__name__)\n \n \n class Slot:\n     def __init__(\n         self,\n-        start_requests: Iterable,\n+        start_requests: Iterable[Request],\n         close_if_idle: bool,\n         nextcall: CallLaterOnce,\n-        scheduler,\n+        scheduler: \"BaseScheduler\",\n     ) -> None:\n         self.closing: Optional[Deferred] = None\n         self.inprogress: Set[Request] = set()\n-        self.start_requests: Optional[Iterator] = iter(start_requests)\n-        self.close_if_idle = close_if_idle\n-        self.nextcall = nextcall\n-        self.scheduler = scheduler\n-        self.heartbeat = LoopingCall(nextcall.schedule)\n+        self.start_requests: Optional[Iterator[Request]] = iter(start_requests)\n+        self.close_if_idle: bool = close_if_idle\n+        self.nextcall: CallLaterOnce = nextcall\n+        self.scheduler: \"BaseScheduler\" = scheduler\n+        self.heartbeat: LoopingCall = LoopingCall(nextcall.schedule)\n \n     def add_request(self, request: Request) -> None:\n         self.inprogress.add(request)\n@@ -75,25 +84,28 @@ class Slot:\n \n \n class ExecutionEngine:\n-    def __init__(self, crawler, spider_closed_callback: Callable) -> None:\n-        self.crawler = crawler\n-        self.settings = crawler.settings\n-        self.signals = crawler.signals\n-        self.logformatter = crawler.logformatter\n+    def __init__(self, crawler: \"Crawler\", spider_closed_callback: Callable) -> None:\n+        self.crawler: \"Crawler\" = crawler\n+        self.settings: Settings = crawler.settings\n+        self.signals: SignalManager = crawler.signals\n+        self.logformatter: LogFormatter = crawler.logformatter\n         self.slot: Optional[Slot] = None\n         self.spider: Optional[Spider] = None\n-        self.running = False\n-        self.paused = False\n-        self.scheduler_cls = self._get_scheduler_class(crawler.settings)\n-        downloader_cls = load_object(self.settings[\"DOWNLOADER\"])\n+        self.running: bool = False\n+        self.paused: bool = False\n+        self.scheduler_cls: Type[\"BaseScheduler\"] = self._get_scheduler_class(\n+            crawler.settings\n+        )\n+        downloader_cls: Type[Downloader] = load_object(self.settings[\"DOWNLOADER\"])\n         self.downloader: Downloader = downloader_cls(crawler)\n         self.scraper = Scraper(crawler)\n-        self._spider_closed_callback = spider_closed_callback\n+        self._spider_closed_callback: Callable = spider_closed_callback\n+        self.start_time: Optional[float] = None\n \n-    def _get_scheduler_class(self, settings: BaseSettings) -> type:\n+    def _get_scheduler_class(self, settings: BaseSettings) -> Type[\"BaseScheduler\"]:\n         from scrapy.core.scheduler import BaseScheduler\n \n-        scheduler_cls = load_object(settings[\"SCHEDULER\"])\n+        scheduler_cls: Type = load_object(settings[\"SCHEDULER\"])\n         if not issubclass(scheduler_cls, BaseScheduler):\n             raise TypeError(\n                 f\"The provided scheduler class ({settings['SCHEDULER']})\"\n@@ -115,7 +127,7 @@ class ExecutionEngine:\n         \"\"\"Gracefully stop the execution engine\"\"\"\n \n         @inlineCallbacks\n-        def _finish_stopping_engine(_) -> Generator[Deferred, Any, None]:\n+        def _finish_stopping_engine(_: Any) -> Generator[Deferred, Any, None]:\n             yield self.signals.send_catch_log_deferred(signal=signals.engine_stopped)\n             self._closewait.callback(None)\n \n@@ -141,7 +153,8 @@ class ExecutionEngine:\n             return self.close_spider(\n                 self.spider, reason=\"shutdown\"\n             )  # will also close downloader\n-        return succeed(self.downloader.close())\n+        self.downloader.close()\n+        return succeed(None)\n \n     def pause(self) -> None:\n         self.paused = True\n@@ -209,7 +222,7 @@ class ExecutionEngine:\n                 extra={\"spider\": self.spider},\n             )\n         )\n-        d.addBoth(lambda _: cast(Slot, self.slot).remove_request(request))\n+        d.addBoth(lambda _: cast(Slot, self.slot).remove_request(request))  # type: ignore[arg-type]\n         d.addErrback(\n             lambda f: logger.info(\n                 \"Error while removing request from slot\",\n@@ -339,6 +352,7 @@ class ExecutionEngine:\n             if isinstance(result, Response):\n                 if result.request is None:\n                     result.request = request\n+                assert spider is not None\n                 logkws = self.logformatter.crawled(result.request, result, spider)\n                 if logkws is not None:\n                     logger.log(*logformatter_adapter(logkws), extra={\"spider\": spider})\n@@ -350,10 +364,12 @@ class ExecutionEngine:\n                 )\n             return result\n \n-        def _on_complete(_):\n+        def _on_complete(_: Any) -> Any:\n+            assert self.slot is not None\n             self.slot.nextcall.schedule()\n             return _\n \n+        assert spider is not None\n         dwld = self.downloader.fetch(request, spider)\n         dwld.addCallbacks(_on_success)\n         dwld.addBoth(_on_complete)\n@@ -362,7 +378,7 @@ class ExecutionEngine:\n     @inlineCallbacks\n     def open_spider(\n         self, spider: Spider, start_requests: Iterable = (), close_if_idle: bool = True\n-    ):\n+    ) -> Generator[Deferred, Any, None]:\n         if self.slot is not None:\n             raise RuntimeError(f\"No free spider slot when opening {spider.name!r}\")\n         logger.info(\"Spider opened\", extra={\"spider\": spider})\n@@ -471,7 +487,7 @@ class ExecutionEngine:\n         return dfd\n \n     @property\n-    def open_spiders(self) -> list:\n+    def open_spiders(self) -> List[Spider]:\n         warnings.warn(\n             \"ExecutionEngine.open_spiders is deprecated, please use ExecutionEngine.spider instead\",\n             category=ScrapyDeprecationWarning,\n\n@@ -2,13 +2,15 @@ import json\n import logging\n from abc import abstractmethod\n from pathlib import Path\n-from typing import Optional, Type, TypeVar\n+from typing import Any, Optional, Type, TypeVar, cast\n \n from twisted.internet.defer import Deferred\n \n from scrapy.crawler import Crawler\n+from scrapy.dupefilters import BaseDupeFilter\n from scrapy.http.request import Request\n from scrapy.spiders import Spider\n+from scrapy.statscollectors import StatsCollector\n from scrapy.utils.job import job_dir\n from scrapy.utils.misc import create_instance, load_object\n \n@@ -20,10 +22,10 @@ class BaseSchedulerMeta(type):\n     Metaclass to check scheduler classes against the necessary interface\n     \"\"\"\n \n-    def __instancecheck__(cls, instance):\n+    def __instancecheck__(cls, instance: Any) -> bool:\n         return cls.__subclasscheck__(type(instance))\n \n-    def __subclasscheck__(cls, subclass):\n+    def __subclasscheck__(cls, subclass: type) -> bool:\n         return (\n             hasattr(subclass, \"has_pending_requests\")\n             and callable(subclass.has_pending_requests)\n@@ -168,26 +170,26 @@ class Scheduler(BaseScheduler):\n \n     def __init__(\n         self,\n-        dupefilter,\n+        dupefilter: BaseDupeFilter,\n         jobdir: Optional[str] = None,\n         dqclass=None,\n         mqclass=None,\n         logunser: bool = False,\n-        stats=None,\n+        stats: Optional[StatsCollector] = None,\n         pqclass=None,\n         crawler: Optional[Crawler] = None,\n     ):\n-        self.df = dupefilter\n-        self.dqdir = self._dqdir(jobdir)\n+        self.df: BaseDupeFilter = dupefilter\n+        self.dqdir: Optional[str] = self._dqdir(jobdir)\n         self.pqclass = pqclass\n         self.dqclass = dqclass\n         self.mqclass = mqclass\n-        self.logunser = logunser\n-        self.stats = stats\n-        self.crawler = crawler\n+        self.logunser: bool = logunser\n+        self.stats: Optional[StatsCollector] = stats\n+        self.crawler: Optional[Crawler] = crawler\n \n     @classmethod\n-    def from_crawler(cls: Type[SchedulerTV], crawler) -> SchedulerTV:\n+    def from_crawler(cls: Type[SchedulerTV], crawler: Crawler) -> SchedulerTV:\n         \"\"\"\n         Factory method, initializes the scheduler with arguments taken from the crawl settings\n         \"\"\"\n@@ -242,6 +244,7 @@ class Scheduler(BaseScheduler):\n             self.df.log(request, self.spider)\n             return False\n         dqok = self._dqpush(request)\n+        assert self.stats is not None\n         if dqok:\n             self.stats.inc_value(\"scheduler/enqueued/disk\", spider=self.spider)\n         else:\n@@ -259,7 +262,8 @@ class Scheduler(BaseScheduler):\n         Increment the appropriate stats, such as: ``scheduler/dequeued``,\n         ``scheduler/dequeued/disk``, ``scheduler/dequeued/memory``.\n         \"\"\"\n-        request = self.mqs.pop()\n+        request: Optional[Request] = self.mqs.pop()\n+        assert self.stats is not None\n         if request is not None:\n             self.stats.inc_value(\"scheduler/dequeued/memory\", spider=self.spider)\n         else:\n@@ -295,6 +299,7 @@ class Scheduler(BaseScheduler):\n                     extra={\"spider\": self.spider},\n                 )\n                 self.logunser = False\n+            assert self.stats is not None\n             self.stats.inc_value(\"scheduler/unserializable\", spider=self.spider)\n             return False\n         else:\n@@ -351,7 +356,7 @@ class Scheduler(BaseScheduler):\n         if not path.exists():\n             return []\n         with path.open(encoding=\"utf-8\") as f:\n-            return json.load(f)\n+            return cast(list, json.load(f))\n \n     def _write_dqs_state(self, dqdir: str, state: list) -> None:\n         with Path(dqdir, \"active.json\").open(\"w\", encoding=\"utf-8\") as f:\n\n@@ -15,6 +15,7 @@ from typing import (\n     Optional,\n     Set,\n     Tuple,\n+    Type,\n     Union,\n )\n \n@@ -26,6 +27,9 @@ from scrapy import Spider, signals\n from scrapy.core.spidermw import SpiderMiddlewareManager\n from scrapy.exceptions import CloseSpider, DropItem, IgnoreRequest\n from scrapy.http import Request, Response\n+from scrapy.logformatter import LogFormatter\n+from scrapy.pipelines import ItemPipelineManager\n+from scrapy.signalmanager import SignalManager\n from scrapy.utils.defer import (\n     aiter_errback,\n     defer_fail,\n@@ -96,16 +100,20 @@ class Slot:\n class Scraper:\n     def __init__(self, crawler: Crawler) -> None:\n         self.slot: Optional[Slot] = None\n-        self.spidermw = SpiderMiddlewareManager.from_crawler(crawler)\n-        itemproc_cls = load_object(crawler.settings[\"ITEM_PROCESSOR\"])\n-        self.itemproc = itemproc_cls.from_crawler(crawler)\n-        self.concurrent_items = crawler.settings.getint(\"CONCURRENT_ITEMS\")\n-        self.crawler = crawler\n-        self.signals = crawler.signals\n-        self.logformatter = crawler.logformatter\n+        self.spidermw: SpiderMiddlewareManager = SpiderMiddlewareManager.from_crawler(\n+            crawler\n+        )\n+        itemproc_cls: Type[ItemPipelineManager] = load_object(\n+            crawler.settings[\"ITEM_PROCESSOR\"]\n+        )\n+        self.itemproc: ItemPipelineManager = itemproc_cls.from_crawler(crawler)\n+        self.concurrent_items: int = crawler.settings.getint(\"CONCURRENT_ITEMS\")\n+        self.crawler: Crawler = crawler\n+        self.signals: SignalManager = crawler.signals\n+        self.logformatter: LogFormatter = crawler.logformatter\n \n     @inlineCallbacks\n-    def open_spider(self, spider: Spider):\n+    def open_spider(self, spider: Spider) -> Generator[Deferred, Any, None]:\n         \"\"\"Open the given spider for scraping and allocate resources for it\"\"\"\n         self.slot = Slot(self.crawler.settings.getint(\"SCRAPER_SLOT_MAX_ACTIVE_SIZE\"))\n         yield self.itemproc.open_spider(spider)\n@@ -135,7 +143,8 @@ class Scraper:\n             raise RuntimeError(\"Scraper slot not assigned\")\n         dfd = self.slot.add_response_request(result, request)\n \n-        def finish_scraping(_):\n+        def finish_scraping(_: Any) -> Any:\n+            assert self.slot is not None\n             self.slot.finish_response(result, request)\n             self._check_if_closing(spider)\n             self._scrape_next(spider)\n@@ -205,9 +214,9 @@ class Scraper:\n         else:  # result is a Failure\n             # TODO: properly type adding this attribute to a Failure\n             result.request = request  # type: ignore[attr-defined]\n-            warn_on_generator_with_return_value(spider, request.errback)\n             dfd = defer_fail(result)\n             if request.errback:\n+                warn_on_generator_with_return_value(spider, request.errback)\n                 dfd.addErrback(request.errback)\n         return dfd.addCallback(iterate_spider_output)\n \n@@ -338,7 +347,7 @@ class Scraper:\n \n     def _itemproc_finished(\n         self, output: Any, item: Any, response: Response, spider: Spider\n-    ) -> None:\n+    ) -> Deferred:\n         \"\"\"ItemProcessor finished for the given ``item`` and returned ``output``\"\"\"\n         assert self.slot is not None  # typing\n         self.slot.itemproc_size -= 1\n\n@@ -13,6 +13,8 @@ from typing import (\n     Callable,\n     Generator,\n     Iterable,\n+    List,\n+    Optional,\n     Tuple,\n     Union,\n     cast,\n@@ -25,6 +27,7 @@ from scrapy import Request, Spider\n from scrapy.exceptions import _InvalidOutput\n from scrapy.http import Response\n from scrapy.middleware import MiddlewareManager\n+from scrapy.settings import BaseSettings\n from scrapy.utils.asyncgen import as_async_generator, collect_asyncgen\n from scrapy.utils.conf import build_component_list\n from scrapy.utils.defer import (\n@@ -41,22 +44,22 @@ logger = logging.getLogger(__name__)\n ScrapeFunc = Callable[[Union[Response, Failure], Request, Spider], Any]\n \n \n-def _isiterable(o) -> bool:\n+def _isiterable(o: Any) -> bool:\n     return isinstance(o, (Iterable, AsyncIterable))\n \n \n class SpiderMiddlewareManager(MiddlewareManager):\n     component_name = \"spider middleware\"\n \n-    def __init__(self, *middlewares):\n+    def __init__(self, *middlewares: Any):\n         super().__init__(*middlewares)\n         self.downgrade_warning_done = False\n \n     @classmethod\n-    def _get_mwlist_from_settings(cls, settings):\n+    def _get_mwlist_from_settings(cls, settings: BaseSettings) -> List[Any]:\n         return build_component_list(settings.getwithbase(\"SPIDER_MIDDLEWARES\"))\n \n-    def _add_middleware(self, mw):\n+    def _add_middleware(self, mw: Any) -> None:\n         super()._add_middleware(mw)\n         if hasattr(mw, \"process_spider_input\"):\n             self.methods[\"process_spider_input\"].append(mw.process_spider_input)\n@@ -98,7 +101,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         exception_processor_index: int,\n         recover_to: Union[MutableChain, MutableAsyncChain],\n     ) -> Union[Generator, AsyncGenerator]:\n-        def process_sync(iterable: Iterable):\n+        def process_sync(iterable: Iterable) -> Generator:\n             try:\n                 for r in iterable:\n                     yield r\n@@ -110,7 +113,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                     raise\n                 recover_to.extend(exception_result)\n \n-        async def process_async(iterable: AsyncIterable):\n+        async def process_async(iterable: AsyncIterable) -> AsyncGenerator:\n             try:\n                 async for r in iterable:\n                     yield r\n@@ -280,7 +283,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         if isinstance(recovered, AsyncIterable):\n             recovered_collected = await collect_asyncgen(recovered)\n             recovered = MutableChain(recovered_collected)\n-        return MutableChain(result, recovered)  # type: ignore[arg-type]\n+        return MutableChain(result, recovered)\n \n     def scrape_response(\n         self,\n@@ -306,7 +309,9 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         )\n         return dfd\n \n-    def process_start_requests(self, start_requests, spider: Spider) -> Deferred:\n+    def process_start_requests(\n+        self, start_requests: Iterable[Request], spider: Spider\n+    ) -> Deferred:\n         return self._process_chain(\"process_start_requests\", start_requests, spider)\n \n     # This method is only needed until _async compatibility methods are removed.\n@@ -314,9 +319,9 @@ class SpiderMiddlewareManager(MiddlewareManager):\n     def _get_async_method_pair(\n         mw: Any, methodname: str\n     ) -> Union[None, Callable, Tuple[Callable, Callable]]:\n-        normal_method = getattr(mw, methodname, None)\n+        normal_method: Optional[Callable] = getattr(mw, methodname, None)\n         methodname_async = methodname + \"_async\"\n-        async_method = getattr(mw, methodname_async, None)\n+        async_method: Optional[Callable] = getattr(mw, methodname_async, None)\n         if not async_method:\n             return normal_method\n         if not normal_method:\n\n@@ -4,7 +4,7 @@ import logging\n import pprint\n import signal\n import warnings\n-from typing import TYPE_CHECKING, Optional\n+from typing import TYPE_CHECKING, Optional, Type, Union\n \n from twisted.internet import defer\n from zope.interface.exceptions import DoesNotImplement\n@@ -22,8 +22,10 @@ from scrapy.core.engine import ExecutionEngine\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.extension import ExtensionManager\n from scrapy.interfaces import ISpiderLoader\n+from scrapy.logformatter import LogFormatter\n from scrapy.settings import Settings, overridden_settings\n from scrapy.signalmanager import SignalManager\n+from scrapy.statscollectors import StatsCollector\n from scrapy.utils.log import (\n     LogCounterHandler,\n     configure_logging,\n@@ -49,20 +51,25 @@ logger = logging.getLogger(__name__)\n \n \n class Crawler:\n-    def __init__(self, spidercls, settings=None, init_reactor: bool = False):\n+    def __init__(\n+        self,\n+        spidercls: Type[Spider],\n+        settings: Union[None, dict, Settings] = None,\n+        init_reactor: bool = False,\n+    ):\n         if isinstance(spidercls, Spider):\n             raise ValueError(\"The spidercls argument must be a class, not an object\")\n \n         if isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n \n-        self.spidercls = spidercls\n-        self.settings = settings.copy()\n+        self.spidercls: Type[Spider] = spidercls\n+        self.settings: Settings = settings.copy()\n         self.spidercls.update_settings(self.settings)\n \n-        self.signals = SignalManager(self)\n+        self.signals: SignalManager = SignalManager(self)\n \n-        self.stats = load_object(self.settings[\"STATS_CLASS\"])(self)\n+        self.stats: StatsCollector = load_object(self.settings[\"STATS_CLASS\"])(self)\n \n         handler = LogCounterHandler(self, level=self.settings.get(\"LOG_LEVEL\"))\n         logging.root.addHandler(handler)\n@@ -80,8 +87,8 @@ class Crawler:\n         self.__remove_handler = lambda: logging.root.removeHandler(handler)\n         self.signals.connect(self.__remove_handler, signals.engine_stopped)\n \n-        lf_cls = load_object(self.settings[\"LOG_FORMATTER\"])\n-        self.logformatter = lf_cls.from_crawler(self)\n+        lf_cls: Type[LogFormatter] = load_object(self.settings[\"LOG_FORMATTER\"])\n+        self.logformatter: LogFormatter = lf_cls.from_crawler(self)\n \n         self.request_fingerprinter: RequestFingerprinter = create_instance(\n             load_object(self.settings[\"REQUEST_FINGERPRINTER_CLASS\"]),\n@@ -89,8 +96,8 @@ class Crawler:\n             crawler=self,\n         )\n \n-        reactor_class = self.settings[\"TWISTED_REACTOR\"]\n-        event_loop = self.settings[\"ASYNCIO_EVENT_LOOP\"]\n+        reactor_class: str = self.settings[\"TWISTED_REACTOR\"]\n+        event_loop: str = self.settings[\"ASYNCIO_EVENT_LOOP\"]\n         if init_reactor:\n             # this needs to be done after the spider settings are merged,\n             # but before something imports twisted.internet.reactor\n@@ -104,11 +111,11 @@ class Crawler:\n             if is_asyncio_reactor_installed() and event_loop:\n                 verify_installed_asyncio_event_loop(event_loop)\n \n-        self.extensions = ExtensionManager.from_crawler(self)\n+        self.extensions: ExtensionManager = ExtensionManager.from_crawler(self)\n \n         self.settings.freeze()\n-        self.crawling = False\n-        self.spider = None\n+        self.crawling: bool = False\n+        self.spider: Optional[Spider] = None\n         self.engine: Optional[ExecutionEngine] = None\n \n     @defer.inlineCallbacks\n\n@@ -39,7 +39,7 @@ class DontCloseSpider(Exception):\n class CloseSpider(Exception):\n     \"\"\"Raise this from callbacks to request the spider to be closed\"\"\"\n \n-    def __init__(self, reason=\"cancelled\"):\n+    def __init__(self, reason: str = \"cancelled\"):\n         super().__init__()\n         self.reason = reason\n \n\n@@ -1,8 +1,11 @@\n import logging\n import os\n+from typing import Any, Dict, Optional, Union\n \n from twisted.python.failure import Failure\n \n+from scrapy import Request, Spider\n+from scrapy.http import Response\n from scrapy.utils.request import referer_str\n \n SCRAPEDMSG = \"Scraped from %(src)s\" + os.linesep + \"%(item)s\"\n@@ -52,7 +55,7 @@ class LogFormatter:\n                     }\n     \"\"\"\n \n-    def crawled(self, request, response, spider):\n+    def crawled(self, request: Request, response: Response, spider: Spider) -> dict:\n         \"\"\"Logs a message when the crawler finds a webpage.\"\"\"\n         request_flags = f\" {str(request.flags)}\" if request.flags else \"\"\n         response_flags = f\" {str(response.flags)}\" if response.flags else \"\"\n@@ -70,8 +73,11 @@ class LogFormatter:\n             },\n         }\n \n-    def scraped(self, item, response, spider):\n+    def scraped(\n+        self, item: Any, response: Union[Response, Failure], spider: Spider\n+    ) -> dict:\n         \"\"\"Logs a message when an item is scraped by a spider.\"\"\"\n+        src: Any\n         if isinstance(response, Failure):\n             src = response.getErrorMessage()\n         else:\n@@ -85,7 +91,9 @@ class LogFormatter:\n             },\n         }\n \n-    def dropped(self, item, exception, response, spider):\n+    def dropped(\n+        self, item: Any, exception: BaseException, response: Response, spider: Spider\n+    ) -> dict:\n         \"\"\"Logs a message when an item is dropped while it is passing through the item pipeline.\"\"\"\n         return {\n             \"level\": logging.WARNING,\n@@ -96,7 +104,9 @@ class LogFormatter:\n             },\n         }\n \n-    def item_error(self, item, exception, response, spider):\n+    def item_error(\n+        self, item: Any, exception, response: Response, spider: Spider\n+    ) -> dict:\n         \"\"\"Logs a message when an item causes an error while it is passing\n         through the item pipeline.\n \n@@ -110,7 +120,9 @@ class LogFormatter:\n             },\n         }\n \n-    def spider_error(self, failure, request, response, spider):\n+    def spider_error(\n+        self, failure: Failure, request: Request, response: Response, spider: Spider\n+    ) -> dict:\n         \"\"\"Logs an error message from a spider.\n \n         .. versionadded:: 2.0\n@@ -124,13 +136,19 @@ class LogFormatter:\n             },\n         }\n \n-    def download_error(self, failure, request, spider, errmsg=None):\n+    def download_error(\n+        self,\n+        failure: Failure,\n+        request: Request,\n+        spider: Spider,\n+        errmsg: Optional[str] = None,\n+    ) -> dict:\n         \"\"\"Logs a download error message from a spider (typically coming from\n         the engine).\n \n         .. versionadded:: 2.0\n         \"\"\"\n-        args = {\"request\": request}\n+        args: Dict[str, Any] = {\"request\": request}\n         if errmsg:\n             msg = DOWNLOADERRORMSG_LONG\n             args[\"errmsg\"] = errmsg\n\n@@ -1,7 +1,7 @@\n import logging\n import pprint\n from collections import defaultdict, deque\n-from typing import Any, Callable, Deque, Dict, Iterable, Tuple, Union, cast\n+from typing import Any, Callable, Deque, Dict, Iterable, List, Tuple, Union, cast\n \n from twisted.internet.defer import Deferred\n \n@@ -30,7 +30,7 @@ class MiddlewareManager:\n             self._add_middleware(mw)\n \n     @classmethod\n-    def _get_mwlist_from_settings(cls, settings: Settings) -> list:\n+    def _get_mwlist_from_settings(cls, settings: Settings) -> List[Any]:\n         raise NotImplementedError\n \n     @classmethod\n@@ -67,17 +67,17 @@ class MiddlewareManager:\n     def from_crawler(cls, crawler):\n         return cls.from_settings(crawler.settings, crawler)\n \n-    def _add_middleware(self, mw) -> None:\n+    def _add_middleware(self, mw: Any) -> None:\n         if hasattr(mw, \"open_spider\"):\n             self.methods[\"open_spider\"].append(mw.open_spider)\n         if hasattr(mw, \"close_spider\"):\n             self.methods[\"close_spider\"].appendleft(mw.close_spider)\n \n-    def _process_parallel(self, methodname: str, obj, *args) -> Deferred:\n+    def _process_parallel(self, methodname: str, obj: Any, *args: Any) -> Deferred:\n         methods = cast(Iterable[Callable], self.methods[methodname])\n         return process_parallel(methods, obj, *args)\n \n-    def _process_chain(self, methodname: str, obj, *args) -> Deferred:\n+    def _process_chain(self, methodname: str, obj: Any, *args: Any) -> Deferred:\n         methods = cast(Iterable[Callable], self.methods[methodname])\n         return process_chain(methods, obj, *args)\n \n\n@@ -3,7 +3,11 @@ Item pipeline\n \n See documentation in docs/item-pipeline.rst\n \"\"\"\n+from typing import Any, List\n \n+from twisted.internet.defer import Deferred\n+\n+from scrapy import Spider\n from scrapy.middleware import MiddlewareManager\n from scrapy.utils.conf import build_component_list\n from scrapy.utils.defer import deferred_f_from_coro_f\n@@ -13,15 +17,15 @@ class ItemPipelineManager(MiddlewareManager):\n     component_name = \"item pipeline\"\n \n     @classmethod\n-    def _get_mwlist_from_settings(cls, settings):\n+    def _get_mwlist_from_settings(cls, settings) -> List[Any]:\n         return build_component_list(settings.getwithbase(\"ITEM_PIPELINES\"))\n \n-    def _add_middleware(self, pipe):\n+    def _add_middleware(self, pipe: Any) -> None:\n         super()._add_middleware(pipe)\n         if hasattr(pipe, \"process_item\"):\n             self.methods[\"process_item\"].append(\n                 deferred_f_from_coro_f(pipe.process_item)\n             )\n \n-    def process_item(self, item, spider):\n+    def process_item(self, item: Any, spider: Spider) -> Deferred:\n         return self._process_chain(\"process_item\", item, spider)\n\n@@ -1,13 +1,16 @@\n+from typing import Any, List, Tuple\n+\n from pydispatch import dispatcher\n+from twisted.internet.defer import Deferred\n \n from scrapy.utils import signal as _signal\n \n \n class SignalManager:\n-    def __init__(self, sender=dispatcher.Anonymous):\n-        self.sender = sender\n+    def __init__(self, sender: Any = dispatcher.Anonymous):\n+        self.sender: Any = sender\n \n-    def connect(self, receiver, signal, **kwargs):\n+    def connect(self, receiver: Any, signal: Any, **kwargs: Any) -> None:\n         \"\"\"\n         Connect a receiver function to a signal.\n \n@@ -22,18 +25,18 @@ class SignalManager:\n         :type signal: object\n         \"\"\"\n         kwargs.setdefault(\"sender\", self.sender)\n-        return dispatcher.connect(receiver, signal, **kwargs)\n+        dispatcher.connect(receiver, signal, **kwargs)\n \n-    def disconnect(self, receiver, signal, **kwargs):\n+    def disconnect(self, receiver: Any, signal: Any, **kwargs: Any) -> None:\n         \"\"\"\n         Disconnect a receiver function from a signal. This has the\n         opposite effect of the :meth:`connect` method, and the arguments\n         are the same.\n         \"\"\"\n         kwargs.setdefault(\"sender\", self.sender)\n-        return dispatcher.disconnect(receiver, signal, **kwargs)\n+        dispatcher.disconnect(receiver, signal, **kwargs)\n \n-    def send_catch_log(self, signal, **kwargs):\n+    def send_catch_log(self, signal: Any, **kwargs: Any) -> List[Tuple[Any, Any]]:\n         \"\"\"\n         Send a signal, catch exceptions and log them.\n \n@@ -43,7 +46,7 @@ class SignalManager:\n         kwargs.setdefault(\"sender\", self.sender)\n         return _signal.send_catch_log(signal, **kwargs)\n \n-    def send_catch_log_deferred(self, signal, **kwargs):\n+    def send_catch_log_deferred(self, signal: Any, **kwargs: Any) -> Deferred:\n         \"\"\"\n         Like :meth:`send_catch_log` but supports returning\n         :class:`~twisted.internet.defer.Deferred` objects from signal handlers.\n@@ -57,7 +60,7 @@ class SignalManager:\n         kwargs.setdefault(\"sender\", self.sender)\n         return _signal.send_catch_log_deferred(signal, **kwargs)\n \n-    def disconnect_all(self, signal, **kwargs):\n+    def disconnect_all(self, signal: Any, **kwargs: Any) -> None:\n         \"\"\"\n         Disconnect all receivers from the given signal.\n \n@@ -65,4 +68,4 @@ class SignalManager:\n         :type signal: object\n         \"\"\"\n         kwargs.setdefault(\"sender\", self.sender)\n-        return _signal.disconnect_all(signal, **kwargs)\n+        _signal.disconnect_all(signal, **kwargs)\n\n@@ -3,44 +3,57 @@ Scrapy extension for collecting scraping stats\n \"\"\"\n import logging\n import pprint\n+from typing import TYPE_CHECKING, Any, Dict, Optional\n+\n+from scrapy import Spider\n+\n+if TYPE_CHECKING:\n+    from scrapy.crawler import Crawler\n \n logger = logging.getLogger(__name__)\n \n \n-class StatsCollector:\n-    def __init__(self, crawler):\n-        self._dump = crawler.settings.getbool(\"STATS_DUMP\")\n-        self._stats = {}\n+StatsT = Dict[str, Any]\n \n-    def get_value(self, key, default=None, spider=None):\n+\n+class StatsCollector:\n+    def __init__(self, crawler: \"Crawler\"):\n+        self._dump: bool = crawler.settings.getbool(\"STATS_DUMP\")\n+        self._stats: StatsT = {}\n+\n+    def get_value(\n+        self, key: str, default: Any = None, spider: Optional[Spider] = None\n+    ) -> Any:\n         return self._stats.get(key, default)\n \n-    def get_stats(self, spider=None):\n+    def get_stats(self, spider: Optional[Spider] = None) -> StatsT:\n         return self._stats\n \n-    def set_value(self, key, value, spider=None):\n+    def set_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:\n         self._stats[key] = value\n \n-    def set_stats(self, stats, spider=None):\n+    def set_stats(self, stats: StatsT, spider: Optional[Spider] = None) -> None:\n         self._stats = stats\n \n-    def inc_value(self, key, count=1, start=0, spider=None):\n+    def inc_value(\n+        self, key: str, count: int = 1, start: int = 0, spider: Optional[Spider] = None\n+    ) -> None:\n         d = self._stats\n         d[key] = d.setdefault(key, start) + count\n \n-    def max_value(self, key, value, spider=None):\n+    def max_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:\n         self._stats[key] = max(self._stats.setdefault(key, value), value)\n \n-    def min_value(self, key, value, spider=None):\n+    def min_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:\n         self._stats[key] = min(self._stats.setdefault(key, value), value)\n \n-    def clear_stats(self, spider=None):\n+    def clear_stats(self, spider: Optional[Spider] = None) -> None:\n         self._stats.clear()\n \n-    def open_spider(self, spider):\n+    def open_spider(self, spider: Spider) -> None:\n         pass\n \n-    def close_spider(self, spider, reason):\n+    def close_spider(self, spider: Spider, reason: str) -> None:\n         if self._dump:\n             logger.info(\n                 \"Dumping Scrapy stats:\\n\" + pprint.pformat(self._stats),\n@@ -48,34 +61,38 @@ class StatsCollector:\n             )\n         self._persist_stats(self._stats, spider)\n \n-    def _persist_stats(self, stats, spider):\n+    def _persist_stats(self, stats: StatsT, spider: Spider) -> None:\n         pass\n \n \n class MemoryStatsCollector(StatsCollector):\n-    def __init__(self, crawler):\n+    def __init__(self, crawler: \"Crawler\"):\n         super().__init__(crawler)\n-        self.spider_stats = {}\n+        self.spider_stats: Dict[str, StatsT] = {}\n \n-    def _persist_stats(self, stats, spider):\n+    def _persist_stats(self, stats: StatsT, spider: Spider) -> None:\n         self.spider_stats[spider.name] = stats\n \n \n class DummyStatsCollector(StatsCollector):\n-    def get_value(self, key, default=None, spider=None):\n+    def get_value(\n+        self, key: str, default: Any = None, spider: Optional[Spider] = None\n+    ) -> Any:\n         return default\n \n-    def set_value(self, key, value, spider=None):\n+    def set_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:\n         pass\n \n-    def set_stats(self, stats, spider=None):\n+    def set_stats(self, stats: StatsT, spider: Optional[Spider] = None) -> None:\n         pass\n \n-    def inc_value(self, key, count=1, start=0, spider=None):\n+    def inc_value(\n+        self, key: str, count: int = 1, start: int = 0, spider: Optional[Spider] = None\n+    ) -> None:\n         pass\n \n-    def max_value(self, key, value, spider=None):\n+    def max_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:\n         pass\n \n-    def min_value(self, key, value, spider=None):\n+    def min_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:\n         pass\n\n@@ -2,6 +2,7 @@ import logging\n import sys\n import warnings\n from logging.config import dictConfig\n+from typing import Tuple\n \n from twisted.python import log as twisted_log\n from twisted.python.failure import Failure\n@@ -14,7 +15,7 @@ from scrapy.utils.versions import scrapy_components_versions\n logger = logging.getLogger(__name__)\n \n \n-def failure_to_exc_info(failure):\n+def failure_to_exc_info(failure: Failure):\n     \"\"\"Extract exc_info from Failure instances\"\"\"\n     if isinstance(failure, Failure):\n         return (failure.type, failure.value, failure.getTracebackObject())\n@@ -206,7 +207,7 @@ class LogCounterHandler(logging.Handler):\n         self.crawler.stats.inc_value(sname)\n \n \n-def logformatter_adapter(logkws):\n+def logformatter_adapter(logkws: dict) -> Tuple[int, str, dict]:\n     \"\"\"\n     Helper that takes the dictionary output from the methods in LogFormatter\n     and adapts it into a tuple of positional arguments for logger.log calls,\n\n@@ -10,6 +10,7 @@ from contextlib import contextmanager\n from functools import partial\n from importlib import import_module\n from pkgutil import iter_modules\n+from typing import TYPE_CHECKING, Any, Callable, Union\n \n from w3lib.html import replace_entities\n \n@@ -18,6 +19,10 @@ from scrapy.utils.datatypes import LocalWeakReferencedCache\n from scrapy.utils.deprecate import ScrapyDeprecationWarning\n from scrapy.utils.python import flatten, to_unicode\n \n+if TYPE_CHECKING:\n+    from scrapy import Spider\n+\n+\n _ITERABLE_SINGLE_VALUES = dict, Item, str, bytes\n \n \n@@ -34,7 +39,7 @@ def arg_to_iter(arg):\n     return [arg]\n \n \n-def load_object(path):\n+def load_object(path: Union[str, Callable]) -> Any:\n     \"\"\"Load an object given its absolute object path, and return it.\n \n     The object can be the import path of a class, function, variable or an\n@@ -249,7 +254,7 @@ def is_generator_with_return_value(callable):\n     return _generator_callbacks_cache[callable]\n \n \n-def warn_on_generator_with_return_value(spider, callable):\n+def warn_on_generator_with_return_value(spider: \"Spider\", callable: Callable) -> None:\n     \"\"\"\n     Logs a warning if a callable is a generator function and includes\n     a 'return' statement with a value different than None\n\n@@ -1,9 +1,11 @@\n import asyncio\n import sys\n from contextlib import suppress\n+from typing import Any, Callable, Dict, Optional, Sequence\n from warnings import catch_warnings, filterwarnings, warn\n \n from twisted.internet import asyncioreactor, error\n+from twisted.internet.base import DelayedCall\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.misc import load_object\n@@ -34,23 +36,23 @@ class CallLaterOnce:\n     it hasn't been already scheduled since the last time it ran.\n     \"\"\"\n \n-    def __init__(self, func, *a, **kw):\n-        self._func = func\n-        self._a = a\n-        self._kw = kw\n-        self._call = None\n+    def __init__(self, func: Callable, *a: Any, **kw: Any):\n+        self._func: Callable = func\n+        self._a: Sequence[Any] = a\n+        self._kw: Dict[str, Any] = kw\n+        self._call: Optional[DelayedCall] = None\n \n-    def schedule(self, delay=0):\n+    def schedule(self, delay: float = 0) -> None:\n         from twisted.internet import reactor\n \n         if self._call is None:\n             self._call = reactor.callLater(delay, self)\n \n-    def cancel(self):\n+    def cancel(self) -> None:\n         if self._call:\n             self._call.cancel()\n \n-    def __call__(self):\n+    def __call__(self) -> Any:\n         self._call = None\n         return self._func(*self._a, **self._kw)\n \n\n@@ -1,6 +1,8 @@\n \"\"\"Helper functions for working with signals\"\"\"\n import collections.abc\n import logging\n+from typing import Any as TypingAny\n+from typing import List, Tuple\n \n from pydispatch.dispatcher import (\n     Anonymous,\n@@ -20,7 +22,9 @@ from scrapy.utils.log import failure_to_exc_info\n logger = logging.getLogger(__name__)\n \n \n-def send_catch_log(signal=Any, sender=Anonymous, *arguments, **named):\n+def send_catch_log(\n+    signal=Any, sender=Anonymous, *arguments, **named\n+) -> List[Tuple[TypingAny, TypingAny]]:\n     \"\"\"Like pydispatcher.robust.sendRobust but it also logs errors and returns\n     Failures instead of exceptions.\n     \"\"\"\n@@ -32,8 +36,9 @@ def send_catch_log(signal=Any, sender=Anonymous, *arguments, **named):\n     )\n     dont_log += (StopDownload,)\n     spider = named.get(\"spider\", None)\n-    responses = []\n+    responses: List[Tuple[TypingAny, TypingAny]] = []\n     for receiver in liveReceivers(getAllReceivers(sender, signal)):\n+        result: TypingAny\n         try:\n             response = robustApply(\n                 receiver, signal=signal, sender=sender, *arguments, **named\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
