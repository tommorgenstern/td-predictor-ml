{"custom_id": "keras#04c593ea76c6d68de554e6fc470fc9744cabe233", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1029 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 20 | Complexity Δ (Sum/Max): 26/16 | Churn Δ: 1029 | Churn Cumulative: 1029 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,591 @@\n+\"\"\"\n+Title: Semantic Image Clustering\n+Author: [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)\n+Date created: 2021/02/28\n+Last modified: 2021/02/28\n+Description: Semantic Clustering by Adopting Nearest neighbors (SCAN) algorithm.\n+Accelerator: GPU\n+\"\"\"\n+\n+\"\"\"\n+## Introduction\n+\n+This example demonstrates how to apply the [Semantic Clustering by Adopting Nearest neighbors\n+(SCAN)](https://arxiv.org/abs/2005.12320) algorithm (Van Gansbeke et al., 2020) on the\n+[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset. The algorithm consists of\n+two phases:\n+\n+1. Self-supervised visual representation learning of images, in which we use the\n+[simCLR](https://arxiv.org/abs/2002.05709) technique.\n+2. Clustering of the learned visual representation vectors to maximize the agreement\n+between the cluster assignments of neighboring vectors.\n+\"\"\"\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import os\n+\n+os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n+\n+from collections import defaultdict\n+import numpy as np\n+import tensorflow as tf\n+import keras\n+from keras import layers\n+import matplotlib.pyplot as plt\n+from tqdm import tqdm\n+\n+\"\"\"\n+## Prepare the data\n+\"\"\"\n+\n+num_classes = 10\n+input_shape = (32, 32, 3)\n+\n+(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n+x_data = np.concatenate([x_train, x_test])\n+y_data = np.concatenate([y_train, y_test])\n+\n+print(\"x_data shape:\", x_data.shape, \"- y_data shape:\", y_data.shape)\n+\n+classes = [\n+    \"airplane\",\n+    \"automobile\",\n+    \"bird\",\n+    \"cat\",\n+    \"deer\",\n+    \"dog\",\n+    \"frog\",\n+    \"horse\",\n+    \"ship\",\n+    \"truck\",\n+]\n+\n+\"\"\"\n+## Define hyperparameters\n+\"\"\"\n+\n+target_size = 32  # Resize the input images.\n+representation_dim = 512  # The dimensions of the features vector.\n+projection_units = 128  # The projection head of the representation learner.\n+num_clusters = 20  # Number of clusters.\n+k_neighbours = 5  # Number of neighbours to consider during cluster learning.\n+tune_encoder_during_clustering = False  # Freeze the encoder in the cluster learning.\n+\n+\"\"\"\n+## Implement data preprocessing\n+\n+The data preprocessing step resizes the input images to the desired `target_size` and applies\n+feature-wise normalization. Note that, when using `keras.applications.ResNet50V2` as the\n+visual encoder, resizing the images into 255 x 255 inputs would lead to more accurate results\n+but require a longer time to train.\n+\"\"\"\n+\n+data_preprocessing = keras.Sequential(\n+    [\n+        layers.Resizing(target_size, target_size),\n+        layers.Normalization(),\n+    ]\n+)\n+# Compute the mean and the variance from the data for normalization.\n+data_preprocessing.layers[-1].adapt(x_data)\n+\n+\"\"\"\n+## Data augmentation\n+\n+Unlike simCLR, which randomly picks a single data augmentation function to apply to an input\n+image, we apply a set of data augmentation functions randomly to the input image.\n+(You can experiment with other image augmentation techniques by following\n+the [data augmentation tutorial](https://www.tensorflow.org/tutorials/images/data_augmentation).)\n+\"\"\"\n+\n+data_augmentation = keras.Sequential(\n+    [\n+        layers.RandomTranslation(\n+            height_factor=(-0.2, 0.2), width_factor=(-0.2, 0.2), fill_mode=\"nearest\"\n+        ),\n+        layers.RandomFlip(mode=\"horizontal\"),\n+        layers.RandomRotation(factor=0.15, fill_mode=\"nearest\"),\n+        layers.RandomZoom(\n+            height_factor=(-0.3, 0.1), width_factor=(-0.3, 0.1), fill_mode=\"nearest\"\n+        ),\n+    ]\n+)\n+\n+\"\"\"\n+Display a random image\n+\"\"\"\n+\n+image_idx = np.random.choice(range(x_data.shape[0]))\n+image = x_data[image_idx]\n+image_class = classes[y_data[image_idx][0]]\n+plt.figure(figsize=(3, 3))\n+plt.imshow(x_data[image_idx].astype(\"uint8\"))\n+plt.title(image_class)\n+_ = plt.axis(\"off\")\n+\n+\"\"\"\n+Display a sample of augmented versions of the image\n+\"\"\"\n+\n+plt.figure(figsize=(10, 10))\n+for i in range(9):\n+    augmented_images = data_augmentation(np.array([image]))\n+    ax = plt.subplot(3, 3, i + 1)\n+    plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n+    plt.axis(\"off\")\n+\n+\"\"\"\n+## Self-supervised representation learning\n+\"\"\"\n+\n+\"\"\"\n+### Implement the vision encoder\n+\"\"\"\n+\n+\n+def create_encoder(representation_dim):\n+    encoder = keras.Sequential(\n+        [\n+            keras.applications.ResNet50V2(\n+                include_top=False, weights=None, pooling=\"avg\"\n+            ),\n+            layers.Dense(representation_dim),\n+        ]\n+    )\n+    return encoder\n+\n+\n+\"\"\"\n+### Implement the unsupervised contrastive loss\n+\"\"\"\n+\n+\n+class RepresentationLearner(keras.Model):\n+    def __init__(\n+        self,\n+        encoder,\n+        projection_units,\n+        num_augmentations,\n+        temperature=1.0,\n+        dropout_rate=0.1,\n+        l2_normalize=False,\n+        **kwargs\n+    ):\n+        super().__init__(**kwargs)\n+        self.encoder = encoder\n+        # Create projection head.\n+        self.projector = keras.Sequential(\n+            [\n+                layers.Dropout(dropout_rate),\n+                layers.Dense(units=projection_units, use_bias=False),\n+                layers.BatchNormalization(),\n+                layers.ReLU(),\n+            ]\n+        )\n+        self.num_augmentations = num_augmentations\n+        self.temperature = temperature\n+        self.l2_normalize = l2_normalize\n+        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n+\n+    @property\n+    def metrics(self):\n+        return [self.loss_tracker]\n+\n+    def compute_contrastive_loss(self, feature_vectors, batch_size):\n+        num_augmentations = tf.shape(feature_vectors)[0] // batch_size\n+        if self.l2_normalize:\n+            feature_vectors = tf.math.l2_normalize(feature_vectors, -1)\n+        # The logits shape is [num_augmentations * batch_size, num_augmentations * batch_size].\n+        logits = (\n+            tf.linalg.matmul(feature_vectors, feature_vectors, transpose_b=True)\n+            / self.temperature\n+        )\n+        # Apply log-max trick for numerical stability.\n+        logits_max = tf.math.reduce_max(logits, axis=1)\n+        logits = logits - logits_max\n+        # The shape of targets is [num_augmentations * batch_size, num_augmentations * batch_size].\n+        # targets is a matrix consits of num_augmentations submatrices of shape [batch_size * batch_size].\n+        # Each [batch_size * batch_size] submatrix is an identity matrix (diagonal entries are ones).\n+        targets = tf.tile(tf.eye(batch_size), [num_augmentations, num_augmentations])\n+        # Compute cross entropy loss\n+        return keras.losses.categorical_crossentropy(\n+            y_true=targets, y_pred=logits, from_logits=True\n+        )\n+\n+    def call(self, inputs):\n+        # Preprocess the input images.\n+        preprocessed = data_preprocessing(inputs)\n+        # Create augmented versions of the images.\n+        augmented = []\n+        for _ in range(self.num_augmentations):\n+            augmented.append(data_augmentation(preprocessed))\n+        augmented = layers.Concatenate(axis=0)(augmented)\n+        # Generate embedding representations of the images.\n+        features = self.encoder(augmented)\n+        # Apply projection head.\n+        return self.projector(features)\n+\n+    def train_step(self, inputs):\n+        batch_size = tf.shape(inputs)[0]\n+        # Run the forward pass and compute the contrastive loss\n+        with tf.GradientTape() as tape:\n+            feature_vectors = self(inputs, training=True)\n+            loss = self.compute_contrastive_loss(feature_vectors, batch_size)\n+        # Compute gradients\n+        trainable_vars = self.trainable_variables\n+        gradients = tape.gradient(loss, trainable_vars)\n+        # Update weights\n+        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n+        # Update loss tracker metric\n+        self.loss_tracker.update_state(loss)\n+        # Return a dict mapping metric names to current value\n+        return {m.name: m.result() for m in self.metrics}\n+\n+    def test_step(self, inputs):\n+        batch_size = tf.shape(inputs)[0]\n+        feature_vectors = self(inputs, training=False)\n+        loss = self.compute_contrastive_loss(feature_vectors, batch_size)\n+        self.loss_tracker.update_state(loss)\n+        return {\"loss\": self.loss_tracker.result()}\n+\n+\n+\"\"\"\n+### Train the model\n+\"\"\"\n+# Create vision encoder.\n+encoder = create_encoder(representation_dim)\n+# Create representation learner.\n+representation_learner = RepresentationLearner(\n+    encoder, projection_units, num_augmentations=2, temperature=0.1\n+)\n+# Create a a Cosine decay learning rate scheduler.\n+lr_scheduler = keras.optimizers.schedules.CosineDecay(\n+    initial_learning_rate=0.001, decay_steps=500, alpha=0.1\n+)\n+# Compile the model.\n+representation_learner.compile(\n+    optimizer=keras.optimizers.AdamW(learning_rate=lr_scheduler, weight_decay=0.0001),\n+)\n+# Fit the model.\n+history = representation_learner.fit(\n+    x=x_data,\n+    batch_size=512,\n+    epochs=50,  # for better results, increase the number of epochs to 500.\n+)\n+\n+\n+\"\"\"\n+Plot training loss\n+\"\"\"\n+\n+plt.plot(history.history[\"loss\"])\n+plt.ylabel(\"loss\")\n+plt.xlabel(\"epoch\")\n+plt.show()\n+\n+\"\"\"\n+## Compute the nearest neighbors\n+\"\"\"\n+\n+\"\"\"\n+### Generate the embeddings for the images\n+\"\"\"\n+\n+batch_size = 500\n+# Get the feature vector representations of the images.\n+feature_vectors = encoder.predict(x_data, batch_size=batch_size, verbose=1)\n+# Normalize the feature vectores.\n+feature_vectors = tf.math.l2_normalize(feature_vectors, -1)\n+\n+\"\"\"\n+### Find the *k* nearest neighbours for each embedding\n+\"\"\"\n+\n+neighbours = []\n+num_batches = feature_vectors.shape[0] // batch_size\n+for batch_idx in tqdm(range(num_batches)):\n+    start_idx = batch_idx * batch_size\n+    end_idx = start_idx + batch_size\n+    current_batch = feature_vectors[start_idx:end_idx]\n+    # Compute the dot similarity.\n+    similarities = tf.linalg.matmul(current_batch, feature_vectors, transpose_b=True)\n+    # Get the indices of most similar vectors.\n+    _, indices = tf.math.top_k(similarities, k=k_neighbours + 1, sorted=True)\n+    # Add the indices to the neighbours.\n+    neighbours.append(indices[..., 1:])\n+\n+neighbours = np.reshape(np.array(neighbours), (-1, k_neighbours))\n+\n+\"\"\"\n+Let's display some neighbors on each row\n+\"\"\"\n+\n+nrows = 4\n+ncols = k_neighbours + 1\n+\n+plt.figure(figsize=(12, 12))\n+position = 1\n+for _ in range(nrows):\n+    anchor_idx = np.random.choice(range(x_data.shape[0]))\n+    neighbour_indicies = neighbours[anchor_idx]\n+    indices = [anchor_idx] + neighbour_indicies.tolist()\n+    for j in range(ncols):\n+        plt.subplot(nrows, ncols, position)\n+        plt.imshow(x_data[indices[j]].astype(\"uint8\"))\n+        plt.title(classes[y_data[indices[j]][0]])\n+        plt.axis(\"off\")\n+        position += 1\n+\n+\"\"\"\n+You notice that images on each row are visually similar, and belong to similar classes.\n+\"\"\"\n+\n+\"\"\"\n+## Semantic clustering with nearest neighbours\n+\"\"\"\n+\n+\"\"\"\n+### Implement clustering consistency loss\n+\n+This loss tries to make sure that neighbours have the same clustering assignments.\n+\"\"\"\n+\n+\n+class ClustersConsistencyLoss(keras.losses.Loss):\n+    def __init__(self):\n+        super().__init__()\n+\n+    def __call__(self, target, similarity, sample_weight=None):\n+        # Set targets to be ones.\n+        target = tf.ones_like(similarity)\n+        # Compute cross entropy loss.\n+        loss = keras.losses.binary_crossentropy(\n+            y_true=target, y_pred=similarity, from_logits=True\n+        )\n+        return tf.math.reduce_mean(loss)\n+\n+\n+\"\"\"\n+### Implement the clusters entropy loss\n+\n+This loss tries to make sure that cluster distribution is roughly uniformed, to avoid\n+assigning most of the instances to one cluster.\n+\"\"\"\n+\n+\n+class ClustersEntropyLoss(keras.losses.Loss):\n+    def __init__(self, entropy_loss_weight=1.0):\n+        super().__init__()\n+        self.entropy_loss_weight = entropy_loss_weight\n+\n+    def __call__(self, target, cluster_probabilities, sample_weight=None):\n+        # Ideal entropy = log(num_clusters).\n+        num_clusters = tf.cast(tf.shape(cluster_probabilities)[-1], tf.dtypes.float32)\n+        target = tf.math.log(num_clusters)\n+        # Compute the overall clusters distribution.\n+        cluster_probabilities = tf.math.reduce_mean(cluster_probabilities, axis=0)\n+        # Replacing zero probabilities - if any - with a very small value.\n+        cluster_probabilities = tf.clip_by_value(\n+            cluster_probabilities, clip_value_min=1e-8, clip_value_max=1.0\n+        )\n+        # Compute the entropy over the clusters.\n+        entropy = -tf.math.reduce_sum(\n+            cluster_probabilities * tf.math.log(cluster_probabilities)\n+        )\n+        # Compute the difference between the target and the actual.\n+        loss = target - entropy\n+        return loss\n+\n+\n+\"\"\"\n+### Implement clustering model\n+\n+This model takes a raw image as an input, generated its feature vector using the trained\n+encoder, and produces a probability distribution of the clusters given the feature vector\n+as the cluster assignments.\n+\"\"\"\n+\n+\n+def create_clustering_model(encoder, num_clusters, name=None):\n+    inputs = keras.Input(shape=input_shape)\n+    # Preprocess the input images.\n+    preprocessed = data_preprocessing(inputs)\n+    # Apply data augmentation to the images.\n+    augmented = data_augmentation(preprocessed)\n+    # Generate embedding representations of the images.\n+    features = encoder(augmented)\n+    # Assign the images to clusters.\n+    outputs = layers.Dense(units=num_clusters, activation=\"softmax\")(features)\n+    # Create the model.\n+    model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n+    return model\n+\n+\n+\"\"\"\n+### Implement clustering learner\n+\n+This model receives the input `anchor` image and its `neighbours`, produces the clusters\n+assignments for them using the `clustering_model`, and produces two outputs:\n+1. `similarity`: the similarity between the cluster assignments of the `anchor` image and\n+its `neighbours`. This output is fed to the `ClustersConsistencyLoss`.\n+2. `anchor_clustering`: cluster assignments of the `anchor` images. This is fed to the `ClustersEntropyLoss`.\n+\"\"\"\n+\n+\n+def create_clustering_learner(clustering_model):\n+    anchor = keras.Input(shape=input_shape, name=\"anchors\")\n+    neighbours = keras.Input(\n+        shape=tuple([k_neighbours]) + input_shape, name=\"neighbours\"\n+    )\n+    # Changes neighbours shape to [batch_size * k_neighbours, width, height, channels]\n+    neighbours_reshaped = tf.reshape(neighbours, shape=tuple([-1]) + input_shape)\n+    # anchor_clustering shape: [batch_size, num_clusters]\n+    anchor_clustering = clustering_model(anchor)\n+    # neighbours_clustering shape: [batch_size * k_neighbours, num_clusters]\n+    neighbours_clustering = clustering_model(neighbours_reshaped)\n+    # Convert neighbours_clustering shape to [batch_size, k_neighbours, num_clusters]\n+    neighbours_clustering = tf.reshape(\n+        neighbours_clustering,\n+        shape=(-1, k_neighbours, tf.shape(neighbours_clustering)[-1]),\n+    )\n+    # similarity shape: [batch_size, 1, k_neighbours]\n+    similarity = tf.linalg.einsum(\n+        \"bij,bkj->bik\", tf.expand_dims(anchor_clustering, axis=1), neighbours_clustering\n+    )\n+    # similarity shape:  [batch_size, k_neighbours]\n+    similarity = layers.Lambda(lambda x: tf.squeeze(x, axis=1), name=\"similarity\")(\n+        similarity\n+    )\n+    # Create the model.\n+    model = keras.Model(\n+        inputs=[anchor, neighbours],\n+        outputs=[similarity, anchor_clustering],\n+        name=\"clustering_learner\",\n+    )\n+    return model\n+\n+\n+\"\"\"\n+### Train model\n+\"\"\"\n+\n+# If tune_encoder_during_clustering is set to False,\n+# then freeze the encoder weights.\n+for layer in encoder.layers:\n+    layer.trainable = tune_encoder_during_clustering\n+# Create the clustering model and learner.\n+clustering_model = create_clustering_model(encoder, num_clusters, name=\"clustering\")\n+clustering_learner = create_clustering_learner(clustering_model)\n+# Instantiate the model losses.\n+losses = [ClustersConsistencyLoss(), ClustersEntropyLoss(entropy_loss_weight=5)]\n+# Create the model inputs and labels.\n+inputs = {\"anchors\": x_data, \"neighbours\": tf.gather(x_data, neighbours)}\n+labels = tf.ones(shape=(x_data.shape[0]))\n+# Compile the model.\n+clustering_learner.compile(\n+    optimizer=keras.optimizers.AdamW(learning_rate=0.0005, weight_decay=0.0001),\n+    loss=losses,\n+)\n+\n+# Begin training the model.\n+clustering_learner.fit(x=inputs, y=labels, batch_size=512, epochs=50)\n+\n+\"\"\"\n+Plot training loss\n+\"\"\"\n+\n+plt.plot(history.history[\"loss\"])\n+plt.ylabel(\"loss\")\n+plt.xlabel(\"epoch\")\n+plt.show()\n+\n+\"\"\"\n+## Cluster analysis\n+\"\"\"\n+\n+\"\"\"\n+### Assign images to clusters\n+\"\"\"\n+\n+# Get the cluster probability distribution of the input images.\n+clustering_probs = clustering_model.predict(x_data, batch_size=batch_size, verbose=1)\n+# Get the cluster of the highest probability.\n+cluster_assignments = tf.math.argmax(clustering_probs, axis=-1).numpy()\n+# Store the clustering confidence.\n+# Images with the highest clustering confidence are considered the 'prototypes'\n+# of the clusters.\n+cluster_confidence = tf.math.reduce_max(clustering_probs, axis=-1).numpy()\n+\n+\"\"\"\n+Let's compute the cluster sizes\n+\"\"\"\n+\n+clusters = defaultdict(list)\n+for idx, c in enumerate(cluster_assignments):\n+    clusters[c].append((idx, cluster_confidence[idx]))\n+\n+for c in range(num_clusters):\n+    print(\"cluster\", c, \":\", len(clusters[c]))\n+\n+\"\"\"\n+Notice that the clusters have roughly balanced sizes.\n+\"\"\"\n+\n+\"\"\"\n+### Visualize cluster images\n+\n+Display the *prototypes*—instances with the highest clustering confidence—of each cluster:\n+\"\"\"\n+\n+num_images = 8\n+plt.figure(figsize=(15, 15))\n+position = 1\n+for c in range(num_clusters):\n+    cluster_instances = sorted(clusters[c], key=lambda kv: kv[1], reverse=True)\n+\n+    for j in range(num_images):\n+        image_idx = cluster_instances[j][0]\n+        plt.subplot(num_clusters, num_images, position)\n+        plt.imshow(x_data[image_idx].astype(\"uint8\"))\n+        plt.title(classes[y_data[image_idx][0]])\n+        plt.axis(\"off\")\n+        position += 1\n+\n+\"\"\"\n+### Compute clustering accuracy\n+\n+First, we assign a label for each cluster based on the majority label of its images.\n+Then, we compute the accuracy of each cluster by dividing the number of image with the\n+majority label by the size of the cluster.\n+\"\"\"\n+\n+cluster_label_counts = dict()\n+\n+for c in range(num_clusters):\n+    cluster_label_counts[c] = [0] * num_classes\n+    instances = clusters[c]\n+    for i, _ in instances:\n+        cluster_label_counts[c][y_data[i][0]] += 1\n+\n+    cluster_label_idx = np.argmax(cluster_label_counts[c])\n+    correct_count = np.max(cluster_label_counts[c])\n+    cluster_size = len(clusters[c])\n+    accuracy = (\n+        np.round((correct_count / cluster_size) * 100, 2) if cluster_size > 0 else 0\n+    )\n+    cluster_label = classes[cluster_label_idx]\n+    print(\"cluster\", c, \"label is:\", cluster_label, \" -  accuracy:\", accuracy, \"%\")\n+\n+\"\"\"\n+## Conclusion\n+\n+To improve the accuracy results, you can: 1) increase the number\n+of epochs in the representation learning and the clustering phases; 2)\n+allow the encoder weights to be tuned during the clustering phase; and 3) perform a final\n+fine-tuning step through self-labeling, as described in the [original SCAN paper](https://arxiv.org/abs/2005.12320).\n+Note that unsupervised image clustering techniques are not expected to outperform the accuracy\n+of supervised image classification techniques, rather showing that they can learn the semantics\n+of the images and group them into clusters that are similar to their original classes.\n+\"\"\"\n\n@@ -0,0 +1,438 @@\n+\"\"\"\n+Title: Image classification via fine-tuning with EfficientNet\n+Author: [Yixing Fu](https://github.com/yixingfu)\n+Date created: 2020/06/30\n+Last modified: 2023/07/10\n+Description: Use EfficientNet with weights pre-trained on imagenet for Stanford Dogs classification.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+\n+## Introduction: what is EfficientNet\n+\n+EfficientNet, first introduced in [Tan and Le, 2019](https://arxiv.org/abs/1905.11946)\n+is among the most efficient models (i.e. requiring least FLOPS for inference)\n+that reaches State-of-the-Art accuracy on both\n+imagenet and common image classification transfer learning tasks.\n+\n+The smallest base model is similar to [MnasNet](https://arxiv.org/abs/1807.11626), which\n+reached near-SOTA with a significantly smaller model. By introducing a heuristic way to\n+scale the model, EfficientNet provides a family of models (B0 to B7) that represents a\n+good combination of efficiency and accuracy on a variety of scales. Such a scaling\n+heuristics (compound-scaling, details see\n+[Tan and Le, 2019](https://arxiv.org/abs/1905.11946)) allows the\n+efficiency-oriented base model (B0) to surpass models at every scale, while avoiding\n+extensive grid-search of hyperparameters.\n+\n+A summary of the latest updates on the model is available at\n+[here](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet), where various\n+augmentation schemes and semi-supervised learning approaches are applied to further\n+improve the imagenet performance of the models. These extensions of the model can be used\n+by updating weights without changing model architecture.\n+\n+## B0 to B7 variants of EfficientNet\n+\n+*(This section provides some details on \"compound scaling\", and can be skipped\n+if you're only interested in using the models)*\n+\n+Based on the [original paper](https://arxiv.org/abs/1905.11946) people may have the\n+impression that EfficientNet is a continuous family of models created by arbitrarily\n+choosing scaling factor in as Eq.(3) of the paper.  However, choice of resolution,\n+depth and width are also restricted by many factors:\n+\n+- Resolution: Resolutions not divisible by 8, 16, etc. cause zero-padding near boundaries\n+of some layers which wastes computational resources. This especially applies to smaller\n+variants of the model, hence the input resolution for B0 and B1 are chosen as 224 and\n+240.\n+\n+- Depth and width: The building blocks of EfficientNet demands channel size to be\n+multiples of 8.\n+\n+- Resource limit: Memory limitation may bottleneck resolution when depth\n+and width can still increase. In such a situation, increasing depth and/or\n+width but keep resolution can still improve performance.\n+\n+As a result, the depth, width and resolution of each variant of the EfficientNet models\n+are hand-picked and proven to produce good results, though they may be significantly\n+off from the compound scaling formula.\n+Therefore, the keras implementation (detailed below) only provide these 8 models, B0 to B7,\n+instead of allowing arbitray choice of width / depth / resolution parameters.\n+\n+## Keras implementation of EfficientNet\n+\n+An implementation of EfficientNet B0 to B7 has been shipped with Keras since v2.3. To\n+use EfficientNetB0 for classifying 1000 classes of images from ImageNet, run:\n+\n+```python\n+from tensorflow.keras.applications import EfficientNetB0\n+model = EfficientNetB0(weights='imagenet')\n+```\n+\n+This model takes input images of shape `(224, 224, 3)`, and the input data should be in the\n+range `[0, 255]`. Normalization is included as part of the model.\n+\n+Because training EfficientNet on ImageNet takes a tremendous amount of resources and\n+several techniques that are not a part of the model architecture itself. Hence the Keras\n+implementation by default loads pre-trained weights obtained via training with\n+[AutoAugment](https://arxiv.org/abs/1805.09501).\n+\n+For B0 to B7 base models, the input shapes are different. Here is a list of input shape\n+expected for each model:\n+\n+| Base model | resolution|\n+|----------------|-----|\n+| EfficientNetB0 | 224 |\n+| EfficientNetB1 | 240 |\n+| EfficientNetB2 | 260 |\n+| EfficientNetB3 | 300 |\n+| EfficientNetB4 | 380 |\n+| EfficientNetB5 | 456 |\n+| EfficientNetB6 | 528 |\n+| EfficientNetB7 | 600 |\n+\n+When the model is intended for transfer learning, the Keras implementation\n+provides a option to remove the top layers:\n+```\n+model = EfficientNetB0(include_top=False, weights='imagenet')\n+```\n+This option excludes the final `Dense` layer that turns 1280 features on the penultimate\n+layer into prediction of the 1000 ImageNet classes. Replacing the top layer with custom\n+layers allows using EfficientNet as a feature extractor in a transfer learning workflow.\n+\n+Another argument in the model constructor worth noticing is `drop_connect_rate` which controls\n+the dropout rate responsible for [stochastic depth](https://arxiv.org/abs/1603.09382).\n+This parameter serves as a toggle for extra regularization in finetuning, but does not\n+affect loaded weights. For example, when stronger regularization is desired, try:\n+\n+```python\n+model = EfficientNetB0(weights='imagenet', drop_connect_rate=0.4)\n+```\n+The default value is 0.2.\n+\n+## Example: EfficientNetB0 for Stanford Dogs.\n+\n+EfficientNet is capable of a wide range of image classification tasks.\n+This makes it a good model for transfer learning.\n+As an end-to-end example, we will show using pre-trained EfficientNetB0 on\n+[Stanford Dogs](http://vision.stanford.edu/aditya86/ImageNetDogs/main.html) dataset.\n+\n+\"\"\"\n+\n+\"\"\"\n+## Setup and data loading\n+\"\"\"\n+\n+import numpy as np\n+import tensorflow_datasets as tfds\n+import tensorflow as tf  # For tf.data\n+import matplotlib.pyplot as plt\n+import keras\n+from keras import layers\n+from keras.applications import EfficientNetB0\n+\n+# IMG_SIZE is determined by EfficientNet model choice\n+IMG_SIZE = 224\n+BATCH_SIZE = 64\n+\n+\n+\"\"\"\n+### Loading data\n+\n+Here we load data from [tensorflow_datasets](https://www.tensorflow.org/datasets)\n+(hereafter TFDS).\n+Stanford Dogs dataset is provided in\n+TFDS as [stanford_dogs](https://www.tensorflow.org/datasets/catalog/stanford_dogs).\n+It features 20,580 images that belong to 120 classes of dog breeds\n+(12,000 for training and 8,580 for testing).\n+\n+By simply changing `dataset_name` below, you may also try this notebook for\n+other datasets in TFDS such as\n+[cifar10](https://www.tensorflow.org/datasets/catalog/cifar10),\n+[cifar100](https://www.tensorflow.org/datasets/catalog/cifar100),\n+[food101](https://www.tensorflow.org/datasets/catalog/food101),\n+etc. When the images are much smaller than the size of EfficientNet input,\n+we can simply upsample the input images. It has been shown in\n+[Tan and Le, 2019](https://arxiv.org/abs/1905.11946) that transfer learning\n+result is better for increased resolution even if input images remain small.\n+\"\"\"\n+\n+dataset_name = \"stanford_dogs\"\n+(ds_train, ds_test), ds_info = tfds.load(\n+    dataset_name, split=[\"train\", \"test\"], with_info=True, as_supervised=True\n+)\n+NUM_CLASSES = ds_info.features[\"label\"].num_classes\n+\n+\n+\"\"\"\n+When the dataset include images with various size, we need to resize them into a\n+shared size. The Stanford Dogs dataset includes only images at least 200x200\n+pixels in size. Here we resize the images to the input size needed for EfficientNet.\n+\"\"\"\n+\n+size = (IMG_SIZE, IMG_SIZE)\n+ds_train = ds_train.map(lambda image, label: (tf.image.resize(image, size), label))\n+ds_test = ds_test.map(lambda image, label: (tf.image.resize(image, size), label))\n+\n+\"\"\"\n+### Visualizing the data\n+\n+The following code shows the first 9 images with their labels.\n+\"\"\"\n+\n+\n+def format_label(label):\n+    string_label = label_info.int2str(label)\n+    return string_label.split(\"-\")[1]\n+\n+\n+label_info = ds_info.features[\"label\"]\n+for i, (image, label) in enumerate(ds_train.take(9)):\n+    ax = plt.subplot(3, 3, i + 1)\n+    plt.imshow(image.numpy().astype(\"uint8\"))\n+    plt.title(\"{}\".format(format_label(label)))\n+    plt.axis(\"off\")\n+\n+\n+\"\"\"\n+### Data augmentation\n+\n+We can use the preprocessing layers APIs for image augmentation.\n+\"\"\"\n+\n+img_augmentation_layers = [\n+    layers.RandomRotation(factor=0.15),\n+    layers.RandomTranslation(height_factor=0.1, width_factor=0.1),\n+    layers.RandomFlip(),\n+    layers.RandomContrast(factor=0.1),\n+]\n+\n+def img_augmentation(images):\n+    for layer in img_augmentation_layers:\n+        images = layer(images)\n+    return images\n+\n+\n+\"\"\"\n+This `Sequential` model object can be used both as a part of\n+the model we later build, and as a function to preprocess\n+data before feeding into the model. Using them as function makes\n+it easy to visualize the augmented images. Here we plot 9 examples\n+of augmentation result of a given figure.\n+\"\"\"\n+\n+for image, label in ds_train.take(1):\n+    for i in range(9):\n+        ax = plt.subplot(3, 3, i + 1)\n+        aug_img = img_augmentation(np.expand_dims(image.numpy(), axis=0))\n+        aug_img = np.array(aug_img)\n+        plt.imshow(aug_img[0].astype(\"uint8\"))\n+        plt.title(\"{}\".format(format_label(label)))\n+        plt.axis(\"off\")\n+\n+\n+\"\"\"\n+### Prepare inputs\n+\n+Once we verify the input data and augmentation are working correctly,\n+we prepare dataset for training. The input data are resized to uniform\n+`IMG_SIZE`. The labels are put into one-hot\n+(a.k.a. categorical) encoding. The dataset is batched.\n+\n+Note: `prefetch` and `AUTOTUNE` may in some situation improve\n+performance, but depends on environment and the specific dataset used.\n+See this [guide](https://www.tensorflow.org/guide/data_performance)\n+for more information on data pipeline performance.\n+\"\"\"\n+\n+\n+# One-hot / categorical encoding\n+def input_preprocess_train(image, label):\n+    image = img_augmentation(image)\n+    label = tf.one_hot(label, NUM_CLASSES)\n+    return image, label\n+\n+\n+def input_preprocess_test(image, label):\n+    label = tf.one_hot(label, NUM_CLASSES)\n+    return image, label\n+\n+\n+ds_train = ds_train.map(input_preprocess_train, num_parallel_calls=tf.data.AUTOTUNE)\n+ds_train = ds_train.batch(batch_size=BATCH_SIZE, drop_remainder=True)\n+ds_train = ds_train.prefetch(tf.data.AUTOTUNE)\n+\n+ds_test = ds_test.map(input_preprocess_test, num_parallel_calls=tf.data.AUTOTUNE)\n+ds_test = ds_test.batch(batch_size=BATCH_SIZE, drop_remainder=True)\n+\n+\n+\"\"\"\n+## Training a model from scratch\n+\n+We build an EfficientNetB0 with 120 output classes, that is initialized from scratch:\n+\n+Note: the accuracy will increase very slowly and may overfit.\n+\"\"\"\n+\n+model = EfficientNetB0(include_top=True, weights=None, classes=NUM_CLASSES, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n+model.compile(\n+    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n+)\n+\n+model.summary()\n+\n+epochs = 40  # @param {type: \"slider\", min:10, max:100}\n+hist = model.fit(ds_train, epochs=epochs, validation_data=ds_test, verbose=2)\n+\n+\n+\"\"\"\n+Training the model is relatively fast. This might make it sounds easy to simply train EfficientNet on any\n+dataset wanted from scratch. However, training EfficientNet on smaller datasets,\n+especially those with lower resolution like CIFAR-100, faces the significant challenge of\n+overfitting.\n+\n+Hence training from scratch requires very careful choice of hyperparameters and is\n+difficult to find suitable regularization. It would also be much more demanding in resources.\n+Plotting the training and validation accuracy\n+makes it clear that validation accuracy stagnates at a low value.\n+\"\"\"\n+\n+import matplotlib.pyplot as plt\n+\n+\n+def plot_hist(hist):\n+    plt.plot(hist.history[\"accuracy\"])\n+    plt.plot(hist.history[\"val_accuracy\"])\n+    plt.title(\"model accuracy\")\n+    plt.ylabel(\"accuracy\")\n+    plt.xlabel(\"epoch\")\n+    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n+    plt.show()\n+\n+\n+plot_hist(hist)\n+\n+\"\"\"\n+## Transfer learning from pre-trained weights\n+\n+Here we initialize the model with pre-trained ImageNet weights,\n+and we fine-tune it on our own dataset.\n+\"\"\"\n+\n+\n+def build_model(num_classes):\n+    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n+    model = EfficientNetB0(include_top=False, input_tensor=x, weights=\"imagenet\")\n+\n+    # Freeze the pretrained weights\n+    model.trainable = False\n+\n+    # Rebuild top\n+    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n+    x = layers.BatchNormalization()(x)\n+\n+    top_dropout_rate = 0.2\n+    x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n+    outputs = layers.Dense(num_classes, activation=\"softmax\", name=\"pred\")(x)\n+\n+    # Compile\n+    model = keras.Model(inputs, outputs, name=\"EfficientNet\")\n+    optimizer = keras.optimizers.Adam(learning_rate=1e-2)\n+    model.compile(\n+        optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n+    )\n+    return model\n+\n+\n+\"\"\"\n+The first step to transfer learning is to freeze all layers and train only the top\n+layers. For this step, a relatively large learning rate (1e-2) can be used.\n+Note that validation accuracy and loss will usually be better than training\n+accuracy and loss. This is because the regularization is strong, which only\n+suppresses training-time metrics.\n+\n+Note that the convergence may take up to 50 epochs depending on choice of learning rate.\n+If image augmentation layers were not\n+applied, the validation accuracy may only reach ~60%.\n+\"\"\"\n+\n+model = build_model(num_classes=NUM_CLASSES)\n+\n+epochs = 25  # @param {type: \"slider\", min:8, max:80}\n+hist = model.fit(ds_train, epochs=epochs, validation_data=ds_test, verbose=2)\n+plot_hist(hist)\n+\n+\"\"\"\n+The second step is to unfreeze a number of layers and fit the model using smaller\n+learning rate. In this example we show unfreezing all layers, but depending on\n+specific dataset it may be desireble to only unfreeze a fraction of all layers.\n+\n+When the feature extraction with\n+pretrained model works good enough, this step would give a very limited gain on\n+validation accuracy. In our case we only see a small improvement,\n+as ImageNet pretraining already exposed the model to a good amount of dogs.\n+\n+On the other hand, when we use pretrained weights on a dataset that is more different\n+from ImageNet, this fine-tuning step can be crucial as the feature extractor also\n+needs to be adjusted by a considerable amount. Such a situation can be demonstrated\n+if choosing CIFAR-100 dataset instead, where fine-tuning boosts validation accuracy\n+by about 10% to pass 80% on `EfficientNetB0`.\n+In such a case the convergence may take more than 50 epochs.\n+\n+A side note on freezing/unfreezing models: setting `trainable` of a `Model` will\n+simultaneously set all layers belonging to the `Model` to the same `trainable`\n+attribute. Each layer is trainable only if both the layer itself and the model\n+containing it are trainable. Hence when we need to partially freeze/unfreeze\n+a model, we need to make sure the `trainable` attribute of the model is set\n+to `True`.\n+\"\"\"\n+\n+\n+def unfreeze_model(model):\n+    # We unfreeze the top 20 layers while leaving BatchNorm layers frozen\n+    for layer in model.layers[-20:]:\n+        if not isinstance(layer, layers.BatchNormalization):\n+            layer.trainable = True\n+\n+    optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n+    model.compile(\n+        optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n+    )\n+\n+\n+unfreeze_model(model)\n+\n+epochs = 10  # @param {type: \"slider\", min:8, max:50}\n+hist = model.fit(ds_train, epochs=epochs, validation_data=ds_test, verbose=2)\n+plot_hist(hist)\n+\n+\"\"\"\n+### Tips for fine tuning EfficientNet\n+\n+On unfreezing layers:\n+\n+- The `BatchNormalization` layers need to be kept frozen\n+([more details](https://keras.io/guides/transfer_learning/)).\n+If they are also turned to trainable, the\n+first epoch after unfreezing will significantly reduce accuracy.\n+- In some cases it may be beneficial to open up only a portion of layers instead of\n+unfreezing all. This will make fine tuning much faster when going to larger models like\n+B7.\n+- Each block needs to be all turned on or off. This is because the architecture includes\n+a shortcut from the first layer to the last layer for each block. Not respecting blocks\n+also significantly harms the final performance.\n+\n+Some other tips for utilizing EfficientNet:\n+\n+- Larger variants of EfficientNet do not guarantee improved performance, especially for\n+tasks with less data or fewer classes. In such a case, the larger variant of EfficientNet\n+chosen, the harder it is to tune hyperparameters.\n+- EMA (Exponential Moving Average) is very helpful in training EfficientNet from scratch,\n+but not so much for transfer learning.\n+- Do not use the RMSprop setup as in the original paper for transfer learning. The\n+momentum and learning rate are too high for transfer learning. It will easily corrupt the\n+pretrained weight and blow up the loss. A quick check is to see if loss (as categorical\n+cross entropy) is getting significantly larger than log(NUM_CLASSES) after the same\n+epoch. If so, the initial learning rate/momentum is too high.\n+- Smaller batch size benefit validation accuracy, possibly due to effectively providing\n+regularization.\n+\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b28879ee5061e54f2855e7727605264859f8f591", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 593 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -267,6 +267,7 @@ lr_scheduler = keras.optimizers.schedules.CosineDecay(\n # Compile the model.\n representation_learner.compile(\n     optimizer=keras.optimizers.AdamW(learning_rate=lr_scheduler, weight_decay=0.0001),\n+    jit_compile=False,\n )\n # Fit the model.\n history = representation_learner.fit(\n@@ -487,6 +488,7 @@ labels = tf.ones(shape=(x_data.shape[0]))\n clustering_learner.compile(\n     optimizer=keras.optimizers.AdamW(learning_rate=0.0005, weight_decay=0.0001),\n     loss=losses,\n+    jit_compile=False,\n )\n \n # Begin training the model.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#60e1e4c4a22da6f4f5a47c06e3fd2b558aa04099", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 27 | Files Changed: 1 | Hunks: 23 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 54 | Churn Cumulative: 647 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -194,21 +194,21 @@ class RepresentationLearner(keras.Model):\n         return [self.loss_tracker]\n \n     def compute_contrastive_loss(self, feature_vectors, batch_size):\n-        num_augmentations = tf.shape(feature_vectors)[0] // batch_size\n+        num_augmentations = keras.ops.shape(feature_vectors)[0] // batch_size\n         if self.l2_normalize:\n-            feature_vectors = tf.math.l2_normalize(feature_vectors, -1)\n+            feature_vectors = keras.utils.normalize(feature_vectors)\n         # The logits shape is [num_augmentations * batch_size, num_augmentations * batch_size].\n         logits = (\n-            tf.linalg.matmul(feature_vectors, feature_vectors, transpose_b=True)\n+            keras.ops.matmul(feature_vectors, keras.ops.transpose(feature_vectors))\n             / self.temperature\n         )\n         # Apply log-max trick for numerical stability.\n-        logits_max = tf.math.reduce_max(logits, axis=1)\n+        logits_max = keras.ops.max(logits, axis=1)\n         logits = logits - logits_max\n         # The shape of targets is [num_augmentations * batch_size, num_augmentations * batch_size].\n         # targets is a matrix consits of num_augmentations submatrices of shape [batch_size * batch_size].\n         # Each [batch_size * batch_size] submatrix is an identity matrix (diagonal entries are ones).\n-        targets = tf.tile(tf.eye(batch_size), [num_augmentations, num_augmentations])\n+        targets = keras.ops.tile(tf.eye(batch_size), [num_augmentations, num_augmentations])\n         # Compute cross entropy loss\n         return keras.losses.categorical_crossentropy(\n             y_true=targets, y_pred=logits, from_logits=True\n@@ -228,7 +228,7 @@ class RepresentationLearner(keras.Model):\n         return self.projector(features)\n \n     def train_step(self, inputs):\n-        batch_size = tf.shape(inputs)[0]\n+        batch_size = keras.ops.shape(inputs)[0]\n         # Run the forward pass and compute the contrastive loss\n         with tf.GradientTape() as tape:\n             feature_vectors = self(inputs, training=True)\n@@ -244,7 +244,7 @@ class RepresentationLearner(keras.Model):\n         return {m.name: m.result() for m in self.metrics}\n \n     def test_step(self, inputs):\n-        batch_size = tf.shape(inputs)[0]\n+        batch_size = keras.ops.shape(inputs)[0]\n         feature_vectors = self(inputs, training=False)\n         loss = self.compute_contrastive_loss(feature_vectors, batch_size)\n         self.loss_tracker.update_state(loss)\n@@ -298,7 +298,7 @@ batch_size = 500\n # Get the feature vector representations of the images.\n feature_vectors = encoder.predict(x_data, batch_size=batch_size, verbose=1)\n # Normalize the feature vectores.\n-feature_vectors = tf.math.l2_normalize(feature_vectors, -1)\n+feature_vectors = keras.utils.normalize(feature_vectors)\n \n \"\"\"\n ### Find the *k* nearest neighbours for each embedding\n@@ -311,9 +311,9 @@ for batch_idx in tqdm(range(num_batches)):\n     end_idx = start_idx + batch_size\n     current_batch = feature_vectors[start_idx:end_idx]\n     # Compute the dot similarity.\n-    similarities = tf.linalg.matmul(current_batch, feature_vectors, transpose_b=True)\n+    similarities = keras.ops.matmul(current_batch, keras.ops.transpose(feature_vectors))\n     # Get the indices of most similar vectors.\n-    _, indices = tf.math.top_k(similarities, k=k_neighbours + 1, sorted=True)\n+    _, indices = keras.ops.top_k(similarities, k=k_neighbours + 1, sorted=True)\n     # Add the indices to the neighbours.\n     neighbours.append(indices[..., 1:])\n \n@@ -360,12 +360,12 @@ class ClustersConsistencyLoss(keras.losses.Loss):\n \n     def __call__(self, target, similarity, sample_weight=None):\n         # Set targets to be ones.\n-        target = tf.ones_like(similarity)\n+        target = keras.ops.ones_like(similarity)\n         # Compute cross entropy loss.\n         loss = keras.losses.binary_crossentropy(\n             y_true=target, y_pred=similarity, from_logits=True\n         )\n-        return tf.math.reduce_mean(loss)\n+        return keras.ops.mean(loss)\n \n \n \"\"\"\n@@ -383,17 +383,17 @@ class ClustersEntropyLoss(keras.losses.Loss):\n \n     def __call__(self, target, cluster_probabilities, sample_weight=None):\n         # Ideal entropy = log(num_clusters).\n-        num_clusters = tf.cast(tf.shape(cluster_probabilities)[-1], tf.dtypes.float32)\n-        target = tf.math.log(num_clusters)\n+        num_clusters = keras.ops.cast(keras.ops.shape(cluster_probabilities)[-1], \"float32\")\n+        target = keras.ops.log(num_clusters)\n         # Compute the overall clusters distribution.\n-        cluster_probabilities = tf.math.reduce_mean(cluster_probabilities, axis=0)\n+        cluster_probabilities = keras.ops.mean(cluster_probabilities, axis=0)\n         # Replacing zero probabilities - if any - with a very small value.\n-        cluster_probabilities = tf.clip_by_value(\n-            cluster_probabilities, clip_value_min=1e-8, clip_value_max=1.0\n+        cluster_probabilities = keras.ops.clip(\n+            cluster_probabilities, 1e-8, 1.0\n         )\n         # Compute the entropy over the clusters.\n-        entropy = -tf.math.reduce_sum(\n-            cluster_probabilities * tf.math.log(cluster_probabilities)\n+        entropy = -keras.ops.sum(\n+            cluster_probabilities * keras.ops.log(cluster_probabilities)\n         )\n         # Compute the difference between the target and the actual.\n         loss = target - entropy\n@@ -441,22 +441,22 @@ def create_clustering_learner(clustering_model):\n         shape=tuple([k_neighbours]) + input_shape, name=\"neighbours\"\n     )\n     # Changes neighbours shape to [batch_size * k_neighbours, width, height, channels]\n-    neighbours_reshaped = tf.reshape(neighbours, shape=tuple([-1]) + input_shape)\n+    neighbours_reshaped = keras.ops.reshape(neighbours, shape=tuple([-1]) + input_shape)\n     # anchor_clustering shape: [batch_size, num_clusters]\n     anchor_clustering = clustering_model(anchor)\n     # neighbours_clustering shape: [batch_size * k_neighbours, num_clusters]\n     neighbours_clustering = clustering_model(neighbours_reshaped)\n     # Convert neighbours_clustering shape to [batch_size, k_neighbours, num_clusters]\n-    neighbours_clustering = tf.reshape(\n+    neighbours_clustering = keras.ops.reshape(\n         neighbours_clustering,\n-        shape=(-1, k_neighbours, tf.shape(neighbours_clustering)[-1]),\n+        shape=(-1, k_neighbours, keras.ops.shape(neighbours_clustering)[-1]),\n     )\n     # similarity shape: [batch_size, 1, k_neighbours]\n-    similarity = tf.linalg.einsum(\n-        \"bij,bkj->bik\", tf.expand_dims(anchor_clustering, axis=1), neighbours_clustering\n+    similarity = keras.ops.einsum(\n+        \"bij,bkj->bik\", keras.ops.expand_dims(anchor_clustering, axis=1), neighbours_clustering\n     )\n     # similarity shape:  [batch_size, k_neighbours]\n-    similarity = layers.Lambda(lambda x: tf.squeeze(x, axis=1), name=\"similarity\")(\n+    similarity = layers.Lambda(lambda x: keras.ops.squeeze(x, axis=1), name=\"similarity\")(\n         similarity\n     )\n     # Create the model.\n@@ -514,11 +514,11 @@ plt.show()\n # Get the cluster probability distribution of the input images.\n clustering_probs = clustering_model.predict(x_data, batch_size=batch_size, verbose=1)\n # Get the cluster of the highest probability.\n-cluster_assignments = tf.math.argmax(clustering_probs, axis=-1).numpy()\n+cluster_assignments = keras.ops.argmax(clustering_probs, axis=-1).numpy()\n # Store the clustering confidence.\n # Images with the highest clustering confidence are considered the 'prototypes'\n # of the clusters.\n-cluster_confidence = tf.math.reduce_max(clustering_probs, axis=-1).numpy()\n+cluster_confidence = keras.ops.max(clustering_probs, axis=-1).numpy()\n \n \"\"\"\n Let's compute the cluster sizes\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2a843a273fc1c5496b90ce0ad2ae02d0fe11f5f7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 651 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -441,7 +441,7 @@ def create_clustering_learner(clustering_model):\n         shape=tuple([k_neighbours]) + input_shape, name=\"neighbours\"\n     )\n     # Changes neighbours shape to [batch_size * k_neighbours, width, height, channels]\n-    neighbours_reshaped = keras.ops.reshape(neighbours, shape=tuple([-1]) + input_shape)\n+    neighbours_reshaped = keras.ops.reshape(neighbours, tuple([-1]) + input_shape)\n     # anchor_clustering shape: [batch_size, num_clusters]\n     anchor_clustering = clustering_model(anchor)\n     # neighbours_clustering shape: [batch_size * k_neighbours, num_clusters]\n@@ -449,7 +449,7 @@ def create_clustering_learner(clustering_model):\n     # Convert neighbours_clustering shape to [batch_size, k_neighbours, num_clusters]\n     neighbours_clustering = keras.ops.reshape(\n         neighbours_clustering,\n-        shape=(-1, k_neighbours, keras.ops.shape(neighbours_clustering)[-1]),\n+        (-1, k_neighbours, keras.ops.shape(neighbours_clustering)[-1]),\n     )\n     # similarity shape: [batch_size, 1, k_neighbours]\n     similarity = keras.ops.einsum(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
