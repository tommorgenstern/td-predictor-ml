{"custom_id": "scrapy#62c7daf785ac0fc0ee75f46059655d2006b96ec0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 27 | Churn Cumulative: 279 | Contributors (this commit): 4 | Commits (past 90d): 7 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,7 +2,8 @@\n Scrapy - a screen scraping framework written in Python\n \"\"\"\n \n-__all__ = ['__version__', 'version_info', 'optional_features', 'twisted_version']\n+__all__ = ['__version__', 'version_info', 'optional_features', 'twisted_version',\n+           'Spider', 'Request', 'FormRequest', 'Selector', 'Item', 'Field']\n \n # Scrapy version\n import pkgutil\n@@ -49,3 +50,9 @@ from twisted import version as _txv\n twisted_version = (_txv.major, _txv.minor, _txv.micro)\n if twisted_version >= (11, 1, 0):\n     optional_features.add('http11')\n+\n+# Declare top-level shortcuts\n+from scrapy.spider import Spider\n+from scrapy.http import Request, FormRequest\n+from scrapy.selector import Selector\n+from scrapy.item import Item, Field\n\n@@ -14,3 +14,21 @@ class ToplevelTestCase(TestCase):\n     def test_optional_features(self):\n         self.assertIs(type(scrapy.optional_features), set)\n         self.assertIn('ssl', scrapy.optional_features)\n+\n+    def test_request_shortcut(self):\n+        from scrapy.http import Request, FormRequest\n+        self.assertIs(scrapy.Request, Request)\n+        self.assertIs(scrapy.FormRequest, FormRequest)\n+\n+    def test_spider_shortcut(self):\n+        from scrapy.spider import Spider\n+        self.assertIs(scrapy.Spider, Spider)\n+\n+    def test_selector_shortcut(self):\n+        from scrapy.selector import Selector\n+        self.assertIs(scrapy.Selector, Selector)\n+\n+    def test_item_shortcut(self):\n+        from scrapy.item import Item, Field\n+        self.assertIs(scrapy.Item, Item)\n+        self.assertIs(scrapy.Field, Field)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#11e62117b745d4e489b0533873be48d09d19136a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 10 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 1515 | Contributors (this commit): 7 | Commits (past 90d): 8 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -10,7 +10,7 @@ __all__ = ['HtmlXPathSelector', 'XmlXPathSelector', 'XPathSelector',\n \n def _xpathselector_css(self, *a, **kw):\n     raise RuntimeError('.css() method not available for %s, '\n-                        'instantiate scrapy.selector.Selector '\n+                        'instantiate scrapy.Selector '\n                         'instead' % type(self).__name__)\n \n XPathSelector = create_deprecated_class(\n@@ -21,7 +21,7 @@ XPathSelector = create_deprecated_class(\n         '_default_type': 'html',\n         'css': _xpathselector_css,\n     },\n-    new_class_path='scrapy.selector.Selector',\n+    new_class_path='scrapy.Selector',\n     old_class_path='scrapy.selector.XPathSelector',\n )\n \n@@ -32,7 +32,7 @@ XmlXPathSelector = create_deprecated_class(\n         '__slots__': (),\n         '_default_type': 'xml',\n     },\n-    new_class_path='scrapy.selector.Selector',\n+    new_class_path='scrapy.Selector',\n     old_class_path='scrapy.selector.XmlXPathSelector',\n )\n \n@@ -43,7 +43,7 @@ HtmlXPathSelector = create_deprecated_class(\n         '__slots__': (),\n         '_default_type': 'html',\n     },\n-    new_class_path='scrapy.selector.Selector',\n+    new_class_path='scrapy.Selector',\n     old_class_path='scrapy.selector.HtmlXPathSelector',\n )\n \n\n@@ -354,7 +354,7 @@ class DeprecatedXpathSelectorTest(unittest.TestCase):\n \n             # subclassing must issue a warning\n             self.assertEqual(len(w), 1, str(cls))\n-            self.assertIn('scrapy.selector.Selector', str(w[0].message))\n+            self.assertIn('scrapy.Selector', str(w[0].message))\n \n             # subclass instance doesn't issue a warning\n             usel = UserClass(text=self.text)\n@@ -363,7 +363,7 @@ class DeprecatedXpathSelectorTest(unittest.TestCase):\n             # class instance must issue a warning\n             sel = cls(text=self.text)\n             self.assertEqual(len(w), 2, str((cls, [x.message for x in w])))\n-            self.assertIn('scrapy.selector.Selector', str(w[1].message))\n+            self.assertIn('scrapy.Selector', str(w[1].message))\n \n             # subclass and instance checks\n             self.assertTrue(issubclass(cls, Selector))\n@@ -378,7 +378,7 @@ class DeprecatedXpathSelectorTest(unittest.TestCase):\n \n             # subclassing must issue a warning\n             self.assertEqual(len(w), 1, str(cls))\n-            self.assertIn('scrapy.selector.Selector', str(w[0].message))\n+            self.assertIn('scrapy.Selector', str(w[0].message))\n \n             # subclass instance doesn't issue a warning\n             usel = UserClass(text=self.text)\n@@ -387,7 +387,7 @@ class DeprecatedXpathSelectorTest(unittest.TestCase):\n             # class instance must issue a warning\n             sel = cls(text=self.text)\n             self.assertEqual(len(w), 2, str((cls, [x.message for x in w])))\n-            self.assertIn('scrapy.selector.Selector', str(w[1].message))\n+            self.assertIn('scrapy.Selector', str(w[1].message))\n \n             # subclass and instance checks\n             self.assertTrue(issubclass(cls, Selector))\n@@ -405,7 +405,7 @@ class DeprecatedXpathSelectorTest(unittest.TestCase):\n \n             # subclassing must issue a warning\n             self.assertEqual(len(w), 1, str(cls))\n-            self.assertIn('scrapy.selector.Selector', str(w[0].message))\n+            self.assertIn('scrapy.Selector', str(w[0].message))\n \n             # subclass instance doesn't issue a warning\n             usel = UserClass(text=self.text)\n@@ -414,7 +414,7 @@ class DeprecatedXpathSelectorTest(unittest.TestCase):\n             # class instance must issue a warning\n             sel = cls(text=self.text)\n             self.assertEqual(len(w), 2, str((cls, [x.message for x in w])))\n-            self.assertIn('scrapy.selector.Selector', str(w[1].message))\n+            self.assertIn('scrapy.Selector', str(w[1].message))\n \n             # subclass and instance checks\n             self.assertTrue(issubclass(cls, Selector))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e8e689acd1c009ddc1f9aa65f932e566b4cbc3ea", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 248 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -6,7 +6,8 @@ import pkgutil\n __version__ = pkgutil.get_data(__package__, 'VERSION').strip()\n if not isinstance(__version__, str):\n     __version__ = __version__.decode('ascii')\n-version_info = tuple(int(v) for v in __version__.split('.')[:3])\n+version_info = tuple(int(v) if v.isdigit() else v\n+                     for v in __version__.split('.'))\n \n import sys, os, warnings\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a19dfafb25d0651eaf254471ef4b2c59658b06a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 856 | Contributors (this commit): 10 | Commits (past 90d): 2 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -17,6 +17,10 @@ class UrlUtilsTest(unittest.TestCase):\n         self.assertTrue(url_is_from_any_domain(url, ['wheele-bin-art.co.uk']))\n         self.assertFalse(url_is_from_any_domain(url, ['art.co.uk']))\n \n+        url = 'http://www.Wheele-Bin-Art.co.uk/get/product/123'\n+        self.assertTrue(url_is_from_any_domain(url, ['wheele-bin-art.CO.UK']))\n+        self.assertTrue(url_is_from_any_domain(url, ['WHEELE-BIN-ART.CO.UK']))\n+\n         url = 'http://192.169.0.15:8080/mypage.html'\n         self.assertTrue(url_is_from_any_domain(url, ['192.169.0.15:8080']))\n         self.assertFalse(url_is_from_any_domain(url, ['192.169.0.15']))\n\n@@ -17,10 +17,10 @@ from scrapy.utils.python import unicode_to_str\n \n def url_is_from_any_domain(url, domains):\n     \"\"\"Return True if the url belongs to any of the given domains\"\"\"\n-    host = parse_url(url).netloc\n+    host = parse_url(url).netloc.lower()\n \n     if host:\n-        return any(((host == d) or (host.endswith('.%s' % d)) for d in domains))\n+        return any(((host == d.lower()) or (host.endswith('.%s' % d.lower())) for d in domains))\n     else:\n         return False\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#89159779d0a586aaee81d46ac46f5e44335c89ec", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 235 | Contributors (this commit): 12 | Commits (past 90d): 3 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -234,12 +234,12 @@ USER_AGENT = 'Scrapy/%s (+http://scrapy.org)' % import_module('scrapy').__versio\n \n TELNETCONSOLE_ENABLED = 1\n TELNETCONSOLE_PORT = [6023, 6073]\n-TELNETCONSOLE_HOST = '0.0.0.0'\n+TELNETCONSOLE_HOST = '127.0.0.1'\n \n WEBSERVICE_ENABLED = True\n WEBSERVICE_LOGFILE = None\n WEBSERVICE_PORT = [6080, 7030]\n-WEBSERVICE_HOST = '0.0.0.0'\n+WEBSERVICE_HOST = '127.0.0.1'\n WEBSERVICE_RESOURCES = {}\n WEBSERVICE_RESOURCES_BASE = {\n     'scrapy.contrib.webservice.crawler.CrawlerResource': 1,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#134bd8a9e08941186a490bfde30a6a265ccfef0d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 82 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 6 | Methods Changed: 7 | Complexity Δ (Sum/Max): 9/5 | Churn Δ: 82 | Churn Cumulative: 772 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -19,6 +19,7 @@ class TextResponse(Response):\n         self._encoding = kwargs.pop('encoding', None)\n         self._cached_benc = None\n         self._cached_ubody = None\n+        self._cached_selector = None\n         super(TextResponse, self).__init__(*args, **kwargs)\n \n     def _set_url(self, url):\n@@ -88,3 +89,19 @@ class TextResponse(Response):\n     @memoizemethod_noargs\n     def _body_declared_encoding(self):\n         return html_body_declared_encoding(self.body)\n+\n+    @property\n+    def selector(self):\n+        from scrapy.selector import Selector\n+        if self._cached_selector is None:\n+            self._cached_selector = Selector(self)\n+        return self._cached_selector\n+\n+    def xpath(self, query):\n+        return self.selector.xpath(query)\n+\n+    def css(self, query):\n+        return self.selector.css(query)\n+\n+    def re(self, regex):\n+        return self.selector.re(regex)\n\n@@ -2,6 +2,7 @@ import unittest\n \n from w3lib.encoding import resolve_encoding\n from scrapy.http import Request, Response, TextResponse, HtmlResponse, XmlResponse, Headers\n+from scrapy.selector import Selector\n \n \n class BaseResponseTest(unittest.TestCase):\n@@ -112,6 +113,7 @@ class BaseResponseTest(unittest.TestCase):\n         self.assertRaises(AttributeError, setattr, r, 'url', 'http://example2.com')\n         self.assertRaises(AttributeError, setattr, r, 'body', 'xxx')\n \n+\n class ResponseText(BaseResponseTest):\n \n     def test_no_unicode_url(self):\n@@ -258,6 +260,45 @@ class TextResponseTest(BaseResponseTest):\n         #r = self.response_class(\"http://www.example.com\", body='PREFIX\\xe3\\xabSUFFIX')\n         #assert u'\\ufffd' in r.body_as_unicode(), repr(r.body_as_unicode())\n \n+    def test_selector(self):\n+        body = \"<html><head><title>Some page</title><body></body></html>\"\n+        response = self.response_class(\"http://www.example.com\", body=body)\n+\n+        self.assertIsInstance(response.selector, Selector)\n+        self.assertEqual(response.selector.type, 'html')\n+        self.assertIs(response.selector, response.selector)  # property is cached\n+        self.assertIs(response.selector.response, response)\n+\n+        self.assertEqual(\n+            response.selector.xpath(\"//title/text()\").extract(),\n+            [u'Some page']\n+        )\n+        self.assertEqual(\n+            response.selector.css(\"title::text\").extract(),\n+            [u'Some page']\n+        )\n+        self.assertEqual(\n+            response.selector.re(\"Some (.*)</title>\"),\n+            [u'page']\n+        )\n+\n+    def test_selector_shortcuts(self):\n+        body = \"<html><head><title>Some page</title><body></body></html>\"\n+        response = self.response_class(\"http://www.example.com\", body=body)\n+\n+        self.assertEqual(\n+            response.xpath(\"//title/text()\").extract(),\n+            response.selector.xpath(\"//title/text()\").extract(),\n+        )\n+        self.assertEqual(\n+            response.css(\"title::text\").extract(),\n+            response.selector.css(\"title::text\").extract(),\n+        )\n+        self.assertEqual(\n+            response.re(\"Some (.*)</title>\"),\n+            response.selector.re(\"Some (.*)</title>\"),\n+        )\n+\n \n class HtmlResponseTest(TextResponseTest):\n \n@@ -328,6 +369,30 @@ class XmlResponseTest(TextResponseTest):\n         self._assert_response_values(r6, 'iso-8859-1', body2)\n         self._assert_response_values(r7, 'utf-8', body2)\n \n+    def test_selector(self):\n+        body = '<?xml version=\"1.0\" encoding=\"utf-8\"?><xml><elem>value</elem></xml>'\n+        response = self.response_class(\"http://www.example.com\", body=body)\n+\n+        self.assertIsInstance(response.selector, Selector)\n+        self.assertEqual(response.selector.type, 'xml')\n+        self.assertIs(response.selector, response.selector)  # property is cached\n+        self.assertIs(response.selector.response, response)\n+\n+        self.assertEqual(\n+            response.selector.xpath(\"//elem/text()\").extract(),\n+            [u'value']\n+        )\n+\n+    def test_selector_shortcuts(self):\n+        body = '<?xml version=\"1.0\" encoding=\"utf-8\"?><xml><elem>value</elem></xml>'\n+        response = self.response_class(\"http://www.example.com\", body=body)\n+\n+        self.assertEqual(\n+            response.xpath(\"//elem/text()\").extract(),\n+            response.selector.xpath(\"//elem/text()\").extract(),\n+        )\n+\n+\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#681c2985bc13e181fdbe09ed6f88a5d645d32358", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 7 | Churn Cumulative: 779 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -102,6 +102,3 @@ class TextResponse(Response):\n \n     def css(self, query):\n         return self.selector.css(query)\n-\n-    def re(self, regex):\n-        return self.selector.re(regex)\n\n@@ -294,10 +294,6 @@ class TextResponseTest(BaseResponseTest):\n             response.css(\"title::text\").extract(),\n             response.selector.css(\"title::text\").extract(),\n         )\n-        self.assertEqual(\n-            response.re(\"Some (.*)</title>\"),\n-            response.selector.re(\"Some (.*)</title>\"),\n-        )\n \n \n class HtmlResponseTest(TextResponseTest):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d37308ad77ecdc0220b26a3ccb5d8fe2eeb47c8b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 734 | Contributors (this commit): 7 | Commits (past 90d): 4 | Contributors (cumulative): 9 | DMM Complexity: 0.0\n\nDIFF:\n@@ -15,7 +15,6 @@ from scrapy.crawler import Crawler\n from scrapy.exceptions import IgnoreRequest\n from scrapy.http import Request, Response\n from scrapy.item import BaseItem\n-from scrapy.selector import Selector\n from scrapy.settings import Settings\n from scrapy.spider import Spider\n from scrapy.utils.console import start_python_console\n@@ -27,7 +26,7 @@ from scrapy.utils.spider import create_spider_for_request\n class Shell(object):\n \n     relevant_classes = (Crawler, Spider, Request, Response, BaseItem,\n-                        Selector, Settings)\n+                        Settings)\n \n     def __init__(self, crawler, update_vars=None, code=None):\n         self.crawler = crawler\n@@ -99,7 +98,6 @@ class Shell(object):\n         self.vars['spider'] = spider\n         self.vars['request'] = request\n         self.vars['response'] = response\n-        self.vars['sel'] = Selector(response)\n         if self.inthread:\n             self.vars['fetch'] = self.fetch\n         self.vars['view'] = open_in_browser\n\n@@ -31,7 +31,7 @@ class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_response_selector_html(self):\n-        xpath = 'sel.xpath(\"//p[@class=\\'one\\']/text()\").extract()[0]'\n+        xpath = 'response.xpath(\"//p[@class=\\'one\\']/text()\").extract()[0]'\n         _, out, _ = yield self.execute([self.url('/html'), '-c', xpath])\n         self.assertEqual(out.strip(), 'Works')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1e7ddc8e520663dd80b704513ee081f966541255", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 16 | Churn Cumulative: 689 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,13 +6,14 @@ See documentation in docs/topics/shell.rst\n from __future__ import print_function\n \n import signal\n+import warnings\n \n from twisted.internet import reactor, threads, defer\n from twisted.python import threadable\n from w3lib.url import any_to_uri\n \n from scrapy.crawler import Crawler\n-from scrapy.exceptions import IgnoreRequest\n+from scrapy.exceptions import IgnoreRequest, ScrapyDeprecationWarning\n from scrapy.http import Request, Response\n from scrapy.item import BaseItem\n from scrapy.settings import Settings\n@@ -98,6 +99,7 @@ class Shell(object):\n         self.vars['spider'] = spider\n         self.vars['request'] = request\n         self.vars['response'] = response\n+        self.vars['sel'] = _SelectorProxy(response)\n         if self.inthread:\n             self.vars['fetch'] = self.fetch\n         self.vars['view'] = open_in_browser\n@@ -155,3 +157,15 @@ def _request_deferred(request):\n \n     request.callback, request.errback = d.callback, d.errback\n     return d\n+\n+\n+class _SelectorProxy(object):\n+\n+    def __init__(self, response):\n+        self._proxiedresponse = response\n+\n+    def __getattr__(self, name):\n+        warnings.warn('\"sel\" shortcut is deprecated. Use \"response.xpath()\", '\n+                      '\"response.css()\" or \"response.selector\" instead',\n+                      category=ScrapyDeprecationWarning, stacklevel=2)\n+        return getattr(self._proxiedresponse.selector, name)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4898d96337c66f7702ad2bf250c799a70557f3d7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 5 | Complexity Δ (Sum/Max): 7/6 | Churn Δ: 40 | Churn Cumulative: 626 | Contributors (this commit): 5 | Commits (past 90d): 6 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -157,3 +157,26 @@ class SingleRequestSpider(MetaSpider):\n         self.meta['failure'] = failure\n         if callable(self.errback_func):\n             return self.errback_func(failure)\n+\n+\n+class DuplicateStartRequestsSpider(Spider):\n+    dont_filter = True\n+    name = 'duplicatestartrequests'\n+    distinct_urls = 2\n+    dupe_factor = 3\n+\n+    def start_requests(self):\n+        for i in range(0, self.distinct_urls):\n+            for j in range(0, self.dupe_factor):\n+                url = \"http://localhost:8998/echo?headers=1&body=test%d\" % i\n+                yield self.make_requests_from_url(url)\n+\n+    def make_requests_from_url(self, url):\n+        return Request(url, dont_filter=self.dont_filter)\n+\n+    def __init__(self, url=\"http://localhost:8998\", *args, **kwargs):\n+        super(DuplicateStartRequestsSpider, self).__init__(*args, **kwargs)\n+        self.visited = 0\n+\n+    def parse(self, response):\n+        self.visited += 1\n\n@@ -5,7 +5,7 @@ from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n from scrapy.utils.test import docrawl, get_testlog\n from scrapy.tests.spiders import FollowAllSpider, DelaySpider, SimpleSpider, \\\n-    BrokenStartRequestsSpider, SingleRequestSpider\n+    BrokenStartRequestsSpider, SingleRequestSpider, DuplicateStartRequestsSpider\n from scrapy.tests.mockserver import MockServer\n from scrapy.http import Request\n \n@@ -113,6 +113,21 @@ class CrawlTestCase(TestCase):\n         #self.assertTrue(spider.seedsseen.index(None) < spider.seedsseen.index(99),\n         #                spider.seedsseen)\n \n+    @defer.inlineCallbacks\n+    def test_start_requests_dupes(self):\n+        settings = {\"CONCURRENT_REQUESTS\": 1}\n+        spider = DuplicateStartRequestsSpider(dont_filter=True,\n+                                              distinct_urls=2,\n+                                              dupe_factor=3)\n+        yield docrawl(spider, settings)\n+        self.assertEqual(spider.visited, 6)\n+\n+        spider = DuplicateStartRequestsSpider(dont_filter=False,\n+                                              distinct_urls=3,\n+                                              dupe_factor=4)\n+        yield docrawl(spider, settings)\n+        self.assertEqual(spider.visited, 3)\n+\n     @defer.inlineCallbacks\n     def test_unbounded_response(self):\n         # Completeness of responses without Content-Length or Transfer-Encoding\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6e4c77c684b078ed9ac96b14f2f6d143b8403608", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 40 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 48 | Churn Cumulative: 142 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,23 +1,55 @@\n+import hashlib\n import unittest\n \n-from scrapy.http import Request\n from scrapy.dupefilter import RFPDupeFilter\n+from scrapy.http import Request\n \n \n class RFPDupeFilterTest(unittest.TestCase):\n \n     def test_filter(self):\n-        filter = RFPDupeFilter()\n-        filter.open()\n+        dupefilter = RFPDupeFilter()\n+        dupefilter.open()\n \n         r1 = Request('http://scrapytest.org/1')\n         r2 = Request('http://scrapytest.org/2')\n         r3 = Request('http://scrapytest.org/2')\n \n-        assert not filter.request_seen(r1)\n-        assert filter.request_seen(r1)\n+        assert not dupefilter.request_seen(r1)\n+        assert dupefilter.request_seen(r1)\n \n-        assert not filter.request_seen(r2)\n-        assert filter.request_seen(r3)\n+        assert not dupefilter.request_seen(r2)\n+        assert dupefilter.request_seen(r3)\n \n-        filter.close('finished')\n+        dupefilter.close('finished')\n+\n+    def test_request_fingerprint(self):\n+        \"\"\"Test if customization of request_fingerprint method will change\n+        output of request_seen.\n+\n+        \"\"\"\n+        r1 = Request('http://scrapytest.org/index.html')\n+        r2 = Request('http://scrapytest.org/INDEX.html')\n+\n+        dupefilter = RFPDupeFilter()\n+        dupefilter.open()\n+\n+        assert not dupefilter.request_seen(r1)\n+        assert not dupefilter.request_seen(r2)\n+\n+        dupefilter.close('finished')\n+\n+        class CaseInsensitiveRFPDupeFilter(RFPDupeFilter):\n+\n+            def request_fingerprint(self, request):\n+                fp = hashlib.sha1()\n+                fp.update(request.url.lower())\n+                return fp.hexdigest()\n+\n+        case_insensitive_dupefilter = CaseInsensitiveRFPDupeFilter()\n+        case_insensitive_dupefilter.open()\n+\n+        assert not case_insensitive_dupefilter.request_seen(r1)\n+        assert case_insensitive_dupefilter.request_seen(r2)\n+\n+        case_insensitive_dupefilter.close('finished')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7199555e1a4966bf4bf1d8bf4a34a48860103cd6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 2055 | Contributors (this commit): 10 | Commits (past 90d): 1 | Contributors (cumulative): 10 | DMM Complexity: 0.5\n\nDIFF:\n@@ -112,6 +112,7 @@ class ExecutionEngine(object):\n             except StopIteration:\n                 slot.start_requests = None\n             except Exception as exc:\n+                slot.start_requests = None\n                 log.err(None, 'Obtaining request from start requests', \\\n                         spider=spider)\n             else:\n@@ -156,7 +157,8 @@ class ExecutionEngine(object):\n         scraper_idle = self.scraper.slot.is_idle()\n         pending = self.slot.scheduler.has_pending_requests()\n         downloading = bool(self.downloader.active)\n-        idle = scraper_idle and not (pending or downloading)\n+        pending_start_requests = self.slot.start_requests is not None\n+        idle = scraper_idle and not (pending or downloading or pending_start_requests)\n         return idle\n \n     @property\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9d226e626683f552d4c7e7791f5696ea3b6cc9aa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 862 | Contributors (this commit): 9 | Commits (past 90d): 2 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -205,7 +205,7 @@ class Scraper(object):\n                 logkws = self.logformatter.dropped(item, ex, response, spider)\n                 log.msg(spider=spider, **logkws)\n                 return self.signals.send_catch_log_deferred(signal=signals.item_dropped, \\\n-                    item=item, spider=spider, exception=output.value)\n+                    item=item, response=response, spider=spider, exception=output.value)\n             else:\n                 log.err(output, 'Error processing %s' % item, spider=spider)\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2899a778e6a85384183d0241a6c6e74ba6cd9ee9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 161 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -30,7 +30,7 @@ class ItemMeta(type):\n             else:\n                 new_attrs[n] = v\n \n-        cls = type.__new__(mcs, class_name, bases, new_attrs)\n+        cls = super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs)\n         cls.fields = cls.fields.copy()\n         cls.fields.update(fields)\n         return cls\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1cd21f97e0fd8f80e4da97235163e2e26f8e059c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 715 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -108,8 +108,8 @@ class RFC2616Policy(object):\n             request.headers['If-None-Match'] = cachedresponse.headers['ETag']\n \n     def _compute_freshness_lifetime(self, response, request, now):\n-        # Reference nsHttpResponseHead::ComputeFresshnessLifetime\n-        # http://dxr.mozilla.org/mozilla-central/netwerk/protocol/http/nsHttpResponseHead.cpp.html#l259\n+        # Reference nsHttpResponseHead::ComputeFreshnessLifetime\n+        # http://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#410\n         cc = self._parse_cachecontrol(response)\n         if 'max-age' in cc:\n             try:\n@@ -142,7 +142,7 @@ class RFC2616Policy(object):\n \n     def _compute_current_age(self, response, request, now):\n         # Reference nsHttpResponseHead::ComputeCurrentAge\n-        # http://dxr.mozilla.org/mozilla-central/netwerk/protocol/http/nsHttpResponseHead.cpp.html\n+        # http://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#366\n         currentage = 0\n         # If Date header is not set we assume it is a fast connection, and\n         # clock is in sync with the server\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b2ca3cf7e6610bf0f3c6ac5aa79c3322c276c439", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 1 | Churn Cumulative: 196 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -71,6 +71,7 @@ class Command(ScrapyCommand):\n         else:\n             self.crawler_process.start()\n             self.results.printErrors()\n+            self.exitcode = 0 if self.results.wasSuccessful() else 1\n \n     def get_requests(self, spider):\n         requests = []\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1fba64d34e604d6a0d830c4b9a58eebe9a5afa7f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 2295 | Contributors (this commit): 21 | Commits (past 90d): 4 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -60,7 +60,8 @@ class ExecutionEngine(object):\n         self.running = False\n         self.paused = False\n         self.scheduler_cls = load_object(self.settings['SCHEDULER'])\n-        self.downloader = Downloader(crawler)\n+        downloader_cls = load_object(self.settings['DOWNLOADER'])\n+        self.downloader = downloader_cls(crawler)\n         self.scraper = Scraper(crawler)\n         self._concurrent_spiders = self.settings.getint('CONCURRENT_SPIDERS', 1)\n         if self._concurrent_spiders != 1:\n\n@@ -66,6 +66,8 @@ DOWNLOAD_HANDLERS_BASE = {\n \n DOWNLOAD_TIMEOUT = 180      # 3mins\n \n+DOWNLOADER = 'scrapy.core.downloader.Downloader'\n+\n DOWNLOADER_HTTPCLIENTFACTORY = 'scrapy.core.downloader.webclient.ScrapyHTTPClientFactory'\n DOWNLOADER_CLIENTCONTEXTFACTORY = 'scrapy.core.downloader.contextfactory.ScrapyClientContextFactory'\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#65f69e16095799e05de79a3f8f449f7b9edb27aa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 232 | Contributors (this commit): 6 | Commits (past 90d): 1 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -123,7 +123,7 @@ except ImportError:\n     from distutils.core import setup\n else:\n     setup_args['install_requires'] = [\n-        'Twisted>=10.0.0',\n+        'Twisted>=10.0.0,<14.0.0',\n         'w3lib>=1.2',\n         'queuelib',\n         'lxml',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2ba78e715b761628eaa346cde60cecfbbbdce222", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 1633 | Contributors (this commit): 18 | Commits (past 90d): 5 | Contributors (cumulative): 21 | DMM Complexity: None\n\nDIFF:\n@@ -108,10 +108,10 @@ class RequestTest(unittest.TestCase):\n     def test_ajax_url(self):\n         # ascii url\n         r = self.request_class(url=\"http://www.example.com/ajax.html#!key=value\")\n-        self.assertEqual(r.url, \"http://www.example.com/ajax.html?_escaped_fragment_=key=value\")\n+        self.assertEqual(r.url, \"http://www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue\")\n         # unicode url\n         r = self.request_class(url=u\"http://www.example.com/ajax.html#!key=value\")\n-        self.assertEqual(r.url, \"http://www.example.com/ajax.html?_escaped_fragment_=key=value\")\n+        self.assertEqual(r.url, \"http://www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue\")\n \n     def test_copy(self):\n         \"\"\"Test Request copy\"\"\"\n\n@@ -82,11 +82,11 @@ def escape_ajax(url):\n     http://code.google.com/web/ajaxcrawling/docs/getting-started.html\n \n     >>> escape_ajax(\"www.example.com/ajax.html#!key=value\")\n-    'www.example.com/ajax.html?_escaped_fragment_=key=value'\n+    'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue'\n     >>> escape_ajax(\"www.example.com/ajax.html?k1=v1&k2=v2#!key=value\")\n-    'www.example.com/ajax.html?k1=v1&k2=v2&_escaped_fragment_=key=value'\n+    'www.example.com/ajax.html?k1=v1&k2=v2&_escaped_fragment_=key%3Dvalue'\n     >>> escape_ajax(\"www.example.com/ajax.html?#!key=value\")\n-    'www.example.com/ajax.html?_escaped_fragment_=key=value'\n+    'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue'\n     >>> escape_ajax(\"www.example.com/ajax.html#!\")\n     'www.example.com/ajax.html?_escaped_fragment_='\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7130df2d96836467c33f87196699076f159df9f0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 79 | Lines Deleted: 31 | Files Changed: 2 | Hunks: 16 | Methods Changed: 15 | Complexity Δ (Sum/Max): 7/4 | Churn Δ: 110 | Churn Cumulative: 569 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: 0.2702702702702703\n\nDIFF:\n@@ -1,22 +1,42 @@\n from __future__ import print_function\n+import time\n+import sys\n from collections import defaultdict\n-from functools import wraps\n-from unittest import TextTestRunner\n+from unittest import TextTestRunner, TextTestResult as _TextTestResult\n \n from scrapy.command import ScrapyCommand\n from scrapy.contracts import ContractsManager\n from scrapy.utils.misc import load_object\n-from scrapy.utils.spider import iterate_spider_output\n from scrapy.utils.conf import build_component_list\n \n \n-def _generate(cb):\n-    \"\"\" create a callback which does not return anything \"\"\"\n-    @wraps(cb)\n-    def wrapper(response):\n-        output = cb(response)\n-        output = list(iterate_spider_output(output))\n-    return wrapper\n+class TextTestResult(_TextTestResult):\n+    def printSummary(self, start, stop):\n+        write = self.stream.write\n+        writeln = self.stream.writeln\n+\n+        run = self.testsRun\n+        plural = \"s\" if run != 1 else \"\"\n+\n+        writeln(self.separator2)\n+        writeln(\"Ran %d contract%s in %.3fs\" % (run, plural, stop - start))\n+        writeln()\n+\n+        infos = []\n+        if not self.wasSuccessful():\n+            write(\"FAILED\")\n+            failed, errored = map(len, (self.failures, self.errors))\n+            if failed:\n+                infos.append(\"failures=%d\" % failed)\n+            if errored:\n+                infos.append(\"errors=%d\" % errored)\n+        else:\n+            write(\"OK\")\n+\n+        if infos:\n+            writeln(\" (%s)\" % (\", \".join(infos),))\n+        else:\n+            write(\"\\n\")\n \n \n class Command(ScrapyCommand):\n@@ -42,8 +62,9 @@ class Command(ScrapyCommand):\n             self.settings['SPIDER_CONTRACTS_BASE'],\n             self.settings['SPIDER_CONTRACTS'],\n         )\n-        self.conman = ContractsManager([load_object(c) for c in contracts])\n-        self.results = TextTestRunner(verbosity=opts.verbose)._makeResult()\n+        conman = ContractsManager([load_object(c) for c in contracts])\n+        runner = TextTestRunner(verbosity=opts.verbose)\n+        result = TextTestResult(runner.stream, runner.descriptions, runner.verbosity)\n \n         # contract requests\n         contract_reqs = defaultdict(list)\n@@ -53,7 +74,7 @@ class Command(ScrapyCommand):\n \n         for spider in args or spiders.list():\n             spider = spiders.create(spider)\n-            requests = self.get_requests(spider)\n+            requests = self.get_requests(spider, conman, result)\n \n             if opts.list:\n                 for req in requests:\n@@ -69,20 +90,23 @@ class Command(ScrapyCommand):\n                 for method in sorted(methods):\n                     print('  * %s' % method)\n         else:\n+            start = time.time()\n             self.crawler_process.start()\n-            self.results.printErrors()\n-            self.exitcode = 0 if self.results.wasSuccessful() else 1\n+            stop = time.time()\n \n-    def get_requests(self, spider):\n+            result.printErrors()\n+            result.printSummary(start, stop)\n+            self.exitcode = int(not result.wasSuccessful())\n+\n+    def get_requests(self, spider, conman, result):\n         requests = []\n \n         for key, value in vars(type(spider)).items():\n             if callable(value) and value.__doc__:\n                 bound_method = value.__get__(spider, type(spider))\n-                request = self.conman.from_method(bound_method, self.results)\n+                request = conman.from_method(bound_method, result)\n \n                 if request:\n-                    request.callback = _generate(request.callback)\n                     requests.append(request)\n \n         return requests\n\n@@ -48,28 +48,40 @@ class ContractsManager(object):\n                 for contract in contracts:\n                     request = contract.add_post_hook(request, results)\n \n+                self._clean_req(request, method, results)\n                 return request\n \n+    def _clean_req(self, request, method, results):\n+        \"\"\" stop the request from returning objects and records any errors \"\"\"\n+\n+        cb = request.callback\n+\n+        @wraps(cb)\n+        def cb_wrapper(response):\n+            try:\n+                output = cb(response)\n+                output = list(iterate_spider_output(output))\n+            except:\n+                case = _create_testcase(method, 'callback')\n+                results.addError(case, sys.exc_info())\n+\n+        def eb_wrapper(failure):\n+            case = _create_testcase(method, 'errback')\n+            exc_info = failure.value, failure.type, failure.getTracebackObject()\n+            results.addError(case, exc_info)\n+\n+        request.callback = cb_wrapper\n+        request.errback = eb_wrapper\n+\n \n class Contract(object):\n     \"\"\" Abstract class for contracts \"\"\"\n \n     def __init__(self, method, *args):\n-        self.testcase_pre = self.create_testcase(method, 'pre-hook')\n-        self.testcase_post = self.create_testcase(method, 'post-hook')\n+        self.testcase_pre = _create_testcase(method, '@%s pre-hook' % self.name)\n+        self.testcase_post = _create_testcase(method, '@%s post-hook' % self.name)\n         self.args = args\n \n-    def create_testcase(self, method, hook):\n-        spider = method.__self__.name\n-\n-        class ContractTestCase(TestCase):\n-            def __str__(_self):\n-                return \"[%s] %s (@%s %s)\" % (spider, method.__name__, self.name, hook)\n-\n-        name = '%s_%s' % (spider, method.__name__)\n-        setattr(ContractTestCase, name, lambda x: x)\n-        return ContractTestCase(name)\n-\n     def add_pre_hook(self, request, results):\n         if hasattr(self, 'pre_process'):\n             cb = request.callback\n@@ -119,3 +131,15 @@ class Contract(object):\n \n     def adjust_request_args(self, args):\n         return args\n+\n+\n+def _create_testcase(method, desc):\n+    spider = method.__self__.name\n+\n+    class ContractTestCase(TestCase):\n+        def __str__(_self):\n+            return \"[%s] %s (%s)\" % (spider, method.__name__, desc)\n+\n+    name = '%s_%s' % (spider, method.__name__)\n+    setattr(ContractTestCase, name, lambda x: x)\n+    return ContractTestCase(name)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#02ac973a61c9ff9567e13de1481a6bbf4f86e012", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 14 | Churn Cumulative: 216 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,4 +1,4 @@\n-from unittest import TextTestRunner\n+from unittest import TextTestResult\n \n from twisted.trial import unittest\n \n@@ -74,8 +74,7 @@ class ContractsManagerTest(unittest.TestCase):\n \n     def setUp(self):\n         self.conman = ContractsManager(self.contracts)\n-        self.results = TextTestRunner()._makeResult()\n-        self.results.stream = None\n+        self.results = TextTestResult(stream=None, descriptions=False, verbosity=0)\n \n     def should_succeed(self):\n         self.assertFalse(self.results.failures)\n@@ -108,14 +107,12 @@ class ContractsManagerTest(unittest.TestCase):\n \n         # returns_item\n         request = self.conman.from_method(spider.returns_item, self.results)\n-        output = request.callback(response)\n-        self.assertEqual([type(x) for x in output], [TestItem])\n+        request.callback(response)\n         self.should_succeed()\n \n         # returns_request\n         request = self.conman.from_method(spider.returns_request, self.results)\n-        output = request.callback(response)\n-        self.assertEqual([type(x) for x in output], [Request])\n+        request.callback(response)\n         self.should_succeed()\n \n         # returns_fail\n@@ -129,8 +126,7 @@ class ContractsManagerTest(unittest.TestCase):\n \n         # scrapes_item_ok\n         request = self.conman.from_method(spider.scrapes_item_ok, self.results)\n-        output = request.callback(response)\n-        self.assertEqual([type(x) for x in output], [TestItem])\n+        request.callback(response)\n         self.should_succeed()\n \n         # scrapes_item_fail\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1c9effd7b4876f3adda6bbad30e47e48e41d2cb6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 61 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -13,8 +13,11 @@ def start_python_console(namespace=None, noipython=False, banner=''):\n                 raise ImportError()\n \n             try:\n-                from IPython.frontend.terminal.embed import InteractiveShellEmbed\n-                sh = InteractiveShellEmbed(banner1=banner)\n+                try:\n+                    from IPython.terminal import embed\n+                except ImportError:\n+                    from IPython.frontend.terminal import embed\n+                sh = embed.InteractiveShellEmbed(banner1=banner)\n             except ImportError:\n                 from IPython.Shell import IPShellEmbed\n                 sh = IPShellEmbed(banner=banner)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f6d48213f688e2b74923d84a926baaeb4004c0f3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 35 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -11,5 +11,5 @@ class Command(ScrapyCommand):\n \n     def run(self, args, opts):\n         crawler = self.crawler_process.create_crawler()\n-        for s in crawler.spiders.list():\n+        for s in sorted(crawler.spiders.list()):\n             print(s)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#752787e63454eb8732899255d9f4311f71bb4ede", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 70 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 7 | Complexity Δ (Sum/Max): 11/11 | Churn Δ: 70 | Churn Cumulative: 1792 | Contributors (this commit): 11 | Commits (past 90d): 5 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -285,6 +285,69 @@ class FilesystemCacheStorage(object):\n             return pickle.load(f)\n \n \n+class LeveldbCacheStorage(object):\n+\n+    def __init__(self, settings):\n+        import leveldb\n+        self._leveldb = leveldb\n+        self.cachedir = data_path(settings['HTTPCACHE_DIR'], createdir=True)\n+        self.expiration_secs = settings.getint('HTTPCACHE_EXPIRATION_SECS')\n+        self.db = None\n+\n+    def open_spider(self, spider):\n+        dbpath = os.path.join(self.cachedir, '%s.leveldb' % spider.name)\n+        self.db = self._leveldb.LevelDB(dbpath)\n+\n+    def close_spider(self, spider):\n+        del self.db\n+\n+    def retrieve_response(self, spider, request):\n+        data = self._read_data(spider, request)\n+        if data is None:\n+            return  # not cached\n+        url = data['url']\n+        status = data['status']\n+        headers = Headers(data['headers'])\n+        body = data['body']\n+        respcls = responsetypes.from_args(headers=headers, url=url)\n+        response = respcls(url=url, headers=headers, status=status, body=body)\n+        return response\n+\n+    def store_response(self, spider, request, response):\n+        key = self._request_key(request)\n+        data = {\n+            'status': response.status,\n+            'url': response.url,\n+            'headers': dict(response.headers),\n+            'body': response.body,\n+        }\n+        batch = self._leveldb.WriteBatch()\n+        batch.Put('%s_data' % key, pickle.dumps(data, protocol=2))\n+        batch.Put('%s_time' % key, str(time()))\n+        self.db.Write(batch)\n+\n+    def _read_data(self, spider, request):\n+        key = self._request_key(request)\n+        try:\n+            ts = self.db.Get('%s_time' % key)\n+        except KeyError:\n+            return  # not found or invalid entry\n+\n+        if 0 < self.expiration_secs < time() - float(ts):\n+            return  # expired\n+\n+        try:\n+            data = self.db.Get('%s_data' % key)\n+        except KeyError:\n+            return  # invalid entry\n+        else:\n+            return pickle.loads(data)\n+\n+    def _request_key(self, request):\n+        return request_fingerprint(request)\n+\n+\n+\n def parse_cachecontrol(header):\n     \"\"\"Parse Cache-Control header\n \n\n@@ -5,6 +5,7 @@ import shutil\n import unittest\n import email.utils\n from contextlib import contextmanager\n+import pytest\n \n from scrapy.http import Response, HtmlResponse, Request\n from scrapy.spider import Spider\n@@ -136,6 +137,12 @@ class FilesystemStorageTest(DefaultStorageTest):\n     storage_class = 'scrapy.contrib.httpcache.FilesystemCacheStorage'\n \n \n+class LeveldbStorageTest(DefaultStorageTest):\n+\n+    pytest.importorskip('leveldb')\n+    storage_class = 'scrapy.contrib.httpcache.LeveldbCacheStorage'\n+\n+\n class DummyPolicyTest(_BaseTest):\n \n     policy_class = 'scrapy.contrib.httpcache.DummyPolicy'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
