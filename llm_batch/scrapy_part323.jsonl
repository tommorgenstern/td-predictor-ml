{"custom_id": "scrapy#9db01a483c729369b272235bfb6e1c66ff62d1f7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 800 | Contributors (this commit): 9 | Commits (past 90d): 14 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -4,7 +4,7 @@ Spider Middleware manager\n See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n from itertools import islice\n-from typing import Any, Callable, Generator, Iterable, Union, AsyncIterable, AsyncGenerator\n+from typing import Any, AsyncGenerator, AsyncIterable, Callable, Generator, Iterable, Union\n \n from twisted.internet.defer import Deferred\n from twisted.python.failure import Failure\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#77bff0db0a6bdfca15295444f0e0eda47b35e702", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 59 | Lines Deleted: 41 | Files Changed: 1 | Hunks: 34 | Methods Changed: 32 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 100 | Churn Cumulative: 1347 | Contributors (this commit): 24 | Commits (past 90d): 6 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,11 +3,10 @@ extracts information from them\"\"\"\n \n import logging\n from collections import deque\n-from collections.abc import Iterable\n-from typing import Union\n+from typing import Union, Optional, Tuple, Set, Deque, Any, Iterable\n \n from itemadapter import is_item\n-from twisted.internet import defer\n+from twisted.internet.defer import Deferred, inlineCallbacks\n from twisted.python.failure import Failure\n \n from scrapy import signals, Spider\n@@ -20,6 +19,9 @@ from scrapy.utils.misc import load_object, warn_on_generator_with_return_value\n from scrapy.utils.spider import iterate_spider_output\n \n \n+QUEUE_TUPLE = Tuple[Union[Response, Failure], Request, Deferred]\n+\n+\n logger = logging.getLogger(__name__)\n \n \n@@ -28,46 +30,46 @@ class Slot:\n \n     MIN_RESPONSE_SIZE = 1024\n \n-    def __init__(self, max_active_size=5000000):\n+    def __init__(self, max_active_size: int = 5000000):\n         self.max_active_size = max_active_size\n-        self.queue = deque()\n-        self.active = set()\n-        self.active_size = 0\n-        self.itemproc_size = 0\n-        self.closing = None\n+        self.queue: Deque[QUEUE_TUPLE] = deque()\n+        self.active: Set[Request] = set()\n+        self.active_size: int = 0\n+        self.itemproc_size: int = 0\n+        self.closing: Optional[Deferred] = None\n \n-    def add_response_request(self, response, request):\n-        deferred = defer.Deferred()\n-        self.queue.append((response, request, deferred))\n-        if isinstance(response, Response):\n-            self.active_size += max(len(response.body), self.MIN_RESPONSE_SIZE)\n+    def add_response_request(self, result: Union[Response, Failure], request: Request) -> Deferred:\n+        deferred = Deferred()\n+        self.queue.append((result, request, deferred))\n+        if isinstance(result, Response):\n+            self.active_size += max(len(result.body), self.MIN_RESPONSE_SIZE)\n         else:\n             self.active_size += self.MIN_RESPONSE_SIZE\n         return deferred\n \n-    def next_response_request_deferred(self):\n+    def next_response_request_deferred(self) -> QUEUE_TUPLE:\n         response, request, deferred = self.queue.popleft()\n         self.active.add(request)\n         return response, request, deferred\n \n-    def finish_response(self, response, request):\n+    def finish_response(self, result: Union[Response, Failure], request: Request) -> None:\n         self.active.remove(request)\n-        if isinstance(response, Response):\n-            self.active_size -= max(len(response.body), self.MIN_RESPONSE_SIZE)\n+        if isinstance(result, Response):\n+            self.active_size -= max(len(result.body), self.MIN_RESPONSE_SIZE)\n         else:\n             self.active_size -= self.MIN_RESPONSE_SIZE\n \n-    def is_idle(self):\n+    def is_idle(self) -> bool:\n         return not (self.queue or self.active)\n \n-    def needs_backout(self):\n+    def needs_backout(self) -> bool:\n         return self.active_size > self.max_active_size\n \n \n class Scraper:\n \n     def __init__(self, crawler):\n-        self.slot = None\n+        self.slot: Optional[Slot] = None\n         self.spidermw = SpiderMiddlewareManager.from_crawler(crawler)\n         itemproc_cls = load_object(crawler.settings['ITEM_PROCESSOR'])\n         self.itemproc = itemproc_cls.from_crawler(crawler)\n@@ -76,32 +78,37 @@ class Scraper:\n         self.signals = crawler.signals\n         self.logformatter = crawler.logformatter\n \n-    @defer.inlineCallbacks\n-    def open_spider(self, spider):\n+    @inlineCallbacks\n+    def open_spider(self, spider: Spider):\n         \"\"\"Open the given spider for scraping and allocate resources for it\"\"\"\n         self.slot = Slot(self.crawler.settings.getint('SCRAPER_SLOT_MAX_ACTIVE_SIZE'))\n         yield self.itemproc.open_spider(spider)\n \n-    def close_spider(self, spider):\n+    def close_spider(self, spider: Spider) -> Deferred:\n         \"\"\"Close a spider being scraped and release its resources\"\"\"\n-        self.slot.closing = defer.Deferred()\n+        if self.slot is None:\n+            raise RuntimeError(\"Scraper slot not assigned\")\n+        self.slot.closing = Deferred()\n         self.slot.closing.addCallback(self.itemproc.close_spider)\n         self._check_if_closing(spider)\n         return self.slot.closing\n \n-    def is_idle(self):\n+    def is_idle(self) -> bool:\n         \"\"\"Return True if there isn't any more spiders to process\"\"\"\n         return not self.slot\n \n-    def _check_if_closing(self, spider):\n+    def _check_if_closing(self, spider: Spider) -> None:\n+        assert self.slot is not None  # typing\n         if self.slot.closing and self.slot.is_idle():\n             self.slot.closing.callback(spider)\n \n-    def enqueue_scrape(self, response, request, spider):\n-        dfd = self.slot.add_response_request(response, request)\n+    def enqueue_scrape(self, result: Union[Response, Failure], request: Request, spider: Spider) -> Deferred:\n+        if self.slot is None:\n+            raise RuntimeError(\"Scraper slot not assigned\")\n+        dfd = self.slot.add_response_request(result, request)\n \n         def finish_scraping(_):\n-            self.slot.finish_response(response, request)\n+            self.slot.finish_response(result, request)\n             self._check_if_closing(spider)\n             self._scrape_next(spider)\n             return _\n@@ -115,12 +122,13 @@ class Scraper:\n         self._scrape_next(spider)\n         return dfd\n \n-    def _scrape_next(self, spider):\n+    def _scrape_next(self, spider: Spider) -> None:\n+        assert self.slot is not None  # typing\n         while self.slot.queue:\n             response, request, deferred = self.slot.next_response_request_deferred()\n             self._scrape(response, request, spider).chainDeferred(deferred)\n \n-    def _scrape(self, result: Union[Response, Failure], request: Request, spider: Spider):\n+    def _scrape(self, result: Union[Response, Failure], request: Request, spider: Spider) -> Deferred:\n         \"\"\"\n         Handle the downloaded response or failure through the spider callback/errback\n         \"\"\"\n@@ -131,7 +139,7 @@ class Scraper:\n         dfd.addCallback(self.handle_spider_output, request, result, spider)\n         return dfd\n \n-    def _scrape2(self, result: Union[Response, Failure], request: Request, spider: Spider):\n+    def _scrape2(self, result: Union[Response, Failure], request: Request, spider: Spider) -> Deferred:\n         \"\"\"\n         Handle the different cases of request's result been a Response or a Failure\n         \"\"\"\n@@ -141,7 +149,7 @@ class Scraper:\n             dfd = self.call_spider(result, request, spider)\n             return dfd.addErrback(self._log_download_errors, result, request, spider)\n \n-    def call_spider(self, result: Union[Response, Failure], request: Request, spider: Spider):\n+    def call_spider(self, result: Union[Response, Failure], request: Request, spider: Spider) -> Deferred:\n         if isinstance(result, Response):\n             if getattr(result, \"request\", None) is None:\n                 result.request = request\n@@ -156,7 +164,7 @@ class Scraper:\n             dfd.addErrback(request.errback)\n         return dfd.addCallback(iterate_spider_output)\n \n-    def handle_spider_error(self, _failure: Failure, request: Request, response: Response, spider: Spider):\n+    def handle_spider_error(self, _failure: Failure, request: Request, response: Response, spider: Spider) -> None:\n         exc = _failure.value\n         if isinstance(exc, CloseSpider):\n             self.crawler.engine.close_spider(spider, exc.reason or 'cancelled')\n@@ -177,7 +185,7 @@ class Scraper:\n             spider=spider\n         )\n \n-    def handle_spider_output(self, result: Iterable, request: Request, response: Response, spider: Spider):\n+    def handle_spider_output(self, result: Iterable, request: Request, response: Response, spider: Spider) -> Deferred:\n         if not result:\n             return defer_succeed(None)\n         it = iter_errback(result, self.handle_spider_error, request, response, spider)\n@@ -185,10 +193,12 @@ class Scraper:\n                        request, response, spider)\n         return dfd\n \n-    def _process_spidermw_output(self, output, request, response, spider):\n+    def _process_spidermw_output(self, output: Any, request: Request, response: Response,\n+                                 spider: Spider) -> Optional[Deferred]:\n         \"\"\"Process each Request/Item (given in the output parameter) returned\n         from the given spider\n         \"\"\"\n+        assert self.slot is not None  # typing\n         if isinstance(output, Request):\n             self.crawler.engine.crawl(request=output, spider=spider)\n         elif is_item(output):\n@@ -205,12 +215,18 @@ class Scraper:\n                 {'request': request, 'typename': typename},\n                 extra={'spider': spider},\n             )\n+        return None\n \n-    def _log_download_errors(self, spider_failure, download_failure, request, spider):\n+    def _log_download_errors(self, spider_failure: Failure, download_failure: Failure, request: Request,\n+                             spider: Spider) -> Union[Failure, None]:\n         \"\"\"Log and silence errors that come from the engine (typically download\n-        errors that got propagated thru here)\n+        errors that got propagated thru here).\n+\n+        spider_failure: the value passed into the errback of self.call_spider()\n+        download_failure: the value passed into _scrape2() from\n+        ExecutionEngine._handle_downloader_output() as \"result\"\n         \"\"\"\n-        if isinstance(download_failure, Failure) and not download_failure.check(IgnoreRequest):\n+        if not download_failure.check(IgnoreRequest):\n             if download_failure.frames:\n                 logkws = self.logformatter.download_error(download_failure, request, spider)\n                 logger.log(\n@@ -230,10 +246,12 @@ class Scraper:\n \n         if spider_failure is not download_failure:\n             return spider_failure\n+        return None\n \n-    def _itemproc_finished(self, output, item, response, spider):\n+    def _itemproc_finished(self, output: Any, item: Any, response: Response, spider: Spider) -> None:\n         \"\"\"ItemProcessor finished for the given ``item`` and returned ``output``\n         \"\"\"\n+        assert self.slot is not None  # typing\n         self.slot.itemproc_size -= 1\n         if isinstance(output, Failure):\n             ex = output.value\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#309a637f32b0f6196eb4f77f19152527dcf3e5e7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 1355 | Contributors (this commit): 24 | Commits (past 90d): 7 | Contributors (cumulative): 24 | DMM Complexity: None\n\nDIFF:\n@@ -3,7 +3,7 @@ extracts information from them\"\"\"\n \n import logging\n from collections import deque\n-from typing import Union, Optional, Tuple, Set, Deque, Any, Iterable\n+from typing import Any, Deque, Iterable, Optional, Set, Tuple, Union\n \n from itemadapter import is_item\n from twisted.internet.defer import Deferred, inlineCallbacks\n@@ -19,7 +19,7 @@ from scrapy.utils.misc import load_object, warn_on_generator_with_return_value\n from scrapy.utils.spider import iterate_spider_output\n \n \n-QUEUE_TUPLE = Tuple[Union[Response, Failure], Request, Deferred]\n+QueueTuple = Tuple[Union[Response, Failure], Request, Deferred]\n \n \n logger = logging.getLogger(__name__)\n@@ -32,7 +32,7 @@ class Slot:\n \n     def __init__(self, max_active_size: int = 5000000):\n         self.max_active_size = max_active_size\n-        self.queue: Deque[QUEUE_TUPLE] = deque()\n+        self.queue: Deque[QueueTuple] = deque()\n         self.active: Set[Request] = set()\n         self.active_size: int = 0\n         self.itemproc_size: int = 0\n@@ -47,7 +47,7 @@ class Slot:\n             self.active_size += self.MIN_RESPONSE_SIZE\n         return deferred\n \n-    def next_response_request_deferred(self) -> QUEUE_TUPLE:\n+    def next_response_request_deferred(self) -> QueueTuple:\n         response, request, deferred = self.queue.popleft()\n         self.active.add(request)\n         return response, request, deferred\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7e23677b52b659b11471a63f3be9905a0bbaf995", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 323 | Lines Deleted: 211 | Files Changed: 9 | Hunks: 96 | Methods Changed: 84 | Complexity Δ (Sum/Max): 12/8 | Churn Δ: 534 | Churn Cumulative: 6827 | Contributors (this commit): 56 | Commits (past 90d): 19 | Contributors (cumulative): 138 | DMM Complexity: 0.8770491803278688\n\nDIFF:\n@@ -1,51 +1,61 @@\n \"\"\"\n-This is the Scrapy engine which controls the Scheduler, Downloader and Spiders.\n+This is the Scrapy engine which controls the Scheduler, Downloader and Spider.\n \n For more information see docs/topics/architecture.rst\n \n \"\"\"\n import logging\n+import warnings\n from time import time\n+from typing import Callable, Iterable, Iterator, Optional, Set, Union\n \n-from twisted.internet import defer, task\n+from twisted.internet.defer import Deferred, inlineCallbacks, succeed\n+from twisted.internet.task import LoopingCall\n from twisted.python.failure import Failure\n \n from scrapy import signals\n from scrapy.core.scraper import Scraper\n-from scrapy.exceptions import DontCloseSpider\n+from scrapy.exceptions import DontCloseSpider, ScrapyDeprecationWarning\n from scrapy.http import Response, Request\n-from scrapy.utils.misc import load_object\n-from scrapy.utils.reactor import CallLaterOnce\n+from scrapy.spiders import Spider\n from scrapy.utils.log import logformatter_adapter, failure_to_exc_info\n+from scrapy.utils.misc import create_instance, load_object\n+from scrapy.utils.reactor import CallLaterOnce\n+\n \n logger = logging.getLogger(__name__)\n \n \n class Slot:\n-\n-    def __init__(self, start_requests, close_if_idle, nextcall, scheduler):\n-        self.closing = False\n-        self.inprogress = set()  # requests in progress\n-        self.start_requests = iter(start_requests)\n+    def __init__(\n+        self,\n+        start_requests: Iterable,\n+        close_if_idle: bool,\n+        nextcall: CallLaterOnce,\n+        scheduler,\n+    ) -> None:\n+        self.closing: Optional[Deferred] = None\n+        self.inprogress: Set[Request] = set()\n+        self.start_requests: Optional[Iterator] = iter(start_requests)\n         self.close_if_idle = close_if_idle\n         self.nextcall = nextcall\n         self.scheduler = scheduler\n-        self.heartbeat = task.LoopingCall(nextcall.schedule)\n+        self.heartbeat = LoopingCall(nextcall.schedule)\n \n-    def add_request(self, request):\n+    def add_request(self, request: Request) -> None:\n         self.inprogress.add(request)\n \n-    def remove_request(self, request):\n+    def remove_request(self, request: Request) -> None:\n         self.inprogress.remove(request)\n         self._maybe_fire_closing()\n \n-    def close(self):\n-        self.closing = defer.Deferred()\n+    def close(self) -> Deferred:\n+        self.closing = Deferred()\n         self._maybe_fire_closing()\n         return self.closing\n \n-    def _maybe_fire_closing(self):\n-        if self.closing and not self.inprogress:\n+    def _maybe_fire_closing(self) -> None:\n+        if self.closing is not None and not self.inprogress:\n             if self.nextcall:\n                 self.nextcall.cancel()\n                 if self.heartbeat.running:\n@@ -54,210 +64,224 @@ class Slot:\n \n \n class ExecutionEngine:\n-\n-    def __init__(self, crawler, spider_closed_callback):\n+    def __init__(self, crawler, spider_closed_callback: Callable) -> None:\n         self.crawler = crawler\n         self.settings = crawler.settings\n         self.signals = crawler.signals\n         self.logformatter = crawler.logformatter\n-        self.slot = None\n-        self.spider = None\n+        self.slot: Optional[Slot] = None\n+        self.spider: Optional[Spider] = None\n         self.running = False\n         self.paused = False\n-        self.scheduler_cls = load_object(self.settings['SCHEDULER'])\n+        self.scheduler_cls = load_object(crawler.settings[\"SCHEDULER\"])\n         downloader_cls = load_object(self.settings['DOWNLOADER'])\n         self.downloader = downloader_cls(crawler)\n         self.scraper = Scraper(crawler)\n         self._spider_closed_callback = spider_closed_callback\n \n-    @defer.inlineCallbacks\n-    def start(self):\n-        \"\"\"Start the execution engine\"\"\"\n+    @inlineCallbacks\n+    def start(self) -> Deferred:\n         if self.running:\n             raise RuntimeError(\"Engine already running\")\n         self.start_time = time()\n         yield self.signals.send_catch_log_deferred(signal=signals.engine_started)\n         self.running = True\n-        self._closewait = defer.Deferred()\n+        self._closewait = Deferred()\n         yield self._closewait\n \n-    def stop(self):\n-        \"\"\"Stop the execution engine gracefully\"\"\"\n+    def stop(self) -> Deferred:\n+        \"\"\"Gracefully stop the execution engine\"\"\"\n+        @inlineCallbacks\n+        def _finish_stopping_engine(_) -> Deferred:\n+            yield self.signals.send_catch_log_deferred(signal=signals.engine_stopped)\n+            self._closewait.callback(None)\n+\n         if not self.running:\n             raise RuntimeError(\"Engine not running\")\n+\n         self.running = False\n-        dfd = self._close_all_spiders()\n-        return dfd.addBoth(lambda _: self._finish_stopping_engine())\n+        dfd = self.close_spider(self.spider, reason=\"shutdown\") if self.spider is not None else succeed(None)\n+        return dfd.addBoth(_finish_stopping_engine)\n \n-    def close(self):\n-        \"\"\"Close the execution engine gracefully.\n-\n-        If it has already been started, stop it. In all cases, close all spiders\n-        and the downloader.\n+    def close(self) -> Deferred:\n+        \"\"\"\n+        Gracefully close the execution engine.\n+        If it has already been started, stop it. In all cases, close the spider and the downloader.\n         \"\"\"\n         if self.running:\n-            # Will also close spiders and downloader\n-            return self.stop()\n-        elif self.open_spiders:\n-            # Will also close downloader\n-            return self._close_all_spiders()\n-        else:\n-            return defer.succeed(self.downloader.close())\n+            return self.stop()  # will also close spider and downloader\n+        if self.spider is not None:\n+            return self.close_spider(self.spider, reason=\"shutdown\")  # will also close downloader\n+        return succeed(self.downloader.close())\n \n-    def pause(self):\n-        \"\"\"Pause the execution engine\"\"\"\n+    def pause(self) -> None:\n         self.paused = True\n \n-    def unpause(self):\n-        \"\"\"Resume the execution engine\"\"\"\n+    def unpause(self) -> None:\n         self.paused = False\n \n-    def _next_request(self, spider):\n-        slot = self.slot\n-        if not slot:\n-            return\n+    def _next_request(self) -> None:\n+        assert self.slot is not None  # typing\n+        assert self.spider is not None  # typing\n \n         if self.paused:\n-            return\n+            return None\n \n-        while not self._needs_backout(spider):\n-            if not self._next_request_from_scheduler(spider):\n-                break\n+        while not self._needs_backout() and self._next_request_from_scheduler() is not None:\n+            pass\n \n-        if slot.start_requests and not self._needs_backout(spider):\n+        if self.slot.start_requests is not None and not self._needs_backout():\n             try:\n-                request = next(slot.start_requests)\n+                request = next(self.slot.start_requests)\n             except StopIteration:\n-                slot.start_requests = None\n+                self.slot.start_requests = None\n             except Exception:\n-                slot.start_requests = None\n-                logger.error('Error while obtaining start requests',\n-                             exc_info=True, extra={'spider': spider})\n+                self.slot.start_requests = None\n+                logger.error('Error while obtaining start requests', exc_info=True, extra={'spider': self.spider})\n             else:\n-                self.crawl(request, spider)\n+                self.crawl(request)\n \n-        if self.spider_is_idle(spider) and slot.close_if_idle:\n-            self._spider_idle(spider)\n+        if self.spider_is_idle() and self.slot.close_if_idle:\n+            self._spider_idle()\n \n-    def _needs_backout(self, spider):\n-        slot = self.slot\n+    def _needs_backout(self) -> bool:\n         return (\n             not self.running\n-            or slot.closing\n+            or self.slot.closing  # type: ignore[union-attr]\n             or self.downloader.needs_backout()\n-            or self.scraper.slot.needs_backout()\n+            or self.scraper.slot.needs_backout()  # type: ignore[union-attr]\n         )\n \n-    def _next_request_from_scheduler(self, spider):\n-        slot = self.slot\n-        request = slot.scheduler.next_request()\n-        if not request:\n-            return\n-        d = self._download(request, spider)\n-        d.addBoth(self._handle_downloader_output, request, spider)\n+    def _next_request_from_scheduler(self) -> Optional[Deferred]:\n+        assert self.slot is not None  # typing\n+        assert self.spider is not None  # typing\n+\n+        request = self.slot.scheduler.next_request()\n+        if request is None:\n+            return None\n+\n+        d = self._download(request, self.spider)\n+        d.addBoth(self._handle_downloader_output, request, self.spider)\n         d.addErrback(lambda f: logger.info('Error while handling downloader output',\n                                            exc_info=failure_to_exc_info(f),\n-                                           extra={'spider': spider}))\n-        d.addBoth(lambda _: slot.remove_request(request))\n+                                           extra={'spider': self.spider}))\n+        d.addBoth(lambda _: self.slot.remove_request(request))\n         d.addErrback(lambda f: logger.info('Error while removing request from slot',\n                                            exc_info=failure_to_exc_info(f),\n-                                           extra={'spider': spider}))\n-        d.addBoth(lambda _: slot.nextcall.schedule())\n+                                           extra={'spider': self.spider}))\n+        d.addBoth(lambda _: self.slot.nextcall.schedule())\n         d.addErrback(lambda f: logger.info('Error while scheduling new request',\n                                            exc_info=failure_to_exc_info(f),\n-                                           extra={'spider': spider}))\n+                                           extra={'spider': self.spider}))\n         return d\n \n-    def _handle_downloader_output(self, response, request, spider):\n-        if not isinstance(response, (Request, Response, Failure)):\n-            raise TypeError(\n-                \"Incorrect type: expected Request, Response or Failure, got \"\n-                f\"{type(response)}: {response!r}\"\n-            )\n+    def _handle_downloader_output(\n+        self, result: Union[Request, Response, Failure], request: Request, spider: Spider\n+    ) -> Optional[Deferred]:\n+        if not isinstance(result, (Request, Response, Failure)):\n+            raise TypeError(f\"Incorrect type: expected Request, Response or Failure, got {type(result)}: {result!r}\")\n+\n         # downloader middleware can return requests (for example, redirects)\n-        if isinstance(response, Request):\n-            self.crawl(response, spider)\n-            return\n-        # response is a Response or Failure\n-        d = self.scraper.enqueue_scrape(response, request, spider)\n-        d.addErrback(lambda f: logger.error('Error while enqueuing downloader output',\n+        if isinstance(result, Request):\n+            self.crawl(result)\n+            return None\n+\n+        d = self.scraper.enqueue_scrape(result, request, spider)\n+        d.addErrback(\n+            lambda f: logger.error(\n+                \"Error while enqueuing downloader output\",\n                 exc_info=failure_to_exc_info(f),\n-                                            extra={'spider': spider}))\n+                extra={'spider': spider},\n+            )\n+        )\n         return d\n \n-    def spider_is_idle(self, spider):\n-        if not self.scraper.slot.is_idle():\n-            # scraper is not idle\n+    def spider_is_idle(self, spider: Optional[Spider] = None) -> bool:\n+        if spider is not None:\n+            warnings.warn(\n+                \"Passing a 'spider' argument to ExecutionEngine.spider_is_idle is deprecated\",\n+                category=ScrapyDeprecationWarning,\n+                stacklevel=2,\n+            )\n+        if self.slot is None:\n+            raise RuntimeError(\"Engine slot not assigned\")\n+        if not self.scraper.slot.is_idle():  # type: ignore[union-attr]\n             return False\n-\n-        if self.downloader.active:\n-            # downloader has pending requests\n+        if self.downloader.active:  # downloader has pending requests\n             return False\n-\n-        if self.slot.start_requests is not None:\n-            # not all start requests are handled\n+        if self.slot.start_requests is not None:  # not all start requests are handled\n             return False\n-\n         if self.slot.scheduler.has_pending_requests():\n-            # scheduler has pending requests\n             return False\n-\n         return True\n \n-    @property\n-    def open_spiders(self):\n-        return [self.spider] if self.spider else []\n+    def crawl(self, request: Request, spider: Optional[Spider] = None) -> None:\n+        \"\"\"Inject the request into the spider <-> downloader pipeline\"\"\"\n+        if spider is not None:\n+            warnings.warn(\n+                \"Passing a 'spider' argument to ExecutionEngine.crawl is deprecated\",\n+                category=ScrapyDeprecationWarning,\n+                stacklevel=2,\n+            )\n+            if spider is not self.spider:\n+                raise RuntimeError(f\"The spider {spider.name!r} does not match the open spider\")\n+        if self.spider is None:\n+            raise RuntimeError(f\"No open spider to crawl: {request}\")\n+        self._schedule_request(request, self.spider)\n+        self.slot.nextcall.schedule()  # type: ignore[union-attr]\n \n-    def has_capacity(self):\n-        \"\"\"Does the engine have capacity to handle more spiders\"\"\"\n-        return not bool(self.slot)\n-\n-    def crawl(self, request, spider):\n-        if spider not in self.open_spiders:\n-            raise RuntimeError(f\"Spider {spider.name!r} not opened when crawling: {request}\")\n-        self.schedule(request, spider)\n-        self.slot.nextcall.schedule()\n-\n-    def schedule(self, request, spider):\n+    def _schedule_request(self, request: Request, spider: Spider) -> None:\n         self.signals.send_catch_log(signals.request_scheduled, request=request, spider=spider)\n-        if not self.slot.scheduler.enqueue_request(request):\n+        if not self.slot.scheduler.enqueue_request(request):  # type: ignore[union-attr]\n             self.signals.send_catch_log(signals.request_dropped, request=request, spider=spider)\n \n-    def download(self, request, spider):\n-        d = self._download(request, spider)\n-        d.addBoth(self._downloaded, self.slot, request, spider)\n-        return d\n-\n-    def _downloaded(self, response, slot, request, spider):\n-        slot.remove_request(request)\n-        return self.download(response, spider) if isinstance(response, Request) else response\n-\n-    def _download(self, request, spider):\n-        slot = self.slot\n-        slot.add_request(request)\n-\n-        def _on_success(response):\n-            if not isinstance(response, (Response, Request)):\n-                raise TypeError(\n-                    \"Incorrect type: expected Response or Request, got \"\n-                    f\"{type(response)}: {response!r}\"\n+    def download(self, request: Request, spider: Optional[Spider] = None) -> Deferred:\n+        \"\"\"Return a Deferred which fires with a Response as result, only downloader middlewares are applied\"\"\"\n+        if spider is None:\n+            spider = self.spider\n+        else:\n+            warnings.warn(\n+                \"Passing a 'spider' argument to ExecutionEngine.download is deprecated\",\n+                category=ScrapyDeprecationWarning,\n+                stacklevel=2,\n             )\n-            if isinstance(response, Response):\n-                if response.request is None:\n-                    response.request = request\n-                logkws = self.logformatter.crawled(response.request, response, spider)\n+            if spider is not self.spider:\n+                logger.warning(\"The spider '%s' does not match the open spider\", spider.name)\n+        if spider is None:\n+            raise RuntimeError(f\"No open spider to crawl: {request}\")\n+        return self._download(request, spider).addBoth(self._downloaded, request, spider)\n+\n+    def _downloaded(\n+        self, result: Union[Response, Request], request: Request, spider: Spider\n+    ) -> Union[Deferred, Response]:\n+        assert self.slot is not None  # typing\n+        self.slot.remove_request(request)\n+        return self.download(result, spider) if isinstance(result, Request) else result\n+\n+    def _download(self, request: Request, spider: Spider) -> Deferred:\n+        assert self.slot is not None  # typing\n+\n+        self.slot.add_request(request)\n+\n+        def _on_success(result: Union[Response, Request]) -> Union[Response, Request]:\n+            if not isinstance(result, (Response, Request)):\n+                raise TypeError(f\"Incorrect type: expected Response or Request, got {type(result)}: {result!r}\")\n+            if isinstance(result, Response):\n+                if result.request is None:\n+                    result.request = request\n+                logkws = self.logformatter.crawled(result.request, result, spider)\n                 if logkws is not None:\n-                    logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n+                    logger.log(*logformatter_adapter(logkws), extra={\"spider\": spider})\n                 self.signals.send_catch_log(\n                     signal=signals.response_received,\n-                    response=response,\n-                    request=response.request,\n+                    response=result,\n+                    request=result.request,\n                     spider=spider,\n                 )\n-            return response\n+            return result\n \n         def _on_complete(_):\n-            slot.nextcall.schedule()\n+            self.slot.nextcall.schedule()\n             return _\n \n         dwld = self.downloader.fetch(request, spider)\n@@ -265,58 +289,52 @@ class ExecutionEngine:\n         dwld.addBoth(_on_complete)\n         return dwld\n \n-    @defer.inlineCallbacks\n-    def open_spider(self, spider, start_requests=(), close_if_idle=True):\n-        if not self.has_capacity():\n+    @inlineCallbacks\n+    def open_spider(self, spider: Spider, start_requests: Iterable = (), close_if_idle: bool = True):\n+        if self.slot is not None:\n             raise RuntimeError(f\"No free spider slot when opening {spider.name!r}\")\n         logger.info(\"Spider opened\", extra={'spider': spider})\n-        nextcall = CallLaterOnce(self._next_request, spider)\n-        scheduler = self.scheduler_cls.from_crawler(self.crawler)\n+        nextcall = CallLaterOnce(self._next_request)\n+        scheduler = create_instance(self.scheduler_cls, settings=None, crawler=self.crawler)\n         start_requests = yield self.scraper.spidermw.process_start_requests(start_requests, spider)\n-        slot = Slot(start_requests, close_if_idle, nextcall, scheduler)\n-        self.slot = slot\n+        self.slot = Slot(start_requests, close_if_idle, nextcall, scheduler)\n         self.spider = spider\n         yield scheduler.open(spider)\n         yield self.scraper.open_spider(spider)\n         self.crawler.stats.open_spider(spider)\n         yield self.signals.send_catch_log_deferred(signals.spider_opened, spider=spider)\n-        slot.nextcall.schedule()\n-        slot.heartbeat.start(5)\n+        self.slot.nextcall.schedule()\n+        self.slot.heartbeat.start(5)\n \n-    def _spider_idle(self, spider):\n-        \"\"\"Called when a spider gets idle. This function is called when there\n-        are no remaining pages to download or schedule. It can be called\n-        multiple times. If some extension raises a DontCloseSpider exception\n-        (in the spider_idle signal handler) the spider is not closed until the\n-        next loop and this function is guaranteed to be called (at least) once\n-        again for this spider.\n+    def _spider_idle(self) -> None:\n         \"\"\"\n-        res = self.signals.send_catch_log(signals.spider_idle, spider=spider, dont_log=DontCloseSpider)\n+        Called when a spider gets idle, i.e. when there are no remaining requests to download or schedule.\n+        It can be called multiple times. If a handler for the spider_idle signal raises a DontCloseSpider\n+        exception, the spider is not closed until the next loop and this function is guaranteed to be called\n+        (at least) once again.\n+        \"\"\"\n+        assert self.spider is not None  # typing\n+        res = self.signals.send_catch_log(signals.spider_idle, spider=self.spider, dont_log=DontCloseSpider)\n         if any(isinstance(x, Failure) and isinstance(x.value, DontCloseSpider) for _, x in res):\n-            return\n+            return None\n+        if self.spider_is_idle():\n+            self.close_spider(self.spider, reason='finished')\n \n-        if self.spider_is_idle(spider):\n-            self.close_spider(spider, reason='finished')\n-\n-    def close_spider(self, spider, reason='cancelled'):\n+    def close_spider(self, spider: Spider, reason: str = \"cancelled\") -> Deferred:\n         \"\"\"Close (cancel) spider and clear all its outstanding requests\"\"\"\n+        if self.slot is None:\n+            raise RuntimeError(\"Engine slot not assigned\")\n \n-        slot = self.slot\n-        if slot.closing:\n-            return slot.closing\n-        logger.info(\"Closing spider (%(reason)s)\",\n-                    {'reason': reason},\n-                    extra={'spider': spider})\n+        if self.slot.closing is not None:\n+            return self.slot.closing\n \n-        dfd = slot.close()\n+        logger.info(\"Closing spider (%(reason)s)\", {'reason': reason}, extra={'spider': spider})\n \n-        def log_failure(msg):\n-            def errback(failure):\n-                logger.error(\n-                    msg,\n-                    exc_info=failure_to_exc_info(failure),\n-                    extra={'spider': spider}\n-                )\n+        dfd = self.slot.close()\n+\n+        def log_failure(msg: str) -> Callable:\n+            def errback(failure: Failure) -> None:\n+                logger.error(msg, exc_info=failure_to_exc_info(failure), extra={'spider': spider})\n             return errback\n \n         dfd.addBoth(lambda _: self.downloader.close())\n@@ -325,19 +343,18 @@ class ExecutionEngine:\n         dfd.addBoth(lambda _: self.scraper.close_spider(spider))\n         dfd.addErrback(log_failure('Scraper close failure'))\n \n-        dfd.addBoth(lambda _: slot.scheduler.close(reason))\n+        dfd.addBoth(lambda _: self.slot.scheduler.close(reason))\n         dfd.addErrback(log_failure('Scheduler close failure'))\n \n         dfd.addBoth(lambda _: self.signals.send_catch_log_deferred(\n-            signal=signals.spider_closed, spider=spider, reason=reason))\n+            signal=signals.spider_closed, spider=spider, reason=reason,\n+        ))\n         dfd.addErrback(log_failure('Error while sending spider_close signal'))\n \n         dfd.addBoth(lambda _: self.crawler.stats.close_spider(spider, reason=reason))\n         dfd.addErrback(log_failure('Stats close failure'))\n \n-        dfd.addBoth(lambda _: logger.info(\"Spider closed (%(reason)s)\",\n-                                          {'reason': reason},\n-                                          extra={'spider': spider}))\n+        dfd.addBoth(lambda _: logger.info(\"Spider closed (%(reason)s)\", {'reason': reason}, extra={'spider': spider}))\n \n         dfd.addBoth(lambda _: setattr(self, 'slot', None))\n         dfd.addErrback(log_failure('Error while unassigning slot'))\n@@ -349,12 +366,26 @@ class ExecutionEngine:\n \n         return dfd\n \n-    def _close_all_spiders(self):\n-        dfds = [self.close_spider(s, reason='shutdown') for s in self.open_spiders]\n-        dlist = defer.DeferredList(dfds)\n-        return dlist\n+    @property\n+    def open_spiders(self) -> list:\n+        warnings.warn(\n+            \"ExecutionEngine.open_spiders is deprecated, please use ExecutionEngine.spider instead\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return [self.spider] if self.spider is not None else []\n \n-    @defer.inlineCallbacks\n-    def _finish_stopping_engine(self):\n-        yield self.signals.send_catch_log_deferred(signal=signals.engine_stopped)\n-        self._closewait.callback(None)\n+    def has_capacity(self) -> bool:\n+        warnings.warn(\"ExecutionEngine.has_capacity is deprecated\", ScrapyDeprecationWarning, stacklevel=2)\n+        return not bool(self.slot)\n+\n+    def schedule(self, request: Request, spider: Spider) -> None:\n+        warnings.warn(\n+            \"ExecutionEngine.schedule is deprecated, please use \"\n+            \"ExecutionEngine.crawl or ExecutionEngine.download instead\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        if self.slot is None:\n+            raise RuntimeError(\"Engine slot not assigned\")\n+        self._schedule_request(request, spider)\n\n@@ -200,7 +200,7 @@ class Scraper:\n         \"\"\"\n         assert self.slot is not None  # typing\n         if isinstance(output, Request):\n-            self.crawler.engine.crawl(request=output, spider=spider)\n+            self.crawler.engine.crawl(request=output)\n         elif is_item(output):\n             self.slot.itemproc_size += 1\n             dfd = self.itemproc.process_item(output, spider)\n\n@@ -67,7 +67,7 @@ class RobotsTxtMiddleware:\n                 priority=self.DOWNLOAD_PRIORITY,\n                 meta={'dont_obey_robotstxt': True}\n             )\n-            dfd = self.crawler.engine.download(robotsreq, spider)\n+            dfd = self.crawler.engine.download(robotsreq)\n             dfd.addCallback(self._parse_robots, netloc, spider)\n             dfd.addErrback(self._logerror, robotsreq, spider)\n             dfd.addErrback(self._robots_error, netloc)\n\n@@ -88,10 +88,8 @@ class MemoryUsage:\n                 self._send_report(self.notify_mails, subj)\n                 self.crawler.stats.set_value('memusage/limit_notified', 1)\n \n-            open_spiders = self.crawler.engine.open_spiders\n-            if open_spiders:\n-                for spider in open_spiders:\n-                    self.crawler.engine.close_spider(spider, 'memusage_exceeded')\n+            if self.crawler.engine.spider is not None:\n+                self.crawler.engine.close_spider(self.crawler.engine.spider, 'memusage_exceeded')\n             else:\n                 self.crawler.stop()\n \n\n@@ -173,7 +173,7 @@ class MediaPipeline:\n                 errback=self.media_failed, errbackArgs=(request, info))\n         else:\n             self._modify_media_request(request)\n-            dfd = self.crawler.engine.download(request, info.spider)\n+            dfd = self.crawler.engine.download(request)\n             dfd.addCallbacks(\n                 callback=self.media_downloaded, callbackArgs=(request, info), callbackKeywords={'item': item},\n                 errback=self.media_failed, errbackArgs=(request, info))\n\n@@ -79,7 +79,7 @@ class Shell:\n         spider = self._open_spider(request, spider)\n         d = _request_deferred(request)\n         d.addCallback(lambda x: (x, spider))\n-        self.crawler.engine.crawl(request, spider)\n+        self.crawler.engine.crawl(request)\n         return d\n \n     def _open_spider(self, request, spider):\n\n@@ -8,11 +8,10 @@ def get_engine_status(engine):\n     \"\"\"Return a report of the current engine status\"\"\"\n     tests = [\n         \"time()-engine.start_time\",\n-        \"engine.has_capacity()\",\n         \"len(engine.downloader.active)\",\n         \"engine.scraper.is_idle()\",\n         \"engine.spider.name\",\n-        \"engine.spider_is_idle(engine.spider)\",\n+        \"engine.spider_is_idle()\",\n         \"engine.slot.closing\",\n         \"len(engine.slot.inprogress)\",\n         \"len(engine.slot.scheduler.dqs or [])\",\n\n@@ -42,7 +42,7 @@ Disallow: /some/randome/page.html\n \"\"\".encode('utf-8')\n         response = TextResponse('http://site.local/robots.txt', body=ROBOTS)\n \n-        def return_response(request, spider):\n+        def return_response(request):\n             deferred = Deferred()\n             reactor.callFromThread(deferred.callback, response)\n             return deferred\n@@ -79,7 +79,7 @@ Disallow: /some/randome/page.html\n         crawler.settings.set('ROBOTSTXT_OBEY', True)\n         response = Response('http://site.local/robots.txt', body=b'GIF89a\\xd3\\x00\\xfe\\x00\\xa2')\n \n-        def return_response(request, spider):\n+        def return_response(request):\n             deferred = Deferred()\n             reactor.callFromThread(deferred.callback, response)\n             return deferred\n@@ -102,7 +102,7 @@ Disallow: /some/randome/page.html\n         crawler.settings.set('ROBOTSTXT_OBEY', True)\n         response = Response('http://site.local/robots.txt')\n \n-        def return_response(request, spider):\n+        def return_response(request):\n             deferred = Deferred()\n             reactor.callFromThread(deferred.callback, response)\n             return deferred\n@@ -122,7 +122,7 @@ Disallow: /some/randome/page.html\n         self.crawler.settings.set('ROBOTSTXT_OBEY', True)\n         err = error.DNSLookupError('Robotstxt address not found')\n \n-        def return_failure(request, spider):\n+        def return_failure(request):\n             deferred = Deferred()\n             reactor.callFromThread(deferred.errback, failure.Failure(err))\n             return deferred\n@@ -138,7 +138,7 @@ Disallow: /some/randome/page.html\n         self.crawler.settings.set('ROBOTSTXT_OBEY', True)\n         err = error.DNSLookupError('Robotstxt address not found')\n \n-        def immediate_failure(request, spider):\n+        def immediate_failure(request):\n             deferred = Deferred()\n             deferred.errback(failure.Failure(err))\n             return deferred\n@@ -150,7 +150,7 @@ Disallow: /some/randome/page.html\n     def test_ignore_robotstxt_request(self):\n         self.crawler.settings.set('ROBOTSTXT_OBEY', True)\n \n-        def ignore_request(request, spider):\n+        def ignore_request(request):\n             deferred = Deferred()\n             reactor.callFromThread(deferred.errback, failure.Failure(IgnoreRequest()))\n             return deferred\n\n@@ -13,6 +13,7 @@ module with the ``runserver`` argument::\n import os\n import re\n import sys\n+import warnings\n from collections import defaultdict\n from urllib.parse import urlparse\n \n@@ -25,6 +26,7 @@ from twisted.web import server, static, util\n \n from scrapy import signals\n from scrapy.core.engine import ExecutionEngine\n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Request\n from scrapy.item import Item, Field\n from scrapy.linkextractors import LinkExtractor\n@@ -381,16 +383,33 @@ class EngineTest(unittest.TestCase):\n         e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)\n         yield e.close()\n \n+    @defer.inlineCallbacks\n+    def test_start_already_running_exception(self):\n+        e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)\n+        yield e.open_spider(TestSpider(), [])\n+        e.start()\n+        yield self.assertFailure(e.start(), RuntimeError).addBoth(\n+            lambda exc: self.assertEqual(str(exc), \"Engine already running\")\n+        )\n+        yield e.stop()\n+\n     @defer.inlineCallbacks\n     def test_close_spiders_downloader(self):\n+        with warnings.catch_warnings(record=True) as warning_list:\n             e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)\n             yield e.open_spider(TestSpider(), [])\n             self.assertEqual(len(e.open_spiders), 1)\n             yield e.close()\n             self.assertEqual(len(e.open_spiders), 0)\n+            self.assertEqual(warning_list[0].category, ScrapyDeprecationWarning)\n+            self.assertEqual(\n+                str(warning_list[0].message),\n+                \"ExecutionEngine.open_spiders is deprecated, please use ExecutionEngine.spider instead\",\n+            )\n \n     @defer.inlineCallbacks\n     def test_close_engine_spiders_downloader(self):\n+        with warnings.catch_warnings(record=True) as warning_list:\n             e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)\n             yield e.open_spider(TestSpider(), [])\n             e.start()\n@@ -398,6 +417,71 @@ class EngineTest(unittest.TestCase):\n             yield e.close()\n             self.assertFalse(e.running)\n             self.assertEqual(len(e.open_spiders), 0)\n+            self.assertEqual(warning_list[0].category, ScrapyDeprecationWarning)\n+            self.assertEqual(\n+                str(warning_list[0].message),\n+                \"ExecutionEngine.open_spiders is deprecated, please use ExecutionEngine.spider instead\",\n+            )\n+\n+    @defer.inlineCallbacks\n+    def test_crawl_deprecated_spider_arg(self):\n+        with warnings.catch_warnings(record=True) as warning_list:\n+            e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)\n+            spider = TestSpider()\n+            yield e.open_spider(spider, [])\n+            e.start()\n+            e.crawl(Request(\"data:,\"), spider)\n+            yield e.close()\n+            self.assertEqual(warning_list[0].category, ScrapyDeprecationWarning)\n+            self.assertEqual(\n+                str(warning_list[0].message),\n+                \"Passing a 'spider' argument to ExecutionEngine.crawl is deprecated\",\n+            )\n+\n+    @defer.inlineCallbacks\n+    def test_download_deprecated_spider_arg(self):\n+        with warnings.catch_warnings(record=True) as warning_list:\n+            e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)\n+            spider = TestSpider()\n+            yield e.open_spider(spider, [])\n+            e.start()\n+            e.download(Request(\"data:,\"), spider)\n+            yield e.close()\n+            self.assertEqual(warning_list[0].category, ScrapyDeprecationWarning)\n+            self.assertEqual(\n+                str(warning_list[0].message),\n+                \"Passing a 'spider' argument to ExecutionEngine.download is deprecated\",\n+            )\n+\n+    @defer.inlineCallbacks\n+    def test_deprecated_schedule(self):\n+        with warnings.catch_warnings(record=True) as warning_list:\n+            e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)\n+            spider = TestSpider()\n+            yield e.open_spider(spider, [])\n+            e.start()\n+            e.schedule(Request(\"data:,\"), spider)\n+            yield e.close()\n+            self.assertEqual(warning_list[0].category, ScrapyDeprecationWarning)\n+            self.assertEqual(\n+                str(warning_list[0].message),\n+                \"ExecutionEngine.schedule is deprecated, please use \"\n+                \"ExecutionEngine.crawl or ExecutionEngine.download instead\",\n+            )\n+\n+    @defer.inlineCallbacks\n+    def test_deprecated_has_capacity(self):\n+        with warnings.catch_warnings(record=True) as warning_list:\n+            e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)\n+            self.assertTrue(e.has_capacity())\n+            spider = TestSpider()\n+            yield e.open_spider(spider, [])\n+            self.assertFalse(e.has_capacity())\n+            e.start()\n+            yield e.close()\n+            self.assertTrue(e.has_capacity())\n+            self.assertEqual(warning_list[0].category, ScrapyDeprecationWarning)\n+            self.assertEqual(str(warning_list[0].message), \"ExecutionEngine.has_capacity is deprecated\")\n \n \n if __name__ == \"__main__\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
