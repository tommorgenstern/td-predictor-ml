{"custom_id": "keras#37dec90af9ed7e713e0dee992137ce76bf5af4d7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 66 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,7 +5,7 @@ from keras_core import testing\n \n \n def example_fn(x):\n-    x = (x + 2) * backend.numpy.ones(x.shape)\n+    x = (x + 2) * backend.numpy.ones_like(x)\n     x = backend.numpy.stack([x, x], axis=-1)\n     return x\n \n@@ -54,3 +54,11 @@ class ComputeOutputSpecTest(testing.TestCase):\n         self.assertTrue(isinstance(out, backend.KerasTensor))\n         self.assertTrue(instance.canary)\n         self.assertEqual(out.shape, (2, 3, 2))\n+\n+        instance = Container()\n+        out = backend.compute_output_spec(\n+            instance.example_meta_fn, backend.KerasTensor((2, None))\n+        )\n+        self.assertTrue(isinstance(out, backend.KerasTensor))\n+        self.assertTrue(instance.canary)\n+        self.assertEqual(out.shape, (2, None, 2))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ecc030607792bb3d9d767428b5b00f6bf4ba62e8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 742 | Contributors (this commit): 8 | Commits (past 90d): 45 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -345,4 +345,6 @@ def while_loop(\n \n \n def stop_gradient(variable):\n-    return variable.requires_grad_(False)\n+    # We can't use `.requires_grad_(False)` here since it only\n+    # works when the tensor is a leaf node in the graph.\n+    return variable.detach()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#550f5bf6ab2d6dad6f6778055ed34b6178c96a8e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 6 | Files Changed: 4 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 11 | Churn Cumulative: 5029 | Contributors (this commit): 13 | Commits (past 90d): 158 | Contributors (cumulative): 28 | DMM Complexity: 1.0\n\nDIFF:\n@@ -86,6 +86,8 @@ class TFOptimizer(base_optimizer.BaseOptimizer):\n             return self.update_step(grad, var, learning_rate)\n \n         for grad, var in grads_and_vars:\n+            if isinstance(var, backend.Variable):\n+                var = var.value\n             distribution.extended.update(\n                 var, apply_grad_to_update_var, args=(grad,), group=False\n             )\n\n@@ -345,6 +345,4 @@ def while_loop(\n \n \n def stop_gradient(variable):\n-    # We can't use `.requires_grad_(False)` here since it only\n-    # works when the tensor is a leaf node in the graph.\n-    return variable.detach()\n+    return variable.requires_grad_(False)\n\n@@ -19,7 +19,6 @@ import collections\n import inspect\n import warnings\n \n-import numpy as np\n from tensorflow import nest\n \n from keras_core import backend\n@@ -896,7 +895,7 @@ class Layer(BackendLayer, Operation):\n         \"\"\"\n         all_vars = self._trainable_variables + self._non_trainable_variables\n         for i, v in enumerate(all_vars):\n-            store[f\"{i}\"] = np.array(v)\n+            store[f\"{i}\"] = v.numpy()\n \n     def load_own_variables(self, store):\n         \"\"\"Loads the state of the layer.\n\n@@ -326,7 +326,7 @@ class BaseOptimizer:\n     def save_own_variables(self, store):\n         \"\"\"Get the state of this optimizer object.\"\"\"\n         for i, variable in enumerate(self.variables):\n-            store[str(i)] = np.array(variable)\n+            store[str(i)] = variable.numpy()\n \n     def load_own_variables(self, store):\n         \"\"\"Set the state of this optimizer object.\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9cb3f24018b30d4b2e4cff168f7cfae94e8276a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 750 | Contributors (this commit): 8 | Commits (past 90d): 47 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -345,4 +345,6 @@ def while_loop(\n \n \n def stop_gradient(variable):\n-    return variable.requires_grad_(False)\n+    # We can't use `.requires_grad_(False)` here since it only\n+    # works when the tensor is a leaf node in the graph.\n+    return variable.detach()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#92475f309e19380d4a6b7bfea773be80d09177ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 752 | Contributors (this commit): 8 | Commits (past 90d): 48 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -46,7 +46,7 @@ def get_default_device():\n def get_device():\n     device = global_state.get_global_attribute(\"torch_device\", None)\n     if device is None:\n-        get_default_device()\n+        return get_default_device()\n     return device\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#30ff5fde9679ddd5916f848a31e1eb3bd3f87405", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1975 | Contributors (this commit): 5 | Commits (past 90d): 13 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -1,8 +1,6 @@\n import re\n import warnings\n \n-import numpy as np\n-\n from keras_core import backend\n from keras_core import initializers\n from keras_core import ops\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#aabe6ffccdc9e9f39ba12ea0550ba372f9c036ad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 1142 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -141,6 +141,9 @@ def save_model_to_hdf5(model, filepath, overwrite=True, include_optimizer=True):\n         if opened_new_file:\n             f.close()\n \n+        # Remove legacy serialization attribute after H5 saving complete\n+        delattr(model, \"use_legacy_config\")\n+\n \n def load_model_from_hdf5(filepath, custom_objects=None, compile=True):\n     \"\"\"Loads a model saved via `save_model_to_hdf5`.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cd6e184528796dbc15e56ab559e5d47e78141734", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 696 | Contributors (this commit): 4 | Commits (past 90d): 22 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -19,8 +19,8 @@ from absl import logging\n from model_benchmark.benchmark_utils import BenchmarkMetricsCallback\n \n import keras_core as keras\n-from keras_core.applications import EfficientNetV2B0\n \n+flags.DEFINE_string(\"model\", \"EfficientNetV2B0\", \"The model to benchmark.\")\n flags.DEFINE_integer(\"epochs\", 1, \"The number of epochs.\")\n flags.DEFINE_integer(\"batch_size\", 4, \"Batch Size.\")\n flags.DEFINE_string(\n@@ -35,6 +35,12 @@ BATCH_SIZE = 32\n IMAGE_SIZE = (224, 224)\n CHANNELS = 3\n \n+MODEL_MAP = {\n+    \"EfficientNetV2B0\": keras.applications.EfficientNetV2B0,\n+    \"Xception\": keras.applications.Xception,\n+    \"ResNet50V2\": keras.applications.ResNet50V2,\n+}\n+\n \n def load_data():\n     # Load cats vs dogs dataset, and split into train and validation sets.\n@@ -55,7 +61,6 @@ def load_data():\n             preprocess_inputs, num_parallel_calls=tf.data.AUTOTUNE\n         )\n         .batch(FLAGS.batch_size)\n-        .cache()\n         .prefetch(tf.data.AUTOTUNE)\n     )\n     val_dataset = (\n@@ -68,8 +73,9 @@ def load_data():\n \n \n def load_model():\n+    model_class = MODEL_MAP[FLAGS.model]\n     # Load the EfficientNetV2B0 model and add a classification head.\n-    model = EfficientNetV2B0(include_top=False, weights=\"imagenet\")\n+    model = model_class(include_top=False, weights=\"imagenet\")\n     classifier = keras.models.Sequential(\n         [\n             keras.Input([IMAGE_SIZE[0], IMAGE_SIZE[1], CHANNELS]),\n\n@@ -221,8 +221,10 @@ class BatchNormalization(Layer):\n                 )\n             )\n         else:\n-            moving_mean = ops.reshape(self.moving_mean, broadcast_shape)\n-            moving_variance = ops.reshape(self.moving_variance, broadcast_shape)\n+            moving_mean = ops.cast(self.moving_mean, inputs.dtype)\n+            moving_variance = ops.cast(self.moving_variance, inputs.dtype)\n+            moving_mean = ops.reshape(moving_mean, broadcast_shape)\n+            moving_variance = ops.reshape(moving_variance, broadcast_shape)\n             outputs = (inputs - moving_mean) / ops.sqrt(\n                 moving_variance + self.epsilon\n             )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dd81bb6e584fc094f14fce1c1c313931bc867c38", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 549 | Lines Deleted: 2 | Files Changed: 4 | Hunks: 3 | Methods Changed: 11 | Complexity Δ (Sum/Max): 16/10 | Churn Δ: 551 | Churn Cumulative: 1099 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -460,8 +460,6 @@ class GAN(keras.Model):\n Let's test-drive it:\n \"\"\"\n \n-keras.config.disable_traceback_filtering()\n-\n # Prepare the dataset. We use both the training & test MNIST digits.\n batch_size = 64\n (x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n\n\n@@ -0,0 +1,274 @@\n+\"\"\"\n+Title: Multi-GPU distributed training with TensorFlow\n+Author: [fchollet](https://twitter.com/fchollet)\n+Date created: 2020/04/28\n+Last modified: 2023/06/29\n+Description: Guide to multi-GPU training for Keras models with TensorFlow.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+There are generally two ways to distribute computation across multiple devices:\n+\n+**Data parallelism**, where a single model gets replicated on multiple devices or\n+multiple machines. Each of them processes different batches of data, then they merge\n+their results. There exist many variants of this setup, that differ in how the different\n+model replicas merge results, in whether they stay in sync at every batch or whether they\n+are more loosely coupled, etc.\n+\n+**Model parallelism**, where different parts of a single model run on different devices,\n+processing a single batch of data together. This works best with models that have a\n+naturally-parallel architecture, such as models that feature multiple branches.\n+\n+This guide focuses on data parallelism, in particular **synchronous data parallelism**,\n+where the different replicas of the model stay in sync after each batch they process.\n+Synchronicity keeps the model convergence behavior identical to what you would see for\n+single-device training.\n+\n+Specifically, this guide teaches you how to use the `tf.distribute` API to train Keras\n+models on multiple GPUs, with minimal changes to your code,\n+on multiple GPUs (typically 2 to 16) installed on a single machine (single host,\n+multi-device training). This is the most common setup for researchers and small-scale\n+industry workflows.\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import os\n+\n+os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n+\n+import tensorflow as tf\n+import keras_core as keras\n+\n+\"\"\"\n+## Single-host, multi-device synchronous training\n+\n+In this setup, you have one machine with several GPUs on it (typically 2 to 16). Each\n+device will run a copy of your model (called a **replica**). For simplicity, in what\n+follows, we'll assume we're dealing with 8 GPUs, at no loss of generality.\n+\n+**How it works**\n+\n+At each step of training:\n+\n+- The current batch of data (called **global batch**) is split into 8 different\n+sub-batches (called **local batches**). For instance, if the global batch has 512\n+samples, each of the 8 local batches will have 64 samples.\n+- Each of the 8 replicas independently processes a local batch: they run a forward pass,\n+then a backward pass, outputting the gradient of the weights with respect to the loss of\n+the model on the local batch.\n+- The weight updates originating from local gradients are efficiently merged across the 8\n+replicas. Because this is done at the end of every step, the replicas always stay in\n+sync.\n+\n+In practice, the process of synchronously updating the weights of the model replicas is\n+handled at the level of each individual weight variable. This is done through a **mirrored\n+variable** object.\n+\n+**How to use it**\n+\n+To do single-host, multi-device synchronous training with a Keras model, you would use\n+the [`tf.distribute.MirroredStrategy` API](\n+    https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy).\n+Here's how it works:\n+\n+- Instantiate a `MirroredStrategy`, optionally configuring which specific devices you\n+want to use (by default the strategy will use all GPUs available).\n+- Use the strategy object to open a scope, and within this scope, create all the Keras\n+objects you need that contain variables. Typically, that means **creating & compiling the\n+model** inside the distribution scope. In some cases, the first call to `fit()` may also\n+create variables, so it's a good idea to put your `fit()` call in the scope as well.\n+- Train the model via `fit()` as usual.\n+\n+Importantly, we recommend that you use `tf.data.Dataset` objects to load data\n+in a multi-device or distributed workflow.\n+\n+Schematically, it looks like this:\n+\n+```python\n+# Create a MirroredStrategy.\n+strategy = tf.distribute.MirroredStrategy()\n+print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n+\n+# Open a strategy scope.\n+with strategy.scope():\n+    # Everything that creates variables should be under the strategy scope.\n+    # In general this is only model construction & `compile()`.\n+    model = Model(...)\n+    model.compile(...)\n+\n+    # Train the model on all available devices.\n+    model.fit(train_dataset, validation_data=val_dataset, ...)\n+\n+    # Test the model on all available devices.\n+    model.evaluate(test_dataset)\n+```\n+\n+Here's a simple end-to-end runnable example:\n+\"\"\"\n+\n+\n+def get_compiled_model():\n+    # Make a simple 2-layer densely-connected neural network.\n+    inputs = keras.Input(shape=(784,))\n+    x = keras.layers.Dense(256, activation=\"relu\")(inputs)\n+    x = keras.layers.Dense(256, activation=\"relu\")(x)\n+    outputs = keras.layers.Dense(10)(x)\n+    model = keras.Model(inputs, outputs)\n+    model.compile(\n+        optimizer=keras.optimizers.Adam(),\n+        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n+        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n+    )\n+    return model\n+\n+\n+def get_dataset():\n+    batch_size = 32\n+    num_val_samples = 10000\n+\n+    # Return the MNIST dataset in the form of a `tf.data.Dataset`.\n+    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n+\n+    # Preprocess the data (these are Numpy arrays)\n+    x_train = x_train.reshape(-1, 784).astype(\"float32\") / 255\n+    x_test = x_test.reshape(-1, 784).astype(\"float32\") / 255\n+    y_train = y_train.astype(\"float32\")\n+    y_test = y_test.astype(\"float32\")\n+\n+    # Reserve num_val_samples samples for validation\n+    x_val = x_train[-num_val_samples:]\n+    y_val = y_train[-num_val_samples:]\n+    x_train = x_train[:-num_val_samples]\n+    y_train = y_train[:-num_val_samples]\n+    return (\n+        tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(\n+            batch_size\n+        ),\n+        tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size),\n+        tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size),\n+    )\n+\n+\n+# Create a MirroredStrategy.\n+strategy = tf.distribute.MirroredStrategy()\n+print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))\n+\n+# Open a strategy scope.\n+with strategy.scope():\n+    # Everything that creates variables should be under the strategy scope.\n+    # In general this is only model construction & `compile()`.\n+    model = get_compiled_model()\n+\n+    # Train the model on all available devices.\n+    train_dataset, val_dataset, test_dataset = get_dataset()\n+    model.fit(train_dataset, epochs=2, validation_data=val_dataset)\n+\n+    # Test the model on all available devices.\n+    model.evaluate(test_dataset)\n+\n+\"\"\"\n+## Using callbacks to ensure fault tolerance\n+\n+When using distributed training, you should always make sure you have a strategy to\n+recover from failure (fault tolerance). The simplest way to handle this is to pass\n+`ModelCheckpoint` callback to `fit()`, to save your model\n+at regular intervals (e.g. every 100 batches or every epoch). You can then restart\n+training from your saved model.\n+\n+Here's a simple example:\n+\"\"\"\n+\n+# Prepare a directory to store all the checkpoints.\n+checkpoint_dir = \"./ckpt\"\n+if not os.path.exists(checkpoint_dir):\n+    os.makedirs(checkpoint_dir)\n+\n+\n+def make_or_restore_model():\n+    # Either restore the latest model, or create a fresh one\n+    # if there is no checkpoint available.\n+    checkpoints = [\n+        checkpoint_dir + \"/\" + name for name in os.listdir(checkpoint_dir)\n+    ]\n+    if checkpoints:\n+        latest_checkpoint = max(checkpoints, key=os.path.getctime)\n+        print(\"Restoring from\", latest_checkpoint)\n+        return keras.models.load_model(latest_checkpoint)\n+    print(\"Creating a new model\")\n+    return get_compiled_model()\n+\n+\n+def run_training(epochs=1):\n+    # Create a MirroredStrategy.\n+    strategy = tf.distribute.MirroredStrategy()\n+\n+    # Open a strategy scope and create/restore the model\n+    with strategy.scope():\n+        model = make_or_restore_model()\n+\n+        callbacks = [\n+            # This callback saves a SavedModel every epoch\n+            # We include the current epoch in the folder name.\n+            keras.callbacks.ModelCheckpoint(\n+                filepath=checkpoint_dir + \"/ckpt-{epoch}.keras\",\n+                save_freq=\"epoch\",\n+            )\n+        ]\n+        model.fit(\n+            train_dataset,\n+            epochs=epochs,\n+            callbacks=callbacks,\n+            validation_data=val_dataset,\n+            verbose=2,\n+        )\n+\n+\n+# Running the first time creates the model\n+run_training(epochs=1)\n+\n+# Calling the same function again will resume from where we left off\n+run_training(epochs=1)\n+\n+\"\"\"\n+## `tf.data` performance tips\n+\n+When doing distributed training, the efficiency with which you load data can often become\n+critical. Here are a few tips to make sure your `tf.data` pipelines\n+run as fast as possible.\n+\n+**Note about dataset batching**\n+\n+When creating your dataset, make sure it is batched with the global batch size.\n+For instance, if each of your 8 GPUs is capable of running a batch of 64 samples, you\n+call use a global batch size of 512.\n+\n+**Calling `dataset.cache()`**\n+\n+If you call `.cache()` on a dataset, its data will be cached after running through the\n+first iteration over the data. Every subsequent iteration will use the cached data. The\n+cache can be in memory (default) or to a local file you specify.\n+\n+This can improve performance when:\n+\n+- Your data is not expected to change from iteration to iteration\n+- You are reading data from a remote distributed filesystem\n+- You are reading data from local disk, but your data would fit in memory and your\n+workflow is significantly IO-bound (e.g. reading & decoding image files).\n+\n+**Calling `dataset.prefetch(buffer_size)`**\n+\n+You should almost always call `.prefetch(buffer_size)` after creating a dataset. It means\n+your data pipeline will run asynchronously from your model,\n+with new samples being preprocessed and stored in a buffer while the current batch\n+samples are used to train the model. The next batch will be prefetched in GPU memory by\n+the time the current batch is over.\n+\"\"\"\n+\n+\"\"\"\n+That's it!\n+\"\"\"\n\n@@ -0,0 +1,275 @@\n+\"\"\"\n+Title: Multi-GPU distributed training with PyTorch\n+Author: [fchollet](https://twitter.com/fchollet)\n+Date created: 2023/06/29\n+Last modified: 2023/06/29\n+Description: Guide to multi-GPU training for Keras models with PyTorch.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+There are generally two ways to distribute computation across multiple devices:\n+\n+**Data parallelism**, where a single model gets replicated on multiple devices or\n+multiple machines. Each of them processes different batches of data, then they merge\n+their results. There exist many variants of this setup, that differ in how the different\n+model replicas merge results, in whether they stay in sync at every batch or whether they\n+are more loosely coupled, etc.\n+\n+**Model parallelism**, where different parts of a single model run on different devices,\n+processing a single batch of data together. This works best with models that have a\n+naturally-parallel architecture, such as models that feature multiple branches.\n+\n+This guide focuses on data parallelism, in particular **synchronous data parallelism**,\n+where the different replicas of the model stay in sync after each batch they process.\n+Synchronicity keeps the model convergence behavior identical to what you would see for\n+single-device training.\n+\n+Specifically, this guide teaches you how to use the `tf.distribute` API to train Keras\n+models on multiple GPUs, with minimal changes to your code,\n+on multiple GPUs (typically 2 to 16) installed on a single machine (single host,\n+multi-device training). This is the most common setup for researchers and small-scale\n+industry workflows.\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\n+Let's start by defining the function that creates the model that we will train,\n+and the function that creates the dataset we will train on (MNIST in this case).\n+\"\"\"\n+\n+import os\n+\n+os.environ[\"KERAS_BACKEND\"] = \"torch\"\n+\n+import torch\n+import numpy as np\n+import keras_core as keras\n+\n+\n+def get_model():\n+    # Make a simple convnet with batch normalization and dropout.\n+    inputs = keras.Input(shape=(28, 28, 1))\n+    x = keras.layers.Rescaling(1.0 / 255.0)(x)\n+    x = keras.layers.Conv2D(\n+        filters=12, kernel_size=3, padding=\"same\", use_bias=False\n+    )(x)\n+    x = keras.layers.BatchNormalization(scale=False, center=True)(x)\n+    x = keras.layers.ReLU()(x)\n+    x = keras.layers.Conv2D(\n+        filters=24,\n+        kernel_size=6,\n+        use_bias=False,\n+        strides=2,\n+    )(x)\n+    x = keras.layers.BatchNormalization(scale=False, center=True)(x)\n+    x = keras.layers.ReLU()(x)\n+    x = keras.layers.Conv2D(\n+        filters=32,\n+        kernel_size=6,\n+        padding=\"same\",\n+        strides=2,\n+        name=\"large_k\",\n+    )(x)\n+    x = keras.layers.BatchNormalization(scale=False, center=True)(x)\n+    x = keras.layers.ReLU()(x)\n+    x = keras.layers.GlobalAveragePooling2D()(x)\n+    x = keras.layers.Dense(256, activation=\"relu\")(x)\n+    x = keras.layers.Dropout(0.5)(x)\n+    outputs = keras.layers.Dense(10)(x)\n+    model = keras.Model(inputs, outputs)\n+    return model\n+\n+\n+def get_dataset():\n+    # Load the data and split it between train and test sets\n+    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n+\n+    # Scale images to the [0, 1] range\n+    x_train = x_train.astype(\"float32\")\n+    x_test = x_test.astype(\"float32\")\n+    # Make sure images have shape (28, 28, 1)\n+    x_train = np.expand_dims(x_train, -1)\n+    x_test = np.expand_dims(x_test, -1)\n+    print(\"x_train shape:\", x_train.shape)\n+    print(x_train.shape[0], \"train samples\")\n+    print(x_test.shape[0], \"test samples\")\n+\n+    # Create a TensorDataset\n+    dataset = torch.utils.data.TensorDataset(\n+        torch.from_numpy(x_train), torch.from_numpy(y_train)\n+    )\n+    return dataset\n+\n+\n+\"\"\"\n+Next, let's define a simple PyTorch training loop that targets\n+a GPU (note the calls to `.cuda()`).\n+\"\"\"\n+\n+\n+def train_model(model, dataloader, num_epochs, optimizer, loss_fn):\n+    for epoch in range(num_epochs):\n+        running_loss = 0.0\n+        for batch_idx, (inputs, targets) in enumerate(dataloader):\n+            inputs = inputs.cuda(non_blocking=True)\n+            targets = targets.cuda(non_blocking=True)\n+\n+            # Forward pass\n+            outputs = model(inputs)\n+            loss = loss_fn(outputs, targets)\n+\n+            # Backward and optimize\n+            optimizer.zero_grad()\n+            loss.backward()\n+            optimizer.step()\n+\n+            running_loss += loss.item()\n+\n+            # Print loss statistics\n+            if (batch_idx + 1) % 10 == 0:\n+                print(\n+                    f\"Epoch {epoch + 1}/{num_epochs}, \"\n+                    f\"Batch {batch_idx + 1}/{len(dataloader)}, \"\n+                    f\"Loss: {running_loss / 10}\"\n+                )\n+                running_loss = 0.0\n+\n+\n+\"\"\"\n+## Single-host, multi-device synchronous training\n+\n+In this setup, you have one machine with several GPUs on it (typically 2 to 16). Each\n+device will run a copy of your model (called a **replica**). For simplicity, in what\n+follows, we'll assume we're dealing with 8 GPUs, at no loss of generality.\n+\n+**How it works**\n+\n+At each step of training:\n+\n+- The current batch of data (called **global batch**) is split into 8 different\n+sub-batches (called **local batches**). For instance, if the global batch has 512\n+samples, each of the 8 local batches will have 64 samples.\n+- Each of the 8 replicas independently processes a local batch: they run a forward pass,\n+then a backward pass, outputting the gradient of the weights with respect to the loss of\n+the model on the local batch.\n+- The weight updates originating from local gradients are efficiently merged across the 8\n+replicas. Because this is done at the end of every step, the replicas always stay in\n+sync.\n+\n+In practice, the process of synchronously updating the weights of the model replicas is\n+handled at the level of each individual weight variable. This is done through a **mirrored\n+variable** object.\n+\n+**How to use it**\n+\n+To do single-host, multi-device synchronous training with a Keras model, you would use\n+the `torch.nn.parallel.DistributedDataParallel` module wrapper.\n+Here's how it works:\n+\n+- We use `torch.multiprocessing.spawn` to spawn multiple Python processes, one\n+per device. Each process will run the `per_device_launch_fn` function.\n+- The `per_device_launch_fn` function does the following:\n+    - It uses `torch.distributed.dist.init_process_group` and `torch.cuda.set_device`\n+    to configure the device to be used for that process.\n+    - It uses `torch.utils.data.distributed.DistributedSampler`\n+    and `torch.utils.data.DataLoader` to turn our data into a distributed data loader.\n+    - It also uses `torch.nn.parallel.DistributedDataParallel` to turn our model into\n+    a distributed PyTorch module.\n+    - It then calls the `train_model` function.\n+- The `train_model` function will then run in each process, with the model using\n+a separate device in each process.\n+\n+Here's the flow, where each step is split into its own utility function:\n+\"\"\"\n+\n+# Config\n+num_gpu = torch.cuda.device_count()\n+num_epochs = 2\n+batch_size = 64\n+print(f\"Running on {num_gpu} GPUs\")\n+\n+\n+def setup_device(current_gpu_index, num_gpus):\n+    # Device setup\n+    os.environ[\"MASTER_ADDR\"] = \"keras-core-torch\"\n+    os.environ[\"MASTER_PORT\"] = \"56492\"\n+    device = torch.device(\"cuda:{}\".format(current_gpu_index))\n+    torch.distributed.dist.init_process_group(\n+        backend=\"nccl\",\n+        init_method=\"env://\",\n+        world_size=num_gpus,\n+        rank=current_gpu_index,\n+    )\n+    torch.cuda.set_device(device)\n+\n+\n+def cleanup():\n+    torch.distributed.dist.dist.destroy_process_group()\n+\n+\n+def prepare_dataloader(dataset, current_gpu_index, num_gpus, batch_size):\n+    sampler = torch.utils.data.distributed.DistributedSampler(\n+        dataset,\n+        num_replicas=num_gpus,\n+        rank=current_gpu_index,\n+        shuffle=False,\n+    )\n+    dataloader = torch.utils.data.DataLoader(\n+        dataset,\n+        sampler=sampler,\n+        batch_size=batch_size,\n+        shuffle=False,\n+    )\n+    return dataloader\n+\n+\n+def per_device_launch_fn(current_gpu_index, num_gpu):\n+    # Setup the process groups\n+    setup_device(current_gpu_index, num_gpu)\n+\n+    #################################################################\n+    ######## Writing a torch training loop for a Keras model ########\n+    #################################################################\n+\n+    dataset = get_dataset()\n+    model = get_model()\n+\n+    # prepare the dataloader\n+    dataloader = prepare_dataloader(\n+        dataset, current_gpu_index, num_gpu, batch_size\n+    )\n+\n+    # Instantiate the torch optimizer\n+    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n+\n+    # Instantiate the torch loss function\n+    loss_fn = torch.nn.CrossEntropyLoss()\n+\n+    # Put model on device\n+    model = model.to(current_gpu_index)\n+    ddp_model = torch.nn.parallel.DistributedDataParallel(\n+        model, device_ids=[current_gpu_index], output_device=current_gpu_index\n+    )\n+\n+    train_model(ddp_model, dataloader, num_epochs, optimizer, loss_fn)\n+\n+    cleanup()\n+\n+\n+\"\"\"\n+Time to spawn:\n+\"\"\"\n+\n+torch.multiprocessing.spawn(\n+    per_device_launch_fn,\n+    args=(num_gpu,),\n+    nprocs=num_gpu,\n+    join=True,\n+)\n+\n+\"\"\"\n+That's it!\n+\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#107d5a12914d963ce80b1f2bed01d4647e50dd19", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 282 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -26,7 +26,8 @@ where the different replicas of the model stay in sync after each batch they pro\n Synchronicity keeps the model convergence behavior identical to what you would see for\n single-device training.\n \n-Specifically, this guide teaches you how to use the `tf.distribute` API to train Keras\n+Specifically, this guide teaches you how to use PyTorch's `DistributedDataParallel`\n+module wrapper to train Keras\n models on multiple GPUs, with minimal changes to your code,\n on multiple GPUs (typically 2 to 16) installed on a single machine (single host,\n multi-device training). This is the most common setup for researchers and small-scale\n@@ -230,10 +231,6 @@ def per_device_launch_fn(current_gpu_index, num_gpu):\n     # Setup the process groups\n     setup_device(current_gpu_index, num_gpu)\n \n-    #################################################################\n-    ######## Writing a torch training loop for a Keras model ########\n-    #################################################################\n-\n     dataset = get_dataset()\n     model = get_model()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2a367f07e0730fe659a4bda64d8695b0b81b81a1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 285 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -27,8 +27,7 @@ Synchronicity keeps the model convergence behavior identical to what you would s\n single-device training.\n \n Specifically, this guide teaches you how to use PyTorch's `DistributedDataParallel`\n-module wrapper to train Keras\n-models on multiple GPUs, with minimal changes to your code,\n+module wrapper to train Keras, with minimal changes to your code,\n on multiple GPUs (typically 2 to 16) installed on a single machine (single host,\n multi-device training). This is the most common setup for researchers and small-scale\n industry workflows.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1c2bc4a2db4f40a66bf2d216ccda95a028324a89", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 16 | Churn Cumulative: 16 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,11 +1,19 @@\n-\"\"\"Image classification benchmark with EfficientNetV2B0.\n+\"\"\"Image classification benchmark.\n+\n+This script runs image classification benchmark with \"dogs vs cats\" datasets. It \n+supports the following 3 models:\n+- EfficientNetV2B0\n+- Xception\n+- ResNet50V2\n \n To run the benchmark, make sure you are in model_benchmark/ directory, and run\n the command below:\n \n-python3 -m model_benchmark.efficient_net_image_classification_benchmark \\\n+python3 -m model_benchmark.image_classification_benchmark \\\n+    --model=\"EfficientNetV2B0\" \\\n     --epochs=2 \\\n-    --batch_size=32\n+    --batch_size=32 \\\n+    --mixed_precision_policy=\"mixed_float16\"\n \"\"\"\n \n import time\n@@ -93,7 +101,7 @@ def main(_):\n     logging.info(\n         \"Benchmarking configs...\\n\"\n         \"=========================\\n\"\n-        f\"MODEL: EfficientNetV2B0\\n\"\n+        f\"MODEL: {FLAGS.model}\\n\"\n         f\"TASK: image classification/dogs-vs-cats \\n\"\n         f\"BATCH_SIZE: {FLAGS.batch_size}\\n\"\n         f\"EPOCHS: {FLAGS.epochs}\\n\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5e637c1e4bf7d4e65c9c930002da4c283576db7c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 558 | Lines Deleted: 12 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 570 | Churn Cumulative: 748 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,19 +1,11 @@\n-\"\"\"Image classification benchmark.\n-\n-This script runs image classification benchmark with \"dogs vs cats\" datasets. It \n-supports the following 3 models:\n-- EfficientNetV2B0\n-- Xception\n-- ResNet50V2\n+\"\"\"Image classification benchmark with EfficientNetV2B0.\n \n To run the benchmark, make sure you are in model_benchmark/ directory, and run\n the command below:\n \n-python3 -m model_benchmark.image_classification_benchmark \\\n-    --model=\"EfficientNetV2B0\" \\\n+python3 -m model_benchmark.efficient_net_image_classification_benchmark \\\n     --epochs=2 \\\n-    --batch_size=32 \\\n-    --mixed_precision_policy=\"mixed_float16\"\n+    --batch_size=32\n \"\"\"\n \n import time\n@@ -101,7 +93,7 @@ def main(_):\n     logging.info(\n         \"Benchmarking configs...\\n\"\n         \"=========================\\n\"\n-        f\"MODEL: {FLAGS.model}\\n\"\n+        f\"MODEL: EfficientNetV2B0\\n\"\n         f\"TASK: image classification/dogs-vs-cats \\n\"\n         f\"BATCH_SIZE: {FLAGS.batch_size}\\n\"\n         f\"EPOCHS: {FLAGS.epochs}\\n\"\n\n@@ -0,0 +1,554 @@\n+\"\"\"\n+Title: Transfer learning & fine-tuning\n+Author: [fchollet](https://twitter.com/fchollet)\n+Date created: 2020/04/15\n+Last modified: 2023/06/25\n+Description: Complete guide to transfer learning & fine-tuning in Keras.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import numpy as np\n+import keras_core as keras\n+from keras_core import layers\n+import tensorflow_datasets as tfds\n+import matplotlib.pyplot as plt\n+\n+\"\"\"\n+## Introduction\n+\n+**Transfer learning** consists of taking features learned on one problem, and\n+leveraging them on a new, similar problem. For instance, features from a model that has\n+learned to identify racoons may be useful to kick-start a model meant to identify\n+ tanukis.\n+\n+Transfer learning is usually done for tasks where your dataset has too little data to\n+ train a full-scale model from scratch.\n+\n+The most common incarnation of transfer learning in the context of deep learning is the\n+ following workflow:\n+\n+1. Take layers from a previously trained model.\n+2. Freeze them, so as to avoid destroying any of the information they contain during\n+ future training rounds.\n+3. Add some new, trainable layers on top of the frozen layers. They will learn to turn\n+ the old features into predictions on a  new dataset.\n+4. Train the new layers on your dataset.\n+\n+A last, optional step, is **fine-tuning**, which consists of unfreezing the entire\n+model you obtained above (or part of it), and re-training it on the new data with a\n+very low learning rate. This can potentially achieve meaningful improvements, by\n+ incrementally adapting the pretrained features to the new data.\n+\n+First, we will go over the Keras `trainable` API in detail, which underlies most\n+ transfer learning & fine-tuning workflows.\n+\n+Then, we'll demonstrate the typical workflow by taking a model pretrained on the\n+ImageNet dataset, and retraining it on the Kaggle \"cats vs dogs\" classification\n+ dataset.\n+\n+This is adapted from\n+[Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python)\n+and the 2016 blog post\n+[\"building powerful image classification models using very little data\"](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html).\n+\"\"\"\n+\n+\"\"\"\n+## Freezing layers: understanding the `trainable` attribute\n+\n+Layers & models have three weight attributes:\n+\n+- `weights` is the list of all weights variables of the layer.\n+- `trainable_weights` is the list of those that are meant to be updated (via gradient\n+ descent) to minimize the loss during training.\n+- `non_trainable_weights` is the list of those that aren't meant to be trained.\n+ Typically they are updated by the model during the forward pass.\n+\n+**Example: the `Dense` layer has 2 trainable weights (kernel & bias)**\n+\"\"\"\n+\n+layer = keras.layers.Dense(3)\n+layer.build((None, 4))  # Create the weights\n+\n+print(\"weights:\", len(layer.weights))\n+print(\"trainable_weights:\", len(layer.trainable_weights))\n+print(\"non_trainable_weights:\", len(layer.non_trainable_weights))\n+\n+\"\"\"\n+In general, all weights are trainable weights. The only built-in layer that has\n+non-trainable weights is the `BatchNormalization` layer. It uses non-trainable weights\n+ to keep track of the mean and variance of its inputs during training.\n+To learn how to use non-trainable weights in your own custom layers, see the\n+[guide to writing new layers from scratch](https://keras.io/guides/making_new_layers_and_models_via_subclassing/).\n+\n+**Example: the `BatchNormalization` layer has 2 trainable weights and 2 non-trainable\n+ weights**\n+\"\"\"\n+\n+layer = keras.layers.BatchNormalization()\n+layer.build((None, 4))  # Create the weights\n+\n+print(\"weights:\", len(layer.weights))\n+print(\"trainable_weights:\", len(layer.trainable_weights))\n+print(\"non_trainable_weights:\", len(layer.non_trainable_weights))\n+\n+\"\"\"\n+Layers & models also feature a boolean attribute `trainable`. Its value can be changed.\n+Setting `layer.trainable` to `False` moves all the layer's weights from trainable to\n+non-trainable.  This is called \"freezing\" the layer: the state of a frozen layer won't\n+be updated during training (either when training with `fit()` or when training with\n+ any custom loop that relies on `trainable_weights` to apply gradient updates).\n+\n+**Example: setting `trainable` to `False`**\n+\"\"\"\n+\n+layer = keras.layers.Dense(3)\n+layer.build((None, 4))  # Create the weights\n+layer.trainable = False  # Freeze the layer\n+\n+print(\"weights:\", len(layer.weights))\n+print(\"trainable_weights:\", len(layer.trainable_weights))\n+print(\"non_trainable_weights:\", len(layer.non_trainable_weights))\n+\n+\"\"\"\n+When a trainable weight becomes non-trainable, its value is no longer updated during\n+ training.\n+\"\"\"\n+\n+# Make a model with 2 layers\n+layer1 = keras.layers.Dense(3, activation=\"relu\")\n+layer2 = keras.layers.Dense(3, activation=\"sigmoid\")\n+model = keras.Sequential([keras.Input(shape=(3,)), layer1, layer2])\n+\n+# Freeze the first layer\n+layer1.trainable = False\n+\n+# Keep a copy of the weights of layer1 for later reference\n+initial_layer1_weights_values = layer1.get_weights()\n+\n+# Train the model\n+model.compile(optimizer=\"adam\", loss=\"mse\")\n+model.fit(np.random.random((2, 3)), np.random.random((2, 3)))\n+\n+# Check that the weights of layer1 have not changed during training\n+final_layer1_weights_values = layer1.get_weights()\n+np.testing.assert_allclose(\n+    initial_layer1_weights_values[0], final_layer1_weights_values[0]\n+)\n+np.testing.assert_allclose(\n+    initial_layer1_weights_values[1], final_layer1_weights_values[1]\n+)\n+\n+\"\"\"\n+Do not confuse the `layer.trainable` attribute with the argument `training` in\n+`layer.__call__()` (which controls whether the layer should run its forward pass in\n+ inference mode or training mode). For more information, see the\n+[Keras FAQ](\n+  https://keras.io/getting_started/faq/#whats-the-difference-between-the-training-argument-in-call-and-the-trainable-attribute).\n+\"\"\"\n+\n+\"\"\"\n+## Recursive setting of the `trainable` attribute\n+\n+If you set `trainable = False` on a model or on any layer that has sublayers,\n+all children layers become non-trainable as well.\n+\n+**Example:**\n+\"\"\"\n+\n+inner_model = keras.Sequential(\n+    [\n+        keras.Input(shape=(3,)),\n+        keras.layers.Dense(3, activation=\"relu\"),\n+        keras.layers.Dense(3, activation=\"relu\"),\n+    ]\n+)\n+\n+model = keras.Sequential(\n+    [\n+        keras.Input(shape=(3,)),\n+        inner_model,\n+        keras.layers.Dense(3, activation=\"sigmoid\"),\n+    ]\n+)\n+\n+model.trainable = False  # Freeze the outer model\n+\n+assert inner_model.trainable == False  # All layers in `model` are now frozen\n+assert (\n+    inner_model.layers[0].trainable == False\n+)  # `trainable` is propagated recursively\n+\n+\"\"\"\n+## The typical transfer-learning workflow\n+\n+This leads us to how a typical transfer learning workflow can be implemented in Keras:\n+\n+1. Instantiate a base model and load pre-trained weights into it.\n+2. Freeze all layers in the base model by setting `trainable = False`.\n+3. Create a new model on top of the output of one (or several) layers from the base\n+ model.\n+4. Train your new model on your new dataset.\n+\n+Note that an alternative, more lightweight workflow could also be:\n+\n+1. Instantiate a base model and load pre-trained weights into it.\n+2. Run your new dataset through it and record the output of one (or several) layers\n+ from the base model. This is called **feature extraction**.\n+3. Use that output as input data for a new, smaller model.\n+\n+A key advantage of that second workflow is that you only run the base model once on\n+ your data, rather than once per epoch of training. So it's a lot faster & cheaper.\n+\n+An issue with that second workflow, though, is that it doesn't allow you to dynamically\n+modify the input data of your new model during training, which is required when doing\n+data augmentation, for instance. Transfer learning is typically used for tasks when\n+your new dataset has too little data to train a full-scale model from scratch, and in\n+such scenarios data augmentation is very important. So in what follows, we will focus\n+ on the first workflow.\n+\n+Here's what the first workflow looks like in Keras:\n+\n+First, instantiate a base model with pre-trained weights.\n+\n+```python\n+base_model = keras.applications.Xception(\n+    weights='imagenet',  # Load weights pre-trained on ImageNet.\n+    input_shape=(150, 150, 3),\n+    include_top=False)  # Do not include the ImageNet classifier at the top.\n+```\n+\n+Then, freeze the base model.\n+\n+```python\n+base_model.trainable = False\n+```\n+\n+Create a new model on top.\n+\n+```python\n+inputs = keras.Input(shape=(150, 150, 3))\n+# We make sure that the base_model is running in inference mode here,\n+# by passing `training=False`. This is important for fine-tuning, as you will\n+# learn in a few paragraphs.\n+x = base_model(inputs, training=False)\n+# Convert features of shape `base_model.output_shape[1:]` to vectors\n+x = keras.layers.GlobalAveragePooling2D()(x)\n+# A Dense classifier with a single unit (binary classification)\n+outputs = keras.layers.Dense(1)(x)\n+model = keras.Model(inputs, outputs)\n+```\n+\n+Train the model on new data.\n+\n+```python\n+model.compile(optimizer=keras.optimizers.Adam(),\n+              loss=keras.losses.BinaryCrossentropy(from_logits=True),\n+              metrics=[keras.metrics.BinaryAccuracy()])\n+model.fit(new_dataset, epochs=20, callbacks=..., validation_data=...)\n+```\n+\n+\"\"\"\n+\n+\"\"\"\n+## Fine-tuning\n+\n+Once your model has converged on the new data, you can try to unfreeze all or part of\n+ the base model and retrain the whole model end-to-end with a very low learning rate.\n+\n+This is an optional last step that can potentially give you incremental improvements.\n+ It could also potentially lead to quick overfitting -- keep that in mind.\n+\n+It is critical to only do this step *after* the model with frozen layers has been\n+trained to convergence. If you mix randomly-initialized trainable layers with\n+trainable layers that hold pre-trained features, the randomly-initialized layers will\n+cause very large gradient updates during training, which will destroy your pre-trained\n+ features.\n+\n+It's also critical to use a very low learning rate at this stage, because\n+you are training a much larger model than in the first round of training, on a dataset\n+ that is typically very small.\n+As a result, you are at risk of overfitting very quickly if you apply large weight\n+ updates. Here, you only want to readapt the pretrained weights in an incremental way.\n+\n+This is how to implement fine-tuning of the whole base model:\n+\n+```python\n+# Unfreeze the base model\n+base_model.trainable = True\n+\n+# It's important to recompile your model after you make any changes\n+# to the `trainable` attribute of any inner layer, so that your changes\n+# are take into account\n+model.compile(optimizer=keras.optimizers.Adam(1e-5),  # Very low learning rate\n+              loss=keras.losses.BinaryCrossentropy(from_logits=True),\n+              metrics=[keras.metrics.BinaryAccuracy()])\n+\n+# Train end-to-end. Be careful to stop before you overfit!\n+model.fit(new_dataset, epochs=10, callbacks=..., validation_data=...)\n+```\n+\n+**Important note about `compile()` and `trainable`**\n+\n+Calling `compile()` on a model is meant to \"freeze\" the behavior of that model. This\n+ implies that the `trainable`\n+attribute values at the time the model is compiled should be preserved throughout the\n+ lifetime of that model,\n+until `compile` is called again. Hence, if you change any `trainable` value, make sure\n+ to call `compile()` again on your\n+model for your changes to be taken into account.\n+\n+**Important notes about `BatchNormalization` layer**\n+\n+Many image models contain `BatchNormalization` layers. That layer is a special case on\n+ every imaginable count. Here are a few things to keep in mind.\n+\n+- `BatchNormalization` contains 2 non-trainable weights that get updated during\n+training. These are the variables tracking the mean and variance of the inputs.\n+- When you set `bn_layer.trainable = False`, the `BatchNormalization` layer will\n+run in inference mode, and will not update its mean & variance statistics. This is not\n+the case for other layers in general, as\n+[weight trainability & inference/training modes are two orthogonal concepts](\n+  https://keras.io/getting_started/faq/#whats-the-difference-between-the-training-argument-in-call-and-the-trainable-attribute).\n+But the two are tied in the case of the `BatchNormalization` layer.\n+- When you unfreeze a model that contains `BatchNormalization` layers in order to do\n+fine-tuning, you should keep the `BatchNormalization` layers in inference mode by\n+ passing `training=False` when calling the base model.\n+Otherwise the updates applied to the non-trainable weights will suddenly destroy\n+what the model has learned.\n+\n+You'll see this pattern in action in the end-to-end example at the end of this guide.\n+\"\"\"\n+\n+\"\"\"\n+## An end-to-end example: fine-tuning an image classification model on a cats vs. dogs dataset\n+\n+To solidify these concepts, let's walk you through a concrete end-to-end transfer\n+learning & fine-tuning example. We will load the Xception model, pre-trained on\n+ ImageNet, and use it on the Kaggle \"cats vs. dogs\" classification dataset.\n+\"\"\"\n+\n+\"\"\"\n+### Getting the data\n+\n+First, let's fetch the cats vs. dogs dataset using TFDS. If you have your own dataset,\n+you'll probably want to use the utility\n+`keras.utils.image_dataset_from_directory` to generate similar labeled\n+ dataset objects from a set of images on disk filed into class-specific folders.\n+\n+Transfer learning is most useful when working with very small datasets. To keep our\n+dataset small, we will use 40% of the original training data (25,000 images) for\n+ training, 10% for validation, and 10% for testing.\n+\"\"\"\n+\n+tfds.disable_progress_bar()\n+\n+train_ds, validation_ds, test_ds = tfds.load(\n+    \"cats_vs_dogs\",\n+    # Reserve 10% for validation and 10% for test\n+    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n+    as_supervised=True,  # Include labels\n+)\n+\n+print(f\"Number of training samples: {train_ds.cardinality()}\")\n+print(f\"Number of validation samples: {validation_ds.cardinality()}\")\n+print(f\"Number of test samples: {test_ds.cardinality()}\")\n+\n+\"\"\"\n+These are the first 9 images in the training dataset -- as you can see, they're all\n+different sizes.\n+\"\"\"\n+\n+plt.figure(figsize=(10, 10))\n+for i, (image, label) in enumerate(train_ds.take(9)):\n+    ax = plt.subplot(3, 3, i + 1)\n+    plt.imshow(image)\n+    plt.title(int(label))\n+    plt.axis(\"off\")\n+\n+\"\"\"\n+We can also see that label 1 is \"dog\" and label 0 is \"cat\".\n+\"\"\"\n+\n+\"\"\"\n+### Standardizing the data\n+\n+Our raw images have a variety of sizes. In addition, each pixel consists of 3 integer\n+values between 0 and 255 (RGB level values). This isn't a great fit for feeding a\n+neural network. We need to do 2 things:\n+\n+- Standardize to a fixed image size. We pick 150x150.\n+- Normalize pixel values between -1 and 1. We'll do this using a `Normalization` layer as\n+part of the model itself.\n+\n+In general, it's a good practice to develop models that take raw data as input, as\n+opposed to models that take already-preprocessed data. The reason being that, if your\n+model expects preprocessed data, any time you export your model to use it elsewhere\n+(in a web browser, in a mobile app), you'll need to reimplement the exact same\n+preprocessing pipeline. This gets very tricky very quickly. So we should do the least\n+ possible amount of preprocessing before hitting the model.\n+\n+Here, we'll do image resizing in the data pipeline (because a deep neural network can\n+only process contiguous batches of data), and we'll do the input value scaling as part\n+of the model, when we create it.\n+\n+Let's resize images to 150x150:\n+\"\"\"\n+\n+resize_fn = keras.layers.Resizing(150, 150)\n+\n+train_ds = train_ds.map(lambda x, y: (resize_fn(x), y))\n+validation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\n+test_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\n+\n+\"\"\"\n+### Using random data augmentation\n+\n+When you don't have a large image dataset, it's a good practice to artificially\n+introduce sample diversity by applying random yet realistic transformations to\n+the training images, such as random horizontal flipping or small random rotations. This\n+helps expose the model to different aspects of the training data while slowing down\n+overfitting.\n+\"\"\"\n+\n+augmentation_layers = [\n+    layers.RandomFlip(\"horizontal\"),\n+    layers.RandomRotation(0.1),\n+]\n+\n+\n+def data_augmentation(x):\n+    for layer in augmentation_layers:\n+        x = layer(x)\n+    return x\n+\n+\n+train_ds = train_ds.map(lambda x, y: (data_augmentation(x), y))\n+\n+\"\"\"\n+Let's batch the data and use prefetching to optimize loading speed.\n+\"\"\"\n+\n+from tensorflow import data as tf_data\n+\n+batch_size = 64\n+\n+train_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\n+validation_ds = (\n+    validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\n+)\n+test_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\n+\n+\"\"\"\n+Let's visualize what the first image of the first batch looks like after various random\n+ transformations:\n+\"\"\"\n+\n+for images, labels in train_ds.take(1):\n+    plt.figure(figsize=(10, 10))\n+    first_image = images[0]\n+    for i in range(9):\n+        ax = plt.subplot(3, 3, i + 1)\n+        augmented_image = data_augmentation(np.expand_dims(first_image, 0))\n+        plt.imshow(np.array(augmented_image[0]).astype(\"int32\"))\n+        plt.title(int(labels[0]))\n+        plt.axis(\"off\")\n+\n+\"\"\"\n+## Build a model\n+\n+Now let's built a model that follows the blueprint we've explained earlier.\n+\n+Note that:\n+\n+- We add a `Rescaling` layer to scale input values (initially in the `[0, 255]`\n+ range) to the `[-1, 1]` range.\n+- We add a `Dropout` layer before the classification layer, for regularization.\n+- We make sure to pass `training=False` when calling the base model, so that\n+it runs in inference mode, so that batchnorm statistics don't get updated\n+even after we unfreeze the base model for fine-tuning.\n+\"\"\"\n+\n+base_model = keras.applications.Xception(\n+    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n+    input_shape=(150, 150, 3),\n+    include_top=False,\n+)  # Do not include the ImageNet classifier at the top.\n+\n+# Freeze the base_model\n+base_model.trainable = False\n+\n+# Create new model on top\n+inputs = keras.Input(shape=(150, 150, 3))\n+\n+# Pre-trained Xception weights requires that input be scaled\n+# from (0, 255) to a range of (-1., +1.), the rescaling layer\n+# outputs: `(inputs * scale) + offset`\n+scale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\n+x = scale_layer(inputs)\n+\n+# The base model contains batchnorm layers. We want to keep them in inference mode\n+# when we unfreeze the base model for fine-tuning, so we make sure that the\n+# base_model is running in inference mode here.\n+x = base_model(x, training=False)\n+x = keras.layers.GlobalAveragePooling2D()(x)\n+x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout\n+outputs = keras.layers.Dense(1)(x)\n+model = keras.Model(inputs, outputs)\n+\n+model.summary(show_trainable=True)\n+\n+\"\"\"\n+## Train the top layer\n+\"\"\"\n+\n+model.compile(\n+    optimizer=keras.optimizers.Adam(),\n+    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n+    metrics=[keras.metrics.BinaryAccuracy()],\n+)\n+\n+epochs = 2\n+print(\"Fitting the top layer of the model\")\n+model.fit(train_ds, epochs=epochs, validation_data=validation_ds)\n+\n+\"\"\"\n+## Do a round of fine-tuning of the entire model\n+\n+Finally, let's unfreeze the base model and train the entire model end-to-end with a low\n+ learning rate.\n+\n+Importantly, although the base model becomes trainable, it is still running in\n+inference mode since we passed `training=False` when calling it when we built the\n+model. This means that the batch normalization layers inside won't update their batch\n+statistics. If they did, they would wreck havoc on the representations learned by the\n+ model so far.\n+\"\"\"\n+\n+# Unfreeze the base_model. Note that it keeps running in inference mode\n+# since we passed `training=False` when calling it. This means that\n+# the batchnorm layers will not update their batch statistics.\n+# This prevents the batchnorm layers from undoing all the training\n+# we've done so far.\n+base_model.trainable = True\n+model.summary(show_trainable=True)\n+\n+model.compile(\n+    optimizer=keras.optimizers.Adam(1e-5),  # Low learning rate\n+    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n+    metrics=[keras.metrics.BinaryAccuracy()],\n+)\n+\n+epochs = 1\n+print(\"Fitting the end-to-end model\")\n+model.fit(train_ds, epochs=epochs, validation_data=validation_ds)\n+\n+\"\"\"\n+After 10 epochs, fine-tuning gains us a nice improvement here.\n+Let's evaluate the model on the test dataset:\n+\"\"\"\n+\n+print(\"Test dataset evaluation\")\n+model.evaluate(test_ds)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
