{"custom_id": "keras#e56c7650e25fbaa2f57dddcbe3ce9257d4e8c401", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 3 | Hunks: 6 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 600 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -50,10 +50,10 @@ class BaseSeparableConv(Layer):\n         activation: Activation function. If `None`, no activation is applied.\n         use_bias: bool, if `True`, bias will be added to the output.\n         depthwise_initializer: An initializer for the depthwise convolution\n-            kernel. If None, then the default initializer ('\"glorot_uniform\"')\n+            kernel. If None, then the default initializer (`\"glorot_uniform\"`)\n             will be used.\n         pointwise_initializer: An initializer for the pointwise convolution\n-            kernel. If None, then the default initializer ('\"glorot_uniform\"')\n+            kernel. If None, then the default initializer (`\"glorot_uniform\"`)\n             will be used.\n         bias_initializer: An initializer for the bias vector. If None, the\n             default initializer ('\"zeros\"') will be used.\n\n@@ -48,10 +48,10 @@ class SeparableConv1D(BaseSeparableConv):\n         activation: Activation function. If `None`, no activation is applied.\n         use_bias: bool, if `True`, bias will be added to the output.\n         depthwise_initializer: An initializer for the depthwise convolution\n-            kernel. If None, then the default initializer ('\"glorot_uniform\"')\n+            kernel. If None, then the default initializer (`\"glorot_uniform\"`)\n             will be used.\n         pointwise_initializer: An initializer for the pointwise convolution\n-            kernel. If None, then the default initializer ('\"glorot_uniform\"')\n+            kernel. If None, then the default initializer (`\"glorot_uniform\"`)\n             will be used.\n         bias_initializer: An initializer for the bias vector. If None, the\n             default initializer ('\"zeros\"') will be used.\n\n@@ -48,10 +48,10 @@ class SeparableConv2D(BaseSeparableConv):\n         activation: Activation function. If `None`, no activation is applied.\n         use_bias: bool, if `True`, bias will be added to the output.\n         depthwise_initializer: An initializer for the depthwise convolution\n-            kernel. If None, then the default initializer ('\"glorot_uniform\"')\n+            kernel. If None, then the default initializer (`\"glorot_uniform\"`)\n             will be used.\n         pointwise_initializer: An initializer for the pointwise convolution\n-            kernel. If None, then the default initializer ('\"glorot_uniform\"')\n+            kernel. If None, then the default initializer (`\"glorot_uniform\"`)\n             will be used.\n         bias_initializer: An initializer for the bias vector. If None, the\n             default initializer ('\"zeros\"') will be used.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8a355940ed12af1a6b69d3c92dfcdf1babfa1cf1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 200 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 7 | Complexity Δ (Sum/Max): 10/7 | Churn Δ: 200 | Churn Cumulative: 331 | Contributors (this commit): 9 | Commits (past 90d): 64 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -77,6 +77,7 @@ from keras_core.layers.preprocessing.random_contrast import RandomContrast\n from keras_core.layers.preprocessing.random_crop import RandomCrop\n from keras_core.layers.preprocessing.random_flip import RandomFlip\n from keras_core.layers.preprocessing.random_translation import RandomTranslation\n+from keras_core.layers.preprocessing.random_zoom import RandomZoom\n from keras_core.layers.preprocessing.rescaling import Rescaling\n from keras_core.layers.preprocessing.resizing import Resizing\n from keras_core.layers.preprocessing.string_lookup import StringLookup\n\n@@ -0,0 +1,116 @@\n+import numpy as np\n+import tensorflow as tf\n+\n+from keras_core import backend\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.layer import Layer\n+\n+\n+@keras_core_export(\"keras_core.layers.RandomZoom\")\n+class RandomZoom(Layer):\n+    \"\"\"A preprocessing layer which randomly zooms images during training.\n+\n+    This layer will randomly zoom in or out on each axis of an image\n+    independently, filling empty space according to `fill_mode`.\n+\n+    Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and\n+    of integer or floating point dtype.\n+    By default, the layer will output floats.\n+\n+    Args:\n+        height_factor: a float represented as fraction of value,\n+            or a tuple of size 2 representing lower and upper bound\n+            for zooming vertically. When represented as a single float,\n+            this value is used for both the upper and\n+            lower bound. A positive value means zooming out,\n+            while a negative value\n+            means zooming in. For instance, `height_factor=(0.2, 0.3)`\n+            result in an output zoomed out by a random amount\n+            in the range `[+20%, +30%]`.\n+            `height_factor=(-0.3, -0.2)` result in an output zoomed\n+            in by a random amount in the range `[+20%, +30%]`.\n+        width_factor: a float represented as fraction of value,\n+            or a tuple of size 2 representing lower and upper bound\n+            for zooming horizontally. When\n+            represented as a single float, this value is used\n+            for both the upper and\n+            lower bound. For instance, `width_factor=(0.2, 0.3)`\n+            result in an output\n+            zooming out between 20% to 30%.\n+            `width_factor=(-0.3, -0.2)` result in an\n+            output zooming in between 20% to 30%. `None` means\n+            i.e., zooming vertical and horizontal directions\n+            by preserving the aspect ratio. Defaults to `None`.\n+        fill_mode: Points outside the boundaries of the input are\n+            filled according to the given mode\n+            (one of `{\"constant\", \"reflect\", \"wrap\", \"nearest\"}`).\n+            - *reflect*: `(d c b a | a b c d | d c b a)`\n+                The input is extended by reflecting about\n+                the edge of the last pixel.\n+            - *constant*: `(k k k k | a b c d | k k k k)`\n+                The input is extended by filling all values beyond\n+                the edge with the same constant value k = 0.\n+            - *wrap*: `(a b c d | a b c d | a b c d)` The input is extended by\n+                wrapping around to the opposite edge.\n+            - *nearest*: `(a a a a | a b c d | d d d d)`\n+                The input is extended by the nearest pixel.\n+        interpolation: Interpolation mode. Supported values: `\"nearest\"`,\n+            `\"bilinear\"`.\n+        seed: Integer. Used to create a random seed.\n+        fill_value: a float represents the value to be filled outside\n+            the boundaries when `fill_mode=\"constant\"`.\n+\n+    Example:\n+\n+    >>> input_img = np.random.random((32, 224, 224, 3))\n+    >>> layer = keras_core.layers.RandomZoom(.5, .2)\n+    >>> out_img = layer(input_img)\n+\n+    Input shape:\n+        3D (unbatched) or 4D (batched) tensor with shape:\n+        `(..., height, width, channels)`, in `\"channels_last\"` format.\n+\n+    Output shape:\n+        3D (unbatched) or 4D (batched) tensor with shape:\n+        `(..., height, width, channels)`, in `\"channels_last\"` format.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        height_factor,\n+        width_factor=None,\n+        fill_mode=\"reflect\",\n+        interpolation=\"bilinear\",\n+        seed=None,\n+        fill_value=0.0,\n+        name=None,\n+        **kwargs,\n+    ):\n+        super().__init__(name=name, **kwargs)\n+        self.seed = seed or backend.random.make_default_seed()\n+        self.layer = tf.keras.layers.RandomZoom(\n+            height_factor=height_factor,\n+            width_factor=width_factor,\n+            fill_mode=fill_mode,\n+            interpolation=interpolation,\n+            seed=self.seed,\n+            name=name,\n+            fill_value=fill_value,\n+            **kwargs,\n+        )\n+\n+    def call(self, inputs, training=True):\n+        if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n+            inputs = tf.convert_to_tensor(np.array(inputs))\n+        outputs = self.layer.call(inputs)\n+        if backend.backend() != \"tensorflow\":\n+            outputs = backend.convert_to_tensor(outputs)\n+        return outputs\n+\n+    def compute_output_shape(self, input_shape):\n+        return tuple(self.layer.compute_output_shape(input_shape))\n+\n+    def get_config(self):\n+        config = self.layer.get_config()\n+        config.update({\"seed\": self.seed})\n+        return config\n\n@@ -0,0 +1,83 @@\n+import numpy as np\n+from absl.testing import parameterized\n+\n+from keras_core import backend\n+from keras_core import layers\n+from keras_core import testing\n+\n+\n+class RandomZoomTest(testing.TestCase, parameterized.TestCase):\n+    @parameterized.named_parameters(\n+        (\"random_zoom_in_4_by_6\", -0.4, -0.6),\n+        (\"random_zoom_in_2_by_3\", -0.2, -0.3),\n+        (\"random_zoom_in_tuple_factor\", (-0.4, -0.5), (-0.2, -0.3)),\n+        (\"random_zoom_out_4_by_6\", 0.4, 0.6),\n+        (\"random_zoom_out_2_by_3\", 0.2, 0.3),\n+        (\"random_zoom_out_tuple_factor\", (0.4, 0.5), (0.2, 0.3)),\n+    )\n+    def test_random_zoom(self, height_factor, width_factor):\n+        self.run_layer_test(\n+            layers.RandomZoom,\n+            init_kwargs={\n+                \"height_factor\": height_factor,\n+                \"width_factor\": width_factor,\n+            },\n+            input_shape=(2, 3, 4),\n+            expected_output_shape=(2, 3, 4),\n+            supports_masking=False,\n+        )\n+\n+    def test_random_zoom_out_correctness(self):\n+        input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1))\n+        expected_output = np.asarray(\n+            [\n+                [0, 0, 0, 0, 0],\n+                [0, 5, 7, 9, 0],\n+                [0, 10, 12, 14, 0],\n+                [0, 20, 22, 24, 0],\n+                [0, 0, 0, 0, 0],\n+            ]\n+        )\n+        expected_output = backend.convert_to_tensor(\n+            np.reshape(expected_output, (1, 5, 5, 1))\n+        )\n+        self.run_layer_test(\n+            layers.RandomZoom,\n+            init_kwargs={\n+                \"height_factor\": (0.5, 0.5),\n+                \"width_factor\": (0.8, 0.8),\n+                \"interpolation\": \"nearest\",\n+                \"fill_mode\": \"constant\",\n+            },\n+            input_shape=None,\n+            input_data=input_image,\n+            expected_output=expected_output,\n+            supports_masking=False,\n+        )\n+\n+    def test_random_zoom_in_correctness(self):\n+        input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1))\n+        expected_output = np.asarray(\n+            [\n+                [6, 7, 7, 8, 8],\n+                [11, 12, 12, 13, 13],\n+                [11, 12, 12, 13, 13],\n+                [16, 17, 17, 18, 18],\n+                [16, 17, 17, 18, 18],\n+            ]\n+        )\n+        expected_output = backend.convert_to_tensor(\n+            np.reshape(expected_output, (1, 5, 5, 1))\n+        )\n+        self.run_layer_test(\n+            layers.RandomZoom,\n+            init_kwargs={\n+                \"height_factor\": (-0.5, -0.5),\n+                \"width_factor\": (-0.5, -0.5),\n+                \"interpolation\": \"nearest\",\n+            },\n+            input_shape=None,\n+            input_data=input_image,\n+            expected_output=expected_output,\n+            supports_masking=False,\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3f4651e67b008de8e8a66d5274db7dd66fe8c86d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 159 | Lines Deleted: 3 | Files Changed: 4 | Hunks: 6 | Methods Changed: 8 | Complexity Δ (Sum/Max): 10/7 | Churn Δ: 162 | Churn Cumulative: 383 | Contributors (this commit): 9 | Commits (past 90d): 70 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -75,6 +75,7 @@ from keras_core.layers.preprocessing.normalization import Normalization\n from keras_core.layers.preprocessing.random_brightness import RandomBrightness\n from keras_core.layers.preprocessing.random_contrast import RandomContrast\n from keras_core.layers.preprocessing.random_crop import RandomCrop\n+from keras_core.layers.preprocessing.random_rotation import RandomRotation\n from keras_core.layers.preprocessing.random_flip import RandomFlip\n from keras_core.layers.preprocessing.random_translation import RandomTranslation\n from keras_core.layers.preprocessing.random_zoom import RandomZoom\n\n@@ -49,11 +49,12 @@ class RandomCrop(Layer):\n     \"\"\"\n \n     def __init__(self, height, width, seed=None, name=None, **kwargs):\n-        super().__init__()\n+        super().__init__(name=name, **kwargs)\n+        self.seed = seed or backend.random.make_default_seed()\n         self.layer = tf.keras.layers.RandomCrop(\n             height=height,\n             width=width,\n-            seed=seed,\n+            seed=self.seed,\n             name=name,\n         )\n         self.supports_masking = False\n@@ -70,4 +71,6 @@ class RandomCrop(Layer):\n         return tuple(self.layer.compute_output_shape(input_shape))\n \n     def get_config(self):\n-        return self.layer.get_config()\n+        config = self.layer.get_config()\n+        config.update({\"seed\": self.seed})\n+        return config\n\n@@ -0,0 +1,103 @@\n+import numpy as np\n+import tensorflow as tf\n+\n+from keras_core import backend\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.layer import Layer\n+\n+\n+@keras_core_export(\"keras_core.layers.RandomRotation\")\n+class RandomRotation(Layer):\n+    \"\"\"A preprocessing layer which randomly rotates images during training.\n+\n+    This layer will apply random rotations to each image, filling empty space\n+    according to `fill_mode`.\n+\n+    By default, random rotations are only applied during training.\n+    At inference time, the layer does nothing. If you need to apply random\n+    rotations at inference time, set `training` to True when calling the layer.\n+\n+    Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and\n+    of integer or floating point dtype.\n+    By default, the layer will output floats.\n+\n+    Input shape:\n+        3D (unbatched) or 4D (batched) tensor with shape:\n+        `(..., height, width, channels)`, in `\"channels_last\"` format\n+\n+    Output shape:\n+        3D (unbatched) or 4D (batched) tensor with shape:\n+        `(..., height, width, channels)`, in `\"channels_last\"` format\n+\n+    Args:\n+        factor: a float represented as fraction of 2 Pi, or a tuple of size 2\n+            representing lower and upper bound for rotating clockwise and\n+            counter-clockwise. A positive values means rotating\n+            counter clock-wise,\n+            while a negative value means clock-wise.\n+            When represented as a single\n+            float, this value is used for both the upper and lower bound.\n+            For instance, `factor=(-0.2, 0.3)`\n+            results in an output rotation by a random\n+            amount in the range `[-20% * 2pi, 30% * 2pi]`.\n+            `factor=0.2` results in an\n+            output rotating by a random amount\n+            in the range `[-20% * 2pi, 20% * 2pi]`.\n+        fill_mode: Points outside the boundaries of the input are filled\n+            according to the given mode\n+            (one of `{\"constant\", \"reflect\", \"wrap\", \"nearest\"}`).\n+            - *reflect*: `(d c b a | a b c d | d c b a)`\n+                The input is extended by reflecting about\n+                the edge of the last pixel.\n+            - *constant*: `(k k k k | a b c d | k k k k)`\n+                The input is extended by\n+                filling all values beyond the edge with\n+                the same constant value k = 0.\n+            - *wrap*: `(a b c d | a b c d | a b c d)` The input is extended by\n+                wrapping around to the opposite edge.\n+            - *nearest*: `(a a a a | a b c d | d d d d)`\n+                The input is extended by the nearest pixel.\n+        interpolation: Interpolation mode. Supported values: `\"nearest\"`,\n+            `\"bilinear\"`.\n+        seed: Integer. Used to create a random seed.\n+        fill_value: a float represents the value to be filled outside\n+            the boundaries when `fill_mode=\"constant\"`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        factor,\n+        fill_mode=\"reflect\",\n+        interpolation=\"bilinear\",\n+        seed=None,\n+        fill_value=0.0,\n+        name=None,\n+        **kwargs,\n+    ):\n+        super().__init__(name=name, **kwargs)\n+        self.seed = seed or backend.random.make_default_seed()\n+        self.layer = tf.keras.layers.RandomRotation(\n+            factor=factor,\n+            fill_mode=fill_mode,\n+            interpolation=interpolation,\n+            seed=self.seed,\n+            fill_value=fill_value,\n+            name=name,\n+            **kwargs,\n+        )\n+\n+    def call(self, inputs, training=True):\n+        if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n+            inputs = tf.convert_to_tensor(np.array(inputs))\n+        outputs = self.layer.call(inputs)\n+        if backend.backend() != \"tensorflow\":\n+            outputs = backend.convert_to_tensor(outputs)\n+        return outputs\n+\n+    def compute_output_shape(self, input_shape):\n+        return tuple(self.layer.compute_output_shape(input_shape))\n+\n+    def get_config(self):\n+        config = self.layer.get_config()\n+        config.update({\"seed\": self.seed})\n+        return config\n\n@@ -0,0 +1,49 @@\n+import numpy as np\n+from absl.testing import parameterized\n+\n+from keras_core import backend\n+from keras_core import layers\n+from keras_core import testing\n+\n+\n+class RandomRotationTest(testing.TestCase, parameterized.TestCase):\n+    @parameterized.named_parameters(\n+        (\"random_rotate_4\", 0.4),\n+        (\"random_rotate_3\", 0.3),\n+        (\"random_rotate_tuple_factor\", (-0.5, 0.4)),\n+    )\n+    def test_random_rotation(self, factor):\n+        self.run_layer_test(\n+            layers.RandomRotation,\n+            init_kwargs={\n+                \"factor\": factor,\n+            },\n+            input_shape=(2, 3, 4),\n+            expected_output_shape=(2, 3, 4),\n+            supports_masking=False,\n+        )\n+\n+    def test_random_rotation_correctness(self):\n+        input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1))\n+        expected_output = np.asarray(\n+            [\n+                [24, 23, 22, 21, 20],\n+                [19, 18, 17, 16, 15],\n+                [14, 13, 12, 11, 10],\n+                [9, 8, 7, 6, 5],\n+                [4, 3, 2, 1, 0],\n+            ]\n+        )\n+        expected_output = backend.convert_to_tensor(\n+            np.reshape(expected_output, (1, 5, 5, 1))\n+        )\n+        self.run_layer_test(\n+            layers.RandomRotation,\n+            init_kwargs={\n+                \"factor\": (0.5, 0.5),\n+            },\n+            input_shape=None,\n+            input_data=input_image,\n+            expected_output=expected_output,\n+            supports_masking=False,\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#61ae4f2f5989b9ff0aeb75e68cbf24c4d93c9e81", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 1 | Files Changed: 3 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 370 | Contributors (this commit): 9 | Commits (past 90d): 68 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -75,8 +75,8 @@ from keras_core.layers.preprocessing.normalization import Normalization\n from keras_core.layers.preprocessing.random_brightness import RandomBrightness\n from keras_core.layers.preprocessing.random_contrast import RandomContrast\n from keras_core.layers.preprocessing.random_crop import RandomCrop\n-from keras_core.layers.preprocessing.random_rotation import RandomRotation\n from keras_core.layers.preprocessing.random_flip import RandomFlip\n+from keras_core.layers.preprocessing.random_rotation import RandomRotation\n from keras_core.layers.preprocessing.random_translation import RandomTranslation\n from keras_core.layers.preprocessing.random_zoom import RandomZoom\n from keras_core.layers.preprocessing.rescaling import Rescaling\n\n@@ -21,6 +21,14 @@ class RandomRotation(Layer):\n     of integer or floating point dtype.\n     By default, the layer will output floats.\n \n+    **Note:** This layer wraps `tf.keras.layers.RandomRotation`. It cannot\n+    be used as part of the compiled computation graph of a model with\n+    any backend other than TensorFlow.\n+    It can however be used with any backend when running eagerly.\n+    It can also always be used as part of an input preprocessing pipeline\n+    with any backend (outside the model itself), which is how we recommend\n+    to use this layer.\n+\n     Input shape:\n         3D (unbatched) or 4D (batched) tensor with shape:\n         `(..., height, width, channels)`, in `\"channels_last\"` format\n\n@@ -17,6 +17,14 @@ class RandomZoom(Layer):\n     of integer or floating point dtype.\n     By default, the layer will output floats.\n \n+    **Note:** This layer wraps `tf.keras.layers.RandomZoom`. It cannot\n+    be used as part of the compiled computation graph of a model with\n+    any backend other than TensorFlow.\n+    It can however be used with any backend when running eagerly.\n+    It can also always be used as part of an input preprocessing pipeline\n+    with any backend (outside the model itself), which is how we recommend\n+    to use this layer.\n+\n     Args:\n         height_factor: a float represented as fraction of value,\n             or a tuple of size 2 representing lower and upper bound\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#69a34302603094432448bf6d5718541a95410bbb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 65 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 8/7 | Churn Δ: 68 | Churn Cumulative: 925 | Contributors (this commit): 4 | Commits (past 90d): 13 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,4 @@\n+import keras_core.operations as ops\n from keras_core.api_export import keras_core_export\n from keras_core.layers.convolutional.base_conv import BaseConv\n \n@@ -22,10 +23,15 @@ class Conv1D(BaseConv):\n         strides: int or tuple/list of 1 integer, specifying the stride length\n             of the convolution. `stride value != 1` is incompatible with\n             `dilation_rate != 1`.\n-        padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n+        padding: string, `\"valid\"`, `\"same\"` or `\"causal\"`(case-insensitive).\n             `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n             the left/right or up/down of the input such that output has the same\n-            height/width dimension as the input.\n+            height/width dimension as the input. `\"causal\"` results in causal\n+            (dilated) convolutions, e.g. `output[t]` does not depend on\n+            `input[t+1:]`. Useful when modeling temporal data where the model\n+            should not violate the temporal order.\n+            See [WaveNet: A Generative Model for Raw Audio, section2.1](\n+            https://arxiv.org/abs/1609.03499).\n         data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n             The ordering of the dimensions in the inputs. `\"channels_last\"`\n             corresponds to inputs with shape `(batch, steps, features)`\n@@ -126,3 +132,39 @@ class Conv1D(BaseConv):\n             bias_constraint=bias_constraint,\n             **kwargs\n         )\n+\n+    def _compute_causal_padding(self):\n+        left_pad = self.dilation_rate[0] * (self.kernel_size[0] - 1)\n+        if self.data_format == \"channels_last\":\n+            causal_padding = [[0, 0], [left_pad, 0], [0, 0]]\n+        else:\n+            causal_padding = [[0, 0], [0, 0], [left_pad, 0]]\n+        return causal_padding\n+\n+    def call(self, inputs):\n+        padding = self.padding\n+        if self.padding == \"causal\":\n+            # Apply causal padding to inputs.\n+            inputs = ops.pad(inputs, self._compute_causal_padding())\n+            padding = \"valid\"\n+\n+        outputs = ops.conv(\n+            inputs,\n+            self.kernel,\n+            strides=list(self.strides),\n+            padding=padding,\n+            dilation_rate=self.dilation_rate,\n+            data_format=self.data_format,\n+        )\n+\n+        if self.use_bias:\n+            if self.data_format == \"channels_last\":\n+                bias_shape = (1,) * (self.rank + 1) + (self.filters,)\n+            else:\n+                bias_shape = (1, self.filters) + (1,) * self.rank\n+            bias = ops.reshape(self.bias, bias_shape)\n+            outputs += bias\n+\n+        if self.activation is not None:\n+            return self.activation(outputs)\n+        return outputs\n\n@@ -30,6 +30,17 @@ class ConvBasicTest(testing.TestCase, parameterized.TestCase):\n             \"input_shape\": (3, 4, 4),\n             \"output_shape\": (3, 4, 6),\n         },\n+        {\n+            \"filters\": 6,\n+            \"kernel_size\": 2,\n+            \"strides\": 1,\n+            \"padding\": \"causal\",\n+            \"data_format\": \"channels_last\",\n+            \"dilation_rate\": (2,),\n+            \"groups\": 2,\n+            \"input_shape\": (3, 4, 4),\n+            \"output_shape\": (3, 4, 6),\n+        },\n         {\n             \"filters\": 6,\n             \"kernel_size\": 2,\n@@ -249,6 +260,15 @@ class ConvCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             \"dilation_rate\": (2,),\n             \"groups\": 2,\n         },\n+        {\n+            \"filters\": 6,\n+            \"kernel_size\": 2,\n+            \"strides\": 1,\n+            \"padding\": \"causal\",\n+            \"data_format\": \"channels_last\",\n+            \"dilation_rate\": (2,),\n+            \"groups\": 2,\n+        },\n         {\n             \"filters\": 6,\n             \"kernel_size\": (2,),\n\n@@ -103,7 +103,7 @@ def compute_conv_output_shape(\n                 f\"`kernel shape={kernel_shape}`, \"\n                 f\"`dilation_rate={dilation_rate}`.\"\n             )\n-    elif padding == \"same\":\n+    elif padding == \"same\" or padding == \"causal\":\n         output_spatial_shape = np.floor((spatial_shape - 1) / strides) + 1\n     output_spatial_shape = tuple([int(i) for i in output_spatial_shape])\n     if data_format == \"channels_last\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#09be5f3f89308ad452fcf5f5b3ca34036b3c9022", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 4438 | Contributors (this commit): 4 | Commits (past 90d): 23 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -1416,8 +1416,8 @@ class NumpyOneInputOpsStaticShapeTest(testing.TestCase):\n         self.assertEqual(knp.ones_like(x).dtype, x.dtype)\n \n     def test_zeros_like(self):\n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.zeros_like(x).shape, (None, 3))\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.zeros_like(x).shape, (2, 3))\n         self.assertEqual(knp.zeros_like(x).dtype, x.dtype)\n \n     def test_pad(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#23dfa7284a4119fee79b32bcee4c7de2652c2541", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 249 | Contributors (this commit): 4 | Commits (past 90d): 9 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -104,9 +104,7 @@ class EpochIterator:\n                     \"sample_weights\", \"the sample weights\", \"PyDataset\"\n                 )\n         elif isinstance(x, types.GeneratorType):\n-            self.data_adapter = generator_data_adapter.GeneratorDataAdapter(\n-                x, shuffle=shuffle\n-            )\n+            self.data_adapter = generator_data_adapter.GeneratorDataAdapter(x)\n             if y is not None:\n                 raise_unsupported_arg(\"y\", \"the targets\", \"PyDataset\")\n             if sample_weight is not None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#44e8d04338b8fe9f8e305eb18fb1fe07c428376f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 7 | Files Changed: 5 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 37 | Churn Cumulative: 2902 | Contributors (this commit): 7 | Commits (past 90d): 48 | Contributors (cumulative): 14 | DMM Complexity: 0.7391304347826086\n\nDIFF:\n@@ -417,4 +417,4 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         tf_keras_layer.bias.assign(bias_weights)\n         outputs = layer(inputs)\n         expected = tf_keras_layer(inputs)\n-        self.assertAllClose(outputs, expected)\n+        self.assertAllClose(outputs, expected, atol=1e-5)\n\n@@ -102,5 +102,7 @@ class MetricTest(testing.TestCase):\n         self.assertEqual(len(metric.variables), 6)\n \n     def test_serialization(self):\n-        # TODO\n-        pass\n+        self.run_class_serialization_test(\n+            ExampleMetric(name=\"mse\"),\n+            custom_objects={\"ExampleMetric\": ExampleMetric},\n+        )\n\n@@ -67,11 +67,15 @@ def get_metric(identifier, y_true, y_pred):\n             y_true, y_pred\n         )\n         if is_binary:\n-            metric_obj = metrics_module.binary_accuracy\n+            metric_obj = metrics_module.BinaryAccuracy(name=str(identifier))\n         elif is_sparse_categorical:\n-            metric_obj = metrics_module.sparse_categorical_accuracy\n+            metric_obj = metrics_module.SparseCategoricalAccuracy(\n+                name=str(identifier)\n+            )\n         else:\n-            metric_obj = metrics_module.categorical_accuracy\n+            metric_obj = metrics_module.CategoricalAccuracy(\n+                name=str(identifier)\n+            )\n \n     if not isinstance(metric_obj, metrics_module.Metric):\n         if isinstance(identifier, str):\n\n@@ -178,6 +178,21 @@ class TestCompileMetrics(testing.TestCase):\n         self.assertAllClose(result[\"mean_squared_error\"], 0.0)\n         self.assertAllClose(result[\"weighted_mean_squared_error\"], 0.0)\n \n+    def test_name_conversions(self):\n+        compile_metrics = CompileMetrics(\n+            metrics=[\"acc\", \"accuracy\"],\n+            weighted_metrics=[],\n+        )\n+        y_true = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n+        y_pred = np.array([[0.4, 0.1], [0.2, 0.6], [0.6, 0.1]])\n+        compile_metrics.build(y_true, y_pred)\n+        compile_metrics.update_state(y_true, y_pred, sample_weight=None)\n+        result = compile_metrics.result()\n+        self.assertTrue(isinstance(result, dict))\n+        self.assertEqual(len(result), 2)\n+        self.assertAllClose(result[\"acc\"], 0.333333)\n+        self.assertAllClose(result[\"accuracy\"], 0.333333)\n+\n \n class TestCompileLoss(testing.TestCase):\n     def test_single_output_case(self):\n\n@@ -104,7 +104,9 @@ class EpochIterator:\n                     \"sample_weights\", \"the sample weights\", \"PyDataset\"\n                 )\n         elif isinstance(x, types.GeneratorType):\n-            self.data_adapter = generator_data_adapter.GeneratorDataAdapter(x)\n+            self.data_adapter = generator_data_adapter.GeneratorDataAdapter(\n+                x, shuffle=shuffle\n+            )\n             if y is not None:\n                 raise_unsupported_arg(\"y\", \"the targets\", \"PyDataset\")\n             if sample_weight is not None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a4808834eefa8b4bdf309b7057e97bea911194b5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 2637 | Contributors (this commit): 15 | Commits (past 90d): 6 | Contributors (cumulative): 15 | DMM Complexity: 0.0\n\nDIFF:\n@@ -671,8 +671,7 @@ def get_training_or_validation_split(samples, labels, validation_split, subset):\n     num_val_samples = int(validation_split * len(samples))\n     if subset == \"training\":\n         io_utils.print_msg(\n-            f\"Using {len(samples) - num_val_samples} \"\n-            f\"files for training.\"\n+            f\"Using {len(samples) - num_val_samples} \" f\"files for training.\"\n         )\n         samples = samples[:-num_val_samples]\n         labels = labels[:-num_val_samples]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6cc9c6dee50bed93645d33b2e240b6fa30df76df", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 81 | Lines Deleted: 901 | Files Changed: 16 | Hunks: 40 | Methods Changed: 141 | Complexity Δ (Sum/Max): -151/0 | Churn Δ: 982 | Churn Cumulative: 5630 | Contributors (this commit): 5 | Commits (past 90d): 63 | Contributors (cumulative): 32 | DMM Complexity: 0.0\n\nDIFF:\n@@ -29,8 +29,5 @@ if backend() == \"tensorflow\":\n elif backend() == \"jax\":\n     print_msg(\"Using JAX backend.\")\n     from keras_core.backend.jax import *  # noqa: F403\n-elif backend() == \"pytorch\":\n-    print_msg(\"Using PyTorch backend.\")\n-    from keras_core.backend.pytorch import *  # noqa: F403\n else:\n     raise ValueError(f\"Unable to import backend : {backend()}\")\n\n@@ -1,123 +0,0 @@\n-import numpy as np\n-import torch\n-\n-from keras_core.backend.common import KerasVariable\n-from keras_core.backend.common import get_autocast_scope\n-from keras_core.backend.common import standardize_dtype\n-from keras_core.backend.common import standardize_shape\n-from keras_core.backend.common.keras_tensor import KerasTensor\n-from keras_core.backend.common.stateless_scope import StatelessScope\n-from keras_core.backend.common.stateless_scope import get_stateless_scope\n-from keras_core.backend.common.stateless_scope import in_stateless_scope\n-\n-\n-def convert_to_tensor(x, dtype=None):\n-    # TODO: Need to address device placement arg of `as_tensor`\n-    if dtype is not None:\n-        dtype = standardize_dtype(dtype)\n-    if isinstance(x, Variable):\n-        if dtype and dtype != x.dtype:\n-            return x.value.astype(dtype)\n-        return x.value\n-    return torch.as_tensor(x, dtype=dtype)\n-\n-\n-def is_tensor(x):\n-    return torch.is_tensor(x)\n-\n-\n-def shape(x):\n-    return x.shape\n-\n-\n-def cast(x, dtype):\n-    return x.to(dtype)\n-\n-\n-def cond(pred, true_fn, false_fn):\n-    if pred:\n-        return true_fn\n-    return false_fn\n-\n-\n-def name_scope(name):\n-    # TODO: PyTorch doesn't have Named Scope\n-    return name\n-\n-\n-def vectorized_map(function, elements):\n-    return torch.vmap(function)(elements)\n-\n-\n-class Variable(KerasVariable):\n-    def _initialize(self, value):\n-        self._value = convert_to_tensor(value, dtype=self._dtype)\n-\n-    def assign(self, value):\n-        value = convert_to_tensor(value, dtype=self.dtype)\n-        if value.shape != self.shape:\n-            raise ValueError(\n-                \"The shape of the target variable and \"\n-                \"the shape of the target value in \"\n-                \"`variable.assign(value)` must match. \"\n-                f\"Received: value.shape={value.shape}; \"\n-                f\"variable.shape={self.value.shape}\"\n-            )\n-        if in_stateless_scope():\n-            scope = get_stateless_scope()\n-            scope.add_update((self, value))\n-        else:\n-            # torch `as_tensor` by default, doesn't copy if tensor is same type\n-            self._value = convert_to_tensor(value, dtype=self.dtype)\n-\n-    @property\n-    def value(self):\n-        if in_stateless_scope():\n-            scope = get_stateless_scope()\n-            value = scope.get_current_value(self)\n-            if value is not None:\n-                return self._maybe_autocast(value)\n-        if self._value is None:\n-            # Unitialized variable. Return a placeholder.\n-            # This is fine because it's only ever used\n-            # during shape inference in a scratch graph\n-            # (anything else would be a bug, to be fixed.)\n-            return self._maybe_autocast(\n-                convert_to_tensor(\n-                    self._initializer(self._shape, dtype=self._dtype),\n-                    dtype=self._dtype,\n-                )\n-            )\n-        return self._maybe_autocast(self._value)\n-\n-    def numpy(self):\n-        return np.array(self.value)\n-\n-    # Overload native accessor.\n-    def __torch_tensor__(self):\n-        return self\n-\n-    def _convert_to_tensor(self, value, dtype=None):\n-        return convert_to_tensor(value, dtype=dtype)\n-\n-\n-# Shape / dtype inference util\n-def compute_output_spec(fn, *args, **kwargs):\n-    raise NotImplementedError(\n-        \"`compute_output_spec` not implemented for PyTorch backend\"\n-    )\n-\n-\n-def traceable_tensor(shape, dtype=None):\n-    \"\"\"Create a \"traceable tensor\".\n-\n-    That's a tensor that can be passed as input\n-    to a stateful backend-native function to\n-    create state during the trace.\n-    \"\"\"\n-    shape = list(shape)\n-    dtype = dtype or \"float32\"\n-    for i, x in enumerate(shape):\n-        if x is None:\n-            shape[i] = 1\n-    return torch.ones(shape, dtype=dtype)\n\\ No newline at end of file\n\n@@ -1,688 +0,0 @@\n-import torch\n-\n-def add(x1, x2):\n-    pass\n-    #return tfnp.add(x1, x2)\n-\n-\n-def einsum(subscripts, *operands, **kwargs):\n-    pass\n-    #return tfnp.einsum(subscripts, *operands, **kwargs)\n-\n-\n-def subtract(x1, x2):\n-    pass\n-    #return tfnp.subtract(x1, x2)\n-\n-\n-def matmul(x1, x2):\n-    pass\n-    #return tfnp.matmul(x1, x2)\n-\n-\n-def multiply(x1, x2):\n-    pass\n-    #return tfnp.multiply(x1, x2)\n-\n-\n-def mean(x, axis=None, keepdims=False):\n-    pass\n-    #return tfnp.mean(x, axis=axis, keepdims=keepdims)\n-\n-\n-def max(x, axis=None, keepdims=False, initial=None):\n-    pass\n-    # The TensorFlow numpy API implementation doesn't support `initial` so we\n-    # handle it manually here.\n-    #if initial is not None:\n-    #    return tf.math.maximum(\n-    #        tfnp.max(x, axis=axis, keepdims=keepdims), initial\n-    #    )\n-\n-    # TensorFlow returns -inf by default for an empty list, but for consistency\n-    # with other backends and the numpy API we want to throw in this case.\n-    #tf.assert_greater(\n-    #    size(x),\n-    #    tf.constant(0, dtype=tf.int64),\n-    #    message=\"Cannot compute the max of an empty tensor.\",\n-    #)\n-\n-    #return tfnp.max(x, axis=axis, keepdims=keepdims)\n-\n-\n-def ones(shape, dtype=\"float32\"):\n-    pass\n-    #return tf.ones(shape, dtype=dtype)\n-\n-\n-def zeros(shape, dtype=\"float32\"):\n-    pass\n-    #return tf.zeros(shape, dtype=dtype)\n-\n-\n-def absolute(x):\n-    pass\n-    #return tfnp.absolute(x)\n-\n-\n-def abs(x):\n-    pass\n-    #return absolute(x)\n-\n-\n-def all(x, axis=None, keepdims=False):\n-    pass\n-    #return tfnp.all(x, axis=axis, keepdims=keepdims)\n-\n-\n-def any(x, axis=None, keepdims=False):\n-    pass\n-    #return tfnp.any(x, axis=axis, keepdims=keepdims)\n-\n-\n-def amax(x, axis=None, keepdims=False):\n-    pass\n-    #return tfnp.amax(x, axis=axis, keepdims=keepdims)\n-\n-\n-def amin(x, axis=None, keepdims=False):\n-    pass\n-    #return tfnp.amin(x, axis=axis, keepdims=keepdims)\n-\n-\n-def append(\n-    x1,\n-    x2,\n-    axis=None,\n-):\n-    pass\n-    #return tfnp.append(x1, x2, axis=axis)\n-\n-\n-def arange(start, stop=None, step=None, dtype=None):\n-    pass\n-    #return tfnp.arange(start, stop, step=step, dtype=dtype)\n-\n-\n-def arccos(x):\n-    pass\n-    #return tfnp.arccos(x)\n-\n-\n-def arcsin(x):\n-    pass\n-    #return tfnp.arcsin(x)\n-\n-\n-def arctan(x):\n-    pass\n-    #return tfnp.arctan(x)\n-\n-\n-def arctan2(x1, x2):\n-    pass\n-    #return tfnp.arctan2(x1, x2)\n-\n-\n-def argmax(x, axis=None):\n-    pass\n-    #return tfnp.argmax(x, axis=axis)\n-\n-\n-def argmin(x, axis=None):\n-    pass\n-    #return tfnp.argmin(x, axis=axis)\n-\n-\n-def argsort(x, axis=-1):\n-    pass\n-    #return tfnp.argsort(x, axis=axis)\n-\n-\n-def array(x, dtype=None):\n-    pass\n-    #return tfnp.array(x, dtype=dtype)\n-\n-\n-def average(x, axis=None, weights=None):\n-    pass\n-    #return tfnp.average(x, weights=weights, axis=axis)\n-\n-\n-def broadcast_to(x, shape):\n-    pass\n-    #return tfnp.broadcast_to(x, shape)\n-\n-\n-def ceil(x):\n-    pass\n-    #return tfnp.ceil(x)\n-\n-\n-def clip(x, x_min, x_max):\n-    pass\n-    #return tfnp.clip(x, x_min, x_max)\n-\n-\n-def concatenate(xs, axis=0):\n-    pass\n-    #return tfnp.concatenate(xs, axis=axis)\n-\n-\n-def conjugate(x):\n-    pass\n-    #return tfnp.conjugate(x)\n-\n-\n-def conj(x):\n-    pass\n-    #return conjugate(x)\n-\n-\n-def copy(x):\n-    pass\n-    #return tfnp.copy(x)\n-\n-\n-def cos(x):\n-    pass\n-    #return tfnp.cos(x)\n-\n-\n-def count_nonzero(x, axis=None):\n-    pass\n-    #return tfnp.count_nonzero(x, axis=axis)\n-\n-\n-def cross(x1, x2, axisa=-1, axisb=-1, axisc=-1, axis=None):\n-    pass\n-    #return tfnp.cross(\n-    #    x1,\n-    #    x2,\n-    #    axisa=axisa,\n-    #    axisb=axisb,\n-    #    axisc=axisc,\n-    #    axis=axis,\n-    #)\n-\n-\n-def cumprod(x, axis=None):\n-    pass\n-    #return tfnp.cumprod(x, axis=axis)\n-\n-\n-def cumsum(x, axis=None):\n-    pass\n-    #return tfnp.cumsum(x, axis=axis)\n-\n-\n-def diag(x, k=0):\n-    pass\n-    #return tfnp.diag(x, k=k)\n-\n-\n-def diagonal(x, offset=0, axis1=0, axis2=1):\n-    pass\n-    #return tfnp.diagonal(\n-    #    x,\n-    #    offset=offset,\n-    #    axis1=axis1,\n-    #    axis2=axis2,\n-    #)\n-\n-\n-def dot(x, y):\n-    pass\n-    #return tfnp.dot(x, y)\n-\n-\n-def empty(shape, dtype=\"float32\"):\n-    pass\n-    #return tfnp.empty(shape, dtype=dtype)\n-\n-\n-def equal(x1, x2):\n-    pass\n-    #return tfnp.equal(x1, x2)\n-\n-\n-def exp(x):\n-    pass\n-    #return tfnp.exp(x)\n-\n-\n-def expand_dims(x, axis):\n-    pass\n-    #return tfnp.expand_dims(x, axis)\n-\n-\n-def expm1(x):\n-    pass\n-    #return tfnp.expm1(x)\n-\n-\n-def flip(x, axis=None):\n-    pass\n-    #return tfnp.flip(x, axis=axis)\n-\n-\n-def floor(x):\n-    pass\n-    #return tfnp.floor(x)\n-\n-\n-def full(shape, fill_value, dtype=None):\n-    pass\n-    return tfnp.full(shape, fill_value, dtype=dtype)\n-\n-\n-def full_like(x, fill_value, dtype=None):\n-    pass\n-    return tfnp.full_like(x, fill_value, dtype=dtype)\n-\n-\n-def greater(x1, x2):\n-    pass\n-    return tfnp.greater(x1, x2)\n-\n-\n-def greater_equal(x1, x2):\n-    pass\n-    return tfnp.greater_equal(x1, x2)\n-\n-\n-def hstack(xs):\n-    pass\n-    return tfnp.hstack(xs)\n-\n-\n-def identity(n, dtype=\"float32\"):\n-    pass\n-    return tfnp.identity(n, dtype=dtype)\n-\n-\n-def imag(x):\n-    pass\n-    return tfnp.imag(x)\n-\n-\n-def isclose(x1, x2):\n-    pass\n-    return tfnp.isclose(x1, x2)\n-\n-\n-def isfinite(x):\n-    pass\n-    return tfnp.isfinite(x)\n-\n-\n-def isinf(x):\n-    pass\n-    return tfnp.isinf(x)\n-\n-\n-def isnan(x):\n-    pass\n-    return tfnp.isnan(x)\n-\n-\n-def less(x1, x2):\n-    pass\n-    return tfnp.less(x1, x2)\n-\n-\n-def less_equal(x1, x2):\n-    pass\n-    return tfnp.less_equal(x1, x2)\n-\n-\n-def linspace(\n-    start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0\n-):\n-    pass\n-    #return tfnp.linspace(\n-    #    start,\n-    #    stop,\n-    #    num=num,\n-    #    endpoint=endpoint,\n-    #    retstep=retstep,\n-    #    dtype=dtype,\n-    #    axis=axis,\n-    #)\n-\n-\n-def log(x):\n-    pass\n-    #return tfnp.log(x)\n-\n-\n-def log10(x):\n-    pass\n-    #return tfnp.log10(x)\n-\n-\n-def log1p(x):\n-    pass\n-    #return tfnp.log1p(x)\n-\n-\n-def log2(x):\n-    pass\n-    #return tfnp.log2(x)\n-\n-\n-def logaddexp(x1, x2):\n-    pass\n-    #return tfnp.logaddexp(x1, x2)\n-\n-\n-def logical_and(x1, x2):\n-    pass\n-    #return tfnp.logical_and(x1, x2)\n-\n-\n-def logical_not(x):\n-    pass\n-    #return tfnp.logical_not(x)\n-\n-\n-def logical_or(x1, x2):\n-    pass\n-    #return tfnp.logical_or(x1, x2)\n-\n-\n-def logspace(start, stop, num=50, endpoint=True, base=10, dtype=None, axis=0):\n-    pass\n-    #return tfnp.logspace(\n-    #    start,\n-    #    stop,\n-    #    num=num,\n-    #    endpoint=endpoint,\n-    #    base=base,\n-    #    dtype=dtype,\n-    #    axis=axis,\n-    #)\n-\n-\n-def maximum(x1, x2):\n-    pass\n-    #return tfnp.maximum(x1, x2)\n-\n-\n-def meshgrid(*x, indexing=\"xy\"):\n-    pass\n-    #return tfnp.meshgrid(*x, indexing=indexing)\n-\n-\n-def min(x, axis=None, keepdims=False, initial=None):\n-    pass\n-    ## The TensorFlow numpy API implementation doesn't support `initial` so we\n-    ## handle it manually here.\n-    #if initial is not None:\n-    #    return tf.math.minimum(\n-    #        tfnp.min(x, axis=axis, keepdims=keepdims), initial\n-    #    )\n-\n-    ## TensorFlow returns inf by default for an empty list, but for consistency\n-    ## with other backends and the numpy API we want to throw in this case.\n-    #tf.assert_greater(\n-    #    size(x),\n-    #    tf.constant(0, dtype=tf.int64),\n-    #    message=\"Cannot compute the min of an empty tensor.\",\n-    #)\n-\n-    return tfnp.min(x, axis=axis, keepdims=keepdims)\n-\n-\n-def minimum(x1, x2):\n-    pass\n-    #return tfnp.minimum(x1, x2)\n-\n-\n-def mod(x1, x2):\n-    pass\n-    #return tfnp.mod(x1, x2)\n-\n-\n-def moveaxis(x, source, destination):\n-    pass\n-    #return tfnp.moveaxis(x, source=source, destination=destination)\n-\n-\n-def nan_to_num(x):\n-    pass\n-    ## Replace NaN with 0\n-    #x = tf.where(tf.math.is_nan(x), 0, x)\n-\n-    ## Replace positive infinitiy with dtype.max\n-    #x = tf.where(tf.math.is_inf(x) & (x > 0), x.dtype.max, x)\n-\n-    ## Replace negative infinity with dtype.min\n-    #x = tf.where(tf.math.is_inf(x) & (x < 0), x.dtype.min, x)\n-\n-    #return x\n-\n-\n-def ndim(x):\n-    pass\n-    #return tfnp.ndim(x)\n-\n-\n-def nonzero(x):\n-    pass\n-    #return tfnp.nonzero(x)\n-\n-\n-def not_equal(x1, x2):\n-    pass\n-    #return tfnp.not_equal(x1, x2)\n-\n-\n-def ones_like(x, dtype=None):\n-    pass\n-    #return tfnp.ones_like(x, dtype=dtype)\n-\n-\n-def outer(x1, x2):\n-    pass\n-    #return tfnp.outer(x1, x2)\n-\n-\n-def pad(x, pad_width, mode=\"constant\"):\n-    pass\n-    #return tfnp.pad(x, pad_width, mode=mode)\n-\n-\n-def prod(x, axis=None, keepdims=False, dtype=None):\n-    pass\n-    #return tfnp.prod(x, axis=axis, keepdims=keepdims, dtype=dtype)\n-\n-\n-def ravel(x):\n-    pass\n-    #return tfnp.ravel(x)\n-\n-\n-def real(x):\n-    pass\n-    #return tfnp.real(x)\n-\n-\n-def reciprocal(x):\n-    pass\n-    #return tfnp.reciprocal(x)\n-\n-\n-def repeat(x, repeats, axis=None):\n-    pass\n-    #return tfnp.repeat(x, repeats, axis=axis)\n-\n-\n-def reshape(x, new_shape):\n-    pass\n-    #return tfnp.reshape(x, new_shape)\n-\n-\n-def roll(x, shift, axis=None):\n-    pass\n-    #return tfnp.roll(x, shift, axis=axis)\n-\n-\n-def sign(x):\n-    pass\n-    #return tfnp.sign(x)\n-\n-\n-def sin(x):\n-    pass\n-    #return tfnp.sin(x)\n-\n-\n-def size(x):\n-    pass\n-    #return tfnp.size(x)\n-\n-\n-def sort(x, axis=-1):\n-    pass\n-    #return tfnp.sort(x, axis=axis)\n-\n-\n-def split(x, indices_or_sections, axis=0):\n-    pass\n-    #return tfnp.split(x, indices_or_sections, axis=axis)\n-\n-\n-def stack(x, axis=0):\n-    pass\n-    #return tfnp.stack(x, axis=axis)\n-\n-\n-def std(x, axis=None, keepdims=False):\n-    pass\n-    #return tfnp.std(x, axis=axis, keepdims=keepdims)\n-\n-\n-def swapaxes(x, axis1, axis2):\n-    pass\n-    #return tfnp.swapaxes(x, axis1=axis1, axis2=axis2)\n-\n-\n-def take(x, indices, axis=None):\n-    pass\n-    #return tfnp.take(x, indices, axis=axis)\n-\n-\n-def take_along_axis(x, indices, axis=None):\n-    pass\n-    #return tfnp.take_along_axis(x, indices, axis=axis)\n-\n-\n-def tan(x):\n-    pass\n-    #return tfnp.tan(x)\n-\n-\n-def tensordot(x1, x2, axes=2):\n-    pass\n-    #return tfnp.tensordot(x1, x2, axes=axes)\n-\n-\n-def round(x, decimals=0):\n-    pass\n-    #return tfnp.round(x, decimals=decimals)\n-\n-\n-def tile(x, repeats):\n-    pass\n-    #return tfnp.tile(x, repeats)\n-\n-\n-def trace(x, offset=0, axis1=0, axis2=1):\n-    pass\n-    #return tfnp.trace(x, offset=offset, axis1=axis1, axis2=axis2)\n-\n-\n-def tri(N, M=None, k=0, dtype=\"float32\"):\n-    pass\n-    #return tfnp.tri(N, M=M, k=k, dtype=dtype)\n-\n-\n-def tril(x, k=0):\n-    pass\n-    #return tfnp.tril(x, k=k)\n-\n-\n-def triu(x, k=0):\n-    pass\n-    return tfnp.triu(x, k=k)\n-\n-\n-def vdot(x1, x2):\n-    pass\n-    #return tfnp.vdot(x1, x2)\n-\n-\n-def vstack(xs):\n-    pass\n-    #return tfnp.vstack(xs)\n-\n-\n-def where(condition, x1, x2):\n-    pass\n-    #return tfnp.where(condition, x1, x2)\n-\n-\n-def divide(x1, x2):\n-    pass\n-    #return tfnp.divide(x1, x2)\n-\n-\n-def true_divide(x1, x2):\n-    pass\n-    #return tfnp.true_divide(x1, x2)\n-\n-\n-def power(x1, x2):\n-    pass\n-    #return tfnp.power(x1, x2)\n-\n-\n-def negative(x):\n-    pass\n-    #return tfnp.negative(x)\n-\n-\n-def square(x):\n-    pass\n-    #return tfnp.square(x)\n-\n-\n-def sqrt(x):\n-    pass\n-    #return tfnp.sqrt(x)\n-\n-\n-def squeeze(x, axis=None):\n-    pass\n-    #return tfnp.squeeze(x, axis=axis)\n-\n-\n-def transpose(x, axes=None):\n-    pass\n-    #return tfnp.transpose(x, axes=axes)\n-\n-\n-def var(x, axis=None, keepdims=False):\n-    pass\n-    #return tfnp.var(x, axis=axis, keepdims=keepdims)\n-\n-\n-def sum(x, axis=None, keepdims=False):\n-    pass\n-    #return tfnp.sum(x, axis=axis, keepdims=keepdims)\n-\n-\n-def eye(N, M=None, k=0, dtype=\"float32\"):\n-    pass\n-    #return tfnp.eye(N, M=M, k=k, dtype=dtype)\n\n@@ -14,11 +14,11 @@ from keras_core.operations.operation_utils import compute_conv_output_shape\n class BaseConv(Layer):\n     \"\"\"Abstract N-D convolution layer (private, used as implementation base).\n \n-    This layer creates a convolution kernel that is convolved\n-    (actually cross-correlated) with the layer input to produce a tensor of\n-    outputs. If `use_bias` is True (and a `bias_initializer` is provided),\n-    a bias vector is created and added to the outputs. Finally, if\n-    `activation` is not `None`, it is applied to the outputs as well.\n+    This layer creates a convolution kernel that is convolved (actually\n+    cross-correlated) with the layer input to produce a tensor of outputs. If\n+    `use_bias` is True (and a `bias_initializer` is provided), a bias vector is\n+    created and added to the outputs. Finally, if `activation` is not `None`, it\n+    is applied to the outputs as well.\n \n     Note: layer attributes cannot be modified after the layer has been called\n     once (except the `trainable` attribute).\n@@ -27,12 +27,12 @@ class BaseConv(Layer):\n         rank: int, the rank of the convolution, e.g. 2 for 2D convolution.\n         filters: int, the dimension of the output space (the number of filters\n             in the convolution).\n-        kernel_size: int or tuple/list of N integers (N=`rank`), specifying the\n-            size of the convolution window.\n-        strides: int or tuple/list of N integers, specifying the stride length\n-            of the convolution. If only one int is specified, the same stride\n-            size will be used for all dimensions. `stride value != 1` is\n-            incompatible with `dilation_rate != 1`.\n+        kernel_size: int or tuple/list of `rank` integers, specifying the size\n+            of the convolution window.\n+        strides: int or tuple/list of `rank` integers, specifying the stride\n+            length of the convolution. If only one int is specified, the same\n+            stride size will be used for all dimensions. `strides > 1` is\n+            incompatible with `dilation_rate > 1`.\n         padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n             `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n             the left/right or up/down of the input such that output has the same\n@@ -44,9 +44,9 @@ class BaseConv(Layer):\n             `(batch, features, steps)`. It defaults to the `image_data_format`\n             value found in your Keras config file at `~/.keras/keras.json`.\n             If you never set it, then it will be `\"channels_last\"`.\n-        dilation_rate: int or tuple/list of N integers, specifying the dilation\n-            rate to use for dilated convolution. If only one int is specified,\n-            the same dilation rate will be used for all dimensions.\n+        dilation_rate: int or tuple/list of `rank` integers, specifying the\n+            dilation rate to use for dilated convolution. If only one int is\n+            specified, the same dilation rate will be used for all dimensions.\n         groups: A positive int specifying the number of groups in which the\n             input is split along the channel axis. Each group is convolved\n             separately with `filters // groups` filters. The output is the\n@@ -97,7 +97,7 @@ class BaseConv(Layer):\n         super().__init__(\n             trainable=trainable,\n             name=name,\n-            activity_regularizer=regularizers.get(activity_regularizer),\n+            activity_regularizer=activity_regularizer,\n             **kwargs,\n         )\n         self.rank = rank\n@@ -144,14 +144,14 @@ class BaseConv(Layer):\n \n         if not all(self.kernel_size):\n             raise ValueError(\n-                \"The argument `kernel_size` cannot contain 0(s). Received: \"\n-                f\"{self.kernel_size}\"\n+                \"The argument `kernel_size` cannot contain 0. Received \"\n+                f\"kernel_size={self.kernel_size}.\"\n             )\n \n         if not all(self.strides):\n             raise ValueError(\n-                \"The argument `strides` cannot contains 0(s). Received: \"\n-                f\"{self.strides}\"\n+                \"The argument `strides` cannot contains 0. Received \"\n+                f\"strides={self.strides}\"\n             )\n \n         if max(self.strides) > 1 and max(self.dilation_rate) > 1:\n\n@@ -14,26 +14,25 @@ from keras_core.layers.layer import Layer\n \n \n class BaseConvTranspose(Layer):\n-    \"\"\"Abstract N-D transpose convolution layer.\n+    \"\"\"Abstract N-D transposed convolution layer.\n \n-    The need for transposed convolutions generally arises\n-    from the desire to use a transformation going in the opposite direction\n-    of a normal convolution, i.e., from something that has the shape of the\n-    output of some convolution to something that has the shape of its input\n-    while maintaining a connectivity pattern that is compatible with\n-    said convolution.\n+    The need for transposed convolutions generally arises from the desire to use\n+    a transformation going in the opposite direction of a normal convolution,\n+    i.e., from something that has the shape of the output of some convolution to\n+    something that has the shape of its input while maintaining a connectivity\n+    pattern that is compatible with said convolution.\n \n     Args:\n         rank: int, the rank of the transposed convolution, e.g. 2 for 2D\n             transposed convolution.\n         filters: int, the dimension of the output space (the number of filters\n             in the transposed convolution).\n-        kernel_size: int or tuple/list of N integers (N=`rank`), specifying the\n-            size of the transposed convolution window.\n-        strides: int or tuple/list of N integers, specifying the stride length\n-            of the transposed convolution. If only one int is specified, the\n-            same stride size will be used for all dimensions.\n-            `stride value != 1` is incompatible with `dilation_rate != 1`.\n+        kernel_size: int or tuple/list of `rank` integers, specifying the size\n+            of the transposed convolution window.\n+        strides: int or tuple/list of `rank` integers, specifying the stride\n+            length of the transposed convolution. If only one int is specified,\n+            the same stride size will be used for all dimensions.\n+            `strides > 1` is incompatible with `dilation_rate > 1`.\n         padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n             `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n             the left/right or up/down of the input such that output has the same\n@@ -45,9 +44,9 @@ class BaseConvTranspose(Layer):\n             `(batch, features, steps)`. It defaults to the `image_data_format`\n             value found in your Keras config file at `~/.keras/keras.json`.\n             If you never set it, then it will be `\"channels_last\"`.\n-        dilation_rate: int or tuple/list of N integers, specifying the dilation\n-            rate to use for dilated convolution. If only one int is specified,\n-            the same dilation rate will be used for all dimensions.\n+        dilation_rate: int or tuple/list of `rank` integers, specifying the\n+            dilation rate to use for dilated convolution. If only one int is\n+            specified, the same dilation rate will be used for all dimensions.\n         activation: Activation function. If `None`, no activation is applied.\n         use_bias: bool, if `True`, bias will be added to the output.\n         kernel_initializer: Initializer for the convolution kernel. If `None`,\n@@ -131,14 +130,14 @@ class BaseConvTranspose(Layer):\n \n         if not all(self.kernel_size):\n             raise ValueError(\n-                \"The argument `kernel_size` cannot contain 0. Received: \"\n-                f\"{self.kernel_size}\"\n+                \"The argument `kernel_size` cannot contain 0. Received \"\n+                f\"kernel_size={self.kernel_size}.\"\n             )\n \n         if not all(self.strides):\n             raise ValueError(\n-                \"The argument `strides` cannot contains 0. Received: \"\n-                f\"{self.strides}\"\n+                \"The argument `strides` cannot contains 0. Received \"\n+                f\"strides={self.strides}.\"\n             )\n \n         if max(self.strides) > 1 and max(self.dilation_rate) > 1:\n\n@@ -39,12 +39,12 @@ class BaseDepthwiseConv(Layer):\n         depth_multiplier: The number of depthwise convolution output channels\n             for each input channel. The total number of depthwise convolution\n             output channels will be equal to `input_channel * depth_multiplier`.\n-        kernel_size: int or tuple/list of N integers (N=`rank`), specifying the\n-            size of the depthwise convolution window.\n-        strides: int or tuple/list of N integers, specifying the stride length\n-            of the depthwise convolution. If only one int is specified, the same\n-            stride size will be used for all dimensions. `stride value != 1` is\n-            incompatible with `dilation_rate != 1`.\n+        kernel_size: int or tuple/list of `rank` integers, specifying the size\n+            of the depthwise convolution window.\n+        strides: int or tuple/list of `rank` integers, specifying the stride\n+            length of the depthwise convolution. If only one int is specified,\n+            the same stride size will be used for all dimensions.\n+            `strides > 1` is incompatible with `dilation_rate > 1`.\n         padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n             `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n             the left/right or up/down of the input such that output has the same\n@@ -56,9 +56,9 @@ class BaseDepthwiseConv(Layer):\n             `(batch, features, steps)`. It defaults to the `image_data_format`\n             value found in your Keras config file at `~/.keras/keras.json`.\n             If you never set it, then it will be `\"channels_last\"`.\n-        dilation_rate: int or tuple/list of N integers, specifying the dilation\n-            rate to use for dilated convolution. If only one int is specified,\n-            the same dilation rate will be used for all dimensions.\n+        dilation_rate: int or tuple/list of `rank` integers, specifying the\n+            dilation rate to use for dilated convolution. If only one int is\n+            specified, the same dilation rate will be used for all dimensions.\n         activation: Activation function. If `None`, no activation is applied.\n         use_bias: bool, if `True`, bias will be added to the output.\n         kernel_initializer: Initializer for the convolution kernel. If `None`,\n@@ -143,14 +143,14 @@ class BaseDepthwiseConv(Layer):\n \n         if not all(self.kernel_size):\n             raise ValueError(\n-                \"The argument `kernel_size` cannot contain 0(s). Received: \"\n-                f\"{self.kernel_size}\"\n+                \"The argument `kernel_size` cannot contain 0. Received \"\n+                f\"kernel_size={self.kernel_size}.\"\n             )\n \n         if not all(self.strides):\n             raise ValueError(\n-                \"The argument `strides` cannot contains 0(s). Received: \"\n-                f\"{self.strides}\"\n+                \"The argument `strides` cannot contains 0. Received \"\n+                f\"strides={self.strides}\"\n             )\n \n         if max(self.strides) > 1 and max(self.dilation_rate) > 1:\n\n@@ -21,8 +21,8 @@ class Conv1D(BaseConv):\n         kernel_size: int or tuple/list of 1 integer, specifying the size of the\n             convolution window.\n         strides: int or tuple/list of 1 integer, specifying the stride length\n-            of the convolution. `stride value != 1` is incompatible with\n-            `dilation_rate != 1`.\n+            of the convolution. `strides > 1` is incompatible with\n+            `dilation_rate > 1`.\n         padding: string, `\"valid\"`, `\"same\"` or `\"causal\"`(case-insensitive).\n             `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n             the left/right or up/down of the input such that output has the same\n@@ -72,9 +72,9 @@ class Conv1D(BaseConv):\n \n     Output shape:\n     - If `data_format=\"channels_last\"`:\n-        A 3D tensor with shape: `(batch_shape, new_steps, channels)`\n+        A 3D tensor with shape: `(batch_shape, new_steps, filters)`\n     - If `data_format=\"channels_first\"`:\n-        A 3D tensor with shape: `(batch_shape, channels, new_steps)`\n+        A 3D tensor with shape: `(batch_shape, filters, new_steps)`\n \n     Returns:\n         A 3D tensor representing `activation(conv1d(inputs, kernel) + bias)`.\n\n@@ -25,8 +25,8 @@ class Conv1DTranspose(BaseConvTranspose):\n         kernel_size: int or tuple/list of 1 integer, specifying the size of the\n             transposed convolution window.\n         strides: int or tuple/list of 1 integer, specifying the stride length\n-            of the transposed convolution. `stride value != 1` is incompatible\n-            with `dilation_rate != 1`.\n+            of the transposed convolution. `strides > 1` is incompatible with\n+            `dilation_rate > 1`.\n         padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n             `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n             the left/right or up/down of the input such that output has the same\n@@ -40,11 +40,6 @@ class Conv1DTranspose(BaseConvTranspose):\n             If you never set it, then it will be `\"channels_last\"`.\n         dilation_rate: int or tuple/list of 1 integers, specifying the dilation\n             rate to use for dilated transposed convolution.\n-        groups: A positive int specifying the number of groups in which the\n-            input is split along the channel axis. Each group is convolved\n-            separately with `filters // groups` filters. The output is the\n-            concatenation of all the `groups` results along the channel axis.\n-            Input channels and `filters` must both be divisible by `groups`.\n         activation: Activation function. If `None`, no activation is applied.\n         use_bias: bool, if `True`, bias will be added to the output.\n         kernel_initializer: Initializer for the convolution kernel. If `None`,\n@@ -71,9 +66,9 @@ class Conv1DTranspose(BaseConvTranspose):\n \n     Output shape:\n     - If `data_format=\"channels_last\"`:\n-        A 3D tensor with shape: `(batch_shape, new_steps, channels)`\n+        A 3D tensor with shape: `(batch_shape, new_steps, filters)`\n     - If `data_format=\"channels_first\"`:\n-        A 3D tensor with shape: `(batch_shape, channels, new_steps)`\n+        A 3D tensor with shape: `(batch_shape, filters, new_steps)`\n \n     Returns:\n         A 3D tensor representing\n\n@@ -20,8 +20,8 @@ class Conv2D(BaseConv):\n         kernel_size: int or tuple/list of 2 integer, specifying the size of the\n             convolution window.\n         strides: int or tuple/list of 2 integer, specifying the stride length\n-            of the convolution. `stride value != 1` is incompatible with\n-            `dilation_rate != 1`.\n+            of the convolution. `strides > 1` is incompatible with\n+            `dilation_rate > 1`.\n         padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n             `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n             the left/right or up/down of the input such that output has the same\n@@ -68,7 +68,7 @@ class Conv2D(BaseConv):\n \n     Output shape:\n     - If `data_format=\"channels_last\"`:\n-        A 4D tensor with shape: `(batch_size, new_height, new_width filters)`\n+        A 4D tensor with shape: `(batch_size, new_height, new_width, filters)`\n     - If `data_format=\"channels_first\"`:\n         A 4D tensor with shape: `(batch_size, filters, new_height, new_width)`\n \n\n@@ -25,8 +25,8 @@ class Conv2DTranspose(BaseConvTranspose):\n         kernel_size: int or tuple/list of 1 integer, specifying the size of the\n             transposed convolution window.\n         strides: int or tuple/list of 1 integer, specifying the stride length\n-            of the transposed convolution. `stride value != 1` is incompatible\n-            with `dilation_rate != 1`.\n+            of the transposed convolution. `strides > 1` is incompatible with\n+            `dilation_rate > 1`.\n         padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n             `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n             the left/right or up/down of the input such that output has the same\n@@ -68,7 +68,7 @@ class Conv2DTranspose(BaseConvTranspose):\n \n     Output shape:\n     - If `data_format=\"channels_last\"`:\n-        A 4D tensor with shape: `(batch_size, new_height, new_width filters)`\n+        A 4D tensor with shape: `(batch_size, new_height, new_width, filters)`\n     - If `data_format=\"channels_first\"`:\n         A 4D tensor with shape: `(batch_size, filters, new_height, new_width)`\n \n\n@@ -20,8 +20,8 @@ class Conv3D(BaseConv):\n         kernel_size: int or tuple/list of 3 integer, specifying the size of the\n             convolution window.\n         strides: int or tuple/list of 3 integer, specifying the stride length\n-            of the convolution. `stride value != 1` is incompatible with\n-            `dilation_rate != 1`.\n+            of the convolution. `strides > 1` is incompatible with\n+            `dilation_rate > 1`.\n         padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n             `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n             the left/right or up/down of the input such that output has the same\n@@ -72,10 +72,10 @@ class Conv3D(BaseConv):\n     - If `data_format=\"channels_last\"`:\n         5D tensor with shape:\n         `(batch_size, new_spatial_dim1, new_spatial_dim2, new_spatial_dim3,\n-        channels)`\n+        filters)`\n     - If `data_format=\"channels_first\"`:\n         5D tensor with shape:\n-        `(batch_size, channels, new_spatial_dim1, new_spatial_dim2,\n+        `(batch_size, filters, new_spatial_dim1, new_spatial_dim2,\n         new_spatial_dim3)`\n \n     Returns:\n\n@@ -25,8 +25,8 @@ class Conv3DTranspose(BaseConvTranspose):\n         kernel_size: int or tuple/list of 1 integer, specifying the size of the\n             transposed convolution window.\n         strides: int or tuple/list of 1 integer, specifying the stride length\n-            of the transposed convolution. `stride value != 1` is incompatible\n-            with `dilation_rate != 1`.\n+            of the transposed convolution. `strides > 1` is incompatible with\n+            `dilation_rate > 1`.\n         padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n             `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n             the left/right or up/down of the input such that output has the same\n@@ -72,10 +72,10 @@ class Conv3DTranspose(BaseConvTranspose):\n     - If `data_format=\"channels_last\"`:\n         5D tensor with shape:\n         `(batch_size, new_spatial_dim1, new_spatial_dim2, new_spatial_dim3,\n-        channels)`\n+        filters)`\n     - If `data_format=\"channels_first\"`:\n         5D tensor with shape:\n-        `(batch_size, channels, new_spatial_dim1, new_spatial_dim2,\n+        `(batch_size, filters, new_spatial_dim1, new_spatial_dim2,\n         new_spatial_dim3)`\n \n     Returns:\n\n@@ -34,8 +34,8 @@ class DepthwiseConv1D(BaseDepthwiseConv):\n         kernel_size: int or tuple/list of 1 integer, specifying the size of the\n             depthwise convolution window.\n         strides: int or tuple/list of 1 integer, specifying the stride length\n-            of the convolution. `stride value != 1` is incompatible\n-            with `dilation_rate != 1`.\n+            of the convolution. `strides > 1` is incompatible with\n+            `dilation_rate > 1`.\n         padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n             `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n             the left/right or up/down of the input such that output has the same\n\n@@ -34,8 +34,8 @@ class DepthwiseConv2D(BaseDepthwiseConv):\n         kernel_size: int or tuple/list of 2 integer, specifying the size of the\n             depthwise convolution window.\n         strides: int or tuple/list of 2 integer, specifying the stride length\n-            of the depthwise convolution. `stride value != 1` is incompatible\n-            with `dilation_rate != 1`.\n+            of the depthwise convolution. `strides > 1` is incompatible with\n+            `dilation_rate > 1`.\n         padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n             `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n             the left/right or up/down of the input such that output has the same\n\n@@ -29,8 +29,8 @@ class SeparableConv1D(BaseSeparableConv):\n             depthwise convolution window.\n         strides: int or tuple/list of 1 integers, specifying the stride length\n             of the depthwise convolution. If only one int is specified, the same\n-            stride size will be used for all dimensions. `stride value != 1` is\n-            incompatible with `dilation_rate != 1`.\n+            stride size will be used for all dimensions. `strides > 1` is\n+            incompatible with `dilation_rate > 1`.\n         padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n             `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n             the left/right or up/down of the input such that output has the same\n\n@@ -29,8 +29,8 @@ class SeparableConv2D(BaseSeparableConv):\n             depthwise convolution window.\n         strides: int or tuple/list of 2 integers, specifying the stride length\n             of the depthwise convolution. If only one int is specified, the same\n-            stride size will be used for all dimensions. `stride value != 1` is\n-            incompatible with `dilation_rate != 1`.\n+            stride size will be used for all dimensions. `strides > 1` is\n+            incompatible with `dilation_rate > 1`.\n         padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n             `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n             the left/right or up/down of the input such that output has the same\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
