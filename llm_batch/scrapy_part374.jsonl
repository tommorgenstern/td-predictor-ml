{"custom_id": "scrapy#8985a04bd1328cd6156a7c33a5db74ad8c81802f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 89 | Lines Deleted: 72 | Files Changed: 2 | Hunks: 63 | Methods Changed: 39 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 161 | Churn Cumulative: 2586 | Contributors (this commit): 55 | Commits (past 90d): 4 | Contributors (cumulative): 67 | DMM Complexity: 0.7619047619047619\n\nDIFF:\n@@ -3,16 +3,18 @@ Item Exporters are used to export/serialize items into different formats.\n \"\"\"\n \n import csv\n-import io\n import marshal\n import pickle  # nosec\n import pprint\n-from collections.abc import Mapping\n+from io import BytesIO, TextIOWrapper\n+from json import JSONEncoder\n+from typing import Any, Callable, Dict, Iterable, Mapping, Optional, Tuple, Union\n from xml.sax.saxutils import XMLGenerator  # nosec\n+from xml.sax.xmlreader import AttributesImpl  # nosec\n \n from itemadapter import ItemAdapter, is_item\n \n-from scrapy.item import Item\n+from scrapy.item import Field, Item\n from scrapy.utils.python import is_listlike, to_bytes, to_unicode\n from scrapy.utils.serialize import ScrapyJSONEncoder\n \n@@ -29,36 +31,42 @@ __all__ = [\n \n \n class BaseItemExporter:\n-    def __init__(self, *, dont_fail=False, **kwargs):\n-        self._kwargs = kwargs\n+    def __init__(self, *, dont_fail: bool = False, **kwargs: Any):\n+        self._kwargs: Dict[str, Any] = kwargs\n         self._configure(kwargs, dont_fail=dont_fail)\n \n-    def _configure(self, options, dont_fail=False):\n+    def _configure(self, options: Dict[str, Any], dont_fail: bool = False) -> None:\n         \"\"\"Configure the exporter by popping options from the ``options`` dict.\n         If dont_fail is set, it won't raise an exception on unexpected options\n         (useful for using with keyword arguments in subclasses ``__init__`` methods)\n         \"\"\"\n-        self.encoding = options.pop(\"encoding\", None)\n-        self.fields_to_export = options.pop(\"fields_to_export\", None)\n-        self.export_empty_fields = options.pop(\"export_empty_fields\", False)\n-        self.indent = options.pop(\"indent\", None)\n+        self.encoding: Optional[str] = options.pop(\"encoding\", None)\n+        self.fields_to_export: Union[Mapping[str, str], Iterable[str], None] = (\n+            options.pop(\"fields_to_export\", None)\n+        )\n+        self.export_empty_fields: bool = options.pop(\"export_empty_fields\", False)\n+        self.indent: Optional[int] = options.pop(\"indent\", None)\n         if not dont_fail and options:\n             raise TypeError(f\"Unexpected options: {', '.join(options.keys())}\")\n \n-    def export_item(self, item):\n+    def export_item(self, item: Any) -> None:\n         raise NotImplementedError\n \n-    def serialize_field(self, field, name, value):\n-        serializer = field.get(\"serializer\", lambda x: x)\n+    def serialize_field(\n+        self, field: Union[Mapping[str, Any], Field], name: str, value: Any\n+    ) -> Any:\n+        serializer: Callable[[Any], Any] = field.get(\"serializer\", lambda x: x)\n         return serializer(value)\n \n-    def start_exporting(self):\n+    def start_exporting(self) -> None:\n         pass\n \n-    def finish_exporting(self):\n+    def finish_exporting(self) -> None:\n         pass\n \n-    def _get_serialized_fields(self, item, default_value=None, include_empty=None):\n+    def _get_serialized_fields(\n+        self, item: Any, default_value: Any = None, include_empty: Optional[bool] = None\n+    ) -> Iterable[Tuple[str, Any]]:\n         \"\"\"Return the fields to export as an iterable of tuples\n         (name, serialized_value)\n         \"\"\"\n@@ -100,22 +108,22 @@ class BaseItemExporter:\n \n \n class JsonLinesItemExporter(BaseItemExporter):\n-    def __init__(self, file, **kwargs):\n+    def __init__(self, file: BytesIO, **kwargs: Any):\n         super().__init__(dont_fail=True, **kwargs)\n-        self.file = file\n+        self.file: BytesIO = file\n         self._kwargs.setdefault(\"ensure_ascii\", not self.encoding)\n-        self.encoder = ScrapyJSONEncoder(**self._kwargs)\n+        self.encoder: JSONEncoder = ScrapyJSONEncoder(**self._kwargs)\n \n-    def export_item(self, item):\n+    def export_item(self, item: Any) -> None:\n         itemdict = dict(self._get_serialized_fields(item))\n         data = self.encoder.encode(itemdict) + \"\\n\"\n         self.file.write(to_bytes(data, self.encoding))\n \n \n class JsonItemExporter(BaseItemExporter):\n-    def __init__(self, file, **kwargs):\n+    def __init__(self, file: BytesIO, **kwargs: Any):\n         super().__init__(dont_fail=True, **kwargs)\n-        self.file = file\n+        self.file: BytesIO = file\n         # there is a small difference between the behaviour or JsonItemExporter.indent\n         # and ScrapyJSONEncoder.indent. ScrapyJSONEncoder.indent=None is needed to prevent\n         # the addition of newlines everywhere\n@@ -127,26 +135,26 @@ class JsonItemExporter(BaseItemExporter):\n         self.encoder = ScrapyJSONEncoder(**self._kwargs)\n         self.first_item = True\n \n-    def _beautify_newline(self):\n+    def _beautify_newline(self) -> None:\n         if self.indent is not None:\n             self.file.write(b\"\\n\")\n \n-    def _add_comma_after_first(self):\n+    def _add_comma_after_first(self) -> None:\n         if self.first_item:\n             self.first_item = False\n         else:\n             self.file.write(b\",\")\n             self._beautify_newline()\n \n-    def start_exporting(self):\n+    def start_exporting(self) -> None:\n         self.file.write(b\"[\")\n         self._beautify_newline()\n \n-    def finish_exporting(self):\n+    def finish_exporting(self) -> None:\n         self._beautify_newline()\n         self.file.write(b\"]\")\n \n-    def export_item(self, item):\n+    def export_item(self, item: Any) -> None:\n         itemdict = dict(self._get_serialized_fields(item))\n         data = to_bytes(self.encoder.encode(itemdict), self.encoding)\n         self._add_comma_after_first()\n@@ -154,7 +162,7 @@ class JsonItemExporter(BaseItemExporter):\n \n \n class XmlItemExporter(BaseItemExporter):\n-    def __init__(self, file, **kwargs):\n+    def __init__(self, file: BytesIO, **kwargs: Any):\n         self.item_element = kwargs.pop(\"item_element\", \"item\")\n         self.root_element = kwargs.pop(\"root_element\", \"items\")\n         super().__init__(**kwargs)\n@@ -162,22 +170,22 @@ class XmlItemExporter(BaseItemExporter):\n             self.encoding = \"utf-8\"\n         self.xg = XMLGenerator(file, encoding=self.encoding)\n \n-    def _beautify_newline(self, new_item=False):\n+    def _beautify_newline(self, new_item: bool = False) -> None:\n         if self.indent is not None and (self.indent > 0 or new_item):\n             self.xg.characters(\"\\n\")\n \n-    def _beautify_indent(self, depth=1):\n+    def _beautify_indent(self, depth: int = 1) -> None:\n         if self.indent:\n             self.xg.characters(\" \" * self.indent * depth)\n \n-    def start_exporting(self):\n+    def start_exporting(self) -> None:\n         self.xg.startDocument()\n-        self.xg.startElement(self.root_element, {})\n+        self.xg.startElement(self.root_element, AttributesImpl({}))\n         self._beautify_newline(new_item=True)\n \n-    def export_item(self, item):\n+    def export_item(self, item: Any) -> None:\n         self._beautify_indent(depth=1)\n-        self.xg.startElement(self.item_element, {})\n+        self.xg.startElement(self.item_element, AttributesImpl({}))\n         self._beautify_newline()\n         for name, value in self._get_serialized_fields(item, default_value=\"\"):\n             self._export_xml_field(name, value, depth=2)\n@@ -185,13 +193,13 @@ class XmlItemExporter(BaseItemExporter):\n         self.xg.endElement(self.item_element)\n         self._beautify_newline(new_item=True)\n \n-    def finish_exporting(self):\n+    def finish_exporting(self) -> None:\n         self.xg.endElement(self.root_element)\n         self.xg.endDocument()\n \n-    def _export_xml_field(self, name, serialized_value, depth):\n+    def _export_xml_field(self, name: str, serialized_value: Any, depth: int) -> None:\n         self._beautify_indent(depth=depth)\n-        self.xg.startElement(name, {})\n+        self.xg.startElement(name, AttributesImpl({}))\n         if hasattr(serialized_value, \"items\"):\n             self._beautify_newline()\n             for subname, value in serialized_value.items():\n@@ -213,17 +221,17 @@ class XmlItemExporter(BaseItemExporter):\n class CsvItemExporter(BaseItemExporter):\n     def __init__(\n         self,\n-        file,\n-        include_headers_line=True,\n-        join_multivalued=\",\",\n-        errors=None,\n-        **kwargs,\n+        file: BytesIO,\n+        include_headers_line: bool = True,\n+        join_multivalued: str = \",\",\n+        errors: Optional[str] = None,\n+        **kwargs: Any,\n     ):\n         super().__init__(dont_fail=True, **kwargs)\n         if not self.encoding:\n             self.encoding = \"utf-8\"\n         self.include_headers_line = include_headers_line\n-        self.stream = io.TextIOWrapper(\n+        self.stream = TextIOWrapper(\n             file,\n             line_buffering=False,\n             write_through=True,\n@@ -235,11 +243,13 @@ class CsvItemExporter(BaseItemExporter):\n         self._headers_not_written = True\n         self._join_multivalued = join_multivalued\n \n-    def serialize_field(self, field, name, value):\n-        serializer = field.get(\"serializer\", self._join_if_needed)\n+    def serialize_field(\n+        self, field: Union[Mapping[str, Any], Field], name: str, value: Any\n+    ) -> Any:\n+        serializer: Callable[[Any], Any] = field.get(\"serializer\", self._join_if_needed)\n         return serializer(value)\n \n-    def _join_if_needed(self, value):\n+    def _join_if_needed(self, value: Any) -> Any:\n         if isinstance(value, (list, tuple)):\n             try:\n                 return self._join_multivalued.join(value)\n@@ -247,7 +257,7 @@ class CsvItemExporter(BaseItemExporter):\n                 pass\n         return value\n \n-    def export_item(self, item):\n+    def export_item(self, item: Any) -> None:\n         if self._headers_not_written:\n             self._headers_not_written = False\n             self._write_headers_and_set_fields_to_export(item)\n@@ -256,36 +266,38 @@ class CsvItemExporter(BaseItemExporter):\n         values = list(self._build_row(x for _, x in fields))\n         self.csv_writer.writerow(values)\n \n-    def finish_exporting(self):\n+    def finish_exporting(self) -> None:\n         self.stream.detach()  # Avoid closing the wrapped file.\n \n-    def _build_row(self, values):\n+    def _build_row(self, values: Iterable[Any]) -> Iterable[Any]:\n         for s in values:\n             try:\n                 yield to_unicode(s, self.encoding)\n             except TypeError:\n                 yield s\n \n-    def _write_headers_and_set_fields_to_export(self, item):\n+    def _write_headers_and_set_fields_to_export(self, item: Any) -> None:\n         if self.include_headers_line:\n             if not self.fields_to_export:\n                 # use declared field names, or keys if the item is a dict\n                 self.fields_to_export = ItemAdapter(item).field_names()\n+            fields: Iterable[str]\n             if isinstance(self.fields_to_export, Mapping):\n                 fields = self.fields_to_export.values()\n             else:\n+                assert self.fields_to_export\n                 fields = self.fields_to_export\n             row = list(self._build_row(fields))\n             self.csv_writer.writerow(row)\n \n \n class PickleItemExporter(BaseItemExporter):\n-    def __init__(self, file, protocol=4, **kwargs):\n+    def __init__(self, file: BytesIO, protocol: int = 4, **kwargs: Any):\n         super().__init__(**kwargs)\n-        self.file = file\n-        self.protocol = protocol\n+        self.file: BytesIO = file\n+        self.protocol: int = protocol\n \n-    def export_item(self, item):\n+    def export_item(self, item: Any) -> None:\n         d = dict(self._get_serialized_fields(item))\n         pickle.dump(d, self.file, self.protocol)\n \n@@ -299,20 +311,20 @@ class MarshalItemExporter(BaseItemExporter):\n                  opened in binary mode, a :class:`~io.BytesIO` object, etc)\n     \"\"\"\n \n-    def __init__(self, file, **kwargs):\n+    def __init__(self, file: BytesIO, **kwargs: Any):\n         super().__init__(**kwargs)\n-        self.file = file\n+        self.file: BytesIO = file\n \n-    def export_item(self, item):\n+    def export_item(self, item: Any) -> None:\n         marshal.dump(dict(self._get_serialized_fields(item)), self.file)\n \n \n class PprintItemExporter(BaseItemExporter):\n-    def __init__(self, file, **kwargs):\n+    def __init__(self, file: BytesIO, **kwargs: Any):\n         super().__init__(**kwargs)\n-        self.file = file\n+        self.file: BytesIO = file\n \n-    def export_item(self, item):\n+    def export_item(self, item: Any) -> None:\n         itemdict = dict(self._get_serialized_fields(item))\n         self.file.write(to_bytes(pprint.pformat(itemdict) + \"\\n\"))\n \n@@ -327,16 +339,20 @@ class PythonItemExporter(BaseItemExporter):\n     .. _msgpack: https://pypi.org/project/msgpack/\n     \"\"\"\n \n-    def _configure(self, options, dont_fail=False):\n+    def _configure(self, options: Dict[str, Any], dont_fail: bool = False) -> None:\n         super()._configure(options, dont_fail)\n         if not self.encoding:\n             self.encoding = \"utf-8\"\n \n-    def serialize_field(self, field, name, value):\n-        serializer = field.get(\"serializer\", self._serialize_value)\n+    def serialize_field(\n+        self, field: Union[Mapping[str, Any], Field], name: str, value: Any\n+    ) -> Any:\n+        serializer: Callable[[Any], Any] = field.get(\n+            \"serializer\", self._serialize_value\n+        )\n         return serializer(value)\n \n-    def _serialize_value(self, value):\n+    def _serialize_value(self, value: Any) -> Any:\n         if isinstance(value, Item):\n             return self.export_item(value)\n         if is_item(value):\n@@ -347,10 +363,10 @@ class PythonItemExporter(BaseItemExporter):\n             return to_unicode(value, encoding=self.encoding)\n         return value\n \n-    def _serialize_item(self, item):\n+    def _serialize_item(self, item: Any) -> Iterable[Tuple[Union[str, bytes], Any]]:\n         for key, value in ItemAdapter(item).items():\n             yield key, self._serialize_value(value)\n \n-    def export_item(self, item):\n-        result = dict(self._get_serialized_fields(item))\n+    def export_item(self, item: Any) -> Dict[Union[str, bytes], Any]:  # type: ignore[override]\n+        result: Dict[Union[str, bytes], Any] = dict(self._get_serialized_fields(item))\n         return result\n\n@@ -11,7 +11,7 @@ import warnings\n from datetime import datetime, timezone\n from pathlib import Path, PureWindowsPath\n from tempfile import NamedTemporaryFile\n-from typing import IO, Any, Callable, Dict, List, Optional, Tuple, Union\n+from typing import IO, Any, Callable, Dict, List, Optional, Tuple, Type, Union\n from urllib.parse import unquote, urlparse\n \n from twisted.internet import defer, threads\n@@ -21,6 +21,7 @@ from zope.interface import Interface, implementer\n \n from scrapy import Spider, signals\n from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n+from scrapy.exporters import BaseItemExporter\n from scrapy.extensions.postprocessing import PostProcessingManager\n from scrapy.utils.boto import is_botocore_available\n from scrapy.utils.conf import feed_complete_default_values_from_settings\n@@ -324,12 +325,12 @@ class FeedSlot:\n         filter,\n         feed_options,\n         spider,\n-        exporters,\n+        exporters: Dict[str, Type[BaseItemExporter]],\n         settings,\n         crawler,\n     ):\n         self.file = None\n-        self.exporter = None\n+        self.exporter: Optional[BaseItemExporter] = None\n         self.storage = storage\n         # feed params\n         self.batch_id = batch_id\n@@ -341,7 +342,7 @@ class FeedSlot:\n         # exporter params\n         self.feed_options = feed_options\n         self.spider = spider\n-        self.exporters = exporters\n+        self.exporters: Dict[str, Type[BaseItemExporter]] = exporters\n         self.settings = settings\n         self.crawler = crawler\n         # flags\n@@ -373,7 +374,7 @@ class FeedSlot:\n     def _get_instance(self, objcls, *args, **kwargs):\n         return build_from_crawler(objcls, self.crawler, *args, **kwargs)\n \n-    def _get_exporter(self, file, format, *args, **kwargs):\n+    def _get_exporter(self, file, format, *args, **kwargs) -> BaseItemExporter:\n         return self._get_instance(self.exporters[format], file, *args, **kwargs)\n \n     def finish_exporting(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#421e08dd4a4d5ed2acf3cd0fca4c7bfa5a6d3eb2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 170 | Lines Deleted: 86 | Files Changed: 8 | Hunks: 92 | Methods Changed: 84 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 256 | Churn Cumulative: 1500 | Contributors (this commit): 34 | Commits (past 90d): 13 | Contributors (cumulative): 77 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,20 +4,31 @@ conditions are met.\n See documentation in docs/topics/extensions.rst\n \"\"\"\n \n+from __future__ import annotations\n+\n import logging\n from collections import defaultdict\n+from typing import TYPE_CHECKING, Any, DefaultDict, Dict\n \n-from scrapy import signals\n+from twisted.python.failure import Failure\n+\n+from scrapy import Request, Spider, signals\n+from scrapy.crawler import Crawler\n from scrapy.exceptions import NotConfigured\n+from scrapy.http import Response\n+\n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n \n logger = logging.getLogger(__name__)\n \n \n class CloseSpider:\n-    def __init__(self, crawler):\n-        self.crawler = crawler\n+    def __init__(self, crawler: Crawler):\n+        self.crawler: Crawler = crawler\n \n-        self.close_on = {\n+        self.close_on: Dict[str, Any] = {\n             \"timeout\": crawler.settings.getfloat(\"CLOSESPIDER_TIMEOUT\"),\n             \"itemcount\": crawler.settings.getint(\"CLOSESPIDER_ITEMCOUNT\"),\n             \"pagecount\": crawler.settings.getint(\"CLOSESPIDER_PAGECOUNT\"),\n@@ -28,7 +39,7 @@ class CloseSpider:\n         if not any(self.close_on.values()):\n             raise NotConfigured\n \n-        self.counter = defaultdict(int)\n+        self.counter: DefaultDict[str, int] = defaultdict(int)\n \n         if self.close_on.get(\"errorcount\"):\n             crawler.signals.connect(self.error_count, signal=signals.spider_error)\n@@ -39,8 +50,8 @@ class CloseSpider:\n         if self.close_on.get(\"itemcount\"):\n             crawler.signals.connect(self.item_scraped, signal=signals.item_scraped)\n         if self.close_on.get(\"timeout_no_item\"):\n-            self.timeout_no_item = self.close_on[\"timeout_no_item\"]\n-            self.items_in_period = 0\n+            self.timeout_no_item: int = self.close_on[\"timeout_no_item\"]\n+            self.items_in_period: int = 0\n             crawler.signals.connect(\n                 self.spider_opened_no_item, signal=signals.spider_opened\n             )\n@@ -50,22 +61,25 @@ class CloseSpider:\n         crawler.signals.connect(self.spider_closed, signal=signals.spider_closed)\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         return cls(crawler)\n \n-    def error_count(self, failure, response, spider):\n+    def error_count(self, failure: Failure, response: Response, spider: Spider) -> None:\n         self.counter[\"errorcount\"] += 1\n         if self.counter[\"errorcount\"] == self.close_on[\"errorcount\"]:\n+            assert self.crawler.engine\n             self.crawler.engine.close_spider(spider, \"closespider_errorcount\")\n \n-    def page_count(self, response, request, spider):\n+    def page_count(self, response: Response, request: Request, spider: Spider) -> None:\n         self.counter[\"pagecount\"] += 1\n         if self.counter[\"pagecount\"] == self.close_on[\"pagecount\"]:\n+            assert self.crawler.engine\n             self.crawler.engine.close_spider(spider, \"closespider_pagecount\")\n \n-    def spider_opened(self, spider):\n+    def spider_opened(self, spider: Spider) -> None:\n         from twisted.internet import reactor\n \n+        assert self.crawler.engine\n         self.task = reactor.callLater(\n             self.close_on[\"timeout\"],\n             self.crawler.engine.close_spider,\n@@ -73,21 +87,22 @@ class CloseSpider:\n             reason=\"closespider_timeout\",\n         )\n \n-    def item_scraped(self, item, spider):\n+    def item_scraped(self, item: Any, spider: Spider) -> None:\n         self.counter[\"itemcount\"] += 1\n         if self.counter[\"itemcount\"] == self.close_on[\"itemcount\"]:\n+            assert self.crawler.engine\n             self.crawler.engine.close_spider(spider, \"closespider_itemcount\")\n \n-    def spider_closed(self, spider):\n-        task = getattr(self, \"task\", False)\n+    def spider_closed(self, spider: Spider) -> None:\n+        task = getattr(self, \"task\", None)\n         if task and task.active():\n             task.cancel()\n \n-        task_no_item = getattr(self, \"task_no_item\", False)\n+        task_no_item = getattr(self, \"task_no_item\", None)\n         if task_no_item and task_no_item.running:\n             task_no_item.stop()\n \n-    def spider_opened_no_item(self, spider):\n+    def spider_opened_no_item(self, spider: Spider) -> None:\n         from twisted.internet import task\n \n         self.task_no_item = task.LoopingCall(self._count_items_produced, spider)\n@@ -98,10 +113,10 @@ class CloseSpider:\n             f\"{self.timeout_no_item} seconds.\"\n         )\n \n-    def item_scraped_no_item(self, item, spider):\n+    def item_scraped_no_item(self, item: Any, spider: Spider) -> None:\n         self.items_in_period += 1\n \n-    def _count_items_produced(self, spider):\n+    def _count_items_produced(self, spider: Spider) -> None:\n         if self.items_in_period >= 1:\n             self.items_in_period = 0\n         else:\n@@ -109,4 +124,5 @@ class CloseSpider:\n                 f\"Closing spider since no items were produced in the last \"\n                 f\"{self.timeout_no_item} seconds.\"\n             )\n+            assert self.crawler.engine\n             self.crawler.engine.close_spider(spider, \"closespider_timeout_no_item\")\n\n@@ -2,18 +2,28 @@\n Extension for collecting core stats like items scraped and start/finish times\n \"\"\"\n \n-from datetime import datetime, timezone\n+from __future__ import annotations\n \n-from scrapy import signals\n+from datetime import datetime, timezone\n+from typing import TYPE_CHECKING, Any, Optional\n+\n+from scrapy import Spider, signals\n+from scrapy.crawler import Crawler\n+from scrapy.statscollectors import StatsCollector\n+\n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n \n \n class CoreStats:\n-    def __init__(self, stats):\n-        self.stats = stats\n-        self.start_time = None\n+    def __init__(self, stats: StatsCollector):\n+        self.stats: StatsCollector = stats\n+        self.start_time: Optional[datetime] = None\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n+        assert crawler.stats\n         o = cls(crawler.stats)\n         crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n         crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n@@ -22,11 +32,12 @@ class CoreStats:\n         crawler.signals.connect(o.response_received, signal=signals.response_received)\n         return o\n \n-    def spider_opened(self, spider):\n+    def spider_opened(self, spider: Spider) -> None:\n         self.start_time = datetime.now(tz=timezone.utc)\n         self.stats.set_value(\"start_time\", self.start_time, spider=spider)\n \n-    def spider_closed(self, spider, reason):\n+    def spider_closed(self, spider: Spider, reason: str) -> None:\n+        assert self.start_time is not None\n         finish_time = datetime.now(tz=timezone.utc)\n         elapsed_time = finish_time - self.start_time\n         elapsed_time_seconds = elapsed_time.total_seconds()\n@@ -36,13 +47,13 @@ class CoreStats:\n         self.stats.set_value(\"finish_time\", finish_time, spider=spider)\n         self.stats.set_value(\"finish_reason\", reason, spider=spider)\n \n-    def item_scraped(self, item, spider):\n+    def item_scraped(self, item: Any, spider: Spider) -> None:\n         self.stats.inc_value(\"item_scraped_count\", spider=spider)\n \n-    def response_received(self, spider):\n+    def response_received(self, spider: Spider) -> None:\n         self.stats.inc_value(\"response_received_count\", spider=spider)\n \n-    def item_dropped(self, item, spider, exception):\n+    def item_dropped(self, item: Any, spider: Spider, exception: BaseException) -> None:\n         reason = exception.__class__.__name__\n         self.stats.inc_value(\"item_dropped_count\", spider=spider)\n         self.stats.inc_value(f\"item_dropped_reasons_count/{reason}\", spider=spider)\n\n@@ -4,22 +4,31 @@ Extensions for debugging Scrapy\n See documentation in docs/topics/extensions.rst\n \"\"\"\n \n+from __future__ import annotations\n+\n import logging\n import signal\n import sys\n import threading\n import traceback\n from pdb import Pdb\n+from types import FrameType\n+from typing import TYPE_CHECKING, Optional\n \n+from scrapy.crawler import Crawler\n from scrapy.utils.engine import format_engine_status\n from scrapy.utils.trackref import format_live_refs\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n logger = logging.getLogger(__name__)\n \n \n class StackTraceDump:\n-    def __init__(self, crawler=None):\n-        self.crawler = crawler\n+    def __init__(self, crawler: Crawler):\n+        self.crawler: Crawler = crawler\n         try:\n             signal.signal(signal.SIGUSR2, self.dump_stacktrace)\n             signal.signal(signal.SIGQUIT, self.dump_stacktrace)\n@@ -28,10 +37,11 @@ class StackTraceDump:\n             pass\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         return cls(crawler)\n \n-    def dump_stacktrace(self, signum, frame):\n+    def dump_stacktrace(self, signum: int, frame: Optional[FrameType]) -> None:\n+        assert self.crawler.engine\n         log_args = {\n             \"stackdumps\": self._thread_stacks(),\n             \"enginestatus\": format_engine_status(self.crawler.engine),\n@@ -44,7 +54,7 @@ class StackTraceDump:\n             extra={\"crawler\": self.crawler},\n         )\n \n-    def _thread_stacks(self):\n+    def _thread_stacks(self) -> str:\n         id2name = dict((th.ident, th.name) for th in threading.enumerate())\n         dumps = \"\"\n         for id_, frame in sys._current_frames().items():\n@@ -55,12 +65,13 @@ class StackTraceDump:\n \n \n class Debugger:\n-    def __init__(self):\n+    def __init__(self) -> None:\n         try:\n             signal.signal(signal.SIGUSR2, self._enter_debugger)\n         except AttributeError:\n             # win32 platforms don't support SIGUSR signals\n             pass\n \n-    def _enter_debugger(self, signum, frame):\n+    def _enter_debugger(self, signum: int, frame: Optional[FrameType]) -> None:\n+        assert frame\n         Pdb().set_trace(frame.f_back)\n\n@@ -1,9 +1,18 @@\n+from __future__ import annotations\n+\n import logging\n+from typing import TYPE_CHECKING, Optional, Tuple, Union\n \n from twisted.internet import task\n \n-from scrapy import signals\n+from scrapy import Spider, signals\n+from scrapy.crawler import Crawler\n from scrapy.exceptions import NotConfigured\n+from scrapy.statscollectors import StatsCollector\n+\n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n \n logger = logging.getLogger(__name__)\n \n@@ -14,30 +23,31 @@ class LogStats:\n     * IPM - Items per Minute\n     \"\"\"\n \n-    def __init__(self, stats, interval=60.0):\n-        self.stats = stats\n-        self.interval = interval\n-        self.multiplier = 60.0 / self.interval\n-        self.task = None\n+    def __init__(self, stats: StatsCollector, interval: float = 60.0):\n+        self.stats: StatsCollector = stats\n+        self.interval: float = interval\n+        self.multiplier: float = 60.0 / self.interval\n+        self.task: Optional[task.LoopingCall] = None\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n-        interval = crawler.settings.getfloat(\"LOGSTATS_INTERVAL\")\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n+        interval: float = crawler.settings.getfloat(\"LOGSTATS_INTERVAL\")\n         if not interval:\n             raise NotConfigured\n+        assert crawler.stats\n         o = cls(crawler.stats, interval)\n         crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n         crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n         return o\n \n-    def spider_opened(self, spider):\n-        self.pagesprev = 0\n-        self.itemsprev = 0\n+    def spider_opened(self, spider: Spider) -> None:\n+        self.pagesprev: int = 0\n+        self.itemsprev: int = 0\n \n         self.task = task.LoopingCall(self.log, spider)\n         self.task.start(self.interval)\n \n-    def log(self, spider):\n+    def log(self, spider: Spider) -> None:\n         self.calculate_stats()\n \n         msg = (\n@@ -52,14 +62,14 @@ class LogStats:\n         }\n         logger.info(msg, log_args, extra={\"spider\": spider})\n \n-    def calculate_stats(self):\n-        self.items = self.stats.get_value(\"item_scraped_count\", 0)\n-        self.pages = self.stats.get_value(\"response_received_count\", 0)\n-        self.irate = (self.items - self.itemsprev) * self.multiplier\n-        self.prate = (self.pages - self.pagesprev) * self.multiplier\n+    def calculate_stats(self) -> None:\n+        self.items: int = self.stats.get_value(\"item_scraped_count\", 0)\n+        self.pages: int = self.stats.get_value(\"response_received_count\", 0)\n+        self.irate: float = (self.items - self.itemsprev) * self.multiplier\n+        self.prate: float = (self.pages - self.pagesprev) * self.multiplier\n         self.pagesprev, self.itemsprev = self.pages, self.items\n \n-    def spider_closed(self, spider, reason):\n+    def spider_closed(self, spider: Spider, reason: str) -> None:\n         if self.task and self.task.running:\n             self.task.stop()\n \n@@ -67,7 +77,9 @@ class LogStats:\n         self.stats.set_value(\"responses_per_minute\", rpm_final)\n         self.stats.set_value(\"items_per_minute\", ipm_final)\n \n-    def calculate_final_stats(self, spider):\n+    def calculate_final_stats(\n+        self, spider: Spider\n+    ) -> Union[Tuple[None, None], Tuple[float, float]]:\n         start_time = self.stats.get_value(\"start_time\")\n         finished_time = self.stats.get_value(\"finished_time\")\n \n\n@@ -4,26 +4,36 @@ MemoryDebugger extension\n See documentation in docs/topics/extensions.rst\n \"\"\"\n \n-import gc\n+from __future__ import annotations\n \n-from scrapy import signals\n+import gc\n+from typing import TYPE_CHECKING\n+\n+from scrapy import Spider, signals\n+from scrapy.crawler import Crawler\n from scrapy.exceptions import NotConfigured\n+from scrapy.statscollectors import StatsCollector\n from scrapy.utils.trackref import live_refs\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n \n class MemoryDebugger:\n-    def __init__(self, stats):\n-        self.stats = stats\n+    def __init__(self, stats: StatsCollector):\n+        self.stats: StatsCollector = stats\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         if not crawler.settings.getbool(\"MEMDEBUG_ENABLED\"):\n             raise NotConfigured\n+        assert crawler.stats\n         o = cls(crawler.stats)\n         crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n         return o\n \n-    def spider_closed(self, spider, reason):\n+    def spider_closed(self, spider: Spider, reason: str) -> None:\n         gc.collect()\n         self.stats.set_value(\n             \"memdebug/gc_garbage_count\", len(gc.garbage), spider=spider\n\n@@ -4,24 +4,32 @@ MemoryUsage extension\n See documentation in docs/topics/extensions.rst\n \"\"\"\n \n+from __future__ import annotations\n+\n import logging\n import socket\n import sys\n from importlib import import_module\n from pprint import pformat\n+from typing import TYPE_CHECKING, List\n \n from twisted.internet import task\n \n from scrapy import signals\n+from scrapy.crawler import Crawler\n from scrapy.exceptions import NotConfigured\n from scrapy.mail import MailSender\n from scrapy.utils.engine import get_engine_status\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n logger = logging.getLogger(__name__)\n \n \n class MemoryUsage:\n-    def __init__(self, crawler):\n+    def __init__(self, crawler: Crawler):\n         if not crawler.settings.getbool(\"MEMUSAGE_ENABLED\"):\n             raise NotConfigured\n         try:\n@@ -30,32 +38,33 @@ class MemoryUsage:\n         except ImportError:\n             raise NotConfigured\n \n-        self.crawler = crawler\n-        self.warned = False\n-        self.notify_mails = crawler.settings.getlist(\"MEMUSAGE_NOTIFY_MAIL\")\n-        self.limit = crawler.settings.getint(\"MEMUSAGE_LIMIT_MB\") * 1024 * 1024\n-        self.warning = crawler.settings.getint(\"MEMUSAGE_WARNING_MB\") * 1024 * 1024\n-        self.check_interval = crawler.settings.getfloat(\n+        self.crawler: Crawler = crawler\n+        self.warned: bool = False\n+        self.notify_mails: List[str] = crawler.settings.getlist(\"MEMUSAGE_NOTIFY_MAIL\")\n+        self.limit: int = crawler.settings.getint(\"MEMUSAGE_LIMIT_MB\") * 1024 * 1024\n+        self.warning: int = crawler.settings.getint(\"MEMUSAGE_WARNING_MB\") * 1024 * 1024\n+        self.check_interval: float = crawler.settings.getfloat(\n             \"MEMUSAGE_CHECK_INTERVAL_SECONDS\"\n         )\n-        self.mail = MailSender.from_settings(crawler.settings)\n+        self.mail: MailSender = MailSender.from_settings(crawler.settings)\n         crawler.signals.connect(self.engine_started, signal=signals.engine_started)\n         crawler.signals.connect(self.engine_stopped, signal=signals.engine_stopped)\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         return cls(crawler)\n \n-    def get_virtual_size(self):\n-        size = self.resource.getrusage(self.resource.RUSAGE_SELF).ru_maxrss\n+    def get_virtual_size(self) -> int:\n+        size: int = self.resource.getrusage(self.resource.RUSAGE_SELF).ru_maxrss\n         if sys.platform != \"darwin\":\n             # on macOS ru_maxrss is in bytes, on Linux it is in KB\n             size *= 1024\n         return size\n \n-    def engine_started(self):\n+    def engine_started(self) -> None:\n+        assert self.crawler.stats\n         self.crawler.stats.set_value(\"memusage/startup\", self.get_virtual_size())\n-        self.tasks = []\n+        self.tasks: List[task.LoopingCall] = []\n         tsk = task.LoopingCall(self.update)\n         self.tasks.append(tsk)\n         tsk.start(self.check_interval, now=True)\n@@ -68,15 +77,18 @@ class MemoryUsage:\n             self.tasks.append(tsk)\n             tsk.start(self.check_interval, now=True)\n \n-    def engine_stopped(self):\n+    def engine_stopped(self) -> None:\n         for tsk in self.tasks:\n             if tsk.running:\n                 tsk.stop()\n \n-    def update(self):\n+    def update(self) -> None:\n+        assert self.crawler.stats\n         self.crawler.stats.max_value(\"memusage/max\", self.get_virtual_size())\n \n-    def _check_limit(self):\n+    def _check_limit(self) -> None:\n+        assert self.crawler.engine\n+        assert self.crawler.stats\n         peak_mem_usage = self.get_virtual_size()\n         if peak_mem_usage > self.limit:\n             self.crawler.stats.set_value(\"memusage/limit_reached\", 1)\n@@ -106,9 +118,10 @@ class MemoryUsage:\n                 {\"virtualsize\": peak_mem_usage / 1024 / 1024},\n             )\n \n-    def _check_warning(self):\n+    def _check_warning(self) -> None:\n         if self.warned:  # warn only once\n             return\n+        assert self.crawler.stats\n         if self.get_virtual_size() > self.warning:\n             self.crawler.stats.set_value(\"memusage/warning_reached\", 1)\n             mem = self.warning / 1024 / 1024\n@@ -126,8 +139,10 @@ class MemoryUsage:\n                 self.crawler.stats.set_value(\"memusage/warning_notified\", 1)\n             self.warned = True\n \n-    def _send_report(self, rcpts, subject):\n+    def _send_report(self, rcpts: List[str], subject: str) -> None:\n         \"\"\"send notification mail with some additional useful info\"\"\"\n+        assert self.crawler.engine\n+        assert self.crawler.stats\n         stats = self.crawler.stats\n         s = f\"Memory usage at engine startup : {stats.get_value('memusage/startup') / 1024 / 1024}M\\r\\n\"\n         s += f\"Maximum memory usage          : {stats.get_value('memusage/max') / 1024 / 1024}M\\r\\n\"\n\n@@ -4,6 +4,8 @@ Mail sending helpers\n See documentation in docs/topics/email.rst\n \"\"\"\n \n+from __future__ import annotations\n+\n import logging\n from email import encoders as Encoders\n from email.mime.base import MIMEBase\n@@ -12,14 +14,20 @@ from email.mime.nonmultipart import MIMENonMultipart\n from email.mime.text import MIMEText\n from email.utils import formatdate\n from io import BytesIO\n+from typing import TYPE_CHECKING\n \n from twisted import version as twisted_version\n from twisted.internet import defer, ssl\n from twisted.python.versions import Version\n \n+from scrapy.settings import BaseSettings\n from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.python import to_bytes\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n logger = logging.getLogger(__name__)\n \n \n@@ -56,7 +64,7 @@ class MailSender:\n         self.debug = debug\n \n     @classmethod\n-    def from_settings(cls, settings):\n+    def from_settings(cls, settings: BaseSettings) -> Self:\n         return cls(\n             smtphost=settings[\"MAIL_HOST\"],\n             mailfrom=settings[\"MAIL_FROM\"],\n@@ -203,7 +211,7 @@ class MailSender:\n             to_addrs,\n             msg,\n             d,\n-            **factory_keywords\n+            **factory_keywords,\n         )\n         factory.noisy = False\n         return factory\n\n@@ -1,14 +1,15 @@\n \"\"\"Some debugging functions for working with the Scrapy engine\"\"\"\n \n+from __future__ import annotations\n+\n # used in global tests code\n from time import time  # noqa: F401\n-from typing import TYPE_CHECKING, Any, List, Tuple\n+from typing import Any, List, Tuple\n \n-if TYPE_CHECKING:\n from scrapy.core.engine import ExecutionEngine\n \n \n-def get_engine_status(engine: \"ExecutionEngine\") -> List[Tuple[str, Any]]:\n+def get_engine_status(engine: ExecutionEngine) -> List[Tuple[str, Any]]:\n     \"\"\"Return a report of the current engine status\"\"\"\n     tests = [\n         \"time()-engine.start_time\",\n@@ -37,7 +38,7 @@ def get_engine_status(engine: \"ExecutionEngine\") -> List[Tuple[str, Any]]:\n     return checks\n \n \n-def format_engine_status(engine: \"ExecutionEngine\") -> str:\n+def format_engine_status(engine: ExecutionEngine) -> str:\n     checks = get_engine_status(engine)\n     s = \"Execution engine status\\n\\n\"\n     for test, result in checks:\n@@ -47,5 +48,5 @@ def format_engine_status(engine: \"ExecutionEngine\") -> str:\n     return s\n \n \n-def print_engine_status(engine: \"ExecutionEngine\") -> None:\n+def print_engine_status(engine: ExecutionEngine) -> None:\n     print(format_engine_status(engine))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6e84648c0717642b069249225857019a87de54b9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 704 | Contributors (this commit): 21 | Commits (past 90d): 1 | Contributors (cumulative): 21 | DMM Complexity: None\n\nDIFF:\n@@ -175,7 +175,7 @@ class UtilsPythonTestCase(unittest.TestCase):\n         self.assertTrue(equal_attributes(a, b, [\"x\", \"y\"]))\n \n         a.y = 1\n-        # differente attributes\n+        # different attributes\n         self.assertFalse(equal_attributes(a, b, [\"x\", \"y\"]))\n \n         # test callable\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#642af407049a5ce8e76b7999c68333670c8c8622", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 163 | Lines Deleted: 93 | Files Changed: 8 | Hunks: 77 | Methods Changed: 60 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 256 | Churn Cumulative: 1784 | Contributors (this commit): 42 | Commits (past 90d): 14 | Contributors (cumulative): 78 | DMM Complexity: 0.5714285714285714\n\nDIFF:\n@@ -1,12 +1,22 @@\n+from __future__ import annotations\n+\n import logging\n from datetime import datetime, timezone\n+from json import JSONEncoder\n+from typing import TYPE_CHECKING, Any, Dict, List, Optional, Union\n \n from twisted.internet import task\n \n-from scrapy import signals\n+from scrapy import Spider, signals\n+from scrapy.crawler import Crawler\n from scrapy.exceptions import NotConfigured\n+from scrapy.statscollectors import StatsCollector\n from scrapy.utils.serialize import ScrapyJSONEncoder\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n logger = logging.getLogger(__name__)\n \n \n@@ -15,32 +25,34 @@ class PeriodicLog:\n \n     def __init__(\n         self,\n-        stats,\n-        interval=60.0,\n-        ext_stats={},\n-        ext_delta={},\n-        ext_timing_enabled=False,\n+        stats: StatsCollector,\n+        interval: float = 60.0,\n+        ext_stats: Dict[str, Any] = {},\n+        ext_delta: Dict[str, Any] = {},\n+        ext_timing_enabled: bool = False,\n     ):\n-        self.stats = stats\n-        self.interval = interval\n-        self.multiplier = 60.0 / self.interval\n-        self.task = None\n-        self.encoder = ScrapyJSONEncoder(sort_keys=True, indent=4)\n-        self.ext_stats_enabled = bool(ext_stats)\n-        self.ext_stats_include = ext_stats.get(\"include\", [])\n-        self.ext_stats_exclude = ext_stats.get(\"exclude\", [])\n-        self.ext_delta_enabled = bool(ext_delta)\n-        self.ext_delta_include = ext_delta.get(\"include\", [])\n-        self.ext_delta_exclude = ext_delta.get(\"exclude\", [])\n-        self.ext_timing_enabled = ext_timing_enabled\n+        self.stats: StatsCollector = stats\n+        self.interval: float = interval\n+        self.multiplier: float = 60.0 / self.interval\n+        self.task: Optional[task.LoopingCall] = None\n+        self.encoder: JSONEncoder = ScrapyJSONEncoder(sort_keys=True, indent=4)\n+        self.ext_stats_enabled: bool = bool(ext_stats)\n+        self.ext_stats_include: List[str] = ext_stats.get(\"include\", [])\n+        self.ext_stats_exclude: List[str] = ext_stats.get(\"exclude\", [])\n+        self.ext_delta_enabled: bool = bool(ext_delta)\n+        self.ext_delta_include: List[str] = ext_delta.get(\"include\", [])\n+        self.ext_delta_exclude: List[str] = ext_delta.get(\"exclude\", [])\n+        self.ext_timing_enabled: bool = ext_timing_enabled\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n-        interval = crawler.settings.getfloat(\"LOGSTATS_INTERVAL\")\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n+        interval: float = crawler.settings.getfloat(\"LOGSTATS_INTERVAL\")\n         if not interval:\n             raise NotConfigured\n         try:\n-            ext_stats = crawler.settings.getdict(\"PERIODIC_LOG_STATS\")\n+            ext_stats: Optional[Dict[str, Any]] = crawler.settings.getdict(\n+                \"PERIODIC_LOG_STATS\"\n+            )\n         except (TypeError, ValueError):\n             ext_stats = (\n                 {\"enabled\": True}\n@@ -48,7 +60,9 @@ class PeriodicLog:\n                 else None\n             )\n         try:\n-            ext_delta = crawler.settings.getdict(\"PERIODIC_LOG_DELTA\")\n+            ext_delta: Optional[Dict[str, Any]] = crawler.settings.getdict(\n+                \"PERIODIC_LOG_DELTA\"\n+            )\n         except (TypeError, ValueError):\n             ext_delta = (\n                 {\"enabled\": True}\n@@ -56,11 +70,14 @@ class PeriodicLog:\n                 else None\n             )\n \n-        ext_timing_enabled = crawler.settings.getbool(\n+        ext_timing_enabled: bool = crawler.settings.getbool(\n             \"PERIODIC_LOG_TIMING_ENABLED\", False\n         )\n         if not (ext_stats or ext_delta or ext_timing_enabled):\n             raise NotConfigured\n+        assert crawler.stats\n+        assert ext_stats is not None\n+        assert ext_delta is not None\n         o = cls(\n             crawler.stats,\n             interval,\n@@ -72,16 +89,16 @@ class PeriodicLog:\n         crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n         return o\n \n-    def spider_opened(self, spider):\n-        self.time_prev = datetime.now(tz=timezone.utc)\n-        self.delta_prev = {}\n-        self.stats_prev = {}\n+    def spider_opened(self, spider: Spider) -> None:\n+        self.time_prev: datetime = datetime.now(tz=timezone.utc)\n+        self.delta_prev: Dict[str, Union[int, float]] = {}\n+        self.stats_prev: Dict[str, Union[int, float]] = {}\n \n         self.task = task.LoopingCall(self.log)\n         self.task.start(self.interval)\n \n-    def log(self):\n-        data = {}\n+    def log(self) -> None:\n+        data: Dict[str, Any] = {}\n         if self.ext_timing_enabled:\n             data.update(self.log_timing())\n         if self.ext_delta_enabled:\n@@ -90,8 +107,8 @@ class PeriodicLog:\n             data.update(self.log_crawler_stats())\n         logger.info(self.encoder.encode(data))\n \n-    def log_delta(self):\n-        num_stats = {\n+    def log_delta(self) -> Dict[str, Any]:\n+        num_stats: Dict[str, Union[int, float]] = {\n             k: v\n             for k, v in self.stats._stats.items()\n             if isinstance(v, (int, float))\n@@ -101,7 +118,7 @@ class PeriodicLog:\n         self.delta_prev = num_stats\n         return {\"delta\": delta}\n \n-    def log_timing(self):\n+    def log_timing(self) -> Dict[str, Any]:\n         now = datetime.now(tz=timezone.utc)\n         time = {\n             \"log_interval\": self.interval,\n@@ -113,7 +130,7 @@ class PeriodicLog:\n         self.time_prev = now\n         return {\"time\": time}\n \n-    def log_crawler_stats(self):\n+    def log_crawler_stats(self) -> Dict[str, Any]:\n         stats = {\n             k: v\n             for k, v in self.stats._stats.items()\n@@ -121,7 +138,9 @@ class PeriodicLog:\n         }\n         return {\"stats\": stats}\n \n-    def param_allowed(self, stat_name, include, exclude):\n+    def param_allowed(\n+        self, stat_name: str, include: List[str], exclude: List[str]\n+    ) -> bool:\n         if not include and not exclude:\n             return True\n         for p in exclude:\n@@ -134,7 +153,7 @@ class PeriodicLog:\n                 return True\n         return False\n \n-    def spider_closed(self, spider, reason):\n+    def spider_closed(self, spider: Spider, reason: str) -> None:\n         self.log()\n         if self.task and self.task.running:\n             self.task.stop()\n\n@@ -6,7 +6,7 @@ from bz2 import BZ2File\n from gzip import GzipFile\n from io import IOBase\n from lzma import LZMAFile\n-from typing import Any, BinaryIO, Dict, List\n+from typing import Any, BinaryIO, Dict, List, cast\n \n from scrapy.utils.misc import load_object\n \n@@ -142,7 +142,7 @@ class PostProcessingManager(IOBase):\n         :return: returns number of bytes written\n         :rtype: int\n         \"\"\"\n-        return self.head_plugin.write(data)\n+        return cast(int, self.head_plugin.write(data))\n \n     def tell(self) -> int:\n         return self.file.tell()\n\n@@ -1,19 +1,27 @@\n+from __future__ import annotations\n+\n import pickle  # nosec\n from pathlib import Path\n+from typing import TYPE_CHECKING, Optional\n \n-from scrapy import signals\n+from scrapy import Spider, signals\n+from scrapy.crawler import Crawler\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.job import job_dir\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n \n class SpiderState:\n     \"\"\"Store and load spider state during a scraping job\"\"\"\n \n-    def __init__(self, jobdir=None):\n-        self.jobdir = jobdir\n+    def __init__(self, jobdir: Optional[str] = None):\n+        self.jobdir: Optional[str] = jobdir\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         jobdir = job_dir(crawler.settings)\n         if not jobdir:\n             raise NotConfigured\n@@ -23,18 +31,20 @@ class SpiderState:\n         crawler.signals.connect(obj.spider_opened, signal=signals.spider_opened)\n         return obj\n \n-    def spider_closed(self, spider):\n+    def spider_closed(self, spider: Spider) -> None:\n         if self.jobdir:\n             with Path(self.statefn).open(\"wb\") as f:\n+                assert hasattr(spider, \"state\")  # set in spider_opened\n                 pickle.dump(spider.state, f, protocol=4)\n \n-    def spider_opened(self, spider):\n+    def spider_opened(self, spider: Spider) -> None:\n         if self.jobdir and Path(self.statefn).exists():\n             with Path(self.statefn).open(\"rb\") as f:\n-                spider.state = pickle.load(f)  # nosec\n+                spider.state = pickle.load(f)  # type: ignore[attr-defined]  # nosec\n         else:\n-            spider.state = {}\n+            spider.state = {}  # type: ignore[attr-defined]\n \n     @property\n     def statefn(self) -> str:\n+        assert self.jobdir\n         return str(Path(self.jobdir, \"spider.state\"))\n\n@@ -4,28 +4,41 @@ StatsMailer extension sends an email when a spider finishes scraping.\n Use STATSMAILER_RCPTS setting to enable and give the recipient mail address\n \"\"\"\n \n-from scrapy import signals\n+from __future__ import annotations\n+\n+from typing import TYPE_CHECKING, List, Optional\n+\n+from twisted.internet.defer import Deferred\n+\n+from scrapy import Spider, signals\n+from scrapy.crawler import Crawler\n from scrapy.exceptions import NotConfigured\n from scrapy.mail import MailSender\n+from scrapy.statscollectors import StatsCollector\n+\n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n \n \n class StatsMailer:\n-    def __init__(self, stats, recipients, mail):\n-        self.stats = stats\n-        self.recipients = recipients\n-        self.mail = mail\n+    def __init__(self, stats: StatsCollector, recipients: List[str], mail: MailSender):\n+        self.stats: StatsCollector = stats\n+        self.recipients: List[str] = recipients\n+        self.mail: MailSender = mail\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n-        recipients = crawler.settings.getlist(\"STATSMAILER_RCPTS\")\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n+        recipients: List[str] = crawler.settings.getlist(\"STATSMAILER_RCPTS\")\n         if not recipients:\n             raise NotConfigured\n-        mail = MailSender.from_settings(crawler.settings)\n+        mail: MailSender = MailSender.from_settings(crawler.settings)\n+        assert crawler.stats\n         o = cls(crawler.stats, recipients, mail)\n         crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n         return o\n \n-    def spider_closed(self, spider):\n+    def spider_closed(self, spider: Spider) -> Optional[Deferred]:\n         spider_stats = self.stats.get_stats(spider)\n         body = \"Global stats\\n\\n\"\n         body += \"\\n\".join(f\"{k:<50} : {v}\" for k, v in self.stats.get_stats().items())\n\n@@ -4,13 +4,17 @@ Scrapy Telnet Console extension\n See documentation in docs/topics/telnetconsole.rst\n \"\"\"\n \n+from __future__ import annotations\n+\n import binascii\n import logging\n import os\n import pprint\n import traceback\n+from typing import TYPE_CHECKING, Any, Dict, List\n \n from twisted.internet import protocol\n+from twisted.internet.tcp import Port\n \n try:\n     from twisted.conch import manhole, telnet\n@@ -22,12 +26,16 @@ except (ImportError, SyntaxError):\n     TWISTED_CONCH_AVAILABLE = False\n \n from scrapy import signals\n+from scrapy.crawler import Crawler\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.decorators import defers\n from scrapy.utils.engine import print_engine_status\n from scrapy.utils.reactor import listen_tcp\n from scrapy.utils.trackref import print_live_refs\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n logger = logging.getLogger(__name__)\n \n # signal to update telnet variables\n@@ -36,7 +44,7 @@ update_telnet_vars = object()\n \n \n class TelnetConsole(protocol.ServerFactory):\n-    def __init__(self, crawler):\n+    def __init__(self, crawler: Crawler):\n         if not crawler.settings.getbool(\"TELNETCONSOLE_ENABLED\"):\n             raise NotConfigured\n         if not TWISTED_CONCH_AVAILABLE:\n@@ -44,14 +52,14 @@ class TelnetConsole(protocol.ServerFactory):\n                 \"TELNETCONSOLE_ENABLED setting is True but required twisted \"\n                 \"modules failed to import:\\n\" + _TWISTED_CONCH_TRACEBACK\n             )\n-        self.crawler = crawler\n-        self.noisy = False\n-        self.portrange = [\n+        self.crawler: Crawler = crawler\n+        self.noisy: bool = False\n+        self.portrange: List[int] = [\n             int(x) for x in crawler.settings.getlist(\"TELNETCONSOLE_PORT\")\n         ]\n-        self.host = crawler.settings[\"TELNETCONSOLE_HOST\"]\n-        self.username = crawler.settings[\"TELNETCONSOLE_USERNAME\"]\n-        self.password = crawler.settings[\"TELNETCONSOLE_PASSWORD\"]\n+        self.host: str = crawler.settings[\"TELNETCONSOLE_HOST\"]\n+        self.username: str = crawler.settings[\"TELNETCONSOLE_USERNAME\"]\n+        self.password: str = crawler.settings[\"TELNETCONSOLE_PASSWORD\"]\n \n         if not self.password:\n             self.password = binascii.hexlify(os.urandom(8)).decode(\"utf8\")\n@@ -61,11 +69,11 @@ class TelnetConsole(protocol.ServerFactory):\n         self.crawler.signals.connect(self.stop_listening, signals.engine_stopped)\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         return cls(crawler)\n \n-    def start_listening(self):\n-        self.port = listen_tcp(self.portrange, self.host, self)\n+    def start_listening(self) -> None:\n+        self.port: Port = listen_tcp(self.portrange, self.host, self)\n         h = self.port.getHost()\n         logger.info(\n             \"Telnet console listening on %(host)s:%(port)d\",\n@@ -73,10 +81,10 @@ class TelnetConsole(protocol.ServerFactory):\n             extra={\"crawler\": self.crawler},\n         )\n \n-    def stop_listening(self):\n+    def stop_listening(self) -> None:\n         self.port.stopListening()\n \n-    def protocol(self):\n+    def protocol(self) -> telnet.TelnetTransport:  # type: ignore[override]\n         class Portal:\n             \"\"\"An implementation of IPortal\"\"\"\n \n@@ -95,9 +103,10 @@ class TelnetConsole(protocol.ServerFactory):\n \n         return telnet.TelnetTransport(telnet.AuthenticatingTelnetProtocol, Portal())\n \n-    def _get_telnet_vars(self):\n+    def _get_telnet_vars(self) -> Dict[str, Any]:\n         # Note: if you add entries here also update topics/telnetconsole.rst\n-        telnet_vars = {\n+        assert self.crawler.engine\n+        telnet_vars: Dict[str, Any] = {\n             \"engine\": self.crawler.engine,\n             \"spider\": self.crawler.engine.spider,\n             \"slot\": self.crawler.engine.slot,\n\n@@ -1,19 +1,29 @@\n-import logging\n+from __future__ import annotations\n \n-from scrapy import signals\n+import logging\n+from typing import TYPE_CHECKING, Optional, Tuple\n+\n+from scrapy import Request, Spider, signals\n+from scrapy.core.downloader import Slot\n+from scrapy.crawler import Crawler\n from scrapy.exceptions import NotConfigured\n+from scrapy.http import Response\n+\n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n \n logger = logging.getLogger(__name__)\n \n \n class AutoThrottle:\n-    def __init__(self, crawler):\n-        self.crawler = crawler\n+    def __init__(self, crawler: Crawler):\n+        self.crawler: Crawler = crawler\n         if not crawler.settings.getbool(\"AUTOTHROTTLE_ENABLED\"):\n             raise NotConfigured\n \n-        self.debug = crawler.settings.getbool(\"AUTOTHROTTLE_DEBUG\")\n-        self.target_concurrency = crawler.settings.getfloat(\n+        self.debug: bool = crawler.settings.getbool(\"AUTOTHROTTLE_DEBUG\")\n+        self.target_concurrency: float = crawler.settings.getfloat(\n             \"AUTOTHROTTLE_TARGET_CONCURRENCY\"\n         )\n         if self.target_concurrency <= 0.0:\n@@ -27,27 +37,29 @@ class AutoThrottle:\n         )\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         return cls(crawler)\n \n-    def _spider_opened(self, spider):\n+    def _spider_opened(self, spider: Spider) -> None:\n         self.mindelay = self._min_delay(spider)\n         self.maxdelay = self._max_delay(spider)\n-        spider.download_delay = self._start_delay(spider)\n+        spider.download_delay = self._start_delay(spider)  # type: ignore[attr-defined]\n \n-    def _min_delay(self, spider):\n+    def _min_delay(self, spider: Spider) -> float:\n         s = self.crawler.settings\n         return getattr(spider, \"download_delay\", s.getfloat(\"DOWNLOAD_DELAY\"))\n \n-    def _max_delay(self, spider):\n+    def _max_delay(self, spider: Spider) -> float:\n         return self.crawler.settings.getfloat(\"AUTOTHROTTLE_MAX_DELAY\")\n \n-    def _start_delay(self, spider):\n+    def _start_delay(self, spider: Spider) -> float:\n         return max(\n             self.mindelay, self.crawler.settings.getfloat(\"AUTOTHROTTLE_START_DELAY\")\n         )\n \n-    def _response_downloaded(self, response, request, spider):\n+    def _response_downloaded(\n+        self, response: Response, request: Request, spider: Spider\n+    ) -> None:\n         key, slot = self._get_slot(request, spider)\n         latency = request.meta.get(\"download_latency\")\n         if latency is None or slot is None:\n@@ -74,11 +86,16 @@ class AutoThrottle:\n                 extra={\"spider\": spider},\n             )\n \n-    def _get_slot(self, request, spider):\n-        key = request.meta.get(\"download_slot\")\n+    def _get_slot(\n+        self, request: Request, spider: Spider\n+    ) -> Tuple[Optional[str], Optional[Slot]]:\n+        key: Optional[str] = request.meta.get(\"download_slot\")\n+        if key is None:\n+            return None, None\n+        assert self.crawler.engine\n         return key, self.crawler.engine.downloader.slots.get(key)\n \n-    def _adjust_delay(self, slot, latency, response):\n+    def _adjust_delay(self, slot: Slot, latency: float, response: Response) -> None:\n         \"\"\"Define delay adjustment policy\"\"\"\n \n         # If a server needs `latency` seconds to respond then\n\n@@ -14,10 +14,11 @@ from email.mime.nonmultipart import MIMENonMultipart\n from email.mime.text import MIMEText\n from email.utils import formatdate\n from io import BytesIO\n-from typing import TYPE_CHECKING\n+from typing import TYPE_CHECKING, Optional\n \n from twisted import version as twisted_version\n-from twisted.internet import defer, ssl\n+from twisted.internet import ssl\n+from twisted.internet.defer import Deferred\n from twisted.python.versions import Version\n \n from scrapy.settings import BaseSettings\n@@ -85,9 +86,10 @@ class MailSender:\n         mimetype=\"text/plain\",\n         charset=None,\n         _callback=None,\n-    ):\n+    ) -> Optional[Deferred]:\n         from twisted.internet import reactor\n \n+        msg: MIMEBase\n         if attachs:\n             msg = MIMEMultipart()\n         else:\n@@ -134,7 +136,7 @@ class MailSender:\n                     \"mailattachs\": len(attachs),\n                 },\n             )\n-            return\n+            return None\n \n         dfd = self._sendmail(rcpts, msg.as_string().encode(charset or \"utf-8\"))\n         dfd.addCallbacks(\n@@ -178,7 +180,7 @@ class MailSender:\n         from twisted.internet import reactor\n \n         msg = BytesIO(msg)\n-        d = defer.Deferred()\n+        d = Deferred()\n \n         factory = self._create_sender_factory(to_addrs, msg, d)\n \n\n@@ -2,17 +2,19 @@ import asyncio\n import sys\n from asyncio import AbstractEventLoop, AbstractEventLoopPolicy\n from contextlib import suppress\n-from typing import Any, Callable, Dict, Optional, Sequence, Type\n+from typing import Any, Callable, Dict, List, Optional, Sequence, Type\n from warnings import catch_warnings, filterwarnings, warn\n \n from twisted.internet import asyncioreactor, error\n from twisted.internet.base import DelayedCall\n+from twisted.internet.protocol import ServerFactory\n+from twisted.internet.tcp import Port\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.misc import load_object\n \n \n-def listen_tcp(portrange, host, factory):\n+def listen_tcp(portrange: List[int], host: str, factory: ServerFactory) -> Port:  # type: ignore[return]\n     \"\"\"Like reactor.listenTCP but tries different ports in a range.\"\"\"\n     from twisted.internet import reactor\n \n@@ -20,8 +22,6 @@ def listen_tcp(portrange, host, factory):\n         raise ValueError(f\"invalid portrange: {portrange}\")\n     if not portrange:\n         return reactor.listenTCP(0, factory, interface=host)\n-    if not hasattr(portrange, \"__iter__\"):\n-        return reactor.listenTCP(portrange, factory, interface=host)\n     if len(portrange) == 1:\n         return reactor.listenTCP(portrange[0], factory, interface=host)\n     for x in range(portrange[0], portrange[1] + 1):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
