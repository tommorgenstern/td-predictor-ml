{"custom_id": "keras#94373fefce87226db29f8270eca75408aa085996", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 16 | Files Changed: 5 | Hunks: 11 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 36 | Churn Cumulative: 20922 | Contributors (this commit): 122 | Commits (past 90d): 27 | Contributors (cumulative): 140 | DMM Complexity: 0.0\n\nDIFF:\n@@ -133,7 +133,7 @@ class InputLayer(base_layer.Layer):\n         batch_size = batch_input_shape[0]\n         input_shape = batch_input_shape[1:]\n     if kwargs:\n-      raise ValueError('Unrecognized keyword arguments:', kwargs.keys())\n+      raise ValueError(f'Unrecognized keyword arguments: {list(kwargs.keys())}')\n \n     if sparse and ragged:\n       raise ValueError(\n@@ -149,8 +149,10 @@ class InputLayer(base_layer.Layer):\n       else:\n         dtype = backend.dtype(input_tensor)\n     elif input_tensor is not None and input_tensor.dtype != dtype:\n-      raise ValueError('`input_tensor.dtype` differs from `dtype`: %s vs. %s' %\n-                       (input_tensor.dtype, dtype))\n+      raise ValueError(\n+          '`input_tensor.dtype` differs from `dtype`. Received: '\n+          f'input_tensor.dtype={input_tensor.dtype} '\n+          f'but expected dtype={dtype}')\n     super(InputLayer, self).__init__(dtype=dtype, name=name)\n     self.built = True\n     self.sparse = True if sparse else False\n@@ -212,8 +214,8 @@ class InputLayer(base_layer.Layer):\n         if not tf_utils.is_symbolic_tensor(input_tensor):\n           raise ValueError('You should not pass an EagerTensor to `Input`. '\n                            'For example, instead of creating an '\n-                           'InputLayer, you should instantiate your model and '\n-                           'directly call it on your input.')\n+                           '`InputLayer`, you should instantiate your model '\n+                           'and directly call it on your input.')\n       self.is_placeholder = False\n       try:\n         self._batch_input_shape = tuple(input_tensor.shape.as_list())\n@@ -357,7 +359,7 @@ def Input(  # pylint: disable=invalid-name\n   \"\"\"\n   if sparse and ragged:\n     raise ValueError(\n-        'Cannot set both sparse and ragged to True in a Keras input.')\n+        'Cannot set both `sparse` and `ragged` to `True` in a Keras `Input`.')\n \n   input_layer_config = {'name': name, 'dtype': dtype, 'sparse': sparse,\n                         'ragged': ragged, 'input_tensor': tensor,\n@@ -375,7 +377,7 @@ def Input(  # pylint: disable=invalid-name\n                      '`shape` does not include the batch '\n                      'dimension.')\n   if kwargs:\n-    raise ValueError('Unrecognized keyword arguments:', kwargs.keys())\n+    raise ValueError(f'Unrecognized keyword arguments: {list(kwargs.keys())}')\n \n   if batch_input_shape:\n     shape = batch_input_shape[1:]\n\n@@ -96,7 +96,8 @@ class InputSpec:\n       axes = axes or {}\n       self.axes = {int(k): axes[k] for k in axes}\n     except (ValueError, TypeError):\n-      raise TypeError('The keys in axes must be integers.')\n+      raise TypeError('Argument `axes` must be a dict with integer keys. '\n+                      f'Received: axes={axes}')\n \n     if self.axes and (self.ndim is not None or self.max_ndim is not None):\n       max_dim = (self.ndim if self.ndim else self.max_ndim) - 1\n\n@@ -29,7 +29,7 @@ class InputSpecTest(tf.test.TestCase):\n     input_spec.InputSpec(shape=[1, None, 2, 3], axes={3: 5, '2': 2})\n     with self.assertRaisesRegex(ValueError, 'Axis 4 is greater than'):\n       input_spec.InputSpec(shape=[1, None, 2, 3], axes={4: 5})\n-    with self.assertRaisesRegex(TypeError, 'keys in axes must be integers'):\n+    with self.assertRaisesRegex(TypeError, 'Argument `axes` must be a dict'):\n       input_spec.InputSpec(shape=[1, None, 2, 3], axes={'string': 5})\n \n \n\n@@ -358,12 +358,13 @@ class Sequential(functional.Functional):\n         # invalid use case of Sequential, but we tolerate it for backwards\n         # compatibility.\n         self._use_legacy_deferred_behavior = True\n-        self._build_input_shape = tf.nest.map_structure(_get_shape_tuple, inputs)\n+        self._build_input_shape = tf.nest.map_structure(\n+            _get_shape_tuple, inputs)\n         if tf.__internal__.tf2.enabled():\n           logging.warning('Layers in a Sequential model should only have a '\n-                          'single input tensor, but we receive a %s input: %s'\n-                          '\\nConsider rewriting this model with the Functional '\n-                          'API.' % (type(inputs), inputs))\n+                          f'single input tensor. Received: inputs={inputs}. '\n+                          'Consider rewriting this model with the Functional '\n+                          'API.')\n       else:\n         self._build_graph_network_for_inferred_shape(inputs.shape, inputs.dtype)\n \n\n@@ -374,8 +374,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       return\n \n     if input_shape is None:\n-      raise ValueError('Input shape must be defined when calling `build` on a '\n-                       'model subclass network.')\n+      raise ValueError('Input shape must be defined when calling `build()` on '\n+                       'a `Model` subclass.')\n     valid_types = (tuple, list, tf.TensorShape, dict)\n     if not isinstance(input_shape, valid_types):\n       raise ValueError('Specified input shape is not one of the valid types. '\n@@ -1566,7 +1566,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     _disallow_inside_tf_function('evaluate')\n     use_cached_eval_dataset = kwargs.pop('_use_cached_eval_dataset', False)\n     if kwargs:\n-      raise TypeError(f'Invalid keyword arguments: {(kwargs,)}')\n+      raise TypeError(f'Invalid keyword arguments: {list(kwargs.keys())}')\n \n     if self.distribute_strategy._should_use_with_coordinator:  # pylint: disable=protected-access\n       self._cluster_coordinator = tf.distribute.experimental.coordinator.ClusterCoordinator(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#854c819228e799f7568032bb9bcde5849b421f69", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 51 | Lines Deleted: 5 | Files Changed: 6 | Hunks: 13 | Methods Changed: 5 | Complexity Δ (Sum/Max): 7/2 | Churn Δ: 56 | Churn Cumulative: 1796 | Contributors (this commit): 7 | Commits (past 90d): 9 | Contributors (cumulative): 30 | DMM Complexity: 0.7333333333333333\n\nDIFF:\n@@ -97,6 +97,8 @@ def image_dataset_from_directory(directory,\n         Whether the images will be converted to\n         have 1, 3, or 4 channels.\n     batch_size: Size of the batches of data. Default: 32.\n+      If `None`, the data will not be batched\n+      (the dataset will yield individual samples).\n     image_size: Size to resize images to after they are read from disk.\n         Defaults to `(256, 256)`.\n         Since the pipeline processes batches of images that must all have\n@@ -216,10 +218,16 @@ def image_dataset_from_directory(directory,\n       num_classes=len(class_names),\n       interpolation=interpolation,\n       crop_to_aspect_ratio=crop_to_aspect_ratio)\n+  dataset = dataset.prefetch(tf.data.AUTOTUNE)\n+  if batch_size is not None:\n     if shuffle:\n       # Shuffle locally at each iteration\n       dataset = dataset.shuffle(buffer_size=batch_size * 8, seed=seed)\n-  dataset = dataset.prefetch(tf.data.AUTOTUNE).batch(batch_size)\n+    dataset = dataset.batch(batch_size)\n+  else:\n+    if shuffle:\n+      dataset = dataset.shuffle(buffer_size=len(dataset), seed=seed)\n+\n   # Users may need to reference `class_names`.\n   dataset.class_names = class_names\n   # Include file paths for images as attribute.\n\n@@ -346,6 +346,16 @@ class ImageDatasetFromDirectoryTest(keras_parameterized.TestCase):\n       _ = image_dataset.image_dataset_from_directory(\n           directory, validation_split=0.2, subset='training')\n \n+  def test_image_dataset_from_directory_not_batched(self):\n+    if PIL is None:\n+      return  # Skip test if PIL is not available.\n+\n+    directory = self._prepare_directory(num_classes=2, count=2)\n+    dataset = image_dataset.image_dataset_from_directory(\n+        directory, batch_size=None, image_size=(18, 18),\n+        label_mode=None, shuffle=False)\n+    sample = next(iter(dataset))\n+    self.assertEqual(len(sample.shape), 3)\n \n if __name__ == '__main__':\n   tf.compat.v1.enable_v2_behavior()\n\n@@ -83,6 +83,8 @@ def text_dataset_from_directory(directory,\n         to control the order of the classes\n         (otherwise alphanumerical order is used).\n     batch_size: Size of the batches of data. Default: 32.\n+      If `None`, the data will not be batched\n+      (the dataset will yield individual samples).\n     max_length: Maximum size of a text string. Texts longer than this will\n       be truncated to `max_length`.\n     shuffle: Whether to shuffle the data. Default: True.\n@@ -163,10 +165,16 @@ def text_dataset_from_directory(directory,\n       label_mode=label_mode,\n       num_classes=len(class_names),\n       max_length=max_length)\n+  dataset = dataset.prefetch(tf.data.AUTOTUNE)\n+  if batch_size is not None:\n     if shuffle:\n       # Shuffle locally at each iteration\n       dataset = dataset.shuffle(buffer_size=batch_size * 8, seed=seed)\n-  dataset = dataset.prefetch(tf.data.AUTOTUNE).batch(batch_size)\n+    dataset = dataset.batch(batch_size)\n+  else:\n+    if shuffle:\n+      dataset = dataset.shuffle(buffer_size=len(dataset), seed=seed)\n+\n   # Users may need to reference `class_names`.\n   dataset.class_names = class_names\n   return dataset\n\n@@ -248,6 +248,14 @@ class TextDatasetFromDirectoryTest(keras_parameterized.TestCase):\n       _ = text_dataset.text_dataset_from_directory(\n           directory, validation_split=0.2, subset='training')\n \n+  def test_text_dataset_from_directory_not_batched(self):\n+    directory = self._prepare_directory()\n+    dataset = text_dataset.text_dataset_from_directory(\n+        directory, batch_size=None, label_mode=None, follow_links=True)\n+\n+    sample = next(iter(dataset))\n+    self.assertEqual(len(sample.shape), 0)\n+\n \n if __name__ == '__main__':\n   tf.compat.v1.enable_v2_behavior()\n\n@@ -61,7 +61,8 @@ def timeseries_dataset_from_array(\n       `data[i], data[i + r], ... data[i + sequence_length]`\n       are used for create a sample sequence.\n     batch_size: Number of timeseries samples in each batch\n-      (except maybe the last one). If `None`, it doesn't make batches.\n+      (except maybe the last one). If `None`, the data will not be batched\n+      (the dataset will yield individual samples).\n     shuffle: Whether to shuffle output samples,\n       or instead draw them in chronological order.\n     seed: Optional int; random seed for shuffling.\n@@ -224,12 +225,15 @@ def timeseries_dataset_from_array(\n     target_ds = sequences_from_indices(\n         targets, indices, start_index, end_index)\n     dataset = tf.data.Dataset.zip((dataset, target_ds))\n+  dataset = dataset.prefetch(tf.data.AUTOTUNE)\n+  if batch_size is not None:\n     if shuffle:\n       # Shuffle locally at each iteration\n       dataset = dataset.shuffle(buffer_size=batch_size * 8, seed=seed)\n-  dataset = dataset.prefetch(tf.data.AUTOTUNE)\n-  if batch_size is not None:\n     dataset = dataset.batch(batch_size)\n+  else:\n+    if shuffle:\n+      dataset = dataset.shuffle(buffer_size=len(dataset), seed=seed)\n   return dataset\n \n \n\n@@ -174,6 +174,14 @@ class TimeseriesDatasetTest(tf.test.TestCase):\n       _ = timeseries.timeseries_dataset_from_array(\n           np.arange(10), None, 3, sequence_stride=0)\n \n+  def test_not_batched(self):\n+    data = np.arange(100)\n+    \n+    dataset = timeseries.timeseries_dataset_from_array(\n+        data, None,\n+        sequence_length=9, batch_size=None, shuffle=True)\n+    sample = next(iter(dataset))\n+    self.assertEqual(len(sample.shape), 1)\n     \n if __name__ == '__main__':\n   tf.compat.v1.enable_v2_behavior()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7ae2b2e97adc59b42749887f36462ed9ca5fb844", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 654 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -143,6 +143,12 @@ class MultiHeadAttention(Layer):\n   Finally, the result tensor with the last dimension as value_dim can take an\n   linear projection and return.\n \n+  When using MultiHeadAttention inside a custom Layer, the custom Layer must\n+  implement `build()` and call MultiHeadAttention's `_build_from_signature()`.\n+  This enables weights to be restored correctly when the model is loaded.\n+  TODO(b/172609172): link to documentation about calling custom build functions\n+  when used in a custom Layer.\n+\n   Examples:\n \n   Performs 1D cross-attention over two sequence inputs with an attention mask.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8beab0b156605add26b741f08aa49a8f94c49129", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 8 | Files Changed: 4 | Hunks: 11 | Methods Changed: 7 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 34 | Churn Cumulative: 10544 | Contributors (this commit): 26 | Commits (past 90d): 20 | Contributors (cumulative): 36 | DMM Complexity: 0.42857142857142855\n\nDIFF:\n@@ -1925,6 +1925,9 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     This function can be subclassed in a layer and will be called after updating\n     a layer weights. It can be overridden to finalize any additional layer state\n     after a weight update.\n+\n+    This function will be called after weights of a layer have been restored\n+    from a loaded model.\n     \"\"\"\n     pass\n \n\n@@ -352,15 +352,22 @@ class DiscretizationAdaptTest(keras_parameterized.TestCase,\n     new_output_data = f(tf.constant(predict_data))[\"discretization\"]\n     self.assertAllClose(new_output_data, expected_output)\n \n-  def test_saved_model_keras(self):\n+  @parameterized.product(\n+      save_format=[\"tf\", \"h5\"],\n+      adapt=[True, False],\n+  )\n+  def test_saved_model_keras(self, save_format, adapt):\n     input_data = [[1], [2], [3]]\n     predict_data = [[0.5], [1.5], [2.5]]\n     expected_output = [[0], [1], [2]]\n \n     cls = discretization.Discretization\n     inputs = keras.Input(shape=(1,), dtype=tf.float32)\n+    if adapt:\n       layer = cls(num_bins=3)\n       layer.adapt(input_data)\n+    else:\n+      layer = cls(bin_boundaries=[1.0, 2.0])\n     outputs = layer(inputs)\n     model = keras.Model(inputs=inputs, outputs=outputs)\n \n@@ -369,7 +376,7 @@ class DiscretizationAdaptTest(keras_parameterized.TestCase,\n \n     # Save the model to disk.\n     output_path = os.path.join(self.get_temp_dir(), \"tf_keras_saved_model\")\n-    model.save(output_path, save_format=\"tf\")\n+    model.save(output_path, save_format=save_format)\n     loaded_model = keras.models.load_model(\n         output_path, custom_objects={\"Discretization\": cls})\n \n\n@@ -342,17 +342,17 @@ class NormalizationAdaptTest(keras_parameterized.TestCase,\n     new_output_data = f(tf.constant(input_data))[\"normalization\"]\n     self.assertAllClose(new_output_data, expected_output)\n \n-  @parameterized.parameters(\n-      {\"adapted\": True},\n-      {\"adapted\": False},\n+  @parameterized.product(\n+      save_format=[\"tf\", \"h5\"],\n+      adapt=[True, False],\n   )\n-  def test_saved_model_keras(self, adapted):\n+  def test_saved_model_keras(self, save_format, adapt):\n     input_data = [[0.], [2.], [0.], [2.]]\n     expected_output = [[-1.], [1.], [-1.], [1.]]\n \n     cls = normalization.Normalization\n     inputs = keras.Input(shape=(1,), dtype=tf.float32)\n-    if adapted:\n+    if adapt:\n       layer = cls(axis=-1)\n       layer.adapt(input_data)\n     else:\n@@ -365,7 +365,7 @@ class NormalizationAdaptTest(keras_parameterized.TestCase,\n \n     # Save the model to disk.\n     output_path = os.path.join(self.get_temp_dir(), \"tf_keras_saved_model\")\n-    model.save(output_path, save_format=\"tf\")\n+    model.save(output_path, save_format=format)\n     loaded_model = keras.models.load_model(\n         output_path, custom_objects={\"Normalization\": cls})\n \n\n@@ -762,6 +762,10 @@ def load_weights_from_hdf5_group(f, model):\n     weight_value_tuples += zip(symbolic_weights, weight_values)\n   backend.batch_set_value(weight_value_tuples)\n \n+  # Perform any layer defined finalization of the layer state.\n+  for layer in model._flatten_layers():\n+    layer.finalize_state()\n+\n \n def load_weights_from_hdf5_group_by_name(f, model, skip_mismatch=False):\n   \"\"\"Implements name-based weight loading (instead of topological loading).\n@@ -883,6 +887,10 @@ def load_weights_from_hdf5_group_by_name(f, model, skip_mismatch=False):\n \n   backend.batch_set_value(weight_value_tuples)\n \n+  # Perform any layer defined finalization of the layer state.\n+  for layer in model._flatten_layers():\n+    layer.finalize_state()\n+\n \n def save_attributes_to_hdf5_group(group, name, data):\n   \"\"\"Saves attributes (data) of the specified name into the HDF5 group.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5171cc7925857e1f826c19ecf7f587184e1230d1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1538 | Contributors (this commit): 8 | Commits (past 90d): 8 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -107,10 +107,10 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n     standardize: Optional specification for standardization to apply to the\n       input text. Values can be:\n         - `None`: No standardization.\n-        - `lower_and_strip_punctuation`: Text will be lowercased and all\n+        - `\"lower_and_strip_punctuation\"`: Text will be lowercased and all\n           punctuation removed.\n-        - `lower`: Text will be lowercased.\n-        - `trip_punctuation`: All punctuation will be removed.\n+        - `\"lower\"`: Text will be lowercased.\n+        - `\"strip_punctuation\"`: All punctuation will be removed.\n         - Callable: Inputs will passed to the callable function, which should\n           standardized and returned.\n     split: Optional specification for splitting the input text. Values can be:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8bab6d630648f660c00585cbe68b9937f0a9a31c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 926 | Contributors (this commit): 7 | Commits (past 90d): 9 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -226,7 +226,7 @@ def image_dataset_from_directory(directory,\n     dataset = dataset.batch(batch_size)\n   else:\n     if shuffle:\n-      dataset = dataset.shuffle(buffer_size=len(dataset), seed=seed)\n+      dataset = dataset.shuffle(buffer_size=1024, seed=seed)\n \n   # Users may need to reference `class_names`.\n   dataset.class_names = class_names\n\n@@ -173,7 +173,7 @@ def text_dataset_from_directory(directory,\n     dataset = dataset.batch(batch_size)\n   else:\n     if shuffle:\n-      dataset = dataset.shuffle(buffer_size=len(dataset), seed=seed)\n+      dataset = dataset.shuffle(buffer_size=1024, seed=seed)\n \n   # Users may need to reference `class_names`.\n   dataset.class_names = class_names\n\n@@ -233,7 +233,7 @@ def timeseries_dataset_from_array(\n     dataset = dataset.batch(batch_size)\n   else:\n     if shuffle:\n-      dataset = dataset.shuffle(buffer_size=len(dataset), seed=seed)\n+      dataset = dataset.shuffle(buffer_size=1024, seed=seed)\n   return dataset\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dd3b88f2995e8a65b77b0c885bedcb45c3cbf8cd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 16229 | Contributors (this commit): 116 | Commits (past 90d): 19 | Contributors (cumulative): 116 | DMM Complexity: None\n\nDIFF:\n@@ -1752,6 +1752,10 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     Also, note the fact that test loss is not affected by\n     regularization layers like noise and dropout.\n \n+    Note: See [this FAQ entry](\n+    https://keras.io/getting_started/faq/#whats-the-difference-between-model-methods-predict-and-call)\n+    for more details about the difference between `Model` methods `predict()`\n+    and `__call__()`.\n \n     Args:\n         x: Input samples. It could be:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7d06227f3ef653a451fffe65d1d7dace7f7945df", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 5 | Methods Changed: 6 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 10 | Churn Cumulative: 19488 | Contributors (this commit): 120 | Commits (past 90d): 23 | Contributors (cumulative): 126 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2283,7 +2283,6 @@ class TestTensorBoardV2(keras_parameterized.TestCase):\n     self.assertEqual(\n         summary_file.scalars,\n         {\n-            _ObservedSummary(logdir=self.train_dir, tag='batch_loss'),\n             _ObservedSummary(logdir=self.train_dir, tag='epoch_loss'),\n             _ObservedSummary(logdir=self.validation_dir, tag='epoch_loss'),\n             _ObservedSummary(\n@@ -2341,7 +2340,6 @@ class TestTensorBoardV2(keras_parameterized.TestCase):\n         summary_file.scalars,\n         {\n             _ObservedSummary(logdir=self.train_dir, tag='epoch_loss'),\n-            _ObservedSummary(logdir=self.train_dir, tag='batch_loss'),\n             _ObservedSummary(logdir=self.train_dir, tag='epoch_learning_rate'),\n             _ObservedSummary(\n                 logdir=self.train_dir, tag='epoch_steps_per_second'),\n@@ -2509,7 +2507,6 @@ class TestTensorBoardV2(keras_parameterized.TestCase):\n             _ObservedSummary(\n                 logdir=self.validation_dir,\n                 tag='evaluation_loss_vs_iterations'),\n-            _ObservedSummary(logdir=self.train_dir, tag='batch_loss'),\n             _ObservedSummary(\n                 logdir=self.train_dir,\n                 tag='model/layer_with_summary/custom_summary'),\n\n@@ -897,7 +897,6 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       outputs = model.distribute_strategy.run(run_step, args=(data,))\n       outputs = reduce_per_replica(\n           outputs, self.distribute_strategy, reduction='first')\n-      write_scalar_summaries(outputs, step=model._train_counter)  # pylint: disable=protected-access\n       return outputs\n \n     # Special case if steps_per_execution is one.\n@@ -3144,12 +3143,6 @@ def _is_scalar(x):\n   return isinstance(x, (tf.Tensor, tf.Variable)) and x.shape.rank == 0\n \n \n-def write_scalar_summaries(logs, step):\n-  for name, value in logs.items():\n-    if _is_scalar(value):\n-      tf.summary.scalar('batch_' + name, value, step=step)\n-\n-\n def _minimum_control_deps(outputs):\n   \"\"\"Returns the minimum control dependencies to ensure step succeeded.\"\"\"\n   if tf.executing_eagerly():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2e48e8c9e85a3dbff0cbee709c5abcc4ff63f048", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 197 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 6 | Methods Changed: 17 | Complexity Δ (Sum/Max): 21/15 | Churn Δ: 198 | Churn Cumulative: 3170 | Contributors (this commit): 3 | Commits (past 90d): 12 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -143,12 +143,17 @@ class _EagerVariableStore(tf.Module):\n   Attributes:\n     vars: a dictionary with string names (same as passed in GetVar) as keys and\n       the corresponding TensorFlow Variables as values.\n+    regularizers: a dictionary with string names as keys and the corresponding\n+      callables that return losses as values.\n+    layers: a dictionary with string names as keys and the corresponding\n+      nested keras layers as values.\n   \"\"\"\n \n   def __init__(self):\n     \"\"\"Create a variable store.\"\"\"\n     self._vars = {}  # A dictionary of the stored TensorFlow variables.\n     self._regularizers = {}  # A dict mapping var names to their regularizers.\n+    self._layers = {}  # A dictionary of stored keras layers.\n     self._store_eager_variables = True\n \n   @contextlib.contextmanager\n@@ -516,6 +521,14 @@ class _EagerVariableStore(tf.Module):\n \n     return v\n \n+  def get_or_create_layer(self, name, create_layer_method):\n+    if name not in self._layers:\n+      layer = create_layer_method()\n+      self._layers[name] = layer\n+      if isinstance(layer, base_layer.Layer):\n+        self._regularizers[name] = lambda: layer.losses\n+    return self._layers[name]\n+\n   def add_regularizer(self, var, regularizer):\n     self._regularizers[var.name] = functools.partial(regularizer, var)\n \n@@ -718,7 +731,10 @@ def track_tf1_style_variables(method):\n       assign them as attributes of your layer so that Keras/Module's standard\n       object-oriented weights (and loss tracking for layers) will kick in.\n       See the intro to modules, layers, and models\n-      [guide](https://www.tensorflow.org/guide/intro_to_modules) for more info\n+      [guide](https://www.tensorflow.org/guide/intro_to_modules) for more info.\n+      As a backup, the `compat.v1.keras.utils.get_or_create_layer` method will\n+      ease tracking nested keras model weights and losses for existing TF1 code,\n+      but new code should use explicit tracking.\n \n   Args:\n     method: The method to decorate. This should belong to a custom tf.Module,\n@@ -932,3 +948,64 @@ class VariableScopeLayer(base_layer.Layer):\n   @track_tf1_style_variables\n   def call(self, *args, **kwargs):\n     return self.forward_pass(*args, **kwargs)\n+\n+\n+@keras_export(v1=[\"keras.utils.get_or_create_layer\"])\n+def get_or_create_layer(name, create_layer_method):\n+  \"\"\"Use this method to track nested keras models in a shim-decorated method.\n+\n+  This method can be used within a `tf.keras.Layer`'s methods decorated by\n+  the`track_tf1_style_variables` shim, to additionally track inner keras Model\n+  objects created within the same method. The inner model's variables and losses\n+  will be accessible via the outer model's `variables` and `losses` attributes.\n+\n+  This enables tracking of inner keras models using TF2 behaviors, with minimal\n+  changes to existing TF1-style code.\n+\n+  Example:\n+\n+  ```python\n+  class NestedLayer(tf.keras.layers.Layer):\n+\n+    def __init__(self, units, *args, **kwargs):\n+      super().__init__(*args, **kwargs)\n+      self.units = units\n+\n+    def build_model(self):\n+      inp = tf.keras.Input(shape=(5, 5))\n+      dense_layer = tf.keras.layers.Dense(\n+          10, name=\"dense\", kernel_regularizer=\"l2\",\n+          kernel_initializer=tf.compat.v1.ones_initializer())\n+      model = tf.keras.Model(inputs=inp, outputs=dense_layer(inp))\n+      return model\n+\n+    @tf.compat.v1.keras.utils.track_tf1_style_variables\n+    def call(self, inputs):\n+      model = tf.compat.v1.keras.utils.get_or_create_layer(\n+          \"dense_model\", self.build_model)\n+      return model(inputs)\n+  ```\n+  The inner model creation should be confined to its own zero-arg function,\n+  which should be passed into this method. In TF1, this method will immediately\n+  create and return the desired model, without any tracking.\n+\n+  Args:\n+    name: A name to give the nested layer to track.\n+    create_layer_method: a Callable that takes no args and returns the nested\n+    layer.\n+\n+  Returns:\n+    The created layer.\n+  \"\"\"\n+  store = vs._get_default_variable_store()  # pylint: disable=protected-access\n+  if not isinstance(store, _EagerVariableStore):\n+    if not tf.compat.v1.executing_eagerly_outside_functions():\n+      # tf1 case; just create and return layer\n+      return create_layer_method()\n+    else:\n+      raise ValueError(\n+          \"Tried to call get_or_create_layer in eager mode from a method not\"\n+          \"decorated with @tf.compat.v1.keras.utils.track_tf1_style_variables.\")\n+  vs_name = tf.compat.v1.get_variable_scope().name\n+  name = f\"{vs_name}/{name}\"\n+  return store.get_or_create_layer(name, create_layer_method)\n\n@@ -1426,5 +1426,124 @@ class TF1VariableScopeLayerTest(tf.test.TestCase, parameterized.TestCase):\n       foo(tf.ones(shape=(4, 4)))\n \n \n+class GetOrCreateLayerTest(tf.test.TestCase, parameterized.TestCase):\n+\n+  @combinations.generate(combinations.combine(mode=[\"eager\"]))\n+  def test_get_or_create_layer_eager(self):\n+\n+    class NestedLayer(variable_scope_shim.VariableScopeLayer):\n+\n+      def __init__(self, units, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.units = units\n+\n+      def build_model(self):\n+        inp = input_layer_module.Input(shape=(5, 5))\n+        dense_layer = core.Dense(\n+            10, name=\"dense\", kernel_regularizer=\"l2\",\n+            kernel_initializer=tf.compat.v1.ones_initializer())\n+        model = training_module.Model(inputs=inp, outputs=dense_layer(inp))\n+        return model\n+\n+      def forward_pass(self, inputs):\n+        model = variable_scope_shim.get_or_create_layer(\n+            \"dense_model\", self.build_model)\n+        return model(inputs)\n+\n+    # enter a variable scope to check module key naming\n+    with tf.compat.v1.variable_scope(\"test_scope\"):\n+      layer = NestedLayer(10)\n+      x = tf.ones(shape=(5, 5))\n+\n+      out1 = layer(tf.expand_dims(x, 0))\n+\n+      model1 = layer.submodules[0]._layers[\"test_scope/dense_model\"]\n+\n+      out2 = layer(tf.expand_dims(x, 0))\n+      # Verify model produces same output on successive calls with same input\n+      self.assertAllEqual(out1, out2)\n+\n+      # Verify the model used on subsequent calls is the same\n+      model2 = layer.submodules[0]._layers[\"test_scope/dense_model\"]\n+      self.assertIs(model1, model2)\n+\n+      # Verify that stored layer computes outputs and losses correctly\n+      weights = {x.name: x for x in layer.variables}\n+      self.assertEqual(weights.keys(), {\"dense/bias:0\", \"dense/kernel:0\"})\n+      self.assertAllEqual(out2, tf.ones(shape=(1, 5, 10)) * 5)\n+      self.assertAllEqual(tf.add_n(layer.losses), [0.5])\n+\n+  @combinations.generate(combinations.combine(mode=[\"eager\"]))\n+  def test_get_or_create_layer_tf_function(self):\n+\n+    class NestedLayer(variable_scope_shim.VariableScopeLayer):\n+\n+      def __init__(self, units, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.units = units\n+\n+      def build_model(self):\n+        inp = input_layer_module.Input(shape=(5, 5))\n+        dense_layer = core.Dense(\n+            10, name=\"dense\", kernel_regularizer=\"l2\",\n+            )\n+        model = training_module.Model(inputs=inp, outputs=dense_layer(inp))\n+        return model\n+\n+      def forward_pass(self, inputs):\n+        model = variable_scope_shim.get_or_create_layer(\n+            \"dense_model\", self.build_model)\n+        return model(inputs)\n+\n+    layer = NestedLayer(10)\n+\n+    @tf.function\n+    def foo(x):\n+      return layer(x), tf.add_n(layer.losses)\n+\n+    # Verify inner model is reused\n+    out1, loss1 = foo(tf.ones(shape=(5, 5)))\n+    out2, loss2 = foo(tf.ones(shape=(5, 5)))\n+    self.assertAllEqual(out1, out2)\n+    self.assertAllEqual(loss1, loss2)\n+\n+  @test_util.run_deprecated_v1\n+  def test_get_or_create_layer_graph(self):\n+\n+    class NestedLayer(object):\n+\n+      def __init__(self, units, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.units = units\n+\n+      def build_model(self):\n+        inp = input_layer_module.Input(shape=(5, 5))\n+        dense_layer = core.Dense(\n+            10, name=\"dense\", kernel_regularizer=\"l2\",\n+            kernel_initializer=tf.compat.v1.ones_initializer())\n+        model = training_module.Model(inputs=inp, outputs=dense_layer(inp))\n+        return model\n+\n+      def __call__(self, inputs):\n+        model = variable_scope_shim.get_or_create_layer(\n+            \"dense_model\", self.build_model)\n+        return model(inputs)\n+\n+    with self.cached_session():\n+      layer = NestedLayer(10)\n+      x = tf.ones(shape=(5, 5))\n+\n+      out1 = layer(tf.expand_dims(x, 0))\n+      self.evaluate(tf.compat.v1.global_variables_initializer())\n+\n+      # verify output\n+      self.assertEqual(out1.shape, tf.TensorShape([1, 5, 10]))\n+      self.assertAllEqual(out1, tf.ones(shape=(1, 5, 10)) * 5)\n+\n+      # verify variables are tracked\n+      weights = {var.name for var in tf.compat.v1.trainable_variables()}\n+      self.assertEqual(weights, {\"dense/bias:0\", \"dense/kernel:0\"})\n+\n+\n if __name__ == \"__main__\":\n   tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4bffd79e2fbae1ac58bed1b33b02e9cc80cec1cb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 218 | Lines Deleted: 36 | Files Changed: 4 | Hunks: 32 | Methods Changed: 22 | Complexity Δ (Sum/Max): 21/20 | Churn Δ: 254 | Churn Cumulative: 1560 | Contributors (this commit): 3 | Commits (past 90d): 28 | Contributors (cumulative): 6 | DMM Complexity: 0.6511627906976745\n\nDIFF:\n@@ -56,6 +56,21 @@ class Adam(optimizer.Optimizer):\n       no higher than this value.\n     global_clipnorm: float. If set, the gradient of all weights is clipped\n       so that their global norm is no higher than this value.\n+    use_ema: boolean, default to False. If True, exponential moving average\n+      (EMA) is applied. EMA consists of computing an exponential moving\n+      average of the weights of the model (as the weight values change after\n+      each training batch), and periodically overwriting the weights with\n+      their moving average.\n+    ema_momentum: float, default to 0.99. Only used if `use_ema=True`. This is\n+      the momentum to use when computing the EMA of the model's weights:\n+      `new_average = ema_momentum * old_average +\n+       (1 - ema_momentum) * current_variable_value`.\n+    ema_overwrite_frequency: int or None, default to 100. Only used if\n+      `use_ema=True`. Every ema_overwrite_frequency steps of iterations, we\n+      overwrite the model variable by its stored moving average. If None, we\n+      do not overwrite model variables in the middle of training, and users\n+      need to explicitly overwrite the model variable by calling\n+      `finalize_variable_update()`.\n     name: Optional name for the operations created when applying gradients.\n       Defaults to `\"Adam\"`.\n \n@@ -92,12 +107,18 @@ class Adam(optimizer.Optimizer):\n                clipnorm=None,\n                clipvalue=None,\n                global_clipnorm=None,\n+               use_ema=False,\n+               ema_momentum=0.99,\n+               ema_overwrite_frequency=100,\n                name='Adam'):\n     super(Adam, self).__init__(\n         name=name,\n         clipnorm=clipnorm,\n         clipvalue=clipvalue,\n-        global_clipnorm=global_clipnorm)\n+        global_clipnorm=global_clipnorm,\n+        use_ema=use_ema,\n+        ema_momentum=ema_momentum,\n+        ema_overwrite_frequency=ema_overwrite_frequency)\n     self._learning_rate = self._build_learning_rate(learning_rate)\n     self.beta_1 = beta_1\n     self.beta_2 = beta_2\n\n@@ -29,7 +29,14 @@ import tensorflow.compat.v2 as tf\n class _BaseOptimizer(tf.Module):\n   \"\"\"Optimizer base class, which only supports non-distribute use case.\"\"\"\n \n-  def __init__(self, name, clipnorm=None, clipvalue=None, global_clipnorm=None):\n+  def __init__(self,\n+               name,\n+               clipnorm=None,\n+               clipvalue=None,\n+               global_clipnorm=None,\n+               use_ema=False,\n+               ema_momentum=0.99,\n+               ema_overwrite_frequency=100):\n     \"\"\"Create a new Optimizer.\n \n     Args:\n@@ -37,14 +44,43 @@ class _BaseOptimizer(tf.Module):\n         the optimizer.\n       clipnorm: float. If set, the gradient of each weight is individually\n         clipped so that its norm is no higher than this value.\n-      clipvalue: float. If set, the gradient of each weight is clipped to be\n-        no higher than this value.\n-      global_clipnorm: float. If set, the gradient of all weights is clipped\n-        so that their global norm is no higher than this value.\n+      clipvalue: float. If set, the gradient of each weight is clipped to be no\n+        higher than this value.\n+      global_clipnorm: float. If set, the gradient of all weights is clipped so\n+        that their global norm is no higher than this value.\n+      use_ema: boolean, default to False. If True, exponential moving average\n+        (EMA) is applied. EMA consists of computing an exponential moving\n+        average of the weights of the model (as the weight values change after\n+        each training batch), and periodically overwriting the weights with\n+        their moving average.\n+      ema_momentum: float, default to 0.99. Only used if `use_ema=True`. This is\n+        the momentum to use when computing the EMA of the model's weights:\n+          `new_average = ema_momentum * old_average + (1 - ema_momentum) *\n+          current_variable_value`.\n+      ema_overwrite_frequency: int or None, default to 100. Only used if\n+        `use_ema=True`. Every ema_overwrite_frequency steps of iterations, we\n+        overwrite the model variable by its stored moving average. If None, we\n+        do not overwrite model variables in the middle of training, and users\n+        need to explicitly overwrite the model variable by calling\n+        `finalize_variable_update()`.\n     \"\"\"\n     self._name = name\n     self._clipnorm = clipnorm\n     self._global_clipnorm = global_clipnorm\n+    self._use_ema = use_ema\n+    if use_ema:\n+      # Verify the arguments related to EMA.\n+      if ema_momentum > 1 or ema_momentum < 0:\n+        raise ValueError(\"`ema_momentum` must be in the range [0, 1]. \"\n+                         f\"Received: ema_momentum={ema_momentum}\")\n+      if ema_overwrite_frequency and not isinstance(\n+          ema_overwrite_frequency, int) or ema_overwrite_frequency < 1:\n+        raise ValueError(\n+            \"`ema_overwrite_frequency` must be an integer > 1 or None. \"\n+            f\"Received: ema_overwrite_frequency={ema_overwrite_frequency}\")\n+    self._ema_momentum = ema_momentum\n+    self._ema_overwrite_frequency = ema_overwrite_frequency\n+\n     if self._clipnorm is not None and self._global_clipnorm is not None:\n       raise ValueError(f\"At most one of `clipnorm` and `global_clipnorm` can \"\n                        f\"be set. Received: clipnorm={self._clipnorm}, \"\n@@ -179,12 +215,20 @@ class _BaseOptimizer(tf.Module):\n     optimizers need to call `super().build(var_list)`.\n \n     Args:\n-      var_list: List of model variables to build optimizers on. For example,\n-        SGD optimizer with momentum will store one momentum variable\n-        corresponding to each model variable.\n+      var_list: List of model variables to build optimizers on. For example, SGD\n+        optimizer with momentum will store one momentum variable corresponding\n+        to each model variable.\n     \"\"\"\n-    if not hasattr(self, \"_index_dict\"):\n+    if getattr(self, \"_built\", False):\n+      return\n     self._build_index_dict(var_list)\n+    if self._use_ema:\n+      self._model_variables_moving_average = []\n+      for var in var_list:\n+        # Make a copy of the model variables, we will use the copy to store the\n+        # moving average of model variables.\n+        self._model_variables_moving_average.append(\n+            self.add_variable_from_reference(var, \"average\", initial_value=var))\n \n   def _build_index_dict(self, var_list):\n     \"\"\"Build variable to index dictionary.\n@@ -203,11 +247,7 @@ class _BaseOptimizer(tf.Module):\n       var_key = self._var_key(var)\n       self._index_dict[var_key] = i\n \n-  def add_variable(self,\n-                   shape,\n-                   dtype=None,\n-                   initializer=\"zeros\",\n-                   name=None):\n+  def add_variable(self, shape, dtype=None, initializer=\"zeros\", name=None):\n     \"\"\"Create an optimizer variable.\n \n     Args:\n@@ -322,6 +362,39 @@ class _BaseOptimizer(tf.Module):\n       self.update_step(grad, var)\n     self.iterations.assign_add(1)\n \n+  def _update_model_variables_moving_average(self, var_list):\n+    \"\"\"Update the stored moving average using the latest value.\"\"\"\n+    if self._use_ema:\n+      for (var, average) in zip(var_list, self._model_variables_moving_average):\n+        average.assign(self._ema_momentum * average +\n+                       (1 - self._ema_momentum) * var)\n+\n+  def _overwrite_model_variables_with_average_value(self, var_list):\n+    \"\"\"Overwrite model variables with its moving average.\"\"\"\n+    if len(var_list) != len(self._model_variables_moving_average):\n+      raise ValueError(f\"The length of model variables ({len(var_list)}) to \"\n+                       f\"override does not match the length of model variables \"\n+                       f\"stored in the optimizer \"\n+                       f\"({len(self._model_variables_moving_average)}). Please \"\n+                       f\"check if the optimizer was called on your model.\")\n+    self._overwrite_model_variables_with_average_value_helper(var_list)\n+\n+  def _overwrite_model_variables_with_average_value_helper(self, var_list):\n+    \"\"\"Helper function that overwrites model variables.\"\"\"\n+    for var, average_var in zip(var_list, self._model_variables_moving_average):\n+      var.assign(average_var)\n+\n+  def finalize_variable_values(self, var_list):\n+    \"\"\"Set the final value of model's trainable variables.\n+\n+    Sometimes there are some extra steps before ending the variable updates,\n+    such as overriding the model variables with its average value.\n+\n+    Args:\n+      var_list: list of model variables.\n+    \"\"\"\n+    self._overwrite_model_variables_with_average_value(var_list)\n+\n   def _serialize_hyperparameter(self, hyperparameter):\n     \"\"\"Serialize a hyperparameter that can be a numeric or callable.\"\"\"\n     if isinstance(hyperparameter, learning_rate_schedule.LearningRateSchedule):\n@@ -382,7 +455,14 @@ class Optimizer(_BaseOptimizer):\n   optimizer, please subclass this class instead of _BaseOptimizer.\n   \"\"\"\n \n-  def __init__(self, name, clipnorm=None, clipvalue=None, global_clipnorm=None):\n+  def __init__(self,\n+               name,\n+               clipnorm=None,\n+               clipvalue=None,\n+               global_clipnorm=None,\n+               use_ema=False,\n+               ema_momentum=0.99,\n+               ema_overwrite_frequency=100):\n     \"\"\"Create a new Optimizer.\n \n     Args:\n@@ -390,12 +470,28 @@ class Optimizer(_BaseOptimizer):\n         the optimizer.\n       clipnorm: float. If set, the gradient of each weight is individually\n         clipped so that its norm is no higher than this value.\n-      clipvalue: float. If set, the gradient of each weight is clipped to be\n-        no higher than this value.\n-      global_clipnorm: float. If set, the gradient of all weights is clipped\n-        so that their global norm is no higher than this value.\n+      clipvalue: float. If set, the gradient of each weight is clipped to be no\n+        higher than this value.\n+      global_clipnorm: float. If set, the gradient of all weights is clipped so\n+        that their global norm is no higher than this value.\n+      use_ema: boolean, default to False. If True, exponential moving average\n+        (EMA) is applied. EMA consists of computing an exponential moving\n+        average of the weights of the model (as the weight values change after\n+        each training batch), and periodically overwriting the weights with\n+        their moving average.\n+      ema_momentum: float, default to 0.99. Only used if `use_ema=True`. This is\n+        the momentum to use when computing the EMA of the model's weights:\n+          `new_average = ema_momentum * old_average + (1 - ema_momentum) *\n+          current_variable_value`.\n+      ema_overwrite_frequency: int or None, default to 100. Only used if\n+        `use_ema=True`. Every ema_overwrite_frequency steps of iterations, we\n+        overwrite the model variable by its stored moving average. If None, we\n+        do not overwrite model variables in the middle of training, and users\n+        need to explicitly overwrite the model variable by calling\n+        `finalize_variable_update()`.\n     \"\"\"\n-    super().__init__(name, clipnorm, clipvalue, global_clipnorm)\n+    super().__init__(name, clipnorm, clipvalue, global_clipnorm, use_ema,\n+                     ema_momentum, ema_overwrite_frequency)\n     self._distribution_strategy = tf.distribute.get_strategy()\n \n   def add_variable_from_reference(self,\n@@ -473,6 +569,19 @@ class Optimizer(_BaseOptimizer):\n         self._distributed_apply_gradients_fn, self._distribution_strategy,\n         grads_and_vars)\n \n+  def _overwrite_model_variables_with_average_value_helper(self, var_list):\n+    \"\"\"Helper function to _overwrite_model_variables_with_average_value.\n+\n+    This function overwrites variables on each device.\n+    Args:\n+      var_list: list of model variables.\n+    \"\"\"\n+    strategy = self._distribution_strategy\n+    # Override model variable by the stored average value on all devices.\n+    for var, average_var in zip(var_list, self._model_variables_moving_average):\n+      strategy.extended.update(\n+          var, lambda a, b: a.assign(b), args=(average_var,))\n+\n   def _distributed_apply_gradients_fn(self, distribution, grads_and_vars,\n                                       **kwargs):\n     \"\"\"`apply_gradients` using a `DistributionStrategy`.\"\"\"\n@@ -485,6 +594,18 @@ class Optimizer(_BaseOptimizer):\n           var, apply_grad_to_update_var, args=(grad,), group=False)\n     self.iterations.assign_add(1)\n \n+    if self._use_ema:\n+      _, var_list = zip(*grads_and_vars)\n+      self._update_model_variables_moving_average(var_list)\n+      should_overwrite_model_vars = (\n+          self._ema_overwrite_frequency and\n+          self.iterations % self._ema_overwrite_frequency == 0)\n+      tf.cond(\n+          should_overwrite_model_vars,\n+          true_fn=lambda: self._overwrite_model_variables_with_average_value(  # pylint: disable=g-long-lambda\n+              var_list),\n+          false_fn=lambda: None)\n+\n \n class RestoredOptimizer(Optimizer):\n \n\n@@ -41,7 +41,7 @@ adagrad_new_fn = tf.__internal__.test.combinations.NamedObject(\n adam_new_fn = tf.__internal__.test.combinations.NamedObject(\n     \"experimentaladam\", lambda: adam_new.Adam(0.002))\n sgd_new_fn = tf.__internal__.test.combinations.NamedObject(\n-    \"experimentalsgd\", lambda: sgd_new.SGD(0.002))\n+    \"experimentalsgdaverage\", lambda: sgd_new.SGD(0.002, use_ema=True))\n \n OPTIMIZER_FN = [\n     adadelta_new_fn,\n@@ -81,8 +81,10 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n \n   def testClipGlobalNorm(self):\n     optimizer = adam_new.Adam(global_clipnorm=1)\n-    grad = [tf.cast([100.0, 100.0], dtype=tf.float32),\n-            tf.cast([100.0, 100.0], dtype=tf.float32)]\n+    grad = [\n+        tf.cast([100.0, 100.0], dtype=tf.float32),\n+        tf.cast([100.0, 100.0], dtype=tf.float32)\n+    ]\n     clipped_grad = optimizer._clip_gradients(grad)\n     self.assertAllClose(clipped_grad[0], [0.5, 0.5])\n \n@@ -125,6 +127,23 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n     with self.assertRaisesRegex(TypeError, \"This optimizer was created with*\"):\n       optimizer.learning_rate = 2.0\n \n+  def testMovingAverageOptimizer(self):\n+    # We set polyak averaging with ema_momentum = 1 so that the\n+    #  moving average is always the original value of the variables.\n+    optimizer = adam_new.Adam(\n+        use_ema=True, ema_momentum=1, ema_overwrite_frequency=2)\n+    x = tf.Variable([1.0, 2.0], dtype=tf.float32)\n+    x_origin = tf.Variable(x)\n+    grads = tf.convert_to_tensor([1.0, 2.0])\n+    # First iteration, we store the moving average, and do not do overriding.\n+    optimizer.apply_gradients(zip([grads], [x]))\n+    self.assertAllEqual(optimizer._model_variables_moving_average[0], x_origin)\n+    self.assertNotAllEqual(x, x_origin)\n+\n+    # Second iteration, we store the moving average, and override model vars.\n+    optimizer.apply_gradients(zip([grads], [x]))\n+    self.assertAllEqual(x, x_origin)\n+\n   def testGetConfig(self):\n     optimizer = adam_new.Adam(\n         learning_rate=np.float64(0.05),\n@@ -216,7 +235,8 @@ class OptimizerRegressionTest(tf.test.TestCase, parameterized.TestCase):\n     grads = tf.convert_to_tensor(np.arange(0.1, 1.1, 0.1))\n     sparse_grads = tf.IndexedSlices(\n         tf.convert_to_tensor([0, 0.2, 0.4, 0.8], dtype=tf.float64),\n-        [0, 2, 4, 6], dense_shape=[len(grads)])\n+        [0, 2, 4, 6],\n+        dense_shape=[len(grads)])\n \n     for _ in range(5):\n       self.assertAllClose(x1, x2)\n@@ -233,12 +253,10 @@ class OptimizerRegressionTest(tf.test.TestCase, parameterized.TestCase):\n         adam_old.Adam(amsgrad=True), adam_new.Adam(amsgrad=True))\n \n   def testAdadelta(self):\n-    self._compare_numerical(\n-        adadelta_old.Adadelta(), adadelta_new.Adadelta())\n+    self._compare_numerical(adadelta_old.Adadelta(), adadelta_new.Adadelta())\n \n   def testAdagrad(self):\n-    self._compare_numerical(\n-        adagrad_old.Adagrad(), adagrad_new.Adagrad())\n+    self._compare_numerical(adagrad_old.Adagrad(), adagrad_new.Adagrad())\n \n   @parameterized.product(nesterov=[True, False])\n   def testSgd(self, nesterov):\n@@ -250,8 +268,7 @@ class DistributedTrainingTest(tf.test.TestCase, parameterized.TestCase):\n \n   @ds_combinations.generate(\n       tf.__internal__.test.combinations.combine(\n-          strategy=STRATEGIES,\n-          optimizer_fn=OPTIMIZER_FN))\n+          strategy=STRATEGIES, optimizer_fn=OPTIMIZER_FN))\n   def testGetGradientsInModel(self, strategy, optimizer_fn):\n     with strategy.scope():\n       model = keras.Sequential(\n@@ -271,13 +288,11 @@ class DistributedTrainingTest(tf.test.TestCase, parameterized.TestCase):\n           self.evaluate(optimizer._accumulated_grads._storage[0]), 0)\n     elif optimizer._name == \"Adagrad\":\n       # Assert the accumulated variable is not 0.\n-      self.assertNotEqual(\n-          self.evaluate(optimizer._accumulators._storage[0]), 0)\n+      self.assertNotEqual(self.evaluate(optimizer._accumulators._storage[0]), 0)\n \n   @ds_combinations.generate(\n       tf.__internal__.test.combinations.combine(\n-          strategy=STRATEGIES,\n-          optimizer_fn=OPTIMIZER_FN))\n+          strategy=STRATEGIES, optimizer_fn=OPTIMIZER_FN))\n   def testGetGradientsInCustomTrainingLoop(self, strategy, optimizer_fn):\n     with strategy.scope():\n       model = keras.Sequential(\n@@ -286,16 +301,20 @@ class DistributedTrainingTest(tf.test.TestCase, parameterized.TestCase):\n       optimizer = optimizer_fn()\n \n       def per_worker_dataset_fn():\n+\n         def dataset_fn(_):\n           x, y = [1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0]\n           ds = tf.data.Dataset.from_tensor_slices((x, y))\n           ds = ds.repeat().batch(6)\n           return ds\n+\n         return strategy.distribute_datasets_from_function(dataset_fn)\n+\n       ds = per_worker_dataset_fn()\n \n       @tf.function\n       def train_step(ds):\n+\n         def replica_fn(data):\n           features, labels = data\n           with tf.GradientTape() as tape:\n\n@@ -60,6 +60,21 @@ class SGD(optimizer.Optimizer):\n       no higher than this value.\n     global_clipnorm: float. If set, the gradient of all weights is clipped\n       so that their global norm is no higher than this value.\n+    use_ema: boolean, default to False. If True, exponential moving average\n+      (EMA) is applied. EMA consists of computing an exponential moving\n+      average of the weights of the model (as the weight values change after\n+      each training batch), and periodically overwriting the weights with\n+      their moving average.\n+    ema_momentum: float, default to 0.99. Only used if `use_ema=True`. This is\n+      the momentum to use when computing the EMA of the model's weights:\n+      `new_average = ema_momentum * old_average +\n+       (1 - ema_momentum) * current_variable_value`.\n+    ema_overwrite_frequency: int or None, default to 100. Only used if\n+      `use_ema=True`.Every ema_overwrite_frequency steps of iterations, we\n+      overwrite the model variable by its stored moving average. If None, we\n+      do not overwrite model variables in the middle of training, and users\n+      need to explicitly overwrite the model variable by calling\n+      `finalize_variable_update()`.\n     name: Optional name prefix for the operations created when applying\n       gradients.  Defaults to `\"SGD\"`.\n \n@@ -101,12 +116,18 @@ class SGD(optimizer.Optimizer):\n                clipnorm=None,\n                clipvalue=None,\n                global_clipnorm=None,\n+               use_ema=False,\n+               ema_momentum=0.99,\n+               ema_overwrite_frequency=100,\n                name='SGD'):\n     super(SGD, self).__init__(\n         name=name,\n         clipnorm=clipnorm,\n         clipvalue=clipvalue,\n-        global_clipnorm=global_clipnorm)\n+        global_clipnorm=global_clipnorm,\n+        use_ema=use_ema,\n+        ema_momentum=ema_momentum,\n+        ema_overwrite_frequency=ema_overwrite_frequency)\n     self._learning_rate = self._build_learning_rate(learning_rate)\n     self.momentum = momentum\n     self.nesterov = nesterov\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
